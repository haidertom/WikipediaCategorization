{"id": "44990517", "url": "https://en.wikipedia.org/wiki?curid=44990517", "title": "Amari distance", "text": "Amari distance\n\nThe Amari distance is a measure between two nonsingular matrices, useful for checking for convergence in independent component analysis algorithms and for comparing solutions.\n"}
{"id": "1525019", "url": "https://en.wikipedia.org/wiki?curid=1525019", "title": "Apostolos Doxiadis", "text": "Apostolos Doxiadis\n\nApostolos K. Doxiadis (; born 1953) is a Greek writer. He is best known for his international bestsellers \"Uncle Petros and Goldbach's Conjecture\" (2000) and \"Logicomix\" (2009).\n\nDoxiadis was born in Australia, where his father, the architect Constantinos Apostolou Doxiadis was working. Soon after his birth, the family returned to Athens, where Doxiadis grew up. Though his earliest interests were in poetry, fiction and the theatre, an intense interest in mathematics led Doxiadis to leave school at age fifteen, to attend Columbia University, in New York, from which he obtained a bachelor's degree in Mathematics in May 1972. He then attended the École Pratique des Hautes Études in Paris from which he got a master's degree, with a thesis on the mathematical modeling of the nervous system. His father’s death and family reasons made him return to Greece in 1975, interrupting his graduate studies. In Greece, although involved for some years with the computer software industry, Doxiadis returned to his childhood and adolescence loves of theatre and the cinema, before becoming a full-time writer.\n\nDoxiadis began to write in Greek. His first published work was \"A Parallel Life\" (\"Βίος Παράλληλος\", 1985), a novella set in the monastic communities of 4th century CE Egypt. His first novel, \"Makavettas\" (\"Μακαβέττας\", 1988), recounted the adventures of a fictional power-hungry colonel at the time of the Greek military junta of 1967–1974. Written in a tongue-in-cheek imitation of Greek folk military memoirs, such as that of Yannis Makriyannis, it follows the plot of Shakespeare’s \"Macbeth\", of which the eponymous hero’s name is a Hellenized form. Doxiadis next novel, \"Uncle Petros and Goldbach’s Conjecture\" (\"Ο Θείος Πέτρος και η Εικασία του Γκόλντμπαχ\", 1992), was the first long work of fiction whose plot takes place in the world of pure mathematics research. The first Greek critics did not find the mathematical themes appealing, and it received mediocre reviews, unlike Doxiadis’s first two works, which were well received. Τhe novella \"The Three Little Men\" (\"Τα Τρία Ανθρωπάκια\", 1998), attempts a modern-day retelling of the tale of a classic fairy-tale.\n\nIn 1998, Doxiadis translated into English, significantly re-working, his third novel, which was published in England in 2000 as \"Uncle Petros and Goldbach's Conjecture\" (UK publisher: Faber and Faber; United States publisher: Bloomsbury USA.) The book became an international bestseller, and has been published to date in more than thirty-five languages. It has received the praise of, among others, Nobel Laureate John Nash, British mathematician Sir Michael Atiyah, critic George Steiner and psychiatrist Oliver Sacks. \"Uncle Petros\" is one of the \"1001 Books You Must Read Before You Die\".\nDoxiadis’ next project, which took over five years to complete, was the graphic novel \"Logicomix\" (2009), a number one bestseller on the New York Times Bestseller List and an international bestseller, already published in over twenty languages. \"Logicomix\" was co-authored with computer scientist Christos Papadimitriou, with art work by Alecos Papadatos (pencils) and Annie Di Donna (color). Renowned comics historian and critic R. C. Harvey, in the \"Comics Journal\", called \"Logicomix\" “a tour-de-force” a “virtuoso performance”, while \"The Sunday Times\"’ Brian Appleyard called it “probably the best and certainly the most extraordinary graphic novel” he has read. \"Logicomix\" is one of Paul Gravett’s \"1001 Comics You Must Read Before you Die.\" \n\nIn the early stage of his career, Doxiadis directed in the professional theatre, in Athens, and worked as translator, translating, among other plays, William Shakespeare’s \"Romeo and Juliet\", \"Hamlet\" and \"Midsummer Night’s Dream\", as well as Eugene O’Neill’s \"Mourning Becomes Electra\".\n\nHe has written two plays for the theatre. The first was a full-length shadow-puppet play \"The Tragical History of Jackson Pollock, Abstract Expressionist\" (1999), in English, of which he also designed and directed the Athens performance. In this play, Doxiadis realized some of his views on “epic theatre”, in other words a theatre based on storytelling. His second play, \"Incompleteness\" (2005), is an imaginary account of the last seventeen days in the life of the great logician Kurt Gödel, which Gödel spent in a Princeton, New Jersey, hospital, refusing to eat out of fear that he was being poisoned. The play was staged in Athens, in 2006, as Dekati Evdomi Nyhta (Seventeenth Night) with the actor Yorgos Kotanidis in the role of Kurt Gödel.\n\nDoxiadis has also written and directed two feature-length films, in Greek, \"Underground Passage\" (\"Υπόγεια Διαδρομή\", 1983) and \"Terirem\" (\"Τεριρέμ\", 1987). The latter won the CICAE (International Confederation of Art Cinemas) prize for Best Film in the 1988 Berlin International Film Festival.\n\nDoxiadis has a lifelong interest in logic, cognitive psychology and rhetoric, as well as the theoretical study of narrative. In 2007, he organized, with mathematician Barry Mazur, a meeting on the theoretical investigation of the relationship of mathematics and narrative, whose proceedings were published as \"Circles Disturbed, The Interplay of Mathematics and Narrative\" (2012). Doxiadis has lectured extensively on his theoretical interests. Doxiadis’ recent work has led him to formulate a theory about the development of deductive proof in Classical Greece, which lays emphasis on influences from pre-existing patterns in narrative and, especially, Archaic Age Poetry.\n\n\"Uncle Petros and Goldbach’s Conjecture\" was the first recipient of the Premio Peano the first international award for books inspired by mathematics and short-listed for the Prix Médicis. \"Logicomix\" has earned numerous awards, among them the Bertrand Russell Society Award, the Royal Booksellers Association Award (the Netherlands), the New Atlantic Booksellers Award (USA), the Prix Tangente (France), the Premio Carlo Boscarato (Italy), the Comicdom Award (Greece). It was chosen as \"Book of the Year\" by \"TIME Magazine\", \"Publishers Weekly\", \"The Washington Post\", \"The Financial Times\", \"The Globe and Mail\", and other publications.\n\n"}
{"id": "3785849", "url": "https://en.wikipedia.org/wiki?curid=3785849", "title": "Apotome (mathematics)", "text": "Apotome (mathematics)\n\nIn the historical study of mathematics, an apotome is a line segment formed from a longer line segment by breaking it into two parts, one of which is commensurable only in power to the whole; the other part is the apotome. In this definition, two line segments are said to be \"commensurable only in power\" when the ratio of their lengths is an irrational number but the ratio of their squared lengths is rational.\n\nTranslated into modern algebraic language, an apotome can be interpreted as a quadratic irrational number formed by subtracting one square root of a rational number from another.\nThis concept of the apotome appears in Euclid's Elements beginning in book X, where Euclid defines two special kinds of apotomes. In an apotome of the first kind, the whole is rational, while in an apotome of the second kind, the part subtracted from it is rational; both kinds of apotomes also satisfy an additional condition. Euclid Proposition XIII.6 states that, if a rational line segment is split into two pieces in the golden ratio, then both pieces may be represented as apotomes.\n"}
{"id": "9858698", "url": "https://en.wikipedia.org/wiki?curid=9858698", "title": "Association for Women in Mathematics", "text": "Association for Women in Mathematics\n\nThe Association for Women in Mathematics (AWM) is a professional society whose mission is to encourage women and girls to study and to have active careers in the mathematical sciences, and to promote equal opportunity for and the equal treatment of women and girls in the mathematical sciences. The AWM was founded in 1971 and incorporated in the state of Massachusetts. AWM has approximately 5200 members, including over 250 institutional members, such as colleges, universities, institutes, and mathematical societies. It offers numerous programs and workshops to mentor women and girls in the mathematical sciences. Much of AWM’s work is supported through federal grants.\n\nThe Association was founded in 1971 as the Association of Women Mathematicians, but the name was changed almost immediately. As reported in \"A Brief History of the Association for Women in Mathematics: The Presidents' Perspectives\", by Lenore Blum, \"As Judy Green remembers (and Chandler Davis, early AWM friend, concurs): 'The formal idea of women getting together and forming a caucus was first made publicly at a MAG [Mathematics Action Group] meeting in 1971 ... in Atlantic City. Joanne Darken, then an instructor at Temple University and now at the Community College of Philadelphia, stood up at the meeting and suggested that the women present remain and form a caucus. I have been able to document six women who remained: me (I was a graduate student at Maryland at the time), Joanne Darken, Mary [W.] Gray (she was already at American University), Diane Laison (then an instructor at Temple), Gloria Olive (a Senior Lecturer at the University of Otago, New Zealand who was visiting the U.S. at the time) and Annie Selden... It's not absolutely clear what happened next, except that I've personally always thought that Mary was responsible for getting the whole thing organized ...'\" Mary W. Gray was the early organizer, placing an advertisement in the February 1971 Notices of the AMS, and writing the first issue of the \"AWM Newsletter\" that May. Early goals of the association focused on equal pay for equal work, as well as equal consideration for admission to graduate school and support while there; for faculty appointments at all levels; for promotion and for tenure; for administrative appointments; and for government grants, positions on review and advisory panels and positions in professional organizations. The AWM holds an annual meeting at the Joint Mathematics Meetings. In 2011 the association initiated a biennial Research Symposium during its 40th anniversary celebration 40 Years and Counting.\n\nThe AWM sponsors three honorary lecture series.\n\n\nThe AWM sponsors several awards and prizes.\n\n\nThree recently created prizes for early-career women are also sponsored by the AWM.\n\n\nThe AWM Fellows program recognizes \"individuals who have demonstrated a sustained commitment to the support and advancement of women in the mathematical sciences\".\n\n\n\n"}
{"id": "2033586", "url": "https://en.wikipedia.org/wiki?curid=2033586", "title": "Back-and-forth method", "text": "Back-and-forth method\n\nIn mathematical logic, especially set theory and model theory, the back-and-forth method is a method for showing isomorphism between countably infinite structures satisfying specified conditions. In particular:\n\n\nSuppose that\n\n\nFix enumerations (without repetition) of the underlying sets:\n\nNow we construct a one-to-one correspondence between \"A\" and \"B\" that is strictly increasing. Initially no member of \"A\" is paired with any member of \"B\".\n\nIt still has to be checked that the choice required in step (1) and (2) can actually be made in accordance to the requirements. Using step (1) as an example:\n\nIf there are already \"a\" and \"a\" in \"A\" corresponding to \"b\" and \"b\" in \"B\" respectively such that \"a\" < \"a\" < \"a\" and \"b\" < \"b\", we choose \"b\" in between \"b\" and \"b\" using density. Otherwise, we choose a suitable large or small element of \"B\" using the fact that \"B\" has neither a maximum nor a minimum. Choices made in step (2) are dually possible. Finally, the construction ends after countably many steps because \"A\" and \"B\" are countably infinite. Note that we had to use all the prerequisites.\n\nAccording to Hodges (1993):\nWhile the theorem on countable densely ordered sets is due to Cantor (1895), the back-and-forth method with which it is now proved was developed by Huntington (1904) and Hausdorff (1914). Later it was applied in other situations, most notably by Roland Fraïssé in model theory.\n\n\n"}
{"id": "7593", "url": "https://en.wikipedia.org/wiki?curid=7593", "title": "Calculator", "text": "Calculator\n\nAn electronic calculator is typically a portable electronic device used to perform calculations, ranging from basic arithmetic to complex mathematics.\n\nThe first solid-state electronic calculator was created in the early 1960s. Pocket-sized devices became available in the 1970s, especially after the Intel 4004, the first microprocessor, was developed by Intel for the Japanese calculator company Busicom. They later became used commonly within the petroleum industry (oil and gas).\n\nModern electronic calculators vary from cheap, give-away, credit-card-sized models to sturdy desktop models with built-in printers. They became popular in the mid-1970s as the incorporation of integrated circuits reduced their size and cost. By the end of that decade, prices had dropped to the point where a basic calculator was affordable to most and they became common in schools.\n\nComputer operating systems as far back as early Unix have included interactive calculator programs such as dc and hoc, and calculator functions are included in almost all personal digital assistant (PDA) type devices, the exceptions being a few dedicated address book and dictionary devices.\n\nIn addition to general purpose calculators, there are those designed for specific markets. For example, there are scientific calculators which include trigonometric and statistical calculations. Some calculators even have the ability to do computer algebra. Graphing calculators can be used to graph functions defined on the real line, or higher-dimensional Euclidean space. , basic calculators cost little, but scientific and graphing models tend to cost more.\n\nIn 1986, calculators still represented an estimated 41% of the world's general-purpose hardware capacity to compute information. By 2007, this diminished to less than 0.05%.\n\nElectronic calculators contain a keyboard with buttons for digits and arithmetical operations; some even contain \"00\" and \"000\" buttons to make larger or smaller numbers easier to enter. Most basic calculators assign only one digit or operation on each button; however, in more specific calculators, a button can perform multi-function working with key combinations.\n\nCalculators usually have liquid-crystal displays (LCD) as output in place of historical light-emitting diode (LED) displays and vacuum fluorescent displays (VFD); details are provided in the section \"Technical improvements\".\n\nLarge-sized figures are often used to improve readability; while using decimal separator (usually a point rather than a comma) instead of or in addition to vulgar fractions. Various symbols for function commands may also be shown on the display. Fractions such as are displayed as decimal approximations, for example rounded to . Also, some fractions (such as , which is ; to 14 significant figures) can be difficult to recognize in decimal form; as a result, many scientific calculators are able to work in vulgar fractions or mixed numbers.\n\nCalculators also have the ability to store numbers into computer memory. Basic calculators usually store only one number at a time; more specific types are able to store many numbers represented in variables. The variables can also be used for constructing formulas. Some models have the ability to extend memory capacity to store more numbers; the extended memory address is termed an array index.\n\nPower sources of calculators are: batteries, solar cells or mains electricity (for old models), turning on with a switch or button. Some models even have no turn-off button but they provide some way to put off (for example, leaving no operation for a moment, covering solar cell exposure, or closing their lid). Crank-powered calculators were also common in the early computer era.\n\nThe following keys are common to most pocket calculators. While the arrangement of the digits is standard, the positions of other keys vary from model to model; the illustration is an example.\n\nIn general, a basic electronic calculator consists of the following components:\n\n\nClock rate of a processor chip refers to the frequency at which the central processing unit (CPU) is running. It is used as an indicator of the processor's speed, and is measured in \"clock cycles per second\" or the SI unit hertz (Hz). For basic calculators, the speed can vary from a few hundred hertz to the kilohertz range.\nA basic explanation as to how calculations are performed in a simple four-function calculator:\n\nTo perform the calculation , one presses keys in the following sequence on most calculators:     .\n\nOther functions are usually performed using repeated additions or subtractions.\n\nMost pocket calculators do all their calculations in BCD rather than a floating-point representation. BCD is common in electronic systems where a numeric value is to be displayed, especially in systems consisting solely of digital logic, and not containing a microprocessor. By employing BCD, the manipulation of numerical data for display can be greatly simplified by treating each digit as a separate single sub-circuit. This matches much more closely the physical reality of display hardware—a designer might choose to use a series of separate identical seven-segment displays to build a metering circuit, for example. If the numeric quantity were stored and manipulated as pure binary, interfacing to such a display would require complex circuitry. Therefore, in cases where the calculations are relatively simple, working throughout with BCD can lead to a simpler overall system than converting to and from binary.\n\nThe same argument applies when hardware of this type uses an embedded microcontroller or other small processor. Often, smaller code results when representing numbers internally in BCD format, since a conversion from or to binary representation can be expensive on such limited processors. For these applications, some small processors feature BCD arithmetic modes, which assist when writing routines that manipulate BCD quantities.\n\nWhere calculators have added functions (such as square root, or trigonometric functions), software algorithms are required to produce high precision results. Sometimes significant design effort is needed to fit all the desired functions in the limited memory space available in the calculator chip, with acceptable calculation time.\n\nThe fundamental difference between a calculator and computer is that a computer can be programmed in a way that allows the program to take different branches according to intermediate results, while calculators are pre-designed with specific functions (such as addition, multiplication, and logarithms) built in. The distinction is not clear-cut: some devices classed as programmable calculators have programming functions, sometimes with support for programming languages (such as RPL or TI-BASIC).\n\nFor instance, instead of a hardware multiplier, a calculator might implement floating point mathematics with code in read-only memory (ROM), and compute trigonometric functions with the CORDIC algorithm because CORDIC does not require much multiplication. Bit serial logic designs are more common in calculators whereas bit parallel designs dominate general-purpose computers, because a bit serial design minimizes chip complexity, but takes many more clock cycles. This distinction blurs with high-end calculators, which use processor chips associated with computer and embedded systems design, more so the Z80, MC68000, and ARM architectures, and some custom designs specialized for the calculator market.\n\nThe first known tools used to aid arithmetic calculations were: bones (used to tally items), pebbles, and counting boards, and the abacus, known to have been used by Sumerians and Egyptians before 2000 BC. Except for the Antikythera mechanism (an \"out of the time\" astronomical device), development of computing tools arrived near the start of the 17th century: the geometric-military compass (by Galileo), logarithms and Napier bones (by Napier), and the slide rule (by Edmund Gunter).\nIn 1642, the Renaissance saw the invention of the mechanical calculator (by Wilhelm Schickard and several decades later Blaise Pascal), a device that was at times somewhat over-promoted as being able to perform all four arithmetic operations with minimal human intervention. Pascal's calculator could add and subtract two numbers directly and thus, if the tedium could be borne, multiply and divide by repetition. Schickard's machine, constructed several decades earlier, used a clever set of mechanised multiplication tables to ease the process of multiplication and division with the adding machine as a means of completing this operation. (Because they were different inventions with different aims a debate about whether Pascal or Schickard should be credited as the \"inventor\" of the adding machine (or calculating machine) is probably pointless.) Schickard and Pascal were followed by Gottfried Leibniz who spent forty years designing a four-operation mechanical calculator, the stepped reckoner, inventing in the process his leibniz wheel, but who couldn't design a fully operational machine. There were also five unsuccessful attempts to design a calculating clock in the 17th century.\nThe 18th century saw the arrival of some notable improvements, first by Poleni with the first fully functional calculating clock and four-operation machine, but these machines were almost always \"one of the kind\". Luigi Torchi invented the first direct multiplication machine in 1834: this was also the second key-driven machine in the world, following that of James White (1822). It was not until the 19th century and the Industrial Revolution that real developments began to occur. Although machines capable of performing all four arithmetic functions existed prior to the 19th century, the refinement of manufacturing and fabrication processes during the eve of the industrial revolution made large scale production of more compact and modern units possible. The Arithmometer, invented in 1820 as a four-operation mechanical calculator, was released to production in 1851 as an adding machine and became the first commercially successful unit; forty years later, by 1890, about 2,500 arithmometers had been sold plus a few hundreds more from two arithmometer clone makers (Burkhardt, Germany, 1878 and Layton, UK, 1883) and Felt and Tarrant, the only other competitor in true commercial production, had sold 100 comptometers.\n\nIt wasn't until 1902 that the familiar push-button user interface was developed, with the introduction of the Dalton Adding Machine, developed by James L. Dalton in the United States.\n\nIn 1921, Edith Clarke invented the \"Clarke calculator\", a simple graph-based calculator for solving line equations involving hyperbolic functions. This allowed electrical engineers to simplify calculations for inductance and capacitance in power transmission lines.\n\nThe Curta calculator was developed in 1948 and, although costly, became popular for its portability. This purely mechanical hand-held device could do addition, subtraction, multiplication and division. By the early 1970s electronic pocket calculators ended manufacture of mechanical calculators, although the Curta remains a popular collectable item.\n\nThe first mainframe computers, using firstly vacuum tubes and later transistors in the logic circuits, appeared in the 1940s and 1950s. This technology was to provide a stepping stone to the development of electronic calculators.\n\nThe Casio Computer Company, in Japan, released the Model \"14-A\" calculator in 1957, which was the world's first all-electric (relatively) compact calculator. It did not use electronic logic but was based on relay technology, and was built into a desk.\nIn October 1961, the world's first \"all-electronic desktop\" calculator, the British Bell Punch/Sumlock Comptometer ANITA (A New Inspiration To Arithmetic/Accounting) was announced. This machine used vacuum tubes, cold-cathode tubes and Dekatrons in its circuits, with 12 cold-cathode \"Nixie\" tubes for its display. Two models were displayed, the Mk VII for continental Europe and the Mk VIII for Britain and the rest of the world, both for delivery from early 1962. The Mk VII was a slightly earlier design with a more complicated mode of multiplication, and was soon dropped in favour of the simpler Mark VIII. The ANITA had a full keyboard, similar to mechanical comptometers of the time, a feature that was unique to it and the later Sharp CS-10A among electronic calculators. The ANITA weighed roughly due to its large tube system. Bell Punch had been producing key-driven mechanical calculators of the comptometer type under the names \"Plus\" and \"Sumlock\", and had realised in the mid-1950s that the future of calculators lay in electronics. They employed the young graduate Norbert Kitz, who had worked on the early British Pilot ACE computer project, to lead the development. The ANITA sold well since it was the only electronic desktop calculator available, and was silent and quick.\n\nThe tube technology of the ANITA was superseded in June 1963 by the U.S. manufactured Friden EC-130, which had an all-transistor design, a stack of four 13-digit numbers displayed on a cathode ray tube (CRT), and introduced Reverse Polish Notation (RPN) to the calculator market for a price of $2200, which was about three times the cost of an electromechanical calculator of the time. Like Bell Punch, Friden was a manufacturer of mechanical calculators that had decided that the future lay in electronics. In 1964 more all-transistor electronic calculators were introduced: Sharp introduced the CS-10A, which weighed and cost 500,000 yen ($), and Industria Macchine Elettroniche of Italy introduced the IME 84, to which several extra keyboard and display units could be connected so that several people could make use of it (but apparently not at the same time).\n\nThere followed a series of electronic calculator models from these and other manufacturers, including Canon, Mathatronics, Olivetti, SCM (Smith-Corona-Marchant), Sony, Toshiba, and Wang. The early calculators used hundreds of germanium transistors, which were cheaper than silicon transistors, on multiple circuit boards. Display types used were CRT, cold-cathode Nixie tubes, and filament lamps. Memory technology was usually based on the delay line memory or the magnetic core memory, though the Toshiba \"Toscal\" BC-1411 appears to have used an early form of dynamic RAM built from discrete components. Already there was a desire for smaller and less power-hungry machines.\n\nThe Olivetti Programma 101 was introduced in late 1965; it was a stored program machine which could read and write magnetic cards and displayed results on its built-in printer. Memory, implemented by an acoustic delay line, could be partitioned between program steps, constants, and data registers. Programming allowed conditional testing and programs could also be overlaid by reading from magnetic cards. It is regarded as the first personal computer produced by a company (that is, a desktop electronic calculating machine programmable by non-specialists for personal use). The Olivetti Programma 101 won many industrial design awards.\nAnother calculator introduced in 1965 was Bulgaria's ELKA 6521, developed by the Central Institute for Calculation Technologies and built at the Elektronika factory in Sofia. The name derives from \"ELektronen KAlkulator\", and it weighed around . It is the first calculator in the world which includes the square root function. Later that same year were released the ELKA 22 (with a luminescent display) and the ELKA 25, with an in-built printer. Several other models were developed until the first pocket model, the ELKA 101, was released in 1974. The writing on it was in Roman script, and it was exported to western countries.\n\nThe \"Monroe Epic\" programmable calculator came on the market in 1967. A large, printing, desk-top unit, with an attached floor-standing logic tower, it could be programmed to perform many computer-like functions. However, the only \"branch\" instruction was an implied unconditional branch (GOTO) at the end of the operation stack, returning the program to its starting instruction. Thus, it was not possible to include any conditional branch (IF-THEN-ELSE) logic. During this era, the absence of the conditional branch was sometimes used to distinguish a programmable calculator from a computer.\n\nThe first handheld calculator was a prototype called \"Cal Tech\", whose development was led by Jack Kilby at Texas Instruments in 1967. It could add, multiply, subtract, and divide, and its output device was a paper tape.\n\nThe electronic calculators of the mid-1960s were large and heavy desktop machines due to their use of hundreds of transistors on several circuit boards with a large power consumption that required an AC power supply. There were great efforts to put the logic required for a calculator into fewer and fewer integrated circuits (chips) and calculator electronics was one of the leading edges of semiconductor development. U.S. semiconductor manufacturers led the world in large scale integration (LSI) semiconductor development, squeezing more and more functions into individual integrated circuits. This led to alliances between Japanese calculator manufacturers and U.S. semiconductor companies: Canon Inc. with Texas Instruments, Hayakawa Electric (later renamed Sharp Corporation) with North-American Rockwell Microelectronics (later renamed Rockwell International), Busicom with Mostek and Intel, and General Instrument with Sanyo.\n\nBy 1970, a calculator could be made using just a few chips of low power consumption, allowing portable models powered from rechargeable batteries. The first portable calculators appeared in Japan in 1970, and were soon marketed around the world. These included the Sanyo ICC-0081 \"Mini Calculator\", the Canon Pocketronic, and the Sharp QT-8B \"micro Compet\". The Canon Pocketronic was a development of the \"Cal-Tech\" project which had been started at Texas Instruments in 1965 as a research project to produce a portable calculator. The Pocketronic has no traditional display; numerical output is on thermal paper tape. As a result of the \"Cal-Tech\" project, Texas Instruments was granted master patents on portable calculators.\n\nSharp put in great efforts in size and power reduction and introduced in January 1971 the Sharp EL-8, also marketed as the Facit 1111, which was close to being a pocket calculator. It weighed 1.59 pounds (721 grams), had a vacuum fluorescent display, rechargeable NiCad batteries, and initially sold for US $395.\n\nHowever, the efforts in integrated circuit development culminated in the introduction in early 1971 of the first \"calculator on a chip\", the MK6010 by Mostek, followed by Texas Instruments later in the year. Although these early hand-held calculators were very costly, these advances in electronics, together with developments in display technology (such as the vacuum fluorescent display, LED, and LCD), led within a few years to the cheap pocket calculator available to all.\n\nIn 1971 Pico Electronics. and General Instrument also introduced their first collaboration in ICs, a full single chip calculator IC for the Monroe Royal Digital III calculator. Pico was a spinout by five GI design engineers whose vision was to create single chip calculator ICs. Pico and GI went on to have significant success in the burgeoning handheld calculator market.\n\nThe first truly pocket-sized electronic calculator was the Busicom LE-120A \"HANDY\", which was marketed early in 1971. Made in Japan, this was also the first calculator to use an LED display, the first hand-held calculator to use a single integrated circuit (then proclaimed as a \"calculator on a chip\"), the Mostek MK6010, and the first electronic calculator to run off replaceable batteries. Using four AA-size cells the LE-120A measures .\n\nThe first European-made pocket-sized calculator, DB 800 is made in May 1971 by Digitron in Buje, Croatia (former Yugoslavia) with four functions and an eight-digit display and special characters for a negative number and a warning that the calculation has too many digits to display.\n\nThe first American-made pocket-sized calculator, the Bowmar 901B (popularly termed \"The Bowmar Brain\"), measuring , came out in the Autumn of 1971, with four functions and an eight-digit red LED display, for $240, while in August 1972 the four-function Sinclair Executive became the first slimline pocket calculator measuring and weighing . It retailed for around £79 ($). By the end of the decade, similar calculators were priced less than £5 ($).\n\nThe first Soviet Union made pocket-sized calculator, the \"Elektronika B3-04\" was developed by the end of 1973 and sold at the start of 1974.\n\nOne of the first low-cost calculators was the Sinclair Cambridge, launched in August 1973. It retailed for £29.95 ($), or £5 ($) less in kit form. The Sinclair calculators were successful because they were far cheaper than the competition; however, their design led to slow and inaccurate computations of transcendental functions.\n\nMeanwhile, Hewlett-Packard (HP) had been developing a pocket calculator. Launched in early 1972, it was unlike the other basic four-function pocket calculators then available in that it was the first pocket calculator with \"scientific\" functions that could replace a slide rule. The $395 HP-35, along with nearly all later HP engineering calculators, used reverse Polish notation (RPN), also called postfix notation. A calculation like \"8 plus 5\" is, using RPN, performed by pressing , , , and ; instead of the algebraic infix notation: , , , . It had 35 buttons and was based on Mostek Mk6020 chip.\n\nThe first Soviet \"scientific\" pocket-sized calculator the \"B3-18\" was completed by the end of 1975.\n\nIn 1973, Texas Instruments (TI) introduced the SR-10, (\"SR\" signifying slide rule) an \"algebraic entry\" pocket calculator using scientific notation for $150. Shortly after the SR-11 featured an added key for entering Pi (π). It was followed the next year by the SR-50 which added log and trig functions to compete with the HP-35, and in 1977 the mass-marketed TI-30 line which is still produced.\n\nIn 1978 a new company, Calculated Industries arose which focused on specialized markets. Their first calculator, the Loan Arranger (1978) was a pocket calculator marketed to the Real Estate industry with preprogrammed functions to simplify the process of calculating payments and future values. In 1985, CI launched a calculator for the construction industry called the Construction Master which came preprogrammed with common construction calculations (such as angles, stairs, roofing math, pitch, rise, run, and feet-inch fraction conversions). This would be the first in a line of construction related calculators.\n\nThe first desktop \"programmable calculators\" were produced in the mid-1960s by Mathatronics and Casio (AL-1000). These machines were very heavy and costly. The first programmable pocket calculator was the HP-65, in 1974; it had a capacity of 100 instructions, and could store and retrieve programs with a built-in magnetic card reader. Two years later the HP-25C introduced \"continuous memory\", i.e., programs and data were retained in CMOS memory during power-off. In 1979, HP released the first \"alphanumeric\", programmable, \"expandable\" calculator, the HP-41C. It could be expanded with random access memory (RAM, for memory) and read-only memory (ROM, for software) modules, and peripherals like bar code readers, microcassette and floppy disk drives, paper-roll thermal printers, and miscellaneous communication interfaces (RS-232, HP-IL, HP-IB).\n\nThe first Soviet programmable desktop calculator ISKRA 123, powered by the power grid, was released at the start of the 1970s. The first Soviet pocket battery-powered programmable calculator, Elektronika \"B3-21\", was developed by the end of 1976 and released at the start of 1977. The successor of B3-21, the Elektronika B3-34 wasn't backward compatible with B3-21, even if it kept the reverse Polish notation (RPN). Thus B3-34 defined a new command set, which later was used in a series of later programmable Soviet calculators. Despite very limited abilities (98 bytes of instruction memory and about 19 stack and addressable registers), people managed to write all kinds of programs for them, including adventure games and libraries of calculus-related functions for engineers. Hundreds, perhaps thousands, of programs were written for these machines, from practical scientific and business software, which were used in real-life offices and labs, to fun games for children. The Elektronika MK-52 calculator (using the extended B3-34 command set, and featuring internal EEPROM memory for storing programs and external interface for EEPROM cards and other periphery) was used in Soviet spacecraft program (for Soyuz TM-7 flight) as a backup of the board computer.\n\nThis series of calculators was also noted for a large number of highly counter-intuitive mysterious undocumented features, somewhat similar to \"synthetic programming\" of the American HP-41, which were exploited by applying normal arithmetic operations to error messages, jumping to nonexistent addresses and other methods. A number of respected monthly publications, including the popular science magazine \"Nauka i Zhizn\" (\"Наука и жизнь\", \"Science and Life\"), featured special columns, dedicated to optimization methods for calculator programmers and updates on undocumented features for hackers, which grew into a whole esoteric science with many branches, named \"yeggogology\" (\"еггогология\"). The error messages on those calculators appear as a Russian word \"YEGGOG\" (\"ЕГГОГ\") which, unsurprisingly, is translated to \"Error\".\n\nA similar hacker culture in the USA revolved around the HP-41, which was also noted for a large number of undocumented features and was much more powerful than B3-34.\n\nThrough the 1970s the hand-held electronic calculator underwent rapid development. The red LED and blue/green vacuum fluorescent displays consumed a lot of power and the calculators either had a short battery life (often measured in hours, so rechargeable nickel-cadmium batteries were common) or were large so that they could take larger, higher capacity batteries. In the early 1970s liquid-crystal displays (LCDs) were in their infancy and there was a great deal of concern that they only had a short operating lifetime. Busicom introduced the Busicom \"LE-120A \"HANDY\"\" calculator, the first pocket-sized calculator and the first with an LED display, and announced the Busicom \"LC\" with LCD. However, there were problems with this display and the calculator never went on sale. The first successful calculators with LCDs were manufactured by Rockwell International and sold from 1972 by other companies under such names as: Dataking \"LC-800\", Harden \"DT/12\", Ibico \"086\", Lloyds \"40\", Lloyds \"100\", Prismatic \"500\" (a.k.a. \"P500\"), Rapid Data \"Rapidman 1208LC\". The LCDs were an early form using the \"Dynamic Scattering Mode DSM\" with the numbers appearing as bright against a dark background. To present a high-contrast display these models illuminated the LCD using a filament lamp and solid plastic light guide, which negated the low power consumption of the display. These models appear to have been sold only for a year or two.\n\nA more successful series of calculators using a reflective DSM-LCD was launched in 1972 by Sharp Inc with the Sharp \"EL-805\", which was a slim pocket calculator. This, and another few similar models, used Sharp's \"Calculator On Substrate\" (COS) technology. An extension of one glass plate needed for the liquid crystal display was used as a substrate to mount the needed chips based on a new hybrid technology. The COS technology may have been too costly since it was only used in a few models before Sharp reverted to conventional circuit boards.\nIn the mid-1970s the first calculators appeared with field-effect, \"twisted nematic\" (TN) LCDs with dark numerals against a grey background, though the early ones often had a yellow filter over them to cut out damaging ultraviolet rays. The advantage of LCDs is that they are passive light modulators reflecting light, which require much less power than light-emitting displays such as LEDs or VFDs. This led the way to the first credit-card-sized calculators, such as the Casio \"Mini Card LC-78\" of 1978, which could run for months of normal use on button cells.\n\nThere were also improvements to the electronics inside the calculators. All of the logic functions of a calculator had been squeezed into the first \"calculator on a chip\" integrated circuits (ICs) in 1971, but this was leading edge technology of the time and yields were low and costs were high. Many calculators continued to use two or more ICs, especially the scientific and the programmable ones, into the late 1970s.\n\nThe power consumption of the integrated circuits was also reduced, especially with the introduction of CMOS technology. Appearing in the Sharp \"EL-801\" in 1972, the transistors in the logic cells of CMOS ICs only used any appreciable power when they changed state. The LED and VFD displays often required added driver transistors or ICs, whereas the LCDs were more amenable to being driven directly by the calculator IC itself.\n\nWith this low power consumption came the possibility of using solar cells as the power source, realised around 1978 by calculators such as the Royal \"Solar 1\", Sharp \"EL-8026\", and Teal \"Photon\".\n\nAt the start of the 1970s, hand-held electronic calculators were very costly, at two or three weeks' wages, and so were a luxury item. The high price was due to their construction requiring many mechanical and electronic components which were costly to produce, and production runs that were too small to exploit economies of scale. Many firms saw that there were good profits to be made in the calculator business with the margin on such high prices. However, the cost of calculators fell as components and their production methods improved, and the effect of economies of scale was felt.\n\nBy 1976, the cost of the cheapest four-function pocket calculator had dropped to a few dollars, about 1/20th of the cost five years before. The results of this were that the pocket calculator was affordable, and that it was now difficult for the manufacturers to make a profit from calculators, leading to many firms dropping out of the business or closing down. The firms that survived making calculators tended to be those with high outputs of higher quality calculators, or producing high-specification scientific and programmable calculators.\n\nThe first calculator capable of symbolic computing was the HP-28C, released in 1987. It could, for example, solve quadratic equations symbolically. The first graphing calculator was the Casio fx-7000G released in 1985.\n\nThe two leading manufacturers, HP and TI, released increasingly feature-laden calculators during the 1980s and 1990s. At the turn of the millennium, the line between a graphing calculator and a handheld computer was not always clear, as some very advanced calculators such as the TI-89, the Voyage 200 and HP-49G could differentiate and integrate functions, solve differential equations, run word processing and PIM software, and connect by wire or IR to other calculators/computers.\n\nThe HP 12c financial calculator is still produced. It was introduced in 1981 and is still being made with few changes. The HP 12c featured the reverse Polish notation mode of data entry. In 2003 several new models were released, including an improved version of the HP 12c, the \"HP 12c platinum edition\" which added more memory, more built-in functions, and the addition of the algebraic mode of data entry.\n\nCalculated Industries competed with the HP 12c in the mortgage and real estate markets by differentiating the key labeling; changing the “I”, “PV”, “FV” to easier labeling terms such as \"Int\", \"Term\", \"Pmt\", and not using the reverse Polish notation. However, CI's more successful calculators involved a line of construction calculators, which evolved and expanded in the 1990s to present. According to Mark Bollman, a mathematics and calculator historian and associate professor of mathematics at Albion College, the \"Construction Master is the first in a long and profitable line of CI construction calculators\" which carried them through the 1980s, 1990s, and to the present.\n\nPersonal computers often come with a calculator utility program that emulates the appearance and functions of a calculator, using the graphical user interface to portray a calculator. One such example is Windows Calculator. Most personal data assistants (PDAs) and smartphones also have such a feature.\n\nIn most countries, students use calculators for schoolwork. There was some initial resistance to the idea out of fear that basic or elementary arithmetic skills would suffer. There remains disagreement about the importance of the ability to perform calculations \"in the head\", with some curricula restricting calculator use until a certain level of proficiency has been obtained, while others concentrate more on teaching estimation methods and problem-solving. Research suggests that inadequate guidance in the use of calculating tools can restrict the kind of mathematical thinking that students engage in. Others have argued that calculator use can even cause core mathematical skills to atrophy, or that such use can prevent understanding of advanced algebraic concepts. In December 2011 the UK's Minister of State for Schools, Nick Gibb, voiced concern that children can become \"too dependent\" on the use of calculators. As a result, the use of calculators is to be included as part of a review of the Curriculum. In the United States, many math educators and boards of education enthusiastically endorsed the National Council of Teachers of Mathematics (NCTM) standards and actively promoted the use of classroom calculators from kindergarten through high school.\n\n\n\n\n"}
{"id": "23335649", "url": "https://en.wikipedia.org/wiki?curid=23335649", "title": "Canonical map", "text": "Canonical map\n\nIn mathematics, a canonical map, also called a natural map, is a map or morphism between objects that arises naturally from the definition or the construction of the objects being mapped against each other. In general it is the map which preserves the widest amount of structure, and it tends to be unique. In the rare cases where latitude in choice remains, the map is either conventionally agreed upon to be the most useful for further analysis, or sometimes simply the most elegant or beautiful known.\n\nA closely related notion is a structure map or structure morphism; the map that comes with the given structure on the object. They are also sometimes called canonical maps.\n\nA canonical isomorphism is a canonical map that is also an isomorphism (i.e., invertible).\n\nIn some contexts, it is necessary to address an issue of \"choices\" of canonical maps or canonical isomorphisms; see prestack for a typical example.\n\n"}
{"id": "4406343", "url": "https://en.wikipedia.org/wiki?curid=4406343", "title": "Chronology of computation of π", "text": "Chronology of computation of π\n\nThe table below is a brief chronology of computed numerical values of, or bounds on, the mathematical constant pi (). For more detailed explanations for some of these calculations, see Approximations of.\n\n"}
{"id": "6166234", "url": "https://en.wikipedia.org/wiki?curid=6166234", "title": "Cryptomorphism", "text": "Cryptomorphism\n\nIn mathematics, two objects, especially systems of axioms or semantics for them, are called cryptomorphic if they are equivalent but not obviously equivalent. This word is a play on the many morphisms in mathematics, but \"cryptomorphism\" is only very distantly related to \"isomorphism\", \"homomorphism\", or \"morphisms\". The equivalence may possibly be in some informal sense, or may be formalized in terms of a bijection or equivalence of categories between the mathematical objects defined by the two cryptomorphic axiom systems.\n\nThe word was coined by Garrett Birkhoff before 1967, for use in the third edition of his book \"Lattice Theory\". Birkhoff did not give it a formal definition, though others working in the field have made some attempts since.\n\nIts informal sense was popularized (and greatly expanded in scope) by Gian-Carlo Rota in the context of matroid theory: there are dozens of equivalent axiomatic approaches to matroids, but two different systems of axioms often look very different. \n\nIn his 1997 book \"Indiscrete Thoughts\", Rota describes the situation as follows:\n\nThough there are many cryptomorphic concepts in mathematics outside of matroid theory and universal algebra, the word has not caught on among mathematicians generally. It is, however, in fairly wide use among researchers in matroid theory.\n\n\n"}
{"id": "47824717", "url": "https://en.wikipedia.org/wiki?curid=47824717", "title": "Echo removal", "text": "Echo removal\n\nEcho removal is the process of removing echo and reverberation artifacts from audio signals. The reverberation is typically modeled as the convolution of a (sometimes time-varying) impulse response with a hypothetical clean input signal, where both the clean input signal (which is to be recovered) and the impulse response are unknown. This is an example of an inverse problem. In almost all cases, there is insufficient information in the input signal to uniquely determine a plausible original image, making it an ill-posed problem. This is generally solved by the use of a regularization term to attempt to eliminate implausible solutions.\n\nThis problem is analogous to deblurring in the image processing domain.\n\n"}
{"id": "33439201", "url": "https://en.wikipedia.org/wiki?curid=33439201", "title": "Electoral Calculus", "text": "Electoral Calculus\n\nElectoral Calculus is a political forecasting web site which attempts to predict future United Kingdom general election results. It considers national factors but excludes local issues.\n\nThe site was developed by Martin Baxter, who is a financial analyst specialising in mathematical modelling.\n\nThe site includes maps, predictions and analysis articles. It has a separate section for elections in Scotland.\n\nThe site is based around the employment of scientific techniques on data about Britain's electoral geography, which can be used to calculate the uniform national swing. It takes account of national polls and trends but excludes local issues.\n\nThe calculations were initially based on what is termed the \"Transition Model\", which is derived from the additive uniform national swing model. This uses national swings in a proportional manner to predict local effects. The \"Strong Transition Model\" was introduced in October 2007, and considers the effects of strong and weak supporters. The models are explained in detail on the web site.\n\nIt was listed by \"The Guardian\" in 2004 as one of the \"100 most useful websites\", being \"the best\" for predictions. In 2012 it was described by PhD student Chris Prosser at the University of Oxford as \"probably the leading vote/seat predictor on the internet\". Its detailed predictions for individual seats have been noted by Paul Evans on the localdemocracy.org.uk blog. Academic Nick Anstead noted in his observations from a 2010 \"Personal Democracy Forum\" event, that Mick Fealty of Slugger O'Toole considered Electoral Calculus to be \"massively improved\" in comparison with the swingometer.\n\nWith reference to the 2010 United Kingdom general election, it was cited by journalists Andrew Rawnsley and Michael White in \"The Guardian\". John Rentoul in \"The Independent\" referred to the site after the election.\n\n"}
{"id": "40435056", "url": "https://en.wikipedia.org/wiki?curid=40435056", "title": "Equation-free modeling", "text": "Equation-free modeling\n\nEquation-free modeling is a method for multiscale computation and computer-aided analysis. It is designed for a class of complicated systems in which one observes evolution at a macroscopic, coarse scale of interest, while accurate models are only given at a finely detailed, microscopic, level of description. The framework empowers one to perform macroscopic computational tasks (over large space-time scales) using only appropriately initialized microscopic simulation on short time and small length scales. The methodology eliminates the derivation of explicit macroscopic evolution equations when these equations conceptually exist but are not available in closed form; hence the term equation-free.\n\nIn a wide range of chemical, physical and biological systems, coherent macroscopic behavior emerges from interactions between microscopic entities themselves (molecules, cells, grains, animals in a population, agents) and with their environment. Sometimes, remarkably, a coarse-scale differential equation model (such as the Navier-Stokes equations for fluid flow, or a reaction-diffusion system) can accurately describe macroscopic behavior. Such macroscale modeling makes use of general principles of conservation (atoms, particles, mass, momentum, energy), and closed into a well-posed system through phenomenological constitutive equations or equations of state. However, one increasingly encounters complex systems that only have known microscopic, fine scale, models. In such cases, although we observe the emergence of coarse-scale, macroscopic behavior, modeling it through explicit closure relations may be impossible or impractical. Non-Newtonian fluid flow, chemotaxis, porous media transport, epidemiology, brain modeling and neuronal systems are some typical examples. Equation-free modeling aims to use such microscale models to predict coarse macroscale emergent phenomena.\n\nPerforming coarse-scale computational tasks directly with fine-scale models is often infeasible: direct simulation over the full space-time domain of interest is often computationally prohibitive. Moreover, modeling tasks, such as numerical bifurcation analysis, are often impossible to perform on the fine-scale model directly: a coarse-scale steady state may not imply a steady state for the fine-scale system, since individual molecules or particles do not stop moving when the gas density or pressure become stationary. Equation-free modeling circumvents such problems by using short bursts of appropriately initialized fine-scale simulation.\n\nDynamic problems invoke the coarse time-stepper. In essence, short bursts of computational experiments with the fine-scale simulator estimate local time derivatives. Given an initial condition for the coarse variables formula_1 at time formula_2, the coarse time-stepper involves four steps:\n\n\nMultiple time steps simulates the system into the macro-future.\nIf the microscale model is stochastic, then an ensemble of microscale simulations may be needed to obtain sufficiently good extrapolation in the time step. Such a coarse time-stepper may be used in many algorithms of traditional continuum numerical analysis, such as numerical bifurcation analysis, optimization, control, and even accelerated coarse-scale simulation.\n\nTraditionally, algebraic formulae determine time derivatives of the coarse model. In our approach, the macroscale derivative is estimated by the inner microscale simulator, in effect performing a closure on demand. A reason for the name \"equation-free\" is by analogy with matrix-free numerical linear algebra; the name emphasizes that macro-level equations are never constructed explicitly in closed form.\n\nThe restriction operator often follows directly from the specific choice of the macroscale variables. For example, when the microscale model evolves an ensemble of many particles, the restriction typically computes the first few moments of the particle distribution (the density, momentum, and energy).\n\nThe lifting operator is usually much more involved. For example, consider a particle model: we need to define a mapping from a few low order moments of the particle distribution to initial conditions for each particle. The assumption that a relation exists that closes in these low order, coarse, moments, implies that the detailed microscale configurations are functionals of the moments (sometimes referred to as slaving ). We assume this relationship is established/emerges on time scales that are fast compared to the overall system evolution (see slow manifold theory and applications ). Unfortunately, the closure (slaving relations) are algebraically unknown (as otherwise the coarse evolution law would be known).\n\nInitializing the unknown microscale modes randomly introduces a lifting error: we rely on the separation of macro and micro time scales to ensure a quick relaxation to functionals of the coarse macrostates (healing). A preparatory step may be required, possibly involving microscale simulations constrained to keep the macrostates fixed. When the system has a unique fixed point for the unknown microscale details conditioned upon the coarse macrostates, a constrained runs algorithm may perform this preparatory step using only the microscale time-stepper.\n\nA toy problem illustrates the basic concepts. For example, consider the differential equation system for two variables formula_12:\nCapital formula_9 denotes the presumed macroscale variable, and lowercase formula_15 the microscale variable. This classification means that we assume a coarse model of the form formula_16 exists, although we do not necessarily know what it is. Arbitrarily define the lifting from any given macrostate formula_9 as formula_18. A simulation using this lifting and the coarse time-stepper is shown in the figure.\nThe solution of the differential equation rapidly moves to the slow manifold formula_19 for any initial data. The coarse time-stepper solution would agree better with the full solution when the 100 factor is increased. The graph shows the lifted solution (blue solid line) formula_20. At times formula_21, the solution is restricted and then lifted again, which here is simply setting formula_22. The slow manifold is shown as a red line. The right plot shows the time derivative of the restricted solution as a function of time (blue curve), as well as the time derivative formula_23 (the coarse time derivative), as observed from a full simulation (red curve).\n\nThe equation-free approach has been applied to many examples. The examples illustrate the various ways to construct and assemble the algorithmic building blocks. Numerical analysis establishes the accuracy and efficiency of this approach. Additional numerical analysis on other methods of this type has also been done.\n\nApplying the equation-free paradigm to a real problem requires considerable care, especially defining the lifting and restriction operators, and the appropriate outer solver.\n\n\nThe recursive projection method enables the computation of bifurcation diagrams using legacy simulation code. It also empowers the coarse time-stepper to perform equation-free bifurcation computations. Consider the coarse time stepper in its effective form\nwhich includes explicit dependence upon one or more parameters formula_25. Bifurcation analysis computes equilibria or periodic orbits, their stability and dependence upon parameter formula_25.\n\nCompute a coarse equilibrium as a fixed point of the coarse time stepper\nIn the equation-free context, the recursive projection method is the outer solver of this equation, and the coarse time-stepper enables this method to be performed using fine scale dynamics.\n\nAdditionally, for problems where the macroscale has continuous symmetries, one can use a template based approach to compute coarse self-similar or travelling wave solutions as fixed points of a coarse time-stepper that also encodes appropriate rescaling and/or shifting of space-time and/or solution.\nFor example, self-similar diffusion solutions may be found as the probability density function of detailed molecular dynamics.\n\nAn alternative to the recursive projection method is to use Newton—Krylov methods.\n\nThe coarse time stepper accelerates simulation over large macroscale times. In the scheme described above, let the large macro-time-step formula_28, and be on the time scale of the slow coarse dynamics. Let the computed formula_29 in terms of the coarse variable, and let the microscale simulation compute formula_30 from a local time simulation with initial condition that the coarse variable formula_31. Then we approximate formula_32 via extrapolating over a gap by\nwhere, for example, simple linear extrapolation would be \nThis scheme is the called coarse projective forward Euler, and is the simplest in the class.\n\nThe formula_35 steps taken before the extrapolation reflect that we must allow the system to settle onto a quasi-equilibrium (from the microscale point of view), so that we can make a reliable extrapolation of the slow dynamics. Then the size of the projective integration step is limited by stability of the slow modes.\n\nHigher order versions of coarse projective integration can be formed, analogous to Adams–Bashforth or Runge–Kutta. Higher order schemes for systems where the microscale noise is still apparent on the macroscale time step are more problematic.\n\nThe spatial analogue of projective integration is the gap-tooth scheme.\nThe idea of the gap-tooth scheme is to perform simulations of small patches of space, the teeth, separated by unsimulated space, the gaps.\nBy appropriately coupling the small patches of simulations we create a large scale, coarse level, simulation of the spatially extended system.\nWhen the microscale simulator is computationally expensive the gap-tooth scheme empowers efficient large scale prediction.\nFurthermore, it does this without us ever having to identify an algebraic closure for a large scale model.\nThe combination of the gap-tooth scheme with coarse projective integration is called patch dynamics.\n\nThe key to the gap-tooth and patch scheme is the coupling of the small patches across unsimulated space.\nSurprisingly, the generic answer is to simply use classic Lagrange interpolation, whether in one dimension or multiple dimensions. This answer is related to the coupling in holistic discretization and theoretical support provided by the theory of slow manifolds.\nThe interpolation provides value or flux boundary conditions as required by the microscale simulator. \nHigh order consistency between the macroscale gap-tooth/patch scheme and the microscale simulation is achieved through high order Lagrange interpolation.\n\nHowever, commonly the microscale is a noisy particle based or agent-based model.\nIn such cases the relevant macroscale variables are averages such as mass and momentum density. \nThen one generally has to form averages over a core of each tooth/patch, and apply the coupling condition over a finite action region on the edges of each tooth/patch.\nThe provisional recommendation is to make these regions as big as half the tooth/patch.\nThat is, for efficiency one makes the microscale tooth/patch as small as possible, but limited by the need to fit in action and core regions big enough to form accurate enough averages.\n\nPatch dynamics is the combination of the gap-tooth scheme and coarse projective integration. Just as for normal projective integration, at the start of each burst of microscale simulation, one has to create an initial condition for each patch that is consistent with the local macroscale variables, and the macroscale gradients from neighboring interpolated patches. The same techniques suffice.\n\nAssumptions and choices about the macroscale evolution are crucial in the equation-free scheme. The key assumption is that the variables we choose for the macroscale coupling must effectively close on the chosen macroscale. If the chosen macroscale length is too small then more coarse scale variables may be needed: for example, in fluid dynamics we conventionally close the PDEs for density, momentum and energy; yet in high speed flow especially at lower densities we need to resolve modes of molecular vibration because they have not equilibrated on the time scales of the fluid flow. Qualitatively the same considerations apply to the equation-free approach.\n\nFor many systems appropriate coarse variables are more-or-less known by experience. However, in complex situations there is a need to automatically detect the appropriate coarse variables, and then use them in the macroscale evolution. This needs much more research utilizing techniques from data mining and manifold learning. In some problems it could be that as well as densities, the appropriate coarse variables also need to include spatial correlations, as in the so-called Brownian bugs.\n\nThe macroscale may have to be treated as a stochastic system, but then the errors are likely to be much larger and the closures more uncertain.\n"}
{"id": "1714764", "url": "https://en.wikipedia.org/wiki?curid=1714764", "title": "Harmonic (mathematics)", "text": "Harmonic (mathematics)\n\nIn mathematics, a number of concepts employ the word harmonic. The similarity of this terminology to that of music is not accidental: the equations of motion of vibrating strings, drums and columns of air are given by formulas involving Laplacians; the solutions to which are given by eigenvalues corresponding to their modes of vibration. Thus, the term \"harmonic\" is applied when one is considering functions with sinusoidal variations, or solutions of Laplace's equation and related concepts.\n\n"}
{"id": "945503", "url": "https://en.wikipedia.org/wiki?curid=945503", "title": "Hellenic Mathematical Society", "text": "Hellenic Mathematical Society\n\nThe Hellenic Mathematical Society (HMS) (Greek: Ελληνική Μαθηματική Εταιρεία) is a learned society which promotes the study of mathematics in Greece. It was founded in 1918, and published the \"Bulletin of the Greek Mathematical Society\".\n\nIt is a member of the European Mathematical Society. \n\n\n\n"}
{"id": "16515493", "url": "https://en.wikipedia.org/wiki?curid=16515493", "title": "History of group theory", "text": "History of group theory\n\nThe history of group theory, a mathematical domain studying groups in their various forms, has evolved in various parallel threads. There are three historical roots of group theory: the theory of algebraic equations, number theory and geometry. Joseph Louis Lagrange, Niels Henrik Abel and Évariste Galois were early researchers in the field of group theory.\nThe earliest study of groups as such probably goes back to the work of Lagrange in the late 18th century. However, this work was somewhat isolated, and 1846 publications of Augustin Louis Cauchy and Galois are more commonly referred to as the beginning of group theory. The theory did not develop in a vacuum, and so three important threads in its pre-history are developed here.\n\nOne foundational root of group theory was the quest of solutions of polynomial equations of degree higher than 4. \n\nAn early source occurs in the problem of forming an equation of degree \"m\" having as its roots \"m\" of the roots of a given equation of degree formula_1. For simple cases the problem goes back to Johann van Waveren Hudde (1659). Nicholas Saunderson (1740) noted that the determination of the quadratic factors of a biquadratic expression necessarily leads to a sextic equation, and Le Sœur (1748) and Edward Waring (1762 to 1782) still further elaborated the idea.\n\nA common foundation for the theory of equations on the basis of the group of permutations was found by Lagrange (1770, 1771), and on this was built the theory of substitutions. He discovered that the roots of all resolvents (\"résolvantes, réduites\") which he examined are rational functions of the roots of the respective equations. To study the properties of these functions he invented a \"Calcul des Combinaisons\". The contemporary work of Alexandre-Théophile Vandermonde (1770) also foreshadowed the coming theory.\n\nPaolo Ruffini (1799) attempted a proof of the impossibility of solving the quintic and higher equations. Ruffini distinguished what are now called intransitive and transitive, and imprimitive and primitive groups, and (1801) uses the group of an equation under the name \"l'assieme delle permutazioni\". He also published a letter from Pietro Abbati to himself, in which the group idea is prominent.\nGalois found that if formula_2 are the \"n\" roots of an equation, there is always a group of permutations of the \"r\"'s such that \nIn modern terms, the solvability of the Galois group attached to the equation determines the solvability of the equation with radicals.\n\nGalois is the first to use the words \"group\" (\"groupe\" in French) and \"primitive\" in their modern meanings. He did not use \"primitive group\" but called \"equation primitive\" an equation whose Galois group is primitive. He discovered the notion of normal subgroups and found that a solvable primitive group may be identified to a subgroup of the affine group of an affine space over a finite field of prime order.\n\nGalois also contributed to the theory of modular equations and to that of elliptic functions. His first publication on group theory was made at the age of eighteen (1829), but his contributions attracted little attention until the publication of his collected papers in 1846 (Liouville, Vol. XI). Galois is honored as the first mathematician linking group theory and field theory, with the theory that is now called Galois theory.\n\nGroups similar to Galois groups are (today) called permutation groups, a concept investigated in particular by Cauchy. A number of important theorems in early group theory are due to Cauchy. Arthur Cayley's \"On the theory of groups, as depending on the symbolic equation formula_3\" (1854) gives the first abstract definition of finite groups.\n\nSecondly, the systematic use of groups in geometry, mainly in the guise of symmetry groups, was initiated by Felix Klein's 1872 Erlangen program. The study of what are now called Lie groups started systematically in 1884 with Sophus Lie, followed by work of Wilhelm Killing, Eduard Study, Issai Schur, Ludwig Maurer, and Élie Cartan. The discontinuous (discrete group) theory was built up by Klein, Lie, Henri Poincaré, and Charles Émile Picard, in connection in particular with modular forms and monodromy.\n\nThe third root of group theory was number theory. Certain abelian group structures had been implicitly used in number-theoretical work by Carl Friedrich Gauss, and more explicitly by Leopold Kronecker. Early attempts to prove Fermat's last theorem were led to a climax by Ernst Kummer by introducing groups describing factorization into prime numbers.\n\nGroup theory as an increasingly independent subject was popularized by Serret, who devoted section IV of his algebra to the theory; by Camille Jordan, whose \"Traité des substitutions et des équations algébriques\" (1870) is a classic; and to Eugen Netto (1882), whose \"Theory of Substitutions and its Applications to Algebra\" was translated into English by Cole (1892). Other group theorists of the 19th century were Joseph Louis François Bertrand, Charles Hermite, Ferdinand Georg Frobenius, Kronecker, and Émile Mathieu; as well as William Burnside, Leonard Eugene Dickson, Otto Hölder, E. H. Moore, Ludwig Sylow, and Heinrich Martin Weber.\n\nThe convergence of the above three sources into a uniform theory started with Jordan's \"Traité\" and Walther von Dyck (1882) who first defined a group in the full modern sense. The textbooks of Weber and Burnside helped establish group theory as a discipline. The abstract group formulation did not apply to a large portion of 19th century group theory, and an alternative formalism was given in terms of Lie algebras.\n\nGroups in the 1870-1900 period were described as the continuous groups of Lie, the discontinuous groups, finite groups of substitutions of roots (gradually being called permutations), and finite groups of linear substitutions (usually of finite fields). During the 1880-1920 period, groups described by presentations came into a life of their own through the work of Cayley, Walther von Dyck, Max Dehn, Jakob Nielsen, Otto Schreier, and continued in the 1920-1940 period with the work of H. S. M. Coxeter, Wilhelm Magnus, and others to form the field of combinatorial group theory.\n\nFinite groups in the 1870-1900 period saw such highlights as the Sylow theorems, Hölder's classification of groups of square-free order, and the early beginnings of the character theory of Frobenius. Already by 1860, the groups of automorphisms of the finite projective planes had been studied (by Mathieu), and in the 1870s Klein's group-theoretic vision of geometry was being realized in his Erlangen program. The automorphism groups of higher dimensional projective spaces were studied by Jordan in his \"Traité\" and included composition series for most of the so-called classical groups, though he avoided non-prime fields and omitted the unitary groups. The study was continued by Moore and Burnside, and brought into comprehensive textbook form by Leonard Dickson in 1901. The role of simple groups was emphasized by Jordan, and criteria for non-simplicity were developed by Hölder until he was able to classify the simple groups of order less than 200. The study was continued by Frank Nelson Cole (up to 660) and Burnside (up to 1092), and finally in an early \"millennium project\", up to 2001 by Miller and Ling in 1900.\n\nContinuous groups in the 1870-1900 period developed rapidly. Killing and Lie's foundational papers were published, Hilbert's theorem in invariant theory 1882, etc. \n\nIn the period 1900-1940, infinite \"discontinuous\" (now called discrete groups) groups gained life of their own. Burnside's famous problem ushered in the study of arbitrary subgroups of finite-dimensional linear groups over arbitrary fields, and indeed arbitrary groups. Fundamental groups and reflection groups encouraged the developments of J. A. Todd and Coxeter, such as the Todd–Coxeter algorithm in combinatorial group theory. Algebraic groups, defined as solutions of polynomial equations (rather than acting on them, as in the earlier century), benefited heavily from the continuous theory of Lie. Bernard Neumann and Hanna Neumann produced their study of varieties of groups, groups defined by group theoretic equations rather than polynomial ones. \n\nContinuous groups also had explosive growth in the 1900-1940 period. Topological groups began to be studied as such. There were many great achievements in continuous groups: Cartan's classification of semisimple Lie algebras, Hermann Weyl's theory of representations of compact groups, Alfréd Haar's work in the locally compact case.\n\nFinite groups in the 1900-1940 grew immensely. This period witnessed the birth of character theory by Frobenius, Burnside, and Schur which helped answer many of the 19th century questions in permutation groups, and opened the way to entirely new techniques in abstract finite groups. This period saw the work of Philip Hall: on a generalization of Sylow's theorem to arbitrary sets of primes which revolutionized the study of finite soluble groups, and on the power-commutator structure of p-groups, including the ideas of regular p-groups and isoclinism of groups, which revolutionized the study of p-groups and was the first major result in this area since Sylow. This period saw Hans Zassenhaus's famous Schur-Zassenhaus theorem on the existence of complements to Hall's generalization of Sylow subgroups, as well as his progress on Frobenius groups, and a near classification of Zassenhaus groups. \n\nBoth depth, breadth and also the impact of group theory subsequently grew. The domain started branching out into areas such as algebraic groups, group extensions, and representation theory. Starting in the 1950s, in a huge collaborative effort, group theorists succeeded to classify all finite simple groups in 1982. Completing and simplifying the proof of the classification are areas of active research.\n\nAnatoly Maltsev also made important contributions to group theory during this time; his early work was in logic in the 1930s, but in the 1940s he proved important embedding properties of semigroups into groups, studied the isomorphism problem of group rings, established the Malçev correspondence for polycyclic groups, and in the 1960s return to logic proving various theories within the study of groups to be undecidable. Earlier, Alfred Tarski proved elementary group theory undecidable.\nThe period of 1960-1980 was one of excitement in many areas of group theory.\n\nIn finite groups, there were many independent milestones. One had the discovery of 22 new sporadic groups, and the completion of the first generation of the classification of finite simple groups. One had the influential idea of the Carter subgroup, and the subsequent creation of formation theory and the theory of classes of groups. One had the remarkable extensions of Clifford theory by Green to the indecomposable modules of group algebras. During this era, the field of computational group theory became a recognized field of study, due in part to its tremendous success during the first generation classification.\n\nIn discrete groups, the geometric methods of Jacques Tits and the availability the surjectivity of Serge Lang's map allowed a revolution in algebraic groups. The Burnside problem had tremendous progress, with better counterexamples constructed in the 1960s and early 1980s, but the finishing touches \"for all but finitely many\" were not completed until the 1990s. The work on the Burnside problem increased interest in Lie algebras in exponent \"p\", and the methods of Michel Lazard began to see a wider impact, especially in the study of \"p\"-groups.\n\nContinuous groups broadened considerably, with \"p\"-adic analytic questions becoming important. Many conjectures were made during this time, including the coclass conjectures.\n\nThe last twenty years of the 20th century enjoyed the successes of over one hundred years of study in group theory.\n\nIn finite groups, post classification results included the O'Nan–Scott theorem, the Aschbacher classification, the classification of multiply transitive finite groups, the determination of the maximal subgroups of the simple groups and the corresponding classifications of primitive groups. In finite geometry and combinatorics, many problems could now be settled. The modular representation theory entered a new era as the techniques of the classification were axiomatized, including fusion systems, Luis Puig's theory of pairs and nilpotent blocks. The theory of finite soluble groups was likewise transformed by the influential book of Klaus Doerk and Trevor Hawkes which brought the theory of projectors and injectors to a wider audience.\n\nIn discrete groups, several areas of geometry came together to produce exciting new fields. Work on knot theory, orbifolds, hyperbolic manifolds, and groups acting on trees (the Bass–Serre theory), much enlivened the study of hyperbolic groups, automatic groups. Questions such as William Thurston's 1982 geometrization conjecture, inspired entirely new techniques in geometric group theory and low-dimensional topology, and was involved in the solution of one of the Millennium Prize Problems, the Poincaré conjecture.\n\nContinuous groups saw the solution of the problem of hearing the shape of a drum in 1992 using symmetry groups of the laplacian operator. Continuous techniques were applied to many aspects of group theory using function spaces and quantum groups. Many 18th and 19th century problems are now revisited in this more general setting, and many questions in the theory of the representations of groups have answers.\n\nGroup theory continues to be an intensely studied matter. Its importance to contemporary mathematics as a whole may be seen from the 2008 Abel Prize, awarded to John Griggs Thompson and Jacques Tits for their contributions to group theory.\n\n"}
{"id": "235124", "url": "https://en.wikipedia.org/wiki?curid=235124", "title": "Index of cryptography articles", "text": "Index of cryptography articles\n\nArticles related to cryptography include:\n\n3-D Secure •\n3-subset meet-in-the-middle attack •\n3-Way •\n40-bit encryption •\n56-bit encryption •\n5-UCO \n\nA5/1 •\nA5/2 •\nABA digital signature guidelines •\nABC (stream cipher) •\nAbraham Sinkov •\nAcoustic cryptanalysis •\nAdaptive chosen-ciphertext attack •\nAdaptive chosen plaintext and chosen ciphertext attack •\nAdvantage (cryptography) •\nADFGVX cipher •\nAdi Shamir •\nAdvanced Access Content System •\nAdvanced Encryption Standard •\nAdvanced Encryption Standard process •\nAdversary •\nAEAD block cipher modes of operation •\nAffine cipher •\nAgnes Meyer Driscoll •\nAKA (security) •\nAkelarre (cipher) •\nAlan Turing •\nAlastair Denniston •\nAl Bhed language •\nAlex Biryukov •\nAlfred Menezes •\nAlgebraic Eraser •\nAlgorithmically random sequence •\nAlice and Bob •\nAll-or-nothing transform •\nAlphabetum Kaldeorum •\nAlternating step generator •\nAmerican Cryptogram Association •\nAN/CYZ-10 •\nAnonymous Internet banking •\nAnonymous publication •\nAnonymous remailer •\nAntoni Palluth •\nAnubis (cipher) •\nArgon2 •\nARIA (cipher) •\nArlington Hall •\nArne Beurling •\nArnold Cipher •\nArray controller based encryption •\nArthur Scherbius •\nArvid Gerhard Damm •\nAsiacrypt •\nAtbash •\nAttack model •\nAuguste Kerckhoffs •\nAuthenticated encryption •\nAuthentication •\nAuthorization certificate •\nAutokey cipher •\nAvalanche effect \n\nB-Dienst •\nBabington Plot •\nBaby-step giant-step •\nBacon's cipher •\nBanburismus •\nBart Preneel •\nBaseKing •\nBassOmatic •\nBATON •\nBB84 •\nBeale ciphers •\nBEAR and LION ciphers •\nBeaufort cipher •\nBeaumanor Hall •\nBent function •\nBerlekamp–Massey algorithm •\nBernstein v. United States •\nBestCrypt •\nBiclique attack •\nBID/60 •\nBID 770 •\nBifid cipher •\nBill Weisband •\nBinary Goppa code •\nBiometric word list •\nBirthday attack •\nBit-flipping attack •\nBitTorrent protocol encryption •\nBiuro Szyfrów •\nBlack Chamber •\nBlaise de Vigenère •\nBletchley Park •\nBlind credential •\nBlinding (cryptography) •\nBlind signature •\nBlock cipher •\nBlock cipher mode of operation •\nBlock size (cryptography) •\nBlowfish (cipher) •\nBlum Blum Shub •\nBlum–Goldwasser cryptosystem •\nBomba (cryptography) •\nBombe •\nBook cipher •\nBooks on cryptography •\nBoomerang attack •\nBoris Hagelin •\nBouncy Castle (cryptography) •\nBroadcast encryption •\nBruce Schneier •\nBrute-force attack •\nBurrows–Abadi–Needham logic •\nBurt Kaliski \n\nC2Net •\nC-36 (cipher machine) •\nC-52 (cipher machine) •\nCAcert.org •\nCaesar cipher •\nCamellia (cipher) •\nCAPICOM •\nCapstone (cryptography) •\nCardan grille •\nCard catalog (cryptology) •\nCarlisle Adams •\nCAST-128 •\nCAST-256 •\nCayley–Purser algorithm •\nCBC-MAC •\nCCM mode •\nCCMP •\nCD-57 •\nCDMF •\nCellular Message Encryption Algorithm •\nCentiban •\nCentral Security Service •\nCentre for Applied Cryptographic Research •\nCentral Bureau •\nCerticom •\nCertificate authority •\nCertificate-based encryption •\nCertificateless cryptography •\nCertificate revocation list •\nCertificate signing request •\nCertification path validation algorithm •\nChaffing and winnowing •\nChallenge-Handshake Authentication Protocol •\nChallenge–response authentication •\nChosen-ciphertext attack •\nChosen-plaintext attack •\nCIKS-1 •\nCipher disk •\nCipher runes •\nCipher security summary •\nCipherSaber •\nCiphertext expansion •\nCiphertext indistinguishability •\nCiphertext-only attack •\nCiphertext stealing •\nCIPHERUNICORN-A •\nCIPHERUNICORN-E •\nClassical cipher •\nClaude Shannon •\nClaw-free permutation •\nCleartext •\nCLEFIA •\nClifford Cocks •\nClipper chip •\nClock (cryptography) •\nClock drift •\nCMVP •\nCOCONUT98 •\nCodebook •\nCode (cryptography) •\nCode talker •\nCodress message •\nCold boot attack •\nCollision attack •\nCollision resistance •\nColossus computer •\nCombined Cipher Machine •\nCommitment scheme •\nCommon Scrambling Algorithm •\nCommunications security •\nCommunications Security Establishment •\nCommunication Theory of Secrecy Systems •\nComparison of disk encryption software •\nComparison of SSH clients •\nCompleteness (cryptography) •\nComplexity trap •\nComputational Diffie–Hellman assumption •\nComputational hardness assumption •\nComputer insecurity •\nComputer and network surveillance •\nCOMSEC equipment •\nConch (SSH) •\nConcrete security •\nConel Hugh O'Donel Alexander •\nConfidentiality •\nConfusion and diffusion •\nContent-scrambling system •\nControlled Cryptographic Item •\nCorkscrew (program) •\nCorrelation immunity •\nCOSIC •\nCovert channel •\nCover (telecommunications) •\nCrab (cipher) •\nCramer–Shoup cryptosystem •\nCRAM-MD5 •\nCRHF •\nCrib (cryptanalysis) •\nCrossCrypt •\nCrowds •\nCrypt (C) •\nCryptanalysis •\nCryptanalysis of the Enigma •\nCryptanalysis of the Lorenz cipher •\nCryptanalytic computer •\nCryptex •\nCryptico •\nCrypto AG •\nCrypto-anarchism •\nCrypto API (Linux) •\nMicrosoft CryptoAPI •\nCryptoBuddy •\nCryptochannel •\nCRYPTO (conference) •\nCryptogram •\nCryptographically Generated Address •\nCryptographically secure pseudorandom number generator •\nCryptographically strong •\nCryptographic Application Programming Interface •\nCryptographic engineering •\nCryptographic hash function •\nCryptographic key types •\nCryptographic Message Syntax •\nCryptographic primitive •\nCryptographic protocol •\nCryptographic Service Provider •\nCryptographie indéchiffrable •\nCryptography •\nCryptography in Japan •\nCryptography newsgroups •\nCryptography standards •\nCryptologia •\nCryptology ePrint Archive •\nCryptology Research Society of India •\nCryptomathic •\nCryptome •\nCryptomeria cipher •\nCryptonomicon •\nCrypTool •\nCrypto phone •\nCrypto-society •\nCryptosystem •\nCryptovirology •\nCRYPTREC •\nCS-Cipher •\nCurve25519 •\nCustom hardware attack •\nCycles per byte •\nCyclometer •\nCypherpunk •\nCyrillic Projector \n\nD'Agapeyeff cipher •\nDaniel J. Bernstein •\nData Authentication Algorithm •\nData Encryption Standard •\nDatagram Transport Layer Security •\nDavid Chaum •\nDavid Kahn •\nDavid Naccache •\nDavid Wagner •\nDavid Wheeler (computer scientist) •\nDavies attack •\nDavies–Meyer hash •\nDEAL •\nDecimal sequences for cryptography •\nDecipherment •\nDecisional Diffie–Hellman assumption •\nDecorrelation theory •\nDecrypt •\nDeCSS •\nDefence Signals Directorate •\nDegree of anonymity •\nDelegated Path Discovery •\nDelegated Path Validation •\nDeniable encryption •\nDerek Taunt •\nDerived unique key per transaction •\nDES Challenges •\nDES supplementary material •\nDES-X •\nDeterministic encryption •\nDFC (cipher) •\nDictionary attack •\nDifferential cryptanalysis •\nDifferential-linear attack •\nDifferential power analysis •\nDiffie–Hellman key exchange •\nDiffie–Hellman problem •\nDigiCipher 2 •\nDigital Fortress •\nDigital rights management •\nDigital signature •\nDigital Signature Algorithm •\nDigital signature forgery •\nDigital timestamping •\nDigital watermarking •\nDilly Knox •\nDining cryptographers problem •\nDiplomatic bag •\nDirect Anonymous Attestation •\nDiscrete logarithm •\nDisk encryption •\nDisk encryption hardware •\nDisk encryption software •\nDistance-bounding protocol •\nDistinguishing attack •\nDistributed.net •\nDMA attack •\ndm-crypt •\nDmitry Sklyarov •\nDomainKeys •\nDon Coppersmith •\nDorabella Cipher •\nDouble Ratchet Algorithm •\nDoug Stinson •\nDragon (cipher) •\nDRYAD •\nDual_EC_DRBG •\nDvorak encoding \n\nE0 (cipher) •\nE2 (cipher) •\nE4M •\nEAP-AKA •\nEAP-SIM •\nEAX mode •\nECC patents •\nECHELON •\nECRYPT •\nEdouard Fleissner von Wostrowitz •\nEdward Hebern •\nEdward Scheidt •\nEdward Travis •\nEFF DES cracker •\nEfficient Probabilistic Public-Key Encryption Scheme •\nEKMS •\nElectronic Communications Act 2000 •\nElectronic money •\nElectronic signature •\nElectronic voting •\nElGamal encryption •\nElGamal signature scheme •\nEli Biham •\nElizebeth Friedman •\nElliptic-curve cryptography •\nElliptic-curve Diffie–Hellman •\nElliptic Curve DSA •\nElliptic curve only hash •\nElonka Dunin •\nEncrypted function •\nEncrypted key exchange •\nEncrypting File System •\nEncryption •\nEncryption software •\nEnigmail •\nEnigma machine •\nEnigma rotor details •\nEntrust •\nErnst Fetterlein •\neSTREAM •\nÉtienne Bazeries •\nEurocrypt •\nEuroCrypt •\nExport of cryptography •\nExtensible Authentication Protocol \n\nFast Software Encryption •\nFast syndrome-based hash •\nFEA-M •\nFEAL •\nFeige–Fiat–Shamir identification scheme •\nFeistel cipher •\nFélix Delastelle •\nFialka •\nFilesystem-level encryption •\nFileVault •\nFill device •\nFinancial cryptography •\nFIPS 140 •\nFIPS 140-2 •\nFirefly (key exchange protocol) •\nFISH (cipher) •\nFish (cryptography) •\nFloradora •\nFluhrer, Mantin and Shamir attack •\nFormat-preserving encryption •\nFortezza •\nFort George G. Meade •\nFortuna (PRNG) •\nFour-square cipher •\nFranciszek Pokorny •\nFrank A. Stevenson •\nFrank Rowlett •\nFreenet •\nFreeOTFE •\nFreeS/WAN •\nFrequency analysis •\nFriedrich Kasiski •\nFritz-chip •\nFROG •\nFROSTBURG •\nFTP over SSH •\nFull disk encryption •\nFull Domain Hash •\nF. W. Winterbotham \n\nGalois/Counter Mode •\nGardening (cryptanalysis) •\nGCHQ Bude •\nGCHQ CSO Morwenstow •\nGDES •\nGeneric Security Services Application Program Interface •\nGeorge Blakley •\nGeorge Scovell •\nGGH encryption scheme •\nGGH signature scheme •\nGilbert Vernam •\nGMR (cryptography) •\nGNU Privacy Guard •\nGnuTLS •\nGoldwasser–Micali cryptosystem •\nGordon Welchman •\nGOST (block cipher) •\nGOST (hash function) •\nGovernment Communications Headquarters •\nGovernment Communications Security Bureau •\nGrain (cipher) •\nGrand Cru (cipher) •\nGreat Cipher •\nGrill (cryptology) •\nGrille (cryptography) •\nGroup-based cryptography •\nGroup signature •\nGrover's algorithm •\nGustave Bertrand •\nGwido Langer \n\nH.235 •\nHAIFA construction •\nHAIPE •\nHans Dobbertin •\nHans-Thilo Schmidt •\nHard-core predicate •\nHardware random number generator •\nHardware security module •\nHarold Keen •\nHarry Hinsley •\nHarvest (computer) •\nHAS-160 •\nHash-based cryptography •\nHashcash •\nHash chain •\nHash function security summary •\nHash list •\nHasty Pudding cipher •\nHAVAL •\nHC-256 •\nHC-9 •\nHeath Robinson (codebreaking machine) •\nHebern rotor machine •\nHenri Braquenié •\nHenryk Zygalski •\nHerbert Yardley •\nHidden Field Equations •\nHideki Imai •\nHierocrypt •\nHigh-bandwidth Digital Content Protection •\nHigher-order differential cryptanalysis •\nHill cipher •\nHistory of cryptography •\nHMAC •\nHMAC-based One-time Password algorithm (HOTP) •\nHorst Feistel •\nHoward Heys •\nHttps •\nHugo Hadwiger •\nHugo Koch •\nHushmail •\nHut 6 •\nHut 8 •\nHX-63 •\nHybrid cryptosystem •\nHyperelliptic curve cryptography •\nHyper-encryption \n\nIan Goldberg •\nIBM 4758 •\nICE (cipher) •\nID-based cryptography •\nIDEA NXT •\nIdentification friend or foe •\nIEEE 802.11i •\nIEEE P1363 •\nI. J. Good •\nIllegal prime •\nImpossible differential cryptanalysis •\nIndex of coincidence •\nIndifferent chosen-ciphertext attack •\nIndocrypt •\nInformation leakage •\nInformation Security Group •\nInformation-theoretic security •\nInitialization vector •\nInteger factorization •\nIntegral cryptanalysis •\nIntegrated Encryption Scheme •\nIntegrated Windows Authentication •\nInterlock protocol •\nIntermediate certificate authorities •\nInternational Association for Cryptologic Research •\nInternational Data Encryption Algorithm •\nInternet Key Exchange •\nInternet Security Association and Key Management Protocol •\nInterpolation attack •\nInvisible ink •\nIPsec •\nIraqi block cipher •\nISAAC (cipher) •\nISO 19092-2 •\nISO/IEC 9797 •\nIvan Damgård \n\nJacques Stern •\nJADE (cypher machine) •\nJames Gillogly •\nJames H. Ellis •\nJames Massey •\nJan Graliński •\nJan Kowalewski •\nJapanese naval codes •\nJava Cryptography Architecture •\nJefferson disk •\nJennifer Seberry •\nJerzy Różycki •\nJoan Daemen •\nJohannes Trithemius •\nJohn Herivel •\nJohn Kelsey (cryptanalyst) •\nJohn R. F. Jeffreys •\nJohn Tiltman •\nJon Lech Johansen •\nJosef Pieprzyk •\nJoseph Desch •\nJoseph Finnegan (cryptographer) •\nJoseph Mauborgne •\nJoseph Rochefort •\nJournal of Cryptology •\nJunger v. Daley \n\nKaisa Nyberg •\nKalyna (cipher) •\nKasiski examination •\nKASUMI •\nKCDSA •\nKeePass •\nKerberos (protocol) •\nKerckhoffs's principle •\nKevin McCurley (cryptographer) •\nKey-agreement protocol •\nKey authentication •\nKey clustering •\nKey (cryptography) •\nKey derivation function •\nKey distribution center •\nKey escrow •\nKey exchange •\nKeyfile •\nKey generation •\nKey generator •\nKey management •\nKeymat •\nKey-recovery attack •\nKey schedule •\nKey server (cryptographic) •\nKey signature (cryptography) •\nKeysigning •\nKey signing party •\nKey size •\nKey space (cryptography) •\nKeystream •\nKey stretching •\nKey whitening •\nKG-84 •\nKHAZAD •\nKhufu and Khafre •\nKiss (cryptanalysis) •\nKL-43 •\nKL-51 •\nKL-7 •\nKleptography •\nKN-Cipher •\nKnapsack problem •\nKnown-key distinguishing attack •\nKnown-plaintext attack •\nKnownSafe •\nKOI-18 •\nKOV-14 •\nKryha •\nKryptos •\nKSD-64 •\nKupyna •\nKuznyechik •\nKW-26 •\nKW-37 •\nKY-3 •\nKY-57 •\nKY-58 •\nKY-68 •\nKYK-13 \n\nLacida •\nLadder-DES •\nLamport signature •\nLars Knudsen •\nLattice-based cryptography •\nLaurance Safford •\nLawrie Brown •\nLCS35 •\nLeo Marks •\nLeonard Adleman •\nLeon Battista Alberti •\nLeo Rosen •\nLeslie Yoxall •\nLEVIATHAN (cipher) •\nLEX (cipher) •\nLibelle (cipher) •\nLinear cryptanalysis •\nLinear-feedback shift register •\nLink encryption •\nList of ciphertexts •\nList of cryptographers •\nList of cryptographic file systems •\nList of cryptographic key types •\nList of cryptology conferences •\nList of telecommunications encryption terms • List of people associated with Bletchley Park • \nList of SFTP server software •\nLOKI •\nLOKI97 •\nLorenz cipher •\nLouis W. Tordella •\nLsh •\nLucifer (cipher) •\nLyra2 \n\nM6 (cipher) •\nM8 (cipher) •\nM-209 •\nM-325 •\nM-94 •\nMacGuffin (cipher) •\nMadryga •\nMAGENTA •\nMagic (cryptography) •\nMaksymilian Ciężki •\nMalcolm J. Williamson •\nMalleability (cryptography) •\nMan-in-the-middle attack •\nMarian Rejewski •\nMARS (cryptography) •\nMartin Hellman •\nMaruTukku •\nMassey–Omura cryptosystem •\nMatt Blaze •\nMatt Robshaw •\nMax Newman •\nMcEliece cryptosystem •\nmcrypt •\nMD2 (cryptography) •\nMD4 •\nMD5 •\nMD5CRK •\nMDC-2 •\nMDS matrix •\nMean shortest distance •\nMeet-in-the-middle attack •\nMental poker •\nMercury (cipher machine) •\nMercy (cipher) •\nMeredith Gardner •\nMerkle signature scheme •\nMerkle–Damgård construction •\nMerkle–Hellman knapsack cryptosystem •\nMerkle's Puzzles •\nMerkle tree •\nMESH (cipher) •\nMessage authentication •\nMessage authentication code •\nMessage forgery •\nMI8 •\nMichael Luby •\nMICKEY •\nMicrodot •\nMilitary Cryptanalysis (book) (William F. Friedman) •\nMilitary Cryptanalytics •\nMimic function •\nMirror writing •\nMISTY1 •\nMitsuru Matsui •\nMMB (cipher) •\nMod n cryptanalysis •\nMQV •\nMS-CHAP •\nMUGI •\nMULTI-S01 •\nMultiSwap •\nMultivariate cryptography \n\nNational Communications Centre •\nNational Cryptologic Museum •\nNational Security Agency •\nNational Cipher Challenge •\nNavajo I •\nNeal Koblitz •\nNeedham–Schroeder protocol •\nNegligible function •\nNEMA (machine) •\nNESSIE •\nNetwork Security Services •\nNeural cryptography •\nNew Data Seal •\nNewDES •\nN-Hash •\nNicolas Courtois •\nNiederreiter cryptosystem •\nNiels Ferguson •\nNigel de Grey •\nNihilist cipher •\nNikita Borisov •\nNimbus (cipher) •\nNIST hash function competition •\nNonlinear-feedback shift register •\nNOEKEON •\nNon-malleable codes •\nNoreen •\nNothing up my sleeve number •\nNSA cryptography •\nNSA encryption systems •\nNSA in fiction •\nNSAKEY •\nNSA Suite A Cryptography •\nNSA Suite B Cryptography •\nNT LAN Manager •\nNTLMSSP •\nNTRU Cryptosystems, Inc. •\nNTRUEncrypt •\nNTRUSign •\nNull cipher •\nNumbers station •\nNUSH •\nNTRU \n\nOblivious transfer •\nOCB mode •\nOded Goldreich •\nOff-the-Record Messaging •\nOkamoto–Uchiyama cryptosystem •\nOMI cryptograph •\nOMNI (SCIP) •\nOne-key MAC •\nOne-time pad •\nOne-time password •\nOne-way compression function •\nOne-way function •\nOnion routing •\nOnline Certificate Status Protocol •\nOP-20-G •\nOpenPGP card •\nOpenSSH •\nOpenSSL •\nOpenswan •\nOpenVPN •\nOperation Ruthless •\nOptimal asymmetric encryption padding •\nOver the Air Rekeying (OTAR) •\nOTFE •\nOtway–Rees protocol \n\nPadding (cryptography) •\nPadding oracle attack •\nPaillier cryptosystem •\nPairing-based cryptography •\nPanama (cryptography) •\nPartitioning cryptanalysis •\nPassive attack •\nPassphrase •\nPassword •\nPassword-authenticated key agreement •\nPassword cracking •\nPassword Hashing Competition •\nPaul Kocher •\nPaulo Pancatuccio •\nPaulo S. L. M. Barreto •\nPaul van Oorschot •\nPBKDF2 •\nPC Bruno •\nPepper (cryptography) •\nPerfect forward secrecy •\nPerforated sheets •\nPermutation cipher •\nPeter Gutmann (computer scientist) •\nPeter Junger •\nPeter Twinn •\nPGP Corporation •\nPGPDisk •\nPGPfone •\nPhelix •\nPhil Zimmermann •\nPhoturis (protocol) •\nPhysical security •\nPhysical unclonable function •\nPig Latin •\nPigpen cipher •\nPike (cipher) •\nPiling-up lemma •\nPinwheel (cryptography) •\nPiotr Smoleński •\nPirate decryption •\nPKC (conference) •\nPKCS •\nPKCS 11 •\nPKCS 12 •\nPKIX •\nPlaintext •\nPlaintext-aware encryption •\nPlayfair cipher •\nPlugboard •\nPMAC (cryptography) •\nPoem code •\nPohlig–Hellman algorithm •\nPoint-to-Point Tunneling Protocol •\nPointcheval–Stern signature algorithm •\nPoly1305 •\nPolyalphabetic cipher •\nPolybius square •\nPortex •\nPost-quantum cryptography •\nPost-Quantum Cryptography Standardization •\nPower analysis •\nPreimage attack •\nPre-shared key •\nPretty Good Privacy •\nPrinter steganography •\nPrivacy-enhanced Electronic Mail •\nPrivate Communications Technology •\nPrivate information retrieval •\nProbabilistic encryption •\nProduct cipher •\nProof-of-work system •\nProtected Extensible Authentication Protocol •\nProvable security •\nProvably secure cryptographic hash function •\nProxy re-encryption •\nPseudo-Hadamard transform •\nPseudonymity •\nPseudorandom function •\nPseudorandom number generator •\nPseudorandom permutation •\nPublic key certificate •\nPublic-key cryptography •\nPublic key fingerprint •\nPublic key infrastructure •\nPURPLE •\nPuTTY •\nPy (cipher) \n\nQ (cipher) •\nQrpff •\nQUAD (cipher) •\nQuadratic sieve •\nQuantum coin flipping •\nQuantum cryptography •\nQuantum digital signature •\nQuantum fingerprinting •\nQuantum key distribution \n\nRabbit (cipher) •\nRabin cryptosystem •\nRabin–Williams encryption •\nRadioGatún •\nRail fence cipher •\nRainbow table •\nRalph Merkle •\nRambutan (cryptography) •\nRandom function •\nRandomness tests •\nRandom number generator attack •\nRandom oracle •\nRC2 •\nRC4 •\nRC5 •\nRC6 •\nRebound attack •\nReciprocal cipher •\nRed/black concept •\nREDOC •\nRed Pike (cipher) •\nReflector (cipher machine) •\nRegulation of Investigatory Powers Act 2000 •\nReihenschieber •\nRekeying (cryptography) •\nRelated-key attack •\nReplay attack •\nReservehandverfahren •\nResidual block termination •\nRijndael key schedule •\nRijndael S-box •\nRing signature •\nRIPEMD •\nRip van Winkle cipher •\nRobert Morris (cryptographer) •\nRobot certificate authority •\nRockex •\nRolf Noskwith •\nRon Rivest •\nRoom 40 •\nRoot certificate •\nRoss J. Anderson •\nRossignols •\nROT13 •\nRotor machine •\nRSA •\nRSA-100 •\nRSA-1024 •\nRSA-110 •\nRSA-120 •\nRSA-129 •\nRSA-130 •\nRSA-140 •\nRSA-150 •\nRSA-1536 •\nRSA-155 •\nRSA-160 •\nRSA-170 •\nRSA-180 •\nRSA-190 •\nRSA-200 •\nRSA-2048 •\nRSA-210 •\nRSA-220 •\nRSA-230 •\nRSA-232 •\nRSA-240 •\nRSA-250 •\nRSA-260 •\nRSA-270 •\nRSA-280 •\nRSA-290 •\nRSA-300 •\nRSA-309 •\nRSA-310 •\nRSA-320 •\nRSA-330 •\nRSA-340 •\nRSA-350 •\nRSA-360 •\nRSA-370 •\nRSA-380 •\nRSA-390 •\nRSA-400 •\nRSA-410 •\nRSA-420 •\nRSA-430 •\nRSA-440 •\nRSA-450 •\nRSA-460 •\nRSA-470 •\nRSA-480 •\nRSA-490 •\nRSA-500 •\nRSA-576 •\nRSA-617 •\nRSA-640 •\nRSA-704 •\nRSA-768 •\nRSA-896 •\nRSA-PSS •\nRSA Factoring Challenge •\nRSA problem •\nRSA Secret-Key Challenge •\nRSA Security •\nRubber-hose cryptanalysis •\nRunning key cipher •\nRussian copulation \n\nS-1 block cipher •\nSAFER •\nSalsa20 •\nSalt (cryptography) •\nSAM card •\nSecurity Support Provider Interface •\nSAML •\nSAVILLE •\nSC2000 •\nSchnorr group •\nSchnorr signature •\nSchoof–Elkies–Atkin algorithm •\nSCIP •\nScott Vanstone •\nScrambler •\nScramdisk •\nScream (cipher) •\nScrypt •\nScytale •\nSeahorse (software) •\nSEAL (cipher) •\nSean Murphy (cryptographer) •\nSECG •\nSecret broadcast •\nSecret decoder ring •\nSecrets and Lies (Schneier) •\nSecret sharing •\nSectéra Secure Module •\nSecure access module •\nSecure channel •\nSecure Communication based on Quantum Cryptography •\nSecure copy •\nSecure cryptoprocessor •\nSecure Electronic Transaction •\nSecure Hash Algorithms •\nSecure Hypertext Transfer Protocol •\nSecure key issuing cryptography •\nSecure multi-party computation •\nSecure Neighbor Discovery •\nSecure Real-time Transport Protocol •\nSecure remote password protocol •\nSecure Shell •\nSecure telephone •\nSecure Terminal Equipment •\nSecure voice •\nSecurID •\nSecurity association •\nSecurity engineering •\nSecurity level •\nSecurity parameter •\nSecurity protocol notation •\nSecurity through obscurity •\nSecurity token •\nSEED •\nSelected Areas in Cryptography •\nSelf-certifying File System •\nSelf-certifying key •\nSelf-shrinking generator •\nSelf-signed certificate •\nSemantic security •\nSerge Vaudenay •\nSerpent (cipher) •\nSession key •\nSHACAL •\nShafi Goldwasser •\nSHA-1 •\nSHA-2 •\nSHA-3 •\nShared secret •\nSHARK •\nShaun Wylie •\nShor's algorithm •\nShrinking generator •\nShugborough inscription •\nSide-channel attack •\nSiemens and Halske T52 •\nSIGABA •\nSIGCUM •\nSIGINT •\nSignal Protocol •\nSignal Intelligence Service •\nSigncryption •\nSIGSALY •\nSILC (protocol) •\nSilvio Micali •\nSimple Authentication and Security Layer •\nSimple public-key infrastructure •\nSimple XOR cipher •\nS/KEY •\nSkein (hash function) •\nSkipjack (cipher) •\nSlide attack •\nSlidex •\nSmall subgroup confinement attack •\nS/MIME •\nSM4 algorithm (formerly SMS4) •\nSnake oil (cryptography) •\nSnefru •\nSNOW •\nSnuffle •\nSOBER-128 •\nSolitaire (cipher) •\nSolomon Kullback •\nSOSEMANUK •\nSpecial Collection Service •\nSpectr-H64 •\nSPEKE (cryptography) •\nSponge function •\nSPNEGO •\nSquare (cipher) •\nSsh-agent •\nSSH File Transfer Protocol •\nSSLeay •\nStafford Tavares •\nStåle Schumacher Ytteborg •\nStandard model (cryptography) •\nStation CAST •\nStation HYPO •\nStation-to-Station protocol •\nStatistical cryptanalysis •\nStefan Lucks •\nSteganalysis •\nSteganography •\nStraddling checkerboard •\nStream cipher •\nStream cipher attacks •\nStrong cryptography •\nStrong RSA assumption •\nStuart Milner-Barry •\nSTU-II •\nSTU-III •\nStunnel •\nSubstitution box •\nSubstitution cipher •\nSubstitution–permutation network •\nSuperencryption •\nSupersingular isogeny key exchange •\nSwedish National Defence Radio Establishment •\nSWIFFT •\nSXAL/MBAL •\nSymmetric-key algorithm •\nSYSKEY \n\nTabula recta •\nTaher Elgamal •\nTamper resistance •\nTcpcrypt •\nTelevision encryption •\nTEMPEST •\nTemporal Key Integrity Protocol •\nTestery •\nThawte •\nThe Alphabet Cipher •\nThe Code Book •\nThe Codebreakers •\nThe Gold-Bug •\nThe Magic Words are Squeamish Ossifrage •\nTheory of Cryptography Conference •\nThe world wonders •\nThomas Jakobsen •\nThree-pass protocol •\nThreshold shadow scheme •\nTICOM •\nTiger (cryptography) •\nTimeline of cryptography •\nTime/memory/data tradeoff attack •\nTime-based One-time Password algorithm (TOTP) •\nTiming attack •\nTiny Encryption Algorithm •\nTom Berson •\nTommy Flowers •\nTopics in cryptography •\nTor (anonymity network) •\nTorus-based cryptography •\nTraffic analysis •\nTraffic-flow security •\nTraitor tracing •\nTransmission security •\nTransport Layer Security •\nTransposition cipher •\nTrapdoor function •\nTrench code •\nTreyfer •\nTrifid cipher •\nTriple DES •\nTrivium (cipher) •\nTrueCrypt •\nTruncated differential cryptanalysis •\nTrusted third party •\nTuring (cipher) •\nTWINKLE •\nTWIRL •\nTwofish •\nTwo-square cipher •\nType 1 encryption •\nType 2 encryption •\nType 3 encryption •\nType 4 encryption •\nTypex \n\nUES (cipher) •\nUltra •\nUMAC •\nUnbalanced Oil and Vinegar •\nUndeniable signature •\nUnicity distance •\nUniversal composability •\nUniversal one-way hash function (UOWHF)\n\nVenona project •\nVerifiable secret sharing •\nVerisign •\nVery smooth hash •\nVEST •\nVIC cipher •\nVideoCrypt •\nVigenère cipher •\nVincent Rijmen •\nVINSON •\nVirtual private network •\nVisual cryptography •\nVoynich manuscript \n\nWadsworth's cipher •\nWAKE •\nWLAN Authentication and Privacy Infrastructure •\nWatermark (data file) •\nWatermarking attack •\nWeak key •\nWeb of trust •\nWhirlpool (hash function) •\nWhitfield Diffie •\nWide Mouth Frog protocol •\nWi-Fi Protected Access •\nWilliam F. Friedman •\nWilliam Montgomery (cryptographer) •\nWinSCP •\nWired Equivalent Privacy •\nWireless Transport Layer Security •\nWitness-indistinguishable proof •\nWorkshop on Cryptographic Hardware and Embedded Systems •\nWorld War I cryptography •\nWorld War II cryptography •\nW. T. Tutte \n\nX.509 •\nXDH assumption •\nXenon (cipher) •\nXiaoyun Wang •\nXML Encryption •\nXML Signature •\nxmx •\nXSL attack •\nXTEA •\nXTR •\nXuejia Lai •\nXXTEA \n\nYarrow algorithm •\nY-stations •\nYuliang Zheng \n\nZeroisation •\nZero-knowledge password proof •\nZero-knowledge proof •\nZfone •\nZodiac (cipher) •\nZRTP •\nZimmermann–Sassaman key-signing protocol •\nZimmermann Telegram \n\n"}
{"id": "37872896", "url": "https://en.wikipedia.org/wiki?curid=37872896", "title": "Kostant's convexity theorem", "text": "Kostant's convexity theorem\n\nIn mathematics, Kostant's convexity theorem, introduced by , states that the projection of every coadjoint orbit of a connected compact Lie group into the dual of a Cartan subalgebra is a convex set. It is a special case of a more general result for symmetric spaces. Kostant's theorem is a generalization of a result of , and for hermitian matrices. They proved that the projection onto the diagonal matrices of the space of all \"n\" by \"n\" complex self-adjoint matrices with given eigenvalues Λ = (λ, ..., λ) is the convex polytope with vertices all permutations of the coordinates of Λ.\n\nKostant used this to generalize the Golden–Thompson inequality to all compact groups.\n\nLet \"K\" be a connected compact Lie group with maximal torus \"T\" and Weyl group \"W\" = \"N\"(\"T\")/\"T\". Let their Lie algebras be formula_1 and formula_2. Let \"P\" be the orthogonal projection of formula_1 onto formula_2 for some Ad-invariant inner product on formula_1. Then for \"X\" in formula_2, \"P\"(Ad(\"K\")⋅\"X\") is the convex polytope with vertices \"w\"(\"X\") where \"w\" runs over the Weyl group.\n\nLet \"G\" be a compact Lie group and σ an involution with \"K\" a compact subgroup fixed by σ and containing the identity component of the fixed point subgroup of σ. Thus \"G\"/\"K\" is a symmetric space of compact type. Let formula_7 and formula_1 be their Lie algebras and let σ also denote the corresponding involution of formula_7. Let formula_10 be the −1 eigenspace of σ and let formula_11 be a maximal Abelian subspace. Let \"Q\" be the orthogonal projection of formula_10 onto formula_11 for some Ad(\"K\")-invariant inner product on formula_10. Then for \"X\" in formula_11, \"Q\"(Ad(\"K\")⋅\"X\") is the convex polytope with vertices the \"w\"(\"X\") where \"w\" runs over the restricted Weyl group (the normalizer of formula_11 in \"K\" modulo its centralizer).\n\nThe case of a compact Lie group is the special case where \"G\" = \"K\" × \"K\", \"K\" is embedded diagonally and σ is the automorphism of \"G\" interchanging the two factors.\n\nKostant's proof for symmetric spaces is given in . There is an elementary proof just for compact Lie groups using similar ideas, due to : it is based on a generalization of the Jacobi eigenvalue algorithm to compact Lie groups.\n\nLet \"K\" be a connected compact Lie group with maximal torus \"T\". For each positive root α there is a homomorphism of SU(2) into \"K\". A simple calculation with 2 by 2 matrices shows that if \"Y\" is in formula_1 and \"k\" varies in this image of SU(2), then \"P\"(Ad(\"k\")⋅\"Y\") traces a straight line between \"P\"(\"Y\") and its reflection in the root α. In particular the component in the α root space—its \"α off-diagonal coordinate\"—can be sent to 0. In performing this latter operation, the distance from \"P\"(\"Y\") to \"P\"(Ad(\"k\")⋅\"Y\") is bounded above by size of the α off-diagonal coordinate of \"Y\". Let \"m\" be the number of positive roots, half the dimension of \"K\"/\"T\". Starting from an arbitrary \"Y\" take the largest off-diagonal coordinate and send it to zero to get \"Y\". Continue in this way, to get a sequence (\"Y\"). Then\n\nThus \"P\"(\"Y\") tends to 0 and\n\nHence \"X\" = \"P\"(\"Y\") is a Cauchy sequence, so tends to \"X\" in formula_2. Since \"Y\" = \"P\"(\"Y\") ⊕ \"P\"(\"Y\"), \"Y\" tends to \"X\". On the other hand, \"X\" lies on the line segment joining \"X\" and its reflection in the root α. Thus \"X\" lies in the Weyl group polytope defined by \"X\". These convex polytopes are thus increasing as \"n\" increases and hence \"P\"(\"Y\") lies in the polytope for \"X\". This can be repeated for each \"Z\" in the \"K\"-orbit of \"X\". The limit is necessarily in the Weyl group orbit of \"X\" and hence \"P\"(Ad(\"K\")⋅\"X\") is contained in the convex polytope defined by \"W\"(\"X\").\n\nTo prove the opposite inclusion, take \"X\" to be a point in the positive Weyl chamber. Then all the other points \"Y\" in the convex hull of \"W\"(\"X\") can be obtained by a series of paths in that intersection moving along the negative of a simple root. (This matches a familiar picture from representation theory: if by duality \"X\" corresponds to a dominant weight λ, the other weights in the Weyl group polytope defined by λ are those appearing in the irreducible representation of \"K\" with highest weight λ. An argument with lowering operators shows that each such weight is linked by a chain to λ obtained by successively subtracting simple roots from λ.) Each part of the path from \"X\" to \"Y\" can be obtained by the process described above for the copies of SU(2) corresponding to simple roots, so the whole convex polytope lies in \"P\"(Ad(\"K\")⋅\"X\").\n\n gave another proof of the convexity theorem for compact Lie groups, also presented in . For compact groups, and showed that if \"M\" is a symplectic manifold with a Hamiltonian action of a torus \"T\" with Lie algebra formula_2, then the image of the moment map\n\nis a convex polytope with vertices in the image of the fixed point set of \"T\" (the image is a finite set). Taking for \"M\" a coadjoint orbit of \"K\" in formula_23, the moment map for \"T\" is the composition\n\nUsing the Ad-invariant inner product to identify formula_23 and formula_1, the map becomes\n\nthe restriction of the orthogonal projection. Taking \"X\" in formula_2, the fixed points of \"T\" in the orbit Ad(\"K\")⋅\"X\" are just the orbit under the Weyl group, \"W\"(\"X\"). So the convexity properties of the moment map imply that the image is the convex polytope with these vertices. gave a simplified direct version of the proof using moment maps.\n\nLet τ be the map τ(\"Y\") = − σ(\"Y\"). The map above has the same image as that of the fixed point set of τ, i.e. Ad(\"K\")⋅\"X\". Its image is the convex polytope with vertices the image of the fixed point set of \"T\" on Ad(\"G\")⋅\"X\", i.e. the points \"w\"(\"X\") for \"w\" in \"W\" = N(\"T\")/C(\"T\").\n\nIn the convexity theorem is deduced from a more general convexity theorem concerning the projection onto the component \"A\" in the Iwasawa decomposition \"G\" = \"KAN\" of a real semisimple Lie group \"G\". The result discussed above for compact Lie groups \"K\" corresponds to the special case when \"G\" is the complexification of \"K\": in this case the Lie algebra of \"A\" can be identified with formula_32. The more general version of Kostant's theorem has also been generalized to semisimple symmetric spaces by . gave a generalization for infinite-dimensional groups.\n\n"}
{"id": "44357613", "url": "https://en.wikipedia.org/wiki?curid=44357613", "title": "Källén function", "text": "Källén function\n\nThe Källén function, also known as triangle function, is a polynomial function in three variables, which appears in geometry and particle physics. In the latter field it is usually denoted by the symbol formula_1. It is named after the theoretical physicist Gunnar Källén, who introduced it as a short-hand in his textbook \"Elementary Particle Physics\".\n\nThe function is given by a quadratic polynomial in three variables\n\nIn geometry the function describes the area formula_3 of a triangle with side lengths formula_4:\nSee also Heron's formula.\n\nThe function appears naturally in the Kinematics of relativistic particles, e.g. when expressing the energy and momentum components in the center of mass frame by Mandelstam variables.\n\nThe function is (obviously) symmetric in permutations of its arguments, as well as independent of a common sign flip of its arguments:\n\nIf formula_7 the polynomial factorizes into two factors\n\nIf formula_9 the polynomial factorizes into four factors\n\nIts most condensed form is\n"}
{"id": "337876", "url": "https://en.wikipedia.org/wiki?curid=337876", "title": "List of calculus topics", "text": "List of calculus topics\n\nThis is a list of calculus topics.\n\n\nDeriviablity & continuty\n\n\n\n\n\n\"See also [[list of numerical analysis topics]]\"\n\n\n\n\n\n\nFor further developments: see [[list of real analysis topics]], [[list of complex analysis topics]], [[list of multivariable calculus topics]].\n\n[[Category:Mathematics-related lists|Calculus]]\n[[Category:Calculus| ]]\n[[Category:Lists of topics|Calculus]]"}
{"id": "32869192", "url": "https://en.wikipedia.org/wiki?curid=32869192", "title": "List of dualities", "text": "List of dualities\n\nIn mathematics, a duality, generally speaking, translates concepts, theorems or mathematical structures into other concepts, theorems or structures, in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of \"A\" is \"B\", then the dual of \"B\" is \"A\".\n\n\n\n\n"}
{"id": "490054", "url": "https://en.wikipedia.org/wiki?curid=490054", "title": "List of exponential topics", "text": "List of exponential topics\n\nThis is a list of exponential topics, by Wikipedia page. See also list of logarithm topics.\n"}
{"id": "508022", "url": "https://en.wikipedia.org/wiki?curid=508022", "title": "List of factorial and binomial topics", "text": "List of factorial and binomial topics\n\nThis is a list of factorial and binomial topics in mathematics. See also binomial (disambiguation).\n\n"}
{"id": "1749665", "url": "https://en.wikipedia.org/wiki?curid=1749665", "title": "List of finite simple groups", "text": "List of finite simple groups\n\nIn mathematics, the classification of finite simple groups states that every finite simple group is cyclic, or alternating, or in one of 16 families of groups of Lie type, or one of 26 sporadic groups.\n\nThe list below gives all finite simple groups, together with their order, the size of the Schur multiplier, the size of the outer automorphism group, usually some small representations, and lists of all duplicates.\n\nThe following table is a complete list of the 18 families of finite simple groups and the 26 sporadic simple groups, along with their orders. Any non-simple members of each family are listed, as well as any members duplicated within a family or between families. (In removing duplicates it is useful to note that no two finite simple groups have the same order, except that the group A = \"A\"(2) and \"A\"(4) both have order 20160, and that the group \"B\"(\"q\") has the same order as \"C\"(\"q\") for \"q\" odd, \"n\" > 2. The smallest of the latter pairs of groups are \"B\"(3) and \"C\"(3) which both have order 4585351680.)\n\nThere is an unfortunate conflict between the notations for the alternating groups A and the groups of Lie type \"A\"(\"q\"). Some authors use various different fonts for A to distinguish them. In particular,\nin this article we make the distinction by setting the alternating groups A in Roman font and the Lie-type groups \"A\"(\"q\") in italic.\n\nIn what follows, \"n\" is a positive integer, and \"q\" is a positive power of a prime number \"p\", with the restrictions noted. The notation (\"a\",\"b\") represents the greatest common divisor of the integers \"a\" and \"b\".\n\nSimplicity: Simple for \"p\" a prime number.\n\nOrder: \"p\"\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Cyclic of order \"p\" − 1.\n\nOther names: Z/\"p\"Z\n\nRemarks: These are the only simple groups that are not perfect.\n\nSimplicity: Solvable for \"n\" < 5, otherwise simple.\n\nOrder: \"n\"!/2 when \"n\" > 1.\n\nSchur multiplier: 2 for \"n\" = 5 or \"n\" > 7, 6 for \"n\" = 6 or 7; see \"Covering groups of the alternating and symmetric groups\"\n\nOuter automorphism group: In general 2. Exceptions: for \"n\" = 1, \"n\" = 2, it is trivial, and for \"n\" = 6, it has order 4 (elementary abelian).\n\nOther names: Alt.\n\nIsomorphisms: A and A are trivial. A is cyclic of order 3. A is isomorphic to \"A\"(3) (solvable). A is isomorphic to \"A\"(4) and to \"A\"(5). A is isomorphic to \"A\"(9) and to the derived group \"B\"(2)′. A is isomorphic to \"A\"(2).\n\nRemarks: An index 2 subgroup of the symmetric group of permutations of \"n\" points when \"n\" > 1.\n\nNotation: \"n\" is a positive integer, \"q\" > 1 is a power of a prime number \"p\", and is the order of some underlying finite field. The order of the outer automorphism group is written as \"d\"⋅\"f\"⋅\"g\", where \"d\" is the order of the group of \"diagonal automorphisms\", \"f\" is the order of the (cyclic) group of \"field automorphisms\" (generated by a Frobenius automorphism), and \"g\" is the order of the group of \"graph automorphisms\" (coming from automorphisms of the Dynkin diagram). The notation (\"a\",\"b\") represents the greatest common divisor of the integers \"a\" and \"b\".\n\nSimplicity: Simple for \"n\" ≥ 1. The group\n\"B\"(2) is solvable.\n\nOrder:\n\"q\"\n(\"q\" − 1),\nwhere\n\"q\" = 2.\n\nSchur multiplier: Trivial for \"n\" ≠ 1, elementary abelian of order 4\nfor \"B\"(8).\n\nOuter automorphism group:\nwhere \"f\" = 2\"n\" + 1.\n\nOther names: Suz(2), Sz(2).\n\nIsomorphisms: \"B\"(2) is the Frobenius group of order 20.\n\nRemarks: Suzuki group are Zassenhaus groups acting on sets of size (2) + 1, and have 4-dimensional representations over the field with 2 elements. They are the only non-cyclic simple groups whose order is not divisible by 3. They are not related to the sporadic Suzuki group.\n\nSimplicity: Simple for \"n\" ≥ 1. The derived group \"F\"(2)′ is simple of index 2\nin \"F\"(2), and is called the Tits group,\nnamed for the Belgian mathematician Jacques Tits.\n\nOrder:\n\"q\"\n(\"q\" − 1),\nwhere\n\"q\" = 2.\n\nThe Tits group has order 17971200 = 2 ⋅ 3 ⋅ 5 ⋅ 13.\n\nSchur multiplier: Trivial for \"n\" ≥ 1 and for the Tits group.\n\nOuter automorphism group:\nwhere \"f\" = 2\"n\" + 1. Order 2 for the Tits group.\n\nRemarks: Unlike the other simple groups of Lie type, the Tits group does not have a BN pair, though its automorphism group does so most authors count it as a sort of honorary group of Lie type.\n\nSimplicity: Simple for \"n\" ≥ 1. The group \"G\"(3) is not simple, but its derived group \"G\"(3)′ is a simple subgroup of index 3.\n\nOrder:\n\"q\"\n(\"q\" − 1),\nwhere\n\"q\" = 3\n\nSchur multiplier: Trivial for \"n\" ≥ 1 and for \"G\"(3)′.\n\nOuter automorphism group:\nwhere \"f\" = 2\"n\" + 1.\n\nOther names: Ree(3), R(3), E(3) .\n\nIsomorphisms: The derived group \"G\"(3)′ is isomorphic to \"A\"(8).\n\nRemarks: \"G\"(3) has a doubly transitive permutation representation on 3 + 1 points and acts on a 7-dimensional vector space over the field with 3 elements.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 = 44352000\n\nSchur multiplier: Order 2.\n\nOuter automorphism group: Order 2.\n\nRemarks: It acts as a rank 3 permutation group on the Higman Sims graph with 100 points, and is contained in Co and in Co.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 = 898128000\n\nSchur multiplier: Order 3.\n\nOuter automorphism group: Order 2.\n\nRemarks: Acts as a rank 3 permutation group on the McLaughlin graph with 275 points, and is contained in Co and in Co.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 17 = 4030387200\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Order 2.\n\nOther names: Held–Higman–McKay group, HHM, \"F\", HTH\n\nRemarks: Centralizes an element of order 7 in the monster group.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 13 ⋅ 29 = 145926144000\n\nSchur multiplier: Order 2.\n\nOuter automorphism group: Trivial.\n\nRemarks: The double cover acts on a 28-dimensional lattice over the Gaussian integers.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 13 = 448345497600\n\nSchur multiplier: Order 6.\n\nOuter automorphism group: Order 2.\n\nOther names: Sz\n\nRemarks: The 6 fold cover acts on a 12-dimensional lattice over the Eisenstein integers. It is not related to the Suzuki groups of Lie type.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 19 ⋅ 31 = 460815505920\n\nSchur multiplier: Order 3.\n\nOuter automorphism group: Order 2.\n\nOther names: O'Nan–Sims group, O'NS, O–S\n\nRemarks:\nThe triple cover has two 45-dimensional representations over the field with 7 elements, exchanged by an outer automorphism.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 19 = 273030912000000\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Order 2.\n\nOther names: \"F\", \"D\"\n\nRemarks: Centralizes an element of order 5 in the monster group.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 31 ⋅ 37 ⋅ 67 = 51765179004000000\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Trivial.\n\nOther names: Lyons–Sims group, LyS\n\nRemarks: Has a 111-dimensional representation over the field with 5 elements.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 13 ⋅ 19 ⋅ 31 = 90745943887872000\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Trivial.\n\nOther names: \"F\", \"E\"\n\nRemarks: Centralizes an element of order 3 in the monster, and is contained in \"E\"(3), so has a 248-dimensional representation over the field with 3 elements.\n\nOrder:\n\nSchur multiplier: Order 2.\n\nOuter automorphism group: Trivial.\n\nOther names: \"F\"\n\nRemarks: The double cover is contained in the monster group. It has a representation of dimension 4371 over the complex numbers (with no nontrivial invariant product), and a representation of dimension 4370 over the field with 2 elements preserving a commutative but non-associative product.\n\nOrder:\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Trivial.\n\nOther names: \"F\", M, Monster group, Friendly giant, Fischer's monster.\n\nRemarks: Contains all but 6 of the other sporadic groups as subquotients. Related to monstrous moonshine. The monster is the automorphism group of the 196,883-dimensional Griess algebra and the infinite-dimensional monster vertex operator algebra, and acts naturally on the monster Lie algebra.\n\n lists the 56 non-cyclic simple groups of order less than a million.\n\n\n\n"}
{"id": "1631654", "url": "https://en.wikipedia.org/wiki?curid=1631654", "title": "List of mathematical identities", "text": "List of mathematical identities\n\nThis page lists mathematical identities, that is, \"identically true relations\" holding in mathematics.\n\n\n\n"}
{"id": "5971813", "url": "https://en.wikipedia.org/wiki?curid=5971813", "title": "List of mathematicians (I)", "text": "List of mathematicians (I)\n\n\n\n\n"}
{"id": "8569325", "url": "https://en.wikipedia.org/wiki?curid=8569325", "title": "Mishnat ha-Middot", "text": "Mishnat ha-Middot\n\nThe Mishnat ha-Middot (; \"treatise of measures\") is considered the earliest known Hebrew treatise on geometry. The treatise was discovered in the Munich Library by Moritz Steinschneider, who dated it between 800 and 1200 C.E. Hermann Schapira argued the treatise dates from an earlier period and Solomon Gandz conjectured Rabbi Nehemiah (c. 150 C.E.) to be the author. The content resembles both the work of Hero of Alexandria (c. 100 C.E.) and that of al-Khwārizmī (c. 800 C.E.) and the proponents of the earlier dating therefore see it linking Greek and Islamic mathematics.\n\nThe \"Mishnat ha-Middot\" argues against the common belief that the Bible defines the geometric ratio π (pi) as being exactly equal to 3 and defines it as 3 1/7 instead. \n\n\n"}
{"id": "57481237", "url": "https://en.wikipedia.org/wiki?curid=57481237", "title": "Modeshape", "text": "Modeshape\n\nIn applied mathematics, mode shapes are a manifestation of eigenvectors which describe the relative displacement of two or more elements in a mechanical system or wave front .\n\n"}
{"id": "22130008", "url": "https://en.wikipedia.org/wiki?curid=22130008", "title": "Multiplication of vectors", "text": "Multiplication of vectors\n\nIn mathematics, Vector multiplication refers to one of several techniques for the multiplication of two (or more) vectors with themselves. It may concern any of the following articles:\n\n\n"}
{"id": "3134388", "url": "https://en.wikipedia.org/wiki?curid=3134388", "title": "Nearest integer function", "text": "Nearest integer function\n\nIn computer science, the nearest integer function of real number \"x\" denoted variously by formula_1, formula_2, formula_3, nint(\"x\"), or Round(\"x\"), is a function which returns the nearest integer to \"x\". To avoid ambiguity when operating on half-integers, a rounding rule must be chosen. On most computer implementations, the selected rule is to round half-integers to the nearest even integer—for example, \nThis is in accordance with the IEEE 754 standards and helps reduce bias in the result.\n\nThere are many other possible rules for tie breaking when rounding a half integer include rounding up, rounding down, rounding to or away from zero, or random rounding up or down.\n\n"}
{"id": "44052223", "url": "https://en.wikipedia.org/wiki?curid=44052223", "title": "Orthomorphism", "text": "Orthomorphism\n\nIn abstract algebra, an orthomorphism is a certain kind of mapping from a group into itself. Let \"G\" be a group, and let \"θ\" be a permutation of \"G\". Then \"θ\" is an orthomorphism of \"G\" if the mapping \"f\" defined by \"f\"(\"x\") = \"x\" \"θ\"(\"x\") is also a permutation of \"G\". A permutation \"φ\" of \"G\" is a complete mapping if the mapping \"g\" defined by \"g\"(\"x\") = \"xφ\"(\"x\") is also a permutation of \"G\". Orthomorphisms and complete mappings are closely related. \n"}
{"id": "40765261", "url": "https://en.wikipedia.org/wiki?curid=40765261", "title": "Point process notation", "text": "Point process notation\n\nIn probability and statistics, point process notation comprises the range of mathematical notation used to symbolically represent random objects known as point processes, which are used in related fields such as stochastic geometry, spatial statistics and continuum percolation theory and frequently serve as mathematical models of random phenomena, representable as points, in time, space or both.\n\nThe notation varies due to the histories of certain mathematical fields and the different interpretations of point processes, and borrows notation from mathematical areas of study such as measure theory and set theory.\n\nThe notation, as well as the terminology, of point processes depends on their setting and interpretation as mathematical objects which under certain assumptions can be interpreted as random sequences of points, random sets of points or random counting measures.\n\nIn some mathematical frameworks, a given point process may be considered as a sequence of points with each point randomly positioned in \"d\"-dimensional Euclidean space R as well as some other more abstract mathematical spaces. In general, whether or not a random sequence is equivalent to the other interpretations of a point process depends on the underlying mathematical space, but this holds true for the setting of finite-dimensional Euclidean space R.\n\nA point process is called \"simple\" if no two (or more points) coincide in location with probability one. Given that often point processes are simple and the order of the points does not matter, a collection of random points can be considered as a random set of points The theory of random sets was independently developed by David Kendall and Georges Matheron. In terms of being considered as a random set, a sequence of random points is a random closed set if the sequence has no accumulation points with probability one\n\nA point process is often denoted by a single letter, for example formula_1, and if the point process is considered as a random set, then the corresponding notation:\n\nis used to denote that a random point formula_3 is an element of (or belongs to) the point process formula_1. The theory of random sets can be applied to point processes owing to this interpretation, which alongside the random sequence interpretation has resulted in a point process being written as:\n\nwhich highlights its interpretation as either a random sequence or random closed set of points. Furthermore, sometimes an uppercase letter denotes the point process, while a lowercase denotes a point from the process, so, for example, the point formula_6 (or formula_7) belongs to or is a point of the point process formula_8, or with set notation, formula_9.\n\nTo denote the number of points of formula_1 located in some Borel set formula_11, it is sometimes written \n\nwhere formula_13 is a random variable and formula_14 is a counting measure, which gives the number of points in some set. In this mathematical expression the point process is denoted by:\n\nformula_1.\n\nOn the other hand, the symbol:\n\nformula_16\n\nrepresents the number of points of formula_1 in formula_11. In the context of random measures, one can write:\n\nformula_19\n\nto denote that there is the set formula_11 that contains formula_21 points of formula_22. In other words, a point process can be considered as a random measure that assigns some non-negative integer-valued measure to sets. This interpretation has motivated a point process being considered just another name for a \"random counting measure\" and the techniques of random measure theory offering another way to study point processes, which also induces the use of the various notations used in integration and measure theory. \n\nThe different interpretations of point processes as random sets and counting measures is captured with the often used notation in which:\n\n\nDenoting the counting measure again with formula_14, this dual notation implies:\n\nIf formula_29 is some measurable function on R, then the sum of formula_30 over all the points formula_31 in formula_32 can be written in a number of ways such as:\n\nwhich has the random sequence appearance, or with set notation as:\n\nor, equivalently, with integration notation as:\n\nwhere formula_36 is the space of all possible counting measures, hence putting an emphasis on the interpretation of formula_1 as a random counting measure. An alternative integration notation may be used to write this integral as:\n\nThe dual interpretation of point processes is illustrated when writing the number of formula_1 points in a set formula_11 as:\n\nwhere the indicator function formula_42 if the point formula_31 is exists in formula_11 and zero otherwise, which in this setting is also known as a Dirac measure. In this expression the random measure interpretation is on the left-hand side while the random set notation is used is on the right-hand side.\n\nThe average or expected value of a sum of functions over a point process is written as:\n\nwhere (in the random measure sense) formula_46 is an appropriate probability measure defined on the space of counting measures formula_36. The expected value of formula_48 can be written as:\n\nwhich is also known as the first moment measure of formula_1. The expectation of such a random sum, known as a \"shot noise process\" in the theory of point processes, can be calculated with .\n\nPoint processes are employed in other mathematical and statistical disciplines, hence the notation may be used in fields such stochastic geometry, spatial statistics or continuum percolation theory, and areas which use the methods and theory from these fields.\n\n"}
{"id": "3710507", "url": "https://en.wikipedia.org/wiki?curid=3710507", "title": "Proof of impossibility", "text": "Proof of impossibility\n\nA proof of impossibility, also known as negative proof, proof of an impossibility theorem, or negative result, is a proof demonstrating that a particular problem cannot be solved, or cannot be solved in general. Often proofs of impossibility have put to rest decades or centuries of work attempting to find a solution. To prove that something is impossible is usually much harder than the opposite task; it is necessary to develop a theory. Impossibility theorems are usually expressible as universal propositions in logic (see universal quantification).\n\nOne of the most famous proofs of impossibility was the 1882 proof of Ferdinand von Lindemann, showing that the ancient problem of squaring the circle cannot be solved, because the number is transcendental (non-algebraic) and only a subset of the algebraic numbers can be constructed by compass and straightedge. Two other classical problems—trisecting the general angle and doubling the cube—were also proved impossible in the nineteenth century.\n\nA problem arising in the sixteenth century was that of creating a general formula using radicals expressing the solution of any polynomial equation of fixed degree \"k\", where \"k\" ≥ 5. In the 1820s, the Abel–Ruffini theorem showed this to be impossible using concepts such as solvable groups from Galois theory, a new subfield of abstract algebra.\n\nAmong the most important proofs of impossibility of the 20th century, were those related to undecidability, which showed that there are problems that cannot be solved in general by any algorithm at all. The most famous is the halting problem.\n\nIn computational complexity theory, techniques like relativization (see oracle machine) provide \"weak\" proofs of impossibility excluding certain proof techniques. Other techniques like proofs of completeness for a complexity class provide evidence for the difficulty of problems by showing them to be just as hard to solve as other known problems that have proved intractable.\n\nOne widely used type of impossibility proof is proof by contradiction. In this type of proof it is shown that if something, such as a solution to a particular class of equations, were possible, then two mutually contradictory things would be true, such as a number being both even and odd. The contradiction implies that the original premise is impossible.\n\nOne type of proof by contradiction is proof by descent. Here it is postulated that something is possible, such as a solution to a class of equations, and that therefore there must be a smallest solution; then starting from the allegedly smallest solution, it is shown that a smaller solution can be found, contradicting the premise that the former solution was the smallest one possible. Thus the premise that a solution exists must be false.\n\nThis method of proof can also be interpreted slightly differently, as the method of \"infinite descent\". One postulates that a positive integer solution exists, whether or not it is the smallest one, and one shows that based on this solution a smaller solution must exist. But by mathematical induction it follows that a still smaller solution must exist, then a yet smaller one, and so on for an infinite number of steps. But this contradicts the fact that one cannot find smaller and smaller positive integers indefinitely; the contradiction implies that the premise that a solution exists is wrong.\n\nThere are two alternative methods of proving wrong a conjecture that something is impossible: by counterexample (constructive proof) and by logical contradiction (non-constructive proof).\n\nThe obvious way to disprove an impossibility conjecture by providing a single counterexample. For example, Euler proposed that at least \"n\" different \"n\" powers were necessary to sum to yet another \"n\" power. The conjecture was disproved in 1966 with a counterexample involving a count of only four different 5th powers summing to another fifth power:\nA proof by counterexample is a constructive proof.\n\nIn contrast, a non-constructive proof that something is \"not\" impossible proceeds by showing it is logically contradictory for \"all\" possible counterexamples to be invalid: At least \"one\" of the items on a list of possible counterexamples must actually be a valid counterexample to the impossibility conjecture. For example, a conjecture that it is impossible for an irrational power raised to an irrational power to be rational was disproved by showing that one of two possible counterexamples must be a valid counterexample, without showing which one it is.\n\nThe proof by Pythagoras (or more likely one of his students) about 500 BCE has had a profound effect on mathematics. It shows that the square root of 2 cannot be expressed as the ratio of two integers (counting numbers). The proof bifurcated \"the numbers\" into two non-overlapping collections—the rational numbers and the irrational numbers. This bifurcation was used by Cantor in his diagonal method, which in turn was used by Turing in his proof that the \"Entscheidungsproblem\" (the decision problem of Hilbert) is undecidable.\n\nProofs followed for various square roots of the primes up to 17.\n\nThere is a famous passage in Plato's \"Theaetetus\" in which it is stated that Teodorus (Plato's teacher) proved the irrationality of\ntaking all the separate cases up to the root of 17 square feet ... .\nA more general proof now exists that:\n\nThat is, it is impossible to express the \"m\"th root of an integer \"N\" as the ratio of two integers \"a\" and \"b\" that share no common prime factor except in cases in which \"b\" = 1.\n\nThree famous questions of Greek geometry were how:\n\n\nFor more than 2,000 years unsuccessful attempts were made to solve these problems; at last, in the 19th century it was proved that the desired constructions are logically impossible.\n\nA fourth problem of the ancient Greeks was to construct an equilateral polygon with a specified number \"n\" of sides, beyond the basic cases \"n\" = 3, 4, 5 that they knew how to construct.\n\nAll of these are problems in Euclidean construction, and Euclidean constructions can be done only if they involve only Euclidean numbers (by definition of the latter) (Hardy and Wright p. 159). Irrational numbers can be Euclidean. A good example is the irrational number the square root of 2. It is simply the length of the hypotenuse of a right triangle with legs both one unit in length, and it can be constructed with straightedge and compass. But it was proved centuries after Euclid that Euclidean numbers cannot involve any operations other than addition, subtraction, multiplication, division, and the extraction of square roots.\n\nBoth trisecting the general angle and doubling the cube require taking cube roots, which are not constructible numbers by compass and straightedge.\n\nformula_2 is not a Euclidean number ... and therefore it is impossible to construct, by Euclidean methods a length equal to the circumference of a circle of unit diameter\n\nA proof exists to demonstrate that any Euclidean number is an algebraic number—a number that is the solution to some polynomial equation. Therefore, because formula_2 was proved in 1882 to be a transcendental number and thus by definition not an algebraic number, it is not a Euclidean number. Hence the construction of a length formula_2 from a unit circle is impossible, and the circle cannot be squared.\n\nThe Gauss-Wantzel theorem showed in 1837 that constructing an equilateral \"n\"-gon is impossible for most values of \"n\".\n\nNagel and Newman consider the question raised by the parallel postulate to be \"...perhaps the most significant development in its long-range effects upon subsequent mathematical history\" (p. 9).\n\nThe question is: can the axiom that two parallel lines \"...will not meet even 'at infinity'\" (footnote, ibid) be derived from the other axioms of Euclid's geometry? It was not until work in the nineteenth century by \"... Gauss, Bolyai, Lobachevsky, and Riemann, that the impossibility of deducing the parallel axiom from the others was demonstrated. This outcome was of the greatest intellectual importance. ...a \"proof\" can be given of the \"impossibility of proving\" certain propositions [in this case, the parallel postlate] within a given system [in this case, Euclid's first four postulates]\". (p. 10)\n\nFermat's Last Theorem was conjectured by Pierre de Fermat in the 1600s, states the impossibility of finding solutions in positive integers for the equation formula_5 with formula_6. Fermat himself gave a proof for the \"n\" = 4 case using his technique of infinite descent, and other special cases were subsequently proved, but the general case was not proved until 1994 by Andrew Wiles.\n\nThis profound paradox presented by Jules Richard in 1905 informed the work of Kurt Gödel (cf Nagel and Newman p. 60ff) and Alan Turing. A succinct definition is found in \"Principia Mathematica\":\nKurt Gödel considered his proof to be “an analogy” of Richard's paradox, which he called “\"Richard's antinomy\"”. See more below about Gödel's proof.\n\nAlan Turing constructed this paradox with a machine and proved that this machine could not answer a simple question: will this machine be able to determine if any machine (including itself) will become trapped in an unproductive ‘infinite loop’ (i.e. it fails to continue its computation of the diagonal number).\n\nTo quote Nagel and Newman (p. 68), \"Gödel's paper is difficult. Forty-six preliminary definitions, together with several important preliminary theorems, must be mastered before the main results are reached\" (p. 68). In fact, Nagel and Newman required a 67-page introduction to their exposition of the proof. But if the reader feels strong enough to tackle the paper, Martin Davis observes that \"This remarkable paper is not only an intellectual landmark, but is written with a clarity and vigor that makes it a pleasure to read\" (Davis in Undecidable, p. 4). It is recommended that most readers see Nagel and Newman first.\n\nSo what did Gödel prove? In his own words:\n\nGödel compared his proof to \"Richard's antinomy\" (an \"antinomy\" is a contradiction or a paradox; for more see Richard's paradox):\n\n\nA number of similar undecidability proofs appeared soon before and after Turing's proof:\n\n\nFor an exposition suitable for non-specialists see Beltrami p. 108ff. Also see Franzen Chapter 8 pp. 137–148, and Davis pp. 263–266. Franzén's discussion is significantly more complicated than Beltrami's and delves into Ω—Gregory Chaitin's so-called \"halting probability\". Davis's older treatment approaches the question from a Turing machine viewpoint. Chaitin has written a number of books about his endeavors and the subsequent philosophic and mathematical fallout from them.\n\nA string is called \"(algorithmically) random\" if it cannot be produced from any shorter computer program. While most strings are random, no particular one can be proved so, except for finitely many short ones:\n\nBeltrami observes that \"Chaitin's proof is related to a paradox posed by Oxford librarian G. Berry early in the twentieth century that asks for 'the smallest positive integer that cannot be defined by an English sentence with fewer than 1000 characters.' Evidently, the shortest definition of this number must have at least 1000 characters. However, the sentence within quotation marks, which is itself a definition of the alleged number is less than 1000 characters in length!\" (Beltrami, p. 108)\n\nThe question \"Does any arbitrary \"Diophantine equation\" have an integer solution?\" is undecidable.That is, it is impossible to answer the question for all cases.\n\nFranzén introduces Hilbert's tenth problem and the MRDP theorem (Matiyasevich-Robinson-Davis-Putnam theorem) which states that \"no algorithm exists which can decide whether or not a Diophantine equation has \"any\" solution at all\". MRDP uses the undecidability proof of Turing: \"... the set of solvable Diophantine equations is an example of a computably enumerable but not decidable set, and the set of unsolvable Diophantine equations is not computably enumerable\" (p. 71).\n\nIn political science, Arrow's impossibility theorem states that it is impossible to devise a voting system that satisfies a set of five specific axioms. This theorem is proved by showing that four of the axioms together imply the opposite of the fifth.\n\nIn economics, Holmström's theorem is an impossibility theorem proving that no incentive system for a team of agents can satisfy all of three desirable criteria.\n\nIn natural science, impossibility assertions (like other assertions) come to be widely accepted as overwhelmingly probable rather than considered proved to the point of being unchallengeable. The basis for this strong acceptance is a combination of extensive evidence of something not occurring, combined with an underlying theory, very successful in making predictions, whose assumptions lead logically to the conclusion that something is impossible.\n\nTwo examples of widely accepted impossibilities in physics are perpetual motion machines, which violate the law of conservation of energy, and exceeding the speed of light, which violates the implications of special relativity. Another is the uncertainty principle of quantum mechanics, which asserts the impossibility of simultaneously knowing both the position and the momentum of a particle. Also Bell's theorem: no physical theory of local hidden variables can ever reproduce all of the predictions of quantum mechanics.\n\nWhile an impossibility assertion in science can never be absolutely proved, it could be refuted by the observation of a single counterexample. Such a counterexample would require that the assumptions underlying the theory that implied the impossibility be re-examined.\n\n\n"}
{"id": "42701243", "url": "https://en.wikipedia.org/wiki?curid=42701243", "title": "Property (mathematics)", "text": "Property (mathematics)\n\nIn mathematics, a property is any characteristic that applies to a given set. Rigorously, a property \"p\" defined for all elements of a set \"X\" is usually defined as a function \"p\": \"X\" → {true, false}, that is true whenever the property holds; or equivalently, as the subset of \"X\" for which \"p\" holds; i.e. the set {\"x\" | \"p\"(\"x\") = true}; \"p\" is its indicator function. However, it may be objected that the rigorous definition defines merely the extension of a property, and says nothing about what causes the property to hold for exactly those values.\n\nExamples of properties include the commutative property of real and complex numbers and the distributive property.\n\n"}
{"id": "837995", "url": "https://en.wikipedia.org/wiki?curid=837995", "title": "Protractor", "text": "Protractor\n\nA protractor is a measuring instrument, typically made of transparent plastic or glass, for measuring angles. Most protractors measure angles in\ndegrees (°). Radian-scale protractors measure angles in radians. Most protractors are divided into 180 equal parts.\n\nThey are used for a variety of mechanical and engineering-related applications, but perhaps the most common use is in geometry lessons in schools.\n\nSome protractors are simple half-discs. More advanced protractors, such as the bevel protractor, have one or two swinging arms, which can be used to help measure the angle.\n\nA bevel protractor is a graduated circular protractor with one pivoted arm; used for measuring or marking off angles. Sometimes Vernier scales are attached to give more precise readings. It has wide application in architectural and mechanical drawing, although its use is decreasing with the availability of modern drawing software or CAD.\n\nUniversal bevel protractors are also used by toolmakers; as they measure angles by mechanical contact they are classed as mechanical protractors.\n\nThe bevel protractor is used to establish and test angles to very close tolerances. It reads to 5 minutes or 1/12° and can measure any angle from 0° to 360°.\nThe bevel protractor consists of a beam, a graduated dial and a blade which is connected to a swivel plate (with Vernier scale) by thumb nut and clamp. When the edges of the beam and blade are parallel, a small mark on the swivel plate coincides with the zero line on the graduated dial. To measure an angle between the beam and the blade of 90° or less, the reading may be obtained direct from the graduation number on the dial indicated by the mark on the swivel plate. To measure an angle of over 90°, subtract the number of degrees as indicated on the dial from 180°, as the dial is graduated from opposite zero marks to 90° each way.\nSince the spaces, both on the main scale and the Vernier scale, are numbered both to the right and to the left from zero, any angle can be measured. The readings can be taken either to the right or to the left, according to the direction in which the zero on the main scale is moved.\n\nThe above picture illustrates a variety of uses of the bevel protractor.\nReading the Vernier scale:\n\nThe bevel protractor Vernier scale may have graduations of 5′ (minutes) or 1/12°. Each space on the Vernier scale is 5′ less than two spaces on the main scale. Twenty four spaces on the Vernier scale equal in extreme length twenty three double degrees. Thus the difference between the space occupied by 2° on a main scale and the space of the Vernier scale is equal to one twenty-fourth of 2°, or 5′.\n\nRead off directly from the main scale the number of whole degrees between 0 on this scale and the 0 of the Vernier scale. Then count, in the same direction, the number of spaces from the zero on the Vernier scale to a line that coincides with a line on the main scale; multiply this number by 5 and the product will be the number of minutes to be added to the whole number of degrees.\nFor example: Zero on the vernier scale has moved 28 whole degrees to the right of the 0 on the main scale and the 3rd line on the vernier scale coincides with a line upon the main scale as indicated. Multiplying 3 by 5, the product, 15, is the number of minutes to be added to the whole number of degrees, thus indicating a setting of 28 degrees and 15 minutes.\n\nProtractors have traditionally been one-sided. This is thought to be because early manufacturing methods set the tone of future production. Unfortunately, the two number scales on a one-sided protractor often confuse learners when first learning to measure and draw angles. However, in 2009, Jake Adamson, a maths teacher working at Musselburgh Grammar School invented and patented the first two sided protractor trademarked \"The Angler\". This was a double sided protractor with one scale on each side, avoiding the confusion of having two scales together and enabling easier measuring and drawing of angles. \"The Angler\" protractor has been widely adopted by schools in the UK.\n\n\n"}
{"id": "510629", "url": "https://en.wikipedia.org/wiki?curid=510629", "title": "Quantitative analyst", "text": "Quantitative analyst\n\nA quantitative analyst (or, in financial jargon, a quant) is a person who specializes in the application of mathematical and statistical methods – such as numerical or quantitative techniques – to financial and risk management problems. The occupation is similar to those in industrial mathematics in other industries.\n\nAlthough the original quantitative analysts were \"sell side quants\" from market maker firms, concerned with derivatives pricing and risk management, the meaning of the term has expanded over time to include those individuals involved in almost any application of mathematics in finance, including the buy side. Examples include statistical arbitrage, quantitative investment management, algorithmic trading, and electronic market making. \n\nQuantitative finance started in 1900 with Louis Bachelier's doctoral thesis \"Theory of Speculation\", which provided a model to price options under a Normal Distribution.\n\nHarry Markowitz's 1952 doctoral thesis \"Portfolio Selection\" and its published version was one of the first efforts in economics journals to formally adapt mathematical concepts to finance (mathematics was until then confined to mathematics, statistics or specialized economics journals). Markowitz formalized a notion of mean return and covariances for common stocks which allowed him to quantify the concept of \"diversification\" in a market. He showed how to compute the mean return and variance for a given portfolio and argued that investors should hold only those portfolios whose variance is minimal among all portfolios with a given mean return. Although the language of finance now involves Itō calculus, management of risk in a quantifiable manner underlies much of the modern theory.\n\nIn 1965 Paul Samuelson introduced stochastic calculus into the study of finance. In 1969 Robert Merton promoted continuous stochastic calculus and continuous-time processes. Merton was motivated by the desire to understand how prices are set in financial markets, which is the classical economics question of \"equilibrium,\" and in later papers he used the machinery of stochastic calculus to begin investigation of this issue.\n\nAt the same time as Merton's work and with Merton's assistance, Fischer Black and Myron Scholes developed the Black–Scholes model, which was awarded the 1997 Nobel Memorial Prize in Economic Sciences. It provided a solution for a practical problem, that of finding a fair price for a European call option, i.e., the right to buy one share of a given stock at a specified price and time. Such options are frequently purchased by investors as a risk-hedging device. In 1981, Harrison and Pliska used the general theory of continuous-time stochastic processes to put the Black–Scholes model on a solid theoretical basis, and showed how to price numerous other derivative securities.\n\nEmanuel Derman's 2004 book \"My Life as a Quant\" helped to both make the role of a quantitative analyst better known outside of finance, and to popularize the abbreviation \"quant\" for a quantitative analyst.\n\nQuantitative analysts often come from applied mathematics, physics or engineering backgrounds rather than economics-related fields, and quantitative analysis is a major source of employment for people with mathematics and physics PhD degrees, or with financial mathematics DEA degrees in the French education system. Typically, a quantitative analyst will also need extensive skills in computer programming, most commonly C, C++, Java, R, MATLAB, Mathematica, Python.\n\nThis demand for quantitative analysts has led to a resurgence in demand for actuarial qualifications as well as creation of specialized Masters and PhD courses in financial engineering, mathematical finance, computational finance, and/or financial reinsurance. In particular, Master's degrees in mathematical finance, financial engineering, operations research, computational statistics, machine learning, and financial analysis are becoming more popular with students and with employers. See Master of Quantitative Finance; Master of Financial Economics.\n\nData science and machine learning analysis and modelling methods are being increasingly employed in portfolio performance and portfolio risk modelling, and as such data science and machine learning Master's graduates are also in demand as quantitative analysts.\n\nIn sales & trading, quantitative analysts work to determine prices, manage risk, and identify profitable opportunities. Historically this was a distinct activity from trading but the boundary between a desk quantitative analyst and a quantitative trader is increasingly blurred, and it is now difficult to enter trading as a profession without at least some quantitative analysis education. In the field of algorithmic trading it has reached the point where there is little meaningful difference. Front office work favours a higher speed to quality ratio, with a greater emphasis on solutions to specific problems than detailed modeling. FOQs typically are significantly better paid than those in back office, risk, and model validation. Although highly skilled analysts, FOQs frequently lack software engineering experience or formal training, and bound by time constraints and business pressures tactical solutions are often adopted.\n\nQuantitative analysis is used extensively by asset managers. Some, such as FQ, AQR or Barclays, rely almost exclusively on quantitative strategies while others, such as Pimco, Blackrock or Citadel use a mix of quantitative and fundamental methods.\n\nMajor firms invest large sums in an attempt to produce standard methods of evaluating prices and risk. These differ from front office tools in that Excel is very rare, with most development being in C++, though Java and C# are sometimes used in non-performance critical tasks. LQs spend more time modeling ensuring the analytics are both efficient and correct, though there is tension between LQs and FOQs on the validity of their results. LQs are required to understand techniques such as Monte Carlo methods and finite difference methods, as well as the nature of the products being modeled.\n\nOften the highest paid form of Quant, ATQs make use of methods taken from signal processing, game theory, gambling Kelly criterion, market microstructure, econometrics, and time series analysis. Algorithmic trading includes statistical arbitrage, but includes techniques largely based upon speed of response, to the extent that some ATQs modify hardware and Linux kernels to achieve ultra low latency.\n\nThis has grown in importance in recent years, as the credit crisis exposed holes in the mechanisms used to ensure that positions were correctly hedged, though in no bank does the pay in risk approach that in front office. A core technique is value at risk, and this is backed up with various forms of stress test (financial), economic capital analysis and direct analysis of the positions and models used by various bank's divisions.\n\nIn the aftermath of the financial crisis, there surfaced the recognition that quantitative valuation methods were generally too narrow in their approach. An agreed upon fix adopted by numerous financial institutions has been to improve collaboration.\n\nModel validation (MV) takes the models and methods developed by front office, library, and modeling quantitative analysts and determines their validity and correctness. The MV group might well be seen as a superset of the quantitative operations in a financial institution, since it must deal with new and advanced models and trading techniques from across the firm. Before the crisis however, the pay structure in all firms was such that MV groups struggle to attract and retain adequate staff, often with talented quantitative analysts leaving at the first opportunity. This gravely impacted corporate ability to manage model risk, or to ensure that the positions being held were correctly valued. An MV quantitative analyst would typically earn a fraction of quantitative analysts in other groups with similar length of experience. In the years following the crisis, this has changed. Regulators now typically talk directly to the quants in the middle office such as the model validators, and since profits highly depend of the regulatory infrastructure, model validation has gained in weight and importance with respect to the quants in the front office.\n\nQuantitative developers are computer specialists that assist, implement and maintain the quantitative models. They tend to be highly specialised language technicians that bridge the gap between software developer and quantitative analysts.\n\nBecause of their backgrounds, quantitative analysts draw from various forms of mathematics: statistics and probability, calculus centered around partial differential equations, linear algebra, discrete mathematics, and econometrics. Some on the buy side may use machine learning. The\nmajority of quantitative analysts have received little formal education in mainstream economics, and often apply a mindset drawn from the physical sciences. Quants use mathematical skills learned from diverse fields such as computer science, physics and engineering. These skills include (but are not limited to) advanced statistics, linear algebra and partial differential equations as well as solutions to these based upon numerical analysis.\n\nCommonly used numerical methods are:\n\nA typical problem for a mathematically oriented quantitative analyst would be to develop a model for pricing, hedging, and risk-managing a complex derivative product. These quantitative analysts tend to rely more on numerical analysis than statistics and econometrics. The mindset is to prefer a deterministically \"correct\" answer, as once there is agreement on input values and market variable dynamics, there is only one correct price for any given security (which can be demonstrated, albeit often inefficiently, through a large volume of Monte Carlo simulations).\n\nA typical problem for a statistically oriented quantitative analyst would be to develop a model for deciding which stocks are relatively expensive and which stocks are relatively cheap. The model might include a company's book value to price ratio, its trailing earnings to price ratio, and other accounting factors. An investment manager might implement this analysis by buying the underpriced stocks, selling the overpriced stocks, or both. Statistically oriented quantitative analysts tend to have more of a reliance on statistics and econometrics, and less of a reliance on sophisticated numerical techniques and object-oriented programming. These quantitative analysts tend to be of the psychology that enjoys trying to find the best approach to modeling data, and can accept that there is no \"right answer\" until time has passed and we can retrospectively see how the model performed. Both types of quantitative analysts demand a strong knowledge of sophisticated mathematics and computer programming proficiency.\n\nOne of the principal mathematical tools of quantitative finance is stochastic calculus.\n\n\n\n\n\n\n"}
{"id": "19985265", "url": "https://en.wikipedia.org/wiki?curid=19985265", "title": "Red auxiliary number", "text": "Red auxiliary number\n\nIn the study of ancient Egyptian mathematics, red auxiliary numbers are numbers written in red ink in the Rhind Mathematical Papyrus, apparently used as aids for arithmetic computations involving fractions.\n\n"}
{"id": "45289334", "url": "https://en.wikipedia.org/wiki?curid=45289334", "title": "Symbol (number theory)", "text": "Symbol (number theory)\n\nIn number theory, a symbol is any of many different generalizations of the Legendre symbol. This article describes the relations between these various generalizations.\n\nThe symbols below are arranged roughly in order of the date they were introduced, which is usually (but not always) in order of increasing generality.\n\n\n"}
{"id": "222390", "url": "https://en.wikipedia.org/wiki?curid=222390", "title": "Table of prime factors", "text": "Table of prime factors\n\nThe tables contain the prime factorization of the natural numbers from 1 to 1000.\n\nWhen \"n\" is a prime number, the prime factorization is just \"n\" itself, written in bold below.\n\nThe number 1 is called a unit. It has no prime factors and is neither prime nor composite.\n\n\"See also: Table of divisors\" (prime and non-prime divisors for 1 to 1000)\nMany properties of a natural number \"n\" can be seen or directly computed from the prime factorization of \"n\".\nThe divisors of \"n\" are all products of some or all prime factors of \"n\" (including the empty product 1 of no prime factors).\nThe number of divisors can be computed by increasing all multiplicities by 1 and then multiplying them.\nDivisors and properties related to divisors are shown in table of divisors.\n\n"}
{"id": "47243080", "url": "https://en.wikipedia.org/wiki?curid=47243080", "title": "The Art of Mathematics", "text": "The Art of Mathematics\n\nThe Art of Mathematics (), written by Hong Sung-Dae (), is a series of mathematics textbooks for high school students in South Korea. First published in 1966, it is aguably the best-selling mathematics textbook series in South Korea, with about 37 million copies sold as of 2006. In Jeongeup, North Jeolla Province, the hometown of Hong Sung-Dae, a street is named in honor of the author.'\n\nChanges in the 11th edition, published 2013-2015, reflect the 2009 revision of South Korea's National Curriculum ( icheon-gu gaejeong gyoyuggwajeong). Each of the six volumes consist of two versions, one for average students ( Gibon-pyeon) and one for higher-ability students ().\n\n\n\n\n\n\n\n"}
{"id": "34038330", "url": "https://en.wikipedia.org/wiki?curid=34038330", "title": "Theta model", "text": "Theta model\n\nThe theta model, or Ermentrout–Kopell canonical model, is a biological neuron model originally developed to model neurons in the animal Aplysia, and later used in various fields of computational neuroscience. The model is particularly well suited to describe neuron bursting, which are rapid oscillations in the membrane potential of a neuron interrupted by periods of relatively little oscillation. Bursts are often found in neurons responsible for controlling and maintaining steady rhythms. For example, breathing is controlled by a small network of bursting neurons in the brain stem. Of the three main classes of bursting neurons (square wave bursting, parabolic bursting, and elliptic bursting), the theta model describes parabolic bursting. Parabolic bursting is characterized by a series of bursts that are regulated by a slower external oscillation. This slow oscillation changes the frequency of the faster oscillation so that the frequency curve of the burst pattern resembles a parabola.\n\nThe model has just one state variable which describes the membrane voltage of a neuron. In contrast, the Hodgkin–Huxley model consists of four state variables (one voltage variable and three gating variables) and the Morris–Lecar model is defined by two state variables (one voltage variable and one gating variable). The single state variable of the theta model, and the elegantly simple equations that govern its behavior allow for analytic, or closed-form solutions (including an explicit expression for the phase response curve). The dynamics of the model take place on the unit circle, and are governed by two cosine functions and a real-valued input function.\n\nSimilar models include the quadratic integrate and fire (QIF) model, which differs from the theta model by only by a change of variables and Plant's model, which consists of Hodgkin–Huxley type equations and also differs from the theta model by a series of coordinate transformations.\n\nDespite its simplicity, the theta model offers enough complexity in its dynamics that it has been used for a wide range of theoretical neuroscience research as well as in research beyond biology, such as in artificial intelligence.\n\nBursting is \"an oscillation in which an observable [part] of the system, such as voltage or chemical concentration, changes periodically between an active phase of rapid spike oscillations (the fast sub-system) and a phase of quiescence\". Bursting comes in three distinct forms: square wave bursting, parabolic bursting, and elliptic bursting. There exist some models that do not fit neatly into these categories by qualitative observation, but it is possible to sort such models by their topology (i.e. such models can be sorted \"by the structure of the fast subsystem\").\n\nAll three forms of bursting are capable of beating and periodic bursting. Periodic bursting (or just bursting) is of more interest because many phenomena are controlled by, or arise from, bursting. For example, bursting due to a changing membrane potential is common in various neurons, including but not limited to cortical chattering neurons, thalamacortical neurons, and pacemaker neurons. Pacemakers in general are known to burst and synchronize as a population, thus generating a robust rhythm that can maintain repetitive tasks like breathing, walking, and eating. Beating occurs when a cell bursts continuously with no periodic quiescent periods, but beating is often considered to be an extreme case and is rarely of primary interest.\n\nBursting cells are important for motor generation and synchronization. For example, the pre-Bötzinger complex in the mammalian brain stem contains many bursting neurons that control autonomous breathing rhythms. Various neocortical neurons (i.e. cells of the neocortex) are capable of bursting, which \"contribute significantly to [the] network behavior [of neocortical neurons]\". The R15 neuron of the abdominal ganglion in \"Aplyisa\", hypothesized to be a [pneurosecretory[] cell (i.e. a cell that produces hormones), is known to produce bursts characteristic of neurosecretory cells. In particular, it is known to produce parabolic bursts.\n\nSince many biological processes involve bursting behavior, there is a wealth of various bursting models in scientific literature. For instance, there exist several models for interneurons and cortical spiking neurons. However, the literature on parabolic bursting models is relatively scarce.\n\nParabolic bursting models are mathematical models that mimic parabolic bursting in real biological systems. Each burst of a parabolic burster has a characteristic feature in the burst structure itself – the frequency at the beginning and end of the burst is low relative to the frequency in the middle of the burst. A frequency plot of one burst resembles a parabola, hence the name \"parabolic burst\". Furthermore, unlike elliptic or square-wave bursting, there is a slow modulating wave which, at its peak, excites the cell enough to generate a burst and inhibits the cell in regions near its minimum. As a result, the neuron periodically transitions between bursting and quiescence.\n\nParabolic bursting has been studied most extensively in the R15 neuron, which is one of six types of neurons of the \"Aplysia\" abdominal ganglion and one of thirty neurons comprising the abdominal ganglion. The \"Aplysia\" abdominal ganglion was studied and extensively characterized because its relatively large neurons and proximity of the neurons to the surface of the ganglion made it an ideal and \"valuable preparation for cellular electrophysical studies\".\n\nEarly attempts to model parabolic bursting were for specific applications, often related to studies of the R15 neuron. This is especially true of R. E. Plant and Carpenter, whose combined works comprise the bulk of parabolic bursting models prior to Ermentrout and Kopell's canonical model.\n\nThough there was no specific mention of the term \"parabolic bursting\" in Plant's papers, Plant's model(s) do involve a slow, modulating oscillation which control bursting in the model(s). This is, by definition, parabolic bursting. Both of Plant's papers on the topic involve a model derived from the Hodgkin–Huxley equations and include extra conductances, which only add to the complexity of the model.\n\nCarpenter developed her model primarily for a square wave burster. The model was capable of producing a small variety of square wave bursts and produced parabolic bursts as a consequence of adding an extra conductance. However, the model applied to only spatial propagation down axons and not situations where oscillations are limited to a small region in space (i.e. it was not suited for \"space-clamped\" situations).\n\nThe lack of a simple, generalizable, space-clamped, parabolic bursting model motivated Ermentrout and Kopell to develop the theta model.\n\nIt is possible to describe a multitude of parabolic bursting cells by deriving a simple mathematical model, called a canonical model. Derivation of the Ermentrout and Kopell canonical model begins with the general form for parabolic bursting, and notation will be fixed to clarify the discussion. The letters formula_1, formula_2, formula_3, formula_4 are reserved for functions; formula_5, formula_6, formula_7 for state variables; formula_8, formula_9, and formula_10 for scalars.\n\nIn the following generalized system of equations for parabolic bursting, the values of formula_1 describe the membrane potential and ion channels, typical of many conductance-based biological neuron models. Slow oscillations are controlled by formula_3, and ultimately described by formula_6. These slow oscillations can be, for example, slow fluctuations in calcium concentration inside a cell. The function formula_2 couples formula_15 to formula_16, thereby allowing the second system, formula_15, to influence the behavior of the first system, formula_16. In more succinct terms, \"formula_5 generates the spikes and formula_6 generates the slow waves\". The equations are:\n\nwhere formula_5 is a vector with formula_9 entries (i.e. formula_25), formula_6 is a vector with formula_10 entries (i.e. formula_28), formula_8 is small and positive, and formula_1, formula_2, formula_3 are smooth (i.e. infinitely differentiable). Additional constraints are required to guarantee parabolic bursting. First, formula_33 must produce a circle in phase space that is invariant, meaning it does not change under certain transformations. This circle must also be attracting in formula_34 with a critical point located at formula_35. The second criterion requires that when formula_36, there exists a stable limit cycle solution. These criteria can be summarized by the following points:\n\n\nThe theta model can be used in place of any parabolic bursting model that satisfies the assumptions above.\n\nThe theta model is a reduction of the generalized system from the previous section and takes the form,\n\nThis model is one of the simplest excitable neuron models. The state variable formula_7 represents the angle in radians, and the input function, formula_44, is typically chosen to be periodic. Whenever formula_7 reaches the value formula_46, the model is said to produce a spike.\n\nThe theta model is capable of a single saddle-node bifurcation and can be shown to be the \"normal form for the saddle-node on a limit cycle bifurcation\" (SNIC). When formula_47, the system is excitable, i.e., given an appriate perturbation the system will produce a spike. Incidentally, when viewed in the plane (formula_34), the unstable critical point is actually a saddle point because formula_49 is attracting in formula_34. When formula_51, formula_52 is also positive, and the system will give rise to a limit cycle. Therefore, the bifurcation point is located at formula_53.\n\nNear the bifurcation point, the theta model resembles the quadratic integrate and fire model:\n\nFor I > 0, the solutions \"blow up\" rather quickly. By resetting the trajectory formula_55 to formula_56 when it reaches formula_57, the total period is then\n\nTherefore, the period diverges as formula_59 and the frequency converges to zero.\n\nWhen formula_44 is some slow wave which can be both negative and positive, the system is capable of producing parabolic bursts. Consider the simple example formula_61, where formula_62 is relatively small. Then for formula_63, formula_44 is strictly positive and formula_7 makes multiple passes through the angle formula_66, resulting in multiple bursts. Note that whenever formula_67 is near zero or formula_66, the theta neuron will spike at relatively a low frequency, and whenever formula_67 is near formula_70 the neuron will spike with very high frequency. When formula_71, the frequency of spikes is zero since the period is infinite since formula_7 can no longer pass through formula_46. Finally, for formula_74, the neuron is excitable and will no longer burst. This qualitative description highlights the characteristics that make the theta model a parabolic bursting model. Not only does the model have periods of quiescence between bursts which are modulated by a slow wave, but the frequency of spikes at the beginning and end of each burst is high relative to the frequency at the middle of the burst.\n\nThe derivation comes in the form of two lemmas in Ermentrout and Kopell (1986). Lemma 1, in summary, states that when viewing the general equations above in a subset formula_75, the equations take the form:\n\nBy lemma 2 in Ermentrout and Kopell 1986, \"There exists a change of coordinates... and a constant, c, such that in new coordinates, the two equations above converge pointwise as formula_78 to the equations\n\nfor all formula_81. Convergence is uniform except near formula_46.\" (Ermentrout and Kopell, 1986). By letting formula_83, resemblance to the theta model is obvious.\n\nIn general, given a scalar phase model of the form\n\nwhere formula_85 represents the perturbation current, a closed form solution of the phase response curve (PRC) does not exist.\n\nHowever, the theta model is a special case of such an oscillator and happens to have a closed-form solution for the PRC. The theta model is recovered by defining formula_1 and formula_2 as\n\nIn the appendix of Ermentrout 1996, the PRC is shown to be formula_90.\n\nThe authors of Soto-Treviño et al. (1996) discuss in great detail the similarities between Plant's (1976) model and the theta model. At first glance, the mechanisms of bursting in both systems are very different: In Plant's model, there are two slow oscillations – one for conductance of a specific current and one for the concentration of calcium. The calcium oscillations are active only when the membrane potential is capable of oscillating. This contrasts heavily against the theta model in which one slow wave modulates the burst of the neuron and the slow wave has no dependence upon the bursts. Despite these differences, the theta model is shown to be similar to Plant's (1976) model by a series of coordinate transformations. In the process, Soto-Trevino, et al. discovered that the theta model was more general than originally believed.\n\nThe quadratic integrate-and-fire (QIF) model was created by Latham et al. in 2000 to explore the many questions related to networks of neurons with low firing rates. It was unclear to Latham et al. why networks of neurons with \"standard\" parameters were unable to generate sustained low frequency firing rates, while networks with low firing rates were often seen in biological systems.\n\nAccording to Gerstner and Kistler (2002), the quadratic integrate-and-fire (QIF) model is given by the following differential equation:\n\nwhere formula_92 is a strictly positive scalar, formula_93 is the membrane potential, formula_94 is the resting potential formula_95 is the minimum potential necessary for the membrane to produce an action potential, formula_96 is the membrane resistance, formula_97 the membrane time constant and formula_98. When there is no input current (i.e. formula_99), the membrane potential quickly returns to rest following a perturbation. When the input current, formula_4, is large enough, the membrane potential (formula_93) surpasses its firing threshold and rises rapidly (indeed, it reaches arbitrarily large values in finite time); this represents the peak of the action potential. To simulate the recovery after the action potential, the membrane voltage is then reset to a lower value formula_102. To avoid dealing with arbitrarily large values in simulation, researchers will often set an upper limit on the membrane potential, above which the membrane potential will be reset; for example Latham et al. (2000) reset the voltage from +20 mV to −80 mV. This voltage reset constitutes an action potential.\n\nThe theta model is very similar to the QIF model since the theta model differs from the QIF model by means of a simple coordinate transform. By scaling the voltage appropriately and letting formula_103 be the change in current from the minimum current required to elicit a spike, the QIF model can be rewritten in the form\n\nSimilarly, the theta model can be rewritten as\n\nThe following proof will show that the QIF model becomes the theta model given an appropriate choice for the coordinate transform.\n\nDefine formula_106. Recall that formula_107, so taking the derivative yields\n\nAn additional substitution and rearranging in terms of formula_7 yields\n\nUsing the trigonometric identities formula_111, formula_112 and formula_52 as defined above, we have that\n\nTherefore, there exists a change of coordinates, namely formula_106, which transforms the QIF model into the theta model. The reverse transformation also exists, and is attained by taking the inverse of the first transformation.\n\nThough the theta model was originally used to model slow cytoplasmic oscillations that modulate fast membrane oscillations in a single cell, Ermentrout and Kopell found that the theta model could be applied just as easily to systems of two electrically coupled cells such that the slow oscillations of one cell modulates the bursts of the other. Such cells serve as the central pattern generator (CPG) of the pyloric system in the lobster stomatograstic ganglion. In such a system, a slow oscillator, called the anterior burster (AB) cell, modulates the bursting cell called the pyloric dilator (PD), resulting in parabolic bursts.\n\nA group led by Boergers, used the theta model to explain why exposure to multiple simultaneous stimuli can reduce the response of the visual cortex below the normal response from a single (preferred) stimulus. Their computational results showed that this may happen due to strong stimulation of a large group of inhibitory neurons. This effect not only inhibits neighboring populations, but has the extra consequence of leaving the inhibitory neurons in disarray, thus increasing the effectiveness of inhibition.\n\nOsan et al. (2002) found that in a network of theta neurons, there exist two different types of waves that propagate smoothly over the network, given a sufficiently large coupling strength. Such traveling waves are of interest because they are frequently observed in pharmacologically treated brain slices, but are hard to measure in intact animals brains. The authors used a network of theta models in favor of a network of leaky integrate-and-fire (LIF) models due to two primary advantages: first, the theta model is continuous, and second, the theta model retains information about \"the delay between the crossing of the spiking threshold and the actual firing of an action potential\". The LIF fails to satisfy both conditions.\n\nThe theta model can also be applied to research beyond the realm of biology. McKennoch et al. (2008) derived a steepest gradient descent learning rule based on theta neuron dynamics. Their model is based on the assumption that \"intrinsic neuron dynamics are sufficient to achieve consistent time coding, with no need to involve the precise shape of postsynaptic currents...\" contrary to similar models like SpikeProp and Tempotron, which depend heavily on the shape of the postsynaptic potential (PSP). Not only could the multilayer theta network perform just about as well as Tempotron learning, but the rule trained the multilayer theta network to perform certain tasks neither SpikeProp nor Tempotron were capable of.\n\nAccording to Kopell and Ermentrout (2004), a limitation of the theta lies in its relative difficulty in electrically coupling two theta neurons. It is possible to create large networks of theta neurons – and much research has been done with such networks – but it may be advantageous to use Quadratic Integrate-and-Fire (QIF) neurons, which allow for electrical coupling in a \"straightforward way\".\n\n\n\n"}
{"id": "41400343", "url": "https://en.wikipedia.org/wiki?curid=41400343", "title": "Timeline of women in mathematics", "text": "Timeline of women in mathematics\n\nThis is a timeline of women in mathematics.\n\n350–370 until 415: The lifetime of Hypatia, a Greek Alexandrine Neoplatonist philosopher in Egypt who was the first well-documented woman in mathematics.\n\n1748: Italian mathematician Maria Agnesi published the first book discussing both differential and integral calculus, called \"Instituzioni analitiche ad uso della gioventù italiana\".\n\n1759: French mathematician Émilie du Châtelet's translation and commentary on Isaac Newton's work \"Principia Mathematica\" was published posthumously; it is still considered the standard French translation.\n\n1827: French mathematician Sophie Germain saw her theorem, known as Germain's Theorem, published in a footnote of a book by the mathematician Adrien-Marie Legendre. In this theorem Germain proved that if \"x\", \"y\", and \"z\" are integers and if \"x\" + \"y\" = \"z\" then either \"x\", \"y\", or \"z\" must be divisible by 5. Germain's theorem was a major step toward proving Fermat's last theorem for the case where n equals 5.\n\n1829: The first public examination of an American girl in geometry was held.\n\n1874: Russian mathematician Sofia Kovalevskaya became the first woman in modern Europe to gain a doctorate in mathematics, which she earned from the University of Göttingen in Germany. \n\n1880: Charlotte Angas Scott of Britain obtained special permission to take the Cambridge Mathematical Tripos Exam, as women were not normally allowed to sit for the exam. She came eighth on the Tripos of all students taking them, but due to her sex, the title of \"eighth wrangler,\" a high honour, went officially to a male student. At the ceremony, however, after the seventh wrangler had been announced, all the students in the audience shouted her name. Because she could not attend the award ceremony, Scott celebrated her accomplishment at Girton College where there were cheers and clapping at dinner, and a special evening ceremony where the students sang \"See the Conquering Hero Comes\", and she received an ode written by a staff member, and was crowned with laurels. \n\n1886: Winifred Edgerton Merrill became the first American woman to earn a PhD in mathematics, which she earned from Columbia University.\n\n1888: The Kovalevskaya top, one of a brief list of known examples of integrable rigid body motion, was discovered by Sofia Kovalevskaya.\n\n1889: Sofia Kovalevskaya was appointed as the first female professor in Northern Europe, at the University of Stockholm.\n\n1890: British woman Philippa Fawcett became the first woman to obtain the top score in the Cambridge Mathematical Tripos Exam.\n\n1913: American mathematician Mildred Sanderson published her theorem about modular invariants in her thesis. It states: “To any modular invariant i of a system of forms under any group G of linear transformations with coefficients in the GF[pn], there corresponds a formal invariant I under G such that I = i for all sets of values in the field of the coefficients of the system of forms.” She was Leonard Dickson’s first female graduate student, and he later wrote of her thesis, “This paper is a highly important contribution to this field of work; its importance lies partly in the fact that it establishes a correspondence between modular and formal invariants. Her main theorem has already been frequently quoted on account of its fundamental character. Her proof is a remarkable piece of mathematics.” E.T. Bell wrote, “Miss Sanderson’s single contribution (1913) to modular invariants has been rated by competent judges as one of the classics of the subject.”\n\n1918: German mathematician Emmy Noether published Noether's (first) theorem, which states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. \n\n1927: American mathematician Anna Pell-Wheeler became the first woman to present a lecture at the American Mathematical Society Colloquium.\n\n1930: Cecilia Kreiger became the first woman to earn a PhD in mathematics in Canada, at the University of Toronto.\n\n1930s: British mathematician Mary Cartwright proved her theorem, now known as Cartwright's theorem, which gives an estimate for the maximum modulus of an analytic function that takes the same value no more than p times in the unit disc. To prove the theorem she used a new approach, applying a technique introduced by Lars Ahlfors for conformal mappings.\n\n1943: Euphemia Haynes became the first African-American woman to earn a Ph.D. in mathematics, which she earned from Catholic University.\n\n1949: American mathematician Gertrude Mary Cox became the first woman elected into the International Statistical Institute.\n\n1956: American mathematician Gladys West began collecting data from satellites at the Naval Surface Warfare Center Dahlgren Division. Her calculations directly impacted the development of accurate GPS systems.\n\n1962: American mathematician Mina Rees became the first woman to win the Yueh-Gin Gung and Dr. Charles Y. Hu Award for Distinguished Service to Mathematics, which is the most prestigious award made by the Mathematical Association of America.\n\n1964: Mary Cartwright became the first woman to win the Sylvester Medal of the Royal Society of London, which is given every three years since 1901 for the encouragement of mathematical research, without regard to nationality.\n\n1966: American mathematician and physics professor Mary L. Boas published \"Mathematical Methods in the Physical Sciences\", which was still widely used in college classrooms as of 1999.\n\n1968: Mary Cartwright became the first woman to win the De Morgan Medal, the London Mathematical Society's premier award.\n\n1970: American mathematician Mina Rees became the first female president of the American Association for the Advancement of Science.\n\n1971: American mathematician Mary Ellen Rudin constructed the first Dowker space.\n\n1971: The Association for Women in Mathematics (AWM) was founded. It is a professional society whose mission is to encourage women and girls to study and to have active careers in the mathematical sciences, and to promote equal opportunity for and the equal treatment of women and girls in the mathematical sciences. It is incorporated in America in the state of Massachusetts.\n\n1971: The Joint Committee on Women in the Mathematical Sciences (JCW), was founded as a committee of the American Mathematical Society (AMS). It is now a joint committee of seven mathematical and statistical societies which works to identify mechanisms for the enhancement of opportunities for women in the mathematical and statistical sciences, recommend actions to the governing bodies of the member societies in support of these opportunities, and document its recommendations by presenting data.\n\n1973: American mathematician Jean Taylor published her dissertation on “Regularity of the Singular Set of Two-Dimensional Area-Minimizing Flat Chains Modulo 3 in R3” which solved a long-standing problem about length and smoothness of soap-film triple function curves.\n\n1974: American mathematician Joan Birman published the book \"Braids, Links, and Mapping Class Groups\". It has become a standard introduction, with many of today’s researchers having learned the subject through it.\n\n1975–1977: American amateur mathematician Marjorie Rice, who had no formal training in mathematics beyond high school, discovered three new types of tessellating pentagons and more than sixty distinct tessellations by pentagons.\n\n1975: American mathematician Julia Robinson became the first female mathematician elected to the National Academy of Sciences.\n\n1979: Mary Ellen Rudin became the first woman to present the Earle Raymond Hedrick Lectures; these lectures were established by the Mathematical Association of America in 1952 to present to the Association a lecturer of known skill as an expositor of mathematics \"who will present a series of at most three lectures accessible to a large fraction of those who teach college mathematics.\"\n\n1979: American mathematician Dorothy Lewis Bernstein became the first female president of the Mathematical Association of America.\n\n1981: Canadian-American mathematician Cathleen Morawetz became the first female mathematician to give a Josiah Willard Gibbs Lecture; these lectures are of a semi-popular nature and are given by invitation, and are usually devoted to mathematics or its applications.\n\n1981: American mathematician Doris Schattschneider became the first female editor of \"Mathematics Magazine.\"\n\n1983: Julia Robinson became the first female president of the American Mathematical Society.\n\n1983: Julia Robinson became the first female mathematician to be awarded a MacArthur Fellowship.\n\n1988: Doris Schattschneider became the first woman to present the J. Sutherland Frame Lectures, which are presented at the summer meeting of the Mathematical Association of America.\n\n1992: American mathematician Gloria Gilmer became the first woman to deliver a major National Association of Mathematicians lecture (it was the Cox–Talbot address).\n\n1995: American mathematician Margaret Wright became the first female president of the Society for Industrial and Applied Mathematics.\n\n1995: Israeli-Canadian mathematician Leah Edelstein-Keshet became the first female president of the Society for Mathematical Biology.\n\n1996: Joan Birman became the first woman to receive the Chauvenet Prize, which is awarded annually by the Mathematical Association of America to the author of an outstanding expository article on a mathematical topic by a member of the association.\n\n1996: Ioana Dumitriu, a New York University sophomore from Romania, became the first woman to be named a Putnam Fellow. Putnam Fellows are the top five (or six, in case of a tie) scorers on The William Lowell Putnam Mathematical Competition.\n\n1998: Melanie Wood became the first female American to make the U.S. International Math Olympiad Team. She won silver medals in the 1998 and 1999 International Mathematical Olympiads.\n\n2002: Susan Howson became the first woman to win the Adams Prize, given annually by the University of Cambridge to a British mathematician under the age of 40.\n\n2002: Melanie Wood became the first American woman and second woman overall to be named a Putnam Fellow in 2002.\n\n2004: Melanie Wood became the first woman to win the Frank and Brennie Morgan Prize for Outstanding Research in Mathematics by an Undergraduate Student. It is an annual award given to an undergraduate student in the US, Canada, or Mexico who demonstrates superior mathematics research.\n\n2004: American Alison Miller became the first ever female gold medal winner on the U.S. International Math Olympiad Team.\n\n2006: Polish-Canadian mathematician Nicole Tomczak-Jaegermann became the first woman to win the CRM-Fields-PIMS prize, which recognizes exceptional achievement in the mathematical sciences.\n\n2006: Stefanie Petermichl, a German mathematical analyst then at the University of Texas at Austin, became the first woman to win the Salem Prize, an annual award given to young mathematicians considered to have done outstanding work in Raphael Salem's field of interest, primarily Fourier series and related areas in analysis. She shared the prize with Artur Avila.\n\n2012: Latvian mathematician Daina Taimina became the first woman to win the Euler Book Prize, which is awarded annually to an author or authors of an outstanding book about mathematics, for her book \"Crocheting Adventures with Hyperbolic Planes.\"\n\n2012: The Working Committee for Women in Mathematics, Chinese Mathematical Society (WCWM-CMS) was founded; it is a national non-profit academic organization in which female mathematicians who are engaged in research, teaching, and applications of mathematics can share their scientific research through academic exchanges both in China and abroad. It is one of the branches of the Chinese Mathematical Society (CMS).\n\n2014: Maryam Mirzakhani became the first woman as well as the first Iranian to be awarded the Fields Medal, which she was awarded for \"her outstanding contributions to the dynamics and geometry of Riemann surfaces and their moduli spaces.\" She shared the prize with Martin Hairer, Manjul Bhargava, and Artur Avila. It is a prize awarded to two, three, or four mathematicians not over 40 years of age at each International Congress of the International Mathematical Union, and is often viewed as the greatest honor a mathematician can receive.\n\n2016: French mathematician Claire Voisin received the CNRS Gold medal, the highest scientific research award in France.\n\nTimeline of women in mathematics in the United States\n"}
{"id": "1458024", "url": "https://en.wikipedia.org/wiki?curid=1458024", "title": "Toy theorem", "text": "Toy theorem\n\nIn mathematics, a toy theorem is a simplified version (special case) of a more general theorem. For instance, by introducing some simplifying assumptions in a theorem, one obtains a toy theorem.\n\nUsually, a toy theorem is used to illustrate the claim of a theorem. It can also be insightful to study proofs of a toy theorem derived from a non-trivial theorem. Toy theorems can also have education value. After presenting a theorem (with, say, a highly non-trivial proof), one can sometimes give some assurance that the theorem really holds, by proving a toy version of the theorem.\n\nFor instance, a toy theorem of the Brouwer fixed-point theorem is obtained by restricting the dimension to one. In this case, the Brouwer fixed-point theorem follows almost immediately from the intermediate value theorem.\n\n"}
{"id": "34882466", "url": "https://en.wikipedia.org/wiki?curid=34882466", "title": "Turing pattern", "text": "Turing pattern\n\nThe concept of a Turing pattern (often referred to in the plural as Turing patterns) was introduced by the English mathematician Alan Turing in a 1952 paper entitled \"The Chemical Basis of Morphogenesis\". This foundational paper describes the way in which patterns in nature such as stripes and spots can arise naturally out of a homogeneous, uniform state.\n\nThe original theory, a reaction–diffusion theory of morphogenesis, has served as an important model in theoretical biology. Reaction–diffusion systems have attracted much interest as a prototype model for pattern formation. Patterns such as fronts, hexagons, spirals, stripes and dissipative solitons are found in various types of reaction-diffusion systems, despite large discrepancies, in the local reaction terms for example.\n\nAs well as in biological organisms, Turing patterns occur in other natural systems – for example, the wind patterns formed in sand. Although Turing's ideas on morphogenesis and Turing patterns remained dormant for many years, they are now inspirational for much research in mathematical biology.\n\n\n"}
{"id": "32802555", "url": "https://en.wikipedia.org/wiki?curid=32802555", "title": "Weisner's method", "text": "Weisner's method\n\nIn mathematics, Weisner's method is a method for finding generating functions for special functions using representation theory of Lie groups and Lie algebras, introduced by . It includes Truesdell's method as a special case, and is essentially the same as Rainville's method.\n\n"}
{"id": "20115268", "url": "https://en.wikipedia.org/wiki?curid=20115268", "title": "Wildfire modeling", "text": "Wildfire modeling\n\nIn computational science, wildfire modeling is concerned with numerical simulation of wildland fires in order to understand and predict fire behavior. Wildfire modeling can ultimately aid wildland fire suppression, namely increase safety of firefighters and the public, reduce risk, and minimize damage. Wildfire modeling can also aid in protecting ecosystems, watersheds, and air quality.\n\nWildfire modeling attempts to reproduce fire behavior, such as how quickly the fire spreads, in which direction, how much heat it generates. A key input to behavior modeling is the Fuel Model, or type of fuel, through which the fire is burning. Behavior modeling can also include whether the fire transitions from the surface (a \"surface fire\") to the tree crowns (a \"crown fire\"), as well as extreme fire behavior including rapid rates of spread, fire whirls, and tall well-developed convection columns. Fire modeling also attempts to estimate fire effects, such as the ecological and hydrological effects of the fire, fuel consumption, tree mortality, and amount and rate of smoke produced.\n\nWildland fire behavior is affected by weather, fuel characteristics, and topography.\n\nWeather influences fire through wind and moisture. Wind increases the fire spread in the wind direction, higher temperature makes the fire burn faster, while higher relative humidity, and precipitation (rain or snow) may slow it down or extinguish it altogether. Weather involving fast wind changes can be particularly dangerous, since they can suddenly change the fire direction and behavior. Such weather includes cold fronts, foehn winds, thunderstorm downdrafts, sea and land breeze, and diurnal slope winds.\n\nWildfire fuel includes grass, wood, and anything else that can burn. Small dry twigs burn faster while large logs burn slower; dry fuel ignites more easily and burns faster than wet fuel.\n\nTopography factors that influence wildfires include the orientation toward the sun, which influences the amount of energy received from the sun, and the slope (fire spreads faster uphill). Fire can accelerate in narrow canyons and it can be slowed down or stopped by barriers such as creeks and roads.\n\nThese factors act in combination. Rain or snow increases the fuel moisture, high relative humidity slows the drying of the fuel, while winds can make fuel dry faster. Wind can change the fire-accelerating effect of slopes to effects such as downslope windstorms (called Santa Anas, foehn winds, East winds, depending on the geographic location). Fuel properties may vary with topography as plant density varies with elevation or aspect with respect to the sun.\n\nIt has long been recognized that \"fires create their own weather.\" That is, the heat and moisture created by the fire feed back into the atmosphere, creating intense winds that drive the fire behavior. The heat produced by the wildfire changes the temperature of the atmosphere and creates strong updrafts, which can change the direction of surface winds. The water vapor released by the fire changes the moisture balance of the atmosphere. The water vapor can be carried away, where the latent heat stored in the vapor is released through condensation.\n\nLike all models in computational science, fire models need to strike a balance between fidelity, availability of data, and fast execution. Wildland fire models span a vast range of complexity, from simple cause and effect principles to the most physically complex presenting a difficult supercomputing challenge that cannot hope to be solved faster than real time.\n\nForest-fire models have been developed since 1940 to the present, but a lot of chemical and thermodynamic questions related to fire behaviour are still to be resolved. Scientists and their forest fire models from 1940 till 2003 are listed in article. Models can be divided into three groups: Empirical, Semi-empirical, and Physically based.\n\nConceptual models from experience and intuition from past fires can be used to anticipate the future. Many semi-empirical fire spread equations, as in those published by the USDA Forest Service, Forestry Canada, Nobel, Bary, and Gill, and Cheney, Gould, and Catchpole for Australasian fuel complexes have been developed for quick estimation of fundamental parameters of interest such as fire spread rate, flame length, and fireline intensity of surface fires at a point for specific fuel complexes, assuming a representative point-location wind and terrain slope. Based on the work by Fons's in 1946, and Emmons in 1963, the quasi-steady equilibrium spread rate calculated for a surface fire on flat ground in no-wind conditions was calibrated using data of piles of sticks burned in a flame chamber/wind tunnel to represent other wind and slope conditions for the fuel complexes tested.\n\nTwo-dimensional fire growth models such as FARSITE and Prometheus, the Canadian wildland fire growth model designed to work in Canadian fuel complexes, have been developed that apply such semi-empirical relationships and others regarding ground-to-crown transitions to calculate fire spread and other parameters along the surface. Certain assumptions must be made in models such as FARSITE and Prometheus to shape the fire growth. For example, Prometheus and FARSITE use the Huygens principle of wave propagation. A set of equations that can be used to propagate (shape and direction) a fire front using an elliptical shape was developed by Richards in 1990. Although more sophisticated applications use a three-dimensional numerical weather prediction system to provide inputs such as wind velocity to one of the fire growth models listed above, the input was passive and the feedback of the fire upon the atmospheric wind and humidity are not accounted for.\n\nA simplified physically based two-dimensional fire spread models based upon conservation laws that use radiation as the dominant heat transfer mechanism and convection, which represents the effect of wind and slope, lead to reaction–diffusion systems of partial differential equations.\n\nMore complex physical models join computational fluid dynamics models with a wildland fire component and allow the fire to feed back upon the atmosphere. These models include NCAR's Coupled Atmosphere-Wildland Fire-Environment (CAWFE) model developed in 2005, WRF-Fire at NCAR and University of Colorado Denver which combines the Weather Research and Forecasting Model with a spread model by the level-set method, University of Utah's Coupled Atmosphere-Wildland Fire Large Eddy Simulation developed in 2009, Los Alamos National Laboratory's FIRETEC developed in, the WUI (Wildland Urban Interface) Fire Dynamics Simulator (WFDS) developed in 2007, and, to some degree, the two-dimensional model FIRESTAR. These tools have different emphases and have been applied to better understand the fundamental aspects of fire behavior, such as fuel inhomogeneities on fire behavior, feedbacks between the fire and the atmospheric environment as the basis for the universal fire shape, and are beginning to be applied to wildland urban interface house-to-house fire spread at the community-scale.\n\nThe cost of added physical complexity is a corresponding increase in computational cost, so much so that a full three-dimensional explicit treatment of combustion in wildland fuels by direct numerical simulation (DNS) at scales relevant for atmospheric modeling does not exist, is beyond current supercomputers, and does not currently make sense to do because of the limited skill of weather models at spatial resolution under 1 km. Consequently, even these more complex models parameterize the fire in some way, for example, papers by Clark use equations developed by Rothermel for the USDA forest service to calculate local fire spread rates using fire-modified local winds. And, although FIRETEC and WFDS carry prognostic conservation equations for the reacting fuel and oxygen concentrations, the computational grid cannot be fine enough to resolve the reaction rate-limiting mixing of fuel and oxygen, so approximations must be made concerning the subgrid-scale temperature distribution or the combustion reaction rates themselves. These models also are too small-scale to interact with a weather model, so the fluid motions use a computational fluid dynamics model confined in a box much smaller than the typical wildfire.\n\nAttempts to create the most complete theoretical model were made by Albini F.A. in USA and Grishin A.M. in Russia. Grishin's work is based on the fundamental laws of physics, conservation and theoretical justifications are provided. The simplified two-dimensional model of running crown forest fire was developed in Belarusian State University by Barovik D.V. and Taranchuk V.B..\n\nData assimilation periodically adjusts the model state to incorporate new data using statistical methods. Because fire is highly nonlinear and irreversible, data assimilation for fire models poses special challenges, and standard methods, such as the ensemble Kalman filter (EnKF) do not work well. Statistical variability of corrections and especially large corrections may result in nonphysical states, which tend to be preceded or accompanied by large spatial gradients. In order to ease this problem, the regularized EnKF penalizes large changes of spatial gradients in the Bayesian update in EnKF. The regularization technique has a stabilizing effect on the simulations in the ensemble but it does not improve much the ability of the EnKF to track the data: The posterior ensemble is made out of linear combinations of the prior ensemble, and if a reasonably close location and shape of the fire cannot be found between the linear combinations, the data assimilation is simply out of luck, and the ensemble cannot approach the data. From that point on, the ensemble evolves essentially without regard to the data. This is called filter divergence. So, there is clearly a need to adjust the simulation state by a position change rather than an additive correction only. The \"morphing EnKF\" combines the ideas of data assimilation with image registration and morphing to provide both additive and position correction in a natural manner, and can be used to change a model state reliably in response to data.\n\nThe limitations on fire modeling are not entirely computational. At this level, the models encounter limits in knowledge about the composition of pyrolysis products and reaction pathways, in addition to gaps in basic understanding about some aspects of fire behavior such as fire spread in live fuels and surface-to-crown fire transition.\n\nThus, while more complex models have value in studying fire behavior and testing fire spread in a range of scenarios, from the application point of view, FARSITE and Palm-based applications of BEHAVE have shown great utility as practical in-the-field tools because of their ability to provide estimates of fire behavior in real time. While the coupled fire-atmosphere models have the ability to incorporate the ability of the fire to affect its own local weather, and model many aspects of the explosive, unsteady nature of fires that cannot be incorporated in current tools, it remains a challenge to apply these more complex models in a faster-than-real-time operational environment. Also, although they have reached a certain degree of realism when simulating specific natural fires, they must yet address issues such as identifying what specific, relevant operational information they could provide beyond current tools, how the simulation time could fit the operational time frame for decisions (therefore, the simulation must run substantially faster than real time), what temporal and spatial resolution must be used by the model, and how they estimate the inherent uncertainty in numerical weather prediction in their forecast. These operational constraints must be used to steer model development.\n\n\n"}
{"id": "52701530", "url": "https://en.wikipedia.org/wiki?curid=52701530", "title": "Xu Yue", "text": "Xu Yue\n\nXu Yue was a second-century mathematician, born in Donglai, Shandong province in China. Little is known of his life except that he was a student of Liu Hong, an astronomer and mathematician in second century China, and had frequent discussions with the Astronomer-Royal of the Astronomical Bureau.\n\nXu Yue wrote a commentary on \"Nine Chapters on Mathematical Art\" and a treatise, \"Notes on Traditions of Arithmetic Methods\". The commentary has been lost but his own work has survived with a commentary from Zhen Luan.\n\n\"Notes on Traditions of Arithmetic Methods\" mentions 14 old methods of calculation. This book was a prescribed mathematical text for the Imperial examinations in 656 and became one of \"The Ten Mathematical Classics\" (算经十书) in 1084.\n"}
{"id": "890891", "url": "https://en.wikipedia.org/wiki?curid=890891", "title": "Zero–one law", "text": "Zero–one law\n\nIn probability theory, a zero–one law is a result that states that an event must have probability 0 or 1 and no intermediate value. Sometimes, the statement is that the limit of certain probabilities must be 0 or 1.\n\nIt may refer to: \n"}
{"id": "58946268", "url": "https://en.wikipedia.org/wiki?curid=58946268", "title": "Zimmer's conjecture", "text": "Zimmer's conjecture\n\nZimmer's conjecture is a statement in mathematics \"which has to do with the circumstances under which geometric spaces exhibit certain kinds of symmetries.\" It was named after the mathematician Robert Zimmer. The conjecture states that there can exist symmetries (specifically higher-rank lattices) in a higher dimension that cannot exist in lower dimensions.\n\nIn 2017, the conjecture was proven by Aaron Brown and Sebastian Hurtado-Salazar of the University of Chicago and David Fisher of Indiana University.\n"}
