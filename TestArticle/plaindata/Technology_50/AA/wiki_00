{"id": "49071369", "url": "https://en.wikipedia.org/wiki?curid=49071369", "title": "Agritech", "text": "Agritech\n\nAgritech is the use of technology in agriculture, horticulture, and aquaculture with the aim of improving yield, efficiency, and profitability. Agritech can be products, services or applications derived from agriculture that improve various input/output processes.\n\nA major turning point for agricultural technology is the Industrial Revolution.\n\nTechnologies and applications in agri-tech include:\n\nDrones\nsatellite photography and sensors\nIoT-based sensor networks\nphase tracking\nweather forecasts\nautomated irrigation\nlight and heat control\nintelligent software analysis for pest and disease prediction, soil management and other involved analytical tasks\nBiotech is another type of agri-tech.\n\nAgriculture has been wrongly perceived in the past as a \"dirty job\" for the old people in rural communities but with the renaissance Technology brought to Agriculture, young people now see it as a potential sector to explore.\n\nThis is the practical use of Technology in Agriculture by a few start-ups in Africa.\n\nThere is a Nigeria’s digital agriculture platform which focused on connecting farm sponsors with real farmers in order to increase food production while promoting youth participation in agriculture.\n\nThis agritech startup is currently disrupting the agriculture ecosystem in the country by connecting small-scale farmers with investors using their platform App which is available on Google and Apple app stores.\n\nFarmers and sponsors all receive a percentage of the profits on harvest. The platform also makes provision for insurance cover for all existing farm projects, so that in the event of unforeseen circumstances, the sponsors’ capital can be refunded. \nDrones can be used on Crop field for scanning with compact multispectral imaging sensors, GPS map creation through onboard cameras, heavy payload transportation, and livestock monitoring with thermal-imaging camera-equipped drones.\n\nHigh-income countries have seen recent improvements in their agricultural management systems through modern remote sensing technology, such as satellites and aircraft and the information they collect. Out of the vast amount of data collected, advice is provided to farmers and fishers to help inform their decisions.\n\nThis has led to better crop yields, higher quality produce and more sustainable agricultural practices in some cases. Big data also informs high-level decision-makers on how to better manage food supply at national and regional levels.\n\nThe use of small Unmanned Aerial Vehicles (UAVs) better known as 'drones' for agricultural purposes is a new emerging technology which could revolutionise the way agricultural entrepreneurs interact with their land, water, crops and infrastructure. UAVs can be made specifically for business use and farming in particular, they can capture geo-referenced, overlapping, high-resolution images (2–5 cm) of 400 hectares in a single flight; can seamlessly upload data and produce agricultural analytics from their data management systems, and fly autonomously from take-off to landing.\n\nA blockchain is a digitized, decentralized, public ledger of all cryptocurrency transactions. Constantly growing as ‘completed’ blocks (the most recent transactions) are recorded and added to it in chronological order, it allows market participants to keep track of digital currency transactions without central recordkeeping. Each node (a computer connected to the network) gets a copy of the blockchain, which is downloaded automatically.\n\nOriginally developed as the accounting method for the virtual currency Bitcoin, blockchains – which use what's known as distributed ledger technology (DLT) – are appearing in a variety of commercial applications today. Currently, the technology is primarily used to verify transactions, within digital currencies though it is possible to digitize, code and insert practically any document into the blockchain. Doing so creates an indelible record that cannot be changed; furthermore, the record’s authenticity can be verified by the entire community using the blockchain instead of a single centralized authority.\n\nToday, Blockchain is revolutionizing the Agriculture sector in many ways including\n\nTrading commodities on the blockchain is helping to reduce middlemen interference by promoting a peer-to-peer model of connecting farmers with end users. It is also helping to promote fairer trades by removing trade barriers, reducing over reliance on United States Dollars and promoting cross-border trading in local currency.\n\nOther methods of leveraging blockchain technology is in settling land disputes through blockchain based land registry, and using QR codes to promote traceability.\n\nInformation Communication Technologies like podcasts, weblogs, social media platforms, e-books are constantly helping to bridge the information gap in the Agriculture sector for farmers and Agripreneurs.\n\nHydroponics is a soilless farming technology that is used to grow vegetables and tomatoes.\n\nIt guarantees an all-year-round production for farmers and insulates these crops from the effects of climate change. \n"}
{"id": "32294777", "url": "https://en.wikipedia.org/wiki?curid=32294777", "title": "Alphagov", "text": "Alphagov\n\nAlphagov was the project name of the experimental prototype website built by the Government Digital Service and launched on 11 May 2011 by the UK Cabinet Office that was open for public comment for two months in order to judge the feasibility of a single domain for British Government web services.\n\nLaunched in response to the report by Martha Lane Fox \"Directgov 2010 and Beyond: Revolution Not Evolution\" that was published in November 2010. Alphagov sought to act as a proof of concept for the way citizens could interact with Government through a series of useful online tools, where they were more useful than published content alone.\n\nAs well as improving the 'citizen experience' of using government web services online the project also identified the potential for £64 million in yearly savings on central government's annual £128 million current web publishing bill.\n\nThis initial consultation period was completed in June 2011. A beta version was then created, which led to the launch of GOV.UK.\n\n\n"}
{"id": "9436002", "url": "https://en.wikipedia.org/wiki?curid=9436002", "title": "Ann Rockley", "text": "Ann Rockley\n\nAnn Rockley is one of Canada's foremost experts in organizing and presenting information online. She is the founder and President of The Rockley Group, based in the greater Toronto Area. She regularly presents papers and workshops on subjects involving the efficient creation, management and delivery of content for organizations in North America and Europe. \nShe was the lead analyst for \"The XML & Component Content Management Report\" on Content Management Systems Watch.\n\nAt university, Ann Rockley took a Bachelor of Science degree in Astronomy. In the early 1980s, she got her first permanent job as a junior technical writer at I. P. Sharp Associates. She went on to work for Cemcorp, Unisys, and American Express, then formed Information Design Solutions with two partners, Heather Fawcett and Sam Ferdinand. According to Gerlinde Schuller, information design is a complex, interdisciplinary, and experimental art. The partnership consulted in usability, document analysis, SGML, and large-scale online documentation projects.\n\nIn 1995, Rockley left Information Design Solutions to start The Rockley Group. She took a master's degree in information science at the University of Toronto while continuing to work at The Rockley Group full-time.\n\nIn the mid-1980s, Rockley took the initiative to revive the Toronto chapter of the Society for Technical Communication, a professional organization for technical communicators, by getting in touch with the international organization's representative for her area, Rennie Charles. The chapter was founded in 1959 but had been dormant for several years. She and Michelle Hutchinson, another technical communicator, assembled a group of their colleagues and presented a plan to hold regular meetings, find speakers of interest, and develop services for technical communicators. The chapter has been active ever since. Among other things, Rockley served as chapter president, produced the newsletter, and was general manager for a very successful three-day, multi-stream conference in 1989. (Hutchinson served as chapter president, produced the newsletter, and organized the hosting of the international society's annual conference in 1997.)\n\nWhen Toronto hosted the international conference, Rockley proposed producing the conference proceedings on CD as well as in book form. She also volunteered to produce the CDs for the 2500 attendees. That was the first time it had been done. The machine-readable format proved to be so popular that it has been provided in one form or another ever since.\n\nRockley regularly presents papers and workshops at the annual meeting of the international Society for Technical Communication. She was named an Associate Fellow of the STC for her contributions to the profession, and in 2005 became a Fellow, the Society's highest honor.\n\nAnn Rockley helped to develop the Information Design certificate program at the University of Toronto. She has taught courses in the program (information design and enterprise content management).\n\nRockley is the lead author of a 2002 book, \"Managing Enterprise Content: A Unified Content Strategy\", co-authored with Pamela Kostur and Steve Manning, who both worked with Ann at The Rockley Group. It has become a standard reference manual for content management. Her methodology includes return on investment calculations, which can justify the content management effort to executives.\n\nShe is also the lead author of a 2009 book,\"DITA 101: Fundamentals of DITA for Authors and Managers\", co-authored with Charles Cooper and Steve Manning. It is a beginner's guide to understanding the Darwin Information Typing Architecture (DITA), an XML-based architecture for authoring, producing, and delivering information.\n\nRockley's 2015 book, \"Intelligent Content: A Primer\", was co-authored with Charles Cooper and Scott Abel. It is a beginner's guide to creating intelligent content—defined as content which is not limited to one purpose, technology or output—and overcoming the challenges to its adoption. Intelligent Content was published by XML Press.\n\nFacing challenges such as putting online 10,000+ pages of documentation for a nuclear power plant, Rockley has been an innovator in devising ways to handle large quantities of online information. She has progressed from information design and online documentation through single sourcing to content management for entire enterprises. She pioneered content reuse for a unified content strategy and incorporated the use of XML for implementing that strategy.\n\nAs her interests became more specialized, Rockley went on to help found another professional organization, the Content Management Professionals (CM Pros), in 2004. She served as the president in 2005.\n\nAnn Rockley is a member of the Organization for the Advancement of Structured Information Standards (OASIS). It is a \"not-for-profit consortium that drives the development, convergence and adoption of open standards for the global information society\". She is the Co-Chair of the DITA for Enterprise Business Documents Subcommittee\n\n"}
{"id": "42076934", "url": "https://en.wikipedia.org/wiki?curid=42076934", "title": "Antique Telescope Society", "text": "Antique Telescope Society\n\nThe Antique Telescope Society (ATS) is a society for people interested in antique telescopes, binoculars, instruments, books, atlases, etc.\n\nThe society has an annual meeting. It also publishes the \"Journal of the Antique Telescope Society\" and has an active email list. The American astronomer Michael D. Reynolds has been President of the Antique Telescope Society.\n\n"}
{"id": "26139453", "url": "https://en.wikipedia.org/wiki?curid=26139453", "title": "Azimuth Systems", "text": "Azimuth Systems\n\nAzimuth Systems is a privately held company located near Boston, Massachusetts. The company’s primary products include wireless channel emulators and wireless test equipment for LTE, WiMAX, 2G/3G cellular and Wi-Fi networks.\n\nIn 2009 Azimuth Systems wrote a White paper entitled \"Improving 4G Wireless Broadband Product Design through Effective Channel Emulation Testing\".\n\nIn April 2010, Azimuth Systems was awarded the \"Best in Test\" award from \"Test & Measurement World\" Magazine. Also, in 2009, Azimuth Systems was awarded the \"4G Wireless Evolution LTE Visionary Award\"by TMC.net. In September 2016, Test solutions major Anritsu Corporation acquired Azimuth Systems.\n\n"}
{"id": "50807669", "url": "https://en.wikipedia.org/wiki?curid=50807669", "title": "Barefoot Networks", "text": "Barefoot Networks\n\nBarefoot Networks is a computer networking company headquartered in Santa Clara, California. The company designs and produces programmable network switch silicon, systems and software.\n\nBarefoot Networks was founded in 2013. The company raised seed funding in 2013. The company is backed by Andreessen Horowitz, Lightspeed Venture Partners and Sequoia Capital. The company's co-founders are Nick McKeown, Martin Izzard, Pat Bosshart. Later, Dan Lenoski joined in 2014 and was also given co-founder status. The company came out of stealth mode on June 14, 2016\n\nBarefoot Tofino<br>\nBarefoot TofinoTM is a P4 programmable switch chip that can run up to speeds of 6.5 Tbit/s.\n\nProgrammability<br>\nP4 is a programming language designed to allow programming of packet forwarding dataplanes.\n\nBarefoot Deep Insight<br>\nBarefoot Deep Insight is a network monitoring system that provides full visibility into every packet in a network. Running on commodity servers, Barefoot Deep Insight interprets, analyzes and pinpoints a myriad of conditions that can impede packet flow, and does so in real time and at line-rate.\n\n"}
{"id": "56345589", "url": "https://en.wikipedia.org/wiki?curid=56345589", "title": "Brigade Media", "text": "Brigade Media\n\nBrigade Media, also known as Brigade, is a civic technology platform that was formed on June 4, 2014, and founded by James Windon, Jason Putorti, John Thrall, Matt Mahan, and Miche Capone. The platform is intended to serve as a way for users to connect with others who share the same or similar views and voice their opinions, create debates, or organize petitions. This process is intended to make the users' concerns more visible to and influential towards United States' policymakers.\n\nJames Wendon is the President of the Brigade platform. He previously acted as the Vice President of Causes and earlier worked with the World Trade Organization in Switzerland. Matt Mahan is the CEO of Brigade and previously served as the CEO of Causes. John Thrall works in Engineering, Jason Putorti works in Design, and Miche Capone specializes in Production. Sean Parker is the Chairman of the startup. He sits on the board of Spotify and was the founding president of Facebook.\n\nOn June 4th, 2014, Brigade Beta became available to download on iOS or Google Play, in its private beta version. In this beginning stage, the app asked users to agree or disagree on a position. Brigade then split up its users into those who agreed on the issue and those who disagreed on the issue. Participants were also allowed to write their own opinions on positions and ask those in their respective group if they were \"for\" or \"against\" the opinion stated.\n\nA few weeks before the November 2016 elections, Brigade created a ballot guide for its users. It ran these voter guides in San Francisco and Manchester, New Hampshire. As the user entered the application, he or she was prompted with questions regarding government and social issues. One could agree, disagree, or click unsure as their answer choices. After completing the questionnaire, the guide gave recommendations on who to vote for and which propositions to pass or not pass. Furthermore, the app also determined these choices based on those a user socializes with on Brigade. Brigade users could then pledge their votes to the candidates and propositions listed on the ballot. With these pledges, the app could track which candidates had more pledged votes in real time. Users were further able to recruit pledges from other users for their favorite candidates or propositions.\n\nBrigade implemented a voter verification service as well. With voter verification, a user can determine how similar or different a political representative's viewpoints are from their own. This data was received from Google's Civic API with geographic information on 520,000 American elected officials.\n\nBrigade Media has acquired Causes,Votizen, and Voter. At the time of its acquisition, Causes was the largest online platform where candidates could campaign. At the time of its acquisition, Votizen functioned as a tool for voters to learn more about their leaders. At the time of its acquisition, Voter aimed to put voters and politicians together via shared viewpoints. These three companies helped Brigade gain social media presence and find intelligent workers in the field.\n\nAs technology has advanced, it was assumed that the average individual would have further access to voting, and therefore, voting numbers would increase. Instead, voter turnout at the elections remains low, with numbers at the presidential elections between 50% and 60% of the US population in the last 50 years. At the midterm election of 2014, only 36.4% of people voted, the lowest percentage since 1942. It is hoped that civic technology will incentivize and educate its users to vote. Brigade's voter ballot was an attempt as a civic technology platform to increase voter participation as well as educate its users about candidates and propositions.\n\nOne mission of Brigade Media is to act as a foreground for its users to connect and organize so that they can voice their opinions on our nation's issues. Another more general goal is to increase voter participation.\n\nBrigade interacts with American voters by linking its users to the same held concerns. The opinions of elected officials on those concerns will be provided and metrics about the candidate most similar in concerns and degree of concern will be available. This data should be useful for both candidates and voters, in that voters can voice their grievances and candidates must respond accordingly.\n\nIn 2014, Brigade Media received about $9.5 million in seed money from Sean Parker, Marc Benioff, and Ron Conway.\n\nBarbara Simons, a computer scientist from IBM, asserts that all current forms of digital voting devices are hackable, and that the best unhackable option is paper.\nFurthermore, a Canadian study revealed that online voting platforms may not improve voter participation. The study found that those who did not already vote on paper ballots did not vote with digital devices. Instead, non-paper voting forms are simply more convenient for those who would have already decided to vote. With this data brought to light, the Independent Panel On Internet Voting did not recommend internet voting to the Legislative Assembly of British Columbia in 2012.\n\nBrigade's Executive Chairman, Sean Parker, was the president of Facebook in mid-2004. Recently, Facebook sold information on over 50 million Facebook users to Cambridge Analytica. Sean Parker's previous relationship with Facebook could provide controversy in his work with Brigade Media and his other projects.\n\nAt the company's formation, Brigade Media also faced a racial controversy. When Brigade was only a few weeks old, the entire leadership division of the start-up was white males. The company addressed the issues by filling in 12 additional positions and noting that women were also present in the organization.\n\nWhen the ballot guide was introduced, about 67% of its users were millennials. This is an accomplishment because millennial participation in the 2014 midterm elections had declined. At the time of the ballot guide, the start-up had 100,000 pledged candidates and 400,000 friends of candidates also pledged. \n\nThe platforms data also saw Donald Trump winning swing states before the polls in the 2016 United States Presidential Election. At this time, Brigade had 200,000 verified users. Within the vote pledges, 94.5% of Republicans pledged to vote for Republican nominee Trump, with 2.2% pledging with Democratic nominee Hillary Clinton. However, on the Democratic pledge side, only 55% pledged for Clinton while 40% of Democrats pledged for Trump.\n"}
{"id": "7878457", "url": "https://en.wikipedia.org/wiki?curid=7878457", "title": "Computer", "text": "Computer\n\nA computer is a device that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming. Modern computers have the ability to follow generalized sets of operations, called \"programs.\" These programs enable computers to perform an extremely wide range of tasks.\n\nComputers are used as control systems for a wide variety of industrial and consumer devices. This includes simple special purpose devices like microwave ovens and remote controls, factory devices such as industrial robots and computer-aided design, and also general purpose devices like personal computers and mobile devices such as smartphones.\n\nEarly computers were only conceived as calculating devices. Since ancient times, simple manual devices like the abacus aided people in doing calculations. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The speed, power, and versatility of computers have been increasing dramatically ever since then.\n\nConventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved.\n\nAccording to the \"Oxford English Dictionary\", the first known use of the word \"computer\" was in 1613 in a book called \"The Yong Mans Gleanings\" by English writer Richard Braithwait: \"I haue [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number.\" This usage of the term referred to a human computer, a person who carried out calculations or computations. The word continued with the same meaning until the middle of the 20th century. Originally, women were often hired as \"human computers\" because they could be paid less than their male counterparts. By 1943, most human computers were women. From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.\n\nThe \"Online Etymology Dictionary\" gives the first attested use of \"computer\" in the \"1640s, [meaning] \"one who calculates,\"; this is an \"... agent noun from compute (v.)\". The \"Online Etymology Dictionary\" states that the use of the term to mean \"calculating machine\" (of any type) is from 1897.\" The \"Online Etymology Dictionary\" indicates that the \"modern use\" of the term, to mean \"programmable digital electronic computer\" dates from \"... 1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine\".\n\nDevices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers. The use of counting rods is one example.\n\nThe abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.\nThe Antikythera mechanism is believed to be the earliest mechanical analog \"computer\", according to Derek J. de Solla Price. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to . Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.\n\nMany mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century. The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235. Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe, an early fixed-wired knowledge processing machine with a gear train and gear-wheels, .\n\nThe sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.\n\nThe planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.\nThe slide rule was invented around 1620–1630, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Slide rules with special scales are still used for quick performance of routine calculations, such as the E6B circular slide rule used for time and distance calculations on light aircraft.\n\nIn the 1770s, Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automaton) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically \"programmed\" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.\n\nThe tide-predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.\n\nThe differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876, Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.\n\nCharles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the \"father of the computer\", he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.\n\nThe machine was about a century ahead of its time. All the parts for his machine had to be made by hand – this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the \"mill\") in 1888. He gave a successful demonstration of its use in computing tables in 1906.\n\nDuring the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers. The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin.\n\nThe art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s, the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (control systems) and aircraft (slide rule).\n\nBy 1938, the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.\nEarly digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.\n\nIn 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer. The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz. Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Rather than the harder-to-implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time. The Z3 was Turing complete.\n\nPurely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes. In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942, the first \"automatic electronic digital computer\". This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.\nDuring World War II, the British at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes which were often run by women. To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus. He spent eleven months from early February 1943 designing and building the first Colossus. After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944 and attacked its first message on 5 February.\n\nColossus was the world's first electronic digital programmable computer. It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1,500 thermionic valves (tubes), but Mark II with 2,400 valves, was both 5 times faster and simpler to operate than Mark I, greatly speeding the decoding process.\nThe U.S.-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus, it was much faster, more flexible, and it was Turing-complete. Like the Colossus, a \"program\" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were six women, often known collectively as the \"ENIAC girls\".\n\nIt combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.\n\nThe principle of the modern computer was proposed by Alan Turing in his seminal 1936 paper, \"On Computable Numbers\". Turing proposed a simple device that he called \"Universal Computing machine\" and that is now known as a universal Turing machine. He proved that such a machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable. The fundamental concept of Turing's design is the stored program, where all the instructions for computing are stored in memory. Von Neumann acknowledged that the central concept of the modern computer was due to this paper. Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.\n\nEarly computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine. With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid by Alan Turing in his 1936 paper. In 1945, Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report \"Proposed Electronic Calculator\" was the first specification for such a device. John von Neumann at the University of Pennsylvania also circulated his \"First Draft of a Report on the EDVAC\" in 1945.\n\nThe Manchester Baby was the world's first stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948. It was designed as a testbed for the Williams tube, the first random-access digital storage device. Although the computer was considered \"small and primitive\" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer. As soon as the Baby had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. Grace Hopper was the first person to develop a compiler for programming language.\n\nThe Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer. Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam. In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951 and ran the world's first regular routine office computer job.\n\nThe bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the \"second generation\" of computers.\nCompared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.\n\nAt the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955, built by the electronics division of the Atomic Energy Research Establishment at Harwell.\n\nThe next great advance in computing power came with the advent of the integrated circuit.\nThe idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952.\n\nThe first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. In his patent application of 6 February 1959, Kilby described his new device as \"a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated\". Noyce also came up with his own idea of an integrated circuit half a year later than Kilby. His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium.\n\nThis new development heralded an explosion in the commercial and personal use of computers and led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term \"microprocessor\", it is largely undisputed that the first single-chip microprocessor was the Intel 4004, designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel.\n\nThe first mobile computers were heavy and ran from mains power. The 50lb IBM 5100 was an early example. Later portables such as the Osborne 1 and Compaq Portable were considerably lighter, but still needed to be plugged in. The first laptops, such as the Grid Compass, removed this requirement by incorporating batteries – and with the continued miniaturization of computing resources and advancements in portable battery life, portable computers grew in popularity in the 2000s. The same developments allowed manufacturers to integrate computing resources into cellular phones.\n\nThese smartphones and tablets run on a variety of operating systems and soon became the dominant computing device on the market, with manufacturers reporting having shipped an estimated 237 million devices in 2Q 2013.\n\nComputers are typically classified based on their uses:\n\n\n\nThe term \"hardware\" covers all of those parts of a computer that are tangible physical objects. Circuits, computer chips, graphic cards, sound cards, memory (RAM), motherboard, displays, power supplies, cables, keyboards, printers and \"mice\" input devices are all hardware.\n\nA general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires.\nInside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a \"1\", and when off it represents a \"0\" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.\n\nWhen unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of input devices are:\n\nThe means through which computer gives output are known as output devices. Some examples of output devices are:\n\nThe control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer. Control systems in advanced computers may change the order of execution of some instructions to improve performance.\n\nA key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.\n\nThe control system's function is as follows—note that this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:\n\n\nSince the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as \"jumps\" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).\n\nThe sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.\n\nThe control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components but since the mid-1970s CPUs have typically been constructed on a single integrated circuit called a \"microprocessor\".\n\nThe ALU is capable of performing two classes of operations: arithmetic and logic. The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) while others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other (\"is 64 greater than 65?\"). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing boolean logic.\n\nSuperscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously. Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.\n\nA computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered \"address\" and can store a single number. The computer can be instructed to \"put the number 123 into the cell numbered 1357\" or to \"add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595.\" The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.\n\nIn almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (2 = 256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.\n\nThe CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.\n\nComputer main memory comes in two principal varieties:\nRAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.\n\nIn more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.\n\nI/O is the means by which a computer exchanges information with the outside world. Devices that provide input or output to the computer are called peripherals. On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.\nI/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics. Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O. A 2016-era flat screen display contains its own computer circuitry.\n\nWhile a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn. One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running \"at the same time\". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed \"time-sharing\" since each program is allocated a \"slice\" of time in turn.\n\nBefore the era of inexpensive computers, the principal use for multitasking was to allow many people to share the same computer. Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a \"time slice\" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.\n\nSome computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.\n\nSupercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general purpose computers. They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful only for specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called \"embarrassingly parallel\" tasks.\n\n\"Software\" refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. Software is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical hardware from which the system is built. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. It is often divided into system software and application software]] Computer hardware and software require each other and neither can be realistically used on its own. When software is stored in hardware that cannot easily be modified, such as with BIOS ROM in an IBM PC compatible computer, it is sometimes called \"firmware\".\n\nThere are thousands of different programming languages—some intended to be general purpose, others useful only for highly specialized applications.\n\nThe defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.\n\nThis section applies to most common RAM machine–based computers.\n\nIn most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called \"jump\" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that \"remembers\" the location it jumped from and another instruction to return to the instruction following that jump instruction.\n\nProgram execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.\n\nComparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:\nOnce told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.\n\nIn most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program, architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.\n\nWhile it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers, it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember – a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.\n\nProgramming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.\n\nMachine languages and the assembly languages that represent them (collectively termed \"low-level programming languages\") tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a smartphone or a hand-held videogame) cannot understand the machine language of an x86 CPU that might be in a PC.\n\nAlthough considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually \"compiled\" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler. High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.\n\nFourth-generation languages (4GL) are less procedural than 3G languages. The benefit of 4GL is that they provide ways to obtain information without requiring the direct help of a programmer.\n\nProgram design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies.\nThe task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge.\n\nErrors in computer programs are called \"bugs\". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to \"hang\", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.\nAdmiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term \"bugs\" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.\n\nFirmware is the technology which has the combination of both hardware and software such as BIOS chip inside a computer. This chip (hardware) is located on the motherboard and has the BIOS set up (software) stored in it.\n\nComputers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre. In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET. The technologies that made the Arpanet possible spread and evolved.\n\nIn time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. \"Wireless\" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.\nA computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word \"computer\" is synonymous with a personal electronic computer, the modern definition of a computer is literally: \"\"A device that computes\", especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information.\" Any device which \"processes information\" qualifies as a computer, especially if the processing is purposeful.\n\nHistorically, computers evolved from mechanical computers and eventually from vacuum tubes to transistors. However, conceptually computational systems as flexible as a personal computer can be built out of almost anything. For example, a computer can be made out of billiard balls (billiard ball computer); an often quoted example. More realistically, modern computers are made out of transistors made of photolithographed semiconductors.\n\nThere is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.\n\nThere are many types of computer architectures:\n\nOf all these abstract machines, a quantum computer holds the most promise for revolutionizing computing. Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms. The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church–Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.\n\nA computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning. Artificial intelligence based products generally fall into two major categories: rule based systems and pattern recognition systems. Rule based systems attempt to represent the rules used by human experts and tend to be expensive to develop. Pattern based systems use data about a problem to generate conclusions. Examples of pattern based systems include voice recognition, font recognition, translation and the emerging field of on-line marketing.\n\nAs the use of computers has spread throughout society, there are an increasing number of careers involving computers.\nThe need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature.\n\n\n"}
{"id": "28192799", "url": "https://en.wikipedia.org/wiki?curid=28192799", "title": "Data custodian", "text": "Data custodian\n\nIn Data Governance groups, responsibilities for data management are increasingly divided between the business process owners and information technology (IT) departments. Two functional titles commonly used for these roles are Data Steward and Data Custodian. \n\nData Stewards are commonly responsible for data content, context, and associated business rules. Data Custodians are responsible for the safe custody, transport, storage of the data and implementation of business rules. Simply put, Data Stewards are responsible for what is stored in a data field, while Data Custodians are responsible for the technical environment and database structure. Common job titles for data custodians are Database Administrator (DBA), Data Modeler, and ETL Developer.\n\nA data custodian ensures:\n\n\n"}
{"id": "3265848", "url": "https://en.wikipedia.org/wiki?curid=3265848", "title": "Department of Science and Technology (South Africa)", "text": "Department of Science and Technology (South Africa)\n\nThe Department of Science and Technology (DST) is the South African government department responsible for scientific research, including space programmes. The current Minister is Mmamoloko Kubayi-Ngubane.\n\nMuch of the department's work is ultimately carried out through various quasi-independent agencies (although still usually government bodies) including: \n\n"}
{"id": "10319663", "url": "https://en.wikipedia.org/wiki?curid=10319663", "title": "DigRF", "text": "DigRF\n\nThe DigRF working group was formed as a MIPI Alliance (MIPI) working group in April 2007. The group is focused on developing specifications for wireless mobile RFIC to base-band IC (BBIC) interfaces in mobile devices.\n\nThe group's current charter is split into short-term and long-term development efforts. The short-term development will focus on a specification targeted for completion by end of 2007 for LTE and WiMax air interface standards. The longer term development will focus on future air interface standards which promise further improvements in high speed, data optimized traffic. In addition, the future work will seek to harmonize efforts with the MIPI's PHY and UniPro working groups.\n\nThese specifications will describe the logical, electrical and timing characteristics of the digital RF-BB Interface with sufficient detail to allow physical implementation of the interface, and with sufficient rigor that implementations of the interface from different suppliers are fully compatible at the physical level.\n\nThere is DigRF version v1.12 for usage in GSM/EDGE handsets, which was specified in 2004. DigRF v3.09 from the year 2006 with its 312 Mbit/s can additionally handle UMTS. The present DigRF v4 draft offers Gbit/s bandwidth for LTE and WiMax.\n"}
{"id": "195113", "url": "https://en.wikipedia.org/wiki?curid=195113", "title": "Digital divide", "text": "Digital divide\n\nA digital divide is an economic and social inequality with regard to access to, use of, or impact of information and communication technologies (ICT). The divide within countries (such as the digital divide in the United States) may refer to inequalities between individuals, households, businesses, or geographic areas, usually at different socioeconomic levels or other demographic categories. The divide between differing countries or regions of the world is referred to as the global digital divide, examining this technological gap between developing and developed countries on an international scale.\n\nThe term \"digital divide\" describes a gap in terms of access to and usage of information and communication technology. It was traditionally considered to be a question of having or not having access, but with a global mobile phone penetration of over 95%, it is becoming a relative inequality between those who have more and less bandwidth and more or fewer skills. Conceptualizations of the digital divide have been described as \"who, with which characteristics, connects how to what\":\nDifferent authors focus on different aspects, which leads to a large variety of definitions of the digital divide. \"For example, counting with only 3 different choices of subjects (individuals, organizations, or countries), each with 4 characteristics (age, wealth, geography, sector), distinguishing between 3 levels of digital adoption (access, actual usage and effective adoption), and 6 types of technologies (fixed phone, mobile... Internet...), already results in 3x4x3x6 = 216 different ways to define the digital divide. Each one of them seems equally reasonable and depends on the objective pursued by the analyst\".\nThe \"digital divide\" is also referred to by a variety of other terms which have similar meanings, though may have a slightly different emphasis: digital inclusion, digital participation, basic digital skills, media literacy and digital accessibility.\n\nThe National Digital Inclusion Alliance, a US-based nonprofit organization, has found the term \"digital divide\" to be problematic, since there are a multiplicity of divides. Instead, they chosen to use the term \"digital inclusion\", providing a definition:\nDigital Inclusion refers to the activities necessary to ensure that all individuals and communities, including the most disadvantaged, have access to and use of Information and Communication Technologies (ICTs). This includes 5 elements: 1) affordable, robust broadband internet service; 2) internet-enabled devices that meet the needs of the user; 3) access to digital literacy training; 4) quality technical support; and 5) applications and online content designed to enable and encourage self-sufficiency, participation and collaboration.\n\nThe infrastructure by which individuals, households, businesses, and communities connect to the Internet address the physical mediums that people use to connect to the Internet such as desktop computers, laptops, basic mobile phones or smartphones, iPods or other MP3 players, gaming consoles such as Xbox or PlayStation, electronic book readers, and tablets such as iPads.\n\nTraditionally the nature of the divide has been measured in terms of the existing numbers of subscriptions and digital devices. Given the increasing number of such devices, some have concluded that the digital divide among individuals has increasingly been closing as the result of a natural and almost automatic process. Others point to persistent lower levels of connectivity among women, racial and ethnic minorities, people with lower incomes, rural residents, and less educated people as evidence that addressing inequalities in access to and use of the medium will require much more than the passing of time. Recent studies have measured the digital divide not in terms of technological devices, but in terms of the existing bandwidth per individual (in kbit/s per capita). \n\nAs shown in the Figure on the side, the digital divide in kbit/s is not monotonically decreasing, but re-opens up with each new innovation. For example, \"the massive diffusion of narrow-band Internet and mobile phones during the late 1990s\" increased digital inequality, as well as \"the initial introduction of broadband DSL and cable modems during 2003–2004 increased levels of inequality\". This is because a new kind of connectivity is never introduced instantaneously and uniformly to society as a whole at once, but diffuses slowly through social networks. As shown by the Figure, during the mid-2000s, communication capacity was more unequally distributed than during the late 1980s, when only fixed-line phones existed. The most recent increase in digital equality stems from the massive diffusion of the latest digital innovations (i.e. fixed and mobile broadband infrastructures, e.g. 3G and fiber optics FTTH).\nMeasurement methodologies of the digital divide, and more specifically an Integrated Iterative Approach General Framework (Integrated Contextual Iterative Approach – ICI) and the digital divide modeling theory under measurement model DDG (Digital Divide Gap) are used to analyze the gap existing between developed and developing countries, and the gap among the 27 members-states of the European Union.\n\nInstead of tracking various kinds of digital divides among fixed and mobile phones, narrow- and broadband Internet, digital TV, etc., it has recently been suggested to simply measure the amount of kbit/s per actor. This approach has shown that the digital divide in kbit/s per capita is actually widening in relative terms: \"While the average inhabitant of the developed world counted with some 40 kbit/s more than the average member of the information society in developing countries in 2001, this gap grew to over 3 Mbit/s per capita in 2010.\" \n\nThe upper graph of the Figure on the side shows that the divide between developed and developing countries has been diminishing when measured in terms of subscriptions per capita. In 2001, fixed-line telecommunication penetration reached 70% of society in developed OECD countries and 10% of the developing world. This resulted in a ratio of 7 to 1 (divide in relative terms) or a difference of 60% (divide in measured in absolute terms). During the next decade, fixed-line penetration stayed almost constant in OECD countries (at 70%), while the rest of the world started a catch-up, closing the divide to a ratio of 3.5 to 1. The lower graph shows the divide not in terms of ICT devices, but in terms of kbit/s per inhabitant. While the average member of developed countries counted with 29 kbit/s more than a person in developing countries in 2001, this difference got multiplied by a factor of one thousand (to a difference of 2900 kbit/s). In relative terms, the fixed-line capacity divide was even worse during the introduction of broadband Internet at the middle of the first decade of the 2000s, when the OECD counted with 20 times more capacity per capita than the rest of the world. This shows the importance of measuring the divide in terms of kbit/s, and not merely to count devices. The International Telecommunications Union concludes that \"the bit becomes a unifying variable enabling comparisons and aggregations across different kinds of communication technologies\".\n\nHowever, research shows that the digital divide is more than just an access issue and cannot be alleviated merely by providing the necessary equipment. There are at least three factors at play: information accessibility, information utilization and information receptiveness. More than just accessibility, individuals need to know how to make use of the information and communication tools once they exist within a community. Information professionals have the ability to help bridge the gap by providing reference and information services to help individuals learn and utilize the technologies to which they do have access, regardless of the economic status of the individual seeking help.\n\nInternet connectivity can be utilized at a variety of locations such as homes, offices, schools, libraries, public spaces, Internet cafe and others. There are also varying levels of connectivity in rural, suburban, and urban areas.\n\nCommon Sense Media, a nonprofit group based in San Francisco, surveyed almost 1,400 parents and reported in 2011 that 47 percent of families with incomes more than $75,000 had downloaded apps for their children, while only 14 percent of families earning less than $30,000 had done so.\n\nThe gap in a digital divide may exist for a number of reasons. Obtaining access to ICTs and using them actively has been linked to a number of demographic and socio-economic characteristics: among them income, education, race, gender, geographic location (urban-rural), age, skills, awareness, political, cultural and psychological attitudes. Multiple regression analysis across countries has shown that income levels and educational attainment are identified as providing the most powerful explanatory variables for ICT access and usage. Evidence was found that Caucasians are much more likely than non-Caucasians to own a computer as well as have access to the Internet in their homes. As for geographic location, people living in urban centers have more access and show more usage of computer services than those in rural areas. Gender was previously thought to provide an explanation for the digital divide, many thinking ICT were male gendered, but controlled statistical analysis has shown that income, education and employment act as confounding variables and that women with the same level of income, education and employment actually embrace ICT more than men (see Women and ICT4D). However, each nation has its own set of causes or the digital divide. For example, the digital divide in Germany is unique because it is not largely due to difference in quality of infrastructure.\n\nOne telling fact is that \"as income rises so does Internet use ...\", strongly suggesting that the digital divide persists at least in part due to income disparities. Most commonly, a digital divide stems from poverty and the economic barriers that limit resources and prevent people from obtaining or otherwise using newer technologies.\n\nIn research, while each explanation is examined, others must be controlled in order to eliminate interaction effects or mediating variables, but these explanations are meant to stand as general trends, not direct causes. Each component can be looked at from different angles, which leads to a myriad of ways to look at (or define) the digital divide. For example, measurements for the intensity of usage, such as incidence and frequency, vary by study. Some report usage as access to Internet and ICTs while others report usage as having previously connected to the Internet. Some studies focus on specific technologies, others on a combination (such as Infostate, proposed by Orbicom-UNESCO, the Digital Opportunity Index, or ITU's ICT Development Index). Based on different answers to the questions of who, with which kinds of characteristics, connects how and why, to what there are hundreds of alternatives ways to define the digital divide. \"The new consensus recognizes that the key question is not how to connect people to a specific network through a specific device, but how to extend the expected gains from new ICTs\". In short, the desired impact and \"the end justifies the definition\" of the digital divide.\n\nDuring the mid-1990s the US Department of Commerce, National Telecommunications & Information Administration (NTIA) began publishing reports about the Internet and access to and usage of the resource. The first of three reports is entitled \"Falling Through the Net: A Survey of the ‘Have Nots’ in Rural and Urban America\" (1995), the second is \"Falling Through the Net II: New Data on the Digital Divide\" (1998), and the final report \"Falling Through the Net: Defining the Digital Divide\" (1999). The NTIA’s final report attempted to clearly define the term digital divide; \"the digital divide—the divide between those with access to new technologies and those without—is now one of America's leading economic and civil rights issues. This report will help clarify which Americans are falling further behind, so that we can take concrete steps to redress this gap.\" Since the introduction of the NTIA reports, much of the early, relevant literature began to reference the NTIA’s digital divide definition. The digital divide is commonly defined as being between the \"haves\" and \"have-nots.\"\n\nThe Facebook Divide, a concept derived from the \"digital divide\", is the phenomenon with regard to access to, use of, or impact of Facebook on individual society and among societies. It is suggested at the International Conference on Management Practices for the New Economy (ICMAPRANE-17) on February 10–11, 2017. Additional concepts of Facebook Native and Facebook Immigrants are suggested at the conference. The Facebook Divide, Facebook native, Facebook immigrants, and Facebook left-behind are concepts for social and business management research. Facebook Immigrants are utilizing Facebook for their accumulation of both bonding and bridging social capital. These Facebook Native, Facebook Immigrants, and Facebook left-behind induced the situation of Facebook inequality. In February 2018, the Facebook Divide Index was introduced at the ICMAPRANE conference in Noida, India, to illustrate the Facebook Divide phenomenon.\n\nOvercoming the divide \n\nAn individual must be able to connect in order to achieve enhancement of social and cultural capital as well as achieve mass economic gains in productivity. Therefore, access is a necessary (but not sufficient) condition for overcoming the digital divide. Access to ICT meets significant challenges that stem from income restrictions. The borderline between ICT as a necessity good and ICT as a luxury good is roughly around the \"magical number\" of US$10 per person per month, or US$120 per year, which means that people consider ICT expenditure of US$120 per year as a basic necessity. Since more than 40% of the world population lives on less than US$2 per day, and around 20% live on less than US$1 per day (or less than US$365 per year), these income segments would have to spend one third of their income on ICT (120/365 = 33%). The global average of ICT spending is at a mere 3% of income. Potential solutions include driving down the costs of ICT, which includes low cost technologies and shared access through Telecentres.\n\nFurthermore, even though individuals might be capable of accessing the Internet, many are thwarted by barriers to entry such as a lack of means to infrastructure or the inability to comprehend the information that the Internet provides. Lack of adequate infrastructure and lack of knowledge are two major obstacles that impede mass connectivity. These barriers limit individuals' capabilities in what they can do and what they can achieve in accessing technology. Some individuals have the ability to connect, but they do not have the knowledge to use what information ICTs and Internet technologies provide them. This leads to a focus on capabilities and skills, as well as awareness to move from mere access to effective usage of ICT.\n\nThe United Nations is aiming to raise awareness of the divide by way of the World Information Society Day which has taken place yearly since May 17, 2006. It also set up the Information and Communications Technology (ICT) Task Force in November 2001. Later UN initiatives in this area are the World Summit on the Information Society, which was set up in 2003, and the Internet Governance Forum, set up in 2006.\n\nIn the year 2000, the United Nations Volunteers (UNV) programme launched its Online Volunteering service, which uses ICT as a vehicle for and in support of volunteering. It constitutes an example of a volunteering initiative that effectively contributes to bridge the digital divide. ICT-enabled volunteering has a clear added value for development. If more people collaborate online with more development institutions and initiatives, this will imply an increase in person-hours dedicated to development cooperation at essentially no additional cost. This is the most visible effect of online volunteering for human development.\n\nSocial media websites serve as both manifestations of and means by which to combat the digital divide. The former describes phenomena such as the divided users demographics that make up sites such as Facebook and Myspace or Word Press and Tumblr. Each of these sites host thriving communities that engage with otherwise marginalized populations. An example of this is the large online community devoted to Afrofuturism, a discourse that critiques dominant structures of power by merging themes of science fiction and blackness. Social media brings together minds that may not otherwise meet, allowing for the free exchange of ideas and empowerment of marginalized discourses.\n\nAttempts to bridge the digital divide include a program developed in Durban, South Africa, where very low access to technology and a lack of documented cultural heritage has motivated the creation of an \"online indigenous digital library as part of public library services.\" This project has the potential to narrow the digital divide by not only giving the people of the Durban area access to this digital resource, but also by incorporating the community members into the process of creating it.\n\nTo address the divide The Gates Foundation began the Gates Library Initiative. The Gates Foundation focused on providing more than just access, they placed computers and provided training in libraries. In this manner if users began to struggle while using a computer, the user was in a setting where assistance and guidance was available. Further, the Gates Library Initiative was \"modeled on the old-fashioned life preserver: The support needs to be around you to keep you afloat.\"\n\nIn nations where poverty compounds effects of the digital divide, programs are emerging to counter those trends. Prior conditions in Kenya—lack of funding, language and technology illiteracy contributed to an overall lack of computer skills and educational advancement for those citizens. This slowly began to change when foreign investment began. In the early 2000s, The Carnegie Foundation funded a revitalization project through the Kenya National Library Service (KNLS). Those resources enabled public libraries to provide information and communication technologies (ICT) to their patrons. In 2012, public libraries in the Busia and Kiberia communities introduced technology resources to supplement curriculum for primary schools. By 2013, the program expanded into ten schools.\n\nCommunity Informatics (CI) provides a somewhat different approach to addressing the digital divide by focusing on issues of \"use\" rather than simply \"access\". CI is concerned with ensuring the opportunity not only for ICT access at the community level but also, according to Michael Gurstein, that the means for the \"effective use\" of ICTs for community betterment and empowerment are available. Gurstein has also extended the discussion of the digital divide to include issues around access to and the use of \"open data\" and coined the term \"data divide\" to refer to this issue area.\n\nOnce an individual is connected, Internet connectivity and ICTs can enhance his or her future social and cultural capital. Social capital is acquired through repeated interactions with other individuals or groups of individuals. Connecting to the Internet creates another set of means by which to achieve repeated interactions. ICTs and Internet connectivity enable repeated interactions through access to social networks, chat rooms, and gaming sites. Once an individual has access to connectivity, obtains infrastructure by which to connect, and can understand and use the information that ICTs and connectivity provide, that individual is capable of becoming a \"digital citizen\".\n\nIn the United States, research provided by Sungard Availability Services notes a direct correlation between a company's access to technological advancements and its overall success in bolstering the economy. The study, which includes over 2,000 IT executives and staff officers, indicates that 69 percent of employees feel they do not have access to sufficient technology in order to make their jobs easier, while 63 percent of them believe the lack of technological mechanisms hinders their ability to develop new work skills. Additional analysis provides more evidence to show how the digital divide also affects the economy in places all over the world. A BCG Report suggests that in countries like Sweden, Switzerland, and the U.K., the digital connection among communities is made easier, allowing for their populations to obtain a much larger share of the economies via digital business. In fact, in these places, populations hold shares approximately 2.5 percentage points higher. During a meeting with the United Nations a Bangladesh representative expressed his concern that poor and undeveloped countries would be left behind due to a lack of funds to bridge the digital gap.\n\nThe digital divide also impacts children's ability to learn and grow in low-income school districts. Without Internet access, students are unable to cultivate necessary tech skills in order to understand today's dynamic economy. Federal Communication Commission's Broadband Task Force created a report showing that about 70% of teachers give students homework that demand access to broadband. Even more, approximately 65% of young scholars use the Internet at home to complete assignments as well as connect with teachers and other students via discussion boards and shared files. A recent study indicates that practically 50% of students say that they are unable to finish their homework due to an inability to either connect to the Internet, or in some cases, find a computer. This has led to a new revelation: 42% of students say they received a lower grade because of this disadvantage. Finally, according to research conducted by the Center for American Progress, \"if the United States were able to close the educational achievement gaps between native-born white children and black and Hispanic children, the U.S. economy would be 5.8 percent—or nearly $2.3 trillion—larger in 2050\".\n\nFurthermore, according to the 2012 Pew Report \"Digital Differences\", a mere 62% of households who make less than $30,000 a year use the Internet, while 90% of those making between $50,000 and $75,000 had access. Studies also show that only 51% of Hispanics and 49% of African Americans have high-speed Internet at home. This is compared to the 66% of Caucasians that too have high-speed Internet in their households. Overall, 10% of all Americans don't have access to high-speed Internet, an equivalent of almost 34 million people. Supplemented reports from the Guardian demonstrate the global effects of limiting technological developments in poorer nations, rather than simply the effects in the United States. Their study shows that the rapid digital expansion excludes those who find themselves in the lower class. 60% of the world's population, almost 4 billion people, have no access to the Internet and are thus left worse off.\n\nSince gender, age, racial, income, and educational gaps in the digital divide have lessened compared to past levels, some researchers suggest that the digital divide is shifting from a gap in access and connectivity to ICTs to a knowledge divide. A knowledge divide concerning technology presents the possibility that the gap has moved beyond access and having the resources to connect to ICTs to interpreting and understanding information presented once connected.\n\nThe second-level digital divide, also referred to as the production gap, describes the gap that separates the consumers of content on the Internet from the producers of content. As the technological digital divide is decreasing between those with access to the Internet and those without, the meaning of the term digital divide is evolving. Previously, digital divide research has focused on accessibility to the Internet and Internet consumption. However, with more and more of the population with access to the Internet, researchers are examining how people use the Internet to create content and what impact socioeconomics are having on user behavior.\nNew applications have made it possible for anyone with a computer and an Internet connection to be a creator of content, yet the majority of user generated content available widely on the Internet, like public blogs, is created by a small portion of the Internet using population. Web 2.0 technologies like Facebook, YouTube, Twitter, and Blogs enable users to participate online and create content without having to understand how the technology actually works, leading to an ever-increasing digital divide between those who have the skills and understanding to interact more fully with the technology and those who are passive consumers of it. Many are only nominal content creators through the use of Web 2.0, posting photos and status updates on Facebook, but not truly interacting with the technology.\n\nSome of the reasons for this production gap include material factors like the type of Internet connection one has and the frequency of access to the Internet. The more frequently a person has access to the Internet and the faster the connection, the more opportunities they have to gain the technology skills and the more time they have to be creative.\n\nOther reasons include cultural factors often associated with class and socioeconomic status. Users of lower socioeconomic status are less likely to participate in content creation due to disadvantages in education and lack of the necessary free time for the work involved in blog or web site creation and maintenance. Additionally, there is evidence to support the existence of the second-level digital divide at the K-12 level based on how educators' use technology for instruction. Schools' economic factors have been found to explain variation in how teachers use technology to promote higher-order thinking skills.\n\nThe global digital divide describes global disparities, primarily between developed and developing countries, in regards to access to computing and information resources such as the Internet and the opportunities derived from such access. As with a smaller unit of analysis, this gap describes an inequality that exists, referencing a global scale.\n\nThe Internet is expanding very quickly, and not all countries—especially developing countries—are able to keep up with the constant changes. The term \"digital divide\" doesn't necessarily mean that someone doesn’t have technology; it could mean that there is simply a difference in technology. These differences can refer to, for example, high-quality computers, fast Internet, technical assistance, or telephone services. The difference between all of these is also considered a gap.\n\nIn fact, there is a large inequality worldwide in terms of the distribution of installed telecommunication bandwidth. In 2014 only 3 countries (China, US, Japan) host 50% of the globally installed bandwidth potential (see pie-chart Figure on the right). This concentration is not new, as historically only 10 countries have hosted 70–75% of the global telecommunication capacity (see Figure). The U.S. lost its global leadership in terms of installed bandwidth in 2011, being replaced by China, which hosts more than twice as much national bandwidth potential in 2014 (29% versus 13% of the global total).\n\nThe global digital divide is a special case of the digital divide, the focus is set on the fact that \"Internet has developed unevenly throughout the world\" causing some countries to fall behind in technology, education, labor, democracy, and tourism. The concept of the digital divide was originally popularized in regard to the disparity in Internet access between rural and urban areas of the United States of America; the \"global\" digital divide mirrors this disparity on an international scale.\n\nThe global digital divide also contributes to the inequality of access to goods and services available through technology. Computers and the Internet provide users with improved education, which can lead to higher wages; the people living in nations with limited access are therefore disadvantaged. This global divide is often characterized as falling along what is sometimes called the north-south divide of \"northern\" wealthier nations and \"southern\" poorer ones.\n\nSome people argue that basic necessities need to be considered before achieving digital inclusion, such as an ample food supply and quality health care. Minimizing the global digital divide requires considering and addressing the following types of access:\nInvolves \"the distribution of ICT devices per capita…and land lines per thousands\". Individuals need to obtain access to computers, landlines, and networks in order to access the Internet. This access barrier is also addressed in Article 21 of the Convention on the Rights of Persons with Disabilities by the United Nations. \nThe cost of ICT devices, traffic, applications, technician and educator training, software, maintenance and infrastructures require ongoing financial means.\nFinancial access and \"the levels of household income play a significant role in widening the gap\" \nEmpirical tests have identified that several socio-demographic characteristics foster or limit ICT access and usage. Among different countries, educational levels and income are the most powerful explanatory variables, with age being a third one. Others, like gender, don't seem to have much of an independent effect after controlling for income, education and employment.\nIn order to use computer technology, a certain level of information literacy is needed. Further challenges include information overload and the ability to find and use reliable information. \nComputers need to be accessible to individuals with different learning and physical abilities including complying with Section 508 of the Rehabilitation Act as amended by the Workforce Investment Act of 1998 in the United States. \nIn illustrating institutional access, Wilson states \"the numbers of users are greatly affected by whether access is offered only through individual homes or whether it is offered through schools, community centers, religious institutions, cybercafés, or post offices, especially in poor countries where computer access at work or home is highly limited\". \nGuillen & Suarez argue that \"democratic political regimes enable a faster growth of the Internet than authoritarian or totalitarian regimes\". The Internet is considered a form of e-democracy and attempting to control what citizens can or cannot view is in contradiction to this. Recently situations in Iran and China have denied people the ability to access certain websites and disseminate information. Iran has prohibited the use of high-speed Internet in the country and has removed many satellite dishes in order to prevent the influence of Western culture, such as music and television.\nMany experts claim that bridging the digital divide is not sufficient and that the images and language needed to be conveyed in a language and images that can be read across different cultural lines. A 2013 study conducted by Pew Research Center noted how participants taking the survey in Spanish were nearly twice as likely to not use the internet.\n\nIn the early 21st century, residents of developed countries enjoy many Internet services which are not yet widely available in developing countries, including:\n\n\nThere are four specific arguments why it is important to \"bridge the gap\":\n\n\nWhile these four arguments are meant to lead to a solution to the digital divide, there are a couple other components that need to be considered. The first one is rural living versus suburban living. Rural areas used to have very minimal access to the Internet, for example. However, nowadays, power lines and satellites are used to increase the availability in these areas. Another component to keep in mind is disabilities. Some people may have the highest quality technologies, but a disability they have may keep them from using these technologies to their fullest extent.\n\nUsing previous studies (Gamos, 2003; Nsengiyuma & Stork, 2005; Harwit, 2004 as cited in James), James asserts that in developing countries, \"internet use has taken place overwhelmingly among the upper-income, educated, and urban segments\" largely due to the high literacy rates of this sector of the population. As such, James suggests that part of the solution requires that developing countries first build up the literacy/language skills, computer literacy, and technical competence that low-income and rural populations need in order to make use of ICT.\n\nIt has also been suggested that there is a correlation between democrat regimes and the growth of the Internet. One hypothesis by Gullen is, \"The more democratic the polity, the greater the Internet use...Government can try to control the Internet by monopolizing control\" and Norris \"et al.\" also contends, \"If there is less government control of it, the Internet flourishes, and it is associated with greater democracy and civil liberties.\n\nFrom an economic perspective, Pick and Azari state that \"in developing nations…foreign direct investment (FDI), primary education, educational investment, access to education, and government prioritization of ICT as all important\". Specific solutions proposed by the study include: \"invest in stimulating, attracting, and growing creative technical and scientific workforce; increase the access to education and digital literacy; reduce the gender divide and empower women to participate in the ICT workforce; emphasize investing in intensive Research and Development for selected metropolitan areas and regions within nations\".\n\nThere are projects worldwide that have implemented, to various degrees, the solutions outlined above. Many such projects have taken the form of Information Communications Technology Centers (ICT centers). Rahnman explains that \"the main role of ICT intermediaries is defined as an organization providing effective support to local communities in the use and adaptation of technology. Most commonly an ICT intermediary will be a specialized organization from outside the community, such as a non-governmental organization, local government, or international donor. On the other hand, a social intermediary is defined as a local institution from within the community, such as a community-based organization.\n\nOther proposed solutions that the Internet promises for developing countries are the provision of efficient communications within and among developing countries, so that citizens worldwide can effectively help each other to solve their own problems. Grameen Banks and Kiva loans are two microcredit systems designed to help citizens worldwide to contribute online towards entrepreneurship in developing communities. Economic opportunities range from entrepreneurs who can afford the hardware and broadband access required to maintain Internet cafés to agribusinesses having control over the seeds they plant.\n\nAt the Massachusetts Institute of Technology, the IMARA organization (from Swahili word for \"power\") sponsors a variety of outreach programs which bridge the Global Digital Divide. Its aim is to find and implement long-term, sustainable solutions which will increase the availability of educational technology and resources to domestic and international communities. These projects are run under the aegis of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and staffed by MIT volunteers who give training, install and donate computer setups in greater Boston, Massachusetts, Kenya, Indian reservations the American Southwest such as the Navajo Nation, the Middle East, and Fiji Islands. The CommuniTech project strives to empower underserved communities through sustainable technology and education. According to Dominik Hartmann of the MIT's Media Lab, interdisciplinary approaches are needed to bridge the global digital divide.\n\nBuilding on the premise that any effective solution must be decentralized, allowing the local communities in developing nations to generate their own content, one scholar has posited that social media—like Facebook, YouTube, and Twitter—may be useful tools in closing the divide. As Amir Hatem Ali suggests, \"the popularity and generative nature of social media empower individuals to combat some of the main obstacles to bridging the digital divide\". Facebook’s statistics reinforce this claim. According to Facebook, more than seventy-five percent of its users reside outside of the US. Moreover, more than seventy languages are presented on its website. The reasons for the high number of international users are due to many the qualities of Facebook and other social media. Amongst them, are its ability to offer a means of interacting with others, user-friendly features, and the fact that most sites are available at no cost. The problem with social media, however, is that it can be accessible, provided that there is physical access. Nevertheless, with its ability to encourage digital inclusion, social media can be used as a tool to bridge the global digital divide.\n\nSome cities in the world have started programs to bridge the digital divide for their residents, school children, students, parents and the elderly. One such program, founded in 1996, was sponsored by the city of Boston and called the Boston Digital Bridge Foundation. It especially concentrates on school children and their parents, helping to make both equally and similarly knowledgeable about computers, using application programs, and navigating the Internet.\n\nFree Basics is a partnership between social networking services company Facebook and six companies (Samsung, Ericsson, MediaTek, Opera Software, Nokia and Qualcomm) that plans to bring affordable access to selected Internet services to less developed countries by increasing efficiency, and facilitating the development of new business models around the provision of Internet access. In the whitepaper realised by Facebook's founder and CEO Mark Zuckerberg, connectivity is asserted as a \"human right\", and Internet.org is created to improve Internet access for people around the world.\n\n\"Free Basics provides people with access to useful services on their mobile phones in markets where internet access may be less affordable. The websites are available for free without data charges, and include content about news, employment, health, education and local information etc. By introducing people to the benefits of the internet through these websites, we hope to bring more people online and help improve their lives.\"\n\nHowever, Free Basics is also accused of violating net neutrality for limiting access to handpicked services. Despite a wide deployment in numerous countries, it has been met with heavy resistance notably in India where the Telecom Regulatory Authority of India eventually banned it in 2016.\n\nSeveral projects to bring internet to the entire world with a satellite constellation have been devised in the last decade, one of these being Starlink by Elon Musk's company SpaceX. Unlike Free Basics, it would provide people with a full internet access and would not be limited to a few selected services. In the same week Starlink was announced, serial-entrepreneur Richard Branson announced his own project OneWeb, a similar constellation with approximately 700 satellites that has already procured communication frequency licenses for their broadcast spectrum and could possibly be operational as early as in 2019.\n\nThe biggest hurdle of these projects is the astronomical financial and logistical costs of launching so many satellites. After the failure of previous satellite-to-consumer space ventures, satellite industry consultant Roger Rusch said \"It's highly unlikely that you can make a successful business out of this.\" Musk has publicly acknowledged this business reality, and indicated in mid-2015 that while endeavoring to develop this technically-complicated space-based communication system he wants to avoid overextending the company and stated that they are being measured in the pace of development.\n\nOne Laptop Per Child (OLPC) is another attempt to narrow the digital divide. This organization, founded in 2005, provides inexpensively produced \"XO\" laptops (dubbed the \"$100 laptop\", though actual production costs vary) to children residing in poor and isolated regions within developing countries. Each laptop belongs to an individual child and provides a gateway to digital learning and Internet access. The XO laptops are designed to withstand more abuse than higher-end machines, and they contain features in context to the unique conditions that remote villages present. Each laptop is constructed to use as little power as possible, have a sunlight-readable screen, and is capable of automatically networking with other XO laptops in order to access the Internet—as many as 500 machines can share a single point of access.\n\nSeveral of the 67 principles adopted at the World Summit on the Information Society convened by the United Nations in Geneva in 2003 directly address the digital divide:\n\n\n\n\n"}
{"id": "38500724", "url": "https://en.wikipedia.org/wiki?curid=38500724", "title": "Electronic media and sleep", "text": "Electronic media and sleep\n\nThe use of computers (including devices such as smartphones, tablet computers and laptops) by children and adolescents before bed has been associated with a reduction in the hours of sleep experienced by frequent users, along with a decreased quality of sleep, in most cases. The results of computer use at night have been linked with tiredness.\n\nA 2010 review concluded that \"the use of electronic media by children and adolescents does have a negative impact on their sleep, although the precise effects and mechanisms remain unclear\", with the most consistent results associating excessive media use with shorter sleep duration and delayed bed times. A 2016 meta-analysis found that \"Bedtime access and use of media devices was significantly associated with inadequate sleep quantity; poor sleep quality; and excessive daytime sleepiness\".\n\nThe American Academy of Pediatrics recommends screen time for children be limited for multiple reasons, among them that \"Too much screen time can also harm the amount and quality of sleep\".\n\nMany apps promise to improve sleep by filtering out blue light produced by media devices; there have been no large studies to assess whether such apps work. Some users express dissatisfaction with the resultant orange tint of screens. Some people use blue-blocking glasses, for the purpose of attempting to block out blue light both from electronic media and from other artificial light sources.\n\n"}
{"id": "37134263", "url": "https://en.wikipedia.org/wiki?curid=37134263", "title": "Emergency disconnect package", "text": "Emergency disconnect package\n\nAn Emergency Disconnect Package (EDP) is a piece of equipment used in the drilling and work-over (servicing or modification) of deep sea oil & gas wells, by Mobile Offshore Drilling Rigs (MODU's) and Well Intervention Vessels (WIV's). The EDP is designed for use in an emergency, when the MODU or WIV needs to quickly disconnect, and move away from the oil/gas well that it is drilling or working-over. Examples of when this might be necessary include unexpected extreme weather that exceeds the MODU/Vessel's capability to maintain its position. \n\nUnder normal operating conditions, the MODU/WIV (which is floating on the sea surface) is connected to the oil/gas well (which is drilled in the sea bed) by a vertical (or near-vertical) piece of steel pipe, called a marine riser. Tools and fluids are moved within the marine riser as required to/from the well. At the bottom of the marine riser, the EDP and other components that connect the riser to the well and allow the well to be shut-in when required, constitute a 'Lower Riser Package' (LRP). \n\nWhen required to do so, the EDP disconnects from the LRP and isolates the riser from the environment. Thus the EDP allows the MODU/WIV to safely and quickly disconnect from the subsea well and move away in an emergency. The EDP is designed to carry out its function while under load with a high disconnection angle.\n\nAn EDP consists of a connector to the rest of the LRP, an isolation valve, an accumulator, a subsea control module and a connection point at the top for connection to the riser pipe. A production retainer valve shuts in the riser and the annulus master valve shuts in the riser. A crossover valve allows circulation of the riser after disconnection.\n<refs. IADC Drilling Lexicon / API RP 17G, Recommended Practice for Completion/Workover Risers / API RP 96, Deepwater Well Design and Construction / GSO Engineering>\n"}
{"id": "7709376", "url": "https://en.wikipedia.org/wiki?curid=7709376", "title": "Excitation filter", "text": "Excitation filter\n\nAn excitation filter is a high quality optical-glass filter commonly used in fluorescence microscopy and spectroscopic applications for selection of the excitation wavelength of light from a light source. Most excitation filters select light of relatively short wavelengths from an excitation light source, as only those wavelengths would carry enough energy to cause the object the microscope is examining to fluoresce sufficiently. The excitation filters used may come in two main types — short pass filters and band pass filters. Variations of these filters exist in the form of notch filters or deep blocking filters (commonly employed as emission filters). Other forms of excitation filters include the use of monochromators, wedge prisms coupled with a narrow slit (for selection of the excitation light) and the use of holographic diffraction gratings, etc. [for beam diffraction of white laser light into the required excitation wavelength (selected for by a narrow slit)].\n\nAn excitation filter is commonly packaged with an emission filter and a dichroic beam splitter in a cube so that the group is inserted together into the microscope. The dichroic beam splitter controls which wavelengths of light go to their respective filter.\n"}
{"id": "54620481", "url": "https://en.wikipedia.org/wiki?curid=54620481", "title": "FIRST Global", "text": "FIRST Global\n\nFIRST Global (For Inspiration and Recognition of Science and Technology) is a trade name for a nonprofit organization, the International First Committee Association. It promotes STEM education and careers for youth through Olympics-style robotics competitions called the FIRST Global Challenge. It was founded by Dean Kamen in 2016 as an expansion of FIRST, an organization with similar objectives. \n\nFIRST Global is a trade name for the International First Committee Association, a nonprofit corporation based in Manchester, New Hampshire, with a 501(c)3 designation from the IRS.\n\nIt was founded by the co-founder of FIRST, Dean Kamen, with the objective of promoting STEM education and careers in the developing world through Olympics-style robotics competitions.\n\nEach year, FIRST Global announces the host city of the next year's Challenge during that year's closing ceremony. For example, during the closing ceremony of the 2017 FIRST Global Challenge, entrepreneur Ricardo Salinas, a founding member of FIRST Global, announced that Mexico City would host the 2018 FIRST Global Challenge.\nThe 2017 FIRST Global Challenge was held in Washington, D.C., from July 17–19, and the challenge was the use of robots to separate different colored balls, representing clean water and impurities in water, symbolizing the Engineering Grand Challenge (based on the Millennium Development Goal) of improving access to clean water in the developing world. Around 160 teams composed of 15- to 18-year-olds from 157 countries participated, and around 60% of teams were created or led by young women. Six continental teams also participated.\n\nThe 2018 FIRST Global Challenge was held in Mexico City from August 15-18. The 2018 Challenge is called Energy Impact and explores the impact of various types of energy on the world and how they can be made more sustainable. In the challenge, robots work together in teams of three to give cubes to human players, turn a crank, and score cubes in goals in order to generate electrical power. The challenge is based on three Engineering Grand Challenges; making solar energy affordable, making fusion energy a reality, and creating carbon sequestration methods.\n\nThe Global STEM Corps is a FIRST Global initiative that connects qualified volunteer mentors with students in developing countries to prepare them for competitions.\n"}
{"id": "25527138", "url": "https://en.wikipedia.org/wiki?curid=25527138", "title": "Federal Ministry of Science and Technology", "text": "Federal Ministry of Science and Technology\n\nThe Federal Ministry of Science & Technology is a Nigerian ministry whose mission is to facilitate the development and deployment of science and technology apparatus to enhance the pace of socio-economic development of the country through appropriate technological inputs into productive activities in the nation.\nIt is headed by a Minister appointed by the President, assisted by a Permanent Secretary, who is a career civil servant.\nPresident Muhammadu Buhari, GCFR on November 11, 2015 swore in Dr. Christopher Ogbonnaya Onu as the Minister of Science and Technology with Dr.(Mrs) Amina Muhammed Bello Shamaki as the permanent secretary in the ministry.\nThe ministry engages in the following activities:\n\nThe Computers for All Nigerians Initiative (CANi) program is focused on enhancing Nigeria's economic and social foundation by supplying access to personal computers (PCs) and internet to its citizens. It is a joint effort between the Federal Ministry of Science and Technology (FMST) and the National Information Technology and Development Agency (NITDA) with local banks and PC producers, as well as private technology companies like Intel and Microsoft.\n\n\nThe Ministry is responsible for a number of parastatals, or government-owned agencies:\n\n\n"}
{"id": "1337272", "url": "https://en.wikipedia.org/wiki?curid=1337272", "title": "FinTS", "text": "FinTS\n\nFinTS (Financial Transaction Services), formerly known as HBCI (Home Banking Computer Interface), is a bank-independent protocol for online banking, developed and used by German banks.\n\nHBCI was originally designed by the two German banking groups Sparkasse and Volksbanken und Raiffeisenbanken and German higher-level associations as the Association of German Banks (in German: Bundesverband deutscher Banken e.V.). The result of this effort was an open protocol specification, which is publicly available. The standardisation effort was necessary to replace the huge number of deprecated homemade software clients and servers (some of them still using BTX emulation).\nWhile IFX (Interactive Financial Exchange), OFX (Open Financial Exchange) and SET (Secure Electronic Transaction) are tailored for the North American market, HBCI is designed to meet the requirements of the European market.\n\nThe FinTS-specification is publicly available on a website run by the ZKA (Central Credit Committee).\n\n\nHBCI has been superseded by its successor FinTS, and as of 2011, 2000 financial institutions in Germany are supporting FinTS.\n\nHBCI 2.2 PIN/TAN (or HBCI+) is an extension to HBCI that added a security method based on PINs and TANs, which had already been in use with BTX and web banking.\n\nFor version 3.0, which formally introduced the PIN/TAN method, the specification was renamed to FinTS, whereas the original DSA- and RSA-based security method retained the name HBCI.\n\nIn version 4.0, the basic message syntax was switched over to XML. Further, the number of roundtrips necessary was reduced, allowing asynchronous communication (e.g. via SMTP) for simple transaction dialogues.\n\n"}
{"id": "23880274", "url": "https://en.wikipedia.org/wiki?curid=23880274", "title": "Flame projector", "text": "Flame projector\n\nIn pyrotechnics, a flame projector is a special effects device that projects a column of flame upwards, for a short, determined and controllable, period, usually on the order of a few seconds. The simplest form of flame projector is simply a vertical tube mounted on a base, containing powder and a hole for an electric match.\n\nFlame projectors can produce flames of different colours by simply adding a colouring agent. Potassium compounds make purple compounds, for instance. Lithium and strontium are red, sodium is bright yellow, copper and boron compounds are blue or green. \n"}
{"id": "11234940", "url": "https://en.wikipedia.org/wiki?curid=11234940", "title": "ISO/TC 37", "text": "ISO/TC 37\n\nISO/TC 37 is a technical committee within the International Organization for Standardization (ISO) that prepares standards and other documents concerning methodology and principles for terminology and language resources.\n\nTitle: Terminology and other language and content resources\n\nScope: Standardization of principles, methods and applications relating to terminology and other language and content resources in the contexts of multilingual communication and cultural diversity\n\nISO/TC 37 is a so-called \"horizontal committee\", providing guidelines for all other technical committees that develop standards on how to manage their terminological problems. However, the standards developed by ISO/TC 37 are not restricted to ISO. Collaboration with industry is sought to ensure that the requirements and needs from all possible users of standards concerning terminology, language and structured content are duly and timely addressed.\n\nInvolvement in standards development is open to all stakeholders and requests can be made to the TC through any liaison or member organization (see the list of current members and liaisons of ISO/TC 37:)\n\nISO/TC 37 standards are therefore fundamental and should form the basis for many localization, translation, and other industry applications.\n\nInternational Standards are developed by experts from industry, academia and business who are delegates of their national standards institution or another organization in liaison. Involvement, therefore, is principally open to all stakeholders. They are based on consensus among those national standards institutes who collaborate in the respective committee by way of membership.\n\nISO/TC 37 develops International Standards concerning:\n\nISO/TC 37 looks upon a long history of terminology unification activities. In the past, terminology experts - even more so experts of terminology theory and methodology - had to struggle for wide recognition. Today their expertise is sought in many application areas, especially in various fields of standardization. The emerging multilingual information society and knowledge society will depend on reliable digital content. Terminology is indispensable here. This is because terminology plays a crucial role wherever and whenever specialized information and knowledge is being prepared (e.g. in research and development), used (e.g. in specialized texts), recorded and processed (e.g. in data banks), passed on (via training and teaching), implemented (e.g. in technology and knowledge transfer), or translated and interpreted. In the age of globalization the need for methodology standards concerning multilingual digital content is increasing - ISO/TC 37 has developed over the years the expertise for methodology standards for science and technology related content in textual form.\n\nThe beginnings of terminology standardization are closely linked to the standardization efforts of IEC (International Electrotechnical Commission, founded in 1906) and ISO (International Organization for Standardization, founded in 1946).\n\nA terminology standard according to ISO/IEC Guide 2 (1996) is defined as \"standard that is concerned with terms, usually accompanied by their definitions, and sometimes by explanatory notes, illustrations, examples, etc.\"\n\nISO 1087-1:2000 defines terminology as \"\"set of designations belonging to one special language\" and designations as \"representation of a concept by a sign which denotes it\".\" Here, concept representation goes beyond terms (being only linguistic signs), which is also supported by the state-of-the-art of terminology science, according to which terminology has three major functions:\n\n\nThe above indicates that terminological data (comprising various kinds of knowledge representation) possibly have a much more fundamental role in domain-related information and knowledge than commonly understood.\n\nToday, terminology standardization can be subdivided into two distinct activities:\n\nThe two are mutually interdependent, since the standardization of terminologies would not result in high-quality terminological data, if certain common principles, rules and methods are not observed. On the other hand, these standardized terminological principles, rules and methods must reflect the state-of-the-art of theory and methodology development in those domains, in which terminological data have to be standardized in connection with the formulation of subject standards.\n\nTerminology gained a special position in the field of standardization at large, which is defined as \"activity of establishing, with regard to actual or potential problems, provisions for common and repeated use, aimed at the achievement of the optimum degree of order in a given context\" (ISO/IEC 1996). Every technical committee or sub-committee or working group has to standardize subject matters, define and standardize its respective terminology. There is a consensus that terminology standardization precedes subject standardization (or \"subject standardization requires terminology standardization\").\n\nISO/TC 37 was put into operation in 1952 in order \"to find out and formulate general principles of terminology and terminological lexicography\" (as terminography was called at that time).\n\nThe history of terminology standardization proper - if one excludes earlier attempts in the field of metrology - started in the International Electrotechnical Commission (IEC), which was founded in London in 1906 following a recommendation passed at the International Electrical Congress, held in St. Louis, United States, on 15 September 1904, to the extent that: \"\"...steps should be taken to secure the co-operation of the technical societies of the world, by the appointment of a representative Commission to consider the question of the standardization of the nomenclature and ratings of electrical apparatus and machinery\".\" From the very beginning, IEC considered it its foremost task to standardize the terminology of electrotechnology for the sake of the quality of its subject standards, and soon embarked upon the International Electrotechnical Vocabulary (IEV), whose first edition, based on many individual terminology standards, was published in 1938. The IEV is still being continued today, covering 77 chapters as parts of the International Standard series IEC 60050. The IEV Online Database can be accessed on Electropedia \n\nThe predecessor to the International Organization for Standardization (ISO), the International Federation of Standardizing Associations (ISA, founded in 1926), made a similar experience. But it went a step further and - triggered by the publication of Eugen Wüster's book \"Internationale Sprachnormung in der Technik\" [International standardization of technical language] (Wüster 1931) - established in 1936 the Technical Committee ISA/TC 37 \"Terminology\" for the sake of formulating general principles and rules for terminology standardization.\n\nISA/TC 37 conceived a scheme of four classes of recommendations for terminology standardization mentioned below, but the Second World War interrupted its pioneering work. Nominally, ISO/TC 37 was established from the very beginning of ISO in 1946, but it was decided to re-activate it only in 1951 and the Committee started operation in 1952. Since then until 2009 the secretariat of ISO/TC 37 has been held by the International Information Centre for Terminology (Infoterm), on behalf of the Austrian Standards Institute Austria. Infoterm, an international non-governmental organization based in Austria, continues to collaborate as a twinning secretariat. After this the administration went to CNIS (China).\n\nTo prepare standards specifying principles and methods for the preparation and management of language resources within the framework of standardization and related activities. Its technical work results in International Standards (and Technical Reports) covering terminological principles and methods as well as various aspects of computer-assisted terminography. ISO/TC 37 is not responsible for the co-ordination of the terminology standardizing activities of other ISO/TCs.\n\n\nISO 639 Codes for the representation of names of languages, with the following parts:\n\n\"Note: Current status is not mentioned here - see ISO Website for most recent status. Many of these are in development.\":\n\n"}
{"id": "295156", "url": "https://en.wikipedia.org/wiki?curid=295156", "title": "Index of electronics articles", "text": "Index of electronics articles\n\nThis is an index of articles relating to electronics and electricity or natural electricity and things that run on electricity and things that use or conduct electricity.\n\n16VSB –\n2VSB –\n32VSB –\n4000 series –\n4VSB –\n555 timer IC –\n7400 series –\n8VSB\n\nAbsolute gain (physics) –\nAcceptance pattern –\nAccess control –\nAccess time –\nAcoustic coupler –\nAdaptive communications –\nAdder –\nAdjacent-channel interference –\nAlarm sensor –\nAliasing –\nAllied Electronics –\nAlternating current –\nAM radio –\nAmateur radio –\nAmbient noise level –\nAmerican Radio Relay League (ARRL) –\nAmmeter –\nAmpere –\nAmplifier –\nAmplitude distortion –\nAmplitude modulation –\nAnalog computer –\nAnalog decoding –\nAnalog –\nAnalog to digital converter –\nAnalogue switch –\nAnalysis of resistive circuits –\nAngular misalignment loss –\nAntenna –\nAntenna aperture –\nAntenna blind cone –\nAntenna gain –\nAntenna height above average terrain –\nAntenna noise temperature –\nAntenna theory –\nAperiodic antenna –\nAperture (antenna) –\nAperture illumination –\nAperture-to-medium coupling loss –\nApollo Guidance Computer –\nArithmetic and logical unit –\nArmstrong oscillator –\nARRL –\nArticulation score –\nAstable –\nAsymmetric Digital Subscriber Line –\nAsynchronous communications system –\nAsynchronous operation –\nAsynchronous start-stop –\nAtmospheric duct –\nAtmospheric waveguide –\nAttenuation –\nAudible ringing tone –\nAudio system measurements –\nAudiophile –\nAutomatic call distributor –\nAutomatic gain control –\nAutomatic link establishment –\nAutomatic number identification –\nAutomatic sounding –\nAutomatic switching system –\nAutovon –\nAvailability –\nAvailable line –\nAvalanche diode –\nAzimuth\n\nBackplane –\nBackscattering –\nBack-to-back connection –\nBackward channel –\nBalance return loss –\nBalanced line –\nBalancing network –\nBall grid array –\nBand gap –\nBand-stop filter –\nBandwidth compression –\nBare particular –\nBarrage jamming –\nBaseband –\nBattery (electricity) –\nBaud –\nBaudot code –\nBCS theory –\nBeam diameter –\nBeam divergence –\nBeam steering –\nBeamwidth –\nBeat frequency oscillator -\nBel –\nBiconical antenna –\nBig ugly dish –\nBilateral synchronization –\nBillboard antenna –\nBinary classification –\nBinary multiplier –\nBinaural recording –\nBipolar junction transistor –\nBipolar signal –\nBit inversion –\nBit pairing –\nBit robbing –\nBit stuffing –\nBit synchronous operation –\nBit-count integrity –\nBits per second –\nBlack facsimile transmission –\nBlack recording –\nBlanketing –\nBluetooth –\nBlu-ray Disc -\nBNC connector –\nBoresight –\nBreadboard –\nBremsstrahlung –\nBridging loss –\nBroadband Internet –\nBroadband wireless access –\nBroadband –\nBroadcasting –\nBurst transmission –\nBusy hour –\nBusy signal –\nBypass\n\nCable modem –\nCable television –\nCaesium standard –\nCall collision –\nCall set-up time –\nCall-second –\nCapacitive coupling –\nCapacitor –\nCapture effect –\nCarbon nanotube –\nCard standards –\nCarrier sense multiple access with collision detection –\nCarrier shift –\nCarrier system –\nCarrier wave –\nCarrier-to-receiver noise density –\nCarson bandwidth rule –\nCassegrain antenna –\nCategory 5 cable –\nCathode ray tube –\nCentral processing unit –\nChadless tape –\nChannel –\nChannel noise level –\nChannel reliability –\nCharacter-count integrity –\nCharacteristic impedance –\nCharge-coupled device –\nChemical vapor deposition –\nChirp –\nChroma subsampling –\nCircuit breaker –\nCircuit noise level –\nCircuit reliability –\nCircuit restoration –\nCircuit switching –\nCircular polarization –\nCirculator –\nCitizens' band radio –\nCladding –\nClapp oscillator –\nClean room –\nClear channel –\nClearing –\nClipping –\nClock gating –\nClock signal –\nClosed waveguide –\nClosed-circuit television –\nCMOS –\nCoaxial cable –\nCo-channel interference –\nCode division multiple access –\nCode word –\nCoherence length –\nCoherence time –\nCoherence –\nCoherent differential phase-shift keying –\nCoherer –\nCoilgun –\nCollinear antenna array –\nCollinear antenna array –\nCollins Radio –\nColpitts oscillator –\nCombat-net radio –\nCombinational logic –\nCombined distribution frame –\nCommon base –\nCommon battery –\nCommon collector –\nCommon control –\nCommon emitter –\nCommonality –\nCommon-mode interference –\nCommunications center –\nCommunications satellite –\nCommunications security –\nCommunications system engineering –\nCommunications system –\nCommunications-electronics –\nCompact audio cassette –\nCompatible sideband transmission –\nComposite image filter -\nComposite video –\nCompulsator –\nComputer –\nConcentrator –\nConditioning equipment –\nConducted interference –\nConduction band –\nConductive coupling –\nConnections per circuit hour –\nConservation of radiance –\nConstant k filter -\nContent delivery –\nContention –\nContinuous Fourier transform –\nContinuous operation –\nContinuous wave –\nConvolution –\nCopper –\nCord circuit –\nCorner reflector –\nCosmic noise –\nCostas loop –\nCoulomb's law –\nCounter (digital) –\nCoupling –\nCovert channel –\nCovert listening device –\nCPU design –\nCQD –\nC-QUAM –\nCritical frequency –\nCross product –\nCrossbar switch –\nCrosstalk –\nCrystal filter –\nCrystal radio receiver –\nCurrent –\nCurrent bias –\nCurrent-to-voltage converter –\nCutback technique –\nCutoff frequency –\nCutoff wavelength\n\nD region –\nD-4 –\nData bank –\nData circuit terminating equipment –\nData compaction –\nData integrity –\nData link –\nData service unit –\nData terminal equipment –\nData transmission circuit –\nData –\nDatasheet –\ndBa –\ndBm –\nDBrn –\nDDR SDRAM –\nDegree of isochronous distortion –\nDelay line –\nDelta modulation –\nDemand assignment –\nDemand factor –\nDemand load –\nDemodulation –\nDemodulator –\nDeparture angle –\nDesign objective –\nDespun antenna –\nDeviation –\nDial-up –\nDiamagnetism –\nDielectric constant –\nDielectric strength –\nDielectric waveguide –\nDielectric –\nDifferential amplifier –\nDiffraction –\nDigital access and cross-connect system –\nDigital Audio Tape –\nDigital circuit –\nDigital filter –\nDigital multiplex hierarchy –\nDigital radio –\nDigital signal processing –\nDigital signal processor –\nDigital to analog converter –\nDigital transmission group –\nDigitizer –\nDIN –\nDiode –\nDIP switch –\nDipole antenna –\nDipole –\nDirect bandgap –\nDirect broadcast satellite –\nDirect current –\nDirect distance dialing –\nDirect ray –\nDirectional antenna –\nDirectional coupler –\nDirective gain –\nDirect-sequence spread spectrum –\nDiscrete Fourier transform –\nDiscrete –\nDispersion-limited operation –\nDisplay device -\nDistortion –\nDistortion-limited operation –\nEmergency locator beacon –\nDistributed switching –\nDiurnal phase shift –\nDiversity reception –\nDOD master clock –\nDoping –\nDouble-sideband suppressed-carrier transmission –\nDouble-slit experiment –\nDrift –\nDrop and insert –\nDropout –\nDual access –\nDual impedance -\nDual in-line package –\nDual-modulus prescaler –\nDual-tone multi-frequency –\nDuobinary signal –\nDuplex –\nDuty cycle –\nDXCC –\nDynamic range\n\nEarphone –\nEarpiece –\nEarth's magnetic field –\nEDIF –\nEEPROM –\nEffective antenna gain contour –\nEffective boresight area –\nEffective data transfer rate –\nEffective Earth radius –\nEffective height –\nEffective input noise temperature –\nEffective isotropically radiated power –\nEffective monopole radiated power –\nEffective radiated power –\nEffective transmission rate –\nEfficiency factor –\nE-layer –\nElectric charge –\nElectric current –\nElectric field –\nElectric motor –\nElectric power transmission –\nElectric power –\nElectrical conduction –\nElectrical conductivity –\nElectrical connector –\nElectrical current –\nElectrical efficiency –\nElement –\nElectrical engineering –\nElectrical generator –\nImpedance –\nInsulation –\nElectrical length –\nLoad –\nElectrical network –\nCircuit –\nElectrical resistance –\nElectrical room –\nElectrical signal –\nElectricity distribution –\nElectricity –\nElectrochemical cell –\nElectrochemistry –\nElectrode –\nElectrodynamics –\nElectrolytic capacitor –\nElectromagnetic environment –\nElectromagnetic induction –\nElectromagnetic interference control –\nElectromagnetic pulse –\nElectromagnetic radiation and health –\nElectromagnetic radiation –\nElectromagnetic spectrum –\nElectromagnetic survivability –\nElectromagnetism –\nElectrometer –\nElectron hole –\nElectron –\nElectronic amplifier –\nElectronic color code –\nColor code –\nElectronic data processing –\nElectronic deception –\nElectronic design automation –\nElectronic filter –\nFilter –\nElectronic imager –\nElectronic mixer –\nElectronic musical instrument –\nElectronic oscillator –\nElectronic power supply –\nElectronic switching system –\nElectronic tagging –\nElectronic test equipment –\nElectronic warfare support measures –\nElectronics –\nElectro-optic effect –\nElectro-optic modulator –\nElectro-optics –\nElectrostatic discharge –\nElectrostatics –\nEmergency Locator Transmitter –\nEmergency Position-Indicating Radio Beacon –\nEmitter coupled logic –\nEnd distortion –\nEndurability –\nEnhanced service –\nEntropy encoding –\nEquilibrium length –\nEquivalent impedance transforms -\nEquivalent noise resistance –\nEquivalent pulse code modulation noise –\nError burst –\nError ratio –\nError-correcting code –\nE-skip –\nExamples of electrical phenomena –\nExtremely Low Frequency (ELF) –\nEye pattern\n\nFab (semiconductors) –\nSemiconductor device fabrication –\nFacsimile converter –\nFading –\nFading distribution –\nFail safe –\nFall time –\nFan-beam antenna –\nFarad –\nFaraday cage –\nFaraday constant –\nFaraday's law of induction –\nFar-field region –\nFault –\nFault management –\nFCC registration program –\nFederal Standard 1037C –\nFeed horn –\nFeedback –\nFerroelectric effect –\nFerromagnetism –\nField (physics) –\nField –\nField effect transistor –\nField strength –\nFPGA Field programmable gate array –\nFilled cable –\nFilter design –\nFilter (signal processing) -\nFlip-flop –\nFluorescent lamp –\nFlutter –\nFlux –\nFlywheel effect –\nFM band –\nFM improvement factor –\nFM improvement threshold –\nFM radio –\nForward error correction –\nFourier series –\nFourier transform (see also List of Fourier-related transforms) –\nFour-wire circuit –\nFour-wire terminating set –\nFractal antenna –\nFrame –\nFrame rate –\nFrame slip –\nFrame synchronization –\nFraming bit –\nFreenet –\nFree-space loss –\nFreeze frame television –\nFrequency assignment –\nFrequency averaging –\nFrequency counter –\nFrequency deviation –\nFrequency frogging –\nFrequency modulation synthesis –\nFrequency modulation –\nFrequency standard –\nFrequency synthesiser –\nFrequency –\nFrequency-division multiplexing –\nFrequency-exchange signaling –\nFrequency-hopping spread spectrum –\nFrequency-shift keying –\nFresnel equations –\nFresnel reflection –\nFresnel zone –\nFront-to-back ratio –\nFuel cell –\nFuse (electrical)\n\nGallium arsenide –\nGalvanic isolation -\nGalvanometer –\nGateway –\nGating –\nGauss –\nGeiger–Müller tube –\nGel electrophoresis –\nGemini Guidance Computer –\nGender changer –\nGlobal Positioning System –\nGlobal system for mobile communications –\nGNU Radio –\nGrade of service –\nGraded-index fiber –\nGround constants –\nGround loop –\nGround plane –\nGround (electricity) –\nGroundwave –\nGuided ray –\nGyrator\n\nHalftone characteristic –\nHall effect –\nHamming code –\nHamming distance –\nHandoff –\nHandshaking –\nHard copy –\nHardware register –\nHarmonic analysis –\nHarmonic oscillator –\nHarmonic –\nHartley oscillator –\nH-channel –\nHeat sink –\nHelical antenna –\nHelmholtz coil –\nHenry (unit) –\nHertz –\nHeterodyne repeater –\nHeterodyne –\nHigh frequency –\nHigh-performance equipment –\nHigh-speed circuit-switched data –\nHop –\nHorn –\nHow to test three-phase electrical supply –\nHow to test three-phase pumps –\nHybrid balance –\nHybrid circuit –\nHybrid coil –\nHybrid coupler –\nHysteresis\n\nIEEE 315-1975 -\nIEEE 802.11 –\nIEEE 802.15 –\nIEEE 802 –\nImage antenna –\nImage impedance -\nImage frequency –\nImage rejection ratio –\nImage response –\nImpedance matching –\nIn-band on-channel –\nIncidental radiator –\nIndependent sideband –\nIndex of cooperation –\nInductive coupling –\nInductive reactance –\nInductor –\nIndustrial Computers -\nInformation transfer –\nInformation-bearer channel –\nInfrared –\nInput/output –\nInsertion gain –\nInsertion loss –\nInside plant –\nIntegrated circuit –\nIntensity modulation –\nIntentional radiator –\nIntercept –\nInterchange circuit –\nIntercharacter interval –\nInterconnect facility –\nInterference –\nInterferometry –\nIntermediate-field region –\nIntermodulation distortion –\nInternational Electrotechnical Commission –\nInteroperability –\nInterposition trunk –\nIntersymbol interference –\nInverse multiplexer –\nInverse-square law –\nIon pump –\nIonosphere –\nISM band –\nIsochronous burst transmission –\nIsochronous signal –\nIsotropic antenna\n\nJam signal –\nJamming –\nJansky –\nJitter\n\nKarnaugh map –\nKendall effect –\nKey pulsing –\nKirchhoff's circuit laws –\nKlystron –\nKnife-edge effect\n\nLaser –\nLaunch angle –\nLaunch numerical aperture –\nLead-lag effect –\nLeaky mode –\nLight bulb –\nLight-dependent resistor –\nLight-emitting diode –\nLightning –\nLimiting –\nLine code –\nLinear feedback shift register –\nLinear regulator –\nLip synchronization –\nList of telephony terminology –\nLists of video game companies -\nLM741 –\nLow noise amplifier –\nLoading characteristic –\nLoading coil –\nLobe –\nLocal battery –\nLogic families –\nLogic gate –\nLogic –\nLog-periodic antenna –\nLong-haul communications –\nLongitudinal redundancy check –\nLong-tailed pair –\nLong-term stability –\nLoop –\nLoop gain –\nLoop-back –\nLow frequency –\nLow-performance equipment –\nLumped element model\n\nMacroelectronics -\nMagnet –\nMagnetic core memory –\nMagnetic field –\nMagnetic flux quantum –\nMagnetic flux –\nMagnetic levitation –\nMagnetism –\nMagneto-optic effect –\nMagnetosphere –\nMagnetron –\nMain distribution frame –\nMain lobe –\nManchester code –\nMaser –\nMask work –\nMaster frequency generator –\nMaximal-ratio combiner –\nMaximum power –\nMaximum usable frequency –\nMaxwell coil –\nMaxwell's demon –\nMaxwell's equations –\nm-derived filter -\nMean time between outages –\nMediation function –\nMedium frequency (MF) –\nMedium-power talker –\nMediumwave –\nMetal -\nMichelson-Morley experiment –\nMicroelectronics –\nMicrophone –\nMicrowave auditory effect –\nMicrowave oven –\nMicrowave –\nMIL-STD-188 –\nMinimum bend radius –\nMode scrambler –\nMode volume –\nModem –\nModular synthesizer –\nModulation factor –\nModulation rate –\nModulation –\nMolecular electronics –\nMonostable –\nMoore's law –\nMorse code –\nMOS Technology 6501 –\nMOS Technology 6502 –\nMOS Technology SID –\nMOS Technology VIC-II –\nMu-law algorithm –\nMulticoupler –\nMulti-element dipole antenna –\nMultimeter –\nMultipath propagation –\nMultiple access –\nMultiple homing –\nMultiplex baseband –\nMultiplexer –\nMultiplexing –\nMultivibrator\n\nN connector –\nNanotechnology –\nNanowire –\nNarrative traffic –\nNarrowband modem –\nNarrowband –\nNational Electrical Code (US) –\nNatural frequency –\nNear-field region –\nNegative resistance –\nNegative-acknowledge character –\nNet gain –\nNetlist –\nNetwork administration –\nNetwork architecture –\nNetwork management –\nNeural network –\nNeutral direct-current telegraph system –\nNI Multisim –\nNickel metal hydride –\nNoise figure –\nNoise level –\nNoise power –\nNoise temperature –\nNoise weighting –\nNoise-cancelling headphone –\nNoise-equivalent power –\nNon-return-to-zero –\nNormalized frequency –\nNorton's theorem -\nNTSC –\nNuclear electromagnetic pulse -\nNuclear magnetic resonance –\nNull –\nNumbers station –\nNumerical aperture –\nNumerically controlled oscillator –\nNyquist interval\n\nOff-hook –\nOff-line –\nOhm –\nOhmmeter –\nOhm's law –\nOliver Heaviside –\nOmnidirectional antenna –\nOne-way trunk –\nOn-hook –\nOn-line –\nOpen circuit –\nOpen spectrum –\nOperational amplifier –\nOptical density –\nOptical fiber –\nOptical path length –\nOptical spectrum –\nOptoelectronic –\nOrthogonal frequency division modulation –\nOrthomode transducer –\nOscilloscope –\nOut-of-band signaling –\nOutside plant –\nOverflow –\nOverhead information –\nOvermodulation –\nOverride –\nOvershoot (signal)\n\nPacket switching –\nPacket-switching node –\nPaired disparity code –\nPAL –\nPar meter –\nParabolic antenna –\nParabolic microphone –\nParallel transmission –\nParasitic element (electrical networks) –\nParity bit –\nPassband –\nPassive radiator -\nPatch bay –\nPath loss –\nPath profile –\nPauli exclusion principle –\nPBER –\nPCB layout guidelines –\nPeak envelope power –\nPeltier effect –\nPerformance measurement period –\nPeriodic antenna –\nPeriscope antenna –\nPermeability –\nPermittivity –\nPersonal Locator Beacon –\nPhantom circuit –\nPhantom loop –\nPhase –\nPhase distortion –\nPhase jitter –\nPhase modulation –\nPhase noise –\nPhase perturbation –\nPhased array –\nPhase-locked loop –\nPhase-shift keying –\nPhilberth-Transformer –\nPhone connector (audio) –\nPhotodiode –\nPhotoelectric effect –\nPhotolithography –\nPhoton –\nPhysical layer –\nPickup –\nPID controller –\nPiezoelectricity –\nPin grid array –\nPirate radio –\nPlanar array –\nPlanck's constant –\nPlesiochronous Digital Hierarchy –\nPoint-to-point construction –\nPolarential telegraph system –\nPolarization –\nPolling –\nPolyphase system –\nPortable people meter –\nPotential difference –\nPotential divider –\nPower –\nPower connector –\nPower supply –\nPreamplifier –\nPreemphasis network –\nPreemphasis –\nPreferred values –\nPreventive maintenance –\nPrimary channel –\nPrimary line constants -\nPrimary time standard –\nPrincipal clock –\nPrinted circuit board –\nProcessor register –\nProduct detector –\nProgrammable logic device –\nPropagation delay –\nPropagation mode –\nPropagation path obstruction –\nPropagation of schema –\nProration –\nPseudorandom noise –\nPseudorandom number sequence –\nPSK31 –\nPulse amplitude –\nPulse duration –\nPulse –\nPulse-address multiple access –\nPulse-code modulation –\nPulsed inductive thruster –\nPulse-width modulation –\nPush-to-talk operation –\nPush-to-type operation –\nPyroelectricity\n\nQ code –\nQRP operation –\nQ-switching –\nQuadrature amplitude modulation –\nQuadrature –\nQuality assurance –\nQuality control –\nQuantum harmonic oscillator –\nQuartz clock –\nQuasi-analog signal –\nQueuing delay\n\nRace hazard –\nRadar –\nRadiation angle –\nRadiation mode –\nRadiation pattern –\nRadiation resistance –\nRadiator –\nRadio beam –\nRadio clock –\nRadio electronics –\nRadio frequency induction –\nRadio frequency –\nRadio horizon range –\nRadio horizon –\nRadio propagation –\nRadio range –\nRadio Row, Manhattan –\nRadio station –\nRadio –\nRadioShack –\nRadiotelephone –\nRadioteletype –\nRadix-64 –\nRailgun –\nRandom access memory –\nRandomizer –\nRay transfer matrix analysis –\nRC circuit –\nRC –\nRCA jack –\nRCA –\nReactance –\nReceive-after-transmit time delay –\nReceived noise power –\nReceiver –\nReceiver attack-time delay –\nReconnaissance satellite –\nRecord medium –\nReference antenna –\nReference circuit –\nReference clock –\nReference noise –\nReference surface –\nReflection coefficient –\nReflections of signals on conducting lines -\nReflective array antenna –\nRefractive index contrast –\nRegenerative circuit –\nRegister transfer level –\nRegistered jack –\nRelational model –\nRelative transmission level –\nRelaxation oscillator –\nRelay –\nRelease time –\nRemote Operations Service Element protocol –\nRemote sensing –\nRepair and maintenance –\nRepeater –\nRepeating coil –\nReproduction speed –\nReradiation –\nResistor color code –\nResistor –\nResonance –\nResponse time –\nResponsivity –\nReturn loss –\nRF connector –\nRF modulator –\nRF power margin –\nRF probe –\nRF shielding –\nRFID –\nRGB color space –\nRhombic antenna –\nRing current –\nRing latency –\nRing modulation –\nRingback signal –\nRingdown –\n\nRL circuit –\nRLC circuit –\nRobot –\nRogowski coil –\nRoot mean square –\nRouting indicator –\nRS-232 –\nRX –\nRydberg formula\n\nS/PDIF –\nSacrificial anode –\nSampling frequency –\nScalar field –\nScanner –\nScanning electron microscope –\nSCART –\nSchematic –\nSchumann resonance –\nScrambler –\nScreen –\nSECAM –\nSecond audio program –\nSecond-order intercept point –\nSecurity management –\nSelf-clocking signal –\nSelf-synchronizing code –\nSemiautomatic switching system –\nSemiconductor device –\nSemiconductor –\nSensitivity –\nSensor Networks –\nSeparate channel signaling –\nSerial access –\nSerial ATA –\nSerial Peripheral Interface Bus –\nSerial transmission –\nSeries and parallel circuits –\nShadow loss –\nShannon limit –\nShannon's theorem –\nShort circuit –\nShortwave –\nShot noise –\nShrinking generator –\nSide lobe –\nSideband –\nSidereal time –\nSiemens –\nSignal (information theory) –\nSignal compression –\nSignal processing gain –\nSignal processing –\nSignal reflection –\nSignal transition –\nSignal-to-crosstalk ratio –\nSignal-to-noise ratio –\nSignature block –\nSignificant condition –\nSilicon bandgap temperature sensor –\nSilicon –\nSimplex circuit –\nSimplex signaling –\nSinc filter –\nSingle frequency networks –\nSingle-phase electric power –\nSingle-frequency signaling –\nSingle-polarized antenna –\nSingle-sideband modulation –\nSkew (antenna) –\nSkin effect –\nSkip zone –\nSkywave –\nSlant range –\nSlewing –\nSlew rate -\nSlot antenna –\nSlow-scan television –\nSoftware-defined radio –\nSolar cell –\nSoldering –\nSolenoid –\nSound card –\nSpace diversity –\nSpace tether –\nSpark gap –\nSpecific detectivity –\nSpecification –\nSpeckle pattern –\nSpectral width –\nSpectrum –\nSpectrum analyzer –\nSpeed of light –\nSpeed of service –\nSPICE –\nSpill-forward feature –\nSpillover –\nSpin glass –\nSpot beam –\nSpread spectrum –\nSpurious emission –\nSquelch –\nStandard telegraph level –\nStandard test signal –\nStandard test tone –\nStanding wave ratio –\nStanding wave –\nStarpath Supercharger –\nStart signal –\nStart-stop transmission –\nStatic electricity –\nSteady-state condition –\nStep-index profile –\nStoletov's law -\nStop signal –\nStopband –\nStore-and-forward switching center –\nStressed environment –\nStrobe light –\nStroke speed –\nSubcarrier –\nSubtractive synthesis –\nSudden ionospheric disturbance –\nSupercomputer –\nSuperconductivity –\nSuperheterodyne receiver –\nSuperparamagnetism –\nSuperposition theorem -\nSupervisory program –\nSuppressed carrier transmission –\nSurface wave –\nSurface-mount technology –\nSurveillance device –\nSurvivability –\nS-Video –\nSwitch –\nSwitched-mode power supply –\nSynchronism –\nSynchronization –\nSynchronizing –\nSynchronous network –\nSynchronous optical networking –\nSynthesizer –\nSystem integrity –\nSystems control\n\nTable of standard electrode potentials –\nTactical communications system –\nTactical communications –\nTactical data information link--A –\nTantalum –\nTape relay –\nT-carrier –\nTechnical control facility –\nTelecommunication –\nCommunications –\nTelecommunications service –\nTeleconference –\nTelegrapher's equations –\nTelegraphy –\nTelemetry –\nTelephone tapping –\nTeletext –\nTeletraining –\nTelevision –\nTelevision reception –\nTEMPEST –\nTensor –\nTesla coil –\nTesla patents –\nTest antenna –\nTether propulsion –\nThermal noise –\nThermistor –\nThévenin's theorem -\nThird-order intercept point –\nTNC connector –\nThree phase –\nTime-assignment speech interpolation –\nTime-division multiple access –\nTime-division multiplexing –\nTime-domain reflectometer –\nTime-out –\nTinfoil hat –\nToll switching trunk –\nTotal harmonic distortion –\nTotal internal reflection –\nTraffic intensity –\nTraffic shaping –\nTransceiver –\nTranscoding –\nTransducer –\nTransformer –\nTransient electromagnetic device –\nTransimpedance amplifier –\nTransistor radio –\nTransistor –\nTransistor-transistor logic –\nTTL –\nTransition metal –\nTransmission coefficient –\nTransmission level point –\nTransmission line –\nTransmission medium –\nMedium –\nTransmit-after-receive time delay –\nTransmitter attack-time delay –\nTransmitter –\nTransmitter-studio link –\nTransparent latch –\nTransponder –\nTransverse redundancy check –\nTraveling-wave tube –\nTRF –\nTriangle wave –\nTrimline telephone –\nTroposphere –\nTropospheric ducting –\nTropospheric wave –\nTuner –\nTwisted pair -\nTX\n\nUltra high frequency –\nUltra Wideband –\nUltraviolet –\nUnavailability –\nUncertainty principle –\nUniform linear array –\nUnijunction transistor –\nUnintentional radiator –\nUplink –\nUpright position (electronics) –\nUser (telecommunications)\n\nVAC –\nVačkář oscillator –\nVacuum tube –\nValence band –\nVariable length buffer –\nVaricap –\nVaristor –\nVDC –\nVector field –\nVeroboard –\nVery high frequency –\nVery-large-scale integration –\nVHSIC hardware description language –\nVideo cassette recorder –\nVideo Game Console -\nVideo Game - \nVideo teleconference –\nVideo teleconferencing unit –\nVideo –\nVintage amateur radio –\nVirtual circuit capability –\nVirtual circuit –\nVirtual ground –\nVoice frequency primary patch bay –\nVoice frequency –\nVolt- \nVoltage bias –\nVoltage-to-current converter –\nVoltmeter –\nVox\n\nWardenclyffe Tower –\nWarner exemption –\nWatt –\nWave impedance –\nWave propagation –\nWave –\nWaveform –\nWaveguide antenna –\nWaveguide –\nWavelength division multiplexing –\nWavelength –\nWheatstone bridge –\nWhip antenna –\nWhite facsimile transmission –\nWideband modem –\nWilliams tube –\nWink pulsing –\nWire wrap –\nWire –\nWireless access point –\nWireless community network –\nWireless network –\nWireless personal area network –\nWireless -\nX-dimension of recorded spot –\nXLR connector\n\nYagi antenna –\nY-delta transform –\nYUV\n\nZero dBm transmission level point –\nZero insertion force (ZIF) -\nZero-dispersion wavelength –\nZigBee –\nZig-zag in-line package –\nZobel network -\nZone melting\nZ-transform –\n"}
{"id": "1922181", "url": "https://en.wikipedia.org/wiki?curid=1922181", "title": "Internal documentation", "text": "Internal documentation\n\nComputer software is said to have Internal Documentation if the notes on how and why various parts of code operate is included within the source code as comments. It is often combined with meaningful variable names with the intention of providing potential future programmers a means of understanding the workings of the code.\n\nThis contrasts with external documentation, where programmers keep their notes and explanations in a separate document.\n\nInternal documentation has become increasingly popular as it cannot be lost, and any programmer working on the code is immediately made aware of its existence and has it readily available.\n"}
{"id": "11025494", "url": "https://en.wikipedia.org/wiki?curid=11025494", "title": "Internationalization Tag Set", "text": "Internationalization Tag Set\n\nThe Internationalization Tag Set (ITS) is a set of attributes and elements designed to provide internationalization and localization support in XML documents.\n\nThe ITS specification identifies concepts (called \"ITS data categories\") which are important for internationalization and localization. It also defines implementations of these concepts through a set of elements and attributes grouped in the ITS namespace. XML developers can use this namespace to integrate internationalization features directly into their own XML schemas and documents.\n\nITS v1.0 includes seven data categories:\n\n\nThe vocabulary is designed to work on two different fronts: First by providing markup usable directly in the XML documents. Secondly, by offering a way to indicate if there are parts of a given markup that correspond to some of the ITS data categories and should be treated as such by ITS processors.\n\nITS applies to both new document types as well as existing ones. It also applies to both markups without any internationalization features as well as the class of documents already supporting some internationalization or localization-related functions.\n\nITS can be specified using global rules and local rules.\n\n\nIn the following ITS markup example, the elements and attributes with the codice_2 prefix are part of the ITS namespace. The codice_1 element lists the different rules to apply to this file. There is one codice_4 rule that indicates that any content inside the codice_5 element should not be translated.\n\nThe codice_6 attributes used in some elements are utilised to override the global rule. Here, to make translatable the content of codice_7 and to make non-translatable the text \"faux pas\".\n\nIn the following ITS markup example, the codice_8 element specifies that any node corresponding to the XPath expression codice_9 has an associated note. The location of that note is expressed by the codice_10 attribute, which holds a relative XPath expression pointing to the node where the note is, here codice_11.\n\nNote also the use of the codice_6 attribute to mark the codice_13 elements as non-translatable.\n\nITS does not have a solution to all XML internationalization and localization issues.\n\nOne reason is that version 1.0 does not have data categories for everything. For example, there is currently no way to indicate a relation source/target in bilingual files where some parts of a document store the source text and some other parts the corresponding translation.\n\nThe other reason is that many aspects of internationalization cannot be resolved with a markup. They have to do with the design of the DTD or the schema itself. There are best practices, design and authoring guidelines that are necessary to follow to make sure documents are correctly internationalized and easy to localize. For example, using attributes to store translatable text is a bad idea for many different reasons, but ITS cannot prevent an XML developer from making such choice.\n\nSome of the ITS 1.0 limitations are being addressed in the version 2.0: See http://www.w3.org/TR/its20/ for more details.\n\n"}
{"id": "5380820", "url": "https://en.wikipedia.org/wiki?curid=5380820", "title": "Investment-specific technological progress", "text": "Investment-specific technological progress\n\nInvestment-specific technological progress refers to progress that requires investment in new equipment and structures embodying the latest technology in order to realize its benefits.\n\nTo model how something is produced, think of a box that in one end takes in inputs such as labor (employees) and capital (equipment, buildings, etc.) and in another end spits out the final good. With this picture in mind now one can ask, how does technological progress affect production? One way of thinking is that technological progress affects \"specific\" inputs (arrows going in) such as equipment and buildings. To realize the benefits of such technological change for production, these inputs must be purchased. So for example, the advent of the microchip (an important technological improvement in computers) will affect the production of Ford cars only if Ford Motor Co.'s assembly plants (the red box) invest in computers with microchips (instead of computers with punched cards) and use them (they are one of the arrows going in the box) in the production of Mustangs (the arrow coming out). As the name suggests, this is \"investment-specific\" technological progress---it requires investing in new machines or buildings which contain or \"embody\" the latest technology. Notice that the term \"investment\" can be general: not only must a firm buy the new technology to reap its benefits, but it also must invest in training its workers and managers to be able to use this new technology (Greenwood & Jovanovic 2001) .\n\nIdentifying \"investment-specific\" technological progress is important, because knowing what type of technological progress is operating in an economy will determine how someone (should) want his or her tax dollars to be spent and how he or she may want to invest his or her savings (Gort et al. 1999). If \"investment-specific\" technological change is the main source of progress, then one would want his or her dollars spent on helping firms buy new equipment and renovate their plants, because these investments will improve production and hence what you consume. Furthermore, one may want to help pay for current employee training in using new technologies (to keep them up to date) or subsidize the education of new employees (who will enter the job market knowing how to use the new technology). So, the type of technological progress will also matter for unemployment and education issues. Finally, if technological progress is \"investment-specific\" you may want to direct your money towards the research and development (R & D) of new technologies (like quantum computers or alternative energy sources) (Krusell 1998).\n\nMore generally, why is any type of technological progress important? Technological change has made our lives easier. Because of technological progress, people can work less, make more money and enjoy more leisure time (Greenwood & Vandenbroucke 2006). Women have been able to break away from the traditional \"housewife\" role, join the labor-force in greater numbers (Greenwood et al. 2005) and become less economically dependent on men (Greenwood & Guner 2009). Finally, technological progress has been shown to affect the fall in child labor starting around 1900 (Greenwood & Seshadri 2005). Figure 1 illustrates this last point: in 1900 child labor's share of the paid labor force began to fall.\n\nAn example of investment-specific technological progress is the microwave oven. The idea of the microwave came to be by accident: in 1946 an engineer noticed that a candy bar in his pocket had melted while working on something completely unrelated to cooking (Gallawa 2005). The development of this good, from melting the candy bar to the home appliance we know today, took time and the investment of resources to make a microwave small and cheap. The first microwave oven cost between 2000 and 3000 dollars and was housed in refrigerator-sized cabinets (Gallawa 2005)! Today, almost any college student can enjoy a 3-minute microwaveable meal in the smallest dorm room. But a microwave's uses do not stop at the dorm room. Many industries have found microwave heating advantageous: it has been used to dry cork, ceramics, paper, leather, and so on (Gallawa 2005). However, for either college students or firms to reap the benefits of quick warming, they must first \"invest\" in a microwave oven (that \"embodies\" the technological advance). To realize the benefits of investment-specific technological progress you must first invest in a technology that embodies it.\n\nWhile measuring technological progress is not easy, economists have found indirect ways of estimating it. If \"'investment-specific'\" technological progress makes producing goods easier, then the price of the goods affected (relative to the price of other goods) should decrease. In particular, \"investment-specific\" technological advance has affected the prices of two inputs into the production process: equipment and structures. Think of equipment as machines (like computers) and structures as buildings. If there is technological progress in the production (or creation) of these goods, then one would expect the price of them to fall or the value of them to rise relative to older versions of the same good.\n\nFigure 2 (the pink line) shows how the price of new producer durables (such as equipment) in the US relative to the price of new consumer nondurables (like clothing) has consistently declined over the past fifty years (Gort et al. 1999). To calculate the relative price of producer durables divide the price that firms pay (for the durable inputs of production) by the price that a regular consumer pays (for things like jeans). We use relative prices so we can say how many units of equipment can be bought instead (or in terms) of buying one unit of consumer goods. Figure 3 (the pink line) says that over time, firms have been able to buy more and more units of equipment instead of one unit of consumption, especially when we take into account that the quality of equipment being acquired has increased (a computer today is much faster than a computer five years ago and we should take that into account when comparing their prices). When changes in quality are not taken into account (which is wrong) it looks like the price of equipment has not decreased as much (see the black line in Figure 2).\n\nMeasuring the price of structures is more complicated than measuring the price of equipment, but economists have again been able to get an idea of how much progress there has been in structures (such as buildings). One approach is that if newer buildings were constructed or designed using newer technologies then they should be worth more than older buildings (because they embody the new technology (Gort et al. 1999). In particular, they should rent for more. As Figure 3 shows, this is true. Renting a square foot in a new building is much more expensive than renting a square foot in a building forty years old. So it must be the case that you are paying for a nicer, more functional and maybe even safer building.\nFigures 2 and 3 suggest that investment-specific technological change is operating in the US. The annual rate of technological progress in equipment and structures has been estimated to be about 3.2% and 1%, respectively (Gort et al. 1999) (Greenwood et al. 1997).\n\nIn the second section it was mentioned that \"investment-specific\" technological change is important since it will affect production (both in quality and size). An important question then is, just how much \"bang for your buck\" do you get with \"investment-specific\" technological change? The answer is quite astounding; economists have found that 37% of growth in US output (production) is due to technological progress in equipment and 15% is due to technological progress in structures (Gort et al. 1999) (Greenwood et al. 1997). All in all, more than half (37% + 15% = 52%) of the growth of the US economy is due to \"investment-specific\" technological change (Gort et al. 1999) (Greenwood et al. 1997).\n\n"}
{"id": "7822632", "url": "https://en.wikipedia.org/wiki?curid=7822632", "title": "MTV-1", "text": "MTV-1\n\nThe MTV-1 Micro TV was the second model of a near pocket-sized television. The first was the Panasonic IC model TR-001 introduced in 1970. The MTV-1 was developed by Clive Sinclair (Sinclair Radionics Ltd). It was shown to the public at trade shows in London and Chicago in January, 1977, and released for sale in 1978. Development spanned 10 years and included a cash infusion of (about ) from the UK government in 1976.\n\nThe MTV-1 used a German AEG Telefunken black-and-white, electrostatic cathode ray tube (CRT), the smallest CRT built into a commercially available product, and included a rechargeable 4-AA-cell NiCad battery pack. It measured and weighed . It was able to receive either PAL or NTSC transmissions on VHF or UHF. A Welsh company, Wolsey Electronics, manufactured it for Sinclair. Custom ICs made by Texas Instruments and Sinclair contributed to its small size and low power consumption.\n\nThe original (about ) price tag proved to be too high to sell many of them, and Sinclair lost over in 1978, eventually selling its remaining inventory to liquidators at greatly reduced prices.\n\nThe MTV-1B, released later in 1978 at the much lower price of , was able to receive only British and South African UHF PAL signals.\n\n\n"}
{"id": "85332", "url": "https://en.wikipedia.org/wiki?curid=85332", "title": "Man page", "text": "Man page\n\nA man page (short for manual page) is a form of software documentation usually found on a Unix or Unix-like operating system. Topics covered include computer programs (including library and system calls), formal standards and conventions, and even abstract concepts. A user may invoke a man page by issuing the codice_1 command.\n\nBy default, codice_1 typically uses a terminal pager program such as codice_3 or codice_4 to display its output.\n\nIn the first two years of the history of Unix, no documentation existed. The \"Unix Programmer's Manual\" was first published on November 3, 1971. The first actual man pages were written by Dennis Ritchie and Ken Thompson at the insistence of their manager Doug McIlroy in 1971. Aside from the man pages, the \"Programmer's Manual\" also accumulated a set of short papers, some of them tutorials (e.g. for general Unix usage, the C programming language, and tools such as Yacc), and others more detailed descriptions of operating system features. The printed version of the manual initially fit into a single binder, but as of PWB/UNIX and the 7th Edition of Research Unix, it was split into two volumes with the printed man pages forming Volume 1.\n\nLater versions of the documentation imitated the first man pages' terseness. Ritchie added a \"How to get started\" section to the Third Edition introduction, and Lorinda Cherry provided the \"Purple Card\" pocket reference for the Sixth and Seventh Editions. Versions of the software were named after the revision of the manual; the seventh edition of the \"Unix Programmer's Manual\", for example, came with the 7th Edition or Version 7 of Unix.\n\nFor the Fourth Edition the man pages were formatted using the troff typesetting package and its set of codice_5 macros (which were completely revised between the Sixth and Seventh Editions of the \"Manual\", but have since not drastically changed). At the time, the availability of online documentation through the manual page system was regarded as a great advance. To this day, virtually every Unix command line application comes with a man page, and many Unix users perceive a program's lack of man pages as a sign of low quality; indeed, some projects, such as Debian, go out of their way to write man pages for programs lacking one. The modern descendants of 4.4BSD also distribute man pages as one of the primary forms of system documentation (having replaced the old codice_5 macros with the newer codice_7).\n\nFew alternatives to codice_1 have enjoyed much popularity, with the possible exception of GNU Project's \"codice_9\" system, an early and simple hypertext system.\nIn addition, some Unix GUI applications (particularly those built using the GNOME and KDE development environments) now provide end-user documentation in HTML and include embedded HTML viewers such as codice_10 for reading the help within the application.\n\nMan pages are usually written in English, but translations into other languages may be available on the system.\nThe default format of the man pages is troff, with either the macro package man (appearance oriented) or mdoc (semantic oriented). This makes it possible to typeset a man page into PostScript, PDF, and various other formats for viewing or printing.\n\nMost Unix systems have a package for the man2html command, which enables users to browse their man pages using an html browser (textproc/man2html on FreeBSD or \"man\" on some Linux distribution).\n\nIn 2010, OpenBSD deprecated troff for formatting manpages in favour of mandoc, a specialised compiler/formatter for manpages with native support for output in PostScript, HTML, XHTML, and the terminal.\n\nIn February 2013, the BSD community saw a new open source mdoc.su service launched, which unified and shortened access to the man.cgi scripts of the major modern BSD projects through a unique nginx-based deterministic URL shortening service for the *BSD man pages.\n\nThere was a hidden easter egg in the man-db version of the man command that would cause the command to return \"gimme gimme gimme\" when run at 00:30 (a reference to the ABBA song Gimme! Gimme! Gimme! (A Man After Midnight). It was introduced in 2011 but first restricted and then removed in 2017 after finally being found.\n\nTo read a manual page for a Unix command, a user can type:\n\nPages are traditionally referred to using the notation \"name(section)\": for example, . The section refers to different ways the topic might be referenced - for example, as a system call, or a shell (command line) command or package, or a package's configuration file, or as a coding construct / header.\n\nThe same page name may appear in more than one section of the manual, such as when the names of system calls, user commands, or macro packages coincide. Examples are and , or and . The syntax for accessing the non-default manual section varies between different man implementations. \n\nOn Solaris and illumos, for example, the syntax for reading is:\n\nOn Linux and BSD derivatives the same invocation would be:\nwhich searches for \"printf\" in section 3 of the man pages.\n\nThe manual is generally split into eight numbered sections, organized as follows (on Research Unix, BSD, macOS and Linux):\n\nUnix System V uses a similar numbering scheme, except in a different order:\n\nOn some systems some of the following sections are available:\n\nSome sections are further subdivided by means of a suffix; for example, in some systems, section 3C is for C library calls, 3M is for the math library, and so on. A consequence of this is that section 8 (system administration commands) is sometimes relegated to the 1M subsection of the main commands section. Some subsection suffixes have a general meaning across sections:\nSome versions of man cache the formatted versions of the last several pages viewed.\n\nAll man pages follow a common layout that is optimized for presentation on a simple ASCII text display, possibly without any form of highlighting or font control. Sections present may include: \n\n\nOther sections may be present, but these are not well standardized across man pages. Common examples include: OPTIONS, EXIT STATUS, RETURN VALUE, ENVIRONMENT, BUGS, FILES, AUTHOR, REPORTING BUGS, HISTORY and COPYRIGHT.\n\n\n"}
{"id": "8254740", "url": "https://en.wikipedia.org/wiki?curid=8254740", "title": "Metadata Encoding and Transmission Standard", "text": "Metadata Encoding and Transmission Standard\n\nThe Metadata Encoding and Transmission Standard (METS) is a metadata standard for encoding descriptive, administrative, and structural metadata regarding objects within a digital library, expressed using the XML schema language of the World Wide Web Consortium (W3C). The standard is maintained as part of the MARC standards of the Library of Congress, and is being developed as an initiative of the Digital Library Federation (DLF).\n\nMETS is an XML Schema designed for the purpose of:\n\nDepending on its use, a METS document could be used in the role of Submission Information Package (SIP), Archival Information Package (AIP), or Dissemination Information Package (DIP) within the Open Archival Information System (OAIS) Reference Model.\n\nMaintaining a library of digital objects requires maintaining metadata about those objects. The metadata necessary for successful management and use of digital objects is both more extensive than and different from the metadata used for managing collections of printed works and other physical materials. METS is intended to promote the preservation of, and interoperability between digital libraries. \n\nThe open flexibility of METS means that there is not a prescribed vocabulary which allows many different types of institutions, with many different document types, to utilize METS. The customization of METS makes it highly functional internally, but creates limitations for interoperability. Interoperability becomes difficult when the exporting and importing institutions have used vocabularies. As a workaround for this problem the creation of institutional profiles has become popular. These profiles document the implementation of METS specific to that institution helping to map content in order for exchanged METS documents to be more usable across institutions.\n\nAs early as 1996 the University of California, Berkeley began working toward the development of a system that combined encoding for an outline of a digital object's structure with metadata for that object. In 1998 this work was expanded upon by the Making of America II project (MoAII). An important objective of this project was the creation of a standard for digital objects that would include defined metadata for the descriptive, administrative, and structural aspects of a digital object. A type of structural and metadata encoding system using an XML Document Type Definition (DTD) was the result of these efforts. The MoAII DTD was limited in that it did not provide flexibility in which metadata terms could be used for the elements in the descriptive, administrative, and structural metadata portions of the object. In 2001, a new version of the DTD was developed that used namespaces separate from the system rather than the vocabulary of the previous DTD. This revision was the foundation for the current METS schema, officially named in April of that year.\n\nAny METS document has the following features:\n\n\nA profile is expressed as an XML document. There is a schema for this purpose. The profile expresses the requirements that a METS document must satisfy.\nA sufficiently explicit METS Profile may be considered a \"data standard\".\n\n\n\n\n"}
{"id": "53680386", "url": "https://en.wikipedia.org/wiki?curid=53680386", "title": "MicroVision, Inc.", "text": "MicroVision, Inc.\n\nMicroVision, Inc. is a US company that develops laser scanning technology for projection, 3D sensing, and image capture. MicroVision's display technology uses a micro-electrical mechanical systems (MEMS) scanning mirror with lasers, optics and electronics to project and/or capture images. The company licenses its products primarily to original equipment manufacturers (OEMs) such as Sony Corporation and STMicroelectronics\n\nThe MEMS scanning micro-mirror is the basis of MicroVision’s technology platform. The MEMS design consists of a silicon device with a millimeter-scale mirror at the center. The mirror is connected to flexures that allow it to swing vertically and horizontally to display (or capture) an image. In projection-mode the MEMS laser beam scanning display method can be compared to raster scanning in a cathode ray tube (CRT) display.\nProduct applications include mobile projection, virtual retinal display, head-mounted display, automotive head-up display, and 3D depth sensing (LiDAR). Head-mounted displays are an emerging area of development. \n\nIn May 2018, Microvision entered into a license agreement with a global technology company to use company's display technology to manufacture and sell display-only engines.\n\n"}
{"id": "3088252", "url": "https://en.wikipedia.org/wiki?curid=3088252", "title": "Ministry of Higher Education and Science (Denmark)", "text": "Ministry of Higher Education and Science (Denmark)\n\nThe Danish Ministry of Higher Education and Science () is the Danish ministry in charge of research and education above high school/upper secondary school.\n\nThe ministry has also been known as the \"Ministry of Science, Innovation and Higher Education\", the \"Ministry of Science, Technology and Innovation of Denmark\", the \"Science Ministry\", the \"Research Ministry\", and the \"Ministry of Research and Technology\".\n\nIts primary purpose is to promote and coordinate the interaction between the industry and trade, centres of research and education and strengthen industry and research policies.\n\nSøren Pind was appointed Minister for the Ministry of Higher Education and Science 28 November 2016. \n\n"}
{"id": "41520190", "url": "https://en.wikipedia.org/wiki?curid=41520190", "title": "Ministry of Science, ICT and Future Planning", "text": "Ministry of Science, ICT and Future Planning\n\nThe Ministry of Science, ICT and Future Planning (MSIP, ) was a ministry of the Government of South Korea. Its purpose is to set, manage, and evaluate science and technology policy, support scientific research and development, develop human resources, conduct R&D leading to the production and consumption of Atomic power, plan national informatization and information protection strategies, manage radio frequency bands, oversee the information and communications technology (ICT) industry, and operate Korea Post. Its headquarters are in Building #4 of the Gwacheon Government Complex in Gwacheon, Gyeonggi Province.\n\nMinistry of Science and ICT succeeds the ministry from 2017.\n\nThe creation of the ministry was announced in February 2013. The ministry was created under a reorganization plan initiated by South Korean President Park Geun-hye in an effort to generate new sources of economic growth in the areas of science and information technology.\n\nThe creation of the ministry was one of Park's core pledges during the 2013 campaign leading to her election.\n\nThe ministry dissolved in July, 2017. Ministry of Science and ICT (과학기술정보통신부) succeeds the former ministry.\n\nChoi Mun-kee was the inaugural Minister of this Ministry. Later Choi Yanghee became the Minister of Science, ICT and Future Planning. He was nominated by President Park Geun-hye.\n\nPolicies on new media, such as cable TV service operators, satellite channels and digital multimedia broadcasting , have been transferred to this ministry. The ministry is expected to contribute to the creation of about 410,000 jobs in these areas by the year 2017, including about 90,000 jobs in business start-ups.\n\nThe ministry will help drive the so-called national informatization project, which seeks to introduce technology into a variety of areas including traditional markets, agriculture, and small- and medium-sized businesses.\n\nThe ministry is responsible for awarding the Korea Science and Technology Award in conjunction with the Korean Federation of Science and Technology Societies and the Korea Mobile App Award in conjunction with the MoneyToday publication.\n\nBecause the ministry not only took over from the former Ministry of Science and Technology, but also assumed responsibility for ICT from the Ministry of Information and Communication and control of Korea Post, some people worry that it has become bloated. Some others worry about its Korean name, as the Korean name translates directly to English as \"Ministry of Future Creation and Science\". Some scientists worry that it hints at Creationism.\n\nOn May 2013, a mission was created within the ministry, stating their purpose as \"Becoming lead gospel on ministry, and change country for god\". According to their business plan, they planned to evangelize one person every month, identifying Islam as a cult organization. People worry about the religious neutrality of public servants. A person in charge said \"the business plan is just a document made by a member of the mission, and is not an official policy\".\n\n"}
{"id": "39207065", "url": "https://en.wikipedia.org/wiki?curid=39207065", "title": "Mobile forms", "text": "Mobile forms\n\nA mobile form is an electronic or digital form application that functions on a smartphone or tablet device. Mobile forms enable users to collect data using mobile devices, and then to send the results back to the source. Mobile forms exist to replace paper forms as a more productive means of data collection, eliminating the need to transcribe or scan paper data results into a back office system.\n\nDepending on the mobile form application provider, some mobile form solutions allow offices to dispatch data to mobile form applications. In addition, other mobile form applications can be connected with various cloud services, servers, and social media platforms.\n\nDepending on the business, the motivating factors to deploying mobile forms may vary. Some businesses implement mobile forms to speed up processes, while others institute mobile forms with field users to reduce costs associated with transporting paper forms back and forth. Furthermore, green-minded businesses implement mobile forms in order to be more environmentally friendly, thus reducing their reliance on paper, ink printing, and subsequent waste.\n\nAdvanced mobile form features include signature capture, bar code capture, photo capture, GPS location form info, time form info, and skip logic.\n\nUses for mobile forms include:\n\n"}
{"id": "32170042", "url": "https://en.wikipedia.org/wiki?curid=32170042", "title": "Mobile technology in Africa", "text": "Mobile technology in Africa\n\nMobile technology in Africa is a fastest growing market. Nowhere is the effect more dramatic than in Africa, where mobile technology often represents the first modern infrastructure of any kind. Only 10% of Internet users are in Africa. However, 50% of Africans have mobile phones and their penetration is expanding rapidly. This means that mobile technology is the largest platform in Africa, and can access a wide range of income groups. AppsAfrica reports Mobile App downloads will reach 98 billion which will have a huge benefit for mobile app developers in Africa \n\nAs a consequence of the wider availability of mobile telephony with respect to fixed telephony, in many African countries most Internet traffic goes through the mobile network. An example is Seychelles, that is the African country with a larger percentage of Internet subscribers, where most Internet users access the net through the mobile network.\n\nSeveral factors contributed to the \"boom\" of mobile telephony in Africa in the 2000s.\n\nA major success factor of mobile telephony in Africa is the scarce diffusion of PSTNs (fixed line networks). In 2000, Sub-Saharan Africa as a whole had fewer telephone lines than Manhattan alone. Fixed line networks hardly reach the remote rural areas where a relevant percentage of the African population lives. Of about 400.000 rural settlements that are estimated to exist in Africa, less than 3% have PSTN access. Mobile telephony providers have taken advantage of this situation, implementing a very aggressive diffusion strategy for mobile networks. In 2006, 45% of rural settlements in Africa had GSM coverage. More recently, coverage has reached 90% of the territory in several countries, including Comoros, Kenya, Malawi, Mauritius, Seychelles, South Africa, and Uganda. Other countries that in 2007 reached above 50% of GSM coverage are Botswana, Burkina Faso, Burundi, Cape Verde, Guinea, Namibia, Rwanda, Senegal, Swaziland, and Togo. As a consequence of the larger diffusion of GSM networks over fixed line networks, \"mobile-telephone booths\" are common in some areas of Africa.\n\nThe fixed line market in Africa is generally based on monopoly (often state monopoly), with a few number of incumbent operators who did not invest in spreading their networks much farther than the larger urban areas. While this situation is changing (for example, both Telecom Kenya and Botswana Telecommunications Corporation have recently been privatized, and a market liberalization strategy has been initiated in several countries), the mobile telephony market is generally more competitive and dynamic.\n\nThe table below outlines the percentage of African countries where telecommunications markets (fixed line telephony, mobile telephony, Internet) are fully competitive, partially competitive, or monopolistic, either \"de iure\" or \"de facto\" (data refer to 2007).\n\nMobile telephony providers that introduced mobile telephony in Africa in the 2000s adopted business models explicitly designed to reach the poorest (and largest) section of the population, with low-priced mobile phones and small denomination prepaid cards.\n\nAnother key success factor in the providers' strategy in Africa has been the cutting down of roaming costs. This is especially relevant in Africa since strong relationships often hold between neighbouring communities that happen to be separated by national borders. Celtel was the first operator to provide free roaming with the 2006 One Network campaign, whereby roaming became free between Uganda, Kenya, and Tanzania. In 2007 this has been extended to Gabon, DR Congo, Congo-Brazzaville, Burkina Faso, Chad, Malawi, Niger, Nigeria, and Sudan. After Celtel, other providers operating in African markets have announced their intent to gradually reduce and eventually abolish roaming costs for certain areas.\n\nMobile technology can be used, not only to generate profit from high income groups, but to provide information and create social change for low income groups. For example, mobile technology is used to provide information on health, education, finances or to access specific groups such as the youth. \nHowever, people who are very poor have very basic phones. Thus non-profit mobile technology is not aimed at advanced smart phones, but ranges from sending out bulk SMSs to USSD, mobi-sites and mobile communities. AppsAfrica writes the next 1 Billion phone users will come from Rural areas \n\nThe ultimate aim of non-profit mobile technology is to make it fat, or as near to free for the end user. This means enlisting donors and getting mobile networks on board. Internationally, companies such as TextToChange, FrontlineSMS, RapidSMS, Ushahidi all work with mobiles in health, disaster relief and aid management.\n\nmHealth is using mobile technology to provide groups with health information. It was pioneered in part by the UN Foundation and Vodafone Foundation through partnerships with the World Health Organization (WHO) and the social enterprise DataDyne, who then joined with other partners in forging the mHealth Alliance. \nmHealth activities come in the form of appointment reminders, community mobilization and health promotion, emergency toll-free telephone services, health call centres, health surveys, information initiatives and patient monitoring among others.\n\nIn June 2011, the first African mobile health summit was held in Cape Town. At the summit, the WHO released a report stating that eighty-three per cent of governments surveyed had at least one mHealth project in their country. However, the majority of mHealth activities were limited in size and scope.\nmHealth initiatives were health call centres (59%), emergency toll-free telephone services (55%), managing emergencies and disasters (54%), and mobile telemedicine (49%).\n\nIn South Africa, companies like Cell-Life and GeoMed and HealthSMS use mobile technology for health.\n\nThe Praekelt Foundation is a South African example of a non profit organisation that is using mobile technology to create social change. Their programmes have currently reached 50 million people across 15 countries in sub-Saharan Africa.\n\nThe founders saw that the technology they were creating for corporate clients could be useful for NGOs to provide information to their target markets. “Full profit want to reach people for different reasons, but people should not be charged for having access to life saving information,” says Marcha Neethling, head of operations at Praekelt Foundation.\n\nOne of the mobile technologies developed by Praekelt Foundation is a mobile community called YoungAfricaLive (YAL). Users do not need to have airtime or data bundles on their phones to use it. The aim of the mobile community was to create a space that would be interactive and fun where young people could talk candidly and learn about love, relationships and sex and HIV/AIDS.\n\nThe mobile community is unique to the Vodacom network. At the end of 2010, Vodacom’s mobile platform, Vodafone Live, was receiving 3.2 million unique users monthly. As (young) people were already using mobile technology to surf the net and download songs etc. it seemed the perfect place to engage with this target group.\n\nThe community is aimed at users between 16 and 24 and users receive daily news and celeb stories. All with a social call to action at the end, they participate in polls, watch videos that link to stories and can engage in anonymous chat rooms. Experts come on to the chat rooms to discuss sexual topics and allow users to ask personal questions anonymously. For example, well known South African sexologist Dr Eve hosts live chats once a week.\n\nUsers have engaged with the community and many of the updated features of the community have come directly from user suggestions. Users have commented saying YoungAfricaLive creates a platform for them to express their ideas, making them proud of their status and encouraging them to be responsible around sex.\n\nThe ongoing challenge with free mobile communities and technology is continuing to engage the service provider to allow the community to be entirely free. “With YoungAfricaLive South Africa, Vodacom is sponsoring the bandwidth, which is a massive investment. .. (thus) sustainability is always a question.”\n\nIn 2011 Vodacom pioneered a project in South Africa to fight crime using mobile phones. They partnered with The Khulisa's Youth out of School Ubuntu Club in Tembisa, Johannesburg and donated a computer and seven mobile phones to the Club. These are used by the young patrollers in the community to keep in touch and to report all crime incidents, as well as update the community on current events.\n\nThe project is based in the Phomolong area of Tembisa, which is notorious for high levels of criminal activity. Each mobile phone donated has internet capabilities and the members of the Club will be allocated a mobile phone that they will use to capture events, interview members of the community and create video clips. These will be uploaded to their Facebook page and website all in an effort to report on criminal activity in the community.\n\nThe South African Police Service also runs a national crime line which they encourage citizens to SMS in and report crimes in their communities.\n\n\n"}
{"id": "56748872", "url": "https://en.wikipedia.org/wiki?curid=56748872", "title": "Operation Serenata de Amor", "text": "Operation Serenata de Amor\n\nOperation Serenata de Amor is an artificial intelligence project to analyze public spending in Brazil. The project has been funded by a recurrent financing campaign since September 7, 2016, and came in the wake of major scandals of misappropriation of public funds in Brazil, such as the Mensalão scandal and what was revealed in the Operation Car Wash investigations. \n\nThe analysis began with data from the National Congress and then expanded to other types of budget and other instances of government, such as the Federal Senate. The project is built through collaboration on GitHub and using a public group with more than 600 participants on Telegram.\n\nThe name \"Serenata de Amor,\" which means \"serenade of love,\" was taken from a popular cashew cream bonbon produced by Chocolates Garoto in Brazil.\n\nThroughout the development of the project new modules have been developed in addition to the main repository:\nOperation Serenata de Amor is an artificial intelligence project for the analysis of public expenditures. It was conceived in March 2016 by data scientist Irio Musskopf, sociologist Eduardo Cuducos and entrepreneur Felipe Cabral. The project was financed collectively in the Catarse platform, where it reached 131% of the collection goal paying 3 months of project development. Ana Schwendler, also a data scientist; Pedro Vilanova \"Tonny\", data journalist; Bruno Pazzim, software engineer; Filipe Linhares, a frontend engineer, Leandro Devegili, an entrepreneur and André Pinho took the first steps towards constructing the platform, such as collecting and structuring the first datasets.\n\nJessica Temporal, data scientist and Yasodara Córdova \"Yaso\", researcher; Tatiana Balachova \"Russa\", UX designer; joined the project after the financing took place.\n\nThe members created a recurring financing campaign, expanding the analysis of public spending to the Federal Senate. Donors make monthly payments ranging from 5 BRL to 200 BRL to maintain group activities. The monthly amount collected is around 10,000 BRL.\n\nIn January 2017, concluding the period financed by the initial campaign, the group carried out an investigation into the suspicious activities found by the data analysis system. 629 complaints were made to the Ombudsman's Office of the Chamber of Deputies, questioning expenses of 216 federal deputies. In addition, the Facebook project page has more than 25,000 followers, and users frequently cite the operation as a benchmark in transparency in the Brazilian government. One of the examples of results obtained by the operation is the case of the Deputy who had to return about 700 BRL to the House after his expenses were analyzed by the platform.\n\nThe platform was able to analyze more than 3 million notes, raising about 8,000 suspected cases in public spending. The community that supports the work of the team benefits from open source repositories, with licenses open for the collaboration. So much so that the two main data scientists of the project presented it at the CivicTechFest in Taipei, obtaining several mentions even in the international press. The technical leader presented the project in Poland during DevConf2017 in Kraków. It was also presented in the Google News Lab in 2017. It was presented by Yaso, when she was the Director of the iniciative, at the MIT Media Lab/Berkman Klein Center Initiative for Artificial Intelligence ethics, and at the Artificial Intelligence and Inclusion Symposium, an iniciative of the Global Network of Internet & Society Centers (NoC). It was also presented both by Irio and Yaso at the Digital Harvard Kennedy School, over a lunch seminar, where the transparency of the platform and the main solutions found were discussed, so that the code and data are always available to verify its suitability.\nThis infographic provides information about the first results of Operation Serenata de Amor, a project that analyzes open data on public spending to find discrepancies \n\nThe project was presented by Yaso to the House Audit and Control Committee of the Chamber of Deputies in August 2017, and raised the interest of House officials who work with open data.\n\nThe operation has been a source of inspiration for other civic projects that aim to work with similar goals. Participation of several team members in events throughout Brazil and abroad can be found on the Internet, such as presentation at OpenDataDay, held at Calango Hackerspace in the Federal District, Campus Party Bahia, Campus Party Brasilia, Friends of Tomorrow, XIII National Meeting of Internal Control, in the event USP Talks Hackfest against corruption in João Pessoa, the latter being also highlighted in the National Press.\n\n\n"}
{"id": "50056803", "url": "https://en.wikipedia.org/wiki?curid=50056803", "title": "Pakathon", "text": "Pakathon\n\nPakathon is a Boston-based, registered global non-profit organization with eight global chapters in four countries: Pakistan, the United States, Canada, and Australia. Founded in 2013, Pakathon aims to create jobs through innovation and entrepreneurship by mobilizing the Pakistani diaspora. By working to foster a global network of like-minded individuals, investors and entrepreneurs, Pakathon aims to solve problems in sectors such as education, healthcare, gender equality, energy and law.\n\nThe organization's flagship event is a weekend-long hackathon hosted in chapter cities where students and professionals develop and pitch their projects to a panel of mentors. Top-performing teams are then invited to present their ideas to a global audience where they compete for funding, mentorship, and additional support, with the aim of advancing their ideas from conception to reality. Two winning teams are then selected: one from Pakistan and another from the international host cities.\n\nIn 2015, Pakathon partnered with the Higher Education Commission of Pakistan to expand Pakathon's innovation-centred programming in up to 50 Pakistani universities.\n\nPakathon's first winning team conceived the idea for ProCheck, a serialization system for authentic medicines in Pakistan. ProCheck allows customers and patients to verify authentic medicines using their mobile phones via text messaging. In November, 2015 ProCheck partnered with Ferozsons Labs, a leading manufacturer of pharmaceuticals in Pakistan, to serialize 35 million units of medicine using ProCheck's track and trace solution. As a result, more than 50,000 patients across the country will be able to distinguish between genuine and counterfeit medicines.\n"}
{"id": "125659", "url": "https://en.wikipedia.org/wiki?curid=125659", "title": "Phonograph cylinder", "text": "Phonograph cylinder\n\nPhonograph cylinders are the earliest commercial medium for recording and reproducing sound. Commonly known simply as \"records\" in their era of greatest popularity (c. 1896–1915), these hollow cylindrical objects have an audio recording engraved on the outside surface, which can be reproduced when they are played on a mechanical cylinder phonograph. In the 1910s, the competing disc record system triumphed in the marketplace to become the dominant commercial audio medium.\n\nOn July 18, 1877, Thomas Edison and his team invented the phonograph. His first successful recording and reproduction of intelligible sounds, achieved early in the following December, used a thin sheet of tin foil wrapped around a hand-cranked grooved metal cylinder. Tin foil was not a practical recording medium for either commercial or artistic purposes and the crude hand-cranked phonograph was only marketed as a novelty, to little or no profit. Edison moved on to developing a practical incandescent electric light and the next improvements to sound recording technology were made by others.\n\nFollowing seven years of research and experimentation at their Volta Laboratory, Charles Sumner Tainter, Alexander Graham Bell and Chichester Bell introduced wax as the recording medium and engraving, rather than indenting, as the recording method. In 1887, their \"Graphophone\" system was being put to the test of practical use by official reporters of the US Congress, with commercial units later being produced by the Dictaphone Corporation. After this system was demonstrated to Edison's representatives, Edison quickly resumed work on the phonograph. He settled on a thicker all-wax cylinder, the surface of which could be repeatedly shaved down for reuse. Both the Graphophone and Edison's \"Perfected Phonograph\" were commercialized in 1888. Eventually, a patent-sharing agreement was signed and the wax-coated cardboard tubes were abandoned in favor of Edison's all-wax cylinders as an interchangeable standard format.\n\nBeginning in 1885, prerecorded wax cylinders were marketed. These have professionally made recordings of songs, instrumental music or humorous monologues in their grooves. At first, the only customers for them were proprietors of nickel-in-the-slot machines—the first juke boxes—installed in arcades and taverns, but within a few years private owners of phonographs were increasingly buying them for home use. Each cylinder can easily be placed on and removed from the mandrel of the machine used to play them. Unlike later, shorter-playing high-speed cylinders, early cylinder recordings were usually cut at a speed of about 120 rpm and can play for as long as 3 minutes. They were made of a relatively soft wax formulation and would wear out after they were played a few dozen times.\nThe buyer could then use a mechanism which left their surfaces shaved smooth so new recordings could be made on them.\n\nCylinder machines of the late 1880s and the 1890s were usually sold with recording attachments. The ability to record as well as play back sound was an advantage of cylinder phonographs over the competition from cheaper disc record phonographs which began to be mass-marketed at the end of the 1890s, as the disc system machines can be used only to play back prerecorded sound.\n\nIn the earliest stages of phonograph manufacturing various competing incompatible types of cylinder recordings were made. A standard system was decided upon by Edison Records, Columbia Phonograph, and other companies in the late 1880s. The standard cylinders are about long, in diameter, and play about of music or other sound.\n\nOver the years the type of wax used in cylinders was improved and hardened so that cylinders could be played with good quality over 100 times. In 1902 Edison Records launched a line of improved hard wax cylinders marketed as \"Edison Gold Molded Records\". The major development of this line of cylinders is that Edison had developed a process that allowed a mold to be made from a master cylinder which then permitted the production of several hundred cylinders to be made from the mold. The process was labeled, \"Gold Moulded\" because of the gold vapor that was given off by gold electrodes used in the process.\n\nOriginally all cylinders sold had to be recorded live on the softer brown wax which wore out in as few as twenty playings. Later cylinders were reproduced either mechanically or by linking phonographs together with rubber tubes. Although not completely satisfactory, the result was good enough to be sold.\n\nCylinders were sold in cardboard tubes with cardboard caps on each end, the upper one a removable lid. Like cylindrical containers for hats, they were simply called \"boxes\", the word still used by experienced collectors. Within them, the earliest soft wax cylinders came swathed in a separate length of thick cotton batting. Later molded hard-wax cylinders were sold in boxes with a cotton lining. Celluloid cylinders were sold in unlined boxes. These protective boxes were normally kept and used to house the cylinders after purchase. Their general appearance allowed bandleader John Philip Sousa to deride their contents as \"canned music\", an epithet he borrowed from Mark Twain, but that did not stop Sousa's band from profiting by recording on cylinders.\n\nThe earliest cylinder boxes have a plain brown paper exterior, sometimes rubber-stamped with the company name. By the late 1890s, record companies usually pasted a generic printed label around the outside of the box, sometimes with a penciled catalog number but no other indication of the identity of the recording inside. A slip of paper stating the title and performer was placed inside the box with the cylinder. At first this information was hand-written or typed on each slip, but printed versions became more common once cylinders were sold in large enough quantities to justify the printing set-up cost. The recording itself usually began with a spoken announcement of the title and performer and also the name of the record company. On a typical Edison record slip from 1903, the consumer is invited to cut off a coupon with the printed information and paste it onto the lid of the box. Alternatively, a circular area within the coupon could be cut out and pasted onto the end of a spindle for that cylinder in one of the specially built cases and cabinets made for storing cylinder records. Only a minority of cylinder record customers purchased such storage units, however. Slightly later, the record number was stamped on the lid, then later still a printed label with the title and artist information was factory-applied to the lid. Shortly after the start of the 20th century, an abbreviated version of this information was impressed into or printed on one edge of the cylinder itself.\n\nIn 1900, Thomas B Lambert was granted a patent that described a process for mass-producing cylinders made from celluloid, an early hard plastic (Henri Lioret of France was producing celluloid cylinders as early as 1893, but they were individually recorded rather than molded). That same year, the Lambert Company of Chicago began selling cylinder records made of the material. They would not break if dropped and could be played thousands of times without wearing out, although the choice of the bright pink color of early cylinders was arguably a marketing error. The color was changed to black in 1903, but brown and blue cylinders were also produced. The coloring was purportedly done because the dye reduced surface noise. Unlike wax, the hard inflexible material could not be shaved and recorded over, but it had the advantage of being nearly permanent.\n\nSuch \"indestructible\" style cylinders are arguably the most durable form of sound recording produced in the entire era of analog audio media; they can withstand a greater number of playbacks before wearing out than later media such as the vinyl record or audio tape. Their only serious shortcoming is that the celluloid slowly shrinks over the years, so that if it is on a core of plaster, metal or other very unyielding material, the ever-increasing tension can ultimately cause the celluloid to split lengthwise. A typical Lambert cylinder will have shrunk by approximately 3 mm in length in the 100 years or so since its manufacture (the actual amount is very dependent on storage conditions). Thus the grooves will no longer be 100 per inch, and the cylinder will skip if played on a typical feed-screw-type machine. The diameter will also have shrunk and many such cylinders will no longer fit on the mandrel unless very carefully reamed to fit. Such cylinders can still be played quite satisfactorily on suitable modern equipment. The Lambert company was put out of business in 1906 due to repeated actions from Edison for patent infringement, which Lambert had not actually committed - it was the cost of defending the actions that eventually sank Lambert.\n\nThis superior technology was licensed by a few companies including the Indestructible Record Company in 1906 and Columbia Phonograph Company in 1908. The Edison Bell company in Europe had separately licensed the technology and were able to market Edison's titles in both wax (popular series) and celluloid (indestructible series). Lambert was able to license the process because the patent was not owned by the now defunct Lambert Company, but by Lambert himself.\n\nEdison had introduced wax cylinders that played for nominally 4 minutes (instead of the usual 2) in 1909 under the \"Amberol\" brand. These were made from a harder (and more easily breakable) form of wax to withstand the smaller stylus used to play them. The longer playing time was achieved by shrinking the groove size and spacing them twice as close together. In 1912, the Edison company eventually acquired Lambert's patents to the celluloid technology, and almost immediately started production under a variation of their existing \"Amberol\" brand as \"Edison Blue Amberol Records\". These new celluloid recordings were given a core made from plaster of Paris. The celluloid material itself was blue in color, but purple was introduced in 1919, \"... for more sophisticated selections\". The use of the plaster core provided resistance to the shrinkage of the celluloid, but the playing surface is still liable to split if stored in less than ideal conditions. The plaster core itself can deteriorate in conditions that are too damp or too dry. Nevertheless, most Blue Amberol cylinders are, today, quite playable on antique phonographs or modern equipment alike (although the plaster core may need some reaming).\n\nEdison made several designs of phonographs both with internal and external horns for playing these improved cylinder records. The internal horn models were called \"Amberolas\". Edison marketed its \"Fireside\" model phonograph with a gearshift and a 'model K' reproducer with two styli that allowed it to play both 2-minute and 4-minute cylinders. Conversion kits were produced for some of the later model 2 minute phonographs adding a gear change and a second 'model H' reproducer. These kits were shipped with a set of 12 (wax) Amberol cylinders in distinctive orange boxes. The purchaser had no choice as to the titles.\n\nIn the era before World War I, phonograph cylinders and disc records competed with each other for public favor.\n\nThe audio fidelity of a sound groove is debatably better if it is engraved on a cylinder due to better linear tracking. This was not resolved until the advent of RIAA standards in the early 1940s—by which time it had already been rendered academic, as cylinder production stopped with Edison's last efforts in October 1929.\n\nThe cylinder system had certain advantages. As noted, wax cylinders could be used for home recordings, and \"indestructible\" types could be played over and over many more times than the disc. Cylinders usually rotated twice as fast as contemporary discs, but the linear velocity was comparable to the innermost grooves of the disc. In theory, this would provide generally poorer audio fidelity. Furthermore, since constant angular velocity translates into constant linear velocity (the radius of the helical track is constant), cylinders were also free from inner groove problems suffered by disc recordings. Around 1900, cylinders were, on average, indeed of notably higher audio quality than contemporary discs, but as disc makers improved their technology by 1910 the fidelity differences between better discs and cylinders became minimal.\n\nCylinder phonographs generally used a worm gear to move the stylus in synchronization with the grooves of the recording, whereas most disc machines relied on the grooves to pull the stylus along. This resulted in cylinder records played a number of times having less degradation than discs, but this added mechanism made cylinder machines more expensive.\n\nBoth the disc records, and the machines to play them, were cheaper to mass-produce than the products of the cylinder system. Disc records were also easier and cheaper to store in bulk, as they could be stacked, or when in paper sleeves put in rows on shelves like books—packed together more densely than cylinder recordings.\n\nMany cylinder phonographs used a belt to turn the mandrel; slight slippage of this belt could make the mandrel turn unevenly, thus resulting in pitch fluctuations. Disc phonographs using a direct system of gears turned more evenly; the heavy metal turntable of disc machines acted as a flywheel, helping to minimize speed wobble.\n\nVirtually all US disc records were single-sided until 1908, when Columbia Records began mass production of discs with recordings pressed on both sides. Except for premium-priced classical records, that quickly became the industry standard. With their capacity effectively doubled, the storage efficiency advantage of discs over the space-wasting cylinder format became even more obvious.\n\nThe disc companies had superior advertising and promotion, most notably the Victor Talking Machine Company in the United States and the Gramophone Company/HMV in the Commonwealth. Great singers like Enrico Caruso were hired to record exclusively, helping put the idea in the public mind that that company's product was superior. Edison tried to get into the disc market with hill-and-dale discs, Edison Disc Records.\n\nCylinder records continued to compete with the growing disc record market into the 1910s, when discs won the commercial battle. In 1912, Columbia Records, which had been selling both discs and cylinders, dropped the cylinder format, while Edison introduced his unique Diamond Disc format. Beginning in 1915, new Edison cylinder issues were simply dubs of Edison discs and therefore had lower audio quality than the disc originals. Although his cylinders continued to be sold in steadily dwindling and eventually minuscule quantities, Edison continued to support owners of cylinder phonographs by making new titles available in that format until the company ceased manufacturing all records and phonographs in November 1929.\n\nCylinder phonograph technology continued to be used for Dictaphone and Ediphone recordings for office use for decades.\n\nIn 1947, Dictaphone replaced wax cylinders with their Dictabelt technology, which cut a mechanical groove into a plastic belt instead of into a wax cylinder. This was later replaced by magnetic tape recording. However, cylinders for older style dictating machines continued to be available for some years, and it was not unusual to encounter cylinder dictating machines into the 1950s.\n\nIn the late 20th and early 21st century some new recordings have been made on cylinders for the novelty effect of using obsolete technology. Probably the most famous of these are by They Might Be Giants, who in 1996 recorded \"I Can Hear You\" and three other songs, performed without electricity, on an 1898 Edison wax recording studio phonograph at the Edison National Historic Site in West Orange, New Jersey. This song was released on \"Factory Showroom\" in 1996 and re-released on the 2002 compilation \"\". The other songs recorded were \"James K. Polk\", \"Maybe I Know\", and \"The Edison Museum\", the last a song about the site of the recording. These recordings were officially released online as MP3 files in 2001.\n\nSmall numbers of cylinders have been manufactured in the 21st century out of modern long-lasting materials. Two companies engaged in such enterprise are the Vulcan Cylinder Record Company of Sheffield, England and the Wizard Cylinder Records Company in Baldwin, New York. Both appear to have started in 2002.\n\nIn 2010 the British steampunk band The Men That Will Not Be Blamed For Nothing released the track \"Sewer\", from their debut album, \"Now That's What I Call Steampunk! Volume 1\" on a wax cylinder in a limited edition of 40, of which only 30 were put on sale. The box set came with instructions on how to make your own cylinder player for less than £20. The BBC covered the release on Television on BBC Click, on BBC Online and on Radio 5 Live.\n\nIn August 2010, Ash International and PARC released the first commercially available glow in the dark phonograph cylinder, a work by Michael Esposito and Carl Michael von Hausswolff, entitled \"The Ghosts Of Effingham\". The cylinder was released in a limited edition of 150 copies, and was produced by Vulcan Records in Sheffield, England.\n\nBecause of the nature of the recording medium, playback of many cylinders can cause degradation of the recording. The replay of cylinders diminishes their fidelity and degrades their recorded signals. Additionally, when exposed to humidity, mold can penetrate cylinders' surface and cause the recordings to have surface noise. Currently, the only professional machine manufactured for the playback of cylinder recordings is the Archéophone player, designed by Henri Chamoux. The Archéophone is used by the Edison National Historic Site, Bowling Green State University (Bowling Green, Ohio), The Department of Special Collections, Donald C Davidson Library at The University of California, Santa Barbara, and many other libraries and archives.\n\nOther modern so-called 'plug-in' mounts, each incorporating the use of a Stanton 500AL MK II magnetic cartridge, have been manufactured from time to time. Information on each may be viewed on the Phonograph Makers Pages link. It is possible to use these on the Edison cylinder players.\n\nAlso of interest is the cylinder player built by BBC engineers working in \"Engineering Operations - Radio\" in 1987. This was equipped with a linear-tracking arm borrowed from a contemporary Revox turntable, and a variety of re-tipped Shure SC35 cartridges.\n\nIn an attempt to preserve the historic content of the recordings, cylinders can be read with a confocal microscope and converted to a digital audio format. The resulting sound clip in most cases sounds better than stylus playback from the original cylinder. Having an electronic version of the original recordings enables archivists to open access to the recordings to a wider audience. This technique also has the potential to allow for reconstruction of damaged or broken cylinders.\n\nModern reproductions of cylinder and disc recordings usually give the impression that the introduction of discs was a quantum leap in audio fidelity, but this is on modern playback equipment; played on equipment from around 1900, the cylinders do not have noticeably more rumble and poorer bass reproduction than the discs. Another factor is that many cylinders are amateur recordings, while disc recording equipment was simply too expensive for anyone but professional engineers; many extremely poor recordings were made on cylinder, while the vast majority of disc recordings were competently recorded. All cylinder recordings were acoustically recorded as were early disc recordings. From the mid-1920s onwards, discs started to be recorded electrically which provided a much enhanced frequency range of recording.\n\nAlso important is the quality of the material: the earliest tinfoil recordings wore out fast. Once the tinfoil was removed from the cylinder it was nearly impossible to re-align in playable condition. The earliest soft wax recordings also wore out quite fast, although they have better fidelity than the early rubber discs.\n\nIn addition to poor states of preservation, the poor impression modern listeners may get of wax cylinders is from their early date, which can compare unfavorably to recordings made even a dozen years later. Other than a single playable example from 1878 (from an experimental phonograph-clock), the oldest playable preserved cylinders are from the year 1888. These include a severely degraded recording of Johannes Brahms, Handel's Israel in Egypt and a short speech by Sir Arthur Sullivan in fairly listenable condition. Somewhat later are the almost unlistenable 1889 amateur recordings of Nina Grieg. The problem with the wax cylinders is that they readily support the growth of mildew which penetrates throughout the cylinder and, if serious enough, renders the recording unplayable. The earliest preserved rubber disc recordings are children's records, featuring animal noises and nursery rhymes. This means that the earliest disc recordings most music lovers will hear are shellac discs made after 1900, after more than ten years of development.\n\n\n\n\n\n"}
{"id": "501281", "url": "https://en.wikipedia.org/wiki?curid=501281", "title": "Post-industrial society", "text": "Post-industrial society\n\nIn sociology, the post-industrial society is the stage of society's development when the service sector generates more wealth than the manufacturing sector of the economy.\n\nThe term was originated by Alain Touraine and is closely related to similar sociological theoretical constructs such as post-Fordism, information society, knowledge economy, post-industrial economy, liquid modernity, and network society. They all can be used in economics or social science disciplines as a general theoretical backdrop in research design.\n\nAs the term has been used, a few common themes, including the ones below have begun to emerge.\n\n\nDaniel Bell popularized the term through his 1974 work \"The Coming of Post-Industrial Society\". Although some have credited Bell with coining the term, French sociologist Alain Touraine published in 1969 the first major work on the post-industrial society. The term was also used extensively by social philosopher Ivan Illich in his 1973 paper \"Tools for Conviviality\" and appears occasionally in Leftist texts throughout the mid-to-late 1960s.\n\nThe term has grown and changed as it became mainstream. The term is now used by admen such as Seth Godin, public policy PhD's such as Keith Boeckelman, and sociologists such as Neil Fligstein and Ofer Sharone. Former U.S. President Bill Clinton even used the term to describe Chinese growth in a round-table discussion in Shanghai in 1998.\n\nThe post-industrialized society is marked by an increased valuation of knowledge. This itself is unsurprising, having been foreshadowed in Daniel Bell's presumption as to how economic employment patterns will evolve in such societies. He asserts employment will grow faster in the tertiary (and quaternary) sector relative to employment in the primary and secondary sector and that the tertiary (and quaternary) sectors will take precedence in the economy. This will continue to occur such that the“impact of the expert”will expand and power will be monopolized by knowledge.\n\nAs tertiary and quaternary sector positions are essentially knowledge-oriented, this will result in a restructuring of education, at least in its nuances. The“new power… of the expert”consequently gives rise to the growing role of universities and research institutes in post-industrial societies. Post-industrial societies themselves become oriented around these places of knowledge production and production of experts as their new foci. Consequently, the greatest beneficiaries in the post-industrial society are young urban professionals. As a new, educated, and politicized generation more impassioned by liberalism, social justice, and environmentalism the shift of power into their hands, as a result of their knowledge endowments, is often cited as a good thing.\n\nThe increasing importance of knowledge in post-industrial societies results in a general increase in expertise through the economy and throughout society. In this manner, it eliminates what Alan Banks and Jim Foster identify as “undesirable work as well as the grosser forms of poverty and inequality.” This effect is supplemented by the aforementioned movement of power into the hands of young educated people concerned with social justice.\n\nEconomists at Berkeley have studied the value of knowledge as a form of capital, adding value to material capital, such as factory or a truck. Speaking along the same lines of their argument, the addition or 'production' of knowledge, could become the basis of what would undoubtably be considered 'post-industrial' policies meant to deliver economic growth.\n\nSimilarly, post-industrial society has serviced the creative culture. Many of those most well-equipped to thrive in an increasingly technological society are young adults with tertiary education. As education itself becomes more and more oriented towards producing people capable of answering the need for self-actualization, creativity, and self-expression, successive generations become more endowed with the ability to contribute to and perpetuate such industries. This nuanced change in education, as well among the emerging class of young professionals, is itself initiated by what James D Wright identifies as an “unprecedented economic affluence and the satiation of basic material needs.” Ellen Dunham-Jones as well observes this feature of post-industrial society where “abundant goods [are] equitably distributed [in order that] laborless leisure and self-determination” can be consumed.\n\nThe post-industrial society is repeatedly stressed to be one where knowledge is power and technology is the instrument. Naturally, where one is creatively inclined, they are advantaged by such a society. The doctrine of “speed, mobility and malleability” is well suited to a dynamic creative industry and as industries of good production decrease in precedence, the way is paved for artists, musicians, and other such types, whose skills are better utilized by the tertiary and quaternary sector. Urban geographer Trevor Barnes, in his work outlining the Vancouver experience in post-war development, evokes the post-industrial condition, citing the emergence and consolidation of a significant video games industry as a constituent of the elite service sector.\n\nThis increased faculty of the post-industrialist society with respects to the creative industry is itself reflected by the economic history of post-industrial societies. As economic activities shift from primarily primary and secondary sector-based to tertiary, and later quaternary, sector-based, cities in which this shift occurs become more open to exchanges of information. This is necessitated by the demands of a tertiary and quaternary sector: in order to better service an industry focused on finance, education, communication, management, training, engineering, and aesthetic design, the city must become points of exchange capable of providing the most updated information from across the globe. Conversely, as cities become a convergence of international ideas, the tertiary and quaternary sector can be expected to grow.\n\nA virtual cult of 'creatives' have sprung up embodying and often describing and defending the post-industrial ethos. They argue that businesses that create intangibles have taken a more prominent role in the wake of manufacturing's decline.\n\nActor and then-artistic director of the Old Vic Theatre, Kevin Spacey, has argued the economic case for the arts in terms of providing jobs and being of greater importance in exports than manufacturing (as well as an educational role) in a guest column he wrote for \"The Times\".\n\nPost-industrialism is criticized for the amount of real fundamental change it produces in society, if any at all. A mild view held by Alan Banks and Jim Foster contends that representations of post-industrial society by advocates assume professional, educated elites were previously less relevant than they have become in the new social order, and that changes that have occurred are minor but greatly embellished. More critical views see the entire process as the highest evolution of capitalism, wherein the system produces commodities as opposed to practical goods and is determined privately instead of socially. This view is complemented by the assertion that “the characteristic feature of a modern [that is, post-industrial] society is that it is a technocracy.” Such societies then become notable for their ability to subvert social consciousness through powers of manipulation rather than powers of coercion, reflective of the “ideology of the ruling class [as] … predominantly managerial.”\n\nIn line with the view that nothing fundamental has changed in the transition from industrial societies to post-industrial societies is the insistence of lingering problems from past development periods. Neo-Malthusian in essence, this outlook focuses on post-industrial society’s continuing struggle with issues of resource scarcity, overpopulation, and environmental degradation, all of which are remnants from its industrial history. This is exacerbated by a “corporate liberalism” that seeks to continue economic growth through “the creation and satisfaction of false needs,” or as Christopher Lasch more derisively refers to it, “subsidized waste.”\n\nUrban development in the context of post-industrialism is also a point of contention. In opposition to the view that the new leaders of post-industrial society are increasingly environmentally aware, this critique asserts that it rather leads to environmental degradation, this being rooted in the patterns of development. Urban sprawl, characterised behaviourally by cities “expanding at the periphery in even lower densities” and physically by “office parks, malls, strips, condo clusters, corporate campuses and gated communities,” is singled out as the main issue. Resulting from a post-industrialist culture of “mobile capital, the service economy, post-Fordist disposable consumerism and banking deregulation,” urban sprawl has caused post-industrialism to become environmentally and socially regressive. Of the former, environmental degradation results from encroachment as cities meet demands on low-density habitation; the wider spread of population consumes more of the environment while necessitating more energy consumption in order to facilitate travel within the ever-growing city, incurring greater pollution. This process evokes the neo-Malthusian concerns of overpopulation and resource scarcity that inevitably lead to environmental deterioration. Of the latter, “post-industrialism’s doctrine of … mobility and malleability” encourage a disconnect between communities where social belonging falls into the category of things considered by the “post-Fordist disposable consumer[ist]” attitude as interchangeable, expendable, and replaceable.\n\nPost-industrialism as a concept is highly Western-centric. Theoretically and effectively, it is only possible in the Global West, which its proponents assume to be solely capable of fully realizing industrialization and then post-industrialization. Herman Kahn optimistically predicted the “economic growth, expanded production and growing efficiency” of post-industrial societies and the resultant “material abundance and… high quality of life” to extend to “almost all people in Western societies” and only “some in Eastern societies.” This prediction is treated elsewhere by contentions that the post-industrial society merely perpetuates capitalism.\n\nRecalling the critical assertion that all modern societies are technocracies, T. Roszak completes the analysis by stating that “all societies are moving in the direction of technocracies.” From this, the foremost “suave technocracies” reside in the West, whereas all others are successively graded in descending order: “vulgar technocracies,” “teratoid technocracies,” and finally “comic opera technocracies.” This view importantly presumes one transition and furthermore one path of transition for societies to undergo, i.e. the one that Western societies are slated to complete. Much like the demographic transition model, this prediction does not entertain the idea of an Eastern or other alternative model of transitional development.\n\nWhen historians and sociologists considered the revolution that followed the agricultural society they did not call it a \"post-agricultural society\". \"Post-industrial society\" signifies only a departure, not a positive description.\n\nOne of the word's early users, Ivan Illich, prefigured this criticism and invented the term Conviviality, or the Convivial Society, to stand as a positive description of his version of a post-industrial society.\n\nA group of scholars (including Allen Scott and Edward Soja) argue that industry remains at the center of the whole process of capitalist accumulation, with services not only becoming increasingly industrialized and automated but also remaining highly dependent on industrial growth.\n\nSome observers, including Soja (building on the theories of the French philosopher of urbanism Henri Lefebvre), suggest that although industry may be based outside of a \"post-industrial\" nation, that nation cannot ignore industry's necessary sociological importance.\n\n"}
{"id": "4746766", "url": "https://en.wikipedia.org/wiki?curid=4746766", "title": "Process", "text": "Process\n\nA process is a set of activities that interact to produce a result.\n\nThings called a process include:\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "47007962", "url": "https://en.wikipedia.org/wiki?curid=47007962", "title": "Push Interactions", "text": "Push Interactions\n\nPush Interactions is an award-winning Canadian provider of Mobile App Development services, design and analysis. Established in 2009 the company is known for providing mobile apps for financial, retail, transportation, education and the medical industries. Formerly known as CollegeMobile the company is based in Saskatoon, Saskatchewan, Canada and was founded in 2009. The company has created mobile apps in use by over 500,000 users worldwide. Founders of the company were also included in the iUsask project at the University of Saskatchewan. Company has been named to Canada's top 25 Canadian Up and Coming ICT Companies among other awards.\n\n"}
{"id": "58488367", "url": "https://en.wikipedia.org/wiki?curid=58488367", "title": "Slice (app)", "text": "Slice (app)\n\nSlice is an online food ordering platform for independent pizzerias. It allows local pizzeria owners to compete with large pizza chains through a mobile-optimized website and tools that enable customers to place orders through the Slice app and social media channels. The platform is used by over 9,000 independent pizzerias in more than 2,500 towns and cities across all 50 states. Slice processed more than $100 million worth of deliveries in 2017 and over 12 million orders since 2010.\n\nSlice was founded in 2010 by Ilir Sela, a third-generation family member in the pizza industry, and was originally called MyPizza. In July 2016, Slice closed on a $3 million Series A funding round. In May 2017, the company raised $15 million led by GGV Capital.\n\n"}
{"id": "266344", "url": "https://en.wikipedia.org/wiki?curid=266344", "title": "Space debris", "text": "Space debris\n\nInitially, the term space debris referred to the natural debris found in the solar system: asteroids, comets, and meteoroids. However, with the 1979 beginning of the NASA Orbital Debris Program, the term also refers to the debris (alt. space waste or space garbage) from the mass of defunct, artificially created objects in space, especially Earth orbit. These include old satellites and spent rocket stages, as well as the fragments from their disintegration and collisions. \n\nAs of December 2016, five satellite collisions have generated space debris. Space debris is also known as \"orbital debris\", \"space junk\", \"space waste\", \"space trash\", \"space litter\" or \"space garbage\".\n, the United States Strategic Command tracked a total of 17,852 artificial objects in orbit above the Earth, including 1,419 operational satellites. However, these are just objects large enough to be tracked. , more than 170 million bits of debris smaller than , about 670,000 pieces of debris 1–10 cm, and around 29,000 larger pieces were estimated to be in orbit around the earth. Collisions with debris have become a hazard to spacecraft; they cause damage akin to sandblasting, especially to solar panels and optics like telescopes or star trackers that cannot be covered with a ballistic Whipple shield (unless it is transparent).\n\nBelow Earth-altitude, pieces of debris are denser than meteoroids; most are dust from solid rocket motors, surface erosion debris like paint flakes, and frozen coolant from RORSAT (nuclear-powered satellites). \nFor comparison, the International Space Station orbits in the range, and the 2009 satellite collision and 2007 antisat test occurred at altitude. The ISS has Whipple shielding; however, known debris with a collision chance over 1/10,000 are avoided by maneuvering the station.\n\nThe Kessler syndrome, a runaway chain reaction of collisions exponentially increasing the amount of debris, has been hypothesized to ensue beyond a critical density. This could affect useful polar-orbiting bands, increases the cost of protection for spacecraft missions and could destroy live satellites. Whether Kessler syndrome is already underway has been debated. The measurement, mitigation, and potential removal of debris are conducted by some participants in the space industry.\n\nThere are estimated to be over 170 million pieces of debris smaller than as of July 2013. There are approximately 670,000 pieces from one to ten cm. The current count of large debris (defined as 10 cm across or larger) is 29,000. The technical measurement cutoff is c. . Over 98 percent of the 1,900 tons of debris in low Earth orbit (as of 2002) was accounted for by about 1,500 objects, each over . Total mass is mostly constant despite addition of many smaller objects, since they reenter the atmosphere sooner. Using a 2008 figure of 8,500 known items, it is estimated at .\n\nIn LEO there are few \"universal orbits\" which keep spacecraft in particular rings (in contrast to GEO, a single widely used orbit). The closest are sun-synchronous orbits that keep a constant angle between the Sun and the orbital plane; they are polar, meaning they cross over the polar regions. LEO satellites orbit in many planes, up to 15 times a day, causing frequent approaches between objects (the density of objects is much higher in LEO).\n\nOrbits are further changed by perturbations (which in LEO include unevenness of the Earth's gravitational field), and collisions can occur from any direction. For these reasons, the Kessler syndrome applies mostly to the LEO region; impacts occur at up to 16 km/s (twice the orbital speed) if head-on – the 2009 satellite collision occurred at 11.7 km/s, creating much spall in the critical size range. These can cross other orbits and lead to a cascade effect. A large-enough collision (e.g. between a space station and a defunct satellite) could make low Earth orbit impassable.\n\nManned missions are mostly at and below, where air drag helps clear zones of fragments. Atmospheric expansion as a result of space weather raises the critical altitude by increasing drag; in the 90s, it was a factor in reduced debris density. Another was fewer launches by Russia; the USSR made most of their launches in the 1970s and 1980s.\n\nAt higher altitudes, where air drag is less significant, orbital decay takes longer. Slight atmospheric drag, lunar perturbations, Earth's gravity perturbations, solar wind and solar radiation pressure can gradually bring debris down to lower altitudes (where it decays), but at very high altitudes this may take millennia. Although high-altitude orbits are less commonly used than LEO and the onset of the problem is slower, the numbers progress toward the critical threshold more quickly.\n\nMany communications satellites are in geostationary orbits (GEO), clustering over specific targets and sharing the same orbital path. Although velocities are low between GEO objects, when a satellite becomes derelict (such as Telstar 401) it assumes a geosynchronous orbit; its orbital inclination increases about .8° and its speed increases about per year. Impact velocity peaks at about . Orbital perturbations cause longitude drift of the inoperable spacecraft and precession of the orbital plane. Close approaches (within 50 meters) are estimated at one per year. The collision debris pose less short-term risk than from an LEO collision, but the satellite would likely become inoperable. Large objects, such as solar-power satellites, are especially vulnerable to collisions.\n\nAlthough the ITU now requires proof a satellite can be moved out of its orbital slot at the end of its lifespan, studies suggest this is insufficient. Since GEO orbit is too distant to accurately measure objects under , the nature of the problem is not well known. Satellites could be moved to empty spots in GEO, requiring less maneuvring and making it easier to predict future motion. Satellites or boosters in other orbits, especially stranded in geostationary transfer orbit, are an additional concern due to their typically high crossing velocity.\n\nDespite efforts to reduce risk, spacecraft collisions have occurred. The European Space Agency telecom satellite Olympus-1 was struck by a meteoroid on 11 August 1993 and eventually moved to a graveyard orbit. On 29 March 2006, the Russian Express-AM11 communications satellite was struck by an unknown object and rendered inoperable; its engineers had enough contact time with the satellite to send it into a graveyard orbit.\n\nIn 1958, the United States launched Vanguard I into a medium Earth orbit (MEO). , it, and the upper stage of its launch rocket, are the oldest surviving man-made space objects still in orbit. In a catalog of known launches until July 2009, the Union of Concerned Scientists listed 902 operational satellites from a known population of 19,000 large objects and about 30,000 objects launched.\n\nAn example of additional dead satellite debris are the remains of the 1970s/80s Soviet RORSAT naval surveillance satellite program. The satellite's BES-5 nuclear reactor were cooled with a coolant loop of sodium-potassium alloy, creating a potential problem when the satellite reached end of life. While many satellites were nominally boosted into medium-altitude graveyard orbits, not all were. Even satellites which had been properly moved to a higher orbit had an eight-percent probability of puncture and coolant release over a 50-year period. The coolant freezes into droplets of solid sodium-potassium alloy, forming additional debris.\n\nThese events continue to occur. For example, in February 2015, the USAF Defense Meteorological Satellite Program Flight 13 (DMSP-F13) exploded on orbit, creating at least 149 debris objects, which were expected to remain in orbit for decades.\n\nAccording to Edward Tufte's book \"Envisioning Information\", space debris includes a glove lost by astronaut Ed White on the first American space-walk (EVA); a camera lost by Michael Collins near Gemini 10; a thermal blanket lost during STS-88; garbage bags jettisoned by Soviet cosmonauts during Mir's 15-year life, a wrench and a toothbrush. Sunita Williams of STS-116 lost a camera during an EVA. During an STS-120 EVA to reinforce a torn solar panel, a pair of pliers was lost, and in an STS-126 EVA, Heidemarie Stefanyshyn-Piper lost a briefcase-sized tool bag.\n\nIn characterizing the problem of space debris, it was learned that much debris was due to rocket upper stages (e.g. the Inertial Upper Stage) which end up in orbit, and break up due to decomposition of unvented unburned fuel. However, a major known impact event involved an (intact) Ariane booster. Although NASA and the United States Air Force now require upper-stage passivation, other launchers do not.\nLower stages, like the Space Shuttle's solid rocket boosters or Apollo program's Saturn IB launch vehicles, do not reach orbit.\n\nOn 11 March 2000 a Chinese Long March 4 CBERS-1 upper stage exploded in orbit, creating a debris cloud.\nA Russian Briz-M booster stage exploded in orbit over South Australia on 19 February 2007. Launched on 28 February 2006 carrying an Arabsat-4A communications satellite, it malfunctioned before it could use up its propellant. Although the explosion was captured on film by astronomers, due to the orbit path the debris cloud has been difficult to measure with radar. By 21 February 2007, over 1,000 fragments were identified. A 14 February 2007 breakup was recorded by Celestrak. Eight breakups occurred in 2006, the most since 1993. Another Briz-M broke up on 16 October 2012 after a failed 6 August Proton-M launch. The amount and size of the debris was unknown. A Long March 7 rocket booster created a fireball visible from portions of Utah, Nevada, Colorado, Idaho and California on the evening of 27 July 2016; its disintegration was widely reported on social media.\n\nA past debris source was the testing of anti-satellite weapons (ASATs) by the U.S. and Soviet Union during the 1960s and 1970s. North American Aerospace Defense Command (NORAD) files only contained data for Soviet tests, and debris from U.S. tests were only identified later. By the time the debris problem was understood, widespread ASAT testing had ended; the U.S. Program 437 was shut down in 1975.\n\nThe U.S. restarted their ASAT programs in the 1980s with the Vought ASM-135 ASAT. A 1985 test destroyed a satellite orbiting at , creating thousands of debris larger than . Due to the altitude, atmospheric drag decayed the orbit of most debris within a decade. A \"de facto\" moratorium followed the test.\n\nChina's government was condemned for the military implications and the amount of debris from the 2007 anti-satellite missile test, the largest single space debris incident in history (creating over 2,300 pieces golf-ball size or larger, over 35,000 or larger, and one million pieces or larger). The target satellite orbited between and , the portion of near-Earth space most densely populated with satellites. Since atmospheric drag is low at that altitude the debris is slow to return to Earth, and in June 2007 NASA's Terra environmental spacecraft maneuvered to avoid impact from the debris.\n\nOn 20 February 2008, the U.S. launched an SM-3 missile from the USS \"Lake Erie\" to destroy a defective U.S. spy satellite thought to be carrying of toxic hydrazine propellant. The event occurred at about , and the resulting debris has a perigee of or lower. The missile was aimed to minimize the amount of debris, which (according to Pentagon Strategic Command chief Kevin Chilton) had decayed by early 2009.\n\nThe vulnerability of satellites to debris and the possibility of attacking LEO satellites to create debris clouds, has triggered speculation that it is possible for countries unable to make a precision attack. An attack on a satellite of 10 tonnes or more would heavily damage the LEO environment.\n\nSpace junk is a threat to active satellites and spaceships. The Earth's orbit may even become impassable as the risk of collision grows too high.\n\nAlthough spacecraft are protected by Whipple shields, solar panels, which are exposed to the Sun, wear from low-mass impacts. These produce a cloud of plasma which is an electrical risk to the panels.\n\nSatellites are believed to have been destroyed by micrometeorites and orbital debris (MMOD). The earliest suspected loss was of Kosmos 1275, which disappeared on 24 July 1981 (a month after launch). Kosmos contained no volatile propellant, therefore, there appeared to be nothing internal to the satellite which could have caused the destructive explosion which took place. However the case has not been proven and another hypothesis forwarded is that the battery exploded. Tracking showed it broke up, into 300 new objects.\n\nMany impacts have been confirmed since. Olympus-1 was struck by a meteoroid on 11 August 1993, and left adrift. On 24 July 1996, the French microsatellite Cerise was hit by fragments of an Ariane-1 H-10 upper-stage booster which exploded in November 1986. On 29 March 2006, the Russian Ekspress AM11 communications satellite was struck by an unknown object and rendered inoperable; its engineers had sufficient time in contact with the spacecraft to send it to a parking orbit out of GEO. On October 13, 2009, Terra suffered a single battery cell failure anomaly and a battery heater control anomaly which were likely the result of an MMOD strike. On March 12, 2010, Aura lost power from one-half of one of its 11 solar panels and this was also attributed to an MMOD strike. On May 22, 2013 GOES-13 was hit by an MMOD which caused it to lose track of the stars that it uses to maintain attitude. It took nearly a month for the spacecraft to return to operation.\n\nThe first major satellite collision occurred on 10 February 2009 at 16:56 UTC. The deactivated Kosmos 2251 and the operational Iridium 33 collided, over northern Siberia. The relative speed of impact was about , or about . Both satellites were destroyed, with accurate estimates of the number of debris unavailable. On 22 January 2013 BLITS (a Russian laser-ranging satellite) was struck by debris suspected to be from the 2007 Chinese anti-satellite missile test, changing its orbit and spin rate.\n\nSatellites frequently have to perform Collision Avoidance Maneuvers and managers have to monitor debris as part of maneuver planning. For example, in January 2017, the European Space Agency planned to alter orbit of one of its $319 million Swarm mission spacecrafts, based on data from the US Joint Space Operations Center, to end the risk of collision from Cosmos-375, an old Russian satellite. Cosmos-375, which was destroyed by Soviet operators when its mission was complete, had previously threatened to impact the International Space Station in 2011.\n\nFrom the early Space Shuttle missions, NASA used NORAD to monitor the Shuttle's orbital path for debris. In the 1980s, this used much of its capacity. The first collision-avoidance maneuver occurred during STS-48 in September 1991, a seven-second thruster burn to avoid debris from Kosmos 955. Similar maneuvers followed on missions 53, 72 and 82.\n\nOne of the first events to publicize the debris problem occurred on \"Challenger\"'s second flight, STS-7. A fleck of paint struck its front window, creating a pit over wide. On STS-59 in 1994, \"Endeavour\"'s front window was pitted about half its depth. Minor debris impacts increased from 1998.\n\nWindow chipping and minor damage to thermal protection system tiles (TPS) was already common by the 1990s. The Shuttle was later flown tail-first to take the debris load mostly on the engines and rear cargo bay (not used in orbit or during descent, and less critical for post-launch operation). When flying to the ISS, the two connected spacecraft were flipped around so the better-armored station shielded the orbiter.\nNASA's study concluded that debris accounted for half of the overall risk to the Shuttle. Executive-level decision to proceed was required if catastrophic impact was likelier than 1 in 200. On a normal (low-orbit) mission to the ISS the risk was c. 1 in 300, but STS-125 (the Hubble repair mission) at was initially calculated at a 1-in-185 risk (due to the 2009 satellite collision). A re-analysis with better debris numbers reduced the estimated risk to 1 in 221, and the mission went ahead.\n\nDebris incidents continued on later Shuttle missions. During STS-115 in 2006 a fragment of circuit board bored a small hole through the radiator panels in \"Atlantis\"' cargo bay. On STS-118 in 2007 debris blew a bullet-like hole through \"Endeavour\"s radiator panel.\n\nImpact wear was notable on Mir, the Soviet space station, since it remained in space for long periods with its original module panels.\n\nAlthough the ISS uses Whipple shielding to protect itself from minor debris, portions (notably its solar panels) cannot be protected easily. In 1989, the ISS panels were predicted to degrade c. 0.23% in four years, and they were overdesigned by 1%. A maneuver is performed if \"there is a greater than one-in-10,000 chance of a debris strike\". , there have been sixteen maneuvers in the fifteen years the ISS had been in orbit.\n\nThe crew sheltered in the Soyuz on three occasions due to late debris-proximity warnings. In addition to the sixteen firings and three Soyuz-capsule shelter orders, one attempted maneuver failed (due to not having the several days' warning necessary to upload the manoeuvre timeline to the station's computer). A March 2009 close call involved debris believed to be a piece of the Kosmos 1275 satellite. In 2013, the ISS did not maneuver to avoid debris, after a record four debris maneuvers the previous year.\n\nAlthough most manned space activity takes place at altitudes below , a Kessler syndrome cascade in that region would rain down into lower altitudes and the decay time scale is such that \"the resulting [low Earth orbit] debris environment is likely to be too hostile for future space use\".\n\nIn a Kessler syndrome, satellite lifetimes would be measured in years or months. New satellites could be launched through the debris field into higher orbits or placed in lower orbits (where decay removes the debris), but the utility of the region between is the reason for its amount of debris.\n\nAlthough most debris burns up in the atmosphere, larger objects can reach the ground intact. According to NASA, an average of one cataloged piece of debris has fallen back to Earth each day for the past 50 years. Despite their size, there has been no significant property damage from the debris.\n\nIn 1969 five sailors on a Japanese ship were injured by space debris. In 1997 an Oklahoma woman, Lottie Williams, was injured when she was hit in the shoulder by a piece of blackened, woven metallic material confirmed as part of the propellant tank of a Delta II rocket which launched a U.S. Air Force satellite the year before.\n\nThe original re-entry plan for Skylab called for the station to remain in space for eight to ten years after its final mission in February 1974. High solar activity expanded the upper atmosphere, resulting in higher-than-expected drag and bringing its orbit closer to Earth than planned. On 11 July 1979 Skylab re-entered the Earth's atmosphere and disintegrated, raining debris along a path over the southern Indian Ocean and Western Australia.\n\nOn 12 January 2001, a Star 48 Payload Assist Module (PAM-D) rocket upper stage re-entered the atmosphere after a \"catastrophic orbital decay\", crashing in the Saudi Arabian desert. It was identified as the upper-stage rocket for NAVSTAR 32, a GPS satellite launched in 1993.\n\nIn the 2003 \"Columbia\" disaster, large parts of the spacecraft reached the ground and entire equipment systems remained intact. More than 83,000 pieces, along with the remains of the six astronauts, were recovered in an area from three to 10 miles around Hemphill in Sabine County, TX. More pieces were found in a line from west Texas to east Louisiana, with the westernmost piece found in Littlefield, TX and the easternmost found southwest of Mora, LA. Although there is significant evidence that debris fell in Nevada, Utah, and New Mexico, debris was only found in Texas, Arkansas and Louisiana. In a rare case of property damage, a foot-long metal bracket smashed through the roof of a dentist office. NASA warned the public to avoid contact with the debris because of the possible presence of hazardous chemicals. 15 years after the failure, people were still sending in pieces with the last,as of February 1, 2018, found in the spring of 2017.\n\nOn 27 March 2007, airborne debris from a Russian spy satellite was seen by the pilot of a LAN Airlines Airbus A340 carrying 270 passengers whilst flying over the Pacific Ocean between Santiago and Auckland. The debris was within of the aircraft.\n\nRadar and optical detectors such as lidar are the main tools for tracking space debris. Although objects under have reduced orbital stability, debris as small as 1 cm can be tracked, however determining orbits to allow re-acquisition is difficult. Most debris remain unobserved. The NASA Orbital Debris Observatory tracked space debris with a liquid mirror transit telescope. FM Radio waves can detect debris, after reflecting off them onto a receiver. Optical tracking may be a useful early-warning system on spacecraft.\n\nThe U.S. Strategic Command keeps a catalog of known orbital objects, using ground-based radar and telescopes, and a space-based telescope (originally to distinguish from hostile missiles). The 2009 edition listed about 19,000 objects. Other data come from the ESA Space Debris Telescope, TIRA, the Goldstone, Haystack, and EISCAT radars and the Cobra Dane phased array radar, to be used in debris-environment models like the ESA Meteoroid and Space Debris Terrestrial Environment Reference (MASTER).\n\nReturned space hardware is a valuable source of information on the directional distribution and composition of the (sub-millimetre) debris flux. The LDEF satellite deployed by mission STS-41-C \"Challenger\" and retrieved by STS-32 \"Columbia\" spent 68 months in orbit to gather debris data. The EURECA satellite, deployed by STS-46 \"Atlantis\" in 1992 and retrieved by STS-57 \"Endeavour\" in 1993, was also used for debris study.\n\nThe solar arrays of Hubble were returned by missions STS-61 \"Endeavour\" and STS-109 \"Columbia\", and the impact craters studied by the ESA to validate its models. Materials returned from Mir were also studied, notably the Mir Environmental Effects Payload (which also tested materials intended for the ISS).\n\nA debris cloud resulting from a single event is studied with scatter plots known as Gabbard diagrams, where the perigee and apogee of fragments are plotted with respect to their orbital period. Gabbard diagrams of the early debris cloud prior to the effects of perturbations, if the data were available, are reconstructed. They often include data on newly observed, as yet uncatalogued fragments. Gabbard diagrams can provide important insights into the features of the fragmentation, the direction and point of impact.\n\nAn average of about one tracked object per day has been dropping out of orbit for the past 50 years, averaging almost three objects per day at solar maximum (due to the heating and expansion of the Earth's atmosphere), but one about every three days at solar minimum, usually 5½ yr later. In addition to natural atmospheric effects, corporations, academics and government agencies have proposed plans and technology to deal with space debris, but , most of these are theoretical, and there is no extant business plan for debris reduction.\n\nA number of scholars have also observed that institutional factors—political, legal, economic and cultural \"rules of the game\"—are the greatest impediment to the cleanup of near-Earth space. There is no commercial incentive, since costs aren't assigned to polluters, but a number of suggestions have been made. However, effects to date are limited. In the US, governmental bodies have been accused of backsliding on previous commitments to limit debris growth, \"let alone tackling the more complex issues of removing orbital debris.\"\n\nUpper stage passivation (e.g. of Delta boosters) by releasing residual propellants reduces debris from orbital explosions; however not all boosters implement this. Although there is no international treaty minimizing space debris, the United Nations Committee on the Peaceful Uses of Outer Space (COPUOS) published voluntary guidelines in 2007. As of 2008, the committee is discussing international \"rules of the road\" to prevent collisions between satellites.\nBy 2013, various legal regimes existed, typically instantiated in the launch licenses that are required for a launch in all spacefaring nations.\n\nThe U.S. has a set of standard practices for civilian (NASA) and military (DoD and USAF) orbital-debris mitigation, as has the European Space Agency. In 2007, the ISO began preparing an international standard for space-debris mitigation. Germany and France have posted bonds to safeguard property from debris damage.\n\nWhen originally proposed in 2015, the OneWeb constellation, initially planned to have ~700 satellites anticipated on orbit after 2018, would only state that they would re-enter the atmosphere within 25 years of retirement.\nBy October 2017, both OneWeb—and also SpaceX, with their large Starlink constellation—had filed documents with the US FCC with more aggressive space debris mitigation plans. Both companies committed to a deorbit plan for post-mission satellites which will explicitly move the satellites into orbits where they will reenter the Earth's atmosphere within approximately one year following end-of-life.\n\nWith a \"one-up, one-down\" launch-license policy for Earth orbits, launchers would rendezvous with, capture and de-orbit a derelict satellite from approximately the same orbital plane. Another possibility is the robotic refueling of satellites. Experiments have been flown by NASA, and SpaceX is developing large-scale on-orbit propellant transfer technology and tanker spacecraft.\n\nAnother approach to debris mitigation is to explicitly design the mission architecture to always leave the rocket second-stage in an elliptical geocentric orbit with a low-perigee, thus ensuring rapid orbital decay and avoiding long-term orbital debris from spent rocket bodies. Such missions require the use of a small kick stage to circularize the orbit, but the kick stage itself may be designed with the excess-propellant capability to be able to self-deorbit.\n\nAlthough the ITU requires geostationary satellites to move to a graveyard orbit at the end of their lives, the selected orbital areas do not sufficiently protect GEO lanes from debris. Rocket stages (or satellites) with enough propellant may make a direct, controlled de-orbit, or if this would require too much propellant, a satellite may be brought to an orbit where atmospheric drag would cause it to eventually de-orbit. This was done with the French Spot-1 satellite, reducing its atmospheric re-entry time from a projected 200 years to about 15 by lowering its altitude from to about .\n\nPassive methods of increasing the orbital decay rate of spacecraft debris have been proposed. Instead of rockets, an electrodynamic tether could be attached to a spacecraft at launch; at the end of its lifetime, the tether would be rolled out to slow the spacecraft. Other proposals include a booster stage with a sail-like attachment and a large, thin, inflatable balloon envelope.\n\nA well-studied solution uses a remotely controlled vehicle to rendezvous with, capture and return debris to a central station.\nOne such system is Space Infrastructure Servicing, a commercially developed refueling depot and service spacecraft for communications satellites in geosynchronous orbit originally scheduled for a 2015 launch. The SIS would be able to \"push dead satellites into graveyard orbits.\" The Advanced Common Evolved Stage family of upper stages is being designed with a high leftover-propellant margin (for derelict capture and de-orbit) and in-space refueling capability for the high delta-v required to de-orbit heavy objects from geosynchronous orbit. A tug-like satellite to drag debris to a safe altitude for it to burn up in the atmosphere has been researched. When debris is identified the satellite creates a difference in potential between the debris and itself, then using its thrusters to move itself and the debris to a safer orbit.\n\nA variation of this approach is for the remotely controlled vehicle to rendezvous with debris, capture it temporarily to attach a smaller de-orbit satellite and drag the debris with a tether to the desired location. The \"mothership\" would then tow the debris-smallsat combination for atmospheric entry or move it to a graveyard orbit. One such system is the proposed Busek ORbital DEbris Remover (ORDER), which would carry over 40 SUL (satellite on umbilical line) de-orbit satellites and propellant sufficient for their removal.\n\nThe laser broom uses a ground-based laser to ablate the front of the debris, producing a rocket-like thrust which slows the object. With continued application, the debris would fall enough to be influenced by atmospheric drag. During the late 1990s, the U.S. Air Force's Project Orion was a laser-broom design. Although a test-bed device was scheduled to launch on a Space Shuttle in 2003, international agreements banning powerful laser testing in orbit limited its use to measurements. The Space Shuttle \"Columbia\" disaster postponed the project and according to Nicholas Johnson, chief scientist and program manager for NASA's Orbital Debris Program Office, \"There are lots of little gotchas in the Orion final report. There's a reason why it's been sitting on the shelf for more than a decade.\"\n\nThe momentum of the laser-beam photons could directly impart a thrust on the debris sufficient to move small debris into new orbits out of the way of working satellites. NASA research in 2011 indicates that firing a laser beam at a piece of space junk could impart an impulse of per second, and keeping the laser on the debris for a few hours per day could alter its course by per day. One drawback is the potential for material degradation; the energy may break up the debris, adding to the problem. A similar proposal places the laser on a satellite in Sun-synchronous orbit, using a pulsed beam to push satellites into lower orbits to accelerate their reentry. A proposal to replace the laser with an Ion Beam Shepherd has been made, and other proposals use a foamy ball of aerogel or a spray of water,\ninflatable balloons,\nelectrodynamic tethers,\nboom electroadhesion,\nand dedicated anti-satellite weapons.\n\nOn 7 January 2010 Star, Inc. reported that it received a contract from the Space and Naval Warfare Systems Command for a feasibility study of the ElectroDynamic Debris Eliminator (EDDE) propellantless spacecraft for space-debris removal.\nIn February 2012 the Swiss Space Center at École Polytechnique Fédérale de Lausanne announced the Clean Space One project, a nanosatellite demonstration project for matching orbit with a defunct Swiss nanosatellite, capturing it and de-orbiting together. The mission has seen several evolutions to reach a pac-man inspired capture model.\n\nA consensus of speakers at a meeting in Brussels on 30 October 2012 organized by the Secure World Foundation (a U.S. think tank) and the French International Relations Institute reported that removal of the largest debris would be required to prevent the risk to spacecraft becoming unacceptable in the foreseeable future (without any addition to the inventory of dead spacecraft in LEO). Removal costs and legal questions about ownership and the authority to remove defunct satellites have stymied national or international action. Current space law retains ownership of all satellites with their original operators, even debris or spacecraft which are defunct or threaten active missions.\n\nOn 28 February 2014, Japan's Japan Aerospace Exploration Agency (JAXA) launched a test \"space net\" satellite. The launch was an operational test only. In December 2016 the country sent a space junk collector via Kounotori 6 to the ISS by which JAXA scientists experiment to pull junk out of orbit using a tether. The system failed to extend a 700-meter tether from a space station resupply vehicle that was returning to Earth. On 6 February the mission was declared a failure and leading researcher Koichi Inoue told reporters that they \"believe the tether did not get released\".\n\nSince 2012, the European Space Agency has designed a mission to remove large space debris from orbit. The mission, e.Deorbit, is scheduled for launch during 2023 with an objective to remove debris heavier than from LEO. Several capture techniques are being studied, including a net, a harpoon and a combination robot arm and clamping mechanism.\n\nHolger Krag of the European Space Agency states that as of 2017 there is no binding international regulatory framework with no progress occurring at the respective UN body in Vienna.\n\nIn 1946 during the Giacobinid meteor shower, Helmut Landsberg collected several small magnetic particles that were apparently associated with the shower. Fred Whipple was intrigued by this and wrote a paper that demonstrated that particles of this size were too small to maintain their velocity when they encountered the upper atmosphere. Instead, they quickly decelerated and then fell to Earth unmelted. In order to classify these sorts of objects, he coined the term \"micro-meteorite\".\n\nWhipple, in collaboration with Fletcher Watson of the Harvard Observatory, led an effort to build an observatory to directly measure the velocity of the meteors that could be seen. At the time the source of the micro-meteorites was not known. Direct measurements at the new observatory were used to locate the source of the meteors, demonstrating that the bulk of material was left over from comet tails, and that none of it could be shown to have an extra-solar origin. Today it is understood that meteoroids of all sorts are leftover material from the formation of the Solar System, consisting of particles from the interplanetary dust cloud or other objects made up from this material, like comets.\n\nThe early studies were based on optical measurements only. In 1957, Hans Pettersson conducted one of the first direct measurements of the fall of space dust on the Earth, estimating it to be 14,300,000 tons per year. This suggested that the meteoroid flux in space was much higher than the number based on telescope observations. Such a high flux presented a very serious risk to missions deeper in space, specifically the high-orbiting Apollo capsules. To determine whether the direct measurement was accurate, a number of additional studies followed, including the Pegasus satellite program. These showed that the rate of meteors passing into the atmosphere, or flux, was in line with the optical measurements, at around 10,000 to 20,000 tons per year.\n\nWhipple's work pre-dated the space race and it proved useful when space exploration started only a few years later. His studies had demonstrated that the chance of being hit by a meteoroid large enough to destroy a spacecraft was extremely remote. However, a spacecraft would be almost constantly struck by micrometeorites, about the size of dust grains.\n\nWhipple had already developed a solution to this problem in 1946. Originally known as a \"meteor bumper\" and now termed the Whipple shield, this consists of a thin foil film held a short distance away from the spacecraft's body. When a micrometeoroid strikes the foil, it vaporizes into a plasma that quickly spreads. By the time this plasma crosses the gap between the shield and the spacecraft, it is so diffused that it is unable to penetrate the structural material below. The shield allows a spacecraft body to be built to just the thickness needed for structural integrity, while the foil adds little additional weight. Such a spacecraft is lighter than one with panels designed to stop the meteoroids directly.\n\nFor spacecraft that spend the majority of their time in orbit, some variety of the Whipple shield has been almost universal for decades. Later research showed that ceramic fibre woven shields offer better protection to hypervelocity (~7 km/s) particles than aluminium shields of equal weight. Another modern design uses multi-layer flexible fabric, as in NASA's design for its never-flown TransHab expandable space habitation module,\nand the Bigelow Expandable Activity Module, which was launched in April 2016 and attached to the ISS for two years of orbital testing.\n\nTo avoid artificial space debris, many—but not all—research satellites are launched on elliptical orbits with perigees inside Earth's atmosphere so they will destroy themselves. Willy Ley predicted in 1960 that \"In time, a number of such accidentally too-lucky shots will accumulate in space and will have to be removed when the era of manned space flight arrives\". After the launch of Sputnik 1 in 1957, the North American Aerospace Defense Command (NORAD) began compiling a database (the Space Object Catalog) of all known rocket launches and objects reaching orbit: satellites, protective shields and upper- and lower-stage booster rockets. NASA published modified versions of the database in two-line element set, and during the early 1980s the CelesTrak bulletin board system re-published them.\nThe trackers who fed the database were aware of other objects in orbit, many of which were the result of in-orbit explosions. Some were deliberately caused during 1960s anti-satellite weapon (ASAT) testing, and others were the result of rocket stages blowing up in orbit as leftover propellant expanded and ruptured their tanks. To improve tracking, NORAD employee John Gabbard kept a separate database. Studying the explosions, Gabbard developed a technique for predicting the orbital paths of their products, and Gabbard diagrams (or plots) are now widely used. These studies were used to improve the modelling of orbital evolution and decay.\n\nWhen the NORAD database became publicly available during the 1970s, NASA scientist Donald J. Kessler applied the technique developed for the asteroid-belt study to the database of known objects. In 1978 Kessler and Burton Cour-Palais co-authored \"Collision Frequency of Artificial Satellites: The Creation of a Debris Belt\", demonstrating that the process controlling asteroid evolution would cause a similar collision process in LEO in decades rather than billions of years. They concluded that by about 2000, space debris would outpace micrometeoroids as the primary ablative risk to orbiting spacecraft.\n\nAt the time, it was widely thought that drag from the upper atmosphere would de-orbit debris faster than it was created. However, Gabbard was aware that the number and type of objects in space were under-represented in the NORAD data and was familiar with its behaviour. In an interview shortly after the publication of Kessler's paper, Gabbard coined the term \"Kessler syndrome\" to refer to the accumulation of debris; it became widely used after its appearance in a 1982 \"Popular Science\" article, which won the Aviation-Space Writers Association 1982 National Journalism Award.\n\nThe lack of hard data about space debris prompted a series of studies to better characterize the LEO environment. In October 1979, NASA provided Kessler with funding for further studies. Several approaches were used by these studies.\n\nOptical telescopes or short-wavelength radar was used to measure the number and size of space objects, and these measurements demonstrated that the published population count was at least 50% too low. Before this, it was believed that the NORAD database accounted for the majority of large objects in orbit. Some objects (typically, U.S. military spacecraft) were found to be omitted from the NORAD list, and others were not included because they were considered unimportant. The list could not easily account for objects under in size—in particular, debris from exploding rocket stages and several 1960s anti-satellite tests.\n\nReturned spacecraft were microscopically examined for small impacts, and sections of Skylab and the Apollo Command/Service Module which were recovered were found to be pitted. Each study indicated that the debris flux was higher than expected and debris was the primary source of collisions in space. LEO already demonstrated the Kessler syndrome.\n\nIn 1978 Kessler found that 42 percent of cataloged debris was the result of 19 events, primarily explosions of spent rocket stages (especially U.S. Delta rockets). He discovered this by first identifying those launches that were described having a large number of objects associated with a payload, then researching the literature to determine the rockets used in the launch. In 1979, this finding resulted in establishment of the NASA Orbital Debris Program after a briefing to NASA senior management, overturning the previously held belief that most unknown debris was from old ASAT tests, but from US upper stage rocket explosions and could be easily managed by depleting the unused fuel following the payload injection the upper stage Delta rocket. Beginning in 1986, when it was discovered that other international agencies were possibly experiencing the same type of problem, NASA expanded its program to include international agencies, the first being the European Space Agency. A number of other Delta components in orbit (Delta was a workhorse of the U.S. space program) had not yet exploded.\n\nDuring the 1980s, the U.S. Air Force conducted an experimental program to determine what would happen if debris collided with satellites or other debris. The study demonstrated that the process differed from micrometeoroid collisions, with large chunks of debris created which would become collision threats.\n\nIn 1991, Kessler published \"Collisional cascading: The limits of population growth in low Earth orbit\" with the best data then available. Citing the USAF conclusions about creation of debris, he wrote that although almost all debris objects (such as paint flecks) were lightweight, most of its mass was in debris about or heavier. This mass could destroy a spacecraft on impact, creating more debris in the critical-mass area. According to the National Academy of Sciences:\n\nA 1-kg object impacting at 10 km/s, for example, is probably capable of catastrophically breaking up a 1,000-kg spacecraft if it strikes a high-density element in the spacecraft. In such a breakup, numerous fragments larger than 1 kg would be created.\n\nKessler's analysis divided the problem into three parts. With a low-enough density, the addition of debris by impacts is slower than their decay rate and the problem is not significant. Beyond that is a critical density, where additional debris leads to additional collisions. At densities beyond this critical mass production exceeds decay, leading to a cascading chain reaction reducing the orbiting population to small objects (several cm in size) and increasing the hazard of space activity. This chain reaction is known as the Kessler syndrome.\n\nIn an early 2009 historical overview, Kessler summed up the situation:\n\nAggressive space activities without adequate safeguards could significantly shorten the time between collisions and produce an intolerable hazard to future spacecraft. Some of the most environmentally dangerous activities in space include large constellations such as those initially proposed by the Strategic Defense Initiative in the mid-1980s, large structures such as those considered in the late-1970s for building solar power stations in Earth orbit, and anti-satellite warfare using systems tested by the USSR, the U.S., and China over the past 30 years. Such aggressive activities could set up a situation where a single satellite failure could lead to cascading failures of many satellites in a period much shorter than years.\n\nDuring the 1980s, NASA and other U.S. groups attempted to limit the growth of debris. One effective solution was implemented by McDonnell Douglas on the Delta booster, by having the booster move away from its payload and vent any propellant remaining in its tanks. This eliminated the pressure buildup in the tanks which caused them to explode in the past. Other countries were slower to adopt this measure and, due especially to a number of launches by the Soviet Union, the problem grew throughout the decade.\n\nA new battery of studies followed as NASA, NORAD and others attempted to better understand the orbital environment, with each adjusting the number of pieces of debris in the critical-mass zone upward. Although in 1981 (when Schefter's article was published) the number of objects was estimated at 5,000, new detectors in the Ground-based Electro-Optical Deep Space Surveillance system found new objects. By the late 1990s, it was thought that most of the 28,000 launched objects had already decayed and about 8,500 remained in orbit. By 2005 this was adjusted upward to 13,000 objects, and a 2006 study increased the number to 19,000 as a result of an ASAT test and a satellite collision. In 2011, NASA said that 22,000 objects were being tracked.\n\nThe growth in the number of objects as a result of the late-1990s studies sparked debate in the space community on the nature of the problem and the earlier dire warnings. According to Kessler's 1991 derivation and 2001 updates, the LEO environment in the altitude range should be cascading. However, only one major incident has occurred: the 2009 satellite collision between Iridium 33 and Cosmos 2251. The lack of obvious short-term cascading has led to speculation that the original estimates overstated the problem. According to Kessler a cascade would not be obvious until it was well advanced, which might take years.\n\nA 2006 NASA model suggested that if no new launches took place the environment would retain the then-known population until about 2055, when it would increase on its own. Richard Crowther of Britain's Defence Evaluation and Research Agency said in 2002 that he believed the cascade would begin about 2015. The National Academy of Sciences, summarizing the professional view, noted widespread agreement that two bands of LEO space—900 to and —were already past critical density.\n\nIn the 2009 European Air and Space Conference, University of Southampton researcher Hugh Lewis predicted that the threat from space debris would rise 50 percent in the next decade and quadruple in the next 50 years. , more than 13,000 close calls were tracked weekly.\n\nA 2011 report by the U.S. National Research Council warned NASA that the amount of orbiting space debris was at a critical level. According to some computer models, the amount of space debris \"has reached a tipping point, with enough currently in orbit to continually collide and create even more debris, raising the risk of spacecraft failures\". The report called for international regulations limiting debris and research of disposal methods.\n\nThe plot of episode 4 (\"Conflict\") of Gerry Anderson's 1970 TV series \"UFO\" includes routine missions for the disposal of spent satellites by bombing.\n\n\"Salvage 1\" (1979 TV series) deals humorously with a scrap dealer who establish a space junk salvage company.\n\n\"Planetes\" is a manga (1999-2004) and anime series (2003-2004) that gives focus on a team which is responsible for the collection and disposal of space debris. The DVDs for the TV series include interviews with NASA's Orbital Debris Program Office.\n\nIn 2009, Rhett & Link wrote a song called \"Space Junk\" and made an accompanying music video for the TV series \"Brink\". The lyrics refer to two men tasked to clean up debris such as satellites and expended rockets.\n\n\"Gravity\" is a 2013 survival film, directed by Alfonso Cuaron, about a disaster on a space mission caused by Kessler syndrome.\n\n\n\n"}
{"id": "36252755", "url": "https://en.wikipedia.org/wiki?curid=36252755", "title": "Stranded asset", "text": "Stranded asset\n\nStranded assets are \"assets that have suffered from unanticipated or premature write-downs, devaluations or conversion to liabilities\". Stranded assets can be caused by a variety of factors and are a phenomenon inherent in the 'creative destruction' of economic growth, transformation and innovation, as such they pose risks to individuals and firms and may have systemic implications. Coal and other hydrocarbon resources may have the potential to become stranded as the world engages in a fossil fuel phase out.\n\nThe term is important to financial risk management in order to avoid economic loss after an asset has been converted to a liability. Accountants have measures to deal with the impairment of assets (e.g. IAS 16) which seek to ensure that an entity’s assets are not carried at more than their recoverable amount. In this context, stranded assets are also defined as an asset that has become obsolete or non-performing, but must be recorded on the balance sheet as a loss of profit.\n\nThe term \"stranded assets\" has gained significant prominence in environmental and climate change discourses, where the focus has been on how environment-related factors (such as climate change policy) could strand assets in different sectors. According to the Stranded Assets Programme at the University of Oxford's Smith School of Enterprise and the Environment, some of the environment-related risk factors that could result in stranded assets are:\n\n\nIn the context of upstream energy production, the IEA defines stranded assets as \"those investments which are made but which, at some time prior to the end of their economic life (as assumed at the investment decision point), are no longer able to earn an economic return, as a result of changes in the market and regulatory environment.\" \n\nThe carbon bubble is one popular example of how an environment-related risk factor could create stranded assets. Another example is pre-end of life decommissioning of nuclear power stations, decided by the German government and debated in Japan after the Fukushima Daiichi nuclear disaster. In financial terms, not only is the payback time of the asset curtailed, acceleration of decommissioning liabilities also increases their net present cost. When decisions result from changes to government legislation, liabilities exceeding decommissioning provisions accumulated over the asset's useful life may need to be shouldered by the tax payer, as opposed to the owner/operator.\n\nIn discussions of electric power generation deregulation, the related term stranded costs represents the existing investments in infrastructure for the incumbent utility that may become redundant in a competitive environment.\n\nTechnology change may cause other stranded assets. For example moving to 'transport as a service' (shared cars) may mean that much car manufacturing capacity, and associated insurance capacity, becomes stranded.\n\n\n"}
{"id": "40626873", "url": "https://en.wikipedia.org/wiki?curid=40626873", "title": "Technological determinism", "text": "Technological determinism\n\nTechnological determinism is a reductionist theory that assumes that a society's technology determines the development of its social structure and cultural values. Technological determinism tries to understand how technology has had an impact on human action and thought. Changes in technology are the primary source for changes in society. The term is believed to have originated from Thorstein Veblen (1857–1929), an American sociologist and economist. The most radical technological determinist in the United States in the 20th century was most likely Clarence Ayres who was a follower of Thorstein Veblen and John Dewey. William Ogburn was also known for his radical technological determinism.\n\nThe first major elaboration of a technological determinist view of socioeconomic development came from the German philosopher and economist Karl Marx, whose theoretical framework was grounded in the perspective that changes in technology, and specifically productive technology, are the primary influence on human social relations and organizational structure, and that social relations and cultural practices ultimately revolve around the technological and economic base of a given society. Marx's position has become embedded in contemporary society, where the idea that fast-changing technologies alter human lives is all-pervasive. \nAlthough many authors attribute a technologically determined view of human history to Marx's insights, not all Marxists are technological determinists, and some authors question the extent to which Marx himself was a determinist. Furthermore, there are multiple forms of technological determinism.\n\nThe term is believed to have been coined by Thorstein Veblen (1857–1929), an American social scientist. Veblen's contemporary, popular historian Charles A. Beard, provided this apt determinist image, \"Technology marches in seven-league boots from one ruthless, revolutionary conquest to another, tearing down old factories and industries, flinging up new processes with terrifying rapidity.\" As to the meaning, it is described as the ascription to machines of \"powers\" that they do not have. Veblen, for instance, asserted that \"the machine throws out anthropomorphic habits of thought.\" There is also the case of Karl Marx who expected that the construction of the railway in India would dissolve the caste system. The general idea, according to Robert Heilbroner, is that technology, by way of its machines, can cause historical change by changing the material conditions of human existence.\n\nOne of the most radical technological determinists was a man named Clarence Ayres, who was a follower of Veblen's theory in the 20th century. Ayers is best known for developing economic philosophies, but he also worked closely with Veblen who coined the technological determinism theory. He often times talked about the struggle between technology and ceremonial structure. One of his most notable theories involved the concept of \"technological drag\" where he explains technology as a self-generating process and institutions as ceremonial and this notion creates a technological over-determinism in the process. \n\nTechnological determinism seeks to show technical developments, media, or technology as a whole, as the key mover in history and social change. It is a theory subscribed by \"hyperglobalist\" who claims that as a consequence of the wide availability of technology, accelerated globalization is inevitable. Therefore, technological development and innovation become the principal motor of social, economic or political change.\n\nStrict adherents to technological determinism do not believe the influence of technology differs based on how much a technology is or can be used. Instead of considering technology as part of a larger spectrum of human activity, technological determinism sees technology as the basis for all human activity.\n\nTechnological determinism has been summarized as 'The belief in technology as a key governing force in society ...' (Merritt Roe Smith). 'The idea that technological development determines social change ...' (Bruce Bimber). It changes the way people think and how they interact with others and can be described as '...a three-word logical proposition: \"Technology determines history\"' (Rosalind Williams) . It is, '... the belief that social progress is driven by technological innovation, which in turn follows an \"inevitable\" course.' (Michael L. Smith). This 'idea of progress' or 'doctrine of progress' is centralised around the idea that social problems can be solved by technological advancement, and this is the way that society moves forward. Technological determinists believe that \"'You can't stop progress', implying that we are unable to control technology\" (Lelia Green). This suggests that we are somewhat powerless and society allows technology to drive social changes because, \"societies fail to be aware of the alternatives to the values embedded in it [technology]\" (Merritt Roe Smith).\n\nTechnological determinism has been defined as an approach that identifies technology, or technological advances, as the central causal element in processes of social change (Croteau and Hoynes). As a technology is stabilized, its design tends to dictate users' behaviors, consequently diminishing human agency. This stance however ignores the social and cultural circumstances in which the technology was developed. Sociologist Claude Fischer (1992) characterized the most prominent forms of technological determinism as \"billiard ball\" approaches, in which technology is seen as an external force introduced into a social situation, producing a series of ricochet effects.\n\nRather than acknowledging that a society or culture interacts with and even shapes the technologies that are used, a technological determinist view holds that \"the uses made of technology are largely determined by the structure of the technology itself, that is, that its functions follow from its form\" (Neil Postman). However, this is not to be confused with Daniel Chandler's \"inevitability thesis\", which states that once a technology is introduced into a culture that what follows is the inevitable development of that technology.\n\nFor example, we could examine why Romance Novels have become so dominant in our society compared to other forms of novels like the Detective or Western novel. We might say that it was because of the invention of the perfect binding system developed by publishers. This was where glue was used instead of the time-consuming and very costly process of binding books by sewing in separate signatures. This meant that these books could be mass-produced for the wider public. We would not be able to have mass literacy without mass production. This example is closely related to Marshall McLuhan's belief that print helped produce the nation state. This moved society on from an oral culture to a literate culture but also introduced a capitalist society where there was clear class distinction and individualism. As Postman maintains \n\nThe printing press, the computer, and television are not therefore simply machines which convey information. They are metaphors through which we conceptualize reality in one way or another. They will classify the world for us, sequence it, frame it, enlarge it, reduce it, argue a case for what it is like. Through these media metaphors, we do not see the world as it is. We see it as our coding systems are. Such is the power of the form of information.\n\nIn examining determinism, hard determinism can be contrasted with soft determinism. A compatibilist says that it is possible for free will and determinism to exist in the world together, while an incompatibilist would say that they can not and there must be one or the other. Those who support determinism can be further divided.\n\nHard determinists would view technology as developing independent from social concerns. They would say that technology creates a set of powerful forces acting to regulate our social activity and its meaning. According to this view of determinism we organize ourselves to meet the needs of technology and the outcome of this organization is beyond our control or we do not have the freedom to make a choice regarding the outcome (autonomous technology). The 20th century French philosopher and social theorist Jacques Ellul could be said to be a hard determinist and proponent of autonomous technique (technology). In his 1954 work \"\", Ellul essentially posits that technology, by virtue of its power through efficiency, determines which social aspects are best suited for its own development through a process of natural selection. A social system's values, morals, philosophy etc. that are most conducive to the advancement of technology allow that social system to enhance its power and spread at the expense of those social systems whose values, morals, philosophy etc. are less promoting of technology. Theodore J. Kaczynski (the Unabomber) can be essentially thought of as a hard determinist. According to Kaczynski, \"objective\" material factors in the human environment are the principal determining factors in the evolution of social systems. While geography, climate, and other \"natural\" factors largely determined the parameters of social conditions for most of human history, technology has recently become the dominant objective factor (largely due to forces unleashed by the industrial revolution) and it has been the principal objective and determining factor.\n\nSoft determinism, as the name suggests, is a more passive view of the way technology interacts with socio-political situations. Soft determinists still subscribe to the fact that technology is the guiding force in our evolution, but would maintain that we have a \"chance\" to make decisions regarding the outcomes of a situation. This is not to say that free will exists, but that the possibility for us to \"roll the dice\" and see what the outcome is exists. A slightly different variant of soft determinism is the 1922 technology-driven theory of social change proposed by William Fielding Ogburn, in which society must adjust to the consequences of major inventions, but often does so only after a period of cultural lag.\n\nIndividuals who consider technology as neutral see technology as neither good nor bad and what matters are the ways in which we use technology. An example of a neutral viewpoint is, \"guns are neutral and its up to how we use them whether it would be 'good or bad'\" (Green, 2001). Mackenzie and Wajcman believe that technology is neutral only if it's never been used before, or if no one knows what it is going to be used for (Green, 2001). In effect, guns would be classified as neutral if and only if society were none the wiser of their existence and functionality (Green, 2001). Obviously, such a society is non-existent and once becoming knowledgeable about technology, the society is drawn into a social progression where nothing is 'neutral about society' (Green). According to Lelia Green, if one believes technology is neutral, one would disregard the cultural and social conditions that technology has produced (Green, 2001). This view is also referred to as technological instrumentalism.\n\nIn what is often considered a definitive reflection on the topic, the historian Melvin Kranzberg famously wrote in the first of his six laws of technology: \"Technology is neither good nor bad; nor is it neutral.\"\n\nScepticism about technological determinism emerged alongside increased pessimism about techno-science in the mid-20th century, in particular around the use of nuclear energy in the production of nuclear weapons, Nazi human experimentation during World War II, and the problems of economic development in the Third World. As a direct consequence, desire for greater control of the course of development of technology gave rise to disenchantment with the model of technological determinism in academia.\n\nModern theorists of technology and society no longer consider technological determinism to be a very accurate view of the way in which we interact with technology, even though determinist assumptions and language fairly saturate the writings of many boosters of technology, the business pages of many popular magazines, and much reporting on technology . Instead, research in science and technology studies, social construction of technology and related fields have emphasised more nuanced views that resist easy causal formulations. They emphasise that \"The relationship between technology and society cannot be reduced to a simplistic cause-and-effect formula. It is, rather, an 'intertwining'\", whereby technology does not determine but \"operates, and are operated upon in a complex social field\" (Murphie and Potts).\n\nIn his article \"Subversive Rationalization: Technology, Power and Democracy with Technology,\" Andrew Feenberg argues that technological determinism is not a very well founded concept by illustrating that two of the founding theses of determinism are easily questionable and in doing so calls for what he calls democratic rationalization (Feenberg 210–212).\n\nProminent opposition to technologically determinist thinking has emerged within work on the social construction of technology (SCOT). SCOT research, such as that of Mackenzie and Wajcman (1997) argues that the path of innovation and its social consequences are strongly, if not entirely shaped by society itself through the influence of culture, politics, economic arrangements, regulatory mechanisms and the like. In its strongest form, verging on social determinism, \"What matters is not the technology itself, but the social or economic system in which it is embedded\" (Langdon Winner).\n\nIn his influential but contested (see Woolgar and Cooper, 1999) article \"Do Artifacts Have Politics?\", Langdon Winner illustrates not a form of determinism but the various sources of the politics of technologies. Those politics can stem from the intentions of the designer and the culture of the society in which a technology emerges or can stem from the technology itself, a \"practical necessity\" for it to function. For instance, New York City urban planner Robert Moses is purported to have built Long Island's parkway tunnels too low for buses to pass in order to keep minorities away from the island's beaches, an example of externally inscribed politics. On the other hand, an authoritarian command-and-control structure is a practical necessity of a nuclear power plant if radioactive waste is not to fall into the wrong hands. As such, Winner neither succumbs to technological determinism nor social determinism. The source of a technology's politics is determined only by carefully examining its features and history.\n\nAlthough \"The deterministic model of technology is widely propagated in society\" (Sarah Miller), it has also been widely questioned by scholars. Lelia Green explains that, \"When technology was perceived as being outside society, it made sense to talk about technology as neutral\". Yet, this idea fails to take into account that culture is not fixed and society is dynamic. When \"Technology is implicated in social processes, there is nothing neutral about society\" (Lelia Green). This confirms one of the major problems with \"technological determinism and the resulting denial of human responsibility for change. There is a loss of human involvement that shape technology and society\" (Sarah Miller).\n\nAnother conflicting idea is that of technological somnambulism, a term coined by Winner in his essay \"Technology as Forms of Life\". Winner wonders whether or not we are simply \"sleepwalking\" through our existence with little concern or knowledge as to how we truly interact with technology. In this view, it is still possible for us to wake up and once again take control of the direction in which we are traveling (Winner 104). However, it requires society to adopt Ralph Schroeder's claim that, \"users don't just passively consume technology, but actively transform it\".\n\nIn opposition to technological determinism are those who subscribe to the belief of social determinism and postmodernism. Social determinists believe that social circumstances alone select which technologies are adopted, with the result that no technology can be considered \"inevitable\" solely on its own merits. Technology and culture are not neutral and when knowledge comes into the equation, technology becomes implicated in social processes. The knowledge of how to create and enhance technology, and of how to use technology is socially bound knowledge. Postmodernists take another view, suggesting that what is right or wrong is dependent on circumstance. They believe technological change can have implications on the past, present and future. While they believe technological change is influenced by changes in government policy, society and culture, they consider the notion of change to be a paradox, since change is constant.\n\nMedia and cultural studies theorist Brian Winston, in response to technological determinism, developed a model for the emergence of new technologies which is centered on the Law of the suppression of radical potential. In two of his books – \"Technologies of Seeing: Photography, Cinematography and Television\" (1997) and \"Media Technology and Society\" (1998) – Winston applied this model to show how technologies evolve over time, and how their 'invention' is mediated and controlled by society and societal factors which suppress the radical potential of a given technology.\n\nOne continued argument for technological determinism is centered on the stirrup and its impact on the creation of feudalism in Europe in the late 8th century/early 9th century. Lynn White is credited with first drawing this parallel between feudalism and the stirrup in his book \"Medieval Technology and Social Change\", stating that as \"it made possible mounted shock combat\", the new form of war made the soldier that much more efficient in supporting feudal townships (White, 2). According to White, the superiority of the stirrup in combat was found in the mechanics of the lance charge: \"The stirrup made possible- though it did not demand- a vastly more effective mode of attack: now the rider could law his lance at rest, held between the upper arm and the body, and make at his foe, delivering the blow not with his muscles but with the combined weight of himself and his charging stallion (White, 2).\" White draws from a large research base, particularly Heinrich Brunner's \"Der Reiterdienst und die Anfänge des Lehnwesens\" in substantiating his claim of the emergence of feudalism. In focusing on the evolution of warfare, particularly that of cavalry in connection with Charles Martel's \"diversion of a considerable part of the Church's vast military riches...from infantry to cavalry\", White draws from Brunner's research and identifies the stirrup as the underlying cause for such a shift in military division and the subsequent emergence of feudalism (White, 4). Under the new brand of warfare garnered from the stirrup, White implicitly argues in favor of technological determinism as the vehicle by which feudalism was created.\n\nThough an accomplished work, White's \"Medieval Technology and Social Change\" has since come under heavy scrutiny and condemnation. The most volatile critics of White's argument at the time of its publication, P.H. Sawyer and R.H. Hilton, call the work as a whole \"a misleading adventurist cast to old-fashioned platitudes with a chain of obscure and dubious deductions from scanty evidence about the progress of technology (Sawyer and Hilton, 90).\" They further condemn his methods and, by association, the validity of technological determinism: \"Had Mr. White been prepared to accept the view that the English and Norman methods of fighting were not so very different in the eleventh century, he would have made the weakness of his argument less obvious, but the fundamental failure would remain: the stirrup cannot alone explain the changes it made possible (Sawyer and Hilton, 91).\" For Sawyer and Hilton, though the stirrup may be useful in the implementation of feudalism, it cannot be credited for the creation of feudalism alone.\n\nDespite the scathing review of White's claims, the technological determinist aspect of the stirrup is still in debate. Alex Roland, author of \"Once More into the Stirrups; Lynne White Jr, Medieval Technology and Social Change\", provides an intermediary stance: not necessarily lauding White's claims, but providing a little defense against Sawyer and Hilton's allegations of gross intellectual negligence. Roland views White's focus on technology to be the most relevant and important aspect of \"Medieval Technology and Social Change\" rather than the particulars of its execution: \"But can these many virtues, can this utility for historians of technology, outweigh the most fundamental standards of the profession? Can historians of technology continue to read and assign a book that is, in the words of a recent critic, \"shot through with over-simplification, with a progression of false connexions between cause and effect, and with evidence presented selectively to fit with [White's] own pre-conceived ideas\"? The answer, I think, is yes, at least a qualified yes (Roland, 574-575).\" Objectively, Roland claims \"Medieval Technology and Social Change\" a variable success, at least as \"Most of White's argument stands... the rest has sparked useful lines of research (Roland, 584).\" This acceptance of technological determinism is ambiguous at best, neither fully supporting the theory at large nor denouncing it, rather placing the construct firmly in the realm of the theoretical. Roland neither views technological determinism as completely dominant over history nor completely absent as well; in accordance with the above criterion of technological determinist structure, would Roland be classified as a \"soft determinist\".\n\nThomas L. Friedman, American journalist, columnist and author, admits to being a technological determinist in his book \"The World is Flat\".\n\nFuturist Raymond Kurzweil's theories about a technological singularity follow a technologically deterministic view of history.\n\nSome interpret Karl Marx as advocating technological determinism, with such statements as \"The Handmill gives you society with the feudal lord: the steam-mill, society with the industrial capitalist\" (\"The Poverty of Philosophy,\" 1847), but others argue that Marx was not a determinist.\n\nTechnological determinist Walter Ong reviews the societal transition from an oral culture to a written culture in his work \"Orality and Literacy.\" He asserts that this particular development is attributable to the use of new technologies of literacy (particularly print and writing,) to communicate thoughts which could previously only be verbalized. He furthers this argument by claiming that writing is purely context dependent as it is a \"secondary modelling system\" (8). Reliant upon the earlier primary system of spoken language, writing manipulates the potential of language as it depends purely upon the visual sense to communicate the intended information. Furthermore, the rather stagnant technology of literacy distinctly limits the usage and influence of knowledge, it unquestionably effects the evolution of society. In fact, Ong asserts that \"more than any other single invention, writing has transformed human consciousness\" (Ong 1982: 78).\n\nMedia determinism is a form of technological determinism, a philosophical and sociological position which posits the power of the media to impact society. Two foundational media determinists are the Canadian scholars Harold Innis and Marshall McLuhan. One of the best examples of technological determinism in media theory is Marshall McLuhan's theory \"the medium is the message\" and the ideas of his mentor Harold Adams Innis. Both these Canadian theorists saw media as the essence of civilization. The association of different media with particular mental consequences by McLuhan and others can be seen as related to technological determinism. It is this variety of determinism that is referred to as media determinism. According to McLuhan, there is an association between communications media/technology and language; similarly, Benjamin Lee Whorf argues that language shapes our perception of thinking (linguistic determinism). For McLuhan, media is a more powerful and explicit determinant than is the more general concept of language. McLuhan was not necessarily a hard determinist. As a more moderate version of media determinism, he proposed that our use of particular media may have subtle influences on us, but more importantly, it is the social context of use that is crucial. See also Media ecology.\nMedia determinism is a form of the popular dominant theory of the relationship between technology and society. In a determinist view, technology takes on an active life of its own and is seen be as a driver of social phenomena. Innis believed that the social, cultural, political, and economic developments of each historical period can be related directly to the technology of the means of mass communication of that period. In this sense, like Dr. Frankenstein's monster, technology itself appears to be alive, or at least capable of shaping human behavior. However, it has been increasingly subject to critical review by scholars. For example, scholar Raymond Williams, criticizes media determinism and rather believes social movements define technological and media processes. With regard to communications media, audience determinism is a viewpoint opposed to media determinism. This is described as instead of media being presented as doing things to people; the stress is on the way people do things with media. Individuals need to be aware that the term \"deterministic\" is a negative one for many social scientists and modern sociologists; in particular they often use the word as a term of abuse.\n\n\n\n"}
{"id": "34066587", "url": "https://en.wikipedia.org/wiki?curid=34066587", "title": "Technological transitions", "text": "Technological transitions\n\nTechnological innovations have occurred throughout history and rapidly increased over the modern age. New technologies are developed and co-exist with the old before supplanting them. Transport offers several examples; from sailing to steam ships to automobiles replacing horse-based transportation. Technological transitions (TT) describe how these technological innovations occur and are incorporated into society. Alongside the technological developments TT considers wider societal changes such as “user practices, regulation, industrial networks (supply, production, distribution), infrastructure, and symbolic meaning or culture”. For a technology to have use, it must be linked to social structures human agency and organisations to fulfil a specific need. Hughes refers to the ‘seamless web’ where physical artefacts, organisations, scientific communities, and social practices combine. A technological system includes technical and non-technical aspects, and it a major shift in the socio-technical configurations (involving at least one new technology) is when a technological transition occurs.\n\nWork on technological transitions draws on a number of fields including history of science, technology studies, and evolutionary economics. The focus of evolutionary economics is on economic change, but as a driver of this technological change has been considered in the literature. Joseph Schumpeter, in his classic \"Theory of Economic Development\" placed the emphasis on non-economic forces as the driver for growth. The human actor, the entrepreneur is seen as the cause of economic development which occurs as a cyclical process. Schumpeter proposed that radical innovations were the catalyst for Kondratiev cycles.\n\nThe Russian economist Kondratiev proposed that economic growth operated in boom and bust cycles of approximately 50 year periods. These cycles were characterised by periods of expansion, stagnation and recession. The period of expansion is associated with the introduction of a new technology, e.g. steam power or the microprocessor. At the time of publication, Kondratiev had considered that two cycles had occurred in the nineteenth century and third was beginning at the turn of the twentieth. Modern writers, such as Freeman and Perez outlined five cycles in the modern age:\n\n\nFreeman and Perez proposed that each cycle consists of pervasive technologies, their production and economic structures that support them. Termed ‘techno-economic paradigms’, they suggest that the shift from one paradigm to another is the result of emergent new technologies. \n\nFollowing the recent economic crisis, authors such as Moody and Nogrady have suggested that a new cycle is emerging from the old, centred on the use of sustainable technologies in a resource depleted world.\n\nThomas Kuhn described how a paradigm shift is a wholesale shift in the basic understanding of a scientific theory. Examples in science include the change of thought from miasma to germ theory as a cause of disease. Building on this work, Giovanni Dosi developed the concept of ’technical paradigms’ and ‘technological trajectories’. In considering how engineers work, the technical paradigm is an outlook on the technological problem, a definition of what the problems and solutions are. It charts the idea of specific progress. By identifying the problems to be solved the paradigm exerts an influence on technological change. The pattern of problem solving activity and the direction of progress is the technological trajectory. In similar fashion, Nelson and Winter (,)defined the concept of the ‘technological regime’ which directs technological change through the beliefs of engineers of what problems to solve. The work of the actors and organisations is the result of organisational and cognitive routines which determines search behaviour. This places boundaries and also trajectories (direction) to those boundaries.\n\nIn analysing (historic) cases of technological transitions researchers from the systems in transition branch of transitions research have used a multi-level perspective (MLP) as a heuristic model to understand changes in socio-technical systems. () Innovation system approaches traditionally focus on the production side. A socio-technical approach combines the science and technology in devising a production, with the application of the technology in fulfilling a societal function. Linking the two domains are the distribution, infrastructure and markets of the product. This approach considers a transition to be multi-dimensional as technology is only one aspect. \n\nThe MLP proposes three analytical levels: the niche, regime and landscape. \n\nNiche (Micro-level)\nRadical innovations occur at the niche level. These act as ‘safe havens’ for fledgling technologies to develop, largely free from market pressures which occur at the regime level. The US Military has acted as niche for major twentieth century technologies such as the aircraft, radio and the internet. More recently, California’s Silicon Valley has provided an arena for ICT focused technologies to emerge. Some innovations will challenge the existing regime while others fail. \n\nRegime (Meso-level)\nThe socio-technical regime, as defined by Geels, includes a web of inter-linking actors across different social groups and communities following a set of rules. In effect, the established practices of a given system. Seven dimensions have been identified in the socio-technical regime: technology, user practices and application, the symbolic meaning of technology, infrastructure, policy and techno-scientific knowledge. Change does occur at the regime level but it is normally slow and incremental unlike the radical change at the niche level. The actors who constitute the existing regime are set to gain from perpetuating the incumbent technology at the expense of the new. This is known as ‘lock-in’.\n\nLandscape (Macro-level)\nExogenous to the previous levels is the socio-technical landscape. A broad range of factors are contained here, such as economic pressures, cultural values, social trends, wars and environmental issues. Change occurs at an even slower rate than at the regime level. \n\nA transition is said to happen when a regime shift has occurred. This is the result of the interplay between the three levels. Regimes are relatively inert and resistant to change being structured to incremental innovation following established trajectories. As such, transitions are difficult to achieve. The current regime is typically suffering internal issues. Pressure from the landscape level may cause ‘cracks’ or ‘windows of opportunity’ through which innovations at the niche level may initially co-exist with the established technology before achieving ascendency. Once the technology has fully embedded into society the transition is said to be completed.\n\nThe MLP has been used in describing a range of historic transitions in socio-technical regimes for mobility, sanitation, food, lighting and so on. While early research focused on historical transitions, a second strand of research was more focused on transitions to sustainable technologies in key sectors such as transport, energy and housing.\n\nGeels presented three historical transitions on system innovation relating to modes of transportation. The technological transition from sailing ships to steamships in the UK will be summarised and shown in the context of a wider system innovation. \n\nGreat Britain was the world’s leading naval power in the nineteenth century, and led the way in the transition from sail to steam. At first, the introduction of steam technology co-existed with the current regime. Steam tugs assisted sail ships into port and hybrid steam / sail ships appeared. Landscape developments create the necessity for improvements in the technology. A demand for trans-Atlantic emigration was prompted by the Irish potato famine, European political instability and the lure of gold in California. The requirement for such arduous journeys had prompted a wealth of innovations at the niche level in steamship-development. From the late 1880s, as steamship technology improved and costs dropped, the new technology was widely diffused and a new regime established. The changes go beyond a technological transition as it involved new ship management and fleet management practices, new supporting infrastructures and new functionalities.\n\nThe nature of transitions varies and the differing qualities result in multiple pathways occurring. Geels and Schot defined five transition paths:\n\n\nSix characteristics of technological transitions have been identified.,\n\n\"Transitions are co-evolutionary and multi-dimensional\"\nTechnological developments occur intertwined with societal needs, wants and uses. A technology is adopted and diffused based on this interplay between innovation and societal requirements. Co-evolution has different aspects. As well as the co-evolution of technology and society, aspects between science, technology, users and culture have been considered.\n\"Multi-actors are involved\"\nScientific and engineering communities are central to the development of a technology, but a wide range of actors are involved in a transition. This can include organisations, policy-makers, government, NGOs, special interest groups and others.\n\n\"Transitions occur at multiple levels\"\nAs shown in the MLP transitions occur through the interplay of processes at different levels. \n\n\"Transitions are a long-term process\"\nComplete system-change takes time and can be decades in the making. Case studies show them to be between 40 and 90 years.\n\n\"Transitions are radical\"\nFor a true transition to occur the technology has to be a radical innovation. \n\n\"Change is Non-linear\"\nThe rate of change will vary over time. For example, the pace of change may be slow at the gestation period (at the niche level) but much more rapid when a breakthrough is occurring.\n\nDiffusion of an innovation is the concept of how it is picked up by society, at what rate and why. Everett (1962).The diffusion of a technological innovation into society can be considered in distinct phases. Pre-development is the gestation period where the new technology has yet to make an impact. Take-off is when the process of a system shift is beginning. A breakthrough is occurring when fundamental changes are occurring in existing structures through the interplay of economic, social and cultural forces. Once the rate of change has decreased and a new balance is achieved, stabilization is said to have occurred. A full transition involves an overhaul of existing rules and change of beliefs which takes time, typically spanning at least a generation. This process can be speeded-up through seismic, unforeseen events such as war or economic strife. \n\nGeels proposed a similar four phased approach which draws on the multi-level perspective (MLP) developed by Dutch scholars. Phases one sees the emergence of a novelty, born from the existing regime. Development then occurs in the niche level at phase two. As before, breakthrough then occurs at phase three. In the parlance of the MLP the new technology, having been developed at the niche level, is in competition with the established regime. To breakthrough and achieve wide diffusion, external factors – ‘windows of opportunity’ are required.\n\nA number of possible circumstances can act as windows of opportunity for the diffusion of new technologies: \n\n\nAlongside external influences, internal drivers catalyse diffusion. These include economic factors such as the price performance ration. Socio-technical perspectives focus on the links between disparate social and technological elements. Following the breakthrough, the final phases see the new technology supersede the old.\n\nThe study of technological transitions has an impact beyond academic interest. The transitions referred to in the literature may relate to historic processes, such as the transportation transitions studied by Geels, but system changes are required to achieve a safe transition to a low carbon-economy. (). Current structural problems are apparent in a range of sectors. Dependency on oil is problematic in the energy sector due to availability, access and contribution to greenhouse gas (GHG) emissions. Transportation is a major user of energy causing significant emission of GHGs. Food production will need to keep pace with an ever-growing world population while overcoming challenges presented by global warming and transportation issues. Incremental change has provided some improvements but a more radical transition is required to achieve a more sustainable future. \n\nDeveloped from the work on technological transitions is the field of transition management. Within this is an attempt to shape the direction of change complex socio-technical systems to more sustainable patterns. Whereas work on technological transitions is largely based on historic processes, proponents of transition management seek to actively steer transitions in progress.\n\nGenus and Coles outlined a number of criticisms against the analysis of technological transitions, in particular when using the MLP. Empirical research on technological transitions occurring now has been limited, with the focus on historic transitions. Depending on the perspective on transition case studies they could be presented as having occurred on a different transition path to what was shown. For example, the bicycle could be considered an intermediate transport technology between the horse and the car. Judged from shorter different time-frame this could appear a transition in its own right. Determining the nature of a transition is problematic; when it started and ended, or whether one occurred in the sense of a radical innovation displacing an existing socio-technical regime. The perception of time casts doubt on whether a transition has occurred. If viewed over a long enough period even inert regimes may demonstrate radical change in the end. The MLP has also been criticised by scholars studying sustainability transitions using Social Practice Theories.\n\n"}
{"id": "361239", "url": "https://en.wikipedia.org/wiki?curid=361239", "title": "Technological utopianism", "text": "Technological utopianism\n\nTechnological utopianism (often called techno-utopianism or technoutopianism) is any ideology based on the premise that advances in science and technology could and should bring about a utopia, or at least help to fulfill one or another utopian ideal.\n\nA techno-utopia is therefore an ideal society, in which laws, government, and social conditions are solely operating for the benefit and well-being of all its citizens, set in the near- or far-future, as advanced science and technology will allow these ideal living standards to exist; for example, post-scarcity, transformations in human nature, the avoidance or prevention of suffering and even the end of death.\n\nTechnological utopianism is often connected with other discourses presenting technologies as agents of social and cultural change, such as technological determinism or media imaginaries.\n\nDouglas Rushkoff, a leading theorist on technology and cyberculture claims that technology gives everyone a chance to voice their own opinions, fosters individualistic thinking, and dilutes hierarchy and power structures by giving the power to the people. He says that the whole world is in the middle of a new Renaissance, one that is centered on technology and self-expression. However, Rushkoff makes it clear that “people don’t live their lives behind a desk with their hands on a keyboard” \n\nA tech-utopia does not disregard any problems that technology may cause, but strongly believes that technology allows mankind to make social, economic, political, and cultural advancements. Overall, Technological Utopianism views technology’s impacts as extremely positive.\n\nIn the late 20th and early 21st centuries, several ideologies and movements, such as the cyberdelic counterculture, the Californian Ideology, transhumanism, and singularitarianism, have emerged promoting a form of techno-utopia as a reachable goal. Cultural critic Imre Szeman argues technological utopianism is an irrational social narrative because there is no evidence to support it. He concludes that it shows the extent to which modern societies place faith in narratives of progress and technology overcoming things, despite all evidence to the contrary.\n\nKarl Marx believed that science and democracy were the right and left hands of what he called the move from the realm of necessity to the realm of freedom. He argued that advances in science helped delegitimize the rule of kings and the power of the Christian Church.\n\n19th-century liberals, socialists, and republicans often embraced techno-utopianism. Radicals like Joseph Priestley pursued scientific investigation while advocating democracy. Robert Owen, Charles Fourier and Henri de Saint-Simon in the early 19th century inspired communalists with their visions of a future scientific and technological evolution of humanity using reason. Radicals seized on Darwinian evolution to validate the idea of social progress. Edward Bellamy’s socialist utopia in \"Looking Backward\", which inspired hundreds of socialist clubs in the late 19th century United States and a national political party, was as highly technological as Bellamy’s imagination. For Bellamy and the Fabian Socialists, socialism was to be brought about as a painless corollary of industrial development.\n\nMarx and Engels saw more pain and conflict involved, but agreed about the inevitable end. Marxists argued that the advance of technology laid the groundwork not only for the creation of a new society, with different property relations, but also for the emergence of new human beings reconnected to nature and themselves. At the top of the agenda for empowered proletarians was \"to increase the total productive forces as rapidly as possible\". The 19th and early 20th century Left, from social democrats to communists, were focused on industrialization, economic development and the promotion of reason, science, and the idea of progress.\n\nSome technological utopians promoted eugenics. Holding that in studies of families, such as the Jukes and Kallikaks, science had proven that many traits such as criminality and alcoholism were hereditary, many advocated the sterilization of those displaying negative traits. Forcible sterilization programs were implemented in several states in the United States.\n\nH.G. Wells in works such as \"The Shape of Things to Come\" promoted technological utopianism.\n\nThe horrors of the 20th century – namely fascist dictatorships and the world wars – caused many to abandon optimism. The Holocaust, as Theodor Adorno underlined, seemed to shatter the ideal of Condorcet and other thinkers of the Enlightenment, which commonly equated scientific progress with social progress.\n\nA movement of techno-utopianism began to flourish again in the dot-com culture of the 1990s, particularly in the West Coast of the United States, especially based around Silicon Valley. The Californian Ideology was a set of beliefs combining bohemian and anti-authoritarian attitudes from the counterculture of the 1960s with techno-utopianism and support for libertarian economic policies. It was reflected in, reported on, and even actively promoted in the pages of \"Wired\" magazine, which was founded in San Francisco in 1993 and served for a number years as the \"bible\" of its adherents.\n\nThis form of techno-utopianism reflected a belief that technological change revolutionizes human affairs, and that digital technology in particular – of which the Internet was but a modest harbinger – would increase personal freedom by freeing the individual from the rigid embrace of bureaucratic big government. \"Self-empowered knowledge workers\" would render traditional hierarchies redundant; digital communications would allow them to escape the modern city, an \"obsolete remnant of the industrial age\".\n\nSimilar forms of \"digital utopianism\" has often entered in the political messages of party and social movements that point to the Web or more broadly to new media as harbingers of political and social change. Its adherents claim it transcended conventional \"right/left\" distinctions in politics by rendering politics obsolete. However, techno-utopianism disproportionately attracted adherents from the libertarian right end of the political spectrum. Therefore, techno-utopians often have a hostility toward government regulation and a belief in the superiority of the free market system. Prominent \"oracles\" of techno-utopianism included George Gilder and Kevin Kelly, an editor of \"Wired\" who also published several books.\n\nDuring the late 1990s dot-com boom, when the speculative bubble gave rise to claims that an era of \"permanent prosperity\" had arrived, techno-utopianism flourished, typically among the small percentage of the population who were employees of Internet startups and/or owned large quantities of high-tech stocks. With the subsequent crash, many of these dot-com techno-utopians had to rein in some of their beliefs in the face of the clear return of traditional economic reality.\n\nIn the late 1990s and especially during the first decade of the 21st century, technorealism and techno-progressivism are stances that have risen among advocates of technological change as critical alternatives to techno-utopianism. However, technological utopianism persists in the 21st century as a result of new technological developments and their impact on society. For example, several technical journalists and social commentators, such as Mark Pesce, have interpreted the WikiLeaks phenomenon and the United States diplomatic cables leak in early December 2010 as a precursor to, or an incentive for, the creation of a techno-utopian transparent society. Cyber-utopianism, first coined by Evgeny Morozov, is another manifestation of this, in particular in relation to the Internet and social networking.\n\nBernard Gendron, a professor of philosophy at the University of Wisconsin–Milwaukee, defines the four principles of modern technological utopians in the late 20th and early 21st centuries as follows:\n\n\nRushkoff presents us with multiple claims that surround the basic principles of Technological Utopianism:\n\n\nCritics claim that techno-utopianism's identification of social progress with scientific progress is a form of positivism and scientism. Critics of modern libertarian techno-utopianism point out that it tends to focus on \"government interference\" while dismissing the positive effects of the regulation of business. They also point out that it has little to say about the environmental impact of technology and that its ideas have little relevance for much of the rest of the world that are still relatively quite poor (see global digital divide).\n\nIn his 2010 study \"System Failure: Oil, Futurity, and the Anticipation of Disaster\", Canada Research Chairholder in cultural studies Imre Szeman argues that technological utopianism is one of the social narratives that prevent people from acting on the knowledge they have concerning the effects of oil on the environment.\n\nIn a controversial article \"Techno-Utopians are Mugged by Reality\", Wall Street Journal explores the concept of the violation of free speech by shutting down social media to stop violence. As a result of British cities being looted consecutively, Prime British Minister David Cameron argued that the government should have the ability to shut down social media during crime sprees so that the situation could be contained. A poll was conducted to see if Twitter users would prefer to let the service be closed temporarily or keep it open so they can chat about the famous television show X-Factor. The end report showed that every Tweet opted for X-Factor. The negative social effects of technological utopia is that society is so addicted to technology that we simply can't be parted even for the greater good. While many Techno-Utopians would like to believe that digital technology is for the greater good, it can also be used negatively to bring harm to the public.\n\nOther critics of a techno-utopia include the worry of the human element. Critics suggest that a techno-utopia may lessen human contact, leading to a distant society. Another concern is the amount of reliance society may place on their technologies in these techno-utopia settings. These criticisms are sometimes referred to as a technological anti-utopian view or a techno-dystopia.\n\nEven today, the negative social effects of a technological utopia can be seen. Mediated communication such as phone calls, instant messaging and text messaging are steps towards a utopian world in which one can easily contact another regardless of time or location. However, mediated communication removes many aspects that are helpful in transferring messages. As it stands today, most text, email, and instant messages offer fewer nonverbal cues about the speaker’s feelings than do face-to-face encounters. This makes it so that mediated communication can easily be misconstrued and the intended message is not properly conveyed. With the absence of tone, body language, and environmental context, the chance of a misunderstanding is much higher, rendering the communication ineffective. In fact, mediated technology can be seen from a dystopian view because it can be detrimental to effective interpersonal communication. These criticisms would only apply to messages that are prone to misinterpretation as not every text based communication requires contextual cues. The limitations of lacking tone and body language in text based communication are likely to be mitigated by video and augmented reality versions of digital communication technologies.\n\n\n"}
{"id": "26149061", "url": "https://en.wikipedia.org/wiki?curid=26149061", "title": "Tectonic weapon", "text": "Tectonic weapon\n\nA tectonic weapon is a hypothetical device or system which could create earthquakes, volcanoes, or other seismic events in specified locations by interfering with the Earth's natural geological processes. It was defined in 1992 by Aleksey Vsevolovidich Nikolayev, corresponding member Russian Academy of Sciences: \"A tectonic or seismic weapon would be the use of the accumulated tectonic energy of the Earth's deeper layers to induce a destructive earthquake\". He added \"to set oneself the objective of inducing an earthquake is extremely doubtful\".\n\nTheoretically, the tectonic weapon functions by creating a powerful charge of elastic energy in the form of deformed volume of the Earth's crust in a region of tectonic activity. This then becomes an earthquake once triggered by a nuclear explosion in the epicenter or a vast electric pulse. As to the question of whether a nuclear explosion can trigger an earthquake, there was the analysis of local seismic recordings within a couple of miles of nuclear tests in the 1960s at Nevada that showed nuclear explosions caused some tectonic stress. An account provided by a member of the Nevada Commission on Nuclear Projects, also claimed that a 1968 underground nuclear test called Faultless successfully induced an earthquake. The United States Geological Survey stated that it produced fresh fault rupture some 1200 meters long. There is also a theory that 1998 earthquake in Afghanistan was triggered by thermonuclear tests conducted in Indian and Pakistani test sites 2-20 days prior. \n\nRoger Clark, lecturer in geophysics at Leeds University said in the respected journal Nature in 1996, responding to a newspaper report that there had been two secret Soviet programs, \"Mercury\" and \"Volcano\", aimed at developing a \"tectonic weapon\" that could set off earthquakes from great distance by manipulating electromagnetism, said \"We don't think it is impossible, or wrong, but past experience suggests it is very unlikely\". According to Nature these programs had been \"unofficially known to Western geophysicists for several years\". According to the story the Mercury program began in 1987, three tests were conducted in Kyrgyzstan, and Volcano's last test occurred in 1992.\n\nSuch weapons, whether or not they exist or are feasible, are a source of concern in official circles. For example, US Secretary of Defense William S. Cohen, said on 28 April 1997 at the Conference on Terrorism, Weapons of Mass Destruction, and U.S. Strategy, University of Georgia, while discussing the dangers of false threats, \"Others are engaging even in an eco-type of terrorism whereby they can alter the climate, set off earthquakes, volcanoes remotely through the use of electromagnetic waves.\"\n\nNew Zealand's unsuccessful Project Seal programme during World War II attempted to create tsunami waves as a weapon. It was reported in 1999 that such a weapon might be viable.\n\nNikola Tesla claimed a small steam-powered mechanical oscillator he was experimenting with in 1898 produced earthquake-like effects, but this has never been replicated. The television show \"MythBusters\" in Episode 60 .E2.80.93 made a small machine based on the same principle but powered by electricity rather than steam; it produced vibrations in a large structure detectable 100 feet away, but no significant shaking, and they judged the effect to be a busted myth.\n\nThe 1978 Convention on the Prohibition of Military or Any Other Hostile Use of Environmental Modification Techniques is an international treaty ratified by 75 states, and signed by a further 17, that prohibits use of environmental modification techniques to cause earthquakes and tsunamis, amongst other phenomena.\n\nAfter natural tectonic phenomena such as the 2010 Haiti earthquake, conspiracy theories, usually relating to the armed forces of the United States and formerly the Soviet Union, often arise, though no evidence is advanced. After the Haiti earthquake it was widely reported that president Hugo Chávez of Venezuela made unsupported allegations that it had been caused by testing of a US tectonic weapon. The newspaper Komsomolskaya Pravda of Moscow reported on page 1 on 30 May 1992 that \"a geophysical or tectonic weapon was actually developed in the USSR despite the UN Convention\", but that Chief Seismologist Major-General V Bochrov of the USSR Ministry of Defence categorically rejected any hints on the existence of tectonic weapons.\n\nWhile the British Tallboy and Grand Slam bombs of World War II were called \"earthquake bombs\", the name came from their way of destroying very hardened targets by shaking their foundations as an earthquake would; they were never intended to cause an actual earthquake.\n\n"}
{"id": "40076768", "url": "https://en.wikipedia.org/wiki?curid=40076768", "title": "Telepharmacy", "text": "Telepharmacy\n\nTelepharmacy is the delivery of pharmaceutical care via telecommunications to patients in locations where they may not have direct contact with a pharmacist. It is an instance of the wider phenomenon of telemedicine, as implemented in the field of pharmacy. Telepharmacy services include drug therapy monitoring, patient counseling, prior authorization and refill authorization for prescription drugs, and monitoring of formulary compliance with the aid of teleconferencing or videoconferencing. Remote dispensing of medications by automated packaging and labeling systems can also be thought of as an instance of telepharmacy. Telepharmacy services can be delivered at retail pharmacy sites or through hospitals, nursing homes, or other medical care facilities.\n\nThe term can also refer to the use of videoconferencing in pharmacy for other purposes, such as providing education, training, and management services to pharmacists and pharmacy staff remotely.\n\nA primary appeal of telepharmacy is its potential to expand access to pharmacy care in smaller rural communities, some of which cannot support a full-time pharmacist or cannot easily recruit a pharmacist to reside in their region. Telepharmacy can potentially give patients in remote locations access to professional pharmacy care that could not be received locally, which can lower costs and improve patient safety through better patient counseling, drug administration monitoring, and compliance monitoring. Sharing of pharmacists between sites can also decrease costs in existing facilities, which might no longer need to employ a full-time pharmacist.\n\nThe potential costs of telepharmacy are broadly the same as those associated with all forms of telemedicine: potentially decreased human interaction between medical professionals and patients, an increased risk of error when medical services are delivered in the absence of a registered professional, and an increased risk that protected health information may be compromised through electronic information storage and transmission.\n\nThe implementation of telepharmacy varies by region and jurisdiction. Factors including geography, laws and regulations, and economics influence its implementation.\n\nA form of telepharmacy has been in use by Australia's Royal Flying Doctor Service since 1942. Medical chests containing medications and equipment are placed in remote communities where they can be administered to patients during a telehealth consultation. Some 3,500 chests were distributed around Australia as of 2006. In one year, Queensland recorded 21,470 telehealth consultations, of which 13.7% resulted in administration of a medication from a medical chest. The medication types administered most often are antibiotics, analgesics and gastrointestinal medications. This system improves access to both emergency and routine medical care in remote parts of Australia and reduces the need for patients to travel to seek medical care.\n\nAnother application of telepharmacy in Queensland has been the provision of pharmaceutical reviews in rural hospitals that lack on-staff pharmacists. Although broader use of telepharmacy could help alleviate a shortage of pharmacists, Australia has lagged the United States in its implementation of telepharmacy, partly because doctors, nurses, and other health care workers provide pharmacy services in rural and remote areas where there are no pharmacists.\n\nImplementation of telepharmacy in the United States began in the 2000s. A combination of factors, including changes in Medicare reimbursement for medications and the recession of 2007–8, led to a decline in the number of independent pharmacies in rural areas. In response to the need for alternative means of delivering pharmacy in services in rural communities lacking a full-time pharmacist, several midwestern and northwestern states with extensive rural areas have led much of the development of policy and implementation methods for telepharmacy.\n\nIn 2001, North Dakota became the first U.S. state to pass regulations allowing retail pharmacies to operate without requiring a pharmacist to be physically present. The next year, state agencies and grants established the North Dakota Telepharmacy Project, which now supports more than fifty remote retail and hospital pharmacy sites throughout North Dakota. In this program, a licensed pharmacist at a central site communicates with remote site pharmacy technicians and patients through videoconferencing. A 2004 study of the program found that telepharmacy delivered the same quality of pharmacy services as traditional facilities, and a study of the operation of one North Dakota telepharmacy business from 2002 through 2004 found that, while medication inventory turnover was lower than the industry average, the remote sites were able to be operated profitably. The success and expansion of this program were an inspiration and model for programs and laws in other states.\n\nThe Community Health Association of Spokane, a network of community health centers in Spokane, Washington, started a telepharmacy program in 2001. The program delivers remote medication dispensing and health counseling to patients at six urban and rural clinics; remote site personnel are connected to pharmacists at the base site by videoconferencing. A survey found that most patients at the remote sites strongly agreed or agreed that they would have had difficulty affording their medications without this program.\n\nThe Alaska Native Medical Center, a hospital in Anchorage, Alaska, providing telehealth services to Alaska Native populations, established a telepharmacy program in 2003 to improve its pharmaceutical services in rural native settlements. The American Society of Health-System Pharmacists gave the program its 2006 Award for Excellence in Medication-Use Safety, concluding that the use of telepharmacy had improved access to pharmaceutical care and enabled pharmacists to monitor medication safety and encourage medication adherence, as well as making pharmacy care more cost-effective.\n\nThe U.S. Navy Bureau of Medicine operates a large-scale telepharmacy program for the use of service personnel. After piloting the program in 2006 at Naval Hospital Pensacola in Florida and Naval Hospital Bremerton in Washington, in 2010 the Navy expanded it to more sites throughout the world. This program represents the largest implementation of telepharmacy to date.\n\nCalifornia passed a Telehealth Advancement Act in 2011 to update the state's legal definitions of telehealth, simplify approval processes for telehealth services, and broaden the range of medical services that may be provided via telehealth. The law establishes legal parity between the direct and remote delivery of pharmacy care. Iowa's first telepharmacy opened in September 2012 after receiving a three-year waiver from the Iowa Board of Pharmacy that allows the facility to operate without a pharmacist on-site.\n\nA 2010 study of the various American states' rural health offices found that telepharmacy in rural medical facilities varied in prevalence across the United States but was still not widespread, and that many states had not yet clearly defined regulations for telepharmacy in hospitals. Adoption and implementation of telepharmacy methods has been slow compared to the spread of the basic technologies involved (internet access, audio/video compression algorithms, microphones and video cameras), despite periodic predictions of a forthcoming boom in the industry. Aside from more intangible factors (such as physicians' and pharmacists' personal uneasiness with the lack of physical interaction with patients), the major obstacles to telepharmacy implementation appear to have been the lack of clear legal regulations for telepharmacy, and the lack of network and software systems to manage (and secure) all of the data used in a professional pharmacy. As of 2010, many of the telepharmacy facilities in active operation were operating as pilot programs or under temporary waivers issued by state regulators because many states still had no clear legal framework for the regulation of remote pharmaceutical sites without pharmacists. Even in states that had regulated retail telepharmacy practices, regulations were often not in place to permit the implementation of telepharmacy in hospital settings. For some pharmacy facilities that might otherwise consider telepharmacy, the cost and complexity of the infrastructure needed to manage patient data across multiple sites can be prohibitive. In addition to the computer hardware required for patient data storage, distribution and teleconferencing, telepharmacy programs must deploy network security tools and procedures adequate to protect patient medical information in compliance with HIPAA and other patient privacy regulations. In 2010 the North Dakota Telepharmacy Project estimated that the computer hardware needed for a typical retail installation costs US$17,300 per site, with an additional cost of US$5,000 to buy a mobile cart for a hospital installation.\n\nAdoption of telepharmacy in Canada began as a response to a nationwide shortage of pharmacists. Canada's first telepharmacy service was started by a hospital in Cranbrook, British Columbia, in June 2003 in order to assist a hospital in a nearby town that was unable to hire a pharmacist. To meet the need for service, a hospital pharmacist in Cranbrook began using telepharmacy technology to oversee pharmacy technicians at the other hospital. A similar service was subsequently extended to other small hospitals in the province; it is also used to provide coverage when a hospital's sole pharmacist is absent due to illness or vacation. Remote dispensing machines for medication began operation in Ontario, Canada, in 2007. After a patient inserts a prescription into the dispensing machine, the prescription is scanned and the patient is connected by telephone videoconference to a pharmacist at a remote site. The pharmacist reviews the prescription, discusses the patient's medication history, and authorizes the machine to dispense medication to the patient. The machines proved successful, with one assessment revealing that 96% of patients using them had their prescription filled in under five minutes. As of 2009, a hospital in Ontario, Canada, was using telepharmacy services in addition to retaining a pharmacist at the hospital; the telepharmacist reviews medication orders, while the on-site pharmacist works with patients and oversees medication safety in the facility. Thus telepharmacy support allows the on-site pharmacist to focus on the more sensitive and nuanced tasks for which physical presence is most helpful.\n\nAfter their success in Canada, remote medication dispensing machines were scheduled to be tested at several hospital locations in the United Kingdom beginning in 2010. In 2013, Maxor National Pharmacy Services, a U.S. company, reported that its remote dispensing machines for medication were being used in Bahrain, Belgium, Cuba, England, Germany, Guam, Italy, Japan, Spain and Venezuela.\n\nIn 2010, Mannings drugstores became the first in Hong Kong to use videoconferencing to allow patients at outlets without full-time pharmacists to consult with pharmacists at other sites.\n"}
{"id": "7452748", "url": "https://en.wikipedia.org/wiki?curid=7452748", "title": "Timeline of carbon nanotubes", "text": "Timeline of carbon nanotubes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2570284", "url": "https://en.wikipedia.org/wiki?curid=2570284", "title": "Universal Data Element Framework", "text": "Universal Data Element Framework\n\nThe Universal Data Element Framework (UDEF) was a controlled vocabulary developed by The Open Group. It provided a framework for categorizing, naming, and indexing data. It assigned to every item of data a structured alphanumeric tag plus a controlled vocabulary name that describes the meaning of the data. This allowed relating data elements to similar elements defined by other organizations.\n\nUDEF defined a dewey-decimal like code for each concept. For example, an \"employee number\" is often used in human resource management. It has a UDEF tag a.5_12.35.8 and a controlled vocabulary description \"Employee.PERSON_Employer.Assigned.IDENTIFIER\". \n\nIn an application used by a hospital, the last name and first name of several people could include the following example concepts:\n\n\nFor the examples above, the following UDEF IDs are available:\n\n\n\n"}
{"id": "38193228", "url": "https://en.wikipedia.org/wiki?curid=38193228", "title": "White–Juday warp-field interferometer", "text": "White–Juday warp-field interferometer\n\nThe White–Juday warp-field interferometer is an experiment designed to detect a microscopic instance of a warping of spacetime. If such a warp is detected, it is hoped that more research into creating an Alcubierre warp bubble will be inspired. A research team led by Harold \"Sonny\" White in collaboration with Dr. Richard Juday at the NASA Johnson Space Center and Dakota State University are conducting experiments, but results so far have been inconclusive.\n\nThe NASA research team led by Harold White and their university partners currently aim to experimentally evaluate several concepts, especially a redesigned energy-density topology, as well as an implication of brane cosmology theory. If space actually were to be embedded in higher dimensions, the energy requirements could be decreased dramatically and a comparatively small energy density could already lead to a measurable (i.e. using an interferometer) curvature of spacetime. The theoretical framework for the experiment dates back to work by Harold White from 2003, as well as work by White and Eric W. Davis from 2006 that was published in the AIP, where they also consider how baryonic matter could, at least mathematically, adopt characteristics of dark energy (see section below). In the process, they described how a toroidal positive energy density may result in a spherical negative-pressure region, possibly eliminating the need for actual exotic matter.\n\nThe metric derived by Alcubierre was mathematically motivated by cosmological inflation.\n\nThe original device proposed by White after finding the energy-decreasing possibilities (see theoretical framework) is a modified Michelson interferometer that uses a λ = 633 nm beam from a helium–neon laser. The beam is split into two paths, with the space-warping device placed in or near one beam path. The space warp would induce a relative phase shift between the split beams that should be detectable, provided that the magnitude of the phase shift created by the change in apparent path length is sufficient. By using 2D analytic signal processing, the magnitude and phase of the field can be extracted for study and comparison to theoretical models. The researchers first tried to see whether the space warping by the electric-field energy of a high-voltage (up to 20 kV) ring (0.5 cm radius) of high-κ barium titanate ceramic capacitors could be detected. After the first tests the experiment was moved to a seismically isolated lab due to very high interference caused by people walking outside the room. The goals circa 2013 were to increase sensitivity up to 1/100 of a wavelength and implement the oscillating field in order to get definite results.\n\nWhite announced the first results of his interferometer experiment at a 2013 space conference. According to White, these results showed a vanishing but non-zero difference between charged and uncharged states after signal processing, but this difference remains inconclusive due to external interference and limits in the computational processing. It is now clear that no exotic matter is involved in such an experiment, but some other concept is being used.\n\nDuring the first two weeks of April 2015, scientists fired lasers through a microwave cavity and noticed highly significant variations in the path time. The readings indicated that some of the laser pulses traveled longer, possibly pointing to a slight warp bubble inside the resonance chamber of the device. However, a small rise in ambient air temperature inside the chamber was also recorded, which could possibly have caused the recorded fluctuation in speeds of the laser pulses. According to Paul March, a NASA JSC researcher, the experiment was to be verified inside a vacuum chamber to remove all interference of air. This was done at the end of April 2015. White does not think, however, that the measured change in path length is due to transient air heating, because the visibility threshold is 40 times larger than the predicted effect from air.\n\nThe experiment used a short, cylindrical, aluminum resonant cavity (and not a tapered resonant cavity or EmDrive) excited at a natural frequency of 1.48 GHz with an input power of 30 watts, over 27000 cycles of data (each 1.5 s cycle energizing the system for 0.75 s and de-energizing it for 0.75 s), which were averaged to obtain a power spectrum that revealed a signal frequency of 0.65 Hz with amplitude clearly above system noise. Four additional tests were successfully conducted that demonstrated repeatability.\n\nThe NASA research team has postulated that their findings could reduce the energy requirements for a spaceship moving at ten times the speed of light (\"warp 2\") from the mass–energy equivalent of the planet Jupiter to that of the Voyager 1 spacecraft (~700 kg) or less.\n\nBy harnessing the physics of cosmic inflation, future spaceships crafted to satisfy the laws of these mathematical equations might actually be able to get somewhere unthinkably fast and without adverse effects.\n\nPhysicist and EarthTech CEO Harold E. Puthoff has explained that, contrary to widespread belief, even the highly blue-shifted light seen on board such a spaceship would not fry its crew, being bathed in strong UV light and X-rays. It would, however, be dangerous to anyone seeing it fly by closely.\n\nResearch on this device and other proposed devices is notable as the original newsletter from the NASA center and later presentations at a NASA conference detailed NASA funding of research in advanced concepts and in this particular case in the work proposed by Miguel Alcubierre, physical effects that have potential applications to space travel. In addition, these news releases included the researchers' enthusiastic descriptions of the potentials with statements such as: \"... while this would be a very modest instantiation of the phenomenon, it would likely be a Chicago pile moment for this area of research...\" Several space technology newsletters and space society organizations have further publicized these claims. Keith Cowing of the blog NASA Watch questions the oversight of this line of research by NASA and requested an explanation. Another journalist says that although a practical warp drive is a long way off, serious efforts to learn more about it are being undertaken now. At the second 100 Year Starship Symposium, White told Space.com, \"We're trying to see if we can generate a very tiny instance of this in a tabletop experiment\", the project is a \"humble experiment\" but said it represents a promising first step: \"The findings I presented today change it from impractical to plausible and worth further investigation.\"\n\n\n"}
