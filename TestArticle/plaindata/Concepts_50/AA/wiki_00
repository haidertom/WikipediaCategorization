{"id": "509995", "url": "https://en.wikipedia.org/wiki?curid=509995", "title": "Abstract and concrete", "text": "Abstract and concrete\n\nAbstract and concrete are classifications that denote whether the object that a term describes has physical referents. Abstract objects have no physical referents, whereas concrete objects do. They are most commonly used in philosophy and semantics. Abstract objects are sometimes called abstracta (sing. abstractum) and concrete objects are sometimes called \"concreta\" (sing. \"concretum\"). An abstract object is an object that does not exist at any particular time or place, but rather exists as a type of thing—i.e., an idea, or abstraction. The term \"abstract object\" is said to have been coined by Willard Van Orman Quine. The study of abstract objects is called abstract object theory.\n\nThe type–token distinction identifies physical objects that are tokens of a particular type of thing. The \"type\" of which it is a part is in itself an abstract object. The abstract-concrete distinction is often introduced and initially understood in terms of paradigmatic examples of objects of each kind:\n\nAbstract objects have often garnered the interest of philosophers because they raise problems for popular theories. In ontology, abstract objects are considered problematic for physicalism and some forms of naturalism. Historically, the most important ontological dispute about abstract objects has been the problem of universals. In epistemology, abstract objects are considered problematic for empiricism. If abstracta lack causal powers or spatial location, how do we know about them? It is hard to say how they can affect our sensory experiences, and yet we seem to agree on a wide range of claims about them. \n\nSome, such as Edward Zalta and arguably, Plato in his Theory of Forms, have held that abstract objects constitute the defining subject matter of metaphysics or philosophical inquiry more broadly. To the extent that philosophy is independent of empirical research, and to the extent that empirical questions do not inform questions about abstracta, philosophy would seem especially suited to answering these latter questions. \n\nIn modern philosophy, the distinction between abstract and concrete was explored by Immanuel Kant and G. W. F. Hegel.\n\nGottlob Frege said that abstract objects, such as numbers, were members of a third realm, different from the external world or from internal consciousness. \n\nAnother popular proposal for drawing the abstract-concrete distinction contends that an object is abstract if it lacks any causal powers. A causal power has the ability to affect something causally. Thus, the empty set is abstract because it cannot act on other objects. One problem for this view is that it is not clear exactly what it is to have a causal power. For a more detailed exploration of the abstract-concrete distinction, follow the link below to the \"Stanford Encyclopedia\" article.\n\nJean Piaget uses the terms \"concrete\" and \"formal\" to describe the different types of learning. Concrete thinking involves facts and descriptions about everyday, tangible objects, while abstract (formal operational) thinking involves a mental process.\nRecently, there has been some philosophical interest in the development of a third category of objects known as the quasi-abstract. Quasi-abstract objects have drawn particular attention in the area of social ontology and documentality. Some argue that the over-adherence to the platonist duality of the concrete and the abstract has led to a large category of social objects having been overlooked or rejected as nonexisting because they exhibit characteristics that the traditional duality between concrete and abstract regards as incompatible. Specially, the ability to have temporal location, but not spatial location, and have causal agency (if only by acting through representatives). These characteristics are exhibited by a number of social objects, including states of the international legal system.\n\n"}
{"id": "27658583", "url": "https://en.wikipedia.org/wiki?curid=27658583", "title": "AeroVironment Global Observer", "text": "AeroVironment Global Observer\n\nThe AeroVironment Global Observer is a concept for a high-altitude, long endurance unmanned aerial vehicle, designed by AeroVironment (AV) to operate as a stratospheric geosynchronous satellite system with regional coverage.\n\nTwo Global Observer aircraft, each flying for up to a week at an altitude of , could alternate coverage over any area on the earth, providing a platform for communications relays, remote sensing, or long-term surveillance. In addition to flying above weather and above other conventional aircraft, operation at this altitude permits communications and sensor payloads on the aircraft to service an area on the surface of the earth up to in diameter, equivalent to more than of coverage. Global Observer may offer greater flexibility than a satellite and longer duration than conventional manned and unmanned aircraft.\n\nThe Global Observer Joint Capabilities Technology Demonstration (JCTD) program had the goal of helping solve the capability gap in persistent ISR and communications relay for the US military and homeland security. The Global Observer JCTD demonstrated a new stratospheric, extreme endurance UAS that could be transitioned for post-JCTD development, extended user evaluation, and fielding. The program was a joint effort with the U.S. Department of Defense, Department of Homeland Security, and AeroVironment that started in September 2007, to culminate in a Joint Operational Utility Assessment (JOUA) in 2011.\n\nThe program provided for the system development, production of two aircraft, development flight testing, and JOUA with ISR and communications relay payload. The flight testing and JOUA was conducted at the Air Force Flight Test Center at Edwards Air Force Base, California. The primary objectives of the Global Observer JCTD Program were:\n\n\n\n\nHigh-altitude, long endurance unmanned aerial vehicles, such as Global Observer, may enable several capabilities that enable rapid and effective actions or countermeasures:\n\nA Global Observer prototype, called \"Odyssey,\" flew in May 2005. It had a , one-third the size of the planned full-sized version, and ran solely on hydrogen fuel-cells powering electric motors that drove eight propellers, flying the aircraft for several hours. The JCTD started in September 2007. In August 2010, Aerovironment announced that the full-sized Global Observer wing had passed wing load testing. The 53 m (175 ft) all-composite wing, which comes in five sections and is designed to maximize wing strength while minimizing weight, had loads applied to it that approximated the maximum loads it is designed to withstand during normal flight, turbulence and maneuvers. In its third year of testing, the demonstrator had also undergone ground and taxi tests as well as taken a \"short hop\" lifting off the ground briefly during taxiing.\n\nThe Global Observer performed its first flight on 5 August 2010, taking off from Edwards AFB and reaching an altitude of for one hour. The flight was performed using battery power.The aircraft completed initial flight testing, consisting of multiple low-altitude flights, at Edwards AFB in August and September 2010. This phase used batteries to power the hybrid-electric aircraft and approximate full aircraft weight and center of gravity for flight control, performance, and responsiveness evaluation. Following this, the program team installed and ground tested the aircraft's hydrogen-fueled generator and liquid hydrogen fuel tanks which will power it for up to a week in the stratosphere.\n\nThe first flight of the Global Observer using hydrogen fuel occurred on 11 January 2011, reaching an altitude of for four hours. On 1 April 2011, Global Observer-1 (GO-1), the first aircraft to be completed, crashed 18 hours into its 9th test flight. AeroVironment said it was undergoing flight test envelope expansion and had been operating for nearly twice the endurance and at a higher altitude than previous flights when the crash occurred. At the time, the second aircraft developed as part of the JCTD program was nearing completion at a company facility; the $140 million program was originally scheduled for completion in late 2011, but the crash delayed this by a year. AeroVironment was looking for sources of incremental funding to provide a bridge between the demonstration and a future procurement program.\n\nIn December 2012, the Pentagon closed the development contract for the Global Observer, the reason being the crash in April 2011. The Global Observer was used as a technology demonstration, not a program for a functioning aircraft. In April 2013, the Pentagon stated that no service or defense agency had advocated for it to be a program. AeroVironment is currently in possession of the second prototype Global Observer. On 6 February 2014, AeroVironment announced that it had teamed with Lockheed Martin to sell the Global Observer to international customers. The partnership is focused around building \"atmospheric satellite systems\" around the UAV. The Global Observer may compete for orders with the Boeing Phantom Eye liquid hydrogen-powered long endurance UAV.\n\n\n\n"}
{"id": "34460605", "url": "https://en.wikipedia.org/wiki?curid=34460605", "title": "Andersen healthcare utilization model", "text": "Andersen healthcare utilization model\n\nThe Andersen Healthcare Utilization Model - is a conceptual model aimed at demonstrating the factors that lead to the use of health services. According to the model, usage of health services (including inpatient care, physician visits, dental care etc.) is determined by three dynamics: predisposing factors, enabling factors, and need. Predisposing factors can be characteristics such as race, age, and health beliefs. For instance, an individual who believes health services are an effective treatment for an ailment is more likely to seek care. Examples of enabling factors could be family support, access to health insurance, one's community etc. Need represents both perceived and actual need for health care services. The original model was developed by Ronald M. Andersen, a health services professor at UCLA, in 1968. The original model was expanded through numerous iterations and its most recent form models past the use of services to end at health outcomes and includes feedback loops.\n\nA major motivation for the development of the model was to offer measures of access. Andersen discusses four concepts within access that can be viewed through the conceptual framework. Potential access is the presence of enabling resources, allowing the individual to seek care if needed. Realized access is the actual use of care, shown as the outcome of interest in the earlier models. The Andersen framework also makes a distinction between equitable and inequitable access. Equitable access is driven by demographic characteristics and need whereas inequitable access is a result of social structure, health beliefs, and enabling resources.\n\nAndersen also introduces the concept of mutability of his factors. The idea here being that if a concept has a high degree of mutability (can be easily changed) perhaps policy would be justified in using its resources to do rather than a factor with low mutability. Characteristics that fall under demographics are quite difficult to change, however, enabling resources is assigned a high degree of mutability as the individual, community, or national policy can take steps to alter the level of enabling resources for an individual. For example, if the government decides to expand the Medicaid program an individual may experience an increase in enabling resources, which in turn may beget an increase in health services usage. The RAND Health Insurance Experiment (HIE) changed a highly mutable factor, out-of-pocket costs, which greatly changed individual rates of health services usage.\n\nThe initial behavior model was an attempt to study of why a family uses health services. However, due to the heterogeneity of family members the model focused on the individual rather than the family as the unit of analysis. Andersen also states that the model functions both to predict and explain use of health services.\n\nA second model was developed in the 1970s in conjunction with Aday and colleagues at the University of Chicago. This iteration includes systematic concepts of health care such as current policy, resources, and organization. The second generation model also extends the outcome of interest beyond utilization to consumer satisfaction.\n\nThe next generation of the model builds upon this idea by including health status (both perceived and evaluated) as outcomes alongside consumer satisfaction. Furthermore, this model include personal health practices as an antecedent to outcomes, acknowledging that it not solely use of health services that drives health and satisfaction. This model emphasizes a more public health approach of prevention, as advocated by Evans and Stoddart wherein personal health practices (i.e. smoking, diet, exercise) are included as a driving force towards health outcomes.\n\nThe 6th iteration of Andersen’s conceptual framework focuses on the individual as the unit of analysis and goes beyond health care utilization, adopting health outcomes as the endpoint of interest. This model is further differentiated from its predecessors by using a feedback loop to illustrate that health outcomes may affect aspects such as health beliefs, and need. It added genetic susceptibility as a predisposing determinant and quality of life as an outcome By using the framework’s relationships we can determine the directionality of the effect following a change in an individual’s characteristics or environment. For example, if one experiences an increase in need as a result of an infection, the Andersen model predicts this will lead to an increased use of services (all else equal). One potential change for a future iteration of this model is to add genetic information under predisposing characteristics. As genetic information becomes more readily available it seems likely this could impact health services usage, as well as health outcomes, beyond what is already accounted for in the current model.\n\nThe model has been criticized for not paying enough attention to culture and social interaction but Andersen argues this social structure is included in the \"predisposing characteristics\" component. Another criticism was the overemphasis of need and at the expense of health beliefs and social structure. However, Andersen argues need itself is a social construct. This is why need is split into perceived and evaluated. Where evaluated need represents a more measurable/objective need, perceived need is partly determined by health beliefs, such as whether or not people think their condition is serious enough to seek health services. Another limitation of the model is its emphasis on health care utilization or adopting health outcomes as a dichotomous factor, present or not present. Other help-seeking models also consider the type of help source, including informal sources. More recent work has taken help-seeking behaviors further, and more real-world, by including online and other non-face-to-face sources.\n"}
{"id": "188401", "url": "https://en.wikipedia.org/wiki?curid=188401", "title": "Axiomatic system", "text": "Axiomatic system\n\nIn mathematics, an axiomatic system is any set of axioms from which some or all axioms can be used in conjunction to logically derive theorems. A theory consists of an axiomatic system and all its derived theorems. An axiomatic system that is completely described is a special kind of formal system. A formal theory typically means an axiomatic system, for example formulated within model theory. A formal proof is a complete rendition of a mathematical proof within a formal system.\n\nAn axiomatic system is said to be \"consistent\" if it lacks contradiction, i.e. the ability to derive both a statement and its denial from the system's axioms.\n\nIn an axiomatic system, an axiom is called \"independent\" if it is not a theorem that can be derived from other axioms in the system. A system will be called independent if each of its underlying axioms is independent. Although independence is not a necessary requirement for a system, consistency usually is, but see neutrosophic logic.\n\nAn axiomatic system will be called \"complete\" if for every statement, either itself or its negation is derivable.\n\nBeyond consistency, relative consistency is also the mark of a worthwhile axiom system. This is when the undefined terms of a first axiom system are provided definitions from a second, such that the axioms of the first are theorems of the second.\n\nA good example is the relative consistency of neutral geometry or absolute geometry with respect to the theory of the real number system. Lines and points are undefined terms in absolute geometry, but assigned meanings in the theory of real numbers in a way that is consistent with both axiom systems.\n\nA model for an axiomatic system is a well-defined set, which assigns meaning for the undefined terms presented in the system, in a manner that is correct with the relations defined in the system. The existence of a concrete model proves the consistency of a system. A model is called concrete if the meanings assigned are objects and relations from the real world}, as opposed to an abstract model which is based on other axiomatic systems.\n\nModels can also be used to show the independence of an axiom in the system. By constructing a valid model for a subsystem without a specific axiom, we show that the omitted axiom is independent if its correctness does not necessarily follow from the subsystem.\n\nTwo models are said to be isomorphic if a one-to-one correspondence can be found between their elements, in a manner that preserves their relationship. An axiomatic system for which every model is isomorphic to another is called categorial (sometimes categorical), and the property of categoriality (categoricity) ensures the completeness of a system.\n\nStating definitions and propositions in a way such that each new term can be formally eliminated by the priorly introduced terms requires primitive notions (axioms) to avoid infinite regress. This way of doing mathematics is called the axiomatic method.\n\nA common attitude towards the axiomatic method is logicism. In their book \"Principia Mathematica\", Alfred North Whitehead and Bertrand Russell attempted to show that all mathematical theory could be reduced to some collection of axioms. More generally, the reduction of a body of propositions to a particular collection of axioms underlies the mathematician's research program. This was very prominent in the mathematics of the twentieth century, in particular in subjects based around homological algebra.\n\nThe explication of the particular axioms used in a theory can help to clarify a suitable level of abstraction that the mathematician would like to work with. For example, mathematicians opted that rings need not be commutative, which differed from Emmy Noether's original formulation. Mathematicians decided to consider topological spaces more generally without the separation axiom which Felix Hausdorff originally formulated.\n\nThe Zermelo-Fraenkel axioms, the result of the axiomatic method applied to set theory, allowed the \"proper\" formulation of set-theory problems and helped to avoid the paradoxes of naïve set theory. One such problem was the Continuum hypothesis. Zermelo–Fraenkel set theory with the historically controversial axiom of choice included is commonly abbreviated ZFC, where C stands for choice. Many authors use ZF to refer to the axioms of Zermelo–Fraenkel set theory with the axiom of choice excluded. Today ZFC is the standard form of axiomatic set theory and as such is the most common foundation of mathematics.\n\nMathematical methods developed to some degree of sophistication in ancient Egypt, Babylon, India, and China, apparently without employing the axiomatic method.\n\nEuclid of Alexandria authored the earliest extant axiomatic presentation of Euclidean geometry and number theory. Many axiomatic systems were developed in the nineteenth century, including non-Euclidean geometry, the foundations of real analysis, Cantor's set theory, Frege's work on foundations, and Hilbert's 'new' use of axiomatic method as a research tool. For example, group theory was first put on an axiomatic basis towards the end of that century. Once the axioms were clarified (that inverse elements should be required, for example), the subject could proceed autonomously, without reference to the transformation group origins of those studies.\n\nNot every consistent body of propositions can be captured by a describable collection of axioms. Call a collection of axioms recursive if a computer program can recognize whether a given proposition in the language is an axiom. Gödel's First Incompleteness Theorem then tells us that there are certain consistent bodies of propositions with no recursive axiomatization. Typically, the computer can recognize the axioms and logical rules for deriving theorems, and the computer can recognize whether a proof is valid, but to determine whether a proof exists for a statement is only soluble by \"waiting\" for the proof or disproof to be generated. The result is that one will not know which propositions are theorems and the axiomatic method breaks down. An example of such a body of propositions is the theory of the natural numbers. The Peano Axioms (described below) thus only partially axiomatize this theory.\n\nIn practice, not every proof is traced back to the axioms. At times, it is not clear which collection of axioms a proof appeals to. For example, a number-theoretic statement might be expressible in the language of arithmetic (i.e. the language of the Peano Axioms) and a proof might be given that appeals to topology or complex analysis. It might not be immediately clear whether another proof can be found that derives itself solely from the Peano Axioms.\n\nAny more-or-less arbitrarily chosen system of axioms is the basis of some mathematical theory, but such an arbitrary axiomatic system will not necessarily be free of contradictions, and even if it is, it is not likely to shed light on anything. Philosophers of mathematics sometimes assert that mathematicians choose axioms \"arbitrarily\", but it is possible that although they may appear arbitrary when viewed only from the point of view of the canons of deductive logic, that appearance is due to a limitation on the purposes that deductive logic serves.\n\nThe mathematical system of natural numbers 0, 1, 2, 3, 4, ... is based on an axiomatic system first written down by the mathematician Peano in 1889. He chose the axioms, in the language of a single unary function symbol \"S\" (short for \"successor\"), for the set of natural numbers to be:\n\n\nIn mathematics, axiomatization is the formulation of a system of statements (i.e. axioms) that relate a number of primitive terms in order that a consistent body of propositions may be derived deductively from these statements. Thereafter, the proof of any proposition should be, in principle, traceable back to these axioms.\n\n\n"}
{"id": "38191512", "url": "https://en.wikipedia.org/wiki?curid=38191512", "title": "Concept-driven strategy", "text": "Concept-driven strategy\n\nA concept-driven strategy is a process for formulating strategy that draws on the explanation of how humans inquire provided by linguistic pragmatic philosophy. This argues that thinking starts by selecting (explicitly or implicitly) a set of concepts (frames, patterns, lens, principles, etc.) gained from our past experiences. These are used to reflect on whatever happens, or is done, in the future.\n\nConcept-driven strategy therefore starts from agreeing and enacting a set of strategic concepts (organizing principles) that \"works best\" for an organisation. For example, a hospital might set its strategy as intending to be Caring, World Class, Local, Evidence Based, and Team Based. A University might set its strategy as intending to be Ranked, Problem Solving, Online, Equis, and Offering Pathways. A commercial corporation might set its strategy as intending to be Innovative, Global, Have Visible Supply Chains, Agile and Market Share Dominant. These strategic concepts make up its \"Statement of Intent\" (or Purpose).\n\nMuch of the strategic management literature mutates Peter Drucker's call for corporations to start the strategic management process by producing a statement of purpose, mission and objectives. This has been mutated into a call to start with a vision, mission and objectives statement. There is an alternative approach which focuses on the Statement of Purpose or Intent. Drucker's example for this statement for a commercial corporation was to state that the corporation's purpose was to create customers. That is, it was going to use the concept of 'customer creation' to coordinate and organise the cognition or mindset of those that worked for the organisation. This was why the corporation existed. Having one concept is now thought to be insufficient. George Armitage Miller's modified The Magical Number Seven, Plus or Minus Two and dialectic suggests a handful of concepts under tension would be preferable.\n\nThe Statement of Purpose, Statement of Intent or concept-driven approach to strategy formulation therefore focuses on setting and enacting a set strategic concepts. If a participatory approach is being used these concepts will be acquired through a process of collaboration with stakeholders. Once agreed the strategic concepts can be used to coordinate activities and act as a set of decision making criteria. The set of concepts that make up the Statement of Intent is then used to make sense of an unpredictable future across an organisation in a co-ordinated manner.\n\nLinguistic pragmatism argues that our prior conceptions interpret our perception (sensory inputs). These conceptions are represented by concepts like running, smiling, justice, reasoning and agility. They are patterns of activity, experienced in our past and remembered. They can be named by those with language and so shared.\n\nBagginni explains pragmatic concepts using the classic example of whether the earth is flat or round.\n\nAnother example would be that we can think of the war in Iraqi differently by reflecting off the concepts of oil security, Imperialism, aggressive capitalism, liberation or democracy. \nThe concept-driven approach to strategy formulation involves setting and using a set of linguistic pragmatic concepts.\n\nThe steps to formulating a participatory concept-driven strategy are:\n\n\nConcept-driven strategy is the name given to a number of similar strategic thinking approaches.\n\nGenerally, the term 'concept-driven' is used to encourage a focus on the 'concepts' being used. See Concept learning or Management Concepts.\n\nSome organisations produce a 'statement of intent' with little thought as to the concepts it contains. However, if it is a short list of concepts, high level objectives, principles, priorities or frames, then concept-driven strategy offers a philosophical basis for these statements.\n\nSome organisations produce a 'strategic principles' statement which again is similar to a statement of intent and the same applies about the concepts approach offering a philosophical basis. The term 'strategic priorities' or 'strategic values' are often used in the same way as strategic principles.\n\nThe literature about 'corporate purpose' is also similar to that of strategic intent. Sometimes, purpose refers to present actions and intent to future ones. If purpose is expressed as a set of concepts, then the concepts approach again provides some philosophical basis.\n\nThere is a connection between 'systems thinking' and concept-driven strategy. The Churchman/Ackoff stream of systems thinking was interested in a developing generic system of concepts for thinking about problems. Rather than a generic set of concepts, the concept-driven approach uses whatever concepts stakeholders think work best for the future of their organisation.\n\nThere is a military planning approach called 'concept-led'. The military-like leadership seems to have moved the concepts from being drivers to be leaders. There seems to be very little difference otherwise.\n\nIn turbulent environments, concepts are thought 'more flexible than objectives' (goals, targets) as they provide why certain actions are preferable. The purpose and intent literature likes to distinguish itself from the objectives literature by saying purpose and intent provide the reasons for (why change), the driver for change. Objectives are where you end up. In complex dynamic situations, there may be many acceptable end points, many of which cannot be anticipated by planners. Arguably the only objective is to survive. How is explained in the statement of intent.\n\nPerhaps strangely, there is a connection between 'metaphor', metaphoric criticism, or conceptual metaphor and concept-driven strategy. Pragmatic concepts are not images but most concepts relate to metaphors. For example, to say an organisation is like a machine, with cogs, or like an adaptive organism, is to use the concepts of machine and organism to reflect on organisations. Much of what has been written about the usefulness of metaphors in planning applies to concepts.\n\nThe term 'strategic frames' is not common given the extensive literature on frame analysis but frames and pragmatic concepts seem to be very similar. Amos Tversky defines a frame as a conception of outcomes.\n\nThe system of strategic concepts listed in a statement of intent, purpose, principles, frames or conceptual metaphor are organizing principle(s).\n\nAlso, as Karl Weick explains sensemaking as the process of conceptualising problems, concept-driven strategy might be thought of as a pragmatic means of sensemaking a strategy.\n\n\n"}
{"id": "3699685", "url": "https://en.wikipedia.org/wiki?curid=3699685", "title": "Conceptual architecture", "text": "Conceptual architecture\n\nConceptual architecture is a form of architecture that utilizes conceptualism, characterized by an introduction of ideas or concepts from outside of architecture often as a means of expanding the discipline of architecture. This produces an essentially different kind of building than one produced by the widely held 'architect as a master-builder' model, in which craft and construction are the guiding principles. The finished building as product is less important in conceptual architecture, than the ideas guiding them, ideas represented primarily by texts, diagrams, or art installations. Architects that work in this vein are Diller + Scofidio, Bernard Tschumi, Peter Eisenman and Rem Koolhaas.\n\nConceptual architecture was studied in the essay, \"Notes on Conceptual Architecture: Towards a Definition\" by Peter Eisenman in 1970, and again by the Harvard Design Magazine in Fall of 2003 and Winter 2004, by a series of articles under the heading \"Architecture as Conceptual Art\". But the understanding of design as a construction of a concept was understood by many modernist architects as well. To quote Louis Kahn on Frank Lloyd Wright:\n\n\n"}
{"id": "42415226", "url": "https://en.wikipedia.org/wiki?curid=42415226", "title": "Conceptual combination", "text": "Conceptual combination\n\nConceptual combination is a fundamental cognitive process by which two or more existing basic concepts are mentally synthesized to generate a composite, higher-order concept. The products of this process are sometimes referred to as \"complex concepts.\" Combining concepts allows individuals to use a finite number of concepts which they already understand to construct a potentially limitless quantity of new, related concepts. It is an essential component of many abilities, such as perception, language, synthetic reasoning, creative thought and abstraction.\n\nConceptual combination is an important concept in the fields of cognitive psychology and cognitive science.\n\nThe mechanism by which conceptual combination occurs is debatable, both on cognitive and neurological levels. As such, multiple models have been developed or applied to better define how the process occurs.\n\nCognitive models attempt to functionally outline the mental computation involved in conceptual combination. \n\nConstraint theory stipulates that the concept that results from an attempt at conceptual combination is controlled by three constraints known as diagnosticity, plausibility and informativeness. \"Diagnosticity\" refers to the a complex concept's possession of the defining properties of its component simple concepts. Because such properties are diagnostic of the component concepts, at least some of them should be diagnostic of the higher-order representations constructed from those concepts. \"Plausibility\" refers to consistency with existing knowledge and prior experience. It is based on the assumption that a complex concept should be reasonably relevant to the context in which it is used. This assumption makes the most sense in a practical, linguistic context, particularly when a speaker is catering to the understanding of the listener. \"Informativeness\" is the property of having more meaning or properties than any individual component. If a complex concept were not distinguishable from any given component, it would be identical to that component. Because nothing can logically be both a component of something and the totality of something simultaneously, a complex concept must at least be the sum of its parts. Many argue that the interaction among component concepts should allow a complex concept to be greater than that sum. If multiple possible ways to structure or interpret a complex concept exist, the one which best satisfies or most satisfies these constraints is the one which will be used. The paradigm upon which constraint theory is based is computational, and therefore views the mind as a processor which operates on the basis of standard problem-solving protocols (i.e. algorithms and heuristics).\n\nThe spreading activation model is a model in connectionist theory sometimes designed to represent how concepts are activated in relation to one another. Though it is typically applied to information search processes like recognition, brainstorming, and recall, it can be used to explain how concepts are combined as well as connected.\n\nSpreading activation models represent memory and knowledge as a network of interlinked concepts. Every concept manifests as a node within this network, with related nodes/concepts linked to one another with connections. Concepts that are more strongly associated with one another either in terms of content or an individual's past experience are correspondingly more strongly linked.\n\nWhen one concept is employed in working memory, the corresponding node is also activated. This activation spreads through the node's links, making it easier to activate nodes to which the activated node is connected. This spreading activation stimulates the linked nodes, pressuring them to activate to an extent proportional to the strength of the connection between the stimulating node and the stimulated node. If sufficient net stimulation is accrued through a stimulated node's links, it will also activate. Thus, being connected to an activated node makes it easier for an inactive node to become active as well; concepts become more readily accessed when individuals are stimulated with related concepts first. This increase in ease of access is known as \"priming.\"\n\nSpreading activation models tend to imply that processing concepts occurs in series; that is, each concept is processed one-at-a-time, one after the other. As such, individuals tend to combine concepts more readily, easily, and quickly if they are more closely linked within the network of concepts. This implication, however, has caused spreading activation to come under a great deal of criticism, particularly with respect to how the concept is employed in feature theories.\n\nThe features and properties of complex concepts are generally assumed to be derived from the simple concepts that compose them. One popularly discussed model involves a two-stage serial process. In the initial stage, features from each of the component simple concepts are retrieved from memory through spreading activation. This allows a complex concept to accrue features with existing relationships with its component simple concepts. During this stage, the basic definition of what the complex concept is and/or means is generates. In the second stage, knowledge and reasoning are employed upon the features accrued in the previous stage to generate further features. For example, one might reason that the complex concept \"white jacket,\" if worn in a blizzard, would make one difficult to see; it would follow that one should ascribe the property of \"good for winter camouflage,\" despite the fact that this property is not closely attached to the component concepts \"white\" nor \"jacket.\" This stage is especially useful when properties of complex concepts contradict those of their component concepts, such as the different colours of milk and chocolate milk.\n\nThis model, however, has come under criticism due to its inconsistency with empirical studies. If conceptual combination employed serial spreading activation, for instance, it should take longer to verify the properties of complex concepts, as they necessarily possess more concepts than their component simple concepts. Research has nonetheless shown that it takes less time to confirm complex concepts' properties than their components and about equal time to reject false properties for both. This occurred even when the properties of the complex concept contradicted those of the simple concepts. Likewise, when experiment participants are exposed to a set of features first, and then asked to verify whether or not they correspond to simple or complex concepts, the participants tend to provide correct verification answers for the complex concepts more quickly.\n\nThe neurological basis of conceptual combination has received considerably less attention than its cognitive basis. Nevertheless, research has revealed several specific brain regions that are intimately involved if not entirely responsible for neural processing involved in conceptual combination.\n\nOf particular relevance is the left anterior temporal lobe. Studies have previously demonstrated an additive effect for stimulation in this subsection of neural cortex tissue. When experiment participants were verbally presented with certain simple concepts, the processing of the information causes electrical stimulation in the region. When the same participants were verbally presented with a single complex concept formed from the combination of the aforementioned simple concepts, the stimulation recorded was equivalent to the sum of the stimulation that resulted from each individual component simple concept. In other words, the stimulation caused by a complex concept is equivalent to the total stimulation caused by its component concepts. More recent data contradicts those results by indicating a multiplicative effect in which the activation caused by a complex concept is the product of the activation levels caused by its component concepts, rather than the sum.\n\nFurther support for the role of the left anterior temporal lobe has been previously established through neuropsychological studies. Semantic dementia is a disorder in which conceptual manipulation, including conceptual combination, is hindered. These indicate that the neural damage associated with semantic dementia occurs within this brain region. Unfortunately, neuropsychological studies that attempt to replicate this pattern have failed, leading uncertainty as to whether initial results were valid.\n\nAs language is the means by which concepts are communicated and expressed, the processed involved in linguistic expression and interpretation are heavily intertwined with combined concepts. Many theories of concept combination mechanisms, including constraint theory were developed within the context of language, and therefore make more sense when applied in a linguistic context. Study into the linguistic aspects of concept combination as has generally been focused on the interpretation mechanism.\n\nA concept that can be expressed using a single word is called a \"lexical concept.\" A lexical concept is usually treated as a basic concept, although it can just as easily be a complex concept.\n\nTwo lexical concepts are often used together as phrases to represent a combined concept of greater specificity. This is most readily seen in the use of adjectives to modify nouns and the use of adverbs to modify verbs and adjectives. Consider, for example, phrases such as \"burnt toast,\" \"eat roughly,\" and \"readily loved.\" Multiple noun lexical concepts can also be used together in order to represent combined concepts. Through this process, a limited pool of nouns can be used to produce an exponentially larger pool of phrases such as \"sound wave,\" \"video game,\" and \"sleeping pill.\"\n\nIn addition to constraint theory, there are two principal theories surrounding the mechanism by which noun-noun combinations are interpreted. The first of these is \"dual-process theory.\" Dual-process theory proposed that there are two means by which people interpreted noun-noun phrases. \"Relational interpretation\" attempts to establish a relationship between the nouns and interprets the combined phrase in terms of that relationship. For example, one might relationally interpret the phrase \"snake mouse\" to refer to a mouse meant to be eaten by snakes, as the two concepts have a predatory relationship. \"Property interpretation\" identifies properties associated with the first noun and then applies them onto the second noun. In this case the phrase \"snake mouse\" might be interpreted as a mouse with poisonous fangs or an elongated body.\n\nThe second principal theory is known as the \"Competition in Relations among Nominals\" theory. It states that the assumed modification effect of a noun on its partner in a novel noun-noun combination is the one which it has been seen to employ most often in the past. For example, \"chocolate cat\" is usually interpreted as \"a cat made of chocolate\" rather than \"a chocolate-eating cat\" simply because the \"made of\" modifier is heavily conditioned to be associated with \"chocolate.\"\n\nExplanations of linguistic expression of complex concepts have been linked to spreading activation models. When an individual identifies a lexical concept through vision or hearing, the corresponding node in that individual's cognitive network is said to activate. This makes it easier for lexical concepts linked to the activated concept to be comprehended, as they are primed. This is consistent with current empirical data, which shows that when individuals are interpreting sentences, they process the linguistic content more quickly when several related words follow one another. In turn, it becomes easier for people to combine these related concepts together and understand them as a relationship, rather than two distinct entities. For example, consider the example, \"John spread butter on a bagel.\" In this sentence, the lexical concepts \"spread,\" \"butter,\" and \"bagel\" are associated with one another and easy to combine into a mental representation of a breakfast scenario. Conversely, consider the example, \"John baked a computer.\" Because \"baked\" and \"computer\" are not related lexical concepts, it takes more effort and time to build a mental representation of this unusual scenario.\n\nHowever, spreading activation models of conceptual combination have been criticized in light of how humans have been observed to combined languages. Those who claim that the theory provides an insufficient account of linguistic conceptual combination refer to the ability of humans to readily understand lexical concept combinations with seemingly no apparent connection with one another. One example of this would be the sentence \"John saw an elephant cloud.\" \"Elephant\" and \"cloud\" do not shared a close association, but it takes little effort to comprehend that the term \"elephant cloud\" refers to a cloud shaped like an elephant. This has led some to conclude that the combination of lexical concepts does not wholly rely on the simultaneous activation of linked lexical concepts alone. Rather, they claim that the process involves the use of existing nodes to generate entirely new concepts independent of their parent concepts.\n\nAlthough many theories of novel noun-noun combination interpretation ignore the effect of social environment, some theorists have attempted to account for any contingencies social context may cause.\n\nWhen lexical concept combinations are interpreted without the influence of social context, the interpretation carried out is termed \"sense generation.\" This includes all processes that would normally occur excepting those dependent on a conversation partner. The \"generation hypothesis\" accordingly states that the interpretation mechanism of a noun-noun combination is essentially the same regardless of context. This does not rule out the possibility that social context can affect sense generation in some way, but it does assert that the basic structure of the process is unaffected. As seen above, debate as to what sense generation entails and how many sub-processes into which it should be divided is a contentious matter in cognitive science.\n\nThe \"anaphor resolution hypothesis\" instead asserts that before sense generation occurs, interpreters first search their memory of recent communication to see if the combination refers to something previously discussed. This process is termed \"anaphor resolution'.' If a referent is identified, interpretation occurs without sense generation in light of that referent. Even if an explicit referent does not exist, anaphor resolution can help facilitate sense generation by providing more information that might hint at the combination's intended meaning.\n\nThe \"dual-process hypothesis\" not to be confused with dual-process theory, states that sense generation and anaphor resolution occur in parallel. Both processes begin to work once the noun-noun combination is presented. Proponents of this hypothesis disagree as to how the interpretation is eventually resolves. Some believe that whichever process reaches a conclusion first provides the answer. Others believe that both provide continuous input to a third, mediating process that eventually makes a decision based on input from both.\n\nCreativity necessitates the employment of existing concepts in novel ways, and therefore requires conceptual combination. Surprisingly, this contribution seems to be limited. Conceptual combination is a significant contributor to convergent thinking, but not divergent thinking. For example, practice with generating new concepts through combination does not improve brainstorming. It does, however, assist in devising creative problem solving methods.\n\nThe psychological community's growing understanding of how concepts are manipulated has allowed educators to teach new concepts more effectively. Tools that are developed based on conceptual combination theory attempt to teach individual tasks, and then challenge students to exercise them together in order to promote both base subject skills and the critical thinking needed to apply them simultaneously to solve new problems. Máder & Vajda, for instance, developed a three-dimensional grid with cells of adjustable height which has been successfully used in numerous activities capable of improving the effectiveness of high school mathematics education.\n"}
{"id": "2381958", "url": "https://en.wikipedia.org/wiki?curid=2381958", "title": "Conceptual model", "text": "Conceptual model\n\nA conceptual model is a representation of a system, made of the composition of concepts which are used to help people know, understand, or simulate a subject the model represents. It is also a set of concepts. Some models are physical objects; for example, a toy model which may be assembled, and may be made to work like the object it represents.\n\nThe term \"conceptual model\" may be used to refer to models which are formed after a conceptualization or generalization process. Conceptual models are often abstractions of things in the real world whether physical or social. Semantic studies are relevant to various stages of concept formation. Semantics is basically about concepts, the meaning that thinking beings give to various elements of their experience.\n\nThe term \"conceptual model\" is normal. It could mean \"a model of concept\" or it could mean \"a model that is conceptual.\" A distinction can be made between \"what models are\" and \"what models are made of\". With the exception of iconic models, such as a scale model of Winchester Cathedral, most models are concepts. But they are, mostly, intended to be models of real world states of affairs. The value of a model is usually directly proportional to how well it corresponds to a past, present, future, actual or potential state of affairs. A model of a concept is quite different because in order to be a good model it need not have this real world correspondence. In artificial intelligence conceptual models and conceptual graphs are used for building expert systems and knowledge-based systems; here the analysts are concerned to represent expert opinion on what is true not their own ideas on what is true.\n\nConceptual models (models that are conceptual) range in type from the more concrete, such as the mental image of a familiar physical object, to the formal generality and abstractness of mathematical models which do not appear to the mind as an image. Conceptual models also range in terms of the scope of the subject matter that they are taken to represent. A model may, for instance, represent a single thing (e.g. the \"Statue of Liberty\"), whole classes of things (e.g. \"the electron\"), and even very vast domains of subject matter such as \"the physical universe.\" The variety and scope of conceptual models is due to then variety of purposes had by the people using them.\nConceptual modeling is the activity of formally describing some aspects of the physical and social world around us for the purposes of understanding and communication.\"\n\nA conceptual model's primary objective is to convey the fundamental principles and basic functionality of the system which it represents. Also, a conceptual model must be developed in such a way as to provide an easily understood system interpretation for the models users. A conceptual model, when implemented properly, should satisfy four fundamental objectives.\n\n\nThe conceptual model plays an important role in the overall system development life cycle. Figure 1 below, depicts the role of the conceptual model in a typical system development scheme. It is clear that if the conceptual model is not fully developed, the execution of fundamental system properties may not be implemented properly, giving way to future problems or system shortfalls. These failures do occur in the industry and have been linked to; lack of user input, incomplete or unclear requirements, and changing requirements. Those weak links in the system design and development process can be traced to improper execution of the fundamental objectives of conceptual modeling. The importance of conceptual modeling is evident when such systemic failures are mitigated by thorough system development and adherence to proven development objectives/techniques.\n\nAs systems have become increasingly complex, the role of conceptual modelling has dramatically expanded. With that expanded presence, the effectiveness of conceptual modeling at capturing the fundamentals of a system is being realized. Building on that realization, numerous conceptual modeling techniques have been created. These techniques can be applied across multiple disciplines to increase the users understanding of the system to be modeled. A few techniques are briefly described in the following text, however, many more exist or are being developed. Some commonly used conceptual modeling techniques and methods include: workflow modeling, workforce modeling, rapid application development, object-role modeling, and the Unified Modeling Language (UML).\n\nData flow modeling (DFM) is a basic conceptual modeling technique that graphically represents elements of a system. DFM is a fairly simple technique, however, like many conceptual modeling techniques, it is possible to construct higher and lower level representative diagrams. The data flow diagram usually does not convey complex system details such as parallel development considerations or timing information, but rather works to bring the major system functions into context. Data flow modeling is a central technique used in systems development that utilizes the structured systems analysis and design method (SSADM).\n\nEntity-relationship modeling (ERM) is a conceptual modeling technique used primarily for software system representation. Entity-relationship diagrams, which are a product of executing the ERM technique, are normally used to represent database models and information systems. The main components of the diagram are the entities and relationships. The entities can represent independent functions, objects, or events. The relationships are responsible for relating the entities to one another. To form a system process, the relationships are combined with the entities and any attributes needed to further describe the process. Multiple diagramming conventions exist for this technique; IDEF1X, Bachman, and EXPRESS, to name a few. These conventions are just different ways of viewing and organizing the data to represent different system aspects.\n\nThe event-driven process chain (EPC) is a conceptual modeling technique which is mainly used to systematically improve business process flows. Like most conceptual modeling techniques, the event driven process chain consists of entities/elements and functions that allow relationships to be developed and processed. More specifically, the EPC is made up of events which define what state a process is in or the rules by which it operates. In order to progress through events, a function/ active event must be executed. Depending on the process flow, the function has the ability to transform event states or link to other event driven process chains. Other elements exist within an EPC, all of which work together to define how and by what rules the system operates. The EPC technique can be applied to business practices such as resource planning, process improvement, and logistics.\n\nThe dynamic systems development method uses a specific process called JEFFF to conceptually model a systems life cycle. JEFFF is intended to focus more on the higher level development planning that precedes a projects initialization. The JAD process calls for a series of workshops in which the participants work to identify, define, and generally map a successful project from conception to completion. This method has been found to not work well for large scale applications, however smaller applications usually report some net gain in efficiency.\n\nAlso known as Petri nets, this conceptual modeling technique allows a system to be constructed with elements that can be described by direct mathematical means. The petri net, because of its nondeterministic execution properties and well defined mathematical theory, is a useful technique for modeling concurrent system behavior, i.e. simultaneous process executions.\n\nState transition modeling makes use of state transition diagrams to describe system behavior. These state transition diagrams use distinct states to define system behavior and changes. Most current modeling tools contain some kind of ability to represent state transition modeling. The use of state transition models can be most easily recognized as logic state diagrams and directed graphs for finite-state machines.\n\nBecause the conceptual modeling method can sometimes be purposefully vague to account for a broad area of use, the actual application of concept modeling can become difficult. To alleviate this issue, and shed some light on what to consider when selecting an appropriate conceptual modeling technique, the framework proposed by Gemino and Wand will be discussed in the following text. However, before evaluating the effectiveness of a conceptual modeling technique for a particular application, an important concept must be understood; Comparing conceptual models by way of specifically focusing on their graphical or top level representations is shortsighted. Gemino and Wand make a good point when arguing that the emphasis should be placed on a conceptual modeling language when choosing an appropriate technique. In general, a conceptual model is developed using some form of conceptual modeling technique. That technique will utilize a conceptual modeling language that determines the rules for how the model is arrived at. Understanding the capabilities of the specific language used is inherent to properly evaluating a conceptual modeling technique, as the language reflects the techniques descriptive ability. Also, the conceptual modeling language will directly influence the depth at which the system is capable of being represented, whether it be complex or simple.\n\nBuilding on some of their earlier work, Gemino and Wand acknowledge some main points to consider when studying the affecting factors: the content that the conceptual model must represent, the method in which the model will be presented, the characteristics of the models users, and the conceptual model languages specific task. The conceptual models content should be considered in order to select a technique that would allow relevant information to be presented. The presentation method for selection purposes would focus on the techniques ability to represent the model at the intended level of depth and detail. The characteristics of the models users or participants is an important aspect to consider. A participant's background and experience should coincide with the conceptual models complexity, else misrepresentation of the system or misunderstanding of key system concepts could lead to problems in that systems realization. The conceptual model language task will further allow an appropriate technique to be chosen. The difference between creating a system conceptual model to convey system functionality and creating a system conceptual model to interpret that functionality could involve to completely different types of conceptual modeling languages.\n\nGemino and Wand go on to expand the affected variable content of their proposed framework by considering the focus of observation and the criterion for comparison. The focus of observation considers whether the conceptual modeling technique will create a \"new product\", or whether the technique will only bring about a more intimate understanding of the system being modeled. The criterion for comparison would weigh the ability of the conceptual modeling technique to be efficient or effective. A conceptual modeling technique that allows for development of a system model which takes all system variables into account at a high level may make the process of understanding the system functionality more efficient, but the technique lacks the necessary information to explain the internal processes, rendering the model less effective.\n\nWhen deciding which conceptual technique to use, the recommendations of Gemino and Wand can be applied in order to properly evaluate the scope of the conceptual model in question. Understanding the conceptual models scope will lead to a more informed selection of a technique that properly addresses that particular model. In summary, when deciding between modeling techniques, answering the following questions would allow one to address some important conceptual modeling considerations.\n\n\nAnother function of the simulation conceptual model is to provide a rational and factual basis for assessment of simulation application appropriateness.\n\nIn cognitive psychology and philosophy of mind, a mental model is a representation of something in the mind, but a mental model may also refer to a nonphysical external model of the mind itself.\n\nA metaphysical model is a type of conceptual model which is distinguished from other conceptual models by its proposed scope; a metaphysical model intends to represent reality in the broadest possible way. This is to say that it explains the answers to fundamental questions such as whether matter and mind are one or two substances; or whether or not humans have free will.\n\nAn epistemological model is a type of conceptual model whose proposed scope is the known and the knowable, and the believed and the believable.\n\nIn logic, a model is a type of interpretation under which a particular statement is true. Logical models can be broadly divided into ones which only attempt to represent concepts, such as mathematical models; and ones which attempt to represent physical objects, and factual relationships, among which are scientific models.\n\nModel theory is the study of (classes of) mathematical structures such as groups, fields, graphs, or even universes of set theory, using tools from mathematical logic. A system that gives meaning to the sentences of a formal language is called a model for the language. If a model for a language moreover satisfies a particular sentence or theory (set of sentences), it is called a model of the sentence or theory. Model theory has close ties to algebra and universal algebra.\n\nMathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures.\n\nA more comprehensive type of mathematical model uses a linguistic version of category theory to model a given situation. Akin to entity-relationship models, custom categories or sketches can be directly translated into database schemas. The difference is that logic is replaced by category theory, which brings powerful theorems to bear on the subject of modeling, especially useful for translating between disparate models (as functors between categories).\n\nA scientific model is a simplified abstract view of a complex reality. A scientific model represents empirical objects, phenomena, and physical processes in a logical way. Attempts to formalize the principles of the empirical sciences use an interpretation to model reality, in the same way logicians axiomatize the principles of logic. The aim of these attempts is to construct a formal system for which reality is the only interpretation. The world is an interpretation (or model) of these sciences, only insofar as these sciences are true.\n\nA statistical model is a probability distribution function proposed as generating data. In a parametric model, the probability distribution function has variable parameters, such as the mean and variance in a normal distribution, or the coefficients for the various exponents of the independent variable in linear regression. A nonparametric model has a distribution function without parameters, such as in bootstrapping, and is only loosely confined by assumptions. Model selection is a statistical method for selecting a distribution function within a class of them; e.g., in linear regression where the dependent variable is a polynomial of the independent variable with parametric coefficients, model selection is selecting the highest exponent, and may be done with nonparametric means, such as with cross validation.\n\nIn statistics there can be models of mental events as well as models of physical events. For example, a statistical model of customer behavior is a model that is conceptual (because behavior is physical), but a statistical model of customer satisfaction is a model of a concept (because satisfaction is a mental not a physical event).\n\nIn economics, a model is a theoretical construct that represents economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified framework designed to illustrate complex processes, often but not always using mathematical techniques. Frequently, economic models use structural parameters. Structural parameters are underlying parameters in a model or class of models. A model may have various parameters and those parameters may change to create various properties.\n\nA system model is the conceptual model that describes and represents the structure, behavior, and more views of a system. A system model can represent multiple views of a system by using two different approaches. The first one is the non-architectural approach and the second one is the architectural approach. The non-architectural approach respectively picks a model for each view. The architectural approach, also known as system architecture, instead of picking many heterogeneous and unrelated models, will use only one integrated architectural model.\n\nIn business process modelling the enterprise process model is often referred to as the \"business process model\". Process models are core concepts in the discipline of process engineering. Process models are:\nThe same process model is used repeatedly for the development of many applications and thus, has many instantiations.\n\nOne possible use of a process model is to prescribe how things must/should/could be done in contrast to the process itself which is really what happens. A process model is roughly an anticipation of what the process will look like. What the process shall be will be determined during actual system development.\n\nConceptual models of human activity systems are used in soft systems methodology (SSM), which is a method of systems analysis concerned with the structuring of problems in management. These models are models of concepts; the authors specifically state that they are not intended to represent a state of affairs in the physical world. They are also used in information requirements analysis (IRA) which is a variant of SSM developed for information system design and software engineering.\n\nLogico-linguistic modeling is another variant of SSM that uses conceptual models. However, this method combines models of concepts with models of putative real world objects and events. It is a graphical representation of modal logic in which modal operators are used to distinguish statement about concepts from statements about real world objects and events.\n\nIn software engineering, an entity-relationship model (ERM) is an abstract and conceptual representation of data. Entity-relationship modeling is a database modeling method, used to produce a type of conceptual schema or semantic data model of a system, often a relational database, and its requirements in a top-down fashion. Diagrams created by this process are called entity-relationship diagrams, ER diagrams, or ERDs.\n\nEntity-relationship models have had wide application in the building of information systems intended to support activities involving objects and events in the real world. In these cases they are models that are conceptual. However, this modeling method can be used to build computer games or a family tree of the Greek Gods, in these cases it would be used to model concepts.\n\nA domain model is a type of conceptual model used to depict the structural elements and their conceptual constraints within a domain of interest (sometimes called the problem domain). A domain model includes the various entities, their attributes and relationships, plus the constraints governing the conceptual integrity of the structural model elements comprising that problem domain. A domain model may also include a number of conceptual views, where each view is pertinent to a particular subject area of the domain or to a particular subset of the domain model which is of interest to a stakeholder of the domain model.\n\nLike entity-relationship models, domain models can be used to model concepts or to model real world objects and events.\n\n\n"}
{"id": "58267", "url": "https://en.wikipedia.org/wiki?curid=58267", "title": "Conceptual schema", "text": "Conceptual schema\n\nA 'conceptual schema' is a high-level description of a business's informational needs. It typically includes only the main concepts and the main relationships among them. Typically this is a first-cut model, with insufficient detail to build an actual database. This level describes the structure of the whole database for a group of users. The conceptual model is also known as the data model that can be used to describe the conceptual schema when a database system is implemented. It hides the internal details of physical storage and targets on describing entities, datatype, relationships and constraints.\n\nA conceptual schema or conceptual data model is a map of concepts and their relationships used for databases. This describes the semantics of an organization and represents a series of assertions about its nature. Specifically, it describes the things of significance to an organization (\"entity classes\"), about which it is inclined to collect information, and characteristics of (\"attributes\") and associations between pairs of those things of significance (\"relationships\").\n\nBecause a conceptual schema represents the semantics of an organization, and not a database design, it may exist on various levels of abstraction. The original ANSI four-schema architecture began with the set of \"external schema\" that each represent one person's view of the world around him or her. These are consolidated into a single \"conceptual schema\" that is the superset of all of those external views. A data model can be as concrete as each person's perspective, but this tends to make it inflexible. If that person's world changes, the model must change. Conceptual data models take a more abstract perspective, identifying the fundamental things, of which the things an individual deals with are just examples.\n\nThe model does allow for what is called inheritance in object oriented terms. The set of instances of an entity class may be subdivided into entity classes in their own right. Thus, each instance of a \"sub-type\" entity class is also an instance of the entity class's \"super-type\". Each instance of the super-type entity class, then is also an instance of one of the sub-type entity classes.\n\nSuper-type/sub-type relationships may be \"exclusive\" or not. A methodology may require that each instance of a super-type may \"only\" be an instance of \"one\" sub-type. Similarly, a super-type/sub-type relationship may be \"exhaustive\" or not. It is exhaustive if the methodology requires that each instance of a super-type \"must be\" an instance of a sub-type. A sub-type named other is often necessary.\n\n\nA data structure diagram (DSD) is a data model or diagram used to describe conceptual data models by providing graphical notations which document entities and their relationships, and the constraints that bind them.\n\n\n\n"}
{"id": "7327", "url": "https://en.wikipedia.org/wiki?curid=7327", "title": "Copernican principle", "text": "Copernican principle\n\nIn physical cosmology, the Copernican principle is an alternative name for the principle of relativity, stating that humans, on the Earth or in the Solar system, are not privileged observers of the universe.\n\nNamed for Copernican heliocentrism, it is a working assumption that arises from a modified cosmological extension of Copernicus's argument of a moving Earth. In some sense, it is equivalent to the mediocrity principle.\n\nHermann Bondi named the principle after Copernicus in the mid-20th century, although the principle itself dates back to the 16th-17th century paradigm shift away from the Ptolemaic system, which placed Earth at the center of the universe. Copernicus proposed that the motion of the planets can be explained by reference to an assumption that the Sun and not Earth is centrally located and stationary. He argued that the apparent retrograde motion of the planets is an illusion caused by Earth's movement around the Sun, which the Copernican model placed at the centre of the universe. Copernicus himself was mainly motivated by technical dissatisfaction with the earlier system and not by support for any mediocrity principle. In fact, although the Copernican heliocentric model is often described as \"demoting\" Earth from its central role it had in the Ptolemaic geocentric model, it was successors to Copernicus, notably the 16th century Giordano Bruno who adopted this new perspective. The earth's central position had been interpreted as being in the \"lowest and filthiest parts\". Instead, as Galileo said, the earth is part of the \"dance of the stars\" rather than the \"sump where the universe's filth and ephemera collect\". In the late 20th Century, Carl Sagan asked, \"Who are we? We find that we live on an insignificant planet of a humdrum star lost in a galaxy tucked away in some forgotten corner of a universe in which there are far more galaxies than people.\"\n\nIn cosmology, if one assumes the Copernican principle and observes that the universe appears isotropic or the same in all directions from our vantage point on Earth, then one can infer that the universe is generally homogeneous or the same everywhere (at any given time) and is also isotropic about any given point. These two conditions make up the cosmological principle. In practice, astronomers observe that the universe has heterogeneous or non-uniform structures up to the scale of galactic superclusters, filaments and great voids. It becomes more and more homogeneous and isotropic when observed on larger and larger scales, with little detectable structure on scales of more than about 200 million parsecs. However, on scales comparable to the radius of the observable universe, we see systematic changes with distance from Earth. For instance, galaxies contain more young stars and are less clustered, and quasars appear more numerous. While this might suggest that Earth is at the center of the universe, the Copernican principle requires us to interpret it as evidence for the evolution of the universe with time: this distant light has taken most of the age of the universe to reach us and show us the universe when it was young. The most distant light of all, cosmic microwave background radiation, is isotropic to at least one part in a thousand.\n\nModern mathematical cosmology is based on the assumption that the Cosmological principle is almost, but not exactly, true on the largest scales. The Copernican principle represents the irreducible philosophical assumption needed to justify this, when combined with the observations.\n\nMichael Rowan-Robinson emphasizes the Copernican principle as the threshold test for modern thought, asserting that: \"It is evident that in the post-Copernican era of human history, no well-informed and rational person can imagine that the Earth occupies a unique position in the universe.\"\n\nBondi and Thomas Gold used the Copernican principle to argue for the perfect cosmological principle which maintains that the universe is also homogeneous in time, and is the basis for the steady-state cosmology. However, this strongly conflicts with the evidence for cosmological evolution mentioned earlier: the universe has progressed from extremely different conditions at the Big Bang, and will continue to progress toward extremely different conditions, particularly under the rising influence of dark energy, apparently toward the Big Freeze or Big Rip.\n\nSince the 1990s the term has been used (interchangeably with \"the Copernicus method\") for J. Richard Gott's Bayesian-inference-based prediction of duration of ongoing events, a generalized version of the Doomsday argument.\n\nThe Copernican principle has never been proven, and in the most general sense cannot be proven, but it is implicit in many modern theories of physics. Cosmological models are often derived with reference to the Cosmological principle, slightly more general than the Copernican principle, and many tests of these models can be considered tests of the Copernican principle.\n\nBefore the term Copernican principle was even coined, Earth was repeatedly shown not to have any special location in the universe. The Copernican Revolution dethroned Earth to just one of many planets orbiting the Sun. Proper motion was mentioned by Halley. William Herschel found that the Solar System is moving through space within our disk-shaped Milky Way galaxy. Edwin Hubble showed that our galaxy is just one of many galaxies in the universe. Examination of our galaxy's position and motion in the universe led to the Big Bang theory and the whole of modern cosmology.\n\nRecent and planned tests relevant to the cosmological and Copernican principles include:\n\nThe standard model of cosmology, the Lambda-CDM model, assumes the Copernican principle and the more general Cosmological principle and observations are largely consistent but there are always unsolved problems. Some cosmologists and theoretical physicists design models lacking the Cosmological or Copernican principles, to constrain the valid values of observational results, to address specific known issues, and to propose tests to distinguish between current models and other possible models.\n\nA prominent example in this context is the observed accelerating universe and the cosmological constant issue. An alternative proposal to dark energy is that the universe is much more inhomogeneous than currently assumed, and specifically that we are in an extremely large low-density void. To match observations we would have to be very close to the centre of this void, immediately contradicting the Copernican principle.\n"}
{"id": "335910", "url": "https://en.wikipedia.org/wiki?curid=335910", "title": "Eight-circuit model of consciousness", "text": "Eight-circuit model of consciousness\n\nThe Eight-Circuit Model of Consciousness is a hypothesis by Timothy Leary, and later expanded on by Robert Anton Wilson and Antero Alli, that \"suggested eight periods [circuits] and twenty-four stages of neurological evolution\". The eight circuits, or eight \"brains\" as referred by other authors, operate within the human nervous system, each corresponding to its own imprint and direct experience of reality. Leary and Alli include three stages for each circuit that details developmental points for each level of consciousness.\nThe first four circuits deal with life on earth, and survival of the species. The last four circuits are post-terrestrial, and deal with the evolution of the species, altered states of consciousness, enlightenment, mystical experiences, psychedelic states of mind, and psychic abilities. The proposal suggests that these altered states of consciousness are recently realized, but not widely utilized. Leary describes the first four as \"larval circuits\", necessary for surviving and functioning in a terrestrial human society, and proposed that the post terrestrial circuits will be useful for future humans who, through a predetermined script, continue to act on their urge to migrate to outer space and live extra-terrestrially. Leary, Wilson, and Alli have written about the idea in depth, and have explored and attempted to define how each circuit operates, both in the lives of individual people and in societies and civilization.\n\nThe term \"circuit\" is equated to a metaphor of the brain being computer hardware, and that the wiring of the brain as circuitry.\n\nLeary uses the eight circuits along with recapitulation theory to explain the evolution of the human species, the personal development of an individual, and the biological evolution of all life.\n\nEach circuit listed has each name from Leary's book \"Exo-Psychology\" after the preface, and Wilson's book \"Quantum Psychology\" pgs.196-201. \"Note:In other books from Leary, Wilson, and Alli, the eight circuits have different names due to different interpretations and findings of each author. Please reference bibliography section for other works on labeling of each circuit.\"\n\nThis circuit is concerned with nourishment, physical safety, comfort and survival, suckling, cuddling, etc. It begins with one spatial dimension, forward/back.\n\nThis circuit is imprinted early in infancy. The imprint will normally last for life unless it is re-imprinted by a powerful experience. Depending on the nature of the imprint, the organism will tend towards one of two basic attitudes:\n\nThis circuit is said to have appeared in the earliest evolution of the invertebrate brain and corresponds to the reptilian brain of triune brain theory. This circuit operates in essentially the same way across mammals, reptiles, fish, primates and humans. \n\nRobert Anton Wilson equated this circuit with the oral stage in the Freudian theory of psychosexual development, and proposed that this circuit is activated in adults by strong opioids.\n\nThe emotional-territorial circuit is imprinted in the toddler stage. It is concerned with domination and submission, territoriality, etc.\n\nThe imprint on this circuit will trigger one of two states:\n\nThis circuit is activated by depressant drugs such as alcohol, barbiturates, and benzodiazepines. This circuit appeared first in territorial vertebrate animals and is preserved across all mammals. It corresponds to the mammalian brain of triune brain theory. Robert Anton Wilson equated this circuit with the anal stage in the Freudian theory of psycho-sexual development. This circuit introduces a 2nd spatial dimension; up/down.\n\nThe first and second circuits both imprint in a binary fashion: trust/suspicion and dominance/submission. Thus there are four possible ways of imprinting the first two circuits:\n\n\nThis circuit is imprinted by human symbol systems. It is concerned with language, handling the environment, invention, calculation, prediction, building a mental \"map\" of the universe, physical dexterity, etc.\n\nThis circuit is activated by stimulant drugs such as amphetamines, cathinones, cocaine, and caffeine. This circuit supposedly appeared first when hominids started differentiating from the rest of the primates.\n\nRobert Anton Wilson, being heavily influenced by General Semantics, writes of this circuit as the 'time-binding circuit'. This means that this circuit's contents – including human know-how, technology, science etc. - are preserved memetically and passed on from generation to generation, constantly mutating and increasing in sophistication.\n\nThis fourth circuit is imprinted by the first orgasm-mating experiences and tribal \"morals\". It is concerned with sexual pleasure (instead of sexual reproduction), local definitions of \"moral\" and \"immoral\", reproduction, rearing of the young, etc. The fourth circuit concerns itself with cultural values and operating within social networks. This circuit is said to have first appeared with the development of tribes. Some have pointed out that entactogens such as MDMA seem to meet some of the requirements needed to activate this circuit.\n\nThis is concerned with neurological-somatic feedbacks, feeling high and blissful, somatic reprogramming, etc. It may be called the rapture circuit.\n\nWhen this circuit is activated, a non-conceptual feeling of well-being arises. This has a beneficial effect on the health of the physical body.\n\nThe fifth circuit is consciousness of the body. There is a marked shift from linear visual space to an all-encompassing aesthetic sensory space. Perceptions are judged not so much for their meaning and utility, but for their aesthetic qualities. Experience of this circuit often accompanies an hedonistic turn-on, a rapturous amusement, a detachment from the previously compulsive mechanism of the first four circuits.\n\nThis circuit is activated by ecstatic experiences via physiological effects of cannabis, Hatha Yoga, tantra and Zen meditation. Robert Anton Wilson writes, \"Tantra yoga is concerned with shifting consciousness entirely into this circuit\" and that \"Prolonged sexual play without orgasm always triggers some Circuit V consciousness\".\n\nLeary describes that this circuit first appeared in the upper classes, with the development of leisure-class civilizations around 2000 BC.\n\n\"Note: Timothy Leary lists this circuit as the sixth, and the neurogenetic circuit as the seventh. In \"Prometheus Rising\", Robert Anton Wilson reversed the order of these two circuits, describing the neurogenetic circuit as the sixth circuit, and the metaprogramming circuit as the seventh. In the subsequently published \"Quantum Psychology\", he reverted this back to the order proposed by Leary.\"\n\nThis circuit is concerned with re-imprinting and re-programming all earlier circuits and the relativity of \"realities\" perceived. The sixth circuit consists of the nervous system becoming aware of itself. Leary says this circuit enables telepathic communication and is activated by low-to-moderate doses of LSD (50-150 µg), moderate doses of peyote, psilocybin mushrooms and meditation/chanting especially when used in a group or ritual setting. This circuit is traced by Leary back to 500 BC.\n\nThis circuit is the connection of the individual's mind to the whole sweep of evolution and life as a whole. It is the part of consciousness that echoes the experiences of the previous generations that have brought the individual's brain-mind to its present level.\n\nIt deals with ancestral, societal and scientific DNA-RNA-brain feedbacks. Those who achieve this mutation may speak of past lives, reincarnation, immortality etc. It corresponds to the collective unconscious in the models of Carl Jung where archetypes reside.\n\nActivation of this circuit may be equated with consciousness of the Great God Pan in his aspect as Life as a whole, or with consciousness of Gaia, the biosphere considered as a single organism.\n\nThis circuit is activated by higher doses of LSD (200-500 µg), higher doses of peyote, higher doses of psilocybin mushrooms, yoga and meditation.\n\nThe circuit first appeared among the Hindus in the early first millennium and later reappeared among the Sufi sects.\n\nThe eighth circuit is concerned with quantum consciousness, non-local awareness (information from beyond ordinary space-time awareness which is limited by the speed of light), illumination. Some of the ways this circuit can get activated are: the awakening of kundalini, shock, a near-death experience, DMT, high doses of LSD and according to Robert Anton Wilson almost any dose of ketamine. This circuit has even been compared to the Buddhist concept of Indra's net from the Avatamsaka Sutra.\n\nLeary stated \"They[The theories presented in \"Info-Psychology\"] are scientific in that they are based on empirical findings from physics, physiology, pharmacology, genetics, astronomy, behavioral psychology, information science, and most importantly, neurology.\" \n\nLeary called his book \"science faction\" or \"psi-phy\" and noted he had written it \"in various prisons to which the author had been sentenced for dangerous ideology and violations of Newtonian and religious laws\".\n\nAlthough Leary propounded the basic premise of eight \"brains\" or brain circuits, he was inspired by sources such as the Hindu \"chakra\" system.\n\nLeary claimed that among other things this model explained the social conflict in the 1960s, where the mainstream was said to be those with four circuits active and characterized by Leary as tribal moralists and clashed with the counter-culturists, who were then said to be those with the fifth circuit active and characterized as individualists and hedonists. \n\nLeary's first book on the subject, \"Neurologic\", only included seven circuits when it was published in 1973. \"Exo-Psychology\", published in 1977, expanded the number of circuits to eight and clarified the subject. In it, he puts forward the theory that the later four circuits are \"post terrestrial;\" intended to develop as we migrate off this planet and colonize others. Once we begin space migration, according to Leary, we will have more ready access to these higher circuits. \"Exo-Psychology\" was re-published as revised by Timothy Leary with additional material in 1989 under the title \"Info-Psychology\" (New Falcon Publishing).\n\nLeary's ideas heavily influenced the work of Robert Anton Wilson. Wilson's book \"Prometheus Rising\" is an in-depth work documenting Leary's eight-circuit model of consciousness. Wilson's published screenplay \"Reality Is What You Can Get Away With\" uses and explains the model. Wilson, like Leary, wrote about the distinction between terrestrial and post-terrestrial life.\n\n\"Angel Tech\" by Antero Alli, is structured around the Eight-circuit model of consciousness. Alli defines the word angel as \"a being of light\" and tech from the word \"techne\" meaning \"art\". The title is defined as \"the art of being light\". It includes suggested activities such as meditations and construction of tarot-card collages associated with each circuit and imprint.\n\nThe model is fairly prominent in chaos magic. This concept has been detailed in \"Chaotopia!\" by Dave Lee, a leading member of the magic society Illuminates of Thanateros. Leary and Wilson were also members of the society. \n\nRolf Von Eckartsberg also appears to have been influenced by the model.\n\n\n\n"}
{"id": "857235", "url": "https://en.wikipedia.org/wiki?curid=857235", "title": "Equivalence principle", "text": "Equivalence principle\n\nIn the theory of general relativity, the equivalence principle is the equivalence of gravitational and inertial mass, and Albert Einstein's observation that the gravitational \"force\" as experienced locally while standing on a massive body (such as the Earth) is the same as the \"pseudo-force\" experienced by an observer in a non-inertial (accelerated) frame of reference.\n\nSomething like the equivalence principle emerged in the early 17th century, when Galileo expressed experimentally that the acceleration of a test mass due to gravitation is independent of the amount of mass being accelerated.\n\nKepler, using Galileo's discoveries, showed knowledge of the equivalence principle by accurately describing what would occur if the moon were stopped in its orbit and dropped towards Earth. This can be deduced without knowing if or in what manner gravity decreases with distance, but requires assuming the equivalency between gravity and inertia.\n\nThe 1/54 ratio is Kepler's estimate of the Moon–Earth mass ratio, based on their diameters. The accuracy of his statement can be deduced by using Newton's inertia law F=ma and Galileo's gravitational observation that distance formula_1. Setting these accelerations equal for a mass is the equivalence principle. Noting the time to collision for each mass is the same gives Kepler's statement that D/D=M/M, without knowing the time to collision or how or if the acceleration force from gravity is a function of distance.\n\nNewton's gravitational theory simplified and formalized Galileo's and Kepler's ideas by recognizing Kepler's \"animal force or some other equivalent\" beyond gravity and inertia were not needed, deducing from Kepler's planetary laws how gravity reduces with distance.\n\nThe equivalence principle was properly introduced by Albert Einstein in 1907, when he observed that the acceleration of bodies towards the center of the Earth at a rate of 1\"\"g\"\" (\"g\" = 9.81 m/s being a standard reference of gravitational acceleration at the Earth's surface) is equivalent to the acceleration of an inertially moving body that would be observed on a rocket in free space being accelerated at a rate of 1\"g\". Einstein stated it thus:\n\nThat is, being on the surface of the Earth is equivalent to being inside a spaceship (far from any sources of gravity) that is being accelerated by its engines. The direction or vector of acceleration equivalence on the surface of the earth is \"up\" or directly opposite the center of the planet while the vector of acceleration in a spaceship is directly opposite from the mass ejected by its thrusters. From this principle, Einstein deduced that free-fall is inertial motion. Objects in free-fall do not experience being accelerated downward (e.g. toward the earth or other massive body) but rather weightlessness and no acceleration. In an inertial frame of reference bodies (and photons, or light) obey Newton's first law, moving at constant velocity in straight lines. Analogously, in a curved spacetime the world line of an inertial particle or pulse of light is \"as straight as possible\" (in space \"and\" time). Such a world line is called a geodesic and from the point of view of the inertial frame is a straight line. This is why an accelerometer in free-fall doesn't register any acceleration; there isn't any.\n\nAs an example: an inertial body moving along a geodesic through space can be trapped into an orbit around a large gravitational mass without ever experiencing acceleration. This is possible because spacetime is radically curved in close vicinity to a large gravitational mass. In such a situation the geodesic lines bend inward around the center of the mass and a free-floating (weightless) inertial body will simply follow those curved geodesics into an elliptical orbit. An accelerometer on-board would never record any acceleration.\n\nBy contrast, in Newtonian mechanics, gravity is assumed to be a force. This force draws objects having mass towards the center of any massive body. At the Earth's surface, the force of gravity is counteracted by the mechanical (physical) resistance of the Earth's surface. So in Newtonian physics, a person at rest on the surface of a (non-rotating) massive object is in an inertial frame of reference. These considerations suggest the following corollary to the equivalence principle, which Einstein formulated precisely in 1911:\n\nEinstein also referred to two reference frames, K and K'. K is a uniform gravitational field, whereas K' has no gravitational field but is uniformly accelerated such that objects in the two frames experience identical forces:\n\nThis observation was the start of a process that culminated in general relativity. Einstein suggested that it should be elevated to the status of a general principle, which he called the \"principle of equivalence\" when constructing his theory of relativity:\n\nEinstein combined (postulated) the equivalence principle with special relativity to predict that clocks run at different rates in a gravitational potential, and light rays bend in a gravitational field, even before he developed the concept of curved spacetime.\n\nSo the original equivalence principle, as described by Einstein, concluded that free-fall and inertial motion were physically equivalent. This form of the equivalence principle can be stated as follows. An observer in a windowless room cannot distinguish between being on the surface of the Earth, and being in a spaceship in deep space accelerating at 1g. This is not strictly true, because massive bodies give rise to tidal effects (caused by variations in the strength and direction of the gravitational field) which are absent from an accelerating spaceship in deep space. The room, therefore, should be small enough that tidal effects can be neglected.\n\nAlthough the equivalence principle guided the development of general relativity, it is not a founding principle of relativity but rather a simple consequence of the \"geometrical\" nature of the theory. In general relativity, objects in free-fall follow geodesics of spacetime, and what we perceive as the force of gravity is instead a result of our being unable to follow those geodesics of spacetime, because the mechanical resistance of matter prevents us from doing so.\n\nSince Einstein developed general relativity, there was a need to develop a framework to test the theory against other possible theories of gravity compatible with special relativity. This was developed by Robert Dicke as part of his program to test general relativity. Two new principles were suggested, the so-called Einstein equivalence principle and the strong equivalence principle, each of which assumes the weak equivalence principle as a starting point. They only differ in whether or not they apply to gravitational experiments.\n\nAnother clarification needed is that the equivalence principle assumes a constant acceleration of 1g without considering the mechanics of generating 1g. If we do consider the mechanics of it, then we must assume the aforementioned windowless room has a fixed mass. Accelerating it at 1g means there is a constant force being applied, which = m*g where m is the mass of the windowless room along with its contents (including the observer). Now, if the observer jumps inside the room, an object lying freely on the floor will decrease in weight momentarily because the acceleration is going to decrease momentarily due to the observer pushing back against the floor in order to jump. The object will then gain weight while the observer is in the air and the resulting decreased mass of the windowless room allows greater acceleration; it will lose weight again when the observer lands and pushes once more against the floor; and it will finally return to its initial weight afterwards. To make all these effects equal those we would measure on a planet producing 1g, the windowless room must be assumed to have the same mass as that planet. Additionally, the windowless room must not cause its own gravity, otherwise the scenario changes even further. These are technicalities, clearly, but practical ones if we wish the experiment to demonstrate more or less precisely the equivalence of 1g gravity and 1g acceleration.\n\nThree forms of the equivalence principle are in current use: weak (Galilean), Einsteinian, and strong.\n\nThe weak equivalence principle, also known as the universality of free fall or the Galilean equivalence principle can be stated in many ways. The strong EP includes (astronomic) bodies with gravitational binding energy (e.g., 1.74 solar-mass pulsar PSR J1903+0327, 15.3% of whose separated mass is absent as gravitational binding energy). The weak EP assumes falling bodies are bound by non-gravitational forces only. Either way:\n\nLocality eliminates measurable tidal forces originating from a radial divergent gravitational field (e.g., the Earth) upon finite sized physical bodies. The \"falling\" equivalence principle embraces Galileo's, Newton's, and Einstein's conceptualization. The equivalence principle does not deny the existence of measurable effects caused by a \"rotating\" gravitating mass (frame dragging), or bear on the measurements of light deflection and gravitational time delay made by non-local observers.\n\nBy definition of active and passive gravitational mass, the force on formula_2 due to the gravitational field of formula_3 is:\n\nLikewise the force on a second object of arbitrary mass due to the gravitational field of mass is:\n\nBy definition of inertial mass:\n\nIf formula_7 and formula_8 are the same distance formula_9 from formula_10 then, by the weak equivalence principle, they fall at the same rate (i.e. their accelerations are the same)\n\nHence:\n\nTherefore:\n\nIn other words, passive gravitational mass must be proportional to inertial mass for all objects.\n\nFurthermore, by Newton's third law of motion:\n\nmust be equal and opposite to\n\nIt follows that:\n\nIn other words, passive gravitational mass must be proportional to active gravitational mass for all objects.\n\nThe dimensionless Eötvös-parameter formula_17 is the difference of the ratios of gravitational and inertial masses divided by their average for the two sets of test masses \"A\" and \"B.\"\n\nTests of the weak equivalence principle are those that verify the equivalence of gravitational mass and inertial mass. An obvious test is dropping different objects, ideally in a vacuum environment, e.g., inside the Fallturm Bremen drop tower.\n\nSee:\n\nExperiments are still being performed at the University of Washington which have placed limits on the differential acceleration of objects towards the Earth, the Sun and towards dark matter in the galactic center. Future satellite experiments – STEP (Satellite Test of the Equivalence Principle), Galileo Galilei, and MICROSCOPE (MICROSatellite à traînée Compensée pour l'Observation du Principe d'Équivalence) – will test the weak equivalence principle in space, to much higher accuracy.\n\nWith the first successful production of antimatter, in particular anti-hydrogen, a new approach to test the weak equivalence principle has been proposed. Experiments to compare the gravitational behavior of matter and antimatter are currently being developed.\n\nProposals that may lead to a quantum theory of gravity such as string theory and loop quantum gravity predict violations of the weak equivalence principle because they contain many light scalar fields with long Compton wavelengths, which should generate fifth forces and variation of the fundamental constants. Heuristic arguments suggest that the magnitude of these equivalence principle violations could be in the 10 to 10 range. Currently envisioned tests of the weak equivalence principle are approaching a degree of sensitivity such that \"non-discovery\" of a violation would be just as profound a result as discovery of a violation. Non-discovery of equivalence principle violation in this range would suggest that gravity is so fundamentally different from other forces as to require a major reevaluation of current attempts to unify gravity with the other forces of nature. A positive detection, on the other hand, would provide a major guidepost towards unification.\n\nWhat is now called the \"Einstein equivalence principle\" states that the weak equivalence principle holds, and that:\nHere \"local\" has a very special meaning: not only must the experiment not look outside the laboratory, but it must also be small compared to variations in the gravitational field, tidal forces, so that the entire laboratory is freely falling. It also implies the absence of interactions with \"external\" fields \"other than the gravitational field\".\n\nThe principle of relativity implies that the outcome of local experiments must be independent of the velocity of the apparatus, so the most important consequence of this principle is the Copernican idea that dimensionless physical values such as the fine-structure constant and electron-to-proton mass ratio must not depend on where in space or time we measure them. Many physicists believe that any Lorentz invariant theory that satisfies the weak equivalence principle also satisfies the Einstein equivalence principle.\n\n\"Schiff's conjecture\" suggests that the weak equivalence principle implies the Einstein equivalence principle, but it has not been proven. Nonetheless, the two principles are tested with very different kinds of experiments. The Einstein equivalence principle has been criticized as imprecise, because there is no universally accepted way to distinguish gravitational from non-gravitational experiments (see for instance Hadley and Durand).\n\nIn addition to the tests of the weak equivalence principle, the Einstein equivalence principle can be tested by searching for variation of dimensionless constants and mass ratios. The present best limits on the variation of the fundamental constants have mainly been set by studying the naturally occurring Oklo natural nuclear fission reactor, where nuclear reactions similar to ones we observe today have been shown to have occurred underground approximately two billion years ago. These reactions are extremely sensitive to the values of the fundamental constants.\n\nThere have been a number of controversial attempts to constrain the variation of the strong interaction constant. There have been several suggestions that \"constants\" do vary on cosmological scales. The best known is the reported detection of variation (at the 10 level) of the fine-structure constant from measurements of distant quasars, see Webb et al. Other researchers dispute these findings. Other tests of the Einstein equivalence principle are gravitational redshift experiments, such as the Pound–Rebka experiment which test the position independence of experiments.\n\nThe strong equivalence principle suggests the laws of gravitation are independent of velocity and location. In particular,\nand\nThe first part is a version of the weak equivalence principle that applies to objects that exert a gravitational force on themselves, such as stars, planets, black holes or Cavendish experiments. The second part is the Einstein equivalence principle (with the same definition of \"local\"), restated to allow gravitational experiments and self-gravitating bodies. The freely-falling object or laboratory, however, must still be small, so that tidal forces may be neglected (hence \"local experiment\").\n\nThis is the only form of the equivalence principle that applies to self-gravitating objects (such as stars), which have substantial internal gravitational interactions. It requires that the gravitational constant be the same everywhere in the universe and is incompatible with a fifth force. It is much more restrictive than the Einstein equivalence principle.\n\nThe strong equivalence principle suggests that gravity is entirely geometrical by nature (that is, the metric alone determines the effect of gravity) and does not have any extra fields associated with it. If an observer measures a patch of space to be flat, then the strong equivalence principle suggests that it is absolutely equivalent to any other patch of flat space elsewhere in the universe. Einstein's theory of general relativity (including the cosmological constant) is thought to be the only theory of gravity that satisfies the strong equivalence principle. A number of alternative theories, such as Brans–Dicke theory, satisfy only the Einstein equivalence principle.\n\nThe strong equivalence principle can be tested by searching for a variation of Newton's gravitational constant \"G\" over the life of the universe, or equivalently, variation in the masses of the fundamental particles. A number of independent constraints, from orbits in the solar system and studies of big bang nucleosynthesis have shown that \"G\" cannot have varied by more than 10%.\n\nThus, the strong equivalence principle can be tested by searching for fifth forces (deviations from the gravitational force-law predicted by general relativity). These experiments typically look for failures of the inverse-square law (specifically Yukawa forces or failures of Birkhoff's theorem) behavior of gravity in the laboratory. The most accurate tests over short distances have been performed by the Eöt–Wash group. A future satellite experiment, SEE (Satellite Energy Exchange), will search for fifth forces in space and should be able to further constrain violations of the strong equivalence principle. Other limits, looking for much longer-range forces, have been placed by searching for the Nordtvedt effect, a \"polarization\" of solar system orbits that would be caused by gravitational self-energy accelerating at a different rate from normal matter. This effect has been sensitively tested by the Lunar Laser Ranging Experiment. Other tests include studying the deflection of radiation from distant radio sources by the sun, which can be accurately measured by very long baseline interferometry. Another sensitive test comes from measurements of the frequency shift of signals to and from the Cassini spacecraft. Together, these measurements have put tight limits on Brans–Dicke theory and other alternative theories of gravity.\n\nIn 2014, astronomers discovered a stellar triple system including a millisecond pulsar PSR J0337+1715 and two white dwarfs orbiting it. The system provided them a chance to test the strong equivalence principle in a strong gravitational field with high accuracy.\n\nOne challenge to the equivalence principle is the Brans–Dicke theory. Self-creation cosmology is a modification of the Brans–Dicke theory. The Fredkin Finite Nature Hypothesis is an even more radical challenge to the equivalence principle and has even fewer supporters.\n\nIn August 2010, researchers from the University of New South Wales, Swinburne University of Technology, and Cambridge University published a paper titled \"Evidence for spatial variation of the fine structure constant\", whose tentative conclusion is that, \"qualitatively, [the] results suggest a violation of the Einstein Equivalence Principle, and could infer a very large or infinite universe, within which our 'local' Hubble volume represents a tiny fraction.\"\n\nIn his book \"Einstein's Mistakes\", pages 226–227, Hans C. Ohanian describes several situations which falsify Einstein's Equivalence Principle. Inertial accelerative effects are analogous to, but not equivalent to, gravitational effects. Ohanian cites Ehrenfest for this same opinion.\n\nDutch physicist and string theorist Erik Verlinde has generated a self-contained, logical derivation of the equivalence principle based on the starting assumption of a holographic universe. Given this situation, gravity would not be a true fundamental force as is currently thought but instead an \"emergent property\" related to entropy. Verlinde's entropic gravity theory apparently leads naturally to the correct observed strength of dark energy; previous failures to explain its incredibly small magnitude have been called by such people as cosmologist Michael Turner (who is credited as having coined the term \"dark energy\") as \"the greatest embarrassment in the history of theoretical physics\". However, it should be noted that these ideas are far from settled and still very controversial.\n\n\n\n"}
{"id": "36188092", "url": "https://en.wikipedia.org/wiki?curid=36188092", "title": "Fa (concept)", "text": "Fa (concept)\n\nFa (;) is a concept in Chinese philosophy that covers ethics, logic, and law. It can be translated as \"law\" in some contexts, but more often as \"model\" or \"standard.\" First gaining importance in the Mohist school of thought, the concept was principally elaborated in Legalism. In Han Fei's philosophy, the king is the sole source of \"fa\" (law), taught to the common people so that there would be a harmonious society free of chance occurrences, disorder, and \"appeal to privilege\". High officials were not to be held above \"fa\" (law or protocol), nor were they to be allowed to independently create their own \"fa\", uniting both executive fiat and rule of law.\n\nXunzi, a philosopher that would end up being foundational in Han dynasty Confucianism, also took up \"fa\", suggesting that it could only be properly assessed by the Confucian sage (ruler), and that the most important \"fa\" were the very rituals that Mozi had ridiculed for their ostentatious waste and lack of benefit for the people at large.\n\nThe concept of \"fa\" first gained importance in the Mohist school of thought. To Mozi, a standard must stand \"three tests\" in order to determine its efficacy and morality. The first of these tests was its origin; if the standard had precedence in the actions or thought of the semi-mythological sage kings of the Xia dynasty whose examples are frequently cited in classical Chinese philosophy. The second test was one of validity; does the model stand up to evidence in the estimation of the people? The third and final test was one of applicability; this final one is a utilitarian estimation of the net good that, if implemented, the standard would have on both the people and the state.\n\nThe third test speaks to the fact that to the Mohists, a \"fa\" was not simply an abstract model, but an active tool. The real-world use and practical application of \"fa\" were vital. Yet \"fa\" as models were also used in later Mohist logic as principles used in deductive reasoning. As classical Chinese philosophical logic was based on analogy rather than syllogism, \"fa\" were used as benchmarks to determine the validity of logical claims through comparison. There were three \"fa\" in particular that were used by these later Mohists (or \"Logicians\") to assess such claims, which were mentioned earlier. The first was considered a \"root\" standard, a concern for precedence and origin. The second, a \"source\", a concern for empiricism. The third, a \"use\", a concern for the consequence and pragmatic utility of a standard. These three \"fa\" were used by the Mohists to both promote social welfare and denounce ostentation or wasteful spending.\n\n"}
{"id": "34802284", "url": "https://en.wikipedia.org/wiki?curid=34802284", "title": "Gauss's Pythagorean right triangle proposal", "text": "Gauss's Pythagorean right triangle proposal\n\nGauss's Pythagorean right triangle proposal is an idea attributed to Carl Friedrich Gauss for a method to signal extraterrestrial beings by constructing an immense right triangle and three squares on the surface of the Earth. The shapes would be a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars.\n\nAlthough credited in numerous sources as originating with Gauss, with exact details of the proposal set out, the specificity of detail, and even whether Gauss made the proposal, have been called into question. Many of the earliest sources do not actually name Gauss as the originator, instead crediting a \"German Astronomer\" or using other nonspecific descriptors, and in some cases naming a different author entirely. The details of the proposal also change significantly upon different retellings. Nevertheless, Gauss's writings reveal a belief and interest in finding a method to contact extraterrestrial life, and that he did, at the least, propose using amplified light using a heliotrope, his own 1818 invention, to signal supposed inhabitants of the Moon.\n\nCarl Friedrich Gauss is credited with an 1820 proposal for a method to signal extraterrestrial beings in the form of drawing an immense right triangle and three squares on the surface of the Earth, intended as a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars. Details vary between sources, but typically the \"drawing\" was to be constructed on the Siberian tundra, and made up of vast strips of pine forest forming the right triangle's borders, with the interior of the drawing and exterior squares composed of fields of wheat. Gauss is said to have been convinced that Mars harbored intelligent life and that this geometric figure, invoking the Pythagorean theorem through the squares on the outside borders (sometimes called a \"windmill diagram\", as originated by Euclid), would demonstrate to such alien observers the reciprocal existence of intelligent life on Earth and its grounding in mathematics. Wheat was said to be chosen by Gauss for contrast with the pine tree borders \"because of its uniform color\".\n\nThe specificity of the proposal's details as it appears in most later sources—even its attribution to Gauss—is called into question in University of Notre Dame Professor Michael J. Crowe's 1986 book, \"The Extraterrestrial Life Debate, 1750–1900\", in which he surveys the origins of the Gauss proposal and observes that:The history of this proposal ... can be traced through two dozen or more pluralist writings reaching back to the first half of the nineteenth century. When this is done, however, it turns out that the story exists in almost as many forms as its retellings. Furthermore, these versions share one characteristic: Never is reference supplied to where in the writings of Gauss ... the [proposal] appear[s]!Some early sources explored by Crowe for the attribution and form of Gauss's proposal include Austrian astronomer, Joseph Johann Littrow's statement in \"Wunder des Himmels\" that \"one of our most distinguished geometers\" proposed that a geometric figure \"for example the well known so-called square of the hypotenuse, be laid out on a large scale, say on a particular broad plain of the earth\". and Patrick Scott's \"Love in the Moon\", in which a \"learned man\" is described as proposing a signal formed by a \"great plantation of tree\" in the form of \"47th Proposition of Euclid\" in \"the great African dessert \". In \"Chambers's Edinburgh Journal\" it was written that a Russian savant had proposed to \"communicate with the moon by cutting a large figure of the forty-seventh proposition of Euclid on the plains of Siberia, which, he said, any fool would understand\".\n\nIn the writings of astronomers Asaph Hall and of Norman Lockyer, each refer separately to a \"German Astronomer\" who proposed the method of contact be by \"fire signals\" from Siberia. Writing in 1902, Simon Newcomb placed the origin of a Siberian triangle \"several hundred miles in extent\" not with Gauss, but at the feet of German astronomer Franz Xaver von Zach. In lectures presented by François Arago at the Paris Observatory, he named Siberia as the location of an extraterrestrial signaling project advanced by an unnamed \"German geometer\", but that the signaling method was to be through the use of mirrors, rather than any large symbol drawn upon the Earth. Despite this version's departure from a geometric figure, the appearance of mirrors as a signaling device has a connection with Gauss's background. Gauss invented the Heliotrope in 1818, an instrument that uses a mirror to reflect sunlight in a manner allowing a square mirror to be seen away even in sunny weather.\n\nGauss wrote of the heliotrope's potential as a celestial signaling device in a March 25, 1822 letter to Heinrich Olbers, by which he reveals a belief and interest in finding a method to contact extraterrestrial life: \"With 100 separate mirrors, each of 16 square feet, used conjointly, one would be able to send good heliotrope-light to the moon ... This would be a discovery even greater than that of America, if we could get in touch with our neighbors on the moon.\" Finally, in the October 1826 issue of the \"Edinburgh New Philosophical Journal\" an unnamed author wrote that in a conversation with Franz von Gruithuisen, Gauss stated words to the effect that \"the plan of erecting a geometrical figure on the plains of Siberia corresponded with his opinion, because, according to his view a correspondence with the inhabitants of the moon could only be begun by means of such mathematical contemplations and ideas, which we and they have in common.\" Crowe concluded in sum that his review of earliest sources failed to confirm the detail of the proposal and Gauss as its author, but that his origination of the idea was not unlikely given the existing evidence.\n"}
{"id": "33920395", "url": "https://en.wikipedia.org/wiki?curid=33920395", "title": "General Group Problem Solving (GGPS) Model", "text": "General Group Problem Solving (GGPS) Model\n\nThe General Group Problem Solving (GGPS) Model is a problem solving methodology, in which a group of individuals will define the desired outcome, identify the gap between the current state and the target and generate ideas for closing the gap by brainstorming. The end result is list of actions needed to achieve the desired results.\n\nSally Fuller and Ramon Aldag argue that group decision-making models have been operating under too narrow of a focus due to the overemphasis of the groupthink phenomenon. In addition, according to them, group decision-making has often been framed in relative isolation, ignoring context and real-world circumstances, which is a likely consequence of testing group decision-making in laboratory studies. They claim that the groupthink model is overly deterministic and an unrealistically restrictive depiction of the group problem-solving process.” To address these problems, they propose a new model that incorporates elements of group decision-making processes from a broader, more comprehensive perspective, offering a more general and generalizable framework for future research. The model includes elements of Irving Janis's original model (1977), but only those that have been consistently supported by the literature. To understand the differences between the two models, we briefly summarize both Janis's model and the GGPS-model first.\n\nJanis defines groupthink as “the mode of thinking that persons engage in when concurrence-seeking becomes so dominant in a cohesive in-group that it tends to over-ride realistic appraisals of alternative courses of action.” In a subsequent article, he elaborates on this by saying: “I use the term \"groupthink\" as a quick and easy way to refer to a mode of thinking that people engage in when they are deeply involved in a cohesive in-group, when the members' strivings for unanimity override their motivation to realistically appraise alternative courses of action. Groupthink refers to a deterioration of mental efficiency, reality testing, and moral judgment that results from in-group pressures.”\nAll this suggests that the original groupthink model was proposed for a rather specific situation, and Janis states that we can only call a phenomenon groupthink if all the warning signs are present (see groupthink symptoms).\n\nThe GGPS-model (developed by Ramon Aldag and Sally Fuller) broadens the perspectives, incorporating elements of the original groupthink model, in a fashion that creates a more widely applicable schematic.\nTwo key differences should be noted in comparison to Janis’ model:\n\n\nThree sets of antecedents are proposed by GGPS: decision characteristics, group structure and decision-making context.\n\nElements belonging here are the importance of the decision, time pressure, structure, procedural requirements, and task characteristics.\n\n\"Examples\": whether the task is simple or complex will make a substantial difference in required member input, as well as in whether a directive leader is necessary. Group interaction is also altered if, given a task, a single correct answer exists and if it becomes obvious to any of the members, since subsequent group interaction will likely be reduced.\n\nElements are cohesiveness, members’ homogeneity, insulation of the group, leader impartiality, leader power, history of the group, probability of future interaction, stage of group development and type of group.\n\n\"Examples\": whether group members anticipate to work together again in the future can have a major impact on to what degree can political motives influence the process. If it’s unlikely that the group will come together again, political influence can be lessened. Stage of group development is important because members of mature group with a long history may feel more comfortable challenging each other’s ideas, thus cohesiveness results in quality decision making and positive outcomes.\n\nElements are organizational political norms, member political motives, prior discussion of issue, prior goal attainment, goal definition, and degree of stress from external threat.\n\n\"Examples\": whether group members identified and pursue a unitary goal or they have multiple, discrepant goals influences the rationality of the decision-making process. Members’ political motives also make a tremendous difference, if individuals have a vested interest in certain outcomes, or there are one or more coalitions present, behavior in the decision making process could be altered.\n\nThe model differentiates two categories of emergent group characteristics: group perceptions and processes.\n\nThese include members’ perceptions of the group’s vulnerability, the inherent morality of the group, member unanimity, and views of opposing groups.\n\nThese include the group’s response to negative feedback, treatment of dissenters, self-censorship, and use of mindguards.\n\nDecision process characteristics are grouped in terms of the first three stages of group problem-solving processes: problem identification, alternative generation and evaluation and choice. Implementation and control stages are not included because they follow the actual decision, but some variables preparing for those stages are indeed included (e.g. development of contingency plans and gathering of control-related information).\n\nElements of this stage are predecisional information search, survey of objectives, and explicit problem definition.\n\n\"Example\": if members of the group fail to explicitly or correctly define the problem, there is a chance that they will solve the wrong problem.\n\nElements are number of alternatives and quality of alternatives.\n\n\"Example\": in generating alternatives, it’s important to differentiate quality and quantity of alternative ideas generated. Some group processes, such as brainstorming, are directed towards generating large numbers of ideas, with the assumption that it will lead to a superior alternative. Defective processes, on the other hand, might lead to large numbers of low quality ideas.\n\nElements are information processing quality, the source of the initial selection of a preferred alternative, emergence of preferred alternative, group decision rule, timing of convergence, reexamination of preferred and rejected alternatives, source of the final solution, development of contingency plans, and gathering of control-related information.\n\n\"Example\": whether the group decides based on a majority rule or a consensus has to be reached, makes a great difference in the process. If a consensus is to be reached, dissent could be discouraged, because dissenters could elongate and jeopardize the process. With a majority rule, dissent is more acceptable.\n\nThe GGPS model includes an array of decision, political and affective outcomes.\n\nDecision outcomes: include acceptance of the decision by those affected by it and/or those who have to implement it, adherence to the decision, implementation success, and decision quality.\n\n\"Example\": if the leader of the group is not satisfied with the decision, he/she might unilaterally reverse it.\n\nPolitical outcomes: include future motivation of the leader, future motivation of the group and future use of the group.\n\n\"Example\": if the outcome did not satisfy the political agenda of the leader, he/she might use the group less or not at all in the future.\n\nAffective outcomes: include satisfaction with the leader, satisfaction with the group process and satisfaction with the decision.\n\n\"Example\": whether members are content with the fairness of the group process, whether trust was developed and preserved, or whether commitment to the decision is strong will greatly influence the group’s future functioning.\n"}
{"id": "18562589", "url": "https://en.wikipedia.org/wiki?curid=18562589", "title": "Gossen's second law", "text": "Gossen's second law\n\nGossen's Second “Law”, named for Hermann Heinrich Gossen (1810–1858), is the assertion that an economic agent will allocate his or her expenditures such that the ratio of the marginal utility of each good or service to its price (the marginal expenditure necessary for its acquisition) is equal to that for every other good or service. Formally,\nwhere\n\nImagine that an agent has spent money on various sorts of goods or services. If the last unit of currency spent on goods or services of one sort bought a quantity with \"less\" marginal utility than that which would have been associated with the quantity of another sort that could have been bought with the money, then the agent would have been \"better off\" instead buying more of that other good or service. Assuming that goods and services are continuously divisible, the only way that it is possible that the marginal expenditure on one good or service should not yield more utility than the marginal expenditure on the other (or \"vice versa\") is if the marginal expenditures yield \"equal\" utility.\n\nAssume that utility, goods, and services have the requisite properties so that formula_7 is well defined for each good or service. An agent then optimizes\nsubject to a budget constraint\nwhere\nUsing the method of Lagrange multipliers, one constructs the function\nand finds the first-order conditions for optimization as\n(which simply implies that all of formula_10 will be spent) and\nso that\nwhich is algebraically equivalent to\nSince every such ratio is equal to formula_17, the ratios are all equal one to another:\n\n\n"}
{"id": "630398", "url": "https://en.wikipedia.org/wiki?curid=630398", "title": "Identity of indiscernibles", "text": "Identity of indiscernibles\n\nThe identity of indiscernibles is an ontological principle that states that there cannot be separate objects or entities that have all their properties in common. That is, entities \"x\" and \"y\" are identical if every predicate possessed by \"x\" is also possessed by \"y\" and vice versa; to suppose two things indiscernible is to suppose the same thing under two names. It states that no two distinct things (such as snowflakes) can be exactly alike, but this is intended as a metaphysical principle rather than one of natural science. A related principle is the indiscernibility of identicals, discussed below.\n\nA form of the principle is attributed to the German philosopher Gottfried Wilhelm Leibniz. It is one of his two great metaphysical principles, the other being the principle of sufficient reason. Both are famously used in his arguments with Newton and Clarke in the Leibniz–Clarke correspondence. Because of its association with Leibniz, the principle is sometimes known as Leibniz's law. \n\nSome philosophers have decided, however, that it is important to exclude certain predicates (or purported predicates) from the principle in order to avoid either triviality or contradiction. An example (detailed below) is the predicate that denotes whether an object is equal to \"x\" (often considered a valid predicate). As a consequence, there are a few different versions of the principle in the philosophical literature, of varying logical strength—and some of them are termed \"the strong principle\" or \"the weak principle\" by particular authors, in order to distinguish between them.\n\nWillard Van Orman Quine thought that the failure of substitutivity in intensional contexts (e.g., \"Sally believes that \"p\"\" or \"It is necessarily the case that \"q\"\") shows that modal logic is an impossible project. Saul Kripke holds that this failure may be the result of the use of the disquotational principle implicit in these proofs, and not a failure of substitutivity as such.\n\nThe identity of indiscernibles has been used to motivate notions of noncontextuality within quantum mechanics.\n\nAssociated with this principle is also the question as to whether it is a logical principle, or merely an empirical principle.\n\nThere are two principles here that must be distinguished (equivalent versions of each are given in the language of the predicate calculus). Note that these are all second-order expressions. Neither of these principles can be expressed in first-order logic (are nonfirstorderizable).\n\nPrinciple 1 doesn't entail reflexivity of = (or any other relation \"R\" substituted for it), but both properties together entail symmetry and transitivity (see proof box). Therefore, Principle 1 and reflexivity is sometimes used as a (second-order) axiomatization for the equality relation.\n\nPrinciple 1 is taken to be a logical truth and (for the most part) uncontroversial. Principle 2, on the other hand, is controversial; Max Black famously argued against it.\n\nThe above formulations are not satisfactory, however: the second principle should be read as having an implicit side-condition excluding any predicates that are equivalent (in some sense) to any of the following:\nIf all such predicates are included, then the second principle as formulated above can be trivially and uncontroversially shown to be a logical tautology: if \"x\" is non-identical to \"y\", then there will always be a putative \"property\" that distinguishes them, namely \"being identical to \"x\"\".\n\nOn the other hand, it is incorrect to exclude all predicates that are materially equivalent (i.e., contingently equivalent) to one or more of the four given above. If this is done, the principle says that in a universe consisting of two non-identical objects, because all distinguishing predicates are materially equivalent to at least one of the four given above (in fact, they are each materially equivalent to two of them), the two non-identical objects are identical—which is a contradiction.\n\nMax Black has argued against the identity of indiscernibles by counterexample. Notice that to show that the identity of indiscernibles is false, it is sufficient that one provides a model in which there are two distinct (numerically nonidentical) things that have all the same properties. He claimed that in a symmetric universe wherein only two symmetrical spheres exist, the two spheres are two distinct objects even though they have all their properties in common.\n\nBlack's argument appears significant because it shows that even relational properties (properties specifying distances between objects in space-time) fail to distinguish two identical objects in a symmetrical universe. Per his argument, two objects are, and will remain, equidistant from the universe's plane of symmetry and each other. Even bringing in an external observer to label the two spheres distinctly does not solve the problem, because it violates the symmetry of the universe.\n\nAs stated above, the principle of indiscernibility of identicals—that if two objects are in fact one and the same, they have all the same properties—is mostly uncontroversial. However, one famous application of the indiscernibility of identicals was by René Descartes in his \"Meditations on First Philosophy\". Descartes concluded that he could not doubt the existence of himself (the famous \"cogito\" argument), but that he \"could\" doubt the existence of his body.\n\nThis argument is criticized by some modern philosophers on the grounds that it allegedly derives a conclusion about what is true from a premise about what people know. What people know or believe about an entity, they argue, is not really a characteristic of that entity. A response may be that the argument in the \"Meditations on First Philosophy\" is that the inability of Descartes to doubt the existence of his mind is part of his mind's essence. One may then argue that identical things should have identical essences.\n\nNumerous counterexamples are given to debunk Descartes' reasoning via \"reductio ad absurdum\", such as the following argument based on a secret identity:\n\n\n\n"}
{"id": "34644725", "url": "https://en.wikipedia.org/wiki?curid=34644725", "title": "Jurisprudence of concepts", "text": "Jurisprudence of concepts\n\nThe jurisprudence of concepts was the first \"sub-school\" of legal positivism, according to which, the written law must reflect concepts, when interpreted. Its main representatives were Ihering, Savigny and Puchta.\n\nThis school was, thus, the preceding trigger of the idea that law comes from a dogmatic source, imposition from man over man and not a \"natural\" consequence of other sciences or of metaphysical faith.\n\nAmong the main characters of the \"jurisprudence of concepts\" are:\n\nSo, according to this school, law should have prevailing sources based upon the legislative process, although needing to be proven by more inclusive ideas of a social sense.\n\n"}
{"id": "855980", "url": "https://en.wikipedia.org/wiki?curid=855980", "title": "Just-world hypothesis", "text": "Just-world hypothesis\n\nThe just-world hypothesis or just-world fallacy is the cognitive bias (or assumption) that a person's actions are inherently inclined to bring morally fair and fitting consequences to that person, to the end of all noble actions being eventually rewarded and all evil actions eventually punished. In other words, the just-world hypothesis is the tendency to attribute consequences to—or expect consequences as the result of—a universal force that restores moral balance. This belief generally implies the existence of cosmic justice, destiny, divine providence, desert, stability, or order, and has high potential to result in fallacy, especially when used to rationalize people's misfortune on the grounds that they \"deserve\" it.\n\nThe hypothesis popularly appears in the English language in various figures of speech that imply guaranteed negative reprisal, such as: \"you got what was coming to you\", \"what goes around comes around\", \"chickens come home to roost\", \"everything happens for a reason\", and \"you reap what you sow\". This hypothesis has been widely studied by social psychologists since Melvin J. Lerner conducted seminal work on the belief in a just world in the early 1960s. Research has continued since then, examining the predictive capacity of the hypothesis in various situations and across cultures, and clarifying and expanding the theoretical understandings of just-world beliefs.\n\nMany philosophers and social theorists have observed and considered the phenomenon of belief in a just world, going back to at least as early as the Pyrrhonist philosopher Sextus Empiricus writing around 180 CE who argued against this belief. Lerner's work made the just-world hypothesis a focus of research in the field of social psychology.\n\nLerner was prompted to study justice beliefs and the just-world hypothesis in the context of social psychological inquiry into negative social and societal interactions. Lerner saw his work as extending Stanley Milgram's work on obedience. He sought to answer the questions of how regimes that cause cruelty and suffering maintain popular support, and how people come to accept social norms and laws that produce misery and suffering.\n\nLerner's inquiry was influenced by repeatedly witnessing the tendency of observers to blame victims for their suffering. During his clinical training as a psychologist, he observed treatment of mentally ill persons by the health care practitioners with whom he worked. Although he knew them to be kindhearted, educated people, they often blamed patients for the patients' own suffering. Lerner also describes his surprise at hearing his students derogate (disparage, belittle) the poor, seemingly oblivious to the structural forces that contribute to poverty. In a study on rewards, he observed that when one of two men was chosen at random to receive a reward for a task, that caused him to be more favorably evaluated by observers, even when the observers had been informed that the recipient of the reward was chosen at random. Existing social psychological theories, including cognitive dissonance, could not fully explain these phenomena. The desire to understand the processes that caused these phenomena led Lerner to conduct his first experiments on what is now called the just-world hypothesis.\n\nIn 1966, Lerner and his colleagues began a series of experiments that used shock paradigms to investigate observer responses to victimization. In the first of these experiments conducted at the University of Kansas, 72 female subjects were made to watch a confederate receiving electrical shocks under a variety of conditions. Initially, subjects were upset by observing the apparent suffering. But as the suffering continued and observers remained unable to intervene, the observers began to derogate the victim. Derogation was greater when the observed suffering was greater. But when subjects were told the victim would receive compensation for her suffering, subjects did not derogate the victim. Lerner and colleagues replicated these findings in subsequent studies, as did other researchers.\n\nTo explain these studies' findings, Lerner theorized that there was a prevalent belief in a just world. A just world is one in which actions and conditions have predictable, appropriate consequences. These actions and conditions are typically individuals' behaviors or attributes. The specific conditions that correspond to certain consequences are socially determined by a society's norms and ideologies. Lerner presents the belief in a just world as functional: it maintains the idea that one can influence the world in a predictable way. Belief in a just world functions as a sort of \"contract\" with the world regarding the consequences of behavior. This allows people to plan for the future and engage in effective, goal-driven behavior. Lerner summarized his findings and his theoretical work in his 1980 monograph \"The Belief in a Just World: A Fundamental Delusion\".\n\nLerner hypothesized that the belief in a just world is crucially important for people to maintain for their own well-being. But people are confronted daily with evidence that the world is not just: people suffer without apparent cause. Lerner explained that people use strategies to eliminate threats to their belief in a just world. These strategies can be rational or irrational. Rational strategies include accepting the reality of injustice, trying to prevent injustice or provide restitution, and accepting one's own limitations. Non-rational strategies include denial, withdrawal, and reinterpretation of the event.\n\nThere are a few modes of reinterpretation that could make an event fit the belief in a just world. One can reinterpret the outcome, the cause, and/or the character of the victim. In the case of observing the injustice of the suffering of innocent people, one major way to rearrange the cognition of an event is to interpret the victim of suffering as deserving. Specifically, observers can blame victims for their suffering on the basis of their behaviors and/or their characteristics. Much psychological research on the belief in a just world has focused on these negative social phenomena of victim blaming and victim derogation in different contexts.\n\nAn additional effect of this thinking is that individuals experience less personal vulnerability because they do not believe they have done anything to deserve or cause negative outcomes. This is related to the self-serving bias observed by social psychologists.\n\nMany researchers have interpreted just-world beliefs as an example of causal attribution. In victim blaming, the causes of victimization are attributed to an individual rather than to a situation. Thus, the consequences of belief in a just world may be related to or explained in terms of particular patterns of causal attribution.\n\nOthers have suggested alternative explanations for the derogation of victims. One suggestion is that derogation effects are based on accurate judgments of a victim's character. In particular, in relation to Lerner's first studies, some have hypothesized that it would be logical for observers to derogate an individual who would allow himself to be shocked without reason. A subsequent study by Lerner challenged this alternative hypothesis by showing that individuals are only derogated when they actually suffer; individuals who agreed to undergo suffering but did not were viewed positively.\n\nAnother alternative explanation offered for the derogation of victims early in the development of the just-world hypothesis was that observers derogate victims to reduce their own feelings of guilt. Observers may feel responsible, or guilty, for a victim's suffering if they themselves are involved in the situation or experiment. In order to reduce the guilt, they may devalue the victim. Lerner and colleagues claim that there has not been adequate evidence to support this interpretation. They conducted one study that found derogation of victims occurred even by observers who were not implicated in the process of the experiment and thus had no reason to feel guilty.\n\nAlternatively, victim derogation and other strategies may only be ways to alleviate discomfort after viewing suffering. This would mean that the primary motivation is not to restore a belief in a just world, but to reduce discomfort caused by empathizing. Studies have shown that victim derogation does not suppress subsequent helping activity and that empathizing with the victim plays a large role when assigning blame. According to Ervin Staub, devaluing the victim should lead to lesser compensation if restoring belief in a just world was the primary motive; instead, there is virtually no difference in compensation amounts whether the compensation precedes or follows devaluation. Psychopathy has been linked to the lack of just-world maintaining strategies, possibly due to dampened emotional reactions and lack of empathy.\n\nAfter Lerner's first studies, other researchers replicated these findings in other settings in which individuals are victimized. This work, which began in the 1970s and continues today, has investigated how observers react to victims of random calamities like traffic accidents, as well as rape and domestic violence, illnesses, and poverty. Generally, researchers have found that observers of the suffering of innocent victims tend to both derogate and blame victims for their suffering. Observers thus maintain their belief in a just world by changing their cognitions about the victims' character.\n\nIn the early 1970s, social psychologists Zick Rubin and Letitia Anne Peplau developed a measure of belief in a just world. This measure and its revised form published in 1975 allowed for the study of individual differences in just-world beliefs. Much of the subsequent research on the just-world hypothesis used these measurement scales.\n\nResearchers have looked at how observers react to victims of rape and other violence. In a formative experiment on rape and belief in a just world by Linda Carli and colleagues, researchers gave two groups of subjects a narrative about interactions between a man and a woman. The description of the interaction was the same until the end; one group received a narrative that had a neutral ending and the other group received a narrative that ended with the man raping the woman. Subjects judged the rape ending as inevitable and blamed the woman in the narrative for the rape on the basis of her behavior, but not her characteristics. These findings have been replicated repeatedly, including using a rape ending and a 'happy ending' (a marriage proposal).\n\nOther researchers have found a similar phenomenon for judgments of battered partners. One study found that observers' labels of blame of female victims of relationship violence increase with the intimacy of the relationship. Observers blamed the perpetrator only in the most significant case of violence, in which a male struck an acquaintance.\n\nResearchers have employed the just-world hypothesis to understand bullying. Given other research on beliefs in a just world, it would be expected that observers would derogate and blame bullying victims, but the opposite has been found: individuals high in just-world belief have stronger anti-bullying attitudes. Other researchers have found that strong belief in a just world is associated with lower levels of bullying behavior. This finding is in keeping with Lerner's understanding of belief in a just world as functioning as a \"contract\" that governs behavior. There is additional evidence that belief in a just world is protective of the well-being of children and adolescents in the school environment, as has been shown for the general population.\n\nOther researchers have found that observers judge sick people as responsible for their illnesses. One experiment showed that persons suffering from a variety of illnesses were derogated on a measure of attractiveness more than healthy individuals were. In comparison to healthy people, victim derogation was found for persons presenting with indigestion, pneumonia, and stomach cancer. Moreover, derogation was found to be higher for those suffering from more severe illnesses, except for those presenting with cancer. Stronger belief in a just world has also been found to correlate with greater derogation of AIDS victims.\n\nMore recently, researchers have explored how people react to poverty through the lens of the just-world hypothesis. Strong belief in a just world is associated with blaming the poor, with weak belief in a just world associated with identifying external causes of poverty including world economic systems, war, and exploitation.\n\nSome research on belief in a just world has examined how people react when they themselves are victimized. An early paper by Dr. Ronnie Janoff-Bulman found that rape victims often blame their own behavior, but not their own characteristics, for their victimization. It was hypothesized that this may be because blaming one's own behavior makes an event more controllable.\n\nThese studies on victims of violence, illness, and poverty and others like them have provided consistent support for the link between observers' just-world beliefs and their tendency to blame victims for their suffering. As a result, the existence of the just-world hypothesis as a psychological phenomenon has become widely accepted.\n\nSubsequent work on measuring belief in a just world has focused on identifying multiple dimensions of the belief. This work has resulted in the development of new measures of just-world belief and additional research. Hypothesized dimensions of just-world beliefs include belief in an unjust world, beliefs in immanent justice and ultimate justice, hope for justice, and belief in one's ability to reduce injustice. Other work has focused on looking at the different domains in which the belief may function; individuals may have different just-world beliefs for the personal domain, the sociopolitical domain, the social domain, etc. An especially fruitful distinction is between the belief in a just world for the self (personal) and the belief in a just world for others (general). These distinct beliefs are differentially associated with positive mental health.\n\nResearchers have used measures of belief in a just world to look at correlates of high and low levels of belief in a just world.\n\nLimited studies have examined ideological correlates of the belief in a just world. These studies have found sociopolitical correlates of just-world beliefs, including right-wing authoritarianism and the protestant work ethic. Studies have also found belief in a just world to be correlated with aspects of religiousness.\n\nStudies of demographic differences, including gender and racial differences, have not shown systematic differences, but do suggest racial differences, with blacks and African Americans having the lowest levels of belief in a just world.\n\nThe development of measures of just-world beliefs has also allowed researchers to assess cross-cultural differences in just-world beliefs. Much research conducted shows that beliefs in a just world are evident cross-culturally. One study tested beliefs in a just world of students in 12 countries. This study found that in countries where the majority of inhabitants are powerless, belief in a just world tends to be weaker than in other countries. This supports the theory of the just-world hypothesis because the powerless have had more personal and societal experiences that provided evidence that the world is not just and predictable.\n\nBelief in unjust world has been linked to increased self-handicapping, criminality, defensive coping, anger and perceived future risk. It may also serve as ego-protective belief for certain individuals by justifying maladaptive behavior.\n\nAlthough much of the initial work on belief in a just world focused on its negative social effects, other research suggests that belief in a just world is good, and even necessary, for mental health. Belief in a just world is associated with greater life satisfaction and well-being and less depressive affect. Researchers are actively exploring the reasons why the belief in a just world might have this relationship to mental health; it has been suggested that such beliefs could be a personal resource or coping strategy that buffers stress associated with daily life and with traumatic events. This hypothesis suggests that belief in a just world can be understood as a positive illusion.\n\nSome studies also show that beliefs in a just world are correlated with internal locus of control. Strong belief in a just world is associated with greater acceptance of and less dissatisfaction with negative events in one's life. This may be one way in which belief in a just world affects mental health. Others have suggested that this relationship holds only for beliefs in a just world for oneself. Beliefs in a just world for others are related instead to the negative social phenomena of victim blaming and victim derogation observed in other studies.\n\nMore than 40 years after Lerner's seminal work on belief in a just world, researchers continue to study the phenomenon. Work continues primarily in the United States, Europe, Australia, and Asia. Researchers in Germany have contributed disproportionately to recent research. Their work resulted in a volume edited by Lerner and German researcher Leo Montada titled \"Responses to Victimizations and Belief in a Just World\".\n\n\n"}
{"id": "25200737", "url": "https://en.wikipedia.org/wiki?curid=25200737", "title": "Knowledge arena", "text": "Knowledge arena\n\nA knowledge arena is a virtual space where individuals can manipulate concepts and relationships to form a concept map. Individuals using a computer with appropriate software can represent concepts and the relationships between concepts in a node-relationship-node formalism. The process of thinking about the concepts and making associations between them has been called \"off-loading\" by McAleese.\n\nThe concept map is a form of a semantic network or semantic graph. It is formally based on graph theory. In the concept map, concepts are represented by nodes. The relationship between nodes are represented by \"typed links\" or \"edges\". (See Graph theory.) In creating a map or graphic representation of what is known an individual intentionally interacts with the graphical interface or map and through a reflective process adds nodes (concepts) and /or adds relationships (edges or typed links) or modifies existing node-relationship-node instances. It is likely that the process of engaging with concepts and relationships between concepts brings about the creation of understandings as well as making the understandings explicit.\n\nMany different claims have been made for the utility of the concept map. The interactive and reflective nature of map creation is highlighted by the use of the description Knowledge Arena. Although maps may represent what an individual knows at a point in time; it is likely that by interacting with the concepts and relationships in the knowledge arena individual continues to create and modify what that individual \"knows\".\n\nSee also \n"}
{"id": "1402030", "url": "https://en.wikipedia.org/wiki?curid=1402030", "title": "Marginal revenue productivity theory of wages", "text": "Marginal revenue productivity theory of wages\n\nThe marginal revenue productivity theory of wages is a theory in neoclassical economics stating that wages are paid at a level equal to the marginal revenue product of labor, MRP (the value of the marginal product of labor), which is the increment to revenues caused by the increment to output produced by the last laborer employed. In a model, this is justified by an assumption that the firm is profit-maximizing and thus would employ labor only up to the point that marginal labor costs equal the marginal revenue generated for the firm. \n\nThe marginal revenue product (MRP) of a worker is equal to the product of the marginal product of labour (MP) (the increment to output from an increment to labor used) and the marginal revenue (MR) (the increment to sales revenue from an increment to output): MRP = MP × MR. The theory states that workers will be hired up to the point when the marginal revenue product is equal to the wage rate. If the marginal revenue brought by the worker is less than the wage rate, then employing that laborer would cause a decrease in profit.\n\nThe idea that payments to factors of production equal their marginal productivity had been laid out by John Bates Clark and Knut Wicksell, in simpler models. Much of the MRP theory stems from Wicksell's model.\n\nThe marginal revenue product of labour MRP is the increase in revenue per unit increase in the variable input = ∆TR/∆L \n\nThe change in output is not limited to that directly attributable to the additional worker. Assuming that the firm is operating with diminishing marginal returns then the addition of an extra worker reduces the average productivity of every other worker (and every other worker affects the marginal productivity of the additional worker).\n\nAs above noted the firm will continue to add units of labor until the MRP equals the wage rate \"w\"—mathematically until\n\nUnder perfect competition, marginal revenue product is equal to marginal physical product (extra unit produced as a result of a new employment) multiplied by price.\n\nThis is because the firm in perfect competition is a price taker. It does not have to lower the price in order to sell additional units of the good.\n\nFirms operating as monopolies or in imperfect competition face downward-sloping demand curves. To sell extra units of output, they would have to lower their output's price. Under such market conditions, marginal revenue product will not equal MPP×Price. This is because the firm is not able to sell output at a fixed price per unit. Thus the MRP curve of a firm in monopoly or in imperfect competition will slope downwards, when plotted against labor usage, at a faster rate than in perfect specific competition.\n"}
{"id": "994704", "url": "https://en.wikipedia.org/wiki?curid=994704", "title": "Mental model", "text": "Mental model\n\nA mental model is an explanation of someone's thought process about how something works in the real world. It is a representation of the surrounding world, the relationships between its various parts and a person's intuitive perception about his or her own acts and their consequences. Mental models can help shape behaviour and set an approach to solving problems (similar to a personal algorithm) and doing tasks.\n\nA mental model is a kind of internal symbol or representation of external reality, hypothesized to play a major role in cognition, reasoning and decision-making. Kenneth Craik suggested in 1943 that the mind constructs \"small-scale models\" of reality that it uses to anticipate events.\n\nJay Wright Forrester defined general mental models as:\nThe image of the world around us, which we carry in our head, is just a model. Nobody in his head imagines all the world, government or country. He has only selected concepts, and relationships between them, and uses those to represent the real system (Forrester, 1971).\n\nIn psychology, the term \"mental models\" is sometimes used to refer to mental representations or mental simulation generally. At other times it is used to refer to and to the mental model theory of reasoning developed by Philip Johnson-Laird and Ruth M.J. Byrne.\n\nThe term \"mental model\" is believed to have originated with Kenneth Craik in his 1943 book \"The Nature of Explanation\". in \"Le dessin enfantin\" (Children's drawings), published in 1927 by Alcan, Paris, argued that children construct internal models, a view that influenced, among others, child psychologist Jean Piaget.\n\nPhilip Johnson-Laird published \"Mental Models: Towards a Cognitive Science of Language, Inference and Consciousness\" in 1983. In the same year, Dedre Gentner and Albert Stevens edited a collection of chapters in a book also titled \"Mental Models\". The first line of their book explains the idea further: \"One function of this chapter is to belabor the obvious; people's views of the world, of themselves, of their own capabilities, and of the tasks that they are asked to perform, or topics they are asked to learn, depend heavily on the conceptualizations that they bring to the task.\" (see the book: \"Mental Models\").\n\nSince then, there has been much discussion and use of the idea in human-computer interaction and usability by researchers including Donald Norman and Steve Krug (in his book \"Don't Make Me Think\"). Walter Kintsch and Teun A. van Dijk, using the term \"situation model\" (in their book \"Strategies of Discourse Comprehension\", 1983), showed the relevance of mental models for the production and comprehension of discourse.\n\nOne view of human reasoning is that it depends on mental models. In this view, mental models can be constructed from perception, imagination, or the comprehension of discourse (Johnson-Laird, 1983). Such mental models are similar to architects' models or to physicists' diagrams in that their structure is analogous to the structure of the situation that they represent, unlike, say, the structure of logical forms used in formal rule theories of reasoning. In this respect, they are a little like pictures in the picture theory of language described by philosopher Ludwig Wittgenstein in 1922. Philip Johnson-Laird and Ruth M.J. Byrne developed a theory of mental models which makes the assumption that reasoning depends, not on logical form, but on mental models (Johnson-Laird and Byrne, 1991).\n\nMental models are based on a small set of fundamental assumptions (axioms), which distinguish them from other proposed representations in the psychology of reasoning (Byrne and Johnson-Laird, 2009). Each mental model represents a possibility. A mental model represents one possibility, capturing what is common to all the different ways in which the possibility may occur (Johnson-Laird and Byrne, 2002). Mental models are iconic, i.e., each part of a model corresponds to each part of what it represents (Johnson-Laird, 2006). Mental models are based on a principle of truth: they typically represent only those situations that are possible, and each model of a possibility represents only what is true in that possibility according to the proposition. However, mental models can represent what is false, temporarily assumed to be true, for example, in the case of counterfactual conditionals and counterfactual thinking (Byrne, 2005).\n\nPeople infer that a conclusion is valid if it holds in all the possibilities. Procedures for reasoning with mental models rely on counter-examples to refute invalid inferences; they establish validity by ensuring that a conclusion holds over all the models of the premises. Reasoners focus on a subset of the possible models of multiple-model problems, often just a single model. The ease with which reasoners can make deductions is affected by many factors, including age and working memory (Barrouillet, et al., 2000). They reject a conclusion if they find a counterexample, i.e., a possibility in which the premises hold, but the conclusion does not (Schroyens, et al. 2003; Verschueren, et al., 2005).\n\nScientific debate continues about whether human reasoning is based on mental models, versus formal rules of inference (e.g., O'Brien, 2009), domain-specific rules of inference (e.g., Cheng & Holyoak, 2008; Cosmides, 2005), or probabilities (e.g., Oaksford and Chater, 2007). Many empirical comparisons of the different theories have been carried out (e.g., Oberauer, 2006).\n\nA mental model is generally:\n\nMental models are a fundamental way to understand organizational learning. Mental models, in popular science parlance, have been described as \"deeply held images of thinking and acting\". Mental models are so basic to understanding the world that people are hardly conscious of them.\n\nS.N. Groesser and M. Schaffernicht (2012) describe three basic methods which are typically used:\nThese methods allow showing a mental model of a dynamic system, as an explicit, written model about a certain system based on internal beliefs. Analyzing these graphical representations has been an increasing area of research across many social science fields. Additionally software tools that attempt to capture and analyze the structural and functional properties of individual mental models such as Mental Modeler, \"a participatory modeling tool based in fuzzy-logic cognitive mapping\", have recently been developed and used to collect/compare/combine mental model representations collected from individuals for use in social science research, collaborative decision-making, and natural resource planning.\n\nIn the simplification of reality, creating a model can find a sense of reality, seeking to overcome systemic thinking and system dynamics.\n\nThese two disciplines can help to construct a better coordination with the reality of mental models and simulate it accurately. They increase the probability that the consequences of how to decide and act in accordance with how to plan.\n\n\nAfter analyzing the basic characteristics, it is necessary to bring the process of changing the mental models, or the process of learning. Learning is a back-loop process, and feedback loops can be illustrated as: single-loop learning or double-loop learning.\n\nMental models affect the way that people work with information, and also how they determine the final decision. The decision itself changes, but the mental models remain the same. It is the predominant method of learning, because it is very convenient.\n\nDouble-loop learning (\"see diagram below\") is used when it is necessary to change the mental model on which a decision depends. Unlike single loops, this model includes a shift in understanding, from simple and static to broader and more dynamic, such as taking into account the changes in the surroundings and the need for expression changes in mental models.\n\n\n\n"}
{"id": "4718632", "url": "https://en.wikipedia.org/wiki?curid=4718632", "title": "Mental representation", "text": "Mental representation\n\nA mental representation (or cognitive representation), in philosophy of mind, cognitive psychology, neuroscience, and cognitive science, is a hypothetical internal cognitive symbol that represents external reality, or else a mental process that makes use of such a symbol: \"a formal system for making explicit certain entities or types of information, together with a specification of how the system does this\".\n\nMental representation is the mental imagery of things that are not actually present to the senses. In contemporary philosophy, specifically in fields of metaphysics such as philosophy of mind and ontology, a mental representation is one of the prevailing ways of explaining and describing the nature of ideas and concepts.\n\nMental representations (or mental imagery) enable representing things that have never been experienced as well as things that do not exist. Think of yourself traveling to a place you have never visited before, or having a third arm. These things have either never happened or are impossible and do not exist, yet our brain and mental imagery allows us to imagine them. Although visual imagery is more likely to be recalled, mental imagery may involve representations in any of the sensory modalities, such as hearing, smell, or taste. Stephen Kosslyn proposes that images are used to help solve certain types of problems. We are able to visualize the objects in question and mentally represent the images to solve it.\n\nMental representations also allow people to experience things right in front of them—though the process of how the brain interprets the representational content is debated.\n\nRepresentationalism (also known as indirect realism) is the view that representations are the main way we access external reality. Another major prevailing philosophical theory posits that concepts are entirely abstract objects.\n\nThe representational theory of mind attempts to explain the nature of ideas, concepts and other mental content in contemporary philosophy of mind, cognitive science and experimental psychology. In contrast to theories of naive or direct realism, the representational theory of mind postulates the actual existence of mental representations which act as intermediaries between the observing subject and the objects, processes or other entities observed in the external world. These intermediaries stand for or represent to the mind the objects of that world.\n\nFor example, when someone arrives at the belief that his or her floor needs sweeping, the representational theory of mind states that he or she forms a mental representation that represents the floor and its state of cleanliness.\n\nThe original or \"classical\" representational theory probably can be traced back to Thomas Hobbes and was a dominant theme in classical empiricism in general. According to this version of the theory, the mental representations were images (often called \"ideas\") of the objects or states of affairs represented. For modern adherents, such as Jerry Fodor, Steven Pinker and many others, the representational system consists rather of an internal language of thought (i.e., mentalese). The contents of thoughts are represented in symbolic structures (the formulas of Mentalese) which, analogously to natural languages but on a much more abstract level, possess a syntax and semantics very much like those of natural languages. For the Spanish logician and cognitive scientist Luis M. Augusto, at this abstract, formal level, the syntax of thought is the set of symbol rules (i.e., operations, processes, etc. on and with symbol structures) and the semantics of thought is the set of symbol structures (concepts and propositions). Content (i.e., thought) emerges from the meaningful co-occurrence of both sets of symbols. For instance, \"8 x 9\" is a meaningful co-occurrence, whereas \"CAT x §\" is not; \"x\" is a symbol rule called for by symbol structures such as \"8\" and \"9\", but not by \"CAT\" and \"§\".\n\nThere are two types of representationalism, strong and weak. Strong representationalism attempts to reduce phenomenal character to intentional content. On the other hand, weak representationalism claims only that phenomenal character supervenes on intentional content. Strong representationalism aims to provide a theory about the nature of phenomenal character, and offers a solution to the hard problem of consciousness. In contrast to this, weak representationalism does not aim to provide a theory of consciousness, nor does it offer a solution to the hard problem of consciousness.\n\nStrong representationalism can be further broken down into restricted and unrestricted versions. The restricted version deals only with certain kinds of phenomenal states e.g. visual perception. Most representationalists endorse an unrestricted version of representationalism. According to the unrestricted version, for any state with phenomenal character that state’s phenomenal character reduces to its intentional content. Only this unrestricted version of representationalism is able to provide a general theory about the nature of phenomenal character, as well as offer a potential solution to the hard problem of consciousness. The successful reduction of the phenomenal character of a state to its intentional content would provide a solution to the hard problem of consciousness once a physicalist account of intentionality is worked out.\n\nWhen arguing against the unrestricted version of representationalism people will often bring up phenomenal mental states that appear to lack intentional content. The unrestricted version seeks to account for all phenomenal states. Thus, for it to be true, all states with phenomenal character must have intentional content to which that character is reduced. Phenomenal states without intentional content therefore serve as a counterexample to the unrestricted version. If the state has no intentional content its phenomenal character will not be reducible to that state’s intentional content, for it has none to begin with.\n\nA common example of this kind of state are moods. Moods are states with phenomenal character that are generally thought to not be directed at anything in particular. Moods are thought to lack directedness, unlike emotions, which are typically thought to be directed at particular things e.g. you are mad \"at\" your sibling, you are afraid \"of\" a dangerous animal. People conclude that because moods are undirected they are also nonintentional i.e. they lack intentionality or aboutness. Because they are not directed at anything they are not about anything. Because they lack intentionality they will lack any intentional content. Lacking intentional content their phenomenal character will not be reducible to intentional content, refuting the representational doctrine.\n\nThough emotions are typically considered as having directedness and intentionality this idea has also been called into question. One might point to emotions a person all of a sudden experiences that do not appear to be directed at or about anything in particular. Emotions elicited by listening to music are another potential example of undirected, nonintentional emotions. Emotions aroused in this way do not seem to necessarily be about anything, including the music that arouses them.\n\nIn response to this objection a proponent of representationalism might reject the undirected nonintentionality of moods, and attempt to identify some intentional content they might plausibly be thought to possess. The proponent of representationalism might also reject the narrow conception of intentionality as being directed at a particular thing, arguing instead for a broader kind of intentionality.\n\nThere are three alternative kinds of directedness/intentionality one might posit for moods. \nIn the case of outward directedness moods might be directed at either the world as a whole, a changing series of objects in the world, or unbound emotion properties projected by people onto things in the world. In the case of inward directedness moods are directed at the overall state of a person’s body. In the case of hybrid directedness moods are directed at some combination of inward and outward things.\n\nEven if one can identify some possible intentional content for moods we might still question whether that content is able to sufficiently capture the phenomenal character of the mood states they are a part of. Amy Kind contends that in the case of all the previously mentioned kinds of directedness (outward, inward, and hybrid) the intentional content supplied to the mood state is not capable of sufficiently capturing the phenomenal aspects of the mood states. In the case of inward directedness, the phenomenology of the mood does not seem tied to the state of one’s body, and even if one’s mood is reflected by the overall state of one’s body that person will not necessarily be aware of it, demonstrating the insufficiency of the intentional content to adequately capture the phenomenal aspects of the mood. In the case of outward directedness, the phenomenology of the mood and its intentional content do not seem to share the corresponding relation they should given that the phenomenal character is supposed to reduce to the intentional content. Hybrid directedness, if it can even get off the ground, faces the same objection.\n\nThere is a wide debate on what kinds of representations exist. There are several philosophers who bring about different aspects of the debate. Such philosophers include Alex Morgan, Gualtiero Piccinini, and Uriah Kriegel—though this is not an exhaustive list.\n\nThere are \"job description\" representations. That is representations that (1) represent something—have intentionality, (2) have a special relation—the represented object does not need to exist, and (3) content plays a causal role in what gets represented: e.g. saying \"hello\" to a friend, giving a glare to an enemy.\n\nStructural representations are also important. These types of representations are basically mental maps that we have in our minds that correspond exactly to those objects in the world (the intentional content). According to Morgan, structural representations are not the same as mental representations—there is nothing mental about them: plants can have structural representations.\n\nThere are also internal representations. These types of representations include those that involve future decisions, episodic memories, or any type of projection into the future.\n\nIn Gualtiero Piccinini's forthcoming work, he discusses topics on natural and nonnatural mental representations. He relies on the natural definition of mental representations given by Grice (1957) where \"P entails that P\". e.g. Those spots mean measles, entails that the patient has measles. Then there are nonnatural representations: \"P does not entail P\". e.g. The 3 rings on the bell of a bus mean the bus is full—the rings on the bell are independent of the fullness of the bus—we could have assigned something else (just as arbitrary) to signify that the bus is full.\n\nThere are also objective and subjective mental representations. Objective representations are closest to tracking theories—where the brain simply tracks what is in the environment. If there is a blue bird outside my window, the objective representation is that of the blue bird. Subjective representations can vary person-to-person. For example, if I am colorblind, that blue bird outside my window will not \"appear\" blue to me since I cannot represent the blueness of blue (i.e. I cannot see the color blue). The relationship between these two types of representation can vary.\n\nEliminativists think that subjective representations don't exist. Reductivists think subjective representations are reducible to objective. Non-reductivists think that subjective representations are real and distinct.\n\n\n"}
{"id": "407044", "url": "https://en.wikipedia.org/wiki?curid=407044", "title": "Negative (photography)", "text": "Negative (photography)\n\nIn photography, a negative is an image, usually on a strip or sheet of transparent plastic film, in which the lightest areas of the photographed subject appear darkest and the darkest areas appear lightest. This reversed order occurs because the extremely light-sensitive chemicals a camera film must use to capture an image quickly enough for ordinary picture-taking are darkened, rather than bleached, by exposure to light and subsequent photographic processing.\n\nIn the case of color negatives, the colors are also reversed into their respective complementary colors. Typical color negatives have an overall dull orange tint due to an automatic color-masking feature that ultimately results in improved color reproduction.\n\nNegatives are normally used to make positive prints on photographic paper by projecting the negative onto the paper with a photographic enlarger or making a contact print. The paper is also darkened in proportion to its exposure to light, so a second reversal results which restores light and dark to their normal order.\n\nNegatives were once commonly made on a thin sheet of glass rather than a plastic film, and some of the earliest negatives were made on paper.\n\nIt is incorrect to call an image a negative solely because it is on a transparent material. Transparent prints can be made by printing a negative onto special positive film, as is done to make traditional motion picture film prints for use in theaters. Some films used in cameras are designed to be developed by reversal processing, which produces the final positive, instead of a negative, on the original film. Positives on film or glass are known as transparencies or diapositives, and if mounted in small frames designed for use in a slide projector or magnifying viewer they are commonly called slides.\n\nA positive image is a normal image. A negative image is a total inversion, in which light areas appear dark and vice versa. A negative color image is additionally color-reversed, with red areas appearing cyan, greens appearing magenta, and blues appearing yellow, and vice versa.\n\nFilm negatives usually have less contrast, but a wider dynamic range, than the final printed positive images. The contrast typically increases when they are printed onto photographic paper. When negative film images are brought into the digital realm, their contrast may be adjusted at the time of scanning or, more usually, during subsequent post-processing.\n\nFilm for cameras that use the 35 mm still format is sold as a long strip of emulsion-coated and perforated plastic spooled in a light-tight cassette. Before each exposure, a mechanism inside the camera is used to pull an unexposed area of the strip out of the cassette and into position behind the camera lens. When all exposures have been made the strip is rewound into the cassette. After the film is chemically developed, the strip shows a series of small negative images. It is usually then cut into sections for easier handling. Medium format cameras use 120 film, which yields a strip of negatives 60 mm wide, and large format cameras capture each image on a single sheet of film which may be as large as 20 x 25 cm (8 x 10 inches) or even larger. Each of these photographed images may be referred to as a negative and an entire strip or set of images may be collectively referred to as \"the negatives\". They are the master images, from which all positive prints will derive, so they are handled and stored with special care.\n\nMany photographic processes create negative images: the chemicals involved react when exposed to light, so that during development they produce deposits of microscopic dark silver particles or colored dyes in proportion to the amount of exposure. However, when a negative image is created from a negative image (just like multiplying two negative numbers in mathematics) a positive image results. This makes most chemical-based photography a two-step process, which uses negative film and ordinary processing. Special films and development processes have been devised so that positive images can be created directly on the film; these are called positive, or slide, or (perhaps confusingly) reversal films and reversal processing.\n\nDespite the market's evolution away from film, there is still a desire and market for products which allow fine art photographers to produce negatives from digital images for their use in alternative processes such as cyanotypes, gum bichromate, platinum prints, and many others.\n\n"}
{"id": "234029", "url": "https://en.wikipedia.org/wiki?curid=234029", "title": "Negative cache", "text": "Negative cache\n\nIn computer programming, negative cache is a cache that also stores \"negative\" responses, i.e. failures. This means that a program remembers the result indicating a failure even after the cause has been corrected. Usually negative cache is a design choice, but it can also be a software bug.\n\nConsider a web browser which attempts to load a page while the network is unavailable. The browser will receive an error code indicating the problem, and may display this error message to the user in place of the requested page. However, it is incorrect for the browser to place the error message in the page cache, as this would lead it to display the error again when the user tries to load the same page - even after the network is back up. The error message must not be cached under the page's URL; until the browser is able to successfully load the page, whenever the user tries to load the page, the browser must make a new attempt.\n\nA frustrating aspect of negative caches is that the user may put a great effort into troubleshooting the problem, and then after determining and removing the root cause, the error still does not vanish.\n\nThere are cases where failure-like states must be cached. For instance, DNS requires that caching nameservers remember negative responses as well as positive ones. If an authoritative nameserver returns a negative response, indicating that a name does not exist, this is cached. The negative response may be perceived as a failure at the application level; however, to the nameserver caching it, it is not a failure. The cache times for negative and positive caching may be tuned independently.\n\nA negative cache is normally only desired if failure is very expensive and the error condition arises automatically without user's action. It creates a situation where the user is unable to isolate the cause of the failure: despite fixing everything he/she can think of, the program still refuses to work. When a failure is cached, the program should provide a clear indication of what must be done to clear the cache, in addition to a description of the cause of the error. In such conditions a negative cache is an example of a design anti-pattern.\n\nNegative cache still may recover if the cached records expires.\n\n"}
{"id": "4639538", "url": "https://en.wikipedia.org/wiki?curid=4639538", "title": "Negative double", "text": "Negative double\n\nThe negative double is a form of takeout double in bridge. It is made by the responder after his right-hand opponent overcalls on the first round of bidding, and is used to show both support for the unbid suits as well as some values. It is treated as forcing, but not unconditionally so. In practice, the negative double is sometimes used as a sort of catch-all, made when no other call properly describes responder's hand. Therefore, a partnership might even treat the negative double as a wide-ranging call that merely shows some values.\n\nUsing the modern negative double convention, it is understood that a double over an initial overcall is conventional, and \"not\" for penalties (but see Playing for penalties). For example, using this convention, the following doubles would be regarded as negative, not for penalty:\n\n\nIn understandings regarding negative doubles, the emphasis is on major suit lengths. This is largely due to the special value that tournament play, especially the pairs game, places on major suits. Since the mid-1980s, the negative double has been used mainly to stand in for a bid in an unbid major suit.\n\nMost partnerships using the negative double agree that it applies only through a particular level of overcall. For example, they may agree that the double of an overcall through 3 is negative, and that beyond 3 a double is for penalties.\n\nAt rubber bridge many players are reluctant to give up the penalty double of an overcall, and so do not use the double as conventional.\n\nThe term \"negative double\" was initially employed to distinguish it from the \"penalty\", or \"business\", or \"positive\" double, and signified a double over an opponent's opening bid whose meaning was a request for partner to bid his best suit. Around 1930, the term \"informatory double\" replaced \"negative double\", and that term later gave way to \"takeout double\" as it is used at present; the original term \"negative double\" fell into disuse.\n\nIn 1957, Alvin Roth in his partnership with Tobias Stone appropriated the abandoned term \"negative double\" to denote a conventional double by responder over an overcall and gave it its current meaning. The bid was also briefly known as \"Sputnik\", because it was as new as the satellite of that name that the Soviet Union had recently launched. The term is still used sometimes in Europe.\n\nThe negative double is generally forcing, but opener might pass to convert the double to a penalty double. There is a special agreement called negative free bids, under which (after the overcall) the bid of a new suit by responder is not forcing. However, most negative doublers play that a new suit response (or free bid), whether at the one level or higher, is forcing.\n\nThe negative double loses even more definition when it can be made with a very broad range of strength, from roughly six HCP up to game forcing values. In a pinch, players use it to \"get by this round of bidding.\"\n\nThe negative double does not cause the partnership to completely lose the ability to penalize an overcall. There are two ways that the overcall can be doubled for penalties. For example:\n\n\nResponder makes a negative double, and opener passes for penalties. This position is analogous to one in which a player makes a takeout double and his partner passes the double, converting it to a penalty double.\n\n\nResponder passes the overcall, opener makes a re-opening double, and responder passes that double for penalties. This can be dangerous, because opener often doesn't know whether responder is simply too weak to make any call, or is hoping that opener can re-open with a double.\n\nThese situations are rare, though, and the more so because some five-card major partnerships play negative doubles \"over minor suit openings only.\" The rationale is that responder knows much more about opener's distribution after a major suit opening than after a minor suit opening, and can better judge whether to play in opener's major suit, to play for penalties by doubling, or to show a suit of his own.\n\nPartnerships have different understandings about the length in unbid suits that is shown by a negative double, and the understandings differ according both to which suits remain unbid and to the current level of the bidding. Nevertheless, the following are popular understandings:\n\n\n"}
{"id": "1762868", "url": "https://en.wikipedia.org/wiki?curid=1762868", "title": "Negative luminescence", "text": "Negative luminescence\n\nNegative luminescence is a physical phenomenon by which an electronic device emits less thermal radiation when an electric current is passed through it than it does in thermal equilibrium (current off). When viewed by a thermal camera, an operating negative luminescent device looks colder than its environment.\n\nNegative luminescence is most readily observed in semiconductors. Incoming infrared radiation is absorbed in the material by the creation of an electron–hole pair. An electric field is used to remove the electrons and holes from the region before they have a chance to recombine and re-emit thermal radiation. This effect occurs most efficiently in regions of low charge carrier density.\n\nNegative luminescence has also been observed in semiconductors in orthogonal electric and magnetic fields. In this case, the junction of a diode is not necessary and the effect can be observed in bulk material. A term that has been applied to this type of negative luminescence is \"galvanomagnetic luminescence\".\n\nNegative luminescence might appear to be a violation of Kirchhoff's law of thermal radiation. This is not true, as the law only applies in thermal equilibrium.\n\nAnother term that has been used to describe negative luminescent devices is \"Emissivity switch\", as an electric current changes the effective emissivity.\n\nThis effect was first seen by Russian physicists in the 1960s in A.F.Ioffe Physicotechnical Institute, Leningrad, Russia. Subsequently, it was studied in semiconductors such as indium antimonide (InSb), germanium (Ge) and indium arsenide (InAs) by workers in West Germany, Ukraine (Institute of Semiconductor Physics, Kiev), Japan (Chiba University) and the United States. It was first observed in the mid-infrared (3-5 µm wavelength) in the more convenient diode structures in InSb heterostructure diodes by workers at the Defence Research Agency, Great Malvern, UK (now QinetiQ). These British workers later demonstrated LWIR band (8-12 µm) negative luminescence using mercury cadmium telluride diodes.\n\nLater the Naval Research Laboratory, Washington DC, started work on negative luminescence in mercury cadmium telluride (HgCdTe). The phenomenon has since been observed by several university groups around the world.\n\n\n\n"}
{"id": "2219887", "url": "https://en.wikipedia.org/wiki?curid=2219887", "title": "Negative refraction", "text": "Negative refraction\n\nNegative refraction is the name for an electromagnetic phenomenon where light rays are refracted at an interface in the reverse sense to that normally expected. Such an effect can be obtained using a metamaterial which has been designed to achieve a negative value for both (electric) permittivity ε and (magnetic) permeability μ, as in such cases the material can be assigned a negative refractive index. Such materials are sometimes called \"double negative\" materials.\n\nNegative refraction occurs at interfaces between materials at which one has an ordinary positive phase velocity (i.e. a positive refractive index), and the other has the more exotic negative phase velocity (a negative refractive index).\n\nNegative phase velocity (NPV) is a property of light propagation in a medium. There are different definitions of NPV, the most common being Veselago's original proposal of opposition of wavevector and (Abraham) Poynting vector, i.e. E×H; other common choices are opposition of wavevector to group velocity, or to energy velocity. The use of \"phase velocity\" in the naming convention, as opposed to the perhaps more appropriate \"wave vector\", follows since phase velocity has the same sign as the wavevector.\n\nA typical criterion used to determine Veselago NPV is that the dot product of the Poynting vector and wavevector is negative (i.e. that formula_1); however this definition is not covariant. Whilst this restriction is rarely of practical significance, the criterion has nevertheless been generalized into a covariant form. For plane waves propagating in a Veselago NPV medium, the electric field, magnetic field and wave vector follow a left-hand rule, rather than the usual right-hand rule. This gives rise to the name \"left-handed (meta)materials\". However, the terms left-handed and right-handed can also arise in the study of chiral media, so this terminology is best avoided.\n\nWe can choose to avoid directly considering the Poynting vector and wavevector or a propagating light field, and consider instead the response of the materials directly: that is, we consider what values of permittivity ε and permeability µ result in negative phase velocity (NPV). Since both ε and µ are in general complex, their imaginary parts do not have to be negative for a passive (i.e. lossy) material to display negative refraction. The most general Veselago criterion applying to ε and µ is that of Depine and Lakhtakia, although other less general forms exist. The Depine-Lakhtakia criterion for negative phase velocity is\n\nwhere formula_3 are the real valued parts of ε and µ, respectively. However, negative refraction (negative refractive index) and negative phase velocity can be distinct from each other, even in passive materials, but also in active materials.\n\nTypically, the refractive index \"n\" is determined using formula_4, where by convention the positive square root is chosen for \"n\". However, in NPV materials, we reverse that convention and pick the negative sign to mimic the fact that the wavevector (and hence phase velocity) are likewise reversed. Strictly speaking, the refractive index is a derived quantity telling us how the wavevector is related to the optical frequency and propagation direction of the light, thus the sign of \"n\" must be chosen to match the physical situation.\n\nThe principal symptom of negative refraction is just that – light rays are \nrefracted on the \"same\" side of the normal on entering the material, as indicated in the diagram, and by a suitably general form of Snell's law.\n\n\n"}
{"id": "26502557", "url": "https://en.wikipedia.org/wiki?curid=26502557", "title": "Negative room pressure", "text": "Negative room pressure\n\nNegative room pressure is an isolation technique used in hospitals and medical centers to prevent cross-contaminations from room to room. It includes a ventilation system that generates negative pressure to allow air to flow into the isolation room but not escape from the room, as air will naturally flow from areas with higher pressure to areas with lower pressure, thereby preventing contaminated air from escaping the room. This technique is used to isolate patients with airborne contagious diseases such as tuberculosis, measles, or chickenpox.\n\nNegative pressure is generated and maintained by a ventilation system that removes more exhaust air from the room than air is allowed into the room. Air is allowed into the room through a gap under the door (typically about one half-inch high). Except for this gap, the room should be as airtight as possible, allowing no air in through cracks and gaps, such as those around windows, light fixtures and electrical outlets. Leakage from these sources can compromise or eliminate room negative pressure.\n\nA smoke test can help determine whether a room is under negative pressure. A tube containing smoke is held near the bottom of the negative pressure room door, about 2 inches in front of the door. The smoke tube is held parallel to the door, and a small amount of smoke is then generated by gently squeezing the bulb. Care is taken to release the smoke from the tube slowly to ensure the velocity of the smoke from the tube does not overpower the air velocity. If the room is at negative pressure, the smoke will travel under the door and into the room. If the room is not a negative pressure, the smoke will be blown outward or will stay stationary.\n\n"}
{"id": "244067", "url": "https://en.wikipedia.org/wiki?curid=244067", "title": "Negative sign (astrology)", "text": "Negative sign (astrology)\n\nIn astrology, a negative, ceptive, dispassive, yin, nocturnal or feminine sign refers to any of the six even-numbered signs of the zodiac: Taurus, Cancer, Virgo, Scorpio, Capricorn or Pisces.\n\nThese signs constitute the earth and water triplicities.\n\nIn astrology there are two groups: positive and negative. These two groups also\ninclude six individual signs that are called zodiac signs. The negative signs associated\nwith the zodiac are Taurus, Cancer, Virgo, Scorpio, Capricorn, and Pisces. The positive\nsigns associated with the zodiac are Aries, Gemini, Leo, Libra, Sagittarius, and\nAquarius. The twelve signs are divided into two\ngroups based upon one's exact time and place of birth. The negative and positive signs\nalternate, starting with Aries as positive and Taurus as negative and continuing this pattern through the list of zodiac signs.\n\nThe signs negative and positive are referred to as a negative-sunsign or a\npositive-sunsign. There are many terms used in astrology to differentiate the\ntwo groups. In Chinese astrology, the two groups are categorized as yin and yang, corresponding respectively to negative and positive. Standen explains that different astrologers may refer to signs by different names. For example, an astrologer may refer to positive signs as masculine and negative signs as feminine. Each sign is divided into two main\ntypes: active and passive. As a general rule, the active type will be called masculine and the passive type will be called feminine.\n\nZodiac signs associated with the negative are Taurus, Cancer, Virgo, Scorpio, Capricorn, and Pisces. Note that there is no value judgment attached to the terms negative or positive. They may be likened to polarities in a magnet: one side is positive and one side is negative. Neither side is \"good\" nor \"bad\"—they are merely different.\n\nThe sunsign effect is a pattern of alternating high and low extraversion-scores for\nthe 12 signs. Introvert is a person who gains energy when alone and spends it when with other people. Extravert is a person who gains energy from socialization, expending it when alone. Jan J.F. van Rooij did an experiment on Introversion-Extraversion: astrology\nversus psychology, to see if those in the negative sunsign were introverted and those in\nthe positive sunsign were negative. Van Rooijs did this experiment on those with\nastrological knowledge and on those with no astrological knowledge. His results showed\nthat negative sunsign people are not that responsive to the outer world and are\naccordingly not influenced that easily by astrological information. They rely more on\ntheir own ideas and feelings, thus proving his point that people who are born with the\nnegative sunsign are introverted, and those born with the positive sunsign are\nextroverted.\n\nEarth and Water are the elements attached to those who are in the negative sign.\nEarth is the element of Taurus, Virgo, and Capricorn. Water is the element of Cancer,\nScorpio, and Pisces. Elements are the basic traits of the signs. They reveal the\nfundamental aspects of the personality.\n“Water signs are attuned to waves of emotion, and often seem to have a built-in\nsonar for reading a mood. This gives them a special sensitivity in relationships, knowing\nwhen to show warmth and when to hold back. At their best, they are a healing force that\nbring people together -- at their worst, they are psychic vampires, able to manipulate\nand drain the life force of the closest to them”. Hall explains that water signs are\nmore in tune with their emotions and are comfortable showing them. Water signs bring a\ncertain presence to a situation; they seek out the problem and fix it.\n“Earth signs are sensual, meaning they engage with life through the five senses.\nIt takes time to sense the dense physical world, and earth signs can operate at a slower,\nmore thorough pace than the other elements. Theyʼre oriented toward whatʼs real, and\noften this makes them very productive, able to create tangible results.”\nEarth signs are described by Hall as earthy people. These signs focus on the things\nthat connect us to the earth: things which bring peace, as opposed to focusing on the material world.\n\n"}
{"id": "362728", "url": "https://en.wikipedia.org/wiki?curid=362728", "title": "Negative temperature", "text": "Negative temperature\n\nIn quantum thermodynamics, certain systems can achieve negative temperature; that is, their temperature can be expressed as a negative quantity on the Kelvin or Rankine scales.\n\nA system with a truly negative temperature on the Kelvin scale is \"hotter\" than any system with a positive temperature. If a negative-temperature system and a positive-temperature system come in contact, heat will flow from the negative- to the positive-temperature system. A standard example of such a system is population inversion in laser physics.\n\nTemperature is loosely interpreted as the average kinetic energy of the system's particles. The existence of negative temperature, let alone negative temperature representing \"hotter\" systems than positive temperature, would seem paradoxical in this interpretation. \nThe paradox is resolved by considering the more rigorous definition of thermodynamic temperature as the tradeoff between internal energy and entropy\ncontained in the system, with \"coldness\", the \"reciprocal\" of temperature, being the more fundamental quantity. \nSystems with a positive temperature will increase in entropy as one adds energy to the system, while \nsystems with a negative temperature will decrease in entropy as one adds energy to the system.\n\nClassical thermodynamic systems cannot achieve negative temperatures: adding heat always increases their entropy. \nThe possibility of a decrease in entropy as energy increases requires the system to \"saturate\" in entropy. This is only possible if the number of high energy state is limited. In classical Boltzmann statistics, the number of high energy states is unlimited (particle speeds can in principle be increased indefinitely).\nSystems bounded by a maximum amount of energy are generally forbidden in classical mechanics, and the phenomenon of negative temperature is strictly a \nquantum mechanical phenomenon. Some systems, however (see the examples below), have a maximum amount of energy that they can hold, and as they approach that maximum energy their entropy actually begins to decrease.\n\nThe definition of thermodynamic temperature as a function of the change in the system's entropy under reversible heat transfer \nformula_1:\nEntropy being a state function, the integral of over any cyclical process is zero.\nFor a system in which the entropy is purely a function of the system's energy , the temperature can be defined as:\n\nEquivalently, thermodynamic beta, or \"coldness\", is defined as \nwhere is the Boltzmann constant.\n\nNote that in classical thermodynamics, is defined in terms of temperature. This is reversed here, is the statistical entropy, a function of the possibly microstates of the system, and temperature conveys information on the distribution of energy levels among the possible microstates.\nFor systems with many degrees of freedom, the statistical and thermodynamic definitions of entropy are generally consistent with each other. \nHowever, for small systems and systems where the number of states decreases with energy, the definitions of statistical entropy and thermodynamic entropy are not necessarily consistent, and the temperatures derived from these entropies are different. \n\nSome theorists have proposed using an alternate definition of entropy originally proposed by Gibbs as a way to resolve these inconsistencies, although this new definition would create other inconsistencies.\n\nNegative temperatures can only exist in a system where there are a limited number of energy states (see below). As the temperature is increased on such a system, particles move into higher and higher energy states, and as the temperature increases, the number of particles in the lower energy states and in the higher energy states approaches equality. (This is a consequence of the definition of temperature in statistical mechanics for systems with limited states.) By injecting energy into these systems in the right fashion, it is possible to create a system in which there are more particles in the higher energy states than in the lower ones. The system can then be characterised as having a negative temperature.\n\nA substance with a negative temperature is not colder than absolute zero, but rather it is hotter than infinite temperature. As Kittel and Kroemer (p. 462) put it, \"The temperature scale from cold to hot runs:\n\nThe corresponding inverse temperature scale, for the quantity \"β\" = 1/\"kT\" (where \"k\" is Boltzmann's constant), runs continuously from low energy to high as +∞, … , 0, …, −∞. Because it avoids the abrupt jump from +∞ to −∞, β is considered more natural than \"T\". Although a system can have multiple negative temperature regions and thus have -∞ to +∞ discontinuities.\n\nIn many familiar physical systems, temperature is associated to the kinetic energy of atoms. Since there is no upper bound on the momentum of an atom, there is no upper bound to the number of energy states available when more energy is added, and therefore no way to get to a negative temperature. However, in statistical mechanics, temperature can correspond to other degrees of freedom than just kinetic energy (see below).\n\nThe distribution of energy among the various translational, vibrational, rotational, electronic, and nuclear modes of a system determines the macroscopic temperature. In a \"normal\" system, thermal energy is constantly being exchanged between the various modes.\n\nHowever, in some situations, it is possible to isolate one or more of the modes. In practice, the isolated modes still exchange energy with the other modes, but the time scale of this exchange is much slower than for the exchanges within the isolated mode. One example is the case of nuclear spins in a strong external magnetic field. In this case, energy flows fairly rapidly among the spin states of interacting atoms, but energy transfer between the nuclear spins and other modes is relatively slow. Since the energy flow is predominantly within the spin system, it makes sense to think of a spin temperature that is distinct from the temperature associated to other modes.\n\nA definition of temperature can be based on the relationship:\n\nThe relationship suggests that a \"positive temperature\" corresponds to the condition where entropy, \"S\", increases as thermal energy, \"q\", is added to the system. This is the \"normal\" condition in the macroscopic world, and is always the case for the translational, vibrational, rotational, and non-spin related electronic and nuclear modes. The reason for this is that there are an infinite number of these types of modes, and adding more heat to the system increases the number of modes that are energetically accessible, and thus increases the entropy.\n\nThe simplest example, albeit a rather nonphysical one, is to consider a system of \"N\" particles, each of which can take an energy of either \"+ε\" or \"-ε\" but are otherwise noninteracting. This can be understood as a limit of the Ising model in which the interaction term becomes negligible. The total energy of the system is\n\nwhere \"σ\" is the sign of the \"i\"th particle and \"j\" is the number of particles with positive energy minus the number of particles with negative energy. From elementary combinatorics, the total number of microstates with this amount of energy is a binomial coefficient:\n\nBy the fundamental assumption of statistical mechanics, the entropy of this microcanonical ensemble is\n\nWe can solve for thermodynamic beta (β = 1/kT) by considering it as a central difference without taking the continuum limit:\n\nhence the temperature\n\nThis entire proof assumes the microcanonical ensemble with energy fixed and temperature being the emergent property. In the canonical ensemble, the temperature is fixed and energy is the emergent property. This leads to (formula_14 refers to microstates):\nFollowing the previous example, we choose a state with two levels and two particles. This leads to microstates formula_18, formula_19, formula_20, and formula_21.\nThe resulting values for formula_28, formula_29, and formula_30 are all increasing with formula_31 and never need to enter a negative temperature regime.\n\nThe previous example is approximately realized by a system of nuclear spins in an external magnetic field. This allows the experiment to be run as a variation of nuclear magnetic resonance spectroscopy. In the case of electronic and nuclear spin systems, there are only a finite number of modes available, often just two, corresponding to spin up and spin down. In the absence of a magnetic field, these spin states are \"degenerate\", meaning that they correspond to the same energy. When an external magnetic field is applied, the energy levels are split, since those spin states that are aligned with the magnetic field will have a different energy from those that are anti-parallel to it.\n\nIn the absence of a magnetic field, such a two-spin system would have maximum entropy when half the atoms are in the spin-up state and half are in the spin-down state, and so one would expect to find the system with close to an equal distribution of spins. Upon application of a magnetic field, some of the atoms will tend to align so as to minimize the energy of the system, thus slightly more atoms should be in the lower-energy state (for the purposes of this example we will assume the spin-down state is the lower-energy state). It is possible to add energy to the spin system using radio frequency (RF) techniques. This causes atoms to \"flip\" from spin-down to spin-up.\n\nSince we started with over half the atoms in the spin-down state, this initially drives the system towards a 50/50 mixture, so the entropy is increasing, corresponding to a positive temperature. However, at some point, more than half of the spins are in the spin-up position. In this case, adding additional energy reduces the entropy, since it moves the system further from a 50/50 mixture. This reduction in entropy with the addition of energy corresponds to a negative temperature. In NMR spectroscopy, this corresponds to pulses with a pulse width of over 180° (for a given spin). While relaxation is fast in solids, it can take several seconds in solutions and even longer in gases and in ultracold systems; several hours were reported for silver and rhodium at picokelvin temperatures. It is still important to understand that the temperature is negative only with respect to nuclear spins. Other degrees of freedom, such as molecular vibrational, electronic and electron spin levels are at a positive temperature, so the object still has positive sensible heat. Relaxation actually happens by exchange of energy between the nuclear spin states and other states (e.g. through the nuclear Overhauser effect with other spins).\n\nThis phenomenon can also be observed in many lasing systems, wherein a large fraction of the system's atoms (for chemical and gas lasers) or electrons (in semiconductor lasers) are in excited states. This is referred to as a population inversion.\n\nThe Hamiltonian for a single mode of a luminescent radiation field at frequency \"ν\" is\nThe density operator in the grand canonical ensemble is\nFor the system to have a ground state, the trace to converge, and the density operator to be generally meaningful, \"βH\" must be positive semidefinite. So if \"hν\" < \"μ\", and \"H\" is negative semidefinite, then \"β\" must itself be negative, implying a negative temperature.\n\nThe two-dimensional systems can exist in negative temperature states.\n\nNegative temperatures have also been achieved in motional degrees of freedom. Using an optical lattice, upper bounds were placed on the kinetic energy, interaction energy and potential energy of cold atoms. This was done by tuning the interactions of the atoms from repulsive to attractive using a Feshbach resonance and changing the overall harmonic potential from trapping to anti-trapping, thus transforming the Bose-Hubbard Hamiltonian from formula_34. Performing this transformation adiabatically while keeping the atoms in the Mott insulator regime, it is possible to go from a low entropy positive temperature state to a low entropy negative temperature state. In the negative temperature state, the atoms macroscopically occupy the maximum momentum state of the lattice. The negative temperature ensembles equilibrated and showed long lifetimes in an anti-trapping harmonic potential.\n\n\n"}
{"id": "13065509", "url": "https://en.wikipedia.org/wiki?curid=13065509", "title": "Paired opposites", "text": "Paired opposites\n\nPaired opposites are an ancient, pre-Socratic method of establishing thesis, antithesis and synthesis in terms of a standard for what is right and proper in natural philosophy.\n\nScalar ranges and coordinate systems are paired opposites within sets. Incorporating dimensions of positive and negative numbers and exponents, or expanding x, y and z coordinates, by adding a fourth dimension of time allows a resolution of position relative to the standard of the scale which is often taken as 0,0,0,0 with additional dimensions added as referential scales are expanded from space and time to mass and energy.\n\nAncient systems frequently scaled their degree of opposition by rate of increase or rate of decrease. Linear increase was enhanced by doubling systems. An acceleration in the rate of increase or decrease could be analyzed arithmetrically, geometrically, or through a wide range of other numerical and physical analysis. Arithmetic and geometric series, and other methods of rating proportionate expansion or contraction could be thought of as convergent or divergent toward a position.\n\nThough unit quantities were first defined by spatial dimensions, and then expanded by adding coordinates of time, the weight or mass a given spatial dimension could contain was also considered and even in antiquity, conditions under which the standard would be established such as at a given temperature, distance from sea level, or density were added.\n\nRates of change over time were then considered as either indexes of production or depletion\n\nPaired opposites are used as poetic diction meaning \"everything\". Common phrases incorporated paired opposites in English include \"all creatures great and small,\" \"working for the man every night and day,\" \"more things in heaven and Earth\" \"searching high and low\" \"in sickness and in health\". In Greek literature, Homer uses the device when he lets Telemachus say, \"I know all things, the good and the evil\" (Od.20:309-10).\nThe same phrase is used in Hebrew in text of Genesis, referring to the Tree of the knowledge of good and evil.\n\nIn quantum mechanics, as well as some fields of mathematics, conjugate variables are a form of paired opposites, in which knowledge of one precludes knowledge of the other. A standard example is the relation between position (x) and momentum (p), which can be expressed in terms of the uncertainty principle as formula_1.\n"}
{"id": "3792982", "url": "https://en.wikipedia.org/wiki?curid=3792982", "title": "Porphyrian tree", "text": "Porphyrian tree\n\nThe Tree of Porphyry is a classic device for illustrating what is also called a \"scale of being\". It was suggested—if not first, then most famously in the European philosophical tradition—by the 3rd century CE Greek neoplatonist philosopher and logician Porphyry. It is also known as \"scala praedicamentalis\".\n\nPorphyry suggests the Porphyrian tree in his introduction (in Greek, \"Isagoge\") to Aristotle's Categories. Porphyry presented Aristotle's classification of categories in a way that was later adopted into tree-like diagrams of dichotomous divisions, which indicate that a species is defined by a genus and a differentia and that this logical process continues until the lowest species is reached, which can no longer be so defined. No illustrations or diagrams occur in editions of Porphyry's original work. But, diagrams were eventually made, and became associated with the scheme that Porphyry describes, following Aristotle.\n\nPorphyry's \"Isagoge\" was originally written in Greek, but was translated into Latin in the early 6th century CE by Boethius. Boethius's translation became the standard philosophical logic textbook in the Middle Ages. Until the late 19th century, theories of categories based on Porphyry's work were still being taught to students of logic.\n\nThe following very helpful passage by philosopher James Franklin gives some hint as to the history of the Porphyrian tree:\n\nThus, the notion of the Porphyrian tree as an actual diagram comes later than Porphyry himself. Still, scholars do speak of Porphyry's tree as in the \"Isagoge\" and they mean by this only that the idea of dividing genera into species via differentiae is found in the \"Isagoge\". But, of course, Porphyry was only following what was already in Aristotle, and Aristotle was following what was already in his teacher, Plato.\n\nThe following Porphyrian tree consists of three columns of words; the middlemost (in boldface) contains the series of genera and species, and we can take as analogous to the trunk of a tree. The extremes (the terms that jut out to the left and right), containing the differentiae, we can take as analogous to the branches of a tree:\n\nThe diagram shows the highest genus to be substance. (Whether substance is a highest genus, really, is not in question here: right now we are only going to discuss what the diagram shows, not whether what it shows is true or false.) The technical term for a highest substance is \"\"summum genus\"\". So, substance is the \"summum genus\" as far as this diagram goes. The diagram shows that the genus substance to have two differentia, namely, \"thinking\" and \"extended\". This indicates that there are two species of the genus substance, thinking substance and extended substance. The diagram does not give a term for the species of thinking substance (this would be \"mind\"), but it does give the term for the species of extended substance, namely, body. That is, body is a species of the genus substance; body is that species of the genus substance that is extended.\n\nNow that we have seen body as a species of substance, we treat body as a genus itself. As a genus, it has two differentia of its own, inanimate and animate. So, there are two species of body, inanimate body and animate body. The diagram does not tell us what the term for inanimate body is, but it indicates a term for animate body, namely, animal. Animal is an animate species of the genus body.\n\nAnd, again, now that we have looked at animal as a species of the genus body, we look at animal now as a genus and consider its differentia, which are shown on the diagram to be irrational and rational. Thus, according to the diagram there are two species of the genus animal, irrational animal and rational animal. We are not told by the diagram what a term for irrational animal is, but the diagram indicates that a rational animal is a human. Thus, human is a rational species of the genus animal.\n\nBeneath human, however, there are no further species. \"This\" and \"that\" if they are considered differentiae, are of a special kind that map the species human not onto a new species but onto particular humans., The particular human Plato is named in the diagram. Plato is not a species (that is why his name is not in bold, unlike the species above). So, human is the lowest species in this diagram. The technical name for the lowest species in such a scheme is the \"\"infima species\"\". So, for this diagram, human is the \"infima species\".\n\n\n\n\n"}
{"id": "780566", "url": "https://en.wikipedia.org/wiki?curid=780566", "title": "Possible world", "text": "Possible world\n\nIn philosophy and logic, the concept of a possible world is used to express modal claims. The concept of possible worlds is common in contemporary philosophical discourse but has been disputed.\n\nThose theorists who use the concept of possible worlds consider the \"actual\" world to be one of the many possible worlds. For each distinct way the world could have been, there is said to be a distinct possible world; the actual world is the one we in fact live in. Among such theorists there is disagreement about the nature of possible worlds; their precise ontological status is disputed, and especially the difference, if any, in ontological status between the actual world and all the other possible worlds. One position on these matters is set forth in David Lewis's modal realism (see below). There is a close relation between propositions and possible worlds. We note that every proposition is either true or false at any given possible world; then the \"modal status\" of a proposition is understood in terms of the \"worlds in which it is true\" and \"worlds in which it is false\". The following are among the assertions we may now usefully make:\n\n\nThe idea of possible worlds is most commonly attributed to Gottfried Leibniz, who spoke of possible worlds as ideas in the mind of God and used the notion to argue that our actually created world must be \"the best of all possible worlds\". Arthur Schopenhauer argued that on the contrary our world must be the worst of all possible worlds, because if it were only a little worse it could not continue to exist.\n\nScholars have found implicit earlier traces of the idea of possible worlds in the works of René Descartes, a major influence on Leibniz, Al-Ghazali (\"The Incoherence of the Philosophers\"), Averroes (\"The Incoherence of the Incoherence\"), Fakhr al-Din al-Razi (\"Matalib al-'Aliya\") and John Duns Scotus. The modern philosophical use of the notion was pioneered by David Lewis and Saul Kripke.\n\nA semantics for modal logic was first introduced in the late-1950s work of Saul Kripke and his colleagues. A statement in modal logic that is \"possible\" is said to be true in at least one possible world; a statement that is \"necessary\" is said to be true in all possible worlds.\n\nFrom this groundwork, the theory of possible worlds became a central part of many philosophical developments, from the 1960s onwards – including, most famously, the analysis of counterfactual conditionals in terms of \"nearby possible worlds\" developed by David Lewis and Robert Stalnaker. On this analysis, when we discuss what \"would\" happen \"if\" some set of conditions \"were\" the case, the truth of our claims is determined by what is true at the nearest possible world (or the \"set\" of nearest possible worlds) where the conditions obtain. (A possible world W is said to be near to another possible world W in respect of R to the degree that the same things happen in W and W in respect of R; the more different something happens in two possible worlds in a certain respect, the \"further\" they are from one another in that respect.) Consider this conditional sentence: \"If George W. Bush hadn't become president of the U.S. in 2001, Al Gore would have.\" The sentence would be taken to express a claim that could be reformulated as follows: \"In all nearest worlds to our actual world (nearest in relevant respects) where George W. Bush didn't become president of the U.S. in 2001, Al Gore became president of the U.S. then instead.\" And on this interpretation of the sentence, if there is or are some nearest worlds to the actual world (nearest in relevant respects) where George W. Bush didn't become president but Al Gore didn't either, then the claim expressed by this counterfactual would be false.\n\nToday, possible worlds play a central role in many debates in philosophy, including especially debates over the Zombie Argument, and physicalism and supervenience in the philosophy of mind. Many debates in the philosophy of religion have been reawakened by the use of possible worlds. Intense debate has also emerged over the ontological status of possible worlds, provoked especially by David Lewis's defense of modal realism, the doctrine that talk about \"possible worlds\" is best explained in terms of innumerable, \"really existing\" worlds beyond the one we live in. The fundamental question here is: \"given\" that modal logic works, and that some possible-worlds semantics for modal logic is correct, \"what has to be true\" of the world, and just what \"are\" these possible worlds that we range over in our interpretation of modal statements? Lewis argued that what we range over are real, concrete \"worlds\" that exist just as unequivocally as our actual world exists, but that are distinguished from the actual world simply by standing in no spatial, temporal, or causal relations with the actual world. (On Lewis's account, the only \"special\" property that the \"actual\" world has is a relational one: that \"we\" are in it. This doctrine is called \"the indexicality of actuality\": \"actual\" is a merely indexical term, like \"now\" and \"here\".) Others, such as Robert Adams and William Lycan, reject Lewis's picture as metaphysically extravagant, and suggest in its place an interpretation of possible worlds as consistent, maximally complete sets of descriptions of or propositions about the world, so that a \"possible world\" is conceived of as a complete \"description\" of \"a way the world could be\" – rather than a \"world that is that way\". (Lewis describes their position, and similar positions such as those advocated by Alvin Plantinga and Peter Forrest, as \"\"ersatz\" modal realism\", arguing that such theories try to get the benefits of possible worlds semantics for modal logic \"on the cheap\", but that they ultimately fail to provide an adequate explanation.) Saul Kripke, in \"Naming and Necessity\", took explicit issue with Lewis's use of possible worlds semantics, and defended a \"stipulative\" account of possible worlds as purely \"formal\" (logical) entities rather than either really existent worlds or as some set of propositions or descriptions.\n\nPossible worlds theory in literary studies uses concepts from possible-world logic and applies them to worlds that are created by fictional texts, fictional universe. In particular, possible-world theory provides a useful vocabulary and conceptual framework with which to describe such worlds. However, a literary world is a specific type of possible world, quite distinct from the possible worlds in logic. This is because a literary text houses its own system of modality, consisting of actual worlds (actual events) and possible worlds (possible events). In fiction, the principle of simultaneity, it extends to cover the dimensional aspect, when it is contemplated that two or more physical objects, realities, perceptions and objects non-physical, can coexist in the same space-time. Thus, a literary universe is granted autonomy in much the same way as the actual universe.\n\nLiterary critics, such as Marie-Laure Ryan, Lubomír Doležel, and Thomas Pavel, have used possible-worlds theory to address notions of literary truth, the nature of fictionality, and the relationship between fictional worlds and reality. Taxonomies of fictional possibilities have also been proposed where the likelihood of a fictional world is assessed. Possible-world theory is also used within narratology to divide a specific text into its constituent worlds, possible and actual. In this approach, the modal structure of the fictional text is analysed in relation to its narrative and thematic concerns. Rein Raud has extended this approach onto \"cultural\" worlds, comparing possible worlds to the particular constructions of reality of different cultures. However, the metaphor of the \"cultural possible worlds\" relates to the framework of cultural relativism and, depending on the ontological status ascribed to possible worlds, warrants different, often controversial claims ranging from ethnocentrism to cultural imperialism.\n\n\n\n"}
{"id": "24097", "url": "https://en.wikipedia.org/wiki?curid=24097", "title": "Principle of bivalence", "text": "Principle of bivalence\n\nIn logic, the semantic principle (or law) of bivalence states that every declarative sentence expressing a proposition (of a theory under inspection) has exactly one truth value, either true or false. A logic satisfying this principle is called a two-valued logic or bivalent logic.\n\nIn formal logic, the principle of bivalence becomes a property that a semantics may or may not possess. It is not the same as the law of excluded middle, however, and a semantics may satisfy that law without being bivalent.\n\nThe principle of bivalence is studied in philosophical logic to address the question of which natural-language statements have a well-defined truth value. Sentences which predict events in the future, and sentences which seem open to interpretation, are particularly difficult for philosophers who hold that the principle of bivalence applies to all declarative natural-language statements. Many-valued logics formalize ideas that a realistic characterization of the notion of consequence requires the admissibility of premises which, owing to vagueness, temporal or quantum indeterminacy, or reference-failure, cannot be considered classically bivalent. Reference failures can also be addressed by free logics.\n\nThe principle of bivalence is related to the law of excluded middle though the latter is a syntactic expression of the language of a logic of the form \"P ∨ ¬P\". The difference between the principle and the law is important because there are logics which validate the law but which do not validate the principle. For example, the three-valued Logic of Paradox (LP) validates the law of excluded middle, but not the law of non-contradiction, ¬(P ∧ ¬P), and its intended semantics is not bivalent. In classical two-valued logic both the law of excluded middle and the law of non-contradiction hold.\n\nMany modern logic programming systems replace the law of the excluded middle with the concept of negation as failure. The programmer may wish to add the law of the excluded middle by explicitly asserting it as true; however, it is not assumed \"a priori\".\n\nThe intended semantics of classical logic is bivalent, but this is not true of every semantics for classical logic. In Boolean-valued semantics (for classical propositional logic), the truth values are the elements of an arbitrary Boolean algebra, \"true\" corresponds to the maximal element of the algebra, and \"false\" corresponds to the minimal element. Intermediate elements of the algebra correspond to truth values other than \"true\" and \"false\". The principle of bivalence holds only when the Boolean algebra is taken to be the two-element algebra, which has no intermediate elements.\n\nAssigning Boolean semantics to classical predicate calculus requires that the model be a complete Boolean algebra because the universal quantifier maps to the infimum operation, and the existential quantifier maps to the supremum; this is called a Boolean-valued model. All finite Boolean algebras are complete.\n\nIn order to justify his claim that true and false are the only logical values, Suszko (1977) observes that every structural Tarskian many-valued propositional logic can be provided with a bivalent semantics.\n\nA famous example is the \"contingent sea battle\" case found in Aristotle's work, \"De Interpretatione\", chapter 9:\n\nThe principle of bivalence here asserts:\n\nAristotle to embrace bivalence for such future contingents; Chrysippus, the Stoic logician, did embrace bivalence for this and all other propositions. The controversy continues to be of central importance in both the philosophy of time and the philosophy of logic.\n\nOne of the early motivations for the study of many-valued logics has been precisely this issue. In the early 20th century, the Polish formal logician Jan Łukasiewicz proposed three truth-values: the true, the false and the \"as-yet-undetermined\". This approach was later developed by Arend Heyting and L. E. J. Brouwer; see Łukasiewicz logic.\n\nIssues such as this have also been addressed in various temporal logics, where one can assert that \"\"Eventually\", either there will be a sea battle tomorrow, or there won't be.\" (Which is true if \"tomorrow\" eventually occurs.)\n\nSuch puzzles as the Sorites paradox and the related continuum fallacy have raised doubt as to the applicability of classical logic and the principle of bivalence to concepts that may be vague in their application. Fuzzy logic and some other multi-valued logics have been proposed as alternatives that handle vague concepts better. Truth (and falsity) in fuzzy logic, for example, comes in varying degrees. Consider the following statement in the circumstance of sorting apples on a moving belt:\n\nUpon observation, the apple is an undetermined color between yellow and red, or it is motled both colors. Thus the color falls into neither category \" red \" nor \" yellow \", but these are the only categories available to us as we sort the apples. We might say it is \"50% red\". This could be rephrased: it is 50% true that the apple is red. Therefore, P is 50% true, and 50% false. Now consider:\n\nIn other words, P and not-P. This violates the law of noncontradiction and, by extension, bivalence. However, this is only a partial rejection of these laws because P is only partially true. If P were 100% true, not-P would be 100% false, and there is no contradiction because P and not-P no longer holds.\n\nHowever, the law of the excluded middle is retained, because P and not-P implies P or not-P, since \"or\" is inclusive. The only two cases where P and not-P is false (when P is 100% true or false) are the same cases considered by two-valued logic, and the same rules apply.\n\nExample of a 3-valued logic applied to vague (undetermined) cases: Kleene 1952 (§64, pp. 332–340) offers a 3-valued logic for the cases when algorithms involving partial recursive functions may not return values, but rather end up with circumstances \"u\" = undecided. He lets \"t\" = \"true\", \"f\" = \"false\", \"u\" = \"undecided\" and redesigns all the propositional connectives. He observes that:\n\nThe following are his \"strong tables\":\nFor example, if a determination cannot be made as to whether an apple is red or not-red, then the truth value of the assertion Q: \" This apple is red \" is \" u \". Likewise, the truth value of the assertion R \" This apple is not-red \" is \" u \". Thus the AND of these into the assertion Q AND R, i.e. \" This apple is red AND this apple is not-red \" will, per the tables, yield \" u \". And, the assertion Q OR R, i.e. \" This apple is red OR this apple is not-red \" will likewise yield \" u \".\n\n"}
{"id": "1137736", "url": "https://en.wikipedia.org/wiki?curid=1137736", "title": "Principle of sufficient reason", "text": "Principle of sufficient reason\n\nThe principle of sufficient reason states that everything must have a reason or a cause. The modern formulation of the principle is usually attributed to Gottfried Leibniz, although the idea was conceived of and utilized by various philosophers who preceded him, including Anaximander, Parmenides, Archimedes, Plato and Aristotle, Cicero, Avicenna, Thomas Aquinas, and Spinoza. Some philosophers have associated the principle of sufficient reason with \"ex nihilo nihil fit\". Hamilton identified the laws of inference modus ponens with the \"law of Sufficient Reason, or of Reason and Consequent\" and modus tollens with its contrapositive expression.\n\nThe principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\n\nA sufficient explanation may be understood either in terms of \"reasons\" or \"causes,\" for like many philosophers of the period, Leibniz did not carefully distinguish between the two. The resulting principle is very different, however, depending on which interpretation is given.\n\nIt is an open question whether the principle of sufficient reason can be applied to axioms within a logic construction like a mathematical or a physical theory, because axioms are propositions accepted as having no justification possible within the system\nThe principle declares that all propositions considered to be true within a system should be deducible from the set axioms at the base of the construction (with some theoretical exceptions: see Gödel's theorem).\n\nLeibniz identified two kinds of truth, necessary and contingent truths. He believed necessary mathematical truths to be derived from the law of identity (and the principle of contradiction): \"Necessary truths are those that can be demonstrated through an analysis of terms, so that in the end they become identities, just as in Algebra an equation expressing an identity ultimately results from the substitution of values [for variables]. That is, necessary truths depend upon the principle of contradiction.\" Leibniz states that the sufficient reason for necessary truths is that their negation is a contradiction.\n\nLeibniz admitted contingent truths on the basis of infinitary reasons, to which God had access but humans did not:\nIn contingent truths, even though the predicate is in the subject, this can never be demonstrated, nor can a proposition ever be reduced to an equality or to an identity, but the resolution proceeds to infinity, God alone seeing, not the end of the resolution, of course, which does not exist, but the connection of the terms or the containment of the predicate in the subject, since he sees whatever is in the series.Without this qualification, the principle can be seen as a description of a certain notion of closed system, in which there is no 'outside' to provide unexplained events with causes. It is also in tension with the paradox of Buridan's ass. Leibniz denied that the paradox of Buridan's ass could ever occur, saying:\n\nLeibniz also used the principle of sufficient reason to refute the idea of absolute space:\n\nI say then, that if space is an absolute being, there would be something for which it would be impossible there should be a sufficient reason. Which is against my axiom. And I prove it thus. Space is something absolutely uniform; and without the things placed in it, one point in space does not absolutely differ in any respect whatsoever from another point of space. Now from hence it follows, (supposing space to be something in itself, beside the order of bodies among themselves,) that 'tis impossible that there should be a reason why God, preserving the same situation of bodies among themselves, should have placed them in space after one particular manner, and not otherwise; why everything was not placed the quite contrary way, for instance, by changing East into West.\n\nThe principle was one of the four recognised laws of thought, that held a place in European pedagogy of logic and reasoning (and, to some extent, philosophy in general) in the 18th and 19th centuries. It was influential in the thinking of Leo Tolstoy, amongst others, in the elevated form that history could not be accepted as random.\n\nA sufficient reason is sometimes described as the coincidence of every single thing that is needed for the occurrence of an effect (i.e. of the so-called \"necessary conditions\"). Such view could perhaps be also applied to indeterministic systems, as long as randomness is in a way incorporated in the preconditions.\n\nHere is how Hamilton, circa 1837–1838, expressed his \"fourth law\" in his LECT. V. LOGIC. 60–61:\n\nAccording to Schopenhauer's \"On the Fourfold Root of the Principle of Sufficient Reason\", there are four distinct forms of the principle.\n\nFirst Form: The Principle of Sufficient Reason of Becoming (principium rationis sufficientis fiendi); appears as the law of causality in the understanding.\n\nSecond Form: The Principle of Sufficient Reason of Knowing (principium rationis sufficientis cognoscendi); asserts that if a judgment is to express a piece of knowledge, it must have a sufficient ground or reason, in which case it receives the predicate true.\n\nThird Form: The Principle of Sufficient Reason of Being (principium rationis sufficientis essendi); the law whereby the parts of space and time determine one another as regards those relations. Example in arithmetic: Each number presupposes the preceding numbers as grounds or reasons of its being; \"I can reach ten only by going through all the preceding numbers; and only by virtue of this insight into the ground of being, do I know that where there are ten, so are there eight, six, four.\"\n\n\"Now just as the subjective correlative to the first class of representations is the understanding, that to the second the faculty of reason, and that to the third pure sensibility, so is the subjective correlative to this fourth class found to be the inner sense, or generally self-consciousness.\" \n\nFourth Form: The Principle of Sufficient Reason of Acting (principium rationis sufficientis agendi); briefly known as the law of motivation. \"Any judgment that does not follow its previously existing ground or reason\" or any state that cannot be explained away as falling under the three previous headings \"must be produced by an act of will which has a motive.\" As his proposition in 43 states, \"Motivation is causality seen from within.\"\n\nSeveral proofs have been prepared in order to demonstrate that the universe is at bottom causal, i.e. works in accord with the principle in question; perhaps not in every single case (randomness might still play a part here and there), but that causality must be the way it works at least \"in general\", in most of what we see; and that our minds are aware of the principle even before any experience. The two famous arguments or proofs were proposed by Immanuel Kant (from the form of Time, temporal ordering of events and \"directionality\" of time) and by Arthur Schopenhauer (by demonstrating how all perception depends on causality and the intellect).\n\nOnce it is agreed (e.g. from a kind of an \"arrow of time\") that causal interconnections, as a form of principle of sufficient reason, indeed must in general exist everywhere in the universe (at least in the large scale), \"backwards\" causality in general might then be precluded using a form of the paradox of free will (i.e. an event that has a future source might cause us to remove that source quick enough and thus causality would not work).\n\n\n"}
{"id": "39812836", "url": "https://en.wikipedia.org/wiki?curid=39812836", "title": "Process reference models", "text": "Process reference models\n\nA process reference model is a model that has generic functionality and can be used more than once in different models. The creator of a process model benefits from existing process reference models by not needing to reinvent the process model but only reusing it as a starting point in creating a process model for a specific purpose.\n\nDuring the identification of processes ideal for reuse, the designer needs to (1) Get approval (2) Provide Organization Scope Context and (3) Identify Process Standardization Opportunities.\n"}
{"id": "1335297", "url": "https://en.wikipedia.org/wiki?curid=1335297", "title": "Spaceship Earth", "text": "Spaceship Earth\n\nSpaceship Earth or Spacecraft Earth is a world view encouraging everyone on Earth to act as a harmonious crew working toward the greater good.\n\nThe earliest known use is a passage in Henry George's best known work, \"Progress and Poverty\" (1879).\nFrom book IV, chapter 2: \nIt is a well-provisioned ship, this on which we sail through space. If the bread and beef above decks seem to grow scarce, we but open a hatch and there is a new supply, of which before we never dreamed. And very great command over the services of others comes to those who as the hatches are opened are permitted to say, \"This is mine!\"\n\nGeorge Orwell later paraphrases Henry George in \"The Road to Wigan Pier\":\n\nThe world is a raft sailing through space with, potentially, plenty of provisions for everybody; the idea that we must all cooperate and see to it that everyone does his fair share of the work and gets his fair share of the provisions seems so blatantly obvious that one would say that no one could possibly fail to accept it unless he had some corrupt motive for clinging to the present system.\n\nIn 1965 Adlai Stevenson made a famous speech to the UN in which he said:\n\nWe travel together, passengers on a little space ship, dependent on its vulnerable reserves of air and soil; all committed for our safety to its security and peace; preserved from annihilation only by the care, the work, and, I will say, the love we give our fragile craft. We cannot maintain it half fortunate, half miserable, half confident, half despairing, half slave—to the ancient enemies of man—half free in a liberation of resources undreamed of until this day. No craft, no crew can travel safely with such vast contradictions. On their resolution depends the survival of us all.\n\nThe following year, \"Spaceship Earth\" became the title of a book by a friend of Stevenson's, the internationally influential economist Barbara Ward.\n\nAlso in 1966, Kenneth E. Boulding, who was influenced by reading Henry George, used the phrase in the title of an essay, \"The Economics of the Coming Spaceship Earth\". Boulding described the past open economy of apparently illimitable resources, which he said he was tempted to call the \"cowboy economy\", and continued: \"The closed economy of the future might similarly be called the 'spaceman' economy, in which the earth has become a single spaceship, without unlimited reservoirs of anything, either for extraction or for pollution, and in which, therefore, man must find his place in a cyclical ecological system\".\n\nThe phrase was also popularized by Buckminster Fuller, who published a book in 1968 under the title of \"Operating Manual for Spaceship Earth\". This quotation, referring to fossil fuels, reflects his approach: \n…we can make all of humanity successful through science's world-engulfing industrial evolution provided that we are not so foolish as to continue to exhaust in a split second of astronomical history the orderly energy savings of billions of years' energy conservation aboard our Spaceship Earth. These energy savings have been put into our Spaceship's life-regeneration-guaranteeing bank account for use only in self-starter functions.\n\nUnited Nations Secretary-General U Thant spoke of Spaceship Earth on Earth Day March 21, 1971 at the ceremony of the ringing of the Japanese Peace Bell: \"May there only be peaceful and cheerful Earth Days to come for our beautiful Spaceship Earth as it continues to spin and circle in frigid space with its warm and fragile cargo of animate life.\"\n\nSpaceship Earth is the name given to the 50 m diameter geodesic sphere that greets visitors at the entrance of Walt Disney World's Epcot theme park. Housed within the sphere is a dark ride that serves to explore the history of communications and promote Epcot's founding principles, \"[a] belief and pride in man's ability to shape a world that offers hope to people everywhere.\" A previous incarnation of the ride, narrated by actor Jeremy Irons and revised in 2008, was explicit in its message:\n\nLike a grand and miraculous spaceship, our planet has sailed through the universe of time, and for a brief moment, we have been among its many passengers….We now have the ability and the responsibility to build new bridges of acceptance and co-operation between us, to create a better world for ourselves and our children as we continue our amazing journey aboard Spaceship Earth.\n\nDavid Deutsch has pointed out that the picture of Earth as a friendly \"spaceship\" habitat is difficult to defend even in metaphorical sense. The Earth environment is harsh and survival is constant struggle for life, including whole species extinction. Humans wouldn't be able to live in most of the areas where they are living now without knowledge necessary to build life-support systems such as houses, heating, water supply, etc.\n\nThe term \"Spaceship Earth\" is frequently used on the labels of Emanuel Bronner's products to refer to the Earth.\n\n"}
{"id": "246976", "url": "https://en.wikipedia.org/wiki?curid=246976", "title": "Square of opposition", "text": "Square of opposition\n\nThe square of opposition is a diagram representing the relations between the four basic categorical propositions.\nThe origin of the square can be traced back to Aristotle making the distinction between two oppositions: contradiction and contrariety.\nBut Aristotle did not draw any diagram. This was done several centuries later by Apuleius and Boethius.\n\nIn traditional logic, a proposition (Latin: \"propositio\") is a spoken assertion (\"oratio enunciativa\"), not the meaning of an assertion, as in modern philosophy of language and logic. A \"categorical proposition\" is a simple proposition containing two terms, subject and predicate, in which the predicate is either asserted or denied of the subject.\n\nEvery categorical proposition can be reduced to one of four logical forms. These are:\n\nIn tabular form:\n\n\nAristotle states (in chapters six and seven of the \"Peri hermaneias\" (Περὶ Ἑρμηνείας, Latin \"De Interpretatione\", English 'On Interpretation')), that there are certain logical relationships between these four kinds of proposition. He says that to every affirmation there corresponds exactly one negation, and that every affirmation and its negation are 'opposed' such that always one of them must be true, and the other false. A pair of affirmative and negative statements he calls a 'contradiction' (in medieval Latin, \"contradictio\"). Examples of contradictories are 'every man is white' and 'not every man is white' (also read as 'some men are not white'), 'no man is white' and 'some man is white'.\n\n'Contrary' (medieval: \"contrariae\") statements, are such that both cannot at the same time be true. Examples of these are the universal affirmative 'every man is white', and the universal negative 'no man is white'. These cannot be true at the same time. However, these are not contradictories because both of them may be false. For example, it is false that every man is white, since some men are not white. Yet it is also false that no man is white, since there are some white men.\n\nSince every statement has a contradictory opposite, and since a contradictory is true when its opposite is false, it follows that the opposites of contraries (which the medievals called subcontraries, \"subcontrariae\") can both be true, but they cannot both be false. Since subcontraries are negations of universal statements, they were called 'particular' statements by the medieval logicians.\n\nAnother logical opposition implied by this, though not mentioned explicitly by Aristotle, is 'alternation' (\"alternatio\"), consisting of 'subalternation' and 'superalternation'. Alternation is a relation between a particular statement and a universal statement of the same quality such that the particular is implied by the other. The particular is the subaltern of the universal, which is the particular's superaltern. For example, if 'every man is white' is true, its contrary 'no man is white' is false. Therefore, the contradictory 'some man is white' is true. Similarly the universal 'no man is white' implies the particular 'not every man is white'.\n\nIn summary:\n\nThese relationships became the basis of a diagram originating with Boethius and used by medieval logicians to classify the logical relationships. The propositions are placed in the four corners of a square, and the relations represented as lines drawn between them, whence the name 'The Square of Opposition'.\n\nSubcontraries, which medieval logicians represented in the form 'quoddam A est B' (some particular A is B) and 'quoddam A non est B' (some particular A is not B) cannot both be false, since their universal contradictory statements (every A is B / no A is B) cannot both be true. This leads to a difficulty that was first identified by Peter Abelard. 'Some A is B' seems to imply 'something is A'. For example, 'Some man is white' seems to imply that at least one thing is a man, namely the man who has to be white, if 'some man is white' is true. But, 'some man is not white' also implies that something is a man, namely the man who is not white, if the statement 'some man is not white' is true. But Aristotelian logic requires that necessarily one of these statements is true. Both cannot be false. Therefore, (since both imply that something is a man) it follows that necessarily something is a man, i.e. men exist. But (as Abelard points out, in the Dialectica) surely men might not exist?\n\nAbelard also points out that subcontraries containing subject terms denoting nothing, such as 'a man who is a stone', are both false.\n\nTerence Parsons argues that ancient philosophers did not experience the problem of existential import as only the A and I forms had existential import.\n\nHe goes on to cite medieval philosopher William of Moerbeke\n\nAnd points to Boethius' translation of Aristotle's work as giving rise to the mistaken notion that the O form has existential import.\n\nIn the 19th century, George Boole argued for requiring existential import on both terms in particular claims (I and O), but allowing all terms of universal claims (A and E) to lack existential import. This decision made Venn diagrams particularly easy to use for term logic. The square of opposition, under this Boolean set of assumptions, is often called the modern Square of opposition. In the modern square of opposition, A and O claims are contradictories, as are E and I, but all other forms of opposition cease to hold; there are no contraries, subcontraries, or subalterns. Thus, from a modern point of view, it often makes sense to talk about 'the' opposition of a claim, rather than insisting as older logicians did that a claim has several different opposites, which are in different kinds of opposition with the claim.\n\nGottlob Frege's \"Begriffsschrift\" also presents a square of oppositions, organised in an almost identical manner to the classical square, showing the contradictories, subalternates and contraries between four formulae constructed from universal quantification, negation and implication.\n\nAlgirdas Julien Greimas' semiotic square was derived from Aristotle's work.\n\nThe traditional square of opposition is now often compared with squares based on inner- and outer-negation \n\nThe square of opposition has been extended to a logical hexagon which includes the relationships of six statements. It was discovered independently by both Augustin Sesmat and Robert Blanché. It has been proven that both the square and the hexagon, followed by a \"logical cube\", belong to a regular series of n-dimensional objects called \"logical bi-simplexes of dimension n.\" The pattern also goes even beyond this.\n\nThe logical square, also called square of opposition or square of Apuleius has its origin in the four marked sentences to be employed in syllogistic reasoning: Every man is bad, the universal affirmative and its negation Not every man is bad (or Some men are not bad), the particular negative on the one hand, Some men are bad, the particular affirmative and its negation No man is bad, the universal negative on the other. Robert Blanché published with Vrin his Structures intellectuelles in 1966 and since then many scholars think that the logical square or square of opposition representing four values should be replaced by the logical hexagon which by representing six values is a more potent figure because it has the power to explain more things about logic and natural language.\n\n\n"}
{"id": "40592503", "url": "https://en.wikipedia.org/wiki?curid=40592503", "title": "Sure-thing principle", "text": "Sure-thing principle\n\nIn decision theory, the sure-thing principle states that a decision maker who would take a certain action if he knew that event \"E\" has occurred, and also if he knew that the negation of \"E\" has occurred, should also take that same action if he knows nothing about \"E\".\n\nThe principle was coined by L.J. Savage:\nHe formulated the principle as a dominance principle, but it can also be framed probabilistically. Jeffrey and later Pearl showed that Savage's principle is only valid when the probability of the event considered (e.g., the winner of the election) is unaffected by the action (buying the property). Under such conditions, the sure-thing principle is a theorem in the \"do\"-calculus (see Bayes networks). Blyth constructed a counterexample to the sure-thing principle using sequential sampling in the context of Simpson's paradox, but this example violates the required action-independence provision.\n\nThe principle is closely related to independence of irrelevant alternatives, and equivalent under the axiom of truth (everything the agent knows is true). It is similarly targeted by the Ellsberg and Allais paradoxes, in which actual people's choices seem to violate this principle. \n"}
{"id": "49605051", "url": "https://en.wikipedia.org/wiki?curid=49605051", "title": "The Circumplex Model of Group Tasks", "text": "The Circumplex Model of Group Tasks\n\nThe Circumplex Model is a graphical representation of emotional states. Fundamentally, it is a circle with pleasant on the left, unpleasant on the right, activation on the top, and deactivation on the bottom. All the other emotions are placed around the circle as combinations of these four basic states. It is based on the theory that people experience emotions as overlapping and ambiguous. Group dynamics are the distinctive behaviors and attitudes observed by people in groups, and the study thereof. It is of most interest in the business world, the workforce, or any other setting where the performance of a group is important. Joseph E McGrath enlarged the circumplex model to include group dynamics, based on the work of Shaw, Carter, Hackman, Steiner, Shiflett, Taylor, Lorge, Davis, Laughlin, and others. There are four quadrants in this model representing: generating a task, choosing correct procedure, conflict resolution, and execution, and again there are subtypes distributed around the circle. He used this model as a research tool to evaluate group task performance.\n\nGroup dynamics involve the influential actions, processes and changes that exist both within and between groups. Group dynamics also involve the scientific study of group processes. Through extensive research in the field of group dynamics, it is now well known that all groups, despite their innumerable differences, possess common properties and dynamics. Social psychological researchers have attempted to organize these commonalities, in order to further understand the genuine nature of group processes.\n\nFor instance, social psychological research indicates that there are numerous goal-related interactions and activities that groups of all sizes undertake . These interactions have been categorized by Robert F. Bales, who spent his entire life attempting to find an answer to the question, \"What do people do when they are in groups?\". To simplify the understanding of group interactions, Bales concluded that all interactions within groups could be categorized as either a \"relationship interaction\" (or socioemotional interaction) or a \"task interaction\".\n\nJust as Bales was determined to identify the basic types of interactions involved in groups, Joseph E. McGrath was determined to identify the various goal-related activities that are regularly displayed by groups. McGrath contributed greatly to the understanding of group dynamics through the development of his circumplex model of group tasks. As intended, McGrath's model effectively organizes all group-related activities by distinguishing between four basic group goals. These goals are referred to as the circumplex model of group task's four quadrants, which are categorized based on the dominant performance process involved in a group's task of interest.\n\nThe four quadrants are as follows: \n\nTo further differentiate the various goal-related group activities, McGrath further sub-divides these four categories, resulting in eight categories in total. The breakdown of these categories is as follows:\n\n1. \"Generating ideas or plans\"\n2. \"Choosing a solution\"\n3. \"Negotiating a solution to a conflict\" \n4. \"Executing a task\" \n\nAccording to McGrath and Kravitz (1982), the four most commonly represented tasks in the group dynamics literature are intellective tasks, decision-making tasks, cognitive conflict tasks and mixed-motive tasks.\n\nThe circumplex model of group tasks takes the organization of goal-related activities a step further by distinguishing between tasks that involve cooperation between group members, cooperation tasks (Types 1, 2, 3 and 8) and tasks that often lead to conflict between group members, conflict tasks (Types 4, 5, 6 and 7). Additionally, McGrath's circumplex model of group tasks also distinguishes between tasks that require action (behavioural tasks) and tasks that require conceptual review (conceptual tasks). 'Behavioural tasks' include Types 1, 6, 7 and 8, while 'conceptual tasks' include Types 2, 3, 4 and 5.\n\nThe circumplex model of group tasks is, evidently, a very detailed and complex model. To allow for a more thorough understanding of its properties, a visual representation of the model has been developed. (Need a diagram of the model)\n\nSince the circumplex model of group tasks is quite detailed and complex, numerous social psychological researchers have attempted to describe the model in various ways to ensure readers obtain an optimal understanding of the model. For instance, according to Stratus and McGrath (1994), the four quadrants and the various task types with which they contain all relate to one another within a two-dimensional space. More specifically, Stratus and McGrath (1994) states that the horizontal dimension of the circumplex model of group tasks visual representation reflect the extent to which a task entails cognitive versus behavioural performance requirements. Likewise, the vertical dimension of the circumplex model of group tasks visual representation reflects the extent and form of interdependence among members.\n"}
{"id": "30746", "url": "https://en.wikipedia.org/wiki?curid=30746", "title": "Theory", "text": "Theory\n\nA theory is a contemplative and rational type of abstract or generalizing thinking, or the results of such thinking. Depending on the context, the results might, for example, include generalized explanations of how nature works. The word has its roots in ancient Greek, but in modern use it has taken on several related meanings.\n\nTheories guide the enterprise of finding facts rather than of reaching goals, and are neutral concerning alternatives among values. A theory can be a body of knowledge, which may or may not be associated with particular explanatory models. To theorize is to develop this body of knowledge.\n\nAs already in Aristotle's definitions, theory is very often contrasted to \"practice\" (from Greek \"\", πρᾶξις) a Greek term for \"doing\", which is opposed to theory because pure theory involves no doing apart from itself. A classical example of the distinction between \"theoretical\" and \"practical\" uses the discipline of medicine: medical theory involves trying to understand the causes and nature of health and sickness, while the practical side of medicine is trying to make people healthy. These two things are related but can be independent, because it is possible to research health and sickness without curing specific patients, and it is possible to cure a patient without knowing how the cure worked.\n\nIn modern science, the term \"theory\" refers to scientific theories, a well-confirmed type of explanation of nature, made in a way consistent with scientific method, and fulfilling the criteria required by modern science. Such theories are described in such a way that scientific tests should be able to provide empirical support for, or empirically contradict (\"falsify\") it. Scientific theories are the most reliable, rigorous, and comprehensive form of scientific knowledge, in contrast to more common uses of the word \"theory\" that imply that something is unproven or speculative (which in formal terms is better characterized by the word \"hypothesis\"). Scientific theories are distinguished from hypotheses, which are individual empirically testable conjectures, and from scientific laws, which are descriptive accounts of how nature behaves under certain conditions.\n\nThe English word \"theory\" derives from a technical term in philosophy in Ancient Greek. As an everyday word, \"theoria\", , meant \"a looking at, viewing, beholding\", but in more technical contexts it came to refer to contemplative or speculative understandings of natural things, such as those of natural philosophers, as opposed to more practical ways of knowing things, like that of skilled orators or artisans. English-speakers have used the word \"theory\" since at least the late 16th century. Modern uses of the word \"theory\" derive from the original definition, but have taken on new shades of meaning, still based on the idea of a theory as a thoughtful and rational explanation of the general nature of things.\n\nAlthough it has more mundane meanings in Greek, the word apparently developed special uses early in the recorded history of the Greek language. In the book \"From Religion to Philosophy\", Francis Cornford suggests that the Orphics used the word \"theoria\" to mean \"passionate sympathetic contemplation\". Pythagoras changed the word to mean a passionate sympathetic contemplation of mathematical knowledge, because he considered this intellectual pursuit the way to reach the highest plane of existence. Pythagoras emphasized subduing emotions and bodily desires to help the intellect function at the higher plane of theory. Thus, it was Pythagoras who gave the word \"theory\" the specific meaning that led to the classical and modern concept of a distinction between theory (as uninvolved, neutral thinking) and practice.\n\nAristotle's terminology, as already mentioned, contrasts theory with \"praxis\" or practice, and this contrast remains today. For Aristotle, both practice and theory involve thinking, but the aims are different. Theoretical contemplation considers things humans do not move or change, such as nature, so it has no human aim apart from itself and the knowledge it helps create. On the other hand, \"praxis\" involves thinking, but always with an aim to desired actions, whereby humans cause change or movement themselves for their own ends. Any human movement that involves no conscious choice and thinking could not be an example of \"praxis\" or doing.\n\nTheories are analytical tools for understanding, explaining, and making predictions about a given subject matter. There are theories in many and varied fields of study, including the arts and sciences. A formal theory is syntactic in nature and is only meaningful when given a semantic component by applying it to some content (e.g., facts and relationships of the actual historical world as it is unfolding). Theories in various fields of study are expressed in natural language, but are always constructed in such a way that their general form is identical to a theory as it is expressed in the formal language of mathematical logic. Theories may be expressed mathematically, symbolically, or in common language, but are generally expected to follow principles of rational thought or logic.\n\nTheory is constructed of a set of sentences that are entirely true statements about the subject under consideration. However, the truth of any one of these statements is always relative to the whole theory. Therefore, the same statement may be true with respect to one theory, and not true with respect to another. This is, in ordinary language, where statements such as \"He is a terrible person\" cannot be judged as true or false without reference to some interpretation of who \"He\" is and for that matter what a \"terrible person\" is under the theory.\n\nSometimes two theories have exactly the same explanatory power because they make the same predictions. A pair of such theories is called indistinguishable or observationally equivalent, and the choice between them reduces to convenience or philosophical preference.\n\nThe form of theories is studied formally in mathematical logic, especially in model theory. When theories are studied in mathematics, they are usually expressed in some formal language and their statements are closed under application of certain procedures called rules of inference. A special case of this, an axiomatic theory, consists of axioms (or axiom schemata) and rules of inference. A theorem is a statement that can be derived from those axioms by application of these rules of inference. Theories used in applications are abstractions of observed phenomena and the resulting theorems provide solutions to real-world problems. Obvious examples include arithmetic (abstracting concepts of number), geometry (concepts of space), and probability (concepts of randomness and likelihood).\n\nGödel's incompleteness theorem shows that no consistent, recursively enumerable theory (that is, one whose theorems form a recursively enumerable set) in which the concept of natural numbers can be expressed, can include all true statements about them. As a result, some domains of knowledge cannot be formalized, accurately and completely, as mathematical theories. (Here, formalizing accurately and completely means that all true propositions—and only true propositions—are derivable within the mathematical system.) This limitation, however, in no way precludes the construction of mathematical theories that formalize large bodies of scientific knowledge.\n\nA theory is \"underdetermined\" (also called \"indeterminacy of data to theory\") if a rival, inconsistent theory is at least as consistent with the evidence. Underdetermination is an epistemological issue about the relation of evidence to conclusions.\n\nA theory that lacks supporting evidence is generally, more properly, referred to as a hypothesis.\n\nIf a new theory better explains and predicts a phenomenon than an old theory (i.e., it has more explanatory power), we are justified in believing that the newer theory describes reality more correctly. This is called an \"intertheoretic reduction\" because the terms of the old theory can be reduced to the terms of the new one. For instance, our historical understanding about \"sound\", \"light\" and \"heat\" have been reduced to \"wave compressions and rarefactions\", \"electromagnetic waves\", and \"molecular kinetic energy\", respectively. These terms, which are identified with each other, are called \"intertheoretic identities.\" When an old and new theory are parallel in this way, we can conclude that the new one describes the same reality, only more completely.\n\nWhen a new theory uses new terms that do not reduce to terms of an older theory, but rather replace them because they misrepresent reality, it is called an \"intertheoretic elimination.\" For instance, the obsolete scientific theory that put forward an understanding of heat transfer in terms of the movement of caloric fluid was eliminated when a theory of heat as energy replaced it. Also, the theory that phlogiston is a substance released from burning and rusting material was eliminated with the new understanding of the reactivity of oxygen.\n\nTheories are distinct from theorems. A \"theorem\" is derived deductively from axioms (basic assumptions) according to a formal system of rules, sometimes as an end in itself and sometimes as a first step toward being tested or applied in a concrete situation; theorems are said to be true in the sense that the conclusions of a theorem are logical consequences of the axioms. \"Theories\" are abstract and conceptual, and are supported or challenged by observations in the world. They are 'rigorously tentative', meaning that they are proposed as true and expected to satisfy careful examination to account for the possibility of faulty inference or incorrect observation. Sometimes theories are incorrect, meaning that an explicit set of observations contradicts some fundamental objection or application of the theory, but more often theories are corrected to conform to new observations, by restricting the class of phenomena the theory applies to or changing the assertions made. An example of the former is the restriction of classical mechanics to phenomena involving macroscopic length scales and particle speeds much lower than the speed of light.\n\nIn science, the term \"theory\" refers to \"a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment.\" Theories must also meet further requirements, such as the ability to make falsifiable predictions with consistent accuracy across a broad area of scientific inquiry, and production of strong evidence in favor of the theory from multiple independent sources (consilience).\n\nThe strength of a scientific theory is related to the diversity of phenomena it can explain, which is measured by its ability to make falsifiable predictions with respect to those phenomena. Theories are improved (or replaced by better theories) as more evidence is gathered, so that accuracy in prediction improves over time; this increased accuracy corresponds to an increase in scientific knowledge. Scientists use theories as a foundation to gain further scientific knowledge, as well as to accomplish goals such as inventing technology or curing disease.\n\nThe United States National Academy of Sciences defines scientific theories as follows:\nThe formal scientific definition of \"theory\" is quite different from the everyday meaning of the word. It refers to a comprehensive explanation of some aspect of nature that is supported by a vast body of evidence. Many scientific theories are so well established that no new evidence is likely to alter them substantially. For example, no new evidence will demonstrate that the Earth does not orbit around the sun (heliocentric theory), or that living things are not made of cells (cell theory), that matter is not composed of atoms, or that the surface of the Earth is not divided into solid plates that have moved over geological timescales (the theory of plate tectonics)...One of the most useful properties of scientific theories is that they can be used to make predictions about natural events or phenomena that have not yet been observed.\n\nFrom the American Association for the Advancement of Science:\nA scientific theory is a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment. Such fact-supported theories are not \"guesses\" but reliable accounts of the real world. The theory of biological evolution is more than \"just a theory.\" It is as factual an explanation of the universe as the atomic theory of matter or the germ theory of disease. Our understanding of gravity is still a work in progress. But the phenomenon of gravity, like evolution, is an accepted fact.\n\nNote that the term \"theory\" would not be appropriate for describing untested but intricate hypotheses or even scientific models.\n\nThe logical positivists thought of scientific theories as \"deductive theories\"—that a theory's content is based on some formal system of logic and on basic axioms. In a deductive theory, any sentence which is a logical consequence of one or more of the axioms is also a sentence of that theory. This is called the received view of theories.\n\nIn the semantic view of theories, which has largely replaced the received view, theories are viewed as scientific models. A model is a logical framework intended to represent reality (a \"model of reality\"), similar to the way that a map is a graphical model that represents the territory of a city or country. In this approach, theories are a specific category of models that fulfill the necessary criteria. (See Theories as models for further discussion.)\n\nIn physics the term \"theory\" is generally used for a mathematical framework—derived from a small set of basic postulates (usually symmetries, like equality of locations in space or in time, or identity of electrons, etc.)—which is capable of producing experimental predictions for a given category of physical systems. One good example is classical electromagnetism, which encompasses results derived from gauge symmetry (sometimes called gauge invariance) in a form of a few equations called Maxwell's equations. The specific mathematical aspects of classical electromagnetic theory are termed \"laws of electromagnetism\", reflecting the level of consistent and reproducible evidence that supports them. Within electromagnetic theory generally, there are numerous hypotheses about how electromagnetism applies to specific situations. Many of these hypotheses are already considered adequately tested, with new ones always in the making and perhaps untested.\n\nAcceptance of a theory does not require that all of its major predictions be tested, if it is already supported by sufficiently strong evidence. For example, certain tests may be infeasible or technically difficult. As a result, theories may make predictions that have not yet been confirmed or proven incorrect; in this case, the predicted results may be described informally using the term \"theoretical.\" These predictions can be tested at a later time, and if they are incorrect, this may lead to revision, invalidation, or rejection of the theory.\n\nA theory can be either \"descriptive\" as in science, or \"prescriptive\" (normative) as in philosophy. The latter are those whose subject matter consists not of empirical data, but rather of ideas. At least some of the elementary theorems of a philosophical theory are statements whose truth cannot necessarily be scientifically tested through empirical observation.\n\nA field of study is sometimes named a \"theory\" because its basis is some initial set of assumptions describing the field's approach to the subject. These assumptions are the elementary theorems of the particular theory, and can be thought of as the axioms of that field. Some commonly known examples include set theory and number theory; however literary theory, critical theory, and music theory are also of the same form.\n\nOne form of philosophical theory is a \"metatheory\" or \"meta-theory\". A metatheory is a theory whose subject matter is some other theory or set of theories. In other words, it is a theory about theories. Statements made in the metatheory about the theory are called metatheorems.\n\nA political theory is an ethical theory about the law and government. Often the term \"political theory\" refers to a general view, or specific ethic, political belief or attitude, about politics.\n\nIn social science, jurisprudence is the philosophical theory of law. Contemporary philosophy of law addresses problems internal to law and legal systems, and problems of law as a particular social institution.\n\nMost of the following are scientific theories; some are not, but rather encompass a body of knowledge or art, such as Music theory and Visual Arts Theories.\n\n\n\n"}
{"id": "39090245", "url": "https://en.wikipedia.org/wiki?curid=39090245", "title": "Transient modelling", "text": "Transient modelling\n\nTransient modelling is a way of looking at a process with the primary criterion of time, observing the pattern of changes in the subject being studied over time. The obverse of it is Steady state, where you might know only the starting and ending figures but do not understand the process by which they were derived.\n\nTransient models will reveal the pattern of a process, which might be sinusoidal or another shape that will help to design a better system to manage that process. Transient models can be done on a spreadsheet with an ability to generate charts, or by any software that can handle data of inputs and outputs and generate some sort of a display. Transient modelling does not need a computer. It is a methodology that has worked for centuries, by observers noting patterns of change against time, analysing the result and proposing improved design solutions.\n\nA simple example is a garden water tank. This is being topped up by rainfall from the roof, but when the tank is full, the remaining water goes to the drain. When the gardener draws water off, the level falls. If the garden is large and the summer is hot, a steady state will occur in summer where the tank is nearly always empty in summer. If the season is wet, the garden is getting water from the sky, and the tank is not being emptied sufficiently, so in steady state it will be observed to be always full. If the gardener has a way of observing the level of water in the tank, and a record of daily rainfall and temperatures, and is precisely metering the amount of water being drawn off every day, the numbers and the dates can be recorded in spreadsheet at daily intervals. After enough samples are taken, a chart can be developed to model the rise and fall pattern over a year, or over 2 years. With a better understanding of the process, it might emerge that a 200litre water tank would run out 20–25 days a year, but a 400-litre water tank would never run out, and a 300-litre tank would run out only 1-2 day a year and therefore that would be an acceptable risk and it would be the most economical solution.\n\nOne of the best examples of transient modelling is transient climate simulation. The analysis of ice cores in glaciers to understand climate change. Ice cores have thousands of layers, each of which represents a winter season of snowfall, and trapped in these are bubbles of air, particle of space dust and pollen which reveal climatic data of the time. By mapping these to a time scale, scientists can analyse the fluctuations over time and make predictions for the future.\n\nTransient modelling is the basis of weather forecasting, of managing ecosystems, rail timetabling, managing the electricity grid, setting the national budget, floating currency, understanding traffic flows on a freeway, solar gains on glass fronted buildings, or even of checking the day-to-day transactions of one's monthly bank statement.\n\nWith the transient modelling approach, you understand the whole process better when the inputs and outputs are graphed against time.\n\n"}
{"id": "14934822", "url": "https://en.wikipedia.org/wiki?curid=14934822", "title": "Type–token distinction", "text": "Type–token distinction\n\nThe type–token distinction is used in disciplines such as logic, linguistics, metalogic, typography, and computer programming to clarify what words mean.\n\nThe sentence \"they drive the same car\" is ambiguous. Do they drive the same \"type\" of car (the same model) or the same instance of a car type (a single vehicle)? Clarity requires us to distinguish words that represent abstract types from words that represent objects that embody or exemplify types. The type–token distinction separates types (abstract descriptive concepts) from tokens (objects that instantiate concepts).\n\nFor example: \"bicycle\" represents a type: the concept of a bicycle; whereas \"my bicycle\" represents a token of that type: an object that instantiates that type. In the sentence \"the bicycle is becoming more popular\" the word \"bicycle\" represents a type that is a concept; whereas in the sentence \"the bicycle is in the garage\" the word \"bicycle\" represents a token: a particular object.\n\nThe words type, concept, property, quality, feature and attribute (all used in describing things) tend to be used with different verbs. E.g. Suppose a rose bush is defined as a plant that is \"thorny\", \"flowering\" and \"bushy\". You might say a rose bush \"instantiates\" these three types, or \"embodies\" these three concepts, or \"exhibits\" these three properties, or \"possesses\" these three qualities, features or attributes.\n\nProperty types (e.g \"height in metres\" or \"thorny\") are often understood ontologically as concepts. Property instances (e.g. height = 1.74) are sometimes understood as measured values, and sometimes understood as sensations or observations of reality.\n\nSome say types exist in descriptions of objects, but not as tangible physical objects. They say one can show someone a particular bicycle, but cannot show someone the type \"bicycle\", as in \"\"the bicycle\" is popular.\". However types do exist in the sense that they appear in mental and documented models.\n\nSome say tokens are objects that are tangible, exist in space and time as physical matter and/or energy. However, tokens can be intangible objects of types such as \"thought\", \"tennis match\", \"government\" and \"act of kindness\".\n\nThere is a related distinction very closely connected with the type-token distinction. This distinction is the distinction between an object, or type of object, and an occurrence of it. In this sense, an occurrence is not necessarily a token. Considering the sentence: \"A rose is a rose is a rose\". We may equally correctly state that there are eight or three words in the sentence. There are, in fact, three word types in the sentence: \"rose\", \"is\" and \"a\". There are eight word tokens in a token copy of the line. The line itself is a type. There are not eight word types in the line. It contains (as stated) only the three word types, 'a', 'is' and 'rose', each of which is unique. So what do we call what there are eight of? They are occurrences of words. There are three occurrences of the word type 'a', two of 'is' and three of 'rose'.\n\nThe need to distinguish tokens of types from occurrences of types arises, not just in linguistics, but whenever types of things have other types of things occurring in them. Reflection on the simple case of occurrences of numerals is often helpful.\n\nIn typography, the type–token distinction is used to determine the presence of a text printed by movable type:\n\nThe word 'letters' was used three times in the above paragraph, each time in a different meaning. The word 'letters' is one of many words having \"type–token ambiguity\". This section disambiguates 'letters' by separating the three senses using terminology standard in logic today. The key distinctions were first made by the American logician-philosopher Charles Sanders Peirce in 1906 using terminology that he established.\n\nThe letters that are created by writing are physical objects that can be destroyed by various means: these are letter TOKENS or letter INSCRIPTIONS. The 26 letters of the alphabet are letter TYPES or letter FORMS.\n\nPeirce's type–token distinction, also applies to words, sentences, paragraphs, and so on: to anything in a universe of discourse of character-string theory, or concatenation theory. There is only one word type spelled el-ee-tee-tee-ee-ar, namely, 'letter'; but every time that word type is written, a new word token has been created.\n\nSome logicians consider a word type to be the class of its tokens. Other logicians counter that the word type has a permanence and constancy not found in the class of its tokens. The type remains the same while the class of its tokens is continually gaining new members and losing old members.\n\nThe word type 'letter' uses only four letter types: el, ee, tee, and ar. Nevertheless, it uses ee twice and tee twice. In standard terminology, the word type 'letter' has six letter OCCURRENCES and the letter type ee OCCURS twice in the word type 'letter'. Whenever a word type is inscribed, the number of letter tokens created equals the number of letter occurrences in the word type.\n\nPeirce's original words are the following.\n\"A common mode of estimating the amount of matter in a ... printed book is to count the number of words. There will ordinarily be about twenty 'thes' on a page, and, of course, they count as twenty words. In another sense of the word 'word,' however, there is but one word 'the' in the English language; and it is impossible that this word should lie visibly on a page, or be heard in any voice ... Such a ... Form, I propose to term a Type. A Single ... Object ... such as this or that word on a single line of a single page of a single copy of a book, I will venture to call a Token. ... In order that a Type may be used, it has to be embodied in a Token which shall be a sign of the Type, and thereby of the object the Type signifies.\" – Peirce 1906, Ogden-Richards, 1923, 280-1.\n\nThese distinctions are subtle but solid and easy to master. This section ends using the new terminology to disambiguate the first paragraph.\n\n\n"}
{"id": "31883", "url": "https://en.wikipedia.org/wiki?curid=31883", "title": "Uncertainty principle", "text": "Uncertainty principle\n\nIn quantum mechanics, the uncertainty principle (also known as Heisenberg's uncertainty principle) is any of a variety of mathematical inequalities asserting a fundamental limit to the precision with which certain pairs of physical properties of a particle, known as complementary variables, such as position \"x\" and momentum \"p\", can be known.\n\nIntroduced first in 1927, by the German physicist Werner Heisenberg, it states that the more precisely the position of some particle is determined, the less precisely its momentum can be known, and vice versa. The formal inequality relating the standard deviation of position \"σ\" and the standard deviation of momentum \"σ\" was derived by Earle Hesse Kennard later that year and by Hermann Weyl in 1928:\n\nwhere is the reduced Planck constant, ).\n\nHistorically, the uncertainty principle has been confused with a somewhat similar effect in physics, called the observer effect, which notes that measurements of certain systems cannot be made without affecting the systems, that is, without changing something in a system. Heisenberg utilized such an observer effect at the quantum level (see below) as a physical \"explanation\" of quantum uncertainty. It has since become clearer, however, that the uncertainty principle is inherent in the properties of all wave-like systems, and that it arises in quantum mechanics simply due to the matter wave nature of all quantum objects. Thus, \"the uncertainty principle actually states a fundamental property of quantum systems and is not a statement about the observational success of current technology\". It must be emphasized that \"measurement\" does not mean only a process in which a physicist-observer takes part, but rather any interaction between classical and quantum objects regardless of any observer.\n\nSince the uncertainty principle is such a basic result in quantum mechanics, typical experiments in quantum mechanics routinely observe aspects of it. Certain experiments, however, may deliberately test a particular form of the uncertainty principle as part of their main research program. These include, for example, tests of number–phase uncertainty relations in superconducting or quantum optics systems. Applications dependent on the uncertainty principle for their operation include extremely low-noise technology such as that required in gravitational wave interferometers.\n\nThe uncertainty principle is not readily apparent on the macroscopic scales of everyday experience. So it is helpful to demonstrate how it applies to more easily understood physical situations. Two alternative frameworks for quantum physics offer different explanations for the uncertainty principle. The wave mechanics picture of the uncertainty principle is more visually intuitive, but the more abstract matrix mechanics picture formulates it in a way that generalizes more easily.\n\nMathematically, in wave mechanics, the uncertainty relation between position and momentum arises because the expressions of the wavefunction in the two corresponding orthonormal bases in Hilbert space are Fourier transforms of one another (i.e., position and momentum are conjugate variables). A nonzero function and its Fourier transform cannot both be sharply localized. A similar tradeoff between the variances of Fourier conjugates arises in all systems underlain by Fourier analysis, for example in sound waves: A pure tone is a sharp spike at a single frequency, while its Fourier transform gives the shape of the sound wave in the time domain, which is a completely delocalized sine wave. In quantum mechanics, the two key points are that the position of the particle takes the form of a matter wave, and momentum is its Fourier conjugate, assured by the de Broglie relation , where is the wavenumber.\n\nIn matrix mechanics, the mathematical formulation of quantum mechanics, any pair of non-commuting self-adjoint operators representing observables are subject to similar uncertainty limits. An eigenstate of an observable represents the state of the wavefunction for a certain measurement value (the eigenvalue). For example, if a measurement of an observable is performed, then the system is in a particular eigenstate of that observable. However, the particular eigenstate of the observable need not be an eigenstate of another observable : If so, then it does not have a unique associated measurement for it, as the system is not in an eigenstate of that observable.\n\nAccording to the de Broglie hypothesis, every object in the universe is a wave, i.e., a situation which gives rise to this phenomenon. The position of the particle is described by a wave function formula_1. The time-independent wave function of a single-moded plane wave of wavenumber \"k\" or momentum \"p\" is\n\nThe Born rule states that this should be interpreted as a probability density amplitude function in the sense that the probability of finding the particle between \"a\" and \"b\" is\n\nIn the case of the single-moded plane wave, formula_4 is a uniform distribution. In other words, the particle position is extremely uncertain in the sense that it could be essentially anywhere along the wave packet. \n\nOn the other hand, consider a wave function that is a sum of many waves, which we may write this as\n\nwhere \"A\" represents the relative contribution of the mode \"p\" to the overall total. The figures to the right show how with the addition of many plane waves, the wave packet can become more localized. We may take this a step further to the continuum limit, where the wave function is an integral over all possible modes\n\nwith formula_7 representing the amplitude of these modes and is called the wave function in momentum space. In mathematical terms, we say that formula_7 is the \"Fourier transform\" of formula_9 and that \"x\" and \"p\" are conjugate variables. Adding together all of these plane waves comes at a cost, namely the momentum has become less precise, having become a mixture of waves of many different momenta.\n\nOne way to quantify the precision of the position and momentum is the standard deviation \"σ\". Since formula_4 is a probability density function for position, we calculate its standard deviation.\n\nThe precision of the position is improved, i.e. reduced σ, by using many plane waves, thereby weakening the precision of the momentum, i.e. increased σ. Another way of stating this is that σ and σ have an inverse relationship or are at least bounded from below. This is the uncertainty principle, the exact limit of which is the Kennard bound. Click the \"show\" button below to see a semi-formal derivation of the Kennard inequality using wave mechanics.\n(Ref ) \n\nIn matrix mechanics, observables such as position and momentum are represented by self-adjoint operators. When considering pairs of observables, an important quantity is the \"commutator\". For a pair of operators and , one defines their commutator as\nIn the case of position and momentum, the commutator is the canonical commutation relation\n\nThe physical meaning of the non-commutativity can be understood by considering the effect of the commutator on position and momentum eigenstates. Let formula_13 be a right eigenstate of position with a constant eigenvalue . By definition, this means that formula_14 Applying the commutator to formula_13 yields\nwhere is the identity operator.\n\nSuppose, for the sake of proof by contradiction, that formula_13 is also a right eigenstate of momentum, with constant eigenvalue . If this were true, then one could write\nOn the other hand, the above canonical commutation relation requires that\nThis implies that no quantum state can simultaneously be both a position and a momentum eigenstate.\n\nWhen a state is measured, it is projected onto an eigenstate in the basis of the relevant observable. For example, if a particle's position is measured, then the state amounts to a position eigenstate. This means that the state is \"not\" a momentum eigenstate, however, but rather it can be represented as a sum of multiple momentum basis eigenstates. In other words, the momentum must be less precise. This precision may be quantified by the standard deviations, \n\nAs in the wave mechanics interpretation above, one sees a tradeoff between the respective precisions of the two, quantified by the uncertainty principle.\n\nThe most common general form of the uncertainty principle is the \"Robertson uncertainty relation\".\n\nFor an arbitrary Hermitian operator formula_22 we can associate a standard deviation\n\nwhere the brackets formula_24 indicate an expectation value. For a pair of operators formula_25 and formula_26, we may define their \"commutator\" as\n\nIn this notation, the Robertson uncertainty relation is given by\n\nThe Robertson uncertainty relation immediately follows from a slightly stronger inequality, the \"Schrödinger uncertainty relation\",\n\nwhere we have introduced the \"anticommutator\",\n\nSince the Robertson and Schrödinger relations are for general operators, the relations can be applied to any two observables to obtain specific uncertainty relations. A few of the most common relations found in the literature are given below.\nSuppose we consider a quantum particle on a ring, where the wave function depends on an angular variable formula_41, which we may take to lie in the interval formula_42. Define \"position\" and \"momentum\" operators formula_25 and formula_26 by\n\nand\n\nwhere we impose periodic boundary conditions on formula_26. Note that the definition of formula_25 depends on our choice to have formula_41 range from 0 to formula_50. These operators satisfy the usual commutation relations for position and momentum operators, formula_51.\n\nNow let formula_52 be any of the eigenstates of formula_26, which are given by formula_54. Note that these states are normalizable, unlike the eigenstates of the momentum operator on the line. Note also that the operator formula_25 is bounded, since formula_41 ranges over a bounded interval. Thus, in the state formula_52, the uncertainty of formula_58 is zero and the uncertainty of formula_59 is finite, so that \nAlthough this result appears to violate the Robertson uncertainty principle, the paradox is resolved when we note that formula_52 is not in the domain of the operator formula_62, since multiplication by formula_41 disrupts the periodic boundary conditions imposed on formula_26. Thus, the derivation of the Robertson relation, which requires formula_65 and formula_66 to be defined, does not apply. (These also furnish an example of operators satisfying the canonical commutation relations but not the Weyl relations.)\n\nFor the usual position and momentum operators formula_67 and formula_68 on the real line, no such counterexamples can occur. As long as formula_69 and formula_70 are defined in the state formula_52, the Heisenberg uncertainty principle holds, even if formula_52 fails to be in the domain of formula_73 or of formula_74.\n\nConsider a one-dimensional quantum harmonic oscillator (QHO). It is possible to express the position and momentum operators in terms of the creation and annihilation operators:\n\nUsing the standard rules for creation and annihilation operators on the eigenstates of the QHO,\nthe variances may be computed directly,\nThe product of these standard deviations is then\n\nIn particular, the above Kennard bound is saturated for the ground state , for which the probability density is just the normal distribution.\n\nIn a quantum harmonic oscillator of characteristic angular frequency ω, place a state that is offset from the bottom of the potential by some displacement \"x\" as\nwhere Ω describes the width of the initial state but need not be the same as ω. Through integration over the , we can solve for the -dependent solution. After many cancelations, the probability densities reduce to\nwhere we have used the notation formula_85 to denote a normal distribution of mean μ and variance σ. Copying the variances above and applying trigonometric identities, we can write the product of the standard deviations as\n\nFrom the relations\n\nwe can conclude the following: (the right most equality holds only when Ω = \"ω\") .\n\nA coherent state is a right eigenstate of the annihilation operator,\nwhich may be represented in terms of Fock states as\n\nOne expects that the factor may be replaced by , \nwhich is only known if either or is convex.\n\nThe mathematician G. H. Hardy formulated the following uncertainty principle: it is not possible for and to both be \"very rapidly decreasing\". Specifically, if in formula_91 is such that\nand\n\nthen, if , while if , then there is a polynomial of degree such that\n\nThis was later improved as follows: if formula_96 is such that\n\nthen\nwhere is a polynomial of degree and is a real positive definite matrix.\n\nThis result was stated in Beurling's complete works without proof and proved in Hörmander (the case formula_99) and Bonami, Demange, and Jaming for the general case. Note that Hörmander–Beurling's version implies the case in Hardy's Theorem while the version by Bonami–Demange–Jaming covers the full strength of Hardy's Theorem. A different proof of Beurling's theorem based on Liouville's theorem appeared in\nref.\n\nA full description of the case as well as the following extension to Schwartz class distributions appears in ref.\n\nTheorem. If a tempered distribution formula_100 is such that\n\nand\nthen\nfor some convenient polynomial and real positive definite matrix of type .\n\nWerner Heisenberg formulated the uncertainty principle at Niels Bohr's institute in Copenhagen, while working on the mathematical foundations of quantum mechanics.\n\nIn 1925, following pioneering work with Hendrik Kramers, Heisenberg developed matrix mechanics, which replaced the ad hoc old quantum theory with modern quantum mechanics. The central premise was that the classical concept of motion does not fit at the quantum level, as electrons in an atom do not travel on sharply defined orbits. Rather, their motion is smeared out in a strange way: the Fourier transform of its time dependence only involves those frequencies that could be observed in the quantum jumps of their radiation.\n\nHeisenberg's paper did not admit any unobservable quantities like the exact position of the electron in an orbit at any time; he only allowed the theorist to talk about the Fourier components of the motion. Since the Fourier components were not defined at the classical frequencies, they could not be used to construct an exact trajectory, so that the formalism could not answer certain overly precise questions about where the electron was or how fast it was going.\n\nIn March 1926, working in Bohr's institute, Heisenberg realized that the non-commutativity implies the uncertainty principle. This implication provided a clear physical interpretation for the non-commutativity, and it laid the foundation for what became known as the Copenhagen interpretation of quantum mechanics. Heisenberg showed that the commutation relation implies an uncertainty, or in Bohr's language a complementarity. Any two variables that do not commute cannot be measured simultaneously—the more precisely one is known, the less precisely the other can be known. Heisenberg wrote:It can be expressed in its simplest form as follows: One can never know with perfect accuracy both of those two important factors which determine the movement of one of the smallest particles—its position and its velocity. It is impossible to determine accurately \"both\" the position and the direction and speed of a particle \"at the same instant\".\n\nIn his celebrated 1927 paper, \"Über den anschaulichen Inhalt der quantentheoretischen Kinematik und Mechanik\" (\"On the Perceptual Content of Quantum Theoretical Kinematics and Mechanics\"), Heisenberg established this expression as the minimum amount of unavoidable momentum disturbance caused by any position measurement, but he did not give a precise definition for the uncertainties Δx and Δp. Instead, he gave some plausible estimates in each case separately. In his Chicago lecture he refined his principle:\n\nKennard in 1927 first proved the modern inequality:\n\nwhere , and , are the standard deviations of position and momentum. Heisenberg only proved relation () for the special case of Gaussian states.\n\nThroughout the main body of his original 1927 paper, written in German, Heisenberg used the word, \"Ungenauigkeit\" (\"indeterminacy\"),\nto describe the basic theoretical principle. Only in the endnote did he switch to the word, \"Unsicherheit\" (\"uncertainty\"). When the English-language version of Heisenberg's textbook, \"The Physical Principles of the Quantum Theory\", was published in 1930, however, the translation \"uncertainty\" was used, and it became the more commonly used term in the English language thereafter.\n\nThe principle is quite counter-intuitive, so the early students of quantum theory had to be reassured that naive measurements to violate it were bound always to be unworkable. One way in which Heisenberg originally illustrated the intrinsic impossibility of violating the uncertainty principle is by utilizing the observer effect of an imaginary microscope as a measuring device.\n\nHe imagines an experimenter trying to measure the position and momentum of an electron by shooting a photon at it.\n\nThe combination of these trade-offs implies that no matter what photon wavelength and aperture size are used, the product of the uncertainty in measured position and measured momentum is greater than or equal to a lower limit, which is (up to a small numerical factor) equal to Planck's constant. Heisenberg did not care to formulate the uncertainty principle as an exact limit (which is elaborated below), and preferred to use it instead, as a heuristic quantitative statement, correct up to small numerical factors, which makes the radically new noncommutativity of quantum mechanics inevitable.\n\nThe Copenhagen interpretation of quantum mechanics and Heisenberg's Uncertainty Principle were, in fact, seen as twin targets by detractors who believed in an underlying determinism and realism. According to the Copenhagen interpretation of quantum mechanics, there is no fundamental reality that the quantum state describes, just a prescription for calculating experimental results. There is no way to say what the state of a system fundamentally is, only what the result of observations might be.\n\nAlbert Einstein believed that randomness is a reflection of our ignorance of some fundamental property of reality, while Niels Bohr believed that the probability distributions are fundamental and irreducible, and depend on which measurements we choose to perform. Einstein and Bohr debated the uncertainty principle for many years.\n\nWolfgang Pauli called Einstein's fundamental objection to the uncertainty principle \"the ideal of the detached observer\" (phrase translated from the German):\n\nThe first of Einstein's thought experiments challenging the uncertainty principle went as follows:\n\nBohr's response was that the wall is quantum mechanical as well, and that to measure the recoil to accuracy , the momentum of the wall must be known to this accuracy before the particle passes through. This introduces an uncertainty in the position of the wall and therefore the position of the slit equal to , and if the wall's momentum is known precisely enough to measure the recoil, the slit's position is uncertain enough to disallow a position measurement.\n\nA similar analysis with particles diffracting through multiple slits is given by Richard Feynman.\n\nBohr was present when Einstein proposed the thought experiment which has become known as Einstein's box. Einstein argued that \"Heisenberg's uncertainty equation implied that the uncertainty in time was related to the uncertainty in energy, the product of the two being related to Planck's constant.\" Consider, he said, an ideal box, lined with mirrors so that it can contain light indefinitely. The box could be weighed before a clockwork mechanism opened an ideal shutter at a chosen instant to allow one single photon to escape. \"We now know, explained Einstein, precisely the time at which the photon left the box.\" \"Now, weigh the box again. The change of mass tells the energy of the emitted light. In this manner, said Einstein, one could measure the energy emitted and the time it was released with any desired precision, in contradiction to the uncertainty principle.\"\n\nBohr spent a sleepless night considering this argument, and eventually realized that it was flawed. He pointed out that if the box were to be weighed, say by a spring and a pointer on a scale, \"since the box must move vertically with a change in its weight, there will be uncertainty in its vertical velocity and therefore an uncertainty in its height above the table. ... Furthermore, the uncertainty about the elevation above the earth's surface will result in an uncertainty in the rate of the clock,\" because of Einstein's own theory of gravity's effect on time.\n\"Through this chain of uncertainties, Bohr showed that Einstein's light box experiment could not simultaneously measure exactly both the energy of the photon and the time of its escape.\"\n\nBohr was compelled to modify his understanding of the uncertainty principle after another thought experiment by Einstein. In 1935, Einstein, Podolsky and Rosen (see EPR paradox) published an analysis of widely separated entangled particles. Measuring one particle, Einstein realized, would alter the probability distribution of the other, yet here the other particle could not possibly be disturbed. This example led Bohr to revise his understanding of the principle, concluding that the uncertainty was not caused by a direct interaction.\n\nBut Einstein came to much more far-reaching conclusions from the same thought experiment. He believed the \"natural basic assumption\" that a complete description of reality would have to predict the results of experiments from \"locally changing deterministic quantities\" and therefore would have to include more information than the maximum possible allowed by the uncertainty principle.\n\nIn 1964, John Bell showed that this assumption can be falsified, since it would imply a certain inequality between the probabilities of different experiments. Experimental results confirm the predictions of quantum mechanics, ruling out Einstein's basic assumption that led him to the suggestion of his \"hidden variables\". These hidden variables may be \"hidden\" because of an illusion that occurs during observations of objects that are too large or too small. This illusion can be likened to rotating fan blades that seem to pop in and out of existence at different locations and sometimes seem to be in the same place at the same time when observed. This same illusion manifests itself in the observation of subatomic particles. Both the fan blades and the subatomic particles are moving so fast that the illusion is seen by the observer. Therefore, it is possible that there would be predictability of the subatomic particles behavior and characteristics to a recording device capable of very high speed tracking...Ironically this fact is one of the best pieces of evidence supporting Karl Popper's philosophy of invalidation of a theory by falsification-experiments. That is to say, here Einstein's \"basic assumption\" became falsified by experiments based on Bell's inequalities. For the objections of Karl Popper to the Heisenberg inequality itself, see below.\n\nWhile it is possible to assume that quantum mechanical predictions are due to nonlocal, hidden variables, and in fact David Bohm invented such a formulation, this resolution is not satisfactory to the vast majority of physicists. The question of whether a random outcome is predetermined by a nonlocal theory can be philosophical, and it can be potentially intractable. If the hidden variables are not constrained, they could just be a list of random digits that are used to produce the measurement outcomes. To make it sensible, the assumption of nonlocal hidden variables is sometimes augmented by a second assumption—that the size of the observable universe puts a limit on the computations that these variables can do. A nonlocal theory of this sort predicts that a quantum computer would encounter fundamental obstacles when attempting to factor numbers of approximately 10,000 digits or more; a potentially achievable task in quantum mechanics.\n\nKarl Popper approached the problem of indeterminacy as a logician and metaphysical realist. He disagreed with the application of the uncertainty relations to individual particles rather than to ensembles of identically prepared particles, referring to them as \"statistical scatter relations\". In this statistical interpretation, a \"particular\" measurement may be made to arbitrary precision without invalidating the quantum theory. This directly contrasts with the Copenhagen interpretation of quantum mechanics, which is non-deterministic but lacks local hidden variables.\n\nIn 1934, Popper published \"Zur Kritik der Ungenauigkeitsrelationen\" (\"Critique of the Uncertainty Relations\") in \"Naturwissenschaften\", and in the same year \"Logik der Forschung\" (translated and updated by the author as \"The Logic of Scientific Discovery\" in 1959), outlining his arguments for the statistical interpretation. In 1982, he further developed his theory in \"Quantum theory and the schism in Physics\", writing:\n[Heisenberg's] formulae are, beyond all doubt, derivable \"statistical formulae\" of the quantum theory. But they have been \"habitually misinterpreted\" by those quantum theorists who said that these formulae can be interpreted as determining some upper limit to the \"precision of our measurements\". [original emphasis]\n\nPopper proposed an experiment to falsify the uncertainty relations, although he later withdrew his initial version after discussions with Weizsäcker, Heisenberg, and Einstein; this experiment may have influenced the formulation of the EPR experiment.\n\nThe many-worlds interpretation originally outlined by Hugh Everett III in 1957 is partly meant to reconcile the differences between Einstein's and Bohr's views by replacing Bohr's wave function collapse with an ensemble of deterministic and independent universes whose \"distribution\" is governed by wave functions and the Schrödinger equation. Thus, uncertainty in the many-worlds interpretation follows from each observer within any universe having no knowledge of what goes on in the other universes.\n\nSome scientists including Arthur Compton and Martin Heisenberg have suggested that the uncertainty principle, or at least the general probabilistic nature of quantum mechanics, could be evidence for the two-stage model of free will. One critique, however, is that apart from the basic role of quantum mechanics as a foundation for chemistry, nontrivial biological mechanisms requiring quantum mechanics are unlikely, due to the rapid decoherence time of quantum systems at room temperature. The standard view, however, is that this decoherence is overcome by both screening and decoherence-free subspaces found in biological cells.\n\nThere is reason to believe that violating the uncertainty principle also strongly implies the violation of the second law of thermodynamics.\n\n"}
