{"id": "56504758", "url": "https://en.wikipedia.org/wiki?curid=56504758", "title": "Art Workers News and Art &amp; Artists", "text": "Art Workers News and Art &amp; Artists\n\nArt Workers News, also known as Art & Artists, was the highly influential artist-run publication of the Foundation for the Community of Artists (FCA), an organization that grew out of the National Art Workers Community (a splinter group of the Art Workers’ Coalition). From 1971 to 1989, the publication was the paper of record for the world of working artists. Its circulation reached a high of 40 thousand subscribers.\nOriginally a four-page newsletter, \"Art Workers News\" grew into a monthly newspaper. During the early 1980s, FCA changed the publication’s name to \"Art&Artists\" and made it a bi-monthly.\n\nRather than reviewing shows, AWN/A&A focused on topics central to the lives of working artists in the 1970s and ’80s, (and continuing today): artist housing, health hazards, business practices, health insurance, women’s issues, issues affecting artists of color, law and the arts and censorship were impacting the daily life of artists. The publication featured, as well, contemporary theories and philosophies in the arts, general news articles, a select number of book reviews, commentaries, editorials and special issues.\n\nThe special issues were wide-ranging, reflecting the times and the interests of the editors and writers who were involved at a given time over the course of its existence.\n\nSpecial issues included: “Art Against Apartheid.” “Art and Sports,” “Are There Too Many Artists?,” “Tilted Arc by Richard Serra,” \"Museums,\" and “Artists Against US Intervention in Central America.”\n\n\"Art Workers News\" was also the publication of record for the Comprehensive Employment Act (CETA) Artists Program in New York City and documented the work of artists in all media associated with the project.\n\n\"AWN/A&A\" produced 160 issues over eighteen years. Editors and production staff were volunteers; writers and photographers were paid (minimally) in accordance with the belief that people, including artists and writers, deserved to be paid for their work. The journal newspaper gave voice to many artists, critics and journalists who were prominent in the cultural milieu of its time, as well as to younger professionals at the start of their careers.\n\nMany of the original newsprint issues and some microfiche editions are catalogued in libraries across the U.S., Canada and Australia. The Fales Library at New York University has most of the volumes, and is a source for researchers, artists, art historians, students and scholars. Gwen Allen, a professor at San Francisco State University includes additional information about the publication in her book, \"Artists’ Magazines: An Alternative Space for Art.\"\n\nElliott Barowitz, painter, media artist and writer was the long-time Executive Editor of the journal and remains keeper of its archives; Senior and Associate Editors, and frequent contributors included artists Walter Thompson, Blaise Tobia and Virginia Maksymowicz; artist/photographer Walter Weissman, and arts writer, Daniel Grant.\n\nThe list of contributors to the publication is long and distinguished. It includes men and women early in their careers who went on to become extremely well known writers, artists, editors, curators and critics. Among them are: Lawrence Alloway, Benny Andrews, Lori Antonacci, Dore Ashton, Rudolf Baranik, Jane Barowitz, Gregory Battcock, Pamela Bickart, Bernie Brown, Eva Cockcroft, Tad Crawford, Jimmie Durham, Saunders Ellis, Josephine Gear, Eunice Golden, Leon Golub, Adam Gopnik, Alex Gross, John Hanhardt, Michael Harrington, Stephanie Harrington, Baird Jones, Marc Kostabi, Barbara Kruger, Tuli Kupferberg, Donald Kuspit, Peter Leggieri, Lucy Lippard, Gerald Marzorati, Carl Methfessel, Michael McCann, Cynthia Navaretta, Barbara Nessim, Susan Ortega, Howardina Pindell, Mel Rosenthal, Larry Rosing, Laurin Raiken, Lee Rosenbaum, Barnaby Ruhe, Nancy St. Paul, Irving Sandler, Jacqueline Skyles, Stephen S’soreff, Juan Sanchez, Nancy Spero, May Stevens, Ted Striggles, Marcia Tucker, Judd Tully, David Troy, Allan Wallach, Tim Yohn, and many others.\n\nAlberto, Alexander and Stimson, Blake, \"Conceptual Art: A Critical Anthology\", Cambridge, MA: MIT Press, 1999\n\nAllen, Gwen, \"Artists’ Magazines: An Alternative Space for Art\", Cambridge, MA: MIT Press, 2011\n\nBryan-Wilson, Julia, \"Art Workers: Radical Practice in the Vietnam War Era\", University of California Press, 2011\n\nKostelanetz, Richard, \"A Dictionary of the Avant Gardes\", New York, NY: Rutledge, 2011\n\nSkiles, Jacqueline, “The National Art Workers' Community: Still Struggling,” \"Art Journal\", Volume 34, Issue 4, 1975\n"}
{"id": "52316070", "url": "https://en.wikipedia.org/wiki?curid=52316070", "title": "Awaze Tribune", "text": "Awaze Tribune\n\nThe Awaze Tribune (AwazeTribune) is an Eritrean news satire organization that publishes articles on international, national, and local news. Based in Asmara, Eritrea. The website carries articles that may cover current events, both real and fictional, satirizing the tone and format of traditional news organizations with stories, editorials, op-ed pieces, and man-in-the-street interviews using a traditional news website layout and an editorial voice modeled after the New York Times, and the usage of the AP Style of news writing.\n\nThe articles of AwazeTribune mostly revolve around the coverage of real or imaginary events in a satirical and offbeat way, taking again most of the press codes. AwazeTribune appeared first an element on Twitter that began in February 2016, during the American Presidential Elections, before being transformed to becoming a website. Since then, many of its articles were relayed in the press.\n\nIn its about page, the website claims to have been founded in 32 A.D., to have covered the baptism of Jesus Christ and the birth of the Prophet Mohammad, and to receive 14 billion unique daily hits.\n\nThe identity of the curators is yet unknown, in a recent interview with Radio France Internationale, one of the anonymous personalities associated with the website flatly declined to be identified stating that \" the message of AT outweighs the identity of the curators.\"\n\nMany users of social media have taken AwazeTribune articles to be literal (especially those which address controversial topics), and have expressed their anger and confusion online failing, according to one of the website's managers, to 'comprehend that the site makes a tongue-in-cheek news commentary revolving around the secretive Eritrean regime'.\n"}
{"id": "1657023", "url": "https://en.wikipedia.org/wiki?curid=1657023", "title": "Basic needs", "text": "Basic needs\n\nThe basic needs approach is one of the major approaches to the measurement of absolute poverty in developing countries. It attempts to define the absolute minimum resources necessary for long-term physical well-being, usually in terms of consumption goods. The poverty line is then defined as the amount of income required to satisfy those needs. The 'basic needs' approach was introduced by the International Labour Organization's World Employment Conference in 1976. \"Perhaps the high point of the WEP was the World Employment Conference of 1976, which proposed the satisfaction of basic human needs as the overriding objective of national and international development policy. The basic needs approach to development was endorsed by governments and workers’ and employers’ organizations from all over the world. It influenced the programmes and policies of major multilateral and bilateral development agencies, and was the precursor to the human development approach.\"\n\nA traditional list of immediate \"basic needs\" is food (including water), shelter and clothing. Many modern lists emphasize the minimum level of consumption of 'basic needs' of not just food, water, clothing and shelter, but also sanitation, education, healthcare, and internet. Different agencies use different lists.\n\nThe basic needs approach has been described as consumption-oriented, giving the impression \"that poverty elimination is all too easy.\" Amartya Sen focused on 'capabilities' rather than consumption.\n\nIn the development discourse, the basic needs model focuses on the measurement of what is believed to be an eradicable level of poverty. Development programs following the basic needs approach do not invest in economically productive activities that will help a society carry its own weight in the future, rather it focuses on allowing the society to consume just enough to rise above the poverty line and meet its basic needs. These programs focus more on subsistence than fairness. Nevertheless, in terms of \"measurement\", the basic needs or absolute approach is important. The 1995 world summit on social development in Copenhagen had, as one of its principal declarations that all nations of the world should develop measures of both absolute and relative poverty and should gear national policies to \"eradicate absolute poverty by a target date specified by each country in its national context.\"\n\nProfessor Chris Sarlo, an economist at Nipissing University in North Bay, Ontario, Canada and a senior fellow of the Fraser Institute, uses Statistics Canada's socio-economic databases, particularly the \"Survey of Household Spending\" to determine the cost of a list of household necessities. The list includes food, shelter, clothing, health care, personal care, essential furnishings, transportation and communication, laundry, home insurance, and miscellaneous; it assumes that education is provided freely to all residents of Canada. This is calculated for various communities across Canada and adjusted for family size. With this information, he determines the proportion of Canadian households that have insufficient income to afford those necessities. Based on his basic needs poverty threshold, the poverty rate in Canada, the poverty rate has declined from about 12% of Canadian households to about 5% since the 1970s. This is in sharp contrast to the results of Statistic Canada, Conference Board of Canada, the Organisation for Economic Co-operation and Development (OECD) and UNESCO reports using the relative poverty measure considered to the most useful for advanced industrial nations like Canada, which Sarlo rejects.\n\nOECD and UNICEF rate Canada's poverty rate much higher using a relative poverty threshold. Statistics Canada's LICO, which Sarlo also rejects, also result in higher poverty rates. According to a 2008 report by the Organisation for Economic Co-operation and Development (OECD), the rate of poverty in Canada, is among the highest of the OECD member nations, the world's wealthiest industrialized nations. There is no official government definition and therefore, measure, for poverty in Canada. However, Dennis Raphael, author of \"Poverty in Canada: Implications for Health and Quality of Life\" reported that the United Nations Development Program (UNDP), the United Nations Children’s Fund (UNICEF), the Organisation for Economic Co-operation and Development (OECD) and Canadian poverty researchers find that relative poverty is the \"most useful measure for ascertaining poverty rates in wealthy developed nations such as Canada.\" In its report released the Conference Board \n\nThe Municipality of Rosario, Batangas, Philippines implemented its Aksyon ng Bayan Rosario 2001 And Beyond Human and Ecological Security Plan using this concept as a core strategy through the \"Minimum Basic Needs Approach to Improved Quality of Life - Community-Based Information System (MBN-CBIS)\" prescribed by the Philippine Government. This approach helped the municipal government identify priority families and communities for intervention, as well as rationalize the allocation of its social development funds.\n\nIn the United States, the equivalent measures are called \"self-sufficiency standards\" or \"living income standards\". Unlike the federal poverty level (FPL), which is calculated from a single, national variable (cost of food), these models assume that different households have different needs, based on factors such as the number and age of children in the household, and the cost of housing in the particular area (usually a county) that they live in. In keeping with the principles of basic needs, these measurements do not include any extra money for entertainment, savings, debt payment, or unusual or avoidable expenses, such as vehicle repairs. It assumes that adults will be working and pay taxes; it also includes costs of all government, charitable, and family subsidies, such as free medical care through Medicaid, free food from the USDA food stamps program or a food bank, or free childcare from a grandparent. All of these costs are ignored by the official FPL measurement, but included in a self-sufficiency standard.\n\nMinimum expenses vary by region. For housing, child care, food, transportation, health care, and other necessary expenses, plus net taxes, a family in middle-class Warren County in northwestern Pennsylvania of one adult and two children (one preschooler, one school-aged) needed a minimum income of $30,269 to pay its own way in 2006. Child care is the largest expense in this budget, followed by housing, taxes, and food. The same family, living in the wealthy Seattle region of Washington would need to earn $48,269 to be self-sufficient while remaining in that location. These figures contrast sharply with the FPL for that year, which was just $16,600 for any three-person household.\n\n\nBasic Needs in Development Planning, Michael Hopkins and Rolph Van Der Hoeven (Gower, Aldershot, UK, 1983)\n\n"}
{"id": "4163", "url": "https://en.wikipedia.org/wiki?curid=4163", "title": "Bertrand Russell", "text": "Bertrand Russell\n\nBertrand Arthur William Russell, 3rd Earl Russell, (; 18 May 1872 – 2 February 1970) was a British philosopher, logician, mathematician, historian, writer, social critic, political activist, and Nobel laureate. At various points in his life, Russell considered himself a liberal, a socialist and a pacifist, but he also admitted that he had \"never been any of these things, in any profound sense.\" Russell was born in Monmouthshire into one of the most prominent aristocratic families in the United Kingdom.\n\nIn the early 20th century, Russell led the British \"revolt against idealism\". He is considered one of the founders of analytic philosophy along with his predecessor Gottlob Frege, colleague G. E. Moore and protégé Ludwig Wittgenstein. He is widely held to be one of the 20th century's premier logicians. With A. N. Whitehead he wrote \"Principia Mathematica\", an attempt to create a logical basis for mathematics. His philosophical essay \"On Denoting\" has been considered a \"paradigm of philosophy\". His work has had a considerable influence on mathematics, logic, set theory, linguistics, artificial intelligence, cognitive science, computer science (see type theory and type system) and philosophy, especially the philosophy of language, epistemology and metaphysics.\n\nRussell was a prominent anti-war activist and he championed anti-imperialism. Occasionally, he advocated preventive nuclear war, before the opportunity provided by the atomic monopoly had passed and \"welcomed with enthusiasm\" world government. He went to prison for his pacifism during World War I. Later, Russell concluded that war against Adolf Hitler's Nazi Germany was a necessary \"lesser of two evils\" and criticized Stalinist totalitarianism, attacked the involvement of the United States in the Vietnam War and was an outspoken proponent of nuclear disarmament. In 1950, Russell was awarded the Nobel Prize in Literature \"in recognition of his varied and significant writings in which he champions humanitarian ideals and freedom of thought\".\n\nBertrand Russell was born on 18 May 1872 at Ravenscroft, Trellech, Monmouthshire, into an influential and liberal family of the British aristocracy. His parents, Viscount and Viscountess Amberley, were radical for their times. Lord Amberley consented to his wife's affair with their children's tutor, the biologist Douglas Spalding. Both were early advocates of birth control at a time when this was considered scandalous. Lord Amberley was an atheist and his atheism was evident when he asked the philosopher John Stuart Mill to act as Russell's secular godfather. Mill died the year after Russell's birth, but his writings had a great effect on Russell's life.\n\nHis paternal grandfather, the Earl Russell, had been asked twice by Queen Victoria to form a government, serving her as Prime Minister in the 1840s and 1860s. The Russells had been prominent in England for several centuries before this, coming to power and the peerage with the rise of the Tudor dynasty (see: Duke of Bedford). They established themselves as one of the leading British Whig families, and participated in every great political event from the Dissolution of the Monasteries in 1536–1540 to the Glorious Revolution in 1688–1689 and the Great Reform Act in 1832.\n\nLady Amberley was the daughter of Lord and Lady Stanley of Alderley. Russell often feared the ridicule of his maternal grandmother, one of the campaigners for education of women.\n\nRussell had two siblings: brother Frank (nearly seven years older than Bertrand), and sister Rachel (four years older). In June 1874 Russell's mother died of diphtheria, followed shortly by Rachel's death. In January 1876, his father died of bronchitis following a long period of depression. Frank and Bertrand were placed in the care of their staunchly Victorian paternal grandparents, who lived at Pembroke Lodge in Richmond Park. His grandfather, former Prime Minister Earl Russell, died in 1878, and was remembered by Russell as a kindly old man in a wheelchair. His grandmother, the Countess Russell (née Lady Frances Elliot), was the dominant family figure for the rest of Russell's childhood and youth.\n\nThe countess was from a Scottish Presbyterian family, and successfully petitioned the Court of Chancery to set aside a provision in Amberley's will requiring the children to be raised as agnostics. Despite her religious conservatism, she held progressive views in other areas (accepting Darwinism and supporting Irish Home Rule), and her influence on Bertrand Russell's outlook on social justice and standing up for principle remained with him throughout his life. (One could challenge the view that Bertrand stood up for his principles, based on his own well-known quotation: \"I would never die for my beliefs because I might be wrong\".) Her favourite Bible verse, \"Thou shalt not follow a multitude to do evil\" (Exodus 23:2), became his motto. The atmosphere at Pembroke Lodge was one of frequent prayer, emotional repression, and formality; Frank reacted to this with open rebellion, but the young Bertrand learned to hide his feelings.\n\nRussell's adolescence was very lonely, and he often contemplated suicide. He remarked in his autobiography that his keenest interests were in religion and mathematics, and that only his wish to know more mathematics kept him from suicide. He was educated at home by a series of tutors. When Russell was eleven years old, his brother Frank introduced him to the work of Euclid, which transformed his life.\n\nDuring these formative years he also discovered the works of Percy Bysshe Shelley. In his autobiography, he writes: \"I spent all my spare time reading him, and learning him by heart, knowing no one to whom I could speak of what I thought or felt, I used to reflect how wonderful it would have been to know Shelley, and to wonder whether I should meet any live human being with whom I should feel so much sympathy\". Russell claimed that beginning at age 15, he spent considerable time thinking about the validity of Christian religious dogma, which he found very unconvincing. At this age, he came to the conclusion that there is no free will and, two years later, that there is no life after death. Finally, at the age of 18, after reading Mill's \"Autobiography\", he abandoned the \"First Cause\" argument and became an atheist.\n\nHe traveled to the continent in 1890 with an American friend, Edward FitzGerald, and with FitzGerald's family he visited the Paris Exhibition of 1889 and was able to climb the Eiffel Tower soon after it was completed.\n\nRussell won a scholarship to read for the Mathematical Tripos at Trinity College, Cambridge, and commenced his studies there in 1890, taking as coach Robert Rumsey Webb. He became acquainted with the younger George Edward Moore and came under the influence of Alfred North Whitehead, who recommended him to the Cambridge Apostles. He quickly distinguished himself in mathematics and philosophy, graduating as seventh Wrangler in the former in 1893 and becoming a Fellow in the latter in 1895.\n\nRussell was 17 years old in the summer of 1889 when he met the family of Alys Pearsall Smith, an American Quaker five years older, who was a graduate of Bryn Mawr College near Philadelphia. He became a friend of the Pearsall Smith family – they knew him primarily as \"Lord John's grandson\" and enjoyed showing him off.\n\nHe soon fell in love with the puritanical, high-minded Alys, and, contrary to his grandmother's wishes, married her on 13 December 1894. Their marriage began to fall apart in 1901 when it occurred to Russell, while he was cycling, that he no longer loved her. She asked him if he loved her and he replied that he did not. Russell also disliked Alys's mother, finding her controlling and cruel. It was to be a hollow shell of a marriage. A lengthy period of separation began in 1911 with Russell's affair with Lady Ottoline Morrell, and he and Alys finally divorced in 1921 to enable Russell to remarry.\n\nDuring his years of separation from Alys, Russell had passionate (and often simultaneous) affairs with a number of women, including Morrell and the actress Lady Constance Malleson. Some have suggested that at this point he had an affair with Vivienne Haigh-Wood, the English governess and writer, and first wife of T. S. Eliot.\n\nRussell began his published work in 1896 with \"German Social Democracy\", a study in politics that was an early indication of a lifelong interest in political and social theory. In 1896 he taught German social democracy at the London School of Economics. He was a member of the Coefficients dining club of social reformers set up in 1902 by the Fabian campaigners Sidney and Beatrice Webb.\n\nHe now started an intensive study of the foundations of mathematics at Trinity.\nIn 1898 he wrote \"An Essay on the Foundations of Geometry\" which discussed the Cayley–Klein metrics used for non-Euclidean geometry.\nHe attended the International Congress of Philosophy in Paris in 1900 where he met Giuseppe Peano and Alessandro Padoa. The Italians had responded to Georg Cantor, making a science of set theory; they gave Russell their literature including the Formulario mathematico. Russell was impressed by the precision of Peano's arguments at the Congress, read the literature upon returning to England, and came upon Russell's paradox. In 1903 he published \"The Principles of Mathematics\", a work on foundations of mathematics. It advanced a thesis of logicism, that mathematics and logic are one and the same.\n\nAt the age of 29, in February 1901, Russell underwent what he called a \"sort of mystic illumination\", after witnessing Whitehead's wife's acute suffering in an angina attack. \"I found myself filled with semi-mystical feelings about beauty ... and with a desire almost as profound as that of the Buddha to find some philosophy which should make human life endurable\", Russell would later recall. \"At the end of those five minutes, I had become a completely different person.\"\n\nIn 1905 he wrote the essay \"On Denoting\", which was published in the philosophical journal \"Mind\". Russell was elected a Fellow of the Royal Society (FRS) in 1908. The three-volume \"Principia Mathematica\", written with Whitehead, was published between 1910 and 1913. This, along with the earlier \"The Principles of Mathematics\", soon made Russell world-famous in his field.\n\nIn 1910 he became a University of Cambridge lecturer at Trinity College where he studied. He was considered for a Fellowship, which would give him a vote in the college government and protect him from being fired for his opinions, but was passed over because he was \"anti-clerical\", essentially because he was agnostic. He was approached by the Austrian engineering student Ludwig Wittgenstein, who became his PhD student. Russell viewed Wittgenstein as a genius and a successor who would continue his work on logic. He spent hours dealing with Wittgenstein's various phobias and his frequent bouts of despair. This was often a drain on Russell's energy, but Russell continued to be fascinated by him and encouraged his academic development, including the publication of Wittgenstein's \"Tractatus Logico-Philosophicus\" in 1922. Russell delivered his lectures on Logical Atomism, his version of these ideas, in 1918, before the end of World War I. Wittgenstein was, at that time, serving in the Austrian Army and subsequently spent nine months in an Italian prisoner of war camp at the end of the conflict.\n\nDuring World War I, Russell was one of the few people to engage in active pacifist activities and in 1916, because of his lack of a Fellowship, he was dismissed from Trinity College following his conviction under the Defence of the Realm Act 1914. He later described this as an illegitimate means the state used to violate freedom of expression, in Free Thought and Official Propaganda. Russell played a significant part in the \"Leeds Convention\" in June 1917, a historic event which saw well over a thousand \"anti-war socialists\" gather; many being delegates from the Independent Labour Party and the Socialist Party, united in their pacifist beliefs and advocating a peace settlement. The international press reported that Russell appeared with a number of Labour MPs, including Ramsay MacDonald and Philip Snowden, as well as former Liberal MP and anti-conscription campaigner, Professor Arnold Lupton. After the event, Russell told Lady Ottoline Morrell that, \"to my surprise, when I got up to speak, I was given the greatest ovation that was possible to give anybody\".\n\nThe Trinity incident resulted in Russell being fined £100, which he refused to pay in hope that he would be sent to prison, but his books were sold at auction to raise the money. The books were bought by friends; he later treasured his copy of the King James Bible that was stamped \"Confiscated by Cambridge Police\".\n\nA later conviction for publicly lecturing against inviting the US to enter the war on the United Kingdom's side resulted in six months' imprisonment in Brixton prison (see \"Bertrand Russell's views on society\") in 1918. He later said of his imprisonment:\n\nRussell was reinstated to Trinity in 1919, resigned in 1920, was Tarner Lecturer 1926 and became a Fellow again in 1944 until 1949.\n\nIn 1924, Bertrand again gained press attention when attending a \"banquet\" in the House of Commons with well-known campaigners, including Arnold Lupton, who had been a Member of Parliament and had also endured imprisonment for \"passive resistance to military or naval service\".\n\nIn 1941, G. H. Hardy wrote a 61-page pamphlet titled \"Bertrand Russell and Trinity\" – published later as a book by Cambridge University Press with a foreword by C. D. Broad – in which he gave an authoritative account about Russell's 1916 dismissal from Trinity College, explaining that a reconciliation between the college and Russell had later taken place and gave details about Russell's personal life. Hardy writes that Russell's dismissal had created a scandal since the vast majority of the Fellows of the College opposed the decision. The ensuing pressure from the Fellows induced the Council to reinstate Russell. In January 1920, it was announced that Russell had accepted the reinstatement offer from Trinity and would begin lecturing from October. In July 1920, Russell applied for a one year leave of absence; this was approved. He spent the year giving lectures in China and Japan. In January 1921, it was announced by Trinity that Russell had resigned and his resignation had been accepted. This resignation, Hardy explains, was completely voluntary and was not the result of another altercation.\n\nThe reason for the resignation, according to Hardy, was that Russell was going through a tumultuous time in his personal life with a divorce and subsequent remarriage. Russell contemplated asking Trinity for another one-year leave of absence but decided against it, since this would have been an \"unusual application\" and the situation had the potential to snowball into another controversy. Although Russell did the right thing, in Hardy's opinion, the reputation of the College suffered due to Russell's resignation since the ‘world of learning’ knew about Russell's altercation with Trinity but not that the rift had healed. In 1925, Russell was asked by the Council of Trinity College to give the \"Tarner Lectures\" on the Philosophy of the Sciences; these would later be the basis for one of Russell's best received books according to Hardy: \"The Analysis of Matter\", published in 1927. In the preface to the Trinity pamphlet, Hardy wrote:\n\nIn August 1920, Russell travelled to Russia as part of an official delegation sent by the British government to investigate the effects of the Russian Revolution. He wrote a four-part series of articles, titled \"Soviet Russia1920\", for the US magazine \"The Nation\". He met Vladimir Lenin and had an hour-long conversation with him. In his autobiography, he mentions that he found Lenin disappointing, sensing an \"impish cruelty\" in him and comparing him to \"an opinionated professor\". He cruised down the Volga on a steamship. His experiences destroyed his previous tentative support for the revolution. He wrote a book \"The Practice and Theory of Bolshevism\" about his experiences on this trip, taken with a group of 24 others from the UK, all of whom came home thinking well of the régime, despite Russell's attempts to change their minds. For example, he told them that he heard shots fired in the middle of the night and was sure these were clandestine executions, but the others maintained that it was only cars backfiring.\nRussell's lover Dora Black, a British author, feminist and socialist campaigner, visited Russia independently at the same time; in contrast to his reaction, she was enthusiastic about the revolution.\n\nThe following autumn Russell, accompanied by Dora, visited Peking (as it was then known in the West) to lecture on philosophy for a year. He went with optimism and hope, seeing China as then being on a new path. Other scholars present in China at the time included John Dewey and Rabindranath Tagore, the Indian Nobel-laureate poet. Before leaving China, Russell became gravely ill with pneumonia, and incorrect reports of his death were published in the Japanese press. When the couple visited Japan on their return journey, Dora took on the role of spurning the local press by handing out notices reading \"Mr. Bertrand Russell, having died according to the Japanese press, is unable to give interviews to Japanese journalists\". Apparently they found this harsh and reacted resentfully.\n\nDora was six months pregnant when the couple returned to England on 26 August 1921. Russell arranged a hasty divorce from Alys, marrying Dora six days after the divorce was finalised, on 27 September 1921. Russell's children with Dora were John Conrad Russell, 4th Earl Russell, born on 16 November 1921, and Katharine Jane Russell (now Lady Katharine Tait), born on 29 December 1923. Russell supported his family during this time by writing popular books explaining matters of physics, ethics, and education to the layman.\n\nFrom 1922 to 1927 the Russells divided their time between London and Cornwall, spending summers in Porthcurno. In the 1922 and 1923 general elections Russell stood as a Labour Party candidate in the Chelsea constituency, but only on the basis that he knew he was extremely unlikely to be elected in such a safe Conservative seat, and he was not on either occasion.\n\nTogether with Dora, Russell founded the experimental Beacon Hill School in 1927. The school was run from a succession of different locations, including its original premises at the Russells' residence, Telegraph House, near Harting, West Sussex. On 8 July 1930 Dora gave birth to her third child Harriet Ruth. After he left the school in 1932, Dora continued it until 1943.\n\nOn a tour through the US in 1927 Russell met Barry Fox (later Barry Stevens) who became a well-known Gestalt therapist and writer in later years. Russell and Fox developed an intensive relationship. In Fox's words: \"... for three years we were very close.\" Fox sent her daughter Judith to Beacon Hill School for some time. From 1927 to 1932 Russell wrote 34 letters to Fox.\n\nUpon the death of his elder brother Frank, in 1931, Russell became the 3rd Earl Russell.\n\nRussell's marriage to Dora grew increasingly tenuous, and it reached a breaking point over her having two children with an American journalist, Griffin Barry. They separated in 1932 and finally divorced. On 18 January 1936, Russell married his third wife, an Oxford undergraduate named Patricia (\"Peter\") Spence, who had been his children's governess since 1930. Russell and Peter had one son, Conrad Sebastian Robert Russell, 5th Earl Russell, who became a prominent historian and one of the leading figures in the Liberal Democrat party.\n\nRussell returned to the London School of Economics to lecture on the science of power in 1937.\n\nDuring the 1930s, Russell became a close friend and collaborator of V. K. Krishna Menon, then secretary of the India League, the foremost lobby in the United Kingdom for Indian self-rule.\n\nRussell opposed rearmament against Nazi Germany. In 1937 he wrote in a personal letter: \"If the Germans succeed in sending an invading army to England we should do best to treat them as visitors, give them quarters and invite the commander and chief to dine with the prime minister.\" In 1940, he changed his view that avoiding a full-scale world war was more important than defeating Hitler. He concluded that Adolf Hitler taking over all of Europe would be a permanent threat to democracy. In 1943, he adopted a stance toward large-scale warfare: \"War was always a great evil, but in some particularly extreme circumstances, it may be the lesser of two evils.\"\n\nBefore World War II, Russell taught at the University of Chicago, later moving on to Los Angeles to lecture at the UCLA Department of Philosophy. He was appointed professor at the City College of New York (CCNY) in 1940, but after a public outcry the appointment was annulled by a court judgment that pronounced him \"morally unfit\" to teach at the college due to his opinions, especially those relating to sexual morality, detailed in \"Marriage and Morals\" (1929). The matter was however taken to the New York Supreme Court by Jean Kay who was afraid that her daughter would be harmed by the appointment, though her daughter was not a student at CCNY. Many intellectuals, led by John Dewey, protested at his treatment. Albert Einstein's oft-quoted aphorism that \"great spirits have always encountered violent opposition from mediocre minds\" originated in his open letter, dated 19 March 1940, to Morris Raphael Cohen, a professor emeritus at CCNY, supporting Russell's appointment. Dewey and Horace M. Kallen edited a collection of articles on the CCNY affair in \"The Bertrand Russell Case\". Russell soon joined the Barnes Foundation, lecturing to a varied audience on the history of philosophy; these lectures formed the basis of \"A History of Western Philosophy\". His relationship with the eccentric Albert C. Barnes soon soured, and he returned to the UK in 1944 to rejoin the faculty of Trinity College.\n\nRussell participated in many broadcasts over the BBC, particularly \"The Brains Trust\" and the Third Programme, on various topical and philosophical subjects. By this time Russell was world-famous outside academic circles, frequently the subject or author of magazine and newspaper articles, and was called upon to offer opinions on a wide variety of subjects, even mundane ones. En route to one of his lectures in Trondheim, Russell was one of 24 survivors (among a total of 43 passengers) of an aeroplane crash in Hommelvik in October 1948. He said he owed his life to smoking since the people who drowned were in the non-smoking part of the plane. \"A History of Western Philosophy\" (1945) became a best-seller and provided Russell with a steady income for the remainder of his life.\n\nIn 1942 Russell argued in favour of a moderate socialism, capable of overcoming its metaphysical principles, in an inquiry on dialectical materialism, launched by the Austrian artist and philosopher Wolfgang Paalen in his journal \"DYN\", saying \"I think the metaphysics of both Hegel and Marx plain nonsense – Marx's claim to be 'science' is no more justified than Mary Baker Eddy's. This does not mean that I am opposed to socialism.\"\n\nIn 1943, Russell expressed support for Zionism: \"I have come gradually to see that, in a dangerous and largely hostile world, it is essential to Jews to have some country which is theirs, some region where they are not suspected aliens, some state which embodies what is distinctive in their culture\".\n\nIn a speech in 1948, Russell said that if the USSR's aggression continued, it would be morally worse to go to war after the USSR possessed an atomic bomb than before it possessed one, because if the USSR had no bomb the West's victory would come more swiftly and with fewer casualties than if there were atom bombs on both sides. At that time, only the United States possessed an atomic bomb, and the USSR was pursuing an extremely aggressive policy towards the countries in Eastern Europe which were being absorbed into the Soviet Union's sphere of influence. Many understood Russell's comments to mean that Russell approved of a first strike in a war with the USSR, including Nigel Lawson, who was present when Russell spoke of such matters. Others, including Griffin, who obtained a transcript of the speech, have argued that he was merely explaining the usefulness of America's atomic arsenal in deterring the USSR from continuing its domination of Eastern Europe.\nHowever, just after the atomic bombs exploded over Hiroshima and Nagasaki, Russell wrote letters, and published articles in newspapers from 1945 to 1948, stating clearly that it was morally justified and better to go to war against the USSR using atomic bombs while the United States possessed them and before the USSR did. In September 1949, one week after the USSR tested its first A-bomb, but before this became known, Russell wrote that USSR would be unable to develop nuclear weapons because following Stalin's purges only science based on Marxist principles would be practiced in the Soviet Union. After it became known that the USSR carried out its nuclear bomb tests, Russell declared his position advocating for the total abolition of atomic weapons.\n\nIn 1948, Russell was invited by the BBC to deliver the inaugural Reith Lectures—what was to become an annual series of lectures, still broadcast by the BBC. His series of six broadcasts, titled \"Authority and the Individual\", explored themes such as the role of individual initiative in the development of a community and the role of state control in a progressive society. Russell continued to write about philosophy. He wrote a foreword to \"Words and Things\" by Ernest Gellner, which was highly critical of the later thought of Ludwig Wittgenstein and of ordinary language philosophy. Gilbert Ryle refused to have the book reviewed in the philosophical journal \"Mind\", which caused Russell to respond via \"The Times\". The result was a month-long correspondence in \"The Times\" between the supporters and detractors of ordinary language philosophy, which was only ended when the paper published an editorial critical of both sides but agreeing with the opponents of ordinary language philosophy. \n\nIn the King's Birthday Honours of 9 June 1949, Russell was awarded the Order of Merit, and the following year he was awarded the Nobel Prize in Literature. When he was given the Order of Merit, George VI was affable but slightly embarrassed at decorating a former jailbird, saying, \"You have sometimes behaved in a manner that would not do if generally adopted\". Russell merely smiled, but afterwards claimed that the reply \"That's right, just like your brother\" immediately came to mind. In 1952 Russell was divorced by Spence, with whom he had been very unhappy. Conrad, Russell's son by Spence, did not see his father between the time of the divorce and 1968 (at which time his decision to meet his father caused a permanent breach with his mother).\n\nIn 1950, Russell attended the inaugural conference for the Congress for Cultural Freedom, a C.I.A.-funded anti-communist organization committed to the deployment of culture as a weapon during the Cold War. Russell was one of the best known patrons of the Congress, until he resigned in 1956\n\nRussell married his fourth wife, Edith Finch, soon after the divorce, on 15 December 1952. They had known each other since 1925, and Edith had taught English at Bryn Mawr College near Philadelphia, sharing a house for 20 years with Russell's old friend Lucy Donnelly. Edith remained with him until his death, and, by all accounts, their marriage was a happy, close, and loving one. Russell's eldest son John suffered from serious mental illness, which was the source of ongoing disputes between Russell and his former wife Dora.\n\nIn September 1961, at the age of 89, Russell was jailed for seven days in Brixton Prison for \"breach of peace\" after taking part in an anti-nuclear demonstration in London. The magistrate offered to exempt him from jail if he pledged himself to \"good behaviour\", to which Russell replied: \"No, I won't.\"\n\nIn 1962 Russell played a public role in the Cuban Missile Crisis: in an exchange of telegrams with Soviet leader Nikita Khrushchev, Khrushchev assured him that the Soviet government would not be reckless. Russell sent this telegram to President Kennedy:\n\nYOUR ACTION DESPERATE. THREAT TO HUMAN SURVIVAL. NO CONCEIVABLE JUSTIFICATION. CIVILIZED MAN CONDEMNS IT. WE WILL NOT HAVE MASS MURDER. ULTIMATUM MEANS WAR... END THIS MADNESS.\n\nAccording to historian Peter Knight, after JFK's assassination, Russell, \"prompted by the emerging work of the lawyer Mark Lane in the US ... rallied support from other noteworthy and left-leaning compatriots to form a Who Killed Kennedy Committee in June 1964, members of which included Michael Foot MP, Caroline Benn, the publisher Victor Gollancz, the writers John Arden and J. B. Priestley, and the Oxford history professor Hugh Trevor-Roper.\" Russell published a highly critical article weeks before the Warren Commission Report was published, setting forth \"16 Questions on the Assassination\" and equating the Oswald case with the Dreyfus affair of late 19th-century France, in which the state wrongly convicted an innocent man. Russell also criticised the American press for failing to heed any voices critical of the official version.\n\nBertrand Russell was opposed to war from early on, his opposition to World War I being used as grounds for his dismissal from Trinity College at Cambridge. This incident fused two of his most controversial causes, as he had failed to be granted Fellow status, which would have protected him from firing, because he was not willing to either pretend to be a devout Christian, or at least avoid admitting he was agnostic.\n\nHe later described the resolution of these issues as essential to freedom of thought and expression, citing the incident in Free Thought and Official Propaganda, where he explained that the expression of any idea, even the most obviously \"bad\", must be protected not only from direct State intervention, but also economic leveraging and other means of being silenced:\n\nRussell spent the 1950s and 1960s engaged in political causes primarily related to nuclear disarmament and opposing the Vietnam War. The 1955 Russell–Einstein Manifesto was a document calling for nuclear disarmament and was signed by eleven of the most prominent nuclear physicists and intellectuals of the time. In 1966–1967, Russell worked with Jean-Paul Sartre and many other intellectual figures to form the Russell Vietnam War Crimes Tribunal to investigate the conduct of the United States in Vietnam. He wrote a great many letters to world leaders during this period.\n\nIn 1956, immediately before and during the Suez Crisis, Russell expressed his opposition to European imperialism in the Middle East. He viewed the crisis as another reminder of the pressing need for a more effective mechanism for international governance, and to restrict national sovereignty to places such as the Suez Canal area \"where general interest is involved\". At the same time the Suez Crisis was taking place, the world was also captivated by the Hungarian Revolution and the subsequent crushing of the revolt by intervening Soviet forces. Russell attracted criticism for speaking out fervently against the Suez war while ignoring Soviet repression in Hungary, to which he responded that he did not criticise the Soviets \"because there was no need. Most of the so-called Western World was fulminating\". Although he later feigned a lack of concern, at the time he was disgusted by the brutal Soviet response, and on 16 November 1956, he expressed approval for a declaration of support for Hungarian scholars which Michael Polanyi had cabled to the Soviet embassy in London twelve days previously, shortly after Soviet troops had already entered Budapest.\n\nIn November 1957 Russell wrote an article addressing US President Dwight D. Eisenhower and Soviet Premier Nikita Khrushchev, urging a summit to consider \"the conditions of co-existence\". Khrushchev responded that peace could indeed be served by such a meeting. In January 1958 Russell elaborated his views in \"The Observer\", proposing a cessation of all nuclear-weapons production, with the UK taking the first step by unilaterally suspending its own nuclear-weapons program if necessary, and with Germany \"freed from all alien armed forces and pledged to neutrality in any conflict between East and West\". US Secretary of State John Foster Dulles replied for Eisenhower. The exchange of letters was published as \"The Vital Letters of Russell, Khrushchev, and Dulles\".\n\nRussell was asked by \"The New Republic\", a liberal American magazine, to elaborate his views on world peace. He urged that all nuclear-weapons testing and constant flights by planes armed with nuclear weapons be halted immediately, and negotiations be opened for the destruction of all hydrogen bombs, with the number of conventional nuclear devices limited to ensure a balance of power. He proposed that Germany be reunified and accept the Oder-Neisse line as its border, and that a neutral zone be established in Central Europe, consisting at the minimum of Germany, Poland, Hungary, and Czechoslovakia, with each of these countries being free of foreign troops and influence, and prohibited from forming alliances with countries outside the zone. In the Middle East, Russell suggested that the West avoid opposing Arab nationalism, and proposed the creation of a United Nations peacekeeping force to guard Israel's frontiers to ensure that Israel was prevented from committing aggression and protected from it. He also suggested Western recognition of the People's Republic of China, and that it be admitted to the UN with a permanent seat on the UN Security Council.\n\nHe was in contact with Lionel Rogosin while the latter was filming his anti-war film \"Good Times, Wonderful Times\" in the 1960s. He became a hero to many of the youthful members of the New Left. In early 1963, in particular, Russell became increasingly vocal in his disapproval of the Vietnam War, and felt that the US government's policies there were near-genocidal. In 1963 he became the inaugural recipient of the Jerusalem Prize, an award for writers concerned with the freedom of the individual in society. In 1964 he was one of eleven world figures who issued an appeal to Israel and the Arab countries to accept an arms embargo and international supervision of nuclear plants and rocket weaponry. In October 1965 he tore up his Labour Party card because he suspected Harold Wilson's Labour government was going to send troops to support the United States in Vietnam.\n\nIn June 1955 Russell had leased Plas Penrhyn in Penrhyndeudraeth, Merionethshire, Wales and on 5 July of the following year it became his and Edith's principal residence.\nRussell published his three-volume autobiography in 1967, 1968, and 1969. Russell made a cameo appearance playing himself in the anti-war Hindi film \"Aman\", by Mohan Kumar, which was released in India in 1967. This was Russell's only appearance in a feature film.\n\nOn 23 November 1969 he wrote to \"The Times\" newspaper saying that the preparation for show trials in Czechoslovakia was \"highly alarming\". The same month, he appealed to Secretary General U Thant of the United Nations to support an international war crimes commission to investigate alleged torture and genocide by the United States in South Vietnam during the Vietnam War. The following month, he protested to Alexei Kosygin over the expulsion of Aleksandr Solzhenitsyn from the Soviet Union of Writers.\n\nOn 31 January 1970 Russell issued a statement condemning \"Israel's aggression in the Middle East\", and in particular, Israeli bombing raids being carried out deep in Egyptian territory as part of the War of Attrition. He called for an Israeli withdrawal to the pre-Six-Day War borders. This was Russell's final political statement or act. It was read out at the International Conference of Parliamentarians in Cairo on 3 February 1970, the day after his death.\n\nRussell died of influenza on 2 February 1970 at his home in Penrhyndeudraeth. His body was cremated in Colwyn Bay on 5 February 1970. In accordance with his will, there was no religious ceremony; his ashes were scattered over the Welsh mountains later that year. He left an estate valued at £69,423 (£1.09 million or US$1.4 million in 2018 money).\nIn 1980 a memorial to Russell was commissioned by a committee including the philosopher A. J. Ayer. It consists of a bust of Russell in Red Lion Square in London sculpted by Marcelle Quinton.\n\nRussell held throughout his life the following styles and honours:\n\nRussell is generally credited with being one of the founders of analytic philosophy. He was deeply impressed by Gottfried Leibniz (1646–1716), and wrote on every major area of philosophy except aesthetics. He was particularly prolific in the field of metaphysics, the logic and the philosophy of mathematics, the philosophy of language, ethics and epistemology. When Brand Blanshard asked Russell why he did not write on aesthetics, Russell replied that he did not know anything about it, \"but that is not a very good excuse, for my friends tell me it has not deterred me from writing on other subjects\".\n\nOn ethics, Russell considered himself a utilitarian.\n\nFor the advancement of science \"and\" protection of the right to freedom of expression, Russell advocated The Will to Doubt, the recognition that all human knowledge is at most a best guess, that one should always remember:\n\nRussell described himself in 1947 as an agnostic, saying: \"Therefore, in regard to the Olympic gods, speaking to a purely philosophical audience, I would say that I am an Agnostic. But speaking popularly, I think that all of us would say in regard to those gods that we were Atheists. In regard to the Christian God, I should, I think, take exactly the same line.\" For most of his adult life, Russell maintained religion to be little more than superstition and, in spite of any positive effects, largely harmful to people. He believed that religion and the religious outlook serve to impede knowledge and foster fear and dependency, and to be responsible for much of our world's wars, oppression, and misery. He was a member of the Advisory Council of the British Humanist Association and President of Cardiff Humanists until his death.\n\nPolitical and social activism occupied much of Russell's time for most of his life. Russell remained politically active almost to the end of his life, writing to and exhorting world leaders and lending his name to various causes.\n\nRussell argued for a \"scientific society\", where war would be abolished, population growth would be limited, and prosperity would be shared. He suggested the establishment of a \"single supreme world government\" able to enforce peace, claiming that \"the only thing that will redeem mankind is co-operation\".\n\nRussell was an active supporter of the Homosexual Law Reform Society, being one of the signatories of A. E. Dyson's 1958 letter to \"The Times\" calling for a change in the law regarding male homosexual practices, which were partly legalised in 1967, when Russell was still alive.\n\nIn \"Reflections on My Eightieth Birthday\" (\"Postscript\" in his \"Autobiography\"), Russell wrote: \"I have lived in the pursuit of a vision, both personal and social. Personal: to care for what is noble, for what is beautiful, for what is gentle; to allow moments of insight to give wisdom at more mundane times. Social: to see in imagination the society that is to be created, where individuals grow freely, and where hate and greed and envy die because there is nothing to nourish them. These things I believe, and the world, for all its horrors, has left me unshaken\".\n\nBelow is a selected bibliography of Russell's books in English, sorted by year of first publication:\n\nRussell was the author of more than sixty books and over two thousand articles. Additionally, he wrote many pamphlets, introductions, and letters to the editor. One pamphlet titled, \"'I Appeal unto Caesar': The Case of the Conscientious Objectors\", ghostwritten for Margaret Hobhouse, the mother of imprisoned peace activist Stephen Hobhouse, allegedly helped secure the release from prison of hundreds of conscientious objectors.\n\nHis works can be found in anthologies and collections, including \"The Collected Papers of Bertrand Russell\", which McMaster University began publishing in 1983. By March 2017 this collection of his shorter and previously unpublished works included 18 volumes, and several more are in progress. A bibliography in three additional volumes catalogues his publications. The Russell Archives held by McMaster's William Ready Division of Archives and Research Collections possess over 40,000 of his letters.\n\n\n\n\n\n\n"}
{"id": "37151950", "url": "https://en.wikipedia.org/wiki?curid=37151950", "title": "Bibliography of encyclopedias: art and artists", "text": "Bibliography of encyclopedias: art and artists\n\nThis is a list of encyclopedias and encyclopedic/biographical dictionaries published on the subject of art and artists in any language. Entries are in the English language unless stated as otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "17970498", "url": "https://en.wikipedia.org/wiki?curid=17970498", "title": "Biofact (archaeology)", "text": "Biofact (archaeology)\n\nIn archaeology, a biofact (or ecofact) is organic material found at an archaeological site that carries archaeological significance. Biofacts are natural objects found alongside artifacts or features, such as animal bones, charcoal, plants, and pollen. Biofacts are passively consumed or handled by humans; as opposed to artefacts, which are purposefully manipulated. Biofacts reveal how people respond to their surroundings.\n\nA common type of biofact is a [plant] [seed]. Plant remains, often referred to as macrobotanicals, provide a variety of information ranging from diet to medicine to textile production. Pollen preserved on archaeological sites informs researchers about the ancient environment, and the foods processed and/or grown by prehistoric people. Pollen, when examined over time, also informs on environmental and dietary changes. A seed can be linked to the species of plant that produced it; if massive numbers of seeds of a cultivated species are found at a site, it may be inferred that the species may have been grown for food or other products that are useful to humans, such as clothing, bedding or building materials.\n\nAnother type of biofact is wood. Wood is made up cellulose, carbohydrates, and lignin. Every year that passes, a new ring is added to the trunk of tree, allowing for dendrochronological dating. Charcoal is burned wood that archaeologist are able to extract. It can be dated using carbon-14, and through other methods, information such as local environment and human adaptation can be revealed from the charcoal. To help determine the date during which a site was occupied, dendrochronological analysis can be used on wood samples. Wood that has been altered by humans is properly an artifact, not a biofact.\n\n"}
{"id": "1227744", "url": "https://en.wikipedia.org/wiki?curid=1227744", "title": "Bourgeois tragedy", "text": "Bourgeois tragedy\n\nBourgeois tragedy (German: \"Bürgerliches Trauerspiel\") is a form of tragedy that developed in 18th-century Europe. It is a fruit of the enlightenment and the emergence of the bourgeois class and its ideals. It is characterized by the fact that its protagonists are ordinary citizens.\n\nThere are a few examples of tragic plays with middle-class protagonists from 17th century England (see domestic tragedy), but only in the 18th century did the general attitude change. The first true bourgeois tragedy was an English play: George Lillo's \"The London Merchant; or, the History of George Barnwell\", which was first performed in 1731. In France, the first \"tragédie bourgeoise\" was \"Sylvie\" by Paul Landois, which came out in 1741. Years later came two plays by Denis Diderot: \"Le fils naturel\" was first staged in 1757 and \"Le père de famille\" in the following year; while these plays were not strictly tragedies, they treat bourgeois lives in a serious manner atypical of contemporary comedy and provided models for more genuinely tragic works.\n\nWhile ordinary people had always been the subject of comedies, classical and neo-classical theorists asserted that tragic heroes should always be men of noble rank. Aristotle articulates this idea in \"ars poetica\" (\"The Poetics\") and it figures prominently in later ancient writings on drama and poetics. Sixteenth- and seventeenth-century critics, including the influential German Martin Opitz, perpetuated the theory that only members of the higher classes were capable of suffering harm serious enough to deserve dramatic reenactment. This rule was followed throughout Europe for centuries: usually, princes and members of the nobility, such as Andreas Gryphius' Carolus Stuardus (i.e. King Charles I of England), Jean Racine's Phèdre (the wife of Theseus, a mythical king of Athens) or William Shakespeare's King Lear, serve as tragic protagonists.\n\nIn Germany, where the new genre was called \"Bürgerliches Trauerspiel\", it was especially successful. Usually, Gotthold Ephraim Lessing's play \"Miss Sara Sampson\", which was first produced in 1755, is said to be the earliest \"Bürgerliches Trauerspiel\" in Germany. However, Christian Leberecht Martini's drama \"Rhynsolt und Sapphira\" is slightly older. Lessing's \"Emilia Galotti\" of 1771 is a classic example of the German Bürgerliches Trauerspiel. Lessing also offered a thorough theoretic justification for his disregard of the old rules in his \"Hamburgische Dramaturgie\". Other important examples of German Bürgerliche Trauerspiele are \"Die Soldaten\" by Jakob Michael Reinhold Lenz (1776) and Friedrich Schiller's \"Kabale und Liebe\" (1784). \n\nBourgeois tragedies tend to propagate the values of the bourgeois class to which their heroes belong. Their ideal is the virtuous citizen, who is excluded from state affairs and whose intentions are focused on his private life and the life of his family. Values like virtue, humanity, individuality and true feelings are cherished in bourgeois tragedies.\n"}
{"id": "22101040", "url": "https://en.wikipedia.org/wiki?curid=22101040", "title": "Caucasology", "text": "Caucasology\n\nCaucasology, or Caucasiology refers to the historical and geopolitical studies of Caucasus region. The branch has more than 150 years history. In 1972, the Caucasiological Center (renamed to International Caucasiological Center in 2000) was founded under the auspices of the Israel President Zalman Shazar.\n\n\n"}
{"id": "384857", "url": "https://en.wikipedia.org/wiki?curid=384857", "title": "Cross-genre", "text": "Cross-genre\n\nA cross-genre (or hybrid genre) is a genre in fiction that blends themes and elements from two or more different genres. As opposed to the (literary and political) conservatism of most genre fiction, cross-genre writing offers opportunities for opening up debates and stimulating discussion.\n\nSuch hybrid genres are not new but a longstanding element in the fictional process: perhaps the most famous example is William Blake's \"Marriage of Heaven and Hell\", with its blend of poetry, prose, and engravings. In contemporary literature Dimitris Lyacos's trilogy Poena Damni (, With the people from the bridge, The First Death) combines fictional prose with drama and poetry in a multilayered narrative developing through the different characters of the work.\n\nFredric Jameson has highlighted the progressive elements in Third World Literature that defies genre expectations such as Xala; and in science fiction like \"The Left Hand of Darkness\" with its exploration of gender roles.\n\nDean Koontz considers himself a cross-genre writer, not a horror writer: “I write cross-genre books-suspense mixed with love story, with humor, sometimes with two tablespoons of science fiction, sometimes with a pinch of horror, sometimes with a sprinkle of paprika...”\n\nDiane P. Freedman, \"An Alchemy of Genres\" (1997)\n"}
{"id": "48403782", "url": "https://en.wikipedia.org/wiki?curid=48403782", "title": "Cultureme", "text": "Cultureme\n\nA cultureme is any portion of cultural behavior apprehended in signs of symbolic value that can be broken down into smaller units or amalgamated into larger ones. A cultureme is a \"cultural information-bearing unit\", the contents of which are recognizable by a group of people. Culturemes are the bridge between linguistic units and culture.\n\nTheir usage can be seen in cultural expressions, phraseologisms, jokes, slogans, literature, religion, folklore, sociology, anthropology, etc. All of which are subcultures in a culture system. Culturemes of this nature have historical relevance that when translated or explained result in a miscommunication and misunderstanding. \n\nThe notion of cultureme is being increasingly used in translation studies and other disciplines. It is a recently used concept that is yet to be defined and distinguished from others, such as phraseme, idiom, symbol, cultural word, etc.\n\nFernando Poyatos breaks down the features of a cultureme into four phases. These phases analysis the broadest of culturemes to the most particular aspects of culturemes.\n\nBasic culturemes are the broadest of culturemes. They characterize the initial semblance of a culture. Basic culturemes are separated into two cultural \"realms,\" urban and rural, and two domains: exterior and interior. Basic culturemes begin with urban and rural realms because of their dichotomy in cultural identity and displays of social interactions. Within either cultures, there are exteriors (what is observed from the outside) and interiors (what is observed from the inside of buildings or establishments that are not seen from an outdoors perspective). The main divisions of culturemes are: urban-exterior, urban-interior, rural-exterior, and rural-interior.\n\nThe significance of basic culturemes is to give a general sense of surroundings. For instance,\n\n\"in \"North America\" (United States and Canada): the larger size of many automobiles, the summery sound and smell of lawn mowing, the smell of city bus exhausts, the basic separation of the 'downtown' business and residential areas in the small town or village, the sirens of their police cars, the smell of hamburger and French fries of many eating establishments, the unique drugstore smell, the 'Licensed' signs in many second-rate restaurants (indicating a cultural attitude towards drinking\"\n\nPrimary culturemes exist in the phase when acculturation occurs, in which one's culture is becoming more complex. The basic four culturemes are subcategorized into environmental (cultural settings) and behavioral (behaviors of people and their interactions). These culturemes are a result of recognizing cultural patterns and \"experiencing it through mere observation or systemic learning.\" \n\nCulturemes are further broken down into settings, in which a specific culture is exemplified. These includes cultures in the school, the park, the bar, etc. These are secondary culturemes. Phase three is the point where the cultures identified in phase two begin to interact; interrelationships are seen among different cultural systems and values.\n\nPhase four delves deeper into human senses, analyzing to the fullest extent possible. The completion of phase four allows one to identify the cultureme's cultural system and subculture. Knowing the cultural system and subculture are important as it helps to navigate through other identifiable systems within the culture.\n\nTranslating a cultureme can be challenging, as connotations are sometimes very strong. For instance: the Spanish word \"alcázar\" means \"the castle\", \"palace\" or \"fortress\", but, as it is of Arabic origin, it recalls eight centuries of history (Al Andalus), which cannot be easily translated into English, so the translator must adopt a crucial decision: either choose the English word \"fortress\" and lose all the historical and cultural connotations, or use the loan word alcazar.\n\nA language may have various cultures and various languages may share the same culture. Cultures may differ in conceptualization that may in turn affect how thoughts are conveyed in their languages. The asymmetry of language complicates the matter of providing a linguistically and culturally sound equivalent of a cultureme in another language. The complex relationship of language and culture is significant in giving culturemes their intended value. Dictionaries further complicate cross-cultural gaps of meaning because without the collaborative effort of all cultures, it is impossible to define the significance of words that may be culturemes in the dictionary. Though, slowly, interactive and multimedia dictionaries have become extensive in information that lessen the bridge across languages and cultures.\n\nCulturemes are translated in several ways:\n\nIn translating, the inability to convey cultural meaning strips a cultureme of its \"cultureme\" title and it simply becomes an ambiguous word. Culture is classified into three categories: Norms, Ideas, and Materials.\n\n\n"}
{"id": "40150388", "url": "https://en.wikipedia.org/wiki?curid=40150388", "title": "Deep social mind", "text": "Deep social mind\n\nDeep social mind is a concept in evolutionary psychology; it refers to the distinctively human capacity to 'read' (that is, to infer) the mental states of others while reciprocally enabling those others to read one's own mental states at the same time. The term 'deep social mind' was first coined in 1999 by Andrew Whiten, professor of Evolutionary and Developmental Psychology at St. Andrews University, Scotland. Together with closely related terms such as 'reflexivity' and 'intersubjectivity', it is now well-established among scholars investigating the evolutionary emergence of human sociality, cognition and communication.\n\nIt is widely agreed that the brain is social in both human and nonhuman primates. But, according to Andrew Whiten, human sociality goes much further than ape sociality. Ape social intelligence is overwhelmingly 'Machiavellian' in the sense of scheming, self-interested and egotistically calculating.\n\nOne consequence is that while an ape may be motivated to 'read' (that is, to infer) the mental states of others around it, it has little motive to reciprocate. Instead of making its own mental states transparent to potential rivals, it seeks to block others from 'reading' its own mind. For example, one way to infer what another primate might be thinking is to detect which way its head is pointed, so as to reconstruct what it might be looking at. In the case of gorillas and chimpanzees, adult apes have evolved eyes which give away very little information concerning direction of gaze. Their eyes are dark-on-dark: the iris is dark brown or even black and the same applies to the sclera and surrounding skin. Looking at the eyes, therefore, it is not easy to detect direction of gaze. In the human case, the eyes are very different, the dark iris standing out against a white surrounding sclera. This feature, combined with the relatively large size of the human eye and its horizontally elongated shape, assists neighbouring conspecifics to detect direction of gaze and, on that basis, engage in mind-reading.\n\nAccording to the 'Deep Social Mind' theory, this means that humans have become cognitively adapted to reflexivity and intersubjectivity: as a species, we are well-adapted to read the minds of trusted others while at the same time assisting those others in reading our own minds. One consequence of this is self-awareness or 'egocentric perspective reversal': I read your mind as you are reading mine. Therefore, between us, we can gain an awareness of our own minds as if from the outside: my mental states as these are reflected in yours and yours as they are reflected in mine. In that sense, if this argument is accepted, our minds mutually interpenetrate. 'Mind' in the human sense is not locked inside this or that skull but instead is relational, stretching between us. According to evolutionary psychologist Michael Tomasello, a human child normally achieves egocentric perspective reversal — viewing its own mental states as if from the standpoint of others — at around one year of age.\n"}
{"id": "43727113", "url": "https://en.wikipedia.org/wiki?curid=43727113", "title": "Dematerialization (art)", "text": "Dematerialization (art)\n\nDematerialization of the art object is an idea in conceptual art. In \"Six Years: The Dematerialization of the Art Object\" Lucy Lippard characterizes the period of 1966 to 1972 as one in which the art object was dematerialised through the new artistic practices of conceptual art.\n"}
{"id": "21046621", "url": "https://en.wikipedia.org/wiki?curid=21046621", "title": "Display case", "text": "Display case\n\nA display case (showcase, display cabinet, or vitrine) is a cabinet with one or often more transparent glass (or plastic, normally acrylic for strength) surfaces, used to display objects for viewing. A display case may appear in an exhibition, museum, retail store, restaurant, or house. Often, labels are included with the displayed objects, providing information such as description or prices. In a museum, the displayed cultural artifacts are normally part of the museum's collection, or are part of a temporary exhibition. In retail or a restaurant, the items are normally being offered for sale. A trophy case is used to display sports trophies or other awards.\n\nA display case may be freestanding on the floor, or built-in (usually a custom installation). Built-in displays may be mounted on the wall, may act as room partitions, or may be hung from the ceiling. On occasion, display cases are built into the floor, such as at the Museum of Sydney (in Sydney, New South Wales, Australia), where the remains of drains and privies are shown in their original context, along with other archeological artifacts.\n\nThere are three types of freestanding showcases: counter, middle floor (mid-floor), and wall. Counter showcases are designed to display objects through one side (the \"customer side\") and have them accessible through the other (the \"clerk side\"). For this reason, the counter displays are most relevant for retail stores. The middle floor cases are built to display objects from all sides, and are meant to be placed in the middle of the room. Wall showcases are meant to be placed against a wall, where the products are displayed and accessed from the same side. These last two types are used heavily – not only by stores – but also by museums, schools, and especially in homes to showcase valuable items or collections. \n\nDisplay cases are typically made by specialist companies with a background in woodworking or welding, and come in standard sizes or often are custom order. Display cases are often designed with security in mind and are normally lockable. They also are made in variety of styles, shapes, and materials as available at a store fixture supplier. They can ship pre-assembled or knockdown (in pieces to be assembled by the customer). Pre-assembled showcases are assembled (and usually tested) by the manufacturer, and are shipped ready-to-use. Knockdown showcases are usually lower in price and cheaper to ship, but may be of poorer quality than pre-assembled, and may arrive missing pieces.\n\n"}
{"id": "2943406", "url": "https://en.wikipedia.org/wiki?curid=2943406", "title": "Ecocriticism", "text": "Ecocriticism\n\nEcocriticism is the study of literature and the environment from an interdisciplinary point of view, where literature scholars analyze texts that illustrate environmental concerns and examine the various ways literature treats the subject of nature. Some ecocritics brainstorm possible solutions for the correction of the contemporary environmental situation, though not all ecocritics agree on the purpose, methodology, or scope of ecocriticism. \nIn the United States, ecocriticism is often associated with the Association for the Study of Literature and Environment (ASLE), which hosts biennial meetings for scholars who deal with environmental matters in literature. ASLE publishes a journal—\"Interdisciplinary Studies in Literature and Environment\" (\"ISLE\")—in which current international scholarship can be found.\n\nEcocriticism is an intentionally broad approach that is known by a number of other designations, including \"green (cultural) studies\", \"ecopoetics\", and \"environmental literary criticism\" and is often informed by other fields such as ecology, sustainable design, biopolitics, environmental history, environmentalism, and social ecology, among others.\n\nIn comparison with other 'political' forms of criticism, there has been relatively little dispute about the moral and philosophical aims of ecocriticism, although its scope has broadened rapidly from nature writing, romantic poetry, and canonical literature to take in film, television, theatre, animal stories, architectures, scientific narratives and an extraordinary range of literary texts. At the same time, ecocriticism has borrowed methodologies and theoretically informed approaches liberally from other fields of literary, social and scientific study.\n\nCheryll Glotfelty's working definition in \"The Ecocriticism Reader\" is that \"ecocriticism is the study of the relationship between literature and the physical environment\", and one of the implicit goals of the approach is to recoup professional dignity for what Glotfelty calls the \"undervalued genre of nature writing\". Lawrence Buell defines \"'ecocriticism' ... as [a] study of the relationship between literature and the environment conducted in a spirit of commitment to environmentalist praxis\".\n\nSimon Estok noted in 2001 that \"ecocriticism has distinguished itself, debates notwithstanding, firstly by the ethical stand it takes, its commitment to the natural world as an important thing rather than simply as an object of thematic study, and, secondly, by its commitment to making connections\".\n\nMore recently, in an article that extends ecocriticism to Shakespearean studies, Estok argues that ecocriticism is more than \"simply the study of Nature or natural things in literature; rather, it is any theory that is committed to effecting change by analyzing the function–thematic, artistic, social, historical, ideological, theoretical, or otherwise–of the natural environment, or aspects of it, represented in documents (literary or other) that contribute to material practices in material worlds\". This echoes the functional approach of the cultural ecology branch of ecocriticism, which analyzes the analogies between ecosystems and imaginative texts and posits that such texts potentially have an ecological (regenerative, revitalizing) function in the cultural system.\n\nAs Michael P. Cohen has observed, \"if you want to be an ecocritic, be prepared to explain what you do and be criticized, if not satirized.\" Certainly, Cohen adds his voice to such critique, noting that one of the problems of ecocriticism has been what he calls its \"praise-song school\" of criticism. All ecocritics share an environmentalist motivation of some sort, but whereas the majority are 'nature endorsing', some are 'nature sceptical'. In part this entails a shared sense of the ways in which 'nature' has been used to legitimise gender, sexual and racial norms (so homosexuality has been seen as 'unnatural', for example), but it also involves scepticism about the uses to which 'ecological' language is put in ecocriticism; it can also involve a critique of the ways cultural norms of nature and the environment contribute to environmental degradation. Greg Garrard has dubbed 'pastoral ecology' the notion that nature undisturbed is balanced and harmonious, while Dana Phillips has criticised the literary quality and scientific accuracy of nature writing in \"The Truth of Ecology\". Similarly, there has been a call to recognize the place of the Environmental Justice movement in redefining ecocritical discourse.\n\nIn response to the question of what ecocriticism is or should be, Camilo Gomides has offered an operational definition that is both broad and discriminating: \"The field of enquiry that analyzes and promotes works of art which raise moral questions about human interactions with nature, while also motivating audiences to live within a limit that will be binding over generations\" (16). He tests it for a film (mal)adaptation about Amazonian deforestation. Implementing the Gomides definition, Joseph Henry Vogel makes the case that ecocriticism constitutes an \"economic school of thought\" as it engages audiences to debate issues of resource allocation that have no technical solution. Ashton Nichols has recently argued that the historical dangers of a romantic version of nature now need to be replaced by \"urbanatural roosting\", a view that sees urban life and the natural world as closely linked and argues for humans to live more lightly on the planet, the way virtually all other species do.\n\nEcocritics investigate such things as the underlying ecological values, what, precisely, is meant by the word nature, and whether the examination of \"place\" should be a distinctive category, much like class, gender or race. Ecocritics examine human perception of wilderness, and how it has changed throughout history and whether or not current environmental issues are accurately represented or even mentioned in popular culture and modern literature. Scholars in ecocriticism engage in questions regarding anthropocentrism, and the \"mainstream assumption that the natural world be seen primarily as a resource for human beings\" as well as critical approaches to changing ideas in \"the material and cultural bases of modern society\". Other disciplines, such as history, economics, philosophy, ethics, and psychology, are also considered by ecocritics to be possible contributors to ecocriticism.\n\nWhile William Rueckert may have been the first person to use the term \"ecocriticism\" (Barry 240) in his 1978 essay entitled \"Literature and Ecology: An Experiment in Ecocriticism,\" ecocriticism as a movement owes much to Rachel Carson's 1962 environmental exposé \"Silent Spring\". Drawing from this critical moment, Rueckert's intent was to focus on \"the application of ecology and ecological concepts to the study of literature\".\n\nEcologically minded individuals and scholars have been publishing progressive works of ecotheory and criticism since the explosion of environmentalism in the late 1960s and 1970s. However, because there was no organized movement to study the ecological/environmental side of literature, these important works were scattered and categorized under a litany of different subject headings: pastoralism, human ecology, regionalism, American Studies etc. British marxist critic Raymond Williams, for example, wrote a seminal critique of pastoral literature in 1973, \"The Country and the City\".\n\nAnother early ecocritical text, Joseph Meeker's \"The Comedy of Survival\" (1974), proposed a version of an argument that was later to dominate ecocriticism and environmental philosophy; that environmental crisis is caused primarily by a cultural tradition in the West of separation of culture from nature, and elevation of the former to moral predominance. Such anthropocentrism is identified in the tragic conception of a hero whose moral struggles are more important than mere biological survival, whereas the science of animal ethology, Meeker asserts, shows that a \"comic mode\" of muddling through and \"making love not war\" has superior ecological value. In the later, \"second wave\" ecocriticism, Meeker's adoption of an ecophilosophical position with apparent scientific sanction as a measure of literary value tended to prevail over Williams's ideological and historical critique of the shifts in a literary genre's representation of nature.\n\nAs Glotfelty noted in \"The Ecocriticism Reader\", \"One indication of the disunity of the early efforts is that these critics rarely cited one another's work; they didn't know that it existed...Each was a single voice howling in the wilderness.\" Nevertheless, ecocriticism—unlike feminist and Marxist criticisms—failed to crystallize into a coherent movement in the late 1970s, and indeed only did so in the USA in the 1990s.\n\nIn the mid-1980s, scholars began to work collectively to establish ecocritism as a genre, primarily through the work of the Western Literature Association in which the revaluation of nature writing as a non-fictional literary genre could function. In 1990, at the University of Nevada, Reno, Glotfelty became the first person to hold an academic position as a professor of Literature and the Environment, and UNR has retained the position it established at that time as the intellectual home of ecocriticism even as ASLE has burgeoned into an organization with thousands of members in the US alone. From the late 1990s, new branches of ASLE and affiliated organizations were started in the UK, Japan, Korea, Australia and New Zealand (ASLEC-ANZ), India (OSLE-India), Taiwan, Canada and Europe.\n\n\n\n"}
{"id": "8766153", "url": "https://en.wikipedia.org/wiki?curid=8766153", "title": "Fictional location", "text": "Fictional location\n\nFictional locations are places that exist only in fiction and not in reality, such as the Negaverse or Planet X. Writers may create and describe such places to serve as backdrop for their fictional works. Fictional locations are also created for use as settings in role-playing games such as \"Dungeons and Dragons\". They may also be used for technical reasons in actual reality for use in the development of specifications, such as the fictional country of Bookland, which is used to allow EAN \"country\" codes 978 and 979 to be used for ISBN numbers assigned to books, and code 977 to be assigned for use for ISSN numbers on magazines and other periodicals.\n\nFictional locations vary greatly in their size. Very small places like a single room are kept out of the umbrella of fictional locations by convention, as are most single buildings. A fictional location can be the size of a university (H.P. Lovecraft's Miskatonic University), a town (Stephen King's Salem's Lot), a county (William Faulkner's Yoknapatawpha County), a state (Winnemac in various Sinclair Lewis stories), a large section of continent (as in north-western Middle-earth, which supposedly represents Europe), a whole planet (Anne McCaffrey's Pern), a whole galaxy (Isaac Asimov's \"Foundation\" books), even a multiverse (\"His Dark Materials\"). In a larger scale, occasionally the term alternate reality is used, but only if it is considered a variant of Earth rather than an original world. Austin Tappan Wright's \"Islandia\" has an invented continent, Karain, on our world. However in fanfiction, along with pastiche and/or parody, it is not considered canon unless they get authorized.\n\nWithin narrative prose, providing a believable location can be greatly enhanced by the provision of maps and other illustrations. This is often considered particularly true for fantasy novels and historical novels which often make great use of the map, but applies equally to science fiction and mysteries: earlier, in mainstream novels by Anthony Trollope, William Faulkner, etc. Fantasy and science fiction novels often also provide sections which provide documentation of various aspects of the environment of the fiction, including languages, character lists, cultures and, of course, locations.\n\nIn an online article on writing Dawn Arkin writes about the importance of location to the author's art:\n\nMaps are an immediate necessity for some works, as they do not take place on our Earth. Writers need working maps to keep straight at a glance whether the castle is north or south of the river, and how long it takes to get between valleys. This can be very helpful in preventing snags when dealing directly with fictional geography.\n\nSometimes an actual geographic corner is used as a model for \"getting it right\", and identifying these can become a game for readers. Authors may turn an island into a continent or vice versa, rotate orientation, or combine two similar locales to get the best (for the story) of both.\n\n\n"}
{"id": "65974", "url": "https://en.wikipedia.org/wiki?curid=65974", "title": "First-person narrative", "text": "First-person narrative\n\nA first-person narrative is a mode of storytelling in which a narrator relays events from their own point of view using the first person It may be narrated by a first person protagonist (or other focal character), first person re-teller, first person witness, or first person peripheral (also called a peripheral narrator). A classic example of a first person protagonist narrator is Charlotte Brontë's \"Jane Eyre\" (1847), in which the title character is also the narrator telling her own story, \"I could not unlove him now, merely because I found that he had ceased to notice me\".\n\nThis device allows the audience to see the narrator's mind's eye view of the fictional universe, but it is limited to the narrator's experiences and awareness of the true state of affairs. In some stories, first-person narrators may relay dialogue with other characters or refer to information they heard from the other characters, in order to try to deliver a larger point of view. Other stories may switch the narrator to different characters to introduce a broader perspective. An unreliable narrator is one that has completely lost credibility due to ignorance, poor insight, personal biases, mistakes, dishonesty, etc., which challenges the reader's initial assumptions.\n\nThe telling of a story in the grammatical first person, i.e. from the perspective of \"I.\" An example would be Herman Melville's \"Moby-Dick\", which begins \"Call me Ishmael.\"\n\nFirst-person narration often includes an embedded listener or reader, who serves as the audience for the tale. First-person narrations may be told by a person directly undergoing the events in the story without being aware of conveying that experience to readers; alternatively, the narrator may be conscious of telling the story to a given audience, perhaps at a given place and time, for a given reason.\n\nA story written in the first person can be told by the main character, a less important character witnessing events, or a person retelling a story they were told by someone else. This point of view is often effective in giving a sense of closeness to the character.\n\nFirst-person narration presents the narrative through the perspective of a particular character. The reader or audience becomes aware of the events and characters of the story through the narrator's views and knowledge. As a participant in events, the conscious narrator, is an imperfect witness by definition, unable to fully see and comprehend events in their entirety as they unfurl, not necessarily objective in their inner thoughts or sharing them fully, and furthermore may be pursuing some hidden agenda. In some cases, the narrator may give or withhold information based on his own experience.\n\nCharacter weaknesses and faults, such as tardiness, cowardice, or vice, may leave the narrator unintentionally absent or unreliable for certain key events. Specific events may further be colored or obscured by a narrator's background, since non-omniscient characters must by definition be laypersons and foreigners to some circles, and limitations such as poor eyesight and illiteracy may also leave important blanks. Another consideration is how much time has elapsed between when the character experienced the events of the story and when they decided to tell them. If only a few days have passed, the story could be related very differently than if the character was reflecting on events of the distant past. The character's motivation is also relevant. Are they just trying to clear up events for their own peace of mind? Make a confession about a wrong they did? Or tell a good adventure tale to their beer-guzzling friends? The reason why a story is told will also affect how it is written. Why is this narrator telling the story in this way, why now, and is he to be trusted? Unstable or malevolent narrators can also lie to the reader. Unreliable narrators are not uncommon.\n\nIn the first-person-plural point of view, narrators tell the story using \"we\". That is, no individual speaker is identified; the narrator is a member of a group that acts as a unit. The first-person-plural point of view occurs rarely but can be used effectively, sometimes as a means to increase the concentration on the character or characters the story is about. Examples include:\n\n\nOther examples include \"Twenty-Six Men and a Girl\" by Maxim Gorky, \"The Treatment of Bibi Haldar\" by Jhumpa Lahiri, \"During the Reign of the Queen of Persia\" by Joan Chase, \"Our Kind\" by Kate Walbert, \"I, Robot\" by Isaac Asimov, and \"We Didn't\" by Stuart Dybek. \n\nFirst-person narrators can also be multiple, as in Ryūnosuke Akutagawa's \"In a Grove\" (the source for the movie \"Rashomon\") and Faulkner's novel \"The Sound and the Fury\". Each of these sources provides different accounts of the same event, from the point of view of various first-person narrators.\n\nThere can also be multiple co-principal characters as narrator, such as in Robert A. Heinlein's \"The Number of the Beast\". The first chapter introduces four characters, including the initial narrator, who is named at the beginning of the chapter. The narrative continues in subsequent chapters with a different character explicitly identified as the narrator for that chapter. Other characters later introduced in the book also have their \"own\" chapters where they narrate the story for that chapter. The story proceeds in linear fashion, and no event occurs more than once, i.e. no two narrators speak \"live\" about the same event.\n\nThe first-person narrator may be the principal character or one who closely observes the principal character (see Emily Brontë's \"Wuthering Heights\" or F. Scott Fitzgerald's \"The Great Gatsby\", each narrated by a minor character). These can be distinguished as \"first person major\" or \"first person minor\" points of view.\n\nThe narrator can be the protagonist (e.g., Gulliver in \"Gulliver's Travels\"), someone very close to him who is privy to his thoughts and actions (Dr. Watson in Sherlock Holmes stories), or an ancillary character who has little to do with the action of the story (such as Nick Carraway in \"The Great Gatsby\"). Narrators can report others' narratives at one or more removes. These are called \"frame narrators\": examples are Mr. Lockwood, the narrator in \"Wuthering Heights\" by Emily Brontë; and the unnamed narrator in \"Heart of Darkness\" by Joseph Conrad. Skilled writers choose to skew narratives, in keeping with the narrator's character, to an arbitrary degree, from ever so slight to extreme. For example, the aforementioned Mr. Lockwood is quite naive, of which fact he appears unaware, simultaneously rather pompous, and recounting a combination of stories, experiences, and servants' gossip. As such, his character is an unintentionally very unreliable narrator, and serves mainly to mystify, confuse, and ultimately leave the events of Wuthering Heights open to a great range of interpretations.\n\nA rare form of first person is the first person omniscient, in which the narrator is a character in the story, but also knows the thoughts and feelings of all the other characters. It can seem like third person omniscient at times. A reasonable explanation fitting the mechanics of the story's world is generally provided or inferred, unless its glaring absence is a major plot point. Two notable examples are \"The Book Thief\" by Markus Zusak, where the narrator is Death, and \"The Lovely Bones\" by Alice Sebold, where a young girl, having been killed, observes, from some post-mortem, extracorporeal viewpoint, her family struggling to cope with her disappearance. Typically, however, the narrator restricts the events relayed in the narrative to those that could reasonably be known. Novice writers may make the mistake of allowing elements of omniscience into a first-person narrative unintentionally and at random, forgetting the inherent human limitations of a witness or participant of the events.\n\nIn autobiographical fiction, the first person narrator is the character of the author (with varying degrees of historical accuracy). The narrator is still distinct from the author and must behave like any other character and any other first person narrator. Examples of this kind of narrator include Jim Carroll in \"The Basketball Diaries\" and Kurt Vonnegut, Jr. in \"Timequake\" (in this case, the first-person narrator is also the author). In some cases, the narrator is writing a book—\"the book in your hands\"—and therefore he has most of the powers and knowledge of the author. Examples include \"The Name of the Rose\" by Umberto Eco, and \"The Curious Incident of the Dog in the Night-Time\" by Mark Haddon. Another example is a fictional \"Autobiography of James T. Kirk\" which was \"Edited\" by David A. Goodman who was the actual writer of that book and playing the part of James Kirk (Gene Roddenberry's Star Trek) as he wrote the novel.\n\nSince the narrator is within the story, he or she may not have knowledge of all the events. For this reason, first-person narrative is often used for detective fiction, so that the reader and narrator uncover the case together. One traditional approach in this form of fiction is for the main detective's principal assistant, the \"Watson\", to be the narrator: this derives from the character of Dr Watson in Sir Arthur Conan Doyle's Sherlock Holmes stories.\n\nFirst-person narratives can appear in several forms; interior monologue, as in Fyodor Dostoevsky's \"Notes from Underground\"; dramatic monologue, also in Albert Camus' \"The Fall\"; or explicitly, as Mark Twain's \"Adventures of Huckleberry Finn.\"\n\nOther forms include temporary first-person narration as a story within a story, wherein a narrator or character observing the telling of a story by another is reproduced in full, temporarily and without interruption shifting narration to the speaker. The first-person narrator can also be the focal character.\n\nWith a first person narrative it is important to consider how the story is being told, i.e., is the character writing it down, telling it out loud, thinking it to themselves? And if they are writing it down, is it something meant to be read by the public, a private diary, or a story meant for one other person? The way the first person narrator is relating the story will affect the language used, the length of sentences, the tone of voice and many other things. A story presented as a secret diary could be interpreted much differently than a public statement.\n\nFirst-person narratives can tend towards a stream of consciousness and Interior monologue, as in Marcel Proust's \"In Search of Lost Time\". The whole of the narrative can itself be presented as a false document, such as a diary, in which the narrator makes explicit reference to the fact that he is writing or telling a story. This is the case in Bram Stoker's \"Dracula\". As a story unfolds, narrators may be aware that they are telling a story and of their reasons for telling it. The audience that they believe they are addressing can vary. In some cases, a frame story presents the narrator as a character in an outside story who begins to tell his own story, as in Mary Shelley's \"Frankenstein\".\n\nFirst-person narrators are often unreliable narrators since a narrator might be impaired (such as both Quentin and Benjy in Faulkner's \"The Sound and the Fury\"), lie (as in \"The Quiet American\" by Graham Greene, or \"The Book of the New Sun\" series by Gene Wolfe), or manipulate their own memories intentionally or not (as in \"The Remains of the Day\" by Kazuo Ishiguro, or in Ken Kesey's \"One Flew Over the Cuckoo's Nest\"). Henry James discusses his concerns about \"the romantic privilege of the 'first person'\" in his preface to \"The Ambassadors\", calling it \"the darkest abyss of romance.\"\n\nOne example of a multi-level narrative structure is Joseph Conrad's novella \"Heart of Darkness\", which has a double framework: an unidentified \"I\" (first person singular) narrator relates a boating trip during which another character, Marlow, uses first person to tell a story that comprises the majority of the work. Within this nested story, it is mentioned that another character, Kurtz, told Marlow a lengthy story; however, its content is not revealed to readers. Thus, there is an \"I\" narrator introducing a storyteller as \"he\" (Marlow), who talks about himself as \"I\" and introduces another storyteller as \"he\" (Kurtz), who in turn presumably told his story from the perspective of \"I\".\n\nFirst person narration is more difficult to achieve in film; however, voice-over narration can create the same structure.\n"}
{"id": "8715248", "url": "https://en.wikipedia.org/wiki?curid=8715248", "title": "Four Treasures of the Study", "text": "Four Treasures of the Study\n\nFour Treasures of the Study, Four Jewels of the Study or Four Friends of the Study is an expression used to denote the brush, ink, paper and ink stone used in Chinese and other East Asian calligraphic traditions. The name appears to originate in the time of the Southern and Northern Dynasties (420–589 AD).\n\nChinese culture is very fond of four word couplets, and the Four Treasures is another example: \"文房四寶: 筆、墨、紙、硯,\" (Pinyin: \"wén fáng sì bǎo: bǐ, mò, zhǐ, yàn\") \"The four jewels of the study: Brush, Ink, Paper, Inkstone.\" In the couplet mentioned, each of the Treasures is referred to by a single epithet; however, each of these are usually known by a compound name (\"i.e.\" The Brush: 毛筆, literally \"\"hair brush/pen\"). The individual treasures have a \"treasured\" form, each being produced in certain areas of China as a speciality for those scholars who would use them.These are called the 4 SPECIAL Treasures of the Study.\n\n The brush () is the oldest Four Treasures member, with archaeological evidence dating to Zhou dynasty (1045 BC–256 BC) illustrations on ancient bones. The oldest brush so far dates to Han dynasty (202 BC–220 AD). Brushes are generally made from animal hair, or —in certain situations—the first hair taken from a baby's head (said to bring good luck in the Imperial examinations). Brush handles are commonly constructed from bamboo, but special brushes may have handles of sandalwood, jade, carved bone/ivory, or other precious materials.\n\nModern brushes are primarily white goat hair (羊毫), black rabbit hair (紫毫), yellow weasel hair (黄鼠毫/狼毫), or a combination mix. Ancient brushes, and some of the more valuable ones available on the market may be made with the hair of any number of different types of animals. Each type of hair has a specific ink capacity, giving distinct brush strokes. Different brushes are used for different styles of calligraphy and writing.\n\nBrushes are classed as soft (軟毫), mixed (兼毫) or hard (硬毫). Hair is laboriously sorted by softness, hardness, thickness, & length, then bundled for specific uses. The most famous and highly prized brushes are a mix of yellow weasel, goat and rabbit hair, known as Húbǐ (湖筆); highly prized since the Ming dynasty (late 14th century) they are currently made in Shanlian (善琏), a town in the Nanxun District, prefecture-level city of Huzhou, of Zhejiang province (浙江).\n\nThe Inkstick (Chinese: 墨 pinyin: \"\") is an artificial ink developed during the Han dynasty. These first writing inks were based on naturally occurring minerals like graphite and vermilion; earliest inks were probably liquids and not preserved. Modern inksticks are generally made of soots from one of three different sources, including lacquer soot, pine soot, and oil soot. Soots are collected, then mixed with glue. Higher quality inksticks also use powdered spices and herbs, adding to aroma and providing some protection to the ink itself. The glue, soot, and spice mixture is then pressed into shape and allowed to dry. This process can take 6 weeks, depending on an inkstick's dimensions.\n\nThe best ink sticks are fine grained and have a light, slightly ringing sound when tapped. They are often decorated with poems, calligraphy, or bas relief, and painted. These particular articles are highly collectable, and often acquired like stamps. The inksticks in highest regard, known as Huīmò (徽墨), contain musk, borneol and other precious aromatics of Chinese medicine. They are still produced today in Shexian (歙县) in Anhui province (安徽).\n\nPaper (Chinese: \"traditional\" 紙, \"simplified\" 纸; Pinyin: \"\") was first developed in China in the first decade of 100 AD. Previous to its invention, bamboo slips and silks were used for writing material. Several methods of paper production developed over the centuries in China. However, the paper which was considered of highest value was that of the Jingxian (泾县) in Anhui province.\n\nThis particular form of paper, known as Xuānzhǐ (宣紙), is soft, fine-textured, moth resistant, has a high tensile strength, and remarkable longevity for such a product – so much so that it has a reputation for lasting \"1,000 years\". The quality of the paper depends on the processing methods used to produce it. Paper may be unprocessed, half processed or processed. The processing determines how well ink or paint is absorbed into the fibre of the paper, as well as the stiffness of the paper itself. Unprocessed papers are very absorbent and quite malleable, whereas processed papers are far more resistant to absorption and are stiffer.\n\nThe inkstone (Chinese: \"traditional\" 硯 or 硯臺; \"simplified\" 砚 or 砚台; Pinyin: \"\" or \"yàn tái\") is used to grind the ink stick into powder. This powder is then mixed with water in a well in the inkstone in order to produce usable ink for calligraphy. The most ideal water for use in ink is slightly salty. Ink was first prepared using a mortar and pestle, but with the advent of inksticks this method slowly vanished. The stone used is generally of a relatively fine whetstone type.\n\nThe earliest known inkstones date back to the Han dynasty. The production of inkstones reached its zenith in the Tang and Song dynasties with inkstones becoming extremely intricate works of art. The most highly sought-after inkstones originated in four different locations in China. Duanshi stones (端石硯) from Duanxi in Guangdong, She stones (歙硯) from Shexian in Anhui, Taohe stones (洮河硯) from the Tao River in South Gansu and Chengni ceramic stones (澄泥硯) which are manufactured by a process which is said to have been developed in Luoyang in Henan.\n\nClassical scholars had more than just the Four treasures in their studies. The other \"Treasures\" include the brush-holder (笔架), brush-hanger (笔挂), paperweights (镇纸), the brush-rinsing pot (笔洗), and the seal (圖章) and seal-ink (印泥).\n\nFor painting, Chinese pigments are also used.\n\n"}
{"id": "1751486", "url": "https://en.wikipedia.org/wiki?curid=1751486", "title": "Human ecosystem", "text": "Human ecosystem\n\nHuman ecosystems are complex cybernetic systems that are increasingly being used by ecological anthropologists and other scholars to examine the ecological aspects of human communities in a way that integrates multiple factors as economics, socio-political organization, psychological factors, and physical factors related to the environment.\n\nA human ecosystem has three central organizing concepts: human environed unit (an individual or group of individuals), environment, interactions and transactions between and within the components. The total environment includes three conceptually distinct, but interrelated environments: the natural, human constructed, and human behavioral. These environments furnish the resources and conditions necessary for life and constitute a life-support system.\n\n\n"}
{"id": "13372", "url": "https://en.wikipedia.org/wiki?curid=13372", "title": "Human geography", "text": "Human geography\n\nHuman geography is the branch of geography that deals with the study of people and their communities,\ncultures, economies, and interactions with the environment by studying their relations with and across space and place. Human geography attends to human patterns of social interaction, as well as spatial level interdependencies, and how they influence or affect the earth's environment. As an intellectual discipline, geography is divided into the sub-fields of physical geography and human geography, the latter concentrating upon the study of human activities, by the application of qualitative and quantitative research methods.\n\nGeography was not recognized as a formal academic discipline until the 18th century, although many scholars had undertaken geographical scholarship for much longer, particularly through cartography.\n\nThe Royal Geographical Society was founded in England in 1830, although the United Kingdom did not get its first full Chair of geography until 1917. The first real geographical intellect to emerge in United Kingdom's geographical minds was Halford John Mackinder, appointed reader at Oxford University in 1887.\n\nThe National Geographic Society was founded in the United States in 1888 and began publication of the \"National Geographic\" magazine which became, and continues to be, a great popularizer of geographic information. The society has long supported geographic research and education on geographical topics.\n\nThe Association of American Geographers was founded in 1904 and was renamed the American Association of Geographers in 2016 to better reflect the increasingly international character of its membership.\n\nOne of the first examples of geographic methods being used for purposes other than to describe and theorize the physical properties of the earth is John Snow's map of the 1854 Broad Street cholera outbreak. Though Snow was primarily a physician and a pioneer of epidemiology rather than a geographer, his map is probably one of the earliest examples of health geography.\n\nThe now fairly distinct differences between the subfields of physical and human geography have developed at a later date. This connection between both physical and human properties of geography is most apparent in the theory of environmental determinism, made popular in the 19th century by Carl Ritter and others, and has close links to the field of evolutionary biology of the time. Environmental determinism is the theory, that people's physical, mental and moral habits are directly due to the influence of their natural environment. However, by the mid-19th century, environmental determinism was under attack for lacking methodological rigor associated with modern science, and later as a means to justify racism and imperialism.\n\nA similar concern with both human and physical aspects is apparent during the later 19th and first half of the 20th centuries focused on regional geography.The goal of regional geography, through something known as regionalisation, was to delineate space into regions and then understand and describe the unique characteristics of each region through both human and physical aspects. With links to (possibilism) (geography) and cultural ecology some of the same notions of causal effect of the environment on society and culture remain with environmental determinism.\n\nBy the 1960s, however, the quantitative revolution led to strong criticism of regional geography. Due to a perceived lack of scientific rigor in an overly descriptive nature of the discipline, and a continued separation of geography from its two subfields of physical and human geography and from geology, geographers in the mid-20th century began to apply statistical and mathematical models in order to solve spatial problems. Much of the development during the quantitative revolution is now apparent in the use of geographic information systems; the use of statistics, spatial modeling, and positivist approaches are still important to many branches of human geography. Well-known geographers from this period are Fred K. Schaefer, Waldo Tobler, William Garrison, Peter Haggett, Richard J. Chorley, William Bunge, and Torsten Hägerstrand.\n\nFrom the 1970s, a number of critiques of the positivism now associated with geography emerged. Known under the term 'critical geography,' these critiques signaled another turning point in the discipline. Behavioral geography emerged for some time as a means to understand how people made perceived spaces and places, and made locational decisions. The more influential 'radical geography' emerged in the 1970s and 1980s. It draws heavily on Marxist's theory and techniques, and is associated with geographers such as David Harvey and Richard Peet. Radical geographers seek to say meaningful things about problems recognized through quantitative methods, provide explanations rather than descriptions, put forward alternatives and solutions, and be politically engaged, rather than using the detachment associated with positivists. (The detachment and objectivity of the quantitative revolution was itself critiqued by radical geographers as being a tool of capital). Radical geography and the links to Marxism and related theories remain an important part of contemporary human geography (See: \"Antipode\"). Critical geography also saw the introduction of 'humanistic geography', associated with the work of Yi-Fu Tuan, which pushed for a much more qualitative approach in methodology.\n\nThe changes under critical geography have led to contemporary approaches in the discipline such as feminist geography, new cultural geography, \"demonic\" geographies, and the engagement with postmodern and post-structural theories and philosophies.\n\nThe primary fields of study in human geography focus around the core fields of:\n\nCultural geography is the study of cultural products and norms - their variation across spaces and places, as well as their relations. It focuses on describing and analyzing the ways language, religion, economy, government, and other cultural phenomena vary or remain constant from one place to another and on explaining how humans function spatially. \n\nDevelopment geography is the study of the Earth's geography with reference to the standard of living and the quality of life of its human inhabitants, study of the location, distribution and spatial organization of economic activities, across the Earth. The subject matter investigated is strongly influenced by the researcher's methodological approach.\n\nEconomic geography examines relationships between human economic systems, states, and other factors, and the biophysical environment.\n\nHealth geography is the application of geographical information, perspectives, and methods to the study of health, disease, and health care. Health geography deals with the spatial relations and patterns between people and the environment. This is a sub-discipline of human geography, researching how and why diseases are spread.\n\nHistorical geography is the study of the human, physical, fictional, theoretical, and \"real\" geographies of the past. Historical geography studies a wide variety of issues and topics. A common theme is the study of the geographies of the past and how a place or region changes through time. Many historical geographers study geographical patterns through time, including how people have interacted with their environment, and created the cultural landscape.\n\nPolitical geography is concerned with the study of both the spatially uneven outcomes of political processes and the ways in which political processes are themselves affected by spatial structures.\n\nPopulation geography is the study of ways in which spatial variations in the distribution, composition, migration, and growth of populations are related to their environment or location.\n\nSettlement geography, including urban geography, is the study of urban and rural areas with specific regards to spatial, relational and theoretical aspects of settlement. That is the study of areas which have a concentration of buildings and infrastructure. These are areas where the majority of economic activities are in the secondary sector and tertiary sectors. In case of urban settlement, they probably have a high population density.\nUrban geography is the study of cities, towns, and other areas of relatively dense settlement. Two main interests are site (how a settlement is positioned relative to the physical environment) and situation (how a settlement is positioned relative to other settlements). Another area of interest is the internal organization of urban areas with regard to different demographic groups and the layout of infrastructure. This subdiscipline also draws on ideas from other branches of Human Geography to see their involvement in the processes and patterns evident in an urban area.\n\nWithin each of the subfields, various philosophical approaches can be used in research; therefore, an urban geographer could be a Feminist or Marxist geographer, etc.\n\nSuch approaches are:\n\n\nAs with all social sciences, human geographers publish research and other written work in a variety of academic journals. Whilst human geography is interdisciplinary, there are a number of journals that focus on human geography.\n\nThese include:\n\n\n\n\n"}
{"id": "3270664", "url": "https://en.wikipedia.org/wiki?curid=3270664", "title": "Incest between twins", "text": "Incest between twins\n\nIncest between twins or twincest is a subclass of sibling incest and includes both heterosexual and homosexual relationships. While in modern Western European culture such behaviour is considered taboo, incest between twins is a common feature in Indo-European, Asian (such as Japan) and Oceanian mythology, and there are a few societies in which the prohibition on it is limited or it is partially accepted.\n\nIn traditional Balinese culture, it was common for a set of twins of the opposite sex to marry each other, since it was assumed that they had had sex in utero. The standard anthropological explanation of this custom is based in explications of the conflicts between descent and affinity in Balinese society. Twin incest was a common feature of Balinese mythology. As in many other mythologies, the Balinese deities frequently marry their siblings without any of the incest-related issues faced by similarly-situated human couples.\n\nThis was commonplace in Southeast Asian creation myths which prominently featured twin or sibling couples. In these stories, the brother usually wooed and wed his sister, who bore his child(ren), but on discovering that they are siblings, they are often (but not always) forced to part.\n\nAccording to Tagalog mythology, Malakás (\"strong\") and Magandá (\"beautiful\"), the first humans on earth, were fraternal twins born of the same bamboo stalk.\n\nAn old Japanese myth says that if two star-crossed lovers commit dual suicide, they are reincarnated as fraternal twins.\n\nTwin incest is a prominent feature in ancient Germanic mythology, and its modern manifestations, such as the relationship between Siegmund and Sieglinde in Richard Wagner's \"Die Walküre\", and a feature in some Greek mythology, such as the story of Byblis and Kaunos. There are strong parallels between the Germanic portrayals of twin incest and those of the Balinese Ramayana, and some scholars have speculated an early Indo-European link.\n\nThe theme also appears in English literature, such as the incest between the twins Polydore and Urania in Delarivier Manley's \"The New Atlantis\".\n\nIn a 1983 review of the scholarly literature on twin homosexuality and twin incest, Ray Bixler concluded that \"most same-sex homosexual twins, if reared with their co-twins, do not attempt or even want to seduce them in adulthood\". His study drew on Edvard Westermarck's hypothesis that sexual desire is generally absent in relationships between members of a nuclear family.\n\nOne case of incest between twins, in which twins who were adopted by separate families as babies later married without knowing they were brother and sister, was mentioned in a House of Lords debate on the Human Fertility and Embryology Bill in January 2008. According to the charity Adults Affected by Adoption, there had been other cases of this sort that had involved siblings. The story was widely publicised in the British press, although its truthfulness was called into question.\n\nCzech identical twins Michal and Radek Cuma are male pornographic actors who in 2009 began having sex together in video performances as \"Milo and Elijah Peters\". They consider themselves both brothers and romantic partners, and report that, outside their video performances with other actors, they have sex solely with each other.\n"}
{"id": "6830515", "url": "https://en.wikipedia.org/wiki?curid=6830515", "title": "Index of history articles", "text": "Index of history articles\n\nHistory is the study of the past. When used as the name of a field of study, \"history\" refers to the study and interpretation of the record of humans, families, and societies as preserved primarily through written sources. This is a list of history topics covered on English Wikipedia:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "56087517", "url": "https://en.wikipedia.org/wiki?curid=56087517", "title": "Iron-Age-Danube project", "text": "Iron-Age-Danube project\n\n\"“Monumentalized Early Iron Age Landscapes in the Danube river basin”\" \n\nIron-Age-Danube is a cross-border project and is part of the Interreg Danube Transnational Programme of the European Union. \n\nIn the Iron-Age-Danube project the early Iron Age landscapes in the Danube river basin are explored with the help of modern technology and traditional archaeological tools. The participating partners from Austria, Croatia, Hungary, Slovakia and Slovenia work together to promote the concept of archaeological landscapes beyond single archaeological sites in these countries.\n\nThe project is funded by the European Union with about 2.169.200 EUR from the European Regional Development Fund (ERDF). These ERDF resources make 85% of the total project volume which is about 2.552.000 EUR.\n\nThe Iron-Age-Danube project lasts from the 1st of January 2017 till the 30th of June 2019.\n\nThe major goal of the project is to communicate a lively image of archaeological research and the Iron Age landscapes to the visitors and to raise the awareness for the importance of archaeological monuments for the human history. \n\nThe project Iron-Age-Danube ignores borderlines between the countries and researches the remains of the Hallstatt period (Iron Age) in the Danube region. The data of this research will help to understand the way of living in the past and the needs of the heritage today.\n\nAnother goal is the protection of the monuments and landscapes as well as their sustainable use for the tourism. To strengthen the local tourism, already existing archaeological parks or trails will be revitalised. New visitor programs in form of digital and analogous tools will be developed and implemented in 9 micro-regions. The micro-regions are: Großklein and Strettweg (Austria), Jalžabet and Kaptol (Croatia), Poštela and Dolenjske Toplice (Slovenia) and Százhalombatta, Süttő and Sopron (Hungary).\n\nThe project partners and associated partners in Austria, Croatia, Hungary, Slovakia and Slovenia are: \n\nOn the 31st of January 2018 the European Year of Cultural Heritage was officially launched by the European Commission and the Iron-Age-Danube project was granted the honor to use the EYCH 2018 logo.\n\nIn June 2018 the Iron-Age-Danube Project was selected by an Independent Jury as one of 21 finalists for the RegioStars Awards in the 2018 edition. The voting for this year’s public choice award started on the 3rd of July 2018.\n\nAt the beginning of July the project partners in Vienna became a part of the road trip project and took the two young travellers with them to do some aerial archaeology.\n\n"}
{"id": "16366", "url": "https://en.wikipedia.org/wiki?curid=16366", "title": "Jurisprudence", "text": "Jurisprudence\n\nJurisprudence or legal theory is the theoretical study of law, principally by philosophers but, from the twentieth century, also by social scientists. Scholars of jurisprudence, also known as jurists or legal theorists, hope to obtain a deeper understanding of legal reasoning, legal systems, legal institutions, and the role of law in society.\n\nModern jurisprudence began in the 18th century and was focused on the first principles of the natural law, civil law, and the law of nations. General jurisprudence can be divided into categories both by the type of question scholars seek to answer and by the theories of jurisprudence, or schools of thought, regarding how those questions are best answered. Contemporary philosophy of law, which deals with general jurisprudence, addresses problems internal to law and legal systems, and problems of law as a particular social institution as law relates to the larger political and social situation in which it exists.\n\nThis article distinguishes three distinct branches of thought in general jurisprudence. To begin with, ancient natural law, as a major branch and theory of jurisprudence, is the idea that there are rational objective limits to the power of legislative rulers. The foundations of law are accessible through reason and it is from these laws of nature that human-created laws gain whatever force they have. Secondly, 'clarificatory' or analytic jurisprudence rejects natural law's fusing of what law is and what it ought to be. It espouses the use of a neutral point of view and descriptive language when referring to aspects of legal systems. It comprises different theories of jurisprudence. For example, \"legal positivism\", holds that there is no necessary connection between law and morality and that the force of law comes from some basic social facts. And \"legal realism\" argues that the real world practice of law is what determines what law is; the law has the force that it does because of what legislators, lawyers and judges do with it. Thirdly, normative jurisprudence is concerned with \"evaluative\" theories of law. It deals with what the goal or purpose of law is, or what moral or political theories provide a foundation for the law. Besides the question \"What is law?\", it tries to answer what the proper function of law was, or what sorts of acts should be subject to legal sanctions, and what sorts of punishment should be permitted.\n\nThe English word is based on the Latin maxim \"jurisprudentia\": \"juris\" is the genitive form of \"jus\" meaning \"law\", and \"prudentia\" means \"prudence\" (also: discretion, foresight, forethought, circumspection; refers to the exercise of good judgment, common sense, and even caution, especially in the conduct of practical matters). The word is first attested in English in 1628, at a time when the word \"prudence\" had the meaning of \"knowledge of or skill in a matter\". The word may have come via the French \"jurisprudence\", which is attested earlier.\n\nAncient Indian jurisprudence is available in various Dharmaśāstra texts starting from the Dharmasutra of Bhodhayana. Jurisprudence already had this meaning in Ancient Rome even if at its origins the discipline was a (\"periti\") in the \"jus\" of \"mos maiorum\" (traditional law), a body of oral laws and customs verbally transmitted \"by father to son\". Praetors established a workable body of laws by judging whether or not singular cases were capable of being prosecuted either by the edicta, the annual pronunciation of prosecutable offense, or in extraordinary situations, additions made to the edicta. An iudex then would judge a remedy according to the facts of the case.\n\nTheir sentences were supposed to be simple interpretations of the traditional customs, but effectively it was an activity that, apart from formally reconsidering for each case what precisely was traditionally in the legal habits, soon turned also to a more equitable interpretation, coherently adapting the law to the newer social instances. The law was then implemented with new evolutive \"Institutiones\" (legal concepts), while remaining in the traditional scheme. Praetors were replaced in the 3rd century BC by a laical body of \"prudentes\". Admission to this body was conditional upon proof of competence or experience.\n\nUnder the Roman Empire, schools of law were created, and the activity constantly became more academic. In the age from the early Roman Empire to the 3rd century, a relevant literature was produced by some notable groups including the Proculians and Sabinians. The scientific depth of the studies was unprecedented in ancient times.\n\nAfter the 3rd century, \"Juris prudentia\" became a more bureaucratic activity, with few notable authors. It was during the Eastern Roman Empire (5th century) that legal studies were once again undertaken in depth, and it is from this cultural movement that Justinian's Corpus Juris Civilis was born.\n\nIn its general context, natural law theory may be compared to both state-of-nature law and general law understood on the basis of an analogy to the physical laws of science. Natural law is often contrasted to positive law which asserts law as the product of human activity and human volition.\n\nAnother approach to natural law jurisprudence generally asserts that human law may be supported by decisive reasons for action. In other words, there must be a compelling rationale behind following human law. There are two readings of the natural law jurisprudential stance.\nNotions of an objective moral order, external to human legal systems, underlie natural law. What is right or wrong can vary according to the interests one is focused upon. Natural law is sometimes identified with the maxim that \"an unjust law is no law at all\", but as John Finnis, the most important of modern natural barristers has argued, this maxim is a poor guide to the classical Thomist position. Strongly related to theories of natural law are classical theories of justice, beginning in the West with Plato's Republic.\n\nAristotle is often said to be the father of natural law. Like his philosophical forefathers Socrates and Plato, Aristotle posited the existence of natural justice or natural right (\"dikaion physikon\", \"δικαίον φυσικόν\", Latin \"ius naturale\"). His association with natural law is largely due to the way in which he was interpreted by Thomas Aquinas. This was based on Aquinas' conflation of natural law and natural right, the latter of which Aristotle posits in Book V of the \"Nicomachean Ethics\" (= Book IV of the \"Eudemian Ethics\"). Aquinas's influence was such as to affect a number of early translations of these passages, though more recent translations render them more literally.\n\nAristotle's theory of justice is bound up in his idea of the golden mean. Indeed, his treatment of what he calls \"political justice\" derives from his discussion of \"the just\" as a moral virtue derived as the mean between opposing vices, just like every other virtue he describes. His longest discussion of his theory of justice occurs in \"Nicomachean Ethics\" and begins by asking what sort of mean a just act is. He argues that the term \"justice\" actually refers to two different but related ideas: general justice and particular justice. When a person's actions are completely virtuous in all matters in relation to others, Aristotle calls them \"just\" in the sense of \"general justice;\" as such this idea of justice is more or less coextensive with virtue. \"Particular\" or \"partial justice\", by contrast, is the part of \"general justice\" or the individual virtue that is concerned with treating others equitably. Aristotle moves from this unqualified discussion of justice to a qualified view of political justice, by which he means something close to the subject of modern jurisprudence. Of political justice, Aristotle argues that it is partly derived from nature and partly a matter of convention. This can be taken as a statement that is similar to the views of modern natural law theorists. But it must also be remembered that Aristotle is describing a view of morality, not a system of law, and therefore his remarks as to nature are about the grounding of the morality enacted as law, not the laws themselves. The passage here is silent as to that question.\n\nThe best evidence of Aristotle's having thought there was a natural law comes from the \"Rhetoric\", where Aristotle notes that, aside from the \"particular\" laws that each people has set up for itself, there is a \"common\" law that is according to nature. The context of this remark, however, suggests only that Aristotle thought that it could be rhetorically advantageous to appeal to such a law, especially when the \"particular\" law of one's own city was adverse to the case being made, not that there actually was such a law; Aristotle, moreover, considered two of the three candidates for a universally valid, natural law suggested in this passage to be wrong. Aristotle's theoretical paternity of the natural law tradition is consequently disputed.\n\nThomas Aquinas, [Thomas of Aquin, or Aquino] (c. 1225 – 7 March 1274) was an Italian philosopher and theologian in the scholastic tradition, known as \"Doctor Angelicus, Doctor Universalis\". He is the foremost classical proponent of natural theology, and the father of the Thomistic school of philosophy, for a long time the primary philosophical approach of the Roman Catholic Church. The work for which he is best known is the \"Summa Theologica\". One of the thirty-five Doctors of the Church, he is considered by many Catholics to be the Church's greatest theologian. Consequently, many institutions of learning have been named after him.\n\nAquinas distinguished four kinds of law: eternal, natural, human and divine:\n\n\nNatural law, of course, is based on \"first principles\":\n\n\"... this is the first precept of the law, that good is to be done and promoted, and evil is to be avoided. All other precepts of the natural law are based on this ...\"\n\nThe desires to live and to procreate are counted by Aquinas among those basic (natural) human values on which all other human values are based.\n\nFrancisco de Vitoria was perhaps the first to develop a theory of \"ius gentium\" (the rights of peoples), and thus is an important figure in the transition to modernity. He extrapolated his ideas of legitimate sovereign power to society at the international level, concluding that this scope as well ought to be ruled by just forms respectable of the rights of all. The common good of the world is of a category superior to the good of each state. This meant that relations between states ought to pass from being justified by force to being justified by law and justice. Some scholars have upset the standard account of the origins of International law, which emphasises the seminal text \"De iure belli ac pacis\" by Grotius, and argued for Vitoria and, later, Suárez's importance as forerunners and, potentially, founders of the field. Others, such as Koskenniemi, have argued that none of these humanist and scholastic thinkers can be understood to have founded international law in the modern sense, instead placing its origins in the post-1870 period.\n\nFrancisco Suárez, regarded as among the greatest scholastics after Aquinas, subdivided the concept of \"ius gentium\". Working with already well-formed categories, he carefully distinguished \"ius inter gentes\" from \"ius intra gentes\". \"Ius inter gentes\" (which corresponds to modern international law) was something common to the majority of countries, although, being positive law, not natural law, was not necessarily universal. On the other hand, \"ius intra gentes\", or civil law, is specific to each nation.\n\nIn his treatise \"Leviathan, (1651)\", Hobbes expresses a view of natural law as a precept, or general rule, found out by reason, by which a man is forbidden to do that which is destructive of his life, or takes away the means of preserving the same; and to omit that by which he thinks it may best be preserved. Hobbes was a social contractarian and believed that the law gained peoples' tacit consent. He believed that society was formed from a state of nature to protect people from the state of war between mankind that exists otherwise. Life is, without an ordered society, \"solitary, poor, nasty, brutish and short\". It is commonly commented that Hobbes' views about the core of human nature were influenced by his times. The English Civil War and the Cromwellian dictatorship had taken place, and he felt absolute authority vested in a monarch, whose subjects obeyed the law, was the basis of a civilized society.\n\nWriting after World War II, Lon L. Fuller defended a secular and procedural form of natural law. He notably emphasised that the (natural) law must meet certain formal requirements (such as being impartial and publicly knowable). To the extent that an institutional system of social control falls short of these requirements, Fuller argues, we are less inclined to recognise it as a system of law, or to give it our respect. Thus, law has an internal morality that goes beyond the social rules by which valid laws are made.\n\nSophisticated positivist and natural law theories sometimes resemble each other more than the above descriptions might suggest, and they may concede certain points to the other \"side\". Identifying a particular theorist as a positivist or a natural law theorist sometimes involves matters of emphasis and degree, and the particular influences on the theorist's work. In particular, the older natural lawyers, such as Aquinas and John Locke made no distinction between analytic and normative jurisprudence. But modern natural lawyers, such as John Finnis claim to be positivists, while still arguing that law is a basically moral creature. His book \"Natural Law and Natural Rights\" (1980, 2011) is a restatement of natural law doctrine.\n\nAnalytic, or 'clarificatory', jurisprudence means the use of a neutral point of view and descriptive language when referring to the aspects of legal systems. This was a philosophical development that rejected natural law's fusing of what law is and what it ought to be. David Hume famously argued in \"A Treatise of Human Nature\" that people invariably slip between describing that the world \"is\" a certain way to saying therefore we \"ought\" to conclude on a particular course of action. But as a matter of pure logic, one cannot conclude that we \"ought\" to do something merely because something \"is\" the case. So analysing and clarifying the way the world \"is\" must be treated as a strictly separate question to normative and evaluative \"ought\" questions.\n\nThe most important questions of analytic jurisprudence are: \"What are laws?\"; \"What is \"the\" law?\"; \"What is the relationship between law and power/sociology?\"; and \"What is the relationship between law and morality?\" Legal positivism is the dominant theory, although there are a growing number of critics who offer their own interpretations.\n\nHistorical jurisprudence came to prominence during the German debate over the proposed codification of German law. In his book \"On the Vocation of Our Age for Legislation and Jurisprudence\", Friedrich Carl von Savigny argued that Germany did not have a legal language that would support codification because the traditions, customs and beliefs of the German people did not include a belief in a code. The Historicists believe that the law originates with society.\n\nThe effort to inform jurisprudence systematically with sociological insights developed strongly from the beginning of the twentieth century, as sociology began to establish itself as a distinct social science, especially in the United States and in continental Europe. In Germany the work of the 'free law' theorists (e.g. Ernst Fuchs, Hermann Kantorowicz and Eugen Ehrlich) encouraged the use of sociological insights in judicial development of law and juristic theory. The most internationally influential advocacy of a 'sociological jurisprudence' occurred in the United States, where Roscoe Pound, for many years the Dean of Harvard Law School, used this term to characterise his legal philosophy throughout the first half of the twentieth century. In the United States many later writers followed Pound's lead or developed distinctive approaches to sociological jurisprudence. In Australia, Julius Stone strongly defended and developed Pound's ideas. In the 1930s a significant split between the sociological jurists and the American legal realists emerged. In the second half of the twentieth century sociological jurisprudence as a distinct movement declined as jurisprudence came more strongly under the influence of analytical legal philosophy but with increasing criticism of dominant orientations of Anglophone legal philosophy in the present century it has attracted renewed interest.\n\nPositivism simply means that law is something that is \"posited\": laws are validly made in accordance with socially accepted rules. The positivist view on law can be seen to cover two broad principles: Firstly, that laws may seek to enforce justice, morality, or any other normative end, but their success or failure in doing so does not determine their validity. Provided a law is properly formed, in accordance with the rules recognized in the society concerned, it is a valid law, regardless of whether it is \"just\" by some other standard. Secondly, that law is nothing more than a set of rules to provide order and governance of society. No legal positivist, however, argues that it follows that the law is therefore to be obeyed, no matter what. This is seen as a separate question entirely.\n\nOne of the earliest legal positivists was Jeremy Bentham. Bentham was an early and staunch supporter of the utilitarian concept (along with Hume), an avid prison reformer, advocate for democracy, and strong atheist. Bentham's views about law and jurisprudence were popularized by his student, John Austin. Austin was the first chair of law at the new University of London from 1829. Austin's utilitarian answer to \"what is law?\" was that law is \"commands, backed by threat of sanctions, from a sovereign, to whom people have a habit of obedience\". Contemporary legal positivists have long abandoned this view, and have criticised its oversimplification, H. L. A. Hart particularly.\n\nHans Kelsen is considered one of the prominent jurists of the 20th century and has been highly influential in Europe and Latin America, although less so in common-law countries. His Pure Theory of Law aims to describe law as binding norms while at the same time refusing, itself, to evaluate those norms. That is, 'legal science' is to be separated from 'legal politics'. Central to the Pure Theory of Law is the notion of a 'basic norm (Grundnorm)'—a hypothetical norm, presupposed by the jurist, from which in a hierarchy all 'lower' norms in a legal system, beginning with constitutional law, are understood to derive their authority or 'bindingness'. In this way, Kelsen contends, the bindingness of legal norms, their specifically 'legal' character, can be understood without tracing it ultimately to some suprahuman source such as God, personified Nature or—of great importance in his time—a personified State or Nation.\n\nIn the Anglophone world, the pivotal writer was H. L. A. Hart, who argued that the law should be understood as a system of social rules. Hart rejected Kelsen's views that sanctions were essential to law and that a normative social phenomenon, like law, can not be grounded in non-normative social facts. Hart revived analytical jurisprudence as an important theoretical debate in the twentieth century through his book \"The Concept of Law\". As the professor of jurisprudence at Oxford University, Hart argued that law is a 'system of rules'.\n\nRules, said Hart, are divided into primary rules (rules of conduct) and secondary rules (rules addressed to officials to administer primary rules). Secondary rules are divided into rules of adjudication (to resolve legal disputes), rules of change (allowing laws to be varied) and the rule of recognition (allowing laws to be identified as valid). The \"rule of recognition\" is a customary practice of the officials (especially barristers and judges) that identifies certain acts and decisions as sources of law. A pivotal book on Hart was written by Neil MacCormick in 1981 (second edition due in 2007), which further refined and offered some important criticisms that led MacCormick to develop his own theory (the best example of which is his recently published \"Institutions of Law\", 2007). Other important critiques have included that of Ronald Dworkin, John Finnis, and Joseph Raz.\n\nIn recent years, debates about the nature of law have become increasingly fine-grained. One important debate is within legal positivism. One school is sometimes called \"exclusive legal positivism\", and it is associated with the view that the legal validity of a norm can never depend on its moral correctness. A second school is labeled \"inclusive legal positivism\", a major proponent of which is Wil Waluchow, and it is associated with the view that moral considerations \"may\" determine the legal validity of a norm, but that it is not necessary that this is the case.\n\nSome philosophers used to contend that positivism was the theory that there is \"no necessary connection\" between law and morality; but influential contemporary positivists, including Joseph Raz, John Gardner, and Leslie Green, reject that view. As Raz points out, it is a necessary truth that there are vices that a legal system cannot possibly have (for example, it cannot commit rape or murder).\n\nJoseph Raz defends the positivist outlook, but criticised Hart's \"soft social thesis\" approach in \"The Authority of Law\". Raz argues that law is authority, identifiable purely through social sources, without reference to moral reasoning. Any categorisation of rules beyond their role as authority is better left to sociology than to jurisprudence.\n\nLegal realism was a view popular with some Scandinavian and American writers. Skeptical in tone, it held that the law should be understood and determined by the actual practices of courts, law offices, and police stations, rather than as the rules and doctrines set forth in statutes or learned treatises. It had some affinities with the sociology of law and sociological jurisprudence. The essential tenet of legal realism is that all law is made by human beings and, thus, is subject to human foibles, frailties and imperfections.\n\nIt has become common today to identify Justice Oliver Wendell Holmes, Jr., as the main precursor of American Legal Realism (other influences include Roscoe Pound, Karl Llewellyn and Justice Benjamin Cardozo). Karl Llewellyn, another founder of the U.S. legal realism movement, similarly believed that the law is little more than putty in the hands of judges who are able to shape the outcome of cases based on their personal values or policy choices. The chief inspiration for Scandinavian legal realism many consider to be the works of Axel Hägerström.\n\nDespite its decline in popularity, realism continues to influence a wide spectrum of jurisprudential schools today, including critical legal studies, feminist legal theory, critical race theory, sociology of law and law and economics.\n\nCritical legal studies are a younger theory of jurisprudence that has developed since the 1970s. It can be generally traced to American legal realism and is considered \"the first movement in legal theory and legal scholarship in the United States to have espoused a committed Left political stance and perspective\". It holds that the law is largely contradictory, and can be best analyzed as an expression of the policy goals of a dominant social group.\n\nKarl Popper originated the theory of critical rationalism. According to Reinhold Zippelius many advances in law and jurisprudence take place by operations of critical rationalism. He writes, \"daß die Suche nach dem Begriff des Rechts, nach seinen Bezügen zur Wirklichkeit und nach der Gerechtigkeit experimentierend voranschreitet, indem wir Problemlösungen versuchsweise entwerfen, überprüfen und verbessern\" (that we empirically search solutions of problems, which harmonise fair with the reality: by projecting, testing and improving the solutions).\n\nContemporary philosopher of law Ronald Dworkin has advocated a more constructivist theory of jurisprudence that can be characterized as a middle path between natural law theories and positivist theories of general jurisprudence. \nIn his book \"Law's Empire\" Dworkin attacked Hart and the positivists for their refusal to treat law as a moral issue. He argues that law is an 'interpretive' concept, that requires barristers to find the best-fitting and most just solution to a legal dispute, given their constitutional traditions. According to him, law is not entirely based on social facts, but includes the morally best justification for the institutional facts and practices that we intuitively regard as legal. It follows on Dworkin's view that one cannot know whether a society has a legal system in force, or what any of its laws are, until one knows some moral truths about the justifications for the practices in that society. It is consistent with Dworkin's view—in contrast with the views of legal positivists or legal realists—that \"no-one\" in a society may know what its laws are, because no-one may know the best justification for its practices.\n\nInterpretation, according to Dworkin's \"integrity theory of law\", has two dimensions. To count as an interpretation, the reading of a text must meet the criterion of \"fit\". Of those interpretations that fit, however, Dworkin maintains that the correct interpretation is the one that puts the political practices of the community in their best light, or makes of them \"the best that they can be\". But many writers have doubted whether there \"is\" a single best justification for the complex practices of any given community, and others have doubted whether, even if there are, they should be counted as part of the law of that community.\n\nConsequences from the operation of legal rules or legal procedures—or from the behavior of legal actors (such as lawyers and judges)—may be either beneficial (therapeutic) or harmful (anti-therapeutic) to people. Therapeutic jurisprudence (\"TJ\") studies law as a social force (or agent) and uses social science methods and data to study the extent to which a legal rule or practice affects the psychological well-being of the people it affects.\n\nIn addition to the question, \"What is law?\", legal philosophy is also concerned with normative, or \"evaluative\" theories of law. What is the goal or purpose of law? What moral or political theories provide a foundation for the law? What is the proper function of law? What sorts of acts should be subject to punishment, and what sorts of punishment should be permitted? What is justice? What rights do we have? Is there a duty to obey the law? What value has the rule of law? Some of the different schools and leading thinkers are as follows.\n\nAretaic moral theories such as contemporary virtue ethics emphasize the role of character in morality. Virtue jurisprudence is the view that the laws should promote the development of virtuous characters by citizens. Historically, this approach is associated mainly with Aristotle or Thomas Aquinas later. Contemporary virtue jurisprudence is inspired by philosophical work on virtue ethics.\n\nDeontology is \"the theory of duty or moral obligation.\" The philosopher Immanuel Kant formulated one influential deontological theory of law. He argued that any rule we follow must be able to be universally applied, i.e. we must be willing for everyone to follow that rule. A contemporary deontological approach can be found in the work of the legal philosopher Ronald Dworkin.\n\nUtilitarianism is the view that the laws should be crafted so as to produce the best consequences for the greatest number of people possible. Historically, utilitarian thinking about law is associated with the philosopher Jeremy Bentham. John Stuart Mill was a pupil of Bentham's and was the torch bearer for utilitarian philosophy through the late nineteenth century. In contemporary legal theory, the utilitarian approach is frequently championed by scholars who work in the law and economics tradition.\n\nJohn Rawls was an American philosopher, a professor of political philosophy at Harvard University and author of \"A Theory of Justice\" (1971), \"Political Liberalism\", \"\", and \"The Law of Peoples\". He is widely considered one of the most important English-language political philosophers of the 20th century. His theory of justice uses a device called the original position to ask us which principles of justice we would choose to regulate the basic institutions of our society if we were behind a 'veil of ignorance.' Imagine we do not know who we are - our race, sex, wealth status, class, or any distinguishing feature - so that we would not be biased in our own favour. Rawls argues from this 'original position' that we would choose exactly the same political liberties for everyone, like freedom of speech, the right to vote and so on. Also, we would choose a system where there is only inequality because that produces incentives enough for the economic well-being of all society, especially the poorest. This is Rawls's famous 'difference principle'. Justice is fairness, in the sense that the fairness of the original position of choice guarantees the fairness of the principles chosen in that position.\n\nThere are many other normative approaches to the philosophy of law, including critical legal studies and libertarian theories of law.\n\n\n\n"}
{"id": "221742", "url": "https://en.wikipedia.org/wiki?curid=221742", "title": "Legal history", "text": "Legal history\n\nLegal history or the history of law is the study of how law has evolved and why it changed. Legal history is closely connected to the development of civilisations and is set in the wider context of social history. Among certain jurists and historians of legal process, it has been seen as the recording of the evolution of laws and the technical explanation of how these laws have evolved with the view of better understanding the origins of various legal concepts; some consider it a branch of intellectual history. Twentieth century historians have viewed legal history in a more contextualised manner more in line with the thinking of social historians. They have looked at legal institutions as complex systems of rules, players and symbols and have seen these elements interact with society to change, adapt, resist or promote certain aspects of civil society. Such legal historians have tended to analyse case histories from the parameters of social science inquiry, using statistical methods, analysing class distinctions among litigants, petitioners and other players in various legal processes. By analysing case outcomes, transaction costs, number of settled cases they have begun an analysis of legal institutions, practices, procedures and briefs that give us a more complex picture of law and society than the study of jurisprudence, case law and civil codes can achieve.\n\nAncient Egyptian law, dating as far back as 3000 BC, had a civil code that was probably broken into twelve books. It was based on the concept of Ma'at, characterised by tradition, rhetorical speech, social equality and impartiality. By the 22nd century BC, Ur-Nammu, an ancient Sumerian ruler, formulated the first extant law code, consisting of casuistic statements (\"if... then...\"). Around 1760 BC, King Hammurabi further developed Babylonian law, by codifying and inscribing it in stone. Hammurabi placed several copies of his law code throughout the kingdom of Babylon as stelae, for the entire public to see; this became known as the Codex Hammurabi. The most intact copy of these stelae was discovered in the 19th century by British Assyriologists, and has since been fully transliterated and translated into various languages, including English, German and French. Ancient Greek has no word for \"law\" as an abstract concept, retaining instead the distinction between divine law (\"thémis\"), human decree (\"nomos\") and custom (\"díkē\"). Yet Ancient Greek law contained major constitutional innovations in the development of democracy.\n\nAncient India and China represent distinct traditions of law, and had historically independent schools of legal theory and practice. The \"Arthashastra\", dating from the 400 BC, and the \"Manusmriti\" from 100 BCE were influential treatises in India, texts that were considered authoritative legal guidance. Manu's central philosophy was tolerance and pluralism, and was cited across South East Asia. But this Hindu tradition, along with Islamic law, was supplanted by the common law when India became part of the British Empire. Malaysia, Brunei, Singapore and Hong Kong also adopted the common law.\n\nThe eastern Asia legal tradition reflects a unique blend of secular and religious influences. Japan was the first country to begin modernising its legal system along western lines, by importing bits of the French, but mostly the German Civil Code. This partly reflected Germany's status as a rising power in the late nineteenth century. Similarly, traditional Chinese law gave way to westernisation towards the final years of the Qing dynasty in the form of six private law codes based mainly on the Japanese model of German law. Today Taiwanese law retains the closest affinity to the codifications from that period, because of the split between Chiang Kai-shek's nationalists, who fled there, and Mao Zedong's communists who won control of the mainland in 1949. The current legal infrastructure in the People's Republic of China was heavily influenced by soviet Socialist law, which essentially inflates administrative law at the expense of private law rights. Today, however, because of rapid industrialisation China has been reforming, at least in terms of economic (if not social and political) rights. A new contract code in 1999 represented a turn away from administrative domination. Furthermore, after negotiations lasting fifteen years, in 2001 China joined the World Trade Organization.\n\n\nThe legal history of the Catholic Church is the history of Catholic canon law, the oldest continuously functioning legal system in the West. Canon law originates much later than Roman law but predates the evolution of modern European civil law traditions. The cultural exchange between the secular (Roman/Barbarian) and ecclesiastical (canon) law produced the jus commune and greatly influenced both civil and common law.\n\nThe history of Latin canon law can be divided into four periods: the \"jus antiquum\", the \"jus novum\", the \"jus novissimum\" and the \"Code of Canon Law\". In relation to the Code, history can be divided into the \"jus vetus\" (all law before the Code) and the \"jus novum\" (the law of the Code, or \"jus codicis\"). Eastern canon law developed separately.\n\nIn the twentieth century, canon law was comprehensively codified. On 27 May 1917, Pope Benedict XV codified the 1917 Code of Canon Law. John XIII, together with his intention to call the Second Vatican Council, announced his intention to reform canon law, which culminated in the 1983 Code of Canon Law, promulgated by John Paul II on 25 January 1983. John Paul II also brought to a close the long process of codifying the legal elements common to all 23 sui juris Eastern Catholic Churches on 18 October 1990 by promulgating the Code of Canons of the Eastern Churches.\n\nOne of the major legal systems developed during the Middle Ages was Islamic law and jurisprudence. A number of important legal institutions were developed by Islamic jurists during the classical period of Islamic law and jurisprudence. One such institution was the \"Hawala\", an early informal value transfer system, which is mentioned in texts of Islamic jurisprudence as early as the 8th century. \"Hawala\" itself later influenced the development of the \"Aval\" in French civil law and the \"Avallo\" in Italian law.\n\nRoman law was heavily influenced by Greek teachings. It forms the bridge to the modern legal world, over the centuries between the rise and decline of the Roman Empire. Roman law, in the days of the Roman republic and Empire, was heavily procedural and there was no professional legal class. Instead a lay person, \"iudex\", was chosen to adjudicate. Precedents were not reported, so any case law that developed was disguised and almost unrecognised. Each case was to be decided afresh from the laws of the state, which mirrors the (theoretical) unimportance of judges' decisions for future cases in civil law systems today. During the 6th century AD in the Eastern Roman Empire, the Emperor Justinian codified and consolidated the laws that had existed in Rome so that what remained was one twentieth of the mass of legal texts from before. This became known as the \"Corpus Juris Civilis\". As one legal historian wrote, \"Justinian consciously looked back to the golden age of Roman law and aimed to restore it to the peak it had reached three centuries before.\"\n\nDuring the Byzantine Empire the Justinian Code was expanded and remained in force until the Empire fell, though it was never officially introduced to the West. Instead, following the fall of the Western Empire and in former Roman countries, the ruling classes relied on the Theodosian Code to govern natives and Germanic customary law for the Germanic incomers - a system known as folk-right - until the two laws blended together. Since the Roman court system had broken down, legal disputes were adjudicated according to Germanic custom by assemblies of learned lawspeakers in rigid ceremonies and in oral proceedings that relied heavily on testimony.\n\nAfter much of the West was consolidated under Charlemagne, law became centralized so as to strengthen the royal court system, and consequently case law, and abolished folk-right. However, once Charlemagne's kingdom definitively splintered, Europe became feudalistic, and law was generally not governed above the county, municipal or lordship level, thereby creating a highly decentralized legal culture that favored the development of customary law founded on localized case law. However, in the 11th century, crusaders, having pillaged the Byzantine Empire, returned with Byzantine legal texts including the Justinian Code, and scholars at the University of Bologna were the first to use them to interpret their own customary laws. Medieval European legal scholars began researching the Roman law and using its concepts and prepared the way for the partial resurrection of Roman law as the modern civil law in a large part of the world. There was, however, a great deal of resistance so that civil law rivaled customary law for much of the late Middle Ages.\n\nAfter the Norman conquest of England, which introduced Norman legal concepts into medieval England, the English King's powerful judges developed a body of precedent that became the common law. In particular, Henry II instituted legal reforms and developed a system of royal courts administered by a small number of judges who lived in Westminster and traveled throughout the kingdom. Henry II also instituted the Assize of Clarendon in 1166, which allowed for jury trials and reduced the number of trials by combat. Louis IX of France also undertook major legal reforms and, inspired by ecclesiastical court procedure, extended Canon-law evidence and inquisitorial-trial systems to the royal courts. Also, judges no longer moved on circuits becoming fixed to their jurisdictions, and jurors were nominated by parties to the legal dispute rather than by the sheriff. In addition, by the 10th century, the Law Merchant, first founded on Scandinavian trade customs, then solidified by the Hanseatic League, took shape so that merchants could trade using familiar standards, rather than the many splintered types of local law. A precursor to modern commercial law, the Law Merchant emphasised the freedom of contract and alienability of property.\n\nThe two main traditions of modern European law are the codified legal systems of most of continental Europe, and the English tradition based on case law.\n\nAs nationalism grew in the 18th and 19th centuries, \"lex mercatoria\" was incorporated into countries' local law under new civil codes. Of these, the French Napoleonic Code and the German Bürgerliches Gesetzbuch became the most influential. As opposed to English common law, which consists of massive tomes of case law, codes in small books are easy to export and for judges to apply. However, today there are signs that civil and common law are converging. European Union law is codified in treaties, but develops through the precedent set down by the European Court of Justice.\n\nThe United States legal system developed primarily out of the English common law system (with the exception of the state of Louisiana, which continued to follow the French civilian system after being admitted to statehood). Some concepts from Spanish law, such as the prior appropriation doctrine and community property, still persist in some US states, particularly those that were part of the Mexican Cession in 1848.\n\nUnder the doctrine of federalism, each state has its own separate court system, and the ability to legislate within areas not reserved to the federal government.\n\n\n\n\n"}
{"id": "8424642", "url": "https://en.wikipedia.org/wiki?curid=8424642", "title": "Linguistic insecurity", "text": "Linguistic insecurity\n\nLinguistic insecurity comprises feelings of anxiety, self-consciousness, or lack of confidence in the mind of a speaker surrounding the use of their own language. Often, this anxiety comes from speakers' belief that their use of language does not conform to the perceived standard and/or the style of language expected by the speakers' interlocutor(s). Linguistic insecurity is situationally induced and is often based on a feeling of inadequacy regarding personal performance in certain contexts, rather than a fixed attribute of an individual. This insecurity can lead to stylistic, and phonetic shifts away from an affected speaker's default speech variety; these shifts may be performed consciously on the part of the speaker, or may be reflective of an unconscious effort to conform to a more prestigious or context-appropriate style of speech. Linguistic insecurity is linked to the perception of speech styles in any community, and so may vary based on socioeconomic class and gender. It is also especially pertinent in multilingual societies.\n\nLinguistic insecurity is the negative self-image a speaker has regarding his or her own speech variety or language as a whole, especially in the perceived difference between phonetic and syntactic characteristics of one's own speech and those characteristics of what is perceived to be the \"correct\" form of the spoken language. Linguistic insecurity arises based on the perception of a lack of correctness regarding one's own speech, rather than any objective deficiencies in a particular speech variety.\n\nIn one of its earliest usages, the term linguistic insecurity was employed by linguist William Labov in his 1972 paper on the social stratification of the pronunciation of /r/ to describe the attitude that employees, at three different retail stores in New York, have towards their own speech patterns, in comparison to the Standard English form. Labov theorized that those employees who had the most extreme shift in style from their own speech variety (a casual style) to the standard form (a more emphatic style) were more insecure in a linguistic sense. The term has since been used to describe any situation in which a speaker is led to hypercorrect, or shift one's patterns of speech, due to a negative attitude or lack of confidence regarding one's normal speech. This lack of confidence need not be consciously acknowledged by a speaker in order for him/her to be affected by linguistic insecurity, and the changes in pronunciation and stylistic shifts indicative of linguistic insecurity can emerge absent of speaker intent. Linguistic insecurity may also be a characteristic of an entire speech community, especially in how it relates to other speech communities of the same language that employ a more standardized form.\n\nAs linguistic insecurity is related to the perception of how one speaks in comparison to a certain form, the notion of standard and prestige forms of languages is important. The standard form of a language is regarded as the codified form of language used in public discourse, while the prestige form is the one perceived to receive the most respect accorded to any variety of the language. Variables that differentiate standard and prestige forms include phonetic realization, vocabulary, syntax, among other features of speech. The status of these forms is related to the concept of language ideology, which explains how varieties of language are correlated with certain moral, social or political values. Many societies value the belief that language homogeneity is beneficial to society; in fact, the existence of a \"common language\" is an intrinsic part of an imagined community, which defines a nation.\nHowever, the concept of a language norm is highly flexible. Nations often codify a standard language that may be different from regional norms. For example, Standard English in the United Kingdom is based on the south-eastern dialect and accent centered around London. In other parts of the UK, various dialects are spoken, such as Scots and Geordie; even in London, there exist Cockney and Estuary accents. Studies of young people in Glasgow show that they self-report linguistic insecurity, describing their own speech as 'slang' in comparison to the 'standard form' and attempting to incline their own speech to the standard.\n\nPrestige forms may also demonstrate linguistic insecurity. Again, in the UK, Received Pronunciation (RP), a prestige accent, has been affected by other varieties of speech. Though the standard form historically aimed towards RP, it is not a perfect imitation. The result is that RP speakers now demonstrate changes in phonetic realization in the direction of the standard.\n\nDespite these shifts, a person using an RP accent would tend to give the impression that he or she is well-educated and part of a higher socioeconomic class. This is because these traits are often associated with RP speakers; they index specific concepts that are presupposed by the community. Similarly, in general, forms of speech gain their status by their association with certain class characteristics. This indexicality does not need to be passive: in Beijing, young urban professionals actively adopt usages considered typical of prestigious Hong Kong and Taiwan speech in an effort to index themselves as cosmopolitan. It also does not need to be positive: speech forms may also index negative characteristics. In his study of attitudes towards varieties of United States English, Preston demonstrates that people often associate the Southern accent with a lack of sophistication, indexing speakers with such an accent as being backwards and conservative; and that Southern speakers themselves perceive their language to be inferior, exhibiting linguistic insecurity.\n\nSpeakers experiencing linguistic insecurity exhibit alterations of their normal speech which are reflective of their insecurity, and often are a result of the speaker attempting to compensate for the perceived deficiencies in their own speech variety. These effects of linguistic insecurity can come in the form of changes in pronunciation, as in the case of the retail store employees in William Labov's example, or even syntactic deviations from the speaker's normal speech variant.\n\nOne documented linguistic effect of linguistic insecurity is hypercorrection. Hypercorrection is the over-application of a perceived rule of grammar in order to appear more formal or to appear to belong to a more prestigious speech community. A common instance of hypercorrection in English is the use of the personal pronouns \"you and I\" as a correction of \"me and you\" in situations in which the accusative personal pronoun \"me\" is more appropriate. Because the use of \"you and I\" is internalized as the more grammatically sound form in the mind of many English speakers, that rule becomes over-applied in a situation when a speaker wants to compensate for perceived linguistic deficiencies. A speaker may try to avoid feelings of linguistic insecurity and perceived stigmatization by projecting a more educated or formal identity and emulating what is perceived as a more prestigious speech variety. Inadvertently, hypercorrection may index a speaker as belonging to the very social class or societal group that led to the linguistic insecurity. For example, linguist Donald Winford found after studying Trinidadian English that there was a knowledge that there was a stigmatization associated with less prestigious phonological variants, creating a situation in which individuals belonging to a \"lower\" social class would attempt to replicate phonological aspects of the more prestigious forms of English, but did not do so successfully, thus engaging in hypercorrection.\n\nSpeakers experiencing linguistic insecurity may also undergo, either consciously or unconsciously, a change in register from their default language variety. Linguistic register refers to a variety of speech in a given language that corresponds to a specific situational purpose or social setting. An example of the phonological impact of register in English is when speaking in a formal setting, it is customary to pronounce words ending in \"-ing\" with a velar nasal rather than substituting it for the [n] sound that is typical of \"-ing\" endings in informal speech. A register shift cannot always be accounted for by documenting the individual phonological differences in speech from one's default speech variety to the newly registered speech variety, but instead may include a difference in the overall \"tenor\" of speech and in the way a speaker gives deference to his/her interlocutors who are more experienced in interacting in that register. Having to navigate in a linguistic register markedly different from one's own speech variety can be a catalyst for hypercorrection and other behavioral effects of linguistic insecurity that can further contribute to a sense of communicative inadequacy if the speaker feels he is not convincingly interacting in that linguistic register.\n\nFindings show that the members of the lower middle class have the greatest tendency toward linguistic insecurity. Labov notes that evidence of their insecurity can be found in their wide range of stylistic variation, fluctuation in given stylistic contexts, conscious striving for correctness, and negative attitude towards their native speech pattern.\nAfter conducting a linguistic survey in 1960s New York City, Labov found evidence that the usage of /r/ by speakers was predictable except in a specific case involving the lower middle class. At the time, the pronunciation of /r/ at the end of words and before consonants became a prestige marker and the degree to which it was realized in casual speech correlated with the socioeconomic status of the respondents. However, members of the lower middle class showed a dramatic increase of r-pronunciation when a more formal style of speech was elicited, even surpassing the usage by the classes above. Labov interpreted this tendency to hypercorrect by adopting the prestigious form of the high ranking class as a sign of the linguistic insecurity of the lower middle class.\nExplanations for why the lower middle class exhibits this tendency have yet to be fully explored. A study conducted by Owens and Baker (1984) shows that the lower middle class of Winnipeg, Manitoba, Canada had highest scores for the CILI (Canadian Index of Linguistic Insecurity), which was adopted from Labov's original test – the ILI (Index of Linguistic Insecurity). In their paper, they hypothesize that this effect can be explained by an interaction between behavior and attitudes about social status. Members the lower middle class are caught between the linguistic behavior of the classes below them and the attitudes of the upper class. Members of the lower middle class accept the idea of correct speech from those above them, but changes in their usage lag behind changes in attitude. They identify the upper class usage as correct and admit that their behavior is different, leading to a disparity that manifests itself as linguistic insecurity. Though Owens and Baker admit that a measure of the mobility aspirations of the respondents is needed to test their explanation, others agree that the effect can be best interpreted as a function of upward social mobility rather than of social class distinctions themselves. In his later work, Labov highlights that it is often the second highest status groups that display the steepest slope of style shifting, most hypercorrection, highest levels on linguistic insecurity tests, and the strongest tendency to stigmatize the speech of others in subjective evaluation tests for that variable. In many cases of socioeconomic stratification, this group is equated with the lower middle class.\n\nIn the Owens and Baker study mentioned above, the authors used the CILI and ILI test to conclude that women are more linguistically insecure than men. Out of a sampling data of 80 participants, 42 of which were female, women scored higher on the ILI and the CILI, which indicates high manifest linguistic insecurity. On the CILI, the mean score was 3.23 for females and 2.10 for males. On the ILI, the means scores were 2.23 for females and 1.40 for males. Though the t-tests for the differences were only significant at .07 and .06 levels, the authors feel that this was due to a small sample size and that the uniformity of the results was enough to confirm their hypothesis. Additionally, these findings are consistent with Labov's original New York study and lead to the conclusion by Owens and Baker that women display more linguistic insecurity than men.\n\nLinguistic insecurity can be heightened in speech communities in which multiple dialects exist beyond the standard language. Insecure speakers suffer from a negative attitude toward the speech of their dialect group and often feel pressured to mask their dialectal versatility since the norm of communication is to use the standard form. Bidialectal speakers, who speak both the standard and their own dialect, are most vulnerable to this problem because they are more aware of linguistic norms and the contexts to which they must adapt their speech to these norms. For monodialectal speakers, conversations can be difficult or stressful because they are locked into their nonstandard dialect and have a harder time explaining themselves in the standard dialect.\n\nAfrican American Vernacular English (AAVE) is a dialect of American English that is associated with the African American ethnic group. Speakers of AAVE (as well as speakers of other dialects found in the United States) have encountered a variety of sociolinguistic problems in many important institutions since Standard American English (SAE) is the predominant form of English used.\n\nOne of these important institutions is school. Concerns about the academic achievement of African American children have motivated researchers to study the role AAVE plays though there are various explanations for how it might affect achievement. Dialectal differences could lead to inappropriate testing procedures or prejudice of educators (having lowered expectations and assuming the child is inarticulate and hesitant). In this environment, AAVE-speaking students may develop linguistic insecurity, leading to a rejection of the standards as \"posh\" or reluctance to speak at all to hide their \"inability\" to use language. AAVE-speaking students have also been shown to hypercorrect in attempts to speak or write in Standard English. Insecurity about what \"sounds right\" may result in the avoidance of the invariant \"be\" by deleting it from an instance in which it would be proper to use it (e.g. \"They said they were told if they didn't follow orders they \"would courtmarshled\" or shot as deserters\").\n\nSpeakers of AAVE may also encounter problems in seeking treatment for mental health problems, where professionals predominantly use Standard American English. Linguistic insecurity can be a cause of miscommunication for AAVE patients. For example, mental health care providers may attribute speaker's behavior to cognitive or emotional deficits, even to a psychopathological extent. In a study of a psychiatric ward, Bucci and Baxter collected data on the impact of linguistic problems of the patients, which included several monodialectal speakers and bidialectal speakers of AAVE. In the case of \"Jimmy\", his background led his therapist to believe that his \"muteness\" resulted from emotional or neurophysiological problems. However, Bucci and Baxter found evidence indicating his position as a monodialectal AAVE speaker made him unwilling to speak. His linguistic insecurity in the clinical setting with a norm of SAE made him reluctant to speak, but he was fluent and expressive in his own speech community and with his descriptions of his experiences outside the ward. Moreover, standard therapeutic techniques may have a negative and opposite effect for linguistically insecure patient. In the case of the bidialectal \"Arlene\", the patient thought that her speech was an obstacle to communication because her therapist often asked her what she meant. The intervention of eliciting answers was meant to encourage Arlene to speak more freely, but her linguistic insecurity led her to focus her attention on the perceived inadequacy of her language style and she responded by saying less rather than more.\n\nOne example of linguistic insecurity arising from dialectal differences can be found in work done by Canut and Keita (1994). They conducted a study of an area in the Mandingo zone of Mali that exhibited a linguistic continuum between two different forms: Bambara and Malinke. The study included two villages (Bendugu and Sagabari), a middle-sized town (Kita), and the capital of Mali (Bamako). Bamako is on the Bambara extreme of the continuum, Sagabari on the Malinke extreme, and Bendugu and Kita in between. The linguistic features important for understanding the differences between the dialects are mainly phonological.\n\nThe area encompassing these four places has relatively high social mobility and those who gain status often move towards Bamako, the capital. The dialects follow this pattern, as those closer to the capital are perceived as more prestigious; the most peripheral form in Sagabari can even prompt mockery of the individual using it. Thus, those speaking a dialect different from Bambara are likely to be affected by linguistic insecurity, particularly those closer to the Malinke end of the continuum.\nSince migration is common, there are many examples of young migrants to the area who display linguistic insecurity. Most migrants who speak Malinke try to hide their origins and assimilate to the higher status society by changing the way that they speak. In their attempts to escape their geosocial status, however, they tend to hypercorrect to the point where they create non-existent terms in Bambara. One example is replacing every /h/ in Malinke with the /f/ used in Bamako, leading one to say 'young boy' /foron/ (which does not exist in Bamako) for 'noble' /horon/.\n\nLinguistic insecurity in relation to creoles has to do with the underlying assumption and classification of these languages as inferior forms of the parent languages from which they are derived. Typical of most non-official languages, creoles are regarded as mere degenerate variants and rudimentary dialects that are subsumed under the main \"standard\" languages for that particular community. With this popular view, creoles are thought to be impoverished, primitive outputs that are far from their European target languages. The negative nonlinguistic implications lead to claims of creole use as being a \"handicap\" for their speakers. This has caused speakers of these creole languages to experience insecurity and lack of confidence in the use of their form of language, which has undermined the prevalence of creoles spoken in societies.\n\nOne explanation concerning the different attitudes of speakers is that some populations are more insistent of the use of their particular form of language, as it is commonly claimed to be more \"pure.\" This assumption places this form as a more prestigious standard, creating a tense environment that promotes feelings of insecurity to those who do not follow this standard (and speak \"impure\" variations).\n\nAn instance of linguistic insecurity can be found in relation to Haitian Creole, which developed from a combination of French and other languages. Although the vast majority in this country grows up hearing and speaking exclusively this creole, it continues to be seen as an inferior, primitive tongue as well as a malformed version of French. This disfavor against the creole, which exists throughout the society, is present even among those who can speak only in that variation. The cause of the view has been attributed to the association of French with prestige, as most of the island's land-owning, well-educated elite speaks this language. These judgments contribute to the widespread belief that success is linked to French and that one must speak French to become part of the middle class with a financially stable job, a notion that places Haitian Creole on a lower status. Though it is the majority of people who cannot participate in the French-driven areas of society, the \"ideology of disrespect and degradation\" surrounding creoles leads to great linguistic insecurity. As Arthur Spears put it, an \"internalized oppression\" is present in these members who relate important figures in society (and their success) to speaking French, devaluing their own language of Haitian Creole.\n\nLinguistic insecurity can arise in multilingual environments in speakers of the non-dominant language or of a non-standard dialect. Issues caused by the linguistic variation range from \"total communication breakdowns involving foreign language speakers to subtle difficulties involving bilingual and bidialectal speakers\". Divergence from the standard variety by minority languages causes \"a range of attitudinal issues surrounding the status of minority languages as a standard linguistic variety\".\n\nAn example of mother-tongue-based linguistic insecurity in a multilingual environment is Quebec French. Due to a general perception of Quebec French as lacking in quality and diverging from the norm, French speaking Quebeckers have suffered from a sense of linguistic insecurity. Though French is widely spoken in Quebec, the French in France is considered by many to be the standard and prestigious form. This comparison and the fact that Quebec French diverges from the standard form of France have caused linguistic insecurity among Quebec speakers.\n\nDue to the separation from France after the Treaty of Paris in 1763 and the multilingual environment, Quebec French become more anglicized through English pronunciations and borrowings. Though French Canadian speakers were aware of the differences between Quebec French and French, the foreign perception of Quebec French as \"non-standard\" was not an issue until the mid 19th century. The opinions of the French elite that Quebec French was \"far removed from the prestigious variety spoken in Paris\" had spread through the general public by the end of the 19th century, causing a deep sense of linguistic insecurity in French speaking Quebec. The insecurity was twofold since Quebeckers spoke neither the dominant English language nor, as they were being told, Standard French.\n\n"}
{"id": "204504", "url": "https://en.wikipedia.org/wiki?curid=204504", "title": "Millennium", "text": "Millennium\n\nA millennium (plural millennia or millenniums) is a period equal to 1000 years, also called kiloyears. It derives from the Latin ', thousand, and ', year. It is often, but not always, related to a particular dating system.\n\nSometimes, it is used specifically for periods of a thousand years that begin at the starting point (initial reference point) of the calendar in consideration (typically the year \"1\"), or in later years that are whole number multiples of a thousand years after it. The term can also refer to an interval of time beginning on any date. Frequently in the latter case (and sometimes also in the former) it may have religious or theological implications (see millenarianism).\n\nThere are two methods of counting years: current years (the count begins at the epoch) and elapsed years (the count is of completed years since the epoch).\n\nThe original method of counting years was ordinal, whether \"1st year AD\" or regnal \"10th year of King Henry VIII\". This ordinal numbering is still used in the names of the millennia and centuries, for example the \"1st millennium\" or the \"10th century\", and sometimes in the names of decades, e.g., \"1st decade of the 11th century\".\n\nThe main issues arise from the \"content\" of the various year ranges. Similar issues affect the \"contents\" of centuries. Decades are not subject to ambiguity, as they are named according to their leading numbers: the decade called the 1990s by its naming does not include 2000.\n\nThose following ordinal year names naturally choose\nThose who are influenced by the leading digit equally naturally choose\n\nThere are two viewpoints about how millennia should be thought of in practice. One viewpoint relies on the formal operation of the calendar, while the other appeals to popular culture. Stephen Jay Gould argued that the choice is arbitrary, and since the question revolves around rules made by people, rather than a natural phenomenon that is subject to experimental measurement, the matter cannot be resolved.\n\nThe ISO 8601, employed in a number of contexts, uses the astronomical calendar, in which year counting starts at 0. Thus, when using this calendar, the millennium starts at x000 and ends at x999. There was a popular debate leading up to the celebrations of the year 2000 as to whether the beginning of that year should be understood (and celebrated) as the beginning of a new millennium. Historically, there has been debate around the turn of previous decades, centuries, and millennia.\n\nThe issue is tied to the convention of using ordinal numbers to count millennia (as in \"the third millennium\"), as opposed to \"the two thousands\", which is unambiguous as it does not depend on which year counting starts. The first convention is common in English speaking countries, but the latter is favored in for example Sweden (\"tvåtusentalet\", which translates literally as the \"two thousands period\").\n\nThose holding that the arrival of the new millennium should be celebrated in the transition from 2000 to 2001 (i.e., December 31, 2000 to January 1, 2001), argued that because the Gregorian calendar has no year zero, the millennia should be counted from 1 AD. Thus the first period of one thousand complete years runs from the beginning of 1 AD to the end of 1000 AD, and the beginning of the second millennium took place at the beginning of 1001. \n\nArthur C. Clarke gave this analogy (from a statement received by Reuters): \"If the scale on your grocer's weighing machine began at 1 instead of 0, would you be happy when he claimed he'd sold you 10 kg of tea?\" This statement illustrates the common confusion about the calendar.\n\nIf one counts from the beginning of AD 1 to the ending of AD 1000, one would have counted 1000 years. The next 1000 years (millennium) would begin on the first day of 1001. So the calendar has not 'cheated' anyone out of a year. Clarke made reference to this viewpoint in his book \"\" referring to the Millennium Celebrations on December 31, 2000. In other words, the argument is based on the fact that the last year of the first two thousand years in the Gregorian calendar was 2000, not 1999.\n\nThe \"year 2000\" has also been a popular phrase referring to an often utopian future, or a year when stories in such a future were set, adding to its cultural significance. There was also media and public interest in the Y2K bug. People liked to compared their odometer as a reason to celebrate the new millennium which goes from 1999 to 2000. Thus, the populist argument was that the new millennium should begin when the zeroes \"rolled over\" to 2000, i.e., the day after December 31, 1999. People felt that the change of the hundreds digit in the year number, and the zeros rolling over, created a sense that a new century had begun.\n\nThis is similar to the common demarcation of decades by their most significant digits, e.g., naming the period 1980 to 1989 as the 1980s or \"the eighties\". Similarly, it would be valid to celebrate the year 2000 as a cultural event in its own right, and name the period 2000 to 2999 would be \"the 2000s\". In other words, the time period between 1 and 999 (999 years only) would be called the \"0s\" and the period between 1000 and 1999 would be the \"1000s\".\n\nThe popular approach was to treat the end of 1999 as the end of a millennium and to hold millennium celebrations at midnight between December 31, 1999 and January 1, 2000, as per viewpoint 2. The cultural and psychological significance of the events listed above combined to cause celebrations to be observed one year earlier than the formal Gregorian date. This does not establish that insistence on the formal Gregorian date is \"incorrect\", though some view it as pedantic.\n\nSome event organisers hedged their bets by calling their 1999 celebrations things like \"Click\" referring to the odometer-like rolling over of the nines to zeros. A second approach was to adopt two different views on the millennium problem and celebrate the new millennium twice.\n\nStephen Jay Gould, in his essay \"Dousing Diminutive Dennis' Debate (or DDDD = 2000)\" (\"Dinosaur in a Haystack\"), discussed the \"high\" versus \"pop\" culture interpretation of the transition. Gould noted that the high culture, strict construction had been the dominant viewpoint at the 20th century's beginning, but that the pop culture viewpoint dominated at its end. Gould also included comments on adjustments to the calendar, such as those by Dionysius Exiguus (the eponymous \"Diminutive Dennis\") and the timing of celebrations over different transitional periods. Further of his essays on this topic are collected in \"Questioning the Millennium: A Rationalist's Guide to a Precisely Arbitrary Countdown\".\n\n"}
{"id": "52220595", "url": "https://en.wikipedia.org/wiki?curid=52220595", "title": "Mughal Tahakhana", "text": "Mughal Tahakhana\n\nMughal Tahakhana \"or\" Shah Shuja Tahakhana is a three storied building known as \"Tahakhana\" \"(, Persian: تاهخانا )\", means cold building or palace. The historical Tahakhana is located at the Gauḍa (region) in Firozpur area in the west of a big pond. It is 15 km from Chapai Nawabganj District in Shahbajpur Union at Shibganj Upazila.\n\nSultan of Bengal, Shah Shuja founded this palace as a 'Temperature Control Unit' in honour of his Murshed Hazrat Shah Syed Niyamatullah, mainly for comfort in winter. Son of Mughal Emperor Shah Jahan, Shah Shuja founded this palace between 1619 and 1658 (or 1639–1660). There is a saying that, when he came to meet with Murshed used the middle wider room. A lot of unknown graves inside the 'Tahakhana Complex' considered as companions or Khadems of Shah Syed Niyamatullah. \nAncient architecture like Gauḍa is rarely found except Tahakhana. Her ceiling and partition are coagulated on the beam by concrete casting. The Mosque and Tahakhana are on the lake name 'Dafe-ul-Balah'. Two stare cases are sank into the lake. Two more structures are on the north west side of the palace, nearer one is a three domed mosque and another one is one domed tomb with bolted veranda. All the buildings are founded by the same time for a specific purpose and considered as a complex unit. Main materiel of the building is brick. Black stone is used for the threshold and wooden vim used for plain roof. The building seems to be single storied from the west side but assumed double storied from the east side, extended by the rooms directly raised the archway from the lake. A hammam is in the west of the building supplied water from an octagonal reservoir. A small family mosque is in the north and at its back side an open room which connected with an octagonal tower. This tower possibly used for contemplation. This octagonal tower balanced the complex. The palace is plastered and engraved by the crafts following Mughal architecture.\n\n"}
{"id": "255991", "url": "https://en.wikipedia.org/wiki?curid=255991", "title": "Narrative", "text": "Narrative\n\nA narrative or story is a report of connected events, real or imaginary, presented in a sequence of written or spoken words, or still or moving images, or both. The word derives from the Latin verb \"narrare\", \"to tell\", which is derived from the adjective \"gnarus\", \"knowing\" or \"skilled\".\n\nNarrative can be organized in a number of thematic or formal categories: non-fiction (such as definitively including creative non-fiction, biography, journalism, transcript poetry, and historiography); fictionalization of historical events (such as anecdote, myth, legend, and historical fiction); and fiction proper (such as literature in prose and sometimes poetry, such as short stories, novels, and narrative poems and songs, and imaginary narratives as portrayed in other textual forms, games, or live or recorded performances).\n\nNarrative is found in all forms of human creativity, art, and entertainment, including speech, literature, theatre, music and song, comics, journalism, film, television and video, video games, radio, gameplay, unstructured recreation, and performance in general, as well as some painting, sculpture, drawing, photography, and other visual arts, as long as a sequence of events is presented. Several art movements, such as modern art, refuse the narrative in favor of the abstract and conceptual.\n\nOral storytelling is the earliest method for sharing narratives. During most people's childhoods, narratives are used to guide them on proper behavior, cultural history, formation of a communal identity, and values, as especially studied in anthropology today among traditional indigenous peoples.\n\nNarratives may also be nested within other narratives, such as narratives told by an unreliable narrator (a character) typically found in noir fiction genre. An important part of narration is the narrative mode, the set of methods used to communicate the narrative through a process narration (see also \"Narrative Aesthetics\" below).\n\nAlong with exposition, argumentation, and description, narration, broadly defined, is one of four rhetorical modes of discourse. More narrowly defined, it is the fiction-writing mode in which the narrator communicates directly to the reader.\n\nA narrative is a telling of some true or fictitious event or connected sequence of events, recounted by a narrator to a narratee (although there may be more than one of each). Narratives are to be distinguished from descriptions of qualities, states, or situations, and also from dramatic enactments of events (although a dramatic work may also include narrative speeches). A narrative consists of a set of events (the story) recounted in a process of narration (or discourse), in which the events are selected and arranged in a particular order (the plot). The category of narratives includes both the shortest accounts of events (for example, \"the cat sat on the mat\", or a brief news item) and the longest historical or biographical works, diaries, travelogues, and so forth, as well as novels, ballads, epics, short stories, and other fictional forms. In the study of fiction, it is usual to divide novels and shorter stories into first-person narratives and third-person narratives. As an adjective, \"narrative\" means \"characterized by or relating to storytelling\": thus narrative technique is the method of telling stories, and narrative poetry is the class of poems (including ballads, epics, and verse romances) that tell stories, as distinct from dramatic and lyric poetry. Some theorists of narratology have attempted to isolate the quality or set of properties that distinguishes narrative from non-narrative writings: this is called narrativity.\n\nOwen Flanagan of Duke University, a leading consciousness researcher, writes, \"Evidence strongly suggests that humans in all cultures come to cast their own identity in some sort of narrative form. We are inveterate storytellers.\" Stories are an important aspect of culture. Many works of art and most works of literature tell stories; indeed, most of the humanities involve stories.\nStories are of ancient origin, existing in ancient Egyptian, ancient Greek, Chinese and Indian cultures and their myths. Stories are also a ubiquitous component of human communication, used as parables and examples to illustrate points. Storytelling was probably one of the earliest forms of entertainment. As noted by Owen Flanagan, narrative may also refer to psychological processes in self-identity, memory and meaning-making.\n\nSemiotics begins with the individual building blocks of meaning called signs; and semantics, the way in which signs are combined into codes to transmit messages. This is part of a general communication system using both verbal and non-verbal elements, and creating a discourse with different\nmodalities and forms.\n\nIn \"On Realism in Art\" Roman Jakobson argues that literature exists as a separate entity. He and many other semioticians prefer the view that all texts, whether spoken or written, are the same, except that some authors encode their texts with distinctive \"literary\" qualities that distinguish them from other forms of discourse. Nevertheless, there is a clear trend to address literary narrative forms as separable from other forms. This is first seen in Russian Formalism through Victor Shklovsky's analysis of the relationship between composition and style, and in the work of Vladimir Propp, who analysed the plots used in traditional folk-tales and identified 31 distinct functional components. This trend (or these trends) continued in the work of the Prague School and of French scholars such as Claude Lévi-Strauss and Roland Barthes. It leads to a structural analysis of narrative and an increasingly influential body of modern work that raises important theoretical questions:\n\nIn literary theoretic approach, narrative is being narrowly defined as fiction-writing mode in which the narrator is communicating directly to the reader. Until the late 19th century, literary criticism as an academic exercise dealt solely with poetry (including epic poems like the \"Iliad\" and \"Paradise Lost,\" and poetic drama like Shakespeare). Most poems did not have a narrator distinct from the author.\n\nBut novels, lending a number of voices to several characters in addition to narrator's, created a possibility of narrator's views differing significantly from the author's views. With the rise of the novel in the 18th century, the concept of the narrator (as opposed to \"author\") made the question of narrator a prominent one for literary theory. It has been proposed that perspective and interpretive knowledge are the essential characteristics, while focalization and structure are lateral characteristics of the narrator.\n\nA writer's choice in the narrator is crucial for the way a work of fiction is perceived by the reader. There is a distinction between first-person and third-person narrative, which Gérard Genette refers to as intradiegetic and extradiegetic narrative, respectively. Intradiagetic narrators are of two types: a homodiegetic narrator participates as a character in the story. Such a narrator cannot know more about other characters than what their actions reveal. A heterodiegetic narrator, in contrast, describes the experiences of the characters that appear in the story in which he or she does not participate.\n\nMost narrators present their story from one of the following perspectives (called narrative modes): first-person, or third-person limited or omniscient. Generally, a first-person narrator brings greater focus on the feelings, opinions, and perceptions of a particular character in a story, and on how the character views the world and the views of other characters. If the writer's intention is to get inside the world of a character, then it is a good choice, although a third-person limited narrator is an alternative that does not require the writer to reveal all that a first-person character would know. By contrast, a third-person omniscient narrator gives a panoramic view of the world of the story, looking into many characters and into the broader background of a story. A third-person omniscient narrator can be an animal or an object, or it can be a more abstract instance that does not refer to itself. For stories in which the context and the views of many characters are important, a third-person narrator is a better choice. However, a third-person narrator does not need to be an omnipresent guide, but instead may merely be the protagonist referring to himself in the third person (also known as third person limited narrator).\n\nA writer may choose to let several narrators tell the story from different points of view. Then it is up to the reader to decide which narrator seems most reliable for each part of the story. It may refer to the style of the writer in which he/she expresses the paragraph written. See for instance the works of Louise Erdrich. William Faulkner's \"As I Lay Dying\" is a prime example of the use of multiple narrators. Faulkner employs stream of consciousness to narrate the story from various perspectives.\n\nIn Indigenous American communities, narratives and storytelling are often told by a number of elders in the community. In this way, the stories are never static because they are shaped by the relationship between narrator and audience. Thus, each individual story may have countless variations. Narrators often incorporate minor changes in the story in order to tailor the story to different audiences.\n\nNarrative is a highly aesthetic art. Thoughtfully composed stories have a number of aesthetic elements. Such elements include the idea of narrative structure, with identifiable beginnings, middles and ends, or exposition-development-climax-denouement, with coherent plot lines; a strong focus on temporality including retention of the past, attention to present action and protention/future anticipation; a substantial focus on character and characterization, \"arguably the most important single component of the novel\" (David Lodge \"The Art of Fiction\" 67); different voices interacting, \"the sound of the human voice, or many voices, speaking in a variety of accents, rhythms and registers\" (Lodge \"The Art of Fiction\" 97; see also the theory of Mikhail Bakhtin for expansion of this idea); a narrator or narrator-like voice, which \"addresses\" and \"interacts with\" reading audiences (see Reader Response theory); communicates with a Wayne Booth-esque rhetorical thrust, a dialectic process of interpretation, which is at times beneath the surface, forming a plotted narrative, and at other times much more visible, \"arguing\" for and against various positions; relies substantially on the use of literary tropes (see Hayden White, \"Metahistory\" for expansion of this idea); is often intertextual with other literatures; and commonly demonstrates an effort toward \"bildungsroman\", a description of identity development with an effort to evince \"becoming\" in character and community. \n\nWithin philosophy of mind, the social sciences and various clinical fields including medicine, narrative can refer to aspects of human psychology. A personal narrative process is involved in a person's sense of personal or cultural identity, and in the creation and construction of memories; it is thought by some to be the fundamental nature of the self. The breakdown of a coherent or positive narrative has been implicated in the development of psychosis and mental disorder, and its repair said to play an important role in journeys of recovery. Narrative Therapy is a school of (family) psychotherapy.\n\nIllness narratives are a way for a person affected by an illness to make sense of his or her experiences. They typically follow one of several set patterns: \"restitution\", \"chaos\", or \"quest\" narratives. In the restitution narrative, the person sees the illness as a temporary detour. The primary goal is to return permanently to normal life and normal health. These may also be called cure narratives. In the chaos narrative, the person sees the illness as a permanent state that will inexorably get worse, with no redeeming virtues. This is typical of diseases like Alzheimer's disease: the patient gets worse and worse, and there is no hope of returning to normal life. The third major type, the quest narrative, positions the illness experience as an opportunity to transform oneself into a better person through overcoming adversity and re-learning what is most important in life; the physical outcome of the illness is less important than the spiritual and psychological transformation. This is typical of the triumphant view of cancer survivorship in the breast cancer culture.\n\nPersonality traits, more specifically the Big Five personality traits, appear to be associated with the type of language or patterns of word use found in an individual's self-narrative. In other words, language use in self-narratives accurately reflects human personality. The linguistic correlates of each Big Five trait are as follows:\n\n\nHuman beings often claim to understand events when they manage to formulate a coherent story or narrative explaining how they believe the event was generated. Narratives thus lie at foundations of our cognitive procedures and also provide an explanatory framework for the social sciences, particularly when it is difficult to assemble enough cases to permit statistical analysis. Narrative is often used in case study research in the social sciences. Here it has been found that the dense, contextual, and interpenetrating nature of social forces uncovered by detailed narratives is often more interesting and useful for both social theory and social policy than other forms of social inquiry.\n\nSociologists Jaber F. Gubrium and James A. Holstein have contributed to the formation of a constructionist approach to narrative in sociology. From their book The Self We Live By: Narrative Identity in a Postmodern World (2000), to more recent texts such as Analyzing Narrative Reality (2009)and Varieties of Narrative Analysis (2012), they have developed an analytic framework for researching stories and storytelling that is centered on the interplay of institutional discourses (big stories) on the one hand, and everyday accounts (little stories) on the other. The goal is the sociological understanding of formal and lived texts of experience, featuring the production, practices, and communication of accounts.\n\nIn order to avoid \"hardened stories,\" or \"narratives that become context-free, portable and ready to be used anywhere and anytime for illustrative purposes\" and are being used as conceptual metaphors as defined by linguist George Lakoff, an approach called narrative inquiry was proposed, resting on the epistemological assumption that human beings make sense of random or complex multicausal experience by the imposition of story structures.\" Human propensity to simplify data through a predilection for narratives over complex data sets typically leads to narrative fallacy. It is easier for the human mind to remember and make decisions on the basis of stories with meaning, than to remember strings of data. This is one reason why narratives are so powerful and why many of the classics in the humanities and social sciences are written in the narrative format. But humans read meaning into data and compose stories, even where this is unwarranted. In narrative inquiry, the way to avoid the narrative fallacy is no different from the way to avoid other error in scholarly research, i.e., by applying the usual methodical checks for validity and reliability in how data are collected, analyzed, and presented. Several criteria for assessing the validity of narrative research was proposed, including the objective aspect, the emotional aspect, the social/moral aspect, and the clarity of the story.\n\nIn mathematical sociology, the theory of comparative narratives was devised in order to describe and compare the structures (expressed as \"and\" in a directed graph where multiple causal links incident into a node are conjoined) of action-driven sequential events.\n\nNarratives so conceived comprise the following ingredients:\n\nThe structure (directed graph) is generated by letting the nodes stand for the states and the directed edges represent how the states are changed by specified actions. The action skeleton can then be abstracted, comprising a further digraph where the actions are depicted as nodes and edges take the form \"action a co-determined (in context of other actions) action b\".\n\nNarratives can be both abstracted and generalised by imposing an algebra upon their structures and thence defining homomorphism between the algebras. The insertion of action-driven causal links in a narrative can be achieved using the method of Bayesian narratives.\n\nDeveloped by Peter Abell, the theory of Bayesian Narratives conceives a narrative as a directed graph comprising multiple causal links (social interactions) of the general form: \"action a causes action b in a specified context\". In the absence of sufficient comparative cases to enable statistical treatment of the causal links, items of evidence in support and against a particular causal link are assembled and used to compute the Bayesian likelihood ratio of the link. Subjective causal statements of the form \"I/she did b because of a\" and subjective counterfactuals \"if it had not been for a I/she would not have done b\" are notable items of evidence.\n\nLinearity is one of several narrative qualities that can be found in a musical composition. As noted by American musicologist, Edward Cone, narrative terms are also present in the analytical language about music. The different components of a fugue — subject, answer, exposition, discussion and summary — can be cited as an example. However, there are several views on the concept of narrative in music and the role it plays.\nOne theory is that of Theodore Adorno, who has suggested that ‘music recites itself, is its own context, narrates without narrative’. Another, is that of Carolyn Abbate, who has suggested that ‘certain gestures experienced in music constitute a narrating voice’. Still others have argued that narrative is a semiotic enterprise that can enrich musical analysis.\nThe French musicologist Jean-Jacques Nattiez contends that ‘the narrative, strictly speaking, is not in the music, but in the plot imagined and constructed by the listeners’. He argues that discussing music in terms of narrativity is simply metaphorical and that the ‘imagined plot’ may be influenced by the work's title or other programmatic information provided by the composer. However, Abbate has revealed numerous examples of musical devices that function as narrative voices, by limiting music’s ability to narrate to rare ‘moments that can be identified by their bizarre and disruptive effect’. Various theorists share this view of narrative appearing in disruptive rather than normative moments in music.\nThe final word is yet to be said, regarding narratives in music, as there is still much to be determined.\n\nA narrative can take on the shape of a story, which gives listeners an entertaining and collaborative avenue for acquiring knowledge. Many cultures use storytelling as a way to record histories, myths, and values. These stories can be seen as living entities of narrative among cultural communities, as they carry the shared experience and history of the culture within them. Stories are often used within indigenous cultures in order to share knowledge to the younger generation. Due to indigenous narratives leaving room for open-ended interpretation, native stories often engage children in the storytelling process so that they can make their own meaning and explanations within the story. This promotes holistic thinking among native children, which works towards merging an individual and world identity. Such an identity upholds native epistemology and gives children a sense of belonging as their cultural identity develops through the sharing and passing on of stories.\n\nFor example, a number of indigenous stories are used to illustrate a value or lesson. In the Western Apache tribe, stories can be used to warn of the misfortune that befalls people when they do not follow acceptable behavior. One story speaks to the offense of a mother's meddling in her married son's life. In the story, the Western Apache tribe is under attack from a neighboring tribe, the Pimas. The Apache mother hears a scream. Thinking it is her son's wife screaming, she tries to intervene by yelling at him. This alerts the Pima tribe to her location, and she is promptly killed due to intervening in her son's life.\n\nIndigenous American cultures use storytelling to teach children the values and lessons of life. \nAlthough storytelling provides entertainment, its primary purpose is to educate. Alaskan Indigenous Natives state that narratives teach children where they fit in, what their society expects of them, how to create a peaceful living environment, and to be responsible, worthy members of their communities. In the Mexican culture, many adult figures tell their children stories in order to teach children values such as individuality, obedience, honesty, trust, and compassion. For example, one of the versions of La Llorona is used to teach children to make safe decisions at night and to maintain the morals of the community.\n\nNarratives are considered by the Canadian Métis community, to help children understand that the world around them is interconnected to their lives and communities. For example, the Métis community share the “Humorous Horse Story” to children, which portrays that horses stumble throughout life just like humans do. Navajo stories also use dead animals as metaphors by showing that all things have purpose. Lastly, elders from Alaskan Native communities claim that the use of animals as metaphors allow children to form their own perspectives while at the same time self-reflecting on their own lives.\n\nAmerican Indian elders also state that storytelling invites the listeners, especially children, to draw their own conclusions and perspectives while self-reflecting upon their lives. Furthermore, they insist that narratives help children grasp and obtain a wide range of perspectives that help them interpret their lives in the context of the story. American Indian community members emphasize to children that the method of obtaining knowledge can be found in stories passed down through each generation. Moreover, community members also let the children interpret and build a different perspective of each story.\n\nIn historiography, according to Lawrence Stone, narrative has traditionally been the main rhetorical device used by historians. In 1979, at a time when the new Social History was demanding a social-science model of analysis, Stone detected a move back toward the narrative. Stone defined narrative as organized chronologically; focused on a single coherent story; descriptive rather than analytical; concerned with people not abstract circumstances; and dealing with the particular and specific rather than the collective and statistical. He reported that, \"More and more of the 'new historians' are now trying to discover what was going on inside people's heads in the past, and what it was like to live in the past, questions which inevitably lead back to the use of narrative.\"\n\nSome philosophers identify narratives with a type of explanation. Mark Bevir argues, for example, that narratives explain actions by appealing to the beliefs and desires of actors and by locating webs of beliefs in the context of historical traditions. Narrative is an alternative form of explanation to that associated with natural science.\n\nHistorians committed to a social science approach, however, have criticized the narrowness of narrative and its preference for anecdote over analysis, and clever examples rather than statistical regularities.\n\nStorytelling rights may be broadly defined as the ethics of sharing narratives (including—but not limited to—firsthand, secondhand and imagined stories). In \"Storytelling Rights: The uses of oral and written texts by urban adolescents\", author Amy Shuman offers the following definition of storytelling rights: “the important and precarious relationship between narrative and event and, specifically, between the participants in an event and the reporters who claim the right to talk about what happened.\"\n\nThe ethics of retelling other people’s stories may be explored through a number of questions: whose story is being told and how, what is the story’s purpose or aim, what does the story promise (for instance: empathy, redemption, authenticity, clarification)--and at whose benefit? Storytelling rights also implicates questions of consent, empathy, and accurate representation. While storytelling—and retelling—can function as a powerful tool for agency and advocacy, it can also lead to misunderstanding and exploitation.\n\nStorytelling rights is notably important in the genre of personal experience narrative. Academic disciplines such as performance, folklore, literature, anthropology, Cultural Studies and other social sciences may involve the study of storytelling rights, often hinging on ethics.\n\n\n\n\n"}
{"id": "2786100", "url": "https://en.wikipedia.org/wiki?curid=2786100", "title": "Narrative criticism", "text": "Narrative criticism\n\nNarrative criticism focuses on the stories a speaker or a writer tells to understand how they help us make meaning out of our daily human experiences. Narrative theory is a means by which we can comprehend how we impose order on our experiences and actions by giving them a narrative form. According to Walter Fisher, narratives are fundamental to communication and provide structure for human experience and influence people to share common explanations and understandings (58). Fisher defines narratives as “symbolic actions-words and/or deeds that have sequence and meaning for those who live, create, or interpret them.” Study of narrative criticism, therefore, includes form (fiction or non-fiction, prose or poetry), genre (myth, history, legend, etc.), structure (including plot, theme, irony, foreshadowing, etc.) characterization, and communicator’s perspective.\n\nCharacteristics of a narrative were defined as early as Aristotle in his \"Poetics\" under plot. He called plot as the “first principle” or the “soul of a tragedy.” According to him, plot is the arrangement of incidents that imitate the action with a beginning, middle, and end. Plot includes introduction of characters, rising action and introduction of complication, development of complication, climax (narrative), and final resolution. As described by White (1981) and Martin (1986), plot involves a structure of action. However, not all narratives contain a plot. Fragmentation occurs as the traditional plot disappears, narratives become less linear, and the burden of meaning making gets shifted from the narrator to the reader.\n\nNarratives can be found in a range of practices such as novels, short stories, plays, films, histories, documentaries, gossip, biographies, television and scholarly books. All of these artifacts make excellent objects for narrative criticism. When performing a narrative criticism, critics should focus on the features of the narrative that allow them to say something meaningful about the artifact. Sample questions from Sonja K Foss offer a guide for analysis:\n\n\nE\n"}
{"id": "15044726", "url": "https://en.wikipedia.org/wiki?curid=15044726", "title": "Neo-Aristotelianism", "text": "Neo-Aristotelianism\n\nNeo-Aristotelianism is a view of literature and rhetorical criticism propagated by the Chicago School — Ronald S. Crane, Elder Olson, Richard McKeon, Wayne Booth, and others — which means.\n\"A view of literature and criticism which takes a pluralistic attitude toward the history of literature and seeks to view literary works and critical theories intrinsically\" \n\nNeo-Artistotelianism was one of the first rhetorical methods of criticism. Its central features were first suggested in Herbert A. Wichelns' \"The Literary Criticism of Oratory\" in 1925. It focused on analyzing the methodology behind a speech's ability to convey an idea to its audience. In 1943, Neo-Aristotelianism was further publicized, gaining popularity after William Norwood Brigance published A History and Criticism of American Public Address.\n\nUnlike rhetorical criticism, which concentrates on the study of speeches and the immediate effect of rhetoric on an audience, Neo-Aristotelianism \"led to the study of a single speaker because the sheer number of topics to cover relating to the rhetor and the speech made dealing with more than a single speaker virtually impossible. Thus, various speeches by different rhetors related by form of topic were not included in the scope of rhetorical criticism.\"\n\nWichelns' work was one of the first that introduced Neo-Aristotelianism. It narrowed down speech to 12 key topics to be studied, similar to many of the topics discussed by Aristotle in the Rhetoric. His topics for speech critique include:\nAccording to Mark S. Klyn, author of Towards a Pluralistic Rhetorical Criticism, \"The Literary Criticism of Oratory\" provided \"substance and structure to a study which heretofore had been formless and ephemeral [...] it literally created the modern discipline of rhetorical criticism.\" Thus regardless of the lack of detail on these topics, it provided a modern structure of critiquing and analyzing speech via Neo-Aristotelianism, according to Donald C. Bryant.\n\n"}
{"id": "35953231", "url": "https://en.wikipedia.org/wiki?curid=35953231", "title": "Outline of books", "text": "Outline of books\n\nThe following outline is provided as an overview of and topical guide to books:\nBook – set of written, printed, illustrated, or blank sheets, made of ink, paper, parchment, or other materials, usually fastened together to hinge at one side.\n\nA book is a medium for a collection of words and/or pictures to represent knowledge, often manifested in bound paper and ink, or in electronic format as e-books.\n\nBooks can be described as all of the following:\n\nPhysical types of books not to be confused with literary genres or types of literature.\n\nKorean book-Jikji-Selected Teachings of Buddhist Sages and Seon Masters-1377]]\n\n\nBook design – the common structural parts of a book include:\n\n\n\n\nHistory of books\n\n\n\n\n\n\nLists of books – list of book lists (bibliographies) on Wikipedia \n\n\nNew York Review of Books – an American magazine containing lietary criticism, and discussions of the contents of various books.\n\n\n\n"}
{"id": "28350858", "url": "https://en.wikipedia.org/wiki?curid=28350858", "title": "Paleofeces", "text": "Paleofeces\n\nPaleofeces (UK: Palaeofaeces) are ancient human feces, often found as part of archaeological excavations or surveys. The term coprolite is often used interchangeably. Intact feces of ancient people may be found in caves in arid climates and in other locations with suitable preservation conditions. They are studied to determine the diet and health of the people who produced them through the analysis of seeds, small bones, and parasite eggs found inside. The feces can contain information about the person excreting the material as well as information about the material itself. They can also be chemically analyzed for more in-depth information on the individual who excreted them, using lipid analysis and ancient DNA analysis. The success rate of usable DNA extraction is relatively high in paleofeces, making it more reliable than skeletal DNA retrieval.\nThe reason this analysis is possible at all is due to the digestive system not being entirely efficient, in the sense that not everything that passes through the digestive system is destroyed. Not all of the surviving material is recognizable, but some of it is. This material is generally the best indicator archaeologists can use to determine ancient diets, as no other part of the archaeological record is as direct an indicator.\n\nThe process that preserves the feces in a way such that they can be analyzed later is called the Maillard reaction. This reaction creates a casing of sugar that preserves the feces from the elements. To extract and analyze the information contained within, researchers generally have to freeze the feces and grind it up into powder for analysis.\n\nAnalysis of archaeological feces has a relatively short history compared to many other archaeological materials. The founder of the discipline is Dr. Eric O. Callen, who pioneered the subject in the late 1950s to mid-1960s. His early papers used coprolite analysis to investigate early Mexican diets, published in The Prehistory of the Tehuacan Valley: Environment, and Subsistence. Despite his work showing promise, archaeological coprolite studies remained a niche topic, with few other researchers becoming involved. After Callen's sudden death in 1970, his work was continued by Vaughn Bryant at Texas A&M University, Department of Anthropology. Coprolite analysis gradually became a topic of serious study. Today coprolite analysis in archaeology has increased exponentially, and they have provided important evidence concerning the evolution of human health and diet, in the Americas and other parts of the world. One of the most famous examples is the coprolite from Paisley Caves, Oregon, which has provided some of the earliest evidence for the human occupation of North America.\n\nA wide variety of methods can be used to analyse ancient feces, ranging from microscopic to molecular. At a basic level the analysis of size and morphology can provide some information on whether they are likely to be human or from another animal. Analyzed contents can include those visible to the naked eye, such as seeds and other plant remains—to the microscopic, including pollen and phytoliths. Parasites in coprolites can give information on the living conditions and health of ancient populations. At the molecular level, ancient DNA analysis can be used both to identify the species and to provide dietary information. A method using lipid analysis can also be used for species identification, based on the range of fecal sterols and bile acids. These molecules vary between species according to gut biochemistry, and so can distinguish between humans and other animals.\n\nAn example of researchers using paleofeces for the gathering of information using DNA analysis occurred at Hinds Cave in Texas by Hendrik Poinar and his team. The fecal samples obtained were over 2,000 years old. From the samples, Poinar was able to gather DNA samples using the analysis methods recounted above. From his research Poinar found that the feces belonged to three Native Americans, based on mtDNA similarities to present day Native Americans. Poinar also found DNA evidence of the food they ate. There were samples of buckthorn, acorns, ocotillo, nightshade and wild tobacco. No visible remnants of these plants were visible in the fecal matter. Along with plant material, there were also DNA sequences of animal species such as bighorn sheep and pronghorn antelope, both of which lack any evidence elsewhere in Hinds Cave, indicating that they were killed and consumed elsewhere.\n\nThis analysis of the diet was very helpful. Previously it was assumed that this population of Native Americans survived with berries being their main source of nutrients. From the paleofeces, it was determined that these assumptions were incorrect and in the approximately 2 days of food that are represented in a fecal sample, 2-4 animal species and 4-8 plant species were represented. The nutritional diversity of this archaic human population was rather extraordinary.\n\nAn example of the use of lipid analysis for identification of species is at the Neolithic site of Catalhoyuk in Turkey. Large midden deposits at the site are frequently found to contain fecal material either as distinct coprolites or compressed 'cess pit' deposits. This was initially thought to be from dog on the basis of digested bone, however an analysis of the lipid profiles showed that many of the coprolites were actually from humans.\n\nThe analysis of parasites from fecal material within cesspits has provided evidence for health and migration in past populations. For example, the identification of fish tapeworm eggs in Acre in the Crusader period indicate that this parasite was transported from northern Europe. The parasite was rarely seen in the Levant area during this time but was common in Northern Europe. It is suggested that it was brought to the region by the incoming Europeans.\n\n"}
{"id": "3091094", "url": "https://en.wikipedia.org/wiki?curid=3091094", "title": "Phantom time hypothesis", "text": "Phantom time hypothesis\n\nThe phantom time hypothesis is a historical conspiracy theory asserted by Heribert Illig. First published in 1991, it hypothesizes a conspiracy by the Holy Roman Emperor Otto III, Pope Sylvester II, and possibly the Byzantine Emperor Constantine VII, to fabricate the Anno Domini dating system retrospectively, in order to place them at the special year of AD 1000, and to rewrite history to legitimize Otto's claim to the Holy Roman Empire. Illig believed that this was achieved through the alteration, misrepresentation and forgery of documentary and physical evidence. According to this scenario, the entire Carolingian period, including the figure of Charlemagne, is a fabrication, with a \"phantom time\" of 297 years (AD 614–911) added to the Early Middle Ages. The proposal has been universally rejected by mainstream historians.\n\nIllig was born in 1947 in Vohenstrauß, Bavaria.\nHe was active in an association dedicated to Immanuel Velikovsky, catastrophism and historical revisionism,\n\"Gesellschaft zur Rekonstruktion der Menschheits- und Naturgeschichte\".\nFrom 1989 to 1994 he acted as editor of the journal \"Vorzeit-Frühzeit-Gegenwart\".\nSince 1995, he has worked as a publisher and author under his own publishing company, \"Mantis-Verlag\", and publishing his own journal, \"Zeitensprünge\".\nOutside of his publications related to revised chronology, he has edited the works of Egon Friedell.\n\nBefore focusing on the early medieval period, Illig published various proposals for revised chronologies of prehistory and of Ancient Egypt.\nHis proposals received prominent coverage in German popular media in the 1990s. His 1996 \"Das erfundene Mittelalter\" also received scholarly recensions, but was universally rejected as fundamentally flawed by historians.\nIn 1997, the journal \"Ethik und Sozialwissenschaften\" offered a platform for critical discussion to Illig's proposal, with a number of historians commenting on its various aspects.\nAfter 1997, there has been little scholarly reception of Illig's ideas, although they continued to be discussed as pseudohistory in German popular media.\nIllig continued to publish on the \"phantom time hypothesis\" until at least 2013.\nAlso in 2013, he published on an unrelated topic of art history, on German Renaissance master Anton Pilgram, but again proposing revisions to conventional chronology, and arguing for the abolition of the art historical category of Mannerism.\n\nThe bases of Illig's hypothesis include:\n\n\nPublications by Illig:\n\n\n\n"}
{"id": "43714648", "url": "https://en.wikipedia.org/wiki?curid=43714648", "title": "Phum Snay", "text": "Phum Snay\n\nPhum Snay () is an Iron Age archaeological site discovered in May 2000 in Preah Neat Prey District, Banteay Meanchey Province, Northwest Cambodia, around from the temple ruins of Angkor. The site was excavated between 2001 and 2003 by primary excavators Dougald O’Reilly of the Australian National University, Pheng Sitha and Thuy Chanthourn. The excavation was intended to discover more about Iron Age life in Cambodia.\n\nThe site was discovered in 2000 during roadworks, that linked Phum Snay village with National Road 6. Investigations revealed the presence of a number of ancient burials associated with material including bronzes and semiprecious stones. Immediate widespread looting by the local inhabitants over an area of made a full assessment of the extent of the archaeological remains difficult. Excavations started in February 2001 on a 15 x 5 m area.\n\nThe site dates roughly between 500 BC and AD 500. These dates were not obtained from stratigraphy as many of the artifacts and burials were disturbed by looting. Chronology was therefore determined by comparison to other nearby sites with similar grave goods and mortuary rituals.\n\nA general analysis was done of the 134 individuals identified through the minimum number of individuals method. Twenty-one burials in total were exhumed that contained human remains. The researchers noted pathological lesions in photos and written descriptions. A few specimens were x-rayed but the images yielded limited results. The majority of the injuries listed were cranial lesions. These were categorized into two groups: those caused by sharp force trauma (henceforth referred to SFT) which left small focused circles or lines and those caused by blunt force trauma (BFT,) resulting in round depressions in the scalp. SFT lesions are generally caused by sharp edged weapons, such as knives, swords and axes. Projectiles, such as arrows or those used by a slingshot should also be considered as SFT objects or items. BFT lesions result from clubs, cudgels and hammers. Perimortem injuries were identified as separate from healed injuries because they often had bone flakes adhered to the edges of the fractures. Researchers were careful to distinguish between lesions caused by violence and those caused by infectious disease.\n\nOut of the sample discovered at Phum Snay, 23.4% of the individuals showed signs of traumatic lesions, a number that is much higher than any other site in Southeast Asia. Both BFT and SFT were present within the skeletal remains, but there was a much higher rate of healed lesions than perimortem lesions. Some individuals had more than one lesion present upon the cranium. Within a sample of fifteen individuals, seventeen BFT lesions could be seen. These BFT lesions were rounded in shape, ranging in diameter from roughly 4 to 22 mm. Estimations suggest that injuries acquired by the individuals at the site were much higher than the remains can represent because a majority of assaults probably caused soft-tissue injuries, which would not be preserved in the archaeological record.\n\nWithin the Phum Snay site, two types of burials were found. These include bodies that were stretched out lying flat and pits where remains were simply in piles. The burials contained a variety of grave goods, including ceramic vessels, bronze ornaments (bangles, rings, and bells), and a large quantity of animal bones. The proportion of left forelimbs of hooved animals suggest a very particular sacrificial practice of animals within the burials. Researchers found that the manner of the burials and the goods contained within were closest in relation to the Óc Eo culture of the Mekong Delta.\n\nAnother major find within Phum Snay was the high percentage of burials containing weapons. The graves contained swords, daggers, spearheads, and projectile points. Due to the context of burials with high rates of traumatic lesions, the presence of these weapons indicates possible military formation, and certainly an increased level of violence than is seen in other sites within Southeast Asia. The weapons and tools found within the site were made of iron with bronze adornments. This solidified the site's place within the Iron Age because iron weapons and tools had many advantages over bronze, such as the high abundance of iron within the Earth, which made the spread of iron very rapid. Also present in the burials of a number of young adult males were ornamental shoulder decorations, which further support the idea of a violence-oriented society. The lack of blunt objects within the burials is slightly puzzling due to the higher rate of BFT lesions than SFT lesions, but this could be explained if the individuals at Phum Snay wore helmets for protection.\n\nThe health of the people living at Phum Snay was also determined from the burials. Rates of attrition, caries, and abscesses in the teeth of the human remains gave an idea of their dietary habits. The main result from the dentition was the presence of a social structure regarding male and female roles within the community. Rates of dental caries in females were higher than the males, which may be attributed to a sexual division of labor. If the men are out hunting, they are receiving a higher level of protein in the food they are consuming, while the females may be eating more cariogenic foods if they are back tending the fields and surrounded by starches and carbohydrates.\n\nMeasurements were taken on the cranial and dental morphologies of the individuals found within Phum Snay to compare them with those of modern Cambodia, Vietnam, Thailand, and Laos. The morphological similarities were found from Q-mode correlation coefficients of cranial and dental metrics. The findings indicated that the morphology of individuals at Phum Snay were significantly different than any of the other groups being compared. Overall, the modern groups have less robusticity than is found in the Phum Snay morphologies.\n\nThe high rates of traumatic lesions and weapons within the burials at Phum Snay suggest a society that values warriors. Fighting was an important part of life to the people, which could be indicative of increasing competition over access to exotic exchange items during the Iron Age. Furthering this idea is the general location of Phum Snay, right outside the Angkorian capital. Increasing competition over resources could have been the driving force behind the formation of a hierarchical society and organized state.\nThe differences in morphological features between the human remains at Phum Snay and other modern Southeast Asian areas lends itself to the two-layer hypothesis. The people at Phum Snay were less affected by the substantial gene flow received by other areas of Southeast Asia.\n\n\n\n"}
{"id": "27702479", "url": "https://en.wikipedia.org/wiki?curid=27702479", "title": "Pizza effect", "text": "Pizza effect\n\nThe pizza effect is a term used especially in religious studies and sociology for the phenomenon of elements of a nation or people's culture being transformed or at least more fully embraced elsewhere, then re-imported back to their culture of origin, or the way in which a community's self-understanding is influenced by (or imposed by, or imported from) foreign sources. It is named after the idea that modern pizza toppings were developed among Italian immigrants in the United States (rather than in native Italy, where in its simpler form it was originally looked down upon), and was later exported back to Italy to be interpreted as a delicacy in Italian cuisine.\n\nRelated phrases include \"hermeneutical feedback loop\", \"re-enculturation\", and \"self-orientalization\". The term \"pizza effect\" was coined by the Austrian-born Hindu monk and professor of Anthropology at Syracuse University, Agehananda Bharati in 1970.\n\nThe original examples given by Agehananda Bharati mostly had to do with popularity and status:\n\nAnalyst Mark Sedgwick wrote that Islamist terrorism, and specifically suicide bombing, can be seen as examples, beginning as isolated interpretations of the concept of shahid, or martyrdom, then being re-exported to the greater Muslim world.\n\nThe Day of the Dead parade in Mexico City was inspired by an event in the James Bond film \"Spectre\", which was fictional at the time of the film's production. \n\nThe founders of the Theosophical Society, Helena Blavatsky and Henry Steel Olcott, were influenced by Eastern religions, then placed their headquarters in Adyar, Chennai, from where they spread their views within India.\n\nSimilarly, Buddhist modernism or \"Protestant Buddhism\" was developed by Westerners, who according to scholar Stephen Jenkins, \"mistook it for an indigenous Sri Lankan product\", and they in turn influenced Sri Lankan Buddhist Anagarika Dharmapala, who, along with the Theosophical Society, was instrumental in spreading Buddhism in both India and the West.\n\nAccording to scholar Kim Knott, Mahatma Gandhi \"was not very interested in religion until he went to London to study law, where he studied the Bhagavad Gita in English in Sir Edwin Arnold's translation, and this deeply influenced his spiritual outlook.\"\n\nThe influence of translations by the British-based Pali Text Society on South Asian Buddhism.\n\nThe religious thought of Ibn Rushd (Averroes), which was taken up by 19th-century Europeans such as Ernest Renan, and thereby regained popularity during the Nahda, the Islamic renaissance.\n\nChicken tikka masala, a dish created in Britain, based on Indian cooking, which then became popular in India.\n\nTeppanyaki, a Western-influenced style created in Japan, popular in the U.S.\n\nSalsa music: the first salsa bands were mainly Puerto Ricans who moved to New York in the 1930s.\n\nHaoqiu zhuan, a Chinese novel. James St. André, author of \"Modern Translation Theory and Past Translation Practice: European Translations of the \"Haoqiu zhuan\"\", wrote that in China the novel was originally \"considered second-rate fiction and stood in danger of being completely forgotten with changes in literary taste in the early twentieth century.\" He stated that the fact there had been interest in translating the novel into English \"gave life and fame\" to \"Haoqiu zhuan\" and therefore affected its standing in China.\n\nScholar David Miller wrote that Westerners were responsible for \"…the renewed interest in the four Vedas and the Upanishads, as texts in themselves apart from the endless number of commentaries that have been written by Indians to interpret and to systematize the texts,\" and that due to this interest, \"Indian scholars have also served up that menu, often in a less appetizing way than their Western counterparts. In so doing they have missed the very life force or essence of Indian ethical traditions.\"\n\nScholar Jørn Borup wrote about an \"inverted pizza-effect\", when a society's modification of another culture gets further re-modified by that same society, such as European philosophers including Martin Heidegger \"appear to have been significantly inspired by Eastern thought - an Eastern thought itself presented through \"Protestant\" or \"Western\" eyes. This transformation is naturally not a unique phenomenon in religious studies, where interpretations, re-interpretations and inventions are seen as common characteristics of religion.\"\n\nStephen Jenkins noted that the feedback phenomenon could continue; in the case of pizza, he wrote that the return of pizza to Italy again influenced American cuisine: \"...pizza-loving American tourists, going to Italy in the millions, sought out authentic Italian pizza. Italians, responding to this demand, developed pizzerias to meet American expectations. Delighted with their discovery of \"authentic\" Italian pizza, Americans subsequently developed chains of \"authentic\" Italian brick-oven pizzerias. Hence, Americans met their own reflection in the other and were delighted.\"\n\nJim Douglas, familiar with Bharati's thesis, applied it to black blues originating in the United States before 1960. The music of Robert Johnson, Muddy Waters, etc. went over to England, where it was embraced by other musicians (especially white men playing electric guitar). Then, this re-packaged blues came back to the US presented by the Rolling Stones, Cream, Led Zeppelin, etc. in the late 1960s where it was embraced by baby boomers (who had never heard of Robert Johnson, etc.). Later, some of these American baby boomers discovered the roots of the British blues-rock in the recordings of the original American blues artists.\n\n"}
{"id": "218879", "url": "https://en.wikipedia.org/wiki?curid=218879", "title": "Pleonasm", "text": "Pleonasm\n\nPleonasm (; , ) is the use of more words or parts of words than are necessary or sufficient for clear expression: for example \"black darkness\" or \"burning fire\". Such redundancy is, by traditional rhetorical criteria, a manifestation of tautology. However, pleonasm may also be used for emphasis, or because the phrase has already become established in a certain form.\n\nMost often, \"pleonasm\" is understood to mean a word or phrase which is useless, clichéd, or repetitive, but a pleonasm can also be simply an unremarkable use of idiom. It can aid in achieving a specific linguistic effect, be it social, poetic, or literary. In particular, pleonasm sometimes serves the same function as rhetorical repetition—it can be used to reinforce an idea, contention, or question, rendering writing clearer and easier to understand. Further, pleonasm can serve as a redundancy check: If a word is unknown, misunderstood, or misheard, or the medium of communication is poor—a wireless telephone connection or sloppy handwriting—pleonastic phrases can help ensure that the entire meaning gets across even if some of the words get lost.\n\nSome pleonastic phrases are part of a language's idiom, like \"tuna fish\" and \"safe haven\" in American English. They are so common that their use is unremarkable and often even unnoticeable for native speakers, although in many cases the redundancy can be dropped with no loss of meaning.\n\nWhen expressing possibility, English speakers often use potentially pleonastic expressions such as \"It may be possible\" or \"maybe it's possible\", where both terms (verb \"may\" / adverb \"maybe\" and adjective \"possible\") have the same meaning under certain constructions. Many speakers of English use such expressions for possibility in general, such that most instances of such expressions by those speakers are in fact pleonastic. Others, however, use this expression only to indicate a distinction between ontological possibility and epistemic possibility, as in \"Both the ontological possibility of X under current conditions and the ontological impossibility of X under current conditions are epistemically possible\" (in logical terms, \"I am not aware of any facts inconsistent with the truth of proposition X, but I am likewise not aware of any facts inconsistent with the truth of the negation of X\"). The habitual use of the double construction to indicate possibility \"per se\" is far less widespread among speakers of most other languages (except in Spanish; see examples); rather, almost all speakers of those languages use one term in a single expression:\n\nIn a satellite-framed language like English, verb phrases containing particles that denote direction of motion are so frequent that even when such a particle is pleonastic, it seems natural to include it (e.g. \"enter into\").\n\nSome pleonastic phrases, when used in professional or scholarly writing, may reflect a standardized usage that has evolved or a meaning familiar to specialists but not necessarily to those outside that discipline. Such examples as \"null and void\", \"terms and conditions\", \"each and every\" are legal doublets that are part of legally operative language that is often drafted into legal documents. A classic example of such usage was that by the Lord Chancellor at the time (1864), Lord Westbury, in the English case of \" Gorely\", when he described a phrase in an Act as \"redundant and pleonastic\". Although this type of usage may be favored in certain contexts, it may also be disfavored when used gratuitously to portray false erudition, obfuscate, or otherwise introduce verbiage. This is especially so in disciplines where imprecision may introduce ambiguities (such as the natural sciences).\n\nIn addition, pleonasms can serve purposes external to meaning. For example, a speaker who is too terse is often interpreted as lacking ease or grace, because, in oral and sign language, sentences are spontaneously created without the benefit of editing. The restriction on the ability to plan often creates much redundancy. In written language, removing words not strictly necessary sometimes makes writing seem stilted or awkward, especially if the words are cut from an idiomatic expression.\n\nOn the other hand, as is the case with any literary or rhetorical effect, excessive use of pleonasm weakens writing and speech; words distract from the content. Writers wanting to conceal a thought or a purpose obscure their meaning with verbiage. William Strunk Jr. advocated concision in \"The Elements of Style\" (1918):\n\nYet, one has only to look at Baroque, Mannerist, and Victorian sources for different opinions.\n\n\nThere are two kinds of pleonasm: syntactic pleonasm and semantic pleonasm.\n\nSyntactic pleonasm occurs when the grammar of a language makes certain function words optional. For example, consider the following English sentences:\n\n\nIn this construction, the conjunction \"that\" is optional when joining a sentence to a verb phrase with \"know\". Both sentences are grammatically correct, but the word \"that\" is pleonastic in this case. By contrast, when a sentence is in spoken form and the verb involved is one of assertion, the use of \"that\" makes clear that the present speaker is making an indirect rather than a direct quotation, such that he is not imputing particular words to the person he describes as having made an assertion; the demonstrative adjective \"that\" also does not fit such an example. Also, some writers may use \"that\" for technical clarity reasons. In some languages, such as French, the word is not optional and should therefore not be considered pleonastic.\n\nThe same phenomenon occurs in Spanish with subject pronouns. Since Spanish is a null-subject language, which allows subject pronouns to be deleted when understood, the following sentences mean the same:\n\nIn this case, the pronoun ('I') is grammatically optional; both sentences mean \"I love you\" (however, they may not have the same tone or \"intention\"—this depends on pragmatics rather than grammar). Such differing but syntactically equivalent constructions, in many languages, may also indicate a difference in register.\n\nThe process of deleting pronouns is called \"pro-dropping\", and it also happens in many other languages, such as Korean, Japanese, Hungarian, Latin, Portuguese, Scandinavian languages, Swahili, some Slavic languages, and the Lao language.\n\nIn contrast, formal English requires an overt subject in each clause. A sentence may not need a subject to have valid meaning, but to satisfy the syntactic requirement for an explicit subject a pleonastic (or dummy pronoun) is used; only the first sentence in the following pair is acceptable English:\n\n\nIn this example the pleonastic \"it\" fills the subject function, however, it does not contribute any meaning to the sentence. The second sentence, which omits the pleonastic it is marked as ungrammatical although no meaning is lost by the omission. Elements such as \"it\", or \"there\" serving as empty subject markers are also called (syntactic) expletives, and also dummy pronouns. Compare:\n\n\nThe pleonastic (), expressing uncertainty in formal French, works as follows:\n\nTwo more striking examples of French pleonastic construction are the word translated as 'today', but originally meaning \"on the day of today\", and the phrase meaning 'What's that?' or 'What is it?', while literally it means \"What is it that it is?\".\n\nThere are examples of the pleonastic, or dummy, negative in English, such as the construction, heard in the New England region of the United States, in which the phrase \"So don't I\" is intended to have the same positive meaning as \"So do I.\"\n\nWhen Robert South said, \"It is a pleonasm, a figure usual in Scripture, by a multiplicity of expressions to signify one notable thing\", he was observing the Biblical Hebrew poetic propensity to repeat thoughts in different words, since written Biblical Hebrew was a comparatively early form of written language and was written using oral patterning, which has many pleonasms. In particular, very many verses of the Psalms are split into two halves, each of which says much the same thing in different words. The complex rules and forms of written language as distinct from spoken language were not as well-developed as they are today when the books making up the Old Testament were written. See also parallelism (rhetoric).\n\nThis same pleonastic style remains very common in modern poetry and songwriting (e.g., \"Anne, with her father / is out in the boat / riding the water / riding the waves / on the sea\", from Peter Gabriel's \"Mercy Street\").\n\n\nSemantic pleonasm is a question more of style and usage than of grammar. Linguists usually call this \"redundancy\" to avoid confusion with syntactic pleonasm, a more important phenomenon for theoretical linguistics. It usually takes one of two forms: Overlap or prolixity.\n\nOverlap: One word's semantic component is subsumed by the other:\n\nProlixity: A phrase may have words which add nothing, or nothing logical or relevant, to the meaning.\n\nAn expression like \"tuna fish\", however, might elicit one of many possible responses, such as:\n\nCareful speakers and writers to are aware of pleonasms, especially with cases such as \"tuna fish\", which is normally used only in some dialects of American English, and would sound strange in other variants of the language, and even odder in translation into other languages.\n\nSimilar situations are:\n\nNot all constructions that are typically pleonasms are so in all cases, nor are all constructions derived from pleonasms themselves pleonastic:\n\nMorphemes, not just words, can enter the realm of pleonasm: Some word-parts are simply optional in various languages and dialects. A familiar example to American English speakers would be the allegedly optional \"-al-\", probably most commonly seen in \"publically\" vs. \"publicly\"—both spellings are considered correct/acceptable in American English, and both pronounced the same, in this dialect, rendering the \"publically\" spelling pleonastic in US English; in other dialects it is \"required\", while it is quite conceivable that in another generation or so of American English it will be \"forbidden\". This treatment of words ending in \"-ic\", \"-ac\", etc., is quite inconsistent in US English—compare \"maniacally\" or \"forensically\" with \"stoicly\" or \"heroicly\"; \"forensicly\" doesn't look \"right\" in any dialect, but \"heroically\" looks internally redundant to many Americans. (Likewise, there are thousands of mostly American Google search results for \"eroticly\", some in reputable publications, but it does not even appear in the 23-volume, 23,000-page, 500,000-definition \"Oxford English Dictionary\" (\"OED\"), the largest in the world; and even American dictionaries give the correct spelling as \"erotically\".) In a more modern pair of words, Institute of Electrical and Electronics Engineers dictionaries say that \"electric\" and \"electrical\" mean exactly the same thing. However, the usual adverb form is \"electrically\". (For example, \"The glass rod is electrically charged by rubbing it with silk\".)\n\nSome (mostly US-based) prescriptive grammar pundits would say that the \"-ly\" not \"-ally\" form is \"correct\" in any case in which there is no \"-ical\" variant of the basic word, and vice versa; i.e. \"maniacally\", not \"maniacly\", is correct because \"maniacal\" is a word, while \"publicly\", not \"publically\", must be correct because \"publical\" is (arguably) not a real word (it does not appear in the \"OED\"). This logic is in doubt, since most if not all \"-ical\" constructions arguably are \"real\" words and most have certainly occurred more than once in \"reputable\" publications, and are also immediately understood by any educated reader of English even if they \"look funny\" to some, or do not appear in popular dictionaries. Additionally, there are numerous examples of words that have very widely accepted extended forms that have skipped one or more intermediary forms, e.g. \"disestablishmentarian\" in the absence of \"disestablishmentary\" (which does not appear in the \"OED\"). At any rate, while some US editors might consider \"-ally\" vs. \"-ly\" to be pleonastic in some cases, the majority of other English speakers would not, and many \"-ally\" words are not pleonastic to anyone, even in American English.\nThe most common definitely pleonastic morphological usage in English is \"irregardless\", which is very widely criticized as being a non-word. The standard usage is \"regardless\", which is already negative; adding the additional negative \"ir-\" is interpreted by some as logically reversing the meaning to \"with regard to/for\", which is certainly not what the speaker intended to convey. (According to most dictionaries that include it, \"irregardless\" appears to derive from confusion between \"regardless\" and \"irrespective\", which have overlapping meanings.)\n\nThere are several instances in Chinese vocabulary where pleonasms and cognate objects are present. Their presence usually indicate the plural form of the noun or the noun in formal context.\n\nIn some instances, the pleonasmic form of the verb is used with the intention as an emphasis to one meaning of the verb, isolating them from their idiomatic and figurative uses. But over time, the pseudo-object, which sometimes repeats the verb, is almost inherently coupled with the it.\n\nFor example, the word ('to sleep') is an intransitive verb, but may express different meaning when coupled with objects of prepositions as in \"to sleep with\". However, in Mandarin, is usually coupled with a pseudo-character , yet it is not entirely a cognate object, to express the act of resting.\n\n\nOne can also find a way around this verb, using another one which does not is used to express idiomatic expressions nor necessitate a pleonasm, because it only has one meaning:\nNevertheless, is a verb used in high-register diction, just like English verbs with Latin roots.\n\nThere is no relationship found between Chinese and English regarding verbs that can take pleonasms and cognate objects. Although the verb \"to sleep\" may take a cognate object as in \"sleep a restful sleep\", it is a pure coincidence, since verbs of this form are more common in Chinese than in English; and when the English verb is used without the cognate objects, its diction is natural and its meaning is clear in every level of diction, as in \"I want to sleep\" and \"I want to have a rest\".\n\nIn some cases, the redundancy in meaning occurs at a syntactic level above the word, such as at the phrase level:\n\nThe redundancy of these two well-known statements is deliberate, for humorous effect. (See Yogi_Berra#Yogi-isms.) But one does hear educated people say \"my predictions about the future of politics\" for \"my predictions about politics\", which are equivalent in meaning. While predictions are necessarily about the future (at least in relation to the time the prediction was made), the nature of this future can be subtle (e.g., \"I predict that he died a week ago\"—the prediction is about future discovery or proof of the date of death, not about the death itself). Generally \"the future\" is assumed, making most constructions of this sort pleonastic. The latter humorous quote above about not making predictions – by Yogi Berra – is not really a pleonasm, but rather an ironic play on words.\n\nBut \"It's \"déjà vu\" all over again.\" could mean that there was earlier another \"déjà vu\" of the same event or idea, which has now arisen for a third time.\n\nRedundancy, and \"useless\" or \"nonsensical\" words (or phrases, or morphemes) can also be inherited by one language from the influence of another, and are not pleonasms in the more critical sense, but actual changes in grammatical construction considered to be required for \"proper\" usage in the language or dialect in question. Irish English, for example, is prone to a number of constructions that non-Irish speakers find strange and sometimes directly confusing or silly:\n\n\nAll of these constructions originate from the application of Irish Gaelic grammatical rules to the English dialect spoken, in varying particular forms, throughout the island.\n\nSeemingly \"useless\" additions and substitutions must be contrasted with similar constructions that are used for stress, humor, or other intentional purposes, such as:\n\n\nThe latter of these is a result of Yiddish influences on modern English, especially East Coast US English.\n\nSometimes editors and grammatical stylists will use \"pleonasm\" to describe simple wordiness. This phenomenon is also called prolixity or logorrhea. Compare:\n\nor even:\n\nThe reader or hearer does not have to be told that loud music has a sound, and in a newspaper headline or other abbreviated prose can even be counted upon to infer that \"burglary\" is a proxy for \"sound of the burglary\" and that the music necessarily must have been loud to drown it out, unless the burglary was relatively quiet (this is not a trivial issue, as it may affect the legal culpability of the person who played the music); the word \"loud\" may imply that the music should have been played quietly if at all. Many are critical of the excessively abbreviated constructions of \"headline-itis\" or \"newsspeak\", so \"loud [music]\" and \"sound of the [burglary]\" in the above example should probably not be properly regarded as pleonastic or otherwise genuinely redundant, but simply as informative and clarifying.\n\nProlixity is also used simply to obfuscate, confuse, or euphemize and is not necessarily redundant or pleonastic in such constructions, though it often is. \"Post-traumatic stress disorder\" (shell shock) and \"pre-owned vehicle\" (used car) are both tumid euphemisms but are not redundant. Redundant forms, however, are especially common in business, political, and even academic language that is intended to sound impressive (or to be vague so as to make it hard to determine what is actually being promised, or otherwise misleading). For example: \"This quarter, we are presently focusing with determination on an all-new, innovative integrated methodology and framework for rapid expansion of customer-oriented external programs designed and developed to bring the company's consumer-first paradigm into the marketplace as quickly as possible.\"\n\nIn contrast to redundancy, an oxymoron results when two seemingly contradictory words are adjoined.\n\nRedundancies sometimes take the form of foreign words whose meaning is repeated in the context:\n\nThese sentences use phrases which mean, respectively, \"the restaurant restaurant\", \"the tar tar\", \"with juice sauce\" and so on. However, many times these redundancies are necessary—especially when the foreign words make up a proper noun as opposed to a common one. For example, \"We went to Il Ristorante\" is acceptable provided the audience can infer that it is a restaurant (if they understand Italian and English it might likely, if spoken rather than written, be misinterpreted as a generic reference and not a proper noun, leading the hearer to ask \"Which ristorante do you mean?\" Such confusions are common in richly bilingual areas like Montreal or the American Southwest when people mix phrases from two languages at once). But avoiding the redundancy of the Spanish phrase in the second example would only leave an awkward alternative: \"La Brea pits are fascinating\".\n\nMost find it best to not even drop articles when using proper nouns made from foreign languages:\n\n\nThis is also similar to the treatment of definite and indefinite articles in titles of books, films, etc. where the article can—some would say \"must\"—be present where it would otherwise be \"forbidden\":\n\n\nSome cross-linguistic redundancies, especially in placenames, occur because a word in one language became the title of a place in another (e.g., the Sahara Desert—\"Sahara\" is an English approximation of the word for \"deserts\" in Arabic). A supposed extreme example is Torpenhow Hill in Cumbria, if etymologized as meaning \"hill\" in the language of each of the cultures that have lived in the area during recorded history, could be translated as \"Hillhillhill Hill\". See the List of tautological place names for many more examples.\n\nAcronyms can also form the basis for redundancies; this is known humorously as RAS syndrome (for Redundant Acronym Syndrome Syndrome):\n\n\nIn all the examples listed above, the word after the acronym repeats a word represented in the acronym—respectively, \"Personal Identification Number number\", \"Automated Teller Machine machine\", \"Random Access Memory memory\", \"Human Immunodeficiency Virus virus\", \"Content Management System system\". (See RAS syndrome for many more examples.) The expansion of an acronym like PIN or HIV may be well known to English speakers, but the acronyms themselves have come to be treated as words, so little thought is given to what their expansion is (and \"PIN\" is also pronounced the same as the word \"pin\"; disambiguation is probably the source of \"PIN number\"; \"SIN number\" for \"Social Insurance Number number\" is a similar common phrase in Canada.) But redundant acronyms are more common with technical (e.g. computer) terms where well-informed speakers recognize the redundancy and consider it silly or ignorant, but mainstream users might not, since they may not be aware or certain of the full expansion of an acronym like \"RAM\".\n\nSome redundancies are simply typographical. For instance, when a short inflexional word like \"the\" occurs at the end of a line, it is very common to accidentally repeat it at the beginning of the line, and a large number of readers would not even notice it.\n\nCarefully constructed expressions, especially in poetry and political language, but also some general usages in everyday speech, may appear to be redundant but are not. This is most common with cognate objects (a verb's object that is cognate with the verb):\n\nOr, a classic example from Latin:\n\nThe words need not be etymologically related, but simply conceptually, to be considered an example of cognate object:\n\nSuch constructions are not actually redundant (unlike \"She slept a sleep\" or \"We wept tears\") because the object's modifiers provide additional information. A rarer, more constructed form is polyptoton, the stylistic repetition of the same word or words derived from the same root:\n\n\nAs with cognate objects, these constructions are not redundant because the repeated words or derivatives cannot be removed without removing meaning or even destroying the sentence, though in most cases they could be replaced with non-related synonyms at the cost of style (e.g., compare \"The only thing we have to fear is terror\".)\n\nIn many cases of semantic pleonasm, the status of a word as pleonastic depends on context. The relevant context can be as local as a neighboring word, or as global as the extent of a speaker's knowledge. In fact, many examples of redundant expressions are not inherently redundant, but can be redundant if used one way, and are not redundant if used another way. The \"up\" in \"climb up\" is not always redundant, as in the example \"He climbed up and then fell down the mountain.\" Many other examples of pleonasm are redundant only if the speaker's knowledge is taken into account. For example, most English speakers would agree that \"tuna fish\" is redundant because tuna is a kind of fish. However, given the knowledge that \"tuna\" can also refer a kind of edible prickly pear, the \"fish\" in \"tuna fish\" can be seen as non-pleonastic, but rather a disambiguator between the fish and the prickly pear.\n\nConversely, to English speakers who do not know Spanish, there is nothing redundant about \"the La Brea tar pits\" because the name \"La Brea\" is opaque: the speaker does not know that it is Spanish for \"the tar\". Similarly, even though scuba stands for \"self-contained underwater breathing apparatus\", a phrase like \"the scuba gear\" would probably not be considered pleonastic because \"scuba\" has been reanalyzed into English as a simple word, and not an acronym suggesting the pleonastic word sequence \"apparatus gear\". (Most do not even know that it is an acronym and do not spell it SCUBA or S.C.U.B.A. Similar examples are radar and laser.)\n\n"}
{"id": "885730", "url": "https://en.wikipedia.org/wiki?curid=885730", "title": "Plot twist", "text": "Plot twist\n\nA plot twist is a literary technique that introduces a radical change in the direction or expected outcome of the plot in a work of fiction. When it happens near the end of a story, it is known as a twist or surprise ending. It may change the audience's perception of the preceding events, or introduce a new conflict that places it in a different context. A plot twist may be foreshadowed, to prepare the audience to accept it. There are a variety of methods used to execute a plot twist, such as withholding information from the audience or misleading them with ambiguous or false information.\n\nRevealing a plot twist to readers or viewers in advance is commonly regarded as a \"spoiler\", since the effectiveness of a plot twist usually relies on the audience not expecting it. Even revealing the fact that a work contains plot twists – especially at the ending – can also be controversial, as it changes the audience's expectations. However, at least one study suggests that this does not affect the enjoyment of a work.\n\nAn early example of the romance genre with multiple twists was the \"Arabian Nights\" tale \"The Three Apples\". It begins with a fisherman discovering a locked chest. The first twist occurs when the chest is broken open and a dead body is found inside. The initial search for the murderer fails, and a twist occurs when two men appear, separately claiming to be the murderer. A complex chain of events finally reveals the murderer to be the investigator's own slave.\n\nLiterary analysts have identified several common categories of plot twists, based on how they are executed.\n\n\"Anagnorisis\", or discovery, is the protagonist's sudden recognition of their own or another character's true identity or nature. Through this technique, previously unforeseen character information is revealed. A notable example of anagnorisis occurs in \"Oedipus Rex\": Oedipus kills his father and marries his mother in ignorance, learning the truth only toward the climax of the play. The earliest use of this device as a twist ending in a murder mystery was in \"The Three Apples\", a medieval \"Arabian Nights\" tale, where the protagonist Ja'far ibn Yahya discovers by chance a key item towards the end of the story that reveals the culprit behind the murder to be his own slave all along.\n\nIn M. Night Shyamalan's 1999 film \"The Sixth Sense\", a main character, who struggles with his failure to help a former patient who shot him, resolves his conflict by helping a boy who believes he communicates with dead people, only to discover that he didn't survive the shooting and is one of the dead people being helped. In the 2001 film \"The Others,\" a mother is convinced that her house is being haunted; at the end of the film, she learns that she and her children are really the ghosts. In the episode of \"The Twilight Zone\" titled \"Five Characters in Search of an Exit\", the viewer discovers at the climax that the characters are discarded toys in a donation bin. In \"Fight Club\", Edward Norton's character realizes that Tyler Durden (Brad Pitt) is his own multiple personality. A mental patient in the horror film \"The Ward\" reveals that the three persons she is talking to are all actually herself. Sometimes the audience may discover that the true identity of a character is, in fact, unknown, as in \"Layer Cake\" or the eponymous assassins in \"V for Vendetta\" and \"The Day of the Jackal\".\n\nFlashback, or analepsis, a sudden, vivid reversion to a past event., surprises the reader with previously unknown information that solves a mystery, places a character in a different light, or reveals the reason for a previously inexplicable action. The Alfred Hitchcock film \"Marnie\" employed this type of surprise ending. Sometimes this is combined with the above category, as the flashback may reveal the true identity of one of the characters, or that the protagonist is related to one of the villain's past victims, as Sergio Leone did with Charles Bronson's character in \"Once Upon a Time in the West\" or Frederick Forsyth's \"The Odessa File\". The TV series \"Boardwalk Empire\" and manga (twice made into a movie) \"Old Boy\" uses similar twists.\n\nAn unreliable narrator twists the ending by revealing, almost always at the end of the narrative, that the narrator has manipulated or fabricated the preceding story, thus forcing the reader to question their prior assumptions about the text. This motif is often used within noir fiction and films, notably in the film \"The Usual Suspects\". An unreliable narrator motif was employed by Agatha Christie in \"The Murder of Roger Ackroyd\", a novel that generated much controversy due to critics' contention that it was unfair to trick the reader in such a manipulative manner. Another example of unreliable narration is a character who has been revealed to be insane and thus causes the audience to question the previous narrative; notable examples of this are in the Terry Gilliam film \"Brazil\", Chuck Palahniuk's \"Fight Club\" (and David Fincher's film adaptation), Gene Wolfe's novel \"Book of the New Sun\", the second episode of \"Alfred Hitchcock Presents\", \"Premonition\", Iain Pears's \"An Instance of the Fingerpost\", \"Shutter Island\" and Kim Newman's \"Life's Lottery\".\n\n\"Peripeteia\" is a sudden reversal of the protagonist's fortune, whether for good or ill, that emerges naturally from the character's circumstances. Unlike the \"deus ex machina\" device, peripeteia must be logical within the frame of the story. An example of a reversal for ill would be Agamemnon's sudden murder at the hands of his wife Clytemnestra in Aeschylus' \"The Oresteia\" or the inescapable situation Kate Hudson's character finds herself in at the end of \"The Skeleton Key\". This type of ending was a common twist ending utilised by \"The Twilight Zone\", most effectively in the episode \"Time Enough at Last\" where Burgess Meredith's character is robbed of all his hope by a simple but devastating accident with his glasses. A positive reversal of fortune would be Nicholas Van Orton's suicide attempt after mistakenly believing himself to have accidentally killed his brother, only to land safely in the midst of his own birthday party, in the film \"The Game\".\n\n\"Deus ex machina\" is a Latin term meaning \"god out of the machine.\" It refers to an unexpected, artificial or improbable character, device or event introduced suddenly in a work of fiction to resolve a situation or untangle a plot. In Ancient Greek theater, the \"deus ex machina\" ('ἀπὸ μηχανῆς θεός') was the character of a Greek god literally brought onto the stage via a crane (μηχανῆς—\"mechanes\"), after which a seemingly insoluble problem is brought to a satisfactory resolution by the god's will. In its modern, figurative sense, the \"deus ex machina\" brings about an ending to a narrative through unexpected (generally happy) resolution to what appears to be a problem that cannot be overcome (see Mel Brooks' \"History of the World, Part I\"). This device is often used to end a bleak story on a more positive note.\n\nPoetic justice is a literary device in which virtue is ultimately rewarded or vice punished in such a way that the reward or punishment has a logical connection to the deed. In modern literature, this device is often used to create an ironic twist of fate in which the villain gets caught up in his/her own trap. For example, in C. S. Lewis' \"The Horse and His Boy\", Prince Rabadash climbs upon a mounting block during the battle in Archenland. Upon jumping down while shouting \"The bolt of Tash falls from above,\" his hauberk catches on a hook and leaves him hanging there, humiliated and trapped. Another example of poetic justice can be found in John Boyne's \"The Boy in the Striped Pyjamas\", in which a concentration camp commander's son is mistakenly caught up with inmates rounded up for gassing, or in Chris Van Allsburg's picture book, \"The Sweetest Fig\", where a cold-hearted dentist is cruel to his dog, and ends up getting his comeuppance.\n\nChekhov's gun refers to a seemingly minor character or plot element introduced early in the narrative that suddenly acquires great importance to the narrative. One example of this is the beggar woman in \"\". At the end of the musical, she is revealed to be the main character's wife. A similar mechanism is the \"plant\", a preparatory device that repeats throughout the story. During the resolution, the true significance of the plant is revealed.\n\nA red herring is a false clue intended to lead investigators toward an incorrect solution. This device usually appears in detective novels and mystery fiction. The red herring is a type of misdirection, a device intended to distract the protagonist, and by extension the reader, away from the correct answer or from the site of pertinent clues or action. The Indian murder mystery film \"\" cast many veteran actors who had usually played villainous roles in previous Indian films as red herrings in this film to deceive the audience into suspecting them. In the bestselling novel \"The Da Vinci Code\", the misdeeds of a key character named \"Bishop Aringarosa\" draw attention away from the true master villain (\"Aringarosa\" literally translates as \"pink herring\"). In the William Diehl novel Primal Fear (also adapted into a film), a defendant named Aaron Stampler is accused of brutally murdering the Archbishop of Chicago. He is revealed to have a dissociative identity disorder, and is not executed on plea of insanity. Near the end, Aaron's lawyer discovers that he feigned his insanity to avoid the death penalty. Agatha Christie's classic \"And Then There Were None\" is another famous example and includes the term as well in a murder ploy where the intended victims are made to guess that one of them will be killed through an act of treachery. A red herring can also be used as a form of false foreshadowing.\n\nA false protagonist is a character presented at the start of the story as the main character, but then disposed of, usually killing them. An example is the film \"Executive Decision\", in which the special-forces team leader, played by highly-billed action star Steven Seagal, is killed shortly after their mission begins.\n\n\"In medias res\" (Latin for \"into the middle of things\") is a literary technique in which narrative proceeds from the middle of the story rather than its beginning. Information such as characterization, setting, and motive is revealed through a series of flashbacks. This technique creates a twist when the cause for the inciting incident is not revealed until the climax. This technique is used within the film \"The Prestige\" in which the opening scenes show one of the main characters drowning and the other being imprisoned. Subsequent scenes reveal the events leading up to these situations through a series of flashbacks. In \"Monsters\", a similar beginning proves to be a flashforward as it is the linear conclusion of the events that then follow; this is not apparent until the end. \"In medias res\" is often used to provide a narrative hook.\n\nA non-linear narrative works by revealing plot and character in non-chronological order. This technique requires the reader to attempt to piece together the timeline in order to fully understand the story. A twist ending can occur as the result of information that is held until the climax and which places characters or events in a different perspective. Some of the earliest known uses of non-linear story telling occur in \"The Odyssey\", a work that is largely told in flashback via the narrator Odysseus. The nonlinear approach has been used in works such as the films \"Mulholland Drive\", \"Sin City\", \"Premonition\", \"Arrival\", \"Pulp Fiction\", \"Memento\", \"Babel\", the television shows \"Lost\", \"How I Met Your Mother\" (especially in many episodes in the later seasons), \"Heroes\", \"Westworld\" and the book \"Catch-22\".\n\nReverse chronology works by revealing the plot in reverse order, i.e., from final event to initial event. Unlike chronological storylines, which progress through causes before reaching a final effect, reverse chronological storylines reveal the final effect before tracing the causes leading up to it; therefore, the initial cause represents a \"twist ending\". Examples employing this technique include the films \"Irréversible\", \"Memento\", \"Happy End\" and \"5x2\", the play \"Betrayal\" by Harold Pinter, and Martin Amis' \"Time's Arrow\". Stephen Sondheim and George Furth's Merrily We Roll Along and the 1934 Kaufman and Hart play that inspired it both tell the story of the main characters in reverse order.\n\n"}
{"id": "3123021", "url": "https://en.wikipedia.org/wiki?curid=3123021", "title": "Pro-war rhetoric", "text": "Pro-war rhetoric\n\nPro-war rhetoric is rhetoric or propaganda designed to convince its audience that war is necessary. The two main analytical approaches to pro-war rhetoric were founded by Ronald Reid, a Professor of Communication Studies at the University of Massachusetts Amherst, and Robert Ivie, a Professor of Rhetoric and Public Communication and Culture at Indiana University (Bloomington). Reid's framework originated from inductively studying propaganda. Ivie uses a deductive approach based on the work of Kenneth Burke, claiming that \"a people strongly committed to the ideal of peace, but simultaneously faced with the reality of war, must believe that the fault for any such disruption of their ideal lies with others\" (Ivie 279).\n\nAccording to Reid, pro-war rhetoric uses three appeals: territorial, ethnocentric, and optimistic..\n\nTerritorial appeals threaten the audience's \"sense of territoriality\" (Reid 260). The audience is more likely to support entering the war on the defensive rather than offensive side because of an actual or threatened invasion.\n\nEthnocentric appeals create a dichotomy between the audience and the enemy. Ethnocentrism should be evoked to a \"high level of emotional intensity\" in attempt to complete two goals: hate the \"inferior alien\" and depict threats to cultural values (Reid 267).\n\nThe most common method of evoking ethnocentric appeals come in the form of Barbarism vs. Gallantry (Reid 269). It depicts the enemy's cultural values with evil characteristics, and the audience's as angelic.\n\nThe optimistic appeal assures the audience that victory is inevitable should they enter into war.(Reid 282).\n\nAccording to Ivie, pro-war rhetoric identifies three topoi; force vs. freedom, irrational vs. rational, and aggression vs. defense.\n\nThis tactic portrays to the audience that they are entering war to provide freedom, and the opponent to force their values upon others (Ivie 284). This is accomplished by implying that the opponent is violent, while the audience's nation is willing to negotiate (Ivie 284).\n\nThis topos holds that the enemy is portrayed as irrational, responding \"more to animalistic drives than principles of law\" (Ivie 288). The enemy has an unenlightened intellect, not based on reason. Rhetors use this argument to prove that when an enemy such as this threatens the well-being of the world, even for a nation committed to neutrality and peace, war is the only choice (Ivie 289).\n\nThis idea portrays the enemy as the voluntary aggressor and the nation of the audience as the passive victims of aggression, only entering into war to ensure security (Ivie 290). \"While the savage has acted against order, the victim has been forced to respond in its defense\" (Ivie 290). Ivie describes the actions as either \"voluntary\" and \"initial\" or \"involuntary\" and \"defensive\" (Ivie 290). The purpose of this topos is to lay the blame on the enemy and justify reasons for the victimized nation to engage in action.\n\n\n\n"}
{"id": "2541498", "url": "https://en.wikipedia.org/wiki?curid=2541498", "title": "Public history", "text": "Public history\n\nPublic history is a broad range of activities undertaken by people with some training in the discipline of history who are generally working outside of specialized academic settings. Public history practice is deeply rooted in the areas of historic preservation, archival science, oral history, museum curatorship, and other related fields. The field has become increasingly professionalized in the United States and Canada since the late 1970s. Some of the most common settings for the practice of public history are museums, historic homes and historic sites, parks, battlefields, archives, film and television companies, and all levels of government.\n\nBecause it incorporates a wide range of practices and takes place in many different settings, public history proves resistant to being precisely defined. Four key elements often emerge from the discourse of those who identify themselves as public historians:\n\nThese elements are expressed in the 1989 mission statement of the U.S.-based National Council on Public History: \"To promote the utility of history in social through professional practice.\". They are also present in a definition drafted by the NCPH board in 2007, stating, \"Public history is a movement, methodology, and approach that promotes the collaborative study and practice of history; its practitioners embrace a mission to make their special insights accessible and useful to the public.\" However, this draft definition prompted some challenges on the H-Public listserv from people in the field, who raised questions about whether public history is solely an endeavor by professional or trained historians, or if shared historical authority should be a key element of the field. Others have pointed out that the existence of many \"publics\" for public history complicates the task of definition. For example, historian Peter Novick has questioned whether much of what is termed public history should actually be called \"private\" history (for example, the creation of corporate histories or archives) or \"popular\" history (for example, research or exhibits conducted outside the norms of the historical discipline). Cathy Stanton has also identified a more radical element in North American public history but has asked: 'how much room is there for the progressive component in the public history movement?' Hilda Kean and Paul Ashton have also discussed the differences in public history in Britain, Australia, New Zealand and the U.S., arguing against 'a rigid demarcation between \"historians\" and \"their publics\"'. A 2008 survey of almost 4,000 practitioners predominantly in the U.S. showed that a substantial proportion (almost one quarter of respondents) expressed some reservations about the term and whether it applied to their own work.\n\nIn general, those who embrace the term \"public historian\" accept that the boundaries of the field are flexible. Its definition remains a work in progress, subject to continual re-evaluation of practitioners' relationships with different audiences, goals, and political, economic, or cultural settings.\n\nPublic history refers to a wide variety of professional and academic fields. Some of these include:\n\nIn addition, a sub-field of scholarly study has developed over the past several decades which focuses on the history and theory of collective memory and history-making. This body of scholarship (typified by the winners of the National Council on Public History Book Award) may also be considered to be \"public history.\"\n\nPublic history has many antecedents. These include history museums, historical societies, public and private archives and collections, hereditary and memorial associations, preservation organizations, historical and heritage projects and offices within government agencies, and depictions of history in popular culture of all kinds (for example, historical fiction). Ludmilla Jordanova has also observed that 'the state... lies at the heart of public history', linking public history to the rise of the nation state. (English Theologian William Paley declared in 1794 that 'public history' was a 'register of the successes and disappointments... and the quarrels of those who engage in contentions [for] power'.) In the late nineteenth and early twentieth centuries, a distinct historical discipline formed within Western universities, and this had the effect of gradually separating scholars who practiced history professionally from amateur or public practitioners. While there continued to be trained historians working in public settings, there was a general retreat from public engagement among professional historians by the middle decades of the twentieth century.\n\nDuring the 1970s, a number of political, economic, social, and historiographical developments worked to reverse this trend, converging to produce a new field that explicitly identified itself as “public history”. The social justice movements of the 1960s and 1970s had sparked an interest in the histories of non-dominant people and groups—for example, women, working-class people, ethnic and racial minorities—rather than the “great men” who had traditionally been the focus of many historical narratives. In Britain, this emerged through the History Workshop Movement. Many historians embraced social history as a subject, and some were eager to become involved in public projects as a way of using their scholarship in activist or public-oriented ways. In the U.S., a severe shortage of academic jobs for historians led many to consider careers outside the academy. At the same time, publicly funded efforts were underway in many Western countries, ranging from national celebrations like the United States Bicentennial to multiculturalist projects in Australia and Canada, paralleled by widespread public interest in genealogy, the tracing of folk and family “roots”, and other history-related activities. In the wake of deindustrialization in many industrial places, governments also supported regeneration or revitalization projects that increasingly included the use of local history and culture as an attraction or a basis for “re-branding” a depressed area. Out of necessity, inclination, or both, a growing number of people with graduate training in history found employment in these kinds of non-academic settings. Public policy decisions like the passage of the U.S. National Historic Preservation Act of 1966 and the Canadian government's addition of “historical researcher” as a civil service category in the 1970s, along with the rise of cultural tourism and the increasing professionalization of many museums and historical societies, have spurred the growth of the field.\n\nIn the U.S., the birth of the public history field can be traced to the University of California, Santa Barbara, where Robert Kelley, a member of the history faculty, obtained a Rockefeller Foundation grant in 1976 to create a graduate program to train young historians for public and private sector careers. Kelley drew on his own extensive experience as a consultant and legal witness in water litigation cases in conceiving the idea of “public history” as a field in its own right. Conferences in Scottsdale, Arizona in 1978 and Montecito, California in 1979 helped to catalyze the new field. The launch of a professional journal, \"The Public Historian\", in 1978, and the founding of the National Council on Public History in 1979 further served to give public-minded historians in the academy and isolated practitioners outside of it a sense that they shared a set of missions, experiences, and methods.\n\nPublic history in Canada has followed a similar trajectory in many ways, including the experience of an academic “jobs crisis” in the 1970s and the importance of government as a source of employment for public historians. In 1983, the University of Waterloo created a Masters program in Public History (now defunct), followed by The University of Western Ontario in 1986, and Carleton University in 2002. Also as in the U.S., Canadian public funding for history and heritage projects has shrunk in the past two decades, with public historians increasingly accountable to funders for the effectiveness of their work. Public history also exists as an identifiable field in Australia and to a lesser extent in Europe and other places. In Latin America, public history finds its highest expression in Brazil, where public history is closely connected with social history and oral history. The Brazilian Public History Network, created in 2012, has been responsible for promoting publications and sponsoring events of national and international scope aimed to foster a creative and cosmopolitan dialogue. As in the U.S. and Canada, there are many public projects involving historians and the interpretation of history that do not necessarily claim the specific label “public history.” The International Federation for Public History,(IFPH-FIHP) was formed in 2010 and became an international association with elected Steering Committee in January 2012. IFPH is also a permanent Internal Commission of the International Committee of Historical Sciences (ICHS-CISH). The IFPH seeks to broaden international exchanges about the practice and teaching of public history and it is one of the constitutive co-operation partners of the journal \"Public History Weekly\".\n\nPublic history continues to develop and define itself. There are currently many graduate and undergraduate public history programs in the U.S., Canada, and other countries (see list and links below). The field has a natural synergy with digital history, with its emphasis on access and broad participation in the creation of historical knowledge. In recent years there has been a growing body of public historical scholarship, including works recognized by the annual NCPH Book Award. In several countries, studies have been conducted to explore how people understand and engage with the past, deepening public historians’ sense of how their own work can best connect with their audiences. While high-profile “history wars” have taken place over public exhibits and interpretations of history in many places in recent years (for example, Australia's ongoing debate over the history of colonisation and indigenous peoples, the furor over Jack Granatstein’s 1998 book “Who Killed Canadian History?”, or the 1994 controversy over the National Museum of American History's planned exhibit on the Enola Gay bomber), public historians tend to welcome these as opportunities to participate in vigorous public discussions over the meanings of the past, debating how people arrive at those meanings.\n\nAn evolving form of locally collected and publicly presented history, seen in projects like If This House Could Talk are a less critical and validated public presentation of history, yet offer engagement at the grass roots level that may encourage new forms of collecting history about the everyday.\n\nThe National Council on Public History's Robert Kelley Memorial Award, “honors distinguished and outstanding achievements by individuals, institutions, non-profit or corporate entities for having made significant inroads in making history relevant to individual lives of ordinary people outside of academia.” Its recipients reflect a broad mix of scholarly, governmental, and popular projects:\n\nAn extensive listing of undergraduate and graduate programs in public history in the U.S., Canada, and elsewhere, are on the National Council on Public History website.\n\n\n"}
{"id": "22417121", "url": "https://en.wikipedia.org/wiki?curid=22417121", "title": "Rabelais and His World", "text": "Rabelais and His World\n\nRabelais and His World (Russian: Творчество Франсуа Рабле и народная культура средневековья и Ренессанса, \"Tvorčestvo Fransua Rable i narodnaja kul'tura srednevekov'ja i Renessansa\"; 1965) is a scholarly work which is considered one of Mikhail Bakhtin's most important texts and now a classic of Renaissance studies. In the work Bakhtin explores \"Gargantua and Pantagruel\" by the French Renaissance writer François Rabelais.\n\nBakhtin argues that for centuries Rabelais’s book has been misunderstood. Bakhtin attempts to redress this and clarify Rabelais's intentions through two methods: recovery of sections of \"Gargantua and Pantagruel\" that were previously either ignored or suppressed, and analysis of the Renaissance social system in order to discover the balance between language that was permitted and language that was not. Through this analysis, Bakhtin pinpoints two important subtexts: \"carnival\" (carnivalesque) which Bakhtin describes as a social institution, and \"grotesque realism,\" which is defined as a literary mode. \n\nThus, in \"Rabelais and His World\" Bakhtin studies the interaction between the social and the literary, as well as the meaning of the body. As written, \"Rabelais and His World\" not only examines the openness of \"Gargantua and Pantagruel\", it also serves as an example of such openness.\n\nBakhtin completed his book on Rabelais (titled \"Rabelais in the History of Realism\") in 1940. After several attempts to get the book published fell through, it was submitted as a dissertation for the Candidate of Sciences degree at the Gorky Institute of World Literature in Moscow. At the dissertation's defense in 1946, all three official opponents were in favor of awarding Bakhtin a higher doctoral degree: the Doctor of Sciences, and their motion was accepted with a narrow majority vote. However, following an assault on the institute published in the press at the time, and after six years of repeated revisions and deliberations, USSR's VAK decided Bakhtin would only receive the Candidate of Sciences degree (roughly equivalent to a research doctorate). The book was eventually published in Russian in 1965, under the title \"Rabelais and Folk Culture of the Middle Ages and Renaissance\". Its 1968 English translation by Hélène Iswolsky was given the title, \"Rabelais and His World\".\n\nFor Bakhtin, \"carnival\" is associated with the collectivity. Those attending a carnival do not merely constitute a crowd; rather the people are seen as a\nwhole, organized in a way that defies socioeconomic and political organization. According to Bakhtin, “[A]ll were considered equal during carnival. Here, in the town square, a special form of free and familiar contact reigned among people who were usually divided by the barriers of caste, property, profession, and age”. The carnival atmosphere holds the lower strata of life most important, as opposed to higher functions (thought, speech, soul) which were usually held dear in the signifying order. At carnival time, the unique sense of time and space causes individuals to feel they are a part of the collectivity, at which point they cease to be themselves. It is at this point that, through costume and mask, an individual exchanges bodies and is renewed. At the same time there arises a heightened awareness of one’s sensual, material, bodily unity and community.\n\nBakhtin’s notion of \"carnival\" is connected with that of the \"grotesque\". In the carnival, usual social hierarchies and proprieties are upended; emphasis is placed on the body in its open dimension, in its connection to the life of the community. This emphasis on the material dimension which links humans, rather than on the differences and separations between them, allows for the consciousness of the historical dimension of human life: for every death, there is a birth, a renewal of the human spirit. This process allows for progress.<br>\nIn the grotesque body, emphasis is placed on the open, the penetrative, and the \"lower stratum.\" The open (the mouth, the anus, the vagina, etc.) and the penetrative (the nose, the penis, etc.) allow exchange between the body and the world (mostly through sex, eating, and drinking), but also to produce degrading material (curses, urine, feces, etc.). The lower stratum (belly, womb, etc.) is the place where renewal happens, where new life is forged, thus connecting degradation to renewal. The grotesque body is one of excess, rebellious to authority and austerity. <br>\nDue to its inscription in time and its emphasis on bodily changes (through eating, evacuation, and sex), the \"grotesque\" has been interpreted by some critics as a dimension of the body that permits to perceive the historicity of man: it is in this reading used as a measuring device.\n\nBakhtin opens this work with a quotation from Alexander Herzen: \"It would be extremely interesting to write the history of laughter\".\n\nOne of the primary expressions of the ancient world's conceptions of laughter is the text that survives in the form of apocryphal letters of Hippocrates about Democritus (Hippocratic Corpus, \"Epistles\" 10–21). The laughter of Democritus had a philosophical character, being directed at the life of man and at all the vain fears and hopes related to the gods and to life after death. Democritus here made of his laughter a complete conception of the world, a certain spiritual premise of the man who has attained maturity and has awakened. Hippocrates finally perfectly agreed with him.\n\n"}
{"id": "43028857", "url": "https://en.wikipedia.org/wiki?curid=43028857", "title": "Realism (art movement)", "text": "Realism (art movement)\n\nRealism was an artistic movement that began in France in the 1850s, after the 1848 Revolution. Realists rejected Romanticism, which had dominated French literature and art since the late 18th century. Realism revolted against the exotic subject matter and the exaggerated emotionalism and drama of the Romantic movement. Instead, it sought to portray real and typical contemporary people and situations with truth and accuracy, and not avoiding unpleasant or sordid aspects of life. Realist works depicted people of all classes in situations that arise in ordinary life, and often reflected the changes brought by the Industrial and Commercial Revolutions. The popularity of such \"realistic\" works grew with the introduction of photography—a new visual source that created a desire for people to produce representations which look objectively real.\n\nThe Realists depicted everyday subjects and situations in contemporary settings, and attempted to depict individuals of all social classes in a similar manner. Classical idealism and Romantic emotionalism and drama were avoided equally, and often sordid or untidy elements of subjects were not smoothed over or omitted. Social realism emphasizes the depiction of the working class, and treating them with the same seriousness as other classes in art, but realism, as the avoidance of artificiality, in the treatment of human relations and emotions was also an aim of Realism. Treatments of subjects in a heroic or sentimental manner were equally rejected.\n\nRealism as an art movement was led by Gustave Courbet in France. It spread across Europe and was influential for the rest of the century and beyond, but as it became adopted into the mainstream of painting it becomes less common and useful as a term to define artistic style. After the arrival of Impressionism and later movements which downgraded the importance of precise illusionistic brushwork, it often came to refer simply to the use of a more traditional and tighter painting style. It has been used for a number of later movements and trends in art, some involving careful illusionistic representation, such as Photorealism, and others the depiction of \"realist\" subject matter in a social sense, or attempts at both.\n\nThe Realist movement began in the mid-19th century as a reaction to Romanticism and History painting. In favor of depictions of 'real' life, the Realist painters used common laborers, and ordinary people in ordinary surroundings engaged in real activities as subjects for their works. The chief exponents of Realism were Gustave Courbet, Jean-François Millet, Honoré Daumier, and Jean-Baptiste-Camille Corot. Jules Bastien-Lepage is closely associated with the beginning of \"Naturalism\", an artistic style that emerged from the later phase of the Realist movement and heralded the arrival of Impressionism.\n\nRealists used unprettified detail depicting the existence of ordinary contemporary life, coinciding in the contemporaneous naturalist literature of Émile Zola, Honoré de Balzac, and Gustave Flaubert.\n\nCourbet was the leading proponent of Realism and he challenged the popular history painting that was favored at the state-sponsored art academy.\nHis groundbreaking paintings \"A Burial at Ornans\" and \"The Stonebreakers\" depicted ordinary people from his native region. Both paintings were done on huge canvases that would typically be used for history paintings. Although Courbet's early works emulated the sophisticated manner of Old Masters such as Rembrandt and Titian, after 1848 he adopted a boldly inelegant style inspired by popular prints, shop signs, and other work of folk artisans. In \"The Stonebreakers\", his first painting to create a controversy, Courbet eschewed the pastoral tradition of representing human subjects in harmony with nature. Rather, he depicted two men juxtaposed against a charmless, stony roadside. The concealment of their faces emphasizes the dehumanizing nature of their monotonous, repetitive labor. \n\nThe French Realist movement had stylistic and ideological equivalents in all other Western countries, developing somewhat later. The Realist movement in France was characterized by a spirit of rebellion against powerful official support for history painting. In countries where institutional support of history painting was less dominant, the transition from existing traditions of genre painting to Realism presented no such schism. An important Realist movement beyond France was the Peredvizhniki or \"Wanderers\" group in Russia who formed in the 1860s and organized exhibitions from 1871 included many realists such as genre artist Vasily Perov, landscape artists Ivan Shishkin, Alexei Savrasov, and Arkhip Kuindzhi, portraitist Ivan Kramskoy, war artist Vasily Vereshchagin, historical artist Vasily Surikov and, especially, Ilya Repin, who is considered by many to be the most renowned Russian artist of the 19th century. \n\nCourbet's influence was felt most strongly in Germany, where prominent realists included Adolph Menzel, Wilhelm Leibl, Wilhelm Trübner, and Max Liebermann. Leibl and several other young German painters met Courbet in 1869 when he visited Munich to exhibit his works and demonstrate his manner of painting from nature. In Italy the artists of the Macchiaioli group painted Realist scenes of rural and urban life. The Hague School were Realists in the Netherlands whose style and subject matter strongly influenced the early works of Vincent van Gogh. In Britain artists such as the American James Abbot McNeill Whistler, as well as English artists Ford Madox Brown, Hubert von Herkomer and Luke Fildes had great success with realist paintings dealing with social issues and depictions of the \"real\" world. \n\nIn the United States, Winslow Homer and Thomas Eakins were important Realists and forerunners of the Ashcan School, an early-20th-century art movement largely based in New York City. The Ashcan School included such artists as George Bellows and Robert Henri, and helped to define American realism in its tendency to depict the daily life of poorer members of society. \n\n"}
{"id": "50267677", "url": "https://en.wikipedia.org/wiki?curid=50267677", "title": "Reginald Piggott", "text": "Reginald Piggott\n\nReginald \"Reg\" Piggott (1930 – c. 2014) was a British book cartographer whose maps were known for their elegance, clarity, and distinctive italic script. His work was published by Cambridge University Press and The Folio Society among other presses. Early in his life, he was a campaigner for better handwriting and in 1957 organised a survey of British handwriting which drew over 25,000 responses and was subsequently published in book form. He advocated the use of a form of italic script to replace the civil service script widely used in Britain which he thought tended to illegibility when written at speed.\n\nReginald Piggott was born in 1930. He married Marjorie. He was a long time resident of Decoy Lodge, Decoy Road, Potter Heigham, Norfolk.\n\nIn February 1957, eleven newspapers and journals published a letter from Reginald Piggott of 10 Finlay Drive, Dennistoun, Glasgow, in which he requested samples of handwriting. By far the greatest response came from readers of \"The Observer\" whose readers sent him 15,000 samples of their writing. In an article in that paper in March 1957, Piggott explained that he had been interested in the history of handwriting since he had begun to study calligraphy and that he wished to develop a \"practical, everyday cursive\" script to replace the civil service style then in use in Britain that was derived from Victorian copperplate writing but which tended to illegibility when written at speed. By gathering samples of handwriting, he hoped to develop a new style that would command widespread acceptance. He made clear that he was not a graphologist and was not offering insights into the character of the writer.\n\nPiggott envisaged a simplified script that dispensed with loops and flourishes and reduced letter forms to as fundamental a shape as possible. In a running script, joins were to be made in the simplest manner. He went on to discuss the beauty of italic script, which he used in his own writing, and the unsuitability of the ball point pen to produce it before concluding that once his research was complete \"and it is certain that the new style is near perfect\", then it could be widely adopted, leading to a great improvement in the handwriting of the nation.\n\nThe results of the survey were published in 1958 as \"Handwriting: A national survey, together with a plan for better modern handwriting\" by which time Piggott had acquired over 25,000 samples of handwriting. He analysed the results of his survey according to multiple characteristics including sex, age and occupation, and concluded with a chapter outlining a plan for better modern writing using a form of italic script. Writing in the \"New Scientist\", Gail Vines placed Piggott's research and the public's enthusiasm for the project in the context of rapid social and technological change in the 1950s that resulted in a moral panic about declining standards of penmanship that reflected a generalised anxiety about threats to standards in a changing world.\n\nPiggott's cartography was known for its clarity and elegance which was aided by his distinctive italic lettering. In 2014, the travel writer Nicholas Crane described him as \"the most brilliant book-cartographer of our generation\".\n\nHe produced numerous maps for Cambridge University Press, and had a long association with The Folio Society for whose books he produced many works, including a double page map of the spread of the Great Fire of London which was included in their volume on that subject published in 2003. He prepared the maps for \"The Oxford Illustrated History of Britain\" (1984) and with his wife Marjorie for volumes of Nikolaus Pevsner's \"The Buildings of England\".\n\nIn 2012, Aurum Press published \"Mile by mile London to Paris\" in which the route between the cities was mapped out by Piggott with a text by Matt Thompson, assuming a journey by the Golden Arrow, when it existed, and the Eurostar more recently.\n\n"}
{"id": "838807", "url": "https://en.wikipedia.org/wiki?curid=838807", "title": "Restoration (cultural heritage)", "text": "Restoration (cultural heritage)\n\nRestoration is a process that attempts to return cultural heritage to some previous state that the restorer imagines was the \"original\". This was commonly done in the past. However, in the late 20th century a separate concept of conservation-restoration was developed that is more concerned with preserving the work of art for the future, and less with making it look pristine. Restoration is controversial, since it often involves some irreversible change to the original material of the artwork with the goal of making it \"look good.\" The attitude that has developed in recent years with the development of conservation is to attempt to make all restoration reversible.\n\nArt conservation can involve the cleaning and stabilization of art work. Ideally, any process used is reversible, departures from that ideal not being undertaken lightly. Cleaning is not a reversible process and can sometimes be controversial due to fears that cleaning would damage a piece, or on the grounds that damage or residue forms part of the history of a given piece and should not be modified. Michelangelo's statue of David has undergone two cleanings to remove dirt that had accumulated on the statue's surface.\n\nArt restoration is vital to the survival of classical paintings and is part of the scientific application of chemistry. It often involves research to determine the original colors and materials of a work.\n\nThe use of watercolor paints to inpaint damages on fresco is an example of a technique utilized to achieve almost complete reversibility. This is the technique used in the 20 year restoration of Da Vinci's \"The Last Supper\" in Milan. One of the most popular inpainting techniques used today is the Tinted Varnish Treatment. This process is done once the piece is fully cleaned and varnished. This last step in the restoration process is to then go into spots where original paint may be missing, or where patched holes and other irregularities may lie. The restorer then goes in with tinted varnish over the top of the non-tinted varnish. This gives the illusion that the spots have been \"re-painted\", while in fact it is just a spot of tinted varnish. Most commonly \"stippling\" is used while using tinted varnish, in order for light to reflect similar to paint. This is done by using tiny dots in a row for variance.\n\nIn 2014, Harvard Art Museums exhibited murals by Mark Rothko from 1961 and 1962 which had faded due to their original display in a dining room. It restored the works to their original appearance by digitally projecting light carefully calibrated at each pixel from a photograph of the original colors, which made up the difference between the current and original appearance without damaging the material.\n\n\n"}
{"id": "3404049", "url": "https://en.wikipedia.org/wiki?curid=3404049", "title": "Semantic analysis (linguistics)", "text": "Semantic analysis (linguistics)\n\nIn linguistics, semantic analysis is the process of relating syntactic structures, from the levels of phrases, clauses, sentences and paragraphs to the level of the writing as a whole, to their language-independent meanings. It also involves removing features specific to particular linguistic and cultural contexts, to the extent that such a project is possible. The elements of idiom and figurative speech, being cultural, are often also converted into relatively invariant meanings in semantic analysis. Semantics, although related to pragmatics, is distinct in that the former deals with word or sentence choice in any given context, while pragmatics considers the unique or particular meaning derived from context or tone. To reiterate in different terms, semantics is about universally coded meaning, and pragmatics, the meaning encoded in words that is then interpreted by an audience.\n\nSemantic analysis can begin with the relationship between individual words. This requires an understanding of lexical hierarchy, including hyponymy and hypernymy, meronomy, polysemy, synonyms, antonyms, and homonyms. It also relates to concepts like connotation (semiotics) and collocation, which is the particular combination of words that can be or frequently are surrounding a single word. This can include idioms, metaphor, and simile, like, \"white as a ghost.\"\n\nWith the availability of enough material to analyze, semantic analysis can be used to catalog and trace the style of writing of specific authors.\n\n"}
{"id": "11166734", "url": "https://en.wikipedia.org/wiki?curid=11166734", "title": "Table of years in art", "text": "Table of years in art\n\nThe table of years in art is a tabular display of all years in art, for overview and quick navigation to any year.\n\n"}
{"id": "22286701", "url": "https://en.wikipedia.org/wiki?curid=22286701", "title": "The Mysterious Numbers of the Hebrew Kings", "text": "The Mysterious Numbers of the Hebrew Kings\n\nThe Mysterious Numbers of the Hebrew Kings (1951) is a reconstruction of the chronology of the kingdoms of Israel and Judah by Edwin R. Thiele. The book was originally his doctoral dissertation and is widely regarded as the definitive work on the chronology of Hebrew Kings. The book is considered the classic and comprehensive work in reckoning the accession of kings, calendars, and co-regencies, based on biblical and extra-biblical sources.\n\nThe chronology of the kings of Israel and Judah rests primarily on a series of reign lengths and cross references within the books of Kings and Chronicles, in which the accession of each king is dated in terms of the reign of his contemporary in either the southern Kingdom of Judah or the northern Kingdom of Israel, and fitting them into the chronology of other ancient civilizations.\n\nHowever, some of the biblical cross references did not seem to match, so that a reign which is said to have lasted for 20 years results in a cross reference that would give a result of either 19 or 21 years. Thiele noticed that the cross references given during the long reign of King Asa of Judah had a cumulative error of 1 year for each succeeding reign of the kings of Israel: the first cross-reference resulted in an error of 1 year, the second gave an error of 2 years, the third of 3 years and so on. He explained this pattern as a result of two different methods of reckoning regnal years: the \"accession year\" method in one and the \"non-accession year\" method in the other. Under the accession year method, if a king died in the middle of a year, the period to the end of that year would be called the \"accession year\" of the new king, whose Year 1 would begin at the new year. Under the non-accession year method the period to the end of the year would be Year 1 of the new king and Year 2 would begin at the start of the new year. Israel appears to have used the non-accession method, while Judah used the accession method until Athaliah seized power in Judah, when Israel's non-accession method appears to have been adopted in Judah.\n\nIn addition, Thiele also concluded that Israel counted years starting in the spring month of Nisan, while Judah counted years starting in the autumn month of Tishri. The cumulative impact of differing new years and different methods of calculating reigns explained, to Thiele, most of the apparent inconsistencies in the cross references.\n\nUnknown to Thiele when he first published his findings, these same conclusions that the northern kingdom used non-accession years and a spring New Year while the southern kingdom used accession years and a fall New Year had been discovered by Valerius Coucke of Belgium some years previously, a fact which Thiele acknowledges in his \"Mysterious Numbers\".\n\nBased on his conclusions, Thiele showed that the 14 years between Ahab and Jehu were really 12 years. This enabled him to date their reigns precisely, for Ahab is mentioned in the Kurk Stele which records the Assyrian advance into Syria/Israel at the Battle of Qarqar in 853 BC, and Jehu is mentioned on the Black Obelisk of Shalmaneser III paying tribute in 841 BC. As these two events are dated by Assyrian chronology as being 12 years apart, Ahab must have fought the Assyrians in his last year and Jehu paid tribute in his first year.\n\nThiele was able to reconcile the Biblical chronological data from the books of Kings and Chronicles with the exception of synchronisms between Hoshea of Israel and Hezekiah of Judah towards the end of the kingdom of Israel and reluctantly concluded that at that point the ancient authors had made a mistake. Oddly, it is at that precise point that he himself makes a mistake, by failing to realize that Hezekiah had a coregency with his father Ahaz, which explains the Hoshea/Hezekiah synchronisms. This correction has been supplied by subsequent writers who built on Thiele’s work, including Thiele’s colleague Siegfried Horn, TC Mitchell and Kenneth Kitchen, and Leslie McFall.\n\nThiele's method in arriving at his chronology has been contrasted with the analytical method employed by Julius Wellhausen and other scholars who follow some form of the documentary hypothesis. Wellhausen taught that the chronological data of the books of Kings and Chronicles were artificially put together at a date much later than the events they were ostensibly describing and were basically not historical. This was a necessary consequence of his \"a priori\" assumption that the biblical books as we have them today were the work of late-date editors who could not possibly have known the correct history of the times they were writing about. Theodore Robinson summarized this position as follows: \"Wellhausen is surely right in believing that the synchronisms in Kings are worthless, being merely a late compilation from the actual figures given.\"\n\nWellhausen's methodology in interpreting the Scriptures and the history of Israel has therefore been classed by RK Harrison as a deductive approach; that is, one that starts with presuppositions and derives a historical reconstruction from those presuppositions. A necessary consequence of this approach has been that no general agreement has been reached on the chronology of the Hebrew kingdom period as calculated by authors who adopted this method. \"The disadvantage of the deductive approach is that nothing is settled for certain; the results obtained are as diverse as the presuppositions of the scholars, since diverse presuppositions produce diverse results.\" In contrast, Thiele's method of determining the chronology of the Hebrew kings was based on induction, that is, making it a matter of first priority to determine the actual methods used by ancient scribes and court recorders in recording the years of kings, as described above. Thiele's inductive method, then, was based on inscriptional evidence from the ancient Near East, and not on the presuppositions followed by liberal scholarship. It is Thiele's method that has produced the determinative studies for the chronology of the kingdom period, not the presupposition-based method, so that even those interpreters who continue in late-date theories for the authorship of Scripture have recognized the credibility of Thiele's scholarship in determining the date for the division of the kingdom after the death of Solomon, as cited above. The work of Thiele and other textual scholars who have followed an inductive (evidence-based) approach is therefore significant in providing an alternative to the methods of the documentary hypothesis, and the success of that approach has been seen as theologically significant in supporting a high view of the inspiration of Scripture, particularly regarding its integrity in the abundant and complex historical data related to the kingdom period.\n\nIf the chronological data of the MT [ Masoretic text ] were not authentic—the actual dates and synchronisms for these various kings—then neither Thiele nor McFall nor anyone else could have constructed a chronology from them that in every case is faithful to the original texts and in every proven instance is consistent with Assyrian and Babylonian chronology. This mathematical demonstration should sit in judgment over the various theories of text formation: If a theory of text formation cannot explain how the chronological data of the MT has produced a chronology that in every respect seems authentic for the four centuries of the monarchic period, then that theory must be rejected as another example of a presupposition-based approach that cannot meet the rational criteria for credibility.\n\nThiele's chronological reconstruction has not been accepted by all scholars, nor has any other scholar’s work in this field. Yet the work of Thiele and those who followed in his steps has achieved acceptance across a wider spectrum than that of any comparable chronology, so that Assyriologist DJ Wiseman wrote “The chronology most widely accepted today is one based on the meticulous study by Thiele,” and, more recently, Leslie McFall: “Thiele’s chronology is fast becoming the consensus view among Old Testament scholars, if it has not already reached that point.” Although criticism has been leveled at numerous specific points in his chronology, his work has won considerable praise even from those who disagree with his final conclusions. Nevertheless, even scholars sharing Thiele's religious convictions have maintained that there are weaknesses in his argument such as unfounded assumptions and assumed circular reasoning.\n\nThis citation, from a critic of Thiele's system, demonstrates the difference mentioned above between the deductive approach based on presuppositions and an inductive approach based on data, not a priori assumptions. Thiele is criticized here for basing his theories on data or evidence, not on presuppositions.\n\nDespite these criticisms Thiele's methodological treatment remains the typical starting point of scholarly treatments of the subject, and his work is considered to have established the date of the division of the Israelite kingdom. This has found independent support in the work of J. Liver, Frank M. Cross, and others studying the chronology of the kings of Tyre. Thiele's work has found widespread recognition and use across various related scholarly disciplines. His date of 931 BCE, in conjunction with the synchronism between Rehoboam and Pharaoh Shishak in 1 Kings 14:25, is used by Egyptologists to give absolute dates to Egypt’s 22nd Dynasty, and his work has also been used by scholars in other disciplines to establish Assyrian and Babylonian dates. Criticism of Thiele's reconstruction led to further research which has refined or even departed from his synthesis. Notable studies of this type include work by Tadmor and McFall.\n\nScholarly attitudes towards the Biblical record of the Israelite monarchies from the late nineteenth century to the mid-twentieth century were largely disparaging, treating the records as essentially fictional and dismissing the value of the regnal synchronisms. In contrast, modern scholarly attitudes to the monarchical chronology and synchronisms in 1 and 2 Kings has been far more positive subsequent to the work of Thiele and those who have developed his thesis further, a change in attitude to which recent archaeology has contributed.\n\n"}
{"id": "16349122", "url": "https://en.wikipedia.org/wiki?curid=16349122", "title": "Theories about religions", "text": "Theories about religions\n\nSociological and anthropological theories about religion (or theories of religion) generally attempt to explain the origin and function of religion. These theories define what they present as universal characteristics of religious belief and practice.\n\nFrom presocratic times, ancient authors advanced prescientific theories about religion.\nHerodotus (484 – 425 BCE) saw the gods of Greece as the same as the gods of Egypt. Euhemerus (about 330 – 264 BCE) regarded gods as excellent historical persons whom admirers eventually came to worship.\n\nScientific theories, inferred and tested by the comparative method, emerged after data from tribes and peoples all over the world became available in the 18th and 19th centuries. Max Müller (1823-1900) has the reputation of having founded the scientific study of religion; he advocated a comparative method that developed into comparative religion. Subsequently, Clifford Geertz (1926-2006) and others questioned the validity of abstracting a general theory of all religions.\n\nTheories of religion can be classified into:\n\nOther dichotomies according to which theories or descriptions of religions can be classified include:\n\n\nEarly essentialists, such as Tylor and Frazer, looked for similar beliefs and practices in all societies, especially the more primitive ones, more or less regardless of time and place. They relied heavily on reports made by missionaries, discoverers, and colonial civil servants. These were all investigators who had a religious background themselves, thus they looked at religion from the inside. Typically they did not practice investigative field work, but used the accidental reports of others. This method left them open to criticism for lack of universality, which many freely admitted. The theories could be updated, however, by considering new reports, which Robert Ranulph Marett (1866-1943) did for Tylor's theory of the evolution of religion.\n\nField workers deliberately sent out by universities and other institutions to collect specific cultural data made available a much greater database than random reports. For example, the anthropologist E. E. Evans-Pritchard (1902-1973) preferred detailed ethnographical study of tribal religion as more reliable. He criticised the work of his predecessors, Müller, Tylor, and Durkheim, as untestable speculation. He called them \"armchair anthropologists\".\n\nA second methodology, functionalism, seeks explanations of religion that are outside of religion; i.e., the theorists are generally (but not necessarily) atheists or agnostics themselves. As did the essentialists, the functionalists proceeded from reports to investigative studies. Their fundamental assumptions, however, are quite different; notably, they apply what is called \"methodological naturalism\". When explaining religion they reject divine or supernatural explanations for the status or origins of religions because they are not scientifically testable. In fact, theorists such as Marett (an Anglican) excluded scientific results altogether, defining religion as the domain of the unpredictable and unexplainable; that is, comparative religion is the rational (and scientific) study of the irrational. The dichotomy between the two classifications is not bridgeable, even though they have the same methods, because each excludes the data of the other.\n\nThe functionalists and some of the later essentialists (among others E. E. Evans-Pritchard) have criticized the substantive view as neglecting social aspects of religion. Such critics go so far as to brand Tylor's and Frazer's views on the origin of religion as unverifiable speculation. The view of monotheism as more evolved than polytheism represents a mere preconception, they assert. There is evidence that monotheism is more prevalent in hunter societies than in agricultural societies. The view of a uniform progression in folkways is criticized as unverifiable, as the writer Andrew Lang (1844–1912) and E. E. Evans-Pritchard assert. The latter criticism presumes that the evolutionary views of the early cultural anthropologists envisaged a uniform cultural evolution. Another criticism supposes that Tylor and Frazer were individualists (unscientific). However, some support that supposed approach as worthwhile, among others the anthropologist Robin Horton. The dichotomy between the two fundamental presumptions - and the question of what data can be considered valid - continues.\n\nEvolutionary theories view religion as either an adaptation or a byproduct. Adaptationist theories view religion as being of adaptive value to the survival of Pleistocene humans. Byproduct theories view religion as a spandrel.\n\nThe anthropologist Edward Burnett Tylor (1832–1917) defined religion as belief in spiritual beings and stated that this belief originated as explanations of natural phenomena. Belief in spirits grew out of attempts to explain life and death. Primitive people used human dreams in which spirits seemed to appear as an indication that the human mind could exist independent of a body. They used this by extension to explain life and death, and belief in the after life. Myths and deities to explain natural phenomena originated by analogy and an extension of these explanations. His theory assumed that the psyches of all peoples of all times are more or less the same and that explanations in cultures and religions tend to grow more sophisticated via monotheist religions, such as Christianity and eventually to science. Tylor saw practices and beliefs in modern societies that were similar to those of primitive societies as \"survivals\", but he did not explain why they survived.\n\nJames George Frazer (1854–1941) followed Tylor's theories to a great extent in his book The Golden Bough, but he distinguished between magic and religion. Magic is used to influence the natural world in the primitive man's struggle for survival. He asserted that magic relied on an uncritical belief of primitive people in contact and imitation. For example, precipitation may be invoked by the primitive man by sprinkling water on the ground. He asserted that according to them magic worked through laws. In contrast religion is faith that the natural world is ruled by one or more deities with personal characteristics with whom can be pleaded, not by laws.\n\nThe theologian Rudolf Otto (1869–1937) focused on religious experience, more specifically moments that he called numinous which means \"Wholly Other\". He described it as \"mysterium tremendum\" (terrifying mystery) and \"mysterium fascinans\" (awe inspiring, fascinating mystery). He saw religion as emerging from these experiences.\n\nHe asserted that these experiences arise from a special, non-rational faculty of the human mind, largely unrelated to other faculties, so religion cannot be reduced to culture or society. Some of his views, among others that the experience of the numinous was caused by a transcendental reality, are untestable and hence unscientific.\n\nHis ideas strongly influenced phenomenologists and Mircea Eliade.\nMircea Eliade's (1907–1986) approach grew out of the phenomenology of religion. Like Otto, he saw religion as something special and autonomous, that cannot be reduced to the social, economical or psychological alone. Like Durkheim, he saw the sacred as central to religion, but differing from Durkheim, he views the sacred as often dealing with the supernatural, not with the clan or society. The daily life of an ordinary person is connected to the sacred by the appearance of the sacred, called hierophany. Theophany (an appearance of a god) is a special case of it. In \"The Myth of the Eternal Return\" Eliade wrote that archaic men wish to participate in the sacred, and that they long to return to lost paradise outside the historic time to escape meaninglessness. The primitive man could not endure that his struggle to survive had no meaning. According to Eliade, man had a nostalgia (longing) for an otherworldly perfection. Archaic man wishes to escape the \"terror of time\" and saw time as cyclic. Historical religions like Christianity and Judaism revolted against this older concept of cyclic time. They provided meaning and contact with the sacred \"in\" history through the god of Israel.\n\nEliade sought and found patterns in myth in various cultures, e.g. sky gods such as Zeus.\n\nEliade's methodology was studying comparative religion of various cultures and societies more or less regardless of other aspects of these societies, often relying on second hand reports. He also used some personal knowledge of other societies and cultures for his theories, among others his knowledge of Hindu folk religion.\n\nHe has been criticized for vagueness in defining his key concepts. Like Frazer and Tylor he has also been accused of out-of-context comparisons of religious beliefs of very different societies and cultures.\nHe has also been accused of having a pro-religious bias (Christian and Hindu), though this bias does not seem essential for his theory.\n\nThe anthropologist Edward Evan Evans-Pritchard (1902–1973) did extensive ethnographic studies among the Azande and Nuer peoples who were considered \"primitive\" by society and earlier scholars. Evans-Pritchard saw these people as different, but not primitive.\n\nUnlike the previous scholars, Evans-Pritchard did not propose a grand universal theory and he did extensive long-term fieldwork among \"primitive\" peoples, studying their culture and religion, among other among the Azande. Not just passing contact, like Eliade.\n\nHe argued that the religion of the Azande (witchcraft and oracles) can not be understood without the social context and its social function. Witchcraft and oracles played a great role in solving disputes among the Azande. In this respect he agreed with Durkheim, though he acknowledged that Frazer and Tylor were right that their religion also had an intellectual explanatory aspect. The Azande's faith in witchcraft and oracles was quite logical and consistent once some fundamental tenets were accepted. Loss of faith in the fundamental tenets could not be endured because of its social importance and hence they had an elaborate system of explanations (or excuses) against disproving evidence. Besides an alternative system of terms or school of thought did not exist.\n\nHe was heavily critical about earlier theorists of primitive religion with the exception of Lucien Lévy-Bruhl, asserting that they made statements about primitive people without having enough inside knowledge to make more than a guess. In spite of his praise of Bruhl's works, Evans-Pritchard disagreed with Bruhl's statement that a member of a \"primitive\" tribe saying \"I am the moon\" is prelogical, but that this statement makes perfect sense within their culture if understood metaphorically.\n\nApart from the Azande, Evans-Pritchard, also studied the neighbouring, but very different Nuer people. The Nuer had had an abstract monotheistic faith, somewhat similar to Christianity and Judaism, though it included lesser spirits. They had also totemism, but this was a minor aspect of their religion and hence a corrective to Durkheim's generalizations should be made. Evans-Pritchard did not propose a theory of religion\"s\", but only a theory of the Nuer religion.\n\nThe anthropologist Clifford Geertz (1926–2006) made several studies in Javanese villages. He avoided the subjective and vague concept of group attitude as used by Ruth Benedict by using the analysis of society as proposed by Talcott Parsons who in turn had adapted it from Max Weber. Parsons' adaptation distinguished all human groups on three levels i.e. \"1.\" an individual level that is controlled by \"2.\" a social system that is in turn controlled by \"3.\" a cultural system. Geertz followed Weber when he wrote that \"man is an animal suspended in webs of significance he himself has spun and the analysis of it to be therefore not an experimental science in search of law but an interpretive one in search of meaning\". Geertz held the view that mere explanations to describe religions and cultures are not sufficient: interpretations are needed too. He advocated what he called \"thick descriptions\" to interpret symbols by observing them in use, and for this work, he was known as a founder of symbolic anthropology. \n\nGeertz saw religion as one of the cultural systems of a society. He defined religion as \nWith symbols Geertz meant a carrier that embodies a conception, because he saw religion and culture as systems of communication.\n\nThis definition emphasizes the mutual reinforcement between world view and ethos.\nThough he used more or less the same methodology as Evans-Pritchard, he did not share Evans-Pritchard's hope that a theory of religion could ever be found. Geertz proposed methodology was not the scientific method of the natural science, but the method of historians studying history.\n\nThe social philosopher Karl Marx (1818–1883) held a materialist worldview.\nAccording to Marx, the dynamics of society were determined by the relations of production, that is, the relations that its members needed to enter into to produce their means of survival.\n\nDeveloping on the ideas of Ludwig Feuerbach, he saw religion as a product of alienation that was functional to relieving people's immediate suffering, and as an ideology that masked the real nature of social relations.\nHe deemed it a contingent part of human culture, that would have disappeared after the abolition of class society.\n\nThese claims were limited, however, to his analysis of the historical relationship between European cultures, political institutions, and their Christian religious traditions.\nMarxist views strongly influenced individuals' comprehension and conclusions about society, among others the anthropological school of cultural materialism.\n\nMarx' explanations for all religions, always, in all forms, and everywhere have never been taken seriously by many experts in the field, though a substantial fraction accept that Marx' views possibly explain some aspects of religions.\n\nSome recent work has suggested that, while the standard account of Marx's analysis of religion is true, it is also only one side of a dialectical account, which takes seriously the disruptive, as well as the passifying moments of religion \nSigmund Freud (1856–1939) saw religion as an illusion, a belief that people very much wanted to be true. Unlike Tylor and Frazer, Freud attempted to explain why religion persists in spite of the lack of evidence for its tenets. Freud asserted that religion is a largely unconscious neurotic response to repression. By repression Freud meant that civilized society demands that we not fulfill all our desires immediately, but that they have to be repressed. Rational arguments to a person holding a religious conviction will not change the neurotic response of a person. This is in contrast to Tylor and Frazer, who saw religion as a rational and conscious, though primitive and mistaken, attempt to explain the natural world.\n\nIn his 1913 book \"Totem and Taboo\" he developed a speculative story about how all monotheist religions originated and developed. In the book he asserted that monotheistic religions grew out of a homicide in a clan of a father by his sons. This incident was subconsciously remembered in human societies.\n\nIn \"Moses and Monotheism\", Freud proposed that Moses had been a priest of Akhenaten who fled Egypt after the pharaoh's death and perpetuated monotheism through a different religion.\n\nFreud's view on religion was embedded in his larger theory of psychoanalysis, which has been criticized as unscientific. Although Freud's attempt to explain the historical origins of religions have not been accepted, his generalized view that all religions originate from unfulfilled psychological needs is still seen as offering a credible explanation in some cases.\nÉmile Durkheim (1858–1917) saw the concept of the sacred as the defining characteristic of religion, not faith in the supernatural. He saw religion as a reflection of the concern for society. He based his view on recent research regarding totemism among the Australian aboriginals. With totemism he meant that each of the many clans had a different object, plant, or animal that they held sacred and that symbolizes the clan. Durkheim saw totemism as the original and simplest form of religion. According to Durkheim, the analysis of this simple form of religion could provide the building blocks for more complex religions. He asserted that moralism cannot be separated from religion. The sacred i.e. religion reinforces group interest that clash very often with individual interests. Durkheim held the view that the function of religion is group cohesion often performed by collectively attended rituals. He asserted that these group meeting provided a special kind of energy, which he called effervescence, that made group members lose their individuality and to feel united with the gods and thus with the group. Differing from Tylor and Frazer, he saw magic not as religious, but as an individual instrument to achieve something.\n\nDurkheim's proposed method for progress and refinement is first to carefully study religion in its simplest form in one contemporary society and then the same in another society and compare the religions then and only between societies that are the same.\nThe empirical basis for Durkheim's view has been severely criticized when more detailed studies of the Australian aboriginals surfaced. More specifically, the definition of religion as dealing with the sacred only, regardless of the supernatural, is not supported by studies of these aboriginals. The view that religion has a social aspect, at the very least, introduced in a generalized very strong form by Durkheim has become influential and uncontested.\n\nDurkheim's approach gave rise to functionalist school in sociology and anthropology Functionalism is a sociological paradigm that originally attempted to explain social institutions as collective means to fill individual biological needs, focusing on the ways in which social institutions fill social needs, especially social stability. Thus because Durkheim viewed society as an \"organismic analogy of the body, wherein all the parts work together to maintain the equilibrium of the whole, religion was understood to be the glue that held society together.\".\nThe anthropologist Bronisław Malinowski (1884–1942) was strongly influenced by the functionalist school and argued that religion originated from coping with death. He saw science as practical knowledge that every society needs abundantly to survive and magic as related to this practical knowledge, but generally dealing with phenomena that humans cannot control.\n\nMax Weber (1864–1920) thought that the truth claims of religious movement were irrelevant for the scientific study of the movements. He portrayed each religion as rational and consistent in their respective societies.\nWeber acknowledged that religion had a strong social component, but diverged from Durkheim by arguing, for example in his book The Protestant Ethic and the Spirit of Capitalism that religion can be a force of change in society. In the book Weber wrote that modern capitalism spread quickly partially due to the Protestant worldly ascetic morale. Weber's main focus was not on developing a theory of religion but on the interaction between society and religion, while introducing concepts that are still widely used in the sociology of religion. These concept include\n\nSomewhat differing from Marx, Weber dealt with status groups, not with class. In status groups the primary motivation is prestige and social cohesion. Status groups have differing levels of access to power and prestige and indirectly to economic resources. In he saw Confucianism as helping a certain status group, i.e. the educated elite to maintain access to prestige and power. He asserted that Confucianism opposition against both extravagance and thrift made it unlikely that capitalism could have originated in China.\n\nHe used the concept of \"Verstehen\" (German for \"understanding\") to describe his method of interpretation of the intention and context of human action.\n\nThe rational choice theory has been applied to religions, among others by the sociologists Rodney Stark (1934 – ) and William Sims Bainbridge (1940 – ). They see religions as systems of \"compensators\", and view human beings as \"rational actors, making choices that she or he thinks best, calculating costs and benefits\". Compensators are a body of language and practices that compensate for some physical lack or frustrated goal. They can be divided into specific compensators (compensators for the failure to achieve specific goals), and general compensators (compensators for failure to achieve any goal). They define religion as a system of compensation that relies on the supernatural. The main reasoning behind this theory is that the compensation is what controls the choice, or in other words the choices which the \"rational actors\" make are \"rational in the sense that they are centered on the satisfaction of wants\".\n\nIt has been observed that social or political movements that fail to achieve their goals will often transform into religions. As it becomes clear that the goals of the movement will not be achieved by natural means (at least within their lifetimes), members of the movement will look to the supernatural to achieve what cannot be achieved naturally. The new religious beliefs are compensators for the failure to achieve the original goals. Examples of this include the counterculture movement in America: the early counterculture movement was intent on changing society and removing its injustice and boredom; but as members of the movement proved unable to achieve these goals they turned to Eastern and new religions as compensators.\n\nMost religions start out their lives as cults or sects, i.e. groups in high tension with the surrounding society, containing different views and beliefs contrary to the societal norm. Over time, they tend to either die out, or become more established, mainstream and in less tension with society. Cults are new groups with a new novel theology, while sects are attempts to return mainstream religions to (what the sect views as) their original purity. Mainstream established groups are called denominations. The comments below about cult formation apply equally well to sect formation.\n\nThere are four models of cult formation: the Psychopathological Model, the Entrepreneurial Model, the Social Model and the Normal Revelations model.\nSome religions are better described by one model than another, though all apply to differing degrees to all religions.\n\nOnce a cult or sect has been founded, the next problem for the founder is to convert new members to it. Prime candidates for religious conversion are those with an openness to religion, but who do not belong or fit well in any existing religious group. Those with no religion or no interest in religion are difficult to convert, especially since the cult and sect beliefs are so extreme by the standards of the surrounding society. But those already happy members of a religious group are difficult to convert as well, since they have strong social links to their preexisting religion and are unlikely to want to sever them in order to join a new one. The best candidates for religious conversion are those who are members of or have been associated with religious groups (thereby showing an interest or openness to religion), yet exist on the fringe of these groups, without strong social ties to prevent them from joining a new group.\n\nPotential converts vary in their level of social connection. New religions best spread through pre-existing friendship networks. Converts who are marginal with few friends are easy to convert, but having few friends to convert they cannot add much to the further growth of the organization. Converts with a large social network are harder to convert, since they tend to have more invested in mainstream society; but once converted they yield many new followers through their friendship network.\n\nCults initially can have quite high growth rates; but as the social networks that initially feed them are exhausted, their growth rate falls quickly. On the other hand, the rate of growth is exponential (ignoring the limited supply of potential converts): the more converts you have, the more missionaries you can have out looking for new converts. But nonetheless it can take a very long time for religions to grow to a large size by natural growth. This often leads to cult leaders giving up after several decades, and withdrawing the cult from the world.\n\nIt is difficult for cults and sects to maintain their initial enthusiasm for more than about a generation. As children are born into the cult or sect, members begin to demand a more stable life. When this happens, cults tend to lose or de-emphasise many of their more radical beliefs, and become more open to the surrounding society; they then become denominations.\n\nThe theory of religious economy sees different religious organizations competing for followers in a religious economy, much like the way businesses compete for consumers in a commercial economy. Theorists assert that a true religious economy is the result of religious pluralism, giving the population a wider variety of choices in religion. According to the theory, the more religions there are, the more likely the population is to be religious and hereby contradicting the secularization thesis.\n"}
{"id": "15920169", "url": "https://en.wikipedia.org/wiki?curid=15920169", "title": "University of California, Irvine School of Humanities", "text": "University of California, Irvine School of Humanities\n\nThe School of Humanities is one of the academic units of the University of California, Irvine. \nUpon the school's opening in 1965, the Division of Humanities was one of the five liberal arts divisions that the campus had to offer. Samuel McCulloch was appointed as UC Irvine's founding dean of Humanities 1953. The School hosts the Thesaurus Linguae Graecae and the University of California Humanities Research Institute.\n\nThe division of humanities is one of the five liberal arts divisions that opened with the campus in 1965. In 1963, Samuel McCulloch was appointed as the founding dean of UC Irvine Division of Humanities and laid its foundation. Upon the campus' opening day, he created four departments within the divion with a total of thirty-one faculty members. The four departments were: English and Comparative Literature, Foreign Languages and Literature, History, and Philosophy. By 1967, the governing body of UC Irvine turned the five liberal arts divisions into individual schools and the Division of Humanities had become the School of Humanities. In 1970 the Humanities Core Course was implemented and there was a focus on strengthening existing Humanities departments for the rest of the 1970s. The school also launched a doctorate in Critical Theory, making UC Irvine to be the only university in the country to offer a doctorate in the field.\nIn the 1980s, Dean Ken Bailes founded the Organized Research Unit in Critical Theory and brought the system wide University of California Humanities Research Institute to UC Irvine. The School of Humanities started gaining national recognition and it expanded and grew throughout the 1990s and 2000s. The Humanities Instructional Building and Humanities Gateway were built and completed the Humanities Quad. The school today continues to expand by adding special programs and by creating cross collaboration with different schools on campus. The school has a total of 13 departments and over 20 majors and several interdisciplinary programs.\nThe University of California, Irvine School of Humanities has various programs that are offered to the undergraduate and graduate students. Each program has their own faculty, classes, and events that are aimed to help students develop further into each respective field. Programs in the School of Humanities are among the highest-ranked in the country, contributing to UCI’s reputation as one of the very best public universities. \nAfrican American Studies is an interdisciplinary program which offers undergraduate students an opportunity to study those societies and cultures established by the people of the African diaspora. The program focuses and encourages students to explore the African American experience from a variety of disciplinary perspectives and theoretical approaches. Some topics explored in this program is the process of colonization and the forced migration of African people, the position of African people in the western hemisphere, the rhetoric produced by and about African people, and the cultural and aesthetic values associated with \"blackness\" and \"Africanness.\" This program offers a B.A. degree program in African American studies and a minor.\nThe Academic English is also known as the English as a Second Language Program (ESL), which offers English language courses for students at UCI. The program offers classes in academic writing, reading and vocabulary development, pronunciation, conversation, and grammar. The goal of the program's courses is to strengthen the English skills of students so they can complete the academic requirements of UCI.\nA Ph.D program offered to graduates in Culture and Theory. The program provides a strong theoretical and critical approach to race, gender, and sexuality studies. The program is most associated with interdisciplinary units in African American Studies, Asian American Studies, Chicano/Latino Studies, and Gender and Sexuality Studies, and works alongside the Critical Theory Emphasis. The program uses a problem-orientated approach to issues of race, gender, and sexuality.\nIn the Global Cultures Program, students are provided with the skills and knowledge that are necessary for understanding the complexity of the diverse world. The mission of the program is: Explore Globalization from a humanistic perspective.\nHumanities Core Course is a year-long freshman program that focuses on developing critical reading and writing skills in relation to text and issues in the humanities. This course satisfies 6 general education requirements: 2 in lower-division writing,3 in Arts and Humanities, 1 in Multicultural Studies.\nThe learning of languages other than English helps foster global literacy and it helps bring international diversity to University of California, Irvine. This program provides instruction in languages not required for undergraduate and graduate degree programs.\nA program that focuses on the ways different people from times have developed their values and traditions. The main mission is to understand the role of religion in human experience and thought.\nThe School of Humanities also offer special programs such as: Humanities and Law, Asian Studies, Jewish Studies, and Latin American Studies.\n\n\n\n\n\n\nU.S. News & World Report\n\n"}
{"id": "7159081", "url": "https://en.wikipedia.org/wiki?curid=7159081", "title": "Victorian burlesque", "text": "Victorian burlesque\n\nVictorian burlesque, sometimes known as travesty or extravaganza, is a genre of theatrical entertainment that was popular in Victorian England and in the New York theatre of the mid 19th century. It is a form of parody in which a well-known opera or piece of classical theatre or ballet is adapted into a broad comic play, usually a musical play, usually risqué in style, mocking the theatrical and musical conventions and styles of the original work, and often quoting or pastiching text or music from the original work. Victorian burlesque is one of several forms of burlesque.\n\nLike ballad opera, burlesques featured musical scores drawing on a wide range of music, from popular contemporary songs to operatic arias, although later burlesques, from the 1880s, sometimes featured original scores. Dance played an important part, and great attention was paid to the staging, costumes and other spectacular elements of stagecraft, as many of the pieces were staged as extravaganzas. Many of the male roles were played by actresses as breeches roles, purposely to show off their physical charms, and some of the older female roles were taken by male actors.\n\nOriginally short, one-act pieces, burlesques were later full-length shows, occupying most or all of an evening's programme. Authors who wrote burlesques included J. R. Planché, H. J. Byron, G. R. Sims, F. C. Burnand, W. S. Gilbert and Fred Leslie.\n\nBurlesque theatre became popular around the beginning of the Victorian era. The word \"burlesque\" is derived from the Italian \"burla\", which means \"ridicule or mockery\". According to the \"Grove Dictionary of Music and Musicians\", Victorian burlesque was \"related to and in part derived from pantomime and may be considered an extension of the introductory section of pantomime with the addition of gags and 'turns'.\" Another antecedent was ballad opera, in which new words were fitted to existing tunes.\nMadame Vestris produced burlesques at the Olympic Theatre beginning in 1831 with \"Olympic Revels\" by J. R. Planché. In these pieces, comedy stemmed from the incongruity and absurdity of the grand classical subjects, with realistic historical dress and settings, being juxtaposed with the everyday modern activities portrayed by the actors. For example, \"Olympic Revels\" opens with the gods of Olympus in classical Greek dress playing whist. In the early burlesques, the words of the songs were written to popular music, as had been done earlier in \"The Beggar's Opera\". Later in the Victorian era, burlesque mixed operetta, music hall and revue, and some of the large-scale burlesque spectacles were known as extravaganzas. The English style of burlesque was successfully launched in New York in the 1840s by the manager and comedian William Mitchell, who had opened his Olympic Theatre in December 1839. Like the London prototypes, his burlesques included characters with nonsensical names such as Wunsuponatyme and The King of Neverminditsnamia, and made fun of all kinds of music currently being presented in the city.\n\nUnlike pantomime, which aimed at all ages and classes, burlesque was aimed at a narrower, highly literate audience; some writers, such as the Brough brothers, aimed at a conservative middle class audience, and H. J. Byron's success was attributed to his skill in appealing to the lower middle classes. Some of the most frequent subjects for burlesque were the plays of Shakespeare and grand opera. From the 1850s onwards, burlesquing of Italian, French and, later in the century, German opera was popular with London audiences. Verdi's \"Il trovatore\" and \"La traviata\" received their British premieres in 1855 and 1856 respectively; British burlesques of them followed quickly. \"Our Lady of the Cameleon\" by Leicester Silk Buckingham and \"Our Traviata\" by William F. Vandervell (both 1857) were followed by five different burlesque treatments of \"Il trovatore\", two of them by H. J. Byron: \"Ill Treated Trovatore, or the Mother the Maiden and the Musicianer\" (1863) and \"Il Trovatore or Larks with a Libretto\" (1880). The operas of Bellini, Bizet, Donizetti, Gounod, Handel, Meyerbeer, Mozart, Rossini, Wagner and Weber were burlesqued. In a 2003 study of the subject, Roberta Montemorra Marvin noted:\n\nBy the 1880s, almost every truly popular opera had become the subject of a burlesque. Generally appearing after an opera's premiere or following a successful revival, they usually enjoyed local production runs, often for a month or longer. The popularity of stage burlesque in general and operatic burlesque in particular seems to have stemmed from the many ways in which it entertained a diverse group, and the manner in which it fed and fed on the circus-like or carnivalesque atmosphere of public Victorian London.\n\nW. S. Gilbert wrote five opera burlesques early in his career, beginning with \"Dulcamara, or the Little Duck and the Great Quack\" (1866), the most successful of which was \"Robert the Devil\" (1868). In the 1870s, Lydia Thompson's burlesque troupe, with Willie Edouin, became famous for their burlesques, by such authors as H. B. Farnie and Robert Reece, both in Britain and the U.S.\n\nThe Shakespeare scholar Stanley Wells notes that although parodies of Shakespeare had appeared even in Shakespeare's lifetime, the heyday of Shakespearean burlesque was the Victorian era. Wells observes that the typical Victorian Shakespeare burlesque \"takes a Shakespeare play as its point of departure and creates from it a mainly comic entertainment, often in ways that bear no relation to the original play.\" Wells gives, as an example of the puns in the texts, the following: Macbeth and Banquo make their first entrance under an umbrella. The witches greet them with \"Hail! hail! hail!\": Macbeth asks Banquo, \"What mean these salutations, noble thane?\" and is told \"These showers of 'Hail' anticipate your 'reign'\". Musically, Shakespearean burlesques were as varied as the others of the genre. An 1859 burlesque of \"Romeo and Juliet\" contained 23 musical numbers, some from opera, such as the serenade from \"Don Pasquale\", and some from traditional airs and popular songs of the day including \"Buffalo Gals\", and \"Nix my Dolly\".\nThe dialogue for burlesques was generally written in rhyming couplets, or, less often, in other verse forms, such as blank verse; it was notable for its bad puns. For example, in \"Faust up to Date\" (1888), a couplet reads:\n\nAccording to \"Grove\", although \"an almost indispensable element of burlesque was the display of attractive women dressed in tights, often in travesty roles ... the plays themselves did not normally tend to indecency.\" Some contemporary critics took a sterner view; in an 1885 article, the critic Thomas Heyward praised Planché (\"fanciful and elegant\") and Gilbert (\"witty, never vulgar\"), but wrote of the genre as a whole, \"the flashy, 'leggy', burlesque, with its 'slangy' songs, loutish 'breakdowns', vulgar jests, paltry puns and witless grimacing at all that is graceful and poetic is simply odious. … Burlesque, insensate, spiritless and undiscriminating, demoralizes both the audience and the players. It debases the public taste.\" Gilbert expressed his own views on the worth of burlesque:\n\nThe question whether burlesque has a claim to rank as art is, I think, one of degree. Bad burlesque is as far removed from true art as is a bad picture. But burlesque in its higher development calls for high intellectual power on the part of its professors. Aristophanes, Rabelais, Geo Cruikshank, the authors of the \"Rejected Addresses\", John Leech, Planché were all in their respective lines professors of true burlesque.\nIn his 1859 Longfellow burlesque \"Hi-A-Wa-Tha\", the American playwright Charles Walcot encapsulated the character of burlesque in the epilogue, addressed to the audience by Mrs. John Wood as Minnehaha:\n\nIn a similar vein, ten years later, Gilbert gave an English viewpoint on burlesque, in his epilogue to \"The Pretty Druidess\":\n\nActresses in burlesque would often play breeches roles, which were male roles played by women; likewise, men eventually began to play older female roles. These reversals allowed viewers to distance themselves from the morality of the play, focusing more on joy and entertainment than catharsis, a definitive shift away from neoclassical ideas.\n\nThe depiction of female sexuality in Victorian burlesque was an example of the connection between women as performers and women as sexual objects in Victorian culture. Throughout the history of theatre the participation of women on stage has been questioned. Victorian culture viewed paid female performance as being closely associated with prostitution, “a profession in which most women in the theatre dabbled, if not took on as a primary source of income.”\n\nBurlesque became the specialty of London's Royal Strand Theatre and Gaiety Theatre from the 1860s to the early 1890s. In the 1860s and 1870s, burlesques were often one-act pieces running less than an hour and using pastiches and parodies of popular songs, opera arias and other music that the audience would readily recognize. Nellie Farren starred as the Gaiety Theatre's \"principal boy\" from 1868, and John D'Auban choreographed the burlesques there from 1868 to 1891. Edward O'Connor Terry joined the theatre in 1876. Early Gaiety burlesques included \"Robert the Devil\" (1868, by Gilbert), \"The Bohemian G-yurl and the Unapproachable Pole\" (1877), \"Blue Beard\" (1882), \"Ariel\" (1883, by F. C. Burnand) and \"Galatea, or Pygmalion Reversed\" (1883).\n\nBeginning in the 1880s, when comedian-writer Fred Leslie joined the Gaiety, composers like Meyer Lutz and Osmond Carr contributed original music to the burlesques, which were extended to a full-length two- or three-act format. These later Gaiety burlesques starred Farren and Leslie. They often included Leslie's libretti, written under his pseudonym, \"A. C. Torr\", and were usually given an original score by Lutz: \"Little Jack Sheppard\" (1885), \"Monte Cristo Jr.\" (1886), \"Pretty Esmeralda\" (1887), \"Frankenstein, or The Vampire's Victim\" (1887), \"Mazeppa\" and \"Faust up to Date\" (1888). \"Ruy Blas and the Blasé Roué\" (1889) made fun of the play \"Ruy Blas\" by Victor Hugo. The title was a pun, and the worse the pun, the more Victorian audiences were amused. The last Gaiety burlesques were \"Carmen up to Data\" (1890), \"Cinder Ellen up too Late\" (1891), and \"Don Juan\" (1892, with lyrics by Adrian Ross).\n\nIn the early 1890s, Farren retired, Leslie died, and musical burlesque went out of fashion in London, as the focus of the Gaiety and other burlesque theatres changed to the new genre of Edwardian musical comedy. In 1896, Seymour Hicks declared that burlesque \"is dead as a doornail and will never be revived.\" From her retirement, Nellie Farren endorsed this judgment.\n\n\n\n"}
{"id": "12289130", "url": "https://en.wikipedia.org/wiki?curid=12289130", "title": "Zhuang studies", "text": "Zhuang studies\n\nZhuang studies (or Zhuangology; Standard Zhuang: Cuenghhag; ) is an interdisciplinary intellectual field concerned with the Zhuang people – their history, anthropology, religion, politics, languages, and literature. The majority of such research is being carried out in the People's Republic of China. Huang Xianfan (黄现璠) is considered by many to be the father of Zhuang studies.\n\nAreas commonly included under this rubric include history of Zhuang, literature of Zhuang, art of Zhuang, music of Zhuang, language of Zhuang, sociology of Zhuang, political science of Zhuang, economics of Zhuang, folklore of Zhuang, and ethnomusicology of Zhuang. It may be compared to other ethnic groups studies disciplines, such as Tai studies and Yao studies. Zhuang studies is sometimes included within a broader regional area of focus including: \"Lingnan studies\", \"Yue people studies\",\"South Asia studies\", or \"ASEAN Studies\".\n\nZhuang studies is a relatively new discipline.\n\nHuang Xianfan may be considered as the first Zhuangologist and he did much to make Zhuang known in China. Since 1950, Huang Xianfan led the group making a largest and deepest investigation on ethnic history and traditional culture in Guangxi history. They had collected a lot of valuable materials and laid a foundation for further research on Zhuang ethnic social and historical culture.That was a very important beginning for later development of Zhuang ethnic research and establishment of Guangxi institute of ethnic studies. Therefore, the inception of Zhuang studies as an authentic academic discipline is thus associated with the first ethnologist Huang Xianfan de Zhuang who is considered as its founder to present day, the other early zhuang studies of note being Huang Zengqing (Huang group's member and the first archaeological researcher of the Guangxi) who in 1957 occupied the first chair for Zhuang's archaeology studies in China and Zhou Zuoqiou, who was primarily the pioneering subject of the zhuang literature in Guangxi Normal University.\n\nThe Bagui School (The first ethnic school in China and pioneer is Huang Xianfan) was particularly significant for the development of the discipline since the early 1950s with Huang Xianfan, Huang Zengqing, Ban Xiouwen, Ou Yang Ruoxiou, Qin Cailuan, Qin Naichang, Qin Shengmin, He Longqun, Yu Shijie, Qin Deqing, Pan Qixu, Huang Hanjin and Zeng Chaoxiong.,\n\nIn 1957, the Guangxi government established the Guangxi institute of ethnic studies to promote Zhuang studies around China. In 1991 the Guangxi Zhuang Studies Society was established, and in April 1999 the first international Zhuang studies conference was held in Wuming with scholars from 8 different countries.\n\nOver the last few decades in other countries the studies of the Zhuang have opened towards other disciplines, resulting in works with interdisciplinary approach. As examples of such open-minded Zhuang researcher we might mention the American anthropologist Jeffrey Barlow, among others, who has done noted research and publications on lexical questions, about Zhuang culture and the modern history of Zhuang. Others are Japanese anthropologist Chikada Sigeyuki, Australian anthropologist David Holm, and many more.\n\n\n"}
