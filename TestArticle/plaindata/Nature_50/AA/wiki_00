{"id": "4327451", "url": "https://en.wikipedia.org/wiki?curid=4327451", "title": "Adolf Dygasiński", "text": "Adolf Dygasiński\n\nAdolf Dygasiński (March 7, 1839, Niegosławice–June 3, 1902, Grodzisk Mazowiecki) was a Polish novelist, publicist and educator. In Polish literature, he was one of the leading representatives of Naturalism. \nDuring his literary career, Dygasiński wrote forty-two short stories and novels. \nSince 1884 his works were being published in book-form and enjoyed considerable success. \nThey were translated into Russian and German. \nIn 1891, Dygasiński went on a trip to Brazil on a trail of Polish emigrants from Partitioned Poland. \nHe produced a series of letters describing the tragic fate of Polish emigrees in South America. In the following years Dygasiński maintained a position of a tutor and coach for numerous wealthy landowning families. Late in life he settled in Warsaw, where he died on June 6, 1902, and was buried at the local Powązkowski Cemetery.\n\nIn his work Dygasiński often focused on topics of rural life and residents of small towns, highlighting the common fate of both, human and animal communities. Some of his most important work include: \n\n\n\n"}
{"id": "25646039", "url": "https://en.wikipedia.org/wiki?curid=25646039", "title": "Boston Journal of Natural History", "text": "Boston Journal of Natural History\n\nThe Boston Journal of Natural History (1834-1863) was a scholarly journal published by the Boston Society of Natural History in mid-19th century Massachusetts. Contributors included Charles T. Jackson, Augustus A. Gould, and others. Each volume featured lithographic illustrations, some in color, drawn/engraved by E.W. Bouvé, B.F. Nutting, A. Sonrel, \"et al.\" and printed by Pendleton's Lithography and other firms.\n\nThe journal was continued by \"Memoirs Read Before the Boston Society of Natural History\" in 1863.\n\n"}
{"id": "4024", "url": "https://en.wikipedia.org/wiki?curid=4024", "title": "Butterfly effect", "text": "Butterfly effect\n\nIn chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n\nThe term, coined by Edward Lorenz, is derived from the metaphorical example of the details of a tornado (the exact time of formation, the exact path taken) being influenced by minor perturbations such as the flapping of the wings of a distant butterfly several weeks earlier. Lorenz discovered the effect when he observed that runs of his weather model with initial condition data that was rounded in a seemingly inconsequential manner would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.\n\nThough Lorenz gave a name to the phenomenon, the idea that small causes may have large effects in general and in weather specifically was earlier recognized by French mathematician and engineer Henri Poincaré and American mathematician and philosopher Norbert Wiener. Edward Lorenz's work placed the concept of \"instability\" of the earth's atmosphere onto a quantitative base and linked the concept of instability to the properties of large classes of dynamic systems which are undergoing nonlinear dynamics and deterministic chaos.\n\nThe butterfly effect can also be demonstrated by very simple systems.\n\nIn \"The Vocation of Man\" (1800), Johann Gottlieb Fichte says that \"you could not remove a single grain of sand from its place without thereby ... changing something throughout all parts of the immeasurable whole\".\n\nChaos theory and the sensitive dependence on initial conditions were described in the literature in a particular case of the three-body problem by Henri Poincaré in 1890. He later proposed that such phenomena could be common, for example, in meteorology.\n\nIn 1898, Jacques Hadamard noted general divergence of trajectories in spaces of negative curvature. Pierre Duhem discussed the possible general significance of this in 1908.\n\nThe idea that one butterfly could eventually have a far-reaching ripple effect on subsequent historic events made its earliest known appearance in \"A Sound of Thunder\", a 1952 short story by Ray Bradbury about time travel.\n\nIn 1961, Lorenz was running a numerical computer model to redo a weather prediction from the middle of the previous run as a shortcut. He entered the initial condition 0.506 from the printout instead of entering the full precision 0.506127 value. The result was a completely different weather scenario.\n\nLorenz wrote:\nIn 1963 Lorenz published a theoretical study of this effect in a highly cited, seminal paper called \"Deterministic Nonperiodic Flow\" (the calculations were performed on a Royal McBee LGP-30 computer). Elsewhere he stated: Following suggestions from colleagues, in later speeches and papers Lorenz used the more poetic butterfly. According to Lorenz, when he failed to provide a title for a talk he was to present at the 139th meeting of the American Association for the Advancement of Science in 1972, Philip Merilees concocted \"Does the flap of a butterfly’s wings in Brazil set off a tornado in Texas?\" as a title. Although a butterfly flapping its wings has remained constant in the expression of this concept, the location of the butterfly, the consequences, and the location of the consequences have varied widely.\n\nThe phrase refers to the idea that a butterfly's wings might create tiny changes in the atmosphere that may ultimately alter the path of a tornado or delay, accelerate or even prevent the occurrence of a tornado in another location. The butterfly does not power or directly create the tornado, but the term is intended to imply that the flap of the butterfly's wings can \"cause\" the tornado: in the sense that the flap of the wings is a part of the initial conditions; one set of conditions leads to a tornado while the other set of conditions doesn't. The flapping wing represents a small change in the initial condition of the system, which cascades to large-scale alterations of events (compare: domino effect). Had the butterfly not flapped its wings, the trajectory of the system might have been vastly different—but it's also equally possible that the set of conditions without the butterfly flapping its wings is the set that leads to a tornado.\n\nThe butterfly effect presents an obvious challenge to prediction, since initial conditions for a system such as the weather can never be known to complete accuracy. This problem motivated the development of ensemble forecasting, in which a number of forecasts are made from perturbed initial conditions.\n\nSome scientists have since argued that the weather system is not as sensitive to initial conditions as previously believed. David Orrell argues that the major contributor to weather forecast error is model error, with sensitivity to initial conditions playing a relatively small role. Stephen Wolfram also notes that the Lorenz equations are highly simplified and do not contain terms that represent viscous effects; he believes that these terms would tend to damp out small perturbations.\n\nRecurrence, the approximate return of a system towards its initial conditions, together with sensitive dependence on initial conditions, are the two main ingredients for chaotic motion. They have the practical consequence of making complex systems, such as the weather, difficult to predict past a certain time range (approximately a week in the case of weather) since it is impossible to measure the starting atmospheric conditions completely accurately.\n\nA dynamical system displays sensitive dependence on initial conditions if points arbitrarily close together separate over time at an exponential rate. The definition is not topological, but essentially metrical.\n\nIf \"M\" is the state space for the map formula_1, then formula_1 displays sensitive dependence to initial conditions if for any x in \"M\" and any δ > 0, there are y in \"M\", with distance \"d\"(. , .) such that formula_3 and such that\n\nfor some positive parameter \"a\". The definition does not require that all points from a neighborhood separate from the base point \"x\", but it requires one positive Lyapunov exponent.\n\nThe simplest mathematical framework exhibiting sensitive dependence on initial conditions is provided by a particular parametrization of the logistic map:\n\nwhich, unlike most chaotic maps, has a closed-form solution:\n\nwhere the initial condition parameter formula_7 is given by formula_8. For rational formula_7, after a finite number of iterations formula_10 maps into a periodic sequence. But almost all formula_7 are irrational, and, for irrational formula_7, formula_10 never repeats itself – it is non-periodic. This solution equation clearly demonstrates the two key features of chaos – stretching and folding: the factor 2 shows the exponential growth of stretching, which results in sensitive dependence on initial conditions (the butterfly effect), while the squared sine function keeps formula_10 folded within the range [0, 1].\n\nThe butterfly effect is most familiar in terms of weather; it can easily be demonstrated in standard weather prediction models, for example. The climate scientists James Annan and William Connolley explain that chaos is important in the development of weather prediction methods; models are sensitive to initial conditions. They add the caveat: \"Of course the existence of an unknown butterfly flapping its wings has no direct bearing on weather forecasts, since it will take far too long for such a small perturbation to grow to a significant size, and we have many more immediate uncertainties to worry about. So the direct impact of this phenomenon on weather prediction is often somewhat overstated.\"\n\nThe potential for sensitive dependence on initial conditions (the butterfly effect) has been studied in a number of cases in semiclassical and quantum physics including atoms in strong fields and the anisotropic Kepler problem. Some authors have argued that extreme (exponential) dependence on initial conditions is not expected in pure quantum treatments; however, the sensitive dependence on initial conditions demonstrated in classical motion is included in the semiclassical treatments developed by Martin Gutzwiller and Delos and co-workers.\n\nOther authors suggest that the butterfly effect can be observed in quantum systems. Karkuszewski et al. consider the time evolution of quantum systems which have slightly different Hamiltonians. They investigate the level of sensitivity of quantum systems to small changes in their given Hamiltonians. Poulin et al. presented a quantum algorithm to measure fidelity decay, which \"measures the rate at which identical initial states diverge when subjected to slightly different dynamics\". They consider fidelity decay to be \"the closest quantum analog to the (purely classical) butterfly effect\". Whereas the classical butterfly effect considers the effect of a small change in the position and/or velocity of an object in a given Hamiltonian system, the quantum butterfly effect considers the effect of a small change in the Hamiltonian system with a given initial position and velocity. This quantum butterfly effect has been demonstrated experimentally. Quantum and semiclassical treatments of system sensitivity to initial conditions are known as quantum chaos.\n\nThe journalist Peter Dizikes, writing in \"The Boston Globe\" in 2008, notes that popular culture likes the idea of the butterfly effect, but gets it wrong. Whereas Lorenz suggested correctly with his butterfly metaphor that predictability \"is inherently limited\", popular culture supposes that each event can be explained by finding the small reasons that caused it. Dizikes explains: \"It speaks to our larger expectation that the world should be comprehensible – that everything happens for a reason, and that we can pinpoint all those reasons, however small they may be. But nature itself defies this expectation.\"\n\n\n\n"}
{"id": "2533237", "url": "https://en.wikipedia.org/wiki?curid=2533237", "title": "Catalytic triad", "text": "Catalytic triad\n\nA catalytic triad is a set of three coordinated amino acids that can be found in the active site of some enzymes. Catalytic triads are most commonly found in hydrolase and transferase enzymes (e.g. proteases, amidases, esterases, acylases, lipases and β-lactamases). An Acid-Base-Nucleophile triad is a common motif for generating a nucleophilic residue for covalent catalysis. The residues form a charge-relay network to polarise and activate the nucleophile, which attacks the substrate, forming a covalent intermediate which is then hydrolysed to release the product and regenerate free enzyme. The nucleophile is most commonly a serine or cysteine amino acid, but occasionally threonine or even selenocysteine. The 3D structure of the enzyme brings together the triad residues in a precise orientation, even though they may be far apart in the sequence (primary structure).\n\nAs well as divergent evolution of function (and even the triad's nucleophile), catalytic triads show some of the best examples of convergent evolution. Chemical constraints on catalysis have led to the same catalytic solution independently evolving in at least 23 separate superfamilies. Their mechanism of action is consequently one of the best studied in biochemistry.\n\nThe enzymes trypsin and chymotrypsin were first purified in the 1930s. A serine in each of trypsin and chymotrypsin was identified as the catalytic nucleophile (by diisopropyl fluorophosphate modification) in the 1950s. The structure of chymotrypsin was solved by X-ray crystallography in the 1960s, showing the orientation of the catalytic triad in the active site. Other proteases were sequenced and aligned to reveal a family of related proteases, now called the S1 family. Simultaneously, the structures of the evolutionarily unrelated papain and subtilisin proteases were found to contain analogous triads. The 'charge-relay' mechanism for the activation of the nucleophile by the other triad members was proposed in the late 1960s. As more protease structures were solved by X-ray crystallography in the 1970s and 80s, homologous (such as TEV protease) and analogous (such as papain) triads were found. The MEROPS classification system in the 1990s and 2000s began classing proteases into structurally related enzyme superfamilies and so acts as a database of the convergent evolution of triads in over 20 superfamilies. Understanding how chemical constraints on evolution led to the convergence of so many enzyme families on the same triad geometries has developed in the 2010s.\n\nSince their initial discovery, there have been increasingly detailed investigations of their exact catalytic mechanism. Of particular contention in the 1990s and 2000s was whether low-barrier hydrogen bonding contributed to catalysis, or whether ordinary hydrogen bonding is sufficient to explain the mechanism. The massive body of work on the charge-relay, covalent catalysis used by catalytic triads has led to the mechanism being the best characterised in all of biochemistry.\n\nEnzymes that contain a catalytic triad use it for one of two reaction types: either to split a substrate (hydrolases) or to transfer one portion of a substrate over to a second substrate (transferases). Triads are an inter-dependent set of residues in the active site of an enzyme and act in concert with other residues (e.g. binding site and oxyanion hole) to achieve nucleophilic catalysis. These triad residues act together to make the nucleophile member highly reactive, generating a covalent intermediate with the substrate that is then resolved to complete catalysis.\n\nCatalytic triads perform covalent catalysis using a residue as a nucleophile. The reactivity of the nucleophilic residue is increased by the functional groups of the other triad members. The nucleophile is polarised and oriented by the base, which is itself bound and stabilised by the acid.\n\nCatalysis is performed in two stages. First, the activated nucleophile attacks the carbonyl carbon and forces the carbonyl oxygen to accept an electron, leading to a tetrahedral intermediate. The build-up of negative charge on this intermediate is typically stabilized by an oxyanion hole within the active site. The intermediate then collapses back to a carbonyl, ejecting the first half of the substrate, but leaving the second half still covalently bound to the enzyme as an acyl-enzyme intermediate. The ejection of this first leaving group is often aided by donation of a proton by the base.\n\nThe second stage of catalysis is the resolution of the acyl-enzyme intermediate by the attack of a second substrate. If this substrate is water then the result is hydrolysis; if it is an organic molecule then the result is transfer of that molecule onto the first substrate. Attack by this second substrate forms a new tetrahedral intermediate, which resolves by ejecting the enzyme's nucleophile, releasing the second product and regenerating free enzyme.\n\nThe side-chain of the nucleophilic residue performs covalent catalysis on the substrate. The lone pair of electrons present on the oxygen or sulphur attacks the electropositive carbonyl carbon. The 20 naturally occurring biological amino acids do not contain any sufficiently nucleophilic functional groups for many difficult catalytic reactions. Embedding the nucleophile in a triad increases its reactivity for efficient catalysis. The most commonly used nucleophiles are the hydroxyl (OH) of serine and the thiol/thiolate ion (SH/S) of cysteine. Alternatively, threonine proteases use the secondary hydroxyl of threonine, however due to steric hindrance of the side chain's extra methyl group such proteases use their \"N\"-terminal amide as the base, rather than a separate amino acid.\n\nUse of oxygen or sulphur as the nucleophilic atom causes minor differences in catalysis. Compared to oxygen, sulphur’s extra d orbital makes it larger (by 0.4 Å) and softer, allows it to form longer bonds (d and d by 1.3-fold), and gives it a lower p\"K\" (by 5 units). Serine is therefore more dependent than cysteine on optimal orientation of the acid-base triad members to reduce its p\"K\" in order to achieve concerted deprotonation with catalysis. The low p\"K\" of cysteine works to its disadvantage in the resolution of the first tetrahedral intermediate as unproductive reversal of the original nucleophilic attack is the more favourable breakdown product. The triad base is therefore preferentially oriented to protonate the leaving group amide to ensure that it is ejected to leave the enzyme sulphur covalently bound to the substrate N-terminus. Finally, resolution of the acyl-enzyme (to release the substrate C-terminus) requires serine to be re-protonated whereas cysteine can leave as S. Sterically, the sulphur of cysteine also forms longer bonds and has a bulkier van der Waals radius and if mutated to serine can be trapped in unproductive orientations in the active site.\n\nVery rarely, the selenium atom of the uncommon amino acid selenocysteine is used as a nucleophile. The deprotonated Se state is strongly favoured when in a catalytic triad.\n\nSince no natural amino acids are strongly nucleophilic, the base in a catalytic triad polarises and deprotonates the nucleophile to increase its reactivity. Additionally, it protonates the first product to aid leaving group departure.\n\nThe base is most commonly histidine since its p\"K\" allows for effective base catalysis, hydrogen bonding to the acid residue, and deprotonation of the nucleophile residue. β-lactamases such as TEM-1 use a lysine residue as the base. Because lysine's p\"K\" is so high (p\"K\"=11), a glutamate and several other residues act as the acid to stabilise its deprotonated state during the catalytic cycle. Threonine proteases use their \"N\"-terminal amide as the base, since steric crowding by the catalytic threonine's methyl prevents other residues from being close enough.\n\nThe acidic triad member forms a hydrogen bond with the basic residue. This aligns the basic residue by restricting its side-chain rotation, and polarises it by stabilising its positive charge. Two amino acids have acidic side chains at physiological pH (aspartate or glutamate) and so are the most commonly used for this triad member. Cytomegalovirus protease uses a pair of histidines, one as the base, as usual, and one as the acid. The second histidine is not as effective an acid as the more common aspartate or glutamate, leading to a lower catalytic efficiency. In some enzymes, the acid member of the triad is less necessary and some act only as a dyad. For example, papain uses asparagine as its third triad member which orients the histidine base but does not act as an acid. Similarly, hepatitis A virus protease contains an ordered water in the position where an acid residue should be.\n\nThe Serine-Histidine-Aspartate motif is one of the most thoroughly characterised catalytic motifs in biochemistry. The triad is exemplified by chymotrypsin, a model serine protease from the PA superfamily which uses its triad to hydrolyse protein backbones. The aspartate is hydrogen bonded to the histidine, increasing the p\"K\" of its imidazole nitrogen from 7 to around 12. This allows the histidine to act as a powerful general base and to activate the serine nucleophile. It also has an oxyanion hole consisting of several backbone amides which stabilises charge build-up on intermediates. The histidine base aids the first leaving group by donating a proton, and also activates the hydrolytic water substrate by abstracting a proton as the remaining OH attacks the acyl-enzyme intermediate.\n\nThe same triad has also convergently evolved in α/β hydrolases such as some lipases and esterases, however orientation of the triad members is reversed. Additionally, brain acetyl hydrolase (which has the same fold as a small G-protein) has also been found to have this triad. The equivalent Ser-His-\"Glu \"triad is used in acetylcholinesterase.\n\nThe second most studied triad is the Cysteine-Histidine-Aspartate motif. Several families of cysteine proteases use this triad set, for example TEV protease and papain. The triad acts similarly to serine protease triads, with a few notable differences. Due to cysteine's low p\"K\", the importance of the Asp to catalysis varies and several cysteine proteases are effectively Cys-His dyads (e.g. hepatitis A virus protease), whilst in others the cysteine is already deprotonated before catalysis begins (e.g. papain). This triad is also used by some amidases, such as \"N\"-glycanase to hydrolyse non-peptide C-N bonds.\n\nThe triad of cytomegalovirus protease uses histidine as both the acid and base triad members. Removing the acid histidine results in only a 10-fold activity loss (compared to >10,000-fold when aspartate is removed from chymotrypsin). This triad has been interpreted as a possible way of generating a less active enzyme to control cleavage rate.\n\nAn unusual triad is found in seldolisin proteases. The low p\"K\" of the glutamate carboxylate group means that it only acts as a base in the triad at very low pH. The triad is hypothesised to be an adaptation to specific environments like acidic hot springs (e.g. kumamolysin) or cell lysosome (e.g. tripeptidyl peptidase).\n\nThe endothelial protease vasohibin uses a cysteine as the nucleophile, but a serine to coordinate the histidine base. Despite the serine being a poor acid, it is still effective in orienting the histidine in the catalytic triad. Some homologues alternatively have a threonine instead of serine at the acid location.\n\nThreonine proteases, such as the proteasome protease subunit and ornithine acyltransferases use the secondary hydroxyl of threonine in a manner analogous to the use of the serine primary hydroxyl. However, due to the steric interference of the extra methyl group of threonine, the base member of the triad is the \"N\"-terminal amide which polarises an ordered water which, in turn, deprotonates the catalytic hydroxyl to increase its reactivity. Similarly, there exist equivalent 'serine only' and 'cysteine only' configurations such as penicillin acylase G and penicillin acylase V which are evolutionarily related to the proteasome proteases. Again, these use their \"N\"-terminal amide as a base.\n\nThis unusual triad occurs only in one superfamily of amidases. In this case, the lysine acts to polarise the middle serine. The middle serine then forms two strong hydrogen bonds to the nucleophilic serine to activate it (one with the side chain hydroxyl and the other with the backbone amide). The middle serine is held in an unusual \"cis\" orientation to facilitate precise contacts with the other two triad residues. The triad is further unusual in that the lysine and \"cis\"-serine both act as the base in activating the catalytic serine, but the same lysine also performs the role of the acid member as well as making key structural contacts.\n\nThe rare, but naturally occurring amino acid selenocysteine (Sec), can also be found as the nucleophile in some catalytic triads. Selenocysteine is similar to cysteine, but contains a selenium atom instead of a sulphur. An example is in the active site of thioredoxin reductase, which uses the selenium for reduction of disulphide in thioredoxin.\n\nIn addition to naturally occurring types of catalytic triads, protein engineering has been used to create enzyme variants with non-native amino acids, or entirely synthetic amino acids. Catalytic triads have also been inserted into otherwise non-catalytic proteins, or protein mimics.\n\nSubtilisin (a serine protease) has had its oxygen nucleophile replaced with each of sulphur, selenium, or tellurium. Cysteine and selenocysteine were inserted by mutagenesis, whereas the non-natural amino acid, tellurocysteine, was inserted using auxotrophic cells fed with synthetic tellurocysteine. These elements are all in the 16th periodic table column (chalcogens), so have similar properties. In each case, changing the nucleophile reduced the enzyme's protease activity, but increased a different activity. A sulphur nucleophile improved the enzymes transferase activity (sometimes called subtiligase). Selenium and tellurium nucleophiles converted the enzyme into a oxidoreductase. When the nucleophile of TEV protease was converted from cysteine to serine, it protease activity was strongly reduced, but was able to be restored by directed evolution.\n\nNon-catalytic proteins have been used as scaffolds, having catalytic triads inserted into them which were then improved by directed evolution. The Ser-His-Asp triad has been inserted into an antibody, as well as a range of other proteins. Similarly, catalytic triad mimics have been created in small organic molecules like diaryl diselenide, and displayed on larger polymers like Merrifield resins, and self-assembling short peptide nanostructures.\n\nThe sophistication of the active site network causes residues involved in catalysis (and residues in contact with these) to be highly evolutionarily conserved. However, there are examples of divergent evolution in catalytic triads, both in the reaction catalysed, and the residues used in catalysis. The triad remains the core of the active site, but it is evolutionarily adapted to serve different functions. Some proteins, called pseudoenzymes, have non-catalytic functions (e.g. regulation by inhibitory binding) and have accumulated mutations that inactivate their catalytic triad.\n\nCatalytic triads perform covalent catalysis via an acyl-enzyme intermediate. If this intermediate is resolved by water, the result is hydrolysis of the substrate. However, if the intermediate is resolved by attack by a second substrate, then the enzyme acts as a transferase. For example, attack by an acyl group results in an acyltransferase reaction. Several families of transferase enzymes have evolved from hydrolases by adaptation to exclude water and favour attack of a second substrate. In different members of the α/β-hydrolase superfamily, the Ser-His-Asp triad is tuned by surrounding residues to perform at least 17 different reactions. Some of these reactions are also achieved with mechanisms that have altered formation, or resolution of the acyl-enzyme intermediate, or that don't proceed via an acyl-enzyme intermediate.\n\nAdditionally, an alternative transferase mechanism has been evolved by amidophosphoribosyltransferases, which has two active sites. In the first active site, a cysteine triad hydrolyses a glutamine substrate to release free ammonia. The ammonia then diffuses though an internal tunnel in the enzyme to the second active site, where it is transferred to a second substrate.\n\nDivergent evolution of active site residues is slow, due to strong chemical constraints. Nevertheless, some protease superfamilies have evolved from one nucleophile to another. This can be inferred when a superfamily (with the same fold) contains families that use different nucleophiles. Such nucleophile switches have occurred several times during evolutionary history, however the mechanisms by which this happen are still unclear.\n\nWithin protease superfamilies that contain a mixture of nucleophiles (e.g. the PA clan), families are designated by their catalytic nucleophile (C=cysteine proteases, S=serine proteases).\nThe enzymology of proteases provides some of the clearest known examples of convergent evolution. The same geometric arrangement of triad residues occurs in over 20 separate enzyme superfamilies. Each of these superfamilies is the result of convergent evolution for the same triad arrangement within a different structural fold. This is because there are limited productive ways to arrange three triad residues, the enzyme backbone and the substrate. These examples reflect the intrinsic chemical and physical constraints on enzymes, leading evolution to repeatedly and independently converge on equivalent solutions.\n\nThe same triad geometries been converged upon by serine proteases such as the chymotrypsin and subtilisin superfamilies. Similar convergent evolution has occurred with cysteine proteases such as viral C3 protease and papain superfamilies. These triads have converged to almost the same arrangement due to the mechanistic similarities in cysteine and serine proteolysis mechanisms.\n\nFamilies of Cysteine proteases\nFamilies of Serine proteases\nThreonine proteases use the amino acid threonine as their catalytic nucleophile. Unlike cysteine and serine, threonine is a secondary hydroxyl (i.e. has a methyl group). This methyl group greatly restricts the possible orientations of triad and substrate as the methyl clashes with either the enzyme backbone or histidine base. When the nucleophile of a serine protease was mutated to threonine, the methyl occupied a mixture of positions, most of which prevented substrate binding. Consequently, the catalytic residue of a threonine protease is located at it \"N\"-terminus.\n\nTwo evolutionarily independent enzyme superfamilies with different protein folds are known to use the \"N\"-terminal residue as a nucleophile: Superfamily PB (proteasomes using the Ntn fold) and Superfamily PE (acetyltransferases using the DOM fold) This commonality of active site structure in completely different protein folds indicates that the active site evolved convergently in those superfamilies.\n\nFamilies of threonine proteases\n"}
{"id": "169146", "url": "https://en.wikipedia.org/wiki?curid=169146", "title": "Cold cathode", "text": "Cold cathode\n\nA cold cathode is a cathode that is not electrically heated by a filament. A cathode may be considered \"cold\" if it emits more electrons than can be supplied by thermionic emission alone. It is used in gas-discharge lamps, such as neon lamps, discharge tubes, and some types of vacuum tube. The other type of cathode is a hot cathode, which is heated by electric current passing through a filament. A cold cathode does not necessarily operate at a low temperature: it is often heated to its operating temperature by other methods, such as the current passing from the cathode into the gas.\n\nA cold-cathode vacuum tube does not rely on external heating of an electrode to provide thermionic emission of electrons. Early cold-cathode devices included the Geissler tube and Plucker tube, and early cathode ray tubes. Study of the phenomena in these devices led to the discovery of the electron.\n\nNeon lamps are used both to produce light as indicators and for special-purpose illumination, and also as circuit elements displaying negative resistance. Addition of a trigger electrode to a device allowed the glow discharge to be initiated by an external control circuit; Bell Laboratories developed a \"trigger tube\" cold-cathode device in 1936.\n\nMany types of cold-cathode switching tube were developed, including various types of thyratron, the krytron, cold-cathode displays (Nixie tube) and others. Voltage regulator tubes rely on the relatively constant voltage of a glow discharge over a range of current and were used to stabilize power-supply voltages in tube-based instruments. A Dekatron is a cold-cathode tube with multiple electrodes that is used for counting. Each time a pulse is applied to a control electrode, a glow discharge moves to a step electrode; by providing ten electrodes in each tube and cascading the tubes, a counter system can be developed and the count observed by the position of the glow discharges. Counter tubes were used widely before development of integrated circuit counter devices.\n\nThe flash tube is a cold-cathode device filled with xenon gas, used to produce an intense short pulse of light for photography or to act as a stroboscope to examine the motion of moving parts.\n\nCold-cathode lamps include cold-cathode fluorescent lamps (CCFLs) and neon lamps. Neon lamps primarily rely on excitation of gas molecules to emit light; CCFLs use a discharge in mercury vapor to develop ultraviolet light, which in turn causes a fluorescent coating on the inside of the lamp to emit visible light.\n\nCold-cathode fluorescent lamps are used for backlighting of LCDs, for example computer monitors and television screens.\n\nIn the lighting industry, “cold cathode” historically refers to luminous tubing larger than 20 mm in diameter and operating on a current of 120 to 240 milliamperes. This larger-diameter tubing is often used for interior alcove and general lighting.\nThe term \"neon lamp\" refers to tubing that is smaller than 15 mm in diameter and typically operates at approximately 40 milliamperes. These lamps are commonly used for neon signs.\n\nThe cathode is the negative electrode. Any gas-discharge lamp has a positive (anode) and a negative electrode. Both electrodes alternate between acting as an anode and a cathode when these devices run with alternating current.\n\nA \"cold cathode\" is distinguished from a hot cathode that is heated to induce thermionic emission of electrons. Discharge tubes with hot cathodes have an envelope filled with low-pressure gas and containing two electrodes. Examples are most common fluorescent lamps, high-pressure discharge lamps and vacuum fluorescent displays.\n\nThe surface of cold cathodes can emit secondary electrons at a ratio greater than unity (breakdown). An electron that leaves the cathode will collide with neutral gas molecules. The collision may just excite the molecule, but sometimes it will knock an electron free to create a positive ion. The original electron and the freed electron continue toward the anode and may create more positive ions (see Townsend avalanche). The result is for each electron that leaves the cathode, several positive ions are generated that eventually crash onto the cathode. Some crashing positive ions may generate a secondary electron. The discharge is self-sustaining when for each electron that leaves the cathode, enough positive ions hit the cathode to free, on average, another electron. External circuitry limits the discharge current. Cold-cathode discharge lamps use higher voltages than hot-cathode ones. The resulting strong electric field near the cathode accelerates ions to a sufficient velocity to create free electrons from the cathode material.\n\nAnother mechanism to generate free electrons from a cold metallic surface is field electron emission. It is used in some x-ray tubes, the field-electron microscope (FEM), and field-emission displays (FEDs).\n\nCold cathodes sometimes have a rare-earth coating to enhance electron emission. Some types contain a source of beta radiation to start ionization of the gas that fills the tube. In some tubes, glow discharge around the cathode is usually minimized; instead there is a so-called positive column, filling the tube. Examples are the neon lamp and nixie tubes. Nixie tubes too are cold-cathode neon displays that are in-line, but not in-plane, display devices.\n\nCold-cathode devices typically use a complex high-voltage power supply with some mechanism for limiting current. Although creating the initial space charge and the first arc of current through the tube may require a very high voltage, once the tube begins to heat up, the electrical resistance drops, thus increasing the electric current through the lamp. To offset this effect and maintain normal operation, the supply voltage is gradually lowered. In the case of tubes with an ionizing gas, the gas can become a very hot plasma, and electrical resistance is greatly reduced. If operated from a simple power supply without current limiting, this reduction in resistance would lead to damage to the power supply and overheating of the tube electrodes.\n\nCold cathodes are used in cold-cathode rectifiers, such as the crossatron and mercury-arc valves, and cold-cathode amplifiers, such as in automatic message accounting and other pseudospark switching applications. Other examples include the thyratron, krytron, sprytron, and ignitron tubes.\n\nA common cold-cathode application is in neon signs and other locations where the ambient temperature is likely to drop well below freezing, The Clock Tower, Palace of Westminster (Big Ben) uses cold-cathode lighting behind the clock faces where continual striking and failure to strike in cold weather would be undesirable. Large cold-cathode fluorescent lamps (CCFLs) have been produced in the past and are still used today when shaped, long-life linear light sources are required. , miniature CCFLs were extensively used as backlights for computer and television liquid-crystal displays. CCFL lifespans vary in LCD televisions depending on transient voltage surges and temperature levels in usage environments.\n\nDue to its efficiency, CCFL technology has expanded into room lighting. Costs are similar to those of traditional fluorescent lighting, but with several advantages: the light emitted is , bulbs turn on instantly to full output and are also dimmable.\n\nIn systems using alternating current but without separate anode structures, the electrodes alternate as anodes and cathodes, and the impinging electrons can cause substantial localized heating, often to red heat. The electrode may take advantage of this heating to facilitate the thermionic emission of electrons when it is acting as a cathode. (\"Instant-start\" fluorescent lamps employ this aspect; they start as cold-cathode devices, but soon localized heating of the fine tungsten-wire cathodes causes them to operate in the same mode as hot-cathode lamps.)\n\nThis aspect is problematic in the case of backlights used for LCD TV displays. New energy-efficiency regulations being proposed in many countries will require variable backlighting; variable backlightling also improves the perceived contrast range, which is desirable for LCD TV sets. However, CCFLs are strictly limited in the degree to which they can be dimmed, both because a lower plasma current will lower the temperature of the cathode, causing erratic operation, and because running the cathode at too low a temperature drastically shortens the life of the lamps. Much research is being directed to this problem, but high-end manufacturers are now turning to high-efficiency white LEDs as a better solution.\n\n\n\"Notes\"\n\n\"Citations\"\n"}
{"id": "7876225", "url": "https://en.wikipedia.org/wiki?curid=7876225", "title": "Compressed earth block", "text": "Compressed earth block\n\nA compressed earth block (CEB), also known as a \"pressed earth block\" or a \"compressed soil block\", is a building material made primarily from damp soil compressed at high pressure to form blocks. Compressed earth blocks use a mechanical press to form blocks out of an appropriate mix of fairly dry inorganic subsoil, non-expansive clay and aggregate. If the blocks are stabilized with a chemical binder such as Portland cement they are called \"compressed stabilized earth block\" (CSEB) or \"stabilized earth block\" (SEB). Typically, around is applied in compression, and the original soil volume is reduced by about half.\n\nCreating CEBs differs from rammed earth in that the latter uses a larger formwork into which earth is poured and manually tamped down, creating larger forms such as a whole wall or more at one time rather than building blocks. CEBs differ from mud bricks in that the latter are not compressed and solidify through chemical changes that take place as they air dry. The compression strength of properly made CEB can meet or exceed that of typical cement or mud brick. Building standards have been developed for CEB.\n\nCEBs are assembled onto walls using standard bricklaying and masonry techniques. The mortar may be a simple slurry made of the same soil/clay mix without aggregate, spread or brushed very thinly between the blocks for bonding, or cement mortar may also be used for high strength, or when construction during freeze-thaw cycles causes stability issues. Hydraform blocks are shaped to be interlocking.\n\nCEB technology has been developed for low-cost construction, as an alternative to adobe, and with some advantages. A commercial industry has been advanced by eco-friendly contractors, manufacturers of the mechanical presses, and by cultural acceptance of the method. In the United States, most general contractors building with CEB are in the Southwestern states: New Mexico, Colorado, Arizona, California, and to a lesser extent in Texas. The methods and presses have been used for many years in Mexico, and in developing countries.\n\nThe South African Department of Water Affairs and Forestry considers that CEB, locally called \"Dutch brick\" is an appropriate technology for a developing country, as are adobe, rammed earth and cob. All use natural building materials.\nIn 2002 the International Institute for Energy Conservation was one of the winners of a World Bank Development Marketplace Award for a project to make an energy-efficient Dutch brick-making machine for home construction in South Africa. By making cheaper bricks that use earth, the project would reduce housing costs while stimulating the building industry.\nThe machine would be mobile, allowing bricks to be made locally from earth.\n\nVarious types of CEB production machines exist, from manual to semi-automated and fully automated, with increasing capital-investment and production rates, and decreased labor. Automated machines are more common in the developed world, and manual machines in the developing world.\n\nThere are many advantages of the CEB system. On-site materials can be used, which reduces cost, minimizes shipping costs for materials, and increases efficiency and sustainability. The wait-time required to obtain materials is minimal, because after the blocks are pressed, materials are available very soon after a short drying period. The uniformity of the blocks simplifies construction, and minimizes or eliminates the need for mortar, thus reducing both the labor and materials costs. The blocks are strong, stable, water-resistant and long-lasting.\n\nCEB had very limited use prior to the 1980s. It was known in the 1950s in South America, where one of the most well-known presses, the Cinva Ram, was developed by Raul Ramirez in the Inter-American Housing Center (CINVA) in Bogota, Colombia. The Cinva Ram is a single-block, manual-press that uses a long, hand-operated lever to drive a cam, generating high pressure.\n\nIndustrial manufacturers produce much larger machines that run with diesel or gasoline engines and hydraulic presses that receive the soil/aggregate mixture through a hopper. This is fed into a chamber to create a block that is then ejected onto a conveyor.\n\nDuring the 1980s, soil-pressing technology became widespread. France, England, Germany, South Africa and Switzerland began to write standards. The Peace Corps, USAID, Habitat for Humanity and other programs began to implement it into housing projects.\n\nCompleted walls require either a reinforced bond beam or a ring beam on top or between floors and if the blocks are not stabilized, a plaster finish, usually stucco wire/stucco cement and/or lime plaster. Stabilized blocks can be left exposed with no outer plaster finish. In tropical environments, polycarbonate varnish is often used to provide an additional layer of wet-weather protection.\n\nStandards for foundations are similar to those for brick walls. A CEB wall is heavy. Footings must be at least 10 inches thick, with a minimum width that is 33 percent greater than the wall width. If a stem wall is used, it shall extend to an elevation not less than eight inches (203 mm) above the exterior finish grade. Rubble-filled foundation trench designs with a reinforced concrete grade beam above are allowed to support CEB construction.\n\nUsing the ASTM D1633-00 stabilization standard, a pressed and cured block must be submerged in water for four hours. It is then pulled from the water and immediately subjected to a compression test. The blocks must score at least a 300 pound-force per square inch (p.s.i) (2 MPa) minimum. This is a higher standard than for adobe, which must score an \"average\" of at least 300 p.s.i. (2 MPa)\n"}
{"id": "13566263", "url": "https://en.wikipedia.org/wiki?curid=13566263", "title": "Dukhin number", "text": "Dukhin number\n\nThe Dukhin number () is a dimensionless quantity that characterizes the contribution of the surface conductivity to various electrokinetic and electroacoustic effects, as well as to electrical conductivity and permittivity of fluid heterogeneous systems. \n\nIt was introduced by Lyklema in “Fundamentals of Interface and Colloid Science”. A recent IUPAC Technical Report used this term explicitly and detailed several means of measurement in physical systems.\n\nThe Dukhin number is a ratio of the surface conductivity formula_1 to the fluid bulk electrical conductivity K multiplied by particle size \"a\":\n"}
{"id": "38103099", "url": "https://en.wikipedia.org/wiki?curid=38103099", "title": "Earth pyramids of Ritten", "text": "Earth pyramids of Ritten\n\nThe earth pyramids of Ritten (German: \"Erdpyramiden am Ritten\"; ) are a natural monument that is located on the Ritten, a plateau not far from Bolzano in northern Italy. The earth pyramids of South Tyrol are a fairly widespread phenomenon which are existing in various locations.\nThe original name in this area for these earth pyramids is \"Lahntürme\", i.e. landslide towers. They are rather unusual formations of their kind which originate from morainic rocks of glacial origin. The columns of the pyramids may be more or less elongated, and the higher they are the thinner they get, ending usually with a stone cover. These earth pyramids are not static, they are constantly evolving, because their life cycle foresees a continuous erosion, or even a final collapse leaving room for new formations.\n\nIn South Tyrol there are other natural monuments like this such as the earth pyramids of Platten, but the ones of Ritten are considered the parents of them all.\n\n"}
{"id": "13551670", "url": "https://en.wikipedia.org/wiki?curid=13551670", "title": "Electroacoustic phenomena", "text": "Electroacoustic phenomena\n\nElectroacoustic phenomena arise when ultrasound propagates through a fluid containing ions. The associated particle motion generates electric signals because ions have electric charge. This coupling between ultrasound and electric field is called electroacoustic phenomena. The fluid might be a simple Newtonian liquid, or complex heterogeneous dispersion, emulsion or even a porous body. There are several different electroacoustic effects depending on the nature of the fluid.\n\n\nHistorically, the IVI was the first known electroacoustic effect. It was predicted by Debye in 1933.\n\nThe streaming vibration current was experimentally observed in 1948 by Williams. A theoretical model was developed some 30 years later by Dukhin and others. This effect opens another possibility for characterizing the electric properties of the surfaces in porous bodies. A similar effect can be observed at a non-porous surface, when sound is bounced off at an oblique angle. The incident and reflected waves superimpose to cause oscillatory fluid motion in the plane of the interface, thereby generating an AC streaming current at the frequency of the sound waves.\n\nThe electrical double layer can be regarded as behaving like a parallel plate capacitor with a compressible dielectric filling. When sound waves induce a local pressure variation, the spacing of the plates varies at the frequency of the excitation, generating an AC displacement current normal to the interface. For practical reasons this is most readily observed at a conducting surface. It is therefore possible to use an electrode immersed in a conducting electrolyte as a microphone, or indeed as a loudspeaker when the effect is applied in reverse.\n\nColloid vibration potential measures the AC potential difference generated between two identical relaxed electrodes, placed in the dispersion, if the latter is subjected to an ultrasonic field. When a sound wave travels through a colloidal suspension of particles whose density differs from that of the surrounding medium, inertial forces induced by the vibration of the suspension give rise to a motion of the charged particles relative to the liquid, causing an alternating electromotive force. The manifestations of this electromotive force may be measured, depending on the relation between the impedance of the suspension and that of the measuring instrument, either as colloid vibration potential or as \"colloid vibration current\".\n\nColloid vibration potential and current was first reported by Hermans and then independently by Rutgers in 1938. It is widely used for characterizing the ζ-potential of various dispersions and emulsions. The effect, theory, experimental verification and multiple applications are discussed in the book by Dukhin and Goetz.\n\nElectric sonic amplitude was experimentally discovered by Cannon with co-authors in early 1980s. It is also widely used for characterizing ζ-potential in dispersions and emulsions. There is review of this effect theory, experimental verification and multiple applications published by Hunter.\n\nWith regard to the theory of CVI and ESA, there was an important observation made by O'Brien, who linked these measured parameters with dynamic electrophoretic mobility μ.\n\nwhere\n\nDynamic electrophoretic mobility is similar to electrophoretic mobility that appears in electrophoresis theory. They are identical at low frequencies and/or for sufficiently small particles.\n\nThere are several theories of the dynamic electrophoretic mobility. Their overview is given in the Ref.5. Two of them are the most important.\n\nThe first one corresponds to the Smoluchowski limit. It yields following simple expression for CVI for sufficiently small particles with negligible CVI frequency dependence:\n\nwhere:\n\nThis remarkably simple equation has same wide range of applicability as Smoluchowski equation for electrophoresis. It is independent on shape of the particles, their concentration.\n\nValidity of this equation is restricted with the following two requirements.\nFirst, it is valid only for a thin double layer, when the Debye length is much smaller than particle's radius a:\n\nSecondly, it neglects the contribution of the surface conductivity. This assumes a small Dukhin number:\n\nRestriction of the thin double layer limits applicability of this Smoluchowski type theory only to aqueous systems with sufficiently large particles and not very low ionic strength. This theory does not work well for nano-colloids, including proteins and polymers at low ionic strength. It is not valid for low- or non-polar fluids.\n\nThere is another theory that is applicable for the other extreme case of a thick double layer, when \n\nThis theory takes into consideration the double layer overlap that inevitably occurs for concentrated systems with thick double layer. This allows introduction of so-called \"quasi-homogeneous\" approach, when overlapped diffuse layers of particles cover the complete interparticle space. The theory becomes much simplified in this extreme case, as shown by Shilov and others. Their derivation predicts that surface charge density σ is a better parameter than ζ-potential for characterizing electroacoustic phenomena in such systems. An expression for CVI simplified for small particles follows:\n\n"}
{"id": "46488727", "url": "https://en.wikipedia.org/wiki?curid=46488727", "title": "Energoland", "text": "Energoland\n\nEnergoland is an information centre for energy and electricity generation which was opened by Slovenské elektrárne on 14 October 2014 at Mochovce, Slovakia, at the site of the nuclear power plant. It is situated between Levice and Nitra. The centre mainly serves the schools and public. The entry is free of charge. The exposition was awarded as an excellent communication project at the PIME conference and evaluated as a five-star training centre by Laura Elizabeth Kennedy, governor of the United States of America in the Board of Governors of the International Atomic Energy Agency in Vienna and chargé d'affaires of permanent representation of the USA in international organisation in Vienna.\n\nIn Energoland, there are more than thirty objects, applications, and interactive expositions. Edutainment (blending education and entertainment) at Mochovce offers information on energy, electricity generation, global warming, and carbon burden. In addition, visitors learn about the energy mix, electricity grid dispatching, or about the radiation around us. The exposition also focuses on power industry, not only with respect to safety and radioactive waste but also dealing with the fuel cycle, nanowolrd of atoms, and Cherenkov radiation. Some of the specialities include the 3D cinema with its own movie Energy Odyssey, a model of emission-free motorcycle, interactive LED floor or thermal mirror, and mobile application using augmented reality, \"Energoland\", downloadable from Google Play or AppStore. Through the Energy Map, various facts and statistics can be searched which are directly shown in the map of the world with the highlighted countries.\nEnergoland also wants its visitors to make their own opinion of the electricity generation and its sources. Therefore, it does not provide information only about nuclear energy but it also brings knowledge about other sources: water, sun, wind, geothermal energy, or fossil fuels.\n\nThe design of the building is the work of Ing. arch. Viktor Šabík (BARAK Architekti) who, together with the QEX Company and Italian agency Piter Design, designed the interior of Energoland, as well.[2] The total area of the building, including the training rooms and offices is 2,000 square metres; the info centre itself takes up the area of 600 m2.\n\nEnergoland is not only a visitor and information centre; it also serves as a training centre for the employees of Slovenské elektrárne and provides office areas. There are many events for the employees of Slovenské elektrárne or public taking place here, such as the 90 reactor-years anniversary, programme within the national round of Olympics in Physics, Science Talks during the Science and Technology Week, Sustainable Development Conference in May 2015, etc.\n\n"}
{"id": "3533357", "url": "https://en.wikipedia.org/wiki?curid=3533357", "title": "Evolution (term)", "text": "Evolution (term)\n\nThe English noun evolution (from Latin \"unfolding, unrolling\") refers to any kind of accumulation of change, or gradual directional change. It is the 3,117th most commonly used word in English.\n\nWhile the term primarily refers to biological evolution, there are various types of chemical evolution and it is also found in economics, historical linguistics, and many other technical fields where systems develop or change gradually over time, e.g. stellar evolution, cultural evolution, the evolution of an idea, metaphysical evolution, spiritual evolution, etc.\n\nThe English term prior to the late 19th century was confined to referring to goal-directed, pre-programmed processes such as embryological development. A pre-programmed task, as in a military maneuver, using this definition, may be termed an \"evolution.\" \n\nThe term \"evolution\" (from its literal meaning of \"unfolding\" of something into its true or explicit form) carries a connotation of gradual improvement or directionality from a beginning to an end point. This contrasts with the more general development, which can indicate change in any direction, or revolution, which implies recurring, periodic change. The term biological devolution is coined as an antonym to \"evolution\", indicating such degeneration or decrease in quality or complexity.\n"}
{"id": "46348501", "url": "https://en.wikipedia.org/wiki?curid=46348501", "title": "Evolution of the cochlea", "text": "Evolution of the cochlea\n\nCochlea is Latin for “snail, shell or screw” and originates from the Greek word κοχλίας \"kohlias\". The modern definition, the auditory portion of the inner ear, originated in the late 17th century. Within the mammalian cochlea exists the organ of Corti, which contains hair cells that are responsible for translating the vibrations it receives from surrounding fluid-filled ducts into electrical impulses that are sent to the brain to process sound. This spiral-shaped cochlea is estimated to have originated during the early Cretaceous Period, around 120 million years ago. Further, the auditory innervation of the spiral-shaped cochlea also traces back to the Cretaceous period. The evolution of the human cochlea is a major area of scientific interest because of its favourable representation in the fossil record. During the last century, many scientists such as evolutionary biologists and paleontologists strove to develop new methods and techniques to overcome the many obstacles associated with working with ancient, delicate artifacts. In the past, scientists were limited in their ability to fully examine specimens without causing damage to them. In more recent times, technologies such as micro-CT scanning became available. These technologies allow for the visual differentiation between fossilized animal materials and other sedimentary remains. With the use of X-ray technologies, it is possible to ascertain some information about the auditory capabilities of extinct creatures, giving insight to human ancestors as well as their contemporary species.\n\nWhile the basic structure of the inner ear in lepidosaurs (lizards and snakes), archosaurs (birds and crocodilians) and mammals is similar, and the organs are considered to be homologous, each group has a unique type of auditory organ. The hearing organ arose within the lagenar duct of stem reptiles, lying between the saccular and lagenar epithelia. In lepidosaurs, the hearing organ, the basilar papilla, is generally small, with at most 2000 hair cells, whereas in archosaurs the basilar papilla can be much longer (>10mm in owls) and contain many more hair cells that show two typical size extremes, the short and the tall hair cells. In mammals, the structure is known as the organ of Corti and shows a unique arrangement of hair cells and supporting cells. All mammalian organs of Corti contain a supporting tunnel made up of pillar cells, on the inner side of which there are inner hair cells and outer hair cells on the outer side. The definitive mammalian middle ear and the elongated cochlea allows for better sensitivity for higher frequencies.\n\nAs in all lepidosaurs and archosaurs, the single-ossicle (columellar) middle ear transmits sound to the footplate of the columella, which sends a pressure wave through the inner ear. In snakes, the basilar papilla is roughly 1mm long and only responds to frequencies below about 1 kHz. In contrast, lizards tend to have two areas of hair cells, one responding below and the other above 1 kHz. The upper frequency limit in most lizards is roughly 5–8 kHz. The longest lizard papillae are about 2mm long and contain 2000 hair cells and their afferent innervating fibers can be very sharply tuned to frequency.\n\nIn birds and crocodilians, the similarity of the structure of the basilar papilla betrays their close evolutionary relationship. The basilar papilla is up to about 10mm long and contains up to 16500 hair cells. While most birds have an upper hearing limit of only about 6 kHz, the barn owl can hear up to 12 kHz and thus close to the human upper limit.\n\nEgg-laying mammals, the monotremes (spiny anteater and platypus), do not have a spiral cochlea, but one shaped more like a banana, up to about 7 mm long. Like in lepidosaurs and archosaurs, it contains a lagena, a vestibular sensory epithelium, at its tip. Only in therian mammals (marsupials and placentals) is the cochlea truly coiled 1.5 to 3.5 times. Whereas in monotremes there are many rows of both inner and outer hair cells in the organ of Corti, in therian (marsupial and placental) mammals the number of inner hair-cell rows is one, and there are generally only three rows of outer hair cells.\n\nAmphibians have unique inner ear structures. There are two sensory papillae involved in hearing, the basilar (higher frequency) and amphibian (lower frequency) papillae, but it is uncertain whether either is homologous to the hearing organs of lepidosaurs, archosaurs and mammals and we have no idea when they arose.\n\nFish have no dedicated auditory epithelium, but use various vestibular sensory organs that respond to sound. In most teleost fishes it is the saccular macula that responds to sound. In some, such as goldfishes, there is also a special bony connection to the gas bladder that increases sensitivity allowing hearing up to about 4 kHz.\n\nThe size of cochlea has been measured throughout its evolution based on the fossil record. In one study, the basal turn of the cochlea was measured, and it was hypothesized that cochlear size correlates with body mass. The size of the basal turn of the cochlea was not different in Neanderthals and Holocene humans, however it became larger in early modern humans and Upper Paleolithic humans. Furthermore, the position and orientation of the cochlea is similar between Neanderthals and Holocene humans, relative to plane of the lateral canal, whereas early modern and upper Paleolithic humans have a more superiorly placed cochlea than Holocene humans. When comparing hominins of the Middle Pleistocene and Neanderthals and Holocene humans, the apex of the cochlea faces more inferiorly in the hominins than the latter two groups. Finally, the cochlea of European middle Pleistocene hominins faces more inferiorly than Neanderthals, modern humans, and Homo erectus.\nHuman beings, along with Apes, are the only mammals that do not have high frequency (>32 kHz) hearing. Humans have long cochleae, but the space devoted to each frequency range is quite large (2.5mm per octave), resulting in a comparatively reduced upper frequency limit. The human cochlea has approximately 2.5 turns around the modiolus (the axis). Humans, like many mammals and birds, are able to perceive auditory signals that displace the eardrum by a mere picometre.\n\nBecause of its prominence and preserved state in the fossil record, until recently, the ear had been used to determine phylogeny. The ear itself contains different portions, including the outer ear, the middle ear, and the inner ear and all of these show evolutionary changes that are often unique to each lineage [14]. It was the independent evolution of a tympanic middle ear in the Triassic era that produced strong selection pressures towards improved hearing organs in the separate lineages of land vertebrates.\n\nThe cochlea is the tri-chambered auditory detection portion of the ear, consisting of the scala media, the scala tympani, and the scala vestibuli. Regarding mammals, placental and marsupial cochleae have similar cochlear responses to auditory stimulation as well as DC resting potentials. This leads to the investigation of the relationship between these therian mammals and researching their ancestral species to trace the origin of the cochlea.\n\nThis spiral-shaped cochlea that is in both marsupial and placental mammals is traced back to approximately 120 million years ago. The development of the most basic basilar papilla (the auditory organ that later evolved into the Organ of Corti in mammals) happened at the same time as the water-to-land transition of vertebrates, approximately 380 million years ago. The actual coiling or spiral nature of the cochlea occurred to save space inside the skull. The longer the cochlea, the higher is the potential resolution of sound frequencies given the same hearing range. The oldest of the truly coiled mammalian cochleae were approximately 4 mm in length.\n\nThe earliest evidence available for primates depicts a short cochlea with prominent laminae, suggesting that they had good high-frequency sensitivity as opposed to low-frequency sensitivity. After this, over a period of around 60 million years, evidence suggests that primates developed longer cochleae and less prominent laminae, which means that they had an improvement in low-frequency sensitivity and a decrease in high-frequency sensitivity. By the early Miocene period, the cycle of the elongation of the cochleae and the deterioration of the laminae was completed. Evidence shows that primates have had an increasing cochlear volume to body mass ratio over time. These changes in the cochlear labyrinth volume negatively affect the highest and lowest audible frequencies, causing a downward shift. Non-primates appear to have smaller cochlear labyrinth volumes overall when compared to primates. Some evidence also suggests that selective forces for the larger cochlear labyrinth may have started after the basal primate node.\nMammals are the subject of a substantial amount of research not only because of the potential knowledge to be gained regarding humans, but also because of their rich and abundant representation in the fossil record. The spiral shape of the cochlea evolved later on in the evolutionary pathway of mammals than previously believed, just before the therians split into the two lineages marsupials and placentals, about 120 million years ago.\n\nParallel to the evolution of the cochlea, prestins show an increased rate of evolution in therian mammals. Prestins are located in the outer hair cells of mammalian cochlea and are considered motor proteins. They are found in the hair cells of all vertebrates, including fish, but are thought to have initially been membrane transporter molecules. A high concentration of prestins are found only in the lateral membranes of therian outer hair cells (there is uncertainty with regard to concentrations in monotremes). This high concentration is not found in inner hair cells, and is also lacking in all hair cell types of non-mammals. Prestin also has a role in motility, which evolved a greater importance in the motor function in land vertebrates, but this developed vastly differently in different lineages. In certain birds and mammals, prestins function as both transporters and motors, but the strongest evolution to robust motor dynamics only evolved in therian mammals. It is hypothesized that this motor system is significant to the therian cochlea at high frequencies because of the distinctive cellular and bony composition of the organ of Corti that allows the prestins to intensify movements of the whole structure.\nModern ultra-sound echolocating species such as bats and toothed whales show highly evolved prestins, and these prestins show identical sequence alterations over time. Unusually, the sequences thus apparently evolved independent from each other during different time periods. Furthermore, the evolution of neurotransmitter receptor systems (acetylcholine) that regulate the motor feedback of the outer hair cells coincides with prestin evolution in therians. This suggests that there was a parallel evolution of a control system and a motor system in the inner ear of therian mammals.\n\nLand vertebrates evolved middle ears independently in each major lineage, and are this the result of parallel evolution. The configurations of the middle ears of monotreme and therian mammals can thus be interpreted as convergent evolution or homoplasy. Thus evidence from fossils demonstrate homoplasies for the detachment of the ear from the jaw. Furthermore, it is apparent that the land-based eardrum, or tympanic membrane, and connecting structures such as the Eustachian tube evolved convergently in multiple different settings as opposed to being a defining morphology.\n"}
{"id": "268020", "url": "https://en.wikipedia.org/wiki?curid=268020", "title": "Evolutionary computation", "text": "Evolutionary computation\n\nIn computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.\n\nIn evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes. In biological terminology, a population of solutions is subjected to natural selection (or artificial selection) and mutation. As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm.\n\nEvolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures. Evolutionary computation is also sometimes used in evolutionary biology as an \"in silico\" experimental procedure to study common aspects of general evolutionary processes.\n\nThe use of evolutionary principles for automated problem solving originated in the 1950s. It was not until the 1960s that three distinct interpretations of this idea started to be developed in three different places.\n\n\"Evolutionary programming\" was introduced by Lawrence J. Fogel in the US, while John Henry Holland called his method a \"genetic algorithm\". In Germany Ingo Rechenberg and Hans-Paul Schwefel introduced \"evolution strategies\". These areas developed separately for about 15 years. From the early nineties on they are unified as different representatives (\"dialects\") of one technology, called \"evolutionary computing\". Also in the early nineties, a fourth stream following the general ideas had emerged – \"genetic programming\". Since the 1990s, nature-inspired algorithms are becoming an increasingly significant part of the evolutionary computation.\n\nThese terminologies denote the field of evolutionary computing and consider evolutionary programming, evolution strategies, genetic algorithms, and genetic programming as sub-areas.\n\nSimulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n\nEvolutionary computing techniques mostly involve metaheuristic optimization algorithms. Broadly speaking, the field includes:\n\n\nEvolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction, mutation, recombination, natural selection and survival of the fittest. Candidate solutions to the optimization problem play the role of individuals in a population, and the cost function determines the environment within which the solutions \"live\" (see also fitness function). Evolution of the population then takes place after the repeated application of the above operators.\n\nIn this process, there are two main forces that form the basis of evolutionary systems: Recombination and mutation create the necessary diversity and thereby facilitate novelty, while selection acts as a force increasing quality.\n\nMany aspects of such an evolutionary process are stochastic. Changed pieces of information due to recombination and mutation are randomly chosen. On the other hand, selection operators can be either deterministic, or stochastic. In the latter case, individuals with a higher fitness have a higher chance to be selected than individuals with a lower fitness, but typically even the weak individuals have a chance to become a parent or to survive.\n\nGenetic algorithms deliver methods to model biological systems and systems biology that are linked to the theory of dynamical systems, since they are used to predict the future states of the system. This is just a vivid (but perhaps misleading) way of drawing attention to the orderly, well-controlled and highly structured character of development in biology.\n\nHowever, the use of algorithms and informatics, in particular of computational theory, beyond the analogy to dynamical systems, is also relevant to understand evolution itself. \n\nThis view has the merit of recognizing that there is no central control of development; organisms develop as a result of local interactions within and between cells. The most promising ideas about program-development parallels seem to us to be ones that point to an apparently close analogy between processes within cells, and the low-level operation of modern computers . Thus, biological systems are like computational machines that process input information to compute next states, such that biological systems are closer to a computation than classical dynamical system .\n\nFurthermore, following concepts from computational theory, micro processes in biological organisms are fundamentally incomplete and undecidable (completeness (logic)), implying that “there is more than a crude metaphor behind the analogy between cells and computers .\n\nThe analogy to computation extends also to the relationship between inheritance systems and biological structure, which is often thought to reveal one of the most pressing problems in explaining the origins of life.\n\nThe list of active researchers is naturally dynamic and non-exhaustive. A network analysis of the community was published in 2007.\n\n\nThe main conferences in the evolutionary computation area include \n\n"}
{"id": "2132454", "url": "https://en.wikipedia.org/wiki?curid=2132454", "title": "Evolutionary graph theory", "text": "Evolutionary graph theory\n\nEvolutionary graph theory is an area of research lying at the intersection of graph theory, probability theory, and mathematical biology. Evolutionary graph theory is an approach to studying how topology affects evolution of a population. That the underlying topology can substantially affect the results of the evolutionary process is seen most clearly in a paper by Erez Lieberman, Christoph Hauert and Martin Nowak.\n\nIn evolutionary graph theory, individuals occupy vertices of a weighted directed graph and the weight w of an edge from vertex \"i\" to vertex \"j\" denotes the probability of \"i\" replacing \"j\". The weight corresponds to the biological notion of fitness where fitter types propagate more readily. \nOne property studied on graphs with two types of individuals is the \"fixation probability\", which is defined as the probability that a single, randomly placed mutant of type A will replace a population of type B. According to the \"isothermal theorem\", a graph has the same fixation probability as the corresponding Moran process if and only if it is isothermal, thus the sum of all weights that lead into a vertex is the same for all vertices. Thus, for example, a complete graph with equal weights describes a Moran process. The fixation probability is\nwhere \"r\" is the relative fitness of the invading type.\n\nGraphs can be classified into amplifiers of selection and suppressors of selection. If the fixation probability of a single advantageous mutation formula_2 is higher than the fixation probability of the corresponding Moran process formula_3 then the graph is an amplifier, otherwise a suppressor of selection. One example of the suppressor of selection is a linear process where only vertex \"i-1\" can replace vertex \"i\" (but not the other way around). In this case the fixation probability is formula_4 (where \"N\" is the number of vertices) since this is the probability that the mutation arises in the first vertex which will eventually replace all the other ones. Since formula_5 for all \"r\" greater than 1, this graph is by definition a suppressor of selection.\n\nEvolutionary graph theory may also be studied in a dual formulation, as a coalescing random walk, or as a stochastic process. We may consider the mutant population on a graph as a random walk between absorbing barriers representing mutant extinction and mutant fixation. For highly symmetric graphs, we can then use martingales to find the \"fixation probability\" as illustrated by Monk (2018).\n\nAlso evolutionary games can be studied on graphs where again an edge between \"i\" and \"j\" means that these two individuals will play a game against each other.\n\nClosely related stochastic processes include the voter model, which was introduced by Clifford and Sudbury (1973) and independently by Holley and Liggett (1975), and which has been studied extensively.\n\n\nA virtual laboratory for studying evolution on graphs:\n"}
{"id": "1159407", "url": "https://en.wikipedia.org/wiki?curid=1159407", "title": "Homeosis", "text": "Homeosis\n\nIn evolutionary developmental biology, homeosis is the transformation of one organ into another, arising from mutation in or misexpression of certain developmentally critical genes, specifically homeotic genes. In animals, these developmental genes specifically control the development of organs on their anteroposterior axis. In plants, however, the developmental genes affected by homeosis may control anything from the development of a stamen or petals to the development of chlorophyll. Homeosis may be caused by mutations in Hox genes, found in animals, or others such as the MADS-box family in plants. Homeosis is a characteristic that has helped insects become as successful and diverse as they are.\n\nHomeotic mutations work by changing segment identity during development. For example, the \"Ultrabithorax\" genotype gives a phenotype wherein metathoracic and first abdominal segments become mesothoracic segments. Another well-known example is \"Antennapedia\": a gain-of-function allele causes legs to develop in the place of antennae.\n\nIn botany, Rolf Sattler has revised the concept of homeosis (replacement) by his emphasis on partial homeosis in addition to complete homeosis; this revision is now widely accepted.\n\nHomeotic mutants in angiosperms are thought to be rare in the wild: in the annual plant \"Clarkia\" (Onagraceae), homeotic mutants are known where the petals are replaced by a second whorl of sepal-like organs, originating via a mutation governed by a single recessive gene. The absence of lethal or deleterious consequences in floral mutants resulting in distinct morphological expressions has been a factor in the evolution of Clarkia, and perhaps also in many other plant groups.\n\nFollowing the work on homeotic mutants by Ed Lewis, the phenomenology of homeosis in animals was further elaborated by discovery of a conserved DNA binding sequence present in many homeotic proteins. \nThus, the 60 amino acid DNA binding protein domain was named the homeodomain, while the 180 bp nucleotide sequence encoding it was named the homeobox. The homeobox gene clusters studied by Ed Lewis were named the Hox genes, although it should be noted that many more homeobox genes are encoded by animal genomes than those in the Hox gene clusters.\n\nThe homeotic-function of certain proteins was first postulated to be that of a \"selector\" as proposed by Antonio Garcia-Bellido. \nBy definition selectors were imagined to be (transcription factor) proteins that stably determined one of two possible cell fates for a cell and its cellular descendants in a tissue. \nWhile most animal homeotic functions are associated with homeobox-containing factors, not all homeotic proteins in animals are encoded by homeobox genes, and further not all homeobox genes are necessarily associated with homeotic functions or (mutant) phenotypes.\nThe concept of homeotic selectors was further elaborated or at least qualified by Michael Akam in a so-called \"post-selector gene\" model that incorporated additional findings and \"walked back\" the \"orthodoxy\" of selector-dependent stable binary switches.\n\nThe concept of tissue compartments is deeply intertwined with the selector model of homeosis because the selector-mediated maintenance of cell fate can be restricted into different organizational units of an animal's body plan.\nIn this context, newer insights into homeotic mechanisms were found by Albert Erives and colleagues by focusing on enhancer DNAs that are co-targeted by homeotic selectors and different combinations of developmental signals.\nThis work identifies a protein biochemical difference between the transcription factors that function as homeotic selectors versus the transcription factors that function as effectors of developmental signaling pathways, such as the Notch signaling pathway and the BMP signaling pathway.\nThis work proposes that homeotic selectors function to \"license\" enhancer DNAs in a restricted tissue compartment so that the enhancers are enabled to read-out developmental signals, which are then integrated via polyglutamine-mediated aggregation.\n\nLike the complex multicellularity seen in animals, the multicellularity of land plants is developmentally organized into tissue and organ units via transcription factor genes with homeotic effects.\nAlthough plants have homeobox-containing genes, plant homeotic factors tend to possess MADS-box DNA binding domains.\nAnimal genomes also possess a small number MADS-box factors.\nThus, in the independent evolution of multicellularity in plants and animals, different eukaryotic transcription factor families were co-opted to serve homeotic functions. \nMADS-domain factors have been proposed to function as co-factors to more specialized factors and thereby help to determine organ identity.\nThis has been proposed to correspond more closely to the interpretation of animal homeotics outlined by Michael Akam.\n\n"}
{"id": "3592098", "url": "https://en.wikipedia.org/wiki?curid=3592098", "title": "Industrial melanism", "text": "Industrial melanism\n\nIndustrial melanism is an evolutionary effect prominent in several arthropods, where dark pigmentation (melanism) has evolved in an environment affected by industrial pollution, including sulphur dioxide gas and dark soot deposits. Sulphur dioxide kills lichens, leaving tree bark bare where in clean areas it is boldly patterned, while soot darkens bark and other surfaces. Darker pigmented individuals have a higher fitness in those areas as their camouflage matches the polluted background better; they are thus favoured by natural selection. This change, extensively studied by Bernard Kettlewell, is a popular teaching example in Darwinian evolution, providing evidence for natural selection. Kettlewell's results have been challenged by zoologists, creationists and the journalist Judith Hooper, but later researchers have upheld Kettlewell's findings.\nIndustrial melanism is widespread in the Lepidoptera (butterflies and moths), involving over 70 species such as \"Odontopera bidentata\" (scalloped hazel) and \"Lymantria monacha\" (dark arches), but the most studied is the evolution of the peppered moth, \"Biston betularia\". It is also seen in a beetle, \"Adalia bipunctata\" (two-spot ladybird), where camouflage is not involved as the insect has conspicuous warning coloration, and in the seasnake \"Emydocephalus annulatus\" where the melanism may help in excretion of trace elements through sloughing of the skin. The rapid decline of melanism that has accompanied the reduction of pollution, in effect a natural experiment, makes natural selection for camouflage \"the only credible explanation\".\n\nOther explanations for the observed correlation with industrial pollution have been proposed, including strengthening the immune system in a polluted environment, absorbing heat more rapidly when sunlight is reduced by air pollution, and the ability to excrete trace elements into melanic scales and feathers.\n\nIndustrial melanism was first noticed in 1900 by the geneticist William Bateson; he observed that the colour morphs were inherited, but did not suggest an explanation for the polymorphism.\n\nIn 1906, the geneticist Leonard Doncaster described the increase in frequency of the melanic forms of several moth species from about 1800 to 1850 in the heavily industrialised north-west region of England.\n\nIn 1924, the evolutionary biologist J. B. S. Haldane constructed a mathematical argument showing that the rapid growth in frequency of the \"carbonaria\" form of the peppered moth, \"Biston betularia\", implied selective pressure.\n\nFrom 1955 onwards, the geneticist Bernard Kettlewell conducted a series of experiments exploring the evolution of melanism in the peppered moth. He used a capture-mark-recapture technique to show that dark forms survived better than light ones.\n\nBy 1973, pollution in England had begun to decrease, and the dark \"carbonaria\" form had declined in frequency. This provided convincing evidence, gathered and analysed by Kettlewell and others such as the entomologist and geneticist Michael Majerus and the population geneticist Laurence M. Cook, that its rise and fall had been caused by natural selection in response to the changing pollution of the landscape.\n\nIndustrial melanism is known from over 70 species of moth that Kettlewell found in England, and many others from Europe and North America.\nAmong these, \"Apamea crenata\" (clouded border brindle moth) and \"Acronicta rumicis\" (knot grass moth) are always polymorphic, though the melanic forms are more common in cities and (like those of the peppered moth) are declining in frequency as those cities become less polluted.\n\nAmong other insects, industrial melanism has been observed in a beetle, \"Adalia bipunctata\", the two-spot ladybird.\n\nIn the vertebrates, industrial melanism is known from the turtle-headed seasnake \"Emydocephalus annulatus\", and may be present in urban feral pigeons.\n\nOriginally, peppered moths lived where light-colored lichens covered the trees. For camouflage from predators against that clean background, they had generally light coloration. During the Industrial Revolution in England, sulphur dioxide pollution in the atmosphere reduced the lichen cover, while soot blackened the bark of urban trees, making the light-colored moths more vulnerable to predation. This provided a selective advantage to the gene responsible for melanism, and the darker-colored moths increased in frequency. The melanic phenotype of \"Biston betularia\" has been calculated to give a fitness advantage as great as 30 per cent. By the end of the 19th century it almost completely replaced the original light-coloured type (var. \"typica\"), forming a peak of 98% of the population in 1895.\n\nMelanic \"B. betularia\" have been widely observed in North America. In 1959, 90% of \"B. betularia\" in Michigan and Pennsylvania were melanic. By 2001, melanism dropped to 6% of the population, following clean air legislation. The drop in melanism was correlated with an increase in species diversity of lichens, a decrease in the atmospheric pollutant sulphur dioxide, and an increase in the pale phenotype. The return of lichens is in turn directly correlated with the reduction in atmospheric sulphur dioxide.\n\nKettlewell's experiments were criticised by the zoologist Theodore David Sargent, who failed to reproduce Kettlewell's results between 1965 and 1969, and argued that Kettlewell had specially trained his birds to give the desired results. \nMichael Majerus however found that Kettlewell was basically correct in concluding that differential bird predation in a polluted environment was the primary cause of industrial melanism in the peppered moth. The story was in turn taken up in a 2002 book \"Of Moths and Men\", by the journalist Judith Hooper, asserting that Kettlewell's findings were fraudulent. The story was picked up by creationists who repeated the assertions of fraudulence. Zoologists including L. M. Cook, B. S. Grant, Majerus and David Rudge however all upheld Kettlewell's account, finding that each of Hooper's and the creationists' claims collapsed when the facts were examined. \n\nIt has been suggested that the demonstrated relationship between melanism and pollution can not be fully proven because the exact reason for increase in survivability can not be tracked and pin-pointed. However, as air quality has improved in industrial areas of America and Britain, through improved regulation, offering the conditions for a natural experiment, melanism has sharply declined in moths including \"B. betularia\" and \"Odontopera bidentata\". Cook and J. R. G. Turner have concluded that \"natural selection is the only credible explanation for the overall decline\", and other biologists working in the area concur with this judgement.\n\nIn 1921, the evolutionary biologist Richard Goldschmidt argued that the observed increase in the melanic form of the black arches moth, \"Lymantria monacha\", could not have been caused by mutation pressure alone, but required a selective advantage from an unknown cause: he did not consider camouflage as an explanation.\n\nNearly a century later, it was suggested that the moth's industrial melanism might, in addition (pleiotropy) to providing camouflage with \"the well-known protective dark coloration\", also confer better immunity to toxic chemicals from industrial pollution. The darker forms have a stronger immune response to foreign objects; these are encapsulated by haemocytes (insect blood cells), and the capsule so formed is then hardened with deposits of the dark pigment, melanin.\n\nA non-camouflage mechanism has been suggested for some vertebrates. In tropical ocean regions subject to industrial pollution the turtle-headed seasnake \"Emydocephalus annulatus\" is more likely to be melanic. These snakes shed their skin every two to six weeks. Sloughed skin contains toxic minerals, higher for dark skin, so industrial melanism could be selected for through improved excretion of trace elements. The same may apply in the case of urban feral pigeons, which have the ability to remove trace metals such as zinc to their feathers. However, toxic lead was not found to accumulate in feathers, so the putative mechanism is limited in its range.\n\nMelanic forms of the two-spot ladybird \"Adalia bipunctata\" are very frequent in and near cities, and rare in unpolluted countryside, so they appear to be industrial. Ladybirds are aposematic (with conspicuous warning coloration), so camouflage cannot explain the distribution. A proposed explanation is that the melanic forms have a thermal advantage directly linked to the pollution aspect of industrialization, since smoke and particulates in the air reduce the amount of sunlight that reaches the habitats of these species. Melanic phenotypes should then be favoured by natural selection, as the dark coloration absorbs the limited sunlight better. A possible explanation might be that in colder environments, the thermal advantages of industrial melanism might increase activity and the likelihood to mate. In the Netherlands, melanic \"A. bipunctata\" had a distinct mating advantage over the non-melanic form.\n\nHowever, thermal melanism failed to explain the distribution of the species near Helsinki where the city forms a relatively warm 'heat island', while near the Finnish coast there is more sunlight as well as more melanism, so the selective pressure driving melanism requires a different explanation. A study in Birmingham similarly found no evidence of thermal melanism but a strong correlation with smoke pollution; melanism declined from 1960 to 1978 as the city became cleaner. Further, the same study found that a related species, \"Adalia decempunctata\", experienced no change in frequency of melanism in the same places in that period.\n"}
{"id": "53384839", "url": "https://en.wikipedia.org/wiki?curid=53384839", "title": "International Union for Vacuum Science, Technique and Applications", "text": "International Union for Vacuum Science, Technique and Applications\n\nThe International Union for Vacuum Science, Technique, and Applications (IUVSTA) is a union of 33 science and technology national member societies whose role is to stimulate international collaboration in the fields of vacuum science, technique and applications, and related multi-disciplinary topics.\n\nIUVSTA is a Member Scientific Associate of the International Council for Science (ICSU).\n\nFounded in 1958, IUVSTA is an interdisciplinary union which represents several thousands of physicists, chemists, materials scientists, engineers and technologists who are active in basic and applied research, development, manufacturing, sales and education. IUVSTA finances advanced scientific workshops, international schools and technical courses, worldwide.\n\nIUVSTA comprises member societies from the following countries:\nArgentina, Australia, Austria, Belgium, Brazil, Bulgaria, China, Croatia, Czech Republic, Finland, France, Germany, Hungary, India, Israel, Iran, Italy, Japan, Korea, Mexico, Netherlands, Pakistan, Philippines, Poland, Portugal, Russian Federation, Slovakia, Slovenia, Spain, Sweden, Switzerland, United Kingdom, and USA.\n\nThe main purposes of the IUVSTA are to organize and sponsor international conferences and educational activities, as well as to facilitate research and technological developments in the field of vacuum science and its applications.\n\nThe history and structure of the Union are described in two articles in scientific journals.\n\nIUVSTA has nine technical divisions:\n\n\n"}
{"id": "31744838", "url": "https://en.wikipedia.org/wiki?curid=31744838", "title": "List of dates predicted for apocalyptic events", "text": "List of dates predicted for apocalyptic events\n\nPredictions of apocalyptic events that would result in the extinction of humanity, a collapse of civilization, or the destruction of the planet have been made since at least the beginning of the Common Era. Most predictions are related to Abrahamic religions, often standing for or similar to the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ. End-time events are usually predicted to occur within the lifetime of the person making the prediction, and are usually made using the Bible, and in particular the New Testament, as either the primary or exclusive source for the predictions. Often this takes the form of mathematical calculations, such as trying to calculate the point where it will have been 6000 years since the supposed creation of the Earth by the Abrahamic God, which according to the Talmud marks the deadline for the Messiah to appear. Predictions of the end from natural events have also been theorised by various scientists and scientific groups. While these predictions are generally accepted as plausible within the scientific community, the events and phenomena are not expected to occur for hundreds of thousands or even billions of years from now.\n\nLittle research has been done into why people make apocalyptic predictions. Historically, it has been done for reasons such as diverting attention from actual crises like poverty and war, pushing political agendas, and promoting hatred of certain groups; antisemitism was a popular theme of Christian apocalyptic predictions in medieval times, while French and Lutheran depictions of the apocalypse were known to feature English and Catholic antagonists respectively. According to psychologists, possible explanations for why people believe in modern apocalyptic predictions include mentally reducing the actual danger in the world to a single and definable source, an innate human fascination with fear, personality traits of paranoia and powerlessness and a modern romanticism involved with end-times due to its portrayal in contemporary fiction. The prevalence of Abrahamic religions throughout modern history is said to have created a culture which encourages the embracement of a future that will be drastically different from the present. Such a culture is credited with the rise in popularity of predictions that are more secular in nature, such as the 2012 phenomenon, while maintaining the centuries-old theme that a powerful force will bring the end of humanity.\n\nPolls conducted in 2012 across 20 countries found over 14% of people believe the world will end in their lifetime, with percentages ranging from 6% of people in France to 22% in the US and Turkey. Belief in the apocalypse is most prevalent in people with lower rates of education, lower household incomes, and those under the age of 35. In the UK in 2015, 23% of the general public believed the apocalypse was likely to occur in their lifetime, compared to 10% of experts from the Global Challenges Foundation. The general public believed the likeliest cause would be nuclear war, while experts thought it would be artificial intelligence. Only 3% of Britons thought the end would be caused by the Last Judgement, compared to 16% of Americans. Between one and three percent of people from both countries thought the apocalypse would be caused by zombies or alien invasion.\n\nThis section lists eschatological predictions, mostly by religious individuals or groups. Most predictions are related to Abrahamic religions, with numerous predictions standing for the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ.\n\n\n"}
{"id": "249527", "url": "https://en.wikipedia.org/wiki?curid=249527", "title": "List of decorative stones", "text": "List of decorative stones\n\nThis is a geographical list of natural stone used for decorative purposes in construction and monumental sculpture produced in various countries.\n\nThe dimension-stone industry classifies stone based on appearance and hardness as either \"granite\", \"marble\" or \"slate\".\n\nThe granite of the dimension-stone industry along with truly granitic rock also includes gneiss, gabbro, anorthosite and even some sedimentary rocks.\n\nNatural stone is used as architectural stone (construction, flooring, cladding, counter tops, curbing, etc.) and as raw block and monument stone for the funerary trade. Natural stone is also used in custom stone engraving. The engraved stone can be either decorative or functional. Natural memorial stones are used as natural burial markers.\n\nMarble\n\nPakistan has more than 300 kinds of marble and natural stone types and variations:\n\nIran have more than 250 kind of Marble stone& Travertine & Onyx & Granite & limestone\nIran is one of the best countries in variety of stones in the world.\nmarble: Dehbid in several sorts .persian silk- shahyadi- namin- khoy- bastam-kashmar-miami- are only some of the Iranian Marbles\ntravertin: hajjiabad -Atashkooh- Darrehbokhari- ... are only 3kinde of IRAN Travertine\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "49316021", "url": "https://en.wikipedia.org/wiki?curid=49316021", "title": "List of fritillaries (butterflies)", "text": "List of fritillaries (butterflies)\n\nThis is a list of butterfly species in diverse genera with the common name fritillary. The term refers to the chequered markings on the wings, usually black on orange, and derives from the Latin \"fritillus\" (meaning dice-box - or, according to some sources, a chequerboard: the fritillary flower, with its chequered markings, has the same derivation). Most fritillaries belong to the family Nymphalidae.\n"}
{"id": "33802674", "url": "https://en.wikipedia.org/wiki?curid=33802674", "title": "List of natural history dealers", "text": "List of natural history dealers\n\nNatural history specimen dealers had an important role in the development of science in the 18th, 19th and early 20th centuries. They supplied the rapidly growing, both in size and number, museums and educational establishments and private collectors whose collections, either in entirety or parts finally entered museums.\nMost sold not just zoological, botanical and geological specimens but also \nequipment and books. Many also sold archaeological and ethnographic items.They purchased \nspecimens from professional and amateur collectors, sometimes collected themselves as well as acting as agents for the sale of \ncollections. Many were based in mercantile centres notably Amsterdam, Hamburg, and London or \nin major cities. Some were specialists and some were taxonomic authorities who wrote scientific works and manuals, some functioned as trading museums or institutes.\n\nThis is a list of natural history dealers from the 16th to the 19th century: here are names that are frequently encountered in museum collections.\n\n\n\n\n\n\n\n\n"}
{"id": "16092414", "url": "https://en.wikipedia.org/wiki?curid=16092414", "title": "List of raised and transitional bogs of Switzerland", "text": "List of raised and transitional bogs of Switzerland\n\nThis is a list of raised and transitional bogs of Switzerland. It is based on the \"Federal Inventory of Raised and Transitional Bogs of National Importance\". The inventory is part of a 1991 Ordinance of the Swiss Federal Council implementing the Federal Law on the Protection of Nature and Cultural Heritage.\n\n\n"}
{"id": "60773", "url": "https://en.wikipedia.org/wiki?curid=60773", "title": "List of woods", "text": "List of woods\n\nThis is a list of woods, in particular those most commonly used in the timber and lumber trade.\n\n\n\n\n\n"}
{"id": "52555162", "url": "https://en.wikipedia.org/wiki?curid=52555162", "title": "Metal vapor synthesis", "text": "Metal vapor synthesis\n\nIn chemistry, metal vapor synthesis (MVS) is a method for preparing metal complexes by combining freshly produced metal atoms or small particles with ligands. In contrast to the high reactivity of such freshly produced metal atoms, bulk metals typically are unreactive toward neutral ligands. The method has been used to prepare compounds that cannot be prepared by traditional synthetic methods, e.g. Ti(η-toluene). The technique relies on a reactor that evaporates the metal, allowing the vapor to impinge on a cold reactor wall that is coated with the organic ligand. The metal evaporates upon being heated resistively or irradiated with an electron beam. The apparatus operates under high vacuum. In a common implementation, the metal vapor and the organic ligand are co-condensed at liquid nitrogen temperatures.\n\nIn several case where compounds are prepared by MVS, related preparations employ conventional routes. Thus, tris(butadiene)molybdenum was first prepared by co-condensation of butadiene and Mo vapor, but yields are higher for the reduction of molybdenum(V) chloride in the presence of the diene.\n"}
{"id": "21898316", "url": "https://en.wikipedia.org/wiki?curid=21898316", "title": "Meteorological intelligence", "text": "Meteorological intelligence\n\nMeteorological intelligence is information measured, gathered, compiled, exploited, analyzed and disseminated by meteorologists, climatologists and hydrologists to characterize the current state and/or predict the future state of the atmosphere at a given location and time. Meteorological intelligence is a subset of environmental intelligence and is synonymous with the term weather intelligence. \n\nThe earliest known use of the term \"meteorological intelligence\" in a written document dates to 1854 on pg. 168 of the Eighth Annual Report of the Board of Regents of the Smithsonian Institution. This report discusses the Smithsonian Institution's initiative to transmit meteorological intelligence via telegraph lines. An early reference to \"meteorological intelligence\" in England dates an 1866 issue of The Edinburgh Review which was a prominent Scottish journal during the 19th century (Reeve 1866, pg. 75). \n\nAnother documented, early use of the term dates to 1874 in a historical compilation entitled, \"The American Historical Record\" (Lossing 1874, pg. 125). In this book, Lossing uses the term to refer to weather observations transmitted over telegraph lines for the purpose of studying the nature of storms with the ultimate goal of enhancing public safety through the issuance of storm warnings. This mission was carried out by the Army Signal Service starting in the 1870s who was responsible for communication (via telegraph) of technical intelligence for the army as well as \"meteorological intelligence\" for the general welfare of the country (Ingersoll 1879, pg. 156).\n\nFrom the viewpoint of the intelligence community, the term meteorological intelligence is more limited in its use referring to the use of clandestine or technical means to learn about environmental conditions over enemy territory (Shulsky and Schmitt 2002) as in the North Atlantic weather war. In the military intelligence context, weather information is often referred to as meteorological or environmental intelligence (Hinsley 1990, pg. 420; Platt 1957, pg. 14; U.S. Congress, pg. 164). \n\nWith regard to private sector meteorology, the term meteorological intelligence is a broad term of art that is primarily associated with observed and forecast weather information provided to decision makers in one of a number of weather sensitive business areas including: Energy, forestry, agriculture, telecommunications, transportation, aviation, entertainment, retail and construction (CMOS 2001, pg. 23) . It is considered a key aspect of weather risk management for the legal and insurance industries.\n\n\n\n"}
{"id": "440959", "url": "https://en.wikipedia.org/wiki?curid=440959", "title": "Mpemba effect", "text": "Mpemba effect\n\nThe Mpemba effect is a process in which hot water can freeze faster than cold water. The phenomenon is temperature-dependent. There is disagreement about the parameters required to produce the effect and about its theoretical basis.\nThe Mpemba effect is named after Erasto Batholomeo Mpemba (b.1950) who discovered it in 1963. There were preceding ancient accounts of similar phenomena, but lacking sufficient detail to attempt verification.\n\nThe phenomenon, when taken to mean \"hot water freezes faster than cold\", is difficult to reproduce or confirm because this statement is ill-defined. Monwhea Jeng proposes as a more precise wording:\n\nThere exists a set of initial parameters, and a pair of temperatures, such that given two bodies of water identical in these parameters, and differing only in initial uniform temperatures, the hot one will freeze sooner.\n\nHowever, even with this definition it is not clear whether \"freezing\" refers to the point at which water forms a visible surface layer of ice; the point at which the entire volume of water becomes a solid block of ice; or when the water reaches . A quantity of water can be at and not be ice; after enough heat has been removed to reach more heat must be removed before the water changes to solid state (ice), so water can be liquid or solid at .\n\nWith the above definition there are simple ways in which the effect might be observed: For example, if the hotter temperature melts the frost on a cooling surface and thus increases the thermal conductivity between the cooling surface and the water container. On the other hand, there may be many circumstances in which the effect is not observed.\n\nVarious effects of heat on the freezing of water were described by ancient scientists such as Aristotle: \"The fact that the water has previously been warmed contributes to its freezing quickly: for so it cools sooner. Hence many people, when they want to cool water quickly, begin by putting it in the sun. So the inhabitants of Pontus when they encamp on the ice to fish (they cut a hole in the ice and then fish) pour warm water round their reeds that it may freeze the quicker, for they use the ice like lead to fix the reeds.\" Aristotle's explanation involved \"antiperistasis\", \"the supposed increase in the intensity of a quality as a result of being surrounded by its contrary quality.\"\n\nEarly modern scientists such as Francis Bacon noted that, \"slightly tepid water freezes more easily than that which is utterly cold.\" In the original Latin, \"aqua parum tepida facilius conglacietur quam omnino frigida.\"\n\nRené Descartes wrote in his \"Discourse on the Method\", \"One can see by experience that water that has been kept on a fire for a long time freezes faster than other, the reason being that those of its particles that are least able to stop bending evaporate while the water is being heated.\" This relates to Descartes' vortex theory.\n\nThe Scottish scientist Joseph Black investigated a special case of this phenomenon comparing previously-boiled with unboiled water; the previously-boiled water froze more quickly. Evaporation was controlled for. He discussed the influence of stirring on the results of the experiment, noting that stirring the unboiled water led to it freezing at the same time as the previously-boiled water, and also noted that stirring the very-cold unboiled water led to immediate freezing. Joseph Black then discussed Fahrenheit's description of supercooling of water (although the term supercooling had not then been coined), arguing, in modern terms, that the previously-boiled water could not be as readily supercooled.\n\nThe effect is named after Tanzanian Erasto Mpemba. He described it in 1963 in Form 3 of Magamba Secondary School, Tanganyika, when freezing ice cream mix that was hot in cookery classes and noticing that it froze before the cold mix. He later became a student at Mkwawa Secondary (formerly High) School in Iringa. The headmaster invited Dr. Denis G. Osborne from the University College in Dar es Salaam to give a lecture on physics. After the lecture, Erasto Mpemba asked him the question, \"If you take two similar containers with equal volumes of water, one at and the other at , and put them into a freezer, the one that started at freezes first. Why?\", only to be ridiculed by his classmates and teacher. After initial consternation, Osborne experimented on the issue back at his workplace and confirmed Mpemba's finding. They published the results together in 1969, while Mpemba was studying at the College of African Wildlife Management.\n\nMpemba and Osborne describe placing samples of water in beakers in the ice box of a domestic refrigerator on a sheet of polystyrene foam. They showed the time for \"freezing to start\" was longest with an initial temperature of and that it was much less at around . They ruled out loss of liquid volume by evaporation as a significant factor and the effect of dissolved air. In their setup most heat loss was found to be from the liquid surface.\nDavid Auerbach describes an effect that he observed in samples in glass beakers placed into a liquid cooling bath. In all cases the water supercooled, reaching a temperature of typically before spontaneously freezing. Considerable random variation was observed in the time required for spontaneous freezing to start and in some cases this resulted in the water which started off hotter (partially) freezing first.\n\nJames Brownridge, a radiation safety officer at the State University of New York, has said that he believes that supercooling is involved. Several molecular dynamics simulations have also supported that changes in hydrogen bonding during supercooling takes a major role in the process.\n\nIn 2016, Burridge and Linden defined the criterion as the time to reach , carried out experiments and reviewed published work to date. They noted that the large difference originally claimed had not been replicated, and that studies showing a small effect could be influenced by variations in the positioning of thermometers. They say, \"We conclude, somewhat sadly, that there is no evidence to support meaningful observations of the Mpemba effect\".\n\nHowever, in 2017, two research groups independently and simultaneously found theoretical evidence of the Mpemba effect and also predicted a new \"inverse\" Mpemba effect in which heating a cooled, far-from-equilibrium system takes less time than another system that is initially closer to equilibrium. Lu and Raz yield a general criterion based on Markovian statistical mechanics, predicting the appearance of the inverse Mpemba effect in the Ising model and diffusion dynamics. Lasanta and co-workers predict also the direct and inverse Mpemba effects for a granular gas in a far-from-equilibrium initial state. In this last work, it is suggested that a very generic mechanism leading to both Mpemba effects is due to a particle velocity distribution function that significantly deviates from the Maxwell-Boltzmann distribution.\n\nThe following explanations have been proposed:\n\n\nA reviewer for \"Physics World\" writes, \"Even if the Mpemba effect is real — if hot water can sometimes freeze more quickly than cold — it is not clear whether the explanation would be trivial or illuminating.\" He pointed out that investigations of the phenomenon need to control a large number of initial parameters (including type and initial temperature of the water, dissolved gas and other impurities, and size, shape and material of the container, and temperature of the refrigerator) and need to settle on a particular method of establishing the time of freezing, all of which might affect the presence or absence of the Mpemba effect. The required vast multidimensional array of experiments might explain why the effect is not yet understood.\n\n\"New Scientist\" recommends starting the experiment with containers at to maximize the effect. In a related study, it was found that freezer temperature also affects the probability of observing the Mpemba phenomenon as well as container temperature.\n\nIn 2012, the Royal Society of Chemistry held a competition calling for papers offering explanations to the Mpemba effect. More than 22,000 people entered and Erasto Mpemba himself announced Nikola Bregović as the winner. Bregović suggests two reasons for the effect — a colder sample gets supercooled rather than frozen, and enhanced convection in the warmer sample speeds up cooling by maintaining the heat gradient on the container walls.\n\nTao and co-workers proposed yet another possible explanation in 2016. On the basis of results from vibrational spectroscopy and modeling with density functional theory-optimized water clusters, they suggest that the reason might lie in the vast diversity and peculiar occurrence of different hydrogen bonds. Their key argument is that the number of strong hydrogen bonds increases as temperature is elevated. The existence of the small strongly-bonded clusters facilitates in turn the nucleation of hexagonal ice when warm water is rapidly cooled down.\n\nOther phenomena in which large effects may be achieved faster than small effects are\n\n\nNotes\n\n"}
{"id": "14389994", "url": "https://en.wikipedia.org/wiki?curid=14389994", "title": "Natural landscape", "text": "Natural landscape\n\nA natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.\n\nIn \"Silent Spring\" (1962) Rachel Carson describes a roadside verge as it used to look: \"Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year\" and then how it looks now following the use of herbicides: \"The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire\". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.\n\nThe phrase \"natural landscape\" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape \"separate\" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape (\"Urlandschaft\") and its opposite cultural landscape (\"Kulturlandschaft\") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase \"natural landscape\" by a geographer can be found in Carl O. Sauer's paper \"The Morphology of Landscape\" (1925).\n\nThe concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, \"that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape.\" With regard to landscape gardening John Aikin, commented in 1794: \"Whatever, therefore, there be of \"novelty\" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms\". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: \"straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape\".\n\nIn his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.\n\nSubsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a \"Landschaftskunde\" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.\n\nThe term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.\n\nMatters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: \"The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations\". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.\n\nThe dualism of the first definition has its roots is an \"ancient concept\", because early people viewed \"nature, or the nonhuman world […] as a divine \"Other\", godlike in its separation from humans\". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. \nWith this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.\n\nWhat is meant by natural, within the American conservation movement, has been changing over the last century and a half.\n\nIn the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and \"avoid disturbances such as logging, grazing, fire and insect outbreaks\". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.\n\nA century later, in the mid-twentieth century, it began to be believed that the earlier policy of \"protection from disturbance was inadequate to preserve park values\", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that \"A national park should represent a vignette of primitive America\". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.\n\nHowever, recent research in various disciplines indicates that a pristine natural or \"primitive\" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.\n\nAlso important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: \"Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it \"involves a dualistic vision in which the human is entirely outside the natural\" and affirms that \"wildness (as opposed to wilderness) can be found anywhere\" even \"in the cracks of a Manhattan sidewalk\". According to Cronon we have to \"abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild.\" Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.\n\nThe landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only \" the high summits of the Cairngorm Mountains, consist entirely of natural elements. These \"high summits\" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.\n\nThe Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.\nVisitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.\n\nNo place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.\nEven the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.\n\nCultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.\n\n"}
{"id": "3627783", "url": "https://en.wikipedia.org/wiki?curid=3627783", "title": "Natural monument", "text": "Natural monument\n\nA natural monument is a natural or natural/cultural feature of outstanding or unique value because of its inherent rarity, representative of aesthetic qualities or cultural significance. \n\nUnder World Commission on Protected Areas guidelines, natural monuments are level III, described as:\nThis is a lower level of protection than level II (national parks) and level I (wilderness areas).\n\nThe European Environment Agency's guidelines for selection of a natural monument are:\n\n\n"}
{"id": "14272151", "url": "https://en.wikipedia.org/wiki?curid=14272151", "title": "Nature religion", "text": "Nature religion\n\nA nature religion is a religious movement that believes nature and the natural world is an embodiment of divinity, sacredness or spiritual power. Nature religions include indigenous religions practiced in various parts of the world by cultures who consider the environment to be imbued with spirits and other sacred entities. It also includes contemporary Pagan faiths which are primarily concentrated in Europe and North America.\n\nThe term \"nature religion\" was first coined by the American religious studies scholar Catherine Albanese, who used it in her work \"Nature Religion in America: From the Algonkian Indians to the New Age\" (1991) and later went on to use it in other studies. Following on from Albanese's development of the term it has since been used by other academics working in the discipline.\n\nCatherine Albanese described nature religion as \"a symbolic center and the cluster of beliefs, behaviours, and values that encircles it\", deeming it to be useful for shining a light on aspects of history that are rarely viewed as religious.\nIn a paper of his on the subject, the Canadian religious studies scholar Peter Beyer described \"nature religion\" as a \"useful analytical abstraction\" to refer to \"any religious belief or practice in which devotees consider nature to be the embodiment of divinity, sacredness, transcendence, spiritual power, or whatever cognate term one wishes to use\". He went on to note that in this way nature religion was not an \"identifiable religious tradition\" such as Buddhism or Christianity are, but that it instead covers \"a range of religious and quasi-religious movements, groups and social networks whose participants may or may not identify with one of the many constructed religions of global society which referred to many other nature religion.\"\n\nPeter Beyer noted the existence of a series of common characteristics which he believed were shared by different nature religions. He remarked that although \"one must be careful not to overgeneralise\", he suspected that there were a series of features which \"occur sufficiently often\" in those nature religions known to recorded scholarship to constitute a pattern.\n\nThe first of these common characteristics was nature religion's \"comparative resistance to institutionalisation and legitimisation in terms of identifiable socio-religious authorities and organisations\", meaning that nature religionists rarely formed their religious beliefs into large, visible socio-political structures such as churches. Furthermore, Beyer noted, nature religionists often held a \"concomitant distrust of and even eschewing of politically orientated power\". Instead of this, he felt that among nature religious communities, there was \"a valuing of community as non-hierarchical\" and a \"conditional optimism with regard to human capacity and the future.\"\n\nIn the sphere of the environment, Beyer noted that nature religionists held to a \"holistic conception of reality\" and \"a valorisation of physical place as vital aspects of their spiritualities\". Similarly, Beyer noted the individualism which was favoured by nature religionists. He remarked that those adhering to such beliefs typically had respect for \"charismatic and hence purely individual authority\" and place a \"strong emphasis on individual paths\" which led them to believe in \"the equal value of individuals and groups\". Along similar lines, he also commented on the \"strong experiential basis\" to nature religionist beliefs \"where personal experience is a final arbiter of truth or validity\".\n\nIn April 1996, the University of Lancaster in North West England held a conference on contemporary Paganism entitled \"Nature Religion Today: Western Paganism, Shamanism and Esotericism in the 1990s\", and ultimately led to the publication of an academic anthology of the same name two years later. This book, \"Nature Religion Today: Paganism in the Modern World\", was edited by members of the University's Department of Religious Studies, a postgraduate named Joanne Pearson and two professors, Richard H. Roberts and Geoffrey Samuel.\n\nIn his study of Wicca, the Pagan studies scholar Ethan Doyle White expressed the view that the category of \"nature religion\" was problematic from a \"historical perspective\" because it solely emphasises the \"commonalities of belief and attitude to the natural world\" that are found between different religions and in doing so divorces these different belief systems from their distinctive socio-cultural and historical backgrounds.\n\n\n"}
{"id": "188094", "url": "https://en.wikipedia.org/wiki?curid=188094", "title": "Nikolaas Tinbergen", "text": "Nikolaas Tinbergen\n\nNikolaas \"Niko\" Tinbergen (; ; 15 April 1907 – 21 December 1988) was a Dutch biologist and ornithologist who shared the 1973 Nobel Prize in Physiology or Medicine with Karl von Frisch and Konrad Lorenz for their discoveries concerning organization and elicitation of individual and social behavior patterns in animals. He is regarded as one of the founders of modern ethology, the study of animal behavior.\n\nIn 1951, he published \"The Study of Instinct\", an influential book on animal behaviour.\nIn the 1960s, he collaborated with filmmaker Hugh Falkus on a series of wildlife films, including \"The Riddle of the Rook\" (1972) and \"Signals for Survival\" (1969), which won the Italia prize in that year and the American blue ribbon in 1971.\n\nBorn in The Hague, Netherlands, he was one of five children of Dirk Cornelis Tinbergen and his wife Jeannette van Eek. His brother, Jan Tinbergen, won the first Bank of Sweden Prize in Economic Sciences in Memory of Alfred Nobel in 1969. They are the only siblings to each win a Nobel Prize. Another brother, Luuk Tinbergen was also a noted biologist.\n\nTinbergen's interest in nature manifested itself when he was young. He studied biology at Leiden University and was a prisoner of war during World War II. Tinbergen's experience as a prisoner of the Nazis led to some friction with longtime intellectual collaborator Konrad Lorenz, and it was several years before the two reconciled.\n\nAfter the war, Tinbergen moved to England, where he taught at the University of Oxford and was a fellow first at Merton College, Oxford and later at Wolfson College, Oxford. Several of his graduate students went on to become prominent biologists including Richard Dawkins, Marian Dawkins, Desmond Morris, Iain Douglas-Hamilton, and Tony Sinclair.\n\nIn 1951 Tinbergen's \"The Study of Instinct\" was published. Behavioural ecologists and evolutionary biologists still recognise the contribution this book offered the field of behavioural science studies. \"The Study of Instinct\" summarises Tinbergen's ideas on innate behavioural reactions in animals and the adaptiveness and evolutionary aspects of these behaviours. By behaviour, he means the total movements made by the intact animal; innate behaviour is that which is not changed by the learning process. The major question of the book is the role of internal and external stimuli in controlling the expression of behaviour.\n\nIn particular, he was interested in explaining 'spontaneous' behaviours: those that occurred in their complete form the first time they were performed and that seemed resistant to the effects of learning. He explains how behaviour can be considered a combination of these spontaneous behaviour patterns and as set series of reactions to particular stimuli. Behaviour is a reaction in that to a certain extent it is reliant on external stimuli, however it is also spontaneous since it is also dependent upon internal causal factors.\n\nHis model for how certain behavioural reactions are provoked was based on work by Konrad Lorenz. Lorenz postulated that for each instinctive act there is a specific energy which builds up in a reservoir in the brain. In this model, Lorenz envisioned a reservoir with a spring valve at its base that an appropriate stimulus could act on, much like a weight on a scale pan pulling against a spring and releasing the reservoir of energy, an action which would lead an animal to express the desired behaviour.\n\nTinbergen added complexity to this model, a model now known as Tinbergen's hierarchical model. He suggested that motivational impulses build up in nervous centres in the brain which are held in check by blocks. The blocks are removed by an innate releasing mechanism that allows the energy to flow to the next centre (each centre containing a block that needs to be removed) in a cascade until the behaviour is expressed. Tinbergen's model shows multiple levels of complexity and that related behaviours are grouped.\n\nAn example is in his experiments with foraging honey bees. He showed that honey bees show curiosity for yellow and blue paper models of flowers, and suggested that these were visual stimuli causing the buildup of energy in one specific centre. However, the bees rarely landed on the model flowers unless the proper odour was also applied. In this case, the chemical stimuli of the odour allowed the next link in the chain to be released, encouraging the bee to land. The final step was for the bee to insert its mouthparts into the flower and initiate suckling. Tinbergen envisioned this as concluding the reaction set for honey bee feeding behaviour.\n\nIn 1973 Tinbergen, along with Konrad Lorenz and Karl von Frisch, were awarded the Nobel Prize in Physiology or Medicine \"for their discoveries concerning organization and elicitation of individual and social behaviour patterns\". The award recognised their studies on genetically programmed behaviour patterns, their origins, maturation and their elicitation by key stimuli. In his Nobel Lecture, Tinbergen addressed the somewhat unconventional decision of the Nobel Foundation to award the prize for Physiology or Medicine to three men who had until recently been regarded as \"mere animal watchers\". Tinbergen stated that their revival of the \"watching and wondering\" approach to studying behaviour could indeed contribute to the relief of human suffering.\n\nThe studies performed by the trio on fish, insects and birds laid the foundation for further studies on the importance of specific experiences during critical periods of normal development, as well as the effects of abnormal psychosocial situations in mammals. At the time, these discoveries were stated to have caused \"a breakthrough in the understanding of the mechanisms behind various symptoms of psychiatric disease, such as anguish, compulsive obsession, stereotypic behaviour and catatonic posture\". Tinbergen’s contribution to these studies included the testing of the hypotheses of Lorenz/von Frisch by means of \"comprehensive, careful, and ingenious experiments\" as well as his work on supernormal stimuli. The work of Tinbergen during this time was also regarded as having possible implications for further research in child development and behaviour.\n\nHe also caused some intrigue by dedicating a large part of his acceptance speech to FM Alexander, originator of the Alexander technique, a method which investigates postural reflexes and responses in human beings.\n\nIn 1950 Tinbergen became member of the Royal Netherlands Academy of Arts and Sciences. He was elected a Fellow of the Royal Society (FRS) in 1962. He was also awarded the Godman-Salvin Medal in 1969 by the British Ornithologists' Union, and in 1973 received the Swammderdam Medal and Wilhelm Bölsche Medal (from the Genootschap ter bervordering van Natuur-, Genees- en Heelkunde of the University of Amsterdam and the Kosmos-Gesellschaft der Naturfreunde respectively).\n\nTinbergen described four questions he believed should be asked of any animal behaviour, which were:\n\n\nIn ethology and sociobiology, causation and ontogeny are summarised as the \"proximate mechanisms\", while adaptation and phylogeny are the \"ultimate mechanisms\". They are still considered as the cornerstone of modern ethology, sociobiology and transdisciplinarity in Human Sciences.\n\nA major body of Tinbergen's research focused on what he termed the supernormal stimulus. This was the concept that one could build an artificial object which was a stronger stimulus or releaser for an instinct than the object for which the instinct originally evolved. He constructed plaster eggs to see which a bird preferred to sit on, finding that they would select those that were larger, had more defined markings, or more saturated colour—and a dayglo-bright one with black polka dots would be selected over the bird's own pale, dappled eggs.\n\nTinbergen found that territorial male three-spined stickleback (a small freshwater fish) would attack a wooden fish model more vigorously than a real male if its underside was redder. He constructed cardboard dummy butterflies with more defined markings that male butterflies would try to mate with in preference to real females. The superstimulus, by its exaggerations, clearly delineated what characteristics were eliciting the instinctual response.\n\nAmong the modern works calling attention to Tinbergen's classic work is Deirdre Barrett's 2010 book, \"Supernormal Stimuli\".\n\nTinbergen applied his observational methods to the problems of autistic children. He recommended a \"holding therapy\" in which parents hold their autistic children for long periods of time while attempting to establish eye contact, even when a child resists the embrace. However, his interpretations of autistic behaviour, and the holding therapy that he recommended, lacked scientific support and the therapy is described as controversial and potentially abusive.\n\nSome of the publications of Tinbergen are:\n\nPublications about Tinbergen and his work:\n\nTinbergen was a member of the advisory committee to the Anti-Concorde Project and was also an atheist.\n\nTinbergen married Elisabeth Rutten and they had five children. Later in life he suffered depression and feared he might, like his brother Luuk, commit suicide. He was treated by his friend, whose ideas he had greatly influenced, John Bowlby. Tinbergen died on 21 December 1988, after suffering a stroke at his home in Oxford, England.\n"}
{"id": "58590304", "url": "https://en.wikipedia.org/wiki?curid=58590304", "title": "Orapa Power Station", "text": "Orapa Power Station\n\nThe Orapa Power Station is a Peak load power generation plant located in the mining town of Orapa in northeastern Botswana in the Central District. It is built within the Debswana Diamond Company Ltd Orapa diamond mine fenced leased area and is operated by the Botswana Power Corporation.\nThe plant construction was initiated when Botswana started experiencing electricity supply challenges from year 2007 when demand for electricity in the country started exceeding the county's electricity generation capacity leading to forced periodic nation wide load shedding excises by the Botswana Power Corporation. The plant was a short term response to the mining industry electricity requirements prior to the development of Mmamabula and Morupule B power stations.\n\nThe plant site is located next to an electrical substation through which it connects to the national grid and was designed to use either natural gas or diesel as fuel. It was commissioned using diesel with a planned conversion to natural gas once construction of a gas pipeline from nearby gas fields was complete and commissioned.\n\nThe plant consists of two 45MW GE LM6000 Sprint Simple Cycle gas turbines fueled using diesel. The energy produced is transferred to the national grid via the interconnected Orapa Substation by use of two short 132kV overhead power lines.\n\n"}
{"id": "33316231", "url": "https://en.wikipedia.org/wiki?curid=33316231", "title": "Pouillet effect", "text": "Pouillet effect\n\nIn physics, the term Pouillet effect refers to an exothermic reaction that takes place when a liquid is added to a powder. It was first observed by Leslie in 1802 when dry sawdust was wetted with water. Claude Pouillet later described this phenomenon in 1822 when it came to be known as the Pouillet effect in France.\n"}
{"id": "827792", "url": "https://en.wikipedia.org/wiki?curid=827792", "title": "Rare Earth hypothesis", "text": "Rare Earth hypothesis\n\nIn planetary astronomy and astrobiology, the Rare Earth hypothesis argues that the origin of life and the evolution of biological complexity such as sexually reproducing, multicellular organisms on Earth (and, subsequently, human intelligence) required an improbable combination of astrophysical and geological events and circumstances.\n\nAccording to the hypothesis, complex extraterrestrial life is an improbable phenomenon and likely to be rare. The term \"Rare Earth\" originates from \"Rare Earth: Why Complex Life Is Uncommon in the Universe\" (2000), a book by Peter Ward, a geologist and paleontologist, and Donald E. Brownlee, an astronomer and astrobiologist, both faculty members at the University of Washington.\n\nA contrary view was argued in the 1970s and 1980s by Carl Sagan and Frank Drake, among others. It holds that Earth is a typical rocky planet in a typical planetary system, located in a non-exceptional region of a common barred-spiral galaxy. Given the principle of mediocrity (in the same vein as the Copernican principle), it is probable that we are typical, and the universe teems with complex life. However, Ward and Brownlee argue that planets, planetary systems, and galactic regions that are as friendly to complex life as the Earth, the Solar System, and our galactic region are rare.\n\nThe Rare Earth hypothesis argues that the evolution of biological complexity requires a host of fortuitous circumstances, such as a galactic habitable zone, a central star and planetary system having the requisite character, the circumstellar habitable zone, a right-sized terrestrial planet, the advantage of a gas giant guardian like Jupiter and a large natural satellite, conditions needed to ensure the planet has a magnetosphere and plate tectonics, the chemistry of the lithosphere, atmosphere, and oceans, the role of \"evolutionary pumps\" such as massive glaciation and rare bolide impacts, and whatever led to the appearance of the eukaryote cell, sexual reproduction and the Cambrian explosion of animal, plant, and fungi phyla. The evolution of human intelligence may have required yet further events, which are extremely unlikely to have happened were it not for the Cretaceous–Paleogene extinction event 66 million years ago removing dinosaurs as the dominant terrestrial vertebrates.\n\nIn order for a small rocky planet to support complex life, Ward and Brownlee argue, the values of several variables must fall within narrow ranges. The universe is so vast that it could contain many Earth-like planets. But if such planets exist, they are likely to be separated from each other by many thousands of light years. Such distances may preclude communication among any intelligent species evolving on such planets, which would solve the Fermi paradox: \"If extraterrestrial aliens are common, why aren't they obvious?\"\n\n\"Rare Earth\" suggests that much of the known universe, including large parts of our galaxy, are \"dead zones\" unable to support complex life. Those parts of a galaxy where complex life is possible make up the galactic habitable zone, primarily characterized by distance from the Galactic Center. As that distance increases:\nItem #1 rules out the outer reaches of a galaxy; #2 and #3 rule out galactic inner regions. Hence a galaxy's habitable zone may be a ring sandwiched between its uninhabitable center and outer reaches.\n\nAlso, a habitable planetary system must maintain its favorable location long enough for complex life to evolve. A star with an eccentric (elliptic or hyperbolic) galactic orbit will pass through some spiral arms, unfavorable regions of high star density; thus a life-bearing star must have a galactic orbit that is nearly circular, with a close synchronization between the orbital velocity of the star and of the spiral arms. This further restricts the galactic habitable zone within a fairly narrow range of distances from the Galactic Center. Lineweaver et al. calculate this zone to be a ring 7 to 9 kiloparsecs in radius, including no more than 10% of the stars in the Milky Way, about 20 to 40 billion stars. Gonzalez, et al. would halve these numbers; they estimate that at most 5% of stars in the Milky Way fall in the galactic habitable zone.\n\nApproximately 77% of observed galaxies are spiral, two-thirds of all spiral galaxies are barred, and more than half, like the Milky Way, exhibit multiple arms. According to Rare Earth, our own galaxy is unusually quiet and dim (see below), representing just 7% of its kind. Even so, this would still represent more than 200 billion galaxies in the known universe.\n\nOur galaxy also appears unusually favorable in suffering fewer collisions with other galaxies over the last 10 billion years, which can cause more supernovae and other disturbances. Also, the Milky Way's central black hole seems to have neither too much nor too little activity.\n\nThe orbit of the Sun around the center of the Milky Way is indeed almost perfectly circular, with a period of 226 Ma (million years), closely matching the rotational period of the galaxy. However, the majority of stars in barred spiral galaxies populate the spiral arms rather than the halo and tend to move in gravitationally aligned orbits, so there is little that is unusual about the Sun's orbit. While the Rare Earth hypothesis predicts that the Sun should rarely, if ever, have passed through a spiral arm since its formation, astronomer Karen Masters has calculated that the orbit of the Sun takes it through a major spiral arm approximately every 100 million years. Some researchers have suggested that several mass extinctions do correspond with previous crossings of the spiral arms.\n\nThe terrestrial example suggests that complex life requires liquid water, requiring an orbital distance neither too close nor too far from the central star, another scale of habitable zone or Goldilocks Principle: \nThe habitable zone varies with the star's type and age.\n\nFor advanced life, the star must also be highly stable, which is typical of middle star life, about 4.6 billion years old. Proper metallicity and size are also important to stability. The Sun has a low 0.1% luminosity variation. To date no solar twin star twin, with an exact match of the sun's luminosity variation, has been found, though some come close. The star must have no stellar companions, as in binary systems, which would disrupt the orbits of planets. Estimates suggest 50% or more of all star systems are binary. The habitable zone for a main sequence star very gradually moves out over its lifespan until it becomes a white dwarf and the habitable zone vanishes.\n\nThe liquid water and other gases available in the habitable zone bring the benefit of greenhouse warming. Even though the Earth's atmosphere contains a water vapor concentration from 0% (in arid regions) to 4% (in rain forest and ocean regions) and – as of February 2018 – only 408.05 parts per million of , these small amounts suffice to raise the average surface temperature by about 40 °C, with the dominant contribution being due to water vapor, which together with clouds makes up between 66% and 85% of Earth's greenhouse effect, with contributing between 9% and 26% of the effect.\n\nRocky planets must orbit within the habitable zone for life to form. Although the habitable zone of such hot stars as Sirius or Vega is wide, hot stars also emit much more ultraviolet radiation that ionizes any planetary atmosphere. They may become red giants before advanced life evolves on their planets.\nThese considerations rule out the massive and powerful stars of type F6 to O (see stellar classification) as homes to evolved metazoan life.\n\nSmall red dwarf stars conversely have small habitable zones wherein planets are in tidal lock, with one very hot side always facing the star and another very cold side; and they are also at increased risk of solar flares (see Aurelia). Life therefore cannot arise in such systems. Rare Earth proponents claim that only stars from F7 to K1 types are hospitable. Such stars are rare: G type stars such as the Sun (between the hotter F and cooler K) comprise only 9% of the hydrogen-burning stars in the Milky Way.\n\nSuch aged stars as red giants and white dwarfs are also unlikely to support life. Red giants are common in globular clusters and elliptical galaxies. White dwarfs are mostly dying stars that have already completed their red giant phase. Stars that become red giants expand into or overheat the habitable zones of their youth and middle age (though theoretically planets at a much greater distance may become habitable).\n\nAn energy output that varies with the lifetime of the star will likely prevent life (e.g., as Cepheid variables). A sudden decrease, even if brief, may freeze the water of orbiting planets, and a significant increase may evaporate it and cause a greenhouse effect that prevents the oceans from reforming.\n\nAll known life requires the complex chemistry of metallic elements. The absorption spectrum of a star reveals the presence of metals within, and studies of stellar spectra reveal that many, perhaps most, stars are poor in metals. Because heavy metals originate in supernova explosions, metallicity increases in the universe over time. Low metallicity characterizes the early universe: globular clusters and other stars that formed when the universe was young, stars in most galaxies other than large spirals, and stars in the outer regions of all galaxies. Metal-rich central stars capable of supporting complex life are therefore believed to be most common in the quiet suburbs of the larger spiral galaxies—where radiation also happens to be weak.\n\nRare Earth proponents argue that a planetary system capable of sustaining complex life must be structured more or less like the Solar System, with small and rocky inner planets and outer gas giants. Without the protection of 'celestial vacuum cleaner' planets with strong gravitational pull, a planet would be subject to more catastrophic asteroid collisions.\n\nObservations of exo-planets have shown that arrangements of planets similar to our Solar System are rare. Most planetary systems have super Earths, several times larger than Earth, close to their star, whereas our Solar System's inner region has only a few small rocky planets and none inside Mercury's orbit. Only 10% of stars have giant planets similar to Jupiter and Saturn, and those few rarely have stable nearly circular orbits distant from their star. Konstantin Batygin and colleagues argue that these features can be explained if, early in the history of the Solar System, Jupiter and Saturn drifted towards the Sun, sending showers of planetesimals towards the super-Earths which sent them spiralling into the Sun, and ferrying icy building blocks into the terrestrial region of the Solar System which provided the building blocks for the rocky planets. The two giant planets then drifted out again to their present position. However, in the view of Batygin and his colleagues: \"The concatenation of chance events required for this delicate choreography suggest that small, Earth-like rocky planets – and perhaps life itself – could be rare throughout the cosmos.\"\n\nRare Earth argues that a gas giant must not be too close to a body where life is developing. Close placement of gas giant(s) could disrupt the orbit of a potential life-bearing planet, either directly or by drifting into the habitable zone.\n\nNewtonian dynamics can produce chaotic planetary orbits, especially in a system having large planets at high orbital eccentricity.\n\nThe need for stable orbits rules out stars with systems of planets that contain large planets with orbits close to the host star (called \"hot Jupiters\"). It is believed that hot Jupiters have migrated inwards to their current orbits. In the process, they would have catastrophically disrupted the orbits of any planets in the habitable zone. To exacerbate matters, hot Jupiters are much more common orbiting F and G class stars.\n\nIt is argued that life requires terrestrial planets like Earth and as gas giants lack such a surface, that complex life cannot arise there.\n\nA planet that is too small cannot hold much atmosphere, making surface temperature low and variable and oceans impossible. A small planet will also tend to have a rough surface, with large mountains and deep canyons. The core will cool faster, and plate tectonics may be brief or entirely absent. A planet that is too large will retain too dense an atmosphere like Venus. Although Venus is similar in size and mass to Earth, its surface atmospheric pressure is 92 times that of Earth, and surface temperature of 735 K (462 °C; 863 °F). Earth had a similar early atmosphere to Venus, but may have lost it in the giant impact event.\n\nRare Earth proponents argue that plate tectonics and a strong magnetic field are essential for biodiversity, global temperature regulation, and the carbon cycle.\nThe lack of mountain chains elsewhere in the Solar System is direct evidence that Earth is the only body with plate tectonics, and thus the only nearby body capable of supporting life.\n\nPlate tectonics depend on the right chemical composition and a long-lasting source of heat from radioactive decay. Continents must be made of less dense felsic rocks that \"float\" on underlying denser mafic rock. Taylor emphasizes that tectonic subduction zones require the lubrication of oceans of water. Plate tectonics also provides a means of biochemical cycling.\n\nPlate tectonics and as a result continental drift and the creation of separate land masses would create diversified ecosystems and biodiversity, one of the strongest defences against extinction. An example of species diversification and later competition on Earth's continents is the Great American Interchange. North and Middle America drifted into South America at around 3.5 to 3 Ma. The fauna of South America evolved separately for about 30 million years, since Antarctica separated. Many species were subsequently wiped out in mainly South America by competing Northern American animals.\n\nThe Moon is unusual because the other rocky planets in the Solar System either have no satellites (Mercury and Venus), or only tiny satellites which are probably captured asteroids (Mars).\n\nThe Giant-impact theory hypothesizes that the Moon resulted from the impact of a Mars-sized body, dubbed Theia, with the young Earth. This giant impact also gave the Earth its axial tilt (inclination) and velocity of rotation. Rapid rotation reduces the daily variation in temperature and makes photosynthesis viable. The \"Rare Earth\" hypothesis further argues that the axial tilt cannot be too large or too small (relative to the orbital plane). A planet with a large tilt will experience extreme seasonal variations in climate. A planet with little or no tilt will lack the stimulus to evolution that climate variation provides. In this view, the Earth's tilt is \"just right\". The gravity of a large satellite also stabilizes the planet's tilt; without this effect the variation in tilt would be chaotic, probably making complex life forms on land impossible.\n\nIf the Earth had no Moon, the ocean tides resulting solely from the Sun's gravity would be only half that of the lunar tides. A large satellite gives rise to tidal pools, which may be essential for the formation of complex life, though this is far from certain.\n\nA large satellite also increases the likelihood of plate tectonics through the effect of tidal forces on the planet's crust. The impact that formed the Moon may also have initiated plate tectonics, without which the continental crust would cover the entire planet, leaving no room for oceanic crust. It is possible that the large scale mantle convection needed to drive plate tectonics could not have emerged in the absence of crustal inhomogeneity. A further theory indicates that such a large moon may also contribute to maintaining a planet's magnetic shield by continually acting upon a metallic planetary core as dynamo, thus protecting the surface of the planet from charged particles and cosmic rays, and helping to ensure the atmosphere is not stripped over time by solar winds.\n\nA terrestrial planet of the right size is needed to retain an atmosphere, like Earth and Venus. On Earth, once the giant impact of Theia thinned Earth's atmosphere, other events were needed to make the atmosphere capable of sustaining life. The Late Heavy Bombardment reseeded Earth with water lost after the impact of Theia. The development of an ozone layer formed protection from ultraviolet (UV) sunlight. Nitrogen and carbon dioxide are needed in a correct ratio for life to form. Lightning is needed for nitrogen fixation. The carbon dioxide gas needed for life comes from sources such as volcanoes and geysers. Carbon dioxide is only needed at low levels (currently at 400 ppm); at high levels it is poisonous. Precipitation is needed to have a stable water cycle. A proper atmosphere must reduce diurnal temperature variation.\n\nRegardless of whether planets with similar physical attributes to the Earth are rare or not, some argue that life usually remains simple bacteria. Biochemist Nick Lane argues that simple cells (prokaryotes) emerged soon after Earth's formation, but since almost half the planet's life had passed before they evolved into complex ones (eukaryotes) all of whom share a common ancestor, this event can only have happened once. In some views, prokaryotes lack the cellular architecture to evolve into eukaryotes because a bacterium expanded up to eukaryotic proportions would have tens of thousands of times less energy available; two billion years ago, one simple cell incorporated itself into another, multiplied, and evolved into mitochondria that supplied the vast increase in available energy that enabled the evolution of complex life. If this incorporation occurred only once in four billion years or is otherwise unlikely, then life on most planets remains simple. An alternative view is that mitochondria evolution was environmentally triggered, and that mitochondria-containing organisms appeared soon after the first traces of atmospheric oxygen.\n\nThe evolution and persistence of sexual reproduction is another mystery in biology. The purpose of sexual reproduction is unclear, as in many organisms it has a 50% cost (fitness disadvantage) in relation to asexual reproduction. Mating types (types of gametes, according to their compatibility) may have arisen as a result of anisogamy (gamete dimorphism), or the male and female genders may have evolved before anisogamy. It is also unknown why most sexual organisms use a binary mating system, and why some organisms have gamete dimorphism. Charles Darwin was the first to suggest that sexual selection drives speciation; without it, complex life would probably not have evolved.\n\nWhile life on Earth is regarded to have spawned relatively early in the planet's history, the evolution from multicellular to intelligent organisms took around 800 million years. Civilizations on Earth have existed for about 12,000 years and radio communication reaching space has existed for less than 100 years. Relative to the age of the Solar System (~4.57 Ga) this is a short time, in which extreme climatic variations, super volcanoes, and large meteorite impacts were absent. These events would severely harm intelligent life, as well as life in general. For example, the Permian-Triassic mass extinction, caused by widespread and continuous volcanic eruptions in an area the size of Western Europe, led to the extinction of 95% of known species around 251.2 Ma ago. About 65 million years ago, the Chicxulub impact at the Cretaceous–Paleogene boundary (~65.5 Ma) on the Yucatán peninsula in Mexico led to a mass extinction of the most advanced species at that time.\n\nIf there were intelligent extraterrestrial civilizations able to make contact with distant Earth, they would have to live in the same 12Ka period of the 800Ma evolution of life.\n\nThe following discussion is adapted from Cramer. The Rare Earth equation is Ward and Brownlee's riposte to the Drake equation. It calculates formula_1, the number of Earth-like planets in the Milky Way having complex life forms, as:\n\nwhere:\nWe assume formula_5. The Rare Earth hypothesis can then be viewed as asserting that the product of the other nine Rare Earth equation factors listed below, which are all fractions, is no greater than 10 and could plausibly be as small as 10. In the latter case, formula_1 could be as small as 0 or 1. Ward and Brownlee do not actually calculate the value of formula_1, because the numerical values of quite a few of the factors below can only be conjectured. They cannot be estimated simply because we have but one data point: the Earth, a rocky planet orbiting a G2 star in a quiet suburb of a large barred spiral galaxy, and the home of the only intelligent species we know, namely ourselves.\n\nThe Rare Earth equation, unlike the Drake equation, does not factor the probability that complex life evolves into intelligent life that discovers technology (Ward and Brownlee are not evolutionary biologists). Barrow and Tipler review the consensus among such biologists that the evolutionary path from primitive Cambrian chordates, e.g., \"Pikaia\" to \"Homo sapiens\", was a highly improbable event. For example, the large brains of humans have marked adaptive disadvantages, requiring as they do an expensive metabolism, a long gestation period, and a childhood lasting more than 25% of the average total life span. Other improbable features of humans include:\n\nWriters who support the Rare Earth hypothesis:\n\nCases against the Rare Earth Hypothesis take various forms.\n\nThe hypothesis concludes, more or less, that complex life is rare because it can evolve only on the surface of an Earth-like planet or on a suitable satellite of a planet. Some biologists, such as Jack Cohen, believe this assumption too restrictive and unimaginative; they see it as a form of circular reasoning.\n\nAccording to David Darling, the Rare Earth hypothesis is neither hypothesis nor prediction, but merely a description of how life arose on Earth. In his view Ward and Brownlee have done nothing more than select the factors that best suit their case.\n\nWhat matters is not whether there's anything unusual about the Earth; there's going to be something idiosyncratic about every planet in space. What matters is whether any of Earth's circumstances are not only unusual but also essential for complex life. So far we've seen nothing to suggest there is.\n\nCritics also argue that there is a link between the Rare Earth Hypothesis and the creationist ideas of intelligent design.\n\nAn increasing number of extrasolar planet discoveries are being made with planets in planetary systems known as of . Rare Earth proponents argue life cannot arise outside Sun-like systems. However, some exobiologists have suggested that stars outside this range may give rise to life under the right circumstances; this possibility is a central point of contention to the theory because these late-K and M category stars make up about 82% of all hydrogen-burning stars.\n\nCurrent technology limits the testing of important Rare Earth criteria: surface water, tectonic plates, a large moon and biosignatures are currently undetectable. Though planets the size of Earth are difficult to detect and classify, scientists now think that rocky planets are common around Sun-like stars. The Earth Similarity Index (ESI) of mass, radius and temperature provides a means of measurement, but falls short of the full Rare Earth criteria.\n\nSome argue that Rare Earth's estimates of rocky planets in habitable zones (formula_3 in the Rare Earth equation) are too restrictive. James Kasting cites the Titius-Bode law to contend that it is a misnomer to describe habitable zones as narrow when there is a 50% chance of at least one planet orbiting within one. In 2013 a study that was published in the journal Proceedings of the National Academy of Sciences calculated that about \"one in five\" of all sun-like stars are expected to have earthlike planets \"within the habitable zones of their stars\"; 8.8 billion of them therefore exist in the Milky Way galaxy alone. On 4 November 2013, astronomers reported, based on \"Kepler\" space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of sun-like stars and red dwarf stars within the Milky Way Galaxy. 11 billion of these estimated planets may be orbiting sun-like stars.\n\nThe requirement for a system to have a Jovian planet as protector (Rare Earth equation factor formula_15) has been challenged, affecting the number of proposed extinction events (Rare Earth equation factor formula_16). Kasting's 2001 review of Rare Earth questions whether a Jupiter protector has any bearing on the incidence of complex life. Computer modelling including the 2005 Nice model and 2007 Nice 2 model yield inconclusive results in relation to Jupiter's gravitational influence and impacts on the inner planets. A study by Horner and Jones (2008) using computer simulation found that while the total effect on all orbital bodies within the Solar System is unclear, Jupiter has caused more impacts on Earth than it has prevented. Lexell's Comet, a 1770 near miss that passed closer to Earth than any other comet in recorded history, was known to be caused by the gravitational influence of Jupiter. Grazier (2017) claims that the idea of Jupiter as a shield is a misinterpretation of a 1996 study by George Wetherill, and using computer models Grazier was able to demonstrate that Saturn protects Earth from more asteroids and comets than does Jupiter.\n\nWard and Brownlee argue that for complex life to evolve (Rare Earth equation factor formula_12), tectonics must be present to generate biogeochemical cycles, and predicted that such geological features would not be found outside of Earth, pointing to a lack of observable mountain ranges and subduction. There is, however, no scientific consensus on the evolution of plate tectonics on Earth. Though it is believed that tectonic motion first began around three billion years ago, by this time photosynthesis and oxygenation had already begun. Furthermore, recent studies point to plate tectonics as an episodic planetary phenomenon, and that life may evolve during periods of \"stagnant-lid\" rather than plate tectonic states.\n\nRecent evidence also points to similar activity either having occurred or continuing to occur elsewhere. The geology of Pluto, for example, described by Ward and Brownlee as \"without mountains or volcanoes ... devoid of volcanic activity\", has since been found to be quite the contrary, with a geologically active surface possessing organic molecules and mountain ranges like Tenzing Montes and Hillary Montes comparable in relative size to those of Earth, and observations suggest the involvement of endogenic processes. Plate tectonics has been suggested as a hypothesis for the Martian dichotomy, and in 2012 geologist An Yin put forward evidence for active plate tectonics on Mars. Europa has long been suspected to have plate tectonics and in 2014 NASA announced evidence of active subduction. In 2017, scientists studying the geology of Charon confirmed that icy plate tectonics also operated on Pluto's largest moon.\n\nKasting suggests that there is nothing unusual about the occurrence of plate tectonics in large rocky planets and liquid water on the surface as most should generate internal heat even without the assistance of radioactive elements. Studies by Valencia and Cowan suggest that plate tectonics may be inevitable for terrestrial planets Earth sized or larger, that is, Super-Earths, which are now known to be more common in planetary systems.\n\nThe hypothesis that molecular oxygen, necessary for animal life, is rare and that a Great Oxygenation Event (Rare Earth equation factor formula_12) could only have been triggered and sustained by tectonics, appears to have been invalidated by more recent discoveries.\n\nWard and Brownlee ask \"whether oxygenation, and hence the rise of animals, would ever have occurred on a world where there were no continents to erode\". Extraterrestrial free oxygen has recently been detected around other solid objects, including Mercury, Venus, Mars Jupiter's four Galilean moons, Saturn's moons Enceladus, Dione and Rhea and even the atmosphere of a comet. This has led scientists to speculate whether processes other than photosynthesis could be capable of generating an environment rich in free oxygen. Wordsworth (2014) concludes that oxygen generated other than through photodissociation may be likely on Earth-like exoplanets, and could actually lead to false positive detections of life. Narita (2015) suggests photocatalysis by titanium dioxide as a geochemical mechanism for producing oxygen atmospheres.\n\nSince Ward & Brownlee's assertion that \"there is irrefutable evidence that oxygen is a necessary ingredient for animal life\", anaerobic metazoa have been found that indeed do metabolise without oxygen. Spinoloricus nov. sp., for example, a species discovered in the hypersaline anoxic L'Atalante basin at the bottom of the Mediterranean Sea in 2010, appears to metabolise with hydrogen, lacking mitochondria and instead using hydrogenosomes. Stevenson (2015) has proposed other membrane alternatives for complex life in worlds without oxygen. In 2017, scientists from the NASA Astrobiology Institute discovered the necessary chemical preconditions for the formation of azotosomes on Saturn's moon Titan, a world that lacks atmospheric oxygen. Independent studies by Schirrmeister and by Mills concluded that Earth's multicellular life existed prior to the Great Oxygenation Event, not as a consequence of it.\n\nNASA scientists Hartman and McKay argue that plate tectonics may in fact slow the rise of oxygenation (and thus stymie complex life rather than promote it). Computer modelling by Tilman Spohn in 2014 found that plate tectonics on Earth may have arisen from the effects of complex life's emergence, rather than the other way around as the Rare Earth might suggest. The action of lichens on rock may have contributed to the formation of subduction zones in the presence of water. Kasting argues that if oxygenation caused the Cambrian explosion then any planet with oxygen producing photosynthesis should have complex life.\n\nThe importance of Earth's magnetic field to the development of complex life has been disputed. Kasting argues that the atmosphere provides sufficient protection against cosmic rays even during times of magnetic pole reversal and atmosphere loss by sputtering. Kasting also dismisses the role of the magnetic field in the evolution of eukaryotes, citing the age of the oldest known magnetofossils.\n\nThe requirement of a large moon (Rare Earth equation factor formula_14) has also been challenged. Even if it were required, such an occurrence may not be as unique as predicted by the Rare Earth Hypothesis. Recent work by Edward Belbruno and J. Richard Gott of Princeton University suggests that giant impactors such as those that may have formed the Moon can indeed form in planetary trojan points ( or Lagrangian point) which means that similar circumstances may occur in other planetary systems.\n\nRare Earth's assertion that the Moon's stabilization of Earth's obliquity and spin is a requirement for complex life has been questioned. Kasting argues that a moonless Earth would still possess habitats with climates suitable for complex life and questions whether the spin rate of a moonless Earth can be predicted. Although the giant impact theory posits that the impact forming the Moon increased Earth's rotational speed to make a day about 5 hours long, the Moon has slowly \"stolen\" much of this speed to reduce Earth's solar day since then to about 24 hours and continues to do so: in 100 million years Earth's solar day will be roughly 24 hours 38 minutes (the same as Mars's solar day); in 1 billion years, 30 hours 23 minutes. Larger secondary bodies would exert proportionally larger tidal forces that would in turn decelerate their primaries faster and potentially increase the solar day of a planet in all other respects like Earth to over 120 hours within a few billion years. This long solar day would make effective heat dissipation for organisms in the tropics and subtropics extremely difficult in a similar manner to tidal locking to a red dwarf star. Short days (high rotation speed) causes high wind speeds at ground level. Long days (slow rotation speed) cause the day and night temperatures to be too extreme.\n\nMany Rare Earth proponents argue that the Earth's plate tectonics would probably not exist if not for the tidal forces of the Moon. The hypothesis that the Moon's tidal influence initiated or sustained Earth's plate tectonics remains unproven, though at least one study implies a temporal correlation to the formation of the Moon. Evidence for the past existence of plate tectonics on planets like Mars which may never have had a large moon would counter this argument. Kasting argues that a large moon is not required to initiate plate tectonics.\n\nRare Earth proponents argue that simple life may be common, though complex life requires specific environmental conditions to arise. Critics consider life could arise on a moon of a gas giant, though this is less likely if life requires volcanicity. The moon must have stresses to induce tidal heating, but not so dramatic as seen on Jupiter's Io. However, the moon is within the gas giant's intense radiation belts, sterilizing any biodiversity before it can get established. Dirk Schulze-Makuch disputes this, hypothesizing alternative biochemistries for alien life. While Rare Earth proponents argue that only microbial extremophiles could exist in subsurface habitats beyond Earth, some argue that complex life can also arise in these environments. Examples of extremophile animals such as the \"Hesiocaeca methanicola\", an animal that inhabits ocean floor methane clathrates substances more commonly found in the outer Solar System, the tardigrades which can survive in the vacuum of space or \"Halicephalobus mephisto\" which exists in crushing pressure, scorching temperatures and extremely low oxygen levels 3.6 kilometres deep in the Earth's crust, are sometimes cited by critics as complex life capable of thriving in \"alien\" environments. Jill Tarter counters the classic counterargument that these species adapted to these environments rather than arose in them, by suggesting that we cannot assume conditions for life to emerge which are not actually known. There are suggestions that complex life could arise in sub-surface conditions which may be similar to those where life may have arisen on Earth, such as the tidally heated subsurfaces of Europa or Enceladus. Ancient circumvental ecosystems such as these support complex life on Earth such as Riftia pachyptila that exist completely independent of the surface biosphere.\n\n\n"}
{"id": "15822723", "url": "https://en.wikipedia.org/wiki?curid=15822723", "title": "Relic woods", "text": "Relic woods\n\nRelic woods () is a natural monument (Protected areas of Ulyanovsk Oblast).\n\nContains two areas:\n1.22 metres high vegetation, consists of linden (near 90%) and birch (near 10%) trees. The store of woodpulp are 310 cube metres at 1 hectare. Square: 48,8 hectares.\n2.23 metres high vegetation, consists of linden (near 90%) and birch (near 10%) trees. The store of woodpulp are 380 cube metres at 1 hectare. Square: 11,3 hectares.\nAnother rocks: maple, filbert.\nAll manage works are prohibited with the aim to save value vegetation. Researching and scientific work is not conducing.\n\ninterest represent radical lime woods age of 80–100 years.\n\n"}
{"id": "4968799", "url": "https://en.wikipedia.org/wiki?curid=4968799", "title": "Sky brightness", "text": "Sky brightness\n\nSky brightness refers to the visual perception of the sky and how it scatters and diffuses light. The fact that the sky is not completely dark at night is easily visible. If light sources (e.g. the Moon and light pollution) were removed from the night sky, it would appear absolutely dark. Silhouettes of objects against the sky itself would not be visible.\n\nThe sky's brightness varies greatly over the day, and the primary cause differs as well. During daytime, when the Sun is above the horizon, the direct scattering of sunlight is the overwhelmingly dominant source of light. During twilight (the duration after sunset or before sunrise until or since, respectively, the full darkness of night), the situation is more complicated, and a further differentiation is required.\n\nTwilight (both dusk and dawn) is divided into three 6° segments that mark the Sun's position below the horizon. At civil twilight, the center of the Sun's disk appears to be between 1/4° and 6° below the horizon. At nautical twilight, the Sun's altitude is between –6° and –12°. At astronomical twilight, the Sun is between –12° and –18°. When the Sun's depth is more than 18°, the sky generally attains its maximum darkness.\n\nSources of the night sky's intrinsic brightness include airglow, indirect scattering of sunlight, scattering of starlight, and light pollution.\n\nWhen physicist Anders Ångström examined the spectrum of the aurora borealis, he discovered that even on nights when the aurora was absent, its characteristic green line was still present. It was not until the 1920s that scientists were beginning to identify and understand the emission lines in aurorae and of the sky itself, and what was causing them. The green line Angstrom observed is in fact an emission line with a wavelength of 557.7 nm, caused by the recombination of oxygen in the upper atmosphere.\n\nAirglow is the collective name of the various processes in the upper atmosphere that result in the emission of photons, with the driving force being primarily UV radiation from the Sun. Several emission lines are dominant: a green line from oxygen at 557.7 nm, a yellow doublet from sodium at 589.0 and 589.6 nm, and red lines from oxygen at 630.0 and 636.4 nm.\n\nThe sodium emissions come from a thin sodium layer approximately 10 km thick at an altitude of 90–100 km, above the mesopause and in the D-layer of the ionosphere. The red oxygen lines originate at altitudes of about 300 km, in the F-layer. The green oxygen emissions are more spatially distributed. How sodium gets to mesospheric heights is not yet well understood, but it is believed to be a combination of upward transport of sea salt and meteoritic dust.\n\nIn daytime, sodium and red oxygen emissions are dominant and roughly 1,000 times as bright as nighttime emissions because in daytime, the upper atmosphere is fully exposed to solar UV radiation. The effect is however not noticeable to the human eye, since the glare of directly scattered sunlight outshines and obscures it.\n\nIndirectly scattered sunlight comes from two directions. From the atmosphere itself, and from outer space. In the first case, the sun has just set but still illuminates the upper atmosphere directly. Because the amount of scattered sunlight is proportional to the number of scatterers (i.e. air molecules) in the line of sight, the intensity of this light decreases rapidly as the sun drops further below the horizon and illuminates less of the atmosphere.\n\nWhen the sun's altitude is < -6° 99% of the atmosphere in zenith is in the Earth's shadow and second order scattering takes over. At the horizon, however, 35% of the atmosphere along the line of sight is still directly illuminated, and continues to be until the sun reaches -12°. From -12° to -18° only the uppermost parts of the atmosphere along the horizon, directly above the spot where the sun is, is still illuminated. After that, all direct illumination ceases and astronomical darkness sets in.\n\nA second source sunlight is the zodiacal light, which is caused by reflection and scattering of sunlight on interplanetary dust. Zodiacal light varies quite a lot in intensity depending on the position of the earth, location of the observer, time of year, and composition and distribution of the reflecting dust.\n\nNot only sunlight is scattered by the molecules in the air. Starlight and the diffuse light of the milky way are also scattered by the air, and it is found that stars up to V magnitude 16 contribute to the diffuse scattered starlight.\n\nOther sources such as galaxies and nebulae don't contribute significantly.\n\nThe total brightness of all the stars was first measured by Burns in 1899, with a calculated result that the total brightness reaching earth was equivalent to that of 2,000 first-magnitude stars with subsequent measurements by others.\n\nLight pollution is an ever-increasing source of sky brightness in urbanized areas. In densely populated areas that do not have stringent light pollution control, the entire night sky is regularly 5 to 50 times brighter than it would be if all lights were switched off, and very often the influence of light pollution is far greater than natural sources (including moonlight). With urbanization and light pollution, one third of humanity, and the majority of those in developed countries, cannot see the Milky Way.\n\nWhen the sun has just set, the brightness of the sky decreases rapidly, thereby enabling us to see the airglow that is caused from such high altitudes that they are still fully sunlit until the sun drops more than about 12° below the horizon. During this time, yellow emissions from the sodium layer and red emissions from the 630 nm oxygen lines are dominant, and contribute to the purplish color sometimes seen during civil and nautical twilight.\n\nAfter the sun has also set for these altitudes at the end of nautical twilight, the intensity of light emanating from earlier mentioned lines decreases, until the oxygen-green remains as the dominant source.\n\nWhen astronomical darkness has set in, the green 557.7 nm oxygen line is dominant, and atmospheric scattering of starlight occurs.\n\nDifferential refraction causes different parts of the spectrum to dominate, producing a golden hour and a blue hour.\n\nThe following table gives the relative and absolute contributions to night sky brightness at zenith on a perfectly dark night at middle latitudes without moonlight and in the absence of any light pollution.\n\nThe total sky brightness in zenith is therefore ~220 S or 21.9 mag/arcsec² in the V-band. Note that the contributions from Airglow and Zodiacal light vary with the time of year, the solar cycle, and the observer's latitude roughly as follows:\n\nwhere \"S\" is the solar 10.7 cm flux in MJy, and various sinusoidally between 0.8 and 2.0 with the 11-year solar cycle, yielding an upper contribution of ~270 S at solar maximum.\n\nThe intensity of zodiacal light depends on the ecliptic latitude and longitude of the point in the sky being observed relative to that of the sun. At ecliptic longitudes differing from the sun's by > 90 degrees, the relation is \nwhere \"β\" is the ecliptic latitude and is smaller than 60°, when larger than 60 degrees the contribution is that given in the table. Along the ecliptic plane there are enhancements in the zodiacal light where it is much brighter near the sun and with a secondary maximum opposite the sun at 180 degrees longitude (the gegenschein).\n\nIn extreme cases natural zenith sky brightness can be as high as ~21.0 mag/arcsec², roughly twice as bright as nominal conditions.\n\n"}
{"id": "49021319", "url": "https://en.wikipedia.org/wiki?curid=49021319", "title": "TALE-likes", "text": "TALE-likes\n\nTranscription Activator Like Effector Likes (TALE-likes) are a group of bacterial DNA binding proteins named for the first and still best studied group, the TALEs of \"Xanthomonas\" bacteria. TALEs are important factors in the plant diseases caused by \"Xanthomonas\" bacteria, but are known primarily for their role in biotechnology as programmable DNA binding proteins, particularly in the context of TALE nucleases. TALE-likes have additionally been found in many strains of the \"Ralstonia solanacearum\" bacterial species complex, in \"Burkholderia rhizoxinica\" strain HKI 454, and in two unknown marine bacteria. Whether or not all these proteins from a single phylogenetic grouping is as yet unclear.\n\nThe unifying feature of the TALE-likes are their tandem arrays of DNA binding repeats. These repeats are, with few exceptions, 33-35 amino acids in length, and composed of two alpha-helices on either side of a flexible loop containing the DNA base binding residues and with neighbouring repeats joined by flexible linker loops. Evidence for this common structure comes in part from solved crystal structures of TALEs and a \"Burkholderia\" TALE-like, but also from the conservation of the code that all TALE-likes use to recognise DNA-sequences.\n\nTALEs are the first identified, best-studied and largest group within the TALE-likes. TALEs are found throughout the bacterial genus \"Xanthomonas\", comprising mostly plant pathogens. Those TALEs which have been studied have all been shown to be secreted as part of the Type III secretion system into host plant cells. Once inside the host cell they translocate to the nucleus, bind specific DNA sequences within host promoters and turn on downstream genes. Every part of this process is thought to be conserved across all TALEs. The single meaningful difference between individual TALEs, based on current understanding, is the specific DNA sequence that each TALE binds. TALEs from even closely related strains differ in the composition of repeats that make up their DNA binding domain. Repeat composition determines DNA binding preference. In particular position 13 of each repeat confers the DNA base preference of each repeat. During early research it was noted that almost all the differences between repeats of a single TALE repeat array are found in positions 12 and 13 and this finding led to the hypothesis that these residues determine base preference. In fact repeat positions 12 and 13, referred to jointly as the Repeat Variable Diresidue (RVD) are commonly said to confer base specificity despite clear evidence that position 13 is the base determining residue. In addition to the repeat domain TALEs also possess a number of conserved features in the domains flanking the repeats. These include domains for type-III-secretion, nuclear localization and transcriptional activation. This allows TALEs to carry out their biological role as effector proteins secreted into host plant cells to activate expression of specific host genes.\n\nDiversity and evolution\n\nWhilst the RVD positions are commonly the only variable positions within a single TALE repeat array it should be noted that there are more differences when comparing repeat arrays of different TALEs. The diversity of TALEs across the Xanthomonas genus is considerable, but a particularly striking finding is that the evolutionary history one arrives at by comparing repeat compositions differs from that found when comparing non-repeat sequences. Repeat arrays of TALEs are thought to evolve rapidly, with a number of recombinatorial processes suggested to shape repeat array evolution. Recombination of TALE repeat arrays has been demonstrated in a forced-selection experiment. This evolutionary dynamism is though to be made possible by the very high sequence identity of TALE repeats, which is a unique feature of TALEs as opposed to other TALE-likes.\n\nT-zero\n\nAnother unique feature of TALEs is a set of four repeat structures at the N-terminal flank of the core repeat array. These structures, termed non-canonical or degenerate repeats have been shown to be vital for DNA binding, though all but one do not contact DNA bases and thus make no contribution to sequence preference. The one exception is repeat -1, which encodes a fixed T-zero preference to all TALEs. This means that the target sequences of TALEs are always preceded by a thymine base. This is thought to be common to all TALEs, with the possible exception of TalC from \"Xanthomonas oryzae pv. oryzae\" strain AXO1947.\n\nDiscovery and molecular properties\n\nIt was noted in the 2002 publication of the genome of reference strain \"Ralstonia solanacearum\" GMI1000 that its genome encodes a protein similar to \"Xanthomonas\" TALEs. Based on similar domain structure and repeat sequences it was presumed that this gene and homologs in other \"Ralstonia\" strains would encode proteins with the same molecular properties as TALEs, including sequence-specific DNA binding. In 2013 this was confirmed by two studies. These genes and the proteins they encode are referred to as RipTALs (Ralstonia injected protein TALE-like) in line with the standard nomenclature of Ralstonia effectors. Whilst the DNA binding code of the core repeats is conserved with TALEs, RipTALs do not share the T-zero preference, instead they have a strict G-zero requirement. In addition repeats within a single RipTAL repeat array have multiple sequence differences beyond the RVD positions, unlike the near-identical repeats of TALEs.\n\nBiological role\n\nSeveral lines of evidence support the idea that RipTALs function as effector proteins, promoting bacterial growth or disease by manipulating the expression of plant genes. They are secreted into plant cells by the Type III secretion system, which is the main delivery system for effector proteins. They are able to function as sequence-specific transcription factors in plant cells. In addition a strain lacking its RipTAL was shown to grow slower inside eggplant leaf tissue than the wild type. Furthermore, a study based on DNA polymorphisms in \"ripTAL\" repeat domain sequences and host plants found a statistically significant connection between host plant and repeat domain variants. This is expected if the RipTALs of different strains are adapted to target genes in specific host plants. Despite this to date no target genes have been identified for any RipTAL.\n\nDiscovery\n\nThe publication of the genome of bacterial strain \"Bukrholderia rhizoxinica\" HKI 454, in 2011 led to the discovery of a set of TALE-like genes that differed considerably in nature from the TALEs and RipTALS. The proteins encoded by these genes were studied for their DNA binding properties by two groups independently and named the Bats (Burkholderia TALE-likes ) or BurrH. This research showed that the repeat units of the \"Burkholderia\" TALE-likes bind DNA with the same code as TALEs, governed by position 13 of each repeat. There are, however, a number of differences.\n\nBiological role\n\n\"Burkholderia\" TALE-likes are composed almost entirely of repeats, lacking the large non-repetitive domains found flanking the repeats in TALEs and RpTALs. Those domains are key to the functions of TALEs and RipTALs allowing them to infiltrate the plant nucleus and turn on gene expression. It is therefore currently unclear what the biological roles of \"Burkholderia\" TALE-likes are. What is clear is that they are not effector proteins secreted into plant cells to act as transcription factors, the biological role of TALEs and RipTALs. It is not unexpected that they may differ in biological roles from TALEs and RipTALs since the life style of the bacterium they derive from is very unlike that of TALE and RipTAL bearing bacteria. \"B. rhizoxinica\" is an endosymbiont, living inside a fungus, \"Rhizopus microsporus\", a plant pathogen. The same fungus is also an opportunistic human pathogen in immuno-compromised patients, but whereas \"B. rhizoxinica\" is necessary for pathogenicity on plant hosts it is irrelevant to human infection. It is unclear whether the \"Burkholderia\" TALE-likes are ever secreted either into the fungus, let alone into host plants.\nUses in Biotechnology\n\nAs noted in the publications on \"Burkholderia\" TALE-likes there may be some advantages to using these proteins as a scaffold for programmable DNA-binding proteins to function as transcription factors or designer-nucleases, compared to TALEs. These advantages are a shorter repeat size, more compact domain structure (no large non-repeat domains), greater repeat sequence diversity enabling the use of PCR on the genes encoding them and making them less vulnerable to recombinatorial repeat loss. In addition Burkholderia TALE-likes have no T-zero requirement relaxing the constraints on DNA target selection. However, to uses of Burkholderia TALE-likes as programmable DNA binding proteins have been published, outside of the original characterization publications.\n\nDiscovery\n\nIn 2007 the results of a sweep of the world's oceans by the J. Craig Venter Institute were made publicly available. The paper in 2014 on \"Burkholderia\" TALE-likes was also the first to report that two entries from that database resembled TALE-likes, based on sequence similarity. These were further characterized and assessed for their DNA-binding potential in 2015. The repeat units encoded by these sequences were found to mediate DNA binding with base preference matching the TALE code, and judged likely to form structures nearly identical to Bat1 repeats based on molecular dynamics simulations. The proteins encoded by these DNA sequences were therefore designated Marine Organism TALE-likes (MOrTLs) 1 and 2.\n\nEvolutionary relationship to other TALE-likes\n\nWhilst repeats of MOrTL1 and 2 both conform structurally and functionally to the TALE-like norm, they differ considerably at the sequence level both from all other TALE-likes and from one another. It is not known whether they are truly homologous to the other TALE-likes, and thus constitute together with the TALEs, RipTALs and Bats a true protein-family. Alternatively they may have evolved independently. It is particularly difficult to judge the relationship to the other TALE-likes because almost nothing is known of the organisms that MOrTL1 and MOrTL2 come from. It is known only that they were found in two separate sea-water samples from the Gulf of Mexico and are likely to be bacteria based on size-exclusion before DNA sequencing.\n"}
{"id": "23083720", "url": "https://en.wikipedia.org/wiki?curid=23083720", "title": "The Evolution of God", "text": "The Evolution of God\n\nThe Evolution of God is a 2009 book by Robert Wright, in which the author explores the history of the concept of God in the three Abrahamic religions through a variety of means, including archeology, history, theology, and evolutionary psychology. The patterns which link Judaism, Christianity, and Islam and the ways in which they have changed their concepts over time are explored as one of the central themes.\n\nOne of the conclusions of the book that Wright tries to make is a reconciliation between science and religion. He also speculates on the future of the concept of God.\n\nAmong other things, Wright discusses the role of evolutionary biology in the development of religion. Geneticist Dean Hamer hypothesized that some people have a specific gene that makes them prone to religious belief, which he calls the God gene, and that over time natural selection has favored these people because their spirituality leads to optimism. Wright, however, thinks the tendency towards religious belief is not an adaptive trait influenced by natural selection, but rather a spandrel - a trait that happens to be supported by adaptations originally selected for other purposes. Wright states that the human brain approaches religious belief based on how it adapted to survive and reproduce in early hunter-gatherer societies.\n\nHe points out four key traits of religion that align with the human brain's survival adaptations:\nHumans have adapted to pay attention to surprising and confusing information, because it could make the difference between life and death. (For instance, if a person left the campsite and mysteriously never returned, it would be wise for the others to be on guard for a predator or some other danger.) Understanding and controlling cause and effect also takes top priority in the human brain, since humans live in complex social groups where predicting and influencing the actions and thoughts of others gains them allies, status, and access to resources. As human cognitive abilities and curiosity expanded over the centuries, their investigation of cause and effect expanded from the strictly social context out into the world at large, opening the doors for religions to explain things like weather and disease.\n\nThough some of these explanations were strange and perhaps dubious, the fact that they could not be completely disproven lent them credibility; it was better to be cautious than dead. Wright uses an example from the Haida people, indigenous to the northwest coast of North America, who would try to appease killer whale deities to calm storms out at sea; they would pour fresh water into the ocean or tie tobacco or deer tallow to the end of a paddle. While some people certainly died despite these offerings, those who survived were a testament to the ritual's possible efficacy.\n\nMysterious and unproven beliefs can also persist in a culture because human brains have adapted to agree with the group consensus even if it goes against one's better judgment or personal beliefs, since a person alienated from the group loses protection, food, and mates. Wright cites the Asch conformity experiments and even posits that Stockholm syndrome is not so much a syndrome as a natural product of evolution, the brain's way of ensuring that a person accepts and is accepted by his or her new social group. In addition, beliefs can persist because once a person publicly announces a belief, social psychologists have found that he or she is inclined to focus on evidence supporting that belief while conveniently ignoring evidence contradicting it, a logical fallacy known as cherry picking.\n\nJournalist and political commentator Andrew Sullivan gave the book a positive review in \"The Atlantic\", saying that the book \"...gave me hope that we can avoid both the barrenness of a world without God and the horrible fusion of fundamentalism and weapons of mass destruction.\" \n\n\"Newsweek\" religion editor, Lisa Miller, described \"The Evolution of God\" as a reframing of the faith vs. reason debate. Drawing a contrast to such authors as Sam Harris, Richard Dawkins and Christopher Hitchens, Miller gives an overall positive review of the book's approach to the examination of the concept of God.\n\nIn a review for \"The New York Times\", Yale professor of psychology Paul Bloom said, \"In his brilliant new book, “The Evolution of God,” Robert Wright tells the story of how God grew up.\" Bloom sums up Wright's controversial stance as, \"Wright’s tone is reasoned and careful, even hesitant, throughout, and it is nice to read about issues like the morality of Christ and the meaning of jihad without getting the feeling that you are being shouted at. His views, though, are provocative and controversial. There is something here to annoy almost everyone.\"\n\nHowever, in a \"New York Times\" review that included a reply from Wright, Nicholas Wade, a writer for the \"Science Times\" section, notes the book is \"a disappointment from the Darwinian perspective\", because evolution \"provides a simpler explanation for moral progression than the deity Wright half invokes.\" Wright replied to Wade's comments, saying Wade had misunderstood Wright's argument and that \"The deity (if there is one–and I’m agnostic on that point) would be realizing moral progress through evolution’s creation of the human moral sense (and through the subsequent development of that moral sense via cultural evolution, particularly technological evolution).\" Wade replied that \"evolution seems to me a sufficient explanation for the moral progress that Mr. Wright correctly discerns in the human condition, so there seemed no compelling need to invoke a deity.\"\n\nTo promote the book, Wright did a variety of interviews, including with the \"New York Times\", \"Publishers Weekly\", and \"Bill Moyers Journal\".\nHe also did a series of videos on Bloggingheads.tv, a website he co-founded with Mickey Kaus. Wright also appeared on \"The Colbert Report\" on August 18, 2009.\n\n\n"}
{"id": "18789195", "url": "https://en.wikipedia.org/wiki?curid=18789195", "title": "The Seven Pillars of Life", "text": "The Seven Pillars of Life\n\nThe Seven Pillars of Life are the essential principles of life described by Daniel E. Koshland in 2002 in order to create a universal definition of life. One stated goal of this universal definition is to aid in understanding and identifying artificial and extraterrestrial life. The seven pillars are Program, Improvisation, Compartmentalization, Energy, Regeneration, Adaptability, and Seclusion. These can be abbreviated as PICERAS.\n\nKoshland defines \"Program\" as an \"organized plan that describes both the ingredients themselves and the kinetics of the interactions among ingredients as the living system persists through time.\" In natural life as it is known on Earth, the program operates through the mechanisms of nucleic acids and amino acids, but the concept of program can apply to other imagined or undiscovered mechanisms.\n\n\"Improvisation\" refers to the living system's ability to change its program in response to the larger environment in which it exists. An example of improvisation on earth is natural selection.\n\n\"Compartmentalization\" refers to the separation of spaces in the living system that allow for separate environments for necessary chemical processes. Compartmentalization is necessary to protect the concentration of the ingredients for a reaction from outside environments.\n\nBecause living systems involve net movement in terms of chemical movement or body movement, and lose energy in those movements through entropy, energy is required for a living system to exist. The main source of energy on Earth is the sun, but other sources of energy exist for life on Earth, such as hydrogen gas or methane, used in chemosynthesis.\n\n\"Regeneration\" in a living system refers to the general compensation for losses and degradation in the various components and processes in the system. This covers the thermodynamic loss in chemical reactions, the wear and tear of larger parts, and the larger decline of components of the system in ageing. Living systems replace these losses by importing molecules from the outside environment, synthesizing new molecules and components, or creating new generations to start the system over again.\n\n\"Adaptability\" is the ability of a living system to respond to needs, dangers, or changes. It is distinguished from improvisation because the response is timely and does not involve a change of the program. Adaptability occurs from a molecular level to a behavioral level through feedback and feedforward systems. For example, an animal seeing a predator will respond to the danger with hormonal changes and escape behavior.\n\nY. N. Zhuravlev and V. A. Avetisov have analyzed Koshland's seven pillars from the context of primordial life and, though calling the concept \"elegant,\" point out that the pillars of compartmentalization, program, and seclusion don't apply well to the non-differentiated earliest life.\n\n"}
{"id": "44244083", "url": "https://en.wikipedia.org/wiki?curid=44244083", "title": "Time and Eternity (philosophy book)", "text": "Time and Eternity (philosophy book)\n\nTime and Eternity - An Essay on the Philosophy of Religion (1st imp. Princeton New Jersey 1952, Princeton University Press, 169 pp) is a philosophy book written by Walter Terence Stace. At the time of writing, Stace was a professor of philosophy at Princeton University, where he had worked since 1932 after a 22-year career in the Ceylon Civil Service. \"Time and Eternity\" was one of his first books about the philosophy of religion and mysticism, after writing throughout most of the 1930s and 1940s that was influenced by phenomenalist philosophy.\n\nIn his introduction Stace writes that \"Time and Eternity\" is an attempt to set out the fundamental nature of religion, and to deal with the conflict between religion and naturalism. He explains that the basic idea set out in the book is that all religious thought is symbolic, and that his influences include Rudolf Otto, especially his \"Mysticism East and West\", and Immanuel Kant. He says he was motivated to write the book in an attempt to add to the \"other half of the truth which I now think naturalism [as espoused in his 1947 essay \"Man Against Darkness\"] misses\".\n\nThe book begins by looking at religion, specifically God as non-being and as being, put by Stace as the negative and positive divine. Stace then defines two orders of being - time and eternity, which he says intersect in the moment of mystic illumination. He goes on to say that the nature of God or eternity is such that all religious language is symbolic and that it is necessarily subject to contradictions.\n\nThe first chapter asks what religion is, stating that religious thought is contradictory, is rooted in intuition, and that God is fundamentally a mystery. The second and third chapters look at the negative divine - the characterisation of God as void, silence or non-being - which Stace maintains is an idea found in all religions. He maintains that mystical experience is shared by all mankind, it is only the theories about it that differ. On this point he says he is in agreement with Otto. In this experience the distinction between subject and object is overcome, indeed there is no difference between the experiencer and the experience.\n\nStace then goes on to explain that all religions say that religious revelation is ineffable, because no words or concepts can be applied to God who is without qualities or predicates. Thus, God cannot be comprehended by the intellect, but is apprehended by intuition. \"... it is of the very nature of intellect to involve the subject-object opposition. But in the mystic experience this opposition is transcended. Therefore the intellect is incapable of understanding it. Therefore it is incomprehensible, ineffable.\"\n\nStace then looks at the positive divine; he asks how concepts can be applied to that which is above all concepts and finds that all propositions about God are symbolical. He defines religious and non-religious symbolism as differing in two respects. Firstly, religious symbols cannot be translated into logical propositions because they refer to an (ineffable) experience rather than a proposition. Secondly, the relationship between the religious symbol and what is symbolised is one of evocation rather than \"meaning\", as meaning is a concept, which is absent in the mystical experience. \"Yet in some way this symbolic language evokes in us some glimpse, some hint, seen dimly through the mists and fogs which envelop us, of that being who stands above all human thought and conception.\" He goes on to write that some of these symbols feel more appropriate than others (e.g. God is love not hate).\n\nNext Stace explains that there are two orders of being: time (or the world) and eternity (or God), and these intersect in the moment of mystic illumination. He maintains these orders are distinct, so one order cannot dictate to the other. Here he says that he agrees with Kant, who made a distinction between the world of phenomena and the noumenon, although he is critical of Kant’s disregard for mystical experience.\n\nLooking at symbolism in religion, Stace states that there are two types of predicates applied to God: first, the ethically-neutral sort, such as God being mind, power or personhood. Secondly, the ethical kind, where he is love, mercy, or righteousness. He explains that the former qualities are justified by an appeal to a hierarchy of being, and the latter to a hierarchy of value. In both cases the more adequate symbol are those that are higher in each hierarchy. In rooting symbolism in hierarchies, Stace explicitly states he is in opposition to Otto who thought religious symbolism was based on analogy between the numen and qualities found in the natural world.\n\nStace next looks at religion’s claims to truth. He draws an analogy between mystical illumination and aesthetic truth, as the truths of both rest on revelation rather than reason. \"Either you directly perceive beauty, or you do not. And either you directly perceive God in intuition, or you do not.\" Further, he maintains the arguments of both mystics and naturalists in denying each other’s positions are invalid, as they concern different realities.\n\nThese separate spheres lead Stace to reflect on both proofs for God and acosmism. He writes that proofs and disproofs for God are equally false, as God is only accessible by intuition and not logic. \"… the production by philosophers of proofs of the unreality of space, time, and the temporal world generally, is a direct result of their mistaking of their mystical propositions for factual propositions.\" Further, proofs of God actually harm religion as they make him a part of the natural order - a point on which he says that he agrees with Kant. Conversely acosmism (the denial of the reality of the world) has its root in the mystical moment, within which there is no other truth, God is the supreme reality and there is no naturalistic world. However this is a symbolic truth, rather than a statement of fact. Its counterpart in naturalism is atheism, which denies the reality of God.\n\nIn the final chapter Stace looks at mysticism and logic. He returns to the idea that theology and mystical philosophies (he gives the examples of Vedanta, Spinoza, Hegel, and Bradley) will always contain contradictions. Known as the doctrine of the Mystery of God, he maintains this is because the intellect is inherently incapable of understanding the Ultimate. All attempts to state the nature of the ultimate necessarily produce contradictions.\n\nVirgil C Aldrich reviewed the book alongside \"Religion and the Modern Mind\" and \"The Gate of Silence\", also by Stace and published in 1952. He points out that all three books mark a new direction for Stace who was previously best known as an empiricist and naturalist. For Aldrich this new intellectual interest results in a sharp dualism in both Stace’s personality and his thought. However, he writes that fortunately Stace’s philosophical background prevents him from supposing that scientific empiricism can confirm religious experience, indeed his religious philosophy is the sort “that a Hume or a Kant can consort with.” Aldrich argues that Stace’s intellectual sophistication is most evident in his ideas about the negative divine, but his thought is liable to all the standard objections where he proposes notions of the positive divine and religious intuition. Specifically, the notion that religious language is evocative of the mystical experience is problematic, because it is difficult to determine what language is adequate without resorting to literal or abstract ideas. Rudolf Otto’s notion of analogy, rejected by Stace, is more robust. Aldrich points out a contradiction in Stace’s reliance on hierarchies of being and values to more adequately refer to God, as this implies continuity between the world and eternity, which Stace denies.\n\nJulius Seelye Bixler reviewed the book twice, in 1952 and 1953. In his first review he wrote that he believed Stace was trying to have his cake and eat it with regards to the truth of both naturalism and mysticism. Bixler also wonders whether the revelation of God can really be free of concepts and thus whether time and eternity are utterly unrelated as Stace maintains. He identifies points in Stace’s thought where there is continuity between these two states and mystical language does appear to refer to concepts. Finally he rejects the book’s analogy of mystical experience to the evocative power of art, maintaining that art must be somewhat related to logic. Nonetheless, Bixler does concur that the book is a fascinating confessio fidei and personal statement. A year later, he reviewed \"Time and Eternity\" alongside \"Religion and the Modern Mind\". As well as reiterating the points he had made earlier, Bixler judges the second book more favourably and recommends reading the two together to better understand the problems they address.\n\nStace was praised for his clarity and ambitious aims in \"Time and Eternity\" by Abraham Kaplan who believed the book was one of the best on the subject for many many years. He pointed out that the book’s distinction between the orders of time and eternity owed much to Kant (which Stace himself acknowledged). Kaplan reflected that it was the book’s emphasis placed on mysticism and a universal religious intuition that would be of particular interest to students of “Oriental and comparative philosophy”. The central idea upon which Stace’s thought stands or falls, for Kaplan, is that religious language is evocative rather than descriptive. In this both religionists and naturalists will find problems. For the former, Stace can only account for the appropriateness of religious language by relying on ‘nearness’ to the divine rather than on resemblance, and this relies on ‘a vague panpsychism’ and levels of being in the manner of Samuel Alexander. While for the naturalist, Stace’s system of religious symbolism is doomed to remain mysterious, because it does not allow religious metaphors to be translated literally and neither can it be said how they evoke the experience to which they refer.\n\nAlso noting the unachievable ambition of solving the conflict between naturalism and religion, Martin A Greenman, remarks that one must come to the book “with a certain mood”. Too critical a mood would blind the reader to its religious insights, while the sensitivity and depth of its philosophic insights would be lost if one were to approach it in a too enthusiastically religious mood. Greenman finishes by justifying Stace’s philosophy to logical positivists by quoting from Wittgenstein's \"Tractatus\": “My propositions are elucidatory in this way: he who understands me finally recognizes them as senseless, when he has climbed out through them, on them, over them….He must surmount these propositions: then he sees the world rightly” (6. 54.) Dorothy M. Emmet found issue with the notion that the mystical experience is the point of intersection between the temporal and eternal orders. She writes that there are difficulties in Stace defining these orders as two distinct “orders of being”, rather than just as a way of speaking, because this then means some statements about the temporal order are relevant to what is said about the eternal order and vice versa. Indeed, the interrelation between these two orders is difficult to maintain. She also questioned Stace’s characterisation of mystical consciousness as being the same everywhere.\n\nMore recently, Maurice Friedman writes about the book in the context of the various attempts to find a universal essence - or perennial philosophy - within religion. He finds that \"Time and Eternity\" is a more systematic attempt at this than those proposed by Aldous Huxley or Ananda Coomaraswamy, but no more successful. For Friedman, the philosophy that Stace lays out in the book is derived from metaphysical speculation (that, like the ideas of Huxley and Coomaraswamy, is influenced by Vedanta), rather than mystical experience. Central to Friedman’s critique is the notion that there is a vast gulf between the mystical experience which Stace defines as beyond thought, and his philosophical system built on this. He also mentions that mystics do not always agree on what experiences, symbols and philosophies are the closest to the divine.\n\nThe book has received more positive support however. Robert C Neville called \"Time and Eternity\" “the most sophisticated treatment of eternity and time in our century so far”. In his \"Thought: A Very Short Introduction\", Tim Bayne says the book contains a “classic” discussion of ineffability. American writer Arthur Goldwag has said that the phrase \"that than which there is no other\" that he encountered in \"Time and Eternity\" was one of a number of factors that contributed to him giving up praying.\n\n"}
{"id": "40624269", "url": "https://en.wikipedia.org/wiki?curid=40624269", "title": "Trabecular cartilage", "text": "Trabecular cartilage\n\nTrabecular cartilages (trabeculae cranii, sometimes simply trabeculae, prechordal cartilages) are paired, rod-shaped cartilages, which develop in the head of the vertebrate embryo. They are the primordia of the anterior part of the cranial base, and are derived from the cranial neural crest cells.\n\nThe trabecular cartilages generally appear as a paired, rod-shaped cartilages at the ventral side of the forebrain and lateral side of the adenohypophysis in the vertebrate embryo. During development, their anterior ends fuse and form the \"trabecula communis\". Their posterior ends fuse with the caudal-most parachordal cartilages.\n\nMost skeletons are of mesodermal origin in vertebrates. Especially axial skeletal elements, such as the vertebrae, are derived from the paraxial mesoderm (e.g., somites), which is regulated by molecular signals from the notochord. Trabecular cartilages, however, originate from the neural crest, and since they are located anterior to the rostral tip of the notochord, they cannot receive signals from the notochord. Due to these specialisations, and their essential role in cranial development, many comparative morphologists and embryologists have argued their developmental or evolutionary origins. The general theory is that the trabecular cartilage is derived from the neural crest mesenchyme which fills anterior to the mandibular arch (premandibular domain).\n\nAs clearly seen in the lamprey, Cyclostome also has a pair of cartilaginous rods in the embryonic head which is similar to the trabecular cartilages in jawed vertebrates.\nHowever, in 1916, Alexej Nikolajevich Sewertzoff pointed out that the cranial base of the lamprey is exclusively originated from the paraxial mesoderm. Then in 1948, reported the detail of the skeletogenesis of the lamprey, and showed that the “trabecular cartilages” in lamprey appear just beside the notochord, in a similar position to the parachordal cartilages in jawed vertebrates. Recent experimental studies also showed that the cartilages are derived from the head mesoderm. The “trabecular cartilages” in the Cyclostome is no longer considered to be the homologue of the trabecular in the jawed vertebrates: the (true) trabecular cartilages were firstly acquired in the Gnathostome lineage.\n\nThe trabecular cartilages were first described in the grass snake by at 1839. In 1874, Thomas Henry Huxley suggested that the trabecular cartilages are a modified part of the splanchnocranium: they arose as the serial homologues of the pharyngeal arches.\n\nThe vertebrate jaw is generally thought to be the modification of the mandibular arch (1st pharyngeal arch). Since the trabecular cartilages appear anterior to the mandibular arch, if the trabecular cartilages are serial homologues of the pharyngeal arches, ancestral vertebrates should possess more than one pharyngeal arch (so-called \"premandibular arches\") anterior to the mandibular arch. The existence of premandibular arch(es) has been accepted by many comparative embryologists and morphologists (e.g., Edwin Stephen Goodrich, Gavin de Beer). Moreover, reported premandibular arches and the corresponding branchiomeric nerves by the reconstruction of the Osteostracans (e.g., \"Cephalaspis\"; recently this arch was reinterpreted as the mandibular arch)\n\nHowever, the existence of the premandibular arch(es) has been rejected, and the trabecular cartilages are no longer assumed to be one of the pharyngeal arches.\n\n"}
{"id": "712222", "url": "https://en.wikipedia.org/wiki?curid=712222", "title": "Transit of Earth from Mars", "text": "Transit of Earth from Mars\n\nA transit of Earth across the Sun as seen from Mars takes place when the planet Earth passes directly between the Sun and Mars, obscuring a small part of the Sun's disc for an observer on Mars. During a transit, Earth would be visible from Mars as a small black disc moving across the face of the Sun. They occur every 26, 79 and 100 years, and every 1,000 years or so there is an extra 53rd-year transit.\n\nTransits of Earth from Mars usually occur in pairs, with one following the other after 79 years; rarely, there are three in the series. The transits also follow a 284-year cycle, occurring at intervals of 100.5, 79, 25.5, and 79 years; a transit falling on a particular date is usually followed by another transit 284 years later. Transits occurring when Mars is at its ascending node are in May, those at descending node happen in November. This cycle corresponds fairly closely to 151 Mars orbits, 284 Earth orbits, and 133 synodic periods, and is analogous to the cycle of transits of Venus from Earth, which follow a cycle of 243 years (121.5, 8, 105.5, 8). There are currently four such active series, containing from 8 to 25 transits. A new one is set to begin in 2394. The last series ending was in 1211.\n\nNo one has ever seen a transit of Earth from Mars, but the next transit will take place on November 10, 2084. The last such transit took place on May 11, 1984.\n\nDuring the event, the Moon could almost always also be seen in transit, although due to the distance between Earth and Moon, sometimes one body completes the transit before the other begins (this last occurred in the 1800 transit, and will happen again in 2394).\n\nA transit of Earth from Mars corresponds to Mars being perfectly uniformly illuminated at opposition from Earth, its phase being 180.0° without any defect of illumination. During the 1879 event, this permitted Charles Augustus Young to attempt a careful measurement of the oblateness (polar compression) of Mars. He obtained the value 1/219, or 0.0046. This is close to the modern value of 1/154 (many sources will cite somewhat different values, such as 1/193, because even a difference of only a couple of kilometers in the values of Mars' polar and equatorial radii gives a considerably different result).\n\nMuch more recently, better measurements of the oblateness of Mars have been made by using radar from the Earth. Also, better measurements have been made by using artificial satellites that have been put into orbit around Mars, including \"Mariner 9\", \"Viking 1\", \"Viking 2\", and Soviet orbiters, and the more recent orbiters that have been sent from the Earth to Mars.\n\nA science fiction short story published in 1971 by Arthur C. Clarke, called \"Transit of Earth\", depicts a doomed astronaut on Mars observing the transit in 1984. This short story was first published in the January 1971 issue of \"Playboy\" magazine.\n\nSometimes Earth only grazes the Sun during a transit. In this case it is possible that in some areas of Mars a full transit can be seen while in other regions there is only a partial transit (no second or third contact). The last transit of this type was on 30 April 1211, and the next such transit will occur on 27 November 4356. It is also possible that a transit of Earth can be seen in some parts of Mars as a partial transit, while in others Earth misses the Sun. Such a transit last occurred on 26 October 664, and the next transit of this type will occur on 14 December 5934.\n\nThe simultaneous occurrence of a transit of Venus and a transit of Earth is extremely rare, and will next occur in the year 571,471.\n\n\n\n"}
{"id": "26821712", "url": "https://en.wikipedia.org/wiki?curid=26821712", "title": "Tsunamis in lakes", "text": "Tsunamis in lakes\n\nA tsunami is defined as a series of water waves caused by the displacement of a large volume of a body of water; in the case of this article the body of water being investigated will be a lake rather than an ocean. Tsunamis in lakes are becoming increasingly important to investigate as a hazard, due to the increasing popularity for recreational uses, and increasing populations that inhabit the shores of lakes. Tsunamis generated in lakes and reservoirs are of high concern because it is associated with a near field source region which means a decrease in warning times to minutes or hours.\n\nInland tsunami hazards can be generated by many different types of earth movement. Some of these include earthquakes in or around lake systems, landslides, debris flow, rock avalanches, and glacier calving. Volcanogenic processes such as gas and mass flow characteristics are discussed in more detail below.\n\nTsunamis in lakes can be generated by fault displacement beneath or around lake systems. Faulting shifts the ground in a vertical motion through reverse, normal or oblique strike slip faulting processes, this displaces the water above causing a tsunami (Figure 1). The reason strike-slip faulting does not cause tsunamis is because there is no vertical displacement within the fault movement, only lateral movement resulting in no displacement of the water. In an enclosed basin such as a lake, tsunamis are referred to as the initial wave produced by coseismic displacement from an earthquake, and the seiche as the harmonic resonance within the lake.\n\nIn order for a tsunami to be generated certain criteria is required:\n\nThese tsunamis are of high damage potential due to being within a lake, making them of a near field source. This means a vast decrease in warning times, resulting in organised emergency evacuations after the generation of the tsunami being virtually impossible, and due to low lying shores even small waves lead to substantial flooding. Planning and education of residents needs to be done beforehand, so that when an earthquake is felt they know to head to higher ground and what routes to take to get there.\n\nLake Tahoe is an example of a lake that is in danger of having a tsunami due to faulting processes. Lake Tahoe in California and Nevada USA lies within an intermountain basin bounded by faults, with most of these faults at the lake bottom or hidden in glaciofluvial deposits. Lake Tahoe has had many prehistoric eruptions and in studies of the lake bottom sediments, a 10m high scarp has displaced the lake bottom sediments, indicating that the water was displaced by the same magnitude, as well as generating a tsunami. A tsunami and seiche in Lake Tahoe can be treated as shallow-water long waves as the maximum water depth is much smaller than the wavelength. This demonstrates the interesting impact that lakes have on the tsunami wave characteristics, as it is very different from ocean tsunami wave characteristics due to the ocean being deeper, and lakes being relatively shallow in comparison. With ocean tsunami waves amplitudes only increase when the tsunami gets close to shore, in lake tsunami waves are generated and stay in a shallow environment.\n\nThis would have a major impact on the 34,000 permanent residences along the lake, not to mention the impact on tourism in the area. Tsunami run-ups would leave areas near the lake inundated due to permanent ground subsidence attributed to the earthquake, with the highest run-ups and amplitudes being attributed to the seiches rather than the actual tsunami. The reason seiches cause so much damage is due to resonance within the bays reflecting the waves where they combine to make larger standing waves. For more information see seiches. Lake Tahoe also experienced a massive collapse of the western edge of the basin that formed McKinney Bay around 50,000 years ago. Is thought to have generated a tsunami/seiche wave with a height approaching .\n\nSub-aerial mass flows (landslides or rapid mass wasting) happen when a large amount of sediment becomes unstable, this can happen for example from the shaking from an earthquake, or saturation of the sediment initiating a sliding layer. This volume of sediment then flows into the lake giving a sudden large displacement of water. Tsunamis generated by sub aerial mass flows are defined in terms of the first initial wave being the tsunami wave and any tsunamis in terms of sub aerial mass flows are characterised into three zones. A splash zone or wave generation zone, this is the region were landslides and water motion are coupled and it extends as far as the landslide travels. Near field area, were the concern is based on the characteristics of the tsunami wave such as amplitude and wavelength which are crucial for predictive purposes. Far field area, the process is influenced mainly by dispersion characteristics and is not often used when investigating tsunamis in lakes, as most lake tsunamis are related only to near field processes.\n\nA modern example of a landslide into a reservoir lake, overtopping a dam, occurred in Italy with the Vajont Dam disaster in 1963. Evidence exists in paleoseismological evidence and other sedimentary core sample proxies of catastrophic rock failures of landslide-triggered lake tsunamis worldwide, including in Lake Geneva during AD 563.\n\nIn the event of the Alpine fault in New Zealand rupturing in the South Island, it is predicted that there would be shaking of approximately magnitude eight in the lake side towns of Queenstown (Lake Wakatipu) and Wanaka (Lake Wanaka). These could possibly cause sub-aerial mass flows that could generate tsunamis within the lakes, this would have a devastating impact on the 28,224 residents (2013 New Zealand census) who occupy these lake towns, not only in the potential losses of life and property, but the damage to the booming tourism industry would take years to rebuild.\n\nThe Otago Regional Council, responsible for the area, has recognised that in such an event, tsunamis could occur in both lakes.\n\nIn this article the focus is on tsunamis generated in lakes by volcanogenic processes in terms of gas build up causing violent lake over turns, with other processes such as pyroclastic flows not accounted for, as it requires more complex modelling . Lake overturns can be incredibly dangerous and occur when gas trapped at the bottom of the lake is heated by rising magma causing an explosion and lake overturn; an example of this is Lake Kivu.\n\nLake Kivu, one of the African Great Lakes, lies on the border between the Democratic Republic of the Congo and Rwanda, and is part of the East Africa Rift. Being part of the rift means it is affected by volcanic activity beneath the lake. This has led to a buildup of methane and carbon dioxide at the bottom of the lake, which can lead to violent limnic eruptions.\n\nLimnic eruptions (also called \"lake over turns\") are due to volcanic interaction with the water at the bottom of the lake that has high gas concentrations, this leads to heating of the lake and this rapid rise in temperature would spark a methane explosion displacing a large amount of water, followed nearly simultaneously by a release of carbon dioxide. This carbon dioxide would suffocate large numbers of people, with a possible tsunami generated from water displaced by the gas explosion affecting all of the 2 million people who occupy the shores of Lake Kivu. This is incredibly important as the warning times for an event such as a lake overturn is incredibly short in the order of minutes and the event itself may not even be noticed. Education of locals and preparation is crucial in this case and a lot of research in this area has been done in order to try to understand what is happening within the lake, in order to try to reduce the effects when this phenomenon does happen.\n\nA lake turn-over in Lake Kivu occurs from one of two scenarios. Either (1) up to another hundred years of gas accumulation leads to gas saturation in the lake, resulting in a spontaneous outburst of gas originating at the depth at which gas saturation has exceeded 100%, or (2) a volcanic or even seismic event triggers a turn-over. In either case a strong vertical lift of a large body of water results in a plume of gas bubbles and water rising up to and through the water surface. As the bubbling water column draws in fresh gas-laden water, the bubbling water column widens and becomes more energetic as a virtual \"chain reaction\" occurs which would look like a watery volcano. Very large volumes of water are displaced, vertically at first, then horizontally away from the centre at surface and horizontally inwards to the bottom of the bubbling water column, feeding in fresh gas-laden water. The speed of the rising column of water increases until it has the potential to rise 25m or more in the centre above lake level. The water column has the potential to widen to well in excess of a kilometre, in a violent disturbance of the whole lake. The watery volcano may take as much as a day to fully develop while it releases upwards of 400 billion cubic metres of gas (~12tcf). Some of these parameters are uncertain, particularly the time taken to release the gas and the height to which the water column can rise. As a secondary effect, particularly if the water column behaves irregularly with a series of surges, the lake surface will both rise by up to several metres and create a series of tsunamis or waves radiating away from the epicentre of the eruption. Surface waters may simultaneously race away from the epicentre at speeds as high as 20-40m/second, slowing as distances from the centre increase. The size of the waves created is unpredictable. Wave heights will be highest if the water column surges periodically, resulting in wave heights is great as 10-20m. This is caused by the ever-shifting pathway that the vertical column takes to the surface. No reliable model exists to predict this overall turnover behaviour. For tsunami precautions it will be necessary for people to move to high ground, at least 20m above lake level. A worse situation may pertain in the Ruzizi River where a surge in lake level would cause flash-flooding of the steeply sloping river valley dropping 700m to Lake Tanganyika, where it is possible that a wall of water from 20-50m high may race down the gorge. Water is not the only problem for residents of the Kivu basin; the more than 400 billion cubic metres of gas released creates a denser-than-air cloud which may blanket the whole valley to a depth of 300m or more. The presence of this opaque gas cloud, which would suffocate any living creatures with its mixture of carbon dioxide and methane laced with hydrogen sulphide, would cause the majority of casualties. Residents would be advised to climb to at least 400m above the lake level to ensure their safety. Strangely the risk of a gas explosion is not great as the gas cloud is only about 20% methane in carbon dioxide, a mixture that is difficult to ignite.\n\nAt 11:24 PM on 21 July 2014, in a period experiencing an earthquake swarm related to the upcoming eruption of Bárðarbunga, an 800m-wide section gave way on the slopes of the Icelandic volcano Askja. Beginning at 350m over water height, it caused a tsunami 20–30 meters high across the caldera, and potentially larger at localized points of impact. Thanks to the late hour, no tourists were present; however, search and rescue observed a steam cloud rising from the volcano, apparently geothermal steam released by the landslide. Whether geothermal activity played a role in the landslide is uncertain. A total of 30-50 million cubic meters was involved in the landslide, raising the caldera's water level by 1–2 meters.\n\nHazard mitigation for tsunamis in lakes is immensely important in the preservation of life, infrastructure and property. In order for hazard management of tsunamis in lakes to function at full capacity there are four aspects that need to be balanced and interacted with each other, these are:\n\n\nWhen all these aspects are taken into consideration and continually managed and maintained, the vulnerability of an area to a tsunami within the lake decreases. This is not because the hazard itself has decreased but the awareness of the people who would be affected makes them more prepared to deal with the situation when it does occur. This reduces recovery and response times for an area, decreasing the amount of disruption and in turn the effect the disaster has on the community.\n\nInvestigation into the phenomena of tsunamis in lakes for this article was restricted by certain limitations. Internationally there has been a fair amount of research into certain lakes but not all lakes that can be affected by the phenomenon have been covered. This is especially true for New Zealand with the possible occurrence of tsunamis in the major lakes recognised as a hazard, but with no further research completed.\n\n\n"}
{"id": "5212064", "url": "https://en.wikipedia.org/wiki?curid=5212064", "title": "Vacuum coffee maker", "text": "Vacuum coffee maker\n\nA vacuum coffee maker brews coffee using two chambers where vapor pressure and vacuum produce coffee. This type of coffee maker is also known as \"vac pot\", \"siphon\" or \"syphon coffee maker,\" and was invented by Loeff of Berlin in the 1830s. These devices have since been used for more than a century in many parts of the world. Design and composition of the vacuum coffee maker varies. The chamber material is borosilicate glass, metal, or plastic, and the filter can be either a glass rod or a screen made of metal, cloth, paper, or nylon. The Napier Vacuum Machine, presented in 1840, was an early example of this technique. While vacuum coffee makers generally were excessively complex for everyday use, they were prized for producing a clear brew, and were quite popular until the middle of the twentieth century. The Bauhaus interpretation of this device can be seen in Gerhard Marcks' Sintrax coffee maker of 1925.\n\nA vacuum coffee maker operates as a siphon, where heating and cooling the lower vessel changes the vapor pressure of water in the lower, first pushing the water up into the upper vessel, then allowing the water to fall back down into the lower vessel. Concretely, the principle of a vacuum coffee maker is to heat water in the lower vessel of the brewer until expansion forces the contents through a narrow tube into an upper vessel containing coffee grounds (as water temperature increases, dense liquid water increasingly converts to less dense water vapor gas, which takes up more space and thus increases pressure); when the water reaches and exceeds the boiling point (so the vapor pressure equals and then exceeds atmospheric pressure), the (water vapor) pressure in the lower vessel exceeds the (atmospheric) pressure in the top vessel and water is pushed up the siphon tube into the upper vessel. During brewing, a small amount of water and sufficient water vapor remain in the lower vessel and are kept hot enough so the pressure will support the column of water in the siphon. When enough time has elapsed that the coffee has finished brewing, the heat is removed and the pressure in the bottom vessel drops, so the force of gravity (acting on the water) and atmospheric pressure (pressing on the liquid in the upper vessel) push the water down into the lower vessel, through a strainer and away from the grounds, ending brewing. The coffee can then be decanted from the lower chamber; the device must usually be taken apart to pour out the coffee.\n\nThe iconic Moka pot coffee maker functions on the same principle but the water is forced up from the bottom chamber through a third middle chamber containing the coffee grounds to the top chamber which has an air gap to prevent the brewed coffee from returning downwards. The prepared coffee is then poured off from the top.\n\nNote that siphons work by \"pushing\" (the water is under pressure – see hydrostatic pressure, not under tension), and it is the changing vapor pressure in the lower vessel, combined with the constant atmospheric pressure in the upper vessel that drive the siphon. When the water cools the pressure in the lower vessel drops as steam condenses into dense water, taking up less volume and hence dropping the pressure. This creates a partial vacuum, causing the atmospheric pressure outside the container (along with gravity) to force the liquid back into the lower vessel.\n\nAn early variation of this principle is called a \"balance siphon\". This implementation has the two chambers arranged side by side on a balance-like device, with a counterweight attached to the heated chamber. Once the vapor has forced the hot water out, the counterweight activates a spring-loaded snuffer which smothers the flame and allows the initial chamber to cool down thus lowering pressure (creating a vacuum) and causing the brewed coffee to seep in.\n\n\n"}
{"id": "10340923", "url": "https://en.wikipedia.org/wiki?curid=10340923", "title": "Vacuum consolidation", "text": "Vacuum consolidation\n\nVacuum consolidation (or vacuum preloading) is a soft soil improvement method that has been successfully used by geotechnical engineers and specialists of ground improvement companies in countries such as Australia, China, Korea, Thailand and France for soil improvement or land reclamation (Chu et al., 2005). It does not necessarily require surcharge fill and vacuum loads of 80kPa or greater can, typically, be maintained for as long as required. However, if loads of 80kPa or greater are needed in order to achieve the target soil improvement, additional surcharge may be placed on top of the vacuum system. The vacuum preloading method is cheaper and faster than the fill surcharge method for an equivalent load in suitable areas. Where the underlying ground consists of permeable materials, such as sand or sandy clay, the cost of the technique will be significantly increased due to the requirement of cut-off walls into non-permeable layers to seal off the vacuum. It has been suggested by Carter et al. (2005) that the settlement resulting from vacuum preloading is less than that from a surcharge load of the same magnitude as vacuum consolidation is influenced by drainage boundary conditions.\n\n"}
{"id": "46333271", "url": "https://en.wikipedia.org/wiki?curid=46333271", "title": "Vertebrate land invasion", "text": "Vertebrate land invasion\n\nThe aquatic to terrestrial transition of vertebrate organisms occurred in the late Devonian era and was an important step in the evolutionary history of modern land vertebrates. The transition allowed animals to escape competitive pressure from the water and explore niche opportunities on land. Fossils from this period have allowed scientists to identify some of the species that existed during this transition, such as Tiktaalik and Acanthostega. Many of these species were also the first to develop adaptations suited to terrestrial over aquatic life, such as neck mobility and hindlimb locomotion.\n\nThe late Devonian vertebrate transition was preceded by the plant and invertebrate terrestrial invasion. These invasions allowed for the appropriate niche development that would ultimately facilitate the vertebrate invasion. While the late Devonian event was the first land invasion by vertebrate organisms, aquatic species have continued to develop adaptations suited to terrestrial life (and vice versa) from the late Devonian to the Holocene.\n\nThe vertebrate species that were important to the initial water to land transition can be qualified as being in one of five groups: Sarcopterygian fishes, prototetrapods, aquatic tetrapods, true tetrapods, and terrestrial tetrapods. Many morphological changes occurred throughout this transition. Mechanical support structures changed from fins to limbs, the method of locomotion changed from swimming to walking, respiratory structures changed from gills to lungs, feeding mechanisms changed from suction feeding to biting, and mode of reproduction changed from larval development to metamorphosis.\n\nLungfish appeared approximately 400 million years ago. It is a species that endured rapid evolution during the Devonian era, which became known as the dipnoan renaissance. The Acanthostega species, known as the fish with legs, is considered a tetrapod by structural findings but is postulated to have perhaps never left the aquatic environment. Its legs are not well-suited to support its weight. The bones of its forearm, the radius and ulna, are very thin at the wrist and also unable to support it on land. It also lacks a sacrum and strong ligaments at the hip, which would be integral to supporting the animal against gravity. In this sense, the species is considered a tetrapod but not one that has adapted well enough to walk on land. Furthermore, its gill bars have a supportive brace characterized for use as an underwater ear because it can pick up noise vibrations through the water. Tetrapods that adapted to terrestrial living adapted these gill bones to pick up sounds through air, and they later became the middle ear bones seen in mammalian tetrapods. Ichthyostega, on the other hand, is considered to be a fully terrestrial tetrapod that perhaps depended on water for its aquatic young. Comparisons between the skeletal features of Acanthostega and Ichthyostega reveal that they had different habits. Acanthostega is likely exclusive to an aquatic environment, while Ichthyostega is progressed in the aquatic to terrestrial transition by living dominantly on the shores.\n\nAn evolutionary timeline of the late Devonian vertebrate terrestrial invasion demonstrates the changes that took place. A group of fish from the Givetian stage began developing limbs, and eventually evolved into aquatic tetrapods in the Famennian stage. Pederpes, Westlothiana, Protogyrinus, and Crassigyrinus descended from these species into the carboniferous period and were the first land vertebrates.\n\nA particularly important transitional species is one known as Tiktaalik. It has a fin, but the fin has bones within it that are similar to mammalian tetrapods. It has an upper arm bone, a lower arm bone, forearm bones, a wrist, and fingerlike projections. Essentially, it is a fin that can support the animal. Similarly, it also has a neck that allows independent head movement from the body. Its ribs are also able to support the body in gravity. Its skeletal features exhibit its ability as a fish that can live in shallow water and also venture onto land.\n\nIt took many millions of years for vertebrates to transition out of water onto land. During this time, both the competitive pressures that would push species out of the water and the niche occupation incentives that would pull species onto land were slowly building. The culmination of these driving factors are what ultimately facilitated the vertebrate transition.\n\nScientists believe that a long period of time where biotic and abiotic factors in the aquatic environment were unfavourable to certain aquatic organisms is what pushed their transition to shallower waters. Some of these push factors are environmental hypoxia, unfavourable aquatic temperatures, and increased salinity. Other constantly present factors such as predation, competition, waterborne diseases and parasites also contributed to the transition.\n\nA theory put forth by Joseph Barrell possibly helps explain what may have initiated these push factors to become relevant in the late Devonian. The extensive oxidized sediments that were present in Europe and North America during the late Devonian are evidence of severe droughts during this time. These droughts would cause small ponds and lakes to dry out, forcing certain aquatic organisms to move on land to find other bodies of water. Natural selection on these organisms eventually led to the evolution of the first terrestrial vertebrates.\n\nThe pull factors were secondary to the push factors, and only became significant once the pressures to leave the aquatic environment became significant. These were largely the niches and opportunities that were available for exploitation in the terrestrial environment, and include higher environmental oxygen partial pressures, favourable temperatures, and the lack of competitors and predators on land. The plants and invertebrates that had preceded the vertebrate invasion also provided opportunities in the form of abundant prey and lack of predators.\n\nThere were many challenges that the first land vertebrates faced. These challenges allowed for rapid natural selection and niche domination, resulting in an adaptive radiation that produced many different vertebrate land species in a relatively short period of time.\n\nDepending on the water depth at which a species lives, the visual perception of many aquatic species is better suited to darker environments than those on land. Similarly, hearing in aquatic organisms is better optimized for sounds underwater, where the speed and amplitude of sound is greater than in air.\n\nHomeostasis was almost definitely a challenge for land invading vertebrates. Gas exchange and water balance are highly different in water and in air. Homeostasis mechanisms suitable for a terrestrial environment may have been necessary to develop before these organisms invaded land.\n\nThe primary anatomical barrier is the development of lungs for proper gas exchange, however other anatomical barriers also exist. The stressors of the musculoskeletal system are different in air than they are in water, and the muscles and bones must be strong enough to withstand the increased effects of gravity on land.\n\nMany behaviours, such as reproduction, are specifically optimized to a wet environment. Navigation and locomotion are also highly different in aquatic environments compared to terrestrial environments.\n\nThe ancestral species of tetrapods that lived entirely in water had tall and narrow skulls with eyes facing sideways and forwards to maximize visibility for predators and prey in the aquatic environment. As the ancestors of early tetrapods started inhabiting shallower waters, these species had flatter skulls with eyes at the tops of their heads, which made it possible to spot food above them. Once the tetrapods transitioned onto land, the lineages evolved to have tall and narrow skulls with eyes facing sideways and forwards again. This allowed them to navigate through the terrestrial environment and look for predators and prey.\n\nFish do not have necks, so the head is directly connected to the shoulders. In contrast, land animals use necks to move their heads so they can look down to see the food on the ground. The greater the mobility of the neck, the more visibility the land animal has. As lineages moved from completely aquatic environments to shallower waters and land, they gradually evolved vertebral columns that increased neck mobility. The first neck vertebra that evolved permitted the animals to have flexion and extension of the head so that they can see up and down. The second neck vertebra evolved to allow rotation of the neck for moving the head left and right. As tetrapod species continued to evolve on land, adaptations included seven or more vertebrae, allowing increasing neck mobility.\n\nThe sacrum connects the pelvis and hindlimbs and is useful for motion on land. The aquatic ancestors of tetrapods did not have a sacrum, so it was speculated to have evolved for locomotive function exclusive to terrestrial environments. However, the Acanthostega species is one of the earliest lineages to have a sacrum, even though it is a fully aquatic species. Once species moved onto land, the trait was adapted for terrestrial locomotion support, which is evidenced by additional vertebrae fusing similarly to permit additional support. This is an example of exaptation, where a trait performs a function that did not arise through natural selection for its current use.\n\nAs the lineages evolved to adapt to terrestrial environments, many lost traits that were better suited for the aquatic environment. Many lost their gills, which were only useful for obtaining oxygen in water. Their tail fins became smaller. They lost the lateral line system, a network of canals along the skull and jaw that are sensitive to vibration, which does not work outside of an aquatic environment.\n\nFor successful land invasion, the species had several pre-adaptations like air-breathing and limb-based locomotion. Aspects such as reproduction and swallowing, however, have bound these species to the aquatic environment. These pre-adaptations have allowed vertebrates to venture onto land hundreds of times, but were not able to accomplish the same degree of prolific radiation into diverse terrestrial species. To understand the potential of future invasions, studies must evaluate the models of evolutionary steps taken in past invasions. The commonalities to current and future invasions may then be elucidated to predict the effects of environmental changes.\n"}
{"id": "26830333", "url": "https://en.wikipedia.org/wiki?curid=26830333", "title": "Water-energy nexus", "text": "Water-energy nexus\n\nThere is no formal definition for the water-energy nexus - the concept refers to the relationship between the water used for energy production, including both electricity and sources of fuel such as oil and natural gas, and the energy consumed to extract, purify, deliver, heat/cool, treat and dispose of water (and wastewater) sometimes referred to as the energy intensity (EI). The relationship is not truly a closed loop as the water used for energy production need not be the same water that is processed using that energy, but all forms of energy production require some input of water making the relationship inextricable.\n\nAmong the first studies to evaluate the water and energy relationship was a life-cycle analysis conducted by Peter Gleick in 1994 that highlighted the interdependence and initiated the joint study of water and energy. In 2014 the US Department of Energy (DOE) released their report on the water-energy nexus citing the need for joint water-energy policies and better understanding of the nexus and its susceptibility to climate change as a matter of national security. The hybrid Sankey diagram in the DOE's 2014 water-energy nexus report summarizes water and energy flows in the US by sector, demonstrating interdependence as well as singling out thermoelectric power as the single largest user of water, used mainly for cooling. \n\nAll types of energy generation consume water either to process the raw materials used in the facility, constructing and maintaining the plant, or to just generate the electricity itself. Renewable power sources such as photovoltaic solar and wind power, which require little water to produce energy, require water in processing the raw materials to build. Water can either be \"used\" or \"consumed,\" and can be categorised as fresh, ground, surface, blue, grey or green among others. Water is considered used if it does not reduce the supply of water to downstream users, i.e. water that is taken and returned to the same source (instream use), such as in thermoelectric plants that use water for cooling and are by far the largest users of water. While used water is returned to the system for downstream uses, it has usually been degraded in some way, mainly due to thermal or chemical pollution, and the natural flow has been altered which does not factor into an assessment if only the quantity of water is considered. Water is consumed when it is removed completely from the system, such as by evaporation or consumption by crops or humans. When assessing water use all these factors must be considered as well as spatiotemporal considerations making precise determination of water use very difficult.\n\nSpang et al. (2014) conducted a study looking at the water consumption for energy production (WCEP) internationally that both showed the variation in energy types produced across countries as well as the vast differences in efficiency of energy production per unit of water use (figure 1). Operations of water distribution systems and power distribution systems under emergency conditions of limited power and water availability is an important consideration for improving the overall resilience of the water - energy nexus. Khatavkar and Mays (2017) present a methodology for control of water distribution and power distribution systems under emergency conditions of drought and limited power availability to ascertain at least minimal supply of cooling water to the power plants. Khatavkar and Mays (2017 b) applied an optimization model for water - energy nexus system for a hypothetical regional level system which showed an improved resilience for several contingency scenarios.\n\nIn 2001 operating water systems in the US consumed approximately 3% of the total annual electricity (~75 TWh). The California's State Water Project (SWP) and Central Valley Project (CVP) are together the largest water system in the world with the highest water lift, over 2000 ft. across the Tehachapi mountains, delivering water from the wetter and relatively rural north of the state, to the agriculturally intensive central valley, and finally to the arid and heavily populated south. Consequently, the SWP and CVP are the single largest consumers of electricity in California consuming approximately 5 TWh of electricity each per year. In 2001, 19% of the state’s total electricity use (~48 TWh/year) was used in processing water including end uses, with the urban sector accounting for 65% of this. In addition to electricity, 30% of California’s natural gas consumption was due to water-related processes, mainly residential water heating, and 88 million gallons of diesel was consumed by groundwater pumps for agriculture. The residential sector alone accounted for 48% of the total combined electricity and natural gas consumed for water-related processes in the state.\n\nAccording to the California Public Utilities Commission (CPUC) Energy Division’s Embedded Energy in Water Studies report:\"“'Energy Intensity' refers to the average amount of energy needed to transport or treat water or wastewater on a per unit basis\"\"Energy Intensity is sometimes used synonymously with embedded or embodied energy. In 2005, water deliveries to Southern California were assessed to have an average EI of 12.7 MWh/MG, nearly two-thirds of which was due to transportation. Following the findings that a fifth of California’s electricity is consumed in water-related processes including end-use, the CPUC responded by authorising a statewide study into the relationship between energy and water that was conducted by the California Institute for Energy and Environment (CIEE), and developed programs to save energy through water conservation.\n\nHydroelectricity is a special case of water used for energy production mainly because hydroelectric power generation is regarded as being clean and renewable, and dams (the main source of hydroelectric production) serve multiple purposes besides energy generation, including flood prevention, storage, control and recreation which make justifiable allocation analyses difficult. Furthermore, the impacts of hydroelectric power generation can be hard to quantify both in terms of evaporative consumptive losses and altered quality of water, since damming results in flows that are much colder than for flowing streams. In some cases the moderation of flows can be seen as a rivalry of water use in time may also need to accounted for in impact analysis.\n\n\n"}
{"id": "49801999", "url": "https://en.wikipedia.org/wiki?curid=49801999", "title": "Wild Seasons (Kay Young)", "text": "Wild Seasons (Kay Young)\n\nWild Seasons: Gathering and Cooking Wild Plants of the Great Plains is a 1993 non-fiction book by author, illustrator, and ethno-botanist Kay Young. It features a variety of wild plants of the great plains area and how to prepare them in appetizing ways. The book includes a number of recipes as well as Young's enthusiasm and advocacy for eating wild crops. It was published by the University of Nebraska Press.\n\n"}
