{"id": "18638007", "url": "https://en.wikipedia.org/wiki?curid=18638007", "title": "Antinutrient", "text": "Antinutrient\n\nAntinutrients are natural or synthetic compounds that interfere with the absorption of nutrients. Nutrition studies focus on these antinutrients commonly found in food sources and beverages.\n\nPhytic acid has a strong binding affinity to minerals such as calcium, magnesium, iron, copper, and zinc. This results in precipitation, making the minerals unavailable for absorption in the intestines. Phytic acids are common in the hulls of nuts, seeds and grains and of great importance in agriculture animal nutrition and eutrophication wise due to the mineral chelation and bound phosphates released in to the environment. Without the need to use phytate (but also nutrient) reducing milling, the amount of phytic acid is commonly reduced in animal feeds by adding histidine acid phosphate type of phytases to them.\n\nProtease inhibitors are substances that inhibit the actions of trypsin, pepsin and other proteases in the gut, preventing the digestion and subsequent absorption of protein. For example, Bowman–Birk trypsin inhibitor is found in soybeans.\n\nLipase inhibitors interfere with enzymes, such as human pancreatic lipase, that catalyze the hydrolysis of some lipids, including fats. For example, the anti-obesity drug orlistat causes a percentage of fat to pass through the digestive tract undigested.\n\nAmylase inhibitors prevent the action of enzymes that break the glycosidic bonds of starches and other complex carbohydrates, preventing the release of simple sugars and absorption by the body. Amylase inhibitors, like lipase inhibitors, have been used as a diet aid and obesity treatment. Amylase inhibitors are present in many types of beans; commercially available amylase inhibitors are extracted from white kidney beans.\n\nOxalic acid and oxalates are present in many plants and in significant amounts particularly in rhubarb, tea, spinach, parsley and purslane. Oxalates bind to calcium and prevent its absorption in the human body.\n\nGlucosinolates prevent the uptake of iodine, affecting the function of the thyroid and thus are considered goitrogens. They are found in plants such as broccoli, brussel sprouts, cabbage, mustard greens, radishes and cauliflower.\n\nExcessive intake of required nutrients can also result in them having an anti-nutrient action. Excessive intake of dietary fiber can reduce the transit time through the intestines to such a degree that other nutrients cannot be absorbed. However, this effect is often not seen in practice and reduction of absorbed minerals can be attributed mainly to the phytic acids in fibrous food. Foods high in calcium eaten simultaneously with foods containing iron can decrease the absorption of iron via an unclear mechanism involving iron transport protein hDMT1, which calcium can inhibit.\n\nSome proteins can also be antinutrients, such as the trypsin inhibitors and lectins found in legumes. These enzyme inhibitors interfere with digestion. Avidin is an antinutrient found in active form in raw egg whites. It binds very tightly to biotin (vitamin B7) and can cause deficiency of B7 in animals and, in extreme cases, in humans.\n\nWidespread form of antinutrients are the flavonoids, which are a group of polyphenolic compounds that include tannins. These compounds chelate metals such as iron and zinc and reduce the absorption of these nutrients, but they also inhibit digestive enzymes and may also precipitate proteins.\n\nSaponins in plants may serve as anti-feedants and can be classified as antinutrients.\n\nAntinutrients are found at some level in almost all foods for a variety of reasons. However, their levels are reduced in modern crops, probably as an outcome of the process of domestication. The possibility now exists to eliminate antinutrients entirely using genetic engineering; but, since these compounds may also have beneficial effects, such genetic modifications could make the foods more nutritious but not improve people's health.\n\nMany traditional methods of food preparation such as fermentation, cooking, and malting increase the nutritive quality of plant foods through reducing certain antinutrients such as phytic acid, polyphenols, and oxalic acid. Such processing methods are widely used in societies where cereals and legumes form a major part of the diet. An important example of such processing is the fermentation of cassava to produce cassava flour: this fermentation reduces the levels of both toxins and antinutrients in the tuber.\n\n"}
{"id": "52157367", "url": "https://en.wikipedia.org/wiki?curid=52157367", "title": "Asbestos and the law (United States)", "text": "Asbestos and the law (United States)\n\nWithin the United States, the use of asbestos is limited by state and federal regulations and legislation. Improper use of asbestos and injury from exposure is addressed through administrative action, litigation, and criminal prosecution. Injury claims arising from asbestos exposure may be tried as mass torts.\n\nAsbestos litigation is the longest, most expensive mass tort in U.S. history, involving more than 8,000 defendants and 700,000 claimants. By the early 1990s, \"more than half of the 25 largest asbestos manufacturers in the US, including Amatex, Carey-Canada, Celotex, Eagle-Picher, Forty-Eight Insulations, Manville Corporation, National Gypsum, Standard Insulation, Unarco, and UNR Industries had declared bankruptcy. Analysts have estimated that the total costs of asbestos litigation in the U.S. alone will eventually reach $200 to $275 billion. The amounts and method of allocating compensation have been the source of many court cases, and government attempts at resolution of existing and future cases.\n\nClaims made against employers by injured workers will typically be in the form of a workers compensation claim, although the long onset for diseases such as mesothelioma may make it impossible for a worker to pursue workers' compensation benefits. However, it is possible for an injured worker to also bring a product liability claim against a third party that is responsible for introducing asbestos into the workplace. Asbestos lawsuits in the U.S. have included the following as defendants:\n\nManufacturers of machinery in which asbestos-containing parts were used have contested liability on the grounds that nearly all of them either did not ship asbestos-containing parts with their products at all (that is, asbestos was installed only by end users) or did not sell replacement parts for their own products (in cases where the plaintiff was allegedly exposed well after any factory-original asbestos-containing parts would have been replaced), and either way cannot be responsible for toxic third-party parts that they did not manufacture, distribute, or sell. In 2008, the Washington Supreme Court, the first state supreme court to reach the issue, decided in favor of the defense. On January 12, 2012, the Supreme Court of California also decided in favor of the defense in \"O'Neil v. Crane Co.\"\n\nAnother area of dispute remains the so-called chrysotile-defense. Manufacturers of some products containing only chrysotile fibers claim that these are not as harmful as amphibole-containing products. As 95% of the products used in the United States historically were mostly chrysotile, this claim is widely disputed by health officials and medical professionals. The World Health Organization recognizes that exposure to all types of asbestos fibers, including chrysotile, can cause cancer of the lung, larynx, and ovary, mesothelioma, and asbestosis.\n\nDefendants in asbestos litigation have accused the lawyers who represent plaintiffs of unethical conduct, but those allegations have not been successful in stopping the litigation, nor have courts been sufficiently convinced by the allegations to sanction the law firms against whom the allegations have been raised.\n\n\nSince the bankruptcy filing of Johns-Manville in 1984, many U.S. and U.K. asbestos manufacturers have avoided litigation by filing bankruptcy. Asbestos bankruptcy trusts are personal injury trusts established by firms that have filed for reorganization under Chapter 11 of the United States Bankruptcy Code to pay personal injury claims caused by exposure to asbestos. At least fifty-six trusts were established from the mid-1970s through 2011. The largest 26 of these trusts paid about 2.4 million claims totaling about $10.9 billion up to 2008. The trusts are governed by trust advisory committees that are generally controlled by lawyers from a few prominent law firms such as Baron & Budd, P.C. and Weitz & Luxenberg P.C.\n\nBankruptcy trusts may pay pennies on the dollar to people injured as a result of asbestos exposure. At the same time, these trusts may permit larger numbers of claimants to receive some kind of compensation, even if greatly reduced from potential recoveries in the tort system.\n\nThe federal Medicare Secondary Payer law imposes penalties for paying settlements directly to claimants without repaying the government for medical costs covered under the same programs under the legal doctrine of subrogation. In late 2016, attorneys general from 13 states sent demand letters to bankruptcy trusts for Armstrong World Industries, Babcock & Wilcox, DII, and Owens Corning. The purpose of the demand letters was to determine if the funds are reimbursing states for medical treatment received under Medicaid and Medicare.\n\nThe pursuit of compensation for asbestos injuries often involves both litigation against solvent defendants and filing claims against asbestos bankruptcy trusts.\n\nThe amount of compensation recovered by an injured plaintiff may depend on whether evidence of exposure to products from bankrupt firms is introduced at trial. If no evidence of exposure from bankrupt firms is presented then increased financial responsibility is likely to be assigned to solvent defendants. Researchers from RAND Corporation found that if a company filed for bankruptcy plaintiffs claimed exposure to their products in interrogatories and depositions at significantly reduced rates.\n\nWhen a plaintiff claiming an asbestos injury has filed a claim against a solvent defendant, courts may extend or reopen discovery when it is discovered that the plaintiff failed to disclose a trust claims. For example, in the 2008 case of \"Edwards v. John Crane-Houdaille, Inc\" production of claim forms was delayed until two weeks before trial. In the 2004 case of \"Stoeckler v. American Oil Co.\" the defendants discovered that the plaintiff did not disclose trust claims only three days after the start of the trial, resulting in the re-opening of discovery. To help avoid this type of issue, Judges will often adopt mandatory disclosure obligations for bankruptcy trust claims.\n\nAccording to a September 2004 of the \"American Journal of Respiratory and Critical Care Medicine\", asbestos is still a hazard for 1.3 million US workers in the construction industry and for workers involved in the maintenance of buildings and equipment.\n\nAsbestos is not part of an ASTM (American Society for Testing and Materials) E 1527-05 Phase I Environmental Site Assessment (ESA). A building survey for asbestos is considered an out-of-scope consideration under the industry standard ASTM 1527-05 Phase I ESA (see ASTM E 1527-05). ASTM Standard E 2356-04 should be consulted by the owner or owner's agent to determine which type of asbestos building survey is appropriate, typically either a baseline survey or a design survey of functional areas. Both types of surveys are explained in detail under ASTM Standard E 2356-04. Typically, a baseline survey is performed by an EPA (or state) licensed asbestos inspector. The baseline survey provides the buyer with sufficient information on presumed asbestos at the facility, often which leads to reduction in the assessed value of the building (due primarily to forthcoming abatement costs). Note: EPA NESHAP (National Emissions Standards for Hazardous Air Pollutants) and OSHA (Occupational Safety and Health Administration) Regulations must be consulted in addition to ASTM Standard E 2356-04 to ensure all statutory requirements are satisfied, ex. notification requirements for renovation/demolition. Asbestos is not a material covered under CERCLA (Comprehensive Environmental Response, Compensation, and Liability Act ) innocent purchaser defense. In some instances, the U.S. EPA includes asbestos contaminated facilities on the NPL (Superfund). Buyers should be careful not to purchase facilities, even with an ASTM E 1527-05 Phase I ESA completed, without a full understanding of all the hazards in a building or at a property, without evaluating non-scope ASTM E 1527-05 materials, such as asbestos, lead, PCBs, mercury, radon, et al. A standard ASTM E 1527-05 does not include asbestos surveys as standard practice.\n\nIn 1988, the United States Environmental Protection Agency (USEPA) issued regulations requiring certain U.S. companies to report the asbestos used in their products.\n\nA Senate subcommittee of the Health Education Labor and Pensions Committee heard testimony on July 31, 2001, regarding the health effects of asbestos. Members of the public, doctors, and scientists called for the United States to join other countries in a ban on the product.\n\nSeveral legislative remedies have been considered by the U.S. Congress but each time rejected for a variety of reasons. In 2005, Congress considered but did not pass legislation entitled the \"Fairness in Asbestos Injury Resolution Act of 2005\". The act would have established a $140 billion trust fund in lieu of litigation, but as it would have proactively taken funds held in reserve by bankruptcy trusts, manufacturers and insurance companies, it was not widely supported either by victims or corporations.\n\nOn April 26, 2005, Dr. Philip J. Landrigan, professor and chair of the Department of Community and Preventive Medicine at Mount Sinai Medical Center in New York City, testified before the US Senate Committee on the Judiciary against this proposed legislation. He testified that many of the bill's provisions were unsupported by medicine and would unfairly exclude a large number of people who had become ill or died from asbestos: \"The approach to the diagnosis of disease caused by asbestos that is set forth in this bill is not consistent with the diagnostic criteria established by the American Thoracic Society. If the bill is to deliver on its promise of fairness, these criteria will need to be revised.\" Also opposing the bill were the American Public Health Association and the Asbestos Workers' Union.\n\nOn June 14, 2006, the Senate Judiciary Committee approved an amendment to the act which would have allowed victims of mesothelioma $1.1M within 30 days of their claim's approval. This version would have also expanded eligible claimants to people exposed to asbestos from the September 11, 2001 attacks on the World Trade Center, and to construction debris in hurricanes Katrina and Rita. Ultimately, the bill's reliance on funding from private entities large and small, as well as debate over a sunset provision and the impact on the U.S. budgetary process caused the bill to fail to leave committee.\nAccording to the Environmental Working Group Action Fund, 10,000 people die each year from asbestos-caused diseases in the United States, including one out of every 125 American men who die over the age of 50. The Environmental Protection Agency (EPA) has no general ban on the use of asbestos. However, asbestos was one of the first hazardous air pollutants regulated under Section 112 of the Clean Air Act of 1970, and many applications have been forbidden by the Toxic Substances Control Act (TSCA).\n\nIn 2010, Washington state passed a ban on hazardous materials in automotive brakes, phasing out asbestos in vehicle brakes, starting in 2014.\n\nIn 2013, Ohio passed became the first state to pass a law requiring transparency in asbestos bankruptcy trust claims. The same year Oklahoma passed a similar law called The Personal Injury Trust Fund Transparency Act. This law applies to all personal injury trusts. It requires plaintiffs to disclose all previously filed and anticipated trust claims for personal injuries within 90 days of filing a personal injury tort but not until at least 180 days before the assigned court date. If the plaintiff anticipates filing a trust claim all proceedings are stayed until their filing is complete. Filing new claims or amending claims after the initial disclosure triggers a new disclosure requirement. The law also allows defendants to stay proceedings by showing that the plaintiff could make a good faith filing with a trust. The law gives plaintiffs ten days to either file such a claim or show that it would probably be unsuccessful.\n\nIn South Carolina in 2015, State Senator Shane Massey introduced Senate Bill 281, \"The Court Transparency Act.\" S.281 would prohibit the state of South Carolina from hiring outside lawyers. Similar bills have been passed into law by 18 states. The bill would also prevent juries from awarding damages that exceed actual out of pocket costs incurred by plaintiffs.\n\nIn June 2015, Texas Governor Greg Abbott, a Republican, signed Texas House Bill 1492 into law. The law was written to end so-called asbestos \"double dipping\" in Texas. This law requires asbestos victims to perform more actions before proceeding to trial, and lowers the standard of proof of asbestos exposure for manufacturers to shift the blame onto other bankrupt companies A year earlier, Wisconsin Governor Scott Walker signed a similar bill into law.\n\nIn June 2016, President Obama signed into law the Frank R. Lautenburg Chemical Safety for the 21st Century Act (H.R. 2576). It serves to reform the TSCA of 1976 and aims to make federal safety regulations on toxic substances and chemicals effective.\n\nIn 2017, Iowa, Mississippi, North Dakota, and South Dakota all passed asbestos trust claims transparency laws.\n\nAsbestos abatement (removal of asbestos) has become a thriving industry in the United States. Strict removal and disposal laws have been enacted to protect the public from airborne asbestos. The Clean Air Act requires that asbestos be wetted during removal and strictly contained, and that workers wear safety gear and masks. The federal government has prosecuted dozens of violations of the act and violations of Racketeer Influenced and Corrupt Organizations Act (RICO) related to the operations. Often these involve contractors who hire undocumented workers without proper training or protection to illegally remove asbestos.\n\nW. R. Grace and Company faces fines of up to $280 million for polluting the town of Libby, Montana. Libby was declared a Superfund disaster area in 2002, and the EPA has spent $54 million in cleanup. Grace was ordered by a court to reimburse the EPA for cleanup costs, but the bankruptcy court must approve any payments.\n\nThe U.S. Supreme Court has dealt with several asbestos-related cases since 1986. Two large class action settlements, designed to limit liability, came before the court in 1997 and 1999. Both settlements were ultimately rejected by the court because they would exclude future claimants, or those who later developed asbestos-related illnesses. These rulings addressed the 20-50 year latency period of serious asbestos-related illnesses.\n\nIn this case a federal appeals court ruled that an insulation installer from Texas could sue asbestos manufactures for failure to warn. Borel's lawyers argued that had warning labels been affixed to Fiberboard's products he would have been able to protect himself more effectively.\n\nThe Manville Corporation, formerly the Johns-Manville Corporation, filed for reorganization and protection under the United States Bankruptcy Code in August 1982. At the time, it was the largest company ever to file bankruptcy, and was one of the richest. Manville was then 181st on the Fortune 500, but was the defendant of 16,500 lawsuits related to the health effects of asbestos. The company was described by Ron Motley, a South Carolina attorney, as \"the greatest corporate mass murderer in history.\" Court documents show that the corporation had a long history of hiding evidence of the ill effects of asbestos from its workers and the public. One of many examples is a memo from Manville's medical director to corporate headquarters:\n\nIn a decision from January 2014, \"Gray v. Garlock Sealing Technologies\" had entered into bankruptcy proceedings, and discovery in the case uncovered evidence of fraud that led to a reduction in estimated future liability to a tenth of what was estimated.\n\nA number of lawsuits have been filed under the Racketeer Influenced and Corrupt Organizations Act (RICO) in response to what defendants claim to be fraudulent asbestos-related lawsuits. RICO suits are civil in nature and brought by private parties. They typically allege that the suits themselves are forms of racketeering or that lawyers and experts had to engage in racketeering activities in order to bring them.\n\nFor example,\n\nSome companies and their executives have faced criminal prosecution for their actions in exposing workers to the dangers of asbestos, or their improper handling of asbestos waste.\n\nOn February 20, 1973 a federal grand jury in Detroit, Michigan indicted Adamo Wrecking Company (\"Adamo\") for violating provisions of the Clean Air Act by knowingly causing the emission of asbestos by failure to wet and remove friable asbestos materials from demolitions.\n\nAdamo was one of a number of demolition contractors indicted throughout the country for the alleged violation of the Clean Air Act. The United States District Court for the Eastern District of Michigan dismissed the criminal indictment on the ground that it was not an \"emission standard,\" but a \"work practice standard,\" which under the terms of the statute, did not carry criminal liability.\n\nThe government appealed and the Sixth Circuit Court of Appeals reversed the decision of the trial court, stating that it erred in determining that it had jurisdiction to review the validity of the standard in a criminal proceeding. Adamo's attorneys appealed to the Supreme Court.\n\nOn January 10, 1978, the Supreme Court ruled in favor of Adamo when it held that the trial court did have jurisdiction to review the standard in a criminal proceeding and also agreed with the trial court that the requirements in the act were \"not standards\" but \"procedures\" and therefore the proceedings were properly dismissed.\n\nA federal grand jury indicted W. R. Grace and Company and seven top executives on February 5, 2005, for its operations of a vermiculite mine in Libby, Montana. The indictment accused Grace of wire fraud, knowing endangerment of residents by concealing air monitoring results, obstruction of justice by interfering with an Environmental Protection Agency (EPA) investigation, violation of the Clean Air Act, providing asbestos materials to schools and local residents, and conspiracy to release asbestos and cover up health problems from asbestos contamination. The Department of Justice said 1,200 residents had developed asbestos-related diseases and some had died, and there could be many more injuries and deaths.\n\nOn June 8, 2006, a federal judge dismissed the conspiracy charge of \"knowing endangerment\" because some of the defendant officials had left the company before the five-year statute of limitations had begun to run. The wire fraud charge was dropped by prosecutors in March.\n\nOn April 2, 1998, three men were indicted in a conspiracy to use homeless men for illegal asbestos removal from an aging Wisconsin manufacturing plant. Then-US Attorney General Janet Reno said, \"Knowingly removing asbestos improperly is criminal. Exploiting the homeless to do this work is cruel.\"\n\nOn December 12, 2004, owners of New York asbestos abatement companies were sentenced to the longest federal jail sentences for environmental crimes in U.S. history, after they were convicted on 18 counts of conspiracy to violate the Clean Air Act and the Toxic Substances Control Act, and actual violations of the Clean Air Act and Racketeer-Influenced and Corrupt Organizations Act. The crimes involved a 10-year scheme to illegally remove asbestos. The RICO counts included obstruction of justice, money laundering, mail fraud and bid rigging, all related to the asbestos cleanup.\n\nOn January 11, 2006, San Diego Gas & Electric Co., two of its employees, and a contractor were indicted by a federal grand jury on charges that they violated safety standards while removing asbestos from pipes in Lemon Grove, California. The defendants were charged with five counts of conspiracy, violating asbestos work practice standards and making false statements.\n"}
{"id": "59060818", "url": "https://en.wikipedia.org/wiki?curid=59060818", "title": "Audacious Inquiry", "text": "Audacious Inquiry\n\nAudacious Inquiry (Ai) is a health information technology company providing cloud-based software to enable secure exchange of actionable data among healthcare organizations. The company also delivers IT consulting and advisory services to advance healthcare interoperability and public health.\n\nAudacious Inquiry assisted Johns Hopkins Medicine, University of Maryland Medical System, MedStar Health, Erickson Living, and the Maryland Department of Health in the establishment of Chesapeake Regional Information System for our Patients (CRISP), which is widely considered one of the most successful health information exchange organizations in the United States. Ai developed the Encounter Notification Service, an event-driven model for standards-based health information exchange, in partnership with CRISP. Ai now supports or directly operates similar, \"connected healthcare\" efforts in several other US states and regions. \n\nAudacious Inquiry developed the Patient Unified Lookup System for Emergencies (PULSE) and now partners with the Sequoia Project to enable PULSE as a nationwide disaster response solution. In 2017 and 2018, providers used PULSE during the California wildfires.\n\nAudacious Inquiry is a certified B Corporation. In 2017, Ai received outside investment from ABS Capital Partners. Ai graduated from the Maryland Center for Entrepreneurship, and the company is headquartered in the BWtech Research Park on the campus of University of Maryland, Baltimore County.\n\nAudacious Inquiry was founded in 2004. The company endured the Great Recession, and managed some growth through that period.\n\n \n"}
{"id": "18208725", "url": "https://en.wikipedia.org/wiki?curid=18208725", "title": "Biorisk", "text": "Biorisk\n\nBiorisk generally refers to the risk associated with biological materials and/or infectious agents. The term has been used frequently for various purposes since the early 1990s. The term is used by regulators, laboratory personnel and industry alike and is used by WHO. WHO/Europe also provides tools and training courses in biosafety and biosecurity.\n\nAn international Laboratory Biorisk Management Standard developed under the auspices of the European Committee for Standardization, defines biorisk as the combination of the probability of occurrence of harm and the severity of that harm where the source of harm is a biological agent or toxin. The source of harm may be an unintentional exposure, accidental release or loss, theft, misuse, diversion, unauthorized access or intentional unauthorized release.\n\nIn Norway, \"Biorisk\" is trademarked by the Norwegian accredited registrar DNV.\n\n"}
{"id": "54619154", "url": "https://en.wikipedia.org/wiki?curid=54619154", "title": "Breastfeeding and medications", "text": "Breastfeeding and medications\n\nBreastfeeding and medications is the description of the medications that can be used by a breastfeeding mother with no or few consequences and those medications which are recommended to be avoided. Some medications are excreted in breastmilk. Almost all medicines pass into breastmilk in small amounts. Some have no effect on the baby and can be used while breastfeeding. The National Institutes of Medicine (US) maintains a database containing information on drugs and other chemicals to which breastfeeding mothers may be exposed. It includes information on the levels of such substances in breast milk and infant blood, and the possible adverse effects in the nursing infant. Suggested therapeutic alternatives to those drugs are provided, where appropriate. All data are derived from reliable sources. Some medications and herbal supplements can be of concern. This can be because the drug can accumulate in breastmilk or have effects on the infant and the mother. Those medications of concern are those medications used to treat substance and alcohol addiction. Other medications of concern are those that are used in smoking cessation. Pain medications and antidepressants need evaluation. \n\nThe determination of the safety of a medication can be evaluated by considering the following:\n\nDrugs can be categorised in one of five categories to determine how safe they are for breastfeeding:\n\n\"Drug which has been taken by a large number of breastfeeding mothers without any observed increase in adverse effects in the infant. Controlled studies in breastfeeding women fail to demonstrate a risk to the infant and the possibility of harm to the breastfeeding infant is remote; or the product is not orally bioavailable in an infant.\"\n\n\"Drug which has been studied in a limited number of breastfeeding women without an increase in adverse effects in the infant. And/or, the evidence of a demonstrated risk which is likely to follow use of this medication in a breastfeeding woman is remote.\"\n\n\"There are no controlled studies in breastfeeding women, however the risk of untoward effects to a breastfed infant is possible; or, controlled studies show only minimal non-threatening adverse effects. Drugs should be given only if the potential benefit justifies the potential risk to the infant. (New medications that have absolutely no published data are automatically categorized in this category, regardless of how safe they may be.)\"\n\n\"There is positive evidence of risk to a breastfed infant or to breastmilk production, but the benefits of use in breastfeeding mothers may be acceptable despite the risk to the infant (e.g. if the drug is needed in a life-threatening situation or for a serious disease for which safer drugs cannot be used or are ineffective).\"\n\n\"Studies in breastfeeding mothers have demonstrated that there is significant and documented risk to the infant based on human experience, or it is a medication that has a high risk of causing significant damage to an infant. The risk of using the drug in breastfeeding women clearly outweighs any possible benefit from breastfeeding. The drug is contraindicated in women who are breastfeeding an infant.\"\n\nOver the counter medications are those medications that do not require a prescription to purchase in the US. Medications that require a prescription to purchase in the US may be available in other countries without a prescription. The following guidelines are recommended:\n\n\nOther substances or chemicals have been evaluated regarding their safe use during pregnancy. Hair dye or solutions used for a 'permanent' do not pass to breastmilk. No adverse reports of using oral antihistamines and breastfeeding are found. Some of the older antihistamines used by a nursing mother can cause drowsiness in the infant. This may be a concern if the infant misses feedings by sleeping instead of nursing.\n\n"}
{"id": "40687401", "url": "https://en.wikipedia.org/wiki?curid=40687401", "title": "Cambridge Pulmonary Hypertension Outcome Review", "text": "Cambridge Pulmonary Hypertension Outcome Review\n\nThe Cambridge Pulmonary Hypertension Outcome Review (CAMPHOR) is a disease specific patient-reported outcome measure which assesses quality of life of patients with pulmonary hypertension (PH). It was the first pulmonary hypertension specific questionnaire for assessing patient reported symptoms, quality of life and functioning.\n\nThe CAMPHOR questionnaire was developed by Galen Research in 2006 to allow for cost-utility analyses for treatments of PH. The theoretical basis for the CAMPHOR is the needs-based model of quality of life, which states that quality of life is highest when an individual has the ability and capacity to satisfy their own needs.\n\nThe CAMPHOR is made up of 3 main dimensions which assess symptoms, functioning and quality of life (QoL). The symptom dimension is made up of 25 symptoms and is broken up into 3 subscales: energy, breathlessness and mood. The QoL scale has 25 items which focus on socialization, role, acceptance, self-esteem, independence, and security. The activity scale has 15 items. Response options include true and not true. Scores for QoL and symptoms range from 0-25, with higher scores indicating worse quality of life. Activity scores range from 0-30, with higher scores indicating more physical limitations.\n\nSince the development of the CAMPHOR, it has been translated and validated into fourteen different languages, including Australian and New Zealand English, German and Swedish.\n\nThe Cambridge Pulmonary Hypertension Outcome Review has been a useful tool in clinical trials as it allows researchers to assess whether new medication or therapy is effective. The CAMPHOR has been utilized in clinical trials which investigate the effects of treprostinil, as well as trials which investigate sildenafil.\n"}
{"id": "2331004", "url": "https://en.wikipedia.org/wiki?curid=2331004", "title": "Cannabis and religion", "text": "Cannabis and religion\n\nDifferent religions have varying stances on the use of cannabis, historically and presently. In ancient history some religions used cannabis as an entheogenic, particularly in the Indian subcontinent where the tradition continues on a more limited basis.\n\nIn the modern era, religions with prohibitions against intoxicants, such as Islam, Buddhism, Bahai, Latter-day Saints (Mormons), and others have opposed the use of cannabis by members, or in some cases opposed the liberalization of cannabis laws. Other groups, such as some Protestant and Jewish factions, have supported the use of medicinal cannabis.\n\nIn the Bahá'í Faith, use of alcohol and other drugs for intoxication, as opposed to medical prescription, is prohibited, see Bahá'í laws. But Bahá'í practice is such laws should be applied with \"tact and wisdom\". The use of tobacco is an individual decision, it is yet strongly frowned on and not explicitly forbidden. Bahá'í authorities have spoken against intoxicant drugs since the earliest stages of the religion, with ‘Abdu’l-Bahá writing:\n\nIn Buddhism, the Fifth Precept is frequently interpreted to mean \"refrain from intoxicating drinks and drugs which lead to heedlessness\", although in some direct translations, the Fifth Precept refers specifically to alcohol. Cannabis and some other psychoactive plants are specifically prescribed in the Mahākāla Tantra for medicinal purposes.\n\nPrior to assuming his position as leader of the Catholic Church, Pope Francis had spoken against recreational cannabis. He stated in 2013 in Buenos Aires: \"A reduction in the spread and influence of drug addiction will not be achieved by a liberalization of drug use.\" The catechism of the Catholic Church states that \"The use of drugs inflicts very grave damage on health and life. Their use, except on strictly therapeutic grounds, is a grave offense.\"\n\nIn The Church of Jesus Christ of Latter-day Saints, there is general prohibition against intoxicating substances. In August 1915, the LDS Church banned the use of cannabis by its members. In 2016, the church's First Presidency urged members to oppose legalization of recreational cannabis use. The LDS Church says it has \"raised no objection to SB 89\" (non-psychoactive medical marijuana in Utah).\n\nThe Georgian Orthodox Church has resisted legalization of cannabis in that country.\n\nThe Arkansas Baptist State Convention voted to discourage medical marijuana in 2016. In 2016, the executive director of the Florida Baptist Convention, Tommy Green, also said that congregations should be encouraged to vote against expanded legalization of medical marijuana in Florida. The \nNational Evangelical Association of Belize opposed the 2017 decriminalization of cannabis in Belize.\n\nThe Assemblies of God USA, as well as other Pentecostal and holiness churches, have historically advocated abstinence from all alcohol, tobacco, and narcotics. Supporters of this view generally cite biblical passages enjoining respect for one's body as well as forbidding intoxication.\n\nOther Protestant churches have endorsed the legality of medical marijuana, including the Presbyterian Church (USA), United Methodist Church, United Church of Christ, and the Episcopal Church.\n\nDuring the Hindu festival of Holi, people consume \"bhang\" which contains cannabis flowers. According to one description, when the \"amrita\" (elixir of life) was produced from the churning of the ocean by the devas and the asuras, Shiva created cannabis from his own body to purify the elixir (whence, for cannabis, the epithet \"angaja\" or \"body-born\"). Another account suggests that the cannabis plant sprang up when a drop of the elixir dropped on the ground. Thus, cannabis is used by sages due to association with elixir and Shiva. \nIn Hinduism, wise drinking of \"bhang\" (which contains cannabis), according to religious rites, is believed to cleanse sins, unite one with Shiva and avoid the miseries of hell in the future life. It is also believed to have medicinal benefits. In contrast, foolish drinking of \"bhang\" without rites is considered a sin.\n\nThe Quran does not directly forbid cannabis; however, cannabis is deemed to be \"khamr\" (an intoxicant) by many religious scholars and therefore generally believed to be \"haraam\" (forbidden). A hadith by the prophet Mohammed states: \"If much intoxicates, then even a little is haraam.\" Despite these prohibitions, cannabis is consumed in many parts of the Islamic world, even sometimes in a religious context particularly within the Sufi mystic movement. In 1378 Soudoun Sheikouni, the Emir of the Joneima in Arabia, prohibited cannabis, considered one of the world's first-attested cannabis bans.\n\nThe Sufi tradition attributes the discovery of cannabis to Jafar Sharazi (Sheikh Haydar), a Sufi leader in the 12th century. Other Sufis attribute its origin to the apocryphal Khidr (\"Green Man\").\n\nSome modern Islamic leaders state that medical cannabis, but not recreational, is permissible in Islam. Imam Mohammad Elahi in Dearborn Heights, Michigan (United States), declared: \"Obviously, smoking marijuana for fun is wrong... It should be permissible only if that is the only option in a medical condition prescribed by medical experts.\"\n\nThough the argument has not been accepted by mainstream scholars, some writers have theorized that cannabis may have been used ritually in early Judaism, though these claims \"have been widely dismissed as erroneous\". Sula Benet (1967) claimed that the plant \"kaneh bosm קְנֵה-בֹשֶׂם\" mentioned five times in the Hebrew Bible, and used in the holy anointing oil of the Book of Exodus, was in fact cannabis, although lexicons of Hebrew and dictionaries of plants of the Bible such as by Michael Zohary (1985), Hans Arne Jensen (2004) and James A. Duke (2010) and others identify the plant in question as either \"Acorus calamus\" or \"Cymbopogon citratus\".\n\nIn the modern era, Orthodox rabbi Moshe Feinstein stated in 1973 that cannabis was not permitted under Jewish law, due to its harmful effects. However Orthodox rabbis Efraim Zalmanovich (2013) and Chaim Kanievsky (2016) stated that medical, but not recreational, cannabis is kosher.\n\nIt is not known when Rastafari first claimed cannabis to be sacred, but it is clear that by the late 1940s Rastafari was associated with cannabis smoking at the Pinnacle community of Leonard Howell. Rastafari see cannabis as a sacramental and deeply beneficial plant that is the Tree of Life mentioned in the Bible. Peter Tosh, among many others, has quoted , \"... the herb is the healing of the nations.\" The use of cannabis, and particularly of long-stemmed water-pipes called chalices, is an integral part of what Rastafari call \"reasoning sessions\" where members join together to discuss life according to the Rasta perspective. They see the use of cannabis as bringing them closer to God (Jah), allowing the user to penetrate the truth of things more clearly.\n\nWhile it is not necessary to use cannabis to be a Rastafari, many use it regularly as a part of their faith, and pipes of cannabis are dedicated to His Imperial Majesty Haile Selassie I before being smoked. According to the anti-cult group the Watchman Fellowship \"The herb is the key to new understanding of the self, the universe, and God. It is the vehicle to cosmic consciousness\" and is believed to burn the corruption out of the human heart. Rubbing the ashes into the skin from smoked cannabis is also considered a healthy practice.\n\nPart of the Rastafari movement, elders of the 20th-century religious movement known as the Ethiopian Zion Coptic Church, consider cannabis to be the \"eucharist\", claiming it as an oral tradition from Ethiopia dating back to the time of Christ.\n\nScientology opposes the use of cannabis, and made \"Truth About Marijuana\" the focus of their 2016 World Health Day presentation.\n\nIn Sikhism, the First Sikh Guru, Guru Nanak, stated that using any mind altering substance (without medical purposes) is a distraction to keeping the mind clean of the name of God. According to the \"Sikh Rehat Maryada\", \"A Sikh must not take hemp (cannabis), opium, liquor, tobacco, in short any intoxicant. His only routine intake should be food and water\".\n\nHowever, there exists a tradition of Sikhs using edible cannabis, often in the form of the beverage bhang, particularly among the Sikh community known as Nihang.\n\nBeginning around the 4th century, Taoist texts mentioned using cannabis in censers. Needham cited the (ca. 570 AD) Taoist encyclopedia \"Wushang Biyao\" 無上秘要 (\"Supreme Secret Essentials\") that cannabis was added into ritual incense-burners, and suggested the ancient Taoists experimented systematically with \"hallucinogenic smokes\". The \"Yuanshi shangzhen zhongxian ji\" 元始上真眾仙記 (\"Records of the Assemblies of the Perfected Immortals\"), which is attributed to Ge Hong (283-343), says:\n\nLady Wei Huacun 魏華存 (252-334) and Xu Mi 許謐 (303-376) founded the Taoist Shangqing School. The Shangqing scriptures were supposedly dictated to Yang Xi 楊羲 (330-c. 386) in nightly revelations from immortals, and Needham proposed Yang was \"aided almost certainly by cannabis\". The \"Mingyi bielu\" 名醫別錄 (\"Supplementary Records of Famous Physicians\"), written by the Taoist pharmacologist Tao Hongjing (456-536), who also wrote the first commentaries to the Shangqing canon, says, \"Hemp-seeds (麻勃) are very little used in medicine, but the magician-technicians (\"shujia\" 術家) say that if one consumes them with ginseng it will give one preternatural knowledge of events in the future.\" A 6th-century AD Taoist medical work, the \"Wuzangjing\" 五臟經 (\"Five Viscera Classic\") says, \"If you wish to command demonic apparitions to present themselves you should constantly eat the inflorescences of the hemp plant.\"\n\nJoseph Needham connected myths about Magu, \"the Hemp Damsel\", with early Daoist religious usages of cannabis, pointing out that Magu was goddess of Shandong's sacred Mount Tai, where cannabis \"was supposed to be gathered on the seventh day of the seventh month, a day of seance banquets in the Taoist communities.\"\n\nOther religions have been founded in the past century that treat cannabis as a sacrament. They include the Santo Daime church, the THC Ministry, Cantheism, the Cannabis Assembly, the Church of Cognitive Therapy (COCT Ministry), Temple 420, Green Faith Ministries, the Church of Cognizance, the Church of the Universe, the Free Marijuana Church of Honolulu, the First Cannabis Church of Florida World Wide, the Free Life Ministry Church of Canthe, the Church of Higher Consciousness, and the federally tax-exempt inFormer Ministry Collective of Palms Springs, CA. The Temple of the True Inner Light believes that cannabis is one of the parts of God's body, along with the classical psychedelics: mescaline, psilocybin, LSD, and DMT. The First Church of Cannabis Inc. officially gained legal recognition in Indiana in 2015 following the passage of that state's Religious Freedom Restoration Act. Nonprofit religious organization Elevation Ministries opened its Denver headquarters, known as the International Church of Cannabis, on April 20, 2017.\n\nModern spiritual figures like Ram Dass openly acknowledge that the use of cannabis has allowed them to gain a more spiritual perspective and use the herb frequently for both its medicinal and mind-altering properties.\n\nIn Mexico, followers of the growing cult of Santa Muerte regularly use marijuana smoke in purification ceremonies, with marijuana often taking the place of incense used in mainstream Catholic rituals.\n\n\n\n"}
{"id": "10828129", "url": "https://en.wikipedia.org/wiki?curid=10828129", "title": "Childbirth positions", "text": "Childbirth positions\n\nThe term childbirth positions (or \"maternal birthing position\") refers to the physical postures the pregnant mother may assume during the process of childbirth. They may also be referred to as delivery positions or labor positions.\n\nIn addition to the lithotomy position, still commonly used by many obstetricians, other positions are successfully used by midwives and traditional birth-attendants around the world. Engelmann's seminal work \"Labor among primitive peoples\" publicised the childbirth positions amongst primitive cultures to the Western world. They frequently use squatting, standing, kneeling and all fours positions, often in a sequence. They are referred to as upright birth positions\n\nVarious people have promoted the adoption of upright birthing positions, particularly squatting, for Western countries, such as Grantly Dick-Read, Janet Balaskas, Moysés Paciornik and Hugo Sabatino. The adoption of the non-lithotomy positions is also promoted by the natural childbirth movement.\n\nBeing upright during labour and birth can increase the available space within the pelvis by 28-30% giving more room to the baby for rotation and descent. There is also a 54% decreased incidence of foetal heart rate abnormalities when the mother is upright. These birthing positions can also reduce the duration of the second stage of labour as well as reduce the risk for emergency caesarian sections by 29%. They are also associated with the lower need for epidural.\n\nDifferent positions may be associated with different rates of perineal injury.\n\nThe squatting position gives a greater increase of pressure in the pelvic cavity with minimal muscular effort. The birth canal will open 20 to 30% more in a squat than in any other position. It is recommended for the second stage of childbirth.\n\nAs most Western adults find it difficult to squat with heels down, compromises are often made such as putting a support under the elevated heels or another person supporting the squatter.\n\nIn ancient Egypt, women delivered babies while squatting on a pair of bricks, known as \"birth bricks\".\n\nSome mothers may choose the all-fours position instinctively. It can help the baby turn around in the case of a malpresentation of the head. Since this position uses gravity, it decreases back pain, as the mother is able to tilt her hips.\n\nSide lying may help slow the baby's descent down the birth canal, thereby giving the perineum more time to naturally stretch. To assume this position, the mother lies on her side with her knees bent. To push, a slight rolling movement is used such that the mother is propped up on one elbow is needed, while one leg is held up. This position does not use gravity but still holds an advantage over the lithotomy position, as it does not position the vena cava under the uterus, which decreases blood flow to mother and child.\n\nIn the lithotomy position, the mother is lying on her back with her legs up in stirrups and her buttocks close to the edge of the table. This position is convenient for the caregiver because it permits him or her more access to the perineum. However, this is not a comfortable position for most patients, considering the pressure on the vaginal walls because the baby's head is uneven and the labor process is working against gravity.\n"}
{"id": "22273190", "url": "https://en.wikipedia.org/wiki?curid=22273190", "title": "Cuban medical internationalism", "text": "Cuban medical internationalism\n\nCuban medical internationalism is the Cuban programme, since the 1959 Cuban Revolution, of sending Cuban medical personnel overseas, particularly to Latin America, Africa and, more recently, Oceania, and of bringing medical students and patients to Cuba. In 2007, \"Cuba has 42,000 workers in international collaborations in 103 different countries, of whom more than 30,000 are health personnel, including no fewer than 19,000 physicians.\" Cuba provides more medical personnel to the developing world than all the G8 countries combined, although this comparison does not take into account G8 development aid spent on developing world healthcare. The Cuban missions have had substantial positive local impact on the populations served. It is widely believed that medical workers are Cuba's most important export commodity.\n\nIn 2007, one academic study on Cuban internationalism surveyed the history of the programme, noting its broad sweep: \"Since the early 1960s, 28,422 Cuban health workers have worked in 37 Latin American countries, 31,181 in 33 African countries, and 7,986 in 24 Asian countries. Throughout a period of four decades, Cuba sent 67,000 health workers to structural cooperation programs, usually for at least two years, in 94 countries ... an average of 3,350 health workers working abroad every year between 1960 and 2000.\"\n\nThe programme was initiated in 1963 as part of Cuba's foreign policy of supporting anti-colonial struggles. It began when Cuba sent a small medical brigade to Algeria, which suffered from the mass withdrawal of French medical personnel during the independence war; some wounded soldiers and war orphans were also transported back to Cuba for treatment. Cuba did this at a time when, following the Cuban revolution, \"half of the country’s 6,000 doctors fled\". Between 1966 and 1974, Cuban doctors worked alongside Cuban artillery in Guinea-Bissau during its independence war against Portugal. Cuba's largest foreign campaign was in Angola: within two years of the campaign, by 1977, \"only one Angolan province out of sixteen was without Cuban health technicians.\" After 1979, Cuba developed a strong relationship with Nicaragua.\n\nHowever, alongside internationalism driven by foreign policy objectives, humanitarian objectives also played a role, with medical teams despatched to countries governed by ideological foes. For example, in 1960, 1972 and 1990 it dispatched emergency assistance teams to Chile and Nicaragua, and Iran following earthquakes. Similarly, Venezuela's Mission Barrio Adentro programme grew out of the emergency assistance provided by Cuban doctors in the wake of the December 1999 mudslides in Vargas state, which killed 20,000 people.\n\nCuban medical missions were sent to Honduras, Guatemala and Haiti following 1998's Hurricane Mitch and Hurricane Georges, and remained there semi-permanently. This has been part of a dramatic expansion of Cuba’s international cooperation in health since 1998. The number of Cuban doctors working abroad jumped from about 5000 in 2003 to more than 25,000 in 2005.\n\nIn Honduras the medical personnel had a substantial impact: \"In the areas they served, infant mortality rates were reduced from 30.8 to 10.1 per 1,000 live births and maternal mortality rates from 48.1 to 22.4 per 1,000 live births between 1998 and 2003.\" However, as one academic paper notes, \"The idea of a nation saving lives and improving the human condition is alien to traditional statecraft and is therefore discounted as a rationale for the Cuban approach.\" In 2004 the 1700 medical personnel in Guatemala received the Order of the Quetzal, the country's highest state honour. A 2005 attempt by Honduras to expel the Cuban mission on the basis that it was threatening Honduran jobs was successfully resisted by trade unions and community organisations.\n\nFollowing the 2004 Asian tsunami, Cuba sent medical assistance to Banda Aceh and Sri Lanka. In response to Hurricane Katrina, Cuba prepared to send 1500 doctors to the New Orleans; the offer was refused. Several months later the mission was dispatched to Pakistan following the 2005 Kashmir earthquake there. Ultimately Cuba sent \"more than 2,500 disaster response experts, surgeons, family doctors, and other health personnel\", who stayed through the winter for more than 6 months. Cuba is helping with the medical crisis in Haiti due to the 2010 Haiti earthquake. All 152 Cuban medical and educational personnel in the Haitian capital Port-au-Prince at the time of the earthquake were reported to be safe, with two suffering minor injuries. In 2014, Cuba sent 103 nurses and 62 doctors to help fight the Ebola virus epidemic in West Africa, the biggest contribution of health care staff by any single country.\n\nCuba's largest and most extensive medical aid effort is with Venezuela. The program grew out of the emergency assistance provided by Cuban doctors in the wake of the December 1999 mudslides in Vargas state, which killed 20,000 people. Under this bilateral effort, also known as the \"oil for doctors\" program, Cuba provides Venezuela with 31,000 Cuban doctors and dentists and provides training for 40,000 Venezuelan medical personnel. In exchange, Venezuela provides Cuba with 100,000 barrels of oil per day. Based on February 2010 prices, the oil is worth $7.5 million per day, or nearly $3 billion per year.\n\nCuba has also sent notable missions to Bolivia (particularly since the 2005 election of Evo Morales) and South Africa, the latter in particular after a post-apartheid brain drain of white doctors. Since 1995, a co-operation agreement with South Africa has seen hundreds of Cuban doctors practice in South Africa, while South Africa sends medical students to Cuba to study. In 2012, the two governments signed another deal, increasing numbers on both sides. South African can now send 1 000 students to Cuba for training which, South Africa believes, will help train the doctors it so desperately needs for the implementation of its National Health Insurance Scheme. After the 1999 violence in East Timor, the country of a million people was left with only 35 physicians and 75% of its population displaced. The number later increased to 79 physicians by 2004, and Cuba sent an additional 182 physicians and technicians.\n\n\"From 1963 to 2004, Cuba was involved in the creation of nine medical faculties in Yemen, Guyana, Ethiopia, Guinea-Bissau, Uganda, Ghana, Gambia, Equatorial Guinea, and Haiti.\"\n\nIn the 2000s, Cuba began establishing or strengthening relations with Pacific Island countries, and providing medical aid to those countries. Cuba's medical aid to Pacific countries has been two-pronged, consisting in sending doctors to Oceania, and in providing scholarships for Pacific students to study medicine in Cuba at Cuba's expense.\n\nThere are currently sixteen doctors providing specialised medical care in Kiribati, with sixteen more scheduled to join them. Cubans have also offered training to I-Kiribati doctors. Cuban doctors have reportedly provided a dramatic improvement to the field of medical care in Kiribati, reducing the child mortality rate in that country by 80%, and winning the proverbial hearts and minds in the Pacific. In response, the Solomon Islands began recruiting Cuban doctors in July 2007, while Papua New Guinea and Fiji considered following suit.\n\nIn 2008, Cuba was due to send doctors to the Solomon Islands, Vanuatu, Tuvalu, Nauru and Papua New Guinea, while seventeen medical students from Vanuatu would study in Cuba. It was reported that it might also provide training for Fiji doctors.\n\nAs of September 2008, fifteen Cuban doctors were serving in Kiribati, sixty-four Pacific students were studying medicine in Cuba, and Cuban authorities were offering \"up to 400 scholarships to young people of that region\". Among those sixty-four students were twenty-five Solomon Islanders, twenty I-Kiribati, two Nauruans and seventeen ni-Vanuatu. Pacific Islanders have been studying in Cuba since 2006.\n\nIn June 2009, Prensa Latina reported that Cuban doctors had \"inaugurated a series of new health services in Tuvalu\". One Cuban doctor had been serving in Tuvalu since October 2008, and two more since February 2009. They had reportedly \"attended 3,496 patients, and saved 53 lives\", having \"opened ultrasound and abortion services, as well as specialized consultations on hypertension, diabetes, and chronic diseases in children\". They had visited all the country's islands, and were training local staff in \"primary health care, and how to deal with seriously ill patients, among other subjects\".\n\nMissions abroad are intended to provide services at low cost to the host country. \"Patients are not charged for services, and the recipient countries are expected to cover only the cost of collective housing, air fare, and limited food and supplies not exceeding $200 a month. While\nCuban doctors are abroad, they continue to receive their salaries as well as a stipend in foreign currency.\" In 2008, the pay for Cuban doctors abroad was 183$ per month, whereas the pay for doctors working domestically was 23$ per month. \nA new term - \"disaster tourism\" - has arisen in response to a growing number of large-scale natural disasters. This phrase refers to individuals, governments and organizations who travel to a disaster area with the primary goal of having an \"experience\" rather than providing meaningful aid. Such aid is often short-lived, and may even get in the way of more serious rescue efforts. Cuban medical internationalism represents a polar opposite to this disaster tourism mentality, with a focus on large-scale, sustained aid targeting the most marginalized and under-served populations across the globe. \n\nSince 1990, Cuba has provided long-term care for 18,000 victims of the Chernobyl disaster, \"offering treatment for hair loss, skin disorders, cancer, leukemia, and other illnesses attributed to radioactivity.\"\n\nIn response to the 1998 Hurricane Mitch, Cuba set up the \"Escuela Latinoamericana de Medicina\" (\"Latin American School of Medicine\"; ELAM) outside Havana, converted from a former naval base. It accepts around 1500 students per year. ELAM forms part of a range of medical education and training initiatives; \"Cubans, with the help of Venezuela, are currently educating more doctors, about 70,000 in all, than all the medical schools in the United States, which typically have somewhere between 64,000 to 68,000 students enrolled in their programs.\"\n\nFollowing the development of cooperation with Venezuela through Mission Barrio Adentro, Mission Milagro / Operación Milagro was set up to provide ophthalmology services to Cuban, Venezuelan and Latin American patients, both in Cuba and in other countries. \"As of August 2007, Cuba had performed over 750,000 eye surgeries, at no cost, including 113,000 surgeries for its own citizens.\"\n\nAlthough humanitarian principles figure, ideological factors were prominent in Cuba's \"doctor diplomacy\", particularly during the Cold War. Subsequently, its continuation has been seen as a vital means to promote Cuba's image abroad and prevent international isolation. For Cuba's re-establishment of diplomatic relations with Honduras in 2002, Cuba's health missions in that country were \"undoubtedly a deciding factor\"; Guatemala reestablished diplomatic relations with Cuba in 1998.\n\nAt the 2009 5th Summit of the Americas, U.S. President Barack Obama commented that at the summit he had heard much about the impact of Cuban \"soft diplomacy\" in the form of its medical internationalism. He said this might be a reminder to the United States that limiting its interactions with Latin American countries to military and drug interdiction might be limiting its influence.\n\nIt has also been suggested that Cuban medical internationalism promotes exports of Cuban medical technology, and may be a source of hard currency (although the targeting of poor countries reduces the hard currency potential of missions abroad). In 2006 Cuba's earnings from medical services (including export of doctors) amounted to US$2,312m – 28% of total export receipts and net capital payments. This exceeded earnings from both nickel and cobalt exports and from tourism. These earnings are achieved despite the fact that a substantial part of Cuba's medical internationalism since 1998 has been organised within the framework of the \"Integrated Health Program\" (Programa Integral de Salud, PIS); this cooperation program is free for the receiving country. Cuba's co-operation with Venezuela provides Cuba with cheap oil in exchange for its medical support to Mission Barrio Adentro. Bloomberg reported in March 2014 that Cuban state-controlled media forecasted revenue of $8.2 million that year from the program.\n\nIt has also been argued that the programme has, particularly in the 1980s and 1990s, \"perform[ed] a critical function in consolidating socialist consciousness\" within Cuba.\n\nAlthough Cuba's large-scale medical training programmes and high doctor-patient ratios give it much latitude, the expansion of doctor diplomacy since 2004, particularly with the Barrio Adentro programme, has been dramatic: the number of Cuban doctors working abroad jumped from about 5000 in 2003 to more than 25,000 in 2005. This has had some impact on the domestic health system, for example with increased waiting times, particularly with regard to family doctors. The number of patients per doctor rose from 139 to 179. In March 2008 Cuba announced a reorganisation of its domestic family doctor programme for greater efficiency.\n\nIn 2000, two Cuban doctors working in Zimbabwe attempted to defect to Canada. They were prevented from doing so by two Zimbabwean soldiers, who handed them over to Cuban officials. United Nations officials said Zimbabwe appeared to have violated national and international laws.\n\nIn August 2006 the United States under George W. Bush created the Cuban Medical Professional Parole program, specifically targeting Cuban medical personnel and encouraging them to defect when they are working in a country outside of Cuba. Of an estimated 40,000 eligible medical personnel, over 1000 had entered the United States under the program by October 2007, according to the chief of staff for U.S. Rep. Lincoln Diaz-Balart. The promised fast-track visa is not always forthcoming, with at least one applicant waiting a year for his visa; although according to Dr. Julio Cesar Alfonso of the Cuban dissident organization \"Outside the Barrio,\" the U.S. government has rejected only a handful of the hundreds of applications for visas. On 12 January 2017, President Obama announced the end of the program, saying that both Cuba and the US work to \"combat diseases that endanger the health and lives of our people. By providing preferential treatment to Cuban medical personnel, the medical parole program contradicts those efforts, and risks harming the Cuban people.\"\n\nAccording to a 2007 paper published in \"The Lancet\" medical journal, \"growing numbers of Cuban doctors sent overseas to work are defecting to the USA\", some via Colombia, where they have sought temporary asylum. In February 2007, at least 38 doctors were requesting asylum in the U.S. embassy in Bogotá after asylum was denied by the Colombian government. Cuban doctors working abroad are reported to be monitored by \"minders\" and subject to curfew.\n\nAccording to Luis Zuñiga, director of human rights for the Cuban American National Foundation, Cuban doctors are \"slave workers\" who labor for meager wages while bolstering Cuba's image as a donor nation and \"the Cuban government exports these doctors as merchandise\".\n\nAn article by Laurie Garrett in \"Foreign Affairs\" warns that lifting of the United States trade and travel restrictions on Cuba could have dire consequences for Cuba's healthcare system, leading to an exodus of thousands of well-trained Cuban healthcare professionals. U.S. companies could also transform the remaining healthcare system into a destination for medical tourism. Garrett concludes that if politicians do not take great care, lifting of the restrictions would rob Cuba of its greatest triumph.\n\n\n\n"}
{"id": "12223156", "url": "https://en.wikipedia.org/wiki?curid=12223156", "title": "Dance and health", "text": "Dance and health\n\nDance is an enjoyable health promoting physical activity which many people worldwide incorporate into their lifestyles today. This physical activity appeals to some who may not be active and therefore may be another alternative of exercise. Dance for health has become an important factor in the prevention, treatment and management in several health circumstances. It is not only significant for your physical health but it also contributes to your mental health and subsidizes social communication Dance is an art which is learned from many cultures. Types of dance can entail body movements, expression and collaboration. Dance and health has been subject of a number of research studies that show dance to be a healthy exercise. However, there are a number of health risks that require attention\n\nDancing can be a way to stay fit for people of all ages, shapes and sizes. It has a wide range of physical, and mental benefits including: improved condition of your heart and lungs, increased muscular strength, endurance and motor fitness, increased aerobic fitness, improved muscle tone and strength, weight management, stronger bones and reduced risk of osteoporosis, better coordination, agility and flexibility, improved balance and spatial awareness, increased physical confidence, improved mental functioning, improved general and psychological well being, greater self-confidence and self-esteem, and better social skills.\nMost forms of dance may be considered an aerobic exercise and as such reduce the risk of cardiovascular disease, help weight control, stress reduction, and bring about other benefits commonly associated with physical fitness. In addition, studies have demonstrated a considerable correlation between dancing and psychological well-being. A large amount of governmental, health, and educational information is available extolling the benefits of dance for health.\n\nBenefits of Cultural dance\nPhysical activity has many physical and mental health outcomes. However, physical inactivity continues to be common. Dance, specifically cultural dance, is a type of physical activity that may appeal to some who are not otherwise active and may be a form of activity that is more acceptable than others in certain cultures.\n\nDance pads have proven useful in tackling obesity in young people and are welcomed in many schools for that reason.\n\nA report by Professor Tim Watson and Dr Andrew Garrett of the University of Hertfordshire compared members of the Royal Ballet with a squad of British national and international swimmers. The dancers scored higher than the swimmers in seven out of ten areas of fitness. An Italian study in 2006 has shown that dance is a very good exercise for heart patients compared to other aerobic exercises like cycling. This may be partly because the patients enjoyed it much more.\n\nA study at the Washington University in St. Louis School of Medicine in 2007 showed Argentine tango was better at improving the mobility of Parkinson's disease sufferers than an exercise class (a later study showed similar benefits from Tai chi). Because of the level of interest a permanent tango class was set up after the study ended. A study by Dr Paul Dougall at Strathclyde University in 2010 concentrating on older women found that Scottish country dancers were more agile, have stronger legs and can walk more briskly than people of the same age who took part in exercises such as swimming, walking, golf and keep-fit classes.\n\nAnother gain of dancing is for those who have high cholesterol, plus drugs and adequate food, dancing can draw. As an aerobic exercise abridged levels of total blood cholesterol, especially in LDL cholesterol, acknowledged as bad and helps boost levels of HDL or good cholesterol. Dancing in general increases, muscle strength and flexibility, which in turn, improves overall range of motion. Dance also increases core strength which can improve balance, coordination, and posture (which reduces back pain).\n\nDance has been repeatedly shown to positively impact a person's mental health. For example, lead study author Anna Duberg, of Sweden's Center for Health Care Sciences, found that, \"despite problems such as stress and other potential challenges in being an adolescent girl, dance can result in high adherence and a positive experience for the participants.\" Dancing had the potential to contribute to new healthy habits. Swedish researchers, writing in the JAMA Pediatrics, studied 112 teenage girls who were struggling with problems including neck and back pain, stress, anxiety, and depression. Half of the girls attended weekly dance classes, while the other half didn't. The girls who took the dance classes improved their mental health and reported a boost in mood—positive effects that lasted up to eight months after the classes ended.\n\nAdditionally, a recent study done in Perth Western Australia by Debbie Duignan (WA Alzheimers Association) explored the use of Wu Tao Dance as a therapy for people with dementia. It was shown that Wu Tao dance helped to reduce symptoms of agitation in people with dementia. The complex mental coordination involved with dancing activates both sensory and motor circuits. Therefore, when one dances, one's brain is both stimulated by the sound of the music and by the dance movements themselves. PET imaging has shown brain regions that become activated during dance learning and performance, including the motor cortex, somatosensory cortex, basal ganglia, and cerebellum. The benefits of dancing on the brain includes memory improvement and strengthened neural connections. Consequently, not only can dance help to reduce symptoms experienced by those with dementia, but it can also reduce the risk of developing dementia in the first place, as shown in a 2003 study in the \"New England Journal of Medicine\" by researchers at the Albert Einstein College of Medicine.\n\nIn addition to improving symptoms of dementia and preventing dementia, frequent dancing can even lead to increased cognitive acuity for individuals of all ages. However, not all kinds of dancing have this power. Those dance styles that allow for the most split-second decisions are the most beneficial; those dance styles with the same, memorized patterns are the least beneficial. For the same reason, those who take the Follow role have a higher opportunity for improving their cognitive acuity since they must make constant split-second decisions as they follow their partner’s lead. The key for improving cognitive acuity is to create new neural connections to increase the complexity of our neuronal synapses. Another important consideration is that the frequency of dancing matters. The more frequent an individual dances, the greater the cognitive improvement.\n\nFurthermore, many cultures agree that there is a mind and body connection, and many cultures use dance to heal this often damaged connection. During the African diaspora, individuals used dance therapy to treat the trauma that resided from their situations. Dance therapy is suggested for patients today as treatment for emotional and therapeutic support, as dance allows individuals to connect with their inner-self.\n\nThere are various health risks of professional dance, as it can be very demanding. As well as sports injuries, repetitive strain injury, and chronic workplace stress. Dancers risk injury within the course of their career, many retiring from active performance in their mid to late 30s. Since dance is a performance art with emphasis on aesthetics, dancers are also at a higher risk of body image problems and eating disorders such as anorexia nervosa or bulimia. \nSome dances, such as ballet, are very strenuous on the body. Research shows that dancers in elite pre-professional companies have 1.38 injuries per 1000 hours of dancing, with dancers averaging about 30.3 hours per week. The most common injury was to the lower extremities, with ankle being the most common. The injuries on average took about 7 days to heal with foot injuries taking the longest at 14 days and thigh injuries being the lowest at 2 days. Another risk dancers face are eating disorders. They are constantly judged based on their looks and expected to have toned, slim bodies. This can lead to a lot of health risks.\n\nMany dance movements, and particularly ballet techniques, such as the turnout of the hips and rising on the toes (en pointe), test the limits of the range of movement of the human body. Dance movements can place stress on the body when not performed correctly; even if perfect form is used, over-repetition can cause repetitive strain injury. The most common injury for ballet dancers is snapping hip syndrome Shoulder injuries can be common in dancers due to lifting, partnering and floor work that is commonly seen in modern dance. The periscapular muscles assist in keeping the shoulder still and steady, these muscles are at greater risk for injuries among dancers.\n\nExamined in the \"Journal of Dance Medicine and Science\", dancers often put off consultation from doctors or physical therapists in the effort to stay employed by a dance company or to stay in rehearsals. When in fact those dancers that \"work through\" their pain more often than not end up worsening their symptoms and prolonging their recovery. Eighty percent of professional dancers will be injured in some way during their careers; 50 percent of dancers from large ballet companies and 40 percent from small companies will miss performances due to injury. The practice of \"plieing\" (bending one's knees deeply) after landing each jump may seem innocuous, but failing to do so may result in shin splints or knee injuries.\n\nOverwork and poor occupational health and safety conditions, a (non-sprung) hard floor, a cold studio or theater, or dancing without sufficient warm up also increase risk of injury. To minimize injury, dance training emphasizes strength building and forming appropriate habits. Choreographers and dance instructors will often put certain demands on their students and dancers without taking into consideration that each dancer is faced with different anatomical limitations. Dancers will strive to achieve the ideal aesthetic in their respective dance technique by over compensating for their limitations and thus presenting themselves with a higher risk for injury. Also damage may result from having a student perform movements for which they are not prepared, care must be taken that the student is not \"pushed\" inappropriately.\n\nA dancer put en pointe at an age where his or her bones have not completely ossified may develop permanent damage; even past the point of ossification, ankle injuries can result if a dancer goes en pointe without sufficient strength. Rachele Quested and Anna Brodrick, the lower extremities are the vulnerable to injury. The most common injury is to the ankle, then leg, foot, knee, hip and finally the thigh. Dancers are trained from a very young age to avoid injury by using plie, turn out, and other means to protect their bodies.\n\nKeeping dancers free of injury is a crucial aspect and will help a lifetime of healthy physical activity and can prevent injuries in the future. By being taught a few simple techniques by parents, teachers, and medical professionals can avert injuries from occurring. Following are a few advice's on preventing injuries. Wearing properly fitting clothing and shoes, drink plenty of fluids to stay hydrated, don’t dance through pain, rest and then start back up again and listen to your teachers for correct technique. For social dance the use of a sprung floor is highly recommended. Because a dance injury can ruin a career professional dancers are increasingly refusing to dance on anything else. In ballet good plieing (bending the knees) on landing helps protect against knee injuries and shin splints. Many types of dance, especially folk dances, have hops in the steps where the impact of landing can be reduced by slightly bending the knee. Warming up and cooling down exercises are recommended before and after exercises to avoid strain, muscle pains, and possible injury. Conditioning is a good way to prevent dance injuries.\n\nRICE (Rest, Ice, Compression, Elevation) is generally regarded as a good first aid therapy for most dance injuries before the ambulance comes, or even for what may be thought of as minor injuries. Pain and inflammation can be reduced using a non-steroidal anti-inflammatory drug (NSAID) in a gel applied to the affected area (not on broken skin). Note, however, that masking pain to continue dancing is dangerous as it can easily make an injury worse.\n\nProfessional dancers may experience chronic workplace stress with an uncertain work situation. The average income for a ballet dancer is low, and competition for jobs is very high. In addition to the stress that may be caused by this, dancers also may experience the psychological distress from technical and physical \"perfectionism\". As with other activities (such as horse jockeying) where weight is a factor, dancers are at a higher risk for developing eating disorders such as anorexia and bulimia. Many young dancers, believing that the ideal dancer must be thin, may begin controlling their diets, sometimes obsessively. Such dancers may be unaware of or may choose to ignore the fact that an emaciated dancer will not have the strength required for ballet. It is also highly relevant that inadequate nutrition in adolescent females has been linked to development of scoliosis, due to decreased oestrogen production and subsequent reduced bone density. A dancer with poor nutrition is at a higher risk for injuries and long-term health problems. A malnourished dancer's performance will be altered and weaken as his or her body starts to break down muscle and bone in order to fuel itself. This puts the dancer at risk for injury and slows the healing of chronic injuries. In a survey of 300 professional dancers, 40% were tobacco smokers in contrast with the Center for Disease Control average of 24% of American women and 29% of American men aged 18–34.\n\nDance science is the scientific study of dance and dancers, as well as the practical application of scientific principles to dance. Its aims are the enhancement of performance, the reduction of injury, and the improvement of well-being and health. Dance requires a high degree of interpersonal and motor skills, and yet seems built into humans. It has therefore increasingly become the subject of neurological studies. The July 2008 edition of Scientific American contains a summary of recent studies and further questions.\n\nAn article in Nature 'Dance reveals symmetry especially in young men'\nshows that dance in Jamaica seems to show evidence of sexual selection and to reveal important information about the dancer. Professor Lee Cronk at Rutgers: \"More symmetrical men put on a better show, and women notice.\" Symmetry is a strong indicator of fitness as it shows developmental stability.\n\nDance therapy or dance movement therapy is a form of expressive therapy, the psychotherapeutic use of movement (and dance) for treating emotional, cognitive, social, behavioral and physical conditions. Many professionals specialize in dancer's health such as in providing complementary or remedial training or improving mental discipline.\n\n\n"}
{"id": "172118", "url": "https://en.wikipedia.org/wiki?curid=172118", "title": "Dry cleaning", "text": "Dry cleaning\n\nDry cleaning is any cleaning process for clothing and textiles using a chemical solvent other than water. It is used to clean fabrics that degrade in water, and delicate fabrics that cannot withstand the rough and tumble of a washing machine and clothes dryer. It can eliminate labor-intensive hand washing.\n\nDespite its name, dry cleaning is not a \"dry\" process; clothes are soaked in a liquid solvent. Tetrachloroethylene (perchloroethylene), which the industry calls \"perc\", is the most widely used solvent. Alternative solvents are trichloroethane and petroleum spirits. \n\nOn March 3, 1821, Thomas L. Jennings became the first African-American to be granted a United States patent. The patent was for his invention of a cleaning process called \"dry scouring\", which was the precursor to dry cleaning. The first commercial \"dry laundry\" using turpentine was founded by Jolly Belin in Paris in 1825.\n\nModern dry cleaning's use of non-water-based solvents to remove soil and stains from clothes was reported as early as 1855. The potential for petroleum-based solvents was recognized by French dye-works operator Jean Baptiste Jolly, who offered a new service that became known as \"nettoyage à sec\"—i.e., dry cleaning. Flammability concerns led William Joseph Stoddard, a dry cleaner from Atlanta, to develop Stoddard solvent (white spirit) as a slightly less flammable alternative to gasoline-based solvents. The use of highly flammable petroleum solvents caused many fires and explosions, resulting in government regulation of dry cleaners. After World War I, dry cleaners began using chlorinated solvents. These solvents were much less flammable than petroleum solvents and had improved cleaning power.\n\nBy the mid-1930s, the dry cleaning industry had adopted tetrachloroethylene (perchloroethylene), or PCE for short, as the solvent. It has excellent cleaning power and is nonflammable and compatible with most garments. Because it is stable, tetrachloroethylene is readily recycled.\n\nTraditionally, the cleaning process was carried out at centralized factories. Small, local cleaners' shops received garments from customers, sent them to the factory, and then had them returned to the shop for collection by the customer. This cycle minimized the risk of fire or dangerous fumes created by the cleaning process. At this time, dry cleaning was carried out in two different machines—one for the cleaning process, and the second to remove the solvent from the garments.\n\nMachines of this era were described as \"vented\"; their drying exhausts were expelled to the atmosphere, the same as many modern tumble-dryer exhausts. This not only contributed to environmental contamination but also much potentially reusable PCE was lost to the atmosphere. Much stricter controls on solvent emissions have ensured that all dry cleaning machines in the Western world are now fully enclosed, and no solvent fumes are vented to the atmosphere. In enclosed machines, solvent recovered during the drying process is returned condensed and distilled, so it can be reused to clean further loads or safely disposed of. The majority of modern enclosed machines also incorporate a computer-controlled drying sensor, which automatically senses when all detectable traces of PCE have been removed. This system ensures that only small amounts of PCE fumes are released at the end of the cycle.\n\nIn terms of mechanism, dry cleaning selectively solubilizes stains on the article. The solvents are non-polar and tend to selectively extract compounds that cause stains. These stains would otherwise only dissolve in aqueous detergents mixtures at high temperatures, potentially damaging delicate fabrics.\n\nNon-polar solvents are also good for some fabrics, especially natural fabrics, as the solvent does not interact with any polar groups within the fabric. Water binds to these polar groups which results in the swelling and stretching of proteins within fibers during laundry. Also, the binding of water molecules interferes with weak attractions within the fiber, resulting in the loss of the fiber's original shape. After the laundry cycle, water molecules will dry off. However, the original shape of the fibers has already been distorted and this commonly results in shrinkage. Non-polar solvents prevent this interaction, protecting more delicate fabrics.\n\nThe usage of an effective solvent coupled with mechanical friction from tumbling effectively removes stains.\n\nA dry-cleaning machine is similar to a combination of a domestic washing machine and clothes dryer. Garments are placed in the washing or extraction chamber (referred to as the 'basket' or 'drum'), which constitutes the core of the machine. The washing chamber contains a horizontal, perforated drum that rotates within an outer shell. The shell holds the solvent while the rotating drum holds the garment load. The basket capacity is between about 10 and 40 kg (22 to 88 lb).\n\nDuring the wash cycle, the chamber is filled approximately one-third full of solvent and begins to rotate, agitating the clothing. The solvent temperature is maintained at 30 degrees Celsius (86 degrees Fahrenheit), as a higher temperature may damage it. During the wash cycle, the solvent in the chamber (commonly known as the 'cage' or 'tackle box') is passed through a filtration chamber and then fed back into the 'cage'. This is known as the cycle and is continued for the wash duration. The solvent is then removed and sent to a distillation unit consisting of a boiler and condenser. The condensed solvent is fed into a separator unit where any remaining water is separated from the solvent and then fed into the 'clean solvent' tank. The ideal flow rate is roughly 8 liters of solvent per kilogram of garments per minute, depending on the size of the machine.\n\nGarments are also checked for foreign objects. Items such as plastic pens that may dissolve in the solvent bath, damaging the textiles. Some textile dyes are \"loose\" and will shed dye during solvent immersion. Fragile items, such as feather bedspreads or tasseled rugs or hangings, may be enclosed in a loose mesh bag. The density of perchloroethylene is around 1.7 g/cm at room temperature (70% heavier than water), and the sheer weight of absorbed solvent may cause the textile to fail under normal force during the extraction cycle unless the mesh bag provides mechanical support.\n\nNot all stains can be removed by dry cleaning. Some need to be treated with spotting solvents — sometimes by steam jet or by soaking in special stain-remover liquids — before garments are washed or dry cleaned. Also, garments stored in soiled condition for a long time are difficult to bring back to their original color and texture.\n\nA typical wash cycle lasts for 8–15 minutes depending on the type of garments and degree of soiling. During the first three minutes, solvent-soluble soils dissolve into the perchloroethylene and loose, insoluble soil comes off. It takes 10–12 minutes after the loose soil has come off to remove the ground-in insoluble soil from garments. Machines using hydrocarbon solvents require a wash cycle of at least 25 minutes because of the much slower rate of solvation of solvent-soluble soils. A dry cleaning surfactant \"soap\" may also be added.\n\nAt the end of the wash cycle, the machine starts a rinse cycle where the garment load is rinsed with freshly distilled solvent dispensed from the solvent tank. This pure solvent rinse prevents discoloration caused by soil particles being absorbed back onto the garment surface from the 'dirty' working solvent.\n\nAfter the rinse cycle, the machine begins the extraction process, which recovers the solvent for reuse. Modern machines recover approximately 99.99% of the solvent employed. The extraction cycle begins by draining the solvent from the washing chamber and accelerating the basket to 350–450 rpm, causing much of the solvent to spin free of the fabric. Until this time, the cleaning is done in normal temperature, as the solvent is never heated in dry cleaning process. When no more solvent can be spun out, the machine starts the drying cycle.\n\nDuring the drying cycle, the garments are tumbled in a stream of warm air (60–63 °C/140–145 °F) that circulates through the basket, evaporating traces of solvent left after the spin cycle. The air temperature is controlled to prevent heat damage to the garments. The exhausted warm air from the machine then passes through a chiller unit where solvent vapors are condensed and returned to the distilled solvent tank. Modern dry cleaning machines use a closed-loop system in which the chilled air is reheated and recirculated. This results in high solvent recovery rates and reduced air pollution. In the early days of dry cleaning, large amounts of perchlorethylene were vented to the atmosphere because it was regarded as cheap and believed to be harmless.\nAfter the drying cycle is complete, a deodorizing (aeration) cycle cools the garments and removes further traces of solvent, by circulating cool outside air over the garments and then through a vapor recovery filter made from activated carbon and polymer resins. After the aeration cycle, the garments are clean and ready for pressing and finishing.\n\nWorking solvent from the washing chamber passes through several filtration steps before it is returned to the washing chamber. The first step is a button trap, which prevents small objects such as lint, fasteners, buttons, and coins from entering the solvent pump.\n\nOver time, a thin layer of filter cake (called \"muck\") accumulates on the lint filter. The muck is removed regularly (commonly once per day) and then processed to recover solvent trapped in the muck. Many machines use \"spin disk filters\", which remove the muck from the filter by centrifugal force while it is back washed with solvent.\n\nAfter the lint filter, the solvent passes through an absorptive cartridge filter. This filter, which contains activated clays and charcoal, removes fine insoluble soil and non-volatile residues, along with dyes from the solvent. Finally, the solvent passes through a polishing filter, which removes any soil not previously removed. The clean solvent is then returned to the working solvent tank. Cooked powder residue is the name for the waste material generated by cooking down or distilling muck. It will contain solvent, powdered filter material (diatomite), carbon, non-volatile residues, lint, dyes, grease, soils, and water. The waste sludge or solid residue from the still contains solvent, water, soils, carbon, and other non-volatile residues. Used filters are another form of waste as is waste water.\n\nTo enhance cleaning power, small amounts of detergent (0.5–1.5%) are added to the working solvent and are essential to its functionality. These detergents emulsify hydrophobic soils and keep soil from redepositing on garments. Depending on the machine's design, either an anionic or a cationic detergent is used.\n\nThe international GINETEX laundry symbol for dry cleaning is a circle. It may have the letter P inside it to indicate perchloroethylene solvent, or the letter F to indicate a flammable solvent (Feuergefährliches Schwerbenzin). A bar underneath the circle indicates that only mild cleaning processes is recommended. A crossed-out empty circle indicates that dry cleaning is not permitted.\n\nPerchloroethylene (PCE, or tetrachloroethylene) has been in use since the 1930s. PCE is the most common solvent, the \"standard\" for cleaning performance. It is a most effective cleaning solvent. It is thermally stable, recyclable, and has low toxicity. It can, however, cause color bleeding/loss, especially at higher temperatures. In some cases it may damage special trims, buttons, and beads on some garments. It is better for oil-based stains (which account for about 10% of stains) than more common water-soluble stains (coffee, wine, blood, etc.). The toxicity of tetrachloroethylene \"is moderate to low\" and \"Reports of human injury are uncommon despite its wide usage in dry cleaning and degreasing\".\n\nThe U.S. state of California classified perchloroethylene a toxic chemical in 1991, and its use will become illegal in that state in 2023. However, it is still probably the most universally used dry cleaning solvent, at the present time.\n\nHydrocarbons are represented by products such as Exxon-Mobil's DF-2000 or Chevron Phillips' EcoSolv, and Pure Dry. These petroleum-based solvents are less aggressive but also less effective than PCE. Although combustible, risk of fire or explosion can be minimized when used properly. Hydrocarbons are however pollutants. Hydrocarbons retain about 10-12% of the market.\n\nTrichloroethylene is more aggressive than PCE but is very rarely used. With superior degreasing properties, it was often used for industrial workwear/overalls cleaning in the past. TCE is classified as carcinogenic to humans by the United States Environmental Protection Agency.\n\nSupercritical CO is an alternative to PCE, however it is inferior in removing some forms of grime. Additive surfactants improve the efficacy of CO\nCarbon dioxide is almost entirely nontoxic. The greenhouse gas potential is also lower than that of many organic solvents.\n\n\"Consumer Reports\" rated supercritical CO superior to conventional methods, but the Drycleaning and Laundry Institute commented on its \"fairly low cleaning ability\" in a 2007 report. Supercritical CO is, overall, a mild solvent which lowers its ability to aggressively attack stains.\n\nOne deficiency with supercritical CO is that its conductivity is low. As mentioned in the Mechanisms section, dry cleaning utilizes both chemical and mechanical properties to remove stains. When solvent interacts with the fabric's surface, the friction dislocates dirt. At the same time, the friction also builds up an electrical charge. Fabrics are very poor conductors and so usually, this build-up is discharged through the solvent. This discharge does not occur in liquid carbon dioxide and the build-up of an electrical charge on the surface of the fabric attracts the dirt back on to the surface, which diminishes its poor clearning efficiency. To compensate for the poor solubility and conductivity of supercritical carbon dioxide, research has focused on additives. For increased solubility, 2-propanol has shown increased cleaning effects for liquid carbon dioxide as it increases the ability of the solvent to dissolve polar compounds.\n\nMachinery for use of supercritical CO is expensive — up to $90,000 more than a PCE machine, making affordability difficult for small businesses. Some cleaners with these machines keep traditional machines on-site for the heavier soiled textiles, but others find plant enzymes to be equally effective and more environmentally sustainable.\n\nFor decades, efforts have been made to replace PCE. These alternatives have not proven economical thus far:\n\n\n"}
{"id": "54866211", "url": "https://en.wikipedia.org/wiki?curid=54866211", "title": "Edematous areola", "text": "Edematous areola\n\nAn edematous areola is a swollen and tender inflammation of the areola of the breast. It can develop after childbirth when large amounts of fluids are given intravenously, use of pitocin or fluid retention for other reasons, and may interfere with successful initiation of breastfeeding. An edematous areola can also develop in women with preeclampsia.\n"}
{"id": "5975230", "url": "https://en.wikipedia.org/wiki?curid=5975230", "title": "Ethnic bioweapon", "text": "Ethnic bioweapon\n\nAn ethnic bioweapon (\"biogenetic weapon\") is a type of theoretical bioweapon that aims to harm only or primarily people of specific ethnicities or genotypes.\n\nOne of the first modern fictional discussions of ethnic weapons is in Robert A. Heinlein's 1942 novel \"Sixth Column\" (republished as \"The Day After Tomorrow\"), in which a race-specific radiation weapon is used against a so-called \"Pan-Asian\" invader.\n\nIn 1997, U.S. Secretary of Defense William Cohen referred to the concept of an ethnic bioweapon as a possible risk. In 1998 some biological weapon experts considered such a \"genetic weapon\" plausible, and believed the former Soviet Union had undertaken some research on the influence of various substances on human genes.\n\nIn its 2000 policy paper Rebuilding America's Defenses, think-tank Project for the New American Century (PNAC) described ethnic bioweapons as a potentially \"politically useful tool\". PNAC went on to provide substantial staffing for the Bush Jr administration.\n\nThe possibility of a \"genetic bomb\" is presented in Vincent Sarich's and Frank Miele's book, \"\", published in 2004. These authors view such weapons as technically feasible but not very likely to be used. (page 248 of paperback edition.)\n\nIn 2004, \"The Guardian\" reported that the British Medical Association (BMA) considered bioweapons designed to target certain ethnic groups as a possibility, and highlighted problems that advances in science for such things as \"treatment to Alzheimer's and other debilitating diseases could also be used for malign purposes\".\n\nIn 2005, the official view of the International Committee of the Red Cross was \"The potential to target a particular ethnic group with a biological agent is probably not far off. These scenarios are not the product of the ICRC's imagination but have either occurred or been identified by countless independent and governmental experts.\"\n\nIn 2008, the US government held a congressional committee, ‘Genetics and other human modification technologies: sensible international regulation or a new kind of arms race?’, during which it was discussed how “we can anticipate a world where rogue (and even not-so-rogue) states and non-state actors attempt to manipulate human genetics in ways that will horrify us”.\n\nIn 2012, \"The Atlantic\" wrote that a specific virus that targets individuals with a specific DNA sequence is within possibility in the near future. The magazine put forward a hypothetical scenario of a virus which caused mild flu to the general population but deadly symptoms to the President of the United States. They cite advances in personalized gene therapy as evidence.\n\nIn 2016, \"Foreign Policy\" magazine suggested the possibility of a virus used as an ethnic bioweapon that could sterilize a \"genetically-related ethnic population.\"\n\nIn November 1998, \"The Sunday Times\" reported that Israel was attempting to build an \"ethno-bomb\" containing a biological agent that could specifically target genetic traits present amongst Arab populations. \"Wired News\" also reported the story, as did \"Foreign Report\".\n\nMicrobiologists and geneticists were skeptical towards the scientific plausibility of such a biological agent. The \"New York Post\", describing the claims as \"blood libel\", reported that the likely source for the story was a work of science fiction by Israeli academic Doron Stanitsky. Stanitsky had sent his completely fictional work about such a weapon to Israeli newspapers two years before. The article also noted the views of genetic researchers who claimed the idea as \"wholly fantastical\", with others claiming that the weapon was theoretically possible.\n\nA planned second installment of the article never appeared, and no sources were ever identified. Neither of the authors of the \"Sunday Times\" story, Uzi Mahnaimi and Marie Colvin, have spoken publicly on the matter.\n\nIn May 2007, a Russian newspaper \"Kommersant\" reported that the Russian government banned all exports of human biosamples.\nThe report claims that the reason for the ban was a secret FSB report about on-going development of \"genetic bioweapons\" targeting Russian population by Western institutions. The report mentions the Harvard School of Public Health, American International Health Alliance, Department of Medical Biotechnology of Jagiellonian University, United States Department of Justice Environment and Natural Resources Division, Institute of Genetics and Biotechnology Warsaw University, and United States Agency for International Development.\n\n\n"}
{"id": "28701828", "url": "https://en.wikipedia.org/wiki?curid=28701828", "title": "European Society for Clinical Nutrition and Metabolism", "text": "European Society for Clinical Nutrition and Metabolism\n\nThe European Society for Clinical Nutrition and Metabolism (ESPEN) is an organization in the field of parenteral and enteral nutrition and promotes basic and clinical research, basic and advanced education, organization of consensus statements about clinical care and care quality control.\n\nIn 1979 an informal meeting laid the foundations of ESPEN, deciding to create a multidisciplinary society devoted to the study of metabolic problems associated with acute diseases and their nutritional implications and management. ESPEN was formally established in 1980 as the European Society for Parenteral and Enteral Nutrition. It later changed its name to the European Society for Clinical Nutrition and Metabolism. \nCongress meetings are held every year in a different European city and gather over 3,000 participants from 82 different countries.\n\nA bimonthly journal named \"Clinical Nutrition\", which goes along with \"Clinical Nutrition Supplements\" and an electronic journal \"e-SPEN\" are the society's official publications, published by Elsevier.\n\nEuropean Parenteral and Enteral National Societies support ESPEN in the form of block members, e.g. the British, German, French, and Austrian societies.\n\nUnder the umbrella of ESPEN, many ongoing projects are supported by its members, such as NutritionDay, Home Artificial Nutrition, and Fight Against Malnutrition.\n\n"}
{"id": "1074264", "url": "https://en.wikipedia.org/wiki?curid=1074264", "title": "Glycemic load", "text": "Glycemic load\n\nThe glycemic load (GL) of food is a number that estimates how much the food will raise a person's blood glucose level after eating it. One unit of glycemic load approximates the effect of consuming one gram of glucose. Glycemic load accounts for how much carbohydrate is in the food and how much each gram of carbohydrate in the food raises blood glucose levels. Glycemic load is based on the glycemic index (GI), and is calculated by multiplying the grams of available carbohydrate in the food by the food's glycemic index, and then dividing by 100.\n\nGlycemic load estimates the impact of carbohydrate consumption using the glycemic index while taking into account the amount of carbohydrate that is consumed. GL is a GI-weighted measure of carbohydrate content. For instance, watermelon has a high GI, but a typical serving of watermelon does not contain much carbohydrate, so the glycemic load of eating it is low. Whereas glycemic index is defined for each type of food, glycemic load can be calculated for any size serving of a food, an entire meal, or an entire day's meals.\n\nGlycemic load of a serving of food can be calculated as its carbohydrate content measured in grams (g), multiplied by the food's GI, and divided by 100. For example, watermelon has a GI of 72. A 100 g serving of watermelon has 5 g of available carbohydrates (it contains a lot of water), making the calculation 5 × 72/100=3.6, so the GL is 4. A food with a GI of 100 and 10 g of available carbohydrates has a GL of 10 (10 × 100/100=10), while a food with 100 g of carbohydrate and a GI of just 10 also has a GL of 10 (100 × 10/100=10).\n\nFor one serving of a food, a GL greater than 20 is considered high, a GL of 11–19 is considered medium, and a GL of 10 or less is considered low. Foods that have a low GL in a typical serving size almost always have a low GI. Foods with an intermediate or high GL in a typical serving size range from a very low to very high GI.\n\nOne 2007 study has questioned the value of using glycemic load as a basis for weight-loss programmes. Das et al. conducted a study on 36 healthy, overweight adults, using a randomised test to measure the efficacy of two diets, one with a high glycemic load and one with a low GL. The study concluded that there is no statistically significant difference between the outcome of the two diets.\n\nGlycemic load appears to be a significant factor in dietary programs targeting metabolic syndrome, insulin resistance, and weight loss; studies have shown that sustained spikes in blood sugar and insulin levels may lead to increased diabetes risk. The Shanghai Women's Health Study concluded that women whose diets had the highest glycemic index were 21 percent more likely to develop type 2 diabetes than women whose diets had the lowest glycemic index. Similar findings were reported in the Black Women's Health Study. A diet program that manages the glycemic load aims to avoid sustained blood-sugar spikes and can help avoid onset of type 2 diabetes. For diabetics, glycemic load is a highly recommended tool for managing blood sugar.\n\nThe data on GI and GL listed in this article is from the University of Sydney (Human Nutrition Unit) GI database.\n\nThe GI was invented in 1981 by Dr Thomas Wolever and Dr David Jenkins at the University of Toronto and is a measure of how quickly a food containing 25 or 50 grams of carbohydrate raises blood-glucose levels. Because some foods typically have a low carbohydrate content, Harvard researchers created the GL, which takes into account the amount of carbohydrates in a given serving of a food and so provides a more useful measure. Liu et al. were the first to show that based on their calculation, the glycemic load of a specific food—calculated as the product of that food's carbohydrate content and its glycemic index value—has direct physiologic meaning in that each unit can be interpreted as the equivalent of 1 g carbohydrate from white bread (or glucose depending on the reference used in determining the glycemic index). It became immediately apparent that such direct physiological quantification of glycemic load would allow patients with diabetes to do “glycemic load” counting as opposed to the conventional “carbohydrate counting” for monitoring the glycemic effect of foods. The concept of glycemic load addresses the concern about rating foods as good or bad solely on the basis of their glycemic index. For example, although the glycemic index for carrots is reported to be as high as 131, the glycemic load for one serving of carrots is small because the amount of carbohydrate in one serving of carrots is minimal (≈7 g carbohydrate). Indeed, ≈700 g carrots (which provides 50 g carbohydrate) must be consumed to produce an incremental glucose response 1.3 times that of 100 g white bread containing 50 g carbohydrate.\n\n! Food !! Glycemic index !! Carbohydrate<br>content<br>(g) !! Glycemic Load (100g serving) !! Insulin index\n"}
{"id": "46575859", "url": "https://en.wikipedia.org/wiki?curid=46575859", "title": "Health Equity Impact Analysis", "text": "Health Equity Impact Analysis\n\nA Health Equity Impact Analysis is a decision support tool which walks users through the steps of identifying how a program, policy or similar initiative will impact population groups in different ways. HEIAs are meant to show, \"inter alia\", unintended potential impacts. The goal is to maximize positive impacts and reduce negative impacts that could potentially widen health disparities between population groups.\n"}
{"id": "808818", "url": "https://en.wikipedia.org/wiki?curid=808818", "title": "Health effects of tea", "text": "Health effects of tea\n\nAlthough health benefits have been assumed throughout the history of using \"Camellia sinensis\" as a common beverage, there is no high-quality evidence that tea confers significant benefits. In clinical research over the early 21st century, tea has been studied extensively for its potential to lower the risk of human diseases, but none of this research is conclusive as of 2017.\n\nIn regions without access to safe drinking water, boiling water to make tea is effective for reducing waterborne diseases by destroying pathogenic microorganisms.\n\nTea drinking accounts for a high proportion of aluminum in the human diet. The levels are safe, but there has been some concern that aluminum traces may be associated with Alzheimer's disease. A 2013 study additionally indicated that some teas contained lead (mostly Chinese) and aluminum (Indian/Sri Lanka blends, China). There is still insufficient evidence to draw firm conclusions on this subject.\n\nMost studies have found no association between tea intake and iron absorption. However, drinking excessive amounts of black tea may inhibit the absorption of iron, and may harm people with anaemia.\n\nConcerns have been raised about the traditional method of over-boiling tea to produce a decoction, which may increase the amount of environmental contaminants released and consumed.\n\nAll tea leaves contain fluoride; however, mature leaves contain as much as 10 to 20 times the fluoride levels of young leaves from the same plant.\n\nThe fluoride content of a tea leaf depends on the leaf picking method used and the fluoride content of the soil from which it has been grown; tea plants absorb this element at a greater rate than other plants. Care in the choice of the location where the plant is grown may reduce the risk. It is speculated that hand-picked tea would contain less fluoride than machine-harvested tea, because there is a much lower chance of harvesting older leaves during the harvest process. A 2013 British study of 38 teas found that cheaper UK supermarket tea blends had the highest levels of fluoride with about 580 mg per kilogram, green teas averaged about 397 mg per kg and pure blends about 132 mg per kg. The researchers suggested that economy teas may use older leaves which contain more fluoride. They calculated a person drinking a litre of economy tea per day would consume about 6 mg of fluoride, above the recommended average dietary intake level of 3-4 mg of fluoride per day, but below the maximum tolerable amount of 10 mg of fluoride per day.\n\nTea contains oxalate, overconsumption of which can cause kidney stones, as well as binding with free calcium in the body. The bioavailability of oxalate from tea is low, thus a possible negative effect requires a large intake of tea. Massive black tea consumption has been linked to kidney failure due to its high oxalate content (acute oxalate nephropathy).\n\nTea also contains theanine and the stimulant caffeine at about 3% of its dry weight, translating to between 30 mg and 90 mg per 8 oz (250 ml) cup depending on type, brand and brewing method. Tea also contains small amounts of theobromine and theophylline. Dry tea has more caffeine by weight than dry coffee; nevertheless, more dry coffee than dry tea is used in typical drink preparations, which results in a cup of brewed tea containing significantly less caffeine than a cup of coffee of the same size.\n\nThe caffeine in tea is a mild diuretic. However, the British Dietetic Association has suggested that tea can be used to supplement normal water consumption, and that \"the style of tea and coffee and the amounts we drink in the UK are unlikely to have a negative effect [on hydration]\".\n\nDrinking caffeinated tea may improve mental alertness. There is preliminary evidence that the caffeine from long-term tea (or coffee) consumption provides a small amount of protection against the progression of dementia or Alzheimer's disease during aging, although the results across numerous studies were inconsistent.\n\nIn 2011, the US Food and Drug Administration (FDA) reported that there was very little evidence to support the claim that green tea consumption may reduce the risk of breast and prostate cancer.\n\nThe US National Cancer Institute reports that in epidemiological studies and the few clinical trials of tea for the prevention of cancer, the results have been inconclusive. The institute \"does not recommend for or against the use of tea to reduce the risk of any type of cancer.\" ... \"Inconsistencies in study findings regarding tea and cancer risk may be due to variability in tea preparation, tea consumption, the bioavailability of tea compounds (the amounts that can be absorbed by the body), lifestyle differences, and individual genetic differences.\" Though there is some positive evidence for risk reduction of breast, prostate, ovarian, and endometrial cancers with green tea, it is weak and inconclusive.\n\nMeta-analyses of observational studies have concluded that black tea consumption does not appear to protect against the development of oral cancers in Asian or Caucasian populations, the development of esophageal cancer or prostate cancer in Asian populations, or the development of lung cancer.\n\nIn preliminary long-term clinical studies, black tea consumption showed evidence for providing a small, reduced risk of stroke, whereas, in another review, green tea and black tea did not have significant effects on the risk of coronary heart disease. Two reviews of randomized controlled trials concluded that long-term consumption of black tea slightly lowers systolic and diastolic blood pressures (about 1–2 mmHg), a finding based on limited evidence. A 2013 Cochrane review found some evidence of benefit from tea consumption on cardiovascular disease, though more research is needed. \n\nTea consumption does not appear to affect the risk of bone fracture including hip fractures or fractures of the humerus in men or women.\n\nAlthough green tea is commonly believed to be a weight loss aid, there is no good evidence that its long-term consumption has any meaningful benefit in helping overweight or obese people to lose weight, or that it helps to maintain a healthy body weight. Use of green tea for attempted weight loss carries a small risk of adverse effects, such as nausea, constipation, and stomach discomfort.\n\n\n"}
{"id": "6401338", "url": "https://en.wikipedia.org/wiki?curid=6401338", "title": "Health regions of Canada", "text": "Health regions of Canada\n\nHealth regions, also called health authorities, are a governance model used by Canada's provincial governments to administer and deliver public health care to all Canadian residents.\n\nHealth care is designated a provincial responsibility under the separation of powers in Canada's federal system. Most health regions or health authorities are organized along geographic boundaries, however, some are organized along operational lines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "10942453", "url": "https://en.wikipedia.org/wiki?curid=10942453", "title": "Health services research", "text": "Health services research\n\nHealth services research (HSR), also known as health systems research or health policy and systems research (HPSR), is a multidisciplinary scientific field that examines how people get access to health care practitioners and health care services, how much care costs, and what happens to patients as a result of this care. Studies in HSR investigate how social factors, health policy, financing systems, organizational structures and processes, medical technology, and personal behaviors affect access to health care, the quality and cost of health care, and quantity and quality of life. Compared with medical research, HSR is a relatively young science that developed through the bringing together of social science perspectives with the contributions of individuals and institutions engaged in delivering health services.\n\nThe primary goals of health services research are to identify the most effective ways to organize, manage, finance, and deliver high quality care; reduce medical errors; and improve patient safety. HSR is more concerned with delivery and access to care, in contrast to medical research, which focuses on the development and evaluation of clinical treatments.\n\nHealth services researchers come from a variety of specializations, including geography, nursing, economics, political science, epidemiology, public health, medicine, biostatistics, operations, management, engineering, pharmacy, psychology, usability and user experience design. While health services research is grounded in theory, its underlying aim is to perform research that can be applied by physicians, nurses, health managers and administrators, and other people who make decisions or deliver care in the health care system. For example, the application of epidemiological methods to the study of health services by managers is a type of health services research that can be described as Managerial epidemiology. \n\nApproaches to HSR include:\n\nMany data and information sources are used to conduct health services research, such as population and health surveys, clinical administrative records, health care program and financial administrative records, vital statistics records (births and deaths), and other special studies.\n\nClaims data on US Medicare and Medicaid beneficiaries are available for analysis. Data is divided into public data available to any entity and research data available only to qualified researchers. The US's Centers for Medicare and Medicaid Services (CMS) delegates some data export functions to a Research Data Assistance Center.\n\n23 Claims data from various states that are not limited to any particular insurer are also available for analysis via AHRQ's HCUP project.\n\nColloquially, health services research departments are often referred to as \"shops\"; in contrast to basic science research \"labs\". Broadly, these shops are hosted by three general types of institutions—government, academic, or non-governmental think tanks or professional societies.\n\nGovernment Sponsored\n\nUniversity Sponsored\n\nThink Tank or Professional Society Sponsored\n\nSeveral governmental agencies exist that sponsor or support HSR, with their remits set by central and devolved governments. These include the National Institute for Health Research (NIHR) and its constitiuent infrastructure (including the CLAHRC programme); Healthcare Improvement Scotland; Health and Care Research Wales; and Health and Social Care Research and Development\nMany universities have HSR units, a web search can find these with relative ease.\n\nSeveral government, academic and non-government agencies conduct or sponsor health services research, notably the Canadian Institute for Health Information and the Canadian Institutes of Health Research (i.e. the third pillar: \"research respecting health systems and services\"). \n\nOthers include the Institute for Clinical Evaluative Sciences (ICES) in Toronto, and the Canadian Collaborative Study of Hip Fractures.\n\nSeveral registries are available for research use, such as Danish Twin Register or Danish Cancer Register.\n\nPublic Health Research Laboratory. \n\n\nPublications:\n\nConferences and events:\n"}
{"id": "2176031", "url": "https://en.wikipedia.org/wiki?curid=2176031", "title": "Healthcare Effectiveness Data and Information Set", "text": "Healthcare Effectiveness Data and Information Set\n\nThe Healthcare Effectiveness Data and Information Set (HEDIS) is a widely used set of performance measures in the managed care industry, developed and maintained by the National Committee for Quality Assurance (NCQA).\n\nHEDIS was designed to allow consumers to compare health plan performance to other plans and to national or regional benchmarks. Although not originally intended for trending, HEDIS results are increasingly used to track year-to-year performance. HEDIS is one component of NCQA's accreditation process, although some plans submit HEDIS data without seeking accreditation. An incentive for many health plans to collect HEDIS data is a Centers for Medicare and Medicaid Services (CMS) requirement that health maintenance organizations (HMOs) submit Medicare HEDIS data in order to provide HMO services for Medicare enrollees under a program called .\n\nHEDIS was originally titled the \"HMO Employer Data and Information Set\" as of version 1.0 of 1991. In 1993, Version 2.0 of HEDIS was known as the \"Health Plan Employer Data and Information Set\". Version 3.0 of HEDIS was released in 1997. In July 2007, NCQA announced that the meaning of \"HEDIS\" would be changed to \"Healthcare Effectiveness Data and Information Set.\"\n\nIn current usage, the \"reporting year\" after the term \"HEDIS\" is one year following the year reflected in the data; for example, the \"HEDIS 2009\" reports, available in June 2009, contain analyses of data collected from \"measurement year\" January–December 2008.\n\nThe 83 HEDIS measures are divided into five \"domains of care\": \n\nMeasures are added, deleted, and revised annually. For example, a measure for the length of stay after giving birth was deleted after legislation mandating minimum length of stay rendered this measure nearly useless. Increased attention to medical care for seniors prompted the addition of measures related to glaucoma screening and osteoporosis treatment for older adults. Other health care concerns covered by HEDIS are immunizations, cancer screenings, treatment after heart attacks, diabetes, asthma, flu shots, access to services, dental care, alcohol and drug dependence treatment, timeliness of handling phone calls, prenatal and postpartum care, mental health care, well-care or preventive visits, inpatient utilization, drug utilization, and distribution of members by age, sex, and product lines.\n\nNew measures in HEDIS 2013 are “Asthma Medication Ratio,” “Diabetes Screening for People With Schizophrenia and Bipolar Disorder Who Are Using Antipsychotic Medications,” “Diabetes Monitoring for People With Diabetes and Schizophrenia,” “Cardiovascular Monitoring for People With Cardiovascular Disease and Schizophrenia,” and “Adherence to Antipsychotic Medications for Individuals With Schizophrenia.”\n\nHEDIS data are collected through surveys, medical charts and insurance claims for hospitalizations, medical office visits and procedures. Survey measures must be conducted by an NCQA-approved external survey organization. Clinical measures use the administrative or hybrid data collection methodology, as specified by NCQA. Administrative data are electronic records of services, including insurance claims and registration systems from hospitals, clinics, medical offices, pharmacies and labs. For example, a measure titled Childhood Immunization Status requires health plans to identify 2 year old children who have been enrolled for at least a year. The plans report the percentage of children who received specified immunizations. Plans may collect data for this measure by reviewing insurance claims or automated immunization records, but this method will not include immunizations received at community clinics that do not submit insurance claims. For this measure, plans are allowed to select a random sample of the population and supplement claims data with data from medical records. By doing so, plans may identify additional immunizations and report more favorable and accurate rates. However, the hybrid method is more costly, time-consuming and requires nurses or medical record reviewers who are authorized to review confidential medical records.\n\nHEDIS results must be audited by an NCQA-approved auditing firm for public reporting. NCQA has an on-line reporting tool called Quality Compass that is available for a fee of several thousand dollars. It provides detailed data on all measures and is intended for employers, consultants and insurance brokers who purchase health insurance for groups. NCQA's web site includes a summary of HEDIS results by health plan. NCQA also collaborates annually with U.S. News & World Report to rank HMOs using an index that combines many HEDIS measures and accreditation status. The \"Best Health Plans\" list is published in the magazine in October and is available on the magazine's web site. Other local business organizations, governmental agencies and media report HEDIS results, usually when they are released in the fall.\n\nProponents cite the following advantages of HEDIS measures:\n\n\nHEDIS was described in 1995 as \"very controversial\". Criticisms of HEDIS measures have included:\n\n\nhttp://www.ncqa.org/Portals/0/HEDISQM/HEDIS2013/HEDIS_2013_October_Update_Final_10.1.12.pdf\n\n"}
{"id": "11186931", "url": "https://en.wikipedia.org/wiki?curid=11186931", "title": "Hydrocolpos", "text": "Hydrocolpos\n\nHydrocolpos is the distension of the vagina caused by accumulation of fluid due to congenital vaginal obstruction. The obstruction is often caused by an imperforate hymen or less commonly a transverse vaginal septum. The fluid consists of cervical and endometrial mucus or in rare instances urine accumulated through a vesicovaginal fistula proximal to the obstruction. In some cases, it is associated with Bardet-Biedl Syndrome. If it occurs in prepubertal girls, it may show up as abdominal swelling. It may be detected by using ultrasound. It may also present at birth as a distended lower abdomen and vagina. It also associated with vaginal atresia.\n"}
{"id": "54576033", "url": "https://en.wikipedia.org/wiki?curid=54576033", "title": "Infant and toddler safety", "text": "Infant and toddler safety\n\nInfant and toddler safety are those actions and modifications put into place to keep babies and toddlers safe from accidental injury and death. Many accidents, injuries and deaths are preventable.\n\nInfants begin to crawl around six to nine months of age. When they crawl, they are exposed to many dangers. Anticipating the development of the baby and toddler aids caregivers in identifying hazards before they are discovered by the child.\n\nUS government agencies recommend that caregivers take the following precautions:\n\n\nToddlers typically enjoy climbing up things with steps. This includes furniture. Heavy furniture in the home is often not secured to the wall. These pieces of furniture can include bookcases and dressers that can weigh hundreds of pounds. Heavy objects like televisions that are on the furniture can also fall onto the child. If the toddler climbs up the furniture it is likely to fall onto the child. This has resulted in the deaths and injuries of children. Even if the children appears uninjured, it is possible that internal injuries have occurred with serious consequences. Often these injuries are not apparent to caregivers and as a consequence treatment can be delayed. Serious head injuries have also occurred.\n\nCaregivers can prevent accidents related to furniture by securring the furniture to the wall. Placing heavier objects into the lowest drawers. Not placing toys on top of the furniture. Constantly monitoring the activities of the toddler. Putting drawer stops onto the drawers to prevent the toddler from opening the drawer. Mount flat-screen televisions out-of-reach and onto the wall.\n\nNo safe levels of lead in the body of a child is considered safe and can cause problems for the rest of their life. Children living in low-income families are more likely to have levels of lead in their bodies. Questions regarding the testing procedures have been called into question. Children are at greater risk as they are more likely to put objects in their mouth such as those that contain lead paint and absorb a greater proportion of the lead that they eat. Treatment is available but prevention is better.\n\nBumper pads installed in cribs have been improved so that an infant cannot get caught between the pad and the bars of the crib.\n\nInfant food safety is the identification of risky food handling practices and the prevention of illness in infants. The most simple and easiest to implement is handwashing. Food for young children, including formula and baby food can contain pathogens that can make the child very ill and even die.\n\nSudden infant death syndrome can cause the death of an infant and often no cause is found. There are some preventative measures that can be taken to reduce the risk of SIDS. These are:\n\nIt is important that caregivers recognize the potential of the abuse of their infant or toddler. An infant or toddler is potentially vulnerable to physical abuse, sexual abuse, psychological abuse and neglect and has inability to verbalize the details of the abuse. Child grooming can be a concern and occurs when a perpetrator wins the trust of caregivers for the purpose of creating an opportunity for them to sexually abuse an infant or toddler. Shaken baby syndrome can often result in serious and permanent brain damage to an infant or toddler. There are preventative measures that can be taken to reduce the risk of injuring a child this way. Those who care for infants and toddlers may benefit from stress reduction. Becoming educated on normal child development can help someone understand that crying is a normal thing for babies and toddlers, especially if they hungry or need a diaper change. Caregivers can contact another person who is willing to give them a break. Those who are drinking alcohol are more likely to injure the infant or toddler. Carefully choosing someone else to watch the infant or toddler can also reduce the risk of injury.\n\nChildren under the age of 3 were 43% are less likely to be injured in a car crash if their car seat was placed in the center of the car. The center position is the safest but the least used position.\n\nForgetting that an infant or toddler is in the car and leaving them where they are exposed to high temperatures can result in death.\n\nToddlers can wander off and fall through ice or be left out in cool or cold weather and experience hypothermia. This low body temperature is often fatal but instances of survival after a near drowning occur.Of all drowning deaths in 2013, 82,000 occurred in children less than five years old.\n\nToddlers have wandered off and drowned in ponds. Toddlers can easily drown in small, shallow ornamental ponds.\n\nAn infant or toddler is more likely than other family members to be injured by an animal because they cannot defend themselves and are lower to the ground. Familiar family pets with no prior history of aggression are more likely to attack the child than unfamiliar pets from other households.\n\nToddlers and infants who can hold objects can choke when a small object is inhaled and blocks the trachea.\n\nBabywipes (diaper wipes) can be a choke hazard\n\nEven high chairs are hazardout\n\n\n \n"}
{"id": "50426357", "url": "https://en.wikipedia.org/wiki?curid=50426357", "title": "Innovative Vector Control Consortium", "text": "Innovative Vector Control Consortium\n\nThe Innovative Vector Control Consortium (IVCC) is a product development partnership that develops new insecticides for vector control and researches ways to use existing pesticides more effectively. IVCC was established in 2005 and became registered as a nonprofit in 2008. Liverpool School of Tropical Medicine is the parent organization of IVCC.\n\nIVCC has partners across both public and private sectors, including in industry and academia, and with funders, non-governmental organizations, and nonprofits. These include NGenIRS and Innovation to Impact as well as with companies including Syngenta.\n\nIt has also partnered with the Armed Forces Pest Management Board to discuss improvements to indoor residual spraying as well as devising new methods of vector control.\n\nIt funds the Malaria Decision Support System project in Malawi. It also funds other projects that focus on malaria and dengue control.\n\n\nIVCC was originally funded by the Bill & Melinda Gates Foundation with a grant of $50.7 million over five years. In 2010 the Gates Foundation awarded it a grant of $50 million to continue its work. In May 2016, it was granted an additional $75 million by the Gates Foundation.\n\nIVCC has also been funded by UKaid, USAID, the Swiss Agency for Development, UNITAID, and Wellcome Trust.\n\n"}
{"id": "5152675", "url": "https://en.wikipedia.org/wiki?curid=5152675", "title": "Inverse care law", "text": "Inverse care law\n\nThe inverse care law is the principle that the availability of good medical or social care tends to vary inversely with the need of the population served. Proposed by Julian Tudor Hart in 1971, the term has since been widely adopted. It is a pun on inverse-square law, a term and concept from physics.\n\nThe law states that:\n\n\"The availability of good medical care tends to vary inversely with the need for it in the population served. This ... operates more completely where medical care is most exposed to market forces, and less so where such exposure is reduced.\" \n\nHart later paraphrased his argument: \"To the extent that health care becomes a commodity it becomes distributed just like champagne. That is rich people gets lots of it. Poor people don’t get any of it.\"\n\nThe Inverse Care Law is a key issue in the debate about health inequality. As Frank Dobson put it when he was Secretary of State for Health: \"Inequality in health is the worst inequality of all. There is no more serious inequality than knowing that you'll die sooner because you're badly off.\" \n\n\n\n"}
{"id": "322276", "url": "https://en.wikipedia.org/wiki?curid=322276", "title": "John Boyd Orr", "text": "John Boyd Orr\n\nJohn Boyd Orr, 1st Baron Boyd Orr of Brechin Mearns, (23 September 1880 – 25 June 1971), styled Sir John Boyd Orr from 1935 to 1949, was a Scottish teacher, medical doctor, biologist and politician who was awarded the Nobel Peace Prize for his scientific research into nutrition and his work as the first Director-General of the United Nations Food and Agriculture Organization (FAO). He was the co-founder and the first President (1960–1971) of the World Academy of Art and Science (WAAS).\n\nJohn Boyd Orr was born at Kilmaurs, near Kilmarnock, East Ayrshire, Scotland, the middle child in a family of seven children. His father, Robert Clark Orr, was a quarry owner, and a man of deep religious convictions, being a member of the Free Church of Scotland. His mother, Annie Boyd, was the daughter of another quarry master, wealthier than Robert Orr, and grandmaster of a Freemason's Lodge.\n\nThe family home was well supplied with books, and his father was widely read in political, sociological and metaphysical subjects, as well as religion. As he grew older, John would regularly discuss these subjects with his father, brothers, and visiting friends. There was also family worship each evening.\n\nWhen John was five years old, the family suffered a setback when a ship owned by Robert Orr was lost at sea. They had to sell their home in Kilmaurs, and moved to West Kilbride, a village on the North Ayrshire coast. According to Kay, the new house and environment were a great improvement on Kilmaurs, despite the family's reduced means. The major part of his upbringing took place in and around West Kilbride. He attended the village school until he was thirteen. Religion was then an important part of junior education in Scotland, and the school gave him a good knowledge of the Bible, which stayed with him for the rest of his life.\n\nAt the age of thirteen, John won a bursary to Kilmarnock Academy, a significant achievement as such bursaries were then rare. The new school was some from his home in West Kilbride, but his father owned a quarry about two miles (3 km) from the Academy, and John was provided with accommodation nearby. His family cut short his education at the Academy because he was spending too much time in the company of the quarry workers (where he picked up a \"wonderful vocabulary of swear words\"), and after four months he returned to the village school in West Kilbride where he continued his education until he was seventeen under the inspirational tutelage of Headmaster John G. Lyons. There he became a pupil teacher at a salary of £10 for the first year, and £20 for the second. This was a particularly demanding time for the young Boyd Orr, as in addition to his teaching duties, and studying at home for his university and teacher-training qualifications, he also had to work every day in his father's business.\n\nAfter four years as a pupil teacher, at the age of 19 he won a Queen's Scholarship to study at a teacher training college in Glasgow, plus a bursary which paid for his lodgings there. At the same time he entered a three-year degree course in theology at the University, for which the fees were also covered.\n\nAs an undergraduate in Glasgow, he explored the interior of the city, usually at weekends. He was shocked by what he found in the poverty-stricken slums and tenements, which then made up a large part of the city. Rickets was obvious among the children, malnutrition (in some cases, associated with drunkenness) was shown by many of the adults, and many of the aged were destitute. In his first teaching job after graduating M.A. in 1902, he was posted to a school in the slums. His first class was overcrowded, and the children ill-fed or actually hungry, inadequately clothed, visibly lousy and physically wretched. He resigned after a few days, realising that he could not teach children in such a condition, and that there was nothing he could do to relieve their misery.\n\nAfter working for a few months in his father's business, he taught for three years at Kyleshill School in Saltcoats, also a poor area, but less squalid than the slums of Glasgow.\n\nBoyd Orr needed to augment his teacher's salary, and decided to do so by instructing an evening class in book-keeping and accountancy. After intensive study he passed the necessary examinations, and duly instructed his class. The knowledge and skills he learned by studying for, and teaching, this class were to prove very useful in his later career.\n\nHowever his heart was not in teaching, and after fulfilling his teaching obligations under the terms of his Queen's Scholarship, he returned to the University to study biology, a subject he had always been interested in since childhood. As a precaution, he entered simultaneously for a degree in medicine.\n\nHe found the university to be a very stimulating environment. Diarmid Noel Paton (son of the artist Joseph Noel Paton) was Regius Professor of Physiology, and Edward Provan Cathcart head of Physiological Chemistry, both men of outstanding scientific ability. He was impressed by Samuel Gemmill, Professor of Clinical Medicine, a philosopher whose deep thinking on social affairs also influenced Boyd Orr's approach to such questions.\n\nHalf-way through his medical studies, his savings ran out. Reluctant to ask his family for support, he bought a block of tenanted flats on mortgage, with the help of a bank overdraft, and used the rents to pay for the rest of his studies. On graduating, he sold the property for a small profit.\n\nHe graduated B.Sc. in 1910, and M.B. Ch.B. in 1912, at the age of 32, placing sixth in a year of 200 students. Two years later, in 1914, he graduated M.D. with honours, receiving the Bellahouston Gold Medal for the most distinguished thesis of the year.\n\nOn leaving the university, he took a position as a ship's surgeon on a ship trading between Scotland and West Africa, choosing this job because it offered the possibility of paying off his bank overdraft faster than any other. He resigned after four months, when he had repaid the debt. He then tried general practice, working as a \"locum\" in the practice of his family doctor in Saltcoats, and was offered a partnership there. Realising that a career in medicine was not for him, he instead accepted the offer of a two-year Carnegie research scholarship, to work in E.P. Cathcart's laboratory. The work he began there covered malnutrition, protein and creatine metabolism, the effect of water intake on nitrogenous metabolism in humans, and the energy expenditure of military recruits in training.\n\nOn 1 April 1914, Boyd Orr took charge of a new research institute in Aberdeen, a project of a joint committee for research into animal nutrition of the North of Scotland College of Agriculture and Aberdeen University. He had been offered the post on the recommendation of E.P. Cathcart, who had originally been offered the job, but had turned it down in favour of a chair in physiology in London.\n\nThe joint committee had allocated a budget of £5,000 for capital expenditure and £1,500 for annual running costs. Boyd Orr recognised immediately that these sums were inadequate. Using his experience in his father's business of drawing up plans and estimating costs, he submitted a budget of £50,000 for capital expenditure and £5,000 for annual running costs. Meanwhile, with the £5,000 he had already been allocated he specified a building, not of wood as had been envisaged by the committee, but of granite and designed so that it could serve as a wing of his proposed £50,000 Institute. He accepted the lowest tender of £5,030, and told the contractors to begin work immediately. The committee were not pleased, but had to accept the \"fait accompli\". When war broke out the contractors were told to finish the walls and roof, but to do no more for the time being.\n\nOn the outbreak of the First World War he was given leave to join the British Army, and asked his former colleague Cathcart to help him obtain a medical commission in an infantry unit overseas. Cathcart thought he would be more useful at home, and his first commission was in a special civilian section of the R.A.M.C. dealing with sanitation. Several divisions of non-conscripted recruits were in training in emergency camps at home, some of them in very poor sanitary conditions. Boyd Orr was able to push through schemes for improvement in hygiene, preventing much sickness.\n\nAfter 18 months he was posted as Medical Officer to an infantry unit, the 1st Sherwood Foresters. He spent much of his time in shell holes, patching up the many wounded. His courage under fire and devotion to duty were recognised by the award of a Military Cross after the Battle of the Somme, and of the Distinguished Service Order after Passchendaele. He also made arrangements for the battalion's diet to be supplemented by vegetables collected from local deserted gardens and fields. As a result, unlike other units, he did not need to send any of the men in his medical charge to hospital. He also prevented his men getting trench foot by personally ensuring they were fitted with boots a size larger than usual.\n\nWorried that he was losing touch with medical and nutritional advances, he asked to be transferred to the navy, where he thought he would have more time available for reading and research. The army was reluctant to let him go, but agreed, since he was still a civilian surgeon. He spent a busy three months in the naval hospital at Chatham, studying hard while practicing medicine in the wards, before being posted to HMS \"Furious\". On board ship his medical duties were light, enabling him to do a great deal of reading. He was later recalled to work studying food requirements of the army.\n\nWhen Boyd Orr returned to Aberdeen in early 1919, his plan for a larger Institute had still not been accepted. Indeed, even his plans for the annual maintenance grant had to be approved by the Professor of Agriculture in Cambridge, Thomas Barlow Wood. Despite gaining the latter's support, his expansion plans were at first rebuffed, although he succeeded in having the annual grant increased to £4,000. In 1920 he was introduced to John Quiller Rowett, a businessman who seemed to have qualms of conscience over the large profits he had made during the war. Shortly afterwards, the government agreed to finance half the cost of Boyd Orr's plan, provided he could raise the other half elsewhere. Rowett agreed to provide £10,000 for the first year, £10,000 for the second year, and gave an additional £2,000 for the purchase of a farm, provided that, \"if any work done at the Institute on animal nutrition was found to have a bearing on human nutrition, the Institute would be allowed to follow up this work\", a condition the Treasury was willing to accept. By September 1922 the buildings were nearly completed, and the renamed Rowett Research Institute was opened shortly thereafter by Queen Mary.\n\nBoyd Orr proved to be an effective fund-raiser from both government and private sources, expanding the experimental farm to around , building a well-endowed library, and expanding the buildings. He also built a centre for accommodating students and scientists attracted by the Institute's growing reputation, a reputation enhanced by Boyd Orr's many publications. His research output suffered from the time and energy he had to devote to fund-raising, and in later life he said, \"I still look with bitter resentment at having to spend half my time in the humiliating job of hunting for money for the Institute.\"\n\nThrough the 1920s, his own research was devoted mainly to animal nutrition, his focus changed to human nutrition both as a researcher and an active lobbyist and propagandist for improving people's diets. In 1927, he proved the value of milk being supplied to school children, which led to free school milk provision in the UK. His 1936 report \"Food, Health and Income\" showed that at least one third of the UK population were so poor that they could not afford to buy sufficient food to provide a healthy diet and revealed that there was a link between low-income, malnutrition and under-achievement in schools.\n\nFrom 1929 to 1944, Boyd Orr was Consultant Director to the Imperial Bureau of Animal Nutrition, later the Commonwealth Bureau of Nutrition (part of the Commonwealth Agricultural Bureaux), which was based at the Rowett Research Institute. During the Second World War he was a member of Churchill's Scientific Committee on Food Policy and helped to formulate food rationing\n\nIn October 1945, Orr was elected Rector of the University of Glasgow after standing as an Independent Progressive candidate. He was elected as an independent Member of Parliament (MP) for the Combined Scottish Universities in a by-election in April 1945, and kept his seat at the general election shortly after. He resigned in 1946.\n\nAfter the Second World War, Boyd Orr resigned from the Rowett Institute, and took several posts, most notably as Director-General of the United Nations' new Food and Agriculture Organization (FAO). Although his tenure in this position was short (1945–1948), he worked not only to alleviate the immediate postwar food shortage through the International Emergency Food Committee (IEFC) but also to propose comprehensive plans for improving food production and its equitable distribution—his proposal to create a World Food Board. Although the board failed to get the support of Britain and the US, Boyd Orr laid a firm foundation for the new U.N. specialized agency.\n\nHe then resigned from the FAO and became director of a number of companies and proved a canny investor in the stock market, making a considerable personal fortune. When he received the Nobel Peace Prize in 1949, he donated the entire financial award to organizations devoted to world peace and a united world government.\nHe was elevated to the peerage in 1949 as Baron Boyd Orr, of Brechin Mearns in the County of Angus.\n\nIn 1960 Boyd Orr was elected the first president of the World Academy of Art and Science, which was set up by eminent scientists of the day concerned about the potential misuse of scientific discoveries, most especially nuclear weapons.\n\nThe University of Glasgow has a Boyd Orr Building and the Boyd Orr Centre for Population and Ecosystem Health named after him, and the University's Hunterian Museum holds his Nobel medal. There is a street named after Boyd Orr in his home town of Kilmaurs in Ayrshire, as well as in Brechin, Angus, Penicuik, Midlothian, and in Saltcoats, Ayrshire.\n\n\n\n"}
{"id": "570749", "url": "https://en.wikipedia.org/wiki?curid=570749", "title": "Laboratory information management system", "text": "Laboratory information management system\n\nA laboratory information management system (LIMS), sometimes referred to as a laboratory information system (LIS) or laboratory management system (LMS), is a software-based solution with features that support a modern laboratory's operations. Key features include—but are not limited to—workflow and data tracking support, flexible architecture, and data exchange interfaces, which fully \"support its use in regulated environments\". The features and uses of a LIMS have evolved over the years from simple sample tracking to an enterprise resource planning tool that manages multiple aspects of laboratory informatics.\n\nThe definition of a LIMS is somewhat controversial: LIMSs are dynamic because the laboratory's requirements are rapidly evolving and different labs often have different needs. Therefore, a working definition of a LIMS ultimately depends on the interpretation by the individuals or groups involved.\n\nHistorically the LIMS, LIS, and process development execution system (PDES) have all performed similar functions. The term \"LIMS\" has tended to refer to informatics systems targeted for environmental, research, or commercial analysis such as pharmaceutical or petrochemical work. \"LIS\" has tended to refer to laboratory informatics systems in the forensics and clinical markets, which often required special case management tools. \"PDES\" has generally applied to a wider scope, including, for example, virtual manufacturing techniques, while not necessarily integrating with laboratory equipment.\n\nIn recent times LIMS functionality has spread even farther beyond its original purpose of sample management. Assay data management, data mining, data analysis, and electronic laboratory notebook (ELN) integration have been added to many LIMS, enabling the realization of translational medicine completely within a single software solution. Additionally, the distinction between LIMS and LIS has blurred, as many LIMS now also fully support comprehensive case-centric clinical data.\n\nUp until the late 1970s, the management of laboratory samples and the associated analysis and reporting were time-consuming manual processes often riddled with transcription errors. This gave some organizations impetus to streamline the collection of data and how it was reported. Custom in-house solutions were developed by a few individual laboratories, while some enterprising entities at the same time sought to develop a more commercial reporting solution in the form of special instrument-based systems.\n\nIn 1982 the first generation of LIMS was introduced in the form of a single centralized minicomputer, which offered laboratories the first opportunity to utilize automated reporting tools. As the interest in these early LIMS grew, industry leaders like Gerst Gibbon of the Federal Energy Technology Center in Pittsburgh began planting the seeds through LIMS-related conferences. By 1988 the second-generation commercial offerings were tapping into relational databases to expand LIMS into more application-specific territory, and International LIMS Conferences were in full swing. As personal computers became more powerful and prominent, a third generation of LIMS emerged in the early 1990s. These new LIMS took advantage of client/server architecture, allowing laboratories to implement better data processing and exchanges.\n\nBy 1995 the client/server tools had developed to the point of allowing processing of data anywhere on the network. Web-enabled LIMS were introduced the following year, enabling researchers to extend operations outside the confines of the laboratory. From 1996 to 2002 additional functionality was included in LIMS, from wireless networking capabilities and georeferencing of samples, to the adoption of XML standards and the development of Internet purchasing.\n\nAs of 2012, some LIMS have added additional characteristics that continue to shape how a LIMS is defined. Additions include clinical functionality, electronic laboratory notebook (ELN) functionality, as well a rise in the software as a service (SaaS) distribution model.\n\nThe LIMS is an evolving concept, with new features and functionality being added often. As laboratory demands change and technological progress continues, the functions of a LIMS will likely also change. Despite these changes, a LIMS tends to have a base set of functionality that defines it. That functionality can roughly be divided into five laboratory processing phases, with numerous software functions falling under each:\n(1) the reception and log in of a sample and its associated customer data, \n(2) the assignment, scheduling, and tracking of the sample and the associated analytical workload, \n(3) the processing and quality control associated with the sample and the utilized equipment and inventory, \n(4) the storage of data associated with the sample analysis, \n(5) the inspection, approval, and compilation of the sample data for reporting and/or further analysis.\n\nThere are several pieces of core functionality associated with these laboratory processing phases that tend to appear in most LIMS:\n\nThe core function of LIMS has traditionally been the management of samples. This typically is initiated when a sample is received in the laboratory, at which point the sample will be registered in the LIMS. Some LIMS will allow the customer to place an \"order\" for a sample directly to the LIMS at which point the sample is generated in an \"unreceived\" state. The processing could then include a step where the sample container is registered and sent to the customer for the sample to be taken and then returned to the lab. The registration process may involve accessioning the sample and producing barcodes to affix to the sample container. Various other parameters such as clinical or phenotypic information corresponding with the sample are also often recorded. The LIMS then tracks chain of custody as well as sample location. Location tracking usually involves assigning the sample to a particular freezer location, often down to the granular level of shelf, rack, box, row, and column. Other event tracking such as freeze and thaw cycles that a sample undergoes in the laboratory may be required.\n\nModern LIMS have implemented extensive configurability as each laboratory's needs for tracking additional data points can vary widely. LIMS vendors cannot typically make assumptions about what these data tracking needs are, and therefore vendors must create LIMS that are adaptable to individual environments. LIMS users may also have regulatory concerns to comply with such as CLIA, HIPAA, GLP, and FDA specifications, affecting certain aspects of sample management in a LIMS solution. One key to compliance with many of these standards is audit logging of all changes to LIMS data, and in some cases a full electronic signature system is required for rigorous tracking of field-level changes to LIMS data.\n\nModern LIMS offer an increasing amount of integration with laboratory instruments and applications. A LIMS may create control files that are \"fed\" into the instrument and direct its operation on some physical item such as a sample tube or sample plate. The LIMS may then import instrument results files to extract data for quality control assessment of the operation on the sample. Access to the instrument data can sometimes be regulated based on chain of custody assignments or other security features if need be.\n\nModern LIMS products now also allow for the import and management of raw assay data results. Modern targeted assays such as qPCR and deep sequencing can produce tens of thousands of data points per sample. Furthermore, in the case of drug and diagnostic development as many as 12 or more assays may be run for each sample. In order to track this data, a LIMS solution needs to be adaptable to many different assay formats at both the data layer and import creation layer, while maintaining a high level of overall performance. Some LIMS products address this by simply attaching assay data as BLOBs to samples, but this limits the utility of that data in data mining and downstream analysis.\n\nThe exponentially growing volume of data created in laboratories, coupled with increased business demands and focus on profitability, have pushed LIMS vendors to increase attention to how their LIMS handles electronic data exchanges. Attention must be paid to how an instrument's input and output data is managed, how remote sample collection data is imported and exported, and how mobile technology integrates with the LIMS. The successful transfer of data files in spreadsheets and other formats is a pivotal aspect of the modern LIMS. In fact, the transition \"from proprietary databases to standardized database management systems such as MySQL\" has arguably had one of the biggest impacts on how data is managed and exchanged in laboratories. In addition to mobile and database electronic data exchange, many LIMS support real-time data exchange with Electronic Health Records used in core hospital or clinic operations.\n\nAside from the key functions of sample management, instrument and application integration, and electronic data exchange, there are numerous additional operations that can be managed in a LIMS. This includes but is not limited to:\n\n\nA LIMS has utilized many architectures and distribution models over the years. As technology has changed, how a LIMS is installed, managed, and utilized has also changed with it. The following represents architectures which have been utilized at one point or another.\n\nA thick-client LIMS is a more traditional client/server architecture, with some of the system residing on the computer or workstation of the user (the client) and the rest on the server. The LIMS software is installed on the client computer, which does all of the data processing. Later it passes information to the server, which has the primary purpose of data storage. Most changes, upgrades, and other modifications will happen on the client side.\n\nThis was one of the first architectures implemented into a LIMS, having the advantage of providing higher processing speeds (because processing is done on the client and not the server). Additionally, thick-client systems have also provided more interactivity and customization, though often at a greater learning curve. The disadvantages of client-side LIMS include the need for more robust client computers and more time-consuming upgrades, as well as a lack of base functionality through a web browser. The thick-client LIMS can become web-enabled through an add-on component.\n\nAlthough there is a claim of improved security through the use of a thick-client LIMS, this is based on the misconception that \"only users with the client application installed on their PC can access server side information\". This secrecy-of-design reliance is known as security through obscurity and ignores an adversary's ability to mimic client-server interaction through, for example, reverse engineering, network traffic interception, or simply purchasing a thick-client license. Such a view is in contradiction of the \"Open Design\" principle of the National Institute of Standards and Technology's \"Guide to General Server Security\" which states that \"system security should not depend on the secrecy of the implementation or its components\", which can be considered as a reiteration of Kerckhoffs's principle.\n\nA thin-client LIMS is a more modern architecture which offers full application functionality accessed through a device's web browser. The actual LIMS software resides on a server (host) which feeds and processes information without saving it to the user's hard disk. Any necessary changes, upgrades, and other modifications are handled by the entity hosting the server-side LIMS software, meaning all end-users see all changes made. To this end, a true thin-client LIMS will leave no \"footprint\" on the client's computer, and only the integrity of the web browser need be maintained by the user. The advantages of this system include significantly lower cost of ownership and fewer network and client-side maintenance expenses. However, this architecture has the disadvantage of requiring real-time server access, a need for increased network throughput, and slightly less functionality. A sort of hybrid architecture that incorporates the features of thin-client browser usage with a thick client installation exists in the form of a web-based LIMS.\n\nSome LIMS vendors are beginning to rent hosted, thin-client solutions as \"software as a service\" (SaaS). These solutions tend to be less configurable than on-premises solutions and are therefore considered for less demanding implementations such as laboratories with few users and limited sample processing volumes.\n\nAnother implementation of the thin client architecture is the maintenance, warranty, and support (MSW) agreement. Pricing levels are typically based on a percentage of the license fee, with a standard level of service for 10 concurrent users being approximately 10 hours of support and additional customer service, at a roughly $200 per hour rate. Though some may choose to opt out of an MSW after the first year, it's often more economical to continue the plan in order to receive updates to the LIMS, giving it a longer life span in the laboratory.\n\nA web-enabled LIMS architecture is essentially a thick-client architecture with an added web browser component. In this setup, the client-side software has additional functionality that allows users to interface with the software through their device's browser. This functionality is typically limited only to certain functions of the web client. The primary advantage of a web-enabled LIMS is the end-user can access data both on the client side and the server side of the configuration. As in a thick-client architecture, updates in the software must be propagated to every client machine. However, the added disadvantages of requiring always-on access to the host server and the need for cross-platform functionality mean that additional overhead costs may arise.\n\nA web-based LIMS architecture is a hybrid of the thick- and thin-client architectures. While much of the client-side work is done through a web browser, the LIMS may also require the support of desktop software installed on the client device. The end result is a process that is apparent to the end-user through a web browser, but perhaps not so apparent as it runs thick-client-like processing in the background. In this case, web-based architecture has the advantage of providing more functionality through a more friendly web interface. The disadvantages of this setup are more sunk costs in system administration and reduced functionality on mobile platforms.\n\nThe disadvantage of a thick client is in the installation and update phases of the applications. Users who want the security, high speed and functionality of a thick client may use Microsoft ClickOnce Technology. This enables the user to install and run a Windows-based smart client application by clicking a link in a web page. The software does not need to be installed at each user workstation one by one. ClickOnce applications can be self-updating; they can check for newer versions as they become available and automatically replace any updated files.\n\nLIMS implementations are notorious for often being lengthy and costly. This is due in part to the diversity of requirements within each lab, but also to the inflexible nature of most LIMS products for adapting to these widely varying requirements. Newer LIMS solutions are beginning to emerge that take advantage of modern techniques in software design that are inherently more configurable and adaptable — particularly at the data layer — than prior solutions. This means not only that implementations are much faster, but also that the costs are lower and the risk of obsolescence is minimized.\n\nUntil recently, the LIMS and Laboratory Information System (LIS) have exhibited a few key differences, making them noticeably separate entities.\n\nA LIMS traditionally has been designed to process and report data related to batches of samples from biology labs, water treatment facilities, drug trials, and other entities that handle complex batches of data. A LIS has been designed primarily for processing and reporting data related to individual patients in a clinical setting.\n\nA LIMS may need to satisfy good manufacturing practice (GMP) and meet the reporting and audit needs of the regulatory bodies and research scientists in many different industries. A LIS, however, must satisfy the reporting and auditing needs of health service agencies e.g. the hospital accreditation agency, HIPAA in the US, or other clinical medical practitioners.\n\nA LIMS is most competitive in group-centric settings (dealing with \"batches\" and \"samples\") that often deal with mostly anonymous research-specific laboratory data, whereas a LIS is usually most competitive in patient-centric settings (dealing with \"subjects\" and \"specimens\") and clinical labs. An LIS is regulated as a medical device by the FDA, and the companies that produce the software are therefore liable for defects. Due to this, an LIS can not be customized by the client.\n\nA LIMS covers standards such as 21 CFR Part 11 from the Food and Drug Administration (United States), ISO/IEC 17025, \nISO 15189, \ngood laboratory practice, and \nGood Automated Manufacturing Practice (GAMP).\n\n"}
{"id": "23281432", "url": "https://en.wikipedia.org/wiki?curid=23281432", "title": "List of current youth hearing conservation programs", "text": "List of current youth hearing conservation programs\n\n\n"}
{"id": "23811783", "url": "https://en.wikipedia.org/wiki?curid=23811783", "title": "Mandated choice", "text": "Mandated choice\n\nMandated choice or mandatory choice is an approach to public policy questions in which people are required by law to state in advance whether or not they are willing to engage in a particular action. The approach contrasts with \"opt-in\" and \"opt-out\" (\"presumed consent\") models of policy formation. The approach has most frequently been applied to cadaveric organ donation, but has increasingly been considered for advance directives as well. One bioethicist, in advocating for a mandatory choice model for living wills, argues that \"while all Americans should have a right to decide how they want their lives to end, it does not follow that they should be able to avoid confronting such a choice.\"\n\nOne of the first considerations of mandated choice appeared in Great Britain's Gore Report, a 1989-1990 study funded by the British Department of Health. From 2011 all those applying for or renewing driving licences online in the UK are required to state whether they wished to donate their organs.\n\nThe American Medical Association endorsed a mandated choice model for organ donation in 1994.\n\nIt has been suggested that individuals could be compelled to choose as part of tax returns, driver's licence applications, and/or state benefits claims.\n\nA 1992 survey found that 90% of American college students favored a mandated choice model for organ donation, compared with only 60% who favored presumed consent. However, Texas implemented such a program, requiring drivers to make a choice on organ donation when obtaining licenses, and found that 80% of drivers declined to donate.\n\nChouhan and Draper propose a modified scheme of mandated choice, in which though all patients are given a choice whether to donate they are actively encouraged to do so.\n\n"}
{"id": "11167175", "url": "https://en.wikipedia.org/wiki?curid=11167175", "title": "Mental Health Awareness Month", "text": "Mental Health Awareness Month\n\nMental Health Awareness Month (also referred to as \"Mental Health Month\") has been observed in May in the United States since 1949, reaching millions of people in the United States through the media, local events, and screenings.\n\nMental Health Awareness Month was started in the United States in 1949 by the Mental Health America organization (then known as the National Association for Mental Health). Each year in mid-March Mental Health America releases a \"toolkit of materials\" to guide preparation for outreach activities during Mental Health Awareness Month. During the month of May, Mental Health America, its affiliates, and other organizations interested in mental health conduct a number of activities which are based on a different theme each year.\n\nThemes from recent years include:\n\nIts purpose is to raise awareness and educate the public about: mental illnesses, such as the 18.1% of Americans who suffer from depression, schizophrenia, and bipolar disorder; the realities of living with these conditions; and strategies for attaining mental health and wellness. It also aims to draw attention to suicide, which can be precipitated by some mental illnesses. Additionally, Mental Health Awareness Month strives to reduce the stigma (negative attitudes and misconceptions) that surrounds mental illnesses. The month came about by presidential proclamation.\n\nMental Health America is not the only organization to run campaigns throughout May. Many other similar organizations choose to host awareness observances that coincide with Mental Health Awareness month. National Children's Mental Health Awareness Day is one such campaign. This event is sponsored by the Substance Abuse and Mental Health Services Administration in partnership with other non-profit and advocacy organizations.\n\nOther months and weeks throughout the year are designated to raise awareness around specific mental health conditions or the mental health of different demographic groups (Minority Mental Health Month, Mental Illness Awareness Week, National Depression Screening Day, etc.).\n\n\n"}
{"id": "3512186", "url": "https://en.wikipedia.org/wiki?curid=3512186", "title": "Mental health consumer", "text": "Mental health consumer\n\nA mental health consumer (or mental health patient) is a person who is obtaining treatment or support for a mental disorder, also known as psychiatric or mental illness. The term was coined by people who use mental health services in an attempt to empower those with mental health issues, usually considered a marginalized segment of society. The term suggests that there is a reciprocal contract between those who provide a service and those who use a service and that individuals have a choice in their treatment and that without them there could not exist mental health providers.\n\nIn the 1970s the term \"patient\" was most commonly used. Mental Health activists of the civil rights times recognized, as did many other groups seeking self-definition, that such labels are metaphors that reflect how identities are perceived and constructed (McDonald 206). In particular, in the mental health field they shape the nature of the relationship between the giver and receiver of psychiatric services, be it one with an emphasis on reciprocity or hierarchy (McLaughlin 2007). Users of psychiatric services repulsed the efforts of experts to define them and sought to develop ways to define themselves (Morrison 2000). In Australia, informal support groups of people who had recovered from episodes of mental ill health were formed during the first wave of moving patients out of psychiatriic hospitals into the community in the 1960s. In the USA and other countries, radical movements to change service delivery and legislation began to be driven by consumers during the 1980s. Activists, such as Judi Chamberlain, pressed for alternatives to psychiatrist dominated and controlled systems of mental health provision. Chamberlain's On Our Own: Patient Controlled Alternatives to the Mental Health System (Chamberlain 1978) helped guide others intent on a more collaborative form of mental health healing.\n\nIn the 1980s with some funding from NIMH, small experimental groups flourished. In 1985 at the First Alternatives Conference attendees agreed upon the term \"consumer\" reflecting the patients' choice of services (Bluebird). The term also implied assumptions of rationality and ability to make choices in one's own best interests rather than be a passive incapacitated recipient of \"expert\" attention (McDonald 2006). \nIn the 1990s many consumer groups were formed, such as Self Help Clearing House and the National Empowerment Center. They continued to press for more peer involvement in alternatives treatments, pointing out that peers support and comfort, in contrast to therapists who attempt to change the behavior and thinking patterns of consumers (Bluebird).\n\nToday, the word mental health consumer has expanded in the popular usage of consumers themselves to include anyone who has received mental health services in the past, anyone who has a behavioral health diagnosis, or simply anyone who has experienced a mental or behavioral disorder. Other terms sometimes used by members of this community for empowerment through positive self-identification include \"peers,\" \"people with mental health disabilities,\" \"psychiatric survivors,\" \"users,\" individuals with \"lived experience\" and \"ex-patients.\" (See the Psychiatric survivors movement for more information). The term \"service users,\" is commonly used in the U.K. In the U.S. \"consumer\" is most frequently used by ex-patients and users of psychiatric and alternative services.\n\nOne can view this term, \"consumer,\" neutrally as a person who receives psychological services, perhaps from a psychologist, a psychiatrist or a social worker. It can be impersonal term relating to the use in the health sector of a large economy. It suggests that the consumer expects to have some influence on service delivery and provides feedback \nto the provider. Used in its more activist sense, consumer groups aim to correct perceived problems in mental health services and to promote consultation with consumers. Consumer theory was devised to interpret the special relationship between a service provider and service user in the context of mental health. Consumer theory examines the consequences and sociological meaning of the relationship.\n\nBluebird, G., \"History of the Consumer/Survivor Movement.\" https://www.power2u.org/downloads/HistoryOfTheConsumerMovement.pdf\nChamberlin, J. (1979). On our own: Patient –controlled alternatives to the mental health system. New York; McGraw-Hill.\nMcDonald, (2006), McDonald, C. (2006) Challenging Social Work: The Context of Practice, Basingstoke, \nPalgrave Macmillan.\nMcLaughlin, H. (2009), \"What's in a Name: ‘Client’, ‘Patient’, ‘Customer’, ‘Consumer’, ‘Expert by Experience’, ‘Service User’—What's Next?\" Br. J. Soc. Work, (2009) 39, 1101–1117.\nMorrison, L. (2000). Committing social change for psychiatric patients: The consumer/survivor movement. \"Humanity & Society\", 24, 389-404.\n\n"}
{"id": "42740507", "url": "https://en.wikipedia.org/wiki?curid=42740507", "title": "Moral injury", "text": "Moral injury\n\nMoral injury refers to an injury to an individual's moral conscience resulting from an act of perceived moral transgression which produces profound emotional shame. The concept of moral injury emphasizes the psychological, social, cultural, and spiritual aspects of trauma. Distinct from pathology, moral injury is a normal human response to an abnormal traumatic event. The concept is currently used in literature with regard to the mental health of military veterans who have witnessed or perpetrated an act in combat that transgressed their deeply held moral beliefs. Moral injury can also be experienced by those who have been transgressed against. For example, when one goes to war believing that the purpose of the war is to eradicate weapons of mass destruction, but finds that not to be the case, the warrior can experience moral injury due to a sense of betrayal. Those who have seen and experienced death, mayhem, destruction, and violence and have had their worldviews shattered – the sanctity of life, safety, love, health, peace, etcetera – can also suffer moral injury. This injury can also occur in the medical space – among physicians and other emergency or first responder care providers who engage in traumatic high impact work environments which can affect their mental health and well-being. \n\nThe term ‘moral injury’ (also abbreviated ‘MI’) was first coined by psychiatrist professor Jonathan Shay and colleagues based upon numerous narratives presented by military/veteran patients given their perception of injustice as a result of leadership malpractice. Shay’s definition of moral injury had three components: ‘Moral injury is present when (i) there has been a betrayal of what is morally right, (ii) by someone who holds legitimate authority and (iii) in a high-stakes situation. Since this original definition, other definitions have subsequently developed. \n\nTo understand the development of the construct of moral injury, it is necessary to examine the history of violence and the psychological consequences. Throughout history, humans have been killing each other, and have shown great reluctance in doing so. Literature on warfare emphasizes the moral anguish soldiers feel in combat, from modern military service members to ancient warriors. Ethical and moral challenges are expected from warfare. Soldiers in the line of duty may witness catastrophic suffering and severe cruelty, causing their fundamental beliefs about humanity and their worldview to be shaken.\n\nResearch has begun to look at the concept of moral injury to understand the impact that combat may have on soldiers, and their mental health afterwards. Currently, no systematic reviews or meta-analyses exist on the construct of moral injury – although a literature review of the various definitions since the inception of moral injury has been undertaken. Some of the literature reflects that moral injury was developed as a response to the inadequacy of mental health diagnoses to encapsulate the moral anguish service members were experiencing after returning home from war. Service members who are deployed into war zones are usually exposed to death, injury, and violence. Military service members represent the population with the highest risk of developing posttraumatic stress disorder (PTSD). PTSD was first included in the third edition of the Diagnostic and Statistical Manual of Mental Disorders, the manual classifying mental health disorders published by the American Psychiatric Association, to begin to address the symptoms that Vietnam veterans exhibited after their wartime experiences. As PTSD has developed as a diagnosis, it requires that individuals are either directly exposed to death, threatened death, serious injury, or sexual violence, witness it in person, learn about it occurring indirectly to a close relative or friend, or are repeatedly exposed to aversive details of traumatic events. PTSD includes four symptom clusters, including intrusion, avoidance, and negative mood and thoughts, and changes in arousal and reactivity. Individuals with PTSD may experience intrusive thoughts as they re-experience the traumatic events, as well as avoiding stimuli that reminds them of the traumatic event, and have increasingly negative thoughts and moods. Additionally, individuals with PTSD may exhibit irritable or aggressive, self-destructive behavior, and hypervigilance, amongst other arousal-related symptoms.\n\nWhile these symptoms can have devastating effects, in the first review of moral injury, Litz and co-authors argued that service members may experience long-term pain and suffering stemming from their time in combat that is not encapsulated or represented by a diagnosis of PTSD. Unlike PTSD’s focus on fear-related symptoms, moral injury focuses on symptoms related to guilt, shame, anger, and disgust. A diagnosis of PTSD in the DSM-III listed an individual experiencing guilt for behaviors that required for their survival as a symptom. However, conceptualizations of PTSD in each subsequent DSM has dropped guilt as a symptom.\n\nWith the inability of current diagnoses to account for moral anguish, research has begun to search to encapsulate moral conflict in warriors. The phrase ‘moral injury’ (originally defined by Jonathan Shay ) was modified by Brett Liz and colleagues (2009) as “perpetrating, failing to prevent, or bearing witness to acts that transgress deeply held moral beliefs and expectations may be deleterious in the long term, emotionally, psychologically, behaviorally, spiritually, and socially\" (p. 695). Treating moral injury is often thought of as “soul repair” due to the nature of moral anguish. As someone wrestles with the impact of what they did, failed to do, or witnessed, it can seem like their entire guiding principles for life have been altered or removed. The consequences of moral injury can be disastrous. An individual with a moral injury can experience severe distress, including major depression, and suicidality. While moral injury can be experienced by people other than military service members, current research has paid special attention to moral injury in military populations.\n\nAlthough moral injury does not only exist among military populations, the exposure to violence that occurs during war times make military and veteran population at a higher risk of developing moral injury. It has been reported that 32% of service members deployed to Iraq and Afghanistan were responsible for the death of an enemy and 60% stated that they had witnessed both women and children who were either ill or wounded that they were unable to provide aid to. Additionally, 20% reported being responsible for the death of a non-combatant. These statistics were taken in 2003 and an updated survey of the number of service members who have been directly responsible for the death of an enemy, a non-combatant, or having to leave sick and wounded women and children behind can shed light onto the magnitude of the issue of moral injury among service members.\n\nDuring times of war a service member’s personal ethical code may clash with what is expected of them during war. Approximately 27% of deployed soldiers have reported having an ethical dilemma to which they did not know how to respond. Research has shown that longer and more frequent deployments can result in an increase in unethical behaviors on the battlefield. This is problematic considering deployment lengths have increased for the war in Iraq and Afghanistan. During times of war the military promotes an ethical pardon on the killing of an enemy, going against the typical moral code for many service members. While a service member is deployed, killing of the enemy is expected and often rewarded. Despite this, when a service member returns home the sociocultural expectations are largely different from when they were deployed. The ethical code back home has not changed, making the transition from deployment to home difficult for some service members. This clash in a personal ethical code and the ethical code and expectations of the military can further increase a service member’s deep-seated feelings of shame and guilt for their actions abroad.\n\nBrett Litz and colleagues can be credited for major developments in the psychological perspective on moral injury. They define moral injury as “perpetrating, failing to prevent, bearing witness to, or learning about acts that transgress deeply held moral beliefs and expectations.” Litz and colleagues focus on the cognitive, behavioral, and emotional aspects of moral injury in a preliminary conceptual model. This model posits that cognitive dissonance occurs after a perceived moral transgression resulting in stable internal global attributions of blame, followed by the experience of shame, guilt, or anxiety, causing the individual to withdraw from others. The result is increased risk of suicide due to demoralization, self-harming, and self-handicapping behaviors.\n\nPsychological risk factors that make an individual more prone to moral injury includes neuroticism and shame-proneness. Protective factors includes self-esteem, forgiving supports, and belief in the just-world hypothesis.\n\nThe concept of moral injury was introduced by clinical psychiatrist Jonathan Shay, and the cultural perspective on moral injury has been developed in his work. He defines moral injury as stemming from the “betrayal of ‘what’s right’ in a high-stakes situation by someone who holds power.” The process of recovery, according to Shay, should consist of “purification” through the \"communalization of trauma.\" Shay places special importance on communication through artistic means of expression. Moral injury can only be absolved when “the trauma survivor… [is] permitted and empowered to voice his or her experience….” For this to occur, there needs to be openness on the part of civilians to hear the veterans’ experiences without prejudice. The culture in the military emphasizes a moral and ethical code that normalizes both killing and violence in times of war. Despite this, decisions made by service members who engage in killing or violence through this cultural lens may still experience psychological and spiritual impact. Fully coming “home” means integration into a culture where one is accepted, valued and respected, has a sense of place, purpose, and social support.\n\nMajor developments in the spiritual perspective on moral injury can be credited to Rita Nakashima Brock and Gabriella Lettini. They emphasize moral injury as “…souls in anguish, not a psychological disorder.” This occurs when veterans struggle with a lost sense of humanity after transgressing deeply held moral beliefs. The Soul Repair Center at Brite Divinity School is dedicated to addressing moral injury from this spiritual perspective. Research by Dr. Lindsay Carey at La Trobe University (Melbourne, Australia) and Tim Hodgson at the University of Queensland (Brisbane, Australia) confirm the importance of spirituality, and that community clergy, or chaplains in particular, have a key role with regard to providing spiritual care for those suffering a moral injury. US Army chaplains, particularly at the US Army Medical Department Center & School, are addressing the spiritual aspects of moral injury and the chaplains' role in assisting the healing process, by teaching and engaging in further research about moral injury. As noted by Carey and Hodgson however, when it comes to conducting research there have emerged approximately seventeen (\"n\" = 17) different definitional variations of 'moral injury' since Jonathan Shay's original term, causing confusion and making both research and treatment interventions by chaplains a serious challenge. While some have argued that moral injury is predominantly a 'psychological issue' or purely a 'spiritual' or 'religious' issue, Carey and Hodgson argue for a 'bio-psycho-social-spiritual' paradigm to be utilsed when defining, screening, assessing, or treating moral injury, so as to ensure that biological, psychological, social and spiritual aspects are equally considered in correlation; that is, moral injury should be considered as a bio-psycho-social-spiritual syndrome. It is with this perspective that Lindsay Carey (Australia), John Swinton (UK) and Daniel Grossoehme (USA), provide a comprehensive holistic defintion of moral injury based on the systematic reviews of Jinkerson plus Hodgson and Carey. \n\n\"Moral injury is a trauma related syndrome caused by the lasting physical, psychological, social and spiritual impact of grievous moral transgressions or violations of an individual’s deeply held moral beliefs and/or ethical standards due to (i) the betrayal of what is right by trusted individuals who hold legitimate authority and/or (ii) by an individual perpetrating, failing to prevent, bearing witness to, or learning about inhumane acts which result in the pain, suffering or death of others and which fundamentally challenges the moral integrity of an individual, organisation or community.\"\n\n\"The violation of deeply-held moral beliefs and ethical standards—irrespective of the actual context of trauma—can lead to considerable moral dissonance, which if unresolved, leads to the development of core and secondary symptoms that often occur concurrently.  The core symptoms commonly identifiable are: (a) shame, (b) guilt, (c) a loss of trust in self, others, and/or transcendental/ultimate beings, and (d) spiritual/existential conflict including an ontological loss of meaning in life.  These core symptomatic features, influence the development of secondary indicators such as (a) depression, (b) anxiety, (c) anger, (d) re-experiencing the moral conflict, (e) social problems (e.g., social alienation) and (f) relationship issues (e.g., collegial, spousal, family), and ultimately (g) self-harm (i.e., self-sabotage, substance abuse, suicidal ideation and death).\" \n\nCarey and Hodgson, using the World Health Organization ICD-10-AM 'Spiritual Intervention Codings' (WHO-SPICs) as a paradigm, argue that clergy/chaplains have four key roles with regard to moral injury: (i) Spiritual Assessments (including screening), (ii) Spiritual Education and Counselling, (iii) Spiritual Support and (iv) Spiritual Ritual and Worship activities. The WHO-SPICs provide a framework for not only ensuring and evaluating the breadth and provision of quality spiritual care, but in addition (given contemporary moral injury is a relativey recent phenomena), the WHO-SPICs also provide a framework for considering the appropriate resourcing and implemention of such interventions for the benefit of those suffering the effects of a moral injury.\n\nThere is little that is known about the treatment of moral injury. Gaudet and colleagues (2015) suggest that current interventions are lacking and new treatment interventions specific to moral injury are necessary. It is not enough to treat moral injury in the same way that depression or PTSD are commonly treated. In spite of the lack of research on the treatment of moral injury, factors such as humility, gratitude, respect and compassion have shown to either be protective or provide for hope for service members.\n\nAlthough there is a delineation between PTSD and moral injury, the shame that many individuals face as a result of moral injury may predict symptoms of posttraumatic stress disorder. When considering the impact of shame in PTSD, shame is known to be highly correlated with each cluster of symptoms of PTSD. Although no definitive treatment for moral injury has been found, it is hypothesized that treating the underlying shame that is often associated with service member’s symptoms of PTSD is necessary. Additionally, it has been shown that allowing feelings of shame to go untreated can have deleterious effects. This can often make the identification of moral injury in a service member difficult because shame tends to increase slowly over time. Shame has been linked to complications such as interpersonal violence, depression, and suicide. Although there are no systematic reviews or meta-analyses on the treatment of moral injury, Litz and colleagues (2009) have hypothesized a modified version of CBT that addresses three key areas of moral injury: “life-threat trauma, traumatic loss, and moral injury Marines from the Iraq and Afghanistan wars.” Although a significant amount of research on moral injury and specifically the treatment of it is still lacking, these proposed treatments and protective factors provide researchers with a starting foundation.\n\nMoral injury in the context of healthcare was directly named in the \"Stat News\" article by Drs. Wendy Dean and Simon Talbot, entitled \"Physicians aren’t ‘burning out.’ They’re suffering from moral injury.\" The article and concept goes on to explain that physicians (in the United States) are caught in double and triple and quadruple binds between their obligations of electronic health records, their own student loans, the requirements for patient load through the hospital and number of procedures performed – all while working towards the goal of trying to provide the best care and healing to patients possible. However, the systemic issues facing physicians often cause deep distress because the patients are suffering, despite a physician's best efforts. This concept of moral injury in healthcare is the expansion of the discussion around compassion fatigue and 'burnout.'\n"}
{"id": "53751629", "url": "https://en.wikipedia.org/wiki?curid=53751629", "title": "Naegele obliquity", "text": "Naegele obliquity\n\nNägele's obliquity is the presentation of the anterior parietal bone to the birth canal during vaginal delivery with the biparietal diameter being oblique to the brim of the pelvis. The synonym for this presentation is anterior asynclitism. It was first described in 1777 by German Karl Nägele.\n"}
{"id": "46559303", "url": "https://en.wikipedia.org/wiki?curid=46559303", "title": "Non-specific effect of vaccines", "text": "Non-specific effect of vaccines\n\nNon-specific effects of vaccines (also called \"heterologous effects\" or \"off-target effects\") are effects which go beyond the specific protective effects against the targeted diseases. Non-specific effects can be strongly beneficial, increasing protection against non-targeted infections, but also at times negative, increasing susceptibility to non-targeted infections. This depends on both the vaccine and the sex of the infant.\n\nAll live attenuated vaccines studied so far (BCG vaccine, measles vaccine, oral polio vaccine, smallpox vaccine) have been shown to reduce mortality more than can be explained by prevention of the targeted infections. In contrast, inactivated vaccines (diphtheria-tetanus-pertussis vaccine (DTP), hepatitis B vaccine, inactivated polio vaccine) may increase overall mortality despite providing protection against the target diseases.\n\nThese effects may be long-lasting, at least up to the time point where a new type of vaccine is given. The non-specific effects can be very pronounced, with significant effects on overall mortality and morbidity. In a situation with herd immunity to the target disease, the non-specific effects can be more important for overall health than the specific vaccine effects.\n\nThe non-specific effects should not be confused with the side effects of vaccines (such as local reactions at the side of vaccination or general reactions such as fever, head ache or rash, which usually resolve within days to weeks – or in rare cases anaphylaxis). Rather, non-specific effects represent a form of general immunomodulation, with important consequences for the immune system's ability to handle subsequent challenges.\n\nIt is estimated that millions of child deaths in low income countries could be prevented every year if the non-specific effects of vaccines were taken into consideration in immunization programs.\n\nThe hypothesis that vaccines have non-specific effects was formulated in the early 1990s by Peter Aaby at the Bandim Health Project in West Africa.\n\nThe first indication of the importance of the non-specific effects of vaccines came in a series of randomized controlled trials (RCTs) in the late 1980s. It was tested whether a high-titer (high-dose) measles vaccine (HTMV) given at 4–6 months of age was as effective against measles infection as the standard measles vaccine (MV) given at 9 months of age. Early administration of the HTMV prevented measles infection just as effectively as did the standard MV given at 9 months of age.\n\nHowever, early administration of the HTMV was associated with twofold \"higher\" overall mortality among females (there was no difference in mortality for males). In other words, the girls given HTMV died more often despite having the same protection against measles as the infants given standard MV. The discovery forced WHO to withdraw the HTMV in 1992. It was later discovered that it was not the HTMV, but rather a subsequent inactivated vaccine (DTP or IPV for different children), that caused the increase in female mortality. Although the mechanism was different than initially thought, this finding represents unexpected effects of a change in the vaccine program not attributable to the disease-specific protection provided by the vaccines.\n\nThis first observation that vaccines could protect against the target disease but at the same time affect mortality after infection with other pathogens, in a sex-differential manner, led to several further studies showing that other vaccines might also have such nonspecific effects.\n\nNumerous observational studies and randomised trials (RCTs) have found that the impact on mortality of live and inactivated vaccines differ markedly. All live vaccines studied so far (BCG, measles vaccine, oral polio vaccine (OPV) and smallpox vaccine) have been shown to reduce mortality more than can be explained by prevention of the targeted infection(s). In contrast, inactivated vaccines (diphtheria-tetanus-pertussis (DTP), hepatitis B, inactivated polio vaccine) may have deleterious effects in spite of providing target disease protection.\n\nThe live attenuated BCG vaccine developed against tuberculosis has been shown to have strong beneficial effects on the ability to combat non-tuberculosis infections.\n\nSeveral studies have suggested that BCG vaccination may reduce atopy, particularly when given early in life. Furthermore, in multiple observational studies BCG vaccination has been shown to provide beneficial effects on overall mortality. These observations encouraged randomised controlled trials to examine BCG vaccination's beneficial non-specific effects on overall health. Since BCG vaccination is recommended to be given at birth in countries that have a high incidence of tuberculosis it would have been unethical to randomize children into 'BCG' vs. 'no BCG' groups. However, many low-income countries delay BCG vaccination for low-birth-weight (LBW) infants; this offered the opportunity to directly test the effect of BCG on overall mortality.\n\nIn the first two randomised controlled trials receipt of BCG+OPV at birth vs. OPV only ('delayed BCG') was associated with strong reductions in neonatal mortality; these effects were seen as early as 3 days after vaccination. BCG protected against sepsis as well as respiratory infections. \nAmong BCG vaccinated children, those who develop a BCG scar or a positive skin test (TST) are less likely to develop sepsis and exhibit an overall reduction in child mortality of around 50%.\n\nIn a recent WHO-commissioned review based on five clinical trials and nine observational studies, it was concluded that \"the results indicated a beneficial effect of BCG on overall mortality in the first 6–12 months of life. Relevant follow-up in some of the trials was short, and all of the observational studies were regarded as being at risk of bias, so the confidence in the findings was rated as very low according to the GRADE criteria and \"There was a suggestion that BCG vaccination may be more beneficial the earlier it is given\". Furthermore, \"estimated effects are in the region of a halving of mortality risk\" and \"any effect of BCG vaccine on all-cause mortality is not likely to be attributable to any great extent to fewer deaths from tuberculosis (i.e. to a specific effect of BCG vaccine against tuberculosis)\". Based on the evidence, the WHO's Strategic Group of Experts on Immunization concluded that \"the non-specific effects on all-cause mortality warrant further research\".\n\nStandard titer measles vaccine is recommended at 9 months of age in low-income countries where measles infection is endemic and often fatal. Many observational studies have shown that measles-vaccinated children have substantially lower mortality than can be explained by the prevention of measles-related deaths. Many of these observational studies were natural experiments, such as studies comparing the mortality before and after the introduction of measles vaccine and other studies where logistical factors rather than maternal choice determined whether a child was vaccinated or not.\n\nThese findings were later supported in randomized trials from 2003 to 2009 in Guinea-Bissau. An intervention group of children given standard titer measles vaccine at 4.5 and 9 month of age had a 30% reduction in all-cause mortality compared to the children in the control group, which were only vaccinated against measles at 9 month of age.\n\nIn a recent WHO-commissioned review based on four randomized trials and 18 observational studies, it was concluded that \"There was consistent evidence of a beneficial effect of measles vaccine, although all observational studies were assessed as being at risk of bias and the GRADE rating was of low confidence. There was an apparent difference between the effect in girls and boys, with girls benefitting more from measles vaccination\", and furthermore \"estimated effects are in the region of a halving of mortality risk\" and \"if these effects are real then they are not fully explained by deaths that were established as due to measles\". Based on the evidence, the WHO's Strategic Advisory Group of Experts on Immunization concluded that \"the non-specific effects on all-cause mortality warrant further research\".\n\nDTP vaccine against diphtheria, tetanus and pertussis does not seem to have the same beneficial effects as BCG, measles vaccine, OPV and smallpox vaccine, and in fact opposite effects are observed. The negative effects are seen as long as DTP vaccine is the most recent vaccine. BCG or measles vaccine given after DTP reverses the negative effects of DTP. The negative effects are seen mostly in females.\n\nThe negative effects are found in several observational studies. However, six WHO-commissioned studies concluded that there were strong beneficial effects of DTP on overall mortality. However, controversy ensued as these studies had important methodological shortcomings. For example, the WHO-commissioned studies had counted \"no information about vaccination\" as \"unvaccinated\", and they had retrospectively updated vaccine information from surviving children, while no similar update could be made for dead children, creating a so-called \"survival bias\" which will always produce highly beneficial effect estimates for the most recent vaccine.\n\nIn a recent WHO-commissioned review of DTP based on ten observational studies, it was concluded that, \"the findings were inconsistent, with a majority of the studies indicating a detrimental effect of DTP, and two studies indicating a beneficial effect. All of the studies were regarded as being at risk of bias, so the confidence in the findings was rated as very low according to the GRADE criteria.\"\n\nFurthermore, \"three observational studies provided a suggestion that simultaneous administration of BCG and DTP may be preferable to the recommended schedule of BCG before DTP; and there was suggestion that mortality risk may be higher when DTP is given with, or after, measles vaccine compared with when it is given before measles vaccine (from five, and three, observational studies, respectively). These results are consistent with hypotheses that DTP vaccine may have detrimental effects on mortality, although a majority of the evidence was generated by a group centred in Guinea-Bissau who have often written in defence of such a hypothesis.\"\n\nWhen smallpox vaccine was introduced in the early 19s century, there were anecdotal descriptions of non-specific beneficial effects. In the second half of the 20th century the potential for beneficial non-specific effects of smallpox vaccine was reviewed, and new evidence on \"para-immune effects\" was added. More recent studies have focused on the phasing out of smallpox vaccine in the 1970s and compared vaccinated and unvaccinated cohorts.\nSmallpox vaccine leaves a very characteristic scar. In low-income countries, having a smallpox vaccine scar has been associated with reductions of more than 40% in overall mortality among adults; in high-income countries smallpox vaccination has been associated with a tendency for reduced risk of asthma, and significantly reduced risk of malignant melanoma and infectious disease hospitalizations. There are no studies that contradict these observations. However it should be noted that no randomized trials testing the effect of smallpox vaccine on overall mortality and morbidity have been conducted.\n\nNon-specific effects are frequently different in males and females. There are accumulating data illustrating that males and females may respond differently to vaccination, both in terms of the quality and quantity of the immune response. If true, then we must consider whether vaccination schedules should differ for males and females, or as has been suggested \"should we treat the sexes differently in order to treat them equally?\"\n\nThe non-specific effects of vaccines can be boosted or diminished when other immunomodulating health interventions such as other vaccines, or vitamins, are provided.\n\nThe beneficial NSEs of live vaccines are stronger with earlier vaccination, possibly due to maternal antibodies. Boosting with live vaccines also seems to enhance the beneficial effects.\n\nThe non-specific effects were primarily observed in low-income countries with high infectious disease burdens, but they may not be limited to these areas. Recent Danish register-based studies have shown that the live attenuated measles-mumps-rubella vaccine (MMR) protects against hospital admissions with infectious diseases and specifically getting ill by respiratory syncytial virus.\n\nThe findings from the epidemiological studies on the non-specific effects of vaccines pose a challenge to the current understanding of vaccines, and how they affect the immune system, and also question whether boys and girls have identical immune systems and should receive the same treatment.\n\nThe mechanisms for these effects are unclear. It is not known how vaccination induces rapid beneficial or harmful changes in the general susceptibility to infectious diseases, but the following mechanisms are likely to be involved.\n\nIt is well known from animal studies that infections, apart from inducing pathogen-specific T-cells, also induce cross-reactive T-cells through epitope sharing, so-called heterologous immunity. Heterologous T-cell immunity can lead to improved clearance of a subsequent cross-reactive challenge, but it may also lead to increased morbidity. This mechanism may explain why DTP could have negative effects.\n\nIt would, however, not explain effects occurring shortly after vaccination, as for instance the rapidly occurring beneficial effects of BCG vaccine, as the heterologous effect would only be expected to be present after some weeks, as the adaptive immune response need time to develop. Also, it is difficult to explain why the effect would vanish once a child receives a new vaccine.\n\nThe concept that not only plants and insects, but also humans have innate immune memory may provide new clues to why vaccines have non-specific effects. Studies into BCG have recently revealed that BCG induces epigenetic changes in the monocytes in adults, leading to increased pro-inflammatory cytokine production upon challenges with unrelated mitogens and pathogens (trained innate immunity).\n\nIn SCID mice that have no adaptive immune system, BCG reduced mortality from an otherwise lethal candida infection. The effects of BCG presented when tested after 2 weeks, but would be expected to occur rapidly after vaccination, and hence might be able to explain the very rapid protection against neonatal septicaemia seen after BCG vaccine.\n\nTrained innate immunity may also explain the generally increased resistance against broad disease categories, such as fevers and lower respiratory tract infections; such effects would be difficult to explain merely by shared epitopes, unless such epitopes were almost universally common on pathogens.\n\nLastly, it is plausible that the effects are reversible by a different vaccine. Hence, trained innate immunity may provide a biological mechanism for the observed non-specific effects of vaccines.\n\nIn 2000 Aaby and colleagues presented data from Guinea-Bissau which suggested that DTP vaccination could, under some circumstances (e.g. absence of pertussis) be associated with increases in overall mortality, at least until children received measles vaccine. In response, WHO sponsored the analysis of a variety of data sets in other populations to test the hypothesis. None of these studies replicated the observation of increased mortality associated with DTP vaccination. WHO subsequently concluded, that the evidence was sufficient to reject the hypothesis for an increased nonspecific mortality following DTP vaccination.\n\nHowever, Aaby and colleagues subsequently pointed out that the studies which failed to show any mortality increase associated with DTP vaccination used methods of analysis that can introduce a bias against finding such an effect.\n\nIn these studies, data on childhood vaccinations were typically collected in periodic surveys, and the information on vaccinations, which occurred between successive home visits, was updated at the time of the second visit. The person-time at risk in unvaccinated and vaccinated states was then divided up according to the date of vaccination during the time interval between visits. This method opens up a potential bias, insofar as the updating of person time at risk from unvaccinated to vaccinated is only possible for children who survive to the second follow-up. Those who die between visits typically do not have vaccinations between the first visit and death recorded, and thus they will tend to be allocated as deaths in unvaccinated children – thus incorrectly inflating the mortality rate among unvaccinated children.\n\nThis bias has been described before, but in different contexts, as the distinction between 'landmark' and 'retrospective updating' analysis of cohort data. The retrospective updating method can lead to a considerable bias in vaccine studies, biasing observed mortality rate ratios towards zero (a large effect), whereas the landmark method leads to a non-specific misclassification and biases the mortality rate ratio towards unity(no effect).\n\nAn additional problem with the literature on the nonspecific effects of vaccines has been the variety and unexpected nature of the hypotheses which have appeared (in particular relating to sex-specific effects), which has meant that it has not always been clear whether some apparent 'effects' were the result of post hoc analyses or whether they were reflections of a priori hypotheses.\n\nThis was discussed at length at a review of the work of Aaby and his colleagues in Copenhagen in 2005. The review was convened by the Danish National Research Foundation and the Novo Nordisk Foundation who have sponsored much of the work of Aaby and his colleagues. An outcome of the review was the explicit formulation of a series of testable hypotheses, agreed by the Aaby group. It was hoped that independent investigators would design and conduct studies powered to confirm or refute these hypotheses.\n\nAlso, the two foundations sponsored a workshop on the analysis of vaccine effects, which was held in London in 2008. The workshop resulted in three papers. The proceedings were forwarded to WHO which subsequently concluded that it would \"keep a watch on the evidence of nonspecific effects of vaccination\".\n\nIn 2013, WHO established a working group tasked with reviewing the evidence for the non-specific effects of BCG, measles and DTP vaccines. Two independent reviews were conducted, an immunological review and an epidemiological review. The results were presented at the April 2014 meeting of WHO's Strategic Gourp of Experts on Immunizations (SAGE). WHO/SAGE concluded that further research into the potential NSEs of vaccines was warranted.\n\nIt would have major consequences for child survival if the non-specific effects of vaccines were taken into consideration in immunization programs: BCG and MV should be given to all children as early as possible; restrictive policies for opening multi-dose vials of BCG and MV should be abandoned.\n\nContrary to current WHO-recommendations, the age of MV should not be raised when measles infection is under control; DTP should not be given simultaneously with MV or after MV; and a booster dose of DTP is likely to have a negative effect on child survival. Finally, eradicating a disease and stopping a live vaccine with beneficial NSEs is likely to have negative effects for the overall health of the affected population.\n\nDr. Frank Shann from Australia recently assessed the consequences of changing the current EPI schedule to an alternative schedule taking non-specific effects into account, and concluded: \"If all neonates in high-mortality regions were given BCG at birth, and the revised immunization schedule ... were adopted, with extra doses of measles vaccine at 14 weeks and 19 months (at a cost of only US $0.60/dose delivered), ~1 million (30%) of the 3.2 million neonatal deaths each year might be prevented in developing countries, and 1.5 million (30%) of the 4.8 million deaths between 1 month and 5 years of age might be prevented\". Furthermore: \"This very large reduction in mortality in children <5 years of age would be achieved at a low cost using only vaccines that are already in the routine EPI schedule\".\n\nIn 2008, Danish crime novel author Sissel-Jo Gazan (author of the Danish crime novel \"Dinosaur Feather\") became interested in the work of the Bandim Health Project and based her science crime novel \"The Arc of the Swallow\" (\"Svalens Graf\") on the research into non-specific effects of vaccines.\n\nThe novel was published in Danish in 2013; it was on the best-seller list for months and won the Readers' Prize 2014 in Denmark. It was published in English in the UK on November 6, 2014 and in the US on April 7, 2015.\n\n"}
{"id": "57043182", "url": "https://en.wikipedia.org/wiki?curid=57043182", "title": "Occupational health concerns of cannabis use", "text": "Occupational health concerns of cannabis use\n\nOccupational health concerns are becoming increasingly important as cannabis becomes legal in more areas of the US. Of note, employers have concerns of workers either coming to work acutely impaired or recent use of cannabis still being detected in the body. Employment issues such as ADA law as it relates to accommodations for cannabis, paying unemployment benefits or paying out workers compensation benefits and disability claims are all important issues. While federal law still prohbitis use, employers in different states have taken different stances based on whether they are federal contractors, perform safety sensitive work or whether the cannabis use is acutely impairing the employee.\n\nCannabis is currently the most commonly used illicit drug in the world and one of the earliest plants cultivated by humans. Early evidence of cannabis use in medicine has been found in China and India for religious and medicinal uses. Archaeological research shows early civilization cultivation of hemp in India to create a variety of products from ropes, textiles and paper. In countries, such as Nepal and China, cannabis seeds are still used today both as a pharmaceutical as well as a food enhancer.\n\nAs cannabis use increases and more states legalize use in the US, there are concerns surrounding the use of cannabis and its effects on job performance and safety. There are also concerns that the concentration of THC have increased over the past 50 years, thereby increasing the effects of the drug.\n\nThe Controlled Substances Act of 1970 currently lists cannabis as a Schedule 1 drug, deeming it to be a substance with high risk of dependence, abuse and no (supervised) medical uses.\n\nThe CATO institute estimates that legalizing drugs would save $41 billion per year along with $46.7 billion in revenue, if taxed at the same rates as alcohol and tobacco.\n\nEmployers and employees have faced new challenges in the workplace with the increasing legislation of cannabis. State law can have provisions, for an employer to refuse to hire based on marijuana use, under the concern of safety, productivity and company reputation. Companies that have “safety sensitive work” or include operating machinery or large vehicles are also free to institute a zero-tolerance policy for its employees. For the approximate 10 million CDL (Commercial Drivers Licenses) drivers in the USA, federal law requires they pass employer drug tests under the Omnibus Transportation Employee Safety Act of 1991.\n\nIndividual states in response to the federal government differed in how they dealt with cannabis. California was the first state to pass medical marijuana in 1996, followed by 23 other states currently permitting the use of medical marijuana.\n\nIn the case of U.S. v. Stacey among several others, the legality of cannabis use has been contested since citizens who argue for protection under state law, are always subject to federal charges While there has been some guidance from the Department of Justice to the DEA to reassess its priorities in the drug war and to target larger threats and substances, the interpretation is still subjective and has not prevented large scale raids on medical marijuana facilities across the country.\n\nCalifornia was the first state to pass medical marijuana in 1996, followed by 23 other states currently permitting the use of medical marijuana For the states that have legalized medical marijuana, employees and employers have had to face new challenges in the workplace. State law can have provisions, for an employer to refuse to hire based on marijuana use, under the concern of safety, productivity and company reputation. Companies that have a lot of “safety sensitive work” or include operating machinery or large vehicles are also free to institute a zero-tolerance policy for its employees. For the approximate 10 million CDL (Commercial Drivers Licenses) drivers in the USA, federal law requires they pass employer drug tests under the Omnibus Transportation Employee Safety Act of 1991.\n\nDrivers that use cannabis show impairment both in simulator and on the road tests, mainly on the abilities to “..concentrate and maintain attention, estimate time and distance, and demonstrate coordination on divided attention tasks—all important requirements for operating a motor vehicle”\n\nThe Department of Transportation (DOT) has also stated in its Drug and Alcohol Testing Regulations that cannabis is not acceptable for any employee engaged in safety sensitive work.Currently, there are federal laws that give employers guidance and protection on how to ensure a drug free workplace despite the increase in cannabis use. According to the National Association of Attorneys General (NAAG), in Colorado the number of positive drug tests for marijuana went up by 20% between 2012-2013, compared to the national average increase of 5%\n\nFor those companies that have received federal grants and have federal contracts over $100,000, they follow The Drug Free Workplace Act of 1988, a comprehensive policy, which includes drug prevention methods, information about employee assistance programs and disciplinary consequences of drug use in the workplace Federal agencies are required to use the Substance Abuse and Mental health Services Administration (SAMHSA) procedures for testing and recommend private employers use the same criteria for consistency \n\nThe policy requires employees to report any criminal drug violations in the workplace to the employer within 5 calendar days which may subsequently result in discipline including termination, regardless of state law \n\nThe Occupational and Safety Act of 1970 includes a general duty clause for employers to protect employees from exposure for those compounds with no regulated standards of safe exposure. It has been defined broadly as to “maintain conditions or adopt practices reasonably necessary and appropriate to protect workers on the job. The ADA prohibits employers from discriminating against disabled individuals, which is defined as “a physical or mental impairment that substantially limits one or more major life activities, a person who has a history or record of such an impairment, or a person who is perceived by others as having such an impairment.”\n\nEmployers under this law have an obligation to provide “reasonable accommodations” to the disabled employee, so they may perform the essential duties of the job, as long as those accommodations don’t pose an undue hardship on the employer\". The ADA section 12114(a) however states that these rights are not granted to employee or applicant who is currently engaging in the illegal use of drugs The ADA does not require employer coverage when the employee is engaging in the “illegal use of drugs”, including marijuana which is still illegal under the Controlled Substance Act\n\nCoats v. Dish Network LLC, held that a “state’s lawful activities statute does not protect a worker’s off-duty use of medical marijuana because this activity is not lawful under federal law” under the ordinary meaning of “permitted by law” \n\nThe Federal Employees Compensation Act (FECA) provides for the payment of workers’ compensation benefits, including wages and medical benefits, to civilian officers and employees of all branches of the federal government, whereas state law has oversight over its own workers’ compensation program\n\nFECA prohibits employees from receiving compensation when the injury “was proximately… caused by the intoxication by… illegal drugs” \n\nColorado Court of Appeals in Beinor v Industrial Claim Appeals\n\nAgreed with the employer and held that unemployment insurance benefits could be withheld due to the violation of the employer’s zero tolerance drug policy. Worker was a sweeper using cannabis legally outside of working hours\n\nColorado State statue “disqualifies an individual from receiving unemployment benefits after the presence of a controlled substance was not medically prescribed was found in the worker’s system during worker hours.”\n\nCannabis has unique pharmacokinetics and metabolism which makes it a challenge to drug test for acute impairment vs recent use. THC is deposited in fat and can be detectable in urine for up to a month or more. However, it is not always an indicator of acute impairment. Further there are privacy concerns, from invasive blood testing for compounds, under the 4th Amendment of the US constitution. Variability in study designs, ethical dilemma of safety and productivity, work type and drug metabolism in users. When smoked, blood levels of THC immediately rise and are distributed primarily in the lungs and brain. •THC is metabolized to the transient psychoactive substance THC-OH, which is finally converted to THC-COOH, the physiologically inactive metabolite\n\nThe immediate euphoria and subsequent impairment from THC-OH, as suggested by older studies may last about 6 hours, with some limited studies showing impairment lasting as long as 24 to 48 hours Specific studies that assess driving and impairment show a return to \"...nonimpaired state within 3-6 hours“\n\nQuickly sequestered into fat tissues therefore decreasing blood concentration\n\nIngesting cannabis has much longer magnified effect of the drug\n\nThe method of testing cannabis in urine can be misleading, due to the potential for interpreting a positive result for THC-COOH, as an indicator of acute impairment.\n\nLack of concentration, impaired learning and memory, alterations in thought formation, expression and sedation. Physiological signs include tachycardia, dilated pupils with injected conjunctiva, dry mouth and increased appetite Studies have also confirmed that impairment is related on a dose response relationship.\n\nFSMB (Federal State Medical Boards) after reviewing over 50 major studies in the field, have offered these 10 recommendations\n\n1)Thoroughly document patient’s visit prior to considering cannabis use for those with good compliance Inadequacy of response to other treatment, comprehensive history and physical\n\n2) Don’t treat close friends or family\n\n3) Inform risks and benefits\n\n4) Compliance Contract, concerns of diversion\n\n5) Regular Visits\n\n6) No relationships with dispensaries or cannabis companies\n\nA large study in France of over 10,000 crashes, concluded an odds ratio of 2.18 for THC level less than 1 ng/ml up to 4.72 for THC 5 ng/ml for being involved in a crash \n\nSome researchers such as \"Donzé N\", \"Ménétrey\" have considered categorizing impairment as the sum of THC, THC-OH and THC-COOH, which would be more accurate for assessing impairment since it would include the active metabolite; however, more research is needed along with better point of care detection\n\nSome Norwegian studies have suggested that individuals with levels of 2-5 ng/mL experienced “moderate established impairment” and levels of THC above 5 -10 ng/ml was likely to be severe impairment.\n\nThe consensus from a meta-analysis study shows that serum levels of an average of 3.8 (ranging from 3.1 to 4.5) for oral cannabis and an average of 3.8 (ranging from 3.3 to 4.5) for smoker cannabis cause a similar impairment to a BAC of around 0.05g/dL, i.e. a 5 ng/mL cutoff would resemble the level of impairment allowed for alcohol under federal testing laws\n\nHowever, since the presence of metabolites does not automatically indicate impairment, an immediate physical exam by a physician is advised to correlate findings clinically to diagnose impairment \n"}
{"id": "54337892", "url": "https://en.wikipedia.org/wiki?curid=54337892", "title": "Online social support", "text": "Online social support\n\nOnline social support is a new form of social support that produced by development of internet, the more people are engaging to express and discuss with other via online community, the more online community getting similar with the social community and have the similar relation between social support and subjective well being. According to Robbins and Rosenfeld (2001), traditionally, listening\nsupport, confirmation, and appreciation are sources of subjective well-being. And Liu\nand Yu (2013) have stated validation, compliment, and encouragement are the most\ncommon types of support from the online community. Also, online friends can be an\nimportant source of social support (Ybarra, Mitchell, Palmer, & Reisner, 2015).\n\nMoreover, the number of Facebook friends associated with stronger perceptions of social support, which in turn associated with reduced stress, and in turn less physical illness and greater well-being which found by Nabi and So (2013).\n\nRozzell, et al. (2014) said that social media tools may allow for social support to be obtained from non-close as well as close relationships, with access to a significant proportion of non-close relationships. Moreover, social support derived from new information and communication technology counteracts the adverse effect of being unemployed (Fieseler, Meckel, & Müller, 2014).\n"}
{"id": "53649451", "url": "https://en.wikipedia.org/wiki?curid=53649451", "title": "PAS 43", "text": "PAS 43\n\nPAS 43 is a British Standard for safe working in recovery of broken-down vehicles.\n\n, the latest standard is PAS 43:2015.\n"}
{"id": "49986510", "url": "https://en.wikipedia.org/wiki?curid=49986510", "title": "Paula method", "text": "Paula method\n\nThe Paula Method is a proposed alternative to Kegel exercises. The idea is that by strengthening your sphincter muscles (eye muscle: orbicularis oculi and mouth muscle: orbicularis oris), the contractions would also strengthen the sphincter muscles in the pelvic floor. Evidence to support its use is lacking.\n"}
{"id": "621189", "url": "https://en.wikipedia.org/wiki?curid=621189", "title": "Perineal massage", "text": "Perineal massage\n\nAntenatal perineal massage (APM) or birth canal widening (BCW) is the massage of a pregnant woman's perineum around the opening to the vagina, performed anywhere in the 4 to 6 weeks before childbirth and usually on 4-6 separate occasions. The practice aims to more gently mimic the massaging action of a baby's head on the opening to the birth canal prior to birth, which enables some of the hard work of labour to be done before the start of labour. The intention is to attempt to prevent tearing of the perineum during birth, and reduce the need for an episiotomy or an instrument (forceps or vacuum extraction) delivery.\n\nThe Oxford Radcliffe NHS Trust Document \"Antenatal Perineal Massage\" 2011 describes the use of the pregnant mother's thumbs being placed just inside the birth canal, whilst she stands with one foot supported on the toilet. She pulls backwards towards her spine, whilst relaxing her pelvic floor, progressively increasing the pressure under her finger tips until this starts to feel uncomfortable. On the next occasion she uses both thumbs pulling backwards and then sideways, thumbs away from each other, to enlarge the 2 cm diameter opening of the birth canal progressively over time to 10 cm. It is not possible to stretch this opening further causing damage, because this will be limited by the distance between the bony walls of the pelvis.\n\nThe goal of APM is to prevent the baby's head from undergoing excessive strain during the last 30 minutes of labour. It seeks to train the mother to relax her pelvic floor to allow the baby's head to pass through the opening, to stretch the two fibrous layers within the Uro-Genital Membrane, a triangular shaped muscular shelf at the front half of the opening of the bony pelvis, through which the 2 cm diameter birth canal and urethra pass, and to transform the fat packed rigid skin at the opening to the birth canal into paper thin stretchy elastic skin, all without using the baby's head. \nIf the external skin (perineal skin) opening has been stretched before birth, to 10 cm, then there is no reason to perform an episiotomy to increase the diameter of the opening of the birth canal. Tearing is less likely as the external skin at the opening has been stretched already and is lax, whilst the underlying muscular pelvic floor has not been damaged. Episiotomy permanently damages the pelvic floor muscle, as the episiotomy cuts through the nerve supply to this muscle, so the larger part of the pelvic floor muscles atrophies and becomes replaced by scar tissue, increasing the mother's chance of developing a prolapse in the future. Antenatal Perineal Massage does not damage the pelvic floor, so protects against a prolapse.\n\nAntenatal perineal massage or pre-birth obstetric massage (birth canal widening) has been used for many generations both by Chukchi Eskimos in Siberia and by African tribal people. Using an index finger and progressing to index, middle and ring finger massage, performed by the pregnant mother's spouse, after sexual intercourse, when the mother is relaxed has also been described enabling the father to take an active role in aiding the birth. Glass bottles of increasing sizes have also been used as the instrument of massage.\n\nThe German obstetrician Mr Welheim Horkel, when visiting a medical mission in the mid-1980s, learned that African tribes used gourds of increasing sizes as instruments of massage, with mothers aiding their daughters' labours in this way.\n\nWest Berkshire England performed an Antenatal Massage trial in 1984, and many small trials have been performed worldwide since. Cochrane Collaborate Reports since 2006 have advised that women should be informed that episiotomy is avoidable if they employ digital Antenatal Perineal Massage.\n\nA randomized clinical trials of perineal massage found similar results in the massage and control groups. They did find a reduced rate of class two and three level tears but did not find a reduction in the risk of pain, urinary or fecal incontinence, or dyspareunia.. \nThe trail titled 'The possibility of antepartal prevention of episiotomy and perineal tears during delivery', concluded in 'Retrospective study at Nemocnice Český Krumlov, Methods: Between February 2014 and November 2015, 315 primiparous women questioned after a vaginal delivery, on the use of methods of birth injury prevention (vaginal dilatators EPI-NO and Aniball, perineal massage, natural methods – raspberry-leaf tea or linseed' concluded that 'There was a significantly higher number of women with intact perineum after the use of vaginal dilatators (43.1% vs.14.1% in control group (p < 0.001)' and 'We also found a significant reduction of episiotomies in this group (29.3% vs. 57.7%, p < 0.001). titled The Journal of Obstetrics and Gynaecology of Czechoslovakia| url=https://aniball.nl/wp-content/uploads/2016/08/Onafhankelijk-onderzoek-1.pdf.\n\nAntenatal Perineal Massage affords mothers worldwide, irrespective of income, the opportunity to shorten the critical last 30 minutes of labour. Mothers with a narrowed opening in their bony pelvis, whether from being born with an abnormally narrow pelvis, from previous fracture or from deformity secondary to infection in the bone (osteomyelitis), may need surgical intervention, as indicated by a failure to progress either when the baby's head fails to enter the bony pelvis or develops fetal distress.\n\n"}
{"id": "573313", "url": "https://en.wikipedia.org/wiki?curid=573313", "title": "Physical disability", "text": "Physical disability\n\nA physical disability is a limitation on a person's physical functioning, mobility, dexterity or stamina. Other physical disabilities include impairments which limit other facets of daily living, such as respiratory disorders, blindness, epilepsy and sleep disorders.\n\nPrenatal disabilities are acquired before birth. These may be due to diseases or substances that the mother has been exposed to during pregnancy, embryonic or fetal developmental accidents or genetic disorders.\n\nPerinatal disabilities are acquired between some weeks before to up to four weeks after birth in humans. These can be due to prolonged lack of oxygen or obstruction of the respiratory tract, damage to the brain during birth (due to the accidental misuse of forceps, for example) or the baby being born prematurely. These may also be caused due to genetic disorders or accidents.\n\nPost-natal disabilities are gained after birth. They can be due to accidents, injuries, obesity, infection or other illnesses. These may also be caused due to genetic disorders.\n\nMobility impairment includes physical defects, including upper or lower limb loss or impairment, poor manual dexterity, and damage to one or multiple organs of the body. Disability in mobility can be a congenital or acquired problem or a consequence of disease. People who have a broken skeletal structure also fall into this category.\n\nVisual impairment is another type of physical impairment. There are hundreds of thousands of people who suffer greatly from minor to various serious vision injuries or impairments. These types of injuries can also result in severe problems or diseases such as blindness and ocular trauma. Some other types of vision impairment include scratched cornea, scratches on the sclera, diabetes-related eye conditions, dry eyes and corneal graft, macular degeneration in old age and retinal detachment. \n\nHearing loss is a partial or total inability to hear. Deaf and hard of hearing people have a rich culture and benefit from learning sign language for communication purposes. People who are only partially deaf can sometimes make use of hearing aids to improve their hearing ability.\n\nPhysical impairment can also be attributed to disorders causing, among others, sleep deficiency, chronic fatigue, chronic pain, and seizures.\n\n"}
{"id": "12751414", "url": "https://en.wikipedia.org/wiki?curid=12751414", "title": "Pregnant patients' rights", "text": "Pregnant patients' rights\n\nPregnant patients rights refers to the choices and legal rights available to a mother experiencing pregnancy or childbirth. Specifically those under medical care within a medical establishment or those under the care of a medical professional regardless of location ( under care of paramedics at home, family doctor via phone, etc. ).\n\nThere are many debates that arise from pregnancy rights, ranging from whether or not fertility treatments are ‘right’ or whether using surrogate mothers is wrong. It comes down to the mother’s right. As a woman, there are more challenges than just the fundamentals of the decisions surrounding their pregnancy. Maternity leave, parental leave and the time allotted for these leaves varies from company to company.\n\nThe International Conference on Population and Development (ICPD) gathered in Cairo in September 1994 to discuss and “formulate a consensus position on population and development for the next 20 years”. One of the other goals was to make education and medical services available to women while they are pregnant, and when the time comes, have delivery options available. A main concern has always been postnatal care; people think that the hardest part is the birth of the child but there are so many additional concerns once the child is born and brought into this world. Complications both prior to pregnancy, during delivery, and after delivery are a potential concern in all births, the ICPD talked about enhancing the available support for all women. Pregnancy rights throughout the world are not going to be the same in every single place but the ICPD is aiming to eliminate discrimination during pregnancy and make all pregnant patients’ rights available to everyone.\n\nNurses and patients sometimes run into troubles because their opinions will often vary in what they think should be done in terms of termination or pre/post natal care. As Kane, 2009 states “The NMC code of professional conduct states that: ‘you must make the care of people your first concern”’ enforcing that the nurses opinions really should be kept to themselves so as to not influence the decision of the patients.\n\nIn Australia pregnant women have the same rights as any other member of society. However, they do have some extra rights when it comes to their rights in the workforce.\n\nUnder the Fair Work Act 2009 section pregnant women are still entitled to the same amount of sick leave as any other individual as being pregnant does not classify as an illness. A Pregnant woman however is able to take unpaid leave for \"special maternity leave\", this is maternity leave that she can take if she has a pregnancy related illness or the pregnancy ends any time after the first trimester due to a miscarriage, a stillbirth or a termination.\n\nSafe jobs is when the women moves to a safer job while being because her original job is dangerous to her and the baby. She will need to provide evidence that she can still work but unable to perform the original tasks and how long she shouldn't work in her job for. An example for proving these would be with a medical certificate. In the circumstances where there is no safer jobs to offer the women can take no safe jobs leave. The employee takes paid leave if she is entitled to unpaid parental leave and unpaid leave if she is not.\n\nDiscrimination in the workforce against pregnant women is illegal. This means that she can not be fired, lose hours, demoted and treated differently because of her pregnancy.\n\nIn Australia the laws on termination change between each state and territory. In Western Australia termination is considered lawful up until 20 weeks of pregnancy. A termination after 20 weeks can only be undertaken if there is two medical practitioners from a panel of six that agree that the women or the fetus has or will have a serious medical condition if the pregnancy continued.\n\nDuring pregnancy, women have the right to seek getting immunised with the influenza vaccine \"Flu Shot\" and the adult dTpa vaccine (pertussis). Both of these vaccines are recommended however it is up to the individual whether or not to go ahead with the vaccinations. \n5. Federal Register of Legislation. (2009). \"Fair Work act. R\"etrieved from https://www.legislation.gov.au/Details/C2016C00332\n\n6. Federal Register of Legislation. (2003). \"Sex Discrimination Amendment (pregnancy and work) act.\" Retrieved from https://www.legislation.gov.au/Details/C2004A01186\n\n7. Parliament of Australia.(1998). \"Abortion law in Australia\". Retrieved from http://www.aph.gov.au/About_Parliament/Parliamentary_Departments/Parliamentary_Library/pubs/rp/rp9899/99rp01\n\n8. Australian Government Department of Health. (2015) \"Immunise Australia Program.\" Retrieved from <nowiki>http://www.immunise.health.gov.au/internet/immunise/publishing.nsf/Content/pregnant-women</nowiki>\n"}
{"id": "26059978", "url": "https://en.wikipedia.org/wiki?curid=26059978", "title": "Progressive disease", "text": "Progressive disease\n\nProgressive disease or progressive illness is a disease or physical ailment whose course in most cases is the worsening, growth, or spread of the disease. This may happen until death, serious debility, or organ failure occurs. Some progressive diseases can be halted and reversed by treatment. Many can be slowed by medical therapy. Some cannot be altered by current treatments.\n\nThough the time distinctions are imprecise, diseases can be \"rapidly progressive\" (typically days to weeks) or \"slowly progressive\" (months to years). Virtually all slowly progressive diseases are also chronic diseases in terms of time course; many of these are also referred to as degenerative diseases. Not all chronic diseases are progressive: a chronic, non-progressive disease may be referred to as a \"static\" condition.\n\n\"Progressive disease\" can also be a clinical endpoint i.e. an endpoint in a clinical trial.\n\nThere are examples of slowly and rapidly progressive diseases affecting all organ systems and parts of the body. The following are some examples of rapidly and slowly progressive diseases affecting various organ systems:\n"}
{"id": "36055163", "url": "https://en.wikipedia.org/wiki?curid=36055163", "title": "Self-rated health", "text": "Self-rated health\n\nSelf-rated health (also called Self-reported health, Self-assessed health, or perceived health) refers to both a single question such as “in general, would you say that your health is excellent, very good, good, fair, or poor?” and a survey questionnaire in which participants assess different dimensions of their own health. This survey technique is commonly used in health research for its ease of use and its power in measuring health.\n\nSelf-rated health measures the present general health and gives answer choices, typically structured like a Likert Scale. The self-rated health question may take different forms. It may be formulated as “in general, would you say that you health is excellent, very good, good, fair, or poor?” as the first question in the SF-36 questionnaire. It may also be formulated as “In general, how would you rate your health today,” with the possible choices being “very good” (1), “good” (2), “moderate” (3), “bad” (4) or “very bad” (5)” as used by the World Health Organization. All questions do not necessarily have five answer choices; it can be more or less.\n\nThe self-rated health question is purposely vague so as to seize people’s own assessment of health according to their own definition of health. Although the answer to the self-rated health question is based on what people think—and thus is subjective—it is a statistically powerful predictor of mortality in the general population.\n\nThe strong association between self-rated health and mortality is used as proof that this measurement is valid, because mortality is considered as the most objective measurement of the general health of an individual.\n\nThe self-rated health question has been found to be a reliable measurement of general health since respondents rated the same general health assessment within a period where their health was unlikely to change. Despite the reliability of the measurement, the self-rated health question “in general, would you say that you health is excellent, very good, good, fair, or poor?” is particularly vague. Thus, this measurement has low level in reliability test than other self-rated measurements that assess a more specific aspect of health.\n\nInterval between response options. \n\nThe response options of the self-rated health question are unevenly spaced, substantially (excellent, very good, good, fair, or poor). Perneger et al. recommended coding response options unevenly, in proportion to the underlying construct of health and, based on Swiss data, proposed \"poor health\" as 1, \"fair\" as 2, \"good\" as 3.7, \"very good\" as 4.5, and \"excellent\" as 5. \n\nSelf-rated health, as measured by a questionnaire, attempts to measure health in all its dimensions. In such a questionnaire, participants answer a series of questions which are typically structured using a Likert Scale. The SF-36 questionnaire is an example of tool for self-assessed overall health. The SF-36 questionnaire addresses several dimensions of physical and mental health.\n\nConsidering that self-reported health is a powerful predictor of mortality and considering its easy application, this subjective measure of health is often used in health research and large-scale surveys. This measure helps follow the evolution of health across time and between populations.\n\n"}
{"id": "48994652", "url": "https://en.wikipedia.org/wiki?curid=48994652", "title": "Sexualization, Media, and Society", "text": "Sexualization, Media, and Society\n\nSexualization, Media, and Society (SMS) is a peer-reviewed, interdisciplinary open-access academic journal, published by Sage, to provide a resource for diverse scholars and activists interested in critically examining the phenomenon of sexualized media as it affects individuals, relationships, communities, and societies.\n\nThe journal was founded in 2015 by co-editors Ana Bridges (University of Arkansas), Deirdre M. Condit (Virginia Commonwealth University), Gail Dines (Wheelock College), Jennifer A. Johnson (Virginia Commonwealth University), and Carolyn West (University of Washington Tacoma).\n"}
{"id": "52905034", "url": "https://en.wikipedia.org/wiki?curid=52905034", "title": "Social Support Questionnaire", "text": "Social Support Questionnaire\n\nThe Social Support Questionnaire (SSQ) is a quantitative, psychometrically sound survey questionnaire intended to measure social support and satisfaction with said social support from the perspective of the interviewee. Degree of social support has been shown to influence the onset and course of certain psychiatric disorders such as clinical depression or schizophrenia. The SSQ was approved for public release in 1981 by Irwin Sarason, Henry Levine, Robert Basham and Barbara Sarason under the University of Washington Department of Psychology and consists of 27 questions. Overall, the SSQ has good test-retest reliability and convergent internal construct validity.\n\nThe questionnaire is designed so that each question has a two-part answer. The first part asks the interviewee to list up to nine people available to provide support that meet the criteria stated in the question. These support individuals are specified using their initials in addition to the relationship to the interviewee. Example questions from the first part includes questions such as “Whom could you count on to help if you had just been fired from your job or expelled from school?” and “Whom do you feel would help if a family member very close to you died?”.\n\nThe second part asks the interviewee to specify how satisfied they are with each of the people stated in the first part. The SSQ respondents use a 6 -point Likert scale to indicate their degree of satisfaction with the support from the above people ranging from “1 - very dissatisfied” to “6 - very satisfied”.\n\nThe Social Support Questionnaire has multiple short forms such as the SSQ3 and the SSQ6.\n\nThe SSQ is based on 4 original studies. The first study set out to determine whether the SSQ had the desired psychometric properties. The second study tried to relate SSQ and a diversity of personality measures such as anxiety, depression and hostility in connection with the Multiple Affect Adjective Checklist. The third study considered the relationship between social support, the prior year’s negative and positive life events, internal-external locus of control and self- esteem in conjunction with the Life Experiences Survey. The fourth study tested the idea that social support could serve as a buffer when faced with difficult life situations via trying to solve a maze and subsequently completing the Cognitive Interference Questionnaire.\n\nThe overall support score (SSQN) is calculated by taking an average of the individual scores across the 27 items. A high score on the SSQ indicates more optimism about life than a low score. Respondents with low SSQ scores have a higher prevalence of negative life events and illness. Scoring is as follows:\n\n1. Add the total number of people for all 27 items (questions). (Max. is 243). Divide by 27 for average item score. This gives you SSQ Number Score, or SSQN.\n\n2. Add the total satisfaction scores for all 27 items (questions). (Max is 162). Divide by 27 for average item score. This gives you SSQ Satisfaction score or SSQS.\n\n3. Finally, you can average the above for the total number of people that are family members - this results in the SSQ family score.\n\nAccording to Sarason, the SSQ takes between fifteen to eighteen minutes to properly administer and has “good” test-retest reliability.\n\nThe SSQ was compared with the depression scale and validity tests show significant negative correlation ranging from -0.22 to -0.43. The SSQ and the optimism scale have a correlation of 0.57. The SSQ and the satisfaction score have a correlation of 0.34. The SSQ has high internal consistency among items.\n\nThe SSQ has been used to show a positive correlation and dependence between Post Traumatic Stress Disorder and Social Support in a study of adolescents and long-term outcomes in Gonaives, Haiti. The study looked at the traumas stemming from the natural disasters of 2004, 2008 and 2010. The SSQ has also been used to show that higher levels of social support correlated with less suicide ideation in Military Medical University Soldiers in Iran in 2015. A low level of social support is an important risk factor in women for dysmenorrhea or menstrual cramps. Low Social Support is the strongest predictor of dysmenorrhea when compared to affect, personality and alexithymia.\n\nThe SSQ3 is a short form of the SSQ and has only three questions. The SSQ3 has acceptable test-test reliability and correlation with personality variables as compared to the long form of the Social Support Questionnaire. The internal reliability was borderline but this low level of internal reliability is as expected since there are only three questions.\n\nThe SSQ6 is a short form of the SSQ. The SSQ6 has been shown to have high correlation with: the SSQ, SSQ personality variables and internal reliability. In the development of the SSQ6, the research suggests that professed social support in adults may be a connected to “early attachment experience.” The SSQ6 consists of the below 6 questions:\n\n1. Whom can you really count on to be dependable when you need help?\n\n2. Whom can you really count on to help you feel more relaxed when you are under pressure or tense?\n\n3. Who accepts you totally, including both your worst and your best points?\n\n4. Whom can you really count on to care about you, regardless of what is happening to you?\n\n5. Whom can you really count on to help you feel better when you are feeling generally down-in-the-dumps?\n\n6. Whom can you count on to console you when you are very upset?\n\nThe Interpersonal Support Evaluation List includes 40 items (questions) with four sub-scales in the areas of Tangible Support, Belonging Support, Self-Esteem Support and Appraisal Support. The interviewee rates each item based on how true or false they feel the item is for themselves. The four total response options are “Definitely True”, “Probably True”, “Probably False”, and “Definitely False”.\n\n"}
{"id": "34104355", "url": "https://en.wikipedia.org/wiki?curid=34104355", "title": "Subjective well-being", "text": "Subjective well-being\n\nSubjective well-being (SWB) is a self-reported measure of well-being, typically obtained by questionnaire.\n\nEd Diener developed a tripartite model of subjective well-being in 1984, which describes how people experience the quality of their lives and includes both emotional reactions and cognitive judgments. It posits \"three distinct but often related components of wellbeing: frequent positive affect, infrequent negative affect, and cognitive evaluations such as life satisfaction.\"\n\nSWB therefore encompasses moods and emotions as well as evaluations of one's satisfaction with general and specific areas of one's life. Concepts encompassed by SWB include happiness. \n\nSWB tends to be stable over time and is strongly related to personality traits. There is evidence that health and SWB may mutually influence each other, as good health tends to be associated with greater happiness, and a number of studies have found that positive emotions and optimism can have a beneficial influence on health.\n\nDiener et al. argued that the various components of SWB represent distinct constructs that need to be understood separately, even though they are closely related. Hence, SWB may be considered \"a general area of scientific interest rather than a single specific construct\". Due to the specific focus on the \"subjective\" aspects of well-being, definitions of SWB typically exclude \"objective\" conditions such as material conditions or health, although these can influence ratings of SWB. Definitions of SWB therefore focus on how a person evaluates his/her own life, including emotional experiences of pleasure versus pain in response to specific events and cognitive evaluations of what a person considers a good life. Components of SWB relating to affect include positive affect (experiencing pleasant emotions and moods) and low negative affect (experiencing unpleasant, distressing emotions and moods), as well as \"overall affect\" or \"hedonic balance\", defined as the overall equilibrium between positive and negative affect, and usually measured as the difference between the two. High positive affect and low negative affect are often highly correlated, but not always.\n\nThere are two components of SWB. One is Affective Balance and the other is Life Satisfaction. An individual's scores on the two measures are summed to produce a total SWB score. In some cases, these scores are kept separate.\nAffective balance refers to the emotions, moods, and feelings a person has. These can be all positive, all negative, or a combination of both positive and negative. Some research shows also that feelings of reward are separate from positive and negative affect.\nLife satisfaction (global judgments of one's life) and satisfaction with specific life domains (e.g. work satisfaction) are considered cognitive components of SWB. The term \"happiness\" is also commonly used in regards to SWB and has been defined variously as \"satisfaction of desires and goals\" (therefore related to life satisfaction), as a \"preponderance of positive over negative affect\" (therefore related to emotional components of SWB), as \"contentment\", and as a \"consistent, optimistic mood state\" and may imply an affective evaluation of one's life as a whole. Life satisfaction can also be known as the \"stable\" component in one's life. Affective concepts of SWB can be considered in terms of momentary emotional states as well as in terms of longer-term moods and tendencies (i.e. how much positive and/or negative affect a person generally experiences over any given period of time). Life satisfaction and in some research happiness are typically considered over long durations, up to one's lifetime. \"Quality of life\" has also been studied as a conceptualization of SWB. Although its exact definition varies, it is usually measured as an aggregation of well-being across several life domains and may include both subjective and objective components.\n\nLife satisfaction and Affect balance are generally measured separately and independently. \nSometimes a single SWB question attempts to capture an overall picture.\n\nThe issue with the such measurements of life satisfaction and affective balance is that they are self-reports. The problem with self-reports is that the participants may be lying or at least not telling the whole truth on the questionnaires. Participants may be lying or holding back from revealing certain things because they are either embarrassed or they may be filling in what they believe the researcher wants to see in the results. To gain more accurate results, other methods of measurement have been used to determine one’s SWB. \n\nAnother way to corroborate or confirm that the self-report results are accurate is through informant reports. Informant reports are given to the participant’s closest friends and family and they are asked to fill out either a survey or a form asking about the participants mood, emotions, and overall lifestyle. The participant may write in the self-report that they are very happy, however that participant’s friends and family record that he/she is always depressed. This would obviously be a contradiction in results which would ultimately lead to inaccurate results. \n\nAnother method of gaining a better understanding of the true results is through ESM, or the Experience Sampling Method. In this measure, participants are given a beeper/pager that will randomly ring throughout the day. Whenever the beeper/pager sounds, the participant will stop what he/she is doing and record the activity they are currently engaged in and their current mood and feelings. Tracking this over a period of a week or a month will give researchers a better understanding of the true emotions, moods, and feelings the participant is experiencing, and how these factors interact with other thoughts and behaviors. A third measurement to ensure validity is the Day Reconstruction Method. In this measure, participants fill out a diary of the previous days’ activities. The participant is then asked to describe each activity and provide a report of how they were feeling, what mood they were experiencing, and any emotions that surfaced. Thus to ensure valid results, a researcher may tend to use self-reports along with another form of measurement mentioned above. Someone with a high level of life satisfaction and a positive affective balance is said to have a high level of SWB.\n\nTheories of the causes of SWB tend to emphasise either top-down or bottom-up influences.\n\nIn the top-down view, global features of personality influence the way a person perceives events. Individuals may therefore have a global tendency to perceive life in a consistently positive or negative manner, depending on their stable personality traits. Top-down theories of SWB suggest that people have a genetic predisposition to be happy or unhappy and this predisposition determines their SWB \"setpoint\". Set Point theory implies that a person's baseline or equilibrium level of SWB is a consequence of hereditary characteristics and therefore, almost entirely predetermined at birth. Evidence for this genetic predisposition derives from behavior-genetic studies that have found that positive and negative affectivity each have high heritability (40% and 55% respectively in one study). Numerous twin studies confirm the notion of set point theory, however, they do not rule out the possibility that is it possible for individuals to experience long term changes in SWB.\n\nDiener et al. note that heritability studies are limited in that they describe long-term SWB in a sample of people in a modern western society but may not be applicable to more extreme environments that might influence SWB and do not provide absolute indicators of genetic effects. Additionally, heritability estimates are inconsistent across studies.\n\nFurther evidence for a genetically influenced predisposition to SWB comes from findings that personality has a large influence on long-term SWB. This has led to the \"dynamic equilibrium model\" of SWB. This model proposes that personality provides a baseline for emotional responses. External events may move people away from the baseline, sometimes dramatically, but these movements tend to be of limited duration, with most people returning to their baseline eventually.\n\nFrom a bottom-up perspective, happiness represents an accumulation of happy experiences. Bottom-up influences include external events, and broad situational and demographic factors, including health and marital status. Bottom-up approaches are based on the idea that there are universal basic human needs and that happiness results from their fulfilment. In support of this view, there is evidence that daily pleasurable events are associated with increased positive affect, and daily unpleasant events or hassles are associated with increased negative affect.\n\nHowever, research suggests that external events account for a much smaller proportion of the variance in self-reports of SWB than top-down factors, such as personality. A theory proposed to explain the limited impact of external events on SWB is hedonic adaptation. Based originally on the concept of a \"hedonic treadmill\", this theory proposes that positive or negative external events temporarily increase or decrease feelings of SWB, but as time passes people tend to become habituated to their circumstances and have a tendency to return to a personal SWB \"setpoint\" or baseline level.\n\nThe hedonic treadmill theory originally proposed that most people return to a neutral level of SWB (i.e. neither happy nor unhappy) as they habituate to events. However, subsequent research has shown that for most people, the baseline level of SWB is at least mildly positive, as most people tend to report being at least somewhat happy in general and tend to experience positive mood when no adverse events are occurring. Additional refinements to this theory have shown that people do not adapt to all life events equally, as people tend to adapt rapidly to some events (e.g. imprisonment), slowly to others (e.g. the death of a loved one), and not at all to others (e.g. noise and sex).\n\nA number of studies have found that SWB constructs are strongly associated with a range of personality traits, including those in the five factor model. Findings from numerous personality studies show that genetics account for 20-48% of the variance in Five-Factor Model and the variance in subjective well-being is also heritable. Specifically, neuroticism predicts poorer subjective well-being whilst extraversion, agreeableness, conscientiousness and openness to experience tend to predict higher subjective well-being. A meta-analysis found that neuroticism, extraversion, agreeableness, and conscientiousness were significantly related to all facets of SWB examined (positive, negative, and overall affect; happiness; life satisfaction; and quality of life). Neuroticism was the strongest predictor of overall SWB and is the strongest predictor of negative affect.\n\nA large number of personality traits are related to SWB constructs, although intelligence has negligible relationships. Positive affect is most strongly predicted by extraversion, to a lesser extent agreeableness, and more weakly by openness to experience. Happiness was most strongly predicted by extraversion, and also strongly predicted by neuroticism, and to a lesser extent by the other three factors. Life satisfaction was significantly predicted by neuroticism, extraversion, agreeableness, and conscientiousness. Quality of life was very strongly predicted by neuroticism, and also strongly predicted by extraversion and conscientiousness, and to a modest extent by agreeableness and openness to experience. One study found that subjective well-being was genetically indistinct from personality traits, especially those that reflected emotional stability (low Neuroticism), and social and physical activity (high Extraversion), and constraint (high Conscientiousness).\n\nDeNeve (1999) argued that there are three trends in the relationship between personality and SWB. Firstly, SWB is closely tied to traits associated with emotional tendencies (emotional stability, positive affectivity, and tension). Secondly, relationship enhancing traits (e.g. trust, affiliation) are important for subjective well-being. Happy people tend to have strong relationships and be good at fostering them. Thirdly, the way people think about and explain events is important for subjective well-being. Appraising events in an optimistic fashion, having a sense of control, and making active coping efforts facilitates subjective well-being. Trust, a trait substantially related to SWB, as opposed to cynicism involves making positive rather than negative attributions about others. Making positive, optimistic attributions rather than negative pessimistic ones facilitates subjective well-being.\n\nThe related trait of eudaimonia or psychological well-being, is also heritable. Evidence from one study supports 5 independent genetic mechanisms underlying the Ryff facets of psychological well-being, leading to a genetic construct of eudaimonia in terms of general self-control, and four subsidiary biological mechanisms enabling the psychological capabilities of purpose, agency, growth, and positive social relations\n\nA person's level of subjective well-being is determined by many different factors and social influences prove to be a strong one. Results from the famous Framingham Heart Study indicate that friends three degrees of separation away (that is, friends of friends of friends) can affect a person's happiness. From abstract: \"A friend who lives within a mile (about 1.6 km) and who becomes happy increases the probability that a person is happy by 25%.\"\n\nResearch indicates that wealth is related to many positive outcomes in life. Such outcomes include: improved health and mental health, greater longevity, lower rates of infant mortality, experience fewer stressful life events, and less frequently the victims of violent crimes However, research suggests that wealth has a smaller impact on SWB than people generally think, even though higher incomes do correlate substantially with life satisfaction reports.\n\nThe relative influence of wealth together with other material components on overall subjective well-being of a person is being studied through new researches. The Well-being Project at Human Science Lab investigates how material well-being and perceptual well-being works as relative determinants in conditioning our mind for positive emotions.\n\nIn a study done by Aknin, Norton, & Dunn (2009), researchers asked participants from across the income spectrum to report their own happiness and to predict the happiness of others and themselves at different income levels. In study 1, predicted happiness ranged between 2.4-7.9 and actual happiness ranged between 5.2-7.7. In study 2, predicted happiness ranged between 15-80 and actual happiness ranged between 50-80. These findings show that people believe that money does more for happiness than it really does. However, some research indicates that while socioeconomic measures of status do not correspond to greater happiness, measures of sociometric status (status compared to people encountered face-to-face on a daily basis) do correlate to increased subjective well-being, above and beyond the effects of extroversion and other factors.\n\nThe Easterlin Paradox also suggests that there is no connection between a society's economic development and its average level of happiness. Through time, the Easterlin has looked at the relationship between happiness and Gross Domestic Product (GDP) across countries and within countries. There are three different phenomena to look at when examining the connection between money and Subjective well-being; rising GDP within a country, relative income within a country, and differences in GDP between countries.\n\nMore specifically, when making comparisons between countries, a principle called the Diminishing Marginal Utility of Income (DMUI) stands strong. Veenhoven (1991) said, \"[W]e not only see a clear positive relationship [between happiness and GNP per capita], but also a curvilinear pattern; which suggest that wealth is subject to a law of diminishing happiness returns.\" Meaning a $1,000 increase in real income, becomes progressively smaller the higher the initial level of income, having less of an impact on subjective well-being. Easterlin (1995) proved that the DMUI is true when comparing countries, but not when looking at rising gross domestic product within countries.\n\nThere are substantial positive associations between health and SWB so that people who rate their general health as \"good\" or \"excellent\" tend to experience better SWB compared to those who rate their health as \"fair\" or \"poor\". A meta-analysis found that self-ratings of general health were more strongly related to SWB than physician ratings of health. The relationship between health and SWB may be bidirectional. There is evidence that good subjective well-being contributes to better health.\nA review of longitudinal studies found that measures of baseline subjective well-being constructs such as optimism and positive affect predicted longer-term health status and mortality. Conversely, a number of studies found that baseline depression predicted poorer longer-term health status and mortality. Baseline health may well have a causal influence on subjective well-being so causality is difficult to establish.\nA number of studies found that positive emotions and optimism had a beneficial impact on cardiovascular health and on immune functioning. Changes in mood are also known to be associated with changes in immune and cardiovascular response.\nThere is evidence that interventions that are successful in improving subjective well-being can have beneficial effects on aspects of health. For example, meditation and relaxation training have been found to increase positive affect and to reduce blood pressure. The effect of specific types of subjective well-being is not entirely clear. For example, how durable the effects of mood and emotions on health are remains unclear. Whether some types of subjective well-being predict health independently of others is also unclear. Meditation has the power to increase happiness because it can improve self-confidence and reduces anxiety, which increases your well-being. Cultivating personal strengths and resources, like humour, social/animal company, and daily occupations, also appears to help people preserve acceptable levels of SWB despite the presence of symptoms of depression, anxiety, and stress.\n\nResearch suggests that probing a patient's happiness is one of the most important things a doctor can do to predict that patient's health and longevity. In health-conscious modern societies, most people overlook the emotions as a vital component of one's health, while over focusing on diet and exercise. According to Diener & Biswas-Diener, people who are happy become less sick than people who are unhappy. There are three types of health: morbidity, survival, and longevity. Evidence suggests that all three can be improved through happiness:\n\n\nA positive relationship has been found between the volume of gray matter in the right precuneus area of the brain, and the subject's subjective happiness score. A 6 week mindfulness based intervention was found to correlate with a significant gray matter increase within the precuneus.\n\nThere are a number of domains that are thought to contribute to subjective well-being. In a study by Hribernik and Mussap (2010), leisure satisfaction was found to predict unique variance in life satisfaction, supporting its inclusion as a distinct life domain contributing to subjective well-being. Additionally, relationship status interacted with age group and gender on differences in leisure satisfaction. The relationship between leisure satisfaction and life satisfaction, however, was reduced when considering the impact of core affect (underlying mood state). This suggests that leisure satisfaction may primarily be influenced by an individual's subjective well-being level as represented by core affect. This has implications for possible limitations in the extent to which leisure satisfaction may be improved beyond pre-existing levels of well-being and mood in individuals.\n\nAlthough all cultures seem to value happiness, cultures vary in how they define happiness. There is also evidence that people in more individualistic cultures tend to rate themselves as higher in subjective well-being compared to people in more collectivistic cultures.\n\nIn Western cultures, predictors of happiness include elements that support personal independence, a sense of personal agency, and self-expression. In Eastern cultures, predictors of happiness focus on an interdependent self that is inseparable from significant others. Compared to people in individualistic cultures, people in collectivistic cultures are more likely to base their judgments of life satisfaction on how significant others appraise their life than on the balance of inner emotions experienced as pleasant versus unpleasant. Pleasant emotional experiences have a stronger social component in East Asian cultures compared to Western ones. For example, people in Japan are more likely to associate happiness with interpersonally engaging emotions (such as friendly feelings), whereas people in the United States are more likely to associate happiness with interpersonally disengaging emotions (pride, for example). There are also cultural differences in motives and goals associated with happiness. For example, Asian Americans tend to experience greater happiness after achieving goals that are pleasing to or approved of by significant others compared to European Americans. There is also evidence that high self-esteem, a sense of personal control and a consistent sense of identity relate more strongly to SWB in Western cultures than they do in Eastern ones. However, this is not to say that these things are unimportant to SWB in Eastern cultures. Research has found that even within Eastern cultures, people with high self-esteem and a more consistent sense of identity are somewhat happier than those who are low in these characteristics. There is no evidence that low self-esteem and so on are actually beneficial to SWB in any known culture.\n\nA large body of research evidence has confirmed that people in individualistic societies report higher levels of happiness than people in collectivistic ones and that socioeconomic factors alone are insufficient to explain this difference. In addition to political and economic differences, individualistic versus collectivistic nations reliably differ in a variety of psychological characteristics that are related to SWB, such as emotion norms and attitudes to the expression of individual needs. Collectivistic cultures are based around the belief that the individual exists for the benefit of the larger social unit, whereas more individualistic cultures assume the opposite. Collectivistic cultures emphasise maintaining social order and harmony and therefore expect members to suppress their personal desires when necessary in order to promote collective interests. Such cultures therefore consider self-regulation more important than self-expression or than individual rights. Individualistic cultures by contrast emphasise the inalienable value of each person and expect individuals to become self-directive and self-sufficient. Although people in collectivistic cultures may gain happiness from the social approval they receive from suppressing self-interest, research seems to suggest that self-expression produces a greater happiness \"payoff\" compared to seeking approval outside oneself.\n\nPositive psychology is particularly concerned with the study of SWB. Positive psychology was founded by Seligman and Csikszentmihalyi (2000) who identified that psychology is not just the study of pathology, weakness, and damage; but it is also the study of strength and virtue. Researchers in positive psychology have pointed out that in almost every culture studied the pursuit of happiness is regarded as one of the most valued goals in life. Understanding individual differences in SWB is of key interest in positive psychology, particularly the issue of why some people are happier than others. Some people continue to be happy in the face of adversity whereas others are chronically unhappy at the best of times. \n\nPositive psychology has investigated how people might improve their level of SWB and maintain these improvements over the longer term, rather than returning to baseline. Lyubomirsky (2001) argued that SWB is influenced by a combination of personality/genetics (studies have found that genetic influences usually account for 35-50% of the variance in happiness measures), external circumstances, and activities that affect SWB. She argued that changing one's external circumstances tends to have only a temporary effect on SWB, whereas engaging in activities (mental and/or physical) that enhance SWB can lead to more lasting improvements in SWB.\nSWB is often used in appraising the wellbeing of populations.\n\n\n"}
{"id": "50429661", "url": "https://en.wikipedia.org/wiki?curid=50429661", "title": "The State of the World's Children", "text": "The State of the World's Children\n\nThe State of the World's Children is an annual report published by the United Nations Children's Emergency Fund (UNICEF). It is the flagship publication of the organization. The first report was published in 1980, having been introduced by James P. Grant (the executive director of UNICEF at the time). Peter Adamson was the author of the report for 15 years. The publication of the 1982–1983 \"The State of the World's Children\" report marked the start of the child survival revolution.\n\nFollowing the end of Grant's tenure at UNICEF and his death in 1995, \"The State of the World's Children\" has received significantly less attention.\n\n\"The State of the World's Children 1982–1983\" launches the child survival revolution and pushed for GOBI (growth monitoring, oral rehydration therapy, breastfeeding, and immunization).\n\nThe 1988 report argues for the need of a \"Grand Alliance\" for children between governments, schools, mass media, etc., to continue the child survival and development revolution.\n\nThe 1991 report features the World Summit for Children, which happened the previous year. The report covers the commitment made by the Summit and serves as a record.\n\nThe 2015 \"The State of the World's Children\" report, titled \"Reimagine the Future\", reviews the work on children's health and rights in the world in the context of the 25th anniversary of the Convention on the Rights of the Child. The report also argued that more innovation is necessary, and highlighted examples including Solar Ear (a solar-rechargeable hearing aid battery charger) and community-based management of acute malnutrition.\n\n"}
{"id": "4694327", "url": "https://en.wikipedia.org/wiki?curid=4694327", "title": "Third-party administrator", "text": "Third-party administrator\n\nA third-party administrator (TPA) is an organization that processes insurance claims or certain aspects of employee benefit plans for a separate entity. It is also a term used to define organizations within the insurance industry which administer other services such as underwriting, customer service. This can be viewed as outsourcing the administration of the claims processing, since the TPA is performing a task traditionally handled by the company providing the insurance or the company itself. Often, in the case of insurance claims, a TPA handles the claims processing for an employer that self-insures its employees. Thus, the employer is acting as an insurance company and underwrites the risk. The risk of loss remains with the employer, and not with the TPA. An insurance company may also use a TPA to manage its claims processing, provider networks, utilization review, or membership functions. While some third-party administrators may operate as units of insurance companies, they are often independent.\n\nThird-party administrators also handle many aspects of other employee benefit plans such as the processing of retirement plans and flexible spending accounts. Many employee benefit plans have highly technical aspects and difficult administration that can make using a specialized entity such as a TPA more cost effective than doing the same processing in house.\n\nThird-party administrators are prominent players in the health care industry and have the expertise and capability to administer all or a portion of the claims process. They are normally contracted by a health insurer or self-insuring companies to administer services, including claims administration, premium collection, enrollment and other administrative activities. A hospital or provider organization desiring to set up its own health plan will often outsource certain responsibilities to a third-party administrator.\n\nFor example, an employer may choose to help finance the health care costs of its employees by contracting with a TPA to administer many aspects of a self-funded health care plan.\n\nThis term is also now commonly used in commercial general liability (CGL) policies or so called \"casualty\" business. In these instances, the liability policies are written with a large (in excess of $50,000) self-insured retention (SIR) that operates somewhat like a deductible, but rather than being paid at the end of a claim (when a loss payment is made to a claimant), the money is paid up front by the insured for costs, expenses, attorney fees etc. as the claim moves forward. If there is a settlement or verdict within the SIR, then that is also paid by the insured up to the limit of the SIR, before the insurer steps in and pays its portion. The TPA acts like a claims adjuster for the insurance company and sometimes works in conjunction with the inside insurance company claims adjuster or an outside claims investigator as well as the defense counsel. The defense counsel in some situations is selected by the TPA. The point is that the larger the SIR, the more responsibility the TPA has over the control of the way the claim is handled and ultimately resolved. Some self-insured retentions are in the millions of dollars and the TPAs are large multinational non-insurance entities that handle all the claims. In contrast, some self-insureds choose not to outsource claims handling to a TPA, preferring instead to handle all claims in house. This is known as self-administration.\n\nRetirement plans such as a 401(k) are often partly managed by an investment company. Instead of handling all the plan contributions by employees, distributions to employees, and other aspects of plan processing, the investment company may contract with a third-party administrator to handle much of the administrative work and only handle the remaining investment work.\n\n"}
{"id": "19154778", "url": "https://en.wikipedia.org/wiki?curid=19154778", "title": "Vaginal cuff", "text": "Vaginal cuff\n\nThe vaginal cuff is the upper portion of the vagina that opens up into the peritoneum and is sutured shut after the removal of the cervix and uterus during a hysterectomy. \n\nThe vaginal cuff is created by suturing together the edges of the surgical site where the cervix was attached to the vagina. This is accomplished by bringing the edges of the vagina together and suturing them together and to the uterosacral ligaments to prevent prolapse. The peritoneum is also sewn into the newly created vaginal cuff. There may be an advantage to using one method of closure over another. The vaginal cuff has a tendency to partially or completely dehisce or open up. \n\nA further complication that can accompany the dehiscence of the vaginal cuff is evisceration or the movement of intestines into the vagina. Some or all of the vaginal cuff can reopen.\n\nThe risk of vaginal cuff complications is related to the approach to hysterectomy: robotic-assisted total laparoscopic hysterectomy, total laparoscopic hysterectomy, laparoscopic-assisted vaginal hysterectomy, total abdominal hysterectomy, and total vaginal hysterectomy.\n\nThe vaginal cuff can be stressed by sexual intercourse, chronic constipation, asthma, COPD, and other actions that increase intra-abdominal pressure. This structure is prone to infection, hematoma and other postoperative complications. Factors that are thought to affect wound healing are radiation treatments, age, pelvic organ prolapse, the use of corticosteroids, concurrent malignancy.\n\nThough rare, estimates of the prevalence of vaginal cuff dehiscence are difficult to assess due to the presence of only case studies and anecdotal reports. If the vaginal cuff is compromised, vaginal evisceration can occur with the small intestine protruding out through the vagina.\n"}
{"id": "41622664", "url": "https://en.wikipedia.org/wiki?curid=41622664", "title": "Women's health nurse practitioner", "text": "Women's health nurse practitioner\n\nA women's health nurse practitioner (WHNP) is a nurse practitioner that specializes in continuing and comprehensive healthcare for women across the lifespan with emphasis on conditions unique to women from menarche through the remainder of their life cycle. \n\nFollowing educational preparation at the master's or doctoral level, WHNPs must become board certified by an approved certification body. Board certification must be maintained by obtaining continuing nursing education credits. In the US, board certification is provided through the National Certification Corporation (awards the WHNP-BC credential).\n\nWHNPs deliver a range of acute, chronic, and preventive healthcare services:\n\n"}
