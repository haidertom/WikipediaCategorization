{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase\n",
    "\n",
    "Here is a fast visulization of our results.\n",
    "\n",
    "Run all cells and when promted, type in an article from the english Wikipedia you wish to categorize, e.g. *Football*. If the article exists, the article is downloaded in the background (you need an Internet connection for this). \n",
    "For each of the Wikipida main topic classifcations, the probability that the article belongs to that category is then calculated and shown in a graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BloomClassify as BC\n",
    "import LSIsimilarity\n",
    "from baseline import Baseline\n",
    "import os\n",
    "import languageProcess as lp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Traindata trainedObjects/LSImodels/categories.pkl\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "peek of closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7d098049467c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mloadModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7d098049467c>\u001b[0m in \u001b[0;36mloadModels\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mLSI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoOfTrainArticle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mCL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_BL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Study/Master/02807_Computational-Tools-for-Data-Science/Project/WikipediaCategorization/WikipediaCategorization/Scripts/LSIsimilarity.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, basepath, noOfTrainArticle)\u001b[0m\n\u001b[1;32m     19\u001b[0m         '''\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainedObjects/LSImodels/categories.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Study/Master/02807_Computational-Tools-for-Data-Science/Project/WikipediaCategorization/WikipediaCategorization/Scripts/LSIsimilarity.py\u001b[0m in \u001b[0;36mload_categories\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded Traindata\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: peek of closed file"
     ]
    }
   ],
   "source": [
    "LSI = LSIsimilarity.LSIsimilarity()\n",
    "CL = BC.BloomClassify(prct = 40, art = 1000)\n",
    "\n",
    "def loadModels():\n",
    "    LSI.train(noOfTrainArticle=1000)\n",
    "    CL.load_BL()\n",
    "\n",
    "def checkArticle(title):\n",
    "    # check if Validationfolder exists\n",
    "    if not os.path.exists(\"../Validation\"): os.mkdir(\"../Validation\")\n",
    "\n",
    "    # get article from wikipedia API\n",
    "    Base = Baseline(folder=\"Validation\")\n",
    "    filename = \"../Validation/\"+title.replace(' ','')+\"_raw.txt\"\n",
    "    with open(filename, 'w') as the_file:\n",
    "        the_file.write(Base.get_dumptext([title]))\n",
    "\n",
    "    #convert to plain text\n",
    "    os.system(\"../wikiextractor/WikiExtractor.py \"+\"../Validation/\"+title.replace(' ','')+\"_raw.txt\"+\" -o\"+\" ../Validation/\"+title.replace(' ','')+\"/ --json\")\n",
    "\n",
    "    #housekeeping\n",
    "    category = os.listdir(\"../Validation\")\n",
    "    if '.DS_Store' in category: category.remove('.DS_Store')\n",
    "\n",
    "    #languageProcess\n",
    "    filepath = \"../Validation/\"+title.replace(' ','')+\"/AA/wiki_00\"\n",
    "    article = lp.languageProcess(filepath)\n",
    "    bfarticle = article.getHighFreqWords()\n",
    "    lsiarticle = article.getWords()\n",
    "    \n",
    "    assert (len(bfarticle)>= 1),\"No article with that name\"\n",
    "    \n",
    "    #BloomFilter\n",
    "    vali_dict = {}\n",
    "    numOfCheckWords = 100\n",
    "\n",
    "    for cat in CL.BFdict.keys():\n",
    "        vali_dict[cat] = sum([1 for word in bfarticle[0].most_common(numOfCheckWords) if CL.BFdict[cat].classify(word[0])])/numOfCheckWords\n",
    "\n",
    "    #Rewrite to return LSIres and BFres\n",
    "    return LSI.compare(lsiarticle), vali_dict\n",
    "\n",
    "def plotGraph(resLSI,resBF):\n",
    "    ind = np.arange(len(resLSI))    # the x locations for the groups\n",
    "    width = 0.2         # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(11,5))\n",
    "\n",
    "    p1 = ax.bar(ind-width,list(resLSI.values()),width, align='center',color='dodgerblue')\n",
    "    p2 = ax.bar(ind,list(resBF.values()),width, align='center',color='darkorange')\n",
    "    \n",
    "    ax.set_xticks(ind + width/2)\n",
    "    ax.set_xticklabels(list(resLSI.keys()),rotation='vertical')\n",
    "    ax.legend((p1[0], p2[0]), ('LSI 1000','BF 1000'))\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_ylabel('right assigned articles')\n",
    "    ax.grid()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "loadModels()\n",
    "\n",
    "\n",
    "def main():\n",
    "    title = input(\"Insert Article name: \")\n",
    "    print(title)\n",
    "    resLSI, resBF = checkArticle(title)\n",
    "    plotGraph(resLSI,resBF)\n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shown in the graph above represent the probability that the given article is within each of the categories. The higher the difference from the highest value to the second highest value, the higher the certainty that the result is valid. Some articles are very differentiatied and clearly assignable to one single Category. E.g. *Football* or *Handball* can not clearly be assigned to any other category than Sport and therefore, the result for these articles is very clear. Other articles like *Facebook* or *Google* don't have a clear direction, as they cover multiple different topics at once. These articles could be assgined to multiple different categories and therefore, our reults are rather unclear on articles like these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
