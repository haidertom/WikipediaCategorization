{"id": "9768022", "url": "https://en.wikipedia.org/wiki?curid=9768022", "title": "1977 in radio", "text": "1977 in radio\n\nIn the year 1977, significant events in radio broadcasting included the President of the United States participating in a call-in radio program.\n\n\n\n\n\n\n"}
{"id": "4238110", "url": "https://en.wikipedia.org/wiki?curid=4238110", "title": "1987 world oil market chronology", "text": "1987 world oil market chronology\n\n|-\n"}
{"id": "24149167", "url": "https://en.wikipedia.org/wiki?curid=24149167", "title": "2004 World Monuments Watch", "text": "2004 World Monuments Watch\n\nThe World Monuments Watch is a flagship advocacy program of the New York-based private non-profit organization World Monuments Fund (WMF) that is dedicated to preserving the historic, artistic, and architectural heritage around the world.\n\nEvery two years, it publishes a select list known as the Watch List of 100 Most Endangered Sites that is in urgent need of preservation funding and protection. It is a call to action on behalf of threatened cultural heritage monuments worldwide. The sites are nominated by governments, conservation professionals, site caretakers, non-government organizations (NGOs), concerned individuals, and others working in the field. An independent panel of international experts then select 100 candidates from these entries to be part of the Watch List, based on the significance of the sites, the urgency of the threat, and the viability of both advocacy and conservation solutions. A site’s inclusion on the Watch List attracts international attention, helping to raise funds needed for its rescue and spurring local governments and communities to take an active role in protecting the cultural landmark.\n\nThe 2004 World Monuments Watch List of 100 Most Endangered Sites was launched on 24 September 2003 by WMF President Bonnie Burnham and WMF partner American Express. For the first time, a site from Antarctica was included, ensuring that the Watch List geographically covers every continent.\n\nThe following countries/territories have multiple sites entered on the 2004 Watch List, listed by the number of sites:\n\nA. Numbers list only meant as a guide on this article. No official reference numbers have been designated for the sites on the Watch List.\nB. Names and spellings used for the sites were based on the official 2004 Watch List as published.\nC. The references to the sites' locations were based on the official 2004 Watch List as published.\nD. Tally includes the transfrontier site of \"Jesuit Guaraní Missions\".\n"}
{"id": "194624", "url": "https://en.wikipedia.org/wiki?curid=194624", "title": "Afrocentrism", "text": "Afrocentrism\n\nAfrocentrism (also Afrocentricity) is an approach to the study of world history that focuses on the history of people of recent African descent. It is in some respects a response to global (Eurocentric) attitudes about African people and their historical contributions; it seeks to correct mistakes and ideas perpetuated by the racist philosophical underpinnings of western academic disciplines as they developed during and since Europe's Early Renaissance as justifying rationales for the enslavement of other peoples, in order to enable more accurate accounts of not only African but all people's contributions to world history. Afrocentricity deals primarily with self-determination and African agency and is a Pan-African point of view for the study of culture, philosophy, and history.\n\nAfrocentrism is a scholarly movement that seeks to conduct research and education on global history subjects, from the perspective of historical African peoples and polities. It takes a critical stance on Euro-centric assumptions and myths about world history, in order to pursue methodological studies of the latter. Some of the critics of the movement believe that it often denies or minimizes European, Near Eastern and Asian cultural influences while exagerating certain aspects of historical African civilizations that independently accomplished a significant level of cultural and technological development. In general, Afrocentrism is usually manifested in a focus on the history of Africa and its role in contemporary African-American culture and Greek philosophy among others.\n\nWhat is today broadly called Afrocentrism evolved out of the work of African-American intellectuals in the late nineteenth and early twentieth centuries, but flowered into its modern form due to the activism of African-American intellectuals in the U.S. Civil Rights Movement and in the development of African-American Studies programs in universities. However, following the development of universities in African colonies in the 1950s, African scholars became major contributors to African historiography. A notable pioneer is the professor Kenneth Dike of the University of Ibadan, who became chairman of the Committee on African Studies at Harvard in the 1970s. In strict terms Afrocentrism, as a distinct historiography, reached its peak in the 1980s and 1990s. Today it is primarily associated with Cheikh Anta Diop, John Henrik Clarke, Ivan van Sertima and Molefi Asante. Cheick Anta Diop is the primary source of African-centred academic literature, the methodology of his research on the Nubian origins of Egyptian civilization earned him authorship of a chapter in UNESCO's General History of Africa manual, called \"Origin of the Ancient Egyptians\" \n\nProponents of Afrocentrism support the claim that the contributions of various Black African people have been downplayed or discredited as part of the legacy of colonialism and slavery's pathology of \"writing Africans out of history\". Major critics of Afrocentricity including Mary Lefkowitz, dissmiss it as pseudohistory, reactive, and obstinately therapeutic. Others, such as Kwame Anthony Appiah believe the Afrocentricity defeats its purpose of dismantling unipolar studies of word history by seeking to replace Eurocentricity with an equally enthnocentric and hierarchical curriculum, and negatively essentializes European culture and people of European descent.\n\nThe term \"Afrocentrism\" dates to 1962. The adjective \"Afrocentric\" appears in a typescript proposal for an entry in \"Encyclopedia Africana\", possibly due to W. E. B. Du Bois. The abstract noun \"Afrocentricity\" dates to the 1970s, and was popularized by Molefi Asante's \"Afrocentricity: The Theory of Social Change\" (1980). Afrocentrists led by Molefe Asante have organised their critics into three categories, Capitulationists, Europeanised Loyalists, and Maskers. \n\nAfrocentrism has its origins in the work of African and African diaspora intellectuals in the late 19th and early 20th centuries, following social changes in the United States and Africa due both to the end of slavery and the decline of colonialism. Following the American Civil War, African Americans in the South gathered together in communities to evade white control, established their own church congregations, and worked hard to gain education. They increasingly took more active public roles despite severe racial discrimination and segregation. American and African intellectuals looked to the African past for a re-evaluation of what its civilizations had achieved and what they meant for contemporary people.\nAs an ideology and political movement, Afrocentrism had its beginnings in activism among black intellectuals, political figures, and historians in the context of the US American civil rights movement. According to U.S. professor Victor Oguejiofor Okafor, concepts of Afrocentricity lie at the core of disciplines such as African American studies. But Wilson J. Moses claims that Afrocentrism roots are not exclusively African:\nIn 1987, Martin Bernal published his \"Black Athena\", in which he claims that ancient Greece was colonized by northern invaders mixing with a colony established by Phoenicia (modern Lebanon). A major theme of the work is the alleged denial by Western academia of the African and (western) Asiatic influence on ancient Greek culture.\n\nIn 2000, Molefi Kete Asante, chair of the Department of African American Studies at Temple University gave a lecture at the University of Liverpool entitled \"Afrocentricity: Toward a New Understanding of African Thought in this Millennium,\" in which he presented many of his ideas:\n\nAsante also stated:\n\nHowever, Wilson J. Moses, said of Asante: \"His second book, \"The Afrocentric Idea\" (1987), was a creative and in some respects brilliant but rambling theoretical work, much influenced by the revolution in \"critical theory\" that occurred in American intellectual life during the late 1970s and early 1980s.\" Some also assert that the definition of Afrocentricity has never sat still long enough to be properly described and accurately critiqued.\n\nAfrocentric education is education designed to empower peoples of the African diaspora. A central premise behind it is that many Africans have been subjugated by limiting their awareness of themselves and indoctrinating them with ideas that work against them. To control a people's culture is to control their tools of self-determination in relationship to others. Like educational leaders of other cultures, proponents assert that what educates one group of people does not necessarily educate and empower another group–so they assert educational priorities distinctly for the Africans in a given context.\n\nThe black church in the United States developed out of the creolization of African spirituality and European-American Christianity; early members of the churches made certain stories their own. During the antebellum years, the idea of deliverance out of slavery, as in the story of Exodus, was especially important. After Reconstruction and the restoration of white supremacy, their hope was based on deliverance from segregation and other abuses. They found much to respond to in the idea of a personal relationship with Jesus, and shaped their churches by the growth of music and worship styles that related to African as well as European-American traditions.\n\nTwentieth-century \"Africentric approaches\" to Christian theology and preaching have been more deliberate. Writers and thinkers emphasize \"Black presence\" in the Christian Bible, including the idea of a \"Black Jesus\".\n\nIn 1966 Maulana Karenga of the US Organization created Kwanzaa as the first specifically African American holiday. Karenga said his goal was to \"give Blacks an alternative to the existing holiday and give Blacks an opportunity to celebrate themselves and history, rather than simply imitate the practice of the dominant society.\"\n\nMany Afrocentrists seek to challenge concepts such as white privilege, color-blind perspectives, and race-neutral pedagogies. There are strong ties between Afrocentricity and Critical race theory.\n\nAfrocentrists agree with the current scientific consensus that holds that Africans exhibit a range of types and physical characteristics, and that such elements as wavy hair or aquiline facial features are part of a continuum of African types that do not depend on admixture with Caucasian groups. They cite work by Hiernaux and Hassan that they believe demonstrates that populations could vary based on micro-evolutionary principles (climate adaptation, drift, selection), and that such variations existed in both living and fossil Africans.\n\nAfrocentrists have condemned what they consider to be attempts at dividing African peoples into racial clusters as new versions of discredited theories, such as the Hamitic hypothesis and the Dynastic Race Theory. These theories, they contend, attempted to identify certain African ethnicities, such as Nubians, Ethiopians and Somalis, as \"Caucasoid\" groups that entered Africa to bring civilization to the natives. They believe that Western academics have traditionally limited the peoples they defined as \"Black\" Africans to those south of the Sahara, but used broader \"Caucasoid\" or related categories to classify peoples of Egypt or North Africa. Afrocentrists also believe strongly in the work of certain anthropologists who have suggested that there is little evidence to support that the first North African populations were closely related to \"Caucasoids\" of Europe and western Asia.\n\nIn 1964 Afrocentric scholar Cheikh Anta Diop expressed a belief in such a double standard:\nFrench historian Jean Vercoutter has claimed that archaeological workers routinely classified Negroid remains as Mediterranean, even though they found such remains in substantial numbers with ancient artefacts.\n\nSome Afrocentrists have adopted a pan-Africanist perspective that people of color are all \"African people\" or \"diasporic Africans,\" citing physical characteristics they exhibit in common with Black Africans. Afrocentric scholar Runoko Rashidi writes that they are all part of the \"global African community.\" Some Afrocentric writers include in the African diaspora the Dravidians of India, \"Negritos\" of Southeast Asia (Thailand, the Philippines and Malaysia); and the aboriginal peoples of Australia and Melanesia.\n\nA few Afrocentrists claim that the Olmecs of Mexico were a hybrid society of Native American peoples and Africans. Mainstream historians of Mesoamerica overwhelmingly reject that view with detailed rebuttals.\n\nIn the 1970s, Ivan van Sertima advanced the theory that the complex civilizations of the Americas were the result of trans-oceanic influence from the Egyptians or other African civilizations. Such a claim is his primary thesis in \"They Came Before Columbus\", published in 1978. The few hyper-diffusionist writers seek to establish that the Olmec people, who built the first highly complex civilization in Mesoamerica and are considered by some to be the mother civilization for all other civilizations of Mesoamerica, were deeply influenced by Africans. Van Sertima said that the Olmec civilization was a hybrid one of Africans and Native Americans. His theory of pre-Columbian American-African contact has since met with considerable and detailed opposition by scholars of Mesoamerica. Van Sertima has been accused of \"doctoring\" and twisting data to fit his conclusions, inventing evidence, and ignoring the work of respected Central and South American scholars in the advance of his own theory.\n\nSeveral Afrocentrists have claimed that important cultural characteristics of ancient Egypt were indigenous to Africa and that these features were present in other early African civilizations such as the later Kerma and the Meroitic civilizations of Nubia. Scholars who have held this view include Marcus Garvey, George James, Cheikh Anta Diop, Martin Bernal, Ivan van Sertima, John Henrik Clarke, Chancellor Williams, and Molefi Kete Asante. The claim has also been made by many Afrocentric scholars that the Ancient Egyptians themselves were Black African (sub-saharan African) rather than North African/Maghrebi, and that the various invasions on Egypt resulted in the Africanity of Ancient Egypt becoming diluted, resulting in the modern diversity seen today.\n\nScholars have challenged the various assertions of Afrocentrists on the cultural and biological characteristics of Ancient Egyptian civilization and its people. At a UNESCO Symposium in the 1970s, the vast majority of the delegates repudiated the Afrocentric assertions. Zahi Hawass has gone on record as saying that the Ancient Egyptians were not black and Ancient Egypt was not a Black African Civilization. Despite contestations, UNESCO decided to include his \"Origin of the ancient Egyptians\" in the General History of Africa, with an editorial comment mentioning the disagreement. \nIt should also be noted that the ancient world did not employ racial categories such as \"Black\" or \"White\" as they had no conception of \"race\", but rather labeled groups according to their land of origin and cultural traits. However, S. O. Y. Keita, a biological anthropologist studying the controversy, finds simplistic political appellations (in the negative or affirmative) describing ancient populations as \"black\" or \"white\" to be inaccurate and instead focuses on the ancestry of ancient Egypt as being a part of the native and diverse biological variation of Africa, which includes a variety of phenotypes and skin gradients. Responding to Hawass' assertion in the press that Egyptians were not Black or that Egypt was not an African civilization, Keita claims that Hawass' statement obscures the reality of research in the Nile valley that paints a very complex picture and that most people familiar with this research would not get up in front of a group to make such a claim so openly.\n\nStephen Howe summarizes the development of the Hamitic hypothesis in the 19th and 20th centuries as Eurocentric. He further describes how some Afrocentric writers adopted 'their version' of it. Howe distinguishes three clusters of controversies related to the history of Ancient Egypt. About the third cluster he says that these are \"controversies that have been especially salient in relation to the United States, have interacted heavily with sensitive issues of current public policy, and involve questions both wide and fundamentally about the United States.\"\n\nWithin Afrocentrism, claims were forwarded involving the contention that African civilizations were founding influences on such distant civilizations as the American Olmec and the Chinese Xia cultures.\n\nYaacov Shavit, a critic of the movement, summarises its goals in the preface to his book \"History in Black\", in which he states:\nOther critics contend that some Afrocentric historical research is grounded in identity politics and myth rather than scholarship. In \"The Skeptic's Dictionary\", philosophy professor Robert Todd Carroll labeled Afrocentrism \"pseudohistorical\". He argued that Afrocentrism's prime goal was to encourage black nationalism and ethnic pride in order to effectively combat the destructive consequences of cultural and universal racism. Similarly, African-American professor Clarence E. Walker who teaches history at the University of California, Davis, has described Afrocentrism as \"a mythology that is racist, reactionary, essentially therapeutic and is eurocentrism in black face.\"\n\nMary Lefkowitz, Professor Emerita of Classical Studies at Wellesley College in Massachusetts, has rejected George James's theories about Egyptian contributions to Greek civilization as being faulty scholarship. She notes that he used sources that predated the deciphering of Egyptian hieroglyphs. He failed to acknowledge that many of his theories were overturned by the evidence of later findings. She contends that ancient Egyptian texts show little similarity to Greek philosophy. Lefkowitz also pointed out that Aristotle could not have stolen his ideas from the great Library at Alexandria as James suggested, because the library was founded \"after\" Aristotle's death. Because of such fundamental errors of fact, Lefkowitz has criticized Afrocentrism as \"an excuse to teach myth as history.\"\n\nIn 1994 the Manhattan Institute, a public policy forum, published \"Alternatives to Afrocentrism\", a collection of highly critical essays by, among others, Lefkowitz, Gerald Early, Stanley Crouch, Wilson Moses, and Frank Yurco. Early, an African American, has been especially critical and dismisses Afrocentrism as just another North American experiment in \"group therapy,\" a kind of \"intellectual fast food\".\n\nIn 2002 Ibrahim Sundiata noted in the \"American Historical Review\" that \nCain Hope Felder, a Professor of New Testament Language and Literature at Howard University and supporter of Afrocentric ideas, has warned Afrocentrists to avoid certain pitfalls, including:\n\nNathan Glazer writes that although Afrocentricity can mean many things, the popular press has generally given most attention to its most outlandish theories. Glazer agrees with many of the findings and conclusions presented in Lefkowitz's book \"Not Out of Africa\". Yet he also argues that Afrocentrism often presents legitimate and relevant scholarship. The late Manning Marable was also a critic of Afrocentrism. He wrote:\nSome Afrocentrists agree in rejecting those works which critics have characterized as examples of bad scholarship. Adisa A. Alkebulan notes that the work of Afrocentric scholars is not fully appreciated because critics use the claims of \"a few non-Afrocentrists\" as \"an indictment against Afrocentricity.\"\n\nIn 1996 the historian August Meier critically reviewed the new work of Mary Lefkowitz on Afrocentrism as \"Eurocentric\". He criticized her book \"Not out of Africa: How Afrocentrism became an Excuse to Teach Myth as History\" for what he saw as her neglect of the African-American historic literature of the 19th and 20th centuries. Meier believes she fails to take the African-American experiences into account, to the extent that she \"fails to answer the question raised in this book's subtitle\".\n\nMaghan Keita describes the controversy over Afrocentrism as a cultural war. He believes certain \"epistemologies\" are warring with each other: the \"epistemology of blackness\" argues for the \"responsibilities and potential of black peoples to function in and contribute to the progress of civilization.\"\n\n\n\n\n\n"}
{"id": "3661620", "url": "https://en.wikipedia.org/wiki?curid=3661620", "title": "Age of Revolution", "text": "Age of Revolution\n\nThe Age of Revolution is the period from approximately 1774 to 1849 in which a number of significant revolutionary movements occurred in many parts of Europe and the Americas. The period is noted for the change in government from absolutist monarchies to constitutionalist states and republics. The Age of Revolution includes the American Revolution, the French Revolution, the Irish Rebellion of 1798, the Haitian Revolution, the revolt of slaves in Latin America, the First Italian War of Independence, Sicilian revolution of 1848, and the 1848 revolutions in Italy; and the independence movements of Spanish and Portuguese colonies in Latin America. In a way, it includes the Industrial Revolution. The period would generally weaken the imperialist European states, who would lose major assets throughout the New World. For the British, the loss of the Thirteen Colonies would bring a change in direction for the British Empire, with Asia and the Pacific becoming new targets for expansion.\n\nThe expression was popularised by the British historian Eric Hobsbawm in his book \".\"\n\nThe Industrial Revolution was the transition to new manufacturing processes in the period from about 1760 to sometime between 1820 and 1840. It marked a major turning point in history and almost every aspect of daily life was influenced in some way. In particular, average income and population began to exhibit unprecedented sustained growth. This led to the expansion of cities that was so rapid, it resulted in social strains and disturbances. For instance, economic grievances associated with this industrialization fed later revolutions such as those that transpired from 1848. New social classes emerged including those that began to reject orthodox politics. This is demonstrated by the rise of the urban middle class, which became a powerful force so that they had to be integrated into the political system. The upheavals also led to new political ideas that were directed against the social arrangements of the preindustrial regime.\n\nThe Thirteen Colonies of British America famously became independent in the American Revolution of 1776. The movement was the first European colony to claim independence and saw the application of elected government, a constitution, and limited civil rights, none of which were unheard of but were unique in their combination. This was the birth of the United States of America.\n\nThe French Revolution was a period of radical social and political upheaval in France from 1789 to 1799 that profoundly affected French and modern history, marking the decline of powerful monarchies and churches and the rise of democracy and nationalism. Popular resentment of the privileges enjoyed by the clergy and aristocracy grew amidst an economic crisis following two expensive wars and years of bad harvests, motivating demands for change. These were couched in terms of Enlightenment ideals and caused the convocation of the Estates-General in May 1789.\n\nThe Haitian Revolution was a slave revolt in the French colony of Saint-Domingue, which culminated in the elimination of slavery there and the founding of the Republic of Haiti. The Haitian Revolution was the only slave revolt which led to the founding of a state. Furthermore, it is generally considered the most successful slave rebellion ever to have occurred and as a defining moment in the histories of both Europe and the Americas. The rebellion began with a revolt of black African slaves in August 1791. It ended in November 1803 with the French defeat at the battle of Vertières. Haiti became an independent country on January 1, 1804.\n\nIn 1798, Irishmen rose against British Rule in Ireland in the hopes of creating a Republic. The rebellion was initiated by the Society of United Irishmen and led by Theobald Wolfe Tone. They rebelled against the government for a number of reasons but most notably due to the sectarian nature of British Rule, which upheld the Penal Laws that discrimated against Catholics and Presbyterians in Irish society. The rebellion failed and led to the Act of Union in 1801.\n\nThe Serbian Revolution was a national uprising and constitutional change in Serbia that took place between 1804 and 1835, during which the territory evolved from an Ottoman province into a rebel territory, a constitutional monarchy, and finally the modern Serbian state. The first part of the period, from 1804 to 1815, was marked by a violent struggle for independence from the Ottoman Empire with two armed uprisings taking place, ending with a ceasefire. During the later period (1815–1835) a peaceful consolidation of political power developed in the increasingly autonomous Serbia, culminating in the recognition of the right to hereditary rule by Serbian princes in 1830 and 1833 and the territorial expansion of the young monarchy. The adoption of the first written Constitution in 1835 abolished feudalism and serfdom, and made the country suzerain. The term \"Serbian Revolution\" was coined by a German academic historiographer, Leopold von Ranke, in his book \"Die Serbische Revolution\", published in 1829. These events marked the foundation of the modern Principality of Serbia.\n\nScholars have characterized the Serbian War of Independence and subsequent national liberation as a revolution because the uprisings were started by broad masses of rural Serbian people who were in severe class conflict with the Turkish landowners as a political and economic masters at the same time, similar to Greece in 1821–1832.\n\nLatin America experienced independence revolutions in the early 19th century that separated the colonies from Spain and Portugal, creating new nations. These movements were generally led by the ethnically Spanish but locally born Creole class; these were often wealthy citizens that held high positions of power but were still poorly respected by the European-born Spaniards. One such Creole was Simón Bolívar, who led several revolutions throughout South America and helped establish Gran Colombia. Another important figure was José de San Martín, who helped create the United Provinces of Rio de la Plata and became the first president of Peru. Some Latin American revolts, such as the Haitian Revolution, were led by slaves.\n\nGreece in the early part of the 1800s was under the rule of the Ottoman Empire. A series of revolts, starting in 1821, began the conflict. The Ottoman Empire sent in forces to suppress the revolts. By 1827, forces from Russia, Great Britain, and France entered the conflict, helping the Greeks drive the Turkish forces off the Peloponnese Peninsula. The Turks finally recognized Greece as a free nation in May 1832.\n\nThe European Revolutions of 1848, known in some countries as the Spring of Nations, Springtime of the Peoples or the Year of Revolution, were a series of political upheavals throughout Europe in 1848. It remains the most widespread revolutionary wave in European history, but within a year, reactionary forces had regained control, and the revolutions collapsed. \n\nThe political impact of the 1848 revolutions was more evident in Austria in comparison to the revolution's effects in countries like Germany. This is attributed to the way the upheavals in Vienna resulted in greater loss of life and gained stronger support from intellectuals, students, and the working class. An account described the German experience as less concerned with national issues, although it succeeded in breaking down class barriers. There was a previously prevalent view that there was only one revolutionary event in Germany but recent scholarship pointed to a fragmented picture of several revolutions happening at the same time. \n\nThe 1848 revolutions were also notable because of the increased participation of women. While women rarely participated in revolutionary activities, there were those who performed supportive and auxiliary roles such as the cases of the women's political club in Vienna, which demanded revolutionary measures from the Austrian Constituent Assembly, and the Parisian women who protested and proposed their own solutions to social problems, particularly those involving their rights and crafts. \n\nThe Eureka Rebellion was a 20 minute shootout between the Miners of Ballarat, Victoria, against the British Redcoats. After the imposition of Gold Mining Licences, that being that a person had one of these to mine gold, and which costed 30 shillings a month to own a license, the miners decided that it was to much. So the Ballarat Miners started rallies at Bakery Hill and burnt their licenses, took an oath under the flag of the Southern Cross, elected Peter Lalor as their rebellion leader, and built a stockade (a makeshift fort) around the diggings. Eventually the Redcoats led by Governor Hotham of Ballarat fired upon the stockade. The miners fired back and lasted 20 minutes before being overthrown by the Redcoats. Most of the miners were arrested by the British, and taken to trial. If found guilty, they would hang for high treason. Even though it lasted only 20 minutes, it was the beginning of Democracy for all Australians.\n\nThe Taiping Rebellion was a revolt against the Qing dynasty in China, fought with religious conviction over regional economic conditions, and lasting from 1850 to 1864. The Taiping forces were run as a cult-like group called the God Worshipping Society by self-proclaimed prophet Hong Xiuquan, and resulted in the rebels seizing the city of Nanjing for a decade. The Taiping Rebellion eventually failed, however, and led to the deaths of more than 20 million people.\n\n"}
{"id": "15161664", "url": "https://en.wikipedia.org/wiki?curid=15161664", "title": "Anthropogeny", "text": "Anthropogeny\n\nAnthropogeny is the study of human origins. It is not simply a synonym for human evolution by natural selection, which is only a part of the processes involved in human origins. Many other factors besides biological evolution were involved, ranging over climatic, geographic, ecological, social, and cultural ones. Anthropogenesis, meaning the process or point of becoming human, is also called hominization.\n\nThe term \"anthropogeny\" was used in the 1839 edition of Hooper's \"Medical Dictionary\" and was defined as \"the study of the generation of man\". The term was popularized by Ernst Heinrich Haeckel (1834–1919), a German naturalist and zoologist, in his groundbreaking books, \"Natural History of Creation\" (German: \"Natürliche Schöpfungsgeschicht\") (1868) and \"The Evolution of Man\" (German: \"Anthropogenie\") (1874). Haeckel was one of the first biologists to publish on evolution. Haeckel used the term Anthropogeny to refer to the study of comparative embryology and defined it as \"the history of the evolution of man\". The term changed over time, however, and came to refer to the study of human origins.\n\nThe last use of the word \"anthropogeny\" in English literature was in 1933 by William K. Gregory. There was a gap in the usage of the term from 1933 to 2008. Anthropogeny was reintroduced in 2008 and is now back in academic use at the Center for Academic Research and Training in Anthropogeny (CARTA) at the University of California, San Diego.\n\nThe root in ancient Greek \"anthropos\" means human and \"-logia\" means discourse or study, and \"wiktionary:-genesis\" means the process of creation or origin.\n\"Anthropology\", therefore, is quite literally the study of humans, whereas \"anthropogeny\" is the breakdown of the word anthropos agai but with the link word -geny (γένη, γένος) which again literally means the study of the birth and gender of humans.\n\nAccording to the ancient Greeks who came up with the terms, anthropology is the sum of many sciences relating to the human study. There is social, economic, civil and comparative anthropology.\n\nAnthropogony derives from the word 'anthropo' again and -gony (γόνοι), meaning 'the causing of, the birth of', both in a literal and metaphorical sense, referring to what caused by human born or/and conceived/created.\n\nAnthropogeny on the other, is whatever derives from humans. -geny ('γένης᾽), meaning 'the born' or who is born of, but also the gender.\n\nAccording to Gregory (1933), anthropologists are interested in measuring and quantifying aspects of being human, whereas anthropogenists are interested in \"piecing together the broken story of the 'big parade' that nature has staged across the ages\". According to the definition of the words, Gregory's statement is wrong.\n\nSo anthropology is the study of humans and anthropogeny, is the study of what humans 'gave birth to', to its core definition, although there have been many confusions by those who do not really have an understanding of the word's origins.\n\nModern anthropology is typically divided into four sub-fields: social anthropology or cultural anthropology, biological anthropology, linguistic anthropology, and archaeology. The field of anthropology has origins in the natural sciences, humanities, and social sciences.\n\nThe field of anthropogeny is also influenced by the natural sciences, humanities, and social sciences, however, given that it is the study of the origin of humans, it is also influenced by fields ranging from anatomy and biomechanics to neurology and genetics.\n\nA comprehensive list of Domains of Scientific Discipline relevant to anthropogeny can be found in the Matrix of Comparative Anthropogeny (MOCA), associated with the Center for Academic Research and Training in Anthropogeny (CARTA), at the University of California, San Diego.\n\n"}
{"id": "26334944", "url": "https://en.wikipedia.org/wiki?curid=26334944", "title": "Auxiliary sciences of history", "text": "Auxiliary sciences of history\n\nAuxiliary (or ancillary) sciences of history are scholarly disciplines which help evaluate and use historical sources and are seen as auxiliary for historical research. Many of these areas of study, classification and analysis were originally developed between the 16th and 19th centuries by antiquaries, and would then have been regarded as falling under the broad heading of antiquarianism. \"History\" was at that time regarded as a largely literary skill. However, with the spread of the principles of empirical source-based history championed by the Göttingen School of History in the late 18th century and later by Leopold von Ranke from the mid-19th century onwards, they have been increasingly regarded as falling within the skill-set of the trained historian.\n\nAuxiliary sciences of history include, but are not limited to:\n\n"}
{"id": "16816176", "url": "https://en.wikipedia.org/wiki?curid=16816176", "title": "Boy Wonder (novel)", "text": "Boy Wonder (novel)\n\nBoy Wonder is a novel by James Robert Baker published in 1988. The novel is a mock of oral history of Los Angeles, California in which we hear the life of Hollywood avant-garde film producer Shark Trager.\n\nOrange County, California 1950: GALE 'SHARK' TRAGER is born to long-suffering WINNIE TRAGER in the backseat of her car at a drive-in. Shark's arch-conservative father MAC TRAGER (who looks uncannily like actor Glenn Ford) learns the news and ends up running someone off the road on his way to the hospital. Shark immediately is a strange kid, suffering from childhood obesity and not helped by the fact that Winnie is a hypochondriac and Mac is a racist bully. When Winnie grows catatonic, Mac brings in sexy GLADYS FRAZER to take over the wife/mother role in the family. Shark grows up, making friends with gay neighbor KENNY ROBERTS, acne-ridden misfit PUS JENKINS and intellect ELLIOT BERNSTEIN. Winnie dies after climbing into an active freezer and Gladys leaves Mac soon after as the neighbors shun her for being a homewrecker.\n\nAs Shark grows up, he falls in love with the movies—his only escape from the bleakness of his own sad childhood—and he becomes an insatiable cinemaphile. Mac's sister LORNA moves in to help Mac raise Shark, but spends her nights fending off Mac's lonely drunken advances. In high school, Shark loses his baby fat and matures into a rakishly handsome young man who strongly resembles Errol Flynn. One day while working at his father's gas station, Shark notices beautiful blonde California teen KATHY PETRO for the first time and he's instantly smitten with her. Kathy is the daughter of powerful local oil magnate JACK PETRO, but everyone at school frowns upon Shark, so Kathy doesn’t give him the time of day. Obsessed with Kathy, Shark takes a movie camera over to her house one night and films her from outside her bathroom window as she masturbates. When Shark tries to get the film developed, he's arrested and Kathy's parents soon learn what a pervert Shark is. Mac freaks out, taking away all of Shark's film equipment as punishment. Shark's relationship with his father only worsens after Shark starts dating an adorable Japanese girl named JUDY OSHIMA. When Mac orders Shark to drop his pants for a VD 'drip check,' Shark accuses the old man of being a homo; Mac is thunderstruck by this accusation—possibly because it's true? Another incident occurs when Kathy Petro makes out with a boy in the local movie theater and Shark, sitting jealously in the seat behind, accosts them. Unfortunately, Shark's girlfriend Judy dies when her moped is struck on the highway. Shark soon gains another friend, easy-going WOODY HAZZARD, a local surfer and small-time drug dealer whom Shark moves in with.\n\nOne day Kathy Petro shows up, trying to buy some acid for her boyfriend, JEFF STUBEN; she gets it, but when Jeff has a bad trip and accidentally cuts his own head off with a chainsaw, the cops storm Shark and Woody's place. Shark swallows all the acid and has a bad trip himself in jail. Shark decides to go to film school at UCLA and meets the sensitive and talented NEAL RIDGES, as well as sophisticated SIMONE GATANE. Shark moves into James Dean's old house in the valley and he and Simone start hooking up as he makes his maiden film, \"PILLOW FUCK\", a wry deconstruction of those insipid 1960s Rock Hudson/Doris Day romantic-comedies. The film is notoriously good and Shark's personal hero, famous French director JEAN-CLAUDE CITROEN, shows up for a screening. Unfortunately Citroen utterly trashes the film afterward and Shark is so devastated that he takes off and burns the only print. Soon after, Shark is living with a gung-ho nut named DRAKE BREWSTER who helps Shark lose weight again and get a security job in Santa Barbara. Shark gets a hold of a graduate student's screenplay and contacts fellow UCLA film student, SUE SCHLOCKMANN, whose father is a low-budget film distributor in Hollywood. Shark pumps up his relationship with Sue and gets her father to give him a producing deal. To everyone's surprise, \"SEX KILL A GO-GO\" is a huge success. When the draft board calls Shark in, he forces himself to ejaculate on the guy in line in front of him to get out of the draft. When Sue notices Shark's infatuation with Kathy Petro, she tries to distance herself from him, but Shark proposes and the two get married instead. Elliot Bernstein, who originally dated Sue and still loves her, is devastated.\n\nShark next does \"REDNECK SCUM\", but when he learns Kathy Petro has gone to France as a model and has started dating Jean-Claude Citroen, he decides to make the movie about them. Shark's obsession with Kathy gets the best of him and he starts dating a fifteen-year-old Lolita-esque Kathy lookalike named CINDY, whom he takes to Cannes along with his latest movie, a murderous lovers-on-the-run art film entitled \"WHITE DESERT.\" Jean-Claude uses his power to get Shark's movie banned from competition, but a late night screening wins over the hearts of the viewers and it becomes a success. Unfortunately, Jean-Claude picks up Cindy and Shark tracks him down on the beach and beats the hell out of him. Shark returns to America only to find Sue's father, angry at Shark's abuse of his daughter, has slipped in some contractual wording and Shark now owes him $6 million. Luckily for Shark, another rival studio buys the pic and Shark's production deal away. Shark makes another movie with a Kathy Petro look-alike in the lead role, an \"EXORCIST\"-esque thriller called \"THE CONDOIST.\" Kathy does her own movie, however, a small art house romantic-comedy—and winds up getting critically panned for her vapid, amateurish performance; Shark sends her an apologetic note. Soon after, Kathy starts dating BETH, a militant lesbian, and Shark makes yet another movie, this time about Kathy and Beth's relationship—a lesbian variation on \"THE SEARCHERS\" which Shark calls \"SCAR.\" Beth's father, however, is a powerful guy in Hollywood and he gets the movie pulled. Unfortunately, Beth's father dies when Beth angrily confronts him, and Shark continues unabated. It's then that Shark meets CAROL VAN DER HOF, a charming socialite with a club foot. Shark ends up partnering with Carol and the two form the perfect personal and professional relationship. Hollywood loves them, although Carol is secretly enamored with Shark. Their next flick, a mod late-60s film entitled \"MONDO JET SET,\" is a huge success. Simultaneously, Woody starts having an affair with Kathy, trying his best to keep it a secret from Shark. Kathy's in love for the first time, but everything changes when Woody starts dating a gay film exec named BRIAN STRAIGHT. Brian hates Shark and when Shark messes with Woody, Brian proposes to Kathy in order to help both their careers. Shark explodes when he learns of this, and he storms over to their house and beats the hell out of Brian.\n\nShark decides to make a movie out of a tragedy that happened in his neighborhood when childhood friend Pus Jenkins murdered a number of local teens. He spends millions on the first scene (an uninterrupted, twenty-minute long shot) and has to blackmail the studio boss with a sex film of his wife in order to keep the movie alive. In the end, Shark comes under fire for hiring Pus himself to play the father of one of the murdered teens. Even worse, Shark starts an affair with Carol, but when he spurns her, she goes on set and cuts off her own club foot. In the climactic scene where the house explodes (they’re filming at Shark's childhood home), Shark races up and dives into the freezer his mother died in just as the bombs go off. Shark survives, but afterward is stabbed by a family member of one of the films’ victims. The studio tries to shelve the film, entitled \"RED SURF,\" but Shark spends all his millions to buy it back. Unfortunately, it's unreleasable and becomes Shark's own \"HEAVEN'S GATE\", effectively destroying his career.\n\nAfterward, Shark falls into drugs and soon becomes homeless. A theater worker, TODD JARRETT, hires Shark to be his projectionist because of his film knowledge, but another incident with Kathy Petro gets him fired. Soon, Shark is a genuine bum down on Venice Beach and all those who once hailed him, now ignore him. One day, however, Woody, Elliot and Carol are having lunch and spot Shark getting beaten up by some black bums. Woody and Elliot rescue Shark and send him to a Nevada detox unit, which just happens to be run by his ex-roommate, Drake Brewster. Even worse, Kathy Petro, in a downward spiral of sleeping pills, has ended up there too. To their own surprise, the long-simmering heat between them finally explodes and the two end up making love. At the same time, Shark is having an affair with another patient, NARGES PAHLAVI-BARDAHL, an Iranian sheik's daughter. Narges promises to bankroll Shark's next film. Shark is back and producing a science-fiction film entitled \"BLUE LIGHT,\" a movie no one thinks will work. He takes all his old compadres, even his father Mac, to Beirut to film. An incident involving his father and Drake Brewster culminates in Muslim fanatics almost killing them, but Shark proves to be a brave man when he bluffs being wired with explosives and talks his way out of danger. The movie becomes the highest-grossing film of all time and Shark returns to the top of Hollywood, but his fascination with Kathy Petro has driven Narges into drug dependency.\n\nOne day, Shark gets a call from the White House: RONALD REAGAN wants to meet him. Shark goes, not knowing Kathy is there at the same time to meet NANCY REAGAN and discuss her biography. Shark and Kathy end up making love in the Roosevelt room, but an unfortunate situation develops when a famous movie star donkey comes through the White House and the animal goes wild, nearly having sex with Kathy. Shark saves her from that fate. Shark and Kathy begin dating, but to Shark's sad surprise, the reality of achieving his life's desire doesn’t live up to the fantasy. Soon Shark writes a touching love story and offers Kathy the lead role. He plays upon her real-life tragedies to get her to cry on demand and the movie, \"HOME TO THE HEART\", is an emotional masterpiece. But Shark's enemies are too many and just after the movie is nominated for all kinds of Academy Awards, rumors of Shark's transgressions, true and false, come out. Shark becomes reviled. Even worse, the White House story comes out and Kathy is made to look like she had sex with the donkey.\n\nShark is slowly losing his mind and when the Academy Awards come around, he and Kathy attend, but they get booed quite a lot. Shark runs into Jean-Claude Citroen in the bathroom and attacks him, and we learn Jean-Claude regrets trashing Shark's first film: turns out he actually considered it a masterpiece but was too frightened by its power to tell Shark the truth. Afterward, Kathy wins for Best Actress and thanks Shark in her acceptance speech, but when a weaker film wins for Best Picture, Shark freaks out and nearly sodomizes his director with the Oscar he believes is rightfully his. Shark and Kathy flee the Awards, but when she puts up a fight in his car, Shark pushes her out onto the freeway. Believing she's dead, a distraught Shark drives back to the theater where he once worked to grab a film canister. The cops are after him. Shark speeds back to the exact spot where he was born, now a movieplex, and drives his car through the wall, killing a bunch of neo-Nazi kids. Unfortunately, cops arrive and shoot Shark dead. Shark is portrayed as a child killer in the media and his legacy is destroyed. Only Woody, Simone and Elliot show up for his funeral. Afterward, Kathy is given the film canister Shark tried to retrieve... and she cries when she sees it's footage of her on the beach when she was fourteen years old, something a young, lovestruck Shark filmed from afar...\n"}
{"id": "1947892", "url": "https://en.wikipedia.org/wiki?curid=1947892", "title": "Bya", "text": "Bya\n\nbya or b.y.a. is an abbreviation for \"billion years ago\". It is commonly used as a unit of time to denote length of time before the present in 10 years. This initialism is often used in the sciences of astronomy, geology, and paleontology.\n\nThe \"billion\" in bya is the 10 \"billion\" of the short scale of the U.S., not the long-scale 10 \"billion\" of some European usage. Billion by this convention (10) is often called a \"thousand million\" in the UK and a \"milliard\" in some other countries. For this reason, there is potential for some confusion, and some scientists prefer the unit Gya, while others prefer Ga (Giga-annum), however bya remains in more widespread use. In 1974, the UK switched from the long scale to the short scale.\n\nRelated units are mya (\"million years ago\"), and byr (\"billion years\"). These are traditionally written in lowercase. Ga or Gya has a capitalized first letter instead.\n\n"}
{"id": "57187471", "url": "https://en.wikipedia.org/wiki?curid=57187471", "title": "Christopher Reinhart", "text": "Christopher Reinhart\n\nChristopher Reinhart is a historian, researcher and research assistant in History Department, Faculty of Humanities, University of Indonesia. His research focuses on ancient history of Indonesia and Southeast Asia. After moving from Bern, he decided to stay in Indonesia and becoming a member of History Department, University of Indonesia. He wrote several articles, journals and studies about Indonesia during his time in Indonesia:\nHe recently moderated a general lecture (openbaar lezing) of Prof. Susanto Zuhdi on History Fair, University of Indonesia \n"}
{"id": "7004287", "url": "https://en.wikipedia.org/wiki?curid=7004287", "title": "Commission on Wartime Relocation and Internment of Civilians", "text": "Commission on Wartime Relocation and Internment of Civilians\n\nThe Commission on Wartime Relocation and Internment of Civilians (CWRIC) was a group of nine people appointed by the U.S. Congress in 1980 to conduct an official governmental study of Executive Order 9066 (1942), related orders during World War II, and their effects on Japanese Americans in the West and Alaska Natives in the Pribilof Islands. In February 1981, the Commission concluded that the incarceration of Japanese Americans during World War II was a \"grave injustice.\" In July 1981, the Commission held public hearings in Washington, D.C. to hear testimony from Japanese-American and Alaska Native witnesses. Public hearings followed in other American cities, including Seattle, San Francisco, Cambridge, New York City, Anchorage, the Aleutian Islands, Pribilof Islands (St. Paul), Chicago, and Los Angeles, where the testimonies were recorded. More than 750 people testified. \n\nIn 1983, the CWRIC issued its findings in \"Personal Justice Denied\", concluding that the incarceration of Japanese Americans had not been justified by military necessity. Rather, the report determined that the decision to incarcerate was based on \"racial prejudice, wartime hysteria, and a failure of political leadership.\" \n\nLastly, the Commission recommended legislative remedies: an official Government apology and redress payments to survivors. Congress passed legislation and on August 10, 1988, the Civil Liberties Act of 1988 was signed into law. The Act's purposes included the government's acknowledging and apologizing for the injustice of the evacuation and internment of US citizens and long-term residents; creating a public education fund to inform the public; making restitution to parties affected; discouraging a similar event from happening in the future; and demonstrating the U.S.' consideration of human rights violations. By this act and a related one in 1992, the US government paid reparations to more than 82,200 Japanese Americans.\n\nJoan Z. Bernstein, \"Chair\"\n\nDaniel E. Lungren, \"Vice-Chair\"\n\nEdward W. Brooke\n\nRobert F. Drinan\n\nArthur S. Flemming\n\nArthur J. Goldberg\n\nIshmael V. Gromoff\n\nWilliam M. Marutani\n\nHugh B. Mitchell\n\n"}
{"id": "20571337", "url": "https://en.wikipedia.org/wiki?curid=20571337", "title": "Confraternity book", "text": "Confraternity book\n\nA confraternity book or liber vitae is a medieval memorial book that records the names of people who have entered into a state of brotherhood with a church in some way, often by visiting it in the capacity of a pilgrim. \n\nThe following is a list of some earlier medieval confraternity books:\n\n"}
{"id": "4807194", "url": "https://en.wikipedia.org/wiki?curid=4807194", "title": "Consequences of the Black Death", "text": "Consequences of the Black Death\n\nThe consequences of the Black Death are the short-term and long-term effects of the Black Death on human populations across the world. They include a series of various biological, social, economic, political and religious upheavals which had profound effects on the course of world history, especially European history. Often referred to as simply \"The Plague\", the Black Death was one of the most devastating pandemics in human history, peaking in Eurasia between 1331 and 1350 with an estimated one-third of the continent's population ultimately succumbing to the disease. Historians estimate that it reduced the total world population from 450 million to between 350 and 375 million. In most parts of Europe, it took nearly 80 years for population sizes to recover, and in some areas more than 150 years.\n\nFrom the perspective of many of the survivors, however, the effect of the plague may have been ultimately favorable, as the massive reduction of the workforce meant their labor was suddenly in higher demand. R.H. Hilton has argued that those English peasants who survived found their situation to be much improved. For many Europeans, the 15th century was a golden age of prosperity and new opportunities. The land was plentiful, wages high, and serfdom had all but disappeared. A century later, as population growth resumed, the lower classes again faced deprivation and famine.\nFigures for the death toll vary widely by area and from source to source, and estimates are frequently revised as historical research brings new discoveries to light. Most scholars estimate that the Black Death killed between 75 and 200 million people in the 14th century, at a time when the entire world population was still less than 500 million. Even where the historical record is considered reliable, only rough estimates of the total number of deaths from the plague are possible.\n\nEurope suffered an especially significant death toll from the plague. Modern estimates range between roughly one-third and one-half of the total European population in the five-year period of 1347 to 1351, during which the most severely affected areas may have lost up to 80 percent of the population.\nContemporary chronicler Jean Froissart, incidentally, estimated the toll to be one-third, which modern scholars consider less an accurate assessment than an allusion to the Book of Revelation meant to suggest the scope of the plague. Deaths were not evenly distributed across Europe, with some areas affected very little while others were all but entirely depopulated.\n\nThe Black Death hit the culture of towns and cities disproportionately hard, although rural areas (where most of the population lived at the time) were also significantly affected. Larger cities were the worst off, as population densities and close living quarters made disease transmission easier. Cities were also strikingly filthy, infested with lice, fleas, and rats, and subject to diseases caused by malnutrition and poor hygiene. Florence's population was reduced from 110,000–120,000 inhabitants in 1338 to 50,000 in 1351. Between 60 and 70 percent of Hamburg's and Bremen's populations died. In Provence, Dauphiné, and Normandy, historians observe a decrease of 60 percent of fiscal hearths. In some regions, two-thirds of the population was annihilated. In the town of Givry, in the Bourgogne region of France, the local friar, who used to note 28 to 29 funerals a year, recorded 649 deaths in 1348, half of them in September. About half of Perpignan's population died over the course of several months (only two of the eight physicians survived the plague). Over 60 percent of Norway's population died between 1348 and 1350. London may have lost two-thirds of its population during the 1348–49 outbreak; England as a whole may have lost 70 percent of its population, which declined from 7 million before the plague to 2 million in 1400.\n\nA few rural areas, especially in Eastern Poland and Lithuania, had such low populations and were so isolated that the plague made little progress there. Other places, including parts of Hungary, the Brabant region, Hainaut, and Limbourg (in modern Belgium), as well as Santiago de Compostela, were unaffected for unknown reasons. Some historians have assumed that the presence of resistant blood groups in the local population helped them resist infection, although these regions were touched by the second plague outbreak in 1360–63 (the \"little mortality\") and later during the numerous resurgences of the plague (in 1366–69, 1374–75, 1400, 1407, etc.). Other areas which escaped the plague were isolated in mountainous regions (e.g. the Pyrenees).\n\nAll social classes were affected, although the lower classes, living together in unhealthy places, were most vulnerable. Alfonso XI of Castile was the only European monarch to die of the plague, but Peter IV of Aragon lost his wife, his daughter, and a niece in six months. Joan of England, daughter of Edward III, died in Bordeaux on her way to Castile to marry Alfonso's son, Pedro. The Byzantine Emperor lost his son, while in the Kingdom of France, Joan of Navarre, daughter of Louis X \"le Hutin\" and Margaret of Burgundy, was killed by the plague, as well as Bonne of Luxembourg, the wife of the future John II of France.\n\nEstimates of the demographic effect of the plague in Asia are based on population figures during this time and estimates of the disease's toll on population centers. The most severe outbreak of plague in the Chinese province of Hubei in 1334 claimed up to 80 percent of the population. China had several epidemics and famines from 1200 to the 1350s and its population decreased from an estimated 125 million to 65 million in the late 14th century.\n\nThe precise demographic effect of the disease in the Middle East is very difficult to calculate. Mortality was particularly high in rural areas, including significant areas of Gaza and Syria. Many rural people fled, leaving their fields and crops, and entire rural provinces are recorded as being totally depopulated. Surviving records in some cities reveal a devastating number of deaths. The 1348 outbreak in Gaza left an estimated 10,000 people dead, while Aleppo recorded a death rate of 500 per day during the same year. In Damascus, at the disease's peak in September and October 1348, a thousand deaths were recorded every day, with overall mortality estimated at between 25 and 38 percent. Syria lost a total of 400,000 people by the time the epidemic subsided in March 1349. In contrast to some higher mortality estimates in Asia and Europe, scholars such as John Fields of Trinity College in Dublin believe the mortality rate in the Middle East was less than one-third of the total population, with higher rates in selected areas.\n\nBecause 14th-century healers were at a loss to explain the cause of the Black Death, many Europeans ascribed supernatural forces, earthquakes and malicious conspiracies, among other things, as possible reasons for the plague's emergence. No one in the 14th century considered rat control a way to ward off the plague, and people began to believe only God's anger could produce such horrific displays of suffering and death. Giovanni Boccaccio, an Italian writer and poet of the era, questioned whether it was sent by God for their correction, or that it came through the influence of the heavenly bodies. Christians accused Jews of poisoning public water supplies in an effort to ruin European civilization. The spreading of this rumor led to complete destruction of entire Jewish towns, and was simply caused by suspicion on part of the Christians, who noticed that the Jews had lost fewer lives to the plague due to their hygienic practices. In February 1349, 2,000 Jews were murdered in Strasbourg. In August of the same year, the Jewish communities of Mainz and Cologne were exterminated.\n\nWhere government authorities were concerned, most monarchs instituted measures that prohibited exports of foodstuffs, condemned black market speculators, set price controls on grain, and outlawed large-scale fishing. At best, they proved mostly unenforceable. At worst, they contributed to a continent-wide downward spiral. The hardest hit lands, like England, were unable to buy grain abroad from France because of the prohibition and from most of the rest of the grain producers because of crop failures from shortage of labour. Any grain that could be shipped was eventually taken by pirates or looters to be sold on the black market. Meanwhile, many of the largest countries, most notably England and Scotland, had been at war, using up much of their treasury and exacerbating inflation. In 1337, on the eve of the first wave of the Black Death, England and France went to war in what would become known as the Hundred Years' War. Malnutrition, poverty, disease and hunger, coupled with war, growing inflation and other economic concerns made Europe in the mid-14th century ripe for tragedy.\n\nEurope had been overpopulated before the plague, and a reduction of 30 to 50 percent of the population could have resulted in higher wages and more available land and food for peasants because of less competition for resources. Historian Walter Scheidel contends that waves of plague following the initial outbreak of the Black Death had a leveling effect that changed the ratio of land to labor, reducing the value of the former while boosting that of the latter, which lowered economic inequality by making landowners and employers less well off while improving the lot of the workers. He states that \"the observed improvement in living standards of the laboring population was rooted in the suffering and premature death of tens of millions over the course of several generations.\" This leveling effect was reversed by a \"demographic recovery that resulted in renewed population pressure.\" In 1357, a third of property in London was unused due to a severe outbreak in 1348–49. However, for reasons that are still debated, population levels declined after the Black Death's first outbreak until around 1420 and did not begin to rise again until 1470, so the initial Black Death event on its own does not entirely provide a satisfactory explanation to this extended period of decline in prosperity. See Medieval demography for a more complete treatment of this issue and current theories on why improvements in living standards took longer to evolve.\n\nThe great population loss brought favourable results to the surviving peasants in England and Western Europe. There was increased social mobility, as depopulation further eroded the peasants' already weakened obligations to remain on their traditional holdings. Seigneurialism never recovered. Land was plentiful, wages high, and serfdom had all but disappeared. It was possible to move about and rise higher in life. Younger sons and women especially benefited. As population growth resumed, however, the peasants again faced deprivation and famine.\n\nIn Eastern Europe, by contrast, renewed stringency of laws tied the remaining peasant population more tightly to the land than ever before through serfdom. Sparsely populated Eastern Europe was less affected by the Black Death and so peasant revolts were less common in the fourteenth and fifteenth centuries, not occurring in the east until the sixteenth through nineteenth centuries.\n\nFurthermore, the plague's great population reduction brought cheaper land prices, more food for the average peasant, and a relatively large increase in per capita income among the peasantry, if not immediately, in the coming century. Since the plague left vast areas of farmland untended, they were made available for pasture and put more meat on the market; the consumption of meat and dairy products went up, as did the export of beef and butter from the Low Countries, Scandinavia and northern Germany. However, the upper class often attempted to stop these changes, initially in Western Europe, and more forcefully and successfully in Eastern Europe, by instituting sumptuary laws. These regulated what people (particularly of the peasant class) could wear, so that nobles could ensure that peasants did not begin to dress and act as a higher class member with their increased wealth. Another tactic was to fix prices and wages so that peasants could not demand more with increasing value. In England, the Statute of Labourers 1351 was enforced, meaning no peasant could ask for more wages than in 1346. This was met with varying success depending on the amount of rebellion it inspired; such a law was one of the causes of the 1381 Peasants' Revolt in England.\n\nThe rapid development of the use was probably one of the consequences of the Black Death, during which many landowning nobility died, leaving their realty to their widows and minor orphans.\n\nIn the wake of the drastic population decline brought on by the plague, wages shot up and labourers could move to new localities in response to wage offers. Local and royal authorities in Western Europe instituted wage controls. These governmental controls sought to freeze wages at the old levels before the Black Death. Within England, for example, the Ordinance of Labourers, enacted in 1349, and the Statute of Labourers, enacted in 1351, restricted both wage increases and the relocation of workers. If workers attempted to leave their current post, employers were given the right to have them imprisoned. The Statute was poorly enforced in most areas, and farm wages in England on average doubled between 1350 and 1450, although they were static thereafter until the end of the 19th century.\n\nCohn, comparing numerous countries, argues that these laws were not primarily designed to freeze wages. Instead, he says the energetic local and royal measures to control labor and artisans' prices was a response to elite fears of the greed and possible new powers of lesser classes that had gained new freedom. Cohn says the laws reflect the anxiety that followed the Black Death's new horrors of mass mortality and destruction, and from elite anxiety about manifestations such as the flagellant movement and the persecution of Jews, Catalans, and beggars.\n\nBy 1200, virtually all of the Mediterranean basin and most of northern Germany had been deforested and cultivated. Indigenous flora and fauna were replaced by domestic grasses and animals and domestic woodlands were lost. With depopulation, this process was reversed. Much of the primeval vegetation returned, and abandoned fields and pastures were reforested.\n\nThe Black Death encouraged innovation of labour-saving technologies, leading to higher productivity. There was a shift from grain farming to animal husbandry. Grain farming was very labor-intensive, but animal husbandry needed only a shepherd and a few dogs and pastureland.\n\nPlague brought an eventual end of Serfdom in Western Europe. The manorial system was already in trouble, but the Black Death assured its demise throughout much of western and central Europe by 1500. Severe depopulation and migration of the village to cities caused an acute shortage of agricultural labourers. Many villages were abandoned. In England, more than 1300 villages were deserted between 1350 and 1500. Wages of labourers were high, but the rise in nominal wages following the Black Death was swamped by post-Plague inflation, so that real wages fell.\n\nLabor was in such a short supply that Lords were forced to give better terms of tenure. This resulted in much lower rents in western Europe. By 1500, a new form of tenure called copyhold became prevalent in Europe. In copyhold, both a Lord and peasant made their best business deal, whereby the peasant got use of the land and the Lord got a fixed annual payment and both possessed a copy of the tenure agreement. Serfdom did not end everywhere. It lingered in parts of Western Europe and was introduced to Eastern Europe after the Black Death.\n\nThere was change in the inheritance law. Before the plague, only sons and especially the elder son inherited the ancestral property. Post plague all sons as well as daughters started inheriting property.\n\nRenewed religious fervor and fanaticism came in the wake of the Black Death. Some Europeans targeted \"groups such as Jews, friars, foreigners, beggars, pilgrims\", lepers and Romani, thinking that they were to blame for the crisis.\n\nDifferences in cultural and lifestyle practices also led to persecution. As the plague swept across Europe in the mid-14th century, annihilating more than half the population, Jews were taken as scapegoats, in part because better hygiene among Jewish communities and isolation in the ghettos meant that Jews were less affected. Accusations spread that Jews had caused the disease by deliberately poisoning wells. European mobs attacked Jewish settlements across Europe; by 1351, 60 major and 150 smaller Jewish communities had been destroyed, and more than 350 separate massacres had occurred.\n\nAccording to Joseph P. Byrne, women also faced persecution during the Black Death. Muslim women in Cairo became scapegoats when the plague struck. Byrne writes that in 1438, the sultan of Cairo was informed by his religious lawyers that the arrival of the plague was Allah's punishment for the sin of fornication and that in accordance with this theory, a law was set in place stating that women were not allowed to make public appearances as they may tempt men into sin. Byrne describes that this law was only lifted when \"the wealthy complained that their female servants could not shop for food.\"\n\nThe Black Death hit the monasteries very hard because of their proximity with the sick who sought refuge there. This left a severe shortage of clergy after the epidemic cycle. Eventually the losses were replaced by hastily trained and inexperienced clergy members, many of whom knew little of the rigors of their predecessors. New colleges were opened at established universities, and the training process sped up. The shortage of priests opened new opportunities for laywomen to assume more extensive and more important service roles in the local parish.\nFlagellants practiced self-flogging (whipping of oneself) to atone for sins. The movement became popular after the Black Death. It may be that the flagellants' later involvement in hedonism was an effort to accelerate or absorb God's wrath, to shorten the time with which others suffered. More likely, the focus of attention and popularity of their cause contributed to a sense that the world itself was ending and that their individual actions were of no consequence.\n\nReformers rarely pointed to failures on the part of the Church in dealing with the catastrophe.\n\nThe Black Death had a profound effect on art and literature. After 1350, European culture in general turned very morbid. The general mood was one of pessimism, and contemporary art turned dark with representations of death. The widespread image of the \"dance of death\" showed death (a skeleton) choosing victims at random. Many of the most graphic depictions come from writers such as Boccaccio and Petrarch. Peire Lunel de Montech, writing about 1348 in the lyric style long out of fashion, composed the following sorrowful \"sirventes\" \"Meravilhar no·s devo pas las gens\" during the height of the plague in Toulouse:\n\nBoccaccio wrote:\n\nAlthough the Black Death highlighted the shortcomings of medical science in the medieval era, it also led to positive changes in the field of medicine. As described by David Herlihy in \"The Black Death and the Transformation of the West,\" more emphasis was placed on “anatomical investigations” following the Black Death. How individuals studied the human body notably changed, becoming a process that dealt more directly with the human body in varied states of sickness and health. Further, at this time, the importance of surgeons became more evident.\n\nA theory put forth by Stephen O'Brien says the Black Death is likely responsible, through natural selection, for the high frequency of the CCR5-Δ32 genetic defect in people of European descent. The gene affects T cell function and provides protection against HIV, smallpox, and possibly plague, though for the last, no explanation as to how it would do that exists. This, however, is now challenged, given that the CCR5-Δ32 gene has been found to be just as common in Bronze Age tissue samples.\n\nThe Black Death also inspired European architecture to move in two different directions: (1) a revival of Greco-Roman styles, and (2) a further elaboration of the Gothic style. Late medieval churches had impressive structures centred on verticality, where one's eye is drawn up towards the high ceiling. The basic Gothic style was revamped with elaborate decoration in the late medieval period. Sculptors in Italian city-states emulated the work of their Roman forefathers while sculptors in northern Europe, no doubt inspired by the devastation they had witnessed, gave way to a heightened expression of emotion and an emphasis on individual differences. A tough realism came forth in architecture as in literature. Images of intense sorrow, decaying corpses, and individuals with faults as well as virtues emerged. North of the Alps, painting reached a pinnacle of precise realism with Early Dutch painting by artists such as Jan van Eyck (c. 1390– by 1441). The natural world was reproduced in these works with meticulous detail whose realism was not unlike photography.\n\n\n"}
{"id": "6324098", "url": "https://en.wikipedia.org/wiki?curid=6324098", "title": "David Kolb", "text": "David Kolb\n\nDavid Kolb (born 1939) is an American philosopher and the Charles A. Dana Professor Emeritus of Philosophy at Bates College in Maine.\n\nKolb received a B.A. from Fordham University in 1963 and an M.A. in 1965. He later received a M.Phil. from Yale University in 1970 and a Ph.D. in 1972. Kolb's Dissertation was titled \"Conceptual Pluralism and Rationality.\" Most of Kolb's writing deals with \"what it means to live with historical connections and traditions at a time when we can no longer be totally defined by that history.\" Professor Kolb taught at the University of Chicago before moving to Bates in 1977 and teaching there until 2005, when he took emeritus status.\n\nKolb has written many articles and published several books including:\n\n\n"}
{"id": "13395101", "url": "https://en.wikipedia.org/wiki?curid=13395101", "title": "From Atlantis to the Sphinx", "text": "From Atlantis to the Sphinx\n\nFrom Atlantis to the Sphinx is a work of non-fiction by British author, Colin Wilson, with the subheading \"Recovering the Lost Wisdom of the Ancient World\".\n\nWilson proposes in the text that the Great Sphinx of Giza was constructed by a technologically advanced people \"nearly 10,000 years before Egyptologists have hypothesized\" by the same people who provided plans for the construction of the pyramids of Egypt, Central and South America.\n\nThe book explores the connection between astronomy and mythology, arguing that ancient man used \"Lunar knowledge\" (intuition) as opposed to modern man's \"Solar knowledge\" (logic) to interpret the universe and therefore possessed an entirely different but equally valid mentality from that of modern man. Wilson proposes that the outlook of ancient man was based on \"seeing the big picture\" rather than logically breaking down the universe into its constituent parts.\n\nWilson develops this idea of civilisations founded on Lunar Knowledge together with astronomy to explain the monumental and seemingly spontaneous achievements of ancient cultures such as the Pyramid Complex at Giza in Egypt.\n\nWilson argues that the essential weakness of James Frazer's \"The Golden Bough\" is that Frazer attributed the fundamental mythological systems to the beginnings of the farming cultures, specifically to fertility. Agreeing with Giorgio de Santillana's thesis developed in \"Hamlet's Mill\", Wilson places the genesis of mythology previous to fertility cultures, linking the fundamental myths to astronomical occurrences such as the Precession of the Equinoxes.\n\nThe main observations drawn by Wilson are that our ancient pre-\"Homo sapiens\" ancestors possessed intelligence equal to that of modern man, their apparent lack of technological achievement being explained by the needlessness of it based on their completely different, intuitive and all-embracing mentality. Over time, a more logical and dissecting mentality evolved leading to the traits which mark modern civilisations.\n"}
{"id": "55837577", "url": "https://en.wikipedia.org/wiki?curid=55837577", "title": "Gail North-Saunders", "text": "Gail North-Saunders\n\nDiane Gail North-Saunders (born March 10, 1944) is a Bahamian historian, archivist, and author. North-Saunders established the Bahamian National Archives and was the director from 1971 until 2004. She was the president of the Bahamas Historical Society from 1989 until 1999. North-Saunders was president of the Association of Caribbean Historians; president of the Caribbean Archives Association, and an executive member of the International Council on Archives. North-Saunders has authored books about Bahamian history including \"Historic Bahamas\", \"Islanders in the Stream: A History of the Bahamian People,\" and \"Race and Class in the Colonial Bahamas, 1880–1960.\"\n\nNorth-Saunders was one of the four women to first represent The Bahamas in an international sports competition as a member of the sprint relay team at the 1962 Central American and Caribbean Games.\n\nDiane Gail North was born to Edward Basil and Audrey Virginia (Isaacs) North on March 10, 1944. During her high school and college years, she was a superior scholar and athlete. North represented the country on the sprint relay team at the 1962 Central American and Caribbean Games, in Kingston, Jamaica. At the event, along with Althea Rolle-Clarke, Elaine Thompson, and Christina Jones-Darville, she was one of the four women to first represent The Bahamas in an international sports competition.\n\nNorth earned a Bachelor of Arts in History in 1966 from University of Newcastle upon Tyne and a postgraduate certificate in Education from the University of Leicester in 1967. She taught history at Government High for two years.\n\nNorth married Winston Saunders in 1968. The couple relocated to England for further schooling. She studied at University College London and worked at the British Council in Public Record Offices to study process for archiving. When they moved back to The Bahamas in 1969, Winston took a position as deputy headmaster at Highbury High School.\n\nUpon returning to The Bahamas, North-Saunders took a position at the library in the Ministry of Education where she organized the records of the old Board of Education to make the first deposit in the National Archives.\n\nSaunders studied under historian Michael Craton at the University of Waterloo to earned a doctorate.\n\nThe Ministry of Education asked North-Saunders to establish the Bahamian National Archives. The archives were held at the Eastern Public Library (the Eastern Post Office) for 16 years. North-Saunders was the director from 1971 until 2004 and director-general of the archives until her retirement in 2008.\n\nNorth-Saunders was president of the Association of Caribbean Historians; president of the Caribbean Archives Association and an executive member of the International Council on Archives.\n\nNorth-Saunders has authored books about Bahamian history including \"Historic Bahamas\", \"Islanders in the Stream: A History of the Bahamian People (Volume 1 and 2)\" with Michael Craton, and \"Race and Class in the Colonial Bahamas, 1880–1960.\"\n\nAfter retirement from the National Archives, North-Saunders remains active in academic pursuits as Scholar-in-residence at the College of The Bahamas. In 2006, her husband Winston died.\n\nNorth-Saunders was awarded the Commonwealth honour of the Order of the British Empire (OBE) in 2003. The University of the West Indies awarded her an honorary degree in 2004. She was inducted into the Bahamas National Sports Hall of Fame in 2013.\n"}
{"id": "14104519", "url": "https://en.wikipedia.org/wiki?curid=14104519", "title": "Historic paint analysis", "text": "Historic paint analysis\n\nHistoric paint analysis is the scientific analysis of architectural finishes, including not only paints but also metallic finishes and clear and translucent finishes used on historic buildings. The primary purpose of such analysis is to determine the color of the finish used at a particular time in the building's history, usually the original construction, but not always. Secondary purposes include determination of ingredients such as media (water, oil, latex, etc.) and pigments (organic pigments, inorganic pigments, dyes, etc.). Paint analysis is also used at times as a dating technique for various building elements.\n\nTypical problems encountered in historic paint analysis include such things as paint loss, surface deterioration, newer materials, substrates, delamination, media and pigment deterioration, and alligatoring.\n\nHistoric architectural paint analysis finds its roots in the early twentieth century in the United States. The historic preservation movement began in 1849 with the preservation of Mount Vernon, the home of George Washington. Early preservationists began to realize that paints and finishes which had survived were very important but may not have been the original, or historic, finishes. Interest in historic wallpapers also developed with the interest in historic paint and color. One of the earliest endeavors came with the restoration of Williamsburg, Virginia funded by John D. Rockefeller in the 1920s. Early investigations by simple scraping of the finishes by Susan Nash of the surviving original buildings yielded a palette that became popularly known as Williamsburg colors. \n\nIn the 1950s and 1960s serious efforts at investigating original paint colors were underway at Independence National Historical Park in Philadelphia by architect Penelope Hartshorne Batcheler. Her pioneering efforts introduced, for the first time in this country, the use of a stereo microscope to more closely examine the 18th century paints at Independence Hall. Batcheler also introduced the use of the Munsell Color System for matching and referencing original paint colors. Her landmark publication, \"Paint Color Research and Restoration\", was the very first publication concerning the analysis of historic architectural paints for determination of original colors. At the same time, in the United Kingdom, microscopy of paint samples was developed by Joyce Plesters of the National Gallery, London who worked mainly with easel paintings but also with samples from wall-paintings.\n\nIn the 1960s and early 1970s Morgan W. Phillips at the Society for the Preservation of New England Antiquities (SPNEA) became involved with historic paint and color analysis, specifically at the Harrison Gray Otis House in Boston. At the same time, E. Blaine Cliver, Historical Architect, who originally worked with Batcheler at the National Park Service (NPS) in Philadelphia, then with the National Trust for Historic Preservation in Washington, DC and later at the Northeast Regional Office of the NPS, became involved with historic paint analysis, especially in a laboratory in Building 28 of the former Boston Navy Yard. In the early 1970s, Frank S. Welsh joined the NPS in Philadelphia and began his research and study of historic paints with Penelope Batcheler, where he introduced the use of the National Bureau of Standards Color Name Charts, (NIST) for naming the colors matched to the Munsell Color System. Welsh also was the first to introduce the term \"paint analysis\" into the lexicon of historic preservation. As an independent historic paint color consultant one of his first major projects was Monticello, the home of Thomas Jefferson. In the mid 1970s, Matthew J. Mosca started working for the National Trust with Blaine Cliver. Later, as a preservation consultant, Mosca researched the historic colors of Mount Vernon.\n\nThe advances in the science of paint color research by these individuals suggested that the popular Williamsburg colors had been matched to faded and aged finishes. During the 1980s and 1990s Colonial Williamsburg consulted with Welsh to undertake a comprehensive paint and color analysis on numerous buildings in the historic area. In his research, \"the first modern scientific paint analysis\" there, confirmed that the Williamsburg color palette did not represent the actual historic colors. In addition he found that in many cases their early efforts had mistakenly matched later paint layers, some nineteenth-century.\n\nHistorically, paint analysis was done on site by carefully removing later paint layers to reveal a sequence of finishes down to the substrate. This was the methodology employed during the early restoration of finishes at Historic Williamsburg. Although this method is employed by a few practitioners, it is not common because of its inherent problems of misinterpretation and failure to address issues such as paint ageing and discoloration. Because finishes analysis is performed under laboratory conditions samples are collected in the field for later analysis and can be collected by the analyst or by his client who then ships them to him.\n\nThe primary purposes of analysis are to determine historic finishes and to determine principle components such as media or basic pigments. There are two methodologies in the preparation of paint samples for microscopic analysis. The first, which is derived from the medical world, is to treat the sample as a specimen and set it into a fixed position in a permanent medium such as paraffin. The specimen is then ground to a flat finish, providing a horizontal surface for viewing under a microscope. The second is to leave the samples in a loose condition with their broken surfaces which then can be manipulated under the microscope to permit a variety of views of the layers. The primary disadvantage to the first method is that the grinding process tends to blur layers together, especially layers of similar or identical colors. It also provide only a single, fixed point of viewing. The second method lacks these disadvantages, although skill and experience is required to manipulate the samples effectively.\n\nFollowing preparation of the samples, they are typically viewed under an optical microscope using either natural north light or polarized artificial light simulating natural north light. North light is essential in order to render the colors accurately without the effects of the yellow spectrum of direct sunlight. Each individual layer is identified and, typically, matched to the Munsell color system. The Munsell color system is a scientific system in which colors have been ranged into a color fan based upon three attributes: hue or color, the chroma or color saturation, and the value or neutral lightness or darkness. Unlike color systems developed by paint manufacturers, the Munsell system provides an unchanging standard of reference which is unaffected by the marketplace and changing tastes in colors.\n\nThe hue notation, the color, indicates the relation of the sample to a visually equally spaced scale of 100 hues. There are 10 major hues, five principal and five intermediate within this scale. The hues are identified by initials indicating the central member of the group: red R, yellow-red YR, yellow Y, yellow-green YG, green G, blue-green BG, blue B, purple-blue PB, purple P, and red-purple PR. The hues in each group are identified by the numbers 1 to 10. The most purplish of the red hues, 1 on the scale of 100, is designated as 1R, the most yellowish as 10R, and the central hue as 5R. The hue 10R can also be expressed as 10, 5Y as 25, and so forth if a notation of the hue as a number is desired. Chroma indicates the degree of departure of a given hue from the neutral gray axis of the same value. It is the strength of saturation of color from neutral gray, written /0 to /14 or further for maximum color saturation.\n\nValue, or lightness, makes up the neutral gray axis of the color wheel, ranging from black, number 1, to white at the top of the axis, number 10. A visual value can be approximated by the help of the neutral gray chips of the Rock or Soil Color chart with ten intervals. The color parameters can be expressed with figures semi-quantitatively as: hue, value/chroma (H, V/C). The color \"medium red\" should serve as an example for presentation with the three color attributes, 5R 5.5/6. This means that 5R is located in the middle of the red hue, 5.5 is the lightness of Munsell value near the middle between light and dark, and 6 is the degree of the Munsell chroma, or the color saturation, which is about in the middle of the saturation scale.\n\n\n"}
{"id": "1219810", "url": "https://en.wikipedia.org/wiki?curid=1219810", "title": "Historic preservation", "text": "Historic preservation\n\nHistoric preservation (US), heritage preservation or heritage conservation (UK), is an endeavour that seeks to preserve, conserve and protect buildings, objects, landscapes or other artifacts of historical significance.\nThis term refers specifically to the preservation of the built environment, and not to preservation of, for example, primeval forests or wilderness.\n\nIn England, antiquarian interests were a familiar gentleman's pursuit since the mid 17th century, developing in tandem with the rise in scientific curiosity. Fellows of the Royal Society were often also Fellows of the Society of Antiquaries.\n\nMany historic sites were damaged as the railways began to spread across the UK; including Trinity Hospital and its church in Edinburgh, Furness Abbey, Berwick and Northampton Castle, and the ancient walls of York, Chester and Newcastle. In 1833 Berkhamsted Castle became the first historic site in England officially protected by statute under the London and Birmingham Railway Acts of 18331837, though the new railway line in 1834 did demolish the castle's gatehouse and outer earthworks to the south.\nAnother early preservation event also occurred at Berkhamsted. In 1866, Lord Brownlow who lived at Ashridge, tried to enclose the adjoining Berkhamsted Common with steel fences in an attempt to claim it as part of his estate. In England from early Anglo-Saxon times, Common land was an area of land which the local community could use as a resource. Across England between 1660 and 1845, 7 million acres of Common land had been enclosed by private land owners by application to parliament. On the night of 6 March 1866, Augustus Smith MP led gangs of local folk and hired men from London's East End in direct action to break the enclosure fences and protect Berkhamsted Common for the people of Berkhamsted in what became known nationally as the Battle of Berkhamsted Common. In 1870, Sir Robert Hunter (later co-founder of the National Trust in 1895) and the Commons Preservation Society succeed in legal action that ensured protection of Berkhamsted Common and other open spaces threatened with enclosure. In 1926 the common was acquired by the National Trust.\n\nBy the mid 19th century, much of Britain's unprotected cultural heritage was being slowly destroyed. Even well-meaning archaeologists like William Greenwell excavated sites with virtually no attempt at their preservation, Stonehenge came under increasing threat by the 1870s. Tourists were chipping off parts of the stones or carving their initials into the rock. The private owners of the monument decided to sell the land to the London and South-Western Railway as the monument was \"not the slightest use to anyone now\". John Lubbock, an MP and botanist emerged as the champion of the country's national heritage. In 1872 he personally bought private land that housed ancient monuments in Avebury, Silbury Hill and elsewhere, from the owners who were threatening to have them cleared away to make room for housing. Soon, he began campaigning in Parliament for legislation to protect monuments from destruction. This finally led to the legislative milestone under the Liberal government of William Gladstone of the Ancient Monuments Protection Act 1882. The first government appointed inspector for this job was the archaeologist Augustus Pitt-Rivers. This legislation was regarded by conservative political elements as a grave assault on the individual rights of property of the owner, and consequently, the inspector only had the power to identify endangered landmarks and offer to purchase them from the owner with his consent. The Act only covered ancient monuments and explicitly did not cover historic buildings or structures. In 1877 the Society for the Protection of Ancient Buildings was founded by the Arts and Crafts designer William Morris to prevent the destruction of historic buildings, followed by the National Trust in 1895 that bought estates from their owners for preservation.\n\nThe Ancient Monuments Protection Act 1882 had only given legal protection to prehistoric sites, such as ancient tumuli. The Ancient Monuments Protection Act 1900 took this further by empowering the government's Commissioners of Work and local County Councils to protect a wider range of properties. Further updates were made in 1910.\nTattershall Castle, Lincolnshire, a medieval manor house had been put up for sale in 1910 with its greatest treasures, the huge medieval fireplaces, still intact. However, when an American bought the house they were ripped out and packaged up for shipping. The former viceroy of India, George Curzon, 1st Marquess Curzon of Kedleston, was outraged at this cultural destruction and stepped in to buy back the castle and reinstall the fireplaces. After a nationwide hunt for them they were finally found in London and returned. He restored the castle and left it to the National Trust on his death in 1925. His experience at Tattershall influenced Lord Curzon to push for tougher heritage protection laws in Britain, which saw passage as the Ancient Monuments Consolidation and Amendment Act 1913.\n\nThe new structure involved the creation of the Ancient Monuments Board to oversee the protection of such monuments. Powers were given for the board, with Parliamentary approval, to issue preservation orders to protect monuments, and extended the public right of access to these. The term \"monument\" was extended to include the lands around it, allowing the protection of the wider landscape.\n\nThe National Trust was founded in 1894 by Octavia Hill, Sir Robert Hunter, and Hardwicke Canon Rawnsley as the first organisation of its type in the world. Its formal purpose is: The preservation for the benefit of the Nation of lands and tenements (including buildings) of beauty or historic interest and, as regards lands, for the preservation of their natural aspect, features and animal and plant life. Also the preservation of furniture, pictures and chattels of any description having national and historic or artistic interest.\n\nIn the early days, the Trust was concerned primarily with protecting open spaces and a variety of threatened buildings; its first property was Alfriston Clergy House and its first nature reserve was Wicken Fen. Its first archaeological monument was White Barrow. The focus on country houses and gardens, which now comprise the majority of its most visited properties, came about in the mid 20th century, when it was realised that the private owners of many of these properties were no longer able to afford to maintain them.\n\nThe Town and Country Planning Act 1944 and the Town and Country Planning Act 1990 took steps toward historic preservation on an unprecedented scale. Concern about the demolition of historic buildings arose in institutions such as the pressure group the Society for the Preservation of Historic Buildings, which appealed against demolition and neglect on a case by case basis.\n\nEnglish Heritage formed in 1983, is a registered charity that looks after the National Heritage Collection in England. This comprises over 400 of England's historic buildings, monuments and sites spanning more than 5,000 years of history. Within its portfolio are Stonehenge, Dover Castle, Tintagel Castle and the best preserved parts of Hadrian's Wall.\n\nOriginally English Heritage was the operating name of an executive non-departmental public body of the British Government, officially titled the Historic Buildings and Monuments Commission for England, that ran the national system of heritage protection and managed a range of historic properties. It was created to combine the roles of existing bodies that had emerged from a long period of state involvement in heritage protection. In 1999 the organisation merged with the Royal Commission on the Historical Monuments of England and the National Monuments Record (England), bringing together resources for the identification and survey of England's historic environment. On 1 April 2015, English Heritage was divided into two parts: Historic England, which inherited the statutory and protection functions of the old organisation, and the new English Heritage Trust, a charity that would operate the historic properties, and which took on the English Heritage operating name and logo. The British government gave the new charity an £80 million grant to help establish it as an independent trust, although the historic properties remained in the ownership of the state.\n\nIn the United States one of the first historic preservation efforts was the\nWashington's Headquarters State Historic Site, in Newburgh, New York. This property has the distinction of being the first-ever property designated and operated as a historic site by a U.S. state, having been so since 1850.\n\nAnother early historic preservation undertaking was that of George Washington's Mount Vernon in 1858. Founded in 1889, the Richmond, Virginia-based Preservation Virginia (formerly known as the Association for the Preservation of Virginia Antiquities) was the United States' first statewide historic preservation group. The American Scenic and Historic Preservation Society was formed in 1895 as the first American organization of its kind in the United States that did not limit its activities to a single historic place or object. The Society operated as a national organization to: protect the natural scenery and the preservation of historic landmarks; to preserve landmarks and records of the past or present; to erect memorials and promote appreciation of the scenic beauty of America.\n\nCharles E. Peterson was an influential figure in the mid-20th century establishing the Historic American Buildings Survey (HABS), advising on the establishment of Independence National Historical Park, helping with the first graduate degree program in historic preservation in the United States at Columbia University, and author.\n\nThe architectural firm of Simons & Lapham (Albert Simons and Samuel Lapham) was an influential supporter of the nation's first historic preservation ordinance in Charleston, South Carolina in 1930, affording that city a regulatory means by which to prevent the destruction of its historic building stock. In 1925, efforts to preserve the historic buildings of the French Quarter in New Orleans led to the creation of the Vieux Carré Commission and later, to the adoption of a historic preservation ordinance.\n\nThe US National Trust for Historic Preservation, another privately funded non-profit organization, began in 1949 with a handful of structures and has developed goals that provide \"leadership, education, advocacy, and resources to save America's diverse historic places and revitalize our communities\" according to the Trust's mission statement. In 1951 the Trust assumed responsibility for its first museum property, Woodlawn Plantation in northern Virginia. Twenty-eight sites in all have subsequently become part of the National Trust, representing the cultural diversity of American history. In New York City, the destruction of Pennsylvania Station in 1964 shocked many nationwide into supporting preservation. The 1960s proved advantageous with new laws and international agreements extending preservation \"from ancient monuments to whole districts and buildings a few decades old.\" On an international level, the New York-based World Monuments Fund was founded in 1965 to preserve historic sites all over the world.\n\nUnder the direction of James Marston Fitch, the first advanced-degree historic preservation program began at Columbia University in 1964. It became the model on which most other graduate historic preservation programs were created. Many other programs were to follow before 1980: M.A. in Preservation Planning from Cornell (1975); M.S. in Historic Preservation from the University of Vermont (1975); M.S. in Historic Preservation Studies from Boston University (1976); M.S. in Historic Preservation from Eastern Michigan University (1979) and M.F.A. in Historic Preservation was one of the original programs at Savannah College of Art & Design. James Marston Fitch also offered guidance and support towards the founding of the Master of Preservation Studies Degree within the Tulane School of Architecture in 1996. The M.Sc. in Building Conservation degree program is offered by the School of Architecture at Rensselaer Polytechnic Institute in Troy, New York. In 2005, Clemson University and the College of Charleston created an M.S. degree program based in Charleston, SC. The first undergraduate programs (B.A.) appeared in 1977 from Goucher College and Roger Williams University (then called Roger Williams College), followed by Mary Washington College in 1979. there were more than fifty historic preservation programs offering certificates, associate, bachelor's, and master's degrees in the United States.\n\nIn Canada, the phrase \"heritage preservation\" is sometimes seen as a specific approach to the treatment of historic places and sites, rather than a general concept of conservation. \"Conservation\" is taken as the more general term, referring to all actions or processes that are aimed at safeguarding the character-defining elements of a cultural resource so as to retain its heritage value and extend its physical life.\n\nHistoric objects in Canada may be granted special designation by any of the three levels of government: the central government, the provincial governments, or a municipal government.\nThe Heritage Canada Foundation acts as Canada's lead advocacy organisation for heritage buildings and landscapes.\nVictor de Stuers is widely considered the man who started historic preservation in the Netherlands. In 1875 the first national department for conservation was established and de Stuers was appointed as the first legal secretary at the Ministry of Home Affairs as chief of the brand new Department of Arts and Sciences. He was the driving force behind \"Monumentenzorg\" (Foundation for Historic Preservation), helped found the \"Rijksmuseum\" (National Museum) and the \"Rijksarchief\" (National Archives).\n\nHowever, it was not until the 20th century that there was national legislation on historic preservation. In 1961 the (“Monuments Act”) was passed. It defined that any physical building or space that was at least fifty years old and “which are of general interest because of their beauty, their meaning to science or their social value” and must thus be preserved. In 1988 this Act was replaced by the \"\" (“Monuments Act 1988”) and in 2015 by the \"\" (“Heritage Law”).\n\nIn 1973, the NGO \"Monumentenwacht\" (“Monument Watch”) was founded with the purpose of providing preventative measures of maintenance for historic buildings. As the majority of the historic preservation programs in the Netherlands, this program is decentralized, managed on the provincial level. Owners of heritage buildings can subscribe to the services of Monumentenwacht and receive regular visits for inspection. The costs are covered through a combination of national and provincial subsidies.\n\nA special kind of preservation that takes place in the Netherlands is the preservation of maritime heritage. Maritime trade was the Dutch specialty which shaped much of their culture and as a country that is 50% under sea level the Dutch history is closely intertwined with water. There are maritime museums in both Amsterdam and Rotterdam that tell the story of the Dutch maritime heritage, but there is not much legal documentation on how to preserve it. For example, according to Sarah Dromgoole, shipwrecks from The Dutch East India Company are found all around the world, which are still property of the Netherlands, but the Dutch government rarely takes responsibility for this property that is found outside of their territory.\n\nIn Macedonia, historic preservation falls under the overarching category of cultural heritage preservation according to the Law on Protection of Cultural Heritage (Закон за заштита на културното наследство). According to this law, which the Macedonian Parliament approved in March 2004, there are three types of cultural heritage: immovable, movable, and intangible. Historical preservation is represented by the protection of monuments and monumental entireties under immovable cultural heritage, and historical items under movable cultural heritage.\n\nAlthough this Law was the first nation-wide establishment of regulations for historic preservation since the Republic of Macedonia gained independence from Yugoslavia on September 8, 1991, several organizations throughout the 20th century have encompassed efforts of historic preservation.\n\nThe “Central office for protection of cultural monuments and natural rarities of the Socialist Republic of Macedonia” has existed since 1949. In 1960, the Central Office was renamed to “National office for protection of cultural monuments”, and granted the status of an independent cultural institution, with authority to execute activities of historic preservation. After the establishment of the Law on Protection of Cultural Heritage in 2004, the Ministry of Culture once again renamed the office to “National center for conservation” and narrowed down its responsibilities to dealing solely with preservation of immovable cultural heritage.\n\nOther organizations which have contributed to the efforts of historic preservation are the Macedonian National Committee of ICOMOS and the NI Institute for Protection of Monuments of Culture and Museum-Ohrid.\n\nThe International Council on Monuments and Sites (ICOMOS) established their branch in Macedonia in 1995 through the initiative of 43 conservationists from Macedonia. The guiding principles of the Macedonian National Committee of ICOMOS are raising the national consciousness about the importance of historic and cultural heritage, decentralization of the discourse about heritage, and effective monitoring of the status of cultural and historic heritage in the country.\n\nThe NI Institute for Protection of Monuments of Culture and Museum - Ohrid is the second oldest institutions for historical preservation established in 1952. In 1956 the Institute was granted authority to protect movable and immovable cultural and historic heritage in the Ohrid region. The Institute has since executed numerous efforts for historic preservation, most notably aiding the recognition of the city of Ohrid as a UNESCO site of cultural heritage in 1979.\n\nToday, the main authority for historic preservation is the Cultural Heritage Protection Office (Управа за заштита на културно наследство). The Office is an independent governmental organization under the Ministry of Culture, divided into three departments:\n\nIn Israel, there are currently two laws concerning historic preservation, Antiquities Law of the State of Israel (1978) and Planning and Building Law (1965). Both laws were adapted from the British law that was implemented during the British Mandate of Palestine.\n\nHowever, these laws are not comprehensive and limited in scope: the Antiquities Law only applies itself to buildings or artifacts dated before 1700 BC. So while efforts discovering and protecting anything older than 1700 BC are well protected, anything from later historical periods is not under the protection of this law. The Planning and Building Law discusses the overall management and regulation of land use in Israel. It has been through several changes and amendments specifically regarding preservation, but over the years it hasn't been enforced and many historical sites were destroyed, as the state was prioritizing developmental and economic interests.\n\nDuring the 1960s, the issue of preservation was gaining public awareness, and as a response to the destruction of Herzliya Hebrew Gymnasium (one of the first educational institutions in Israel) in 1959, a wave of shock and anger led to extensive public debate.\n\nIn 1984, The Council for Conservation of Heritage Sites in Israel was established, at the recommendation of the Knesset and the Committee of Education. Its aims include locating remains of historic settlements, protect and conserve them as well as developing conservation principles that are specific to Israel’s historic situations and are aligned with international standards. The council used to operate under the Society for the Protection of Nature in Israel but in 2008 registers as an independent non-profit. Today, it is the organization responsible for the most historical preservation endeavors as well as efforts to add amendments to existing laws to provide a comprehensive and effective framework for preservation in Israel.\n\nA different, separate effort in preservation comes from the Israeli Defense Force (IDF). The IDF surveyed 94 military bases and found that about 80 of them include sites worth preserving, and for each of these bases there is a preservation plan. The IDF is working towards maintaining these building as well as communicating their value to the soldiers in these bases. Buildings include Knights Templar sites, old military bases used by the British or German or buildings from the Ottoman period.\n\nA historic district in the United States is a group of buildings, properties, or sites that have been designated by one of several entities on different levels as historically or architecturally significant. Buildings, structures, objects and sites within a historic district are normally divided into two categories, contributing and non-contributing. Districts greatly vary in size, some having hundreds of structures while others have just a few.\n\nThe U.S. federal government designates historic districts through the U.S. Department of Interior, under the auspices of the National Park Service. Federally designated historic districts are listed on the National Register of Historic Places. Historic districts allows rural areas to preserve their characters through historic preservation programs. These include \"Main Street\" programs that can be used to redevelop rural downtowns. Using historic preservation programs as an economic development tool for local governments in rural areas has enabled some of those areas to take advantage of their history and develop a tourism market that in turn provides funds for maintaining an economic stability that these areas would not have seen otherwise.\n\nA similar concept exists in the United Kingdom: a Conservation area is designated in accordance with the Planning (Listed Buildings and Conservation Areas) Act 1990 in order to protect a zone in which there are buildings of architectural or cultural heritage interest.\n\nIn 1835, the English poet William Wordsworth described the Lake District as a \"sort of national property, in which every man has a right and interest who has an eye to perceive and a heart to enjoy.\"\n\nIt was, however, the United States that led the world in the creation of National Parks, areas of unspoiled natural wilderness, where the intrusion of civilization are intentionally minimal.\n\nThe department of the interior designated several areas of Morristown, New Jersey as the first historic park in the United States national park system. It became designated as the Morristown National Historical Park. The community had permanent settlements that date to 1715, is termed the military capital of the American Revolution, and contains many designations of sites and locations. The park includes three major sites in Morristown.\n\nIn the United Kingdom, James Bryce the ambassador to the US praised the system of National Parks and campaigned to have them introduced in Great Britain. Little came of it until mounting public pressure during the early 20th century from the Ramblers' Association and other groups led to the National Parks and Access to the Countryside Act 1949.\n\nAccording to UNESCO’s 1972 World Heritage Convention, landscapes and sites of outstanding universal value can be designated as World Heritage Sites. The World Heritage Convention encompasses historic preservation under the category of “cultural heritage”. According to Article 1 of the Convention, monuments, groups of buildings, and sites “which are of outstanding universal value from the point of view of history, art or science” are to be designated cultural heritage.\n\nA requirement of such designation is that the designating nation has appropriate legal, scientific, technical, administrative and financial measures in place to identify, protect, conserve, present, and rehabilitate world heritage sites. However, according to Article 6 of the Convention, while sovereignty of the State where the site is located is not to be compromised, the State acknowledges that protection of heritage sites is a duty of the entire international community.\n\nThe World Heritage convention’s counterpart, The World Heritage Committee, is the body responsible for the practical implementation of the Convention as well as managing and deciding how to use the World Heritage Fund. The Committee also gets to have the final say when determining whether a property will be included in the World Heritage List.\n\nThe Committee meets once a year and includes representatives from 21 states that are part of the States Parties. Yearly reports are available to the public on the World Heritage website and include outlines of decisions made, outcomes, working documents and various reports.\n\nAlthough preservation efforts can have benefits for the owners of historical buildings, such as tax cuts and subsidies, there are also drawbacks.\n\nOne such drawback is that after a neighborhood has been designated to be historically preserved, there is less construction. On the long term this can affect the value of property and investment in housing, both in the neighborhood itself and the neighborhoods directly surrounding it.\n\nA second concern that has been raised is that buildings that need to be historically preserved are sometimes still inhabited. In some cases their inability to make changes to the building can lead to dangerous or unhealthy situations for residents.\n\nIt is not true that nothing could be changed or renovated, but the owner of the building would need to ask permission at the appropriate preservation society, slowing the process down severely. The exact policies are country dependent.\n\n\nAlthough volunteers continue to play a large role in historic preservation activities, the field has seen an increased level of professionalization. Today, there are many career options in historic preservation in the public, non-profit and private sectors. Institutes of secondary education (universities, colleges, etc.) in the United States offer both certificate and degree (A.A.S, B.A., B.F.A., B.S., M.A., M.F.A., M.D.S, M.H.P., M.S., and PhD) programs in historic preservation. Some pupils—at schools with such programmes available—choose to enroll in \"joint degree\" programs, earning a degree in historic preservation along with one in another, related subject, often an MArch, MUP or JD degree.\n\nPossible career fields include:\n\n\n"}
{"id": "47499031", "url": "https://en.wikipedia.org/wiki?curid=47499031", "title": "Historical anthropology", "text": "Historical anthropology\n\nHistorical anthropology is a historiographical movement which applies methodologies and objectives from Social and Cultural Anthropology to the study of historical societies. Like most such movements, it is understood in different ways by different scholars, and to some may be synonymous with the history of mentalities, cultural history, ethnohistory, microhistory, history from below or \"Alltagsgeschichte\". Anthropologists whose work has been particularly inspirational to historical anthropology include Emile Durkheim, Clifford Geertz, Arnold van Gennep, Jack Goody, Lucien Lévy-Bruhl, Marcel Mauss and Victor Turner.\n\nPeter Burke has contrasted historical anthropology with Social History, finding that historical anthropology tends to focus on qualitative rather than quantitative data, smaller communities, and symbolic aspects of culture. Thus it reflects a turn away, in the 1960s, in Marxist historiography from 'the orthodox Marxist approach to human behaviour in which actors are seen as motivated in the first instance by economics, and only secondarily by culture or ideology', in the work of historians such as E. P. Thompson.\n\nHistorical anthropology was rooted in the Annales School, associated with a succession of major historians such as Fernand Braudel, Jacques Le Goff, Emmanuel Le Roy Ladurie and Pierre Nora, along with researchers from elsewhere on the Continent such as Carlo Ginzburg. The label \"historical anthropology\" has been actively promoted by some recent Annales School historians, such as Jean-Claude Schmitt. Established in 1929 by Marc Bloch and Lucien Febvre, the review Annales. Histoire, Sciences sociales is still among the most influential French publications for research in historical anthropology.\n\nHistorical anthropology has been open to similar criticisms to anthropology: 'as Bernard Cohn and John and Jean Comaroff have observed, studies in which societies were represented in this way were often partial, biased, and unwitting handmaidens to the domination of non-Western peoples by Europeans and Americans'. But since the Second World War, increasingly reflexive approaches have led to sophisticated developments of the field, and the banner of 'historical anthropology' has often attracted Anglo-American historians in ways that the Annales School did not: key figures have been Sidney Mintz, Jay O'Brien, William Roseberry, Marshall Sahlins, Jane Schneider, Peter Schneider, Eric Wolf, Peter Burke, and people from elsewhere in the world such as Aaron Gurevich.\n\n"}
{"id": "444914", "url": "https://en.wikipedia.org/wiki?curid=444914", "title": "Informbiro period", "text": "Informbiro period\n\nInformbiro (also the Informbiro period or the time of the Informbiro) was a period in the history of Yugoslavia which spanned from 1948 to 1955, characterised by conflict and schism with the Soviet Union. The word \"Informbiro\" is the Yugoslav name for the Cominform, an abbreviation for \"Information Bureau,\" from \"Communist Information Bureau\".\n\nThe Tito–Stalin, or Yugoslav–Soviet split took place in the spring and early summer of 1948. Its title pertains to Josip Broz Tito, at the time the Yugoslav Prime Minister (President of the Federal Assembly), and Soviet Premier Joseph Stalin. In the West, Tito was thought of as a loyal communist leader, second only to Stalin in the Eastern Bloc. However, having largely liberated itself with only limited Red Army support, Yugoslavia steered an independent course, and was constantly experiencing tensions with the Soviet Union. Yugoslavia and the Yugoslav government considered themselves allies of Moscow, while Moscow considered Yugoslavia a satellite and often treated it as such. Previous tensions erupted over a number of issues, but after the Moscow meeting, an open confrontation was beginning.\n\nNext came an exchange of letters directly between the Communist Party of the Soviet Union (CPSU), and the Communist Party of Yugoslavia (KPJ). In the first CPSU letter of 27 March 1948, the Soviets accused the Yugoslavs of denigrating Soviet socialism via statements such as \"socialism in the Soviet Union has ceased to be revolutionary\". It also claimed that the KPJ was not \"democratic enough\", and that it was not acting as a vanguard that would lead the country to socialism. The Soviets said that they \"could not consider such a Communist party organization to be Marxist-Leninist, Bolshevik\". The letter also named a number of high-ranking officials as \"dubious Marxists\" (Milovan Đilas, Aleksandar Ranković, Boris Kidrič, and Svetozar Vukmanović-Tempo) inviting Tito to purge them, and thus cause a rift in his own party. Communist officials Andrija Hebrang and Sreten Žujović supported the Soviet view.\n\nTito, however, saw through it, refused to compromise his own party, and soon responded with his own letter. The KPJ response on 13 April 1948 was a strong denial of the Soviet accusations, both defending the revolutionary nature of the party, and re-asserting its high opinion of the Soviet Union. However, the KPJ noted also that \"no matter how much each of us loves the land of socialism, the Soviet Union, he can in no case love his own country less\". In a speech, the Yugoslav Prime Minister stated\nThe 31 page-long Soviet answer of 4 May 1948 admonished the KPJ for failing to admit and correct its mistakes, and went on to accuse it of being too proud of their successes against the Germans, maintaining that the Red Army had \"saved them from destruction\" (an implausible statement, as Tito's partisans had successfully campaigned against Axis forces for four years before the appearance of the Red Army there). This time, the Soviets named Josip Broz Tito and Edvard Kardelj as the principal \"heretics\", while defending Hebrang and Žujović. The letter suggested that the Yugoslavs bring their \"case\" before the Cominform. The KPJ responded by expelling Hebrang and Žujović from the party, and by answering the Soviets on 17 May 1948 with a letter which sharply criticized Soviet attempts to devalue the successes of the Yugoslav resistance movement.\n\nOn 19 May 1948, a correspondence by Mikhail A. Suslov informed Josip Broz Tito that the Communist Information Bureau, or Cominform (\"Informbiro\" in Serbo-Croatian), would be holding a session on 28 June 1948 in Bucharest almost completely dedicated to the \"Yugoslav issue\". The Cominform was an association of communist parties that was the primary Soviet tool for controlling the political developments in the Eastern Bloc. The date of the meeting, 28 June, was carefully chosen by the Soviets as the triple anniversary of the Battle of Kosovo Field (1389), the assassination of Archduke Ferdinand in Sarajevo (1914), and the adoption of the Vidovdan Constitution (1921).\n\nTito, personally invited, refused to attend under a dubious excuse of illness. When an official invitation arrived on 19 June 1948, Tito again refused. On the first day of the meeting, 28 June, the Cominform adopted the prepared text of a resolution, known in Yugoslavia as the \"Resolution of the Informbiro\" (\"Rezolucija Informbiroa\"). In it, the other Cominform (\"Informbiro\") members expelled Yugoslavia, citing \"nationalist elements\" that had \"managed in the course of the past five or six months to reach a dominant position in the leadership\" of the KPJ. The resolution warned Yugoslavia that it was on the path back to bourgeois capitalism due to its nationalist, independence-minded positions, and accused the party itself of \"Trotskyism\". This was followed by the severing of relations between Yugoslavia and the Soviet Union, beginning the period of Soviet–Yugoslav conflict between 1948 and 1955 known as the Informbiro Period.\n\nAfter the break with the Soviet Union, Yugoslavia found itself economically and politically isolated as the country's Eastern Bloc-oriented economy began to falter. At the same time, Stalinist Yugoslavs, known in Yugoslavia as \"cominformists\", began fomenting civil and military unrest. A number of cominformist rebellions and military insurrections took place, along with acts of sabotage. However, the Yugoslav security service led by Aleksandar Ranković, the UDBA, was quick and efficient in cracking down on insurgent activity. Invasion appeared imminent, as Soviet military units massed along the border with the People's Republic of Hungary, while the Hungarian People's Army was quickly increased in size from 2 to 15 divisions. The UDBA began arresting alleged Cominformists even under suspicion of being pro-Soviet.\n\nHowever, from the start of the crisis, Tito began making overtures to the United States and the West. Consequently, Stalin's plans were thwarted as Yugoslavia began shifting its alignment. The West welcomed the Yugoslav-Soviet rift and in 1949 commenced a flow of economic aid, assisted in averting famine in 1950, and covered much of Yugoslavia's trade deficit for the next decade. The United States began shipping weapons to Yugoslavia in 1951. Tito, however, was wary of becoming too dependent on the West as well, and military security arrangements concluded in 1953 as Yugoslavia refused to join NATO and began developing a significant military industry of its own. With the American response in the Korean War serving as an example of the West's commitment, Stalin began backing down from war with Yugoslavia.\n\nThe term refers to the Cominform Resolution of June 28, 1948 (resulting from the Tito–Stalin Split) that accused the Communist Party of Yugoslavia (KPJ), among other things, of \"depart[ing] from Marxism-Leninism\", exhibiting an \"anti-Soviet attitude,\" \"meeting criticism with hostility\" and \"reject[ing] to discuss the situation at an Informbureau meeting.\" Following these allegations, the resolution expelled the KPJ from Cominform. As a result, Yugoslavia fell outside of the Soviet sphere of influence, and the country's brand of communism, with its independence from the Soviet line, was called \"Titoism\" by Moscow and considered treasonous. Party purges against suspected \"Titoites\" were conducted throughout Eastern Europe.\n\nSignificant evidence supports the opinion that the actual reason for the Cominform Resolution was the unwillingness of Josip Broz Tito to obey the instructions of Joseph Stalin. The most serious disputes concerned policy in the Balkans. In particular, Yugoslavia was considered to be pushing too fast towards unification with Bulgaria and Albania. Although following Stalin's proposal for a series of such unifications, Tito was seen to be proceeding without proper consultation with Moscow. Another issue was Tito's eagerness to export revolution to Greece, in contravention of Stalin's Percentages Agreement with the capitalist powers.\n\nThe Cominform Resolution is seen as a failed attempt by Stalin to command obedience not only from Tito, but from other national Communist parties as well.\n\nIn his memoirs, Nikita Khrushchev asserted that he was \"absolutely sure that if the Soviet Union bordered Yugoslavia, Stalin would have intervened militarily.\" The Soviets planned an invasion with Hungarian, Romanian, and Soviet troops, and in January 1951 large military maneuvers in Hungary simulated an invasion with the assumption of NATO intervention on the Yugoslav side. The threat of war declined, however; Yugoslavia was important to the West because of its importance to the defense of Italy and Greece, and the United States' strong defense of South Korea in the Korean War had likely helped discourage the Soviets, Béla Király stating that it \"nipped Stalin's pet project in the bud\". The country became an informal NATO member; in February 1951 the British Chiefs of Staff announced that a Soviet attack of Yugoslavia \"would lead to world war\", in June Koča Popović visited Washington DC for joint planning discussions, and by the mid 1950s the United States provided the country with one half billion dollars in military aid.\n\nThis period was also marked by dissent within the League of Communists of Yugoslavia and subsequent repression and deportations of many pro-Soviet members to labor camps and prisons, notably Goli Otok island.\n\nKhrushchev reconciled with Tito after Stalin's 1953 death, but Yugoslavia remained outside the Eastern bloc and an informal NATO member. Tito dramatically changed his domestic policies and created an amnesty programme. Most of the prisons were closed and destroyed, and the government also loosened controls in the media to a much wider extent than in the rest of the Eastern bloc.\n\nThis period figures prominently in Yugoslav literature and cinema.\n\n\n\n\n\n\n\\\n"}
{"id": "57058149", "url": "https://en.wikipedia.org/wiki?curid=57058149", "title": "Institut Nova Història", "text": "Institut Nova Història\n\nThe Institut Nova Història (INH) is a Catalan cultural foundation with headquarters in Barcelona. Its members, of which the most prominent is the Catalan nationalist writer Jordi Bilbeny, hold that history has been systematically manipulated by the Spanish (or \"Castilian\") state since the 15th century to eliminate the Catalan contribution to world history. They claim that major historical figures, including Christopher Columbus, Erasmus, Miguel de Cervantes, William Shakespeare, Leonardo da Vinci, Saint Teresa of Ávila and others were Catalan. The institute's work has been criticised as pseudohistory.\n\nThe institute promotes research, study and dissemination of ideas about Catalan influence on history through publications, conferences and symposia, in particular the annual \"Symposium on the Catalan Discovery of America\" in Arenys de Munt. It is funded by the Catalan autonomous government, and has received support from Catalan nationalist politicians.\n\nThe INH was created in 2007 following a split in the Foundation of Historical Studies of Catalonia. The principal researcher of the INH is Jordi Bilbeny, a member of the \"Arenysian Movement for Self-Determination\", a local independentist movement, and of the Arenys de Munt's Popular Unity Candidacy, who launched the \"Symposium on the Catalan Discovery of America\" in 2001. From its inception, the INH has undertaken the study and dissemination of the Catalan background and/or culture of Christopher Columbus, Miguel de Cervantes, \"Lazarillo de Tormes\", all topics put forward by Bilbeny. The president of the Institute is Albert Codinas, vice president of the , secretary of the , and co-founder of the Platform for the Right to Decide. Víctor Cucurull, another researcher, is president of the , and a member of the national secretariat of the Catalan National Assembly\n\nThe INH has supported and organized, together with the municipality of Arenys de Munt, the \"Simposis sobre la descoberta catalana d'Amèrica\" (Symposium on the Catalan Discovery of America), held since 2001, where speakers present research on the Discovery of the Americas by the Catalans. In 2013, the Institute organized the first Universitat Nova Història in Pla de l'Estany, under the sponsorship of the Provincial Council of Girona, the municipality of Arenys de Mar, Cercle Català de Negocis (Catalan Business Circle), asamblea.cat, the Regional Council of Pla de l'Estany and Fundació Catalunya Estat.\n\nThe INH considers that the history of Catalonia has been manipulated and distorted since the end of the 15th century to favour the construct of a centralised Spanish (\"Castilian\") state. By minimising the role of Catalonia—or the Crown of Aragon—in Spanish collective history through falsification, concealment and censorship or \"appropriation\" of certain historical episodes, the prevalence of Castilian ideology is established. Among its claims are:\n\nThe INH is funded by the Catalan autonomous government. It has collaborated with the separatist Catalan National Assembly in organising conferences on the history of Catalonia. In 2013 it was one of the recipients of the (President Lluís Companys National Awards) by the Sants-Montjuïc branch of Esquerra Republicana de Catalunya. In 2012 the former president of the Generalitat de Catalunya, Jordi Pujol, wrote to Bilbeny congratulating him on his book \"Discovery and Catalan conquest of America. A history rewritten by the Castilians\", noting that the books published by the INH \"are very convincing\", while in 2014, the former vice-president of the Generalitat de Catalunya, Josep Lluís Carod-Rovira, participated at the reissue of Bilbeny's \"Brief account of the destruction of history\", where he spoke in praise of the book.\n\nThe 2015 INH symposium in Arenys de Munt, under the title \"The Catalan Discovery of America\", was financed by the councils of Arenys de Munt and Arenys de Mar, together with associations such as the of Arenys de Munt. Muriel Casals, representative of the separatist coalition Junts pel Sí, gave the closing speech entitled \"From the erased past to political independence\".\n\nINH members have given talks on their theories in the main Catalan government-owned television and radio channels such as TV3 and Catalunya Ràdio.\n\nThe INH has been described by one academic as \"a group of Catalan self-styled scholars trying to prove and promote the idea that, throughout History, a massive conspiracy by the Crown of Castille, the Inquisition, and any institution that may be related to Castille, has been orchestrated against the nation of Catalonia to deprive it of its History and cultural identity in order to promote Castile\". It was criticised in the Catalan parliament by the opposition party Ciudadanos spokeswoman, Carina Mejías, who described its theories as \"ridiculous\". Javier Barraycoa, vice-chancellor of the Abat Oliba CEU University, former secretary of the in Catalonia, and member of the anti-Catalanist organisation Somatemps, said that the INH \"has absolutely no recognition in the academic world\" and that its members \"have not been invited to any serious congress because it is impossible for them to defend these things.\"\n\n"}
{"id": "28341615", "url": "https://en.wikipedia.org/wiki?curid=28341615", "title": "International Summer University (ISU) Kassel", "text": "International Summer University (ISU) Kassel\n\nThe International Summer University is a short-term program that is held during the summer season by the University of Kassel every year. In addition to the international master programs, University Kassel organizes the ISU as one of its internationalization strategies.\n\nThe program of the ISU is designed for university students as well as researchers and professionals who want to learn more about German and European perspectives in Environmental Sciences and German Language & Culture. Environmental science has been one of the strengths of Uni Kassel since its foundation in 1971. During the winter season the University Kassel offers the International Winter University (IWU) Kassel. The theme “sustainability” does not only cover the academic content of the ISU/IWU, but also the program design which encourages continuous learning within and outside classrooms.\n\nThe International Summer University started as one of the Hessen International Summer Universities that have been designed to attract international students to Hessen. Established in 2001, ISU offers short-term credit-bearing academic program featuring the strengths of University Kassel in the areas of Environmental Sciences and German Language & Culture studies. Apart from the academic components with specific foci, ISU is unique in its host-family program for students to experience first-hand German lifestyle. The University’s long-standing partnerships with industries in the region and experiences in university-industry knowledge transfer also bring additional benefit to ISU students by giving them opportunities to witness the application of academic knowledge in industrial settings.\n\nSince its inception, ISU has brought some 50 international students to Kassel every year. Some of these students returned to Kassel for further studies and many of them have become the ambassadors of the University and the city of Kassel in their home countries.\n\nThe uniqueness of ISU lies in the concern for the sustainability of global environment and cultural heritage. While ISU Kassel embraces globalization and innovation in the study program, the best practices of the program have been kept – arrangement of host families – to ensure that the visiting students are fully engaged in the local community and culture during their short stay in Kassel. Students are arranged to live with carefully selected host families and to practice their classroom knowledge with tandem partners and tutors. Workshops (e.g. German dancing, cooking, sports), field trips and excursions in Kassel and neighboring areas are also organized to introduce local culture and traditions to the students. Since the program is held during the term time, ISU students also have abundant opportunities to meet local students and participate in student activities on campus.\n\nThe study program of ISU includes German language courses which are designed to meet the criteria of the Common European Framework of Reference for Languages (European Language Portfolio) at the following levels: A1, B1 and C1. Students will study in small groups under the supervision of experienced teachers qualified in \"'teaching German as a foreign language\"'. Interactive and innovative teaching methods are characteristics of this German language course.\n\nThe seminars for the engineering module usually cover the following topics: adaptation strategies to climate change, earthquake engineering, environmental engineering, and integrated environmental science environmental science. Seminars for the German culture module usually covers topics such as history of germany, German politics of the 20th century, intercultural communication, and German fairy tales. Students proficient in German language may choose the German-taught seminar of \"Deutsche Geschichte und Politik im 20. Jahrhundert\".\n\nStudents will be awarded 9 ECTS upon completion of the German language course and two academic seminars in either the engineering module or culture module. The seminars are taught by experienced faculty members in English and optionally in German. Students of the engineering module are arranged to visit industrial partners. Students from the cultural module are arranged to visit the museums and cultural establishments in Kassel.\n\nKassel is a mid-sized city located in northern Hessen in the center of Germany. The city has around 198,500 inhabitants and a total area of 107 km2. Kassel is known as the \"Capital of the German Fairy Tale Route\" for it is the place where Jacob Grimm and Wilhelm Grimm spent more than 30 years of their lives to collect and compile the Grimm's Fairy Tales (German: \"Grimms Märchen\") collection. Kassel is also the city of \"documenta\" where the world’s largest exhibition of contemporary art takes place every five years.\n\nIn 1971, Uni Kassel was founded in the city of Kassel as a reform university. The University is one of the first universities to have developed an international and inter-disciplinary perspective in research and learning. Being the youngest public university in the state of Hessen, it has currently a student population of over 25,000, which includes some 3,200 international students from over 120 countries.\n\n"}
{"id": "10049655", "url": "https://en.wikipedia.org/wiki?curid=10049655", "title": "List of Jurchen chieftains", "text": "List of Jurchen chieftains\n\nThe Jurchens were a Tungusic people who inhabited the region of Manchuria (present-day Northeast China) until the 17th century, when they adopted the name \"Manchu\".\n\n\nLocated on the banks of Hun River(渾江)\n\n\n\nSynonyms: Wu-liang-ha, Orankha, Oranke (兀良哈/乙良哈) according to Korean records, Orangai (瓦爾哈;オランカイ) according to Japanese records.<br>\nLocation: They settled south of the Suifen River (绥芬河 or 速平江), on the north-west of Hui-ning under the leadership of one of Ahacu (阿哈出)'s sons.\n\n\n\nLocated near the banks of Songhua River\n\n\nLocation: banks of Yehe River south of Changchun\n\nLocation: south of the Yehe Clan (east of Kaiyuan), the soutehrnmost among the Haixi Jurchens.\n\n\nLocation: Hulan River (north of Harbin)\n\n\n\n"}
{"id": "54405097", "url": "https://en.wikipedia.org/wiki?curid=54405097", "title": "List of Parthian and Sasanian inscriptions", "text": "List of Parthian and Sasanian inscriptions\n\nThis is a list of Parthian and Sasanian inscription, which include remaining official inscriptions on rocks, as well as minor ones written on bricks, metal, wood, hide, papyri, and gems. Their significance is in the areas of linguistics, history, and study of religion in Persia. Some of the inscriptions are lost and are known only through tradition.\n\nEarly royal Sasanian inscriptions were trilingual: Middle Persian (in Inscriptional Pahlavi), Parthian (in Inscriptional Parthian) and Greek. Since the rule of Narseh, Greek was omitted. Book Pahlavi script replaced Inscriptional Pahlavi in late Middle Persian inscriptions.\n\n"}
{"id": "21288006", "url": "https://en.wikipedia.org/wiki?curid=21288006", "title": "List of antiquarian societies", "text": "List of antiquarian societies\n\nA list of Antiquarian Societies.\n\nAn antiquarian society is a learned society or professional association for antiquarians, people who study history with particular attention to ancient artifacts, archaeological and historic sites, and/or historic archives and manuscripts.\n\n\n\n\n\n"}
{"id": "104985", "url": "https://en.wikipedia.org/wiki?curid=104985", "title": "List of events named massacres", "text": "List of events named massacres\n\nThe following is a list of events for which one of the commonly accepted names includes the word \"massacre.\"\n\n\"Massacre\" is defined in the \"Oxford English Dictionary\" as \"the indiscriminate and brutal slaughter of people or (less commonly) animals; carnage, butchery, slaughter in numbers\". It also states that the term is used \"in the names of certain massacres of history\".\nThe first recorded use in English of the word \"massacre\" in the name of an event is due to Christopher Marlow who in c. 1600 referred to what is now known as the St. Bartholomew's Day massacre as \"The massacre at Paris\"\n\nThe purpose of the list is to trace such of the term \"massacre\" specifically.\nThere are many alternative terms with similar connotations, such as butchery, carnage, bloodbath, mass killing, atrocity, etc. as well as euphemisms such as \"Vespers\", \"Blutgericht\" or \"attack\", \"incident\", \"tragedy\" (etc.), use of which are outside the scope of this list.\n\"Massacre\" is also used figuratively to describe dramatic events that did not involve any deaths, such as the \"Hilo massacre\" and the \"Saturday Night Massacre\"; this usage is also outside of the scope of this list.\n\n\n"}
{"id": "52917482", "url": "https://en.wikipedia.org/wiki?curid=52917482", "title": "List of international presidential trips made by Mário Soares", "text": "List of international presidential trips made by Mário Soares\n\nBelow is a list of international presidential trips made by Mário Soares as President of the Portuguese Republic.\n\n"}
{"id": "844257", "url": "https://en.wikipedia.org/wiki?curid=844257", "title": "List of rulers of Peshawar", "text": "List of rulers of Peshawar\n\nList of Afghan Rulers in present-day Afghanistan with capital at Peshawar \n"}
{"id": "829548", "url": "https://en.wikipedia.org/wiki?curid=829548", "title": "Lost history", "text": "Lost history\n\nMany significant locations, cultures/groups, and objects throughout history have been lost, inspiring archaeologists and treasure-hunters around the world to search for them. \nThe existence of some of these places or items, particularly those from ancient history, is legendary and remains in doubt. \nExcluded from these lists are lost works—literary, historical, or mythological writings which have no current complete copies but are referenced by their contemporaries.\n\n\n\n\n"}
{"id": "24241117", "url": "https://en.wikipedia.org/wiki?curid=24241117", "title": "Main street manager", "text": "Main street manager\n\nA main street manager is a United States professional who helps small cities and towns maintain and improve their main street typically through a government program or public-private partnership. Objectives may include economic, preservation, restoration, marketing, and relations between business, consumers and the government.\n\nThe primary objective is to develop and administer the main street program. The National Trust for Historic Preservation describes ten standards of performance for administering a main street program.\n\n\n"}
{"id": "620396", "url": "https://en.wikipedia.org/wiki?curid=620396", "title": "Origin of language", "text": "Origin of language\n\nThe evolutionary emergence of language in the human species has been a subject of speculation for several centuries. The topic is difficult to study because of the lack of direct evidence. Consequently, scholars wishing to study the origins of language must draw inferences from other kinds of evidence such as the fossil record, archaeological evidence, contemporary language diversity, studies of language acquisition, and comparisons between human language and systems of communication existing among animals (particularly other primates). Many argue that the origins of language probably relate closely to the origins of modern human behavior, but there is little agreement about the implications and directionality of this connection.\n\nThis shortage of empirical evidence has led many scholars to regard the entire topic as unsuitable for serious study. In 1866, the Linguistic Society of Paris banned any existing or future debates on the subject, a prohibition which remained influential across much of the western world until late in the twentieth century. Today, there are various hypotheses about how, why, when, and where language might have emerged. Despite this, there is scarcely more agreement today than a hundred years ago, when Charles Darwin's theory of evolution by natural selection provoked a rash of armchair speculation on the topic. Since the early 1990s, however, a number of linguists, archaeologists, psychologists, anthropologists, and others have attempted to address with new methods what some consider one of the hardest problems in science.\n\nOne can sub-divide approaches to the origin of language according to some underlying assumptions:\n\n\nNoam Chomsky, a prominent proponent of discontinuity theory, argues that a single chance mutation occurred in one individual in the order of 100,000 years ago, installing the language faculty (a component of the mid-brain) in \"perfect\" or \"near-perfect\" form. \nA majority of linguistic scholars hold continuity-based theories, but they vary in how they envision language development. Among those who see language as mostly innate, some—notably Steven Pinker—avoid speculating about specific precursors in nonhuman primates, stressing simply that the language faculty must have evolved in the usual gradual way. Others in this intellectual camp—notably Ib Ulbæk—hold that language evolved not from primate communication but from primate cognition, which is significantly more complex.\n\nThose who see language as a socially learned tool of communication, such as Michael Tomasello, see it developing from the cognitively controlled aspects of primate communication, these being mostly gestural as opposed to vocal. Where vocal precursors are concerned, many continuity theorists envisage language evolving from early human capacities for song.\n\nTranscending the continuity-versus-discontinuity divide, some scholars view the emergence of language as the consequence of some kind of social transformation that, by generating unprecedented levels of public trust, liberated a genetic potential for linguistic creativity that had previously lain dormant. \"Ritual/speech coevolution theory\" exemplifies this approach. Scholars in this intellectual camp point to the fact that even chimpanzees and bonobos have latent symbolic capacities that they rarely—if ever—use in the wild. Objecting to the sudden mutation idea, these authors argue that even if a chance mutation were to install a language organ in an evolving bipedal primate, it would be adaptively useless under all known primate social conditions. A very specific social structure—one capable of upholding unusually high levels of public accountability and trust—must have evolved before or concurrently with language to make reliance on \"cheap signals\" (words) an evolutionarily stable strategy.\n\nBecause the emergence of language lies so far back in human prehistory, the relevant developments have left no direct historical traces; neither can comparable processes be observed today. Despite this, the emergence of new sign languages in modern times—Nicaraguan Sign Language, for example—may potentially offer insights into the developmental stages and creative processes necessarily involved. Another approach inspects early human fossils, looking for traces of physical adaptation to language use. In some cases, when the DNA of extinct humans can be recovered, the presence or absence of genes considered to be language-relevant —FOXP2, for example—may prove informative. Another approach, this time archaeological, involves invoking symbolic behavior (such as repeated ritual activity) that may leave an archaeological trace—such as mining and modifying ochre pigments for body-painting—while developing theoretical arguments to justify inferences from symbolism in general to language in particular.\n\nThe time range for the evolution of language and/or its anatomical prerequisites extends, at least in principle, from the phylogenetic divergence of \"Homo\" (2.3 to 2.4 million years ago) from \"Pan\" (5 to 6 million years ago) to the emergence of full behavioral modernity some 50,000–150,000 years ago. Few dispute that \"Australopithecus\" probably lacked vocal communication significantly more sophisticated than that of great apes in general, but scholarly opinions vary as to the developments since the appearance of \"Homo\" some 2.5 million years ago. Some scholars assume the development of primitive language-like systems (\"proto-language\") as early as \"Homo habilis\", while others place the development of symbolic communication only with \"Homo erectus\" (1.8 million years ago) or with \"Homo heidelbergensis\" (0.6 million years ago) and the development of language proper with \"Homo sapiens,\" currently estimated at less than 200,000 years ago.\n\nUsing statistical methods to estimate the time required to achieve the current spread and diversity in modern languages, Johanna Nichols—a linguist at the University of California, Berkeley—argued in 1998 that vocal languages must have begun diversifying in our species at least 100,000 years ago. A further study by Q. D. Atkinson suggests that successive population bottlenecks occurred as our African ancestors migrated to other areas, leading to a decrease in genetic and phenotypic diversity. Atkinson argues that these bottlenecks also affected culture and language, suggesting that the further away a particular language is from Africa, the fewer phonemes it contains. By way of evidence, Atkinson claims that today's African languages tend to have relatively large numbers of phonemes, whereas languages from areas in Oceania (the last place to which humans migrated), have relatively few. Relying heavily on Atkinson's work, a subsequent study has explored the rate at which phonemes develop naturally, comparing this rate to some of Africa's oldest languages. The results suggest that language first evolved around 50,000–150,000 years ago, which is around the time when modern \"Homo sapiens\" evolved. Estimates of this kind are not universally accepted, but jointly considering genetic, archaeological, palaeontological and much other evidence indicates that language probably emerged somewhere in sub-Saharan Africa during the Middle Stone Age, roughly contemporaneous with the speciation of \"Homo sapiens.\"\n\n In 1861, historical linguist Max Müller published a list of speculative theories concerning the origins of spoken language:\n\n\nMost scholars today consider all such theories not so much wrong—they occasionally offer peripheral insights—as naïve and irrelevant. The problem with these theories is that they are so narrowly mechanistic. They assume that once our ancestors had stumbled upon the appropriate ingenious \"mechanism\" for linking sounds with meanings, language automatically evolved and changed.\n\nFrom the perspective of signalling theory, the main obstacle to the evolution of language-like communication in nature is not a mechanistic one. Rather, it is the fact that symbols—arbitrary associations of sounds or other perceptible forms with corresponding meanings—are unreliable and may well be false. As the saying goes, \"words are cheap\". The problem of reliability was not recognized at all by Darwin, Müller or the other early evolutionary theorists.\n\nAnimal vocal signals are, for the most part, intrinsically reliable. When a cat purrs, the signal constitutes direct evidence of the animal's contented state. We trust the signal, not because the cat is inclined to be honest, but because it just cannot fake that sound. Primate vocal calls may be slightly more manipulable, but they remain reliable for the same reason—because they are hard to fake. Primate social intelligence is \"Machiavellian\"—self-serving and unconstrained by moral scruples. Monkeys and apes often attempt to deceive each other, while at the same time remaining constantly on guard against falling victim to deception themselves. Paradoxically, it is theorized that primates' resistance to deception is what blocks the evolution of their signalling systems along language-like lines. Language is ruled out because the best way to guard against being deceived is to ignore all signals except those that are instantly verifiable. Words automatically fail this test.\n\nWords are easy to fake. Should they turn out to be lies, listeners will adapt by ignoring them in favor of hard-to-fake indices or cues. For language to work, then, listeners must be confident that those with whom they are on speaking terms are generally likely to be honest. A peculiar feature of language is \"displaced reference\", which means reference to topics outside the currently perceptible situation. This property prevents utterances from being corroborated in the immediate \"here\" and \"now\". For this reason, language presupposes relatively high levels of mutual trust in order to become established over time as an evolutionarily stable strategy. This stability is born of a longstanding mutual trust and is what grants language its authority. A theory of the origins of language must therefore explain why humans could begin trusting cheap signals in ways that other animals apparently cannot (see signalling theory).\n\nThe \"mother tongues\" hypothesis was proposed in 2004 as a possible solution to this problem. W. Tecumseh Fitch suggested that the Darwinian principle of 'kin selection'—the convergence of genetic interests between relatives—might be part of the answer. Fitch suggests that languages were originally 'mother tongues'. If language evolved initially for communication between mothers and their own biological offspring, extending later to include adult relatives as well, the interests of speakers and listeners would have tended to coincide. Fitch argues that shared genetic interests would have led to sufficient trust and cooperation for intrinsically unreliable signals—words—to become accepted as trustworthy and so begin evolving for the first time.\n\nCritics of this theory point out that kin selection is not unique to humans. Other primate mothers also share genes with their progeny, as do all other animals, so why is it only humans who speak? Furthermore, it is difficult to believe that early humans restricted linguistic communication to genetic kin: the incest taboo must have forced men and women to interact and communicate with more distant relatives. So even if we accept Fitch's initial premises, the extension of the posited 'mother tongue' networks from close relatives to more distant relatives remains unexplained. Fitch argues, however, that the extended period of physical immaturity of human infants and the postnatal growth of the human brain give the human-infant relationship a different and more extended period of intergenerational dependency than that found in any other species.\n\nIb Ulbæk invokes another standard Darwinian principle—'reciprocal altruism'—to explain the unusually high levels of intentional honesty necessary for language to evolve. 'Reciprocal altruism' can be expressed as the principle that \"if you scratch my back, I'll scratch yours\". In linguistic terms, it would mean that \"if you speak truthfully to me, I'll speak truthfully to you\". Ordinary Darwinian reciprocal altruism, Ulbæk points out, is a relationship established between frequently interacting individuals. For language to prevail across an entire community, however, the necessary reciprocity would have needed to be enforced universally instead of being left to individual choice. Ulbæk concludes that for language to evolve, society as a whole must have been subject to moral regulation.\n\nCritics point out that this theory fails to explain when, how, why or by whom 'obligatory reciprocal altruism' could possibly have been enforced. Various proposals have been offered to remedy this defect. A further criticism is that language doesn't work on the basis of reciprocal altruism anyway. Humans in conversational groups don't withhold information to all except listeners likely to offer valuable information in return. On the contrary, they seem to want to advertise to the world their access to socially relevant information, broadcasting that information without expectation of reciprocity to anyone who will listen.\n\nGossip, according to Robin Dunbar in his book \"Grooming, Gossip and the Evolution of Language\", does for group-living humans what manual grooming does for other primates—it allows individuals to service their relationships and so maintain their alliances on the basis of the principle: \"if you scratch my back, I'll scratch yours\". Dunbar argues that as humans began living in increasingly larger social groups, the task of manually grooming all one's friends and acquaintances became so time-consuming as to be unaffordable. In response to this problem, humans developed 'a cheap and ultra-efficient form of grooming'—\"vocal grooming\". To keep allies happy, one now needs only to 'groom' them with low-cost vocal sounds, servicing multiple allies simultaneously while keeping both hands free for other tasks. Vocal grooming then evolved gradually into vocal language—initially in the form of 'gossip'. Dunbar's hypothesis seems to be supported by the fact that the structure of language shows adaptations to the function of narration in general.\n\nCritics of this theory point out that the very efficiency of 'vocal grooming'—the fact that words are so cheap—would have undermined its capacity to signal commitment of the kind conveyed by time-consuming and costly manual grooming. A further criticism is that the theory does nothing to explain the crucial transition from vocal grooming—the production of pleasing but meaningless sounds—to the cognitive complexities of syntactical speech.\n\nThe ritual/speech coevolution theory was originally proposed by social anthropologist Roy Rappaport before being elaborated by anthropologists such as Chris Knight, Jerome Lewis, Nick Enfield, Camilla Power and Ian Watts. Cognitive scientist and robotics engineer Luc Steels is another prominent supporter of this general approach, as is biological anthropologist/neuroscientist Terrence Deacon.\n\nThese scholars argue that there can be no such thing as a 'theory of the origins of language'. This is because language is not a separate adaptation but an internal aspect of something much wider—namely, human symbolic culture as a whole. Attempts to explain language independently of this wider context have spectacularly failed, say these scientists, because they are addressing a problem with no solution. Can we imagine a historian attempting to explain the emergence of credit cards independently of the wider system of which they are a part? Using a credit card makes sense only if you have a bank account institutionally recognized within a certain kind of advanced capitalist society—one where electronic communications technology and digital computers have already been invented and fraud can be detected and prevented. In much the same way, language would not work outside a specific array of social mechanisms and institutions. For example, it would not work for a nonhuman ape communicating with others in the wild. Not even the cleverest nonhuman ape could make language work under such conditions.\n\nLanguage consists of digital contrasts whose cost is essentially zero. As pure social conventions, signals of this kind cannot evolve in a Darwinian social world — they are a theoretical impossibility. Being intrinsically unreliable, language works only if you can build up a reputation for trustworthiness within a certain kind of society—namely, one where symbolic cultural facts (sometimes called 'institutional facts') can be established and maintained through collective social endorsement. In any hunter-gatherer society, the basic mechanism for establishing trust in symbolic cultural facts is collective \"ritual\". Therefore, the task facing researchers into the origins of language is more multidisciplinary than is usually supposed. It involves addressing the evolutionary emergence of human symbolic culture as a whole, with language an important but subsidiary component.\n\nCritics of the theory include Noam Chomsky, who terms it the 'non-existence' hypothesis—a denial of the very existence of language as an object of study for natural science. Chomsky's own theory is that language emerged in an instant and in perfect form, prompting his critics in turn to retort that only something that does not exist—a theoretical construct or convenient scientific fiction—could possibly emerge in such a miraculous way. The controversy remains unresolved.\n\nWhile it is possible to imitate the making of tools like those made by early Homo under circumstances of demonstration, research on primate tool cultures show that non-verbal cultures are vulnerable to environmental change. In particular, if the environment in which a skill can be used disappears for a longer period of time than an individual ape's or early human's lifespan, the skill will be lost if the culture is imitative and non-verbal. Chimpanzees, macaques and capuchin monkeys are all known to lose tool techniques under such circumstances. Researchers on primate culture vulnerability therefore argue that since early Homo species as far back as Homo habilis retained their tool cultures despite many climate change cycles at the timescales of centuries to millennia each, these species had sufficiently developed language abilities to verbally describe complete procedures, and therefore grammar and not only two-word \"proto-language\".\n\nThe theory that early Homo species had sufficiently developed brains for grammar is also supported by researchers who study brain development in children, noting that grammar is developed while connections across the brain are still significantly lower than adult level. These researchers argue that these lowered system requirements for grammatical language make it plausible that the genus Homo had grammar at connection levels in the brain that were significantly lower than those of Homo sapiens and that more recent steps in the evolution of the human brain were not about language.\n\nStructural linguistics is an approach to the study of language founded by Ferdinand de Saussure and popularised through his posthumously published book \"Course in General Linguistics\" (1916).\n\nAccording to structural anthropologist Claude Lévi-Strauss, language and meaning—in opposition to \"knowledge, which develops slowly and progressively\"—must have appeared in an instant:\n\nWhatever may have been the moment and the circumstances of its appearance in the ascent of animal life, language can only have arisen all at once. Things cannot have begun to signify gradually. In the wake of a transformation which is not a subject of study for the social sciences, but for biology and psychology, a shift occurred from a stage when nothing had a meaning to another stage when everything had meaning.\n\nLévi-Strauss's hypothesis is a necessary consequence of Saussure's view of language as a formal system of differential elements: if a signifier only acquires meaning through its difference from other signifiers, then a hypothetical \"first signifier\" always already implies another signifier (\"everything else\" or \"the rest of the universe\") from which the first signifier is different. Thus, language, according to structuralism, must have appeared all at once and not gradually since a semi-language is impossible.\n\nAccording to Chomsky's single mutation theory, the emergence of language resembled the formation of a crystal; with digital infinity as the seed crystal in a super-saturated primate brain, on the verge of blossoming into the human mind, by physical law, once evolution added a single small but crucial keystone. Whilst some suggest it follows from this theory that language appeared rather suddenly within the history of human evolution, Chomsky, writing with computational linguist and computer scientist Robert C. Berwick, suggests it is completely compatible with modern biology. They note \"none of the recent accounts of human language evolution seem to have completely grasped the shift from conventional Darwinism to its fully stochastic modern version—specifically, that there are stochastic effects not only due to sampling like directionless drift, but also due to directed stochastic variation in fitness, migration, and heritability—indeed, all the \"forces\" that affect individual or gene frequencies. ... All this can affect evolutionary outcomes—outcomes that as far as we can make out are not brought out in recent books on the evolution of language, yet would arise immediately in the case of any new genetic or individual innovation, precisely the kind of scenario likely to be in play when talking about language's emergence.\"\n\nCiting evolutionary geneticist Svante Pääbo they concur that a substantial difference must have occurred to differentiate \"Homo sapiens\" from Neanderthals to \"prompt the relentless spread of our species who had never crossed open water up and out of Africa and then on across the entire planet in just a few tens of thousands of years. ... What we do not see is any kind of \"gradualism\" in new tool technologies or innovations like fire, shelters, or figurative art.\" Berwick and Chomsky therefore suggest language emerged approximately between 200,000 years ago and 60,000 years ago (between the arrival of the first anatomically modern humans in southern Africa, and the last exodus from Africa, respectively). \"That leaves us with about 130,000 years, or approximately 5,000–6,000 generations of time for evolutionary change. This is not 'overnight in one generation' as some have (incorrectly) inferred—but neither is it on the scale of geological eons. It's time enough—within the ballpark for what Nilsson and Pelger (1994) estimated as the time required for the full evolution of a vertebrate eye from a single cell, even without the invocation of any 'evo-devo' effects.\"\n\nThe gestural theory states that human language developed from gestures that were used for simple communication.\n\nTwo types of evidence support this theory.\n\nResearch has found strong support for the idea that verbal language and sign language depend on similar neural structures. Patients who used sign language, and who suffered from a left-hemisphere lesion, showed the same disorders with their sign language as vocal patients did with their oral language. Other researchers found that the same left-hemisphere brain regions were active during sign language as during the use of vocal or written language.\n\nPrimate gesture is at least partially genetic: different nonhuman apes will perform gestures characteristic of their species, even if they have never seen another ape perform that gesture. For example, gorillas beat their breasts. This shows that gestures are an intrinsic and important part of primate communication, which supports the idea that language evolved from gesture.\n\nFurther evidence suggests that gesture and language are linked. In humans, manually gesturing has an effect on concurrent vocalizations, thus creating certain natural vocal associations of manual efforts. Chimpanzees move their mouths when performing fine motor tasks. These mechanisms may have played an evolutionary role in enabling the development of intentional vocal communication as a supplement to gestural communication. Voice modulation could have been prompted by preexisting manual actions.\n\nThere is also the fact that, from infancy, gestures both supplement and predict speech. This addresses the idea that gestures quickly change in humans from a sole means of communication (from a very young age) to a supplemental and predictive behavior that we use despite being able to communicate verbally. This too serves as a parallel to the idea that gestures developed first and language subsequently built upon it.\n\nTwo possible scenarios have been proposed for the development of language, one of which supports the gestural theory:\nThe first perspective that language evolved from the calls of our ancestors seems logical because both humans and animals make sounds or cries. One evolutionary reason to refute this is that, anatomically, the center that controls calls in monkeys and other animals is located in a completely different part of the brain than in humans. In monkeys, this center is located in the depths of the brain related to emotions. In the human system, it is located in an area unrelated to emotion. Humans can communicate simply to communicate—without emotions. So, anatomically, this scenario does not work. Therefore, we resort to the idea that language was derived from gesture (we communicated by gesture first and sound was attached later).\n\nThe important question for gestural theories is why there was a shift to vocalization. Various explanations have been proposed: \nA comparable hypothesis states that in 'articulate' language, gesture and vocalisation are intrinsically linked, as language evolved from equally intrinsically linked dance and song.\nHumans still use manual and facial gestures when they speak, especially when people meet who have no language in common. There are also, of course, a great number of sign languages still in existence, commonly associated with deaf communities. These sign languages are equal in complexity, sophistication, and expressive power, to any oral language. The cognitive functions are similar and the parts of the brain used are similar. The main difference is that the \"phonemes\" are produced on the outside of the body, articulated with hands, body, and facial expression, rather than inside the body articulated with tongue, teeth, lips, and breathing. (Compare the motor theory of speech perception.)\n\nCritics of gestural theory note that it is difficult to name serious reasons why the initial pitch-based vocal communication (which is present in primates) would be abandoned in favor of the much less effective non-vocal, gestural communication. However, Michael Corballis has pointed out that it is supposed that primate vocal communication (such as alarm calls) cannot be controlled consciously, unlike hand movement, and thus is not credible as precursor to human language; primate vocalization is rather homologous to and continued in involuntary reflexes (connected with basic human emotions) such as screams or laughter (the fact that these can be faked does not disprove the fact that genuine involuntary responses to fear or surprise exist). Also, gesture is not generally less effective, and depending on the situation can even be advantageous, for example in a loud environment or where it is important to be silent, such as on a hunt. Other challenges to the \"gesture-first\" theory have been presented by researchers in psycholinguistics, including David McNeill.\n\nProponents of the motor theory of language evolution have primarily focused on the visual domain and communication through observation of movements. The \"Tool-use sound hypothesis\" suggests that the production and perception of sound also contributed substantially, particularly \"incidental sound of locomotion\" (\"ISOL\") and \"tool-use sound\" (\"TUS\"). Human bipedalism resulted in rhythmic and more predictable \"ISOL\". That may have stimulated the evolution of musical abilities, auditory working memory, and abilities to produce complex vocalizations, and to mimic natural sounds. Since the human brain proficiently extracts information about objects and events from the sounds they produce, \"TUS\", and mimicry of \"TUS\", might have achieved an iconic function. The prevalence of sound symbolism in many extant languages supports this idea. Self-produced TUS activates multimodal brain processing (motor neurons, hearing, proprioception, touch, vision), and \"TUS\" stimulates primate audiovisual mirror neurons, which is likely to stimulate the development of association chains. Tool use and auditory gestures involve motor-processing of the forelimbs, which is associated with the evolution of vertebrate vocal communication. The production, perception, and mimicry of \"TUS\" may have resulted in a limited number of vocalizations or protowords that were associated with tool use. A new way to communicate about tools, especially when out of sight, would have had selective advantage. A gradual change in acoustic properties and/or meaning could have resulted in arbitrariness and an expanded repertoire of words. Humans have been increasingly exposed to \"TUS\" over millions of years, coinciding with the period during which spoken language evolved.\n\nIn humans, functional MRI studies have reported finding areas homologous to the monkey mirror neuron system in the inferior frontal cortex, close to Broca's area, one of the language regions of the brain. This has led to suggestions that human language evolved from a gesture performance/understanding system implemented in mirror neurons. Mirror neurons have been said to have the potential to provide a mechanism for action-understanding, imitation-learning, and the simulation of other people's behavior. This hypothesis is supported by some cytoarchitectonic homologies between monkey premotor area F5 and human Broca's area. Rates of vocabulary expansion link to the ability of children to vocally mirror non-words and so to acquire the new word pronunciations. Such speech repetition occurs automatically, quickly and separately in the brain to speech perception. Moreover, such vocal imitation can occur without comprehension such as in speech shadowing and echolalia. Further evidence for this link comes from a recent study in which the brain activity of two participants was measured using fMRI while they were gesturing words to each other using hand gestures with a game of charades—a modality that some have suggested might represent the evolutionary precursor of human language. Analysis of the data using Granger Causality revealed that the mirror-neuron system of the observer indeed reflects the pattern of activity of in the motor system of the sender, supporting the idea that the motor concept associated with the words is indeed transmitted from one brain to another using the mirror system.\n\nNot all linguists agree with the above arguments, however. In particular, supporters of Noam Chomsky argue against the possibility that the mirror neuron system can play any role in the hierarchical recursive structures essential to syntax.\n\nAccording to Dean Falk's 'putting the baby down' theory, vocal interactions between early hominid mothers and infants sparked a sequence of events that led, eventually, to our ancestors' earliest words. The basic idea is that evolving human mothers, unlike their counterparts in other primates, couldn't move around and forage with their infants clinging onto their backs. Loss of fur in the human case left infants with no means of clinging on. Frequently, therefore, mothers had to put their babies down. As a result, these babies needed to be reassured that they were not being abandoned. Mothers responded by developing 'motherese'—an infant-directed communicative system embracing facial expressions, body language, touching, patting, caressing, laughter, tickling and emotionally expressive contact calls. The argument is that language somehow developed out of all this.\n\nIn \"The Mental and Social Life of Babies\", psychologist Kenneth Kaye noted that no usable adult language could have evolved without interactive communication between very young children and adults. \"No symbolic system could have survived from one generation to the next if it could not have been easily acquired by young children under their normal conditions of social life.\"\n\nThe ‘from where to what’ model is a language evolution model that is derived primarily from the organization of language processing in the brain. According to recent models, language is processed in 2 separate pathways: the auditory ventral stream (red arrows in figure) and auditory dorsal stream (blue arrows in figure). The auditory ventral stream passes from the anterior auditory cortex to the temporal pole and from there to the inferior frontal gyrus (Broca’s area). The role of this pathway is established with sound recognition in both humans and other primates (and is accordingly known as the ‘auditory what pathway’). The auditory dorsal stream passes from the posterior auditory cortex to the inferior parietal lobule and from there to the inferior frontal gyrus. This pathway was traditionally called the Wernicke-Broca pathway. Whereas in non-human primates this pathway is responsible only for sound localization (and is known as the auditory ‘where’ pathway’), in humans it has several additional functions, such as: speech repetition and production, audio-visual integration (Such as during lip-reading), perception and production of intonations, phonological long-term memory (long-term memory storage for the names of words) and phonological working memory (temporary storage of the names of words). Some evidence also indicates a role in discrimination of individuals by their voices. In accordance with the ‘from where to what’ model, each of these functions of the auditory dorsal stream indicates of an intermediate stage in the evolution of language. Supporting the hypothesis that developments of the auditory dorsal stream resulted with the emergence of language is a diffusion imaging study that compared macaque monkeys, chimpanzees and humans and reported of strengthening of the auditory dorsal stream from monkeys to chimpanzees and from chimpanzees to humans, whereas little changes are noted for the auditory ventral stream between the three species. A study that compared the skulls of early members of the species homo (Homo habilis) to their predecessors (Australopithecus) and to contemporary apes reported only of increase in size of the auditory dorsal stream (inferior parietal lobule and inferior frontal gyrus).\n\nThe ‘from where to what’ model argues that the co-localization of sound localization, audio-visual integration and discrimination of voices is evidence that speech originated for the purpose of exchanging contact calls (stage 1 in figure). These calls are used by con-specifics in order to announce to the troop of their presence or location. In this scenario, speech could have began when a mother and offspring accidentally separated in the forest, and the two took turns emitting contact calls until they were re-united. This model is consistent with studies that induced mutations in mice to genes that cause speech dyspraxia in humans (FOXP2, SRPX2) and resulted with infant mice no longer emitting contact calls with their mothers. Like human speech, the detection and emission of contact call in monkeys is also left hemisphere dominant and lesion to the left, but not right, hemisphere interfered with the discrimination of contact calls. Further demonstrating the role of the auditory dorsal stream are studies of marmoset monkeys that sacrificed monkeys after hearing a contact call, emitting a contact call or both. The researchers reported of significant increase in the number of active neurons in the auditory dorsal stream when the monkeys heard and then responded to contact calls. Studies that recorded activity from the inferior frontal gyrus of monkeys also reported of activity immediately prior to emitting a contact call. \n\nContact calls are similar to human speech as they are both vocal and practice turn taking. However, the contact calls of monkeys and apes are mostly rigid in their structure (although Masataka reported of infant macaque monkeys learning to modify the pitch of their contact call to imitate mothers; and Hihara reported of macaque monkeys learning to use contact call to request specific objects). A possible transition from contact call to speech is that due to a mutation in the auditory dorsal stream, early hominans (i.e., Homo habilis) became capable of modifying the contact calls with intonations (stage 2 in figure). This enabled infants to emit two types of contact calls, one that signals the mother the infant is safe but desires her attention and another that signals immediate danger. Such a development would have provided the mother with an advantage as it informs her whether she needs to drop everything she carries and rush to the baby, or she can take her time. As generations passed and vocal control improved, mothers and offspring became capable of communicating with very simple yes-no question answer conversations (stage 3 in figure). In such a scenario, a baby is emitting a low-level distress call to signal desire to eat berries, and the mother responding with a low-level distress call to permit the activity or high-level distress call to prohibit it. This use of intonations in order to emit two types of calls, one for low level of distress and another for high level of distress could explain why humans today use intonations to transform words into questions or commands (for example, converting the word ‘food’ into a question or command with different intonations). Consistent with this model, a study that examined patients with deterioration of the auditory dorsal stream (phonological dementia) reported of impaired ability to discriminate questions from statements using intonations. Similarly, a study that used diffused weighted imaging to identify the auditory ventral and dorsal streams in healthy people reported that interfering with the auditory dorsal stream using TMS resulted with individuals failing to discriminate questions from statements using intonations. \n\nIn a follow up article, based on the roles of the auditory dorsal stream in vocal mimicry, lip-reading and phonological working memory and long-term memory, a few additional intermediate stages in the evolution of language are proposed. In accordance with the model, as generations passed, vocal control increased from just modifying calls with intonations to emitting novel calls (stage 4 in figure). During this time, infants learned these call by imitating their parents’ lip-movements (stage 5 in figure). This hypothesis explains the role of the auditory dorsal stream in many studies of lip-speech integration as occurs in lip-reading. A role for reading lips as a mean for the acquisition of novel calls can also explain the distinctive large human lips, which are not found with other apes. Supporting the model are studies of patients with damage to the auditory dorsal stream that were reported with impaired ability to repeat sentences and nonsense words (conduction aphasia), or impaired ability to name objects (anomia).\n\nLearning calls by offspring imitating the lip-movements of their parents eventually reached a maximum limit of efficiency. At this point, the offspring were able to learn new vocalizations soon after hearing them, and the memory lasted for their entire life (stage 6 in figure). Such a transition could explain the tendency of babies, at their first year, to mimic their parents by imitating their lip-movements and that after the first year this mimicry process ends. New calls, such as foreign phonemes, learned after the first few years are also much more challenging to learn. The development of a memory store for vocalizations in the parietal lobe of the auditory dorsal stream could have provided the neurological infra-structure for rehearsing lists of calls, thus resulting with poly-syllabic words (stage 7 in figure). Eventually the rehearsal of lists of syllables enabled the rehearsal of lists of words, which made the communication with sentences possible. \n\n'Grammaticalisation' is a continuous historical process in which free-standing words develop into grammatical appendages, while these in turn become ever more specialized and grammatical. An initially 'incorrect' usage, in becoming accepted, leads to unforeseen consequences, triggering knock-on effects and extended sequences of change. Paradoxically, grammar evolves because, in the final analysis, humans care less about grammatical niceties than about making themselves understood. If this is how grammar evolves today, according to this school of thought, we can legitimately infer similar principles at work among our distant ancestors, when grammar itself was first being established.\n\nIn order to reconstruct the evolutionary transition from early language to languages with complex grammars, we need to know which hypothetical sequences are plausible and which are not. In order to convey abstract ideas, the first recourse of speakers is to fall back on immediately recognizable concrete imagery, very often deploying metaphors rooted in shared bodily experience. A familiar example is the use of concrete terms such as 'belly' or 'back' to convey abstract meanings such as 'inside' or 'behind'. Equally metaphorical is the strategy of representing temporal patterns on the model of spatial ones. For example, English speakers might say 'It is going to rain,' modeled on 'I am going to London.' This can be abbreviated colloquially to 'It's gonna rain.' Even when in a hurry, we don't say 'I'm gonna London'—the contraction is restricted to the job of specifying tense. From such examples we can see why grammaticalization is consistently unidirectional—from concrete to abstract meaning, not the other way around.\n\nGrammaticalization theorists picture early language as simple, perhaps consisting only of nouns. Even under that extreme theoretical assumption, however, it is difficult to imagine what would realistically have prevented people from using, say, 'spear' as if it were a verb ('Spear that pig!'). People might have used their nouns as verbs or their verbs as nouns as occasion demanded. In short, while a noun-only language might seem theoretically possible, grammaticalization theory indicates that it cannot have remained fixed in that state for any length of time.\n\nCreativity drives grammatical change. This presupposes a certain attitude on the part of listeners. Instead of punishing deviations from accepted usage, listeners must prioritize imaginative mind-reading. Imaginative creativity—emitting a leopard alarm when no leopard was present, for example—is not the kind of behavior which, say, vervet monkeys would appreciate or reward. Creativity and reliability are incompatible demands; for 'Machiavellian' primates as for animals generally, the overriding pressure is to demonstrate reliability. If humans escape these constraints, it is because in our case, listeners are primarily interested in mental states.\n\nTo focus on mental states is to accept fictions—inhabitants of the imagination—as potentially informative and interesting. Take the use of metaphor. A metaphor is, literally, a false statement. Think of Romeo's declaration, 'Juliet is the sun!' Juliet is a woman, not a ball of plasma in the sky, but human listeners are not (or not usually) pedants insistent on point-by-point factual accuracy. They want to know what the speaker has in mind. Grammaticalization is essentially based on metaphor. To outlaw its use would be to stop grammar from evolving and, by the same token, to exclude all possibility of expressing abstract thought.\n\nA criticism of all this is that while grammaticalization theory might explain language change today, it does not satisfactorily address the really difficult challenge—explaining the initial transition from primate-style communication to language as we know it. Rather, the theory assumes that language already exists. As Bernd Heine and Tania Kuteva acknowledge: \"Grammaticalization requires a linguistic system that is used regularly and frequently within a community of speakers and is passed on from one group of speakers to another\". Outside modern humans, such conditions do not prevail.\n\nHuman language is used for self-expression; however, expression displays different stages. The consciousness of self and feelings represents the stage immediately prior to the external, phonetic expression of feelings in the form of sound, i.e., language. Intelligent animals such as dolphins, Eurasian magpies, and chimpanzees live in communities, wherein they assign themselves roles for group survival and show emotions such as sympathy. When such animals view their reflection (mirror test), they recognize themselves and exhibit self-consciousness. Notably, humans evolved in a quite different environment than that of these animals. Human survival became easier with the development of tools, shelter, and fire, thus facilitating further advancement of interaction, self-expression, and tool-making. The increasing brain size allowed advanced provisioning and tools and the technological advances during the Palaeolithic era that built upon the previous evolutionary innovations of bipedalism and hand versatility allowed the development of human language.\n\nAccording to a study investigating the song differences between white-rumped munias and its domesticated counterpart (Bengalese finch), the wild munias use a highly stereotyped song sequence, whereas the domesticated ones sing a highly unconstrained song. In wild finches, song syntax is subject to female preference—sexual selection—and remains relatively fixed. However, in the Bengalese finch, natural selection is replaced by breeding, in this case for colorful plumage, and thus, decoupled from selective pressures, stereotyped song syntax is allowed to drift. It is replaced, supposedly within 1000 generations, by a variable and learned sequence. Wild finches, moreover, are thought incapable of learning song sequences from other finches. In the field of bird vocalization, brains capable of producing only an innate song have very simple neural pathways: the primary forebrain motor center, called the robust nucleus of arcopallium, connects to midbrain vocal outputs, which in turn project to brainstem motor nuclei. By contrast, in brains capable of learning songs, the arcopallium receives input from numerous additional forebrain regions, including those involved in learning and social experience. Control over song generation has become less constrained, more distributed, and more flexible.\n\nOne way to think about human evolution is that we are self-domesticated apes. Just as domestication relaxed selection for stereotypic songs in the finches—mate choice was supplanted by choices made by the aesthetic sensibilities of bird breeders and their customers—so might our cultural domestication have relaxed selection on many of our primate behavioral traits, allowing old pathways to degenerate and reconfigure. Given the highly indeterminate way that mammalian brains develop—they basically construct themselves \"bottom up\", with one set of neuronal interactions setting the stage for the next round of interactions—degraded pathways would tend to seek out and find new opportunities for synaptic hookups. Such inherited de-differentiations of brain pathways might have contributed to the functional complexity that characterizes human language. And, as exemplified by the finches, such de-differentiations can occur in very rapid time-frames.\n\nA distinction can be drawn between speech and language. Language is not necessarily spoken: it might alternatively be written or signed. Speech is among a number of different methods of encoding and transmitting linguistic information, albeit arguably the most natural one.\n\nSome scholars view language as an initially cognitive development, its 'externalisation' to serve communicative purposes occurring later in human evolution. According to one such school of thought, the key feature distinguishing human language is recursion, (in this context, the iterative embedding of phrases within phrases). Other scholars—notably Daniel Everett—deny that recursion is universal, citing certain languages (e.g. Pirahã) which allegedly lack this feature.\n\nThe ability to ask questions is considered by some to distinguish language from non-human systems of communication. Some captive primates (notably bonobos and chimpanzees), having learned to use rudimentary signing to communicate with their human trainers, proved able to respond correctly to complex questions and requests. Yet they failed to ask even the simplest questions themselves. Conversely, human children are able to ask their first questions (using only question intonation) at the babbling period of their development, long before they start using syntactic structures. Although babies from different cultures acquire native languages from their social environment, all languages of the world without exception—tonal, non-tonal, intonational and accented—use similar rising \"question intonation\" for yes–no questions. This fact is a strong evidence of the universality of question intonation. In general, according to some authors, sentence intonation/pitch is pivotal in spoken grammar and is the basic information used by children to learn the grammar of whatever language.\n\nOne of the intriguing abilities that language users have is that of high-level reference (or deixis), the ability to refer to things or states of being that are not in the immediate realm of the speaker. This ability is often related to theory of mind, or an awareness of the other as a being like the self with individual wants and intentions. According to Chomsky, Hauser and Fitch (2002), there are six main aspects of this high-level reference system:\n\n\nSimon Baron-Cohen (1999) argues that theory of mind must have preceded language use, based on evidence of use of the following characteristics as much as 40,000 years ago: intentional communication, repairing failed communication, teaching, intentional persuasion, intentional deception, building shared plans and goals, intentional sharing of focus or topic, and pretending. Moreover, Baron-Cohen argues that many primates show some, but not all, of these abilities. Call and Tomasello's research on chimpanzees supports this, in that individual chimps seem to understand that other chimps have awareness, knowledge, and intention, but do not seem to understand false beliefs. Many primates show some tendencies toward a theory of mind, but not a full one as humans have.\nUltimately, there is some consensus within the field that a theory of mind is necessary for language use. Thus, the development of a full theory of mind in humans was a necessary precursor to full language use.\n\nIn one particular study, rats and pigeons were required to press a button a certain number of times to get food. The animals showed very accurate distinction for numbers less than four, but as the numbers increased, the error rate increased. Matsuzawa (1985) attempted to teach chimpanzees Arabic numerals. The difference between primates and humans in this regard was very large, as it took the chimps thousands of trials to learn 1–9 with each number requiring a similar amount of training time; yet, after learning the meaning of 1, 2 and 3 (and sometimes 4), children easily comprehend the value of greater integers by using a successor function (i.e. 2 is 1 greater than 1, 3 is 1 greater than 2, 4 is 1 greater than 3; once 4 is reached it seems most children have an \"a-ha!\" moment and understand that the value of any integer \"n\" is 1 greater than the previous integer). Put simply, other primates learn the meaning of numbers one by one, similar to their approach to other referential symbols, while children first learn an arbitrary list of symbols (1, 2, 3, 4...) and then later learn their precise meanings. These results can be seen as evidence for the application of the \"open-ended generative property\" of language in human numeral cognition.\n\nHockett (1966) details a list of features regarded as essential to describing human language. In the domain of the lexical-phonological principle, two features of this list are most important:\n\n\nThe sound system of a language is composed of a finite set of simple phonological items. Under the specific phonotactic rules of a given language, these items can be recombined and concatenated, giving rise to morphology and the open-ended lexicon. A key feature of language is that a simple, finite set of phonological items gives rise to an infinite lexical system wherein rules determine the form of each item, and meaning is inextricably linked with form. Phonological syntax, then, is a simple combination of pre-existing phonological units. Related to this is another essential feature of human language: lexical syntax, wherein pre-existing units are combined, giving rise to semantically novel or distinct lexical items.\n\nCertain elements of the lexical-phonological principle are known to exist outside of humans. While all (or nearly all) have been documented in some form in the natural world, very few coexist within the same species. Bird-song, singing nonhuman apes, and the songs of whales all display phonological syntax, combining units of sound into larger structures apparently devoid of enhanced or novel meaning. Certain other primate species do have simple phonological systems with units referring to entities in the world. However, in contrast to human systems, the units in these primates' systems normally occur in isolation, betraying a lack of lexical syntax. There is new evidence to suggest that Campbell's monkeys also display lexical syntax, combining two calls (a predator alarm call with a \"boom\", the combination of which denotes a lessened threat of danger), however it is still unclear whether this is a lexical or a morphological phenomenon.\n\nPidgins are significantly simplified languages with only rudimentary grammar and a restricted vocabulary. In their early stage pidgins mainly consist of nouns, verbs, and adjectives with few or no articles, prepositions, conjunctions or auxiliary verbs. Often the grammar has no fixed word order and the words have no inflection.\n\nIf contact is maintained between the groups speaking the pidgin for long periods of time, the pidgins may become more complex over many generations. If the children of one generation adopt the pidgin as their native language it develops into a creole language, which becomes fixed and acquires a more complex grammar, with fixed phonology, syntax, morphology, and syntactic embedding. The syntax and morphology of such languages may often have local innovations not obviously derived from any of the parent languages.\n\nStudies of creole languages around the world have suggested that they display remarkable similarities in grammar and are developed uniformly from pidgins in a single generation. These similarities are apparent even when creoles do not share any common language origins. In addition, creoles share similarities despite being developed in isolation from each other. Syntactic similarities include subject–verb–object word order. Even when creoles are derived from languages with a different word order they often develop the SVO word order. Creoles tend to have similar usage patterns for definite and indefinite articles, and similar movement rules for phrase structures even when the parent languages do not.\n\nField primatologists can give us useful insights into great ape communication in the wild. An important finding is that nonhuman primates, including the other great apes, produce calls that are graded, as opposed to categorically differentiated, with listeners striving to evaluate subtle gradations in signalers' emotional and bodily states. Nonhuman apes seemingly find it extremely difficult to produce vocalizations in the absence of the corresponding emotional states. In captivity, nonhuman apes have been taught rudimentary forms of sign language or have been persuaded to use lexigrams—symbols that do not graphically resemble the corresponding words—on computer keyboards. Some nonhuman apes, such as Kanzi, have been able to learn and use hundreds of lexigrams.\n\nThe Broca's and Wernicke's areas in the primate brain are responsible for controlling the muscles of the face, tongue, mouth, and larynx, as well as recognizing sounds. Primates are known to make \"vocal calls\", and these calls are generated by circuits in the brainstem and limbic system.\n\nIn the wild, the communication of vervet monkeys has been the most extensively studied. They are known to make up to ten different vocalizations. Many of these are used to warn other members of the group about approaching predators. They include a \"leopard call\", a \"snake call\", and an \"eagle call\". Each call triggers a different defensive strategy in the monkeys who hear the call and scientists were able to elicit predictable responses from the monkeys using loudspeakers and prerecorded sounds. Other vocalizations may be used for identification. If an infant monkey calls, its mother turns toward it, but other vervet mothers turn instead toward that infant's mother to see what she will do.\n\nSimilarly, researchers have demonstrated that chimpanzees (in captivity) use different \"words\" in reference to different foods. They recorded vocalizations that chimps made in reference, for example, to grapes, and then other chimps pointed at pictures of grapes when they heard the recorded sound.\n\nA study published in \"Homo: Journal of Comparative Human Biology\" in 2017 claims that \"A. ramidus\", a hominin dated at approximately 4.5Ma, shows the first evidence of an anatomical shift in the hominin lineage suggestive of increased vocal capability. This study compared the skull of \"A. ramidus\" with twenty nine chimpanzee skulls of different ages and found that in numerous features \"A. ramidus\" clustered with the infant and juvenile measures as opposed to the adult measures. Significantly, such affinity with the shape dimensions of infant and juvenile chimpanzee skull architecture was argued may have resulted in greater vocal capability. This assertion was based on the notion that the chimpanzee vocal tract ratios that prevent speech are a result of growth factors associated with puberty—growth factors absent in \"A. ramidus\" ontogeny. \"A. ramidus\" was also found to have a degree of cervical lordosis more conducive to vocal modulation when compared with chimpanzees as well as cranial base architecture suggestive of increased vocal capability.\n\nWhat was significant in this study was the observation that the changes in skull architecture that correlate with reduced aggression are the same changes necessary for the evolution of early hominin vocal ability. In integrating data on anatomical correlates of primate mating and social systems with studies of skull and vocal tract architecture that facilitate speech production, the authors argue that paleoanthropologists to date have failed to grasp the important relationship between early hominin social evolution and language capacity.\n\nWhile the skull of \"A. ramidus\", according to the authors, lacks the anatomical impediments to speech evident in chimpanzees, it is unclear what the vocal capabilities of this early hominin were. While they suggest \"A. ramidus\"—based on similar vocal tract ratios—may have had vocal capabilities equivalent to a modern human infant or very young child, they concede this is obviously a debatable and speculative hypothesis. However, they do claim that changes in skull architecture through processes of social selection were a necessary prerequisite for language evolution. As they write:\n\nRegarding articulation, there is considerable speculation about the language capabilities of early \"Homo\" (2.5 to 0.8 million years ago). Anatomically, some scholars believe features of bipedalism, which developed in australopithecines around 3.5 million years ago, would have brought changes to the skull, allowing for a more L-shaped vocal tract. The shape of the tract and a larynx positioned relatively low in the neck are necessary prerequisites for many of the sounds humans make, particularly vowels.\nOther scholars believe that, based on the position of the larynx, not even Neanderthals had the anatomy necessary to produce the full range of sounds modern humans make. It was earlier proposed that differences between \"Homo sapiens\" and Neanderthal vocal tracts could be seen in fossils, but the finding that the Neanderthal hyoid bone (see below) was identical to that found in \"Homo sapiens\" has weakened these theories. Still another view considers the lowering of the larynx as irrelevant to the development of speech.\n\nSteven Mithen proposed the term \"Hmmmmm\" for the pre-linguistic system of communication used by archaic \"Homo.\" beginning with \"Homo ergaster\" and reaching the highest sophistication in the Middle Pleistocene with \"Homo heidelbergensis\" and \"Homo neanderthalensis.\" \"Hmmmmm\" is an acronym for \"h\"olistic (non-compositional), \"m\"anipulative (utterances are commands or suggestions, not descriptive statements), \"m\"ulti-\"m\"odal (acoustic as well as gestural and facial), \"m\"usical, and \"m\"imetic.\n\n\"Homo heidelbergensis\" was a close relative (most probably a migratory descendant) of \"Homo ergaster.\" Some researchers believe this species to be the first hominin to make controlled vocalizations, possibly mimicking animal vocalizations, and that as \"Homo heidelbergensis\" developed more sophisticated culture, proceeded from this point and possibly developed an early form of symbolic language.\n\nThe discovery in 1989 of the (Neanderthal) Kebara 2 hyoid bone suggests that Neanderthals may have been anatomically capable of producing sounds similar to modern humans. The hypoglossal nerve, which passes through the hypoglossal canal, controls the movements of the tongue, which may have enabled voicing for size exaggeration (see size exaggeration hypothesis below) or may reflect speech abilities.\n\nHowever, although Neanderthals may have been anatomically able to speak, Richard G. Klein in 2004 doubted that they possessed a fully modern language. He largely bases his doubts on the fossil record of archaic humans and their stone tool kit. For 2 million years following the emergence of \"Homo habilis,\" the stone tool technology of hominins changed very little. Klein, who has worked extensively on ancient stone tools, describes the crude stone tool kit of archaic humans as impossible to break down into categories based on their function, and reports that Neanderthals seem to have had little concern for the final aesthetic form of their tools. Klein argues that the Neanderthal brain may have not reached the level of complexity required for modern speech, even if the physical apparatus for speech production was well-developed. The issue of the Neanderthal's level of cultural and technological sophistication remains a controversial one.\n\nBased on computer simulations used to evaluate that evolution of language that resulted in showing three stages in the evolution of syntax, Neanderthals are thought to have been in stage 2, showing they had something more evolved than proto-language but not quite as complex as the language of modern humans.\n\nAnatomically modern humans begin to appear in the fossil record in Ethiopia some 200,000 years ago.\nAlthough there is still much debate as to whether behavioural modernity emerged in Africa at around the same time, a growing number of archaeologists nowadays invoke the southern African Middle Stone Age use of red ochre pigments—for example at Blombos Cave—as evidence that modern anatomy and behaviour co-evolved. These archaeologists argue strongly that if modern humans at this early stage were using red ochre pigments for ritual and symbolic purposes, they probably had symbolic language as well.\n\nAccording to the recent African origins hypothesis, from around 60,000 – 50,000 years ago a group of humans left Africa and began migrating to occupy the rest of the world, carrying language and symbolic culture with them.\n\nThe larynx or voice box is an organ in the neck housing the vocal folds, which are responsible for phonation. In humans, the larynx is \"descended\". Our species is not unique in this respect: goats, dogs, pigs and tamarins lower the larynx temporarily, to emit loud calls. Several deer species have a permanently lowered larynx, which may be lowered still further by males during their roaring displays. Lions, jaguars, cheetahs and domestic cats also do this. However, laryngeal descent in nonhumans (according to Philip Lieberman) is not accompanied by descent of the hyoid; hence the tongue remains horizontal in the oral cavity, preventing it from acting as a pharyngeal articulator.\n\nDespite all this, scholars remain divided as to how \"special\" the human vocal tract really is. It has been shown that the larynx does descend to some extent during development in chimpanzees, followed by hyoidal descent. As against this, Philip Lieberman points out that only humans have evolved permanent and substantial laryngeal descent in association with hyoidal descent, resulting in a curved tongue and two-tube vocal tract with 1:1 proportions. Uniquely in the human case, simple contact between the epiglottis and velum is no longer possible, disrupting the normal mammalian separation of the respiratory and digestive tracts during swallowing. Since this entails substantial costs—increasing the risk of choking while swallowing food—we are forced to ask what benefits might have outweighed those costs. The obvious benefit—so it is claimed—must have been speech. But this idea has been vigorously contested. One objection is that humans are in fact \"not\" seriously at risk of choking on food: medical statistics indicate that accidents of this kind are extremely rare. Another objection is that in the view of most scholars, speech as we know it emerged relatively late in human evolution, roughly contemporaneously with the emergence of \"Homo sapiens.\" A development as complex as the reconfiguration of the human vocal tract would have required much more time, implying an early date of origin. This discrepancy in timescales undermines the idea that human vocal flexibility was \"initially\" driven by selection pressures for speech, thus not excluding that it was selected for e.g. improved singing ability.\n\nTo lower the larynx is to increase the length of the vocal tract, in turn lowering formant frequencies so that the voice sounds \"deeper\"—giving an impression of greater size. John Ohala argues that the function of the lowered larynx in humans, especially males, is probably to enhance threat displays rather than speech itself. Ohala points out that if the lowered larynx were an adaptation for speech, we would expect adult human males to be better adapted in this respect than adult females, whose larynx is considerably less low. In fact, females invariably outperform males in verbal tests, falsifying this whole line of reasoning. W. Tecumseh Fitch likewise argues that this was the original selective advantage of laryngeal lowering in our species. Although (according to Fitch) the initial lowering of the larynx in humans had nothing to do with speech, the increased range of possible formant patterns was subsequently co-opted for speech. Size exaggeration remains the sole function of the extreme laryngeal descent observed in male deer. Consistent with the size exaggeration hypothesis, a second descent of the larynx occurs at puberty in humans, although only in males. In response to the objection that the larynx is descended in human females, Fitch suggests that mothers vocalising to protect their infants would also have benefited from this ability.\n\nIn 2011, Quentin Atkinson published a survey of phonemes from 500 different languages as well as language families and compared their phonemic diversity by region, number of speakers and distance from Africa. The survey revealed that African languages had the largest number of phonemes, and Oceania and South America had the smallest number. After allowing for the number of speakers, the phonemic diversity was compared to over 2000 possible origin locations. Atkinson's \"best fit\" model is that language originated in central and southern Africa between 80,000 and 160,000 years ago. This predates the hypothesized southern coastal peopling of Arabia, India, southeast Asia, and Australia. It would also mean that the origin of language occurred at the same time as the emergence of symbolic culture.\n\nThe search for the origin of language has a long history rooted in mythology. Most mythologies do not credit humans with the invention of language but speak of a divine language predating human language. Mystical languages used to communicate with animals or spirits, such as the language of the birds, are also common, and were of particular interest during the Renaissance.\n\nVāc is the Hindu goddess of speech, or \"speech personified\". As Brahman's \"sacred utterance\", she has a cosmological role as the \"Mother of the Vedas\". The Aztecs' story maintains that only a man, Coxcox, and a woman, Xochiquetzal, survived a flood, having floated on a piece of bark. They found themselves on land and begat many children who were at first born unable to speak, but subsequently, upon the arrival of a dove, were endowed with language, although each one was given a different speech such that they could not understand one another.\n\nIn the Old Testament, the Book of Genesis (11) says that God prevented the Tower of Babel from being completed through a miracle that made its construction workers start speaking different languages. After this, they migrated to other regions, grouped together according to which of the newly created languages they spoke, explaining the origins of languages and nations outside of the fertile crescent.\n\nHistory contains a number of anecdotes about people who attempted to discover the origin of language by experiment. The first such tale was told by Herodotus (\"Histories\" 2.2). He relates that Pharaoh Psammetichus (probably Psammetichus I, 7th century BC) had two children raised by a shepherd, with the instructions that no one should speak to them, but that the shepherd should feed and care for them while listening to determine their first words. When one of the children cried \"bekos\" with outstretched arms the shepherd concluded that the word was Phrygian, because that was the sound of the Phrygian word for \"bread\". From this, Psammetichus concluded that the first language was Phrygian. King James V of Scotland is said to have tried a similar experiment; his children were supposed to have spoken Hebrew.\n\nBoth the medieval monarch Frederick II and Akbar are said to have tried similar experiments; the children involved in these experiments did not speak. The current situation of deaf people also points into this direction.\n\nModern linguistics does not begin until the late 18th century, and the Romantic or animist theses of Johann Gottfried Herder and Johann Christoph Adelung remained influential well into the 19th century. The question of language origin seemed inaccessible to methodical approaches, and in 1866 the Linguistic Society of Paris famously banned all discussion of the origin of language, deeming it to be an unanswerable problem. An increasingly systematic approach to historical linguistics developed in the course of the 19th century, reaching its culmination in the Neogrammarian school of Karl Brugmann and others.\n\nHowever, scholarly interest in the question of the origin of language has only gradually been rekindled from the 1950s on (and then controversially) with ideas such as universal grammar, mass comparison and glottochronology.\n\nThe \"origin of language\" as a subject in its own right emerged from studies in neurolinguistics, psycholinguistics and human evolution. The \"Linguistic Bibliography\" introduced \"Origin of language\" as a separate heading in 1988, as a sub-topic of psycholinguistics. Dedicated research institutes of evolutionary linguistics are a recent phenomenon, emerging only in the 1990s.\n\n"}
{"id": "38058647", "url": "https://en.wikipedia.org/wiki?curid=38058647", "title": "Origin of the Moon", "text": "Origin of the Moon\n\nThe origin of the Moon is usually explained by a Mars-sized body striking the Earth, making a debris ring that eventually collected into a single natural satellite, the Moon, but there are a number of variations on this giant-impact hypothesis, as well as alternate explanations, and research into how the Moon came to be continues. Other proposed scenarios include captured body, fission, formed together (condensation theory), planetesimal collisions (formed from asteroid-like bodies), and collision theories.\n\nThe standard giant-impact hypothesis suggests the Mars-sized body, called Theia, impacted Earth, creating a large debris ring around Earth, which then accreted to form the Moon. This collision also resulted in the 23.5° tilted axis of the earth, thus causing the seasons. The Moon's oxygen isotopic ratios seem to be essentially identical to Earth's. Oxygen isotopic ratios, which may be measured very precisely, yield a unique and distinct signature for each solar system body. If Theia had been a separate protoplanet, it probably would have had a different oxygen isotopic signature from Earth, as would the ejected mixed material. Also, the Moon's titanium isotope ratio (Ti/Ti) appears so close to the Earth's (within 4 ppm) that little if any of the colliding body's mass could likely have been part of the Moon.\n\nSome theories have been stated that presume the Earth had no large moons early in the formation of the Solar System, 4.6 billion years ago, Earth being basically rock and lava. Theia, an early protoplanet the size of Mars, hit Earth in such a way that it ejected a considerable amount of material away from Earth. Some proportion of this ejecta escaped into space, but the rest consolidated into a single body in orbit about Earth, creating the Moon.\n\nThe hypothesis requires a collision between a body about 90% of the present size of Earth, and another the diameter of Mars (half of the terrestrial radius and a tenth of its mass). The colliding body has sometimes been referred to as Theia, the mother of Selene, the Moon goddess in Greek mythology. This size ratio is needed in order for the resulting system to have sufficient angular momentum to match the current orbital configuration. Such an impact would have put enough material into orbit around Earth to have eventually accumulated to form the Moon.\n\nComputer simulations show a need for a glancing blow, which causes a portion of the collider to form a long arm of material that then shears off. The asymmetrical shape of the Earth following the collision then causes this material to settle into an orbit around the main mass. The energy involved in this collision is impressive: possibly trillions of tons of material would have been vaporized and melted. In parts of the Earth, the temperature would have risen to 10,000 °C (18,000 °F).\n\nThe Moon's relatively small iron core is explained by Theia's core accreting into that of Earth. The lack of volatiles in the lunar samples is also explained in part by the energy of the collision. The energy liberated during the reaccreation of material in orbit around Earth would have been sufficient to melt a large portion of the Moon, leading to the generation of a magma ocean.\n\nThe newly formed Moon orbited at about one-tenth the distance that it does today, and spiraled outward because of tidal friction transferring angular momentum from the rotations of both bodies to the Moon's orbital motion. Along the way, the Moon's rotation became tidally locked to Earth, so that one side of the Moon continually faces toward Earth. Also, the Moon would have collided with and incorporated any small pre-existing satellites of Earth, which would have shared the Earth's composition, including isotopic abundances. The geology of the Moon has since been more independent of the Earth. Although this hypothesis explains many aspects of the Earth–Moon system, there are still a few unresolved problems, such as the Moon's volatile elements not being as depleted as expected from such an energetic impact.\n\nAnother issue is lunar and Earth isotope comparisons. In 2001, the most precise measurement yet of the isotopic signatures of moon rocks was published. Surprisingly, the Apollo lunar samples carried an isotopic signature identical to Earth rocks, but different from other Solar system bodies. Because most of the material that went into orbit to form the Moon was thought to come from Theia, this observation was unexpected. In 2007, researchers from Caltech showed that the likelihood of Theia having an identical isotopic signature as the Earth is very small (<1 percent). Published in 2012, an analysis of titanium isotopes in Apollo lunar samples showed that the Moon has the same composition as Earth, which conflicts with the Moon forming far from Earth's orbit.\n\nTo help resolve these problems, a new theory published in 2012 posits that two bodies—each five times the size of Mars—collided, then re-collided, forming a large disc of debris that eventually formed Earth and the Moon. The paper was called “Forming a Moon with an Earth-like composition via a Giant Impact,” by R.M Canup.\n\nA 2012 study on the depletion of zinc isotopes on the Moon supported the giant-impact origin for Earth and the Moon.\n\nIn 2013, a study was released that indicated that water in lunar magma is indistinguishable from carbonaceous chondrites and nearly the same as that of Earth in isotopic composition.\n\nThe giant-impact hypothesis was again challenged in September 2013, with a growing sense that lunar origins are more complicated.\n\nIn 2018 researchers at Harvard and the University of California developed computer models demonstrating that one possible outcome of a planetary collision is that it creates a synestia, a mass of vaporized rock and metal which form a torus extending beyond the lunar orbit. The synestia will eventually shrink and cool to create the satellite and reform the impacted planet.\n\nAnother possibility is that before the giant impact, Earth had one or more normal satellites that shared its composition. Following the impact, the Moon formed closer to Earth than these satellites, and then spiraled outward, colliding with them. (If the Moon was more massive than the other satellites, its tidal effect on Earth would have been greater, making it spiral outward faster.) This led to the Moon being covered with material with the same composition as the satellites, and so Earth.\n\nIn 2017, planetary researchers at Weizmann Institute of Science in Rehovot, Israel offered a new theory that suggests the moon was forged in a brutal rain of cosmic debris that repeatedly hammered the fledgling Earth over millions of years. They determined that a series of smaller impacts, which were likely more common in the early solar system, could blast enough Earth rocks and dirt into orbit to form a small moonlet. As repeated impacts created more balls of debris, the moonlets could merge over time into one large moon.\n\nThis hypothesis states that the Moon was captured by the Earth. This was popular until the 1980s, and some things in favor of this model include the Moon's size, orbit, and tidal locking.\n\nOne problem is understanding the capture mechanism. A close encounter with Earth typically results in either collision or altered trajectories. For this hypothesis to function, there might have been a large atmosphere around the primitive Earth, which would slow the movement of the Moon by natural aerobraking before it could escape. That hypothesis may also explain the irregular satellite orbits of Jupiter and Saturn. This hypothesis also has difficulty explaining the essentially identical oxygen isotope ratios of the two bodies.\n\nThis is the now discredited hypothesis that an ancient, rapidly spinning Earth expelled a piece of its mass. This was proposed by George Darwin (son of the famous biologist Charles Darwin) in the 1800s and retained some popularity until Apollo. The Austrian geologist Otto Ampherer in 1925 also suggested the emerging of the Moon as cause for continental drift.\n\nIt was proposed that the Pacific Ocean represented the scar of this event. Today it is known that the oceanic crust that makes up this ocean basin is relatively young, about 200 million years old or less, whereas the Moon is much older. The Moon does not consist of oceanic crust but of mantle material, which originated inside the proto-Earth in the Precambrian.\n\nThe hypothesis of accretion suggests that the Earth and the Moon formed together as a double system from the primordial accretion disk of the Solar System.\nThe problem with this hypothesis is that it does not explain the angular momentum of the Earth-Moon system or why the Moon has a relatively small iron core compared to the Earth (25% of its radius compared to 50% for the Earth).\n\nIn 2011, it was theorized that a second moon existed 4.5 billion years ago, and later had an impact with the Moon, as a part of the accretion process in the formation of the Moon.\n\nOne hypothesis, presented only as a possibility, was that the Earth took the Moon from Venus.\n\nUranium–lead dating of Apollo 14 zircon fragments shows the age of the moon to be about 4.51 billion years.\n\n\n"}
{"id": "57062570", "url": "https://en.wikipedia.org/wiki?curid=57062570", "title": "Pontifical Committee for Historical Sciences", "text": "Pontifical Committee for Historical Sciences\n\nThe Pontifical Committee of Historical Sciences is a division of the Roman Curia established on 7 April 1954 by Pope Pius XII. \n\nPope Pius XII created this Committee on 7 April 1954 as the successor to the Commission of Cardinals for Historical Studies, which Pope Leo XIII had created on 18 August 1883 with the apostolic letter \"Saepenumero considerantes\". That commission was created to contribute to the development and proper use of historical sciences, especially late in the nineteenth century when parts of the Vatican's historical records, known as the Vatican Secret Archive, was opened to scholars. The new Committee was created to foster cooperation with the International Committee of Historical Sciences, which was scheduled to hold its convention in Rome in 1955.\n\nThe Committee is charged with promoting the use of ecclesiastical archives for historical research and fostering cooperation with ecclesiastical and extra-ecclesial institutions and associations, particularly at the international level. It sponsors seminars and conferences, both independently and in concert with peer organizations; reviews the historical data presented in the \"Pontifical Yearbook\"; collaborates with multinational scientific initiatives (UNESCO); and sponsors internships for the study of Latin and Greek.\n\nIn 2011, the Committee held a conference devoted to St. Catherine of Siena. In 2012 it marked the 1700th anniversary of the Battle of Ponte Milvio with a conference called \"Constantine the Great. To the roots of Europe\". A 2014 conference considered the legacy of Pope Pius X. In 2017, the Committee sponsored a three-day conference called \"Luther: 500 Years Later\" to examine the non-theological context of the origins of the Protestant Reformation.\n\n\n\n"}
{"id": "158687", "url": "https://en.wikipedia.org/wiki?curid=158687", "title": "Pre-Columbian trans-oceanic contact theories", "text": "Pre-Columbian trans-oceanic contact theories\n\nPre-Columbian trans-oceanic contact theories relate to visits or interactions with the Americas and/or indigenous peoples of the Americas by people from Africa, Asia, Europe, or Oceania before Columbus's first voyage to the Caribbean in 1492. Such contact is generally accepted in prehistory, but has been hotly debated in the historic period.\n\nTwo historical cases of pre-Columbian contact are accepted amongst the scientific and scholarly mainstream. Successful explorations led to Norse settlement of Greenland and the L'Anse aux Meadows settlement in Newfoundland some 500 years before Columbus.\n\nThe scientific and scholarly responses to other post-prehistory, pre-Columbian contact claims have varied. Some such contact claims are examined in reputable peer-reviewed sources. Other contact claims, typically based on circumstantial and ambiguous interpretations of archaeological finds, cultural comparisons, comments in historical documents, and narrative accounts, have been dismissed as fringe science or pseudoarcheology.\n\nNorse journeys to Greenland and Canada are supported by historical and archaeological evidence. A Norse colony in Greenland was established in the late 10th century, and lasted until the mid 15th century, with court and parliament assemblies (\"þing\") taking place at Brattahlíð and a bishop at Garðar. The remains of a Norse settlement at L'Anse aux Meadows in Newfoundland, Canada, were discovered in 1960 and are dated to around the year 1000 (carbon dating estimate 990–1050 CE), L'Anse aux Meadows is the only site widely accepted as evidence of pre-Columbian trans-oceanic contact. It was named a World Heritage site by UNESCO in 1978. It is also notable for its possible connection with the attempted colony of Vinland established by Leif Erikson around the same period or, more broadly, with Norse exploration of the Americas.\n\nFew sources describing contact between indigenous peoples and Norse people exist. Contact between the Thule people (ancestors of the modern Inuit) and Norse between the 12th or 13th centuries is known. The Norse Greenlanders called these incoming settlers \"skrælingar\". Conflict between the Greenlanders and the \"skrælings\" is recorded in the \"Icelandic Annals\". The term skrælings is also used in the Vínland sagas, which relate to events during the 10th century, when describing trade and conflict with native peoples.\n\nThe sweet potato, which is native to the Americas, was widespread in Polynesia when Europeans first reached the Pacific. Sweet potato has been radiocarbon-dated in the Cook Islands to 1000 CE, and current thinking is that it was brought to central Polynesia c. 700 CE and spread across Polynesia from there. It has been suggested that it was brought by Polynesians who had traveled to South America and back, or that South Americans brought it to the Pacific. It is possible that the plant could successfully float across the ocean if discarded from the cargo of a boat. Phylogenetic analysis supports the hypothesis of at least two separate introductions of sweet potatoes from South America into Polynesia, including one before and one after European contact. (see also #Linguistics.)\n\nA team of academics headed by the University of York's Mummy Research Group and BioArch, while examining a Peruvian mummy at the Bolton Museum, found that it had been embalmed using a tree resin. Before this it was thought that Peruvian mummies were naturally preserved. The resin, found to be that of an \"Araucaria\" conifer related to the 'monkey puzzle tree', was from a variety found only in Oceania and probably New Guinea. \"Radiocarbon dating of both the resin and body by the University of Oxford's radiocarbon laboratory confirmed they were essentially contemporary, and date to around CE 1200.\"\n\nResearchers including Kathryn Klar and Terry Jones have proposed a theory of contact between Hawaiians and the Chumash people of Southern California between 400 and 800 CE. The sewn-plank canoes crafted by the Chumash and neighboring Tongva are unique among the indigenous peoples of North America, but similar in design to larger canoes used by Polynesians for deep-sea voyages. \"Tomolo'o\", the Chumash word for such a craft, may derive from \"kumula'au\", the Hawaiian term for the logs from which shipwrights carve planks to be sewn into canoes. The analogous Tongva term, \"tii'at\", is unrelated. If it occurred, this contact left no genetic legacy in California or Hawaii. This theory has attracted limited media attention within California, but most archaeologists of the Tongva and Chumash cultures reject it on the grounds that the independent development of the sewn-plank canoe over several centuries is well-represented in the material record.\n\nThe existence of chicken bones dating from 1321 to 1407 in Chile and thought to be genetically linked to South Pacific Island chicken landraces suggested further evidence of South Pacific contact with South America. The genetic link between the South American Mapuche (to whom the chickens were thought to originally belong) chicken bones and South Pacific Island species has been rejected by a more recent genetic study which concluded that \"The analysis of ancient and modern specimens reveals a unique Polynesian genetic signature\" and that \"a previously reported connection between pre-European South America and Polynesian chickens most likely resulted from contamination with modern DNA, and that this issue is likely to confound ancient DNA studies involving haplogroup E chicken sequences.\"\n\nIn recent years, evidence has emerged suggesting a possibility of pre-Columbian contact between the Mapuche people (Araucanians) of south-central Chile and Polynesians. Chicken bones found at the site El Arenal in the Arauco Peninsula, an area inhabited by Mapuche, support a pre-Columbian introduction of chicken to South America. The bones found in Chile were radiocarbon-dated to between 1304 and 1424, before the arrival of the Spanish. Chicken DNA sequences taken were matched to those of chickens in American Samoa and Tonga, and dissimilar to European chicken. However, a later report in the same journal looking at the same mtDNA concluded that the Chilean chicken specimen clusters with the same European/Indian subcontinental/Southeast Asian sequences, providing no support for a Polynesian introduction of chickens to South America.\n\nDutch linguists and specialists in Amerindian languages Willem Adelaar and Pieter Muysken have suggested that two lexical items may be shared by Polynesian languages and languages of South America. One is the name of the sweet potato, which was domesticated in the New World. Proto-Polynesian *\"kumala\" (compare Easter Island \"kumara\", Hawaiian \"ʻuala\", Māori \"kumāra\"; apparent cognates outside Eastern Polynesian may be borrowed from Eastern Polynesian languages, calling Proto-Polynesian status and age into question) may be connected with Quechua and Aymara \"k’umar ~ k’umara\". A possible second is the word for 'stone axe', Easter Island \"toki\", New Zealand Maori \"toki\" 'adze', Mapuche \"toki\", and further afield, Yurumanguí \"totoki\" 'axe'. According to Adelaar and Muysken, the similarity in the word for sweet potato \"constitutes near proof of incidental contact between inhabitants of the Andean region and the South Pacific\", though according to Adelaar and Muysken the word for axe is not as convincing. The authors argue that the presence of the word for sweet potato suggests sporadic contact between Polynesia and South America, but no migrations.\n\nIn December 2007, several human skulls were found in a museum in Concepción, Chile. These skulls originated from Mocha Island, an island just off the coast of Chile in the Pacific Ocean, formerly inhabited by the Mapuche. Craniometric analysis of the skulls, according to Lisa Matisoo-Smith of the University of Otago and José Miguel Ramírez Aliaga of the Universidad de Valparaíso, suggests that the skulls have \"Polynesian features\" – such as a pentagonal shape when viewed from behind, and rocker jaws.\n\nFrom 2007 to 2009, geneticist Erik Thorsby and colleagues have published two studies in \"Tissue Antigens\" that evidence an Amerindian genetic contribution to the Easter Island population, determining that it was probably introduced before European discovery of the island.\n\nIn 2014, geneticist Anna-Sapfo Malaspinas of The Center for GeoGenetics at the University of Copenhagen published a study in \"Current Biology\" that found human genetic evidence of contact between the populations of Easter Island and South America, approximately 600 years ago (i.e. 1400 CE ± 100 years).\n\nSome members of the now extinct Botocudo people, who lived in the interior of Brazil, were found in research published in 2013 to have been members of mtDNA haplogroup B4a1a1, which is normally found only among Polynesians and other subgroups of Austronesians. This was based on an analysis of fourteen skulls. Two belonged to B4a1a1 (while twelve belonged to subclades of mtDNA Haplogroup C1 common among Native Americans). The research team examined various scenarios, none of which they could say for certain were correct. They dismissed a scenario of direct contact in prehistory between Polynesia and Brazil as \"too unlikely to be seriously entertained.\" While B4a1a1 is also found among the Malagasy people of Madagascar (which experienced significant Austronesian settlement in prehistory), the authors described as \"fanciful\" suggestions that B4a1a1 among the Botocudo resulted from the African slave trade (which included Madagascar).\n\nA genetic study published in \"Nature\" in July 2015 stated that \"some Amazonian Native Americans descend partly from a ... founding population that carried ancestry more closely related to indigenous Australians, New Guineans and Andaman Islanders than to any present-day Eurasians or Native Americans\". The authors, who included David Reich, added: \"This signature is not present to the same extent, or at all, in present-day Northern and Central Americans or in a ~12,600-year-old Clovis-associated genome, suggesting a more diverse set of founding populations of the Americas than previously accepted.\" This appears to conflict with an article published roughly simultaneously in \"Science\" which adopts the previous consensus perspective. The ancestors of all Native Americans entered the Americas as a single migration wave from Siberia no earlier than ~23 ka, separate from the Inuit and diversified into \"northern\" and \"southern\" Native American branches ~13 ka. There is evidence of post-divergence gene flow between some Native Americans and groups related to East Asians/Inuit and Australo-Melanesians .\n\nSimilar cultures of peoples across the Bering Strait in both Siberia and Alaska suggest human travel between the two places ever since the strait was formed. After Paleo-Indians arrived during the Ice Age and began the settlement of the Americas, a second wave of people from Asia came to Alaska around 8000 BC. These \"Na-Dene\" peoples, who share many linguistic and genetic similarities not found in other parts of the Americas, populated the far north of the Americas and only made it as south as Oasisamerica. By 4000 BC, it is theorized, \"Eskimo\" peoples began coming to the Americas from Siberia. \"Eskimo\" tribes live today in both Asia and North America and there is much evidence they lived in Asia even in prehistoric times.\n\nBronze artifacts discovered in a 1,000-year-old house in Alaska suggest pre-Columbian trade. Bronze working had not been developed in Alaska at the time and suggest the bronze came from nearby Asia—possibly China, Korea, or Russia. Also inside the house were found the remains of obsidian artifacts, which have a chemical signature that indicates the obsidian is from the Anadyr River valley in Russia.\n\nA 2013 genetic study suggests the possibility of contact between Ecuador and East Asia. The study suggests that the contact could have been trans-oceanic or a late-stage coastal migration that did not leave genetic imprints in North America. This contact could explain the alleged similarity between the pottery of the Valdivia culture of Ecuador and the Jōmon culture of Northeast Asia.\n\nOther researchers have argued that the Olmec civilization came into existence with the help of Chinese refugees, particularly at the end of the Shang dynasty. In 1975, Betty Meggers of the Smithsonian Institution argued that the Olmec civilization originated due to Shang Chinese influences around 1200 BCE. In a 1996 book, Mike Xu, with the aid of Chen Hanping, claimed that celts from La Venta bear Chinese characters. These claims are unsupported by mainstream Mesoamerican researchers.\n\nOther claims have been made for early Chinese contact with North America.\n\nIn 1882 artifacts identified at the time as Chinese coins were discovered in British Columbia. A contemporary account states that:In the summer of 1882 a miner found on De Foe (Deorse?) creek, Cassiar district, Br. Columbia, thirty Chinese coins in the auriferous sand, twenty-five feet below the surface. They appeared to have been strung, but on taking them up the miner let them drop apart. The earth above and around them was as compact as any in the neighborhood. One of these coins I examined at the store of Chu Chong in Victoria. Neither in metal nor markings did it resemble the modern coins, but in its figures looked more like an Aztec calendar. So far as I can make out the markings, this is a Chinese chronological cycle of sixty years, invented by Emperor Huungti, 2637 BCE, and circulated in this form to make his people remember it. In 1885, a vase containing similar discs was also discovered, wrapped in the roots of a tree around 300 years old. Grant Keddie, Curator of Archeology at the Royal BC Museum, examined a photograph of a coin from Cassiar taken in the 1940s (whereabouts now unknown) and he believes that the character style and the evidence that it was machine-ground show it to be a 19th-century copy of a Ming Dynasty temple token.\n\nA group of Chinese Buddhist missionaries led by Hui Shen before 500 CE claimed to have visited a location called Fusang. Although Chinese mapmakers placed this territory on the Asian coast, others have suggested as early as the 1800s that Fusang might have been in North America, due to perceived similarities between portions of the California coast and Fusang as depicted by Asian sources.\n\nIn his book \"1421: The Year China Discovered the World\", the British author Gavin Menzies made the controversial claim that the fleet of Zheng He arrived in America in 1421. Professional historians contend that Zheng He reached the eastern coast of Africa, and dismiss Menzies's hypothesis as entirely without proof.\n\nIn 1973 and 1975 doughnut-shaped stones were discovered off the coast of California that resembled stone anchors used by Chinese fishermen. These (sometimes called the \"Palos Verdes stones\") were initially thought to be up to 1500 years old and proof of pre-Columbian contact by Chinese sailors. Later geological investigations showed them to be a local rock known as Monterey shale, and they are thought to have been used by Chinese settlers fishing off the coast in the nineteenth century.\n\nIn June 2016, Purdue University published the results of research on six metal and composite metal artifacts excavated from a late prehistoric archaeological context at Cape Espenberg on the northern coast of the Seward Peninsula in Alaska. Also part of the research team was Robert J. Speakman, of the Center for Applied Isotope Studies at the University of Georgia, and Victor Mair, of East Asian Languages and Civilizations at the University of Pennsylvania. The report is the first evidence that metal from Asia reached prehistoric North America before the contact with Europeans, stating that X-ray fluorescence identified two of these artifacts as smelted industrial alloys with large proportions of tin and lead. The presence of smelted alloys in a prehistoric Inuit context in northwest Alaska was demonstrated for the first time and indicates the movement of Eurasian metal across the Bering Strait into North America before sustained contact with Europeans.\n\nSmithsonian archaeologist Betty Meggers wrote that pottery associated with the Valdivia culture of coastal Ecuador dated to 3000–1500 BCE exhibited similarities to pottery produced during the Jōmon period in Japan, arguing that contact between the two cultures might explain the similarities. Chronological and other problems have led most archaeologists to dismiss this idea as implausible. The suggestion has been made that the resemblances (which are not complete) are simply due to the limited number of designs possible when incising clay.\n\nAlaskan anthropologist Nancy Yaw Davis claims that the Zuni people of New Mexico exhibit linguistic and cultural similarities to the Japanese. The Zuni language is a linguistic isolate, and Davis contends that the culture appears to differ from that of the surrounding natives in terms of blood type, endemic disease, and religion. Davis speculates that Buddhist priests or restless peasants from Japan may have crossed the Pacific in the 13th century, traveled to the American Southwest, and influenced Zuni society.\n\nIn the 1890s, lawyer and politician James Wickersham argued that pre-Columbian contact between Japanese sailors and Native Americans was highly probable, given that from the early 17th century to the mid-19th century several dozen Japanese ships were carried from Asia to North America along the powerful Kuroshio Currents. Such Japanese ships landed from the Aleutian Islands in the north to Mexico in the south, carrying a total of 293 persons in the 23 cases where head-counts were given in historical records. In most cases, the Japanese sailors gradually made their way home on merchant vessels. In 1834 a dismasted, rudderless Japanese ship crashed near Cape Flattery. Three survivors of the ship were enslaved by Makahs for a period before being rescued by members of the Hudson's Bay Company. They were never able to return to their homeland due to Japan's isolationist policy. Another Japanese ship crashed in about 1850 near the mouth of the Columbia River, Wickersham writes, and the sailors were assimilated into the local Native American population. While admitting there was no definitive proof of pre-Columbian contact between Japanese and North Americans, Wickersham thought it implausible that such contacts as outlined above would have started only after Europeans arrived in North America.\n\nIn 1879, Alexander Cunningham described the carvings on the Stupa of Bharhut from c. 200 BCE and described some fruit like detail as a custard-apple (\"Annona squamosa\"). He wasn't aware that botanists believed this plant to be indigenous only in the Americas, but others quickly pointed out the difficulty, as it was generally believed that the custard-apple had not been brought to India before Vasco da Gama's discovery of the sea route in 1498. This suggestion was generally disregarded but a 2009 study claimed to have found carbonized remains that date to 2000 BCE and appear like seeds of custard apple.\n\nGrafton Elliot Smith claimed that certain details in the Mayan stelae at Copán represented an Asian elephant. He wrote \"Elephants and Ethnologists\", a book on the topic in 1924. Contemporary archaeologists suggested that it was based on a tapir and his suggestions have generally been dismissed by subsequent research.\n\nSome carving details from around the 12th century in Karnataka that appeared like ears of maize (\"Zea mays\"), a crop from the New World, were interpreted by Carl Johannessen in 1989 as evidence of pre-Columbian contact. These suggestions were dismissed by multiple Indian researchers based on several lines of evidence. The object has been claimed by some to represent a Muktaphala, an imaginary fruit bedecked with pearls.\n\n Proposed claims for an African presence in Mesoamerica stem from attributes of the Olmec culture, the claimed transfer of African plants to the Americas, interpretations of European and Arabic historical accounts and certain genetic studies of Mexican populations. \nThe Olmec culture existed from roughly 1200 BCE to 400 BCE. The idea that the Olmecs are related to Africans was suggested by José Melgar, who discovered the first colossal head at Hueyapan (now Tres Zapotes) in 1862. More recently, Ivan Van Sertima has argued that these statues depict settlers or explorers from Africa, but his views have been the target of severe scholarly criticism.\n\nLeo Wiener's \"Africa and the Discovery of America\" suggests similarities between Mandinka and native Mesoamerican religious symbols such as the winged serpent and the sun disk, or Quetzalcoatl, and words that have Mande roots and share similar meanings across both cultures such as \"kore\", \"gadwal\", and \"qubila\" (in Arabic) or \"kofila\" (in Mandinka). \n\nNorth African sources describe what some consider to be visits to the New World by a Mali fleet in 1311.\nAccording to the abstract of Columbus's log made by Bartolomé de las Casas, the purpose of Columbus’s third voyage was to test both the claims of King John II of Portugal that \"canoes had been found which set out from the coast of Guinea [West Africa] and sailed to the west with merchandise\" as well as the claims of the native inhabitants of the Caribbean island of Hispaniola that \"from the south and the southeast had come black people whose spears were made of a metal called guanín...from which it was found that of 32 parts: 18 were gold, 6 were silver, and 8 copper.\" Another supporting claim was made by Washington Irving, in his “Life of Columbus”, who wrote that in 1503 when Columbus was on the Mosquito Coast “There was no pure gold to be met with here, all their ornaments were of guanin; but the natives assured the Adelantado that in proceeding along the coast, the ships would soon arrive at a country where gold was in great abundance.”\n\nBrazilian researcher Niede Guidon, who led the Pedra Furada sites excavations \"... said she believed that humans … might have come not overland from Asia but by boat from Africa\", with the journey taking place 100,000 years ago. Michael R. Waters, a geoarchaeologist at Texas A&M University noted the absence of genetic evidence in modern populations to support Guidon's claim.\n\nEarly Chinese accounts of Muslim expeditions state that Muslim sailors reached a region called Mulan Pi (\"magnolia skin\") (). Mulan Pi is mentioned in \"Lingwai Daida\" (1178) by Zhou Qufei and \"Zhufan Zhi\" (1225) by Chao Jukua, together referred to as the \"Sung Document\". Mulan Pi is normally identified as Spain of the Almoravid dynasty (Al-Murabitun), though some fringe theories hold that it is instead some part of the Americas.\n\nOne supporter of the interpretation of Mulan Pi as part of the Americas was historian Hui-lin Li in 1961, and while Joseph Needham was also open to the possibility, he doubted that Arab ships at the time would have been able to withstand a return journey over such a long distance across the Atlantic Ocean and points out that a return journey would have been impossible without knowledge of prevailing winds and currents.\n\nAccording to Muslim historian Abu al-Hasan 'Alī al-Mas'ūdī (871-957), Khashkhash Ibn Saeed Ibn Aswad () sailed over the Atlantic Ocean and discovered a previously unknown land ( \"\") in 889 and returned with a shipload of valuable treasures.\n\nUsing gold obtained by expansion of the African coastal trade down the west African coast, the Phoenician state of Carthage minted gold staters in 350 BCE bearing a pattern, in the reverse exergue of the coins, interpreted as a map of the Mediterranean with the Americas shown to the west across the Atlantic. Reports of the discovery of putative Carthaginian coins in North America are based on modern replicas, that may have been buried at sites from Massachusetts to Nebraska in order to confuse and mislead archaeological investigation.\n\nThe Bat Creek inscription and Los Lunas Decalogue Stone have led some to suggest the possibility that Jewish seafarers may have come to America after fleeing the Roman Empire at the time of the Jewish Revolt.\n\nScholar Cyrus H. Gordon believed that Phoenicians and other Semitic groups had crossed the Atlantic in antiquity, ultimately arriving in both North and South America. This opinion was based on his own work on the Bat Creek inscription. Similar ideas were also held by John Philip Cohane; Cohane even claimed that many geographical names in America have a Semitic origin.\n\nThe Solutrean hypothesis argues that Europeans migrated to the New World during the Paleolithic era, circa 16,000 to 13,000 BCE. This hypothesis proposes contact partly on the basis of perceived similarities between the flint tools of the Solutrean culture in modern-day France, Spain and Portugal (which thrived circa 20,000 to 15,000 BCE), and the Clovis culture of North America, which developed circa 9000 BCE.\nThe Solutrean hypothesis was proposed in the mid-1990s. It has little support amongst the scientific community, and genetic markers are inconsistent with the idea.\n\nEvidence of contacts with the civilizations of Classical Antiquity—primarily with the Roman Empire, but sometimes also with other cultures of the age—have been based on isolated archaeological finds in American sites that originated in the Old World. The Bay of Jars in Brazil has been yielding ancient clay storage jars that resemble Roman amphorae for over 150 years. It has been proposed that the origin of these jars is a Roman wreck, although it has been suggested that they could be 15th or 16th century Spanish olive oil jars.\n\nRomeo Hristov argues that a Roman ship, or the drifting of such a shipwreck to the American shores, is a possible explanation of archaeological finds (like the Tecaxic-Calixtlahuaca bearded head) from ancient Rome in America. Hristov claims that the possibility of such an event has been made more likely by the discovery of evidences of travels from Romans to Tenerife and Lanzarote in the Canaries, and of a Roman settlement (from the 1st century BCE to the 4th century CE) on Lanzarote island.\n\nIn 1950, an Italian botanist, Domenico Casella, suggested that a depiction of a pineapple was represented among wall paintings of Mediterranean fruits at Pompeii. According to Wilhelmina Feemster Jashemski, this interpretation has been challenged by other botanists, who identify it as a pine cone from the Umbrella pine tree, which is native to the Mediterranean area.\n\nA small terracotta head sculpture, with a beard and European-like features, was found in 1933 (in the Toluca Valley, 72 kilometres southwest of Mexico City) in a burial offering under three intact floors of a pre-colonial building dated to between 1476 and 1510. The artifact has been studied by Roman art authority Bernard Andreae, director emeritus of the German Institute of Archaeology in Rome, Italy, and Austrian anthropologist Robert von Heine-Geldern, both of whom stated that the style of the artifact was compatible with small Roman sculptures of the 2nd century. If genuine and if not placed there after 1492 (the pottery found with it dates to between 1476 and 1510) the find provides evidence for at least a one-time contact between the Old and New Worlds.\n\nAccording to ASU's Michael E. Smith, John Paddock, a leading Mesoamerican scholar, used to tell his classes in the years before he died that the artifact was planted as a joke by Hugo Moedano, a student who originally worked on the site. Despite speaking with individuals who knew the original discoverer (García Payón), and Moedano, Smith says he has been unable to confirm or reject this claim. Though he remains skeptical, Smith concedes he cannot rule out the possibility that the head was a genuinely buried Post-classic offering at Calixtlahuaca.\n\nHenry I Sinclair, Earl of Orkney and feudal baron of Roslin (c. 1345 – c. 1400) was a Scottish nobleman. He is best known today because of a modern legend that he took part in explorations of Greenland and North America almost 100 years before Christopher Columbus. In 1784, he was identified by Johann Reinhold Forster as possibly being the Prince Zichmni described in letters allegedly written around the year 1400 by the Zeno brothers of Venice, in which they describe a voyage throughout the North Atlantic under the command of Zichmni.\n\nHenry was the grandfather of William Sinclair, 1st Earl of Caithness, the builder of Rosslyn Chapel (near Edinburgh, Scotland). The authors Robert Lomas and Christopher Knight believe some carvings in the chapel to be ears of New World corn or maize. This crop was unknown in Europe at the time of the chapel's construction, and was not cultivated there until several hundred years later. Knight and Lomas view these carvings as evidence supporting the idea that Henry Sinclair travelled to the Americas well before Columbus. In their book they discuss meeting with the wife of the botanist Adrian Dyer, and that Dyer's wife told him that Dyer agreed that the image thought to be maize was accurate. In fact Dyer found only one identifiable plant among the botanical carvings and suggested that the \"maize\" and \"aloe\" were stylized wooden patterns, only coincidentally looking like real plants. Specialists in medieval architecture interpret these carvings as stylised depictions of wheat, strawberries or lilies.\nSome have conjectured that Columbus was able to persuade the Catholic Monarchs of Castile and Aragon to support his planned voyage only because they were aware of some recent earlier voyage across the Atlantic. Some suggest that Columbus himself visited Canada or Greenland before 1492, because according to Bartolomé de las Casas he wrote he had sailed 100 leagues past an island he called Thule in 1477. Whether he actually did this and what island he visited, if any, is uncertain. Columbus is thought to have visited Bristol in 1476. Bristol was also the port from which John Cabot sailed in 1497, crewed mostly by Bristol sailors. In a letter of late 1497 or early 1498 the English merchant John Day wrote to Columbus about Cabot's discoveries, saying that land found by Cabot was \"discovered in the past by the men from Bristol who found 'Brasil' as your lordship knows\". There may be records of expeditions from Bristol to find the \"isle of Brazil\" in 1480 and 1481. Trade between Bristol and Iceland is well documented from the mid 15th century.\n\nGonzalo Fernández de Oviedo y Valdés records several such legends in his \"General y natural historia de las Indias\" of 1526, which includes biographical information on Columbus. He discusses the then-current story of a Spanish caravel that was swept off its course while on its way to England, and wound up in a foreign land populated by naked tribesmen. The crew gathered supplies and made its way back to Europe, but the trip took several months and the captain and most of the men died before reaching land. The ship's pilot, a man called Alonso Sánchez, and very few others finally made it to Portugal, but all were very ill. Columbus was a good friend of the pilot, and took him to be treated in his own house, and the pilot described the land they had seen and marked it on a map before dying. People in Oviedo's time knew this story in several versions, but Oviedo regarded it as myth.\n\nIn 1925, Soren Larsen wrote a book claiming that a joint Danish-Portuguese expedition landed in Newfoundland or Labrador in 1473 and again in 1476. Larsen claimed that Didrik Pining and Hans Pothorst served as captains, while João Vaz Corte-Real and the possibly mythical John Scolvus served as navigators, accompanied by Álvaro Martins. Nothing beyond circumstantial evidence has been found to support Larsen's claims.\n\nThe legend of Saint Brendan, an Irish monk, involves a fantastical journey into the Atlantic Ocean in search of Paradise in the 6th century. Since the discovery of the New World, various authors have tried to link the Brendan legend with an early discovery of America. In 1977 The voyage was successfully recreated by Tim Severin using an ancient Irish Currach.\n\nAccording to a British myth, Madoc was a prince from Wales who explored the Americas as early as 1170. While most scholars consider this legend to be untrue, it was used as justification for British claims to the Americas, based on the notion of a Briton arriving before other European nationalities.\n\nBiologist and controversial amateur epigrapher Barry Fell claims that Irish Ogham writing has been found carved into stones in the Virginias. Linguist David H. Kelley has criticized some of Fell's work but nonetheless argued that genuine Celtic Ogham inscriptions have in fact been discovered in America. However, others have raised serious doubts about these claims.\n\nTraces of coca and nicotine found in some Egyptian mummies have led to speculation that Ancient Egyptians may have had contact with the New World. The initial discovery was made by a German toxicologist, Svetlana Balabanova, after examining the mummy of a priestess called Henut Taui. Follow-up tests of the hair shaft, performed to rule out contamination, gave the same results.\n\nA television show reported that examination of numerous Sudanese mummies undertaken by Balabanova mirrored what was found in the mummy of Henut Taui. Balabanova suggested that the tobacco may be accounted for since it may have also been known in China and Europe, as indicated by analysis run on human remains from those respective regions. Balabanova proposed that such plants native to the general area may have developed independently, but have since gone extinct. Other explanations include fraud, though curator Alfred Grimm of the Egyptian Museum in Munich disputes this. Skeptical of Balabanova's findings, Rosalie David, Keeper of Egyptology at the Manchester Museum, had similar tests performed on samples taken from the Manchester mummy collection and reported that two of the tissue samples and one hair sample did test positive for nicotine. Sources of nicotine other than tobacco and sources of cocaine in the Old World are discussed by the British biologist Duncan Edlin.\n\nMainstream scholars remain skeptical, and they do not see this as proof of ancient contact between Africa and the Americas, especially because there may be possible Old World sources. Two attempts to replicate Balabanova's finds of cocaine failed, suggesting \"that either Balabanova and her associates are misinterpreting their results or that the samples of mummies tested by them have been mysteriously exposed to cocaine.\"\n\nA re-examination in the 1970s of the mummy of Ramesses II revealed the presence of fragments of tobacco leaves in its abdomen. This became a popular topic in fringe literature and the media and was seen as proof of contact between Ancient Egypt and the New World. The investigator, Maurice Bucaille, noted that when the mummy was unwrapped in 1886 the abdomen was left open and that \"it was no longer possible to attach any importance to the presence inside the abdominal cavity of whatever material was found there, since the material could have come from the surrounding environment.\" Following the renewed discussion of tobacco sparked by Balabanova's research and its mention in a 2000 publication by Rosalie David, a study in the journal \"Antiquity\" suggested that reports of both tobacco and cocaine in mummies \"ignored their post-excavation histories\" and pointed out that the mummy of Ramesses II had been moved five times between 1883 and 1975.\n\nIn 2010 Sigríður Sunna Ebenesersdóttir published a genetic study showing that over 350 living Icelanders carried mitochondrial DNA of a new type that is similar to the type found only in Native American and East Asian populations. Using the deCODE genetics database, Sigríður Sunna determined that the DNA entered the Icelandic population not later than 1700, and likely several centuries earlier. However Sigríður Sunna also states that \"...while a Native American origin seems most likely for [this new haplogroup], an Asian or European origin cannot be ruled out\".\n\nIn 1009, legends report that Norse explorer Thorfinn Karlsefni abducted two children from Markland, an area on the North American mainland where Norse explorers visited but did not settle. The two children were then taken to Greenland, where they were baptized and taught to speak Norse.\n\nIn 1420, Danish geographer Claudius Clavus Swart wrote that he personally had seen \"pygmies\" from Greenland who were caught by Norsemen in a small skin boat. Their boat was hung in Nidaros Cathedral in Trondheim along with another, longer boat also taken from \"pygmies\". Clavus Swart's description fits the Inuit and two of their types of boats, the kayak and the umiak. Similarly, the Swedish clergyman Olaus Magnus wrote in 1505 that he saw in Oslo Cathedral two leather boats taken decades earlier. According to Olaus, the boats were captured from Greenland pirates by one of the Haakons, which would place the event in the 14th century.\n\nIn Ferdinand Columbus's biography of his father Christopher, he says that in 1477 his father saw in Galway, Ireland two dead bodies which had washed ashore in their boat. The bodies and boat were of exotic appearance, and have been suggested to have been Inuit who had drifted off course.\n\nIt has been suggested that the Norse took other indigenous peoples to Europe as slaves over the following centuries, because they are known to have taken Scottish and Irish slaves.\n\nThere is also evidence of Inuit coming to Europe under their own power or as captives after 1492. A substantial body of Greenland Inuit folklore first collected in the 19th century told of journeys by boat to Akilineq, here depicted as a rich country across the ocean.\n\nPre-Columbian contact between Alaska and Kamchatka via the subarctic Aleutian Islands would have been conceivable, but the two settlement waves on this archipelago started on the American side and its western continuation, the Commander Islands, remained uninhabited until after Russian explorers encountered the Aleut people in 1741. There is no genetic or linguistic evidence for earlier contact along this route.\n\nIn 1650, a British preacher in Norfolk, Thomas Thorowgood, published \"Jewes in America or Probabilities that the Americans are of that Race\", for the New England missionary society. Tudor Parfitt writes:The society was active in trying to convert the Indians but suspected that they might be Jews and realized they better be prepared for an arduous task. Thorowgood's tract argued that the native population of North America were descendants of the Ten Lost Tribes.\n\nIn 1652 Sir Hamon L'Estrange, an English author writing on history and theology, published \"Americans no Jews, or improbabilities that the Americans are of that Race\" in response to the tract by Thorowgood. In response to L'Estrange, Thorowgood published a second edition of his book in 1660 with a revised title and included a foreword written by John Eliot, a Puritan missionary who had translated the Bible into an Indian language.\n\nThe Book of Mormon, a sacred text of the Latter Day Saint movement published by founder and leader Joseph Smith Jr in 1830 at the age of twenty-four, states that some ancient inhabitants of the New World are descendants of Semitic peoples who sailed from the Old World. Mormon groups such as the Foundation for Ancient Research and Mormon Studies attempt to study and expand on these ideas. Scientific consensus rejects these claims.\n\nThe National Geographic Society, in a 1998 letter to the Institute for Religious Research, stated \"Archaeologists and other scholars have long probed the hemisphere's past and the society does not know of anything found so far that has substantiated the Book of Mormon.\"\n\nSome LDS scholars hold the view that archaeological study of Book of Mormon claims are not meant to vindicate the literary narrative. For example, Terryl Givens, professor of English at the University of Richmond, points out that there is a lack of historical accuracy in the Book of Mormon in relation to modern archaeological knowledge.\n\nIn the 1950s, Professor M. Wells Jakeman popularized a belief that the Izapa Stela 5 represents the Book of Mormon prophets Lehi and Nephi's tree of life vision, and was a validation of the historicity of the claims of pre-Columbian settlement in the Americas. His interpretations of the carving and its connection to pre-Columbian contact have been disputed. Since that time, scholarship on the Book of Mormon has concentrated on cultural parallels rather than \"smoking gun\" sources.\n\n"}
{"id": "11447298", "url": "https://en.wikipedia.org/wiki?curid=11447298", "title": "Prime Minister's Prize for Australian History", "text": "Prime Minister's Prize for Australian History\n\nThe Prime Minister's Prize for Australian History was created by the Prime Minister of Australia, John Howard following the Australian History Summit held in Canberra on 17 August 2006. The Summit looked at how the Australian government could strengthen Australian history in the school curriculum. The winner (or winners) receive a gold medallion and a grant worth A$100,000.\n\nThe prize is awarded to an individual or a group, for an outstanding publication or body of work that contributes significantly to an understanding of Australian history. The subject of works submitted can include, but are not limited to:\n\nIn 2012, the prize was incorporated into the Prime Minister's Literary Awards.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "38563314", "url": "https://en.wikipedia.org/wiki?curid=38563314", "title": "Primitive communism", "text": "Primitive communism\n\nPrimitive communism is a concept originating from Karl Marx and Friedrich Engels who argued that hunter-gatherer societies were traditionally based on egalitarian social relations and common ownership. A primary inspiration for both Marx and Engels were Lewis Henry Morgan's descriptions of \"communism in living\" as practised by the Iroquois Nation of North America. In Marx's model of socioeconomic structures, societies with primitive communism had no hierarchical social class structures or capital accumulation.\n\nEngels offered the first detailed theorization of primitive communism in 1884, with publication of \"The Origin of the Family, Private Property, and the State\". Marx and Engels used the term more broadly than Marxists did later, and applied it not only to hunter-gatherers but also to some subsistence agriculture communities. There is also no agreement among later scholars, including Marxists, on the historical extent, or longevity, of primitive communism.\n\nMarx and Engels also noted how capitalist accumulation latched itself onto social organizations of \"primitive communism\". For instance, in private correspondence the same year that \"The Origin of the Family\" was published, Engels attacked European colonialism, describing the Dutch regime in Java directly organizing agricultural production and profiting from it, \"on the basis of the old communistic village communities\". He added that cases like the Dutch East Indies, British India and the Russian Empire showed \"how today primitive communism furnishes ... the finest and broadest basis of exploitation\".\n\nIn a primitive communist society, all able bodied persons would have engaged in obtaining food, and everyone would share in what was produced by hunting and gathering. There would be no private property, which is distinguished from personal property such as articles of clothing and similar personal items, because primitive society produced no surplus; what was produced was quickly consumed. The few things that existed for any length of time (tools, housing) were held communally, in Engels' view in association with matrilocal residence and matrilineal descent. There would have been no state.\n\nDomestication of animals and plants following the Neolithic Revolution through herding and agriculture was seen as the turning point from primitive communism to class society as it was followed by private ownership and slavery, with the inequality that they entailed. In addition, parts of the population specialized in different activities, such as manufacturing, culture, philosophy, and science which is said to lead to the development of social classes.\nEgalitarian and communist-like hunter gatherer societies have been studied and described by many well-known social anthropologists including James Woodburn, Richard Lee, Alan Barnard and, more recently, Jerome Lewis. Anthropologists such as Christopher Boehm, Chris Knight and Jerome Lewis offer theoretical accounts to explain how communistic, assertively egalitarian social arrangements might have emerged in the prehistoric past. Despite differences in emphasis, these and other anthropologists follow Engels in arguing that evolutionary change—resistance to primate-style sexual and political dominance—culminated eventually in a revolutionary transition. Richard Borshay Lee criticizes the mainstream and dominant culture's long-time bias against the possible existence of primitive communism, deriding \"Bourgeois ideology [that] would have us believe that primitive communism doesn't exist. In popular consciousness it is lumped with romanticism, exoticism: the noble savage.\"\n\n\n\n\n"}
{"id": "33575421", "url": "https://en.wikipedia.org/wiki?curid=33575421", "title": "Rennes-le-Château", "text": "Rennes-le-Château\n\nRennes-le-Château () is a small commune approximately 5 km (3 miles) south of Couiza, in the Aude department in Languedoc in southern France.\n\nThis small French hilltop village is known internationally, and receives tens of thousands of visitors per year, because of various conspiracy theories, about an alleged buried treasure discovered by its 19th-century priest Bérenger Saunière, the precise nature of which is disputed by those who believe in its existence.\n\nMountains frame both ends of the region—the Cevennes to the northeast and the Pyrenees to the south. The area is known for beautiful scenery, with jagged ridges, deep river canyons and rocky limestone plateaus, with large caves underneath.\n\nRennes-le-Château was the site of a prehistoric encampment, and later a Roman colony, or at least Roman villa or temple, such as is confirmed to have been built at Fa, west of Couiza, part of the Roman province of Gallia Narbonensis, the wealthiest part of Roman Gaul.\n\nRennes-le-Château was part of Septimania in the 6th and 7th centuries. It has been claimed that it was once an important Visigothic town, with some 30,000 people living in the city around 500-600 AD. However, British archaeologist Bill Putnam and British physicist John Edwin Wood argued that while there may have been a Visigothic town on the site of the present village, it would have had \"a population closer to 300 than 30,000\".\n\nBy 1050 the Counts of Toulouse held control over the area, building a castle in Rennes-le-Château around 1002, though nothing remains above ground of this medieval structure—the present ruin is from the 17th or 18th century.\n\nSeveral castles in the surrounding Languedoc region were central to the battle between the Catholic Church and the Cathars at the beginning of the 13th century. Other castles guarded the volatile border with Spain. Whole communities were wiped out in the campaigns of the Catholic authorities to rid the area of the Cathar heretics, the Albigensian Crusades, and again when French Protestants fought against the French monarchy two centuries before the French Revolution.\n\nThe village church dedicated to Saint Mary Magdalene has been rebuilt several times. The earliest church of which there is any evidence on the site may date to the 8th century. However, this original church was almost certainly in ruins by the 10th or 11th century, when another church was built upon the site—remnants of which can be seen in Romanesque pillared arcades on the north side of the apse. This survived in poor repair until the 19th century, when it was renovated by the local priest, Bérenger Saunière. Surviving receipts and existing account books belonging to Saunière reveal that the renovation of the church, including works on the presbytery and cemetery, cost 11,605 Francs over a ten-year period between 1887 and 1897.\n\nOne of the new elements was the Latin inscription \"Terribilis est locus iste\" above the front doors, taken from the Common Dedication of a Church, which translates as: \"This is a place of awe\"; the rest of the dedication reads \"this is God's house, the gate of heaven, and it shall be called the royal court of God.\" The first part of the dedication is above the front doors—the rest inscribed on the arches over the two front doors of the church.\n\nInside the church, one of the added figures was of a devil holding up the holy water stoup. Its original head was stolen in 1996 and has never been recovered. A devil like figure holding up the holy water stoup is a rare and unusual choice for the interior decoration of a Church but not exclusive to the Church of Saint Mary Magdalene; a similar subject matter can be seen in the Saint Vincent Collegiate church in Montréal, a short distance from Rennes-le-Château.\n\nThe new figures and statues were not made especially for this church, but were chosen by Saunière from a catalogue published by Giscard, sculptor and painter in Toulouse who, among other things, offered statues and sculptures for church refurbishment.\n\nSaunière also funded the construction of the Tour Magdala, a tower-like structure originally named the Tour de L'horloge and later renamed after Saint Mary Magdalene. Saunière used it as his library. The structure includes a circular turret with twelve crenellations, on a belvedere that connected it to an orangery. The tower has a promenade linking it to the Villa Bethania, which was not actually used by the priest. He stated at his trial that it was intended as a home for retired priests. Surviving receipts and existing account books belonging to Saunière reveal that the construction of his estate (including the purchases of land) between 1898 and 1905 cost 26,417 Francs.\n\nFollowing Sauniere's renovations and redecoratations, the church was re-dedicated in 1897 by his bishop, Monsignor Billard.\n\nIn 1910–1911, Bérenger Saunière was summoned by the bishopric to appear before an ecclesiastical trial to face charges of trafficking in masses. He was found guilty and suspended of the priesthood. When asked to produce his account books, he refused to attend his trial.\nBelievers in the enigma have suggested that Saunière's estate was set up on a large-scale checkerboard, while others have claimed that Saunière produced a Mirror image of selected architectural features of his property. They also allege that Maurice Barrès's novels Roman à clef and The Sacred Hill are largely based on the Rennes-le-Château story involving Bérenger Saunière (while novels by Jules Verne are cited to show that the enigma predates Abbé Saunière).\n\nThe village received up to around 100,000 tourists each year at the height of popularity of Dan Brown's bestselling novel \"The Da Vinci Code\". The modern reputation of Rennes-le-Château rises mainly from claims and stories dating from the mid-1950s concerning the local 19th-century priest Father Bérenger Saunière. These stories influenced the authors of the worldwide bestseller \"The Holy Blood and the Holy Grail\" in 1982, and that work in turn influenced Dan Brown when he wrote \"The Da Vinci Code\", published in 2003.\n\nThe first known popular article about Father Bérenger Saunière was written by Roger Crouquet in the Belgian magazine \"Le Soir illustré\", published in 1948. The author was visiting the Aude to meet his friend Monsieur Jean Mauhin, who originated from Belgium and had moved to Quillan to open a factory making bells and hats, and at his suggestion decided it would be a good idea to visit Rennes-le-Château. There Crouquet collected the testimonies of the villagers about Saunière. One person told how the priest \"preferred wine and women to practising the priesthood. At the end of the last century he had a rather original idea. He placed in foreign newspapers, especially in the United States, an advertisement announcing that the poor priest of Rennes-le-Château lived among heretics and had only the most meagre of resources. He moved the Christians of the whole world to such pity by announcing that the old church, an architectural gem, was heading for unavoidable destruction if urgent restoration work was not undertaken as soon as possible.\" Crouquet also added: \"The stoup which decorates the entrance to the chapel is carried by a horned devil with cloven hooves. An old woman remarked to us: 'It's the old priest, changed into a devil'.\"\n\nCrouquet's article faded into obscurity and it was left to Noël Corbu, a local man who had opened a restaurant in Saunière's former estate (called \"L'Hotel de la Tour\") in the mid-1950s, to turn the village into a household name. Corbu began circulating stories that Father Saunière had discovered \"parchments\" while renovating his church in 1892 that were to do with the treasure of Blanche of Castile, and which \"according to the archives\" consisted of 28,500,000 gold pieces. This was the treasure of the French crown assembled by Blanche de Castile to pay the ransom of Saint Louis IX, a prisoner of the infidels, the surplus of which she had hidden at Rennes-le-Château. Saunière had only found one part of it, so it was necessary to continue his investigations.\n\nCorbu also claimed that Rennes-le-Château was the capital of the Visigoths called \"Rhedae\", but this was another exaggeration: it was Narbonne that held that position. His claim can be traced back to a book by Louis Fédié entitled \"Le comté de Razès et le diocèse d'Alet\" (1880), that contained a chapter on the history of Rennes-le-Château; published as a booklet in 1994. Noël Corbu incorporated this story into his essay \"L'histoire de Rennes-le-Château\" that was deposited at the Departmental Archives at Carcassonne on 14 June 1962. Fédié's assertions concerning the population and importance of Rennes-le-Château have since been contradicted by archaeology and the work of more recent historians.\n\nCorbu's story was published in the book by Robert Charroux \"Trésors du monde\" in 1962, that caught the attention of Pierre Plantard, who decided to use and adapt Corbu's story for his own gain involving the mythological history of the Priory of Sion, that inspired the 1967 book \"L'Or de Rennes\" by author Gérard de Sède. Sède's book contained reproductions of \"parchments\" allegedly discovered by the priest Bérenger Saunière alluding to the survival of the line of Dagobert II, from which Plantard claimed to descent. Plantard and Sède fell out over book royalties and Philippe de Chérisey, Plantard's friend, revealed to have forged the parchments as part of the plot. At the same time, Plantard and Chérisey were also involved in planting fabricated documents in France's Bibliothèque Nationale that dealt with the secret history of the Priory of Sion.\n\nCorbu's story inspired author Robert Charroux to develop an active interest, and in 1958, with his wife Yvette and other members of The Treasure Seekers' Club that he founded in 1951, scanned the village and its church looking for treasure using a metal detector.\n\nIn 1969, Henry Lincoln, a British supporting actor and screenwriter for the BBC, read Gérard de Sède's book while on holiday in the Cévennes.He produced three BBC2 Chronicle documentaries between 1972 and 1979 and worked some of its material into the 1982 non-fictional bestseller, \"Holy Blood, Holy Grail\", that he co-wrote with Michael Baigent and Richard Leigh. The book alleged that the Priory of Sion guarded the Merovingian dynasty's bloodline, that the dynasty descended from a supposed marriage of Jesus Christ and Mary Magdalene and that Pierre Plantard was a modern-day descendant; Bérenger Saunière allegedly discovered that secret and amasses his wealth by blackmailing of the Holy See.\n\nThe blood-line hypotheses of Lincoln, Baigent and Leigh were later picked up in 2003 by Dan Brown's bestselling novel \"The Da Vinci Code\". Brown's novel never specifically mentioned Rennes-le-Château, but some key characters in the book had related names, such as Saunière, named after the priest, and \"Leigh Teabing\", whose name was derived from Richard Leigh and Michael Baigent. The two authors brought (and lost) a plagiarism suit against Brown in 2006. The extraordinary popularity of \"The Da Vinci Code\" reignited the interest of tourists, who come to the village to see sites associated with Saunière and Rennes-le-Château.\n\nThe sudden interest in Saunière's church generated by the stories circulated by Noël Corbu in the 1950s inspired two excavations of the church of St Mary Magdalene. The first was conducted by Dr André Malacan in May 1956, who, after excavating the subsoil of the church at the depth of approximately one metre, discovered some bones that included a skull bearing an incision, but they failed to discover \"anything of any interest\". Dr Malacan died in 1997, and the skull remained in the possession of his family until May 2014, when it was finally handed back to the village following several years of legal wrangling(carbon-dating of the skull has dated it to between 1281 and 1396). Between 1959 and 1963, Jacques Cholet, an engineer from Paris, also conducted several digs in the church, and also failed to discover anything of interest.\n\nIn November 1956, Monsieur Cotte of the \"Société des arts et des sciences de Carcassonne\" asked the membership during its monthly session about the treasure of Rennes-le-Château, which led to an investigation of the subject matter. Two members conducted on-the-spot research in March 1957 that lasted for one year. Local historian René Descadeillas commented: \"They found no evidence anywhere to support the assertion that, down the ages, any individual, family, group or clan could have accumulated a precious treasure-hoard at Rennes and then concealed it in the locality or its environs. What is more, the activities of the Abbé Saunière were undoubtedly eloquent of the sort of stratagems that he was accustomed to using in order to enrich himself.\"\n\nIn more recent times, a much-publicised 2003 excavation of the floor of the Tour Magdala by the Mayor of the village produced a stone, and not any anticipated treasure, following-up claims made by a Canadian who said he was related to one of the foremen who supervised Saunière's works. Another request, at the same time, was also made to excavate the church, but permission was refused by the \"Directions Régionales des Affaires Culturelles\" (or DRAC), the archaeological body of France.\n\nThe entire area around Rennes-le-Château became the focus of sensational claims in the 1950s and 1960s involving Blanche of Castile, the Merovingians, the Knights Templar, the Cathars, the treasures of the Temple of Solomon that was the booty of the Visigoths that included the Ark of the Covenant and the Menorah (the seven-branched candlestick from the Temple of Jerusalem). From the 1970s onwards claims have extended to the Priory of Sion, the Rex Deus, the Holy Grail, ley lines, sacred geometry alignments, the remains of Jesus Christ, alleged references to Mary Magdalene settling in the south of France, and even flying saucers. Well-known French authors like Jules Verne and Maurice Leblanc are suspected of leaving clues in their novels about their knowledge of the \"mystery\" of Rennes-le-Château.\n\nChristiane Amiel has commented:\n\nNo new theory has ever succeeded in entirely replacing any of the previous ones and, as the researches have intensified, so the various lines of investigation have accumulated and crossed in a system of ramifications in which criticism of one line of approach simply gives rise to others\n\nand\n\nToday the vogue is for analysing and checking the most minute details, for comparing and contrasting rival theories, for reviving old and unexplored lines of enquiry in a new guise, and for an unbridled pluralism which mixes together erudition and extrapolation, and makes recourse to geology, history, prehistory, esotericism, religious history, mysticism, the paranormal, ufology and other fields.\n\nRennes-le-Château conspiracy theories continue to be a popular ingredient in a publishing industry that is growing exponentially, and is the subject of press articles, radio and television programmes and films. Websites and blogs devoted to the alleged 'mysteries' exist in many different countries and authors' interviews can be accessed on podcasts.\n\nArchaeologist Paul Bahn considered the various claims surrounding the village of Rennes-le-Château as pure myth \"so beloved of occultists and 'aficionados' of the Unexplained\". He ranks the stories among those of the Bermuda Triangle, Atlantis and ancient astronauts as a source of \"ill-informed and lunatic books\". Likewise another archaeologist Bill Putnam, co-author with John Edwin Wood of \"The Treasure of Rennes-le-Château, A Mystery Solved\" (2003, 2005) has dismissed all of the popular allegations as pseudo-history.\n\nLaura Miller, contributor to \"The New York Times\" books section, commented how the village of Rennes-le-Château had become \"a town that had become the French equivalent of Roswell or Loch Ness as a result of popular books by Gérard de Sède.\"\n\nChristiane Amiel commented in 2008 that the treasure of Rennes-le-Château \"seems to elude all the investigations that people make into it. Like the fairy gold which, in the popular fables, turns into manure as soon as a human being touches it, it remains impalpable. It can only exist as long as it remains on the distinctive level of the dream, between the real and the imaginary.\"\n\n\n\n"}
{"id": "58555711", "url": "https://en.wikipedia.org/wiki?curid=58555711", "title": "Sebastian Sobecki", "text": "Sebastian Sobecki\n\nSebastian Sobecki (born 1973) is a medievalist specialising in English literature, history, and manuscript studies. \n\nSobecki is Professor of Medieval English Literature and Culture at the University of Groningen, the oldest chair (founded in 1886) for English literature in the Netherlands. He also holds by courtesy the Professorship of Old Germanic, established in 1881. Having received his education at the University of Cambridge, Sobecki became an Assistant Professor at McGill University before being appointed at Groningen. He works on medieval English and early Tudor literature, particularly on Chaucer, Gower, manuscripts, politics, law, travel, and Anglo-European relations. Sobecki was awarded the John Hurt Fisher Prize by the John Gower Society.\n\nSobecki has written widely on medieval and early modern topics, across 10 published and forthcoming books, and more than 50 academic articles and chapters, many of which have appeared in leading journals, including \"Speculum\", \"English Literary History\", \"Studies in the Age of Chaucer\", \"Renaissance Studies\", \"The English Historical Review\", \"The Chaucer Review\", \"The Library\", \"New Medieval Literatures\", and \"The Review of English Studies\". Together with Michelle Karnes (University of Notre Dame), Sobecki has been appointed editor of the journal \"Studies in the Age of Chaucer\". He is completing his third authored book, \"The Material Politics of England’s Fifteenth-Century Literature\" (Oxford: Oxford University Press) and two volumes in the Oxford edition of Richard Hakluyt's \"Principal Navigations\". Sobecki is also editing \"The Cambridge History of Medieval Travel Writing\".\n\nHe has made a number of important archival discoveries, such as identifying John Gower's autograph hand, finding a letter written for Margery Kempe's son, locating rebels linked to \"Piers Plowman\", revealing the author (John Peyton) of the earliest English description of Poland, and demonstrating connections between tax records and the General Prologue to Geoffrey Chaucer's \"Canterbury Tales\". Sobecki is also the voice behind the popular video recording of John Skelton's 'Speke Parott'. \n\n"}
{"id": "43244107", "url": "https://en.wikipedia.org/wiki?curid=43244107", "title": "Shapell Manuscript Foundation", "text": "Shapell Manuscript Foundation\n\nThe Shapell Manuscript Foundation (SMF) is a non-profit independent educational organization dedicated to research and the collection of historical documents and original manuscripts. The Foundation focuses on the histories of the United States and the Holy Land, with emphasis on the people and events of the 19th and 20th centuries.\n\nThe Shapell Manuscript Collection is a private holding of primary source documents relating to various events and historical figures in American, Jewish, and Holy Land history. Included in the Collection are signed documents, photographs and rare books. It is particularly rich with items from the Civil War era, Mark Twain, Albert Einstein, and many other historical figures. Among the many noteworthy items are exceptional letters written by George Washington, Abraham Lincoln, and John F. Kennedy.\n\nThe Shapell Manuscript Foundation's website offers a large digitized selection of the Collection online for public viewing and research. The online Collection is continuously expanding its offerings of primary source documents and historical artifacts as the digitization process continues.\n\nThe Shapell Roster Project is the ongoing research of Jewish Civil War soldiers and their stories. As part of the Foundation's contribution to the Civil War Sesquicentennial Celebration, the Shapell Manuscript Foundation is preparing a new roster of Jewish Civil War soldiers. The Roster is based on a parallel section in Simon Wolf's book, The American Jew as Soldier, Patriot and Citizen, (1895), but uses modern research tools and technology to expand and correct the almost 120-year-old roster.\n\nIn addition, the Roster will provide documentation (letters, photographs and official documents) for each soldier discussed in the list. At the completion of the project all information and documents will be available online as well as be available in book form. The Roster is scheduled to be published in 2017.\n\nIn addition to a general focus on the historic manuscripts of universally recognized world-renowned individuals, the foundation's collection frequently relates to the history of American Jewish life. The collected manuscripts explore such topics as the lives of Jewish soldiers during the American Civil War and other topics which bring light to bear on the role of American Jews on the general society around them.\n\nThe documents and artifacts in the Foundation's holdings have been on display at various exhibitions internationally. While the Foundation produces its own exhibitions, it also serves as a resource for other institutions' research efforts and exhibitions. The Foundation has collaborated with various historical institutes by creating and enhancing exhibitions with items on loan, and by contributing original research.\n\nThe Foundation regularly exhibits at the National Library of Israel at the Hebrew University in Jerusalem. The administrative offices are located in the United States and Israel.\nSMF loaned items to the Library of Congress for the traveling exhibition \"With Malice Toward None: The Abraham Lincoln Bicentennial Exhibition.\" The exhibition toured from February 2009 until April 2011 and included letters written by Lincoln, signed portraits and more.\n\nFrom December 2010 to March 2013 SMF collaborated on the joint exhibition at the National Library of Israel for the exhibit: \"Dreamland: American Travelers to the Holy Land in the 19th Century.\" A sequel to that exhibition, \"Dreams and Diplomacy in the Holy Land: American Consuls in Jerusalem in the 19th Century,\" opened in March 2013 and closed March, 2016.\n\nThe Smithsonian National Museum of American History exhibited several documents on loan from SMF at the exhibition: \"Changing America: The Emancipation Proclamation, 1863, and the March on Washington, 1963\" held from December 2012 until September 2013.\n\nIn 2015, SMF presented \"With Firmness in the Right: Lincoln and the Jews\" in collaboration with the New-York Historical Society and the Abraham Lincoln Presidential Library and Museum. The exhibition was produced in conjunction with the publication of the book, Lincoln and the Jews: A History, which contains original research and was co-authored by Professor Jonathan D. Sarna.\n\n\n\n"}
{"id": "23308932", "url": "https://en.wikipedia.org/wiki?curid=23308932", "title": "Sieradz Land", "text": "Sieradz Land\n\nSieradz Land () is a historical region of Poland, the southeastern part of Greater Poland. It has been also the name of the administrative unit from 14th-18th centuries (former Duchy of Sieradz) of the same borders (and a little different from the Sieradz Voivodeship, which included furthermore smaller Wieluń Land); the sejmik used to be held in Szadek. It has been a part of Archdiocese of Gniezno, and Uniejów used to be a residence of the primate. It has 9,700 km and about 950,000 inhabitants. Its traditional capital is Sieradz, while other bigger cities are Piotrków Trybunalski (another historically important locality), Radomsko, Tomaszów Mazowiecki (partly in Łęczyca Land), Bełchatów, Zduńska Wola, and Pabianice (a suburb of Łódź). It lies at the Warta and on the left bank of Pilica rivers, and these are mainly forested areas.\n"}
{"id": "44634486", "url": "https://en.wikipedia.org/wiki?curid=44634486", "title": "The Great Binge", "text": "The Great Binge\n\nThe Great Binge is a 21st Century neologism, coined by amateur historian Gradus Protus van den Belt, describing the period in history covering roughly 1870 to 1914. It is so known because of the widespread use and availability of narcotics such as opium, heroin, cocaine, morphine, and absinthe. During this period these drugs were widely available and incredibly popular among both men and women of many social classes in many parts of the world. They were marketed to both adults and children, often included in patent medicines such as cough syrups, pain relievers, and asthma medicines. They were administered to infants and women with menstrual cramps, and included in food and beverages such as Coca-Cola. Literary characters such as Sherlock Holmes were portrayed using morphine and cocaine. Holmes is described as having a particular penchant for overt injections of a 7% solution of cocaine - though only when lacking adequate mental stimulation. \n\nThe period ended with a series of laws regulating narcotic drugs in various countries and internationally. The International Opium Convention, signed in the Hague in 1912 by 11 countries and entering into force in 1915, was the first stab at a comprehensive drug control treaty internationally and inspired domestic drug control laws such as the Harrison Narcotics Tax Act in the United States.\n\n‘Binge’ is 19th century slang, although the meaning has evolved. However, the specific application of the term ‘great binge’ relative to drug use and popular attitudes towards drug use circa 1870–1914 is relatively recent. As well as its use by Bryars and Harper cited above, it was used by British author and comedian Stephen Fry to describe this period in 'More Fool Me' and by academic Nicholas J. Saunders in 'The Poppy: A History of Conflict, Loss, Remembrance, and Redemption', speaking primarily of the USA, to describe a 19th century “characterised by various narcotics that were legal, widely available, widely consumed and deeply embedded in popular culture”. Popular use goes back a little further. Esquire Magazine used it in this context in 2009: Dracula “appeared right in the middle of what historians call the Great Binge, a period in the late nineteenth and early twentieth centuries when cocaine and heroin use ran rampant”. A Canadian psychologist, Romeo Vitelli, used the term online in a post dated September 2, 2007.\n"}
{"id": "31375", "url": "https://en.wikipedia.org/wiki?curid=31375", "title": "The History of the Decline and Fall of the Roman Empire", "text": "The History of the Decline and Fall of the Roman Empire\n\nThe History of the Decline and Fall of the Roman Empire is a six-volume work by the English historian Edward Gibbon. It traces Western civilization (as well as the Islamic and Mongolian conquests) from the height of the Roman Empire to the fall of Byzantium. Volume I was published in 1776 and went through six printings. Volumes II and III were published in 1781; volumes IV, V, and VI in 1788–1789.\n\nThe six volumes cover the history, from 98 to 1590, of the Roman Empire, the history of early Christianity and then of the Roman State Church, and the history of Europe, and discusses the decline of the Roman Empire among other things.\n\nGibbon’s work remains a great literary achievement and a very readable introduction to the period, but considerable progress has since been made in history and archaeology, and his interpretations no longer represent current academic knowledge or thought. In \"The World of Late Antiquity\" (1971), Peter Brown offers an alternative interpretation of the period between the second and eighth centuries CE, and \"Framing the Early Middle Ages\" (2005) by Christopher Wickham presents hitherto unavailable evidence from both documentary and archaeological sources.\n\nGibbon offers an explanation for the fall of the Roman Empire, a task made difficult by a lack of comprehensive written sources, though he was not the only historian to attempt it.\n\nAccording to Gibbon, the Roman Empire succumbed to barbarian invasions in large part due to the gradual loss of civic virtue among its citizens. \nHe began an ongoing controversy about the role of Christianity, but he gave great weight to other causes of internal decline and to attacks from outside the Empire. \n\nLike other Enlightenment thinkers and British citizens of the age steeped in institutional anti-Catholicism, Gibbon held in contempt the Middle Ages as a priest-ridden, superstitious Dark Age. It was not until his own era, the \"Age of Reason\", with its emphasis on rational thought, it was believed, that human history could resume its progress.\n\nGibbon's tone was detached, dispassionate, and yet critical. He can lapse into moralisation and aphorism:\n\nGibbon provides the reader with a glimpse of his thought process with extensive notes along the body of the text, a precursor to the modern use of footnotes. Gibbon's footnotes are famous for their idiosyncratic and often humorous style, and have been called \"Gibbon's table talk.\" They provide an entertaining moral commentary on both ancient Rome and 18th century Great Britain. This technique enabled Gibbon to compare ancient Rome to his own contemporary world. Gibbon's work advocates a rationalist and progressive view of history.\n\nGibbon's citations provide in-depth detail regarding his use of sources for his work, which included documents dating back to ancient Rome. The detail within his asides and his care in noting the importance of each document is a precursor to modern-day historical footnoting methodology.\n\nThe work is notable for its erratic but exhaustively documented notes and research. John Bury, following him 113 years later with his own \"History of the Later Roman Empire\", commended the depth and accuracy of Gibbon's work. Unusually for 18th century historians, Gibbon was not content with second-hand accounts when primary sources were accessible. \"I have always endeavoured\", Gibbon wrote, \"to draw from the fountain-head; that my curiosity, as well as a sense of duty, has always urged me to study the originals; and that, if they have sometimes eluded my search, I have carefully marked the secondary evidence, on whose faith a passage or a fact were reduced to depend.\" The \"Decline and Fall\" is a literary monument and a massive step forward in historical method.\n\nNumerous tracts were published criticising his work. In response, Gibbon defended his work with the 1779 publication of, \"A Vindication ... of the Decline and Fall of the Roman Empire\". His remarks on Christianity aroused particularly vigorous attacks, but in the mid-twentieth century, at least one author claimed that \"church historians allow the substantial justness of [Gibbon's] main positions.\"\n\nGibbon's comments on the Quran and Muhammad reflected his view of the secular origin of the text. He outlined in chapter 33 the widespread tale of the Seven Sleepers, and remarked \"This popular tale, which Mahomet might learn when he drove his camels to the fairs of Syria, is introduced, as a divine revelation, into the Quran.\" His presentation of Muhammad's life again reflected his secular approach: \"in his private conduct, Mahomet indulged the appetites of a man, and abused the claims of a prophet. A special revelation dispensed him from the laws which he had imposed on his nation: the female sex, without reserve, was abandoned to his desires; and this singular prerogative excited the envy, rather than the scandal, the veneration, rather than the envy, of the devout Mussulmans.\".\n\nGibbon described the Jews as \"a race of fanatics, whose dire and credulous superstition seemed to render them the implacable enemies not only of the Roman government, but also of humankind\".\n\nBecause of his view Gibbon has been charged with antisemitism.\n\nGibbon challenged Church history by estimating far smaller numbers of Christian martyrs than had been traditionally accepted. The Church's version of its early history had rarely been questioned before. Gibbon, however, knew that modern Church writings were secondary sources, and he shunned them in favor of primary sources.\n\nHistorian S.P. Foster says that Gibbon:\n\nVolume I was originally published in sections, as was common for large works at the time. The first two were well received and widely praised. The last quarto in Volume I, especially Chapters XV and XVI, was highly controversial, and Gibbon was attacked as a \"paganist\". Gibbon thought that Christianity had hastened the Fall, but also ameliorated the results:\n\nVoltaire was deemed to have influenced Gibbon's claim that Christianity was a contributor to the fall of the Roman Empire. As one pro-Christian commenter put it in 1840:\n\nGibbon wrote:\n\nOthers such as John Julius Norwich, despite their admiration for his furthering of historical methodology, consider Gibbon's hostile views on the Byzantine Empire flawed and blame him somewhat for the lack of interest shown in the subject throughout the 19th and early 20th centuries. This view might well be admitted by Gibbon himself: \"But it is not my intention to expatiate with the same minuteness on the whole series of the Byzantine history.\" However the Russian historian George Ostrogorsky writes, \"Gibbon and Lebeau were genuine historians—and Gibbon a very great one—and their works, in spite of factual inadequacy, rank high for their presentation of their material.\"\n\nGibbon's initial plan was to write a history \"\"of the decline and fall of the \"city\" of Rome\"\", and only later expanded his scope to the whole Roman Empire: If I prosecute this \"History\", I shall not be unmindful of the decline and fall of the \"city\" of Rome; an interesting object, to which my plan was originally confined.\n\nAlthough he published other books, Gibbon devoted much of his life to this one work (1772–1789). His autobiography \"Memoirs of My Life and Writings\" is devoted largely to his reflections on how the book virtually \"became\" his life. He compared the publication of each succeeding volume to a newborn child.\n\nGibbon continued to revise and change his work even after publication. The complexities of the problem are addressed in Womersley's introduction and appendices to his complete edition.\n\n\nMany writers have used variations on the series title (including using \"Rise and Fall\" in place of \"Decline and Fall\"), especially when dealing with large nations or empires. Piers Brendon notes that Gibbon's work, \"became the essential guide for Britons anxious to plot their own imperial trajectory. They found the key to understanding the British Empire in the ruins of Rome.\"\n\nand in film:\n\nand in television:\n\nThe title and author are also cited in Noël Coward's comedic poem \"I Went to a Marvellous Party\". And in the poem \"The Foundation of Science Fiction Success\", Isaac Asimov acknowledged that his Foundation series – an epic tale of the fall and rebuilding of a galactic empire – was written \"with a tiny bit of cribbin' / from the works of Edward Gibbon\".\n\nIn 1995, an established journal of classical scholarship, \"Classics Ireland\", published punk musician's Iggy Pop's reflections on the applicability of \"The Decline and Fall of the Roman Empire\" to the modern world in a short article, \"Caesar Lives\", (vol. 2, 1995) in which he notedAmerica is Rome. Of course, why shouldn't it be? We are all Roman children, for better or worse ... I learn much about the way our society really works, because the system-origins – military, religious, political, colonial, agricultural, financial – are all there to be scrutinised in their infancy. I have gained perspective.\n\n\n\n"}
{"id": "31427982", "url": "https://en.wikipedia.org/wiki?curid=31427982", "title": "The Whitfield Prize", "text": "The Whitfield Prize\n\nThe Whitfield Prize (or Whitfield Book Prize) is a prize of £1000 awarded annually by the Royal Historical Society to the best work on a subject of British or Irish history published within the United Kingdom or Republic of Ireland during the calendar year. To be eligible for the award, the book must be the first history work published by the author.\n\nThe prize was founded in 1976 out of the bequest of Archibald Stenton Whitfield. Originally, the prize was £400; five years later, it was increased to £600. Currently, the prize is £1000.\n\nSource: Royal Historical Society\n"}
{"id": "27919989", "url": "https://en.wikipedia.org/wiki?curid=27919989", "title": "Timeline of European exploration", "text": "Timeline of European exploration\n\nThe following timeline covers European exploration from 1418 to 1957.\n\nThe 15th century witnessed the rounding of the feared Cape Bojador and Portuguese exploration of the west coast of Africa, while in the last decade of the century the Spanish sent expeditions to the New World, focusing on exploring the Caribbean Sea, and the Portuguese discovered the sea route to India. In the 16th century, various countries sent exploring parties into the interior of the Americas, as well as to their respective west and east coasts north to California and Labrador and south to Chile and Tierra del Fuego. In the 17th century, the Russians explored and conquered Siberia in search of sables, while the Dutch roughly worked on the chart for Australia. The 18th century saw the first extensive exploration of the South Pacific and the discovery of Alaska, while the nineteenth was dominated by exploration of the polar regions (not to mention excursions into the heart of Africa). By the 20th century, the poles themselves had been reached.\n\n\n\n\n \n\n\n\n"}
{"id": "30762076", "url": "https://en.wikipedia.org/wiki?curid=30762076", "title": "Traditional society", "text": "Traditional society\n\nIn sociology, traditional society refers to a society characterized by an orientation to the past, not the future, with a predominant role for custom and habit. Such societies are marked by a lack of distinction between family and business, with the division of labor influenced primarily by age, gender, and status.\n\nTraditional society has often been contrasted with modern industrial society, with figures like Durkheim and Pierre Bourdieu stressing such polarities as community vs. society or mechanical vs. organic solidarity; while Claude Lévi-Strauss saw traditional societies as 'cold' societies in that they refused to allow the historical process to define their social sense of legitimacy.\n\nWithin modernisation theory, traditional society is also the first stage of economic development as established in W.W. Rostow's Economic Growth Model. Classified as \"pre-newtonian,\" science and technology are not practiced. Life is agrarian, and family or clan relationships are the basis for social structures.\n\nHowever, theories positing the simple, unilineal evolution of societies from traditional to modern industrial are now seen as too simplistic, relying on an ideal typology revolving round such polarities as subsistence/growth; face-to-face/impersonal; informal social control or formal social control; collective ownership/private ownership. Recent work has emphasised instead the variety of traditional cultures, and the existence of intermediate forms as well as of 'alternative' modernisations.\n\nTraditional societies have been seen as characterised by powerful collective memories sanctioned by ritual, and with social guardians ensuring continuity of communal practices.\n\nPractice theory however has recently emphasised the role of ritual in facilitating change, as well as continuity.\n\nFredric Jameson saw 20th century modernisation as encountering two main kinds of traditional society, tribal, as in Africa, and bureaucratic imperial, as in China and India, but a much wider diversity of traditional societies has existed over time.\n\nFor most of human existence, small tribes of hunter-gatherers leading an almost static existence formed the only social organisation: where they survived into the 20th century, as in Australia, paintings, songs, myths and rituals were all used to cement links to a deep-reaching sense of continuity with ancestors and ancestral ways.\n\nThe invention of farming some 10,000 years ago led to the development of agrarian societies, whether nomadic or peasant, the latter in particular almost always dominated by a strong sense of traditionalism. Within agrarian society, however, a wide diversity still existed. Homeric Greece was a society marked by powerful kinship bonds, fixed status and rigidly defined social expectations; with the classical polis, however, though festivals, in M. I. Finley's words, still \"recreated for their audiences the unbroken web of all life, stretching back over generations of men to the gods\", new and more complex voluntary forms of social and public life balanced traditional society in a new equilibrium.\n\nMedieval Europe was an intensely local society of self-perpetuating peasant households, living within a slow moving culture dominated by customary law and by respect for ancient authority and pervaded with an ahistorical political mentality focused upon the concepts of experience, usage, and law-as-custom.\nIn some villages, you find digging being done as a community. People farm together and when harvesting time comes, they share the harvests. So there is communal work as a means of simplifying work and this earns them security to themselves and their produce. This is witnessed in some parts of Buganda like in Lumanyo, Maddu, Gomba District.\n\nMuch of the focus of Enlightenment thinking was directed at undoing the mindset of traditional society, and replacing a focus upon such concepts as rural, hierarchical, customary or status with one centred on the ideas of urban, egalitarian, progressive or contractual. Modernism and modernity continued the process of challenging and overcoming traditional society.\n\nJameson, however, has seen as a defining feature of postmodernism the global elimination of residual, 'traditional' enclaves, giving it its one-dimensional, temporal nature that is no longer offset by living examples of the past alongside the new.\n\nGlobal media such as the Internet have been seen as effective means of recreating traditional cultures. However, a key contrast now with traditional societies as they were is that participation has become voluntary instead of being ascriptive: fixed in space, social stratification and role expectations.\n\n\n\n"}
{"id": "262830", "url": "https://en.wikipedia.org/wiki?curid=262830", "title": "Translatio imperii", "text": "Translatio imperii\n\nTranslatio imperii (Latin for \"transfer of rule\") is a historiographical concept, originating in the Middle Ages, in which history is viewed as a linear succession of transfers of an \"imperium\" that invests supreme power in a singular ruler, an \"emperor\" (or sometimes even several emperors, e.g., the Eastern Roman Empire and the Western Holy Roman Empire). The concept is closely linked to \"translatio studii\" (the geographic movement of learning). Both terms are thought to have their origins in the second chapter of the Book of Daniel in the Hebrew Bible (verses 39–40).\n\nJacques Le Goff describes the \"translatio imperii\" concept as \"typical\" for the Middle Ages for several reasons:\n\nEach medieval author described the \"translatio imperii\" as a succession leaving the supreme power in the hands of the monarch ruling the region of the author's provenance:\n\nLater, continued and reinterpreted by modern and contemporary movements and authors (some known examples):\n\nMedieval and Renaissance authors often linked this transfer of power by genealogically attaching a ruling family to an ancient Greek or Trojan hero; this schema was modeled on Virgil's use of Aeneas (a Trojan hero) as progenitor of the city of Rome in his \"Aeneid\". Continuing with this tradition, the twelfth-century Anglo-Norman authors Geoffrey of Monmouth (in his \"Historia Regum Britanniae\") and Wace (in his \"Brut\") linked the founding of Britain to the arrival of Brutus of Troy, son of Aeneas.\n\nIn a similar way, the French Renaissance author Jean Lemaire de Belges (in his \"Les Illustrations de Gaule et Singularités de Troie\") linked the founding of Celtic Gaul to the arrival of the Trojan Francus (i.e. Astyanax), the son of Hector; and of Celtic Germany to the arrival of Bavo, the cousin of Priam; in this way he established an illustrious genealogy for Pepin and Charlemagne (the legend of Francus would also serve as the basis for Ronsard's epic poem, \"La Franciade\").\n\nThe cardinal point in the idea of the \"Translatio imperii\" is the link between the Byzantine to the Holy Roman Empire.\n\n"}
{"id": "7535573", "url": "https://en.wikipedia.org/wiki?curid=7535573", "title": "Umberto Meoli", "text": "Umberto Meoli\n\nUmberto Meoli (26 August 1920 – 17 May 2002) was an Italian historian of economics, known as a maverick of the Italian Left who eschewed Marxism in favour of British pragmatism.\n\nMeoli was born in Padua, one of nineteen brothers; his father was a pharmacist from a small town near Benevento, and his mother was from Padua.\n\nIn 1940, at age 20, Meoli began his service in the Italian Army fighting in World War II, and, three years later, with the Resistance after the 1943 armistice between Italy and the Allied armed forces. As a result, Meoli was imprisoned by the Benito Mussolini Fascist government, and spent several months in the Palazzo Giusti detention center in Padua. The Palazzo Giusti detention center was notorious for the cruelty of some Fascists in Padua during the Italian Social Republic. For example, Giovanni Gonelli, a barely literate jail keeper at Palazzo Giusti in Padua and a member of the Banda Carità, apparently enjoyed dehumanizing his prisoners by refusing their requests for food or blankets. The sentence of the Appellate Court of 8 January 1946 states that Gonelli would tell prisoners who asked for water to \"piss and drink.\" There is no doubt that Meoli experienced such cruelty while imprisoned in Palazzo Giusti, and that this cruelty affected his thoughts.\n\nIn 1961 Meoli married Rachel Toulmin, an Englishwoman and lecturer at Padua University. Rachel Toulmin was the younger sister of Stephen Toulmin, a historian and philosopher of science at the University of Southern California. \n\nMeoli was briefly a Communist, and after graduating, he worked for a time in the trade unions (\"Camera del Lavoro\") in Vicenza, but he was soon at odds with the rigid militancy of organized labor. He found his vocation in the late 1950s, when he obtained a teaching post at the University of Parma.\n\nIt was during his time at \"Camera del Lavoro\" that Meoli became influenced by the Canadian Harry Gordon Johnson, one of the most active and prolific economists of all time. Gordon's main research was in the area of international trade, international finance, and monetary policy. In 1961, Meoli made a special journey to the University of Manchester in England to meet Johnson. He quickly became a close friend of both Philip Andrews, A former President of the Royal Economic Society, Andrews was senior researcher at Nuffield College at the time and Elizabeth Brunner with whom Meoli shared an admiration for Alfred Marshall and a more skeptical view of John Maynard Keynes.\n\nIn 1961, Meoli published the first book in steady flow of books, in which would increasingly concentrate on the history of economic thought and ideas.\n\nBy 1970, Meoli became Professor of the History of Economic Thought at Camera del Lavoro, and shortly afterwards was invited to occupy a similar chair at the University of Venice.\n\nIn addition to his chair positions, from 1992 to 1998 Umberto Meoli was president of the Italian Association for the History of Economic Thought (\"L'Associazione Italiana per la Storia del Pensiero Economico\", AISPE).\n\nMeoli always thought of himself as a man of the Left. However, he early became convinced of the unacceptable limitations of Marxist economic theory, and his books and articles struck many of his leftist colleagues as a kind of apostasy. Meoli's writings exhibited an ever-growing respect for British pragmatism, and the economic liberalism of Adam Smith, David Ricardo, and Alfred Marshall. As an example of his unconventional approach, the figure of Gustav von Schmoller has been highlighted by authors like Francesco Traniello, who has indicated the importance that Schmoller, like Gustav von Schönberg, Adolph Wagner and Albert Schäffle, among others, gave the ethical element of the political Economy, whereas Umberto Meoli associates the figure of Schmoller to those of Lujo Brentano and Karl Bücher as the most representative authors of the development of the Economic Historiography. Eventually seen in Italian academic circles as a maverick and a great Anglophile, Meoli's leftist friends tolerated the irony, largely because Meoli would comment on their dismay with sudden eruptions of laughter.\n\n\n\nLauer, A. Robert. \"A Revaluation of Pasolini's \"Salò\".\" \"CLCWeb: Comparative Literature and Culture\" 4.1 (2002): \n\n"}
{"id": "4006694", "url": "https://en.wikipedia.org/wiki?curid=4006694", "title": "Unwitting testimony", "text": "Unwitting testimony\n\nUnwitting testimony refers to the unintentional evidence provided by historical sources. They may demonstrate the attitudes of an author, or the culture to which he or she belongs. This term was originally used by the British historian Arthur Marwick.\n\n"}
{"id": "1028430", "url": "https://en.wikipedia.org/wiki?curid=1028430", "title": "Urban revolution", "text": "Urban revolution\n\nIn anthropology and archaeology, the Urban Revolution is the process by which small, kin-based, nonliterate agricultural villages were transformed into large, socially complex, urban societies. \n\nThe term \"urban revolution\" was introduced in the 1930s by V. Gordon Childe, an Australian archaeologist. Childe also coined the term Neolithic Revolution to describe the earlier process by which hunter-gatherer societies domesticated crops and animals and began a farming lifestyle. Childe was the first to synthesize and organize the large volume of new archaeological data in the early 20th century in social terms. Whereas previous archaeologists had concentrated on chronology and technology, Childe applied concepts and theories from the social sciences to interpret archaeological finds.\nChilde first discussed the Urban Revolution in his 1936 book, \"Man Makes Himself\", and then his 1950 article in the journal \"Town Planning Review\" brought the concept to a much larger audience. In that paper, he presented a 10-point model for the changes that characterized the Urban Revolution:\n\n\nAlthough sometimes interpreted as a model of the origins of cities and urbanism, Childe's concept in fact describes the transition from agricultural villages to state-level, urban societies. This change, which occurred independently in several parts of the world, is recognized as one of the most significant changes in human Sociocultural evolution.\nAlthough contemporary models for the origins of complex urban societies have progressed beyond Childe's original formulation, there is general agreement that he correctly identified one of the most far-reaching social transformations prior to the Industrial Revolution, as well as the major processes involved in the change.\n\n"}
