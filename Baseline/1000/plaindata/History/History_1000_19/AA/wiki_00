{"id": "8900382", "url": "https://en.wikipedia.org/wiki?curid=8900382", "title": "1952 in radio", "text": "1952 in radio\n\nThe year 1952 in radio involved some significant events.\n\n\n\n"}
{"id": "4238124", "url": "https://en.wikipedia.org/wiki?curid=4238124", "title": "1989 world oil market chronology", "text": "1989 world oil market chronology\n\n|-\n"}
{"id": "13594858", "url": "https://en.wikipedia.org/wiki?curid=13594858", "title": "1993 in radio", "text": "1993 in radio\n\nThe year 1993 in radio involved some significant events.\n\n\n\n\n\n"}
{"id": "24152050", "url": "https://en.wikipedia.org/wiki?curid=24152050", "title": "2000 World Monuments Watch", "text": "2000 World Monuments Watch\n\nThe World Monuments Watch is a flagship advocacy program of the New York-based private non-profit organization World Monuments Fund (WMF) and American Express to call upon every government in the world, preservation organizations, and other groups and individuals to nominate sites and monuments that are particularly endangered. At the same time, the nominators commit themselves to participate in a carefully planned preservation project.\n\nEvery two years, the program publishes a select list known as the Watch List of 100 Most Endangered Sites that is in urgent need of preservation funding and protection. The sites are chosen from these nominations by an independent panel of international experts, based on the significance of the site, the urgency of the problem, and the viability of the proposal for action. WMF would then publicize their plight and help find the resources and expertise to carry out the preservation projects for the 100 sites on the Watch List. The leverage from the listing also spurs government agencies and local donors to allocate funds and take an active role in protecting the cultural landmark, in addition to grants directly coming from WMF and American Express.\n\nThe 2000 World Monuments Watch List of 100 Most Endangered Sites was launched on 14 September 1999 by WMF President Bonnie Burnham.\n\nThe following countries/territories have multiple sites entered on the 2000 Watch List, listed by the number of sites:\n\nA. Numbers list only meant as a guide on this article. No official reference numbers have been designated for the sites on the Watch List.\nB. Names and spellings used for the sites were based on the official 2000 Watch List as published.\nC. The references to the sites' locations and periods of construction were based on the official Watch List as published.\n"}
{"id": "2595860", "url": "https://en.wikipedia.org/wiki?curid=2595860", "title": "3rd century in architecture", "text": "3rd century in architecture\n\n\"See also:\" \n2nd century in architecture, \n4th century in architecture and the \narchitecture timeline.\n\n"}
{"id": "1527026", "url": "https://en.wikipedia.org/wiki?curid=1527026", "title": "564 BC", "text": "564 BC\n\nThe year 564 BC was a year of the pre-Julian Roman calendar. In the Roman Empire, it was known as year 190 \"Ab urbe condita\". The denomination 564 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years. \n"}
{"id": "13029312", "url": "https://en.wikipedia.org/wiki?curid=13029312", "title": "A Chart of Biography", "text": "A Chart of Biography\n\nIn 1765, 18th-century British polymath Joseph Priestley published A Chart of Biography and its accompanying prose description as a supplement to his \"Lectures on History and General Policy\". Priestley believed that the chart and \"A New Chart of History\" (1769) would allow students to \"trace out distinctly the dependence of events to distribute them into such periods and divisions as shall lay the whole claim of past transactions in a just and orderly manner.\" \n\nThe \"Chart of Biography\" covers a vast timespan, from 1200 BC to 1800 AD, and includes two thousand names. Priestley organized his list into six categories: Statesman and Warriors; Divines and Metaphysicians; Mathematicians and Physicians (natural philosophers were placed here); Poets and Artists; Orators and Critics (prose fiction authors were placed here); and Historians and Antiquarians (lawyers were placed here). Priestley's \"principle of selection\" was fame, not merit; therefore, as he mentions, the chart is a reflection of current opinion. He also wanted to ensure that his readers would recognize the entires on the chart. Priestley had difficulty assigning all of the people listed to individual categories; he attempted to list them in the category under which their most important work had been done. Machiavelli is therefore listed as a historian rather than a statesman and Cicero is listed as a statesman instead of an orator. The chart was also arranged in order of importance; \"statesmen are placed on the lower margin, where they are easier to see, because they are the names most familiar to readers.\" \n\nBoth \"Charts\" were popular for decades—the \"A New Chart of History\" went through fifteen editions by 1816. The trustees of Warrington were so impressed with Priestley's lectures and charts that they arranged for the University of Edinburgh to grant him a Doctor of Law degree in 1764.\n\n"}
{"id": "26381867", "url": "https://en.wikipedia.org/wiki?curid=26381867", "title": "Aboiteau", "text": "Aboiteau\n\nAboiteau farming on reclaimed marshland is a labor-intensive method in which earthen dykes are constructed to stop high tides from inundating marshland. A wooden sluice or aboiteau (plural aboiteaux) is then built into the dyke, with a hinged door (clapper valve) that swings open at low tide to allow fresh water to drain from the farmland but swings shut at high tide to prevent salt water from inundating the fields.\n\nAboiteau farming is intimately linked with the story of French Acadian colonization of the shores of Canada's Bay of Fundy in the 17th and 18th centuries. In the Kamouraska region of the St. Lawrence Valley of Quebec, aboiteau diking of salt marshes was closely tied to the modernization of agriculture in the 19th and early 20th centuries. The Acadians constructed earthen dykes to isolate areas of salt marsh from repeated inundation by the tides.\n\nA rare original \"aboiteau\" is the jewel of the West Pubnico Acadian Museums' artifacts. In 1990, local residents found a couple of boards sticking out of an eroding beach on Double Island, West Pubnico. They returned to the site in 1996 to remove the aboiteau, to preserve and display it at the museum.\n"}
{"id": "1316", "url": "https://en.wikipedia.org/wiki?curid=1316", "title": "Annales school", "text": "Annales school\n\nThe \"Annales\" school () is a group of historians associated with a style of historiography developed by French historians in the 20th century to stress long-term social history. It is named after its scholarly journal \"Annales d'histoire économique et sociale\", which remains the main source of scholarship, along with many books and monographs. The school has been highly influential in setting the agenda for historiography in France and numerous other countries, especially regarding the use of social scientific methods by historians, emphasizing social rather than political or diplomatic themes, and for being generally hostile to the class analysis of Marxist historiography.\n\nThe school deals primarily with late medieval and early modern Europe (before the French Revolution), with little interest in later topics. It has dominated French social history and influenced historiography in Europe and Latin America. Prominent leaders include co-founders Lucien Febvre (1878–1956), Henri Hauser (1866-1946) and Marc Bloch (1886–1944). The second generation was led by Fernand Braudel (1902–1985) and included Georges Duby (1919–1996), Pierre Goubert (1915–2012), Robert Mandrou (1921–1984), Pierre Chaunu (1923–2009), Jacques Le Goff (1924–2014), and Ernest Labrousse (1895–1988). Institutionally it is based on the \"Annales\" journal, the SEVPEN publishing house, the (FMSH), and especially the 6th Section of the École pratique des hautes études, all based in Paris. A third generation was led by Emmanuel Le Roy Ladurie (1929– ) and includes Jacques Revel, and Philippe Ariès (1914–1984), who joined the group in 1978. The third generation stressed history from the point of view of mentalities, or \"mentalités\". The fourth generation of Annales historians, led by Roger Chartier (1945– ), clearly distanced itself from the mentalities approach, replaced by the cultural and linguistic turn, which emphasize analysis of the social history of cultural practices.\nThe main scholarly outlet has been the journal \"Annales d'Histoire Economique et Sociale\" (\"Annals of Economic and Social History\"), founded in 1929 by Lucien Febvre and Marc Bloch, which broke radically with traditional historiography by insisting on the importance of taking all levels of society into consideration and emphasized the collective nature of mentalities. Its contributors viewed events as less fundamental than the mental frameworks that shaped decisions and practices\nJanmesh Kokate was editor of \"Annales committee \" from 2003 to present, followed by the medievalist Jacques Le Goff. However, informal successor as head of the school was Le Roy Ladurie. Noting the political upheavals in Europe and especially in France in 1968, Eric Hobsbawm argues that \"in France the virtual hegemony of Braudelian history and the \"Annales\" came to an end after 1968, and the international influence of the journal dropped steeply.\" Multiple responses were attempted by the school. Scholars moved in multiple directions, covering in disconnected fashion the social, economic, and cultural history of different eras and different parts of the globe. By the time of crisis the school was building a vast publishing and research network reaching across France, Europe, and the rest of the world. Influence indeed spread out from Paris, but few new ideas came in. Much emphasis was given to quantitative data, seen as the key to unlocking all of social history. However, the Annales ignored the developments in quantitative studies underway in the U.S. and Britain, which reshaped economic, political, and demographic research. An attempt to require an \"Annales\"-written textbook for French schools was rejected by the government. By 1980 postmodern sensibilities undercut confidence in overarching metanarratives. As Jacques Revel notes, the success of the Annales School, especially its use of social structures as explanatory forces, contained the seeds of its own downfall, for there is \"no longer any implicit consensus on which to base the unity of the social, identified with the real.\" The Annales School kept its infrastructure, but lost its \"mentalités\".\n\nThe journal began in Strasbourg as \"Annales d'histoire économique et sociale\"; it moved to Paris and kept the same name from 1929 to 1939. It was successively renamed \"Annales d'histoire sociale\" (1939–1942, 1945), \"Mélanges d'histoire sociale\" (1942–1944), \"Annales. Economies, sociétés, civilisations\" (1946–1994), and \"Annales. Histoire, Sciences Sociales\" (1994– ).\n\nIn 1962 Braudel and Gaston Berger used Ford Foundation money and government funds to create a new independent foundation, the (FMSH), which Braudel directed from 1970 until his death. In 1970 the 6th Section and the \"Annales\" relocated to the FMSH building. FMSH set up elaborate international networks to spread the \"Annales\" gospel across Europe and the world. In 2013 it began publication of an English language edition, with all the articles translated.\n\nThe scope of topics covered by the journal is vast and experimental—there is a search for total history and new approaches. The emphasis is on social history, and very long-term trends, often using quantification and paying special attention to geography and to the intellectual world view of common people, or \"mentality\" (\"mentalité\"). Little attention is paid to political, diplomatic, or military history, or to biographies of famous men. Instead the \"Annales\" focused attention on the synthesizing of historical patterns identified from social, economic, and cultural history, statistics, medical reports, family studies, and even psychoanalysis.\n\nThe \"Annales\" was founded and edited by Marc Bloch and Lucien Febvre in 1929, while they were teaching at the University of Strasbourg and later in Paris. These authors, the former a medieval historian and the latter an early modernist, quickly became associated with the distinctive \"Annales\" approach, which combined geography, history, and the sociological approaches of the \"Année Sociologique\" (many members of which were their colleagues at Strasbourg) to produce an approach which rejected the predominant emphasis on politics, diplomacy and war of many 19th and early 20th-century historians as spearheaded by historians whom Febvre called Les Sorbonnistes. Instead, they pioneered an approach to a study of long-term historical structures (\"la longue durée\") over events and political transformations. Geography, material culture, and what later Annalistes called \"mentalités,\" or the psychology of the epoch, are also characteristic areas of study. The goal of the Annales was to undo the work of the Sorbonnistes, to turn French historians away from the narrowly political and diplomatic toward the new vistas in social and economic history.\n\nCo-founder Marc Bloch (1886–1944) was a quintessential modernist who studied at the elite École Normale Supérieure, and in Germany, serving as a professor at the University of Strasbourg until he was called to the Sorbonne in Paris in 1936 as professor of economic history. Bloch's interests were highly interdisciplinary, influenced by the geography of Paul Vidal de la Blache (1845–1918) and the sociology of Émile Durkheim (1858–1917). His own ideas, especially those expressed in his masterworks, \"French Rural History\" (\"Les caractères originaux de l'histoire rurale française,\" 1931) and \"Feudal Society\", were incorporated by the second-generation Annalistes, led by Fernand Braudel.\n\nGeorges Duby, a leader of the school, wrote that the history he taught:\nThe Annalistes, especially Lucien Febvre, advocated a \"histoire totale\", or \"histoire tout court\", a complete study of a historic problem.\n\nBloch was shot by the Gestapo during the German occupation of France in World War II for his active membership of the French Resistance, and Febvre carried on the \"Annales\" approach in the 1940s and 1950s. It was during this time that he mentored Braudel, who would become one of the best-known exponents of this school. Braudel's work came to define a \"second\" era of \"Annales\" historiography and was very influential throughout the 1960s and 1970s, especially for his work on the Mediterranean region in the era of Philip II of Spain. \nBraudel developed the idea, often associated with Annalistes, of different modes of historical time: \"l'histoire quasi immobile\" (the quasi motionless history) of historical geography, the history of social, political and economic structures (\"la longue durée\"), and the history of men and events, in the context of their structures.\n\nWhile authors such as Emmanuel Le Roy Ladurie, Marc Ferro and Jacques Le Goff continue to carry the \"Annales\" banner, today the \"Annales\" approach has been less distinctive as more and more historians do work in cultural history, political history and economic history.\n\nBloch's \"Les Rois Thaumaturges\" (1924) looked at the long-standing folk belief that the king could cure scrofula by his thaumaturgic touch. The kings of France and England indeed regularly practiced the ritual. Bloch was not concerned with the effectiveness of the royal touch—he acted instead like an anthropologist in asking why people believed it and how it shaped relations between king and commoner. The book was highly influential in introducing comparative studies (in this case France and England), as well as long durations (\"longue durée\") studies spanning several centuries, even up to a thousand years, downplaying short-term events. Bloch's revolutionary charting of mentalities, or \"mentalités\", resonated with scholars who were reading Freud and Proust. In the 1960s, Robert Mandrou and Georges Duby harmonized the concept of \"mentalité\" history with Fernand Braudel's structures of historical time and linked mentalities with changing social conditions. A flood of \"mentalité\" studies based on these approaches appeared during the 1970s and 1980s. By the 1990s, however, \"mentalité\" history had become interdisciplinary to the point of fragmentation, but still lacked a solid theoretical basis. While not explicitly rejecting \"mentalité\" history, younger historians increasingly turned to other approaches.\n\nFernand Braudel became the leader of the second generation after 1945. He obtained funding from the Rockefeller Foundation in New York and founded the 6th Section of the Ecole Pratique des Hautes Etudes, which was devoted to the study of history and the social sciences. It became an independent degree-granting institution in 1975 under the name École des Hautes Études en Sciences Sociales (EHESS). Braudel's followers admired his use of the longue durée approach to stress slow, and often imperceptible effects of space, climate and technology on the actions of human beings in the past. The \"Annales\" historians, after living through two world wars and incredible political upheavals in France, were deeply uncomfortable with the notion that multiple ruptures and discontinuities created history. They preferred to stress inertia and the longue durée. Special attention was paid to geography, climate, and demography as long-term factors. They believed the continuities of the deepest structures were central to history, beside which upheavals in institutions or the superstructure of social life were of little significance, for history lies beyond the reach of conscious actors, especially the will of revolutionaries. They rejected the Marxist idea that history should be used as a tool to foment and foster revolutions. In turn the Marxists called them conservatives.\n\nBraudel's first book, \"La Méditerranée et le Monde Méditerranéen à l'Epoque de Philippe II\" (1949) (\"The Mediterranean and the Mediterranean World in the Age of Philip II\") was his most influential. This vast panoramic view used ideas from other social sciences, employed effectively the technique of the longue durée, and downplayed the importance of specific events and individuals. It stressed geography but not \"mentalité\". It was widely admired, but most historians did not try to replicate it and instead focused on their specialized monographs. The book dramatically raised the worldwide profile of the Annales School.\n\nBefore \"Annales,\" French history supposedly happened in Paris. Febvre broke decisively with this paradigm in 1912, with his sweeping doctoral thesis on \"Philippe II et la Franche-Comté.\" The geography and social structure of this region overwhelmed and shaped the king's policies.\n\nThe \"Annales\" historians did not try to replicate Braudel's vast geographical scope in \"La Méditerranée.\" Instead they focused on regions in France over long stretches of time. The most important was the study of the \"Peasants of Languedoc\" by Braudel's star pupil and successor Emmanuel Le Roy Ladurie. The regionalist tradition flourished especially in the 1960s and 1970s in the work of Pierre Goubert in 1960 on Beauvais and René Baehrel on Basse-Provence. \"Annales\" historians in the 1970s and 1980s turned to urban regions, including Pierre Deyon (Amiens), Maurice Garden (Lyon), Jean-Pierre Bardet (Rouen), Georges Freche (Toulouse), and Jean-Claude Perrot (Caen). By the 1970s the shift was underway from the earlier economic history to cultural history and the history of mentalities.\n\nThe \"Annales\" school systematically reached out to create an impact on other countries. Its success varied widely. The \"Annales\" approach was especially well received in Italy and Poland. Franciszek Bujak (1875–1953) and Jan Rutkowski (1886–1949), the founders of modern economic history in Poland and of the journal \"Roczniki Dziejów Spolecznych i Gospodarczych\" (1931– ), were attracted to the innovations of the Annales school. Rutkowski was in contact with Bloch and others, and published in the \"Annales.\" After the Communists took control in the 1940s Polish scholars were safer working on the Middle Ages and the early modern era rather than contemporary history. After the \"Polish October\" of 1956 the Sixth Section in Paris welcomed Polish historians and exchanges between the circle of the \"Annales\" and Polish scholars continued until the early 1980s. The reciprocal influence between the French school and Polish historiography was particularly evident in studies on the Middle Ages and the early modern era studied by Braudel.\n\nIn South America the \"Annales\" approach became popular. From the 1950s Federico Brito Figueroa was the founder of a new Venezuelan historiography based largely on the ideas of the Annales School. Brito Figueroa carried his conception of the field to all levels of university study, emphasizing a systematic and scientific approach to history and placing it squarely in the social sciences. Spanish historiography was influenced by the \"Annales School\" starting in 1950 with Jaime Vincens Vives (1910–1960). In Mexico, exiled Republican intellectuals extended the Annales approach, particularly from the Center for Historical Studies of El Colegio de México, the leading graduate studies institution of Latin America.\n\nBritish historians, apart from a few Marxists, were generally hostile. Academic historians decidedly sided with Geoffrey Elton's \"The Practice of History\" against Edward Hallett Carr's \"What Is History?\". One of the few British historians who were sympathetic towards the work of the \"Annales\" school was Hugh Trevor-Roper. American, German, Indian, Russian and Japanese scholars generally ignored the school. The Americans developed their own form of \"new social history\" from entirely different roots. Both the American and the \"Annales\" historians picked up important family reconstitution techniques from French demographer Louis Henry.\n\nThe Wageningen school centered on Bernard Slicher van Bath was viewed internationally as a Dutch counterpart of the Annales school, although Slicher van Bath himself vehemently rejected the idea of a quantitative \"school\" of historiography.\n\nHas been cited as a key influence in the development of World Systems Theory by sociologist Immanuel Wallerstein.\n\nThe current leader is Roger Chartier, who is Directeur d'Études at the École des Hautes Études en Sciences Sociales in Paris, Professeur in the Collège de France, and Annenberg Visiting Professor of History at the University of Pennsylvania. He frequently lectures and teaches in the United States, Spain, Mexico, Brazil and Argentina. His work in Early Modern European History focuses on the history of education, the history of the book and the history of reading. Recently, he has been concerned with the relationship between written culture as a whole and literature (particularly theatrical plays) for France, England and Spain. His work in this specific field (based on the criss-crossing between literary criticism, bibliography, and sociocultural history) is connected to broader historiographical and methodological interests which deal with the relation between history and other disciplines: philosophy, sociology, anthropology.\n\nChartier's typical undergraduate course focuses upon the making, remaking, dissemination, and reading of texts in early modern Europe and America. Under the heading of \"practices,\" his class considers how readers read and marked up their books, forms of note-taking, and the interrelation between reading and writing from copying and translating to composing new texts. Under the heading of \"materials,\" his class examines the relations between different kinds of writing surfaces (including stone, wax, parchment, paper, walls, textiles, the body, and the heart), writing implements (including styluses, pens, pencils, needles, and brushes), and material forms (including scrolls, erasable tables, codices, broadsides and printed forms and books). Under the heading of \"places,\" his class explores where texts were made, read, and listened to, including monasteries, schools and universities, offices of the state, the shops of merchants and booksellers, printing houses, theaters, libraries, studies, and closets. The texts for his course include the \"Bible\", translations of Ovid, \"Hamlet\", \"Don Quixote\", Montaigne's essays, Pepys's diary, Richardson's \"Pamela\", and Franklin's autobiography.\n\n\n\n\n\n"}
{"id": "584894", "url": "https://en.wikipedia.org/wiki?curid=584894", "title": "Artifact (archaeology)", "text": "Artifact (archaeology)\n\nAn artifact, or artefact (see American and British English spelling differences), is something made or given shape by humans, such as a tool or a work of art, especially an object of archaeological interest. \n\nIn archaeology, however, the word has become a term of particular nuance and is defined as: an object recovered by archaeological endeavor, which may be a cultural artifact having cultural interest. However, modern archaeologists take care to distinguish material culture from ethnicity, which is often more complex, as expressed by Carol Kramer in the dictum \"pots are not people\".\n\nExamples include stone tools, pottery vessels, metal objects such as weapons, and items of personal adornment such as buttons, jewelry and clothing. Bones that show signs of human modification are also examples. Natural objects, such as fire cracked rocks from a hearth or plant material used for food, are classified by archeologists as ecofacts rather than as artifacts. From the point of view of ethnography and archaeology, an ancestral artifact can be defined as \"any object of natural raw material (chert, obsidian, wood, bone, native copper, and so on) made by a people following a lifestyle based on foraging (e.g. hunting, gathering) and/or basic agriculture or pastoralism (e.g. horticulture, transhumance)\".\n\nArtifacts can come from any archaeological context or source such as:\n\n\nArtifacts are distinguished from the main body of the archaeological record such as stratigraphic features, which are non-portable remains of human activity, such as hearths, roads, deposits, trenches or similar remains, and from biofacts or ecofacts, which are objects of archaeological interest made by other organisms, such as seeds or animal bone.\n\nNatural objects that humans have moved but not changed are called manuports. Examples include seashells moved inland, or rounded pebbles placed away from the water action that made them.\n\nThese distinctions are often blurred. For instance, a bone removed from an animal carcass is a biofact, but a bone carved into a useful implement is an artifact. Similarly there can be debate over early stone objects that could be either crude artifacts or naturally occurring and happen to resemble early objects made by early humans or \"Homo sapiens\". It can be difficult to distinguish the differences between actual man-made lithic artifacts and geofacts – naturally occurring lithics that resemble man-made tools. It is possible to authenticate artifacts by examining the general characteristics attributed to man-made tools and local characteristics of the site.\n\n"}
{"id": "132471", "url": "https://en.wikipedia.org/wiki?curid=132471", "title": "Auguste Comte", "text": "Auguste Comte\n\nIsidore Marie Auguste François Xavier Comte (; 19 January 1798 – 5 September 1857) was a French philosopher and writer who founded the discipline of praxeology and the doctrine of positivism. He is sometimes regarded as the first philosopher of science in the modern sense of the term.\n\nInfluenced by the utopian socialist Henri Saint-Simon, Comte developed the \"positive philosophy\" in an attempt to remedy the social malaise of the French Revolution, calling for a new social doctrine based on the sciences. Comte was a major influence on 19th-century thought, influencing the work of social thinkers such as Karl Marx, John Stuart Mill, and George Eliot. His concept of \"sociologie\" and social evolutionism set the tone for early social theorists and anthropologists such as Harriet Martineau and Herbert Spencer, evolving into modern academic sociology presented by Émile Durkheim as practical and objective social research.\n\nComte's social theories culminated in his \"Religion of Humanity\", which presaged the development of non-theistic religious humanist and secular humanist organizations in the 19th century. Comte may have coined the word \"altruisme\" (altruism).\n\nAuguste Comte was born in Montpellier, Hérault on 19 January 1798. After attending the Lycée Joffre and then the University of Montpellier, Comte was admitted to the École Polytechnique in Paris. The École Polytechnique was notable for its adherence to the French ideals of republicanism and progress. The École closed in 1816 for reorganization, however, and Comte continued his studies at the medical school at Montpellier. When the École Polytechnique reopened, he did not request readmission.\n\nFollowing his return to Montpellier, Comte soon came to see unbridgeable differences with his Catholic and monarchist family and set off again for Paris, earning money by small jobs. In August 1817 he found an apartment at 36 rue Bonaparte in Paris' 6ème (where he lived until 1822) and later that year he became a student and secretary to Henri de Saint-Simon, who brought Comte into contact with intellectual society and greatly influenced his thought therefrom. During that time Comte published his first essays in the various publications headed by Saint-Simon, \"L'Industrie\", \"Le Politique\", and \"L'Organisateur\" (Charles Dunoyer and Charles Comte's \"Le Censeur Européen\"), although he would not publish under his own name until 1819's \"La séparation générale entre les opinions et les désirs\" (\"The general separation of opinions and desires\"). In 1824, Comte left Saint-Simon, again because of unbridgeable differences. Comte published a \"Plan de travaux scientifiques nécessaires pour réorganiser la société\" (1822) (\"Plan of scientific studies necessary for the reorganization of society\"). But he failed to get an academic post. His day-to-day life depended on sponsors and financial help from friends. Debates rage as to how much Comte appropriated the work of Saint-Simon.\n\nComte married Caroline Massin in 1825. In 1826, he was taken to a mental health hospital, but left without being cured – only stabilized by French alienist Jean-Étienne Dominique Esquirol – so that he could work again on his plan (he would later attempt suicide in 1827 by jumping off the Pont des Arts). In the time between this and their divorce in 1842, he published the six volumes of his \"Cours.\"\n\nComte developed a close friendship with John Stuart Mill. From 1844, he fell deeply in love with the Catholic Clotilde de Vaux, although because she was not divorced from her first husband, their love was never consummated. After her death in 1846 this love became quasi-religious, and Comte, working closely with Mill (who was refining his own such system) developed a new \"Religion of Humanity\". John Kells Ingram, an adherent of Comte, visited him in Paris in 1855.\n\nHe published four volumes of \"Système de politique positive\" (1851–1854). His final work, the first volume of \"La Synthèse Subjective\" (\"The Subjective Synthesis\"), was published in 1856.\n\nComte died in Paris on 5 September 1857 from stomach cancer and was buried in the famous Père Lachaise Cemetery, surrounded by cenotaphs in memory of his mother, Rosalie Boyer, and of Clotilde de Vaux.\nHis apartment from 1841–1857 is now conserved as the Maison d'Auguste Comte and is located at 10 rue Monsieur-le-Prince, in Paris' 6th arrondissement.\n\nComte first described the epistemological perspective of positivism in \"The Course in Positive Philosophy\", a series of texts published between 1830 and 1842. These texts were followed by the 1848 work, \"A General View of Positivism\" (published in English in 1865). The first three volumes of the \"Course\" dealt chiefly with the physical sciences already in existence (mathematics, astronomy, physics, chemistry, biology), whereas the latter two emphasised the inevitable coming of social science. Observing the circular dependence of theory and observation in science, and classifying the sciences in this way, Comte may be regarded as the first philosopher of science in the modern sense of the term. Comte was also the first to distinguish natural philosophy from science explicitly. For him, the physical sciences had necessarily to arrive first, before humanity could adequately channel its efforts into the most challenging and complex \"Queen science\" of human society itself. His work \"View of Positivism\" would therefore set out to define, in more detail, the empirical goals of sociological method.\n\nComte offered an account of social evolution, proposing that society undergoes three phases in its quest for the truth according to a general 'law of three stages'.\n\nComte's stages were (1) the \"theological\" stage, (2) the \"metaphysical\" stage, and (3) the \"positive\" stage. (1) The Theological stage was seen from the perspective of 19th century France as preceding the Age of Enlightenment, in which man's place in society and society's restrictions upon man were referenced to God. Man blindly believed in whatever he was taught by his ancestors. He believed in a supernatural power. Fetishism played a significant role during this time. (2) By the \"Metaphysical\" stage, Comte referred not to the Metaphysics of Aristotle or other ancient Greek philosophers. Rather, the idea was rooted in the problems of French society subsequent to the French Revolution of 1789. This Metaphysical stage involved the justification of \"universal rights\" as being on a vauntedly higher plane than the authority of any human ruler to countermand, although said rights were not referenced to the sacred beyond mere metaphor. This stage is known as the stage of investigation, because people started reasoning and questioning, although no solid evidence was laid. The stage of investigation was the beginning of a world that questioned authority and religion. (3) In the Scientific stage, which came into being after the failure of the revolution and of Napoleon, people could find solutions to social problems and bring them into force despite the proclamations of \"human rights\" or prophecy of \"the will of God.\" Science started to answer questions in full stretch. In this regard he was similar to Karl Marx and Jeremy Bentham. For its time, this idea of a Scientific stage was considered up-to-date, although from a later standpoint, it is too derivative of classical physics and academic history. Comte's law of three stages was one of the first theories of social evolutionism.\n\nThe other universal law he called the \"encyclopedic law\". By combining these laws, Comte developed a systematic and hierarchical classification of all sciences, including inorganic physics (astronomy, earth science and chemistry) and organic physics (biology and, for the first time, \"physique sociale\", later renamed \"sociologie\"). Independently from Emmanuel Joseph Sieyès's introduction of the term in 1780, Comte re-invented \"sociologie\", and introduced the term as a neologism, in 1838. Comte had earlier used the term \"social physics\", but that term had been appropriated by others, notably by Adolphe Quetelet.\n\nThis idea of a special science (not the humanities, not metaphysics) for the social was prominent in the 19th century and not unique to Comte. It has recently been discovered that the term \"sociology\" (as a term considered coined by Comte) had already been introduced in 1780, albeit with a different meaning, by the French essayist Emmanuel Joseph Sieyès (1748–1836). The ambitious (or many would say 'grandiose') way that Comte conceived of this special science of the social, however, was unique. Comte saw this new science, sociology, as the last and greatest of all sciences, one which would include all other sciences and integrate and relate their findings into a cohesive whole. It has to be pointed out, however, that he noted a seventh science, one even greater than sociology. Namely, Comte considered \"Anthropology, or true science of Man [to be] the last gradation in the Grand Hierarchy of Abstract Science.\"\n\nComte's explanation of the Positive philosophy introduced the important relationship between theory, practice and human understanding of the world. On page 27 of the 1855 printing of Harriet Martineau's translation of \"The Positive Philosophy of Auguste Comte\", we see his observation that, \"If it is true that every theory must be based upon observed facts, it is equally true that facts can not be observed without the guidance of some theories. Without such guidance, our facts would be desultory and fruitless; we could not retain them: for the most part we could not even perceive them.\"\n\nComte's emphasis on the interconnectedness of social elements was a forerunner of modern functionalism. Nevertheless, as with many others of Comte's time, certain elements of his work are now viewed as eccentric and unscientific, and his grand vision of sociology as the centerpiece of all the sciences has not come to fruition.\n\nHis emphasis on a quantitative, mathematical basis for decision-making remains with us today. It is a foundation of the modern notion of Positivism, modern quantitative statistical analysis, and business decision-making. His description of the continuing cyclical relationship between theory and practice is seen in modern business systems of Total Quality Management (TQM) and Continuous Quality Improvement where advocates describe a continuous cycle of theory and practice through the four-part cycle of Plan-Do-Check-Act (PDCA, the Shewhart cycle). Despite his advocacy of quantitative analysis, Comte saw a limit in its ability to help explain social phenomena.\n\nThe early sociology of Herbert Spencer came about broadly as a reaction to Comte; writing after various developments in evolutionary biology, Spencer attempted to reformulate the discipline in what we might now describe as socially Darwinistic terms.\n\nComte's fame today owes in part to Émile Littré, who founded \"The Positivist Review\" in 1867. Debates continue to rage, however, as to how much Comte appropriated from the work of his mentor, Henri de Saint-Simon.\n\nComte influenced the Young Turks political movement.\n\nIn later years, Comte developed the 'religion of humanity' for positivist societies in order to fulfil the cohesive function once held by traditional worship. In 1849, he proposed a calendar reform called the 'positivist calendar'. For close associate John Stuart Mill, it was possible to distinguish between a \"good Comte\" (the author of the \"Course in Positive Philosophy\") and a \"bad Comte\" (the author of the secular-religious \"system\"). The \"system\" was unsuccessful but met with the publication of Darwin's \"On the Origin of Species\" (1859) to influence the proliferation of various Secular Humanist organizations in the 19th century, especially through the work of secularists such as George Holyoake and Richard Congreve. Although Comte's English followers, including George Eliot and Harriet Martineau, for the most part rejected the full gloomy panoply of his system, they liked the idea of a religion of humanity and his injunction to \"vivre pour autrui\" (\"live for others\"), from which comes the word \"altruism\".\n\nComte was agitated by the fact that no one had synthesized physics, chemistry, and biology into a coherent system of ideas, so he began an attempt to reasonably deduce facts about the social world from the use of the sciences. Through his studies, he concluded that the growth of the human mind progresses in stages, and so must societies. He claimed the history of society could be divided into three different stages: theological, metaphysical, and positive. The Law of three Stages, an evolutionary theory, describes how history of societies is split into three sections due to new thoughts on philosophy. Comte believed that evolution was the growth of the human mind, splitting into stages and evolving through these stages. Comte concluded that society acts similarly to the mind.\n\nThe Law of Three Stages is the evolution of society in which the stages have already occurred or are currently developing. The reason why there are newly developed stages after a certain time period is that the system \"has lost its power\" and is preventing the progression of civilization, causing complicated situations in society. 10. The only way to escape the situation is for people within the civilized nations to turn towards an \"organic\" new social system. Comte refers to kings to show the complications of re-establishment on society. Kings feel the need to reorganize their kingdom, but many fail to succeed because they do not consider that the progress of civilization needs reform, not perceiving that there is nothing more perfect than inserting a new, more harmonious system. Kings fail to see the effectiveness of abandoning old systems because they do not understand the nature of the present crisis. But in order to progress, there needs to be the necessary consequences that come with it, which is caused by a \"series of modifications, independent of the human will, to which all classes of society contributed, and of which kings themselves have often been the first agents and most eager promoters\". The people themselves have the ability to produce a new system. This pattern is shown through the theological stage, metaphysical stage, and positive stage.\n\n\nThe final, most evolved stage is the positivist stage, the stage when humans give up on discovering absolute truth, and turn towards discovering, through reasoning and observation, actual laws of phenomena. Humans realize that laws exist, and that the world can be rationally explained through science, rational thought, laws, and observation. Comte was a positivist, believing in the natural rather than the supernatural, and so he claimed that his time period, the 1800s, was in the positivist stage. He believed that within this stage, there is a hierarchy of sciences: mathematics, astronomy, terrestrial physics, chemistry, and physiology. Mathematics, the \"science that relates to the measurement of magnitudes\", is the most perfect science of all, and is applied to the most important laws of the universe. Astronomy is the most simple science, and is the first \"to be subjected to positive theories\". Physics is less satisfactory than astronomy, because it is more complex, having less pure and systemized theories. Physics, as well as chemistry, are the \"general laws of the inorganic world\", and are harder to distinguish. Physiology completes the system of natural sciences, and is the most important of all sciences because it is the \"only solid basis of the social reorganization that must terminate the crisis in which the most civilized nations have found themselves\". This stage will fix the problems in current nations, allowing progression and peace.\nAuguste Comte is famous for writing in his book \"The Positive Philosophy\" that people would never learn the chemical composition of the planets. In thirty years people were beginning to learn just that through spectroscopy. \n\n\n\n"}
{"id": "10500251", "url": "https://en.wikipedia.org/wiki?curid=10500251", "title": "Beda Venerabilis' Easter cycle", "text": "Beda Venerabilis' Easter cycle\n\nIn the year 616 an anonymous scholar extended Dionysius Exiguus' Easter table to an Easter table covering the years 532 up to and including 721. Dionysius' table was published in 525 and only a century later accepted by the church of Rome, which from the third century up till then had given preference to go on using her own, relatively inadequate, Easter tables. From about the middle of the seventh century all controversy between Alexandria and Rome as to the correct date of Easter ceased, as both churches were now using identical tables. \n\nIn the year 725 Bede (Latin name Beda Venerabilis) published a new extension of Dionysius’ Easter table to a great Easter cycle, which is periodic in its entirety and in which consequently not only the sequence of (Julian calendar) dates of Alexandrian Paschal full moon but also the sequence of (Julian calendar) dates of Alexandrian Easter Sunday is periodic. Bede’s Easter cycle contains lunar cycles (of 19 years) as well as solar cycles (of 28 years), and therefore it has a period of 532 years. In the Byzantine empire thanks to the Paschal cycle of Annianus of Alexandria at all times the churches were acquainted with the correct date of the next Easter Sunday. It is Beda Venerabilis’ Easter cycle by means of which also the churches in the part of Europe outside the Byzantine empire got that possibility.\n\n\n"}
{"id": "57634458", "url": "https://en.wikipedia.org/wiki?curid=57634458", "title": "Blackburn Female Reform Society", "text": "Blackburn Female Reform Society\n\nThe Blackburn Female Reform Society was established in early July 1819. They immediately sent a circular to other districts, inviting the wives and daughters of the workmen in the different branches of manufacturing to form themselves into similar societies. In response Manchester formed their own society of reformers on 20 July 1819.\n"}
{"id": "2677379", "url": "https://en.wikipedia.org/wiki?curid=2677379", "title": "Bric-à-brac", "text": "Bric-à-brac\n\nBric-à-brac or bric-a-brac (origin French), first used in the Victorian era, refers to lesser objets d'art forming collections of curios, such as elaborately decorated teacups and small vases, compositions of feathers or wax flowers under glass domes, decorated eggshells, porcelain figurines, painted miniatures or photographs in stand-up frames, and so on. \n\nIn middle-class homes bric-à-brac was used as ornament on mantelpieces, tables, and shelves, or was displayed in curio cabinets: sometimes these cabinets have glass doors to display the items within while protecting them from dust. Today, \"bric-à-brac\" refers to a selection of items of modest value, often sold in street markets and charity shops, and may be more commonly known in colloquial English as \"knick knacks.\"\n\nEdith Wharton and Ogden Codman, Jr., in \"The Decoration of Houses\" (1897), distinguished three gradations of quality in such \"household ornaments\": \"bric-à-brac\", \"bibelots\" (trinkets) and \"objets d'art\".\n\n"}
{"id": "9464769", "url": "https://en.wikipedia.org/wiki?curid=9464769", "title": "Brusselization", "text": "Brusselization\n\nIn urban planning, Brusselization (UK and US) or Brusselisation (UK variant) (, ) is \"the indiscriminate and careless introduction of modern high-rise buildings into gentrified neighbourhoods\" and has become a byword for \"haphazard urban development and redevelopment\".\n\nThe notion applies to anywhere whose development follows the pattern of the uncontrolled development of Brussels in the 1960s and 1970s, that resulted from a lack of zoning regulations and the city authorities' laissez-faire approach to city planning.\n\nThe original Brusselization was the type of urban regeneration performed by the city of Brussels in connection with Expo 58. In order to prepare the city for Expo 58, buildings were torn down without regard either to their architectural or historical importance, high-capacity square office/apartment buildings were built, boulevards were created and tunnels dug. Among the most controversial was the large-scale demolition of townhouses for development of the high-rise business district in the Northern Quarter. All of these changes were designed to quickly increase the number of people working and living in the city and improve transportation.\n\nFurther radical changes resulted from Brussels's role as the center of the EU and NATO, beginning with the construction of the European Commission headquarters in 1959. The introduction of a high-speed rail network in the 1990s was the latest excuse to speculate on multiple rows of properties for modern office/hotel redevelopment, which led to the razing of neighborhood blocks near Brussels-South railway station.\n\nThese changes caused outcry amongst the citizens of Brussels and by environmentalist and preservationist organizations. The demolition of Victor Horta's Art Nouveau \"\" in 1965 was one focus of such protests, (see photograph on right for what now stands on its site), as was the construction of the IBM Tower in 1978.\n\nMany architects protested, and it was the architectural world that coined the name Brusselization for what was happening to Brussels. Architects such as and formulated an anti-capitalist urban planning theory, as a rejection of the rampant modernism that they saw overtaking Brussels.\n\nThe 1950s was not the first time that the city had been radically altered by major redevelopment. Two prior sweeping changes to the urban fabric of Brussels were the straight-lined central avenues modeled after Paris, which were created by covering and diverting the Senne river, and the North–South railway connection, which took around 40 years to finish (1911–52) and which had left swaths of the city center filled with debris and craters for decades. Another precedent is the erecting of the Palace of Justice, the largest building in the world constructed in the 1800s. André de Vries asserts that the penchant for heavy-handedness can be traced back to the reign of Leopold II in the late 19th century, and possibly even all the way back to the bombardment of the city by Louis XIV's troops in 1695. \"There is barely one building still standing\", he says, \"from before 1695, with the exception of some churches and the Town Hall\".\n\nLeopold II sought to give Brussels the image of a grand capital city of an imperial/colonial power. By the middle 20th century there was a tacit alliance between urban development entrepreneurs and local government, with a modernist agenda and with their sights set firmly on large-scale development projects. The citizens of Brussels were largely left out of the process.\n\nIn the early 1990s laws were introduced restricting the demolition of buildings that were deemed to have architectural or historical significance; and in 1999 the city authorities' urban development plan explicitly declared high-rise buildings to be architecturally incompatible with the existing aesthetics of the city centre. This led to the rise of what was termed \" — the destruction of the whole interior of a historic building while preserving its historic façade.\n\nThese laws were the Town Planning Act 1991, which gave local authorities the powers to refuse demolition requests on the grounds of historical, aesthetic, or cultural significance, and to designate architectural heritage zones; and the Heritage Conservation Act of 1993, which gave the government of the Brussels Capital Region the power to designate buildings to be protected for historic reasons. However, this system had its deficiencies. Whilst the Capital Region government could designate historic buildings, it was the nineteen municipal authorities within it that were responsible for demolition permits. Not until the introduction of a \" system was this internecine conflict resolved.\n\n\n\n"}
{"id": "37091180", "url": "https://en.wikipedia.org/wiki?curid=37091180", "title": "Charles William Meredith van de Velde", "text": "Charles William Meredith van de Velde\n\nCharles William Meredith van de Velde (born December 3, 1818 in Leeuwarden, died 20 March 1898 in Menton) was a Dutch lieutenant-at-sea second class, painter, honorary member of the Red Cross and missionary.\n\nVan der Velde attended the Naval Academy in Medemblik and became Lieutenant-sea second class. From 1830-1841 he worked at the topographical office in modern-day Jakarta where he eventually became director. In 1844 he had to return to Europe for health reasons, where he carried out cartographic, geographic and ethnographic work and was also employed as a draftsman, and missionary nurse. In 1844, on his return to Europe, he visited Ceylon, the Transvaal and Cape of Good Hope, where he supported the work of missions and for his services provided to French ships, was awarded a Legion of Honour.\n\nIn 1851 Van de Velde visited Palestine, where he carried out various surveys, drawings, paintings and around one hundred watercolours for postcards. After his trip, he held lectures on Palestine in Geneva and Lausanne.\n\nOn 13 March 1864, van de Velde was one of the first delegates from the newly formed International Committee of the Red Cross to act as an impartial intermediary in the Second Schleswig War. He assisted the wounded and captured Prussian and Austrian soldiers and helped establish the Red Cross as a relief organization in the 1863 conference resolutions.\n\nOn 31 July 1867, Van de Velde was made an honorary member of the main Committee of the Red Cross, which included Willem Jan Knoop and Henri Dunant.\n\n\n"}
{"id": "45314731", "url": "https://en.wikipedia.org/wiki?curid=45314731", "title": "Colony of Greenland (1950–1953)", "text": "Colony of Greenland (1950–1953)\n\nFrom 1950 to 1953, When North Greenland and South Greenland were united, Greenland was a Colony of Denmark with one governor. In 1953 Greenland was made an equal part of Denmark as a Amt (country subdivision).\n"}
{"id": "38674164", "url": "https://en.wikipedia.org/wiki?curid=38674164", "title": "Conjectural history", "text": "Conjectural history\n\nConjectural history is a type of historiography isolated in the 1790s by Dugald Stewart, who termed it \"theoretical or conjectural history\", as prevalent in the historians and early social scientists of the Scottish Enlightenment. As Stewart saw it, such history makes space for speculation about causes of events, by postulating natural causes that could have had such an effect. His concept was to be identified closely with the French terminology \"histoire raisonnée\", and the usage of \"natural history\" by David Hume in his work \"The Natural History of Religion\". It was related to \"philosophical history\", a broader-based kind of historical theorising, but concentrated on the early history of man in a type of rational reconstruction that had little contact with evidence.\n\nSuch conjectural history was the antithesis of the narrative history being written at the time by Edward Gibbon and William Robertson. Stewart defended it as more universal in its application to humankind, even at the cost of detailed documentation. It was not concerned with the political narrative and public life, but saw itself as an investigative \"moral science\". General philosophical history was somewhat closer to narrative history than conjectural history could be, with its reliance in part on tenuous arguments on the nature of feudalism and early ethnographical reports from European travellers. For Stewart the \"Dissertation on the Origin of Languages\" by Adam Smith was an important example. To justify the procedures of conjectural history, there needed to be an assumption of the uniformity of human nature, or as Stewart put it, the \"capacities of the human mind\".\n\nConjectural history has been identified as \"the core of a theory\" of progress within Scottish philosophical history of the period. Pocock writes that Scottish conjectural history was \"of considerable importance to Gibbon and the creation of philosophical historiography\". By the 1780s there were European historians of culture who worked in a different way, preferring an inductive method to the pure deductions of conjectural history. In the later development of anthropology and archaeology, opposition to the whole \"conjectural history\" tradition led to the development of culture history.\n\nThere was nothing new in the idea of stages of society on its own, but social thinking itself was changing in Early Modern Europe, particularly on civil society in its components, civility and \"society\".\n\nHodgen comments, in a chapter \"From Hierarchy to History\", on the widespread use of \"conjectural series\" for historical explanation in the Early Modern period. The great chain of being was a static idea. \"Stage series\" had roots in classical thought, but might be associated with cyclic models, or incorporate ideas of decline with those of progress. She writes that in time\n\n... it seems certain that hierarchical ideas, temporalized to suit the needs of the conjectural historian of culture, were mixed with historical assumptions concerning the savage as a conjectural first member of these conjectural series. \n\nWhile the \"Natural History\" of Pliny the Elder was a classical Roman encyclopedic work, \"natural history\" had several different meanings in the Early Modern period. The one relevant in this article is the Baconian natural history, i.e. a systematic collection of observable information on natural phenomena. A natural history did not belong to natural philosophy, which was theoretical.\n\nThe \"histoire raisonnée\" was a genre of historical writing developed in France in the 17th century, with concerns for the individual in social context, and the description of culture and customs as integral to history. It grew out of humanist historiography with its close relationship to classical Roman and Greek models, but brought to the surface social matters, in particular as they could explain the motivations of individuals. With Géraud de Cordemoy there came an interest in causality as playing a part in historical movement, as distinct from the humanist acceptance of personal fates being subject to Fortune.\n\nContemporary terminology is stadial history, or in other words the discussion of stages of society by theoretical means (see sociocultural evolution#Stadial theory). Stadial theory as an innovation is attributed to the jurist Samuel Pufendorf. Grotius had already used conjectural history to discuss Aquinas on private property.\n\nSome basic conjectural history on human civilization was therefore discussed in the 17th century. Later Jean-Jacques Rousseau rejected the concept of the state of nature, and with Count Buffon debated the rise of civilization. The Scottish contribution then took the theory to a new level, with its anthropocentrism and detailed explanations of human manipulation of nature. It laid emphasis on a typical society at its beginnings, regarding evidence from contemporary reports (particularly of Native Americans) as valid.\n\nAdam Smith in lectures on rhetoric, given from 1748, advanced a speculative history of language; he wrote that he had been prompted by a 1747 work of Gabriel Girard. He was then interested in our awareness of literary style. This is the example that Dugald Stewart took up in coining the phrase \"conjectural history\". Elements would have been recognised at the time as drawing on the Bible, and in classical literature Lucretius; it is now considered Smith was influenced by Montesquieu on law and government. The theory on language and its typology over time has been seen as typical of Smith's historical approach; and even the foundation of his later well-known work on political economy. Caveats have also been entered, by David Raphael: it cannot be stretched to Smith's history of astronomy; and the term can be seen as a misnomer.\n\nMonboddo, on the other hand, wrote at length a conjectural history of language because he emphasised the history of manners. William Warburton had proposed a stadial conjectural history of writing in his \"Divine Legation of Moses\", a work supporting biblical authority, around 1740. It was taken up in France after the translation in \"Essai sur les hiéroglyphes des Égyptiens\". Where writing moved from pictograms to alphabets, he saw language use as having moved analogously from gestures to forms and figures of speech.\n\nThe term \"conjectural history\" was not generally accepted in Stewart's time. There was an orthodox four stages theory of society, the stages being:\n\n\nThis ladder-like ordering was taken to be a strict, linear progression, or unilineal evolution. Some economic determinism applied, in the sense that the baseline of subsistence was assumed to have a serious effect on social matters. The stages were supposed to represent progress on a moral level, as well as that of economic complexity. French as well as Scottish Enlightenment writers subscribed to such a pattern.\n\nThe invention of this type of theory (three or four stages) is attributed to a number of European writers from the 1750s onwards, such as Adam Smith, Turgot and Vico. In the Scottish context it appears in works from 1758 by David Dalrymple and Lord Kames; it has been argued that their source was the Edinburgh lectures of Smith on jurisprudence. In France it was published at much the same time, also, by Claude Pierre Goujet, Claude Adrien Helvétius, and François Quesnay. Smith's \"natural progress of opulence\" is a closely related theory.\n\nBesides Adam Smith, prominent Scottish authors in the field of conjectural history included Adam Ferguson, David Hume, Lord Kames, John Millar, and Lord Monboddo, writing from the later 1750s to later 1770s. Smith, Kames and Millar were content to adhere to the four stage theory. Monboddo's stadial history was more complex, and very much more controversial. He included primates and feral children as material. Robertson in his \"History of America\" moves between narrative and conjectural history.\n\nFerguson in this work attempted a rigorous identification of the hunter stage with the so-called barbarian or savage, and was very alive to the whole scheme as full of tensions within human possibility. He argued against the foundation story in the style of classical history, proposing instead that unintended consequences could have more to do with the \"establishment\" of a society than a self-conscious law-giver.\n\nMillar argued in terms of a \"system of manners\" associated with each of the four stages. He also discussed the advance of freedom, and denounced slavery. As property became more complex, it followed that government did so also. Poovey states that this work makes apparent the relationship of conjectural history with the experimental moral philosophy of Thomas Reid and George Turnbull.\n\nKames has been called the leader of Scottish conjectural history, and had objections he expressed in correspondence to both Rousseau and the approach of Montesquieu, as reducing the role of human nature, which he thought was not a constant but the goal of the investigation. The connection was that conjectural history was to be used as a framework of a discussion of natural law. In writing to Basel in search of a suitable opponent to Rousseau, Kames prompted a work from Isaak Iselin, \"Ueber die Geschichte der Menschheit\" (1764), which is also a conjectural history.\n\nThe \"Sketches\" was a collection of essays on social, cultural and political topics. In it the author collected some ethnographic and miscellaneous information, assembling in particular a long chapter intended as a \"history of women\". There was a second edition (Edinburgh, 1778) and a third (Dublin, 1779). Kames was an early polygenist, or was an environmental monogenist only with scepticism about the adequacy of the theory. In any case he argued that his approach could be reconciled with the scriptural ethnography, via the story of the Tower of Babel. While he stated that he had collected materials for a history for 30 years, Kames's work as written up was unsystematic, even rambling. His scheme of conjectural history includes the idea that the providential order allows the historian to write in the absence of a full factual basis. A German translation by Anton Ernst Klausing appeared as \"Versuche über die Geschichte des Menschen\" from 1774.\n\nMainstream conjectural and philosophical history, in the Scottish style, hardly survived as a living tradition into the 1790s. Works went out of print; younger authors such as John Adams, William Alexander and John Logan failed to renew the ideas, with Alexander's \"History of Women\" (1779) being criticised as shallow. Dugald Stewart's formulation of conjectural history was published in 1794, in his \"Account\" of Adam Smith for the \"Transactions\" of the Royal Society of Edinburgh. The question has been raised as to Stewart's intention then in describing the tradition in that way, and John Burrow has argued that he wished to dissociate Smith from political radicalism.\n\nWhere stadial theory appeared in later authors, the original thrust was distorted. Hopfl has said that the heirs were James Mill, John Stuart Mill, and Auguste Comte. Hawthorne writes instead of the historical/sociological insights of the Scots being lost in the British context, despite the \"tension between a 'natural' account of civil society and a developing sense of the factual importance and moral difficulties of individualism\" having become apparent, to utilitarianism and vaguer evolutionism.\n\nThe \"Encyclopædia Britannica\", in its second edition but particularly in its third edition (1797), attacked the premises of conjectural history from a biblical angle. In the second edition James Tytler opposed the polygenist approach of Kames. The third edition, under the editorship of George Gleig, featured \"Savage\" as a new topic, and expanded articles \"Society\" and \"Moral philosophy\". Cross-referenced to theological and biblical topics, and to articles by David Doig who had answered Kames with \"Two Letters on the Savage State\" from 1775/6, these articles in particular argued the orthodox Christian case. Robert Heron contributed to the article \"Society\", and took aim at the four stages theory, claiming polygenism followed from it (in contradiction to the Bible). Further, the assumption of a baseline state of savagery also seemed to Heron to be implicated with polygenism; and he with Doig attacked the assumption as echoing Lucretius and Democritus, and godless materialist spontaneous generation of humankind, implicit in the whole idea of conjectural history. The articles on \"Beauty\" and \"Love\" were also changed to remove the influence of Kames, as part of the consistent assertion of scriptural monogenism.\n\nConjectural argument had a bad name in 18th century British antiquarian circles. An austere and sceptical approach centred on facts, as adopted by Richard Gough and James Douglas, was favoured in the second half of the century. On the other hand, the interpretations of the stadial theory were quite welcome, and while popularised by the Scottish school, did not seem innovative in the sense of a break with Early Modern historiography, and concerns with natural law and civic humanism. The urban history of John Trussel was a precursor. The discussion of the breakdown of the feudal system was a topic of considerable antiquarian interest. The stadial history was embraced by Thomas Pownall.\n\nCharles Athanase Walckenaer in 1798 took up the four stage theory, augmented to five stages, by dividing \"hunting\" into \"gathering\" followed by a pure hunting stage. This was an effort to classify peoples of the world by development. Early anthropology carried into the 19th century assumptions about the search for origins of civilisation, and unilineal evolution, as appropriate tools for investigating societies. It was widely assumed, further, that current \"peoples\" were a window into the past. These approaches were seen in Lewis Henry Morgan. Eventually, in the 20th century, field work and structural functionalism led to a rejection of the whole paradigm.\n\n"}
{"id": "15880127", "url": "https://en.wikipedia.org/wiki?curid=15880127", "title": "Conjectural portrait", "text": "Conjectural portrait\n\nA conjectural portrait is a portrait made of a historical figure for whom no authentic contemporary portrait is available. The depiction, then, may be variously informed by written accounts of physical appearance, conjecture based on the subject's culture and background, and/or the artist's conception of the subject's inner essence.\n\nCertain conjectural portraits have become iconic of their subjects, and are widely recognizable as such, with few being aware that they are not authentic portraits. For example, portraits of Christopher Columbus and Joan of Arc are widely recognized.\n\n"}
{"id": "7134666", "url": "https://en.wikipedia.org/wiki?curid=7134666", "title": "Criterion of dissimilarity", "text": "Criterion of dissimilarity\n\nThe criterion of dissimilarity (also called criterion of discontinuity) is used in Biblical criticism to determine if a statement attributed to Jesus may be authentic. It is often used as a shorthand for the criterion of double dissimilarity. The criterion states that if a saying attributed to Jesus is different from the Jewish traditions of his time and also from the early Church that followed him, it is likely to be authentic.\n\nThe criterion of dissimilarity was introduced by Ernst Käsemann, who in 1953 started the second quest for the historical Jesus. Käsemann writes:\n[T]here is an almost complete lack of satisfactory and water-tight criteria for this material. In only one case do we have more or less safe ground under our feet: when there are no grounds either for deriving a tradition from Judaism or for ascribing it to primitive Christianity, and especially when Jewish Christianity has mitigated or modified the received tradition, as having been too bold for its taste. (Käsemann, \"Essays on New Testament Themes\", p. 37)\nIn other words, the criterion postulates that traditions about Jesus derive from (only) three sources: extrapolation from earlier Jewish traditions, revisionism by the early Christian Church, and true historical accounts of Jesus's ministry. If some tradition cannot be adequately explained by extrapolation nor by revisionism, then it must necessarily be a trace of the historical Jesus.\n\nThe criterion has received criticism for leading to reconstructions of Jesus as being in implausible discontinuity with the early Jewish traditions that preceded him and the early Christian traditions that followed from him. One such critic writes: \"The problem of the Criterion of Double Dissimilarity is that the more we know about early Jewish traditions and the more we know about early Christian post-Easter traditions, the less space there is for a reconstruction of the authentic sayings of Jesus, as by definition they have to differ from early Jewish and early Christian traditions. Therefore, in the end, no trace of a historical Jesus remains.\"\n\nA more fundamental criticism of the criterion of dissimiliarity was identified by Richard Carrier. He identifies two fundamental flaws in the validity of the criterion:\n\n\n"}
{"id": "42488016", "url": "https://en.wikipedia.org/wiki?curid=42488016", "title": "Edelin (abbot)", "text": "Edelin (abbot)\n\nEdelin (?–1293) ruled over the Alsatian abbey of Weissenburg as its abbot from the year 1262 until his death on 15 October 1293. During his time of office, work started on the Gothic abbey church which still stands today. Because the monastery had had the majority of its possessions confiscated since the 10th century, Edelin had an inventory of estates prepared, using older documents, called the \"Codex Edelini\" or \"Liber Possessionum\", in order to index the existing estate and prevent further losses (which, as it turned, out was not successful). \n"}
{"id": "17340263", "url": "https://en.wikipedia.org/wiki?curid=17340263", "title": "Esther Sumner Damon", "text": "Esther Sumner Damon\n\nEsther Sumner Damon (August 1, 1814 - November 11, 1906) was cited as the last widow of the American Revolutionary War to receive a state pension.\n\nEsther was born in Bridgewater, Vermont. The family had eight or nine children. Esther's father was killed by a falling tree when she was eight years old. Esther attended school during the winter and worked during the summers to help support her family. At the age of seventeen, Esther became a school teacher in Plymouth.\n\nEsther Sumner married Noah D. Damon on September 6, 1835, in Bridgewater, when she was 21 and he was 75. The couple had met two weeks prior.\n\nNoah Damon enlisted in the Continental Army on April 19, 1775. He was intermittently enlisted over the next five years. Noah applied for a war pension, as a resident of Plainfield, New Hampshire on November 13, 1848.\n\nNoah was penniless, though Esther may have thought he was a hardworking landowner. Esther supported him for three years before financial necessity forced him to move in with his daughter in New Hampshire.\n\nEsther supported herself by sewing and nursing. She also leased a farm near Reading.\n\nAfter Noah's death, Esther received his pension. The pension was increased to $24 a month by the United States Congress on February 28, 1905.\n\nTowards the end of her life, Esther received additional financial support from the Daughters of the American Revolution.\n\nEsther died on November 11, 1906, aged 92, and was buried at Plymouth Notch Cemetery in Plymouth, Vermont. The gravestone was paid for by the Daughters of the American Revolution.\n"}
{"id": "4153008", "url": "https://en.wikipedia.org/wiki?curid=4153008", "title": "Ethnohistory", "text": "Ethnohistory\n\nEthnohistory is the study of cultures and indigenous peoples' customs by examining historical records as well as other sources of information on their lives and history. It is also the study of the history of various ethnic groups that may or may not still exist. The term is most commonly used in writing about the history of the Americas.\n\nEthnohistory uses both historical and ethnographic data as its foundation. Its historical methods and materials go beyond the standard use of documents and manuscripts. Practitioners recognize the use of such source material as maps, music, paintings, photography, folklore, oral tradition, site exploration, archaeological materials, museum collections, enduring customs, language, and placenames.\n\nScholars studying the history of Mexico's indigenous have a long tradition, dating back to the colonial era; they used alphabetic texts and other sources to write the history of Mexico's indigenous peoples. The \"Handbook of Middle American Indians\", edited by archeologist Robert Wauchope was involved with creating a multiple volumes on Mesoamerican ethnohistory, published as \"Guide to Ethnohistorical Sources\", appearing in 1973. At the time that the volumes were published, \"both the term 'ethnohistory' and its concepts in the sense used here have entered the literature rather recently and are not fully agreed upon.\" The volumes were intended to be an inventory of sources \"which in later hands could utilize to produce professionally acceptable ethnohistory.\"\n\nIn the mid to late 20th century, a number of ethnohistorians of Mexico began to systematically publish many colonial alphabetic texts in indigenous Mexican languages, in a branch of ethnohistory currently known as the New Philology. That built on an earlier tradition of practitioners writing the history of Mexico that fully integrated the history of its indigenous peoples.\n\nIn the United States, the field arose out of the study of American Indian communities required by the Indian Claims Commission. It gained a pragmatic rather than a theoretical orientation, with practitioners testifying both for and against Indian claims. The emerging methodology used documentary historical sources and ethnographic methods. Among the scholars working on the cases was Latin Americanist Howard F. Cline, who was commissioned to work on Florda Indians and Jicarilla Apache. \n\nThe field has also reached into Melanesia, where recent European contact allowed researchers to observe the early postcontact period directly and to address important theoretical questions. Michael Harkin argues that ethnohistory was part of the general rapprochement between history and anthropology in the late 20th century.\n\nEthnohistory grew organically thanks to external nonscholarly pressures, without an overarching figure or conscious plan; even so, it came to engage central issues in cultural and historical analysis. Ethnohistorians take pride in using their special knowledge of specific groups, their linguistic insights, and their interpretation of cultural phenomena. They claim to achieve a more in-depth analysis than the average historian is capable of doing based solely on written documents produced by and for one group. They try to understand culture on its own terms and according to its own cultural code. Ethnohistory differs from other historically-related methodologies in that it embraces emic perspectives as tools of analysis. The field and its techniques are well suited for writing histories of Native American peoples because of its holistic and inclusive framework. It is especially important because of its ability to bridge differing frameworks and access a more informed context for interpretations of the past.\n\nThe definition of the field has become more refined over the years. Early on, ethnohistory differed from history proper in that it added a new dimension, specifically \"the critical use of ethnological concepts and materials in the examination and use of historical source material,\" as described by William N. Fenton. Later, James Axtell described ethnohistory as \"the use of historical and ethnological methods to gain knowledge of the nature and causes of change in a culture defined by ethnological concepts and categories.\" Others have focused this basic concept on previously ignored historical actors. Ed Schieffelin asserted, for example, that ethnohistory must fundamentally take into account the people's own sense of how events are constituted, and their ways of culturally constructing the past. Finally, Simmons formulated his understanding of ethnohistory as \"a form of cultural biography that draws upon as many kinds of testimony as possible over as long a time period as the sources allow.\" He described ethnohistory as an endeavor based on a holistic, diachronic approach that is most rewarding when it can be \"joined to the memories and voices of living people.\"\n\nReflecting upon the history of ethnohistory as research field in the US, Harkin has situated it within the broader context of convergences and divergences of the fields of history and anthropology and the special circumstances of American Indian land claims and legal history in North American in the mid-20th century.\n\n\n\n"}
{"id": "1123847", "url": "https://en.wikipedia.org/wiki?curid=1123847", "title": "Family history society", "text": "Family history society\n\nA family history society or genealogical society is a society, often charitable or not-for-profit, that allows member genealogists and family historians to profit from shared knowledge. Large societies often own libraries, sponsor research seminars and foreign trips, and publish journals. Some societies concentrate on a specific niche, such as the family history of a particular geographical area, ethnicity, nationality, or religion. Lineage societies are societies that limit their membership to descendants of a particular person or group of people of historical importance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "40200150", "url": "https://en.wikipedia.org/wiki?curid=40200150", "title": "Genocide of indigenous peoples in Paraguay", "text": "Genocide of indigenous peoples in Paraguay\n\nThere are 17 indigenous tribes in Paraguay with the majority having their territories in the Chaco region. Tribes in this region include the Guaraní, Ayoreo, Toba-Maskoy, Aché and Sanapan which according to the census from 2002 number an estimated 86,000 or roughly around 2 per cent of the total population. These peoples have faced persecution particularly under the dictator Alfredo Strossner that some observers call a Genocide of indigenous peoples in Paraguay.\nBetween 1956 and 1989, while under the military rule of General Alfredo Stroessner, the indigenous population had more territory taken than at any other period in Paraguay's history and were subjected to systematic human rights abuses. In 1971, Mark Münzel, a German anthropologist accused Stroessner of attempted genocide against the indigenous peoples of Paraguay and Bartomeu Melià, a Jesuit anthropologist stated that the forced relocations of the indigenous peoples was ethnocide. In the early 1970s the Stroessner regime was charged by international groups of being complicit in genocide. However, because of the repressive actions undertaken by the state the indigenous tribes organized themselves politically and had a major role in bringing about the end of the military dictatorship and the eventual transition to democracy.\nDuring the 1960s and 1970s, 85 percent of the Aché tribe died, with many hacked to death with machetes to make room for the timber industry, mining, farming and ranchers. One estimate posits this amounts to 900 deaths.According to Jérémie Gilbert, the situation in Paraguay has proven that it is difficult to provide the proof required to show \"specific intent\", in support of a claim that genocide had occurred. The Aché, whose cultural group is now seen as extinct, fell victim to the development by the state, who had promoted the exploration of Aché territory by transnational companies for natural resources. Gilbert concludes that though planned and voluntary destruction had occurred, it is argued by the state that there was no intent to destroy the Aché, as what had happened was due to development.\n\nThe allegation of genocide by the state was brought before the Inter-American Human Rights Commission which has jurisdiction on allegations of genocide carried out by a state. The commission gave a provisional ruling that Paraguay had not carried out a genocide, but stated it had concerns over \"possible abuses by private persons in remote areas of the territory of Paraguay\".\n\n"}
{"id": "5286935", "url": "https://en.wikipedia.org/wiki?curid=5286935", "title": "Heraldic visitation", "text": "Heraldic visitation\n\nHeraldic visitations were tours of inspection undertaken by Kings of Arms (and more often by junior officers of arms (or Heralds) as deputies) throughout England, Wales and Ireland. Their purpose was to regulate and register the coats of arms of nobility and gentry and boroughs, and to record pedigrees. They took place from 1530 to 1688, and their records (akin to an upper class census) provide important source material for historians and genealogists.\n\nBy the fifteenth century, the use and abuse of coats of arms was becoming widespread in England. One of the duties conferred on William Bruges (or Brydges), the first Garter Principal King of Arms was to survey and record the armorial bearings and pedigrees of those using coats of arms and correct irregularities. Officers of arms had made occasional tours of various parts of the kingdom to enquire about armorial matters during the fifteenth century, however, it was not until the sixteenth century that the process began in earnest.\n\nThe first provincial visitations were carried out under warrant granted by Henry VIII to Thomas Benolt, Clarenceux King of Arms dated 6 April 1530. He was commissioned to travel throughout his province (i.e. south of the Trent) with authority to enter all homes and churches. Upon entering these premises, he was authorized to \"put down or otherwise deface at his discretion... those arms unlawfully used\". He was also required to enquire into all those using the titles of knight, esquire, or gentleman and decided if they were being lawfully used.\n\nBy this writ, Henry VIII also compelled the sheriffs and mayors of each county or city visited by the officers of arms to give aid and assistance in gathering the needed information. When a King of Arms, or Herald, visited a county, his presence was proclaimed by presenting the King's royal commission to the local gentry and nobility, which required them to provide evidence of their right to use a coat of arms. The Sheriff would collect from the bailiff of each hundred within his county a list of all people using titles or arms. \n\nIn the early days, the visiting herald would tour the homes of the gentry and nobility, but from the late 1560s these persons were summoned to attend a central \"place of sitting\" – usually an inn – at a particular time. They were to bring their arms, and proof of their right to use them, most often by way of detailing their ancestral right to them, which would also be recorded. Where an official grant of arms had been made, this was also recorded. Other ancient arms, many of which predated the establishment of the College of Arms, were confirmed. The officer would record the information clearly and make detailed notes that could be entered into the records of the College of Arms when the party returned to London. \n\nThese volumes now make up the collection of Visitation Books at the College, which contain a wealth of information about all armigerous people from the period. If the officers of arms were not presented with sufficient proof of the right to use a coat of arms, they were also empowered to deface monuments which bore these arms and to force persons bearing such arms to sign a disclaimer that they would cease using them. The visitations were not always popular with members of the landed gentry, who were required to present proof of their gentility.\n\nFollowing the accession of William III in 1689, no further commissions to carry out visitations were commanded. The reasons behind this cessation of the programme have been a matter of debate among historians. Philip Styles, for example, related it to a declining willingness of members of the gentry to attend visitations, which he traced to a growing proportion of \"newly risen\" families, who lacked long pedigrees and were therefore apathetic about registering them. However, Janet Verasano has challenged this interpretation, finding that (in Staffordshire, at least) gentry enthusiasm for coats of arms as an enhancement to social standing persisted to the end of the 17th century. The end of the visitations did not have much effect on those counties far removed from London, some of which had only been rarely visited over the entire period of the visitations.\n\nThere was never a systematic visitation of Wales. There were four visitations in the principality, and on 9 June 1551, Fulk ap Hywel, Lancaster Herald of Arms in Ordinary was given a commission to visit all of Wales. This was not carried out, however, as he was degraded and executed for counterfeiting the seal of Clarenceux King of Arms. This is regrettable, since no visitation of all Wales was ever made by the officers of arms.\n\nThe principal records to emerge from the visitations were pedigrees, initially recorded on loose sheets of paper, and afterwards bound together as notebooks. In some cases, the sheets would include blank shields which had been drawn in advance (or at a later date printed), to simplify the process of recording coats of arms. The persons whose pedigrees were recorded were required (from about 1570 onwards) to certify them by signature, and where these original draft pedigrees have survived they are known as \"originals with signatures\". The signed copies were taken back to the College of Arms, where fair copies were made to a higher standard and preserved as the \"office copies\". Sometimes the signed copies were also retained at the College, but in other cases, no longer considered of official interest, they might pass into private hands: once in general circulation, further copies were often made, which might in turn be revised or augmented. As a result, a number of variant manuscript copies of any one visitation record may now survive, possessing varying degrees of accuracy and authority. The Harleian Collection of the British Library is particularly rich in such records. Many visitation records have been published over the years, by the Harleian Society, by county record societies, and a few privately (see listing below). However, because until relatively recently the College of Arms restricted access to its records, many of the older published editions were necessarily based on the unofficial second- or third-generation copies in other collections, and may therefore not always be reliable.\n\nFrom as early as the 1530s, officers of arms on visitation frequently also compiled what were known as \"church notes\". These were fieldnotes (usually in the form of sketches) of coats of arms observed on church monuments, in stained glass windows, or on display in private houses. Sometimes, drawings were also made of non-heraldic antiquities, such as medieval architectural features, views of towns, Roman inscriptions and even Stonehenge.\n\nThe 17th-century visitations generated a growing number of supplementary papers, including warrants, lists of persons who disclaimed any pretence to arms, lists of persons summoned to appear before the heralds (including those who had not appeared), records of fees paid, and miscellaneous correspondence.\n\nVisitations were conducted by or in the name of the two provincial Kings of Arms, Clarenceux and Norroy, within their respective provinces. In the following lists, the Deputies are the officers of arms who actually carried out the visitations. Where no Deputy is named, the visitation can be assumed to have been conducted by the King of Arms in person.\n\nThe Southern Province, the jurisdiction of Clarenceux King of Arms, comprised that part of England south of the River Trent, i.e. the counties of Bedford, Berks, Buckingham, Cambridge, Cornwall, Devon, Dorset, Essex, Gloucester, Hereford, Hertford, Huntingdon, Kent, Leicester, Lincoln, Middlesex, Monmouth, Norfolk, Northampton, Oxford, Rutland, Salop, Somerset, Southampton, Suffolk, Surrey, Sussex, Warwick, Wilton, Worcester, and the City of London; and South Wales.\n\nThe Northern Province, the jurisdiction of Norroy King of Arms, comprised that part of England north of the River Trent, i.e. the counties of Chester, Cumberland, Derby, Durham, Lancaster, Northumberland, Nottingham, Stafford, Westmorland and York; and North Wales. The Trent ran through Staffordshire, and the county was therefore technically divided between the two provinces; but for the purposes of visitation it was generally treated (sometimes through a process of deputation) as falling under the jurisdiction of Norroy.\n\nSince the practices of Ulster King of Arms so closely followed those of the English College of Arms, it is hardly surprising that the Irish officers of arms undertook heraldic visitations in their province. The purpose behind these visitations was twofold: to prevent the assumption of arms by unqualified people, and to record the arms of the gentry that were unknown to Ulster office. The first visitation was held by Nicholas Narbon, the second Ulster King of Arms, in 1569. He was authorized to reform practices which were contrary to good armorial practice. He conducted six visitations (Dublin in 1568–1573, Drogheda and Ardee in 1570, Dublin in 1572, Swords in 1572, Cork in 1574, and Limerick in 1574). One of his successors, Daniel Molyneux had the commission renewed, and mounted several visitations. Although Molyneux's last visitation – of Wexford – was the last proper visitation, two other expeditions occurred after 1618 by subsequent Ulster Kings of Arms. The visitations were not very extensive. The officers would not often be found in the disturbed countryside. Thus the visitations are confined to areas under firm control of the Dublin administration.\n\nToday, the original visitation and related manuscripts are in the custody of the Chief Herald of Ireland. Copies are also deposited at the College of Arms in London.\n\n\n\n\n\n\n(see also: Cornish heraldry)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "364326", "url": "https://en.wikipedia.org/wiki?curid=364326", "title": "Historicity of Jesus", "text": "Historicity of Jesus\n\nThe historicity of Jesus concerns the degree to which sources show Jesus of Nazareth existed as a historical figure. A second issue is closely tied to historical research practices and methodologies for analyzing the reliability of primary sources and other historical evidence.\n\nVirtually all New Testament scholars and Near East historians, applying the standard criteria of historical investigation, find that the historicity of Jesus is effectively certain although they differ about the beliefs and teachings of Jesus as well as the accuracy of the details of his life that have been described in the gospels. While scholars have criticized Jesus scholarship for religious bias and lack of methodological soundness, with very few exceptions such critics generally do support the historicity of Jesus and reject the Christ myth theory that Jesus never existed.\n\nThe historicity of Jesus is distinct from the related study of the historical Jesus, which refers to scholarly reconstructions of the life of Jesus, based primarily on critical analysis of the gospel texts. Historicity, by contrast, as a subject of study different from history proper, is concerned with two different fundamental issues. Firstly, it is concerned with the systemic processes of social change, and, secondly, the social context and intentions of the authors of the sources by which we can establish the truth of historical events, separating mythic accounts from factual circumstances.\n\nAll extant sources that mention Jesus were written after his death. The Christian Testament represents sources that have become canonical for Christianity, and there are many apocryphal texts that are examples of the wide variety of writings in the first centuries AD that are related to Jesus. Many scholars have questioned the authenticity and reliability of these sources, and few events mentioned in the gospels are universally accepted.\n\nThe seven Pauline epistles considered by scholarly consensus to be genuine are dated to between AD 50 and 60 (\"i.e.\", approximately twenty to thirty years after the generally accepted time period for the death of Jesus) and are the earliest surviving Christian texts that may include information about Jesus. Although Paul provides relatively little biographical information about Jesus and admits that he never knew Jesus personally, he does make it clear that he considers Jesus to have been a real person and a Jew. Moreover, he claims to have met with James, the brother of Jesus.\n\nNon-Christian sources used to study and establish the historicity of Jesus include the Jewish historian Josephus and Roman historian Tacitus. These sources are compared to Christian sources, such as the Pauline letters and synoptic gospels, and are usually independent of each other; that is, the Jewish sources do not draw upon the Roman sources. Similarities and differences between these sources are used in the authentication process.\n\nIn Books and \" of Antiquities of the Jews\", written around AD 93 to 94, Jewish historian Josephus twice refers to the biblical Jesus. The general scholarly view holds that the longer passage, known as the Testimonium Flavianum, most likely consists of an authentic nucleus that was subjected to later Christian interpolation or forgery. On the other hand, Louis H. Feldman states that \"few have doubted the genuineness\" of the reference found in to \"the brother of Jesus, who was called Christ, whose name was James\".\n\nThe Roman historian Tacitus, in his \"Annals\" (written \"ca.\" AD 115), , describes Nero's scapegoating of the Christians following the Fire of Rome. He writes that founder of the sect was named Christus (the Christian title for Jesus); that he was executed under Pontius Pilate; and that the movement, initially checked, broke out again in Judea and even in Rome itself. Some scholars question the historical value of the passage on various grounds.\n\nHistorian Michael Grant asserts that if conventional standards of historical textual criticism are applied to the New Testament, \"we can no more reject Jesus' existence than we can reject the existence of a mass of pagan personages whose reality as historical figures is never questioned.\"\n\nThe historical reliability of the gospels refers to the reliability and historic character of the four New Testament gospels as historical documents. Little in the four canonical gospels is considered to be historically reliable.\n\nMost scholars of antiquity agree that Jesus existed, but scholars differ on the historicity of specific episodes described in the biblical accounts of Jesus. The only two events subject to \"almost universal assent\" are that Jesus was baptized by John the Baptist and that, between one and three years later, he was crucified by the order of the Roman Prefect Pontius Pilate. Elements whose historical authenticity are disputed include the two accounts of the nativity of Jesus, the miraculous events including turning water into wine, walking on water and the resurrection, and certain details about the crucifixion.\n\nThe Synoptic Gospels are the primary sources of historical information about Jesus and of the religious movement he founded. These religious gospels–the Gospel of Matthew, the Gospel of Mark, and the Gospel of Luke–recount the life, ministry, crucifixion and resurrection of a Jew named Jesus who spoke Aramaic. There are different hypotheses regarding the origin of the texts because the gospels of the New Testament were written in Greek for Greek-speaking communities, and were later translated into Syriac, Latin, and Coptic.\nThe fourth gospel, the Gospel of John, differs greatly from the Synoptic Gospels. Historians often study the historical reliability of the Acts of the Apostles when studying the reliability of the gospels, as the Book of Acts was seemingly written by the same author as the Gospel of Luke.\n\nHistorians subject the gospels to critical analysis by differentiating authentic, reliable information from possible inventions, exaggerations, and alterations. Since there are more textual variants in the New Testament (200–400 thousand) than it has letters (c. 140 thousand), scholars use textual criticism to determine which gospel variants could theoretically be taken as 'original'. To answer this question, scholars have to ask who wrote the gospels, when they wrote them, what was their objective in writing them, what sources the authors used, how reliable these sources were, and how far removed in time the sources were from the stories they narrate, or if they were altered later. Scholars may also look into the internal evidence of the documents, to see if, for example, a document has misquoted texts from the Hebrew Tanakh, has made incorrect claims about geography, if the author appears to have hidden information, or if the author has fabricated a prophecy. Finally, scholars turn to external sources, including the testimony of early church leaders, to writers outside the church, primarily Jewish and Greco-Roman historians, who would have been more likely to have criticized the church, and to archaeological evidence.\n\nThere is widespread disagreement among scholars on the details of the life of Jesus mentioned in the gospel narratives, and on the meaning of his teachings, and the only two events subject to \"almost universal assent\" are that Jesus was baptized by John the Baptist and was crucified by the order of the Roman Prefect Pontius Pilate.\n\nAccording to New Testament scholar James Dunn, nearly all modern scholars consider the baptism of Jesus and his crucifixion to be historically certain. He states that these \"two facts in the life of Jesus command almost universal assent\" and \"rank so high on the 'almost impossible to doubt or deny' scale of historical 'facts' they are obvious starting points for an attempt to clarify the what and why of Jesus' mission.\" John P. Meier views the crucifixion of Jesus as historical fact and states that based on the \"criterion of embarrassment\" Christians would not have invented the painful death of their leader.\nThe criterion of embarrassment is also used to argue in favor of the historicity of the baptism of Jesus by John the Baptist as it is a story which the early Christian Church would have never wanted to invent. Based on this criterion, given that John baptised for the remission of sins, and Jesus was viewed as without sin, the invention of this story would have served no purpose, and would have been an embarrassment given that it positioned John above Jesus.\n\nAmy-Jill Levine has summarized the situation by stating that \"there is a consensus of sorts on the basic outline of Jesus' life\" in that most scholars agree that Jesus was baptized by John the Baptist, and over a period of one to three years debated Jewish authorities on the subject of God, gathered followers, and was crucified by Roman prefect Pontius Pilate who officiated 26–36 AD. There is much in dispute as to his previous life, childhood, family and place of residence, of which the canonical gospels are almost completely silent.\n\nScholars attribute varying levels of certainty to other episodes. Some assume that there are eight elements about Jesus and his followers that can be viewed as historical facts, namely:\n\nScholarly agreement on this extended list is not universal.\n\nThe Mishnah ( 200) may refer to Jesus and reflect the early Jewish traditions of portraying Jesus as a sorcerer or magician. Other references to Jesus and his execution exist in the Talmud, but they aim to discredit his actions, not deny his existence.\n\nSince the 18th century, three separate scholarly quests for the historical Jesus have taken place, each with distinct characteristics and based on different research criteria, which were often developed during that phase. The portraits of Jesus that have been constructed in these processes have often differed from each other, and from the dogmatic image portrayed in the gospel accounts.\nCurrently modern scholarly research on the historical Jesus focuses on what is historically probable, or plausible about Jesus.\n\nThe mainstream profiles in the third quest may be grouped together based on their primary theme as \"apocalyptic prophet\", \"charismatic healer\", \"Cynic philosopher\", \"Jewish Messiah\" and \"prophet of social change\", but there is little scholarly agreement on a single portrait, or the methods needed to construct it. There are, however, overlapping attributes among the portraits, and scholars who differ on some attributes may agree on others.\n\nWhile there is widespread scholarly agreement on the existence of Jesus, and a basic consensus on the general outline of his life, the portraits of Jesus constructed in the quests have often differed from each other, and from the image portrayed in the gospel accounts. There are overlapping attributes among the portraits, and while pairs of scholars may agree on some attributes, those same scholars may differ on other attributes, and there is no single portrait of the historical Jesus that satisfies most scholars.\n\nNearly all modern scholars of antiquity agree that Jesus existed and most biblical scholars and classical historians see the theories of his non-existence as effectively refuted. There is no evidence today that the existence of Jesus was ever denied in antiquity by those who opposed Christianity. Geoffrey Blainey notes that \"a few scholars argue that Jesus... did not even exist,\" and that they \"rightly point out that contemporary references to him were extremely rare.\" Bart Ehrman states \"Jesus is not mentioned in any Roman sources of his day\", but explains that this is not at all surprising, since the vast majority of historical figures from antiquity are not mentioned in contemporary sources, and further states that the sources written after Jesus's death provide ample evidence to support his existence as a person. Richard Carrier and Raphael Lataster assert that there is no independent evidence of Jesus’s existence outside the New Testament.\n\nCertain scholars, particularly in Europe, have recently made the claim that while there are a number of plausible Jesuses that could have existed, there can be no certainty as to which Jesus was the biblical Jesus, and that there should also be more scholarly research and debate on this topic.\n\nThe Christ myth theory is \"the view that the person known as Jesus of Nazareth had no historical existence.\"\n\nIn modern scholarship, the Christ myth theory is a fringe theory and finds virtually no support from scholars.\n\n\n"}
{"id": "5777097", "url": "https://en.wikipedia.org/wiki?curid=5777097", "title": "Historism", "text": "Historism\n\nHistorism is a philosophical and historiographical theory, founded in 19th-century Germany (as \"Historismus\") and especially influential in 19th- and 20th-century Europe. In those times there was not a single natural, humanitarian or philosophical science that would not reflect, in one way or another, the historical type of thought (cf. comparative historical linguistics etc.). It pronounces the historicity of humanity and its binding to tradition.\n\nHistorist historiography rejects historical teleology and bases its explanations of historical phenomena on sympathy and understanding (see Hermeneutics) for the events, acting persons, and historical periods. The historist approach takes to its extreme limits the common observation that human institutions (language, Art, religion, law, State) are subject to perpetual change.\n\n\"Historism\" is not to be confused with \"historicism\", nevertheless the English habits of using both words are very similar. (The term \"historism\" is sometimes reserved to identify the specific current called \"Historismus\" in the tradition of German philosophy and historiography.)\n\nBecause of the power held on the social sciences by logical positivism, historism or historicism is deemed unpopular.\n\nKarl Popper, one of the most distinguished critics of historicism, criticized historism, too. He differentiated between both phenomena as follows: The term \"historicism\" is used in his influential books \"The Poverty of Historicism\" and \"The Open Society and Its Enemies\" to describe “an approach to the social sciences which assumes that \"historical prediction\" is their primary aim, and which assumes that this aim is attainable by discovering the 'rhythms' or the 'patterns', the 'laws' or the 'trends' that underlie the evolution of history”. Popper wrote with reference to Hegel's theory of history, which he criticized extensively. By \"historism\" on the contrary, he means the tendency to regard every argument or idea as completely accounted for by its historical context, as opposed to assessing it by its merits. \"Historism\" does not aim for the 'laws' of history, but premises the individuality of each historical situation.\n\nOn the basis of Popper's definitions, the historian Stefan Berger proposes as a proper word usage: \n\nNotable exponents of historism were primarily the German 19th-century historians Leopold von Ranke and Johann Gustav Droysen, 20th-century historian Friedrich Meinecke, and the philosopher Wilhelm Dilthey. Dilthey was influenced by Ranke. The jurists Friedrich Carl von Savigny and Karl Friedrich Eichhorn were strongly influenced by the ideas of historism and founded the German Historical School of Law. The Italian philosopher and historian Benedetto Croce and his British colleague Robin George Collingwood were important European exponents of historism in the late 19th and early 20th century. Collingwood was influenced by Dilthey.\n\nRanke's arguments can be viewed as an antidote to the lawlike and quantitative approaches common in sociology and most other social sciences.\n\nThe principle of historism has a universal methodological significance in Marxism. The essence of this principle, in brief, is\nGeorg G. Iggers is one of the most important critical authors on historism. His book \"The German Conception of History: The National Tradition of Historical Thought from Herder to the Present\", first published in 1968 (by Wesleyan University Press, Middletown, Ct.) is a \"classic” among critiques of historism.\n\nAnother critique is presented by the German philosopher Friedrich Nietzsche, whose essay \"Vom Nutzen und Nachteil der Historie für das Leben\" (\"On the Use and Abuse of History for Life\", 1874; see \"The Untimely Meditations\") denounces “a malignant historical fever”. Nietzsche contends that the historians of his times, the historists, damaged the powers of human life by relegating it to the past instead of opening it to the future. For this reason, he calls for a return, beyond historism, to humanism.\n\n20th-century German historians promoting some aspects of historism are Ulrich Muhlack, Thomas Nipperdey and Jörn Rüsen.\n\nAlso the Spanish philosopher José Ortega y Gasset was influenced by historism.\n\n\n"}
{"id": "14914", "url": "https://en.wikipedia.org/wiki?curid=14914", "title": "Industrial Revolution", "text": "Industrial Revolution\n\nThe Industrial Revolution was the transition to new manufacturing processes in the period from about 1760 to sometime between 1820 and 1840. This transition included going from hand production methods to machines, new chemical manufacturing and iron production processes, the increasing use of steam power, the development of machine tools and the rise of the factory system.\n\nTextiles were the dominant industry of the Industrial Revolution in terms of employment, value of output and capital invested. The textile industry was also the first to use modern production methods.\n\nThe Industrial Revolution began in Great Britain, and many of the technological innovations were of British origin. By the mid-18th century Britain was the world's leading commercial nation, controlling a global trading empire with colonies in North America and the Caribbean, and with some political influence on the Indian subcontinent, through the activities of the East India Company. The development of trade and the rise of business were major causes of the Industrial Revolution.\n\nThe Industrial Revolution marks a major turning point in history; almost every aspect of daily life was influenced in some way. In particular, average income and population began to exhibit unprecedented sustained growth. Some economists say that the major impact of the Industrial Revolution was that the standard of living for the general population began to increase consistently for the first time in history, although others have said that it did not begin to meaningfully improve until the late 19th and 20th centuries.\n\nGDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy, while the Industrial Revolution began an era of per-capita economic growth in capitalist economies. Economic historians are in agreement that the onset of the Industrial Revolution is the most important event in the history of humanity since the domestication of animals and plants.\n\nAlthough the structural change from agriculture to industry is widely associated with Industrial Revolution, in United Kingdom it was already almost complete by 1760.\n\nThe precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic and social changes. Eric Hobsbawm held that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s or 1840s, while T. S. Ashton held that it occurred roughly between 1760 and 1830. Rapid industrialization first began in Britain, starting with mechanized spinning in the 1780s, with high rates of growth in steam power and iron production occurring after 1800. Mechanized textile production spread from Great Britain to continental Europe and the United States in the early 19th century, with important centres of textiles, iron and coal emerging in Belgium and the United States and later textiles in France.\n\nAn economic recession occurred from the late 1830s to the early 1840s when the adoption of the original innovations of the Industrial Revolution, such as mechanized spinning and weaving, slowed and their markets matured. Innovations developed late in the period, such as the increasing adoption of locomotives, steamboats and steamships, hot blast iron smelting and new technologies, such as the electrical telegraph, widely introduced in the 1840s and 1850s, were not powerful enough to drive high rates of growth. Rapid economic growth began to occur after 1870, springing from a new group of innovations in what has been called the Second Industrial Revolution. These new innovations included new steel making processes, the large-scale manufacture of machine tools and the use of increasingly advanced machinery in steam-powered factories.\n\nThe earliest recorded use of the term \"Industrial Revolution\" seems to have been in a letter from 6 July 1799 written by French envoy Louis-Guillaume Otto, announcing that France had entered the race to industrialise. In his 1976 book \"\", Raymond Williams states in the entry for \"Industry\": \"The idea of a new social order based on major industrial change was clear in Southey and Owen, between 1811 and 1818, and was implicit as early as Blake in the early 1790s and Wordsworth at the turn of the [19th] century.\" The term \"Industrial Revolution\" applied to technological change was becoming more common by the late 1830s, as in Jérôme-Adolphe Blanqui's description in 1837 of \"la révolution industrielle\". Friedrich Engels in \"The Condition of the Working Class in England\" in 1844 spoke of \"an industrial revolution, a revolution which at the same time changed the whole of civil society\". However, although Engels wrote in the 1840s, his book was not translated into English until the late 1800s, and his expression did not enter everyday language until then. Credit for popularising the term may be given to Arnold Toynbee, whose 1881 lectures gave a detailed account of the term.\n\nSome historians, such as John Clapham and Nicholas Crafts, have argued that the economic and social changes occurred gradually and the term \"revolution\" is a misnomer. This is still a subject of debate among some historians.\n\nThe commencement of the Industrial Revolution is closely linked to a small number of innovations, beginning in the second half of the 18th century. By the 1830s the following gains had been made in important technologies:\n\nIn 1750 Britain imported 2.5 million pounds of raw cotton, most of which was spun and woven by cottage industry in Lancashire. The work was done by hand in workers' homes or occasionally in shops of master weavers. In 1787 raw cotton consumption was 22 million pounds, most of which was cleaned, carded and spun on machines. The British textile industry used 52 million pounds of cotton in 1800, which increased to 588 million pounds in 1850.\n\nThe share of value added by the cotton textile industry in Britain was 2.6% in 1760, 17% in 1801 and 22.4% in 1831. Value added by the British woollen industry was 14.1% in 1801. Cotton factories in Britain numbered approximately 900 in 1797. In 1760 approximately one-third of cotton cloth manufactured in Britain was exported, rising to two-thirds by 1800. In 1781 cotton spun amounted to 5.1 million pounds, which increased to 56 million pounds by 1800. In 1800 less than 0.1% of world cotton cloth was produced on machinery invented in Britain. In 1788 there were 50,000 spindles in Britain, rising to 7 million over the next 30 years.\n\nWages in Lancashire, a core region for cottage industry and later factory spinning and weaving, were about six times those in India in 1770, when overall productivity in Britain was about three times higher than in India.\n\nParts of India, China, Central America, South America and the Middle-East have a long history of hand manufacturing cotton textiles, which became a major industry sometime after 1000 AD. In tropical and subtropical regions where it was grown, most was grown by small farmers alongside their food crops and was spun and woven in households, largely for domestic consumption. In the 15th century China began to require households to pay part of their taxes in cotton cloth. By the 17th century almost all Chinese wore cotton clothing. Almost everywhere cotton cloth could be used as a medium of exchange. In India a significant amount of cotton textiles were manufactured for distant markets, often produced by professional weavers. Some merchants also owned small weaving workshops. India produced a variety of cotton cloth, some of exceptionally fine quality.\n\nCotton was a difficult raw material for Europe to obtain before it was grown on colonial plantations in the Americas. The early Spanish explorers found Native Americans growing unknown species of excellent quality cotton: sea island cotton (\"Gossypium barbadense\") and upland green seeded cotton \"Gossypium hirsutum\". Sea island cotton grew in tropical areas and on barrier islands of Georgia and South Carolina, but did poorly inland. Sea island cotton began being exported from Barbados in the 1650s. Upland green seeded cotton grew well on inland areas of the southern U.S., but was not economical because of the difficulty of removing seed, a problem solved by the cotton gin. A strain of cotton seed brought from Mexico to Natchez, Mississippi, USA in 1806 became the parent genetic material for over 90% of world cotton production today; it produced bolls that were three to four times faster to pick.\n\nThe Age of Discovery was followed by a period of colonialism beginning around the 16th century. Following the discovery of a trade route to India around southern Africa by the Portuguese, the Dutch established the Verenigde Oostindische Compagnie (abbr. VOC) or Dutch East India Company and the British founded the East India Company, along with smaller companies of different nationalities which established trading posts and employed agents to engage in trade throughout the Indian Ocean region and between the Indian Ocean region and North Atlantic Europe. One of the largest segments of this trade was in cotton textiles, which were purchased in India and sold in Southeast Asia, including the Indonesian archipelago, where spices were purchased for sale to Southeast Asia and Europe. By the mid-1760s cloth was over three-quarters of the East India Company's exports. Indian textiles were in demand in North Atlantic region of Europe where previously only wool and linen were available; however, the amount of cotton goods consumed in Western Europe was minor until the early 19th century.\n\nBy 1600 Flemish refugees began weaving cotton cloth in English towns where cottage spinning and weaving of wool and linen was well established; however, they were left alone by the guilds who did not consider cotton a threat. Earlier European attempts at cotton spinning and weaving were in 12th century Italy and 15th century southern Germany, but these industries eventually ended when the supply of cotton was cut off. The Moors in Spain grew, spun and wove cotton beginning around the 10th century.\n\nBritish cloth could not compete with Indian cloth because India's labor cost was approximately one-fifth to one-sixth that of Britain's. In 1700 and 1721 the British government passed Calico Acts in order to protect the domestic woollen and linen industries from the increasing amounts of cotton fabric imported from India.\n\nThe demand for heavier fabric was met by a domestic industry based around Lancashire that produced fustian, a cloth with flax warp and cotton weft. Flax was used for the warp because wheel-spun cotton did not have sufficient strength, but the resulting blend was not as soft as 100% cotton and was more difficult to sew.\n\nOn the eve of the Industrial Revolution, spinning and weaving were done in households, for domestic consumption and as a cottage industry under the putting-out system. Occasionally the work was done in the workshop of a master weaver. Under the putting-out system, home-based workers produced under contract to merchant sellers, who often supplied the raw materials. In the off season the women, typically farmers' wives, did the spinning and the men did the weaving. Using the spinning wheel, it took anywhere from four to eight spinners to supply one hand loom weaver.\n\nThe flying shuttle, patented in 1733 by John Kay, with a number of subsequent improvements including an important one in 1747, doubled the output of a weaver, worsening the imbalance between spinning and weaving. It became widely used around Lancashire after 1760 when John's son, Robert, invented the drop box, which facilitated changing thread colors.\n\nLewis Paul patented the roller spinning frame and the flyer-and-bobbin system for drawing wool to a more even thickness. The technology was developed with the help of John Wyatt of Birmingham. Paul and Wyatt opened a mill in Birmingham which used their new rolling machine powered by a donkey. In 1743 a factory opened in Northampton with 50 spindles on each of five of Paul and Wyatt's machines. This operated until about 1764. A similar mill was built by Daniel Bourn in Leominster, but this burnt down. Both Lewis Paul and Daniel Bourn patented carding machines in 1748. Based on two sets of rollers that travelled at different speeds, it was later used in the first cotton spinning mill. Lewis's invention was later developed and improved by Richard Arkwright in his water frame and Samuel Crompton in his spinning mule.\n\nIn 1764 in the village of Stanhill, Lancashire, James Hargreaves invented the spinning jenny, which he patented in 1770. It was the first practical spinning frame with multiple spindles. The jenny worked in a similar manner to the spinning wheel, by first clamping down on the fibres, then by drawing them out, followed by twisting. It was a simple, wooden framed machine that only cost about £6 for a 40-spindle model in 1792, and was used mainly by home spinners. The jenny produced a lightly twisted yarn only suitable for weft, not warp.\n\nThe spinning frame or water frame was developed by Richard Arkwright who, along with two partners, patented it in 1769. The design was partly based on a spinning machine built for Thomas High by clockmaker John Kay, who was hired by Arkwright. For each spindle the water frame used a series of four pairs of rollers, each operating at a successively higher rotating speed, to draw out the fibre, which was then twisted by the spindle. The roller spacing was slightly longer than the fibre length. Too close a spacing caused the fibres to break while too distant a spacing caused uneven thread. The top rollers were leather-covered and loading on the rollers was applied by a weight. The weights kept the twist from backing up before the rollers. The bottom rollers were wood and metal, with fluting along the length. The water frame was able to produce a hard, medium count thread suitable for warp, finally allowing 100% cotton cloth to be made in Britain. A horse powered the first factory to use the spinning frame. Arkwright and his partners used water power at a factory in Cromford, Derbyshire in 1771, giving the invention its name.\n\nSamuel Crompton's Spinning Mule was introduced in 1779. Mule implies a hybrid because it was a combination of the spinning jenny and the water frame, in which the spindles were placed on a carriage, which went through an operational sequence during which the rollers stopped while the carriage moved away from the drawing roller to finish drawing out the fibres as the spindles started rotating. Crompton's mule was able to produce finer thread than hand spinning and at a lower cost. Mule spun thread was of suitable strength to be used as warp, and finally allowed Britain to produce highly competitive yarn in large quantities.\n\nRealising that the expiration of the Arkwright patent would greatly increase the supply of spun cotton and lead to a shortage of weavers, Edmund Cartwright developed a vertical power loom which he patented in 1785. In 1776 he patented a two-man operated loom which was more conventional. Cartwright built two factories; the first burned down and the second was sabotaged by his workers. Cartwright's loom design had several flaws, the most serious being thread breakage. Samuel Horrocks patented a fairly successful loom in 1813. Horock's loom was improved by Richard Roberts in 1822 and these were produced in large numbers by Roberts, Hill & Co.\n\nThe demand for cotton presented an opportunity to planters in the Southern United States, who thought upland cotton would be a profitable crop if a better way could be found to remove the seed. Eli Whitney responded to the challenge by inventing the inexpensive cotton gin. A man using a cotton gin could remove seed from as much upland cotton in one day as would previously, working at the rate of one pound of cotton per day, have taken a woman two months to process.\n\nThese advances were capitalised on by entrepreneurs, of whom the best known is Richard Arkwright. He is credited with a list of inventions, but these were actually developed by such people as Thomas Highs and John Kay; Arkwright nurtured the inventors, patented the ideas, financed the initiatives, and protected the machines. He created the cotton mill which brought the production processes together in a factory, and he developed the use of power – first horse power and then water powerwhich made cotton manufacture a mechanised industry. Other inventors increased the efficiency of the individual steps of spinning (carding, twisting and spinning, and rolling) so that the supply of yarn increased greatly. Before long steam power was applied to drive textile machinery. Manchester acquired the nickname Cottonopolis during the early 19th century owing to its sprawl of textile factories.\n\nAlthough mechanization dramatically decreased the cost of cotton cloth, by the mid-19th century machine-woven cloth still could not equal the quality of hand-woven Indian cloth, in part due to the fineness of thread made possible by the type of cotton used in India, which allowed high thread counts. However, the high productivity of British textile manufacturing allowed coarser grades of British cloth to undersell hand-spun and woven fabric in low-wage India, eventually destroying the industry.\n\nThe earliest European attempts at mechanized spinning were with wool; however, wool spinning proved more difficult to mechanize than cotton. Productivity improvement in wool spinning during the Industrial Revolution was significant but was far less than that of cotton.\n\nArguably the first highly mechanised factory was John Lombe's water-powered silk mill at Derby, operational by 1721. Lombe learned silk thread manufacturing by taking a job in Italy and acting as an industrial spy; however, because the Italian silk industry guarded its secrets closely, the state of the industry at that time is unknown. Although Lombe's factory was technically successful, the supply of raw silk from Italy was cut off to eliminate competition. In order to promote manufacturing the Crown paid for models of Lombe's machinery which were exhibited in the Tower of London.\n\nBar iron was the commodity form of iron used as the raw material for making hardware goods such as nails, wire, hinges, horse shoes, wagon tires, chains, etc. and for structural shapes. A small amount of bar iron was converted into steel. Cast iron was used for pots, stoves and other items where its brittleness was tolerable. Most cast iron was refined and converted to bar iron, with substantial losses. Bar iron was also made by the bloomery process, which was the predominant iron smelting process until the late 18th century.\n\nIn the UK in 1720 there were 20,500 tons of cast iron produced with charcoal and 400 tons with coke. In 1750 charcoal iron production was 24,500 and coke iron was 2,500 tons. In 1788 the production of charcoal cast iron was 14,000 tons while coke iron production was 54,000 tons. In 1806 charcoal cast iron production was 7,800 tons and coke cast iron was 250,000 tons.\n\nIn 1750 the UK imported 31,200 tons of bar iron and either refined from cast iron or directly produced 18,800 tons of bar iron using charcoal and 100 tons using coke. In 1796 the UK was making 125,000 tons of bar iron with coke and 6,400 tons with charcoal; imports were 38,000 tons and exports were 24,600 tons. In 1806 the UK did not import bar iron but exported 31,500 tons.\n\nA major change in the iron industries during the era of the Industrial Revolution was the replacement of wood and other bio-fuels with coal. For a given amount of heat, coal required much less labour to mine than cutting wood and converting it to charcoal, and coal was much more abundant than wood, supplies of which were becoming scarce before the enormous increase in iron production that took place in the late 18th century. By 1750 coke had generally replaced charcoal in smelting of copper and lead and was in widespread use in making glass. In the smelting and refining of iron, coal and coke produced inferior iron to that made with charcoal because of the coal's sulfur content. Low sulfur coals were known, but they still contained harmful amounts. Conversion of coal to coke only slightly reduces the sulfur content. A minority of coals are coking.\n\nAnother factor limiting the iron industry before the Industrial Revolution was the scarcity of water power to power blast bellows. This limitation was overcome by the steam engine.\n\nUse of coal in iron smelting started somewhat before the Industrial Revolution, based on innovations by Sir Clement Clerke and others from 1678, using coal reverberatory furnaces known as cupolas. These were operated by the flames playing on the ore and charcoal or coke mixture, reducing the oxide to metal. This has the advantage that impurities (such as sulphur ash) in the coal do not migrate into the metal. This technology was applied to lead from 1678 and to copper from 1687. It was also applied to iron foundry work in the 1690s, but in this case the reverberatory furnace was known as an air furnace. (The foundry cupola is a different, and later, innovation.)\n\nBy 1709 Abraham Darby made progress using coke to fuel his blast furnaces at Coalbrookdale. However, the coke pig iron he made was not suitable for making wrought iron and was used mostly for the production of cast iron goods, such as pots and kettles. He had the advantage over his rivals in that his pots, cast by his patented process, were thinner and cheaper than theirs.\n\nCoke pig iron was hardly used to produce wrought iron until 1755-56, when Darby's son Abraham Darby II built furnaces at Horsehay and Ketley where low sulfur coal was available (and not far from Coalbrookdale). These new furnaces were equipped with water-powered bellows, the water being pumped by Newcomen steam engines. The Newcomen engines were not attached directly to the blowing cylinders because the engines alone could not produce a steady air blast. Abraham Darby III installed similar steam-pumped, water-powered blowing cylinders at the Dale Company when he took control in 1768. The Dale Company used several Newcomen engines to drain its mines and made parts for engines which it sold throughout the country.\n\nSteam engines made the use of higher-pressure and volume blast practical; however, the leather used in bellows was expensive to replace. In 1757, iron master John Wilkinson patented a hydraulic powered blowing engine for blast furnaces. The blowing cylinder for blast furnaces was introduced in 1760 and the first blowing cylinder made of cast iron is believed to be the one used at Carrington in 1768 that was designed by John Smeaton. Cast iron cylinders for use with a piston were difficult to manufacture; the cylinders had to be free of holes and had to be machined smooth and straight to remove any warping. James Watt had great difficulty trying to have a cylinder made for his first steam engine. In 1774 John Wilkinson, who built a cast iron blowing cylinder for his iron works, invented a precision boring machine for boring cylinders. After Wilkinson bored the first successful cylinder for a Boulton and Watt steam engine in 1776, he was given an exclusive contract for providing cylinders. After Watt developed a rotary steam engine in 1782, they were widely applied to blowing, hammering, rolling and slitting.\n\nThe solutions to the sulfur problem were the addition of sufficient limestone to the furnace to force sulfur into the slag and the use of low sulfur coal. Use of lime or limestone required higher furnace temperatures to form a free-flowing slag. The increased furnace temperature made possible by improved blowing also increased the capacity of blast furnaces and allowed for increased furnace height. In addition to lower cost and greater availability, coke had other important advantages over charcoal in that it was harder and made the column of materials (iron ore, fuel, slag) flowing down the blast furnace more porous and did not crush in the much taller furnaces of the late 19th century.\n\nAs cast iron became cheaper and widely available, it began being a structural material for bridges and buildings. A famous early example was the Iron Bridge built in 1778 with cast iron produced by Abraham Darby III. However, most cast iron was converted to wrought iron.\n\nEurope relied on the bloomery for most of its wrought iron until the large scale production of cast iron. Conversion of cast iron was done in a finery forge, as it long had been. An improved refining process known as potting and stamping was developed, but this was superseded by Henry Cort's puddling process. Cort developed two significant iron manufacturing processes: rolling in 1783 and puddling in 1784. Puddling produced a structural grade iron at a relatively low cost.\n\nPuddling was a means of decarburizing molten pig iron by slow oxidation in a reverberatory furnace by manually stirring it with a long rod. The decarburized iron, having a higher melting point than cast iron, was raked into globs by the puddler. When the glob was large enough, the puddler would remove it. Puddling was backbreaking and extremely hot work. Few puddlers lived to be 40. Because puddling was done in a reverberatory furnace, coal or coke could be used as fuel. The puddling process continued to be used until the late 19th century when iron was being displaced by steel. Because puddling required human skill in sensing the iron globs, it was never successfully mechanised. Rolling was an important part of the puddling process because the grooved rollers expelled most of the molten slag and consolidated the mass of hot wrought iron. Rolling was 15 times faster at this than a trip hammer. A different use of rolling, which was done at lower temperatures than that for expelling slag, was in the production of iron sheets, and later structural shapes such as beams, angles and rails.\nThe puddling process was improved in 1818 by Baldwyn Rogers, who replaced some of the sand lining on the reverberatory furnace bottom with iron oxide. In 1838 John Hall patented the use of roasted tap cinder (iron silicate) for the furnace bottom, greatly reducing the loss of iron through increased slag caused by a sand lined bottom. The tap cinder also tied up some phosphorus, but this was not understood at the time. Hall's process also used iron scale or rust, which reacted with carbon in the molten iron. Hall's process, called \"wet puddling\", reduced losses of iron with the slag from almost 50% to around 8%.\n\nPuddling became widely used after 1800. Up to that time British iron manufacturers had used considerable amounts of iron imported from Sweden and Russia to supplement domestic supplies. Because of the increased British production, imports began to decline in 1785 and by the 1790s Britain eliminated imports and became a net exporter of bar iron.\n\nHot blast, patented by James Beaumont Neilson in 1828, was the most important development of the 19th century for saving energy in making pig iron. By using preheated combustion air, the amount of fuel to make a unit of pig iron was reduced at first by between one-third using coke or two-thirds using coal; however, the efficiency gains continued as the technology improved. Hot blast also raised the operating temperature of furnaces, increasing their capacity. Using less coal or coke meant introducing fewer impurities into the pig iron. This meant that lower quality coal or anthracite could be used in areas where coking coal was unavailable or too expensive; however, by the end of the 19th century transportation costs fell considerably.\n\nShortly before the Industrial Revolution an improvement was made in the production of steel, which was an expensive commodity and used only where iron would not do, such as for cutting edge tools and for springs. Benjamin Huntsman developed his crucible steel technique in the 1740s. The raw material for this was blister steel, made by the cementation process.\n\nThe supply of cheaper iron and steel aided a number of industries, such as those making nails, hinges, wire and other hardware items. The development of machine tools allowed better working of iron, causing it to be increasingly used in the rapidly growing machinery and engine industries.\n\nThe development of the stationary steam engine was an important element of the Industrial Revolution; however, during the early period of the Industrial Revolution, most industrial power was supplied by water and wind. In Britain by 1800 an estimated 10,000 horsepower was being supplied by steam. By 1815 steam power had grown to 210,000 hp.\n\nThe first commercially successful industrial use of steam power was due to Thomas Savery in 1698. He constructed and patented in London a low-lift combined vacuum and pressure water pump, that generated about one horsepower (hp) and was used in numerous water works and in a few mines (hence its \"brand name\", \"The Miner's Friend\"). Savery's pump was economical in small horsepower ranges, but was prone to boiler explosions in larger sizes. Savery pumps continued to be produced until the late 18th century.\n\nThe first successful piston steam engine was introduced by Thomas Newcomen before 1712. A number of Newcomen engines were installed in Britain for draining hitherto unworkable deep mines, with the engine on the surface; these were large machines, requiring a significant amount of capital to build, and produced upwards of . They were also used to power municipal water supply pumps. They were extremely inefficient by modern standards, but when located where coal was cheap at pit heads, opened up a great expansion in coal mining by allowing mines to go deeper. Despite their disadvantages, Newcomen engines were reliable and easy to maintain and continued to be used in the coalfields until the early decades of the 19th century. By 1729, when Newcomen died, his engines had spread (first) to Hungary in 1722, Germany, Austria, and Sweden. A total of 110 are known to have been built by 1733 when the joint patent expired, of which 14 were abroad. In the 1770s the engineer John Smeaton built some very large examples and introduced a number of improvements. A total of 1,454 engines had been built by 1800.\n\nA fundamental change in working principles was brought about by Scotsman James Watt. With financial support from his business partner Englishman Matthew Boulton, he had succeeded by 1778 in perfecting his steam engine, which incorporated a series of radical improvements, notably the closing off of the upper part of the cylinder, thereby making the low-pressure steam drive the top of the piston instead of the atmosphere, use of a steam jacket and the celebrated separate steam condenser chamber. The separate condenser did away with the cooling water that had been injected directly into the cylinder, which cooled the cylinder and wasted steam. Likewise, the steam jacket kept steam from condensing in the cylinder, also improving efficiency. These improvements increased engine efficiency so that Boulton & Watts engines used only 20–25% as much coal per horsepower-hour as Newcomen's. Boulton and Watt opened the Soho Foundry for the manufacture of such engines in 1795.\n\nBy 1783 the Watt steam engine had been fully developed into a double-acting rotative type, which meant that it could be used to directly drive the rotary machinery of a factory or mill. Both of Watt's basic engine types were commercially very successful, and by 1800, the firm Boulton & Watt had constructed 496 engines, with 164 driving reciprocating pumps, 24 serving blast furnaces, and 308 powering mill machinery; most of the engines generated from .\n\nUntil about 1800 the most common pattern of steam engine was the beam engine, built as an integral part of a stone or brick engine-house, but soon various patterns of self-contained rotative engines (readily removable, but not on wheels) were developed, such as the table engine. Around the start of the 19th century, at which time the Boulton and Watt patent expired, the Cornish engineer Richard Trevithick and the American Oliver Evans began to construct higher-pressure non-condensing steam engines, exhausting against the atmosphere. High pressure yielded an engine and boiler compact enough to be used on mobile road and rail locomotives and steam boats.\n\nThe development of machine tools, such as the engine lathe, planing, milling and shaping machines powered by these engines, enabled all the metal parts of the engines to be easily and accurately cut and in turn made it possible to build larger and more powerful engines.\n\nSmall industrial power requirements continued to be provided by animal and human muscle until widespread electrification in the early 20th century. These included crank-powered, treadle-powered and horse-powered workshop and light industrial machinery.\n\nPre-industrial machinery was built by various craftsmen – millwrights built water and windmills, carpenters made wooden framing, and smiths and turners made metal parts. Wooden components had the disadvantage of changing dimensions with temperature and humidity, and the various joints tended to rack (work loose) over time. As the Industrial Revolution progressed, machines with metal parts and frames became more common. Other important uses of metal parts were in firearms and threaded fasteners, such as machine screws, bolts and nuts. There was also the need for precision in making parts. Precision would allow better working machinery, interchangeability of parts and standardization of threaded fasteners.\n\nThe demand for metal parts led to the development of several machine tools. They have their origins in the tools developed in the 18th century by makers of clocks and watches and scientific instrument makers to enable them to batch-produce small mechanisms.\n\nBefore the advent of machine tools, metal was worked manually using the basic hand tools of hammers, files, scrapers, saws and chisels. Consequently, the use of metal machine parts was kept to a minimum. Hand methods of production were very laborious and costly and precision was difficult to achieve.\n\nThe first large precision machine tool was the cylinder boring machine invented by John Wilkinson in 1774. It used for boring the large-diameter cylinders on early steam engines. Wilkinson's boring machine differed from earlier cantilevered machines used for boring cannon in that the cutting tool was mounted on a beam that ran through the cylinder being bored and was supported outside on both ends.\n\nThe planing machine, the milling machine and the shaping machine were developed in the early decades of the 19th century. Although the milling machine was invented at this time, it was not developed as a serious workshop tool until somewhat later in the 19th century.\n\nHenry Maudslay, who trained a school of machine tool makers early in the 19th century, was a mechanic with superior ability who had been employed at the Royal Arsenal, Woolwich. He worked as an apprentice in the Royal Gun Foundry of Jan Verbruggen. In 1774 Jan Verbruggen had installed a horizontal boring machine in Woolwich which was the first industrial size Lathe in the UK. Maudslay was hired away by Joseph Bramah for the production of high-security metal locks that required precision craftsmanship. Bramah patented a lathe that had similarities to the slide rest lathe. Maudslay perfected the slide rest lathe, which could cut machine screws of different thread pitches by using changeable gears between the spindle and the lead screw. Before its invention screws could not be cut to any precision using various earlier lathe designs, some of which copied from a template. The slide rest lathe was called one of history's most important inventions. Although it was not entirely Maudslay's idea, he was the first person to build a functional lathe using a combination of known innovations of the lead screw, slide rest and change gears.\n\nMaudslay left Bramah's employment and set up his own shop. He was engaged to build the machinery for making ships' pulley blocks for the Royal Navy in the Portsmouth Block Mills. These machines were all-metal and were the first machines for mass production and making components with a degree of interchangeability. The lessons Maudslay learned about the need for stability and precision he adapted to the development of machine tools, and in his workshops he trained a generation of men to build on his work, such as Richard Roberts, Joseph Clement and Joseph Whitworth.\n\nJames Fox of Derby had a healthy export trade in machine tools for the first third of the century, as did Matthew Murray of Leeds. Roberts was a maker of high-quality machine tools and a pioneer of the use of jigs and gauges for precision workshop measurement.\n\nThe impact of machine tools during the Industrial Revolution was not that great because other than firearms, threaded fasteners and a few other industries there were few mass-produced metal parts. The techniques to make mass-produced metal parts made with sufficient precision to be interchangeable is largely attributed to a program of the U.S. Department of War which perfected interchangeable parts for firearms in the early 19th century.\n\nIn the half century following the invention of the fundamental machine tools the machine industry became the largest industrial sector of the U.S. economy, by value added.\n\nThe large-scale production of chemicals was an important development during the Industrial Revolution. The first of these was the production of sulphuric acid by the lead chamber process invented by the Englishman John Roebuck (James Watt's first partner) in 1746. He was able to greatly increase the scale of the manufacture by replacing the relatively expensive glass vessels formerly used with larger, less expensive chambers made of riveted sheets of lead. Instead of making a small amount each time, he was able to make around in each of the chambers, at least a tenfold increase.\n\nThe production of an alkali on a large scale became an important goal as well, and Nicolas Leblanc succeeded in 1791 in introducing a method for the production of sodium carbonate. The Leblanc process was a reaction of sulphuric acid with sodium chloride to give sodium sulphate and hydrochloric acid. The sodium sulphate was heated with limestone (calcium carbonate) and coal to give a mixture of sodium carbonate and calcium sulphide. Adding water separated the soluble sodium carbonate from the calcium sulphide. The process produced a large amount of pollution (the hydrochloric acid was initially vented to the air, and calcium sulphide was a useless waste product). Nonetheless, this synthetic soda ash proved economical compared to that from burning specific plants (barilla) or from kelp, which were the previously dominant sources of soda ash, and also to potash (potassium carbonate) produced from hardwood ashes.\n\nThese two chemicals were very important because they enabled the introduction of a host of other inventions, replacing many small-scale operations with more cost-effective and controllable processes. Sodium carbonate had many uses in the glass, textile, soap, and paper industries. Early uses for sulphuric acid included pickling (removing rust) iron and steel, and for bleaching cloth.\n\nThe development of bleaching powder (calcium hypochlorite) by Scottish chemist Charles Tennant in about 1800, based on the discoveries of French chemist Claude Louis Berthollet, revolutionised the bleaching processes in the textile industry by dramatically reducing the time required (from months to days) for the traditional process then in use, which required repeated exposure to the sun in bleach fields after soaking the textiles with alkali or sour milk. Tennant's factory at St Rollox, North Glasgow, became the largest chemical plant in the world.\n\nAfter 1860 the focus on chemical innovation was in dyestuffs, and Germany took world leadership, building a strong chemical industry. Aspiring chemists flocked to German universities in the 1860–1914 era to learn the latest techniques. British scientists by contrast, lacked research universities and did not train advanced students; instead, the practice was to hire German-trained chemists.\n\nIn 1824 Joseph Aspdin, a British bricklayer turned builder, patented a chemical process for making portland cement which was an important advance in the building trades. This process involves sintering a mixture of clay and limestone to about , then grinding it into a fine powder which is then mixed with water, sand and gravel to produce concrete. Portland cement was used by the famous English engineer Marc Isambard Brunel several years later when constructing the Thames Tunnel. Cement was used on a large scale in the construction of the London sewerage system a generation later.\n\nAnother major industry of the later Industrial Revolution was gas lighting. Though others made a similar innovation elsewhere, the large-scale introduction of this was the work of William Murdoch, an employee of Boulton & Watt, the Birmingham steam engine pioneers. The process consisted of the large-scale gasification of coal in furnaces, the purification of the gas (removal of sulphur, ammonia, and heavy hydrocarbons), and its storage and distribution. The first gas lighting utilities were established in London between 1812 and 1820. They soon became one of the major consumers of coal in the UK. Gas lighting affected social and industrial organisation because it allowed factories and stores to remain open longer than with tallow candles or oil. Its introduction allowed nightlife to flourish in cities and towns as interiors and streets could be lighted on a larger scale than before.\n\nA new method of producing glass, known as the cylinder process, was developed in Europe during the early 19th century. In 1832 this process was used by the Chance Brothers to create sheet glass. They became the leading producers of window and plate glass. This advancement allowed for larger panes of glass to be created without interruption, thus freeing up the space planning in interiors as well as the fenestration of buildings. The Crystal Palace is the supreme example of the use of sheet glass in a new and innovative structure.\n\nA machine for making a continuous sheet of paper on a loop of wire fabric was patented in 1798 by Nicholas Louis Robert who worked for Saint-Léger Didot family in France. The paper machine is known as a Fourdrinier after the financiers, brothers Sealy and Henry Fourdrinier, who were stationers in London. Although greatly improved and with many variations, the Fourdriner machine is the predominant means of paper production today.\n\nThe method of continuous production demonstrated by the paper machine influenced the development of continuous rolling of iron and later steel and other continuous production processes.\n\nThe British Agricultural Revolution is considered one of the causes of the Industrial Revolution because improved agricultural productivity freed up workers to work in other sectors of the economy. However, per-capita food supply in Europe was stagnant or declining and did not improve in some parts of Europe until the late 18th century.\n\nIndustrial technologies that affected farming included the seed drill, the Dutch plough, which contained iron parts, and the threshing machine.\n\nJethro Tull invented an improved seed drill in 1701. It was a mechanical seeder which distributed seeds evenly across a plot of land and planted them at the correct depth. This was important because the yield of seeds harvested to seeds planted at that time was around four or five. Tull's seed drill was very expensive and not very reliable and therefore did not have much of an impact. Good quality seed drills were not produced until the mid 18th century.\n\nJoseph Foljambe's \"Rotherham plough\" of 1730 was the first commercially successful iron plough. The threshing machine, invented by Andrew Meikle in 1784, displaced hand threshing with a flail, a laborious job that took about one-quarter of agricultural labour. It took several decades to diffuse and was the final straw for many farm labourers, who faced near starvation, leading to the 1830 agricultural rebellion of the Swing Riots.\n\nMachine tools and metalworking techniques developed during the Industrial Revolution eventually resulted in precision manufacturing techniques in the late 19th century for mass-producing agricultural equipment, such as reapers, binders and combine harvesters.\n\nCoal mining in Britain, particularly in South Wales, started early. Before the steam engine, pits were often shallow bell pits following a seam of coal along the surface, which were abandoned as the coal was extracted. In other cases, if the geology was favourable, the coal was mined by means of an adit or drift mine driven into the side of a hill. Shaft mining was done in some areas, but the limiting factor was the problem of removing water. It could be done by hauling buckets of water up the shaft or to a sough (a tunnel driven into a hill to drain a mine). In either case, the water had to be discharged into a stream or ditch at a level where it could flow away by gravity. The introduction of the steam pump by Thomas Savery in 1698 and the Newcomen steam engine in 1712 greatly facilitated the removal of water and enabled shafts to be made deeper, enabling more coal to be extracted. These were developments that had begun before the Industrial Revolution, but the adoption of John Smeaton's improvements to the Newcomen engine followed by James Watt's more efficient steam engines from the 1770s reduced the fuel costs of engines, making mines more profitable. The Cornish engine, developed in the 1810s, was much more efficient than the Watt steam engine.\n\nCoal mining was very dangerous owing to the presence of firedamp in many coal seams. Some degree of safety was provided by the safety lamp which was invented in 1816 by Sir Humphry Davy and independently by George Stephenson. However, the lamps proved a false dawn because they became unsafe very quickly and provided a weak light. Firedamp explosions continued, often setting off coal dust explosions, so casualties grew during the entire 19th century. Conditions of work were very poor, with a high casualty rate from rock falls.\n\nAt the beginning of the Industrial Revolution, inland transport was by navigable rivers and roads, with coastal vessels employed to move heavy goods by sea. Wagonways were used for conveying coal to rivers for further shipment, but canals had not yet been widely constructed. Animals supplied all of the motive power on land, with sails providing the motive power on the sea. The first horse railways were introduced toward the end of the 18th century, with steam locomotives being introduced in the early decades of the 19th century. Improving sailing technologies boosted average sailing speed 50% between 1750 and 1830.\n\nThe Industrial Revolution improved Britain's transport infrastructure with a turnpike road network, a canal and waterway network, and a railway network. Raw materials and finished products could be moved more quickly and cheaply than before. Improved transportation also allowed new ideas to spread quickly.\n\nBefore and during the Industrial Revolution navigation on several British rivers was improved by removing obstructions, straightening curves, widening and deepening and building navigation locks. Britain had over 1000 miles of navigable rivers and streams by 1750.\n\nCanals and waterways allowed bulk materials to be economically transported long distances inland. This was because a horse could pull a barge with a load dozens of times larger than the load that could be drawn in a cart.\n\nBuilding of canals dates to ancient times. The Grand Canal in China, \"the world's largest artificial waterway and oldest canal still in existence,\" parts of which were started between the 6th and 4th centuries BC, is long and links Hangzhou with Beijing.\n\nIn the UK, canals began to be built in the late 18th century to link the major manufacturing centres across the country. Known for its huge commercial success, the Bridgewater Canal in North West England, which opened in 1761 and was mostly funded by The 3rd Duke of Bridgewater. From Worsley to the rapidly growing town of Manchester its construction cost £168,000 (£ ), but its advantages over land and river transport meant that within a year of its opening in 1761, the price of coal in Manchester fell by about half. This success helped inspire a period of intense canal building, known as Canal Mania. New canals were hastily built in the aim of replicating the commercial success of the Bridgewater Canal, the most notable being the Leeds and Liverpool Canal and the Thames and Severn Canal which opened in 1774 and 1789 respectively.\n\nBy the 1820s a national network was in existence. Canal construction served as a model for the organisation and methods later used to construct the railways. They were eventually largely superseded as profitable commercial enterprises by the spread of the railways from the 1840s on. The last major canal to be built in the United Kingdom was the Manchester Ship Canal, which upon opening in 1894 was the largest ship canal in the world, and opened Manchester as a port. However it never achieved the commercial success its sponsors had hoped for and signalled canals as a dying mode of transport in an age dominated by railways, which were quicker and often cheaper.\n\nBritain's canal network, together with its surviving mill buildings, is one of the most enduring features of the early Industrial Revolution to be seen in Britain.\n\nFrance was known for having an excellent system of roads at the time of the Industrial Revolution; however, most of the roads on the European Continent and in the U.K. were in bad condition and dangerously rutted.\n\nMuch of the original British road system was poorly maintained by thousands of local parishes, but from the 1720s (and occasionally earlier) turnpike trusts were set up to charge tolls and maintain some roads. Increasing numbers of main roads were turnpiked from the 1750s to the extent that almost every main road in England and Wales was the responsibility of a turnpike trust. New engineered roads were built by John Metcalf, Thomas Telford and most notably John McAdam, with the first 'macadamised' stretch of road being Marsh Road at Ashton Gate, Bristol in 1816. The major turnpikes radiated from London and were the means by which the Royal Mail was able to reach the rest of the country. Heavy goods transport on these roads was by means of slow, broad wheeled, carts hauled by teams of horses. Lighter goods were conveyed by smaller carts or by teams of pack horse. Stagecoaches carried the rich, and the less wealthy could pay to ride on carriers carts.\n\nReducing friction was one of the major reasons for the success of railroads compared to wagons. This was demonstrated on an iron plate covered wooden tramway in 1805 at Croydon, England.\n“A good horse on an ordinary turnpike road can draw two thousand pounds, or one ton. A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration. Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together. A horse was then attached, which drew the wagons with ease, six miles in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load.”\n\nRailways were made practical by the widespread introduction of inexpensive puddled iron after 1800, the rolling mill for making rails, and the development of the high-pressure steam engine also around 1800.\n\nWagonways for moving coal in the mining areas had started in the 17th century and were often associated with canal or river systems for the further movement of coal. These were all horse drawn or relied on gravity, with a stationary steam engine to haul the wagons back to the top of the incline. The first applications of the steam locomotive were on wagon or plate ways (as they were then often called from the cast-iron plates used). Horse-drawn public railways did not begin until the early years of the 19th century when improvements to pig and wrought iron production were lowering costs. See: Metallurgy\n\nSteam locomotives began being built after the introduction of high-pressure steam engines after the expiration of the Boulton and Watt patent in 1800. High-pressure engines exhausted used steam to the atmosphere, doing away with the condenser and cooling water. They were also much lighter weight and smaller in size for a given horsepower than the stationary condensing engines. A few of these early locomotives were used in mines. Steam-hauled public railways began with the Stockton and Darlington Railway in 1825.\n\nThe rapid introduction of railways followed the 1829 Rainhill Trials, which demonstrated Robert Stephenson's successful locomotive design and the 1828 development of Hot blast, which dramatically reduced the fuel consumption of making iron and increased the capacity the blast furnace.\n\nOn 15 September 1830, the Liverpool and Manchester Railway was opened, the first inter-city railway in the world and was attended by Prime Minister, the Duke of Wellington. The railway was engineered by Joseph Locke and George Stephenson, linked the rapidly expanding industrial town of Manchester with the port town of Liverpool. The opening was marred by problems, due to the primitive nature of the technology being employed, however problems were gradually ironed out and the railway became highly successful, transporting passengers and freight. The success of the inter-city railway, particularly in the transport of freight and commodities, led to Railway Mania.\n\nConstruction of major railways connecting the larger cities and towns began in the 1830s but only gained momentum at the very end of the first Industrial Revolution. After many of the workers had completed the railways, they did not return to their rural lifestyles but instead remained in the cities, providing additional workers for the factories.\n\nOther developments included more efficient water wheels, based on experiments conducted by the British engineer John Smeaton the beginnings of a machine industry and the rediscovery of concrete (based on hydraulic lime mortar) by John Smeaton, which had been lost for 1300 years.\n\nPrior to the Industrial Revolution, most of the workforce was employed in agriculture, either as self-employed farmers as landowners or tenants, or as landless agricultural labourers. It was common for families in various parts of the world to spin yarn, weave cloth and make their own clothing. Households also spun and wove for market production. At the beginning of the Industrial Revolution India, China and regions of Iraq and elsewhere in Asia and the Middle East produced most of the world's cotton cloth while Europeans produced wool and linen goods.\n\nIn Britain by the 16th century the putting-out system, by which farmers and townspeople produced goods for market in their homes, often described as \"cottage industry\", was being practiced. Typical putting out system goods included spinning and weaving. Merchant capitalist typically provided the raw materials, paid workers by the piece, and were responsible for the sale of the goods. Embezzlement of supplies by workers and poor quality were common problems. The logistical effort in procuring and distributing raw materials and picking up finished goods were also limitations of the putting out system.\n\nSome early spinning and weaving machinery, such as a 40 spindle jenny for about six pounds in 1792, was affordable for cottagers. Later machinery such as spinning frames, spinning mules and power looms were expensive (especially if water powered), giving rise to capitalist ownership of factories.\n\nThe majority of textile factory workers during the Industrial Revolution were unmarried women and children, including many orphans. They typically worked for 12 to 14 hours per day with only Sundays off. It was common for women take factory jobs seasonally during slack periods of farm work. Lack of adequate transportation, long hours and poor pay made it difficult to recruit and maintain workers. Many workers, such as displaced farmers and agricultural workers, who had nothing but their labour to sell, became factory workers out of necessity. (See: British Agricultural Revolution, Threshing machine)\n\nThe change in the social relationship of the factory worker compared to farmers and cottagers was viewed unfavourably by Karl Marx, however, he recognized the increase in productivity made possible by technology.\n\nSome economists, such as Robert E. Lucas, Jr., say that the real impact of the Industrial Revolution was that \"for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth ... Nothing remotely like this economic behaviour is mentioned by the classical economists, even as a theoretical possibility.\" Others, however, argue that while growth of the economy's overall productive powers was unprecedented during the Industrial Revolution, living standards for the majority of the population did not grow meaningfully until the late 19th and 20th centuries, and that in many ways workers' living standards declined under early capitalism: for instance, studies have shown that real wages in Britain only increased 15% between the 1780s and 1850s, and that life expectancy in Britain did not begin to dramatically increase until the 1870s. Similarly, the average height of the population declined during the Industrial Revolution, implying that their nutritional status was also decreasing. Real wages were not keeping up with the price of food.\n\nDuring the Industrial Revolution, the life expectancy of children increased dramatically. The percentage of the children born in London who died before the age of five decreased from 74.5% in 1730–1749 to 31.8% in 1810–1829.\n\nThe effects on living conditions the industrial revolution have been very controversial, and were hotly debated by economic and social historians from the 1950s to the 1980s. A series of 1950s essays by Henry Phelps Brown and Sheila V. Hopkins later set the academic consensus that the bulk of the population, that was at the bottom of the social ladder, suffered severe reductions in their living standards. During 1813–1913, there was a significant increase in worker wages.\n\nChronic hunger and malnutrition were the norm for the majority of the population of the world including Britain and France, until the late 19th century. Until about 1750, in large part due to malnutrition, life expectancy in France was about 35 years and about 40 years in Britain. The United States population of the time was adequately fed, much taller on average and had life expectancy of 45–50 years although U.S. life expectancy declined by a few years by the mid 19th century. Food consumption per capita also declined during an episode known as the Antebellum Puzzle.\n\nFood supply in Great Britain was adversely affected by the Corn Laws (1815-1846). The Corn Laws, which imposed tariffs on imported grain, were enacted to keep prices high in order to benefit domestic producers. The Corn Laws were repealed in the early years of the Great Irish Famine.\n\nThe initial technologies of the Industrial Revolution, such as mechanized textiles, iron and coal, did little, if anything, to lower food prices. In Britain and the Netherlands, food supply increased before the Industrial Revolution due to better agricultural practices; however, population grew too, as noted by Thomas Malthus. This condition is called the Malthusian trap, and it finally started to overcome by transportation improvements, such as canals, improved roads and steamships. Railroads and steamships were introduced near the end of the Industrial Revolution.\n\nThe very rapid growth in population in the 19th century in the cities included the new industrial and manufacturing cities, as well as service centers such as Edinburgh and London. The critical factor was financing, which was handled by building societies that dealt directly with large contracting firms. Private renting from housing landlords was the dominant tenure. P. Kemp says this was usually of advantage to tenants. People moved in so rapidly that there was not enough capital to build adequate housing for everyone, so low-income newcomers squeezed into increasingly overcrowded slums. Clean water, sanitation, and public health facilities were inadequate; the death rate was high, especially infant mortality, and tuberculosis among young adults. Cholera from polluted water and typhoid were endemic. Unlike rural areas, there were no famines such as devastated Ireland in the 1840s.\n\nA large exposé literature grew up condemning the unhealthy conditions. By far the most famous publication was by one of the founders of the Socialist movement, \"The Condition of the Working Class in England\" in 1844 Friedrich Engels described backstreet sections of Manchester and other mill towns, where people lived in crude shanties and shacks, some not completely enclosed, some with dirt floors. These shanty towns had narrow walkways between irregularly shaped lots and dwellings. There were no sanitary facilities. Population density was extremely high. Not everyone lived in such poor conditions. The Industrial Revolution also created a middle class of businessmen, clerks, foremen and engineers who lived in much better conditions.\n\nConditions improved over the course of the 19th century due to new public health acts regulating things such as sewage, hygiene and home construction. In the introduction of his 1892 edition, Engels notes that most of the conditions he wrote about in 1844 had been greatly improved. For example, the Public Health Act 1875 led to the more sanitary byelaw terraced house.\n\nIn \"The Condition of the Working Class in England\" in 1844 Friedrich Engels described how untreated sewage created awful odors and turned the rivers green in industrial cities.\n\nIn 1854 John Snow traced a cholera outbreak in Soho to fecal contamination of a public water well by a home cesspit. Snow's findings that cholera could be spread by contaminated water took some years to be accepted, but his work led to fundamental changes in the design of public water and waste systems.\n\nPre-industrial water supply relied on gravity systems and pumping of water was done by water wheels. Pipes were typically made of wood. Steam powered pumps and iron pipes allowed the widespread piping of water to horse watering troughs and households.\n\nThe invention of the paper machine and the application of steam power to the industrial processes of printing supported a massive expansion of newspaper and popular book publishing, which contributed to rising literacy and demands for mass political participation.\n\nConsumers benefited from falling prices for clothing and household articles such as cast iron cooking utensils, and in the following decades, stoves for cooking and space heating. Coffee, tea, sugar, tobacco and chocolate became affordable to many in Europe. Watches and household clocks became popular consumer items.\n\nMeeting the demands of the consumer revolution and growth in wealth of the middle classes in Britain, potter and entrepreneur Josiah Wedgwood, founder of Wedgwood fine china and porcelain, created goods such as tableware, which was starting to become a common feature on dining tables.\n\nThe Industrial Revolution was the first period in history during which there was a simultaneous increase in both population and per capita income.\n\nAccording to Robert Hughes in \"The Fatal Shore\", the population of England and Wales, which had remained steady at six million from 1700 to 1740, rose dramatically after 1740. The population of England had more than doubled from 8.3 million in 1801 to 16.8 million in 1850 and, by 1901, had nearly doubled again to 30.5 million. Improved conditions led to the population of Britain increasing from 10 million to 40 million in the 1800s. Europe's population increased from about 100 million in 1700 to 400 million by 1900.\n\nThe growth of modern industry since the late 18th century led to massive urbanisation and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In 1800, only 3% of the world's population lived in cities, compared to nearly 50% today (the beginning of the 21st century). Manchester had a population of 10,000 in 1717, but by 1911 it had burgeoned to 2.3 million.\n\nWomen's historians have debated the effect of the Industrial Revolution and capitalism generally on the status of women. Taking a pessimistic side, Alice Clark argued that when capitalism arrived in 17th century England, it lowered the status of women as they lost much of their economic importance. Clark argues that in 16th-century England, women were engaged in many aspects of industry and agriculture. The home was a central unit of production and women played a vital role in running farms, and in some trades and landed estates. Their useful economic roles gave them a sort of equality with their husbands. However, Clark argues, as capitalism expanded in the 17th century, there was more and more division of labour with the husband taking paid labour jobs outside the home, and the wife reduced to unpaid household work. Middle- and upper-class women were confined to an idle domestic existence, supervising servants; lower-class women were forced to take poorly paid jobs. Capitalism, therefore, had a negative effect on powerful women.\n\nIn a more positive interpretation, Ivy Pinchbeck argues that capitalism created the conditions for women's emancipation. Tilly and Scott have emphasised the continuity in the status of women, finding three stages in English history. In the pre-industrial era, production was mostly for home use and women produce much of the needs of the households. The second stage was the \"family wage economy\" of early industrialisation; the entire family depended on the collective wages of its members, including husband, wife and older children. The third or modern stage is the \"family consumer economy,\" in which the family is the site of consumption, and women are employed in large numbers in retail and clerical jobs to support rising standards of consumption.\n\nIdeas of thrift and hard work characterized middle-class families as the Industrial Revolution swept Europe. These values were displayed in Samuel Smiles' book \"Self-Help\", in which he states that the misery of the poorer classes was \"voluntary and self-imposed - the results of idleness, thriftlessness, intemperance, and misconduct.\"\n\nIn terms of social structure, the Industrial Revolution witnessed the triumph of a middle class of industrialists and businessmen over a landed class of nobility and gentry. Ordinary working people found increased opportunities for employment in the new mills and factories, but these were often under strict working conditions with long hours of labour dominated by a pace set by machines. As late as the year 1900, most industrial workers in the United States still worked a 10-hour day (12 hours in the steel industry), yet earned from 20% to 40% less than the minimum deemed necessary for a decent life; however, most workers in textiles, which was by far the leading industry in terms of employment, were women and children. For workers of the laboring classes, industrial life \"was a stony desert, which they had to make habitable by their own efforts.\" Also, harsh working conditions were prevalent long before the Industrial Revolution took place. Pre-industrial society was very static and often cruel – child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution.\n\nIndustrialisation led to the creation of the factory. The factory system contributed to the growth of urban areas, as large numbers of workers migrated into the cities in search of work in the factories. Nowhere was this better illustrated than the mills and associated industries of Manchester, nicknamed \"Cottonopolis\", and the world's first industrial city. Manchester experienced a six-times increase in its population between 1771 and 1831. Bradford grew by 50% every ten years between 1811 and 1851 and by 1851 only 50% of the population of Bradford was actually born there.\n\nIn addition, between 1815 and 1939, 20 percent of Europe's population left home, pushed by poverty, a rapidly growing population, and the displacement of peasant farming and artisan manufacturing. They were pulled abroad by the enormous demand for labor overseas, the ready availability of land, and cheap transportation. Still, many did not find a satisfactory life in their new homes, leading 7 million of them to return to Europe. This mass migration had large demographic impacts: in 1800, less than one percent of the world population consisted of overseas Europeans and their descendants; by 1930, they represented 11 percent. The Americas felt the brunt of this huge emigration, largely concentrated in the United States.\n\nFor much of the 19th century, production was done in small mills, which were typically water-powered and built to serve local needs. Later, each factory would have its own steam engine and a chimney to give an efficient draft through its boiler.\n\nIn other industries, the transition to factory production was not so divisive. Some industrialists themselves tried to improve factory and living conditions for their workers. One of the earliest such reformers was Robert Owen, known for his pioneering efforts in improving conditions for workers at the New Lanark mills, and often regarded as one of the key thinkers of the early socialist movement.\n\nBy 1746 an integrated brass mill was working at Warmley near Bristol. Raw material went in at one end, was smelted into brass and was turned into pans, pins, wire, and other goods. Housing was provided for workers on site. Josiah Wedgwood and Matthew Boulton (whose Soho Manufactory was completed in 1766) were other prominent early industrialists, who employed the factory system.\n\nThe Industrial Revolution led to a population increase but the chances of surviving childhood did not improve throughout the Industrial Revolution, although \"infant\" mortality rates were reduced markedly. There was still limited opportunity for education and children were expected to work. Employers could pay a child less than an adult even though their productivity was comparable; there was no need for strength to operate an industrial machine, and since the industrial system was completely new, there were no experienced adult labourers. This made child labour the labour of choice for manufacturing in the early phases of the Industrial Revolution between the 18th and 19th centuries. In England and Scotland in 1788, two-thirds of the workers in 143 water-powered cotton mills were described as children.\n\nChild labour existed before the Industrial Revolution but with the increase in population and education it became more visible. Many children were forced to work in relatively bad conditions for much lower pay than their elders, 10–20% of an adult male's wage. Children as young as four were employed. Beatings and long hours were common, with some child coal miners and hurriers working from 4 am until 5 pm. Conditions were dangerous, with some children killed when they dozed off and fell into the path of the carts, while others died from gas explosions. Many children developed lung cancer and other diseases and died before the age of 25. Workhouses would sell orphans and abandoned children as \"pauper apprentices\", working without wages for board and lodging. Those who ran away would be whipped and returned to their masters, with some masters shackling them to prevent escape. Children employed as mule scavengers by cotton mills would crawl under machinery to pick up cotton, working 14 hours a day, six days a week. Some lost hands or limbs, others were crushed under the machines, and some were decapitated. Young girls worked at match factories, where phosphorus fumes would cause many to develop phossy jaw. Children employed at glassworks were regularly burned and blinded, and those working at potteries were vulnerable to poisonous clay dust.\n\nReports were written detailing some of the abuses, particularly in the coal mines and textile factories, and these helped to popularise the children's plight. The public outcry, especially among the upper and middle classes, helped stir change in the young workers' welfare.\n\nPoliticians and the government tried to limit child labour by law but factory owners resisted; some felt that they were aiding the poor by giving their children money to buy food to avoid starvation, and others simply welcomed the cheap labour. In 1833 and 1844, the first general laws against child labour, the Factory Acts, were passed in Britain: Children younger than nine were not allowed to work, children were not permitted to work at night, and the work day of youth under the age of 18 was limited to twelve hours. Factory inspectors supervised the execution of the law, however, their scarcity made enforcement difficult. About ten years later, the employment of children and women in mining was forbidden. Although laws such as these decreased the number of child labourers, child labour remained significantly present in Europe and the United States until the 20th century.\n\nThe Industrial Revolution concentrated labour into mills, factories and mines, thus facilitating the organisation of \"combinations\" or trade unions to help advance the interests of working people. The power of a union could demand better terms by withdrawing all labour and causing a consequent cessation of production. Employers had to decide between giving in to the union demands at a cost to themselves or suffering the cost of the lost production. Skilled workers were hard to replace, and these were the first groups to successfully advance their conditions through this kind of bargaining.\n\nThe main method the unions used to effect change was strike action. Many strikes were painful events for both sides, the unions and the management. In Britain, the Combination Act 1799 forbade workers to form any kind of trade union until its repeal in 1824. Even after this, unions were still severely restricted. One British newspaper in 1834 described unions as \"the most dangerous institutions that were ever permitted to take root, under shelter of law, in any country...\"\n\nIn 1832, the Reform Act extended the vote in Britain but did not grant universal suffrage. That year six men from Tolpuddle in Dorset founded the Friendly Society of Agricultural Labourers to protest against the gradual lowering of wages in the 1830s. They refused to work for less than ten shillings a week, although by this time wages had been reduced to seven shillings a week and were due to be further reduced to six. In 1834 James Frampton, a local landowner, wrote to the Prime Minister, Lord Melbourne, to complain about the union, invoking an obscure law from 1797 prohibiting people from swearing oaths to each other, which the members of the Friendly Society had done. James Brine, James Hammett, George Loveless, George's brother James Loveless, George's brother in-law Thomas Standfield, and Thomas's son John Standfield were arrested, found guilty, and transported to Australia. They became known as the Tolpuddle Martyrs. In the 1830s and 1840s, the Chartist movement was the first large-scale organised working class political movement which campaigned for political equality and social justice. Its \"Charter\" of reforms received over three million signatures but was rejected by Parliament without consideration.\n\nWorking people also formed friendly societies and co-operative societies as mutual support groups against times of economic hardship. Enlightened industrialists, such as Robert Owen also supported these organisations to improve the conditions of the working class.\n\nUnions slowly overcame the legal restrictions on the right to strike. In 1842, a general strike involving cotton workers and colliers was organised through the Chartist movement which stopped production across Great Britain.\n\nEventually, effective political organisation for working people was achieved through the trades unions who, after the extensions of the franchise in 1867 and 1885, began to support socialist political parties that later merged to become the British Labour Party.\n\nThe rapid industrialisation of the English economy cost many craft workers their jobs. The movement started first with lace and hosiery workers near Nottingham and spread to other areas of the textile industry owing to early industrialisation. Many weavers also found themselves suddenly unemployed since they could no longer compete with machines which only required relatively limited (and unskilled) labour to produce more cloth than a single weaver. Many such unemployed workers, weavers, and others, turned their animosity towards the machines that had taken their jobs and began destroying factories and machinery. These attackers became known as Luddites, supposedly followers of Ned Ludd, a folklore figure. The first attacks of the Luddite movement began in 1811. The Luddites rapidly gained popularity, and the British government took drastic measures, using the militia or army to protect industry. Those rioters who were caught were tried and hanged, or transported for life.\n\nUnrest continued in other sectors as they industrialised, such as with agricultural labourers in the 1830s when large parts of southern Britain were affected by the Captain Swing disturbances. Threshing machines were a particular target, and hayrick burning was a popular activity. However, the riots led to the first formation of trade unions, and further pressure for reform.\n\nThe traditional centers of hand textile production such as India, parts of the Middle East and later China could not withstand the competition from machine-made textiles, which over a period of decades destroyed the hand made textile industries and left millions of people without work, many of whom starved.\n\nThe Industrial Revolution also generated an enormous and unprecedented economic division in the world, as measured by the share of manufacturing output.\nCheap cotton textiles increased the demand for raw cotton; previously, it had primarily been consumed in subtropical regions where it was grown, with little raw cotton available for export. Consequently, prices of raw cotton rose. Some cotton had been grown in the West Indies, particularly in Hispaniola, but Haitian cotton production was halted by the Haitian Revolution in 1791. The invention of the cotton gin in 1792 allowed Georgia green seeded cotton to be profitable, leading to the widespread growth of cotton plantations in the United States and Brazil. In 1791 world cotton production was estimated to be 490,000,000 pounds with U.S. production accounting to 2,000,000 pounds. By 1800 U.S. production was 35,000,000 pounds, of which 17,790,000 were exported. In 1945 the U.S. produced seven-eights of the 1,169,600,000 pounds of world production.\n\nThe Americas, particularly the U.S., had labor shortages and high priced labor, which made slavery attractive. America's cotton plantations were highly efficient and profitable, and able to keep up with demand. The U.S. Civil war created a \"cotton famine\" that lead to increased production in other areas of the world, including new colonies in Africa.\n\nThe origins of the environmental movement lay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution. The emergence of great factories and the concomitant immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centers; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste. The first large-scale, modern environmental laws came in the form of Britain's Alkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseous hydrochloric acid) given off by the Leblanc process, used to produce soda ash. An Alkali inspector and four sub-inspectors were appointed to curb this pollution. The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust and fumes under supervision.\n\nThe manufactured gas industry began in British cities in 1812–1820. The technique used produced highly toxic effluent that was dumped into sewers and rivers. The gas companies were repeatedly sued in nuisance lawsuits. They usually lost and modified the worst practices. The City of London repeatedly indicted gas companies in the 1820s for polluting the Thames and poisoning its fish. Finally, Parliament wrote company charters to regulate toxicity. The industry reached the US around 1850 causing pollution and lawsuits.\n\nIn industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to demand and achieve reforms. Typically the highest priority went to water and air pollution. The Coal Smoke Abatement Society was formed in Britain in 1898 making it one of the oldest environmental NGOs. It was founded by artist Sir William Blake Richmond, frustrated with the pall cast by coal smoke. Although there were earlier pieces of legislation, the Public Health Act 1875 required all furnaces and fireplaces to consume their own smoke. It also provided for sanctions against factories that emitted large amounts of black smoke. The provisions of this law were extended in 1926 with the Smoke Abatement Act to include other emissions, such as soot, ash, and gritty particles and to empower local authorities to impose their own regulations.\n\nThe Industrial Revolution on Continental Europe came a little later than in Great Britain. In many industries, this involved the application of technology developed in Britain in new places. Often the technology was purchased from Britain or British engineers and entrepreneurs moved abroad in search of new opportunities. By 1809, part of the Ruhr Valley in Westphalia was called 'Miniature England' because of its similarities to the industrial areas of England. The German, Russian and Belgian governments all provided state funding to the new industries. In some cases (such as iron), the different availability of resources locally meant that only some aspects of the British technology were adopted.\n\nBelgium was the second country, after Britain, in which the Industrial Revolution took place and the first in continental Europe: Wallonia (French speaking southern Belgium) was the first region to follow the British model successfully. Starting in the middle of the 1820s, and especially after Belgium became an independent nation in 1830, numerous works comprising coke blast furnaces as well as puddling and rolling mills were built in the coal mining areas around Liège and Charleroi. The leader was a transplanted Englishman John Cockerill. His factories at Seraing integrated all stages of production, from engineering to the supply of raw materials, as early as 1825.\n\nWallonia exemplified the radical evolution of industrial expansion. Thanks to coal (the French word \"houille\" was coined in Wallonia), the region geared up to become the 2nd industrial power in the world after Britain. But it is also pointed out by many researchers, with its \"Sillon industriel\", 'Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, [...] there was a huge industrial development based on coal-mining and iron-making...'. Philippe Raxhon wrote about the period after 1830: \"It was not propaganda but a reality the Walloon regions were becoming the second industrial power all over the world after Britain.\" \"The sole industrial centre outside the collieries and blast furnaces of Walloon was the old cloth making town of Ghent.\" Michel De Coster, Professor at the Université de Liège wrote also: \"The historians and the economists say that Belgium was the second industrial power of the world, in proportion to its population and its territory [...] But this rank is the one of Wallonia where the coal-mines, the blast furnaces, the iron and zinc factories, the wool industry, the glass industry, the weapons industry... were concentrated.\" \n\nWallonia was also the birthplace of a strong Socialist party and strong trade-unions in a particular sociological landscape. At the left, the \"Sillon industriel\", which runs from Mons in the west, to Verviers in the east (except part of North Flanders, in another period of the industrial revolution, after 1920). Even if Belgium is the second industrial country after Britain, the effect of the industrial revolution there was very different. In 'Breaking stereotypes', Muriel Neven and Isabelle Devious say:\n\nThe industrial revolution changed a mainly rural society into an urban one, but with a strong contrast between northern and southern Belgium. During the Middle Ages and the Early Modern Period, Flanders was characterised by the presence of large urban centres [...] at the beginning of the nineteenth century this region (Flanders), with an urbanisation degree of more than 30 per cent, remained one of the most urbanised in the world. By comparison, this proportion reached only 17 per cent in Wallonia, barely 10 per cent in most West European countries, 16 per cent in France and 25 per cent in Britain. Nineteenth century industrialisation did not affect the traditional urban infrastructure, except in Ghent [...] Also, in Wallonia the traditional urban network was largely unaffected by the industrialisation process, even though the proportion of city-dwellers rose from 17 to 45 per cent between 1831 and 1910. Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, where there was a huge industrial development based on coal-mining and iron-making, urbanisation was fast. During these eighty years the number of municipalities with more than 5,000 inhabitants increased from only 21 to more than one hundred, concentrating nearly half of the Walloon population in this region. Nevertheless, industrialisation remained quite traditional in the sense that it did not lead to the growth of modern and large urban centres, but to a conurbation of industrial villages and towns developed around a coal-mine or a factory. Communication routes between these small centres only became populated later and created a much less dense urban morphology than, for instance, the area around Liège where the old town was there to direct migratory flows.\n\nThe industrial revolution in France followed a particular course as it did not correspond to the main model followed by other countries. Notably, most French historians argue France did not go through a clear \"take-off\". Instead, France's economic growth and industrialisation process was slow and steady through the 18th and 19th centuries. However, some stages were identified by Maurice Lévy-Leboyer:\n\nBased on its leadership in chemical research in the universities and industrial laboratories, Germany, which was unified in 1871, became dominant in the world's chemical industry in the late 19th century. At first the production of dyes based on aniline was critical.\n\nGermany's political disunity – with three dozen states – and a pervasive conservatism made it difficult to build railways in the 1830s. However, by the 1840s, trunk lines linked the major cities; each German state was responsible for the lines within its own borders. Lacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways. In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railroad construction, and the railways were a major impetus for the growth of the new steel industry. Observers found that even as late as 1890, their engineering was inferior to Britain's. However, German unification in 1870 stimulated consolidation, nationalisation into state-owned companies, and further rapid growth. Unlike the situation in France, the goal was support of industrialisation, and so heavy lines crisscrossed the Ruhr and other industrial districts, and provided good connections to the major ports of Hamburg and Bremen. By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight, and pulled ahead of France\n\nDuring the period 1790–1815 Sweden experienced two parallel economic movements: an \"agricultural revolution\" with larger agricultural estates, new crops and farming tools and a commercialisation of farming, and a \"protoindustrialisation\", with small industries being established in the countryside and with workers switching between agricultural work in summer and industrial production in winter. This led to economic growth benefiting large sections of the population and leading up to a \"consumption revolution\" starting in the 1820s.\n\nDuring 1815–1850 the protoindustries developed into more specialised and larger industries. This period witnessed increasing regional specialisation with mining in Bergslagen, textile mills in Sjuhäradsbygden and forestry in Norrland. Several important institutional changes took place in this period, such as free and mandatory schooling introduced 1842 (as first country in the world), the abolition of the national monopoly on trade in handicrafts in 1846, and a stock company law in 1848.\n\nDuring 1850–1890, Sweden experienced a veritable explosion in export, dominated by crops, wood and steel. Sweden abolished most tariffs and other barriers to free trade in the 1850s and joined the gold standard in 1873.\n\nDuring 1890–1930, Sweden experienced the second industrial revolution. New industries developed with their focus on the domestic market: mechanical engineering, power utilities, papermaking and textile.\n\nThe industrial revolution began about 1870 as Meiji period leaders decided to catch up with the West. The government built railroads, improved roads, and inaugurated a land reform programme to prepare the country for further development. It inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (Foreign government advisors in Meiji Japan).\n\nIn 1871, a group of Japanese politicians known as the Iwakura Mission toured Europe and the United States to learn western ways. The result was a deliberate state-led industrialisation policy to enable Japan to quickly catch up. The Bank of Japan, founded in 1882, used taxes to fund model steel and textile factories. Education was expanded and Japanese students were sent to study in the west.\n\nModern industry first appeared in textiles, including cotton and especially silk, which was based in home workshops in rural areas.\n\nDuring the late 18th an early 19th centuries when the UK and parts of Western Europe began to industrialise, the US was primarily an agricultural and natural resource producing and processing economy. The building of roads and canals, the introduction of steamboats and the building of railroads were important for handling agricultural and natural resource products in the large and sparsely populated country of the period.\n\nImportant American technological contributions during the period of the Industrial Revolution were the cotton gin and the development of a system for making interchangeable parts, the latter aided by the development of the milling machine in the US. The development of machine tools and the system of interchangeable parts were the basis for the rise of the US as the world's leading industrial nation in the late 19th century.\n\nOliver Evans invented an automated flour mill in the mid-1780s that used control mechanisms and conveyors so that no labour was needed from the time grain was loaded into the elevator buckets until flour was discharged into a wagon. This is considered to be the first modern materials handling system an important advance in the progress toward mass production.\n\nThe United States originally used horse-powered machinery for small scale applications such as grain milling, but eventually switched to water power after textile factories began being built in the 1790s. As a result, industrialisation was concentrated in New England and the Northeastern United States, which has fast-moving rivers. The newer water-powered production lines proved more economical than horse-drawn production. In the late 19th century steam-powered manufacturing overtook water-powered manufacturing, allowing the industry to spread to the Midwest.\n\nThomas Somers and the Cabot Brothers founded the Beverly Cotton Manufactory in 1787, the first cotton mill in America, the largest cotton mill of its era, and a significant milestone in the research and development of cotton mills in the future. This mill was designed to use horse power, but the operators quickly learned that the horse-drawn platform was economically unstable, and had economic losses for years. Despite the losses, the Manufactory served as a playground of innovation, both in turning a large amount of cotton, but also developing the water-powered milling structure used in Slater's Mill.\n\nIn 1793, Samuel Slater (1768–1835) founded the Slater Mill at Pawtucket, Rhode Island. He had learned of the new textile technologies as a boy apprentice in Derbyshire, England, and defied laws against the emigration of skilled workers by leaving for New York in 1789, hoping to make money with his knowledge. After founding Slater's Mill, he went on to own 13 textile mills. Daniel Day established a wool carding mill in the Blackstone Valley at Uxbridge, Massachusetts in 1809, the third woollen mill established in the US (The first was in Hartford, Connecticut, and the second at Watertown, Massachusetts.) The John H. Chafee Blackstone River Valley National Heritage Corridor retraces the history of \"America's Hardest-Working River', the Blackstone. The Blackstone River and its tributaries, which cover more than from Worcester, Massachusetts to Providence, Rhode Island, was the birthplace of America's Industrial Revolution. At its peak over 1100 mills operated in this valley, including Slater's mill, and with it the earliest beginnings of America's Industrial and Technological Development.\n\nMerchant Francis Cabot Lowell from Newburyport, Massachusetts memorised the design of textile machines on his tour of British factories in 1810. Realising that the War of 1812 had ruined his import business but that a demand for domestic finished cloth was emerging in America, on his return to the United States, he set up the Boston Manufacturing Company. Lowell and his partners built America's second cotton-to-cloth textile mill at Waltham, Massachusetts, second to the Beverly Cotton Manufactory. After his death in 1817, his associates built America's first planned factory town, which they named after him. This enterprise was capitalised in a public stock offering, one of the first uses of it in the United States. Lowell, Massachusetts, using of canals and 10,000 horsepower delivered by the Merrimack River, is considered by some as a major contributor to the success of the American Industrial Revolution. The short-lived utopia-like Waltham-Lowell system was formed, as a direct response to the poor working conditions in Britain. However, by 1850, especially following the Irish Potato Famine, the system had been replaced by poor immigrant labour.\n\nA major U.S. contribution to industrialization was the development of techniques to make interchangeable parts from metal. Precision metal machining techniques were developed by the U.S. Department of War to make interchangeable parts for small firearms. The development work took place at the Federal Arsenals at Springfield Armory and Harpers Ferry Armory. Techniques for precision machining using machine tools included using fixtures to hold the parts in proper position, jigs to guide the cutting tools and precision blocks and gauges to measure the accuracy. The milling machine, a fundamental machine tool, is believed to have been invented by Eli Whitney, who was a government contractor who built firearms as part of this program. Another important invention was the Blanchard lathe, invented by Thomas Blanchard. The Blanchard lathe, or pattern tracing lathe, was actually a shaper that could produce copies of wooden gun stocks. The use of machinery and the techniques for producing standardized and interchangeable parts became known as the American system of manufacturing.\n\nPrecision manufacturing techniques made it possible to build machines that mechanized the shoe industry. and the watch industry. The industrialisation of the watch industry started 1854 also in Waltham, Massachusetts, at the Waltham Watch Company, with the development of machine tools, gauges and assembling methods adapted to the micro precision required for watches.\n\n \nSteel is often cited as the first of several new areas for industrial mass-production, which are said to characterise a \"Second Industrial Revolution\", beginning around 1850, although a method for mass manufacture of steel was not invented until the 1860s, when Sir Henry Bessemer invented a new furnace which could convert molten pig iron into steel in large quantities. However, it only became widely available in the 1870s after the process was modified to produce more uniform quality. Bessemer steel was being displaced by the open hearth furnace near the end of the 19th century.\n\nThis Second Industrial Revolution gradually grew to include chemicals, mainly the chemical industries, petroleum (refining and distribution), and, in the 20th century, the automotive industry, and was marked by a transition of technological leadership from Britain to the United States and Germany.\n\nThe increasing availability of economical petroleum products also reduced the importance of coal and further widened the potential for industrialisation.\n\nA new revolution began with electricity and electrification in the electrical industries. The introduction of hydroelectric power generation in the Alps enabled the rapid industrialisation of coal-deprived northern Italy, beginning in the 1890s.\n\nBy the 1890s, industrialisation in these areas had created the first giant industrial corporations with burgeoning global interests, as companies like U.S. Steel, General Electric, Standard Oil and Bayer AG joined the railroad and ship companies on the world's stock markets.\n\nThe causes of the Industrial Revolution were complicated and remain a topic for debate, with some historians believing the Industrial Revolution was an outgrowth of social and institutional changes brought by the end of feudalism in Britain after the English Civil War in the 17th century. The Enclosure movement and the British Agricultural Revolution made food production more efficient and less labour-intensive, forcing the farmers who could no longer be self-sufficient in agriculture into cottage industry, for example weaving, and in the longer term into the cities and the newly developed factories. The colonial expansion of the 17th century with the accompanying development of international trade, creation of financial markets and accumulation of capital are also cited as factors, as is the scientific revolution of the 17th century. A change in marrying patterns to getting married later made people able to accumulate more human capital during their youth, thereby encouraging economic development.\n\nUntil the 1980s, it was universally believed by academic historians that technological innovation was the heart of the Industrial Revolution and the key enabling technology was the invention and improvement of the steam engine. However, recent research into the Marketing Era has challenged the traditional, supply-oriented interpretation of the Industrial Revolution.\n\nLewis Mumford has proposed that the Industrial Revolution had its origins in the Early Middle Ages, much earlier than most estimates. He explains that the model for standardised mass production was the printing press and that \"the archetypal model for the industrial era was the clock\". He also cites the monastic emphasis on order and time-keeping, as well as the fact that medieval cities had at their centre a church with bell ringing at regular intervals as being necessary precursors to a greater synchronisation necessary for later, more physical, manifestations such as the steam engine.\n\nThe presence of a large domestic market should also be considered an important driver of the Industrial Revolution, particularly explaining why it occurred in Britain. In other nations, such as France, markets were split up by local regions, which often imposed tolls and tariffs on goods traded among them. Internal tariffs were abolished by Henry VIII of England, they survived in Russia until 1753, 1789 in France and 1839 in Spain.\n\nGovernments' grant of limited monopolies to inventors under a developing patent system (the Statute of Monopolies in 1623) is considered an influential factor. The effects of patents, both good and ill, on the development of industrialisation are clearly illustrated in the history of the steam engine, the key enabling technology. In return for publicly revealing the workings of an invention the patent system rewarded inventors such as James Watt by allowing them to monopolise the production of the first steam engines, thereby rewarding inventors and increasing the pace of technological development. However, monopolies bring with them their own inefficiencies which may counterbalance, or even overbalance, the beneficial effects of publicising ingenuity and rewarding inventors. Watt's monopoly prevented other inventors, such as Richard Trevithick, William Murdoch, or Jonathan Hornblower, whom Boulton and Watt sued, from introducing improved steam engines, thereby retarding the spread of steam power.\n\nOne question of active interest to historians is why the Industrial Revolution occurred in Europe and not in other parts of the world in the 18th century, particularly China, India, and the Middle East (which pioneered in shipbuilding, textile production, water mills, and much more in the period between 750 and 1100), or at other times like in Classical Antiquity or the Middle Ages. A recent account argued that Europeans have been characterized for thousands of years by a freedom-loving culture originating from the aristocratic societies of early Indo-European invaders. Many historians, however, have challenged this explanation as being not only Eurocentric, but also ignoring historical context. In fact, before the Industrial Revolution, \"there existed something of a global economic parity between the most advanced regions in the world economy.\" These historians have suggested a number of other factors, including education, technological changes (see Scientific Revolution in Europe), \"modern\" government, \"modern\" work attitudes, ecology, and culture.\n\nChina was the world's most technologically advanced country for many centuries; however, China stagnated economically and technologically and was surpassed by Western Europe before the Age of Exploration, by which time China banned imports and denied entry to foreigners. China was also a totalitarian society. Modern estimates of per capita income in Western Europe in the late 18th century are of roughly 1,500 dollars in purchasing power parity (and Britain had a per capita income of nearly 2,000 dollars) whereas China, by comparison, had only 450 dollars. India was essentially feudal, politically fragmented and not as economically advanced as Western Europe.\n\nHistorians such as David Landes and Max Weber credit the different belief systems in Asia and Europe with dictating where the revolution occurred. The religion and beliefs of Europe were largely products of Judaeo-Christianity and Greek thought. Conversely, Chinese society was founded on men like Confucius, Mencius, Han Feizi (Legalism), Lao Tzu (Taoism), and Buddha (Buddhism), resulting in very different worldviews. Other factors include the considerable distance of China's coal deposits, though large, from its cities as well as the then unnavigable Yellow River that connects these deposits to the sea.\n\nRegarding India, the Marxist historian Rajani Palme Dutt said: \"The capital to finance the Industrial Revolution in India instead went into financing the Industrial Revolution in Britain.\" In contrast to China, India was split up into many competing kingdoms after the decline of the Mughal Empire, with the major ones in its aftermath including the Marathas, Sikhs, Bengal Subah, and Kingdom of Mysore. In addition, the economy was highly dependent on two sectors – agriculture of subsistence and cotton, and there appears to have been little technical innovation. It is believed that the vast amounts of wealth were largely stored away in palace treasuries by totalitarian monarchs prior to the British take over.\n\nEconomic historian Joel Mokyr has argued that political fragmentation (the presence of a large number of European states) made it possible for heterodox ideas to thrive, as entrepreneurs, innovators, ideologues and heretics could easily flee to a neighboring state in the event that the one state would try to suppress their ideas and activities. This is what set Europe apart from the technologically advanced, large unitary empires such as China and India by providing \"an insurance against economic and technological stagnation\". China had both a printing press and movable type, and India had similar levels scientific and technological achievement as Europe in 1700, yet the Industrial Revolution would occur in Europe, not China or India. In Europe, political fragmentation was coupled with an \"integrated market for ideas\" where Europe's intellectuals used the lingua franca of Latin, had a shared intellectual basis in Europe's classical heritage and the pan-European institution of the Republic of Letters.\n\nIn addition, Europe's monarchs desperately needed revenue, pushing them into alliances with their merchant classes. Small groups of merchants were granted monopolies and tax-collecting responsibilities in exchange for payments to the state. Located in a region \"at the hub of the largest and most varied network of exchange in history,\" Europe advanced as the leader of the Industrial Revolution. In the Americas, Europeans found a windfall of silver, timber, fish, and maize, leading historian Peter Stearns to conclude that \"Europe's Industrial Revolution stemmed in great part from Europe's ability to draw disproportionately on world resources.\"\n\nGreat Britain provided the legal and cultural foundations that enabled entrepreneurs to pioneer the Industrial Revolution. Key factors fostering this environment were: (1) The period of peace and stability which followed the unification of England and Scotland; (2) no trade barriers between England and Scotland; (3) the rule of law (enforcing property rights and respecting the sanctity of contracts); (4) a straightforward legal system that allowed the formation of joint-stock companies (corporations); (5) absence of tolls, which had largely disappeared from Britain by the 15th century, but were an extreme burden on goods elsewhere in the world, and (6) a free market (capitalism).\n\nGeographical and natural resource advantages of Great Britain were the fact that it had extensive coastlines and many navigable rivers in an age where water was the easiest means of transportation and having the highest quality coal in Europe.\n\nThere were two main values that really drove the Industrial Revolution in Britain. These values were self-interest and an entrepreneurial spirit. Because of these interests, many industrial advances were made that resulted in a huge increase in personal wealth and a consumer revolution. These advancements also greatly benefitted the British society as a whole. Countries around the world started to recognise the changes and advancements in Britain and use them as an example to begin their own Industrial Revolutions.\n\nThe debate about the start of the Industrial Revolution also concerns the massive lead that Great Britain had over other countries. Some have stressed the importance of natural or financial resources that Britain received from its many overseas colonies or that profits from the British slave trade between Africa and the Caribbean helped fuel industrial investment. However, it has been pointed out that slave trade and West Indian plantations provided only 5% of the British national income during the years of the Industrial Revolution. Even though slavery accounted for so little, Caribbean-based demand accounted for 12% of Britain's industrial output.\nInstead, the greater liberalisation of trade from a large merchant base may have allowed Britain to produce and use emerging scientific and technological developments more effectively than countries with stronger monarchies, particularly China and Russia. Britain emerged from the Napoleonic Wars as the only European nation not ravaged by financial plunder and economic collapse, and having the only merchant fleet of any useful size (European merchant fleets were destroyed during the war by the Royal Navy). Britain's extensive exporting cottage industries also ensured markets were already available for many early forms of manufactured goods. The conflict resulted in most British warfare being conducted overseas, reducing the devastating effects of territorial conquest that affected much of Europe. This was further aided by Britain's geographical position – an island separated from the rest of mainland Europe.\nAnother theory is that Britain was able to succeed in the Industrial Revolution due to the availability of key resources it possessed. It had a dense population for its small geographical size. Enclosure of common land and the related agricultural revolution made a supply of this labour readily available. There was also a local coincidence of natural resources in the North of England, the English Midlands, South Wales and the Scottish Lowlands. Local supplies of coal, iron, lead, copper, tin, limestone and water power, resulted in excellent conditions for the development and expansion of industry. Also, the damp, mild weather conditions of the North West of England provided ideal conditions for the spinning of cotton, providing a natural starting point for the birth of the textiles industry.\n\nThe stable political situation in Britain from around 1688 following the Glorious Revolution, and British society's greater receptiveness to change (compared with other European countries) can also be said to be factors favouring the Industrial Revolution. Peasant resistance to industrialisation was largely eliminated by the Enclosure movement, and the landed upper classes developed commercial interests that made them pioneers in removing obstacles to the growth of capitalism. (This point is also made in Hilaire Belloc's \"The Servile State\".)\n\nThe French philosopher Voltaire wrote about capitalism and religious tolerance in his book on English society, \"Letters on the English\" (1733), noting why England at that time was more prosperous in comparison to the country's less religiously tolerant European neighbours. \"Take a view of the Royal Exchange in London, a place more venerable than many courts of justice, where the representatives of all nations meet for the benefit of mankind. There the Jew, the Mahometan [Muslim], and the Christian transact together, as though they all professed the same religion, and give the name of infidel to none but bankrupts. There the Presbyterian confides in the Anabaptist, and the Churchman depends on the Quaker's word. If one religion only were allowed in England, the Government would very possibly become arbitrary; if there were but two, the people would cut one another's throats; but as there are such a multitude, they all live happy and in peace.\"\n\nBritain's population grew 280% 1550–1820, while the rest of Western Europe grew 50–80%. Seventy percent of European urbanisation happened in Britain 1750–1800. By 1800, only the Netherlands was more urbanised than Britain. This was only possible because coal, coke, imported cotton, brick and slate had replaced wood, charcoal, flax, peat and thatch. The latter compete with land grown to feed people while mined materials do not. Yet more land would be freed when chemical fertilisers replaced manure and horse's work was mechanised. A workhorse needs for fodder while even early steam engines produced four times more mechanical energy.\n\nIn 1700, 5/6 of coal mined worldwide was in Britain, while the Netherlands had none; so despite having Europe's best transport, most urbanised, well paid, literate people and lowest taxes, it failed to industrialise. In the 18th century, it was the only European country whose cities and population shrank. Without coal, Britain would have run out of suitable river sites for mills by the 1830s.\n\nEconomic historian Robert Allen has argued that high wages, cheap capital and very cheap energy in Britain made it the ideal place for the industrial revolution to occur. These factors made it vastly more profitable to invest in research and development, and to put technology to use in Britain than other societies. However, two 2018 studies in \"The Economic History Review\" showed that wages were not particularly high in the British spinning sector or the construction sector, casting doubt on Allen's explanation.\n\nKnowledge of innovation was spread by several means. Workers who were trained in the technique might move to another employer or might be poached. A common method was for someone to make a study tour, gathering information where he could. During the whole of the Industrial Revolution and for the century before, all European countries and America engaged in study-touring; some nations, like Sweden and France, even trained civil servants or technicians to undertake it as a matter of state policy. In other countries, notably Britain and America, this practice was carried out by individual manufacturers eager to improve their own methods. Study tours were common then, as now, as was the keeping of travel diaries. Records made by industrialists and technicians of the period are an incomparable source of information about their methods.\n\nAnother means for the spread of innovation was by the network of informal philosophical societies, like the Lunar Society of Birmingham, in which members met to discuss 'natural philosophy' (\"i.e.\" science) and often its application to manufacturing. The Lunar Society flourished from 1765 to 1809, and it has been said of them, \"They were, if you like, the revolutionary committee of that most far reaching of all the eighteenth century revolutions, the Industrial Revolution\". Other such societies published volumes of proceedings and transactions. For example, the London-based Royal Society of Arts published an illustrated volume of new inventions, as well as papers about them in its annual \"Transactions\".\n\nThere were publications describing technology. Encyclopaedias such as Harris's \"Lexicon Technicum\" (1704) and Abraham Rees's \"Cyclopaedia\" (1802–1819) contain much of value. \"Cyclopaedia\" contains an enormous amount of information about the science and technology of the first half of the Industrial Revolution, very well illustrated by fine engravings. Foreign printed sources such as the \"Descriptions des Arts et Métiers\" and Diderot's \"Encyclopédie\" explained foreign methods with fine engraved plates.\n\nPeriodical publications about manufacturing and technology began to appear in the last decade of the 18th century, and many regularly included notice of the latest patents. Foreign periodicals, such as the \"Annales des Mines\", published accounts of travels made by French engineers who observed British methods on study tours.\n\nAnother theory is that the British advance was due to the presence of an entrepreneurial class which believed in progress, technology and hard work. The existence of this class is often linked to the Protestant work ethic (see Max Weber) and the particular status of the Baptists and the dissenting Protestant sects, such as the Quakers and Presbyterians that had flourished with the English Civil War. Reinforcement of confidence in the rule of law, which followed establishment of the prototype of constitutional monarchy in Britain in the Glorious Revolution of 1688, and the emergence of a stable financial market there based on the management of the national debt by the Bank of England, contributed to the capacity for, and interest in, private financial investment in industrial ventures.\n\nDissenters found themselves barred or discouraged from almost all public offices, as well as education at England's only two universities at the time (although dissenters were still free to study at Scotland's four universities). When the restoration of the monarchy took place and membership in the official Anglican Church became mandatory due to the Test Act, they thereupon became active in banking, manufacturing and education. The Unitarians, in particular, were very involved in education, by running Dissenting Academies, where, in contrast to the universities of Oxford and Cambridge and schools such as Eton and Harrow, much attention was given to mathematics and the sciences – areas of scholarship vital to the development of manufacturing technologies.\n\nHistorians sometimes consider this social factor to be extremely important, along with the nature of the national economies involved. While members of these sects were excluded from certain circles of the government, they were considered fellow Protestants, to a limited extent, by many in the middle class, such as traditional financiers or other businessmen. Given this relative tolerance and the supply of capital, the natural outlet for the more enterprising members of these sects would be to seek new opportunities in the technologies created in the wake of the scientific revolution of the 17th century.\n\nDuring the Industrial Revolution an intellectual and artistic hostility towards the new industrialisation developed, associated with the Romantic movement. Romanticism revered the traditionalism of rural life and recoiled against the upheavals caused by industrialization, urbanization and the wretchedness of the working classes. Its major exponents in English included the artist and poet William Blake and poets William Wordsworth, Samuel Taylor Coleridge, John Keats, Lord Byron and Percy Bysshe Shelley. The movement stressed the importance of \"nature\" in art and language, in contrast to \"monstrous\" machines and factories; the \"Dark satanic mills\" of Blake's poem \"And did those feet in ancient time\". Mary Shelley's novel \"Frankenstein\" reflected concerns that scientific progress might be two-edged. French Romanticism likewise was highly critical of industry.\n\n\n\n"}
{"id": "31289908", "url": "https://en.wikipedia.org/wiki?curid=31289908", "title": "International Oral History Association", "text": "International Oral History Association\n\nThe International Oral History Association (IOHA) is a professional association established to provide a forum for oral historians around the world. IOHA was formally constituted in June 1996 at the IXth International Oral History Conference in Gothenburg, Sweden. It holds international meetings biennially and publishes a newsletter and journal, \"Words and Silences\", in English and Spanish, and maintains a website at http://ioha.org/\n\n\n<nowiki>*</nowiki> The IOHA was formally constituted\n"}
{"id": "31253510", "url": "https://en.wikipedia.org/wiki?curid=31253510", "title": "Lawrence W. Levine Award", "text": "Lawrence W. Levine Award\n\nThe Lawrence W. Levine Award is an annual book award made by the Organization of American Historians (OAH). The award goes to the best book in American cultural history. The award is named for Professor Lawrence W. Levine, President of the OAH 1992-1993, who wrote extensively in the field. A committee of 5 members of the OAH, chosen annually by the President, makes the award. The winner receives $1000.\n\nSource: Organization of American Historians\n"}
{"id": "171574", "url": "https://en.wikipedia.org/wiki?curid=171574", "title": "Legacy of the Great Irish Famine", "text": "Legacy of the Great Irish Famine\n\nThe legacy of the Great Famine in Ireland ( or \"An Drochshaol\", litt: \"The Bad Life\") followed a catastrophic period of Irish history between 1845 and 1852 during which time the population of Ireland was reduced by 20 to 25 percent.\n\nThe famine was a watershed in the history of Ireland. Its effects permanently changed the island's demographic, political and cultural landscape. For both the native Irish and those in the resulting diaspora, the famine entered folk memory and became a rallying point for various nationalist movements. Modern historians regard it as a dividing line in the Irish historical narrative, referring to the preceding period of Irish history as \"pre-Famine.\"\n\nPolitical reaction resulting from the Great Irish Famine was muted, because of the extremely limited electoral franchise that existed at the time. While Irish politics in the 1820s to 1840s had been dominated by the Catholic Emancipation and \"Repeal\" movements under Daniel O'Connell, it was not until the 1880s under Charles Stewart Parnell, nearly forty years after the Famine, that a major Irish nationalist political movement, the Home Rule League (later known as the 'Parliamentary Party') appeared. Parnell was also instrumental in establishing the Irish Land League, to achieve land reform. (The Independent Irish Party, formed in June 1852, disintegrated within four years, but it was in major decline from 1853 when tenants benefited from a recovery in agricultural prices.)\n\nOutside the mainstream, too, reaction was slow. The 1848 Young Ireland rebellion under Thomas Davis, though occurring at the start of the Famine, was hardly impacted upon \"by\" the Famine, as much as by the clash between the \"constitutional\" nationalism and Catholicism of O'Connell and the pluralist republicanism of Davis. Another rebellion would not occur again until the 1860s under the Fenians/Irish Republican Brotherhood. Historians have speculated that, such was the economic and social impact on Ireland, the nation was numbed into inaction for decades afterwards; in other words, that politics mattered less to people than survival after the traumatic experiences of the late 1840s and early 1850s.\n\nThough its electorate was a small part of the population (as elsewhere in the United Kingdom of Great Britain and Ireland), those Irish privileged to vote continued until the mid-1880s to vote for the two major British political parties, the Conservatives and the Liberals, with more votes and seats going to the latter, even though it had been the party of government during the Famine. Because of the skewed franchise, a large body of voters continued to vote for unionists, who wished to maintain the Union that joined Britain and Ireland.\n\nThe British Royal Family avoided some censure, due to their perceived impotence in political affairs. Although some believed the myth that Queen Victoria (known in Ireland in later decades as the \"Famine Queen\") had only donated a miserly £5 to famine relief, in fact the sum was £2,000, the equivalent of £61,000 today, from her personal resources. She also was patron of a charity that fundraised. On instruction of the Lord Lieutenant of Ireland Victoria made what was largely seen as a propaganda visit in 1849. However, this visit was conducted under stringent security measures and was not free from protests or controversy. The celebrations associated with her visit just after the famine were compared to \"illuminating a graveyard\" in a newspaper editorial at the time.\n\nIt is estimated that one and a half million people died during the Famine and that a million emigrated between 1846 and 1851. A large proportion of these were Irish speakers, and the poorest districts, from which emigration continued to flow, were generally Irish-speaking. The Famine was not the only reason for the decline of the language (the general exclusion of Irish from public life and the influence of the English-speaking clergy and middle classes also played a part) but it was a conspicuous element. This led to the creation of an Ireland which thought of itself as essentially English-speaking, though with a persistent and influential reaction in the form of organisations such as the Gaelic League and the growth of a network of urban Irish-speaking activists from the late nineteenth century on.\n\nIn pre-Famine Ireland Irish was the language both of a rich folk culture and a strong literary tradition. The latter persisted in the form of Irish language manuscripts containing both prose and poetry: a single collection would give the reader access to a substantial part of the literature. Many such manuscripts were taken to America by emigrants in the 1840s and after.\n\nThe emigration of numerous Irish speakers to America as an immediate or long-term result of the Famine led to a movement there for the maintenance of the Irish language. This was marked in part by the foundation of Philo-Celtic Societies and the founding of the monthly journal \"An Gaodhal\" in 1881, the first such publication anywhere in which Irish was extensively used.\n\nIf the political elite in Ireland remained tolerant of British political parties and the monarchy, emigrants were not so. Many Irish emigrants to the United States quickly associated with separatist republican groups and organisations like the IRB. The political liberties and freedom of opportunity they encountered in the States confirmed for them the potential of an independent Ireland and often made them more passionate and optimistic than some of their brethren at home.\n\nThe Famine and its causes became a major platform for emigrant anger, as it was the main cause for most of them being emigrants in the first place. John Mitchel, a journalist by trade (who had written for Thomas Davis's newspaper, \"The Nation\" before leaving to set up his own paper, only to be arrested, tried for sedition and transported to the penal colony of Van Diemen's Land) proved to be a superb campaigner against British rule in Ireland. Analysing the famine, he wrote:\n\nMitchel's commentary expressed the anger felt by many emigrants, who saw themselves as the dispossessed, forced from Ireland by a famine they blamed on British government policies. The famine became a constant issue with Irish Americans, who to an extent unrivalled among other emigrant communities in the United States, remained emotionally attached to their native land. Leaders such as John Devoy in later decades came to play a major role in supporting Irish independence. It was no accident that the President of Dáil Éireann, Éamon de Valera in 1920 chose to travel to the United States, not elsewhere, in his efforts to get the Irish Republic recognised and accepted, or that when Michael Collins launched special bonds to fund the new Republic, many were sold to Irish Americans.\n\nA claim was made by a US professor of law, Francis A. Boyle that the Famine was genocide by the British against the Irish, meaning that the famine was part of a deliberate policy of planned extermination. One US historian, James Mullin, insists that what happened can be described as genocide, sometimes accusing other historians, statisticians and researchers who state otherwise of pushing a British point of view, or of revisionism, rewriting history to make excuses for British imperialism. However more US, British and Irish historians, such as Professors F.S.L. Lyons, John A. Murphy, Roy Foster, and James S. Donnelly, Jr, as well as historians Cecil Woodham-Smith, Peter Gray, Ruth Dudley Edwards and Cormac Ó Gráda have denied claims of a deliberate policy of genocide. All historians generally agree that British policies during the Famine (particularly those applied by the ministry of Lord John Russell) were misguided, ill-informed and counter-productive, and that had a similar crisis occurred in England instead of Ireland then the government's response would have been very different.\n\nThe famine killed one million Irish through hunger and related diseases such as cholera. A million others emigrated during the famine, with millions more following them in the following decades. The vast majority of these people were Roman Catholic, traditionally less inclined towards loyalty to the Crown. However, while it could easily be said that the famine and its after-effects ended conclusively any chance of Ireland ever being a military or economic threat to Britain again, it should be also noted that the famine's long term demographic effects were less the result of deaths from starvation (which, as with most famines, affected the old, the very young and the sick disproportionately) and more the result of emigration (which primarily affected the young population of reproductive age).\n\nThe historical evidence suggests that earlier Irish famines caused mortality of comparable magnitude to the famine of the 1840s. Some famines, in particular the that of the early 1740s, might have even killed a larger proportion of the Irish population compared to that of the mid-19th century. Nevertheless, after earlier famines the population recovered relatively quickly if only because most of the survivors did not have the resources to emigrate. By comparison, during the first half of the nineteenth century the real cost of a transatlantic voyage would fall to a point that by the 1840s even the poorest people could secure passage, albeit under wretched conditions. It seems virtually certain that economic factors alone would have caused considerable emigration from Ireland even without mass starvation. A comparison between the historical populations of Ireland and Scotland proves useful with regards to this point - even though Scotland has not experienced a catastrophic famine in the past two centuries, nevertheless, it has experienced steady emigration over that time, albeit on a slower and steadier pace compared to Ireland. As a result, while the population of Scotland surpassed that of Ireland around the beginning of the 20th century, by the end of the century the population of Ireland once again exceeded that of Scotland.\n\nIreland commemorated the 150th anniversary of the Great Famine in the 1990s. It was a contrast, in many ways, with the 100th anniversary in the 1940s. Then, only a few commemorations were held—the most significant of which was a commissioned volume of Famine history edited by R. Dudley Edwards and T. Desmond Williams (though not published until 1956), and the 'Famine Survey' undertaken by the Irish Folklore Commission in 1945. The 1990s marked a significant shift in attitudes towards commemorating the Famine, as hundreds of events took place in Ireland and throughout the Irish diaspora, some of which received sponsorship from the National Famine Commemoration Committee based in the Department of the Taoiseach, led by TD Avril Doyle. At the Great Famine Event held in Millstreet, a statement from British Prime Minister Tony Blair was read aloud, apologising for the failure of past British governments to adequately address the crisis. A large amount of new famine studies were produced, many detailing for the first time local experiences. Historians re-examined all aspects of the Famine experience; from practical issues like the number of deaths and emigrants, to the long-term impact it had on society, sexual behaviour, land holdings, property rights and the entire Irish identity. In 2010 Britain failed to send a diplomatic representative to the opening of the National Famine Commemoration, while 14 other nations did.\n\nThe Famine is also commemorated in song, both from the period and from modern times. Irish novelist and songwriter, Brendan Graham has written a number of novels and songs on An Gorta Mor – the Great Irish Famine. His book publishing deal with Harper Collins originated from a number of songs he had written about An Gorta Mor, resulting in the publication of his best selling 'documentary novel' of the Famine – The Whitest Flower (HarperCollins, London, Sydney, Toronto, 1998). The Sunday Times, Canberra called The Whitest Flower – 'An important addition to the Irish national story'. The Whitest Flower, with its song 'soundtrack', was a required text for Boston's MIT Women's Studies Course. Along with its sequel The Element of Fire (HarperCollins, 2001), The Whitest Flower was also listed as 'support fiction' for Ireland's Leaving Certificate, History syllabus.\n\n\"The Voice\" written by Brendan Graham and performed by Eimear Quinn won the 1996 Eurovision Song Contest for Ireland. The lyrics refer to Ireland's troubled history and point clearly to Famine times in Ireland, \"I am The Voice of your hunger and pain\".\n\nAs well as entering the British Pop Charts, The Voice was one of the songs studied for the UK GCSE Music Syllabus, 1998. Eimear Quinn's version featured in the Pierce Brosnan movie – The Nephew, while the song found a new life in America with the recording and widespread PBS broadcasting of it as part of the group Celtic Woman's rise to prominence there. Author and sociologist E. Moore Quinn in her book 'Irish American Folklore in New England', published in 2009, quotes the full lyric of The Voice.\n\n\"The Fairhaired Boy\" – This song written by Brendan Graham was recorded by Cathy Jordan and Dervish on their 2003 album, 'Spirit'. Carmel Conway also recorded the song on her 2009 album 'This Beautiful Day'. It is also performed in the 'Cois Tine – Stories of Liam O' Flaherty' – by singer and violinist Fionnuala Howard. A song of emigration from Ireland during Famine times, The Fairhaired Boy tells of the sorrow of parting – 'Soon you'll in California be or Colorado bound'. In The Whitest Flower, Graham's heroine Ellen sings the song to Roberteen, a young neighbour from Ireland whom she finds dying in the lazaretto (fever shed) at Canada's Quarantine Island of Grosse Ile. The song is written in a traditional narrative style song form where there are no choruses, the hook of the song being contained in the last line of each stanza with the pull of the story being used to keep the listener's interest alive.\n\n\"Crucán na bPáiste\" – 'the burial place of (unbaptised) children' – lies on a hilltop in Maamtrasna, Co.Mayo, overlooking Lough Nafooey, and Lough Mask in Ireland. It is a lament by a mother for a child she buries there during Aimsir an Drochshaoil ('The Time of the Bad Life' – the Famine). The song was written by Graham for Ellen Rua, one of the characters in his second novel, The Brightest Day, The Darkest Night, also published by Harper Collins. It has been recorded and performed by a number of artistes most notably Karen Matheson (as part of the Transatlantic Sessions), Cathy Jordan of Dervish and Eimear Quinn. Graham reveals the story of how the song came to be written in his Sunday Miscellany radio piece for RTÉ, called \"Effin' Songs\", recorded live in Ireland's National Concert Hall, with Eimear Quinn and the RTÉ Concert Orchestra and piper Neil Martin, following with the song itself.\n\n\"Crucán na bPáiste had become a claw in my gut – and my pilgrimage. Over many months it inched out in me its cry...focal by focal...line by line...until I was set free and it had found its epiphany. I had learned to keep out of the way...let the song write itself. This, I suppose is the real answer to the question with which we started. The truly special songs write us...we don't write them; we don't find them...they find us\".\n\nAnother of Graham's Famine songs, \"Ochón an Gorta Mor / Lament of the Great Hunger\", was commissioned by the Irish Government, as part of the Ceól Reoite (Frozen Music – after Goethe's 'Architecture is frozen music') Millennial Project. Fourteen Irish composers were asked to pick a monument of national significance and to write a piece of music/song which would release from it the music frozen within. Graham chose the Curvilinear Glasshouses at Dublin's National Botanic Gardens, constructed at the time of An Gorta Mor, by monies diverted from research to find a cure for the potato blight afflicting Ireland. The glasshouses looked down over the Gardens' 'vegetable patch', where the blight was first discovered in Ireland in August 1845. Graham has described the 'frozen music' locked within the architecture of the Curvilinear Glasshouses as 'a lament for a famished people'. A song for unaccompanied voice it has been recorded by Róisín Elsafty, on the Ceól Reoite album and as a 'hidden track' by Cathy Jordan on the Dervish album, Spirit . The song was also performed by Nuala Ní Chanainn in the 2002 production of Aistir/Voyage by the Swiss- based, Cathy Sharp Dance Ensemble.\n\n\"You Raise Me Up\" - It was in fact reading Graham's novel The Whitest Flower, that led Norwegian composer, Rolf Lovland to contact Graham with a melody. This melody in turn inspired Graham to write the lyric -You Raise Me Up, which has been recorded by some 400 artists (including Westlife, Josh Groban, Brian Kennedy and Secret Garden, Daniel O'Donnell, Helene Fischer, Il Divo, Russell Watson and Paul Potts) and has become one of the most successful songs in popular music history.\n\nBrendan Graham has also recently written a number of integrated song and narrative pieces including Writing the Famine in Fiction and Song, for The National Famine Commemoration Week in Ireland, 2010. This was narrated by the author with songs performed by Cathy Jordan accompanied on piano by Feargal Murray.\n\nIn Quebec, 2011, his shorter narrative and song piece – From Famine to Freedom – Ireland to Grosse Ile – was performed by the Quebec Symphony Orchestra and soloist Méav Ní Mhaolchatha, with Graham's narration translated into French. It included a first performance with orchestra of The Whitest Flower, Graham's title song for the soundtrack to his book. In response to the view handed down at the time of Ireland's Famine that \"The judgement of God…sent the calamity to teach the Irish a lesson, that calamity must not be too much mitigated\" (Charles E. Trevelyan – Permanent Assistant Secretary at the British Treasury with prime responsibility for Famine relief in Ireland), Graham's song calls to task a vengeful God:-\n\nIn August 2012 Brendan Graham composed and presented From Famine to Freedom—Ireland to Australia, a commemoration in word and song of those who suffered during An Gorta Mór–The Great Irish Famine–and of those who fled the Famine to establish a new life Australia. Graham wrote a new song called, \"Orphan Girl\", dedicated to the memory of the 4,112, mainly teenage Irish orphan girls, who were given a free passage to Australia from Workhouses in every county of Ireland between 1848 and 1850. Brendan Graham was joined by Australian singer-songwriter, Sarah Calderwood who unites classic with contemporary folk. She is the charismatic front woman of Australia's premier Celtic group, Sunas, who released the ABC best-selling debut album ‘As Night Falls’. Graham and Calderwood were also joined on the day by the inspirational Australian Girls Choir, who have previously performed for Nelson Mandela, Oprah Winfrey, Queen Elizabeth II and President Obama. The commemoration in words and song was a community day of celebration and remembering, especially for those who are descendants of these Irish Workhouse young women.For more information see www.irishfaminememorial.org\n\nMore recently in March 2013 The Possible Dreams Choir toured Victoria in Australia, singing \"Orphan Girl\" (sung by Fortunate and Nomcebo) and \"You Raise Me Up\" as part of their Voices for the Voiceless tour. Possible Dreams International, Inc is a non-profit organisation which partners with rural and remote communities in Swaziland, Southern Africa to empower families and individuals living with extreme poverty, malnutrition and endemic disease. So in Graham's \"Famine in Song\" lyrics, there resonates a global reach.\n\nA famous modern song on the famine is \"The Fields of Athenry\", by Pete St. John. Written in three verses, it deals with a fictitious but realistic story of \"Michael\" being deported to Botany Bay for stealing corn to feed his starving family. Performed in folk, traditional and even reggae versions, it is often sung by supporters of Glasgow's Celtic F.C., many of whom are of Irish descent. The song itself sums up the sense of despair, anger and bitterness of famine victims. The song was also covered by Boston punk rock band, the Dropkick Murphys on their 2003 \"Blackout\" album.\nLuka Bloom's song 'Forgiveness' from his album Salty Heaven is sung from the point of view of an Irish Famine refugee who has relocated to Canada and who despite his suffering has chosen forgiveness over bitterness.\n\nLuka Bloom's brother Christy Moore also has a song, written by Bloom but recorded by Moore, called 'The City of Chicago,' that chronicles the effects of the Famine and the subsequent mass emigration.\n\nPagan metal band Primordial also have a song about the Famine named \"The Coffin Ships\" on their 2005 album \"The Gathering Wilderness\".\n\nAnother related song is \"Famine\" by Sinéad O'Connor, released on the Universal Mother album. The lyrics emphasise the political aspect of the famine.\n\nIreland has been at the forefront of international famine relief. In 1985 Bob Geldof, Irish rock star and founder of Live Aid, revealed that the people of Ireland had given more to his fundraising efforts per head of population than any other nation in the world. Irish NGOs \"Goal\", \"Concern\", \"Trócaire\" and \"Gorta\" play a central role in helping famine victims throughout Africa. In 2000, Bono, lead singer with Irish band U2, played a central role in campaigning for debt relief for African nations in the Jubilee 2000 campaign. The Irish famine experience continues to influence many Irish people in their attitudes towards the developing world and famine victims everywhere.\n\n\n"}
{"id": "33699756", "url": "https://en.wikipedia.org/wiki?curid=33699756", "title": "List of royal titles", "text": "List of royal titles\n\nRoyal titles include:\n\n"}
{"id": "39224823", "url": "https://en.wikipedia.org/wiki?curid=39224823", "title": "List of shipyards in Chile", "text": "List of shipyards in Chile\n\nIn order to support amphibious operations during the landing in Pisagua by carrying significant quantities of cargo, and landing troops directly onto an unimproved shore, the Government built flat-bottomed landing craft, called \"Chalanas\". They transported 1,200 men in the first landing and took onboard 600 men in less than 2 hours for the second landing.\n\nASMAR or Astilleros y Maestranzas de la Armada is a Chilean state-owned shipyard service dealing both with military and civilian vessels.\n\n\nLocated in \"Caleta de la Barca\", (today \"Caleta Abarca\"), this shipyard registers works:\n\nLocated in Valdivia, Chile\n\n\n\n1845 in Valparaíso\n\n\nLocated in Valparíso\n\n\nMARCO is a company founded in Iquique, dedicated to construction, repair, and rebuild of steel vessels of up to 95M Length overall. The shipyard also offers consulting and engineering work services.\n\n\nLocated in Valdivia\n\n\n\n"}
{"id": "40671", "url": "https://en.wikipedia.org/wiki?curid=40671", "title": "List of sports history organisations", "text": "List of sports history organisations\n\nThis is a list of sports history organizations\n\n"}
{"id": "204504", "url": "https://en.wikipedia.org/wiki?curid=204504", "title": "Millennium", "text": "Millennium\n\nA millennium (plural millennia or millenniums) is a period equal to 1000 years, also called kiloyears. It derives from the Latin ', thousand, and ', year. It is often, but not always, related to a particular dating system.\n\nSometimes, it is used specifically for periods of a thousand years that begin at the starting point (initial reference point) of the calendar in consideration (typically the year \"1\"), or in later years that are whole number multiples of a thousand years after it. The term can also refer to an interval of time beginning on any date. Frequently in the latter case (and sometimes also in the former) it may have religious or theological implications (see millenarianism).\n\nThere are two methods of counting years: current years (the count begins at the epoch) and elapsed years (the count is of completed years since the epoch).\n\nThe original method of counting years was ordinal, whether \"1st year AD\" or regnal \"10th year of King Henry VIII\". This ordinal numbering is still used in the names of the millennia and centuries, for example the \"1st millennium\" or the \"10th century\", and sometimes in the names of decades, e.g., \"1st decade of the 11th century\".\n\nThe main issues arise from the \"content\" of the various year ranges. Similar issues affect the \"contents\" of centuries. Decades are not subject to ambiguity, as they are named according to their leading numbers: the decade called the 1990s by its naming does not include 2000.\n\nThose following ordinal year names naturally choose\nThose who are influenced by the leading digit equally naturally choose\n\nThere are two viewpoints about how millennia should be thought of in practice. One viewpoint relies on the formal operation of the calendar, while the other appeals to popular culture. Stephen Jay Gould argued that the choice is arbitrary, and since the question revolves around rules made by people, rather than a natural phenomenon that is subject to experimental measurement, the matter cannot be resolved.\n\nThe ISO 8601, employed in a number of contexts, uses the astronomical calendar, in which year counting starts at 0. Thus, when using this calendar, the millennium starts at x000 and ends at x999. There was a popular debate leading up to the celebrations of the year 2000 as to whether the beginning of that year should be understood (and celebrated) as the beginning of a new millennium. Historically, there has been debate around the turn of previous decades, centuries, and millennia.\n\nThe issue is tied to the convention of using ordinal numbers to count millennia (as in \"the third millennium\"), as opposed to \"the two thousands\", which is unambiguous as it does not depend on which year counting starts. The first convention is common in English speaking countries, but the latter is favored in for example Sweden (\"tvåtusentalet\", which translates literally as the \"two thousands period\").\n\nThose holding that the arrival of the new millennium should be celebrated in the transition from 2000 to 2001 (i.e., December 31, 2000 to January 1, 2001), argued that because the Gregorian calendar has no year zero, the millennia should be counted from 1 AD. Thus the first period of one thousand complete years runs from the beginning of 1 AD to the end of 1000 AD, and the beginning of the second millennium took place at the beginning of 1001. \n\nArthur C. Clarke gave this analogy (from a statement received by Reuters): \"If the scale on your grocer's weighing machine began at 1 instead of 0, would you be happy when he claimed he'd sold you 10 kg of tea?\" This statement illustrates the common confusion about the calendar.\n\nIf one counts from the beginning of AD 1 to the ending of AD 1000, one would have counted 1000 years. The next 1000 years (millennium) would begin on the first day of 1001. So the calendar has not 'cheated' anyone out of a year. Clarke made reference to this viewpoint in his book \"\" referring to the Millennium Celebrations on December 31, 2000. In other words, the argument is based on the fact that the last year of the first two thousand years in the Gregorian calendar was 2000, not 1999.\n\nThe \"year 2000\" has also been a popular phrase referring to an often utopian future, or a year when stories in such a future were set, adding to its cultural significance. There was also media and public interest in the Y2K bug. People liked to compared their odometer as a reason to celebrate the new millennium which goes from 1999 to 2000. Thus, the populist argument was that the new millennium should begin when the zeroes \"rolled over\" to 2000, i.e., the day after December 31, 1999. People felt that the change of the hundreds digit in the year number, and the zeros rolling over, created a sense that a new century had begun.\n\nThis is similar to the common demarcation of decades by their most significant digits, e.g., naming the period 1980 to 1989 as the 1980s or \"the eighties\". Similarly, it would be valid to celebrate the year 2000 as a cultural event in its own right, and name the period 2000 to 2999 would be \"the 2000s\". In other words, the time period between 1 and 999 (999 years only) would be called the \"0s\" and the period between 1000 and 1999 would be the \"1000s\".\n\nThe popular approach was to treat the end of 1999 as the end of a millennium and to hold millennium celebrations at midnight between December 31, 1999 and January 1, 2000, as per viewpoint 2. The cultural and psychological significance of the events listed above combined to cause celebrations to be observed one year earlier than the formal Gregorian date. This does not establish that insistence on the formal Gregorian date is \"incorrect\", though some view it as pedantic.\n\nSome event organisers hedged their bets by calling their 1999 celebrations things like \"Click\" referring to the odometer-like rolling over of the nines to zeros. A second approach was to adopt two different views on the millennium problem and celebrate the new millennium twice.\n\nStephen Jay Gould, in his essay \"Dousing Diminutive Dennis' Debate (or DDDD = 2000)\" (\"Dinosaur in a Haystack\"), discussed the \"high\" versus \"pop\" culture interpretation of the transition. Gould noted that the high culture, strict construction had been the dominant viewpoint at the 20th century's beginning, but that the pop culture viewpoint dominated at its end. Gould also included comments on adjustments to the calendar, such as those by Dionysius Exiguus (the eponymous \"Diminutive Dennis\") and the timing of celebrations over different transitional periods. Further of his essays on this topic are collected in \"Questioning the Millennium: A Rationalist's Guide to a Precisely Arbitrary Countdown\".\n\n"}
{"id": "440114", "url": "https://en.wikipedia.org/wiki?curid=440114", "title": "Mu (lost continent)", "text": "Mu (lost continent)\n\nMu is the name of a suggested lost continent whose concept and name were proposed by 19th-century traveler and writer Augustus Le Plongeon, who claimed that several ancient civilizations, such as those of Egypt and Mesoamerica, were created by refugees from Mu — which he stated was located in the Atlantic Ocean. This concept was popularized and expanded by James Churchward, who asserted that Mu was once located in the Pacific.\n\nThe existence of Mu was already being disputed in Le Plongeon's time. Currently scientists dismiss the concept of Mu (and other alleged lost continents such as Lemuria) as physically impossible, arguing that a continent can neither sink nor be destroyed in the short period of time required by this premise. Mu's existence is considered to have no factual basis.\n\nThe mythical idea of Mu first appeared in the works of Augustus Le Plongeon (1825–1908), after his investigations of the Maya ruins in Yucatán. He claimed that he had translated ancient Mayan writings which supposedly showed that the Maya civilization of the Yucatán was older than those of Greece and Egypt, and told the story of an even older continent.\n\nLe Plongeon actually got the name \"Mu\" from Charles Étienne Brasseur de Bourbourg, who in 1864 mistranslated what was then called the Troano Codex using the de Landa alphabet. Brasseur believed that a word which he read as \"Mu\" referred to a land which had been submerged by a catastrophe. Le Plongeon then identified this lost land with Atlantis, and turned it into a continent which had supposedly sunk into the Atlantic Ocean:\n\nLe Plongeon claimed that the civilization of ancient Egypt was founded by Queen Moo, a refugee from the land's demise. Other refugees supposedly fled to Central America and became the Maya.\n\nMu, as a lost Pacific Ocean continent, was later popularised by James Churchward (1851–1936) in a series of books, beginning with \"Lost Continent of Mu, the Motherland of Man\" (1926), re-edited later as \"The Lost Continent Mu\" (1931). Other popular books in the series are \"The Children of Mu\" (1931), and \"The Sacred Symbols of Mu\" (1933).\n\nChurchward claimed that \"more than fifty years ago\", while he was a soldier in India, he befriended a high-ranking temple priest who showed him a set of ancient \"sunburnt\" clay tablets, supposedly in a long lost \"Naga-Maya language\" which only two other people in India could read. Having mastered the language himself, Churchward found out that they originated from \"the place where [man] first appeared—Mu\". The 1931 edition states that \"all matter of science in this work are based on translations of two sets of ancient tablets\": the clay tablets he read in India, and a collection of 2,500 stone tablets that had been uncovered by William Niven in Mexico.\n\nChurchward gave a vivid description of Mu as the home of an advanced civilization, the Naacal, which flourished between 50,000 and 12,000 years ago, was dominated by a “white race\", and was \"superior in many respects to our own\". At the time of its demise, about 12,000 years ago, Mu had 64,000,000 inhabitants and many large cities, and colonies on the other continents.\n\nChurchward claimed that the landmass of Mu was located in the Pacific Ocean, and stretched east–west from the Marianas to Easter Island, and north–south from Hawaii to Mangaia. He claimed that according to the creation myth he read in the Indian tablets, Mu had been lifted above sea level by the expansion of underground volcanic gases. Eventually Mu \"was completely obliterated in almost a single night\": after a series of earthquakes and volcanic eruptions, \"the broken land fell into that great abyss of fire\" and was covered by \"fifty millions of square miles of water.\"\n\nChurchward claimed that Mu was the common origin of the great civilizations of Egypt, Greece, Central America, India, Burma and others, including Easter Island, and was in particular the source of ancient megalithic architecture. As evidence for his claims, he pointed to symbols from throughout the world, in which he saw common themes of birds, the relation of the Earth and the sky, and especially the Sun. Churchward claims that the king of Mu was named Ra and he relates this to the Egyptian god of the sun, Ra, and the Rapa Nui word for Sun, \"ra’a\", which he incorrectly spells \"\"raa\".\" He claimed to have found symbols of the Sun in \"Egypt, Babylonia, Peru and all ancient lands and countries – it was a universal symbol.\"\n\nChurchward attributed all megalithic art in Polynesia to the people of Mu. He claimed that symbols of the sun are found \"depicted on stones of Polynesian ruins\", such as the stone hats (\"pukao\") on top of the giant \"moai\" statues of Easter Island. Citing W.J. Johnson, Churchward describes the cylindrical hats as \"spheres\" that \"seem to show red in the distance\", and asserts that they “represent the Sun as Ra.” He also incorrectly claimed that some of them are made of \"red sandstone\", which does not exist on the island. The platforms on which the statues rest (\"ahu\") are described by Churchward as being \"platform-like accumulations of cut and dressed stone\", which were supposedly left in their current positions \"awaiting shipment to some other part of the continent for the building of temples and palaces\". He also cites the pillars \"erected by the Māori of New Zealand\" as an example of this lost civilization's handiwork. In Churchward's view, the present-day Polynesians are not descendants of the dominant members of the lost civilization of Mu, responsible for these great works, but are instead descendants of survivors of the cataclysm that adopted \"the first cannibalism and savagery\" in the world.\n\nJames Bramwell and William Scott-Elliott claimed that the cataclysmic events on Mu began 800,000 years ago and went on until the last catastrophe, which occurred in precisely 9564 BC.\n\nIn the 1930s, Atatürk, founder of the Turkish Republic, was interested in Churchward's work and considered Mu as a possible location of the original homeland of the Turks.\n\nMasaaki Kimura has suggested that certain underwater features located off the coast of Yonaguni Island, Japan (popularly known as the Yonaguni Monument) are ruins of Mu (or \"ruins of the lost world of Muin\" according to CNN).\n\nModern geological knowledge rules out \"lost continents\" of any significant size. According to the theory of plate tectonics, which has been extensively confirmed since the 1970s, the Earth's crust consists of lighter \"sial\" rocks (continental crust rich in aluminium silicates) that float on heavier \"sima\" rocks (oceanic crust richer in magnesium silicates). The sial is generally absent in the ocean floor where the crust is a few kilometers thick, while the continents are huge solid blocks tens of kilometers thick. Since continents float on the sima much like icebergs float on water, a continent cannot simply \"sink\" under the ocean.\n\nIt is true that continental drift and seafloor spreading can change the shape and position of continents and occasionally break a continent into two or more pieces (as happened to Pangaea). However, these are very slow processes that occur in geological time scales (hundreds of millions of years). Over the scale of history (tens of thousands of years), the sima under the continental crust can be considered solid, and the continents are basically anchored on it. It is almost certain that the continents and ocean floors have retained their present position and shape for the whole span of human existence.\n\nThere is also no conceivable event that could have \"destroyed\" a continent, since its huge mass of sial rocks would have to end up somewhere—and there is no trace of it at the bottom of the oceans. The Pacific Ocean islands are not part of a submerged landmass but rather the tips of isolated volcanoes.\n\nThis is the case, in particular, of Easter Island, which is a recent volcanic peak surrounded by deep ocean (3,000 m deep at 30 km off the island). After visiting the island in the 1930s, Alfred Metraux observed that the \"moai\" platforms are concentrated along the current coast of the island, which implies that the island's shape has changed little since they were built. Moreover, the \"Triumphal Road\" that Pierre Loti had reported ran from the island to the submerged lands below, is actually a natural lava flow. Furthermore, while Churchward was correct in his claim that the island has no sandstone or sedimentary rocks, the point is moot because the \"pukao\" are all made of native volcanic scoria.\n\nThere is evidence that the civilizations of the Americas and the Old World developed independently of each other and, in fact, agriculture and urban societies probably first developed after the end of the Ice Age, somewhere in the Levant some 10,000 years ago and gradually spread outwards from there to the rest of the Old World. The development of the oldest known cities, such as Çatalhöyük, can more easily be attributed to local and gradual evolution than to the coming of refugees from a \"superior civilization\". \n\nEaster Island was first settled around 300 AD and the \"pukao\" on the \"moai\" are regarded as ceremonial, or traditional headdresses. There is no evidence of a highly advanced civilization on the island.\n\nOther researchers who have tried to use the de Landa alphabet have reported that it produces only gibberish. Recent research into the Mayan \"alphabet\" has shown it to not consist of letters but logograms. Recent translations of the Troano Codex have shown it to be a treatise on astrology.\n\n\n\n\n\n\n"}
{"id": "404013", "url": "https://en.wikipedia.org/wiki?curid=404013", "title": "National Historic Landmark", "text": "National Historic Landmark\n\nA National Historic Landmark (NHL) is a building, district, object, site, or structure that is officially recognized by the United States government for its outstanding historical significance. Of over 90,000 places listed on the country's National Register of Historic Places, only some 2,500 are recognized as National Historic Landmarks.\n\nA National Historic Landmark District may include contributing properties that are buildings, structures, sites or objects, and it may include non-contributing properties. Contributing properties may or may not also be separately listed.\n\nPrior to 1935, efforts to preserve cultural heritage of national importance were made by piecemeal efforts of the United States Congress. In 1935, Congress passed the Historic Sites Act, which authorized the Interior Secretary authority to formally record and organize historic properties, and to designate properties as having \"national historical significance\", and gave the National Park Service authority to administer historically significant federally owned properties. Over the following decades, surveys such as the Historic American Buildings Survey amassed information about culturally and architecturally significant properties in a program known as the Historic Sites Survey. Most of the designations made under this legislation became National Historic Sites, although the very first designation, made December 20, 1935, was for a National Memorial, the Gateway Arch National Park (then known as the Jefferson National Expansion Memorial) in St. Louis, Missouri. The first National Historic Site designation was made for the Salem Maritime National Historic Site on March 17, 1938.\n\nIn 1960, the National Park Service took on the administration of the survey data gathered under this legislation, and the National Historic Landmark program began to take more formal shape. When the National Register of Historic Places was established in 1966, the National Historic Landmark program was encompassed within it, and rules and procedures for inclusion and designation were formalized. Because listings (either on the National Register, or as an NHL) often triggered local preservation laws, legislation in 1980 amended the listing procedures to require owner agreement to the designations.\n\nOn October 9, 1960, 92 properties were announced as designated NHLs by Secretary of the Interior Fred A. Seaton. The first of these was a political nomination: the Sergeant Floyd Monument in Sioux City, Iowa was officially designated on June 30 of that year, but for various reasons, the public announcement of the first several NHLs was delayed.\n\nNHLs are designated by the United States Secretary of the Interior because they are:\n\nMore than 2,500 NHLs have been designated. Most, but not all, are in the United States.\n\nThere are NHLs in all 50 states and the District of Columbia. Three states (Pennsylvania, Massachusetts, and New York) account for nearly 25 percent of the nation's NHLs. Three cities within these states (Boston, Philadelphia, and New York City) all separately have more NHLs than 40 of the 50 states. In fact, New York City alone has more NHLs than all but five states: Virginia, California, Pennsylvania, Massachusetts, and New York (the latter of which has the most NHLs of all 50 states). There are 74 NHLs in the District of Columbia.\n\nSome NHLs are in U.S. commonwealths and territories, associated states, and foreign states. There are 15 in Puerto Rico, the Virgin Islands, and other U.S. commonwealths and territories; five in U.S.-associated states such as Micronesia; and one in Morocco.\n\nOver 100 ships or shipwrecks have been designated as NHLs.\n\nAbout half of the National Historic Landmarks are privately owned. The National Historic Landmarks Program relies on suggestions for new designations from the National Park Service, which also assists in maintaining the landmarks. A friends' group of owners and managers, the National Historic Landmark Stewards Association, works to preserve, protect and promote National Historic Landmarks.\n\nIf not already listed on the National Register of Historic Places, an NHL is automatically added to the Register upon designation. About three percent of Register listings are NHLs.\n\n\n"}
{"id": "38347912", "url": "https://en.wikipedia.org/wiki?curid=38347912", "title": "Pre-Māori settlement of New Zealand theories", "text": "Pre-Māori settlement of New Zealand theories\n\nSince the 18th century, Europeans have been interested in the origins of human migration and the settlement of New Zealand. Captain James Cook, who arrived in 1769, believed that the Māori were Polynesian and had come from southeast Asia; however, some other early visitors speculated that they might be descended from ancient Greeks, Romans or Egyptians, and some Christian missionaries thought that the Māori ancestors belonged to the lost tribes of Israel.\n\nDuring the 19th century, ideas about Aryan migrations became popular and these were applied to New Zealand. Edward Tregear's \"The Aryan Maori\" (1885) suggested that Aryans from India migrated to the southeast Asia and then to the islands of the Pacific, including New Zealand.\n\nIn the early 20th century, the Moriori people were thought to be possibly of Melanesian rather than Polynesian origin, but they are now regarded as descended from early Maori of the Archaic or Moa-hunter period.\n\nAlthough modern archaeology has largely clarified questions of the origin and dates of the earliest migrations, some writers have continued to speculate that what is now New Zealand was discovered by 'Celts', Greeks or Egyptians, before the arrival of the Polynesian ancestors of the Māori.\n\nMaori oral traditions speak of spirits or fairy folk living in parts of New Zealand when they arrived. They are known by various names, but most commonly as Patupaiarehe and Turehu.\n\nMartin Doutré argued in a 1999 book that New Zealand had a pre-Polynesian Celtic population, and that boulders with petroglyphs on a hill in Silverdale in Auckland are artifacts left by those people. An earlier presentation of the theory of pre-Polynesian white settlement of New Zealand was Kerry Bolton's 1987 pamphlet \"Lords of the Soil\", which states that \"Polynesia has been occupied by peoples of the Europoid race since ancient times\".\n\nOther books presenting such theories have included \"The Great Divide: The Story of New Zealand & its Treaty\", (2012) by Ian Wishart, a journalist, and \"To the Ends of the Earth\" by Maxwell C. Hill, Gary Cook and Noel Hilliam, which claims that New Zealand was discovered by explorers from ancient Egypt and Greece.\n\nDavid Rankin, a Ngāpuhi elder, has drawn attention to Maori legends suggesting that people, some of them with fair skin, were already present in the islands when the Maori arrived, and has claimed the existence of a conspiracy among academics to suppress inquiry.\n\nHistorians and archaeologists dismiss the theories. Michael King wrote in his history of New Zealand, \"Despite a plethora of amateur theories about Melanesian, South American, Egyptian, Phoenician and Celtic colonisation of New Zealand, there is not a shred of evidence that the first human settlers were anything other than Polynesian\", and Richard Hill, professor of New Zealand Studies at Victoria University of Wellington, said in 2012, \"Not one of [the theories] has ever passed any remote academic scrutiny.\" Hugh Laracy of the University of Auckland called them \"wild speculation\" that has been \"thoroughly disposed of by academic specialists\".\n\nAnother historian, Vincent O'Malley, and the New Zealand Archaeological Association regard the theories as having a racist or at least a political element, seeking to cast doubt on Waitangi Tribunal claims. Scott Hamilton in \"No to Nazi Pseudo-history: an Open Letter\" further explains objections to the theories of Bolton and Doutré (and the website \"Ancient Celtic New Zealand\").\n\n\n\n"}
{"id": "11716813", "url": "https://en.wikipedia.org/wiki?curid=11716813", "title": "Preservation of meaning", "text": "Preservation of meaning\n\nPreservation of meaning in library, archival or museum collections involves understanding spiritual, ritual, or cultural perceptions of value for specific objects, and ensuring these values are maintained and respected. Meaning is something assigned to objects of cultural or spiritual significance based on interpretations and perceived values by user populations, a process known as social construction of an object (Barker, 77). When moved to memory institutions such as libraries or museums, these objects of social construction require unique approaches to preservation and maintenance in order to remain relevant as representations of cultural or spiritual societies.\n\nIn many memory institutions of the Western World, including libraries and museums, focus is often placed on the informational content and physical attributes, or artifactual value, of collected materials. Preservation policies are primarily concerned with the maintenance of these two things, either through reformatting to preserve textual information, or repairs and environmental controls to ensure continued existence of their physical structure (Foot, 19). However, it is necessary to look beyond the physical and informational aspects of objects in order to ensure we are also preserving the integrity of the spiritual or cultural values which may be fundamental in defining the object. \nConcerns arise when actions taken to preserve the physical object may compromise the spiritual or cultural integrity of a given object. Artifacts, including books, throughout history and the present were created and utilized according to rules and taboos that may not be inherently understood in today's world of mass consumer goods and material culture. While not all books, documents, and artifacts have rituals or socially constructed beliefs associated with their continued existence, it is a relevant issue for many cultural and religious collections.\n\nAn extreme example of how efforts to physically preserve may compromise the socially constructed meaning of the object is the Shinto Shrines of Ise Jingu, in Japan (Maré, 2004). These shrines are of significant cultural and spiritual value to the Japanese people, but every twenty years since the time of Emperor Temmu in the 7th century C.E. the buildings are completely destroyed and rebuilt. The rebuilding process is based on descriptions in the Documents on the rituals of the Great Shrine of Ise which dates from 804 C.E. and ensures that the recreations are exact replicas of the shrines taken down. Physical preservation of these monuments would damage the spiritual and cultural integrity of the process and purpose behind the continuous rebuilding.\n\nSimilarly, in the Buddhist faith, materials are considered to have a life, which must be allowed to progress and end naturally. Impermanence (anitya) in the Buddhist faith relates to the natural end of all things, nirvana, and acts of physical preservation would be contrary to this belief (Karlström). Buddhist shrines are an example of such items that hold spiritual meaning. \n\nPhysical preservation can also work to maintain spiritual and culture integrity of an object, especially in the case of musical instruments. At the violin museum in Cremona, Italy, nine instruments crafted by the Amatis, Guarneris, and by Antonio Stradivari are played ceremoniously six days a week – both to keep them in good physical, playable condition, and to maintain their cultural significance and meaning. From the island of Java in Indonesia, the instruments of the gamelan are treated with respect and reverence, and played on a regular basis to maintain their physical and spiritual life. They are played only for certain occasions and only by certain individuals trained in the art. They must also be handled and stored with care to be sure no one should compromise their spiritual integrity by stepping over them. (Kartomi and Mendonça) Gamelan instruments can be found in museums throughout North America, but to accurately remain gamelan instruments, they must be treated and respected according to both their physical and spiritual properties.\n\nHowever, examples of the significance of meaning and spiritual integrity exist in North America as well. \"Jish\" is a Navajo medicine bundle used in religious rituals related to curing or prevention, and specific care and established provenance is necessary to preserve the existence of \"jish\" versus a simple bundle of herb and grasses. NAGPRA, or North American Graves Protection and Repatriation Act has helped to highlight this issue of meaning and spiritual integrity in the United States. The act not only covers objects and remains recovered from American Indian graves, but the subsequent literary material developed based on the examination and analysis of these objects. (Grose)\n\nPreservation of meaning is perhaps most relevant and recognized in museum collections, but this does not mean there are not key books and documents that are defined by their social construction as well as their physical existence. When a book printed on acid paper turns to dust, there is no doubt that it ceases to exist as book. However, when books are stacked on top of a Koran, or a Guru Granth Sahib is repaired rather than ceremoniously cremated, the spiritual integrity of these texts is compromised or destroyed, and the physical object is void of spiritual or cultural meaning. It may at this point actually cease to be what it had been previously identified as, at least in terms of its cultural or spiritual meaning.\n\nAdmittedly, more work and research needs to be done in regard to preservation of meaning in the area of Library Science. However, it is necessary for libraries and archival institutions to assess their preservation priorities in consideration of social and cultural meaning in tandem with physical attributes and informational context. Preservation and curatorial policies and activities can take into account how materials are housed, handled, repaired, or analyzed not just in terms of preserving physical qualities, but spiritual and cultural meaning as well.\n\n\n"}
{"id": "1340095", "url": "https://en.wikipedia.org/wiki?curid=1340095", "title": "Prosopography", "text": "Prosopography\n\nIn historical studies, prosopography is an investigation of the common characteristics of a historical group, whose individual biographies may be largely untraceable, by means of a collective study of their lives, in multiple career-line analysis. Prosopographical research has the goal of learning about patterns of relationships and activities through the study of collective biography; it collects and analyses statistically relevant quantities of biographical data about a well-defined group of individuals. This makes it a valuable technique for studying many pre-modern societies.\n\nBritish historian Lawrence Stone (1919–1999) brought the term to general attention in an explanatory article in 1971. The word is drawn from the figure of prosopopeia in classical rhetoric, introduced by Quintilian, in which an absent or imagined person is figured forth—the \"face created\" as the Greek suggests—in words, as if present.\n\nStone noted two uses of prosopography as an historian's tool: first, in uncovering deeper interests and connections beneath the superficial rhetoric of politics, in order to examine the structure of the political machine; and second, in analysing the changing roles in society of particular status groups—holders of offices, members of associations—and assessing social mobility through family origins and social connections of recruits to those offices or memberships. \"Invented as a tool of political history,\" Stone observed, \"it is now being increasingly employed by the social historians.\"\n\nA certain mass of data is required for prosopographical research. The collection of data underlies the creation of a prosopography and, in contemporary research, this is usually in the form of an electronic database. But data assembly is not the goal of the research; rather, the objective is to understand patterns and relationships by analysing the data. A uniform set of criteria needs to be applied to the group in order to achieve meaningful results. And, as with any historical study, understanding the context of the lives studied is essential.\n\nIn the words of prosopographer Katharine Keats-Rohan, \"prosopography is about what the analysis of the sum of data about many individuals can tell us about the different types of connection between them, and hence about how they operated within and upon the institutions—social, political, legal, economic, intellectual—of their time\".\n\nIn this sense prosopography is clearly related to, but distinct from, both biography and genealogy. Whilst biography and prosopography overlap, and prosopography is interested in the details of individuals' lives, a prosopography is more than the plural of biography. A prosopography is not just any collection of biographies—the lives must have enough in common for relationships and connections to be uncovered. Genealogy, as practiced by family historians, has as its goal the reconstruction of familial relationships, and as such, well-conducted genealogical research may form the basis of a prosopography, but the goals of prosopographical research are generally wider. \n\nThe nature of prosopographical research has developed over time. In his 1971 essay, Lawrence Stone discussed an \"older\" form of prosopography which was principally concerned with well-known social elites, many of whom were already well-known historical figures. Their genealogies were well-researched, and social webs and kinship linking could be traced, allowing a prosopography of a \"power elite\" to emerge. Prominent examples which Stone drew upon were the work of Charles A. Beard and Sir Lewis Namier. Charles Beard's \"An Economic Interpretation of the Constitution of the United States\" (1913) offered an explanation of the form and content of the U.S. Constitution by looking at the class background and economic interests of the Founding Fathers. Sir Lewis Namier produced an equally influential study of the 18th century British House of Commons, and inspired a circle of historians whom Stone light-heartedly termed \"Namier Inc.\" \n\nStone contrasted this older prosopography with what in 1971 was the newer form of quantitative prosopography, which concern was with much wider populations including, particularly, \"ordinary people\". An example of this kind of work, published slightly later, is Emmanuel Le Roy Ladurie's pioneering work of microhistory, \"Montaillou: The Promised Land of Error\" (1978), which developed a picture of patterns of kinship and heresy, daily and seasonal routine, in a small Occitan village, the last pocket of Cathars, over a 30-year period from 1294 to 1324. Stone anticipated that this new form of prosopography would become dominant as part of a growing wave of Social Science History. But prosopography and other associated forms of social science and quantitative history went into a period of decline during the 1980s. In the 1990s, however, perhaps because of developments in computing, and particularly in database software, prosopography was revived. The \"new prosopography\" has since become clearly established as an important approach in historical research.\n\n\n\n"}
{"id": "1793017", "url": "https://en.wikipedia.org/wiki?curid=1793017", "title": "Reception theory", "text": "Reception theory\n\nReception theory is a version of reader response literary theory that emphasizes each particular reader's reception or interpretation in making meaning from a literary text. Reception theory is generally referred to as audience reception in the analysis of communications models. In literary studies, reception theory originated from the work of Hans-Robert Jauss in the late 1960s, and the most influential work was produced during the 1970s and early 1980s in Germany and the US (Fortier 132), with some notable work done in other Western European countries. A form of reception theory has also been applied to the study of historiography.\n\nThe cultural theorist Stuart Hall was one of the main proponents of reception theory, first developed in his 1973 essay 'Encoding and Decoding in the Television Discourse'. His approach, called the encoding/decoding model of communication, is a form of textual analysis that focuses on the scope of \"negotiation\" and \"opposition\" by the audience. This means that a \"text\"—be it a book, movie, or other creative work—is not simply passively accepted by the audience, but that the reader/viewer interprets the meanings of the text based on her or his individual cultural background and life experiences. In essence, the meaning of a text is not inherent within the text itself, but is created within the relationship between the text and the reader. \n\nHall also developed a theory of encoding and decoding, Hall's theory, which focuses on the communication processes at play in texts that are in televisual form. \n\nReception theory has since been extended to the spectators of performative events, focusing predominantly on the theatre. Susan Bennett is often credited with beginning this discourse. Reception theory has also been applied to the history and analysis of landscapes, through the work of the landscape historian John Dixon Hunt, as Hunt recognized that the survival of gardens and landscapes is largely related to their public reception.\n\nA basic acceptance of the meaning of a specific text tends to occur when a group of readers have a shared cultural background and interpret the text in similar ways. It is likely that the less shared heritage a reader has with the artist, the less he or she will be able to recognise the artist's intended meaning, and it follows that if two readers have vastly different cultural and personal experiences, their reading of a text will vary greatly. Umberto Eco coined the term aberrant decoding to describe the case when the reader's interpretation differs from what the artist intended.\n\nIn literature, the interaction between text and reader occurs within a framework that controls and limits the interaction, through genre, tone, structure, and the social conditions of the reader and author, whereas in landscapes the interaction occurs through movement and viewing, framed by typology instead of genre and tone. Instead of an \"implied reader\", reception theory of landscapes assumes an \"implied visitor\", who is an abstracted concatenation of responses of many visitors at different times. \n\nThe theory recognizes that there is no single reading of a landscape that fulfills its entire potential, and that it is important to examine the motives of visitors and the factors influencing their visits (whether they read guidebooks about the place before visiting, or had strong feelings about the place or the designer, for instance). \n\nOne key difference between reception theory in literature and reception theory in landscape architecture is that while literary works are accessible only to the imagination, physical landscapes are accessible to the senses as well as to the imagination.\n\nReception theoretical analysis of architecture differs from typical writing on the history and analysis of landscapes, which tends to focus on the intentions of the designers, the conditions leading to the creation of the design, and the building process. Reception theory also tends to de-emphasize commonly used terms of description like 'formal' and 'picturesque', unless those terms were known to have meaning to landscape visitors themselves.\n\nAccording to Harold Marcuse, reception history is \"the history of the meanings that have been imputed to historical events. It traces the different ways in which participants, observers, historians and other retrospective interpreters have attempted to make sense of events both as they unfolded and over time since then, to make those events meaningful for the present in which they lived and live.\"\n\n\n\n"}
{"id": "35122793", "url": "https://en.wikipedia.org/wiki?curid=35122793", "title": "Smihula waves", "text": "Smihula waves\n\nSmihula waves (or Smihula cycles, Smihula waves of technological revolutions, economic waves of technological revolutions) are long-term waves of technological progress which are reflected also in long-term economic waves. They are a crucial notion of Daniel Šmihula's theory of technological progress.\n\nThe Smihula's theory of waves of technological revolutions is based on the idea that the main technological innovations are introduced in society and the economy not continually but in specific waves, and the time spans of these waves is shortening due to technological progress.\n\nThe time period with the highest concentration of technological innovations is labeled as a \"technological revolution\"\nA period of technological revolution (an innovation phase) is associated with economic revival. When new but also already-proven and reliable technologies are available, the interest in new technological development temporary declines and investments are diverted from research to their maximal practical utilization. This period we can designate as an application phase. It is also associated with economic growth and perhaps even an economic boom. However, at a certain moment profitability (profit/price ratio) from new innovations and new sectors declines to the level acquired from older traditional sectors. Markets are saturated by technological products – (market saturation – everybody has a mobile phone, every small town has a railway station) and new capital investment in this originally new sector will not bring any above-average profit (e.g. the first railways connected the biggest cities with many potential passengers, later ones had ever smaller and smaller customer potential, and the level of profit from each new railway was therefore lower than from the previous one). At this moment economic stagnation and crisis begin – but a will to risk and to try something new emerges. The stagnation and crisis are therefore overcome by a new technological revolution with new innovations which will revitalize the economy. And this new technological revolution is the beginning of a new wave.\n\nThe internal structure of each long wave of technological innovations with economic implications is as follows:\n\na) innovation phase – technological revolution (an economic revival after the crisis from the end of a previous wave)\n\nb) application phase (an economic boom)\n\nc) saturation of economy and society with innovations, impossibility of further extensive growth (an economic crisis)\n\nIn Smihula theory technological revolutions are the main engine of economic development, and hence long-term economic cycles are dependent on these waves of technological innovation.\nSmihula identified during the modern age in society six waves of technological innovations begun by technological revolutions (one of them is a hypothetical revolution in the near future).\nUnlike other scholars he believed that it is possible to find similar technological revolutions and long-term economic waves dependant on them even in pre-modern ages. (This is the most original part of the Smihula's theory.)\n\nPre-modern technological waves:\n\nModern technological waves:\nTheory of Smihula waves of technological revolutions is popular among supporters of the long economic waves (e.g. Kondratieff cycles) and among scholars who believs that the economic crisis in 2007–2012 was a result of the technological stagnation.\n\nRussian sociologist A.A. Davydov believes that he even identified a specific mathematical formula for the lengths of Smihula waves which is based on the Fibonacci sequence.\n\nAs Smihula published his theory in the time of revived interest in long economic cycles and when a link between economic cycles and technological revolutions was generally accepted (e.g. in works of Carlota Perez), it did not evoke strong criticism or opposition.\nOn the other side it has the same problem as the other long-cycles theories – it is sometime hard to support them by exact data and the potential curve of a long time development is always modified by other short-time factors – therefore its course is always only a rather abstract reconstruction. Also the idea of concentration of the most important innovation in certain bordered periods seems to be very logical, but its verification depends on a very subjective definition of the \"most important\" innovations.\nSmihula's theory of long waves of technological innovations and economic cycles dependent on them is more popular in Russia, Brazil and India than in Europe.\n"}
{"id": "2760372", "url": "https://en.wikipedia.org/wiki?curid=2760372", "title": "The Complete Library of Congress Recordings", "text": "The Complete Library of Congress Recordings\n\nJelly Roll Morton: The Complete Library of Congress Recordings is a 2005 box set of recordings from jazz pioneer Jelly Roll Morton. The set spans 128 tracks over eight CDs. It won two Grammy Awards in 2006, Best Historical Album and Best Album Notes.\n\nIn 1938, noted musicologist and Morton biographer Alan Lomax conducted a series of interviews with Morton at the Library of Congress. Richard Cook and Brian Morton describe these recordings as Jelly Roll Morton's \"virtual history of the birth pangs of jazz as it happened in the New Orleans of the turn of the century. His memory was unimpaired, although he chose to tell things as he preferred to remember them, perhaps; and his hands were still in complete command of the keyboard.\"\n\nExcerpts from the sessions first appeared on a 1948 album. Riverside Records issued the recordings as LP records in 1955. Ron Wynn and Bruce Boyd Raeburn note that \"though the albums came out posthumously, the interviews generated tremendous new interest in Morton's life and music.\" During the 1990s, Rounder Records released a series of compact discs including the musical content, but not the dialogue, from the 1938 sessions. Both the Riverside and earlier Rounder releases were heavily expurgated, and as recently as 2008, when selections from the complete Rounder collection were featured in a BBC Radio 4 documentary on Morton, presenter Marybeth Hamilton noted that, even then, some of the recordings were still considered unsuitable for broadcast, due to the obscene nature of some of the lyrics and Morton's narration.\n\nIn 2005, Rounder released the 1938 recordings in their entirety as part of an eight-disc box set. The first seven discs include Lomax's 1938 interviews, in which Morton describes his life and the early days of jazz, plays piano, and sings. The eighth disc includes 1949 recordings of Morton's contemporaries, reminiscing about Morton and providing musical demonstrations.\n\nThe set was originally released in a piano-shaped box and included a copy of \"Mister Jelly Roll\", Lomax's biography about Morton. The set also includes a PDF file including additional liner notes, complete transcriptions of the recorded dialogue and lyrics, additional unrecorded interviews and archival documents and photos.\n\nIn 2007, Rounder released \"Jelly Roll Morton: The Library of Congress Recordings by Alan Lomax\", a single disc consisting of selected highlights from the box set.\n\narwulf arwulf, writing for allmusic, described the recordings as having been \"beautifully restored.\"\n\nHarvey Pekar, writing for \"The Austin Chronicle\", gave the set a five-star rating (of a possible five), noting that \"[Morton's] oral history here is provocative, and his playing bears out some of the hard-to-believe statements that have been made by (and about) him.\"\n\nRichard Cook and Brian Morton, writing for \"The Penguin Guide to Jazz\", gave the set a four-star rating (of a possible four), describing it as \"surely the most comprehensive coverage of the speech and music to date.… It is a wonderfully illustrated lecture on Morton's music by the man who created it. Indispensable records for anyone interested in jazz history.\"\n\n"}
{"id": "1823869", "url": "https://en.wikipedia.org/wiki?curid=1823869", "title": "The Exodus", "text": "The Exodus\n\nThe Exodus is the founding myth of the Israelites. Spread over the books of Exodus, Leviticus, Numbers, and Deuteronomy, it tells of the enslavement that befell the children of Israel in Egypt, their liberation through the hand of Yahweh and the revelations at Sinai, and their wanderings in the wilderness up to borders of Canaan, the land their God has given them. Its message is that Israel was delivered from slavery by Yahweh and therefore belongs to him through the Mosaic covenant, the terms of which are that Yahweh will protect his chosen people for all time, so long as they will keep his laws and worship only him. The narrative and its laws remain central to Judaism, recounted daily in Jewish prayers and celebrated in festivals such as Passover, as well as serving as an inspiration and model for non-Jewish groups from early Protestants fleeing persecution in Europe to African-Americans striving for freedom and civil rights.\n\nThe traditions behind the Exodus story can be traced in the writings of the 8th-century BCE prophets, beyond which their history is obscured by centuries of transmission. No historical basis for the biblical Exodus story exists; instead, archaeology suggests a native Canaanite origin for ancient Israel.\n\nThe story of the Exodus is told in the books of Exodus, Leviticus, Numbers, and Deuteronomy, the last four of the five books of the Torah (also called the Pentateuch). It begins with the Israelites in slavery. Their prophet Moses leads them out of Egypt and through the wilderness to Mount Sinai, where Yahweh reveals himself to his people and establishes the Mosaic covenant: they are to keep his \"torah\" (i.e. law, instruction), and in return he will give them the land of Canaan. The Israelites accept the covenant and receive their laws, and, with Yahweh now present in their midst, journey on from Sinai, towards the promised land, but when told that the land is filled with giants they refuse to go on, and Yahweh condemns them to remain in the desert until the generation that left Egypt passes away. After thirty-eight years at the oasis of Kadesh Barnea the next generation travel on to the borders of Canaan, where Moses addresses them for the final time, reviewing their travels and giving them further laws. The Exodus ends with the death of Moses on Mount Nebo and his burial by Yahweh, while the Israelites prepare for the conquest of the land.\n\nThe climax of the Exodus is the covenant (binding legal agreement) between God and Israel mediated by Moses at Sinai: Yahweh will protect Israel as his chosen people for all time, and Israel will keep Yahweh's laws and worship only him. The covenant is described in stages: at Exodus 24:3–8 the Israelites agree to abide by the \"book of the covenant\" that Moses has just read to them; shortly afterwards God writes the \"words of the covenant\" – the Ten Commandments – on stone tablets; and finally, as the people gather in Moab to cross into Canaan, the land God has promised them, Moses makes a new covenant between Yahweh and Israel \"beside the covenant he made with them at Horeb\" (Deuteronomy 29:1). The laws are set out in a number of codes:\n\nScholars are broadly agreed that the publication of the Torah took place in the mid-Persian period (the 5th century BCE), echoing a traditional Jewish view which gives Ezra, the leader of the Jewish community on its return from Babylon, a pivotal role in its promulgation. The tradition behind it stretches back some two hundred years before then, to a point in the late 7th century BCE when various oral and written elements were drawn together into works which were the fore-runners of the Torah we know today. The first trace appears in the northern prophets Amos (possibly) and Hosea (certainly), both active in the 8th century BCE in northern Israel, but their southern contemporaries Isaiah and Micah show no knowledge of an exodus. The story may, therefore, have originated a few centuries earlier, perhaps the 9th or 10th BCE, and there are signs that it took different forms in Israel, in the Transjordan region, and in the southern Kingdom of Judah before being unified in the Persian era.\n\nMany theories have been advanced to explain the composition of the Torah, but two have been especially influential. The first of these, Persian Imperial authorisation, advanced by Peter Frei in 1985, holds that the Persian authorities required the Jews of Jerusalem to present a single body of law as the price of local autonomy. Frei's theory was demolished at an interdisciplinary symposium held in 2000, but the relationship between the Persian authorities and Jerusalem remains a crucial question. The second theory, associated with Joel P. Weinberg and called the \"Citizen-Temple Community\", proposes that the Exodus story was composed to serve the needs of a post-exilic Jewish community organised around the Temple, which acted in effect as a bank for those who belonged to it. The Torah (the Exodus story) served as an \"identity card\" defining who belonged to this community (i.e., to Israel), thus reinforcing Israel's unity through its new institutions.\n\nThe Exodus is at the centre of Jewish identity. It is remembered daily in Jewish prayers and celebrated each year at the feasts of Pesach (Passover) and Shavuot, the two being known respectively as \"the time of our freedom\" and \"the time our Torah was given\". The two are closely linked, with Pesach announcing that the freedom it introduces is only fully realised with the giving of the law (the Torah). A third Jewish festival, Sukkot, the Festival of Booths, commemorates how the Israelites lived in booths following the exodus from their previous homes in Egypt. The Exodus roots Jewish religion in history, in contrast to pagan religions which are oriented towards nature. The festivals now associated with the exodus (Passover, Shavuot, and Sukkot) began as agricultural and seasonal feasts but became completely subsumed into the central Exodus myth of Israel's deliverance from oppression at the hands of God. The idea that the relationship between God and Israel is defined by the covenant (\"brit\") made at Sinai is central to Jewish identity, together with the laws given to Israel and the thirteen attributes of God revealed there. The fringes worn at the corners of traditional Jewish prayer shawls are a physical reminder of the obligation to observe the laws given at the climax of Exodus: \"Look at it and recall all the commandments of the Lord\" (Numbers).\n\nThe Exodus has also resonated through non-Jewish culture. Some influences have been trivial but curiously significant – medieval Irish and Scottish legendary history, for example, derived the name of Scotland from Scota, supposedly a daughter of the pharaoh of the Exodus who later emigrated to the British isles. Others have been more significant: the hostility of the exodus tradition to the State (specifically to Egypt and the pharaoh) played a role in the Puritan Revolution in 17th-century England,\nmany early American settlers interpreted their flight from religious persecution in Europe to a new life overseas as a type of exodus, Thomas Jefferson and Benjamin Franklin recommended that the Great Seal of the United States show Moses leading the Israelites across the Red Sea, and African Americans suffering under slavery and racial oppression interpreted their situation in terms of the Exodus, making it a catalyst for social change. Mormon pioneers to Utah compared their journey to the biblical Exodus and adopted many place names.\n\nWhile the Exodus story is no older than the Babylonian exile, there are indications that some historical memories underlie it: the name of Moses is Egyptian, for example, and many scholars have found it improbable that a humiliating tradition of slavery would simply be invented. Egyptologist Jan Assmann suggests that it has no single origin, but rather combines numerous historical experiences into \"a coherent story that is fictional as to its composition but historical as to some of its components\" (Assmann, 2014). Thus the memory of Egyptian oppression may be based on the harsh treatment of Canaanites inside Canaan in the 2nd millennium, when the region was ruled by Egypt: these memories could later have been transferred to Egypt itself, and a new exodus story created. A historical Moses associated with a small group may have been later generalised into the savior of Israel, while the history of the Hyksos, who were Canaanite rulers of the Egyptian Delta in the 16th century BCE, may have formed the basis of the descent into Egypt and the exodus.\n\nThe meaning a reader takes away from a text depends on his or her understanding of the literary \"type\" to which it belongs: as one scholar puts it in his discussion of the Genesis creation myth, \"it makes an enormous difference whether the first chapters of Genesis are read as scientific cosmology, creation myth, or historical saga\" (Wood, 1990). There is an almost universal consensus among scholars that the Exodus story is best understood as myth. More specifically, its can be understood as a \"charter\" (or foundation) myth, a myth told to explain a society's origins and to provide the ideological foundation for its culture and institutions.\n\nAfter a century of research by archaeologists and Egyptologists the consensus of modern scholars is that the Bible does not give an accurate account of the origins of Israel. There is no indication that the Israelites ever lived in Ancient Egypt, and the Sinai Peninsula shows almost no sign of any occupation for the entire 2nd millennium BCE (even Kadesh-Barnea, where the Israelites are said to have spent 38 years, was uninhabited prior to the establishment of the Israelite monarchy). In contrast to the absence of evidence for the Egyptian captivity and wilderness wanderings, there are ample signs of Israel's evolution within Canaan from native Canaanite roots. While a few scholars discuss the historicity, or at least plausibility, of the Exodus story, the majority of archaeologists have abandoned it, in the phrase used by archaeologist William Dever, as \"a fruitless pursuit\".\n\nDetails point to a 1st millennium date for the composition of the narrative: Ezion-Geber (one of the Stations of the Exodus), for example, dates to a period between the 8th and 6th centuries BCE with possible further occupation into the 4th century BCE, and those place-names on the Exodus route which have been identified – Goshen, Pithom, Succoth, Ramesses and Kadesh Barnea – point to the geography of the 1st millennium rather than the 2nd. Similarly, Pharaoh's fear that the Israelites might ally themselves with foreign invaders seems unlikely in the context of the late 2nd millennium, when Canaan was part of an Egyptian empire and Egypt faced no enemies in that direction, but does make sense in a 1st millennium context, when Egypt was considerably weaker and faced invasion first from the Achaemenid Empire and later from the Seleucid Empire. The mention of the dromedary in Exodus 9:3 also suggests a later date – the widespread domestication of the camel as a herd animal is thought not to have taken place before the late 2nd millennium, after the Israelites had already emerged in Canaan, and they did not become widespread in Egypt until c. 200–100 BCE.\n\nThe chronology of the Exodus narrative is symbolic: for example, its culminating event, the erection of the Tabernacle as Yahweh's dwelling-place among his people, occurs in the year 2666 Anno Mundi (Year of the World, meaning 2666 years after God creates the world), and two-thirds of the way through a four thousand year era which culminates in or around the re-dedication of the Second Temple in 164 BCE.\n"}
{"id": "9730490", "url": "https://en.wikipedia.org/wiki?curid=9730490", "title": "The Lost Tomb of Jesus", "text": "The Lost Tomb of Jesus\n\nThe Lost Tomb of Jesus is a documentary co-produced and first broadcast on the Discovery Channel and Vision TV in Canada on March 4, 2007, covering the discovery of the Talpiot Tomb. It was directed by Canadian documentary and film maker Simcha Jacobovici and produced by Felix Golubev and Ric Esther Bienstock, while James Cameron served as executive producer. (Jacobovici and Cameron had previously created \"The Exodus Decoded\".) The film was released in conjunction with a book about the same subject, \"The Jesus Family Tomb\", issued in late February 2007 and co-authored by Jacobovici and Charles R. Pellegrino. The documentary and book's claims are the subject of controversy within the archaeological and theological fields, as well as among linguistic and biblical scholars.\n\nThe film describes the finding of the Talpiot Tomb during a housing construction project, and posits that it was the family tomb of Jesus. The film states that ten ossuaries were found in the cave, of which six are the subject of the film. Further, it claims that one of the ten ossuaries went missing years ago, presumably stolen.\n\nThe excavation report for the predecessor of the Israel Antiquities Authority was written by Amos Kloner, now professor of archaeology at Israel's Bar-Ilan University. Kloner dissociated himself from the claims made in the documentary. He said it was incorrect to call it \"never before reported information\" and that he had published all the details in the journal \"Antiqot\" in 1996. He had not said it was the tomb of Jesus' family. \"I think it is very unserious [sic] work. I do scholarly work…,\" Kloner said. \"[This film] is all nonsense.\"\nSix of the nine remaining ossuaries bear inscriptions. \"The Lost Tomb of Jesus\" posits that three of those carry the names of figures from the New Testament. The meanings of the epigraphs are disputed. The makers of the documentary claim that four leading epigraphers have corroborated their interpretation of the inscriptions. As translated in \"The Lost Tomb of Jesus\" and \"The Jesus Family Tomb\", they read as follows:\n\n\nThe film further claims that the tenth ossuary, which went missing years ago, is the James Ossuary purported to contain the body of James, the brother of Jesus.\n\nIn \"The Jesus Family Tomb\", Simcha Jacobovici claims the James Ossuary would have been a part of this tomb, but was removed by artifact dealers, and thus discovered separately (citation needed). The James Ossuary's authenticity has been called into question, and Oded Golan, one of its past owners, was charged with fraud in connection to the artifact, but exonerated on all counts of forgery.\n\nBen Witherington III, who worked with Jacobovici on a Discovery Channel documentary on the James Ossuary, denies this connection on two grounds:\n\nAnother consideration was that the measurements of the James Ossuary did not match the measurements listed for the tenth ossuary, which is no longer stored with the rest of the collection. The James Ossuary was listed as being approximately 50 centimeters (19.6 inches) long by 30 centimeters (11.8 inches) wide on one end, and 25.5 centimeters (10 inches) on the other end (citation needed). The tenth ossuary in the Talpiot collection is listed as 60 centimeters (23.6 inches) long by 26 centimeters (10.2 inches) by 30 centimeters (11.8 inches) (citation needed). Furthermore, Amos Kloner has stated that the tenth ossuary had no inscription. Also, Joe Zias, former curator of the Rockefeller Museum who received and catalogued the ossuaries, refuted this claim on his personal site (citation needed).\n\nNew information has now shown that the discrepancy in the measurements had to do with measuring the base of the ossuary, which is indeed 50 centimeters (19.6 inches), rather than the length. The top length of the James Ossuary, not the base, which is trapezoidal in shape, according to the latest re-measurement carried out by the Israel Antiquities Authority, is 57.5 centimeters (22.6 inches.) (citation needed). While compelling, this does not prove that the James Ossuary is the missing tenth Talpiot ossuary.\n\nAnalysis of mitochondrial DNA performed by Lakehead University on the remains found in the ossuary marked \"Jesus son of Joseph\" and the one marked \"Mariamne\" or \"Mary\" (who some claim to be Mary Magdalene) found that the two occupants were not blood relations on their mothers' side. Based on these tests, the makers of the documentary suggest that \"Jesus\" and \"Mariamne\" were probably married \"because otherwise they would not have been buried together in a family tomb,\" but the remains were not dated using radiocarbon to further sustain this supposition, neither was any announced DNA testing done on the others ossuaries to see if any familial relation existed there. Additionally, scholars argue the DNA tests only prove that they did not have the same mother and they could easily have been father/daughter, cousins, half brother/sister, or any number of possibilities that do not include a matrilineage line.\n\nThe film proposes new interpretations of the events regarding Jesus depicted in the New Testament, as seen by mainstream Christianity. The film's suggestions contradict\nthe basis of the faith and may be considered blasphemous by Christians:\n\nThe claim that Jesus was married also undermines the theological metaphor of the Church being the \"Bride of Christ\" (found in the writings of the New Testament). Jimmy Akin, director of Apologetics and Evangelization at Catholic Answers, wrote: \"This image would never have arisen if there was a Mrs. Jesus living right there in Jerusalem…. We know about [the wives of religion founders] because they were honored figures as wives of The Founder, and if Jesus had a wife then (a) we would know about it and (b) the whole Church-as-the-Bride-of-Christ metaphor would never have come into existence.\" As for a possible \"son of Jesus,\" he noted: \"We tend to know about even the daughters of religious founders. Muhammad's daughter Fatima comes to mind. It would be much harder to sneak a forgotten son by the eyes of history…. It's not just hard to sneak sons past because patriarchal cultures focus more on sons; it's also because of this: In traditional societies, the son is looked on as the father's natural successor.\"\n\nThe filmmakers denied that the claims made in the film contradicted key teachings of Christianity, such as the resurrection and ascension. The film's religious consultant James Tabor stated that the fact that Jesus' tomb was discovered does not put in doubt biblical accounts of his resurrection, which he said could have been spiritual. With regard to the ascension, the documentary's website suggests that while the tomb's discovery does not render impossible the notion of a spiritual ascension, it does contradict the belief that Jesus physically ascended to heaven.\n\nFinding someone's remains in Jesus' tomb conforms to the Muslim belief that a substitute for him was crucified, while he was raised bodily to heaven. The Islamic view of his disappearance, as mentioned in the Qur'an, states: \"That they said (in boast), \"We killed Al-Masih 'Isa the son of Maryam, the Messenger of Allah\"; but they killed him not, nor crucified him, but so it was made to appear to them\". The general Muslim interpretation of the verse is that God, to revenge from Judas' betrayal to Jesus (the fatherless prophet), made his face similar to that of Jesus, while Jesus ascended into heaven and is to return near the end of time and kill the anti-Christ. Accordingly, the discovered remains in his tomb would then actually belong to Judas, a Roman guard, or a volunteering disciple.\n\nFollowing the March 4, 2007, airing of \"The Lost Tomb of Jesus\" on the Discovery Channel, American journalist Ted Koppel aired a program entitled \"The Lost Tomb of Jesus—A Critical Look\", whose guests included the director Simcha Jacobovici, James Tabor, Chair of the Department of Religious Studies at the University of North Carolina at Charlotte who served as a consultant and advisor on the documentary, Jonathan Reed, Professor of Religion at the University of LaVerne and co-author of \"Excavating Jesus Beneath the Stones, Behind the Text,\" and William Dever, an archaeologist with over 50 years experience in Middle Eastern archaeological digs.\n\nAlan Cooperman, writer of \"The Washington Post\" article also states this: \"Similar assessments came yesterday from two Israeli scholars, Amos Kloner, who originally excavated the tomb, and Joe Zias, former curator of archaeology at the Israeli Antiquities Authority. Kloner told the Jerusalem Post that the documentary is \"nonsense.\" Zias described it in an e-mail to \"The Washington Post\" as a \"hyped up film which is intellectually and scientifically dishonest.\"\n\nIsraeli archaeologist Amos Kloner, who was among the first to examine the tomb when it was first discovered, said the names marked on the coffins were very common at the time.\n\"I don't accept the news that it was used by Jesus or his family,\" and \"The documentary filmmakers are using it to sell their film.\" he told the BBC News website.\n\nDuring the documentary \"The Lost Tomb of Jesus\", various professionals had claimed:\n\n\nDuring Ted Koppel's critique, \"The Lost Tomb of Jesus—a Critical Look\", Koppel revealed he had denials from these three people that Simcha Jacobovici had misquoted in the documentary.\n\n\nThe archaeologist William Dever summed it up when he stated on Koppel's critical analysis, \"The Lost Tomb of Jesus—A Critical Look\", that Jacobovici's and Cameron's \"conclusions were already drawn in the beginning\" of the inquiry and that their \"argument goes far beyond any reasonable interpretation.\"\n\nThree skulls were found on the floor of the tomb in 1980 which the film makers assert was usual but others disagree: \"This too was decidedly not typical. In ancient Jerusalem, the dead were placed inside tombs; in tombs, the dead were placed inside ossuaries. If anything was left behind, it was a lamp or a bottle of perfume, not skulls.\"\n\nEarly Christianity scholar R. Joseph Hoffmann, chair of the Committee for the Scientific Examination of Religion, says the film alerts the public to the fact that there are no secure conclusions when it comes to the foundational history of a religious tradition. But he charges that the film \"is all about bad assumptions,\" beginning with the assumption that the boxes contain Jesus of Nazareth and his family. From his view as a historian specializing in the social history of earliest Christianity, he found it \"amazing how evidence falls into place when you begin with the conclusion—and a hammer.\"\n\nWhen interviewed about the upcoming documentary, Amos Kloner, who oversaw the original archaeological dig of this tomb in 1980 said:\n\n\"Newsweek\" reports that the archaeologist who personally numbered the ossuaries dismissed any potential connection:\n\nThe aforementioned Joe Zias has published in his own site a \"viewers' guide\" to the Talpiot Tomb documentary, in which he systematically rebuts the film's argumentation and gives much background information about the people involved in it.\n\nPfann also thinks the inscription read as \"Jesus\" has been misread and suggests that the name \"Hanun\" might be a more accurate rendering.\n\n\"The Washington Post\" reports that William G. Dever (mentioned above as excavating ancient sites in Israel for 50 years) offered the following:\n\nAsbury Theological Seminary's Ben Witherington III points out some other circumstantial problems with linking this tomb to Jesus' family:\n\nThe Archaeological Institute of America, self-described on their website as \"North America's oldest and largest organization devoted to the world of archeology,\" has published online their own criticism of the \"Jesus tomb\" claim: \n\"The identification of the Talpiyot tomb as the tomb of Jesus and his family is based on a string of problematic and unsubstantiated claims [...] [It] contradicts the canonical Gospel accounts of the death and burial of Jesus and the earliest Christian traditions about Jesus. This claim is also inconsistent with all of the available information—historical and archaeological—about how Jews in the time of Jesus buried their dead, and specifically the evidence we have about poor, non-Judean families like that of Jesus. It is a sensationalistic claim without any scientific basis or support.\"\n\nDarrell Bock, a New Testament scholar and research professor of New Testament studies at Dallas Theological Seminary points out some of the inconsistencies, including: \"If Jesus' family came from Galilee, why would they have a family tomb in Jerusalem?\"\n\nBen Witherington points out an inconsistency related to the James Ossuary. He points out that the James Ossuary came from Silwan, not Talpiot. In addition, the James Ossuary had dirt on it that \"matched up with the soil in that particular spot in Jerusalem.\" In his opinion, this is problematic, because \"the ossuaries that came out of Talpiot came out of a rock cave from a different place, and without such soil in it.\" Therefore, he believes that it is difficult to believe that the one known family member of Jesus was buried separately and far away from Jesus' family.\n\nIn addition, during the trial of antiquities dealer Oded Golan there has been testimony from former FBI agent Gerald Richard that a photo of the James ossuary, showing it in Golan's home, was taken in the 1970s, based on tests done by the FBI photo lab. This would make it impossible for the James Ossuary to have been discovered with the rest of the Talpiot ossuaries in the 1980s.\n\nWith reference to the DNA tests, Witherington wrote in his blog: \"[T]he most the DNA evidence can show is that several of these folks are interrelated…. We would need an independent control sample from some member of Jesus' family to confirm that these were members of Jesus' family. We do not have that at all.\" This quote clarifies the fact that the documentarians do not believe they have tested the DNA and have proven it to be Jesus. They simply used DNA testing to prove that the \"Jesus son of Joseph\" and the \"Mariamne\" in this tomb were not maternally related (i.e. that they did not have the same mother or grandmother). The film asserted that this DNA evidence suggests they were probably spouses. Critics contend they could have been paternally related (e.g. father and daughter, or grandfather and granddaughter), or related by someone else's marriage. Mariamne could just as well have been the wife of one of the other two males in the ossuary.\n\n\"The New York Times\" article of February 27, 2007, (reprinted in full on many websites) states:\nThe documentary's director and its driving force, Simcha Jacobovici…, said there was enough mitochondrial DNA for a laboratory in Ontario to conclude that the bodies in the \"Jesus\" and \"Mary Magdalene\" ossuaries were not related on their mothers' side. From this, Mr. Jacobovici deduced that they were a couple, because otherwise they would not have been buried together in a family tomb. In an interview, Mr. Jacobovici was asked why the filmmakers did not conduct DNA testing on the other ossuaries to determine whether the one inscribed Judah, son of Jesus was genetically related to either the Jesus or Mary Magdalene boxes; or whether the Jesus remains were actually the offspring of Mary. \"We're not scientists. At the end of the day we can't wait till every ossuary is tested for DNA,\" he said. \"We took the story that far. At some point you have to say, I've done my job as a journalist.\"\nIn the televised debate following the airing of the film, Ted Koppel pressed Jacobovici on the same question and received the same response. According to the authors of one blog, \"the response is manifestly disingenuous. The question, in fact, necessarily arises whether the team or one of its members decided not to proceed with any further DNA tests. Such tests may have revealed that none of the ossuaries are related—hence defeating the underlying presupposition that the crypt was in fact a family tomb, and thereby eliminating any valid basis at all for producing and showing the film.\"\n\nWilliam G. Dever said that some of the inscriptions on the ossuaries are unclear, but that all of the names are common. \"I've known about these ossuaries for many years and so have many other archaeologists, and none of us thought it was much of a story because these are rather common Jewish names from that period. It's a publicity stunt, and it will make these guys very rich, and it will upset millions of innocent people because they don't know enough to separate fact from fiction.\"\n\nJodi Magness, an archaeologist at the University of North Carolina-Chapel Hill, notes that at the time of Jesus, wealthy families buried their dead in tombs cut by hand from solid rock, putting the bones in niches in the walls and then, later, transferring them to ossuaries. \"If Jesus' family had been wealthy enough to afford a rock-cut tomb, it would have been in Nazareth, not Jerusalem,\" Magness writes.\n\nAccording to Magness, the names on the Talpiot ossuaries indicate that the tomb belonged to a family from Judea, the area around Jerusalem, where people were known by their first name and father's name. As Galileans, Jesus and his family members would have used their first name and hometown. \"This whole case (for the tomb of Jesus) is flawed from beginning to end.\"\n\nThere is no information on analyzing relation of \"Mary\" and \"Jesus son of Joseph\" or any other tomb occupants. In Jewish tradition of the time, after one year, when bodies in rock-cut tombs were decomposed, bones were collected, cleaned and then finally placed in an ossuary. Due to this conduct there is no real assurance that what scientists have really examined are remnants of \"Mariamne e Mara\" and \"Jesus son of Joseph.\"\n\nDavid Mavorah, a curator of the Israel Museum in Jerusalem, points out that the names on the ossuaries were extremely common. \"We know that Joseph, Jesus and Mariamne were all among the most common names of the period. To start with all these names being together in a single tomb and leap from there to say this is the tomb of Jesus is a little far-fetched, to put it politely.\" David Mavorah is an expert of Israeli Antiquity, and (presumably) not an expert of statistics. However, Andrey Feuerverger, the statistician cited by the makers of the documentary, has said that determination of the identity of those in the tomb was the purview of biblical historians, and not statisticians. For another interpretation of the statistics see the statistics section above.\n\nProfessor Amos Kloner, former Jerusalem district archaeologist of the Israel Antiquities Authority and the first archaeologist to examine the tomb in 1980, told the \"Yedioth Ahronoth\" newspaper that the name Jesus had been found 71 times in burial caves at around that time. Furthermore, he said that the inscription on the ossuary is not clear enough to ascertain, and although the idea fails to hold up by archaeological standards it makes for profitable television. Quote: \"The new evidence is not serious, and I do not accept that it is connected to the family of Jesus…. They just want to get money for it.\"\n\nRichard Bauckham, professor at the University of St Andrews, catalogued ossuary names from that region since 1980. He records that based on the catalogue, \"Jesus\" was the 6th most popular name of Jewish men, and \"Mary/Mariamne\" was the single most popular name of Jewish women at that time. Therefore, finding two ossuaries containing the names \"Jesus\" and \"Mary/Mariamne\" is not significant at all, and the chances of it being the ossuaries of Jesus and Mary Magdalene are \"very small indeed.\"\n\nConcerning the inscription attributed to Jesus son of Joseph, Steve Caruso, a professional Aramaic translator using a computer to visualize different interpretations, claims that although it is \"possible\" to read it as \"Yeshua\" that \"overall it is a very strong possibility that this inscription is not <nowiki>'</nowiki>\"Yeshua` bar Yehosef\".'\"\n\nThe name \"Mary\" and its derivatives may have been used by up to 25% of Jewish women at that time.\n\nLawrence E. Stager, the Dorot professor of archaeology of Israel at Harvard, said the documentary was \"exploiting the whole trend that caught on with \"The Da Vinci Code.\" One of the problems is there are so many biblically illiterate people around the world that they don't know what is real judicious assessment and what is what some of us in the field call 'fantastic archaeology.'\"\n\nWilliam G. Dever said, \"I'm not a Christian. I'm not a believer. I don't have a dog in this fight. I just think it's a shame the way this story is being hyped and manipulated.\"\n\nJodi Magness criticized the decision of the documentary makers to make their claims at a news conference rather than in a peer-reviewed scientific article. By going directly to the media, she said, the filmmakers \"have set it up as if it's a legitimate academic debate, when the vast majority of scholars who specialize in archeology of this period have flatly rejected this.\"\n\nJoe Zias, former curator of archeology at the Israeli Antiquities Authority, described it in an e-mail to \"The Washington Post\" as a \"hyped-up film which is intellectually and scientifically dishonest.\" He also wrote an extended Viewers Guide to Understanding the Talpiot Tomb documentary, published on his web site.\nFrançois Bovon has also written to say that his comments were misused. In a letter to the Society of Biblical Literature, he wrote:\n\nFollowing a symposium at Princeton in January 2008 media interest in the Talpiot tomb was reignited. \"Time\" and CNN devoted extensive coverage, implying that the case had been re-opened.\n\nScholars who had been present at the symposium then accused Jacobovici and Cameron of misleading the media in claiming the symposium reopened their theory as viable. Several scholars, including all the archaeologists and epigraphers, who delivered papers at the symposium issued an open letter of complaint claiming misrepresentation, saying that Jacobovici and Cameron's claims of support from the symposium are \"nothing further from the truth\".\n\nOn March 15, 2007, Discovery Channel released a DVD of the documentary with a listed running time of \"2 hours.\"\n\n\n\n\n"}
{"id": "26442520", "url": "https://en.wikipedia.org/wiki?curid=26442520", "title": "Thirlwall Prize", "text": "Thirlwall Prize\n\nSince 1884, the Thirlwall Prize was instituted at Cambridge University, England, in the memory of Bishop Connop Thirlwall, and has been awarded during odd-numbered years, for the best essay about British history or literature for a subject with original research. It was instituted on the condition that a foundation a medal is awarded in alternate years for the best dissertation involving original historical research, together with a sum of money to defray the expenses of publication. From 1885, the Prince Consort Prize was awarded in alternate years.\n\nWinners of the Thirlwall Prize include:\n\n\n\n"}
{"id": "15518062", "url": "https://en.wikipedia.org/wiki?curid=15518062", "title": "Twentieth Century Society", "text": "Twentieth Century Society\n\nThe Twentieth Century Society (C20) is a British charity which campaigns for the preservation of architectural heritage from 1914 onwards. The society's interests embrace buildings and artefacts that characterise 20th-century Britain. It is formally recognised as one of the National Amenity Societies, and as such is a statutory consultee on alterations to listed buildings within its period of interest, and must be notified of any proposed work to a listed building which involves any element of demolition.\n\nThe society was formed as The Thirties Society in 1979, the year in which the prominent \"Thirties – British art and design before the War\" exhibition was shown at the Hayward Gallery. Its establishment was inspired by and loosely modelled on the Victorian Society, which aims to protect pre-1914 Victorian and Edwardian buildings. Bevis Hillier was the first president, and Clive Aslet the first honorary secretary. In 1992, the society changed its name to The Twentieth Century Society, as it was felt that \"Thirties Society\" failed to indicate its interest in the protection of buildings from other periods as well.\n\n\n"}
{"id": "1632710", "url": "https://en.wikipedia.org/wiki?curid=1632710", "title": "Universal history", "text": "Universal history\n\nA universal history is a work aiming at the presentation of the history of humankind as a whole, coherent unit.\n\nA universal chronicle or world chronicle traces history from the beginning of written information about the past up to the present. \nUniversal history embraces the events of all times and nations in so far as scientific treatment of them is possible.\n\nUniversal history in the Western tradition is commonly divided into three parts, viz. ancient, medieval, and modern time. The division on ancient and medieval periods is less sharp or absent in the Arabic and Asian historiographies. A synoptic view of universal history led some scholars, beginning with Karl Jaspers, to distinguish the Axial Age synchronous to \"classical antiquity\" of the Western tradition. Jaspers also proposed a more universal periodization—prehistory, history and planetary history. All distinguished earlier periods belong to the second period (history) which is a relatively brief transitory phase between two much longer periods.\n\nThe roots of historiography in the 19th century are bound up with the concept that history written with a strong connection to the primary sources could be integrated with \"the big picture\", i.e. to a general, universal history. For example, Leopold von Ranke, probably the pre-eminent historian of the 19th century, founder of Rankean historical positivism, the classic mode of historiography that now stands against postmodernism, attempted to write a Universal History at the close of his career. The works of world historians Oswald Spengler and Arnold J. Toynbee are examples of attempts to integrate primary source-based history and Universal History. Spengler's work is more general; Toynbee created a theory that would allow the study of \"civilizations\" to proceed with integration of source-based history writing and Universal History writing. Both writers attempted to incorporate teleological theories into general presentations of the history. Toynbee found as the \"telos\" (\"goal\") of universal history the emergence of a single World State.\n\nA project of Universal history may be seen in the Hebrew Bible, \nwhich from the point of view of its redactors in the 5th century BC presents a history of humankind from creation to the Flood, \nand from there a history of the Israelites down to the present. The Seder Olam is a 2nd-century CE rabbinic interpretation of this chronology.\n\nIn Greco-Roman antiquity, the first universal history was written by Ephorus (fl. 4th century BC). This work has been lost, but its influence can be seen in the ambitions of Polybius (203–120 BC) and Diodorus (fl. 1st century BC) to give comprehensive accounts of their worlds. Herodotus' \"History\" is the earliest surviving member of the Greco-Roman world-historical tradition, although under some definitions of universal history it does not qualify as universal because it reflects no attempt to describe an overall direction of history or a principle or set of principles governing or underlying it. Polybius was the first to attempt a universal history in this stricter sense of the term:\n\n\"Metamorphoses\" by Ovid has been considered as a universal history because of its comprehensive chronology—from the creation of mankind to the death of Julius Caesar a year before the poet's birth. In Leipzig are preserved five fragments dating to the 2nd century AD and coming from a world chronicle. Its author is unknown, but was perhaps a Christian. Later, universal history provided an influential lens on the rise of Christianity in the Roman Empire in such works as Eusebius's \"Ecclesiastical History\", Augustine's \"City of God\", and Orosius' \"History Against the Pagans\".\n\nDuring the Han Dynasty (202 BCE – 220 CE) of China, Sima Qian (145–86 BC) was the first Chinese historian to attempt a universal history—from the earliest mythological origins of his civilization to his present day—in his \"Records of the Grand Historian\". Although his generation was the first in China to discover the existence of kingdoms in Central Asia and India, his work did not attempt to cover the history of these regions.\n\nThe \"universal chronicle\" traces history from the beginning of the world up to the present and was an especially popular genre of historiography in medieval Western Europe. The universal chronicle differs from the ordinary chronicle in its much broader chronological and geographical scope, giving, in principle, a continuous account of the progress of world history from the creation of the world up to the author's own times, but in practice often narrowing down to a more limited geographical range as it approaches those times.\n\nThe \"Chronica\" of Eusebius of Caesarea (c. 275–339) is considered to be the starting point of this tradition. The second book of this work consisted of a set of concordance tables (\"Chronici canones\") that for the first time synchronized the several concurrent chronologies in use with different peoples. Eusebius' chronicle became known to the Latin West through the translation by Jerome (c. 347–420).\n\nUniversal chronicles are sometimes organized around a central ideological theme, such as the Augustinian idea of the tension between the heavenly and the earthly state, as depicted in the City of God, which plays a major role in Otto von Freising's \"Historia de duabus civitatibus\". Agustine’s thesis depicts the history of the world as universal warfare between God and the Devil. This metaphysical war is not limited by time but only by geography as it takes place on planet Earth. In this war God moves (by divine intervention/ Providence) those governments, political /ideological movements and military forces aligned (or aligned the most) with the Catholic Church (the City of God) in order to oppose by all means—including military—those governments, political/ideological movements and military forces aligned (or aligned the most) with the Devil (the City of Devil).\n\nIn other cases, any obvious theme may be lacking. Some universal chronicles bear a more or less encyclopedic character, with many digressions on non-historical subjects, as is the case with the \"Chronicon\" of Helinand of Froidmont.\n\nOther notable universal chroniclers of the Medieval West include Bede (c. 672 or 673–735), the Christherre-Chronik, Helinand of Froidmont (c. 1160—after 1229), Isidore of Seville (c. 560–636), Jans der Enikel, Matthew Paris (c. 1200-1259), Ranulf Higdon (c. 1280-1363), Rudolf von Ems, Sigebert of Gembloux (c. 1030–1112), Otto von Freising (c. 1114–1158), and Vincent of Beauvais (c. 1190-1264?).\n\nThe tradition of universal history can even be seen in the works of medieval historians whose purpose may not have been to chronicle the ancient past, but nonetheless included it in a local history of more recent times. One such example is the \"Decem Libri Historiarum\" of Gregory of Tours (d. 594), where only the first of his ten books describes creation and ancient history, while the last six books focus on events in his own lifetime and region. While this reading of Gregory is currently a widely accepted hypothesis in historical circles, the central purpose of Gregory's writing is still a topic of hot debate.\n\nIn the medieval Islamic world (13th century), universal history in this vein was taken up by Muslim historians such as Tarikh-i Jahangushay-i Juvaini (\"The History of The World Conqueror\") by Ala'iddin Ata-Malik Juvayni, Jami al-Tawarikh (\"Compendium of Chronicles\") by Rashid-al-Din Hamadani (now held at the University of Edinburgh) and the \"Muqaddimah\" by Ibn Khaldun.\n\nAn early European project was the \"Universal History\" of George Sale and others, written in the mid-18th century.\n\nChristian writers as late as Bossuet in his \"Discours sur l'histoire universelle\" (Speech of Universal History) are still reflecting on and continuing the Medieval tradition of universal history.\nSpeech of Universal History is considered by many Catholics as an actual second edition or continuation of the City of God. In this work Bossuet continues to provide an update of universal history according to Augustine’s thesis of universal war between those humans that follow God and those who follow the Devil. This concept of world history guided by Divine Providence in a universal war between God and Devil is part of the official doctrine of the Catholic Church as most recently stated in the Second Vatican Council' s Gaudium et Spes document: \"The Church . . . holds that in her most benign Lord and Master can be found the key, the focal point and the goal of man, as well as of all human history...all of human life, whether individual or collective, shows itself to be a dramatic struggle between good and evil, between light and darkness...The Lord is the goal of human history the focal point of the longings of history and of civilization, the center of the human race, the joy of every heart and the answer to all its yearnings.\"\n\nIn the 19th century, universal histories proliferated.\n\nPhilosophers such as Kant, Herder, Schiller and Hegel, and political philosophers such as Marx and Herbert Spencer, presented general theories of history that shared essential characteristics with the Biblical account: they conceived of history as a coherent whole, governed by certain basic characteristics or immutable principles. Kant who was one of the earliest thinkers to use the term \"Universal History\" described its meaning in \"Idea for a Universal History with a Cosmopolitan Purpose\":\nAncient history is the study of the past from the beginning of recorded human history to the Early Middle Ages. In India, the period includes the early period of the Middle Kingdoms, and, in China, the time up to the Qin Dynasty is included.\n\nThe Bronze Age forms part of the three-age system. In this system, it follows the Neolithic Age in some areas of the world. In the 24th century BC, the Akkadian Empire was founded. The First Intermediate Period of Egypt (c. 22nd century BC) was followed by the Middle Kingdom of Egypt between the 21st to 17th centuries BC. The Sumerian Renaissance also developed c. 21st century BC. Around the 18th century BC, the Second Intermediate Period of Egypt began. By 1600 BC, Mycenaean Greece developed, the beginning of the Shang Dynasty in China emerged and there was evidence of a fully developed Chinese writing system. Also around 1600 BC, the beginning of Hittite dominance of the Eastern Mediterranean region is seen. From the 16th to 11th centuries BC the New Kingdom of Egypt dominated the Nile Valley. Between 1550 BC and 1292 BC, the Amarna Period developed.\n\nThe Iron Age is the last principal period in the three-age system, preceded by the Bronze Age. Its date and context vary depending on the country or geographical region.\nDuring the 13th to 12th centuries BC, the Ramesside Period occurred in Egypt. Around c. 1200 BC, the Trojan War was thought to have taken place. By c. 1180 BC, the disintegration of the Hittite Empire was underway.\n\nIn 1046 BC, the Zhou force, led by King Wu of Zhou, overthrows the last king of the Shang Dynasty. The Zhou Dynasty is established in China shortly thereafter. In 1000 BC, the Mannaeans Kingdom begins in Western Asia. Around the 10th to 7th centuries BC, the Neo-Assyrian Empire forms in Mesopotamia. In 800 BC, the rise of Greek city-states begins. In 776 BC, the first recorded Olympic Games are held.\n\nThe post-classical era, also known as the Middle Ages, is a historical period following the Iron Age, fully underway by the 5th century and lasting to the 15th century, and preceding the early Modern Era. The medieval history is the middle period, or the middle age, in a three-period division of history: Classic, Medieval, and Modern. The precise dates of the beginning, culmination, and end of the medieval history are more or less arbitrarily assumed according to the point of view adopted. Any hard and fast line drawn to designate either the beginning or close of the period in question is arbitrary. The widest limits given, viz., the irruption of the Visigoths over the boundaries of the Roman Empire, for the beginning, and the Middle Ages of the 16th century, for the close, may be taken as inclusively sufficient, and embrace, beyond dispute, every movement or phase of history that can be claimed as properly belonging to the medieval history.\n\nIn Europe, the period saw the large-scale European Migration and fall of the Western Roman Empire. In South Asia, the middle kingdoms of India were the classical period of the region. The \"Medieval\" period on the Indian subcontinent lasts for some 1,500 years, and ends in the 13th century. During the late medieval history, several Islamic empires were established in the Indian subcontinent. In East Asia, the Mid-Imperial China age begins with the reunification of China and ends with China was conquered by the Mongol Empire. The Golden Horde invaded North and West Asia and parts of eastern Europe in the 13th century and established and maintained their khanate until the end of the medieval history.\n\nThe Early medieval history saw the continuation of trends set up in ancient history (and, for Europe, late Antiquity). The period is usually considered to open with those migrations of the German Tribes which led to the destruction of the Roman Empire in the West in 375, when the Huns fell upon the Gothic tribes north of the Black Sea and forced the Visigoths over the boundaries of the Roman Empire on the lower Danube. A later date, however, is sometimes assumed, viz., when Odoacer deposed Romulus Augustulus, the last of the Roman Emperors of the West, in 476. Depopulation, deurbanization, and increased barbarian invasion were seen across the Old World. North Africa and the Middle East, once part of the Eastern Roman Empire, became Islamic. Later in European history, the establishment of the feudal system allowed a return to systemic agriculture. There was sustained urbanization in northern and western Europe.\n\nDuring the High medieval history in Europe, Christian-oriented art and architecture flourished and Crusades were mounted to recapture the Holy Land from Muslim control. The influence of the emerging states in Europe was tempered by the ideal of an international Christendom. The codes of chivalry and courtly love set rules for proper European behavior, while the European Scholastic philosophers attempted to reconcile Christian faith and reason.\n\nDuring the Late medieval history in Europe, the centuries of prosperity and growth came to a halt.\nThe close of the medieval history is also variously fixed; some make it coincide with the rise of Humanism and the Renaissance in Italy, in the 14th century; with the Fall of Constantinople, in 1453; with the discovery of America by Columbus in 1492; or, again, with the great religious schism of the 16th century. A series of famines and plagues, such as the medieval Great Famine and the Black Death, reduced the population around half before the calamities in the late medieval history. Along with depopulation came social unrest and endemic warfare. Western Europe experienced serious peasant risings: the Jacquerie, the Peasants' Revolt, and the Hundred Years' War. To add to the many problems of the period, the unity of the Catholic Church was shattered by the Western Schism. Collectively the events are a crisis of the Late medieval history.\n\nModern history describes the historical period after the Middle history. Modern history can be further broken down into the \"early modern period\" and the \"late modern period\" after the French Revolution and the Industrial Revolution. \"Contemporary history\" describes the span of historic events that are immediately relevant to the present time. The Great Divergence refers to the period of time in which the process by which the Western Europe and the parts of the New World overcame pre-modern growth constraints and emerged during the 19th century as the powerful and wealthy world civilization of the time, eclipsing Qing China, Mughal India, Tokugawa Japan, and the Ottoman Empire.\n\nThe modern era began approximately in the 16th century. Many major events caused Europe to change around the start of the 16th century, starting with the Fall of Constantinople in 1453, the fall of Muslim Spain and the discovery of the Americas in 1492, and Martin Luther's Protestant Reformation in 1517. In England the modern period is often dated to the start of the Tudor period with the victory of Henry VII over Richard III at the Battle of Bosworth in 1485. Early modern European history is usually seen to span from around the start of the 15th century, through the Age of Reason and the Age of Enlightenment in the 17th and 18th centuries, until the beginning of the Industrial Revolution in the late 18th century.\n\nThe modern era includes the early period, called the early modern period, which lasted from c. 1500 to around c. 1800 (most often 1815). Particular facets of early modernity include:\n\nThe early period ended in a time of political and economic change as a result of mechanization in society, the American Revolution, the first French Revolution; other factors included the redrawing of the map of Europe by the of the Congress of Vienna and the peace established by Second Treaty of Paris which ended the Napoleonic Wars.\n\nAs a result of the Industrial Revolutions and the earlier political revolutions, the worldviews of Modernism emerged. The industrialization of many nations was initiated with the industrialization of Britain. Particular facets of the late modernity period include:\n\nOther important events in the development of the Late modern period include:\n\nThe contemporary \"Great Divergence\" is a term given to a period starting in late 1970s when inequality grew substantially in the United States and to a lesser extent in other countries such as Canada and the United Kingdom. The term originated with Nobel laureate, Princeton economist and \"New York Times\" columnist Paul Krugman, and is a reference to the \"Great Compression\", an earlier era in the 1930s and 40s when income became dramatically more equal in the United States and elsewhere.\n\n\n\n\n"}
{"id": "33692", "url": "https://en.wikipedia.org/wiki?curid=33692", "title": "World history", "text": "World history\n\nWorld history or global history (not to be confused with diplomatic, transnational or international history) is a field of historical study that emerged as a distinct academic field in the 1980s. It examines history from a global perspective. It is not to be confused with comparative history, which, like world history, deals with the history of multiple cultures and nations, but does not do so on a global scale. World history looks for common patterns that emerge across all cultures. World historians use a thematic approach, with two major focal points: integration (how processes of world history have drawn people of the world together) and difference (how patterns of world history reveal the diversity of the human experiences).\n\nJerry H. Bentley has observed that 'the term \"world history\" has never been a clear signifier with a stable referent', and that usage of the term overlaps with universal history, comparative history, global history, big history, macro history, and transnational history, amongst others.\n\nThe advent of world history as a distinct academic field of study can be traced to the 1960s, but the pace quickened in the 1980s. A key step was the creation of the World History Association and graduate programs at a handful of universities. Over the next decades scholarly publications, professional and academic organizations, and graduate programs in World History proliferated. World History has often displaced Western Civilization in the required curriculum of American high schools and universities, and is supported by new textbooks with a world history approach.\n\nWorld History attempts to recognise and address two structures that have profoundly shaped professional history-writing:\nThus World History tends to study networks, connections, and systems that cross traditional boundaries of historical study like linguistic, cultural, and national borders. World History is often concerned to explore social dynamics that have led to large-scale changes in human society, such as industrialisation and the spread of capitalism, and to analyse how large-scale changes like these have affected different parts of the world. Like other branches of history-writing in the second half of the twentieth century, World History has a scope far beyond historians' traditional focus on politics, wars, and diplomacy, taking in a panoply of subjects like gender history, social history, cultural history, and environmental history.\n\n\nThe study of world history, as distinct from national history, has existed in many world cultures. However, early forms of world history were not truly global, and were limited to only the regions known by the historian.\n\nIn Ancient China, Chinese world history, that of China and the surrounding people of East Asia, was based on the dynastic cycle articulated by Sima Qian in circa 100 BC. Sima Qian's model is based on the Mandate of Heaven. Rulers rise when they united China, then are overthrown when a ruling dynasty became corrupt. Each new dynasty begins virtuous and strong, but then decays, provoking the transfer of Heaven's mandate to a new ruler. The test of virtue in a new dynasty is success in being obeyed by China and neighboring barbarians. After 2000 years Sima Qian's model still dominates scholarship, although the dynastic cycle is no longer used for modern Chinese history.\n\nIn Ancient Greece, Herodotus (5th century BC), as founder of Greek historiography, presents insightful and lively discussions of the customs, geography, and history of Mediterranean peoples, particularly the Egyptians. However, his great rival Thucydides promptly discarded Herodotus's all-embracing approach to history, offering instead a more precise, sharply focused monograph, dealing not with vast empires over the centuries but with 27 years of war between Athens and Sparta. In Rome, the vast, patriotic history of Rome by Livy (59 BC-17 AD) approximated Herodotean inclusiveness; Polybius (c.200-c.118 BC) aspired to combine the logical rigor of Thucydides with the scope of Herodotus.\n\nIn Central Asia, The Secret History of Mongols is regarded as the single significant native Mongolian account of Genghis Khan. The Secret History is regarded as a piece of classic literature in both Mongolia and the rest of the world.\n\nIn the Middle East, Ala'iddin Ata-Malik Juvayni (1226–1283) was a Persian historian who wrote an account of the Mongol Empire entitled Ta' rīkh-i jahān-gushā (History of the World Conqueror). The standard edition of Juvayni is published under the title Ta' rīkh-i jahān-gushā, ed. Mirza Muhammad Qazwini, 3 vol, Gibb Memorial Series 16 (Leiden and London, 1912–37). An English translation by John Andrew Boyle \"The History of the World-Conqueror\" was republished in 1997.\n\nRashīd al-Dīn Fadhl-allāh Hamadānī (1247–1318), was a Persian physician of Jewish origin, polymathic writer and historian, who wrote an enormous Islamic history, the Jami al-Tawarikh, in the Persian language, often considered a landmark in intercultural historiography and a key document on the Ilkhanids (13th and 14th century). His encyclopedic knowledge of a wide range of cultures from Mongolia to China to the Steppes of Central Eurasia to Persia, the Arabic-speaking lands, and Europe, provide the most direct access to information on the late Mongol era. His descriptions also highlight the manner in which the Mongol Empire and its emphasis on trade resulted in an atmosphere of cultural and religious exchange and intellectual ferment, resulting in the transmission of a host of ideas from East to West and vice versa.\n\nOne Muslim scholar, Ibn Khaldun (1332-1409) broke with traditionalism and offered a model of historical change in \"Muqaddimah,\" an exposition of the methodology of scientific history. Ibn Khaldun focused on the reasons for the rise and fall of civilization, arguing that the causes of change are to be sought in the economic and social structure of society. His work was largely ignored in the Muslim world. Otherwise the Muslim, Chinese and Indian intellectuals held fast to a religious traditionalism, leaving them unprepared to advise national leaders on how to confront the European intrusion into Asia after 1500 AD.\n\nDuring the Renaissance in Europe, history was written about states or nations. The study of history changed during the Enlightenment and Romanticism. Voltaire described the history of certain ages that he considered important, rather than describing events in chronological order. History became an independent discipline. It was not called \"philosophia historiae\" anymore, but merely history (\"historia\").Voltaire, in the 18th century, attempted to revolutionize the study of world history. First, Voltaire concluded that the traditional study of history was flawed. The Christian Church, one of the most powerful entities in his time, had presented a framework for studying history. Voltaire, when writing \"History of Charles XII\" (1731) and \"The Age of Louis XIV\" (1751), instead choose to focus economics, politics and culture. These aspects of history were mostly unexplored by his contemporaries and would each develop into their own sections of world history. Above all else, Voltaire regarded truth as the most essential part of recording world history. Nationalism and religion only subtracted from objective truth, so Voltaire freed himself for their influence when he recorded history.\n\nGiambattista Vico (1668–1744) in Italy wrote \"Scienza nuva seconda\" (The New Science) in 1725, which argued history as the expression of human will and deeds. He thought that men are historical entities and that human nature changes over time. Each epoch should be seen as a whole in which all aspects of culture—art, religion, philosophy, politics, and economics—are interrelated (a point developed later by Oswald Spengler). Vico showed that myth, poetry, and art are entry points to discovering the true spirit of a culture. Vico outlined a conception of historical development in which great cultures, like Rome, undergo cycles of growth and decline. His ideas were out of fashion during the Enlightenment, but influenced the Romantic historians after 1800.\n\nA major theoretical foundation for world history was given by German philosopher G. W. F. Hegel, who saw the modern Prussian state as the latest (though often confused with the highest) stage of world development.\n\nG.W.F. Hegel developed three lenses through which he believed world history could be viewed. Documents produced during a historical period, such as journal entries and contractual agreements, were considered by Hegel to be part of Original History. These documents are produced by a person enveloped within a culture, making them conduits of vital information but also limited in their contextual knowledge. Documents which pertain to Hegel’s Original History are classified by modern historians as primary sources.\n\nReflective History, Hegel’s second lens, are documents written with some temporal distance separating the event which is discussed in the academic writing. What limited this lens, according to Hegel, was the imposition of the writers own cultural values and views on the historical event. This criticism of Reflective History was later formalized by Anthropologists Franz Boa and coined as Cultural relativism by Alain Locke. Both of these lenses were considered to be partially flawed by Hegel.\n\nHegel termed the lens which he advocated to view world history through as Philosophical History. In order to view history through this lens, one must analyze events, civilizations, and periods objectively. When done in this fashion, the historian can then extract the prevailing theme from their studies. This lens differs from the rest because it is void of any cultural biases and takes a more analytical approach to history. World History can be a broad topic, so focusing on extracting the most valuable information from certain periods may be the most beneficial approach. This third lens, as did Hegel’s definitions of the other two, affected the study of history in the early modern period and our contemporary period.\n\nAnother early modern historian was Adam Ferguson. Ferguson’s main contribution to the study of world history was his \"An Essay on the History of Civil Society\" (1767). According to Ferguson, world history was a combination of two forms of history. One was natural history; the aspects of our world which god created. The other, which was more revolutionary, was social history. For him, social history was the progress humans made towards fulfilling God’s plan for humanity. He believed that progress, which could be achieved through individuals pursuing commercial success, would bring us closer to a perfect society; but we would never reach one. However, he also theorized that a complete dedication to commercial success could lead to societal collapse—like what happened in Rome—because people would lose morality. Through this lens, Ferguson viewed world history as humanities struggle to reach an ideal society.\n\nHenry Home, Lord Kames was a philosopher during the Enlightenment and contributed to the study or world history. In his major historical work, \"Sketches on the History of Man\", Home’s outlined the four stages of human history which he observed. The first and most primitive stage was small hunter-gatherer groups. Then, in order to form larger groups, humans transitioned into the second stage when they began to domesticate animals. The third stage was the development of agriculture. This new technology established trade and higher levels of cooperation amongst sizable groups of people. With the gathering of people into agricultural villages, laws and social obligations needed to be developed so a form of order could be maintained. The fourth, and final stage, involved humans moving into market towns and seaports where agriculture was not the focus. Instead, commerce and other forms of labor arouse in a society. By defining the stages of human history, Homes influenced his successors. He also contributed to the development of other studies such as sociology and anthropology.\n\nWorld history became a popular genre in the 20th century with universal history. In the 1920s, several best-sellers dealt with the history of the world, including surveys \"The Story of Mankind\" (1921) by Hendrik Willem van Loon and \"The Outline of History\" (1918) by H.G. Wells. Influential writers who have reached wide audiences include H. G. Wells, Oswald Spengler, Arnold J. Toynbee, Pitirim Sorokin, Carroll Quigley, Christopher Dawson, and Lewis Mumford. Scholars working the field include Eric Voegelin, William Hardy McNeill and Michael Mann. With evolving technologies such as dating methods and surveying laser technology called LiDAR, contemporary historians have access to knew information which changes how past civilizations are studied.\n\nSpengler's \"Decline of the West\" (2 vol 1919–1922) compared nine organic cultures: Egyptian (3400 BC-1200 BC), Indian (1500 BC-1100 BC), Chinese (1300 BC-AD 200), Classical (1100 BC-400 BC), Byzantine (AD 300–1100), Aztec (AD 1300–1500), Arabian (AD 300–1250), Mayan (AD 600–960), and Western (AD 900–1900). His book was a smashing success among intellectuals worldwide as it predicted the disintegration of European and American civilization after a violent \"age of Caesarism,\" arguing by detailed analogies with other civilizations. It deepened the post-World War I pessimism in Europe, and was warmly received by intellectuals in China, India, and Latin America who hoped his predictions of the collapse of European empires would soon come true.\n\nIn 1936–1954, Toynbee's ten-volume \"A Study of History\" came out in three separate installments. He followed Spengler in taking a comparative topical approach to independent civilizations. Toynbee said they displayed striking parallels in their origin, growth, and decay. Toynbee rejected Spengler's biological model of civilizations as organisms with a typical life span of 1,000 years. Like Sima Qian, Toynbee explained decline as due to their moral failure. Many readers rejoiced in his implication (in vols. 1–6) that only a return to some form of Catholicism could halt the breakdown of western civilization which began with the Reformation. Volumes 7–10, published in 1954, abandoned the religious message, and his popular audience slipped away, while scholars picked apart his mistakes.,\n\nMcNeill wrote \"The Rise of the West\" (1963) to improve upon Toynbee by showing how the separate civilizations of Eurasia interacted from the very beginning of their history, borrowing critical skills from one another, and thus precipitating still further change as adjustment between traditional old and borrowed new knowledge and practice became necessary. McNeill took a broad approach organized around the interactions of peoples across the Earth. Such interactions have become both more numerous and more continual and substantial in recent times. Before about 1500, the network of communication between cultures was that of Eurasia. The term for these areas of interaction differ from one world historian to another and include \"world-system\" and \"ecumene.\" Whatever it is called, the importance of these intercultural contacts has begun to be recognized by many scholars.\n\nT. Walter Wallbank and Alastair M. Taylor co-authored \"Civilization Past & Present\", the first world-history textbook published in the United States (1942). With additional authors, this very successful work went through numerous editions up to the first decade of the twenty-first century. According to the Golden Anniversary edition of 1992, the ongoing objective of \"Civilization Past & Present\" \"was to present a survey of world cultural history, treating the development and growth of civilization not as a unique European experience but as a global one through which all the great culture systems have interacted to produce the present-day world. It attempted to include all the elements of history – social, economic, political, religious, aesthetic, legal, and technological.\" In college curricula of the United States, world history became a popular replacement for courses on Western Civilization. Professors Patrick Manning, previously of Northeastern University and now at the University of Pittsburgh's World History Center; and Ross E. Dunn at San Diego State are leaders in promoting innovative teaching methods.\n\nIn schools of architecture in the U.S., the National Architectural Accrediting Board now requires that schools teach history that includes a non-west or global perspective. This reflects a decade-long effort to move past the standard Euro-centric approach that had dominated the field.\n\nIn recent years, the relationship between African and world history has shifted rapidly from one of antipathy to one of engagement and synthesis. Reynolds (2007) surveys the relationship between African and world histories, with an emphasis on the tension between the area studies paradigm and the growing world-history emphasis on connections and exchange across regional boundaries. A closer examination of recent exchanges and debates over the merits of this exchange is also featured. Reynolds sees the relationship between African and world history as a measure of the changing nature of historical inquiry over the past century.\n\nThe Marxist theory of historical materialism claims the history of the world is fundamentally determined by the \"material conditions\" at any given time – in other words, the relationships which people have with each other in order to fulfil basic needs such as feeding, clothing and housing themselves and their families. Overall, Marx and Engels claimed to have identified five successive stages of the development of these material conditions in Western Europe.\n\nThe theory divides the history of the world into the following periods: Primitive communism; Slave society; Feudalism; Capitalism; and Socialism.\n\nRegna Darnell and Frederic Gleach argue that, in the Soviet Union, the Marxian theory of history was the only accepted orthodoxy, and stifled research into other schools of thought on history. However, adherents of Marx's theories argue that Stalin distorted Marxism.\n\n\n\n\n\n\n\n\n"}
