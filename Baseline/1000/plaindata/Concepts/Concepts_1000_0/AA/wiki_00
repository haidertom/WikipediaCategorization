{"id": "48624755", "url": "https://en.wikipedia.org/wiki?curid=48624755", "title": "Acceptability", "text": "Acceptability\n\nAcceptability is the characteristic of a thing being subject to acceptance for some purpose. A thing is acceptable if it is sufficient to serve the purpose for which it is provided, even if it is far less usable for this purpose than the ideal example. A thing is unacceptable (or has the characteristic of unacceptability) if it deviates so far from the ideal that it is no longer sufficient to serve the desired purpose, or if it goes against that purpose. From a logical perspective, a thing can be said to be acceptable if it has no characteristics that make it unacceptable:\n\nHungarian mathematician Imre Lakatos developed a concept of acceptability \"taken as \"a measure of the approximation to the truth\"\". This concept was criticized in its applicability to philosophy as requiring that better theories first be eliminated. Acceptability is also a key premise of negotiation, wherein opposing sides each begin from a point of seeking their ideal solution, and compromise until they reach a solution that both sides find acceptable:\n\nWhere an unacceptable proposal has been made, \"a counterproposal is generated if there are any acceptable ones that have had already been explored\". Since the acceptability of proposition to a participant in a negotiation is only known to that participant, the participant may act as though a proposal that is actually acceptable to them is not, in order to obtain a more favorable proposal. \n\nOne concept of acceptability that has been widely studied is acceptable risk in situations affecting human health. The idea of not increasing lifetime risk by more than one in a million has become commonplace in public health discourse and policy. It is a heuristic measure. It provides a numerical basis for establishing a negligible increase in risk.\n\nEnvironmental decision making allows some discretion for deeming individual risks potentially \"acceptable\" if less than one in ten thousand chance of increased lifetime risk. Low risk criteria such as these provide some protection for a case where individuals may be exposed to multiple chemicals e.g. pollutants, food additives or other chemicals. In practice, a true zero-risk is possible only with the suppression of the risk-causing activity.\n\nStringent requirements of 1 in a million may not be technologically feasible or may be so prohibitively expensive as to render the risk-causing activity unsustainable, resulting in the optimal degree of intervention being a balance between risks vs. benefit. For example, emissions from hospital incinerators result in a certain number of deaths per year. However, this risk must be balanced against the alternatives. There are public health risks, as well as economic costs, associated with all options. The risk associated with no incineration is potential spread of infectious diseases, or even no hospitals. Further investigation identifies options such as separating noninfectious from infectious wastes, or air pollution controls on a medical incinerator.\n\nAcceptable variance is the range of variance in any direction from the ideal value that remains acceptable. In project management, variance can be defined as \"the difference between what is planned and what is actually achieved\". Degrees of variance \"can be classified into negative variance, zero variance, acceptable variance, and unacceptable variance\". In software testing, for example, \"[g]enerally 0-5% is considered as acceptable variance\" from an ideal value.\n\nAcceptance testing is a practice used in chemical and engineering fields, intended to check ahead of time whether or not a thing will be acceptable.\n"}
{"id": "27658583", "url": "https://en.wikipedia.org/wiki?curid=27658583", "title": "AeroVironment Global Observer", "text": "AeroVironment Global Observer\n\nThe AeroVironment Global Observer is a concept for a high-altitude, long endurance unmanned aerial vehicle, designed by AeroVironment (AV) to operate as a stratospheric geosynchronous satellite system with regional coverage.\n\nTwo Global Observer aircraft, each flying for up to a week at an altitude of , could alternate coverage over any area on the earth, providing a platform for communications relays, remote sensing, or long-term surveillance. In addition to flying above weather and above other conventional aircraft, operation at this altitude permits communications and sensor payloads on the aircraft to service an area on the surface of the earth up to in diameter, equivalent to more than of coverage. Global Observer may offer greater flexibility than a satellite and longer duration than conventional manned and unmanned aircraft.\n\nThe Global Observer Joint Capabilities Technology Demonstration (JCTD) program had the goal of helping solve the capability gap in persistent ISR and communications relay for the US military and homeland security. The Global Observer JCTD demonstrated a new stratospheric, extreme endurance UAS that could be transitioned for post-JCTD development, extended user evaluation, and fielding. The program was a joint effort with the U.S. Department of Defense, Department of Homeland Security, and AeroVironment that started in September 2007, to culminate in a Joint Operational Utility Assessment (JOUA) in 2011.\n\nThe program provided for the system development, production of two aircraft, development flight testing, and JOUA with ISR and communications relay payload. The flight testing and JOUA was conducted at the Air Force Flight Test Center at Edwards Air Force Base, California. The primary objectives of the Global Observer JCTD Program were:\n\n\n\n\nHigh-altitude, long endurance unmanned aerial vehicles, such as Global Observer, may enable several capabilities that enable rapid and effective actions or countermeasures:\n\nA Global Observer prototype, called \"Odyssey,\" flew in May 2005. It had a , one-third the size of the planned full-sized version, and ran solely on hydrogen fuel-cells powering electric motors that drove eight propellers, flying the aircraft for several hours. The JCTD started in September 2007. In August 2010, Aerovironment announced that the full-sized Global Observer wing had passed wing load testing. The 53 m (175 ft) all-composite wing, which comes in five sections and is designed to maximize wing strength while minimizing weight, had loads applied to it that approximated the maximum loads it is designed to withstand during normal flight, turbulence and maneuvers. In its third year of testing, the demonstrator had also undergone ground and taxi tests as well as taken a \"short hop\" lifting off the ground briefly during taxiing.\n\nThe Global Observer performed its first flight on 5 August 2010, taking off from Edwards AFB and reaching an altitude of for one hour. The flight was performed using battery power.The aircraft completed initial flight testing, consisting of multiple low-altitude flights, at Edwards AFB in August and September 2010. This phase used batteries to power the hybrid-electric aircraft and approximate full aircraft weight and center of gravity for flight control, performance, and responsiveness evaluation. Following this, the program team installed and ground tested the aircraft's hydrogen-fueled generator and liquid hydrogen fuel tanks which will power it for up to a week in the stratosphere.\n\nThe first flight of the Global Observer using hydrogen fuel occurred on 11 January 2011, reaching an altitude of for four hours. On 1 April 2011, Global Observer-1 (GO-1), the first aircraft to be completed, crashed 18 hours into its 9th test flight. AeroVironment said it was undergoing flight test envelope expansion and had been operating for nearly twice the endurance and at a higher altitude than previous flights when the crash occurred. At the time, the second aircraft developed as part of the JCTD program was nearing completion at a company facility; the $140 million program was originally scheduled for completion in late 2011, but the crash delayed this by a year. AeroVironment was looking for sources of incremental funding to provide a bridge between the demonstration and a future procurement program.\n\nIn December 2012, the Pentagon closed the development contract for the Global Observer, the reason being the crash in April 2011. The Global Observer was used as a technology demonstration, not a program for a functioning aircraft. In April 2013, the Pentagon stated that no service or defense agency had advocated for it to be a program. AeroVironment is currently in possession of the second prototype Global Observer. On 6 February 2014, AeroVironment announced that it had teamed with Lockheed Martin to sell the Global Observer to international customers. The partnership is focused around building \"atmospheric satellite systems\" around the UAV. The Global Observer may compete for orders with the Boeing Phantom Eye liquid hydrogen-powered long endurance UAV.\n\n\n\n"}
{"id": "38235255", "url": "https://en.wikipedia.org/wiki?curid=38235255", "title": "Affirmation and negation", "text": "Affirmation and negation\n\nIn linguistics and grammar, affirmation and negation (abbreviated respectively ' and ') are the ways that grammar encode negative and positive polarity in verb phrases, clauses, or other utterances. Essentially an affirmative (positive) form is used to express the validity or truth of a basic assertion, while a negative form expresses its falsity. Examples are the sentences \"Jane is here\" and \"Jane is not here\"; the first is affirmative, while the second is negative.\n\nThe grammatical category associated and with affirmative and negative is called polarity. This means that a sentence, verb phrase, etc. may be said to have either affirmative or negative polarity (its polarity may be either affirmative or negative). Affirmative is typically the unmarked polarity, whereas a negative statement is marked in some way, whether by a negating word or particle such as English \"not\", an affix such as Japanese -\"nai\", or by other means, which reverses the meaning of the predicate. The process of converting affirmative to negative is called negation – the grammatical rules for negation vary from language to language, and a given language may have more than one method of doing so.\n\nAffirmative and negative responses (especially, though not exclusively, to questions) are often expressed using particles or words such as \"yes\" and \"no\", where \"yes\" is the affirmative and \"no\" the negative particle.\n\nSpecial affirmative and negative words (particles) are often found in responses to questions, and sometimes to other assertions by way of agreement or disagreement. In English, these are \"yes\" and \"no\" respectively, in French \"oui\", \"si\" and \"non\", in Swedish \"ja\", \"jo\" and \"nej\", and so on. Not all languages make such common use of particles of this type; in some (such as Welsh) it is more common to repeat the verb or another part of the predicate, with or without negation accordingly.\n\nComplications sometimes arise in the case of responses to negative statements or questions; in some cases the response that confirms a negative statement is the negative particle (as in English: \"You're not going out? No.\"), but in some languages this is reversed. Some languages have a distinct form to answer a negative question, such as French \"si\" and Swedish \"jo\" (these serve to contradict the negative statement suggested by the first speaker).\n\nLanguages have a variety of grammatical rules for converting affirmative verb phrases or clauses into negative ones.\n\nIn many languages, an affirmative is made negative by the addition of a particle, meaning \"not\". This may be added before the verb phrase, as with the Spanish \"no\":\nOther examples of negating particles preceding the verb phrase include Italian \"non\", Russian не \"nye\" and Polish \"nie\" (they can also be found in constructed languages: \"ne\" in Esperanto and \"non\" in Interlingua). In some other languages the negating particle follows the verb or verb phrase, as in Dutch:\nParticles following the verb in this way include \"not\" in archaic and dialectal English (\"you remember not\"), \"nicht\" in German (\"ich schlafe nicht\", \"I am not sleeping\"), and \"inte\" in Swedish (\"han hoppade inte\", \"he did not jump\").\n\nIn French, particles are added both before the verb phrase (\"ne\") and after the verb (\"pas\"):\nHowever, in colloquial French the first particle is often omitted: \"Je sais pas\". Similar use of two negating particles can also be found in Afrikaans: \"Hy kan nie Afrikaans praat nie\" (\"He cannot speak Afrikaans\").\n\nIn standard Modern English, negation is achieved by adding \"not\" after an auxiliary verb (which here means one of a special grammatical class of verbs that also includes forms of the copula \"be\"; see English auxiliaries). If no such verb is present then the dummy auxiliary \"do\" (\"does\", \"did\") is introduced – see \"do\"-support. For example:\nDifferent rules apply in subjunctive, imperative and non-finite clauses. For more details see . (In Middle English, the particle \"not\" could follow any verb, e.g. \"I see not the horse.\")\n\nIn some languages, like Welsh, verbs have special inflections to be used in negative clauses. (In some language families, this may lead to reference to a negative mood.) An example is Japanese, which conjugates verbs in the negative after adding the suffix \"-nai\" (indicating negation), e.g. \"taberu\" (\"eat\") and \"tabenai\" (\"do not eat\"). It could be argued that English has joined the ranks of these languages, since negation requires the use of an auxiliary verb and a distinct syntax in most cases; the form of the basic verb can change on negation, as in \"he sings\" vs. \"he doesn't sing\". Zwicky and Pullum have shown that \"n't\" is an inflectional suffix, not a clitic or a derivational suffix.\n\nComplex rules for negation also apply in Finnish; see . In some languages negation may also affect the dependents of the verb; for example in some Slavic languages, such as Russian, the case of a direct object often changes from accusative to genitive when the verb is negated.\n\nNegation can be applied not just to whole verb phrases, clauses or sentences, but also to specific elements (such as adjectives and noun phrases) within sentences. Ways in which this can be done again depend on the grammar of the language in question. English generally places \"not\" before the negated element, as in \"I witnessed not a debate, but a war.\" There are also negating affixes, such as the English prefixes \"non-\", \"un-\", \"in-\", etc. Such elements are called privatives.\n\nThere also exist elements which carry a specialized negative meaning, including pronouns such as \"nobody\", \"none\" and \"nothing\", determiners such as \"no\" (as in \"no apples\"), and adverbs such as \"never\", \"no longer\" and \"nowhere\".\n\nAlthough such elements themselves have negative force, in some languages a clause in which they appear is additionally marked for ordinary negation. For example, in Russian, \"I see nobody\" is expressed as я никого́ не ви́жу \"ja nikovó nye vízhu\", literally \"I nobody not see\" – the ordinary negating particle не \"nye\" (\"not\") is used in addition to the negative pronoun никого́ \"nikovó\" (\"nobody\"). Italian behaves in a similar way: \"Non ti vede nessuno\", \"nobody can see you\", although \"Nessuno ti vede\" is also a possible clause with exactly the same meaning.\n\nIn Russian, all of the elements (\"not\", \"never\", \"nobody\", \"nowhere\") would appear together in the sentence in their negative form. In Italian, a clause works much as in Russian, but \"non\" does not have to be there, and can be there only before the verb if it precedes all other negative elements: \"Tu non porti mai nessuno da nessuna parte\". \"Nobody ever brings you anything here\", however, could be translated \"Nessuno qui ti porta mai niente\" or \"Qui non ti porta mai niente nessuno\". In French, where simple negation is performed using \"ne ... pas\" (see above), specialized negatives appear in combination with the first particle (\"ne\"), but \"pas\" is omitted:\nIn Ancient Greek, a simple negative (οὐ \"ou\" \"not\" or μή \"mḗ\" \"not (modal)\") following another simple or compound negative (e.g. οὐδείς \"oudeís\" \"nobody\") results in an affirmation, whereas a compound negative following a simple or compound negative strengthens the negation:\n\nSimple grammatical negation of a clause in principle has the effect of converting a proposition to its logical negation – replacing an assertion that something is the case by an assertion that it is not the case.\n\nIn some cases, however, particularly when a particular modality is expressed, the semantic effect of negation may be somewhat different. For example, in English, the meaning of \"you must not go\" is not in fact the exact negation of that of \"you must go\" – this would be expressed as \"you don't have to go\" or \"you needn't go\". The negation \"must not\" has a stronger meaning (the effect is to apply the logical negation to the following infinitive rather than to the full clause with \"must\"). For more details and other similar cases, see the relevant sections of English modal verbs.\n\nIn some cases, by way of irony, an affirmative statement may be intended to have the meaning of the corresponding negative, or vice versa. For examples see antiphrasis and sarcasm.\n\nFor the use of double negations or similar as understatements (\"not unappealing\", \"not bad\", etc.) see litotes.\n\n\n\n"}
{"id": "103533", "url": "https://en.wikipedia.org/wiki?curid=103533", "title": "Analogy", "text": "Analogy\n\nAnalogy (from Greek ἀναλογία, \"analogia\", \"proportion\", from \"ana-\" \"upon, according to\" [also \"against\", \"anew\"] + \"logos\" \"ratio\" [also \"word, speech, reckoning\"]) is a cognitive process of transferring information or meaning from a particular subject (the analog, or source) to another (the target), or a linguistic expression corresponding to such a process. In a narrower sense, analogy is an inference or an argument from one particular to another particular, as opposed to deduction, induction, and abduction, in which at least one of the premises, or the conclusion, is general rather than particular in nature. The term analogy can also refer to the relation between the source and the target themselves, which is often (though not always) a similarity, as in the biological notion of analogy.\nAnalogy plays a significant role in problem solving, as well as decision making, argumentation, perception, generalization, memory, creativity, invention, prediction, emotion, explanation, conceptualization and communication. It lies behind basic tasks such as the identification of places, objects and people, for example, in face perception and facial recognition systems. It has been argued that analogy is \"the core of cognition\". Specific analogical language comprises exemplification, comparisons, metaphors, similes, allegories, and parables, but \"not\" metonymy. Phrases like \"and so on\", \"and the like\", \"as if\", and the very word \"like\" also rely on an analogical understanding by the receiver of a message including them. Analogy is important not only in ordinary language and common sense (where proverbs and idioms give many examples of its application) but also in science, philosophy, law and the humanities. The concepts of association, comparison, correspondence, mathematical and morphological homology, homomorphism, iconicity, isomorphism, metaphor, resemblance, and similarity are closely related to analogy. In cognitive linguistics, the notion of conceptual metaphor may be equivalent to that of analogy. Analogy is also a basis for any comparative arguments as well as experiments whose results are transmitted to objects that have been not under examination (e.g., experiments on rats when results are applied to humans).\n\nAnalogy has been studied and discussed since classical antiquity by philosophers, scientists, theologists and lawyers. The last few decades have shown a renewed interest in analogy, most notably in cognitive science.\n\nWith respect to the terms \"source\" and \"target\" there are two distinct traditions of usage:\n\n\nIn ancient Greek the word \"αναλογια\" (\"analogia\") originally meant proportionality, in the mathematical sense, and it was indeed sometimes translated to Latin as \"proportio\". From there analogy was understood as identity of relation between any two ordered pairs, whether of mathematical nature or not. Kant's \"Critique of Judgment\" held to this notion. Kant argued that there can be exactly the same relation between two completely different objects. The same notion of analogy was used in the US-based SAT tests, that included \"analogy questions\" in the form \"A is to B as C is to \"what\"?\" For example, \"Hand is to palm as foot is to ____?\" These questions were usually given in the Aristotelian format: HAND : PALM : : FOOT : ____ While most competent English speakers will immediately give the right answer to the analogy question (\"sole\"), it is more difficult to identify and describe the exact relation that holds both between pairs such as \"hand\" and \"palm\", and between \"foot\" and \"sole\". This relation is not apparent in some lexical definitions of \"palm\" and \"sole\", where the former is defined as \"the inner surface of the hand\", and the latter as \"the underside of the foot\". Analogy and abstraction are different cognitive processes, and analogy is often an easier one. This analogy is not comparing \"all\" the properties between a hand and a foot, but rather comparing the \"relationship\" between a hand and its palm to a foot and its sole. While a hand and a foot have many dissimilarities, the analogy focuses on their similarity in having an inner surface. A computer algorithm has achieved human-level performance on multiple-choice analogy questions from the SAT test. The algorithm measures the similarity of relations between pairs of words (e.g., the similarity between the pairs HAND:PALM and FOOT:SOLE) by statistical analysis of a large collection of text. It answers SAT questions by selecting the choice with the highest relational similarity.\n\nGreek philosophers such as Plato and Aristotle actually used a wider notion of analogy. They saw analogy as a shared abstraction. Analogous objects did not share necessarily a relation, but also an idea, a pattern, a regularity, an attribute, an effect or a philosophy. These authors also accepted that comparisons, metaphors and \"images\" (allegories) could be used as arguments, and sometimes they called them \"analogies\". Analogies should also make those abstractions easier to understand and give confidence to the ones using them.\n\nThe Middle Age saw an increased use and theorization of analogy. Roman lawyers had already used analogical reasoning and the Greek word \"analogia\". Medieval lawyers distinguished \"analogia legis\" and \"analogia iuris\" (see below). In Islamic logic, analogical reasoning was used for the process of qiyas in Islamic sharia law and fiqh jurisprudence. In Christian theology, analogical arguments were accepted in order to explain the attributes of God. Aquinas made a distinction between \"equivocal\", \"univocal\" and \"analogical\" terms, the last being those like \"healthy\" that have different but related meanings. Not only a person can be \"healthy\", but also the food that is good for health (see the contemporary distinction between polysemy and homonymy). Thomas Cajetan wrote an influential treatise on analogy. In all of these cases, the wide Platonic and Aristotelian notion of analogy was preserved. James Francis Ross in \"Portraying Analogy\" (1982), the first substantive examination of the topic since Cajetan's \"De Nominum Analogia\", demonstrated that analogy is a systematic and universal feature of natural languages, with identifiable and law-like characteristics which explain how the meanings of words in a sentence are interdependent.\n\nOn the contrary, Ibn Taymiyya, Francis Bacon and later John Stuart Mill argued that analogy is simply a special case of induction. In their view analogy is an inductive inference from common known attributes to another probable common attribute, which is known only about the source of the analogy, in the following form:\n\nThis view does not accept analogy as an autonomous mode of thought or inference, reducing it to induction. However, autonomous analogical arguments are still useful in science, philosophy and the humanities (see below), which makes this reduction philosophically uninteresting. Moreover, induction tries to achieve general conclusions, while analogy looks for particular ones.\n\nContemporary cognitive scientists use a wide notion of analogy, extensionally close to that of Plato and Aristotle, but framed by Gentner's (1983) structure mapping theory. The same idea of mapping between source and target is used by conceptual metaphor and conceptual blending theorists. Structure mapping theory concerns both psychology and computer science. According to this view, analogy depends on the mapping or alignment of the elements of source and target. The mapping takes place not only between objects, but also between relations of objects and between relations of relations. The whole mapping yields the assignment of a predicate or a relation to the target. Structure mapping theory has been applied and has found considerable confirmation in psychology. It has had reasonable success in computer science and artificial intelligence (see below). Some studies extended the approach to specific subjects, such as metaphor and similarity.\n\nKeith Holyoak and Paul Thagard (1997) developed their multiconstraint theory within structure mapping theory. They defend that the \"coherence\" of an analogy depends on structural consistency, semantic similarity and purpose. Structural consistency is maximal when the analogy is an isomorphism, although lower levels are admitted. Similarity demands that the mapping connects similar elements and relations of source and target, at any level of abstraction. It is maximal when there are identical relations and when connected elements have many identical attributes. An analogy achieves its purpose insofar as it helps solve the problem at hand. The multiconstraint theory faces some difficulties when there are multiple sources, but these can be overcome. Hummel and Holyoak (2005) recast the multiconstraint theory within a neural network architecture. A problem for the multiconstraint theory arises from its concept of similarity, which, in this respect, is not obviously different from analogy itself. Computer applications demand that there are some \"identical\" attributes or relations at some level of abstraction. The model was extended (Doumas, Hummel, and Sandhofer, 2008) to learn relations from unstructured examples (providing the only current account of how symbolic representations can be learned from examples).\n\nMark Keane and Brayshaw (1988) developed their \"Incremental Analogy Machine\" (IAM) to include working memory constraints as well as structural, semantic and pragmatic constraints, so that a subset of the base analog is selected and mapping from base to target occurs in a serial manner. Empirical evidence shows that human analogical mapping performance is influenced by information presentation order.\n\nEqaan Doug and his team challenged the shared structure theory and mostly its applications in computer science. They argue that there is no line between perception, including high-level perception, and analogical thought. In fact, analogy occurs not only after, but also before and at the same time as high-level perception. In high-level perception, humans make representations by selecting relevant information from low-level stimuli. Perception is necessary for analogy, but analogy is also necessary for high-level perception. Chalmers et al. conclude that analogy actually is high-level perception. Forbus et al. (1998) claim that this is only a metaphor. It has been argued (Morrison and Dietrich 1995) that Hofstadter's and Gentner's groups do not defend opposite views, but are instead dealing with different aspects of analogy.\n\nAntoine Cornuéjols has presented analogy as a \"principle of economy\" and \"computational complexity\".\n\nReasoning by analogy is a process of, from a given pair \"(x,f(x))\", extrapolating the function \"f\". In the standard modeling, analogical reasoning involves two \"objects\": the \"source\" and the \"target\". The target is supposed to be incomplete and in need for a complete description using the source. The target has an existing part \"S\" and a missing part \"R\". We assume that we can isolate a situation of the source \"S\", which corresponds to a situation of target \"S\", and the result of the source \"R\", which correspond to the result of the target \"R\". With \"B\", the relation between \"S\" and \"R\", we want \"B\", the relation between \"S\" and \"R\".\n\nIf the source and target are completely known:\n\nUsing Kolmogorov complexity \"K(x)\", defined as the size of the smallest description of \"x\" and Solomonoff's approach to induction, Rissanen (89), Wallace & Boulton (68) proposed the principle of minimum description length. This principle leads to minimize the complexity \"K(target | Source)\" of producing the target from the source.\n\nThis is unattractive in Artificial Intelligence, as it requires a computation over abstract Turing machines. Suppose that \"M\" and \"M\" are local theories of the source and the target, available to the observer. The best analogy between a source case and a target case is the analogy that minimizes:\n\nIf the target is completely unknown:\n\nAll models and descriptions \"M\", \"M\", \"B\", \"S\", and \"S\" leading to the minimization of:\n\nare also those who allow to obtain the relationship \"B\", and thus the most satisfactory \"R\" for formula (1).\n\nThe analogical hypothesis, which solves an analogy between a source case and a target case, has two parts:\n\nHowever, a \"cognitive agent\" may simply reduce the amount of information necessary for the interpretation of the source and the target, without taking into account the cost of data replication. So, it may prefer to the minimization of (2) the minimization of the following simplified formula:\n\n\nLogicians analyze how analogical reasoning is used in arguments from analogy.\nAn analogy can be stated using \"is to\" and \"as\" to represent the analogous relationship between two pairs of expressions, for example, \"Smile is to mouth, as wink is to eye.\" In the field of mathematics and logic, this can be formalized with colon notation to represent the relationships, using single colon for ratio, and double colon for equality.\n\nIn the field of testing, the colon notation of ratios and equality is often borrowed, so that the example above might be rendered, \"Smile : mouth :: wink : eye\" and pronounced the same way.\n\n\n\n\nSome types of analogies can have a precise mathematical formulation through the concept of isomorphism. In detail, this means that given two mathematical structures of the same type, an analogy between them can be thought of as a bijection between them which preserves some or all of the relevant structure. For example, formula_1 and formula_2 are isomorphic as vector spaces, but the complex numbers, formula_2, have more structure than formula_1 does: formula_2 is a field as well as a vector space.\n\nCategory theory takes the idea of mathematical analogy much further with the concept of functors. Given two categories C and D, a functor \"f\" from C to D can be thought of as an analogy between C and D, because \"f\" has to map objects of C to objects of D and arrows of C to arrows of D in such a way that the compositional structure of the two categories is preserved. This is similar to the structure mapping theory of analogy of Dedre Gentner, in that it formalizes the idea of analogy as a function which satisfies certain conditions.\n\nSteven Phillips and William H. Wilson use category theory to mathematically demonstrate how the analogical reasoning in the human mind, that is free of the spurious inferences that plague conventional artificial intelligence models, (called \"systematicity\"), could arise naturally from the use of relationships between the internal arrows that keep the internal structures of the categories rather than the mere relationships between the objects (called \"representational states\"). Thus, the mind may use analogies between domains whose internal structures fit according with a natural transformation and reject those that do not.\n\nSee also case-based reasoning.\n\nIn anatomy, two anatomical structures are considered to be \"analogous\" when they serve similar functions but are not evolutionarily related, such as the legs of vertebrates and the legs of insects. Analogous structures are the result of convergent evolution and should be contrasted with homologous structures.\n\nOften a physical prototype is built to model and represent some other physical object. For example, wind tunnels are used to test scale models of wings and aircraft, which act as an analogy to full-size wings and aircraft.\n\nFor example, the MONIAC (an analog computer) used the flow of water in its pipes as an analog to the flow of money in an economy.\n\nWhere there is dependence and hence interaction between a pair or more of biological or physical participants communication occurs and the stresses produced describe internal models inside the participants. Pask in his Conversation Theory asserts there exists an analogy exhibiting both similarities and differences between any pair of the participants' internal models or concepts.\n\nAnalogical reasoning plays a very important part in morality. This may be in part because morality is supposed to be impartial and fair. If it is wrong to do something in a situation A, and situation B is analogous to A in all relevant features, then it is also wrong to perform that action in situation B. Moral particularism accepts analogical moral reasoning, rejecting both deduction and induction, since only the former can do without moral principles.\n\nIn law, analogy is primarily used to resolve issues on which there is no previous authority. A distinction can be made between analogical reasoning employed in statutory law and analogical reasoning present in precedential law (case law).\n\nIn statutory law analogy is used in order to fill the so-called lacunas or gaps or loopholes.\n\nFirst, a gap arises when a specific case or legal issue is not explicitly dealt with in written law. Then, one may try to identify a statutory provision which covers the cases that are similar to the case at hand and apply to this case this provision by analogy. Such a gap, in civil law countries, is referred to as a gap extra legem (outside of the law), while analogy which liquidates it is termed analogy extra legem (outside of the law). The very case at hand is named: an unprovided case.\n\nSecond, a gap comes into being when there is a statutory provision which applies to the case at hand but this provision leads in this case to an unwanted outcome. Then, upon analogy to another statutory provision that covers cases similar to the case at hand, this case is resolved upon this provision instead of the provision that applies to it directly. This gap is called a gap contra legem (against the law), while analogy which fills this gap is referred to as analogy contra legem (against the law).\n\nThird, a gap occurs when there is a statutory provision which regulates the case at hand, but this provision is vague or equivocal. In such circumstances, to decide the case at hand, one may try to ascertain the meaning of this provision by recourse to statutory provisions which address cases that are similar to the case at hand or other cases that are regulated by vague/equivocal provision. A gap of this type is named gap intra legem (within the law) and analogy which deals with it is referred to as analogy intra legem (within the law).\n\nThe similarity upon which statutory analogy depends on may stem from the resemblance of raw facts of the cases being compared, the purpose (the so-called ratio legis which is generally the will of the legislature) of a statutory provision which is applied by analogy or some other sources.\n\nStatutory analogy may be also based upon more than one statutory provision or even a spirit of law. In the latter case, it is called analogy iuris (from the law in general) as opposed to analogy legis (from a specific legal provision or provisions).\n\nIn statutory law analogy is also sometimes applied in order to liquidate the so-called conflicting or logical gap (i.e. the situation when two or more statutory provisions contradict each other) or the sui generis gap which stems from the lack of statutory regulation enabling the delivering of a decision whose passing is required by the law. Some other - less common as so-called ‘pertinent application of law’, ejusdem generis, typological notions or presence of analogical pattern of reasoning in an a fortiori and comparative argument - usages are also distinguished.\n\nFirst, in precedential law (case law), analogies can be drawn from precedent cases (cases decided in past). The judge who decides the case at hand may find that the facts of this case are similar to the facts of one of precedential cases to an extent that the outcomes of these cases are justified to be the same or similar. Such use of analogy in precedential law pertains mainly to the so-called: cases of first impression, i.e. the cases which as yet have not been regulated by any binding judicial precedent (are not covered by a ratio decidendi of such a precedent).\n\nSecond, in precedential law, reasoning from (dis)analogy is amply employed, while a judge is distinguishing a precedent. That is, upon the discerned differences between the case at hand and the precedential case, a judge reject to decide the case upon the precedent whose ratio decidendi (precedential rule) embraces the case at hand.\n\nThird, there is also much room for some other usages of analogy in the province of precedential law. One of them is resort to analogical reasoning, while resolving the conflict between two or more precedents which all apply to the case at hand despite dictating different legal outcome for that case. Analogy can also take part in ascertaining the contents of ratio decidendi, deciding upon obsolete precedents or quoting precedents form other jurisdictions. It is too visible in legal eductaion, notably in the US (the so-called 'case method').\n\nAn argument from analogy employed in precedential law is called case analogy as opposed to analogy employed in statutory law which is accordingly termed statutory analogy.\n\nIn precedential law as well as in statutory law, analogy is also considered as a means of application of legal rules (statutory and precedential), serving thus as an alternative to legal deduction (legal syllogism). Then, there are compared instances to which a given rule applies with certainty with the facts of the case at hand. If the sufficient (relevant) similarity between them obtains, the rule is applied to the case at hand. Otherwise, the rule is deemed as inadequate for this case. Such analogy becomes a legal method.\n\nApplication of legal rules through analogy is more typical of the common law legal systems, especially when one deals with the so-called holdings (the denotation of a binding element of a judicial precedent in the US), being in civil law legal systems rather a proposition than an official mode of applying the law.\n\nThe instances from which analogy starts here off are called: base points, typical instances or paradigmatic cases.\n\nIn legal matters, sometimes the use of analogy is forbidden (by the very law or common agreement between judges and scholars). The most common instances concern criminal, administrative and tax law.\n\nAnalogy should not be resorted to in criminal matters whenever its outcome would be unfavorable to the accused or suspect. Such a ban finds its footing in the very principle: “\"nullum crimen, nulla poena sine lege\"”, a principle which is understood in the way that there is no crime (punishment) unless it is expressly provided for in a statutory provision or an already existing judicial precedent.\n\nAnalogy should be applied with caution in the domain of tax law. Here, the principle: “\"nullum tributum sine lege\"” justifies a general ban on the employment of analogy that would lead to an increase in taxation or whose results would – for some other reason(s) – be to the detriment to the interests of taxpayers.\n\nExtending by analogy those provisions of administrative law that restrict human rights and the rights of the citizens (particularly the category of the so-called “individual rights” or “basic rights”) is as a rule prohibited. Analogy generally should also not be resorted to in order to make the citizen's burdens and obligations larger or more vexatious.\n\nThe other limitations on the use of analogy in law, among many others, pertain to:\n\n\nIn civil (private) law, the use of analogy is as a rule permitted or even ordered by law. But also in this branch of law there are some restrictions confining the possible scope of the use of an analogical argument. Such is, for instance, the prohibition to use analogy in relation to provisions regarding time limits or a general ban on the recourse to analogical arguments which lead to extension of those statutory provisions which envisage some obligations or burdens or which order (mandate) something. The other examples concern the usage of analogy in the field of property law, especially when one is going to create some new property rights by it or to extend these statutory provisions whose terms are unambiguous (unequivocal) and plain (clear), e.g.: be of or under cartian age.\n\nThe aforementioned bans on the use of analogy concern rather analogy which goes beyond the possible linguistic meaning of a statutory provision in question and do not pertain to analogy whose conclusions would remain within this meaning.\n\nAnalogy in law – apart from the terminological distinctions mentioned above – can be found also under such Latin names and phrases as:\n\n\nLegal analogy is sometimes claimed to be of a different nature than analogy that occurs in empirical science and everyday life. It is due to several peculiar factors. First, there is the lack of possibility of verification of conclusions of legal analogy on empirical grounds, which entails the necessity of performance of a legal analogical argument both heuristic and probative function. Second, legal analogy, as the law itself, is by definition prescriptive, non-descriptive. Third, it has an obligatory character: a judge is in many circumstances obliged to reason by analogy (treat similar cases in a similar manner). Fourth, the use of analogy in law rather does not hinge on complex underling doctrines or theories. Fifth, serious practical consequences flow from the use of analogy in law. Sixth, the points of comparison are easily recognizable in case of legal analogy. Seventh, analogy in law becomes a vehicle for extension of authority. Eighth, how to reason by analogy is a subject of legal training and education. Ninth, legal analogy has gained enormous amount of attention and scrutiny amongst scholars.\n\nLegal analogy usually assumes the classical structure:\n\nA case A possesses features X, Y, Z and has ascribed legal consequence G (the first premise).\n\nAn unregulated (unprovided) case B possesses features X, Y, Z (the second premise).\n\nTherefore, the case B should be ascribed the legal consequence G (the analogical conclusion).\n\nor:\n\nThere is a rule in force which addresses cases which features are A, B, C, D (the first premise).\n\nThere are unregulated (unprovided) cases which features are A, B, C and E or cases which features are A, B, C, D and E or cases which features are A, B, C and non-D (the second premise).\n\nTherefore, there should be also a rule in force which addresses cases which features are A, B, C and E or A, B, C, D and E or A, B, C and non-D that prescribes the same or similar legal consequece for these cases as the rule which addresses cases which features are A, B, C, D (the analogical conclusion).\n\nLegal analogy can, however, assume also the structure of (mathematical) proportion, i.e.: A is to B as C is to D or A is to B as B is to C.\n\nThe contemporary proponents of proportional analogy, including legal one, are Chaïm Perelman and Lucie Olbrechts Tyteca.,\n\nSpecifically, in law, analogy of proportion takes the form:\n\n1) Determination of the relation that obtains between the facts of the regulated (provided) case and its legal consequence.\n\n2) Determination of the relation that obtains between the facts of the case at hand and their posited legal consequence (i.e. the consequence that is supposed to be potentially adequate for this case).\n\n3) Having ascertained that the relations pointed out in points 1 and 2 are identical or similar to each other, attribution to the case at hand the legal consequence which has been posited for that case.\n\nAnalogies as defined in rhetoric are a comparison between words, but an analogy can be used in teaching as well. An analogy as used in teaching would be comparing a topic that students are already familiar with, with a new topic that is being introduced so that students can get a better understanding of the topic and relate back to previous knowledge. Shawn Glynn, a professor in the department of educational psychology and instructional technology at the University of Georgia, developed a theory on teaching with analogies and developed steps to explain the process of teaching with this method. The steps for teaching with analogies are as follows: Step one is introducing the new topic that is about to be taught and giving some general knowledge on the subject. Step two is reviewing the concept that the students already know to ensure they have the proper knowledge to assess the similarities between the two concepts. Step three is finding relevant features within the analogy of the two concepts. Step four is finding similarities between the two concepts so students are able to compare and contrast them in order to understand. Step five is indicating where the analogy breaks down between the two concepts. And finally, step six is drawing a conclusion about the analogy and comparison of the new material with the already learned material. Typically this method is used to learn topics in science.\n\nIn 1989 Kerry Ruef, a teacher, began an entire program, which she titled The Private Eye Project. It is a method of teaching that revolves around using analogies in the classroom to better explain topics. She thought of the idea to use analogies as a part of curriculum because she was observing objects once and she said, \"my mind was noting what else each object reminded me of...\" This led her to teach with the question, \"what does [the subject or topic] remind you of?\" The idea of comparing subjects and concepts led to the development of The Private Eye Project as a method of teaching. The program is designed to build critical thinking skills with analogies as one of the main themes revolving around it. While Glynn focuses on using analogies to teach science, The Private Eye Project can be used for any subject including writing, math, art, social studies, and invention. It is now used by thousands of schools around the country.\nThere are also various pedagogic innovations now emerging that use visual analogies for cross-disciplinary teaching and research, for instance between science and the humanities.\n\nThe Fourth Lateran Council of 1215 taught: \"For between creator and creature there can be noted no similarity so great that a greater dissimilarity cannot be seen between them.\"\n\nThe theological exploration of this subject is called the \"analogia entis\". The consequence of this theory is that all true statements concerning God (excluding the concrete details of Jesus' earthly life) are analogical and approximations, without that implying any falsity. Such analogical and true statements would include \"God is\", \"God is Love\", \"God is a consuming fire\", \"God is near to all who call him\", or God as Trinity, where \"being\", \"love\", \"fire\", \"distance\", \"number\" must be classed as analogies that allow human cognition of what is infinitely beyond positive or negative language.\n\nThe use of theological statements in syllogisms must take into account their essential analogical character, in that every analogy breaks down when stretched beyond its intended meaning.\n\n\nVisual analogies have been developed that enable researchers to \"investigate literary studies by means of attractive analogies taken principally from science and mathematics. These analogies bring to literary discourse a stock of exciting visual ideas for teaching and research...\" \n\n\n\n"}
{"id": "5354120", "url": "https://en.wikipedia.org/wiki?curid=5354120", "title": "Apollonian and Dionysian", "text": "Apollonian and Dionysian\n\nThe Apollonian and Dionysian is a philosophical and literary concept, or also a dichotomy, based on Apollo and Dionysus in Greek mythology. Some Western philosophical and literary figures have invoked this dichotomy in critical and creative works, most notably Friedrich Nietzsche and later followers. \n\nIn Greek mythology, Apollo and Dionysus are both sons of Zeus. Apollo is the god of the sun, of rational thinking and order, and appeals to logic, prudence and purity. Dionysus is the god of wine and dance, of irrationality and chaos, and appeals to emotions and instincts. The Ancient Greeks did not consider the two gods to be opposites or rivals, although often the two deities were entwined by nature.\n\nAlthough the use of the concepts of the Apollonian and Dionysian is linked to Nietzsche's \"The Birth of Tragedy\", the terms were used before him in German culture. The poet Hölderlin spoke of them, while Winckelmann talked of Bacchus, the god of wine. After Nietzsche, others have continued to make use of the distinction. For example, Rudolf Steiner treated in depth the Apollonian and Dionysian and placed them in the general history and spiritual evolution of mankind.\n\nNietzsche's aesthetic usage of the concepts, which was later developed philosophically, first appeared in his book \"The Birth of Tragedy\", which was published in 1872. His major premise here was that the fusion of Dionysian and Apollonian \"Kunsttriebe\" (\"artistic impulses\") form dramatic arts, or tragedies. He goes on to argue that this fusion has not been achieved since the ancient Greek tragedians. Nietzsche is adamant that the works of Aeschylus and Sophocles represent the apex of artistic creation, the true realization of tragedy; it is with Euripides that tragedy begins its downfall (\"Untergang\"). Nietzsche objects to Euripides's use of Socratic rationalism (the dialectic) in his tragedies, claiming that the infusion of ethics and reason robs tragedy of its foundation, namely the fragile balance of the Dionysian and Apollonian.\n\nTo further the split, Nietzsche diagnoses the Socratic Dialectic as being diseased in the manner that it deals with looking at life. The scholarly dialectic is directly opposed to the concept of the Dionysian because it only seeks to negate life; it uses reason to always deflect, but never to create. Socrates rejects the intrinsic value of the senses and life for \"higher\" ideals. Nietzsche claims in \"The Gay Science\" that when Socrates drinks the hemlock, he sees the hemlock as the cure for life, proclaiming that he has been sick a long time. (Section 340.) In contrast, the Dionysian existence constantly seeks to affirm life. Whether in pain or pleasure, suffering or joy, the intoxicating revelry that Dionysus has for life itself overcomes the Socratic sickness and perpetuates the growth and flourishing of visceral life force—a great Dionysian 'Yes', to a Socratic 'No'. \nThe interplay between the Apollonian and Dionysian is apparent, Nietzsche claimed in \"The Birth of Tragedy\", from their use in Greek tragedy: the tragic hero of the drama, the main protagonist, struggles to make order of his unjust fate, though he dies unfulfilled in the end. For the audience of such a drama, Nietzsche claimed, this tragedy allows them to sense an underlying essence, what he called the \"Primordial Unity\", which revives our Dionysian nature—which is almost indescribably pleasurable. However, he later dropped this concept saying it was \"...burdened with all the errors of youth\" (Attempt at Self-Criticism, §2), the overarching theme was a sort of metaphysical solace or connection with the heart of creation.\n\nDifferent from Kant's idea of the sublime, the Dionysian is all-inclusive rather than alienating to the viewer as a sublimating experience. The sublime needs critical distance, while the Dionysian demands a closeness of experience. According to Nietzsche, the critical distance, which separates man from his closest emotions, originates in Apollonian ideals, which in turn separate him from his essential connection with self. The Dionysian embraces the chaotic nature of such experience as all-important; not just on its own, but as it is intimately connected with the Apollonian. The Dionysian magnifies man, but only so far as he realizes that he is one and the same with all ordered human experience. The godlike unity of the Dionysian experience is of utmost importance in viewing the Dionysian as it is related to the Apollonian, because it emphasizes the harmony that can be found within one's chaotic experience.\n\nNietzsche's idea has been interpreted as an expression of \"fragmented consciousness\" or existential instability by a variety of modern and post-modern writers, especially Martin Heidegger, Michel Foucault and Gilles Deleuze. According to Peter Sloterdijk, the Dionysian and the Apollonian form a dialectic; they are contrasting, but Nietzsche does not mean one to be valued more than the other. Truth being \"primordial pain\", our existential being is determined by the Dionysian/Apollonian dialectic.\n\nExtending the use of the Apollonian and Dionysian onto an argument on interaction between the mind and physical environment, Abraham Akkerman has pointed to masculine and feminine features of city form.\n\nAnthropologist Ruth Benedict used the terms to characterize cultures that value restraint and modesty (Apollonian) and ostentatiousness and excess (Dionysian). An example of an Apollonian culture in Benedict's analysis was the Zuñi people as opposed to the Dionysian Kwakiutl people. The theme was developed by Benedict in her main work \"Patterns of Culture\".\n\nAlbert Szent-Györgyi, who wrote that \"a discovery must be, by definition, at variance with existing knowledge\", divided scientists into two categories: the Apollonians and the Dionysians. He called scientific dissenters, who explored \"the fringes of knowledge\", Dionysians. He wrote, \"In science the Apollonian tends to develop established lines to perfection, while the Dionysian rather relies on intuition and is more likely to open new, unexpected alleys for research...The future of mankind depends on the progress of science, and the progress of science depends on the support it can find. Support mostly takes the form of grants, and the present methods of distributing grants unduly favor the Apollonian.\"\n\nAmerican humanities scholar Camille Paglia writes about the Apollonian and Dionysian in her 1990 bestseller \"Sexual Personae\". The broad outline of her concept has roots in Nietzschean discourse, an admitted influence, although Paglia's ideas diverge significantly.\n\nThe Apollonian and Dionysian concepts comprise a dichotomy that serves as the basis of Paglia's theory of art and culture. For Paglia, the Apollonian is light and structured while the Dionysian is dark and chthonic (she prefers \"Chthonic\" to Dionysian throughout the book, arguing that the latter concept has become all but synonymous with hedonism and is inadequate for her purposes, declaring that \"the Dionysian is no picnic.\"). The Chthonic is associated with females, wild/chaotic nature, and unconstrained sex/procreation. In contrast, the Apollonian is associated with males, clarity, celibacy and/or homosexuality, rationality/reason, and solidity, along with the goal of oriented progress: \"Everything great in western civilization comes from struggle against our origins.\"\n\nShe argues that there is a biological basis to the Apollonian/Dionysian dichotomy, writing: \"The quarrel between Apollo and Dionysus is the quarrel between the higher cortex and the older limbic and reptilian brains.\" Moreover, Paglia attributes all the progress of human civilization to masculinity revolting against the Chthonic forces of nature, and turning instead to the Apollonian trait of ordered creation. The Dionysian is a force of chaos and destruction, which is the overpowering and alluring chaotic state of wild nature. Rejection of – or combat with – Chthonianism by socially constructed Apollonian virtues accounts for the historical dominance of men (including asexual and homosexual men; and childless and/or lesbian-leaning women) in science, literature, arts, technology and politics. As an example, Paglia states: \"The male orientation of classical Athens was inseparable from its genius. Athens became great not despite but because of its misogyny.\"\n\n"}
{"id": "5370", "url": "https://en.wikipedia.org/wiki?curid=5370", "title": "Category of being", "text": "Category of being\n\nIn ontology, the different kinds or ways of being are called categories of being; or simply categories. To investigate the categories of being is to determine the most fundamental and the broadest classes of entities. A distinction between such categories, in making the categories or applying them, is called an ontological distinction.\n\nThe process of abstraction required to discover the number and names of the categories has been undertaken by many philosophers since Aristotle and involves the careful inspection of each concept to ensure that there is no higher category or categories under which that concept could be subsumed. The scholars of the twelfth and thirteenth centuries developed Aristotle's ideas, firstly, for example by Gilbert of Poitiers, dividing Aristotle's ten categories into two sets, primary and secondary, according to whether they inhere in the subject or not:\nSecondly, following Porphyry’s likening of the classificatory hierarchy to a tree, they concluded that the major classes could be subdivided to form subclasses, for example, Substance could be divided into Genus and Species, and Quality could be subdivided into Property and Accident, depending on whether the property was necessary or contingent. An alternative line of development was taken by Plotinus in the second century who by a process of abstraction reduced Aristotle’s list of ten categories to five: Substance, Relation, Quantity, Motion and Quality. Plotinus further suggested that the latter three categories of his list, namely Quantity, Motion and Quality correspond to three different kinds of relation and that these three categories could therefore be subsumed under the category of Relation. This was to lead to the supposition that there were only two categories at the top of the hierarchical tree, namely Substance and Relation, and if relations only exist in the mind as many supposed, to the two highest categories, Mind and Matter, reflected most clearly in the dualism of René Descartes.\n\nAn alternative conclusion however began to be formulated in the eighteenth century by Immanuel Kant who realised that we can say nothing about Substance except through the relation of the subject to other things. In the sentence \"This is a house\" the substantive subject \"house\" only gains meaning in relation to human use patterns or to other similar houses. The category of Substance disappears from Kant's tables, and under the heading of Relation, Kant lists \"inter alia\" the three relationship types of Disjunction, Causality and Inherence. The three older concepts of Quantity, Motion and Quality, as Peirce discovered, could be subsumed under these three broader headings in that Quantity relates to the subject through the relation of Disjunction; Motion relates to the subject through the relation of Causality; and Quality relates to the subject through the relation of Inherence. Sets of three continued to play an important part in the nineteenth century development of the categories, most notably in G.W.F. Hegel's extensive tabulation of categories, and in C.S. Peirce's categories set out in his work on the logic of relations. One of Peirce's contributions was to call the three primary categories Firstness, Secondness and Thirdness which both emphasises their general nature, and avoids the confusion of having the same name for both the category itself and for a concept within that category.\n\nIn a separate development, and building on the notion of primary and secondary categories introduced by the Scholastics, Kant introduced the idea that secondary or \"derivative\" categories could be derived from the primary categories through the combination of one primary category with another. This would result in the formation of three secondary categories: the first, \"Community\" was an example that Kant gave of such a derivative category; the second, \"Modality\", introduced by Kant, was a term which Hegel, in developing Kant's dialectical method, showed could also be seen as a derivative category; and the third, \"Spirit\" or \"Will\" were terms that Hegel and Schopenhauer were developing separately for use in their own systems. Karl Jaspers in the twentieth century, in his development of existential categories, brought the three together, allowing for differences in terminology, as Substantiality, Communication and Will. This pattern of three primary and three secondary categories was used most notably in the nineteenth century by Peter Mark Roget to form the six headings of his Thesaurus of English Words and Phrases. The headings used were the three objective categories of Abstract Relation, Space (including Motion) and Matter and the three subjective categories of Intellect, Feeling and Volition, and he found that under these six headings all the words of the English language, and hence any possible predicate, could be assembled.\n\nIn the twentieth century the primacy of the division between the subjective and the objective, or between mind and matter, was disputed by, among others, Bertrand Russell and Gilbert Ryle. Philosophy began to move away from the metaphysics of categorisation towards the linguistic problem of trying to differentiate between, and define, the words being used. Ludwig Wittgenstein’s conclusion was that there were no clear definitions which we can give to words and categories but only a \"halo\" or \"corona\" of related meanings radiating around each term. Gilbert Ryle thought the problem could be seen in terms of dealing with \"a galaxy of ideas\" rather than a single idea, and suggested that category mistakes are made when a concept (e.g. \"university\"), understood as falling under one category (e.g. abstract idea), is used as though it falls under another (e.g. physical object). With regard to the visual analogies being used, Peirce and Lewis, just like Plotinus earlier, likened the terms of propositions to points, and the relations between the terms to lines. Peirce, taking this further, talked of univalent, bivalent and trivalent relations linking predicates to their subject and it is just the number and types of relation linking subject and predicate that determine the category into which a predicate might fall. Primary categories contain concepts where there is one dominant kind of relation to the subject. Secondary categories contain concepts where there are two dominant kinds of relation. Examples of the latter were given by Heidegger in his two propositions \"the house is on the creek\" where the two dominant relations are spatial location (Disjunction) and cultural association (Inherence), and \"the house is eighteenth century\" where the two relations are temporal location (Causality) and cultural quality (Inherence). A third example may be inferred from Kant in the proposition \"the house is impressive or sublime\" where the two relations are spatial or mathematical disposition (Disjunction) and dynamic or motive power (Causality). Both Peirce and Wittgenstein introduced the analogy of colour theory in order to illustrate the shades of meanings of words. Primary categories, like primary colours, are analytical representing the furthest we can go in terms of analysis and abstraction and include Quantity, Motion and Quality. Secondary categories, like secondary colours, are synthetic and include concepts such as Substance, Community and Spirit.\n\nOne of Aristotle’s early interests lay in the classification of the natural world, how for example the genus \"animal\" could be first divided into \"two-footed animal\" and then into \"wingless, two-footed animal\". He realised that the distinctions were being made according to the qualities the animal possesses, the quantity of its parts and the kind of motion that it exhibits. To fully complete the proposition \"this animal is…\" Aristotle stated in his work on the Categories that there were ten kinds of predicate where...\n\n\"…each signifies either substance or quantity or quality or relation or where or when or being-in-a-position or having or acting or being acted upon\".\n\nHe realised that predicates could be simple or complex. The simple kinds consist of a subject and a predicate linked together by the \"categorical\" or inherent type of relation. For Aristotle the more complex kinds were limited to propositions where the predicate is compounded of two of the above categories for example \"this is a horse running\". More complex kinds of proposition were only discovered after Aristotle by the Stoic, Chrysippus, who developed the \"hypothetical\" and \"disjunctive\" types of syllogism and these were terms which were to be developed through the Middle Ages and were to reappear in Kant's system of categories.\n\n\"Category\" came into use with Aristotle's essay \"Categories\", in which he discussed univocal and equivocal terms, predication, and ten categories:\n\nPlotinus in writing his \"Enneads\" around AD 250 recorded that \"philosophy at a very early age investigated the number and character of the existents… some found ten, others less…. to some the genera were the first principles, to others only a generic classification of existents\". He realised that some categories were reducible to others saying \"why are not Beauty, Goodness and the virtues, Knowledge and Intelligence included among the primary genera?\" He concluded that such transcendental categories and even the categories of Aristotle were in some way posterior to the three Eleatic categories first recorded in Plato's dialogue \"Parmenides\" and which comprised the following three coupled terms: \n\nPlotinus called these \"the hearth of reality\" deriving from them not only the three categories of Quantity, Motion and Quality but also what came to be known as \"the three moments of the Neoplatonic world process\":\n\nPlotinus likened the three to the centre, the radii and the circumference of a circle, and clearly thought that the principles underlying the categories were the first principles of creation. \"From a single root all being multiplies\". Similar ideas were to be introduced into Early Christian thought by, for example, Gregory of Nazianzus who summed it up saying \"Therefore Unity, having from all eternity arrived by motion at duality, came to rest in trinity\".\n\nIn the \"Critique of Pure Reason\" (1781), Immanuel Kant argued that the categories are part of our own mental structure and consist of a set of \"a priori\" concepts through which we interpret the world around us. These concepts correspond to twelve logical functions of the understanding which we use to make judgements and there are therefore two tables given in the \"Critique\", one of the Judgements and a corresponding one for the Categories. To give an example, the logical function behind our reasoning from ground to consequence (based on the Hypothetical relation) underlies our understanding of the world in terms of cause and effect (the Causal relation). In each table the number twelve arises from, firstly, an initial division into two: the Mathematical and the Dynamical; a second division of each of these headings into a further two: Quantity and Quality, and Relation and Modality respectively; and, thirdly, each of these then divides into a further three subheadings as follows.\nTable of Judgements\n\nMathematical\nDynamical\n\nTable of Categories\n\nMathematical\nDynamical\n\nCriticism of Kant's system followed, firstly, by Arthur Schopenhauer, who amongst other things was unhappy with the term \"Community\", and declared that the tables \"do open violence to truth, treating it as nature was treated by old-fashioned gardeners\", and secondly, by W.T.Stace who in his book \"The Philosophy of Hegel\" suggested that in order to make Kant's structure completely symmetrical a third category would need to be added to the Mathematical and the Dynamical. This, he said, Hegel was to do with his category of Notion.\n\nG.W.F. Hegel in his \"Science of Logic\" (1812) attempted to provide a more comprehensive system of categories than Kant and developed a structure that was almost entirely triadic. So important were the categories to Hegel that he claimed \"the first principle of the world, the Absolute, is a system of categories… the categories must be the reason of which the world is a consequent\".\n\nUsing his own logical method of combination, later to be called the Hegelian dialectic, of arguing from thesis through antithesis to synthesis, he arrived, as shown in W.T.Stace's work cited, at a hierarchy of some 270 categories. The three very highest categories were Logic, Nature and Spirit. The three highest categories of Logic, however, he called Being, Essence and Notion which he explained as follows:\nSchopenhauer's category that corresponded with Notion was that of Idea, which in his \"Four-Fold Root of Sufficient Reason\" he complemented with the category of the Will. The title of his major work was \"The World as Will and Idea\". The two other complementary categories, reflecting one of Hegel's initial divisions, were those of Being and Becoming. At around the same time, Goethe was developing his colour theories in the \"Farbenlehre\" of 1810, and introduced similar principles of combination and complementation, symbolising, for Goethe, \"the primordial relations which belong both to nature and vision\". Hegel in his \"Science of Logic\" accordingly asks us to see his system not as a tree but as a circle.\n\nCharles Sanders Peirce, who had read Kant and Hegel closely, and who also had some knowledge of Aristotle, proposed a system of merely three phenomenological categories: Firstness, Secondness, and Thirdness, which he repeatedly invoked in his subsequent writings. Like Hegel, C.S.Peirce attempted to develop a system of categories from a single indisputable principle, in Peirce's case the notion that in the first instance he could only be aware of his own ideas. \n\nAlthough Peirce's three categories correspond to the three concepts of relation given in Kant's tables, the sequence is now reversed and follows that given by Hegel, and indeed before Hegel of the three moments of the world-process given by Plotinus. Later, Peirce gave a mathematical reason for there being three categories in that although monadic, dyadic and triadic nodes are irreducible, every node of a higher valency is reducible to a \"compound of triadic relations\". Ferdinand de Saussure, who was developing \"semiology\" in France just as Peirce was developing \"semiotics\" in the US, likened each term of a proposition to \"the centre of a constellation, the point where other coordinate terms, the sum of which is indefinite, converge\".\n\nEdmund Husserl (1962, 2000) wrote extensively about categorial systems as part of his phenomenology.\n\nFor Gilbert Ryle (1949), a category (in particular a \"category mistake\") is an important semantic concept, but one having only loose affinities to an ontological category.\n\nContemporary systems of categories have been proposed by John G. Bennett (The Dramatic Universe, 4 vols., 1956–65), Wilfrid Sellars (1974), Reinhardt Grossmann (1983, 1992), Johansson (1989), Hoffman and Rosenkrantz (1994), Roderick Chisholm (1996), Barry Smith (ontologist) (2003), and Jonathan Lowe (2006).\n\n\n\n"}
{"id": "714069", "url": "https://en.wikipedia.org/wiki?curid=714069", "title": "Church–Turing–Deutsch principle", "text": "Church–Turing–Deutsch principle\n\nIn computer science and quantum physics, the Church–Turing–Deutsch principle (CTD principle) is a stronger, physical form of the Church–Turing thesis formulated by David Deutsch in 1985.\n\nThe principle states that a universal computing device can simulate every physical process.\n\nThe principle was stated by Deutsch in 1985 with respect to finitary machines and processes. He observed that classical physics, which makes use of the concept of real numbers, cannot be simulated by a Turing machine, which can only represent computable reals. Deutsch proposed that quantum computers may actually obey the CTD principle, assuming that the laws of quantum physics can completely describe every physical process.\n\nAn earlier version of this thesis for classical computers was stated by Alan Turing's friend and student Robin Gandy in 1980.\n\n\n"}
{"id": "7327", "url": "https://en.wikipedia.org/wiki?curid=7327", "title": "Copernican principle", "text": "Copernican principle\n\nIn physical cosmology, the Copernican principle is an alternative name for the principle of relativity, stating that humans, on the Earth or in the Solar system, are not privileged observers of the universe.\n\nNamed for Copernican heliocentrism, it is a working assumption that arises from a modified cosmological extension of Copernicus's argument of a moving Earth. In some sense, it is equivalent to the mediocrity principle.\n\nHermann Bondi named the principle after Copernicus in the mid-20th century, although the principle itself dates back to the 16th-17th century paradigm shift away from the Ptolemaic system, which placed Earth at the center of the universe. Copernicus proposed that the motion of the planets can be explained by reference to an assumption that the Sun and not Earth is centrally located and stationary. He argued that the apparent retrograde motion of the planets is an illusion caused by Earth's movement around the Sun, which the Copernican model placed at the centre of the universe. Copernicus himself was mainly motivated by technical dissatisfaction with the earlier system and not by support for any mediocrity principle. In fact, although the Copernican heliocentric model is often described as \"demoting\" Earth from its central role it had in the Ptolemaic geocentric model, it was successors to Copernicus, notably the 16th century Giordano Bruno who adopted this new perspective. The earth's central position had been interpreted as being in the \"lowest and filthiest parts\". Instead, as Galileo said, the earth is part of the \"dance of the stars\" rather than the \"sump where the universe's filth and ephemera collect\". In the late 20th Century, Carl Sagan asked, \"Who are we? We find that we live on an insignificant planet of a humdrum star lost in a galaxy tucked away in some forgotten corner of a universe in which there are far more galaxies than people.\"\n\nIn cosmology, if one assumes the Copernican principle and observes that the universe appears isotropic or the same in all directions from our vantage point on Earth, then one can infer that the universe is generally homogeneous or the same everywhere (at any given time) and is also isotropic about any given point. These two conditions make up the cosmological principle. In practice, astronomers observe that the universe has heterogeneous or non-uniform structures up to the scale of galactic superclusters, filaments and great voids. It becomes more and more homogeneous and isotropic when observed on larger and larger scales, with little detectable structure on scales of more than about 200 million parsecs. However, on scales comparable to the radius of the observable universe, we see systematic changes with distance from Earth. For instance, galaxies contain more young stars and are less clustered, and quasars appear more numerous. While this might suggest that Earth is at the center of the universe, the Copernican principle requires us to interpret it as evidence for the evolution of the universe with time: this distant light has taken most of the age of the universe to reach us and show us the universe when it was young. The most distant light of all, cosmic microwave background radiation, is isotropic to at least one part in a thousand.\n\nModern mathematical cosmology is based on the assumption that the Cosmological principle is almost, but not exactly, true on the largest scales. The Copernican principle represents the irreducible philosophical assumption needed to justify this, when combined with the observations.\n\nMichael Rowan-Robinson emphasizes the Copernican principle as the threshold test for modern thought, asserting that: \"It is evident that in the post-Copernican era of human history, no well-informed and rational person can imagine that the Earth occupies a unique position in the universe.\"\n\nBondi and Thomas Gold used the Copernican principle to argue for the perfect cosmological principle which maintains that the universe is also homogeneous in time, and is the basis for the steady-state cosmology. However, this strongly conflicts with the evidence for cosmological evolution mentioned earlier: the universe has progressed from extremely different conditions at the Big Bang, and will continue to progress toward extremely different conditions, particularly under the rising influence of dark energy, apparently toward the Big Freeze or Big Rip.\n\nSince the 1990s the term has been used (interchangeably with \"the Copernicus method\") for J. Richard Gott's Bayesian-inference-based prediction of duration of ongoing events, a generalized version of the Doomsday argument.\n\nThe Copernican principle has never been proven, and in the most general sense cannot be proven, but it is implicit in many modern theories of physics. Cosmological models are often derived with reference to the Cosmological principle, slightly more general than the Copernican principle, and many tests of these models can be considered tests of the Copernican principle.\n\nBefore the term Copernican principle was even coined, Earth was repeatedly shown not to have any special location in the universe. The Copernican Revolution dethroned Earth to just one of many planets orbiting the Sun. Proper motion was mentioned by Halley. William Herschel found that the Solar System is moving through space within our disk-shaped Milky Way galaxy. Edwin Hubble showed that our galaxy is just one of many galaxies in the universe. Examination of our galaxy's position and motion in the universe led to the Big Bang theory and the whole of modern cosmology.\n\nRecent and planned tests relevant to the cosmological and Copernican principles include:\n\nThe standard model of cosmology, the Lambda-CDM model, assumes the Copernican principle and the more general Cosmological principle and observations are largely consistent but there are always unsolved problems. Some cosmologists and theoretical physicists design models lacking the Cosmological or Copernican principles, to constrain the valid values of observational results, to address specific known issues, and to propose tests to distinguish between current models and other possible models.\n\nA prominent example in this context is the observed accelerating universe and the cosmological constant issue. An alternative proposal to dark energy is that the universe is much more inhomogeneous than currently assumed, and specifically that we are in an extremely large low-density void. To match observations we would have to be very close to the centre of this void, immediately contradicting the Copernican principle.\n"}
{"id": "33548913", "url": "https://en.wikipedia.org/wiki?curid=33548913", "title": "Dehaene–Changeux model", "text": "Dehaene–Changeux model\n\nThe Dehaene–Changeux model (DCM), also known as the global neuronal workspace or the global cognitive workspace model is a part of Bernard Baars's \"global workspace model\" for consciousness.\n\nIt is a computer model of the neural correlates of consciousness programmed as a neural network. It attempts to reproduce the swarm behaviour of the brain's \"higher cognitive functions\" such as consciousness, decision-making and the central executive functions. It was developed by cognitive neuroscientists Stanislas Dehaene and Jean-Pierre Changeux beginning in 1986. It has been used to provide a predictive framework to the study of inattentional blindness and the solving of the Tower of London test.\n\nThe Dehaene–Changeux model was initially established as a spin glass neural network attempting to represent learning and to then provide a stepping stone towards artificial learning among other objectives. It would later be used to predict observable reaction times within the priming paradigm and in inattentional blindness.\n\nThe Dehaene–Changeux model is a meta neural network (i.e. a network of neural networks) composed of a very large number of integrate-and-fire neurons programmed in either a stochastic or deterministic way. The neurons are organised in complex thalamo-cortical columns with long-range connexions and a critical role played by the interaction between von Economo's areas. Each thalamo-cortical column is composed of pyramidal cells and inhibitory interneurons receiving a long-distance excitatory neuromodulation which could represent noradrenergic input.\n\nAmong others Cohen & Hudson (2002) had already used \"\"Meta neural networks as intelligent agents for diagnosis \" Similarly to Cohen & Hudson, Dehaene & Changeux have established their model as an interaction of meta-neural networks (thalamocortical columns) themselves programmed in the manner of a \"hierarchy of neural networks that together act as an intelligent agent\"\", in order to use them as a system composed of a large scale of inter-connected intelligent agents for predicting the self-organized behaviour of the neural correlates of consciousness. It may also be noted that Jain et al. (2002) had already clearly identified spiking neurons as intelligent agents since the lower bound for computational power of networks of spiking neurons is the capacity to simulate in real-time for boolean-valued inputs any Turing machine. The DCM being composed of a very large number of interacting sub-networks which are themselves intelligent agents, it is formally a Multi-agent system programmed as a Swarm or neural networks and \"a fortiori\" of spiking neurons.\n\nThe DCM exhibits several surcritical emergent behaviors such as multistability and a Hopf bifurcation between two very different regimes which may represent either sleep or arousal with a various all-or-none behaviors which Dehaene et al. use to determine a testable taxonomy between different states of consciousness. \n\nThe Dehaene-Changeux Model contributed to the study of nonlinearity and self-organized criticality in particular as an explanatory model of the brain's emergent behaviors, including consciousness. Studying the brain's phase-locking and large-scale synchronization, Kitzbichler et al. (2011a) confirmed that criticality is a property of human brain functional network organization at all frequency intervals in the brain's physiological bandwidth.\n\nFurthermore, exploring the neural dynamics of cognitive efforts after, \"inter alia\", the Dehaene-Changeux Model, Kitzbichler et al. (2011b) demonstrated how cognitive effort breaks the modularity of mind to make human brain functional networks transiently adopt a more efficient but less economical configuration. Werner (2007a) used the Dehaene-Changeux Global Neuronal Workspace to defend the use of statistical physics approaches for exploring phase transitions, scaling and universality properties of the so-called \"Dynamic Core\" of the brain, with relevance to the macroscopic electrical activity in EEG and EMG. Furthermore, building from the Dehaene-Changeux Model, Werner (2007b) proposed that the application of the twin concepts of scaling and universality of the theory of non-equilibrium phase transitions can serve as an informative approach for elucidating the nature of underlying neural-mechanisms, with emphasis on the dynamics of recursively reentrant activity flow in intracortical and cortico-subcortical neuronal loops. Friston (2000) also claimed that \"the nonlinear nature of asynchronous coupling enables the rich, context-sensitive interactions that characterize real brain dynamics, suggesting that it plays a role in functional integration that may be as important as synchronous interactions\".\n\nIt contributed to the study of phase transition in the brain under sedation, and notably GABA-ergic sedation such as that induced by propofol (Murphy et al. 2011, Stamatakis et al. 2010). The Dehaene-Changeux Model was contrasted and cited in the study of collective consciousness and its pathologies (Wallace et al. 2007). Boly et al. (2007) used the model for a reverse somatotopic study, demonstrating a correlation between baseline brain activity and somatosensory perception in humans. Boly et al. (2008) also used the DCM in a study of the baseline state of consciousness of the human brain's default network.\n\n\n"}
{"id": "4110028", "url": "https://en.wikipedia.org/wiki?curid=4110028", "title": "Distinction (philosophy)", "text": "Distinction (philosophy)\n\nDistinction, the fundamental philosophical abstraction, involves the recognition of difference.\n\nIn classical philosophy, there were various ways in which things could be distinguished. The merely logical or virtual distinction, such as the difference between concavity and convexity, involves the mental apprehension of two definitions, but which cannot be realized outside the mind, as any concave line would be a convex line considered from another perspective. A real distinction involves a level of ontological separation, as when squirrels are distinguished from llamas (for no squirrel is a llama, and no llama is a squirrel). A real distinction is thus different than a merely conceptual one, in that in a real distinction, one of the terms can be realized in reality without the other being realized.\n\nLater developments include Duns Scotus's formal distinction, which developed in part out of the recognition in previous authors that there need to be an intermediary between logical and real distinctions.\n\nSome relevant distinctions to the history of Western philosophy include:\n\nWhile the there are anticipation of this distinction prior to Kant in the British Empiricists (and even further in Scholastic thought), it was Kant who introduced the terminology. The distinction concerns the relation of a subject to its predicate: analytic claims are those in which the subject contains the predicate, as in \"All bodies are extended.\" Synthetic claims bring two concepts together, as in \"All events are caused.\" The distinction was recently called into question by W.V.O. Quine, in his paper \"Two Dogmas of Empiricism.\"\n\nThe origins of the distinction are less clear, and it concerns the origins of knowledge. \"A posteriori\" knowledge arises from, or is caused by, experience. \"A priori\" knowledge may come temporally after experience, but its certainty is not derivable from the experience itself. Saul Kripke was the first major think to propose that there are analytic \"a posteriori\" knowledge claims.\n\nAristotle makes the distinction between actuality and potentiality. Actuality is a realization of the way a thing could be, while potency refers simply to the way a thing could be. There are two levels to each: matter itself can be anything, and becomes something actually by causes, making it something which then has the ability to be in a certain way, and that ability can then be realized. The matter of an ax can be an ax, then is made into an ax. The ax thereby is able to cut, and reaches a new form of actuality in actually cutting.\n\nThe major distinction Aquinas makes is that of essence and existence. It is a distinction already in Avicenna, but Aquinas maps the distinction onto the actuality/potentiality distinction of Aristotle, such that the essence of a thing is in potency to the existence of a thing, which is that thing's actuality.\n\nIn Kant, the distinction between appearance and thing-in-itself is foundational to his entire philosophical project. The distinction separates the way a thing appears to us on the one hand, and the way a thing really is.\n"}
{"id": "21345091", "url": "https://en.wikipedia.org/wiki?curid=21345091", "title": "Explanatory model", "text": "Explanatory model\n\nAn explanatory model is a useful description of why and how a thing works or an explanation of why a phenomenon is the way it is. The explanatory model is used as a substitute for \"the full explanation\" of the thing in question: \n\nExplanatory models do not claim to be a complete description/explanation of the absolute about the thing/phenomenon, nor do they even claim to, necessarily, be fully accurate. The description/explanation does, however, need to fit well enough to a sufficient portion of all the knowledge, observations and theoretical circumstances known about the thing/phenomenon, so that the explanatory model becomes useful.\nThat is: the description/explanation in an explanatory model, should be useful/helpful when one is about to make a decision or choice or when trying to successfully understand, explain or in some other way relate to the reality of the world around .Tewodros kassa\n\nAs most, if not all, explanations of anything, to a certain degree depend on axioms, and thereby are incomplete and not really \"the \"full\" explanation\", then, strictly speaking, all explanations are in fact explanatory models.<br>\nYet, the term \"explanatory model\" generally is used only when one feels the need to \"emphasize awareness of\" the incompleteness of an explanation (due to intentional simplification or due to lack of knowledge and understanding).\n\nBy being mindful of the difference between on the one hand: \"absolute reality\" and on the other hand: \"the explanatory models that one has become accustomed to\", then one will be better equipped to avoid erroneously rejecting important new knowledge, even when this new knowledge seem to clearly contradict that which one \"knows\" from before.\n\n"}
{"id": "928779", "url": "https://en.wikipedia.org/wiki?curid=928779", "title": "First principle", "text": "First principle\n\nA first principle is a basic, foundational, self-evident proposition or assumption that cannot be deduced from any other proposition or assumption. In philosophy, first principles are taught by Aristotelians, and nuanced versions of first principles are referred to as postulates by Kantians. In mathematics, first principles are referred to as axioms or postulates. In physics and other sciences, theoretical work is said to be from first principles, or \"ab initio\", if it starts directly at the level of established science and does not make assumptions such as empirical model and parameter fitting.\n\nIn a formal logical system, that is, a set of propositions that are consistent with one another, it is possible that some of the statements can be deduced from other statements. For example, in the syllogism, \"All men are mortal; Socrates is a man; Socrates is mortal\" the last claim can be deduced from the first two.\n\nA first principle is an axiom that cannot be deduced from any other within that system. The classic example is that of Euclid's Elements; its hundreds of geometric propositions can be deduced from a set of definitions, postulates, and common notions: all three types constitute first principles.\n\nIn philosophy \"first principles\" are also commonly referred to as \"a priori\" terms and arguments, which are contrasted to \"a posteriori\" terms, reasoning or arguments, in that the former are simply assumed and exist prior to the reasoning process and the latter are deduced or inferred after the initial reasoning process. First principles are generally treated in the realm of philosophy known as epistemology, but are an important factor in any metaphysical speculation. \n\nIn philosophy \"first principles\" are often somewhat synonymous with \"a priori\", datum and axiomatic reasoning.\n\nTerence Irwin writes:\nProfoundly influenced by Euclid, Descartes was a rationalist who invented the foundationalist system of philosophy. He used the \"method of doubt\", now called Cartesian doubt, to systematically doubt everything he could possibly doubt, until he was left with what he saw as purely indubitable truths. Using these self-evident propositions as his axioms, or foundations, he went on to deduce his entire body of knowledge from them. The foundations are also called \"a priori\" truths. His most famous proposition is \"Je pense, donc je suis.\" (\"I think, therefore I am\", or \"Cogito ergo sum\")\n\nDescartes describes the concept of a first principle in the following excerpt from the preface to the \"Principles of Philosophy\" (1644):\nIn physics, a calculation is said to be \"from first principles\", or \"ab initio\", if it starts directly at the level of established laws of physics and does not make assumptions such as empirical model and fitting parameters.\n\nFor example, calculation of electronic structure using Schrödinger's equation within a set of approximations that do not include fitting the model to experimental data is an \"ab initio\" approach.\n\n\n"}
{"id": "18562589", "url": "https://en.wikipedia.org/wiki?curid=18562589", "title": "Gossen's second law", "text": "Gossen's second law\n\nGossen's Second “Law”, named for Hermann Heinrich Gossen (1810–1858), is the assertion that an economic agent will allocate his or her expenditures such that the ratio of the marginal utility of each good or service to its price (the marginal expenditure necessary for its acquisition) is equal to that for every other good or service. Formally,\nwhere\n\nImagine that an agent has spent money on various sorts of goods or services. If the last unit of currency spent on goods or services of one sort bought a quantity with \"less\" marginal utility than that which would have been associated with the quantity of another sort that could have been bought with the money, then the agent would have been \"better off\" instead buying more of that other good or service. Assuming that goods and services are continuously divisible, the only way that it is possible that the marginal expenditure on one good or service should not yield more utility than the marginal expenditure on the other (or \"vice versa\") is if the marginal expenditures yield \"equal\" utility.\n\nAssume that utility, goods, and services have the requisite properties so that formula_7 is well defined for each good or service. An agent then optimizes\nsubject to a budget constraint\nwhere\nUsing the method of Lagrange multipliers, one constructs the function\nand finds the first-order conditions for optimization as\n(which simply implies that all of formula_10 will be spent) and\nso that\nwhich is algebraically equivalent to\nSince every such ratio is equal to formula_17, the ratios are all equal one to another:\n\n\n"}
{"id": "11273068", "url": "https://en.wikipedia.org/wiki?curid=11273068", "title": "Growing block universe", "text": "Growing block universe\n\nAccording to the growing block universe theory of time (or the growing block view), the past and present exist and the future does not exist. The present is an objective property, to be compared with a moving spotlight. By the passage of time more of the world comes into being; therefore, the block universe is said to be growing. The growth of the block is supposed to happen in the present, a very thin slice of spacetime, where more of spacetime is continually coming into being.\n\nThe growing block view is an alternative to both eternalism (according to which past, present, and future all exist) and presentism (according to which only the present exists). It is held to be closer to common-sense intuitions than the alternatives. C. D. Broad was a proponent of the theory (1923). Some modern defenders are Michael Tooley (in 1997) and Peter Forrest (in 2004).\n\nRecently several philosophers, David Braddon-Mitchell (2004), Craig Bourne and Trenton Merricks have noted that if the growing block view is correct then we have to conclude that we do not know whether now is now. (The first occurrence of \"now\" is an indexical and the second occurrence of \"now\" is the objective tensed property. Their observation implies the following sentence: \"This part of spacetime has the property of being present\".)\n\nTake Socrates discussing, in the past, with Gorgias, and at the same time thinking that the discussion is occurring now. According to the growing block view, tense is a real property of the world so his thought is about now, the objective present. He thinks, tenselessly, that his thought is occurring on the edge of being. But we know he is wrong because he is in the past; he does not know that now is now. But how can we be sure we are not in the same position? There is nothing special with Socrates. Therefore, we do not know whether now is now.\n\nHowever, some have argued that there is an ontological distinction between the past and the present. For instance, Forrest (2004) argues that although there exists a past, it is lifeless and inactive. Consciousness, as well as the flow of time, is not active within the past and can only occur at the boundary of the block universe in which the present exists.\n\n\n"}
{"id": "4852151", "url": "https://en.wikipedia.org/wiki?curid=4852151", "title": "Hamilton's principle", "text": "Hamilton's principle\n\nIn physics, Hamilton's principle is William Rowan Hamilton's formulation of the principle of stationary action (see that article for historical formulations). It states that the dynamics of a physical system is determined by a variational problem for a functional based on a single function, the Lagrangian, which contains all physical information concerning the system and the forces acting on it. The variational problem is equivalent to and allows for the derivation of the \"differential\" equations of motion of the physical system. Although formulated originally for classical mechanics, Hamilton's principle also applies to classical fields such as the electromagnetic and gravitational fields, and plays an important role in quantum mechanics, quantum field theory and criticality theories.\n\nHamilton's principle states that the true evolution q(\"t\") of a system described by \"N\" generalized coordinates q = (\"q\", \"q\", ..., \"q\") between two specified states q = q(\"t\") and q = q(\"t\") at two specified times \"t\" and \"t\" is a stationary point (a point where the variation is zero), of the action functional\n\nwhere formula_2 is the Lagrangian function for the system. In other words, any \"first-order\" perturbation of the true evolution results in (at most) \"second-order\" changes in formula_3. The action formula_3 is a functional, i.e., something that takes as its input a function and returns a single number, a scalar. In terms of functional analysis, Hamilton's principle states that the true evolution of a physical system is a solution of the functional equation\n\n</math>\n\nRequiring that the true trajectory q(\"t\") be a stationary point of the action functional formula_3 is equivalent to a set of differential equations for q(\"t\") (the Euler–Lagrange equations), which may be derived as follows.\n\nLet q(\"t\") represent the true evolution of the system between two specified states q = q(\"t\") and q = q(\"t\") at two specified times \"t\" and \"t\", and let ε(\"t\") be a small perturbation that is zero at the endpoints of the trajectory\n\nTo first order in the perturbation ε(\"t\"), the change in the action functional formula_7 would be\n\nwhere we have expanded the Lagrangian \"L\" to first order in the perturbation ε(\"t\").\n\nApplying integration by parts to the last term results in\n\nThe boundary conditions formula_6 causes the first term to vanish\n\nHamilton's principle requires that this first-order change formula_12 is zero for all possible perturbations ε(\"t\"), i.e., the true path is a stationary point of the action functional formula_3 (either a minimum, maximum or saddle point). This requirement can be satisfied if and only if\n\nThese equations are called the Euler–Lagrange equations for the variational problem.\n\nThe conjugate momentum \"p\" for a generalized coordinate \"q\" is defined by the equation\n\nAn important special case of the Euler–Lagrange equation occurs when \"L\" does not contain a generalized coordinate \"q\" explicitly,\n\nthat is, the conjugate momentum is a \"constant of the motion\".\n\nIn such cases, the coordinate \"q\" is called a cyclic coordinate. For example, if we use polar coordinates \"t, r, θ\" to describe the planar motion of a particle, and if \"L\" does not depend on \"θ\", the conjugate momentum is the conserved angular momentum.\n\nTrivial examples help to appreciate the use of the action principle via the Euler–Lagrange equations. A free particle (mass \"m\" and velocity \"v\") in Euclidean space moves in a straight line. Using the Euler–Lagrange equations, this can be shown in polar coordinates as follows. In the absence of a potential, the Lagrangian is simply equal to the kinetic energy \nin orthonormal (\"x\",\"y\") coordinates, where the dot represents differentiation with respect to the curve parameter (usually the time, \"t\"). Therefore, upon application of the Euler–Lagrange equations,\n\nAnd likewise for \"y\". Thus the Euler–Lagrange formulation can be used to derive Newton's laws.\n\nIn polar coordinates (\"r\", φ) the kinetic energy and hence the Lagrangian becomes\n\nThe radial \"r\" and \"φ\" components of the Euler–Lagrange equations become, respectively\n\nThe solution of these two equations is given by\n\nfor a set of constants \"a, b, c, d\" determined by initial conditions.\nThus, indeed, \"the solution is a straight line\" given in polar coordinates: \"a\" is the velocity, \"c\" is the distance of the closest approach to the origin, and \"d\" is the angle of motion.\n\nHamilton's principle is an important variational principle in elastodynamics. As opposed to a system composed of rigid bodies, deformable bodies have an infinite number of degrees of freedom and occupy continuous regions of space; consequently, the state of the system is described by using continuous functions of space and time. The extended Hamilton Principle for such bodies is given by\n\nwhere \"T\" is the kinetic energy, \"U\" is the elastic energy, \"W\" is the work done by\nexternal loads on the body, and \"t\", \"t\" the initial and final times. If the system is conservative, the work done by external forces may be derived from a scalar potential \"V\". In this case,\n\nThis is called Hamilton's principle and it is invariant under coordinate transformations.\n\nHamilton's principle and Maupertuis' principle are occasionally confused and both have been called (incorrectly) the principle of least action. They differ in three important ways: \n\n\n\nThe action principle can be extended to obtain the equations of motion for fields, such as the electromagnetic field or gravity.\n\nThe Einstein equation utilizes the \"Einstein–Hilbert action\" as constrained by a variational principle.\n\nThe path of a body in a gravitational field (i.e. free fall in space time, a so-called geodesic) can be found using the action principle.\n\nIn quantum mechanics, the system does not follow a single path whose action is stationary, but the behavior of the system depends on all imaginable paths and the value of their action. The action corresponding to the various paths is used to calculate the path integral, that gives the probability amplitudes of the various outcomes.\n\nAlthough equivalent in classical mechanics with Newton's laws, the action principle is better suited for generalizations and plays an important role in modern physics. Indeed, this principle is one of the great generalizations in physical science. In particular, it is fully appreciated and best understood within quantum mechanics. Richard Feynman's path integral formulation of quantum mechanics is based on a stationary-action principle, using path integrals. Maxwell's equations can be derived as conditions of stationary action.\n\n\n"}
{"id": "369116", "url": "https://en.wikipedia.org/wiki?curid=369116", "title": "Hume's fork", "text": "Hume's fork\n\nHume's fork is an explanation, developed by later philosophers, of David Hume's 1730s division of \"relations of ideas\" from \"matters of fact and real existence\". A distinction is made between necessary versus contingent (concerning reality), versus (concerning knowledge), and analytic versus synthetic (concerning language). Relations of abstract ideas align on one side (necessary, \"a priori\", analytic), whereas concrete truths align on the other (contingent, \"a posteriori\", synthetic).\n\nThe \"necessary\" is generally true in all possible worlds—usually by mere logical validity—whereas the \"contingent\" hinges on the way the real world is. The \"a priori\" is knowable before or without, whereas the \"a posteriori\" is knowable only after or through, experience in an area of interest. The \"analytic\" is a statement true by virtue of its terms' meanings, and therefore a tautology—necessarily true but uninformative—whereas the \"synthetic\" is true by its terms' meanings in relation to a state of facts. In other words, analytic propositions are true by virtue of their meaning, while synthetic propositions are true by how their meaning relates to the world. Philosophers have used the terms differently, and there is debate over whether there is a legitimate distinction. \n\nHume's strong empiricism, as in Hume's fork as well as Hume's problem of induction, was taken as a threat to Newton's theory of motion. Immanuel Kant responded with rationalism in his 1781 \"Critique of Pure Reason\", where Kant attributed to the mind a causal role in sensory experience by the mind's aligning the environmental input by arranging those sense data into the experience of space and time. Kant thus reasoned existence of the synthetic \"a priori\"—combining meanings of terms with states of facts, yet known true without experience of the particular instance—replacing the two prongs of Hume's fork with a three-pronged-fork thesis (Kant's pitchfork) and thus saving Newton's law of universal gravitation.\n\nIn 1919, Newton's theory fell to Einstein's general theory of relativity. In the late 1920s, the logical positivists rejected Kant's synthetic \"a priori\" and asserted Hume's fork, so called, while hinging it at language—the analytic/synthetic division—while presuming that by holding to analyticity, they could develop a logical syntax entailing, as a consequence of Hume's fork, both necessity and aprioricity, thus restricting science to claims verifiable as either false or true. In the early 1950s, Willard Van Orman Quine undermined the analytic/synthetic division by explicating ontological relativity, as every term in any statement has its meaning contingent on a vast network of knowledge and belief, the speaker's conception of the entire world. By the early 1970s, Saul Kripke established the necessary \"a posteriori\", since if the Morning Star and the Evening Star are the same star, they are the same star by necessity, but this is known true by a human only through relevant experience.\n\nHume's fork remains basic in Anglo-American philosophy. Many deceptions and confusions are foisted by surreptitious or unwitting conversion of a synthetic claim to an analytic claim, rendered true by necessity but merely a tautology, for instance the \"No true Scotsman\" move. Simply put, Hume's fork has limitations. Related concerns are Hume's distinction of demonstrative versus probable reasoning and Hume's law. Hume makes other, important two-category distinctions, such as beliefs versus desires and as impressions versus ideas.\n\nThe first distinction is between two different areas of human study:\n\nHume's fork is often stated in such a way that statements are divided up into two types:\n\n\nIn modern terminology, members of the first group are known as analytic propositions and members of the latter as synthetic propositions. This terminology comes from Kant (Introduction to \"Critique of Pure Reason\", Section IV).\n\nInto the first class fall statements such as \"all bodies are extended\", \"all bachelors are unmarried\", and truths of mathematics and logic. Into the second class fall statements like \"the sun rises in the morning\", and \"all bodies have mass\".\n\nHume wants to prove that certainty does not exist in science. First, Hume notes that statements of the second type can never be entirely certain, due to the fallibility of our senses, the possibility of deception (see e.g. the modern brain in a vat theory) and other arguments made by philosophical skeptics. It is always logically possible that any given statement about the world is false.\n\nSecond, Hume claims that our belief in cause-and-effect relationships between events is not grounded on reason, but rather arises merely by habit or custom. Suppose one states: \"Whenever someone on earth lets go of a stone it falls.\" While we can grant that in every instance thus far when a rock was dropped on Earth it went down, this does not make it logically necessary that in the future rocks will fall when in the same circumstances. Things of this nature rely upon the future conforming to the same principles which governed the past. But that isn't something that we can know based on past experience—all past experience could tell us is that in the past, the future has resembled the past.\n\nThird, Hume notes that relations of ideas can be used only to prove other relations of ideas, and mean nothing outside of the context of how they relate to each other, and therefore tell us nothing about the world. Take the statement \"An equilateral triangle has three sides of equal length.\" While some earlier philosophers (most notably Plato and Descartes) held that logical statements such as these contained the most formal reality, since they are always true and unchanging, Hume held that, while true, they contain no formal reality, because the truth of the statements rests on the definitions of the words involved, and not on actual things in the world, since there is no such thing as a true triangle or exact equality of length in the world. So for this reason, relations of ideas cannot be used to prove matters of fact.\n\nThe results claimed by Hume as consequences of his fork are drastic. According to him, relations of ideas can be proved with certainty (by using other relations of ideas), however, they don't really mean anything about the world. Since they don't mean anything about the world, relations of ideas cannot be used to prove matters of fact. Because of this, matters of fact have no certainty and therefore cannot be used to prove anything. Only certain things can be used to prove other things for certain, but only things about the world can be used to prove other things about the world. But since we can't cross the fork, nothing is both certain and about the world, only one or the other, and so it is impossible to prove something about the world with certainty.\n\nIf accepted, Hume's fork makes it pointless to try to prove the existence of God (for example) as a matter of fact. If God is not literally made up of physical matter, and does not have an observable effect on the world, making a statement about God is not a matter of fact. Therefore, a statement about God must be a relation of ideas. In this case if we prove the statement \"God exists,\" it doesn't really tell us anything about the world; it is just playing with words. It is easy to see how Hume's fork voids the causal argument and the ontological argument for the existence of a non-observable God. However, this does not mean that the validity of Hume's fork would imply that God definitely does not exist, only that it would imply that the existence of God cannot be proven as a matter of fact without worldly evidence. \n\nHume rejected the idea of any meaningful statement that did not fall into this schema, saying:\nIf we take in our hand any volume; of divinity or school metaphysics, for instance; let us ask, Does it contain any abstract reasoning concerning quantity or number? No. Does it contain any experimental reasoning concerning matter of fact and existence? No. Commit it then to the flames: for it can contain nothing but sophistry and illusion. — \"An Enquiry Concerning Human Understanding\"\n"}
{"id": "30679665", "url": "https://en.wikipedia.org/wiki?curid=30679665", "title": "Inherent bad faith model", "text": "Inherent bad faith model\n\nThe inherent bad faith model of information processing is a theory in political psychology that was first put forth by Ole Holsti to explain the relationship between John Foster Dulles' beliefs and his model of information processing. \n\nIt is the most widely studied model of one's opponent. A state is presumed to be implacably hostile, and contra-indicators of this are ignored. They are dismissed as propaganda ploys or signs of weakness. An example is John Foster Dulles' position regarding the Soviet Union.\n\n"}
{"id": "2892645", "url": "https://en.wikipedia.org/wiki?curid=2892645", "title": "Instantiation principle", "text": "Instantiation principle\n\nThe instantiation principle or principle of instantiation or principle of exemplification is the concept in metaphysics and logic (first put forward by David Malet Armstrong) that there can be no uninstantiated or unexemplified properties (or universals). In other words, it is impossible for a property to exist which is not had by some object.\n\nConsider a chair. Presumably chairs did not exist 150,000 years ago. Thus, according to the principle of instantiation, the property of being a chair did not exist 150,000 years ago either. Similarly, if all red objects were to suddenly go out of existence, then the property of being red would likewise go out of existence.\n\nTo make the principle more plausible in the light of these examples, the existence of properties or universals is not tied to their actual existence now, but to their existence in space-time considered as a whole. Thus, any property which \"is\", \"has been\", or \"will be\" instantiated exists. The property of being red would exist even if all red things were to be destroyed, because it has been instantiated. This broadens the range of properties which exist if the principle is true. \n\nThose who endorse the principle of instantiation are known as \"in re\" (in thing or in reality) realists or 'immanent realists'.\n\n"}
{"id": "29594530", "url": "https://en.wikipedia.org/wiki?curid=29594530", "title": "Logical hexagon", "text": "Logical hexagon\n\nThe logical hexagon (also called the hexagon of opposition) is a conceptual model of the relationships between the truth values of six statements. It is an extension of Aristotle's square of opposition. It was discovered independently by both Augustin Sesmat and Robert Blanché.\n\nThis extension consists in introducing two statements U and Y. Whereas U is the disjunction of A and E, Y is the conjunction of the two traditional particulars I and O.\n\nThe traditional square of opposition demonstrates two sets of contradictories A and O, and E and I (i.e. they cannot both be true and cannot both be false), two contraries A and E (i.e. they can both be false, but cannot both be true), and two subcontraries I and O (i.e. they can both be true, but cannot both be false) according to Aristotle’s definitions. However, the logical hexagon provides that U and Y are also contradictory.\n\nThe logical hexagon may be interpreted in various ways, including as a model of traditional logic, quantifications, modal logic, order theory, or paraconsistent logic.\n\nFor instance, the statement A may be interpreted as \"Whatever x may be, if x is a man, then x is white.\"\n\nThe statement E may be interpreted as \"Whatever x may be, if x is a man, then x is non-white.\"\n\nThe statement I may be interpreted as \"There exists at least one x that is both a man and white.\"\n\nThe statement O may be interpreted as \"There exists at least one x that is both a man and non-white\"\n\nThe statement Y may be interpreted as \"There exists at least one x that is both a man and white and there exists at least one x that is both a man and non-white\"\n\nThe statement U may be interpreted as \"One of two things, either whatever x may be, if x is a man, then x is white or whatever x may be, if x is a man, then x is non-white.\"\n\nThe logical hexagon may be interpreted in various ways, including as a model of traditional logic, quantifications, modal logic, order theory, or paraconsistent logic.\n\nThe logical hexagon may be interpreted as a model of modal logic such that\n\n\nIt has been proven that both the square and the hexagon, followed by a “logical cube”, belong to a regular series of n-dimensional objects called “logical bi-simplexes of dimension n.” The pattern also goes even beyond this.\n\n\n"}
{"id": "1148564", "url": "https://en.wikipedia.org/wiki?curid=1148564", "title": "Marginal concepts", "text": "Marginal concepts\n\nIn economics, marginal concepts are associated with a \"specific change\" in the quantity used of a good or service, as opposed to some notion of the over-all significance of that class of good or service, or of some total quantity thereof.\n\nConstraints are conceptualized as a \"border\" or \"margin\". The location of the margin for any individual corresponds to his or her \"endowment\", broadly conceived to include opportunities. This endowment is determined by many things including physical laws (which constrain how forms of energy and matter may be transformed), accidents of nature (which determine the presence of natural resources), and the outcomes of past decisions made both by others and by the individual himself or herself.\n\nA value that holds true given particular constraints is a \"marginal\" value. A change that would be affected as or by a specific loosening or tightening of those constraints is a \"marginal\" change, as large as the smallest relevant division of that good or service. For reasons of tractability, it is often assumed in neoclassical analysis that goods and services are continuously divisible. In such context, a marginal change may be an infinitesimal change or a limit. However, strictly speaking, the smallest relevant division may be quite large.\n\nThe marginal use of a good or service is the specific use to which an agent would put a given increase, or the specific use of the good or service that would be abandoned in response to a given decrease.\n\nThe marginal utility of a good or service is the utility of the specific use to which an agent would put a given increase in that good or service, or of the specific use that would be abandoned in response to a given decrease. In other words, marginal utility is the utility of the marginal use.\n\nThe marginal rate of substitution is the rate of substitution is the least favorable rate, at the margin, at which an agent is willing to exchange units of one good or service for units of another.\n\nA marginal benefit is a benefit (howsoever ranked or measured) associated with a marginal change.\n\nThe term “marginal cost” may refer to an opportunity cost at the margin, or to marginal \"pecuniary\" cost — that is to say marginal cost measured by forgone money.\n\nOther marginal concepts include (but are not limited to):\n\nMarginalism is the use of marginal concepts to explain economic phenomena.\n\nThe related concept of elasticity is the ratio of the incremental percentage change in one variable with respect to an incremental percentage change in another variable.\n"}
{"id": "571001", "url": "https://en.wikipedia.org/wiki?curid=571001", "title": "Marginal propensity to consume", "text": "Marginal propensity to consume\n\nIn economics, the marginal propensity to consume (MPC) is a metric that quantifies induced consumption, the concept that the increase in personal consumer spending (consumption) occurs with an increase in disposable income (income after taxes and transfers). The proportion of disposable income which individuals spend on consumption is known as propensity to consume. MPC is the proportion of additional income that an individual consumes. For example, if a household earns one extra dollar of disposable income, and the marginal propensity to consume is 0.65, then of that dollar, the household will spend 65 cents and save 35 cents. Obviously, the household cannot spend \"more\" than the extra dollar (without borrowing).\n\nAccording to John Maynard Keynes, marginal propensity to consume is less than one.\n\nMathematically, the formula_1 function is expressed as the derivative of the consumption function formula_2 with respect to disposable income formula_3, i.e., the instantaneous slope of the formula_2-formula_3 curve.\n\nor, approximately,\n\nMarginal propensity to consume can be found by dividing change in consumption by a change in income, or formula_10. The MPC can be explained with the simple example:\nHere formula_11; formula_12\nTherefore, formula_13 or 83%.\nFor example, suppose you receive a bonus with your paycheck, and it's $500 on top of your normal annual earnings. You suddenly have $500 more in income than you did before. If you decide to spend $400 of this marginal increase in income on a new business suit, your marginal propensity to consume will be 0.8 (formula_14).\n\nThe marginal propensity to consume is measured as the ratio of the change in consumption to the change in income, thus giving us a figure between 0 and 1. The MPC can be more than one if the subject borrowed money or dissaved to finance expenditures higher than their income. The MPC can also be less than zero if an increase in income leads to a reduction in consumption (which might occur if, for example, the increase in income makes it worthwhile to save up for a particular purchase). One minus the MPC equals the marginal propensity to save (in a two sector closed economy), which is crucial to Keynesian economics and a key variable in determining the value of the multiplier.\n\nIn a standard Keynesian model, the MPC is less than the average propensity to consume (APC) because in the short-run some (autonomous) consumption does not change with income. Falls (increases) in income do not lead to reductions (increases) in consumption because people reduce (add to) savings to stabilize consumption. Over the long-run, as wealth and income rise, consumption also rises; the marginal propensity to consume out of long-run income is closer to the average propensity to consume.\n\nThe MPC is not strongly influenced by interest rates; consumption tends to be stable relative to income. In theory one might think that higher interest rates would induce more saving (the substitution effect) but higher interest rates also mean than people do not have to save as much for the future.\n\nEconomists often distinguish between the marginal propensity to consume out of permanent income, and the marginal propensity to consume out of temporary income, because if consumers expect a change in income to be permanent, then they have a greater incentive to increase their consumption. This implies that the Keynesian multiplier should be \"larger\" in response to permanent changes in income than it is in response to temporary changes in income (though the earliest Keynesian analyses ignored these subtleties). However, the distinction between permanent and temporary changes in income is often subtle in practice, and it is often quite difficult to designate a particular change in income as being permanent or temporary. What is more, the marginal propensity to consume should also be affected by factors such as the prevailing interest rate and the general level of consumer surplus that can be derived from purchasing.\n\nMPC's importance depends on the multiplier theory. MPC determines the value of the multiplier. The higher the MPC, the higher the multiplier and vice versa. The relationship between the multiplier and the propensity to consume is as follows:\n\nSince formula_18 is the MPC, the multiplier formula_25 is, by definition, equal to formula_29. The multiplier can also be derived from MPS (marginal propensity to save) and it is the reciprocal of MPS, formula_30\n\nThe above table shows that the size of the multiplier varies directly with the MPC and inversely with the MPS. Since the MPC is always greater than zero and less than one (i.e. formula_31), the multiplier is always between one and infinity (formula_32). If the multiplier is one, it means that the whole increment of income is saved and nothing is spent because the MPC is zero. On the other hand, an infinite multiplier implies that MPC is equal one and the entire increment of income is spent on consumption. It will soon lead to full employment in the economy and then create a limitless inflationary spiral. But these are rare phenomenon. Therefore, the multiplier coefficient varies between one and infinity.\n\nWhen income increases, the MPC falls but more than the APC. Conversely, when income falls, the MPC rises and the APC also rises but at a slower rate than the former. Such changes are only possible during cyclical fluctuations whereas in the short-run there is no change in the MPC and formula_33.\nKeynes is concerned primarily with the MPC, for his analysis pertains to the short-run while the APC is useful in the long-run analysis. The post-Keynesian economists have come to the conclusion that over the long-run APC and MPC are equal and approximate 0.9. In the Keynesian analysis the MPC is given more prominence. Its value is assumed to be positive and less than unity which means that when income increases the whole of it is not spent on consumption. On the contrary, when income falls, consumption expenditure does not decline in the same proportion and never becomes zero. The Keynesian hypothesis is that the marginal propensity to consume is positive but less than unity (formula_34) is of great analytical and practical significance. Besides telling us that consumption is an increasing function of income and it increases by less than the increment of income, this hypothesis helps in explaining\n1) The theoretical possibility of general overproduction or \"underemployment equilibrium\" and also\n2) The relative stability of a highly developed industrial economy. For it implies that the gap between income and consumption at all high levels of income is too wide to be easily filled by investment with the possible consequences that the economy may fluctuate around underemployment equilibrium.\nThus the economic significance of the MPC lies in filling the gap between income and consumption through planned investment to maintain the desired level of income.\n\nThe MPC is higher in the case of poorer people than in rich. When a person earns a higher income, the cost of their basic human needs amount to a smaller fraction of this income, and correspondingly their average propensity to save is higher than that of a person with a lower income. The marginal propensity to save of the richer classes is greater than that of the poorer classes. If, at any time, it is desired to increase aggregate consumption, then the purchasing power should be transferred from the richer classes (with low propensity to consume) to the poorer classes (with a higher propensity to consume). Likewise, if it is desired to reduce community consumption, the purchasing power must be taken away from the poorer classes by taxing consumption. The marginal propensity to consume is higher in a poor country and lower in the case of rich country. The reason is same as stated above. In the case of rich country, most common of the basic needs of the people have already been satisfied, and all the additional increments of income are saved, resulting in a higher marginal propensity to save but in a lower marginal propensity to consume. In a poor country, on the other hand, most of the basic needs of the people remain unsatisfied so that additional increments of income go to increase consumption, resulting in a higher marginal propensity to consume and a lower marginal propensity to save. This is the reason MPC is higher in the underdeveloped countries of Asia and Africa, and lower in developed countries such as the United States, the United Kingdom, Singapore and Germany.\n\nMuch of the current discussion seems to rely on the MPC being unique to a country, and homogeneous across such an economic entity; and the theory and the mathematical formulae apply to this use of the term. However, individuals have an MPC, and furthermore MPC is not homogeneous across society. Even if it was, the nature of the consumption is not homogeneous. Some consumption may be seen as more benevolent (to the economy) than others. Therefore, spending could be targeted where it would do most benefit, and thus generate the highest (closest to 1) MPC. This has traditionally been regarded as construction or other major projects (which also bring a direct benefit in the form of the finished product).\nClearly, some sectors of society are likely to have a much higher MPC than others. Someone with above average wealth or income or both may have a very low (short-term, at least) MPC of nearly zero—saving most of any extra income. But a pensioner, for example, will have an MPC of 1 or even greater than 1. This is because a pensioner is quite likely to spend every penny of any extra income. Further, if the extra income is seen as regular extra income, and guaranteed into the future, the pensioner may actually spend MORE than the extra £1. This would occur where the extra income stream gives confidence that the individual does not need to put aside as much in the form of savings; or perhaps can even dip into existing savings.\nMore importantly, this consumption is much more likely to occur in local small business—local shops, pubs and other leisure activities for example. These types of businesses are themselves likely to have a high MPC, and again the nature of their consumption is likely to be in the same, or next tier of businesses, and also of a benevolent nature.\nOther individuals with a high, and benevolent, MPC would include almost anyone on a low income—students, parents with young children, and the unemployed.\n\n"}
{"id": "4718632", "url": "https://en.wikipedia.org/wiki?curid=4718632", "title": "Mental representation", "text": "Mental representation\n\nA mental representation (or cognitive representation), in philosophy of mind, cognitive psychology, neuroscience, and cognitive science, is a hypothetical internal cognitive symbol that represents external reality, or else a mental process that makes use of such a symbol: \"a formal system for making explicit certain entities or types of information, together with a specification of how the system does this\".\n\nMental representation is the mental imagery of things that are not actually present to the senses. In contemporary philosophy, specifically in fields of metaphysics such as philosophy of mind and ontology, a mental representation is one of the prevailing ways of explaining and describing the nature of ideas and concepts.\n\nMental representations (or mental imagery) enable representing things that have never been experienced as well as things that do not exist. Think of yourself traveling to a place you have never visited before, or having a third arm. These things have either never happened or are impossible and do not exist, yet our brain and mental imagery allows us to imagine them. Although visual imagery is more likely to be recalled, mental imagery may involve representations in any of the sensory modalities, such as hearing, smell, or taste. Stephen Kosslyn proposes that images are used to help solve certain types of problems. We are able to visualize the objects in question and mentally represent the images to solve it.\n\nMental representations also allow people to experience things right in front of them—though the process of how the brain interprets the representational content is debated.\n\nRepresentationalism (also known as indirect realism) is the view that representations are the main way we access external reality. Another major prevailing philosophical theory posits that concepts are entirely abstract objects.\n\nThe representational theory of mind attempts to explain the nature of ideas, concepts and other mental content in contemporary philosophy of mind, cognitive science and experimental psychology. In contrast to theories of naive or direct realism, the representational theory of mind postulates the actual existence of mental representations which act as intermediaries between the observing subject and the objects, processes or other entities observed in the external world. These intermediaries stand for or represent to the mind the objects of that world.\n\nFor example, when someone arrives at the belief that his or her floor needs sweeping, the representational theory of mind states that he or she forms a mental representation that represents the floor and its state of cleanliness.\n\nThe original or \"classical\" representational theory probably can be traced back to Thomas Hobbes and was a dominant theme in classical empiricism in general. According to this version of the theory, the mental representations were images (often called \"ideas\") of the objects or states of affairs represented. For modern adherents, such as Jerry Fodor, Steven Pinker and many others, the representational system consists rather of an internal language of thought (i.e., mentalese). The contents of thoughts are represented in symbolic structures (the formulas of Mentalese) which, analogously to natural languages but on a much more abstract level, possess a syntax and semantics very much like those of natural languages. For the Spanish logician and cognitive scientist Luis M. Augusto, at this abstract, formal level, the syntax of thought is the set of symbol rules (i.e., operations, processes, etc. on and with symbol structures) and the semantics of thought is the set of symbol structures (concepts and propositions). Content (i.e., thought) emerges from the meaningful co-occurrence of both sets of symbols. For instance, \"8 x 9\" is a meaningful co-occurrence, whereas \"CAT x §\" is not; \"x\" is a symbol rule called for by symbol structures such as \"8\" and \"9\", but not by \"CAT\" and \"§\".\n\nThere are two types of representationalism, strong and weak. Strong representationalism attempts to reduce phenomenal character to intentional content. On the other hand, weak representationalism claims only that phenomenal character supervenes on intentional content. Strong representationalism aims to provide a theory about the nature of phenomenal character, and offers a solution to the hard problem of consciousness. In contrast to this, weak representationalism does not aim to provide a theory of consciousness, nor does it offer a solution to the hard problem of consciousness.\n\nStrong representationalism can be further broken down into restricted and unrestricted versions. The restricted version deals only with certain kinds of phenomenal states e.g. visual perception. Most representationalists endorse an unrestricted version of representationalism. According to the unrestricted version, for any state with phenomenal character that state’s phenomenal character reduces to its intentional content. Only this unrestricted version of representationalism is able to provide a general theory about the nature of phenomenal character, as well as offer a potential solution to the hard problem of consciousness. The successful reduction of the phenomenal character of a state to its intentional content would provide a solution to the hard problem of consciousness once a physicalist account of intentionality is worked out.\n\nWhen arguing against the unrestricted version of representationalism people will often bring up phenomenal mental states that appear to lack intentional content. The unrestricted version seeks to account for all phenomenal states. Thus, for it to be true, all states with phenomenal character must have intentional content to which that character is reduced. Phenomenal states without intentional content therefore serve as a counterexample to the unrestricted version. If the state has no intentional content its phenomenal character will not be reducible to that state’s intentional content, for it has none to begin with.\n\nA common example of this kind of state are moods. Moods are states with phenomenal character that are generally thought to not be directed at anything in particular. Moods are thought to lack directedness, unlike emotions, which are typically thought to be directed at particular things e.g. you are mad \"at\" your sibling, you are afraid \"of\" a dangerous animal. People conclude that because moods are undirected they are also nonintentional i.e. they lack intentionality or aboutness. Because they are not directed at anything they are not about anything. Because they lack intentionality they will lack any intentional content. Lacking intentional content their phenomenal character will not be reducible to intentional content, refuting the representational doctrine.\n\nThough emotions are typically considered as having directedness and intentionality this idea has also been called into question. One might point to emotions a person all of a sudden experiences that do not appear to be directed at or about anything in particular. Emotions elicited by listening to music are another potential example of undirected, nonintentional emotions. Emotions aroused in this way do not seem to necessarily be about anything, including the music that arouses them.\n\nIn response to this objection a proponent of representationalism might reject the undirected nonintentionality of moods, and attempt to identify some intentional content they might plausibly be thought to possess. The proponent of representationalism might also reject the narrow conception of intentionality as being directed at a particular thing, arguing instead for a broader kind of intentionality.\n\nThere are three alternative kinds of directedness/intentionality one might posit for moods. \nIn the case of outward directedness moods might be directed at either the world as a whole, a changing series of objects in the world, or unbound emotion properties projected by people onto things in the world. In the case of inward directedness moods are directed at the overall state of a person’s body. In the case of hybrid directedness moods are directed at some combination of inward and outward things.\n\nEven if one can identify some possible intentional content for moods we might still question whether that content is able to sufficiently capture the phenomenal character of the mood states they are a part of. Amy Kind contends that in the case of all the previously mentioned kinds of directedness (outward, inward, and hybrid) the intentional content supplied to the mood state is not capable of sufficiently capturing the phenomenal aspects of the mood states. In the case of inward directedness, the phenomenology of the mood does not seem tied to the state of one’s body, and even if one’s mood is reflected by the overall state of one’s body that person will not necessarily be aware of it, demonstrating the insufficiency of the intentional content to adequately capture the phenomenal aspects of the mood. In the case of outward directedness, the phenomenology of the mood and its intentional content do not seem to share the corresponding relation they should given that the phenomenal character is supposed to reduce to the intentional content. Hybrid directedness, if it can even get off the ground, faces the same objection.\n\nThere is a wide debate on what kinds of representations exist. There are several philosophers who bring about different aspects of the debate. Such philosophers include Alex Morgan, Gualtiero Piccinini, and Uriah Kriegel—though this is not an exhaustive list.\n\nThere are \"job description\" representations. That is representations that (1) represent something—have intentionality, (2) have a special relation—the represented object does not need to exist, and (3) content plays a causal role in what gets represented: e.g. saying \"hello\" to a friend, giving a glare to an enemy.\n\nStructural representations are also important. These types of representations are basically mental maps that we have in our minds that correspond exactly to those objects in the world (the intentional content). According to Morgan, structural representations are not the same as mental representations—there is nothing mental about them: plants can have structural representations.\n\nThere are also internal representations. These types of representations include those that involve future decisions, episodic memories, or any type of projection into the future.\n\nIn Gualtiero Piccinini's forthcoming work, he discusses topics on natural and nonnatural mental representations. He relies on the natural definition of mental representations given by Grice (1957) where \"P entails that P\". e.g. Those spots mean measles, entails that the patient has measles. Then there are nonnatural representations: \"P does not entail P\". e.g. The 3 rings on the bell of a bus mean the bus is full—the rings on the bell are independent of the fullness of the bus—we could have assigned something else (just as arbitrary) to signify that the bus is full.\n\nThere are also objective and subjective mental representations. Objective representations are closest to tracking theories—where the brain simply tracks what is in the environment. If there is a blue bird outside my window, the objective representation is that of the blue bird. Subjective representations can vary person-to-person. For example, if I am colorblind, that blue bird outside my window will not \"appear\" blue to me since I cannot represent the blueness of blue (i.e. I cannot see the color blue). The relationship between these two types of representation can vary.\n\nEliminativists think that subjective representations don't exist. Reductivists think subjective representations are reducible to objective. Non-reductivists think that subjective representations are real and distinct.\n\n\n"}
{"id": "348044", "url": "https://en.wikipedia.org/wiki?curid=348044", "title": "Meta-system", "text": "Meta-system\n\nMeta-systems have several definitions. In general, they link the concepts \"system\" and \"meta-\". A \"meta-system\" is about other systems, such as describing, generalizing, modelling, or analyzing the other system(s).\n\nAccording to Turchin and Joslyn (1997), this \"natural\" systemic definition is not sufficient for their Theory of Meta-system Transition, it also is not equivalent to the definition of \"system of systems\" in Autopoietic Systems Theory.\n\nIn economics, meta-systems are like what Bataille calls general economies as opposed to the restricted economies of systems.\n\nA book about the difference between general and restricted economies is \"Complementarity\" by Arshad Naim. In this case \"meta\" is defined as what is beyond: the meta-system is what lies beyond the system.\n\nIn mathematics, biology and psychology, many variables have occurred within structures and systems that determined the results, discoveries, rates and value(s) of sets, systems, and developments within systems, structures, systems within structures and sets of structures.\n\nA mathematical-modelling rule system for a domain D is an example of a meta-system in mathematics and science, for similar and consistency of concrete or frequency found in models within a domain. \nThese are all modes or models; where commonalities are more consistent with consecutive scores or values within a ranged order and are good indicators for gauging probabilities, traits (psychology) or properties (biology).\n\nMeta-systems in cultural studies and sociology refer to contexts, milieux, situations, ecosystems, environments and the biological process with the use of commonalities in behavioral traits and human developments found surrounding a social or scientific system which the system must interact with in order to remain viable. Meta-systems have different structures and also are complementary to other structures of such systems. Without this complementarity in the values, bondings, or tact, the systems could not remain productive, viable or operational.\n\nThe term \"meta-system\" (or \"metasystem\") in cybernetics is synonymous with management system or control system. Stafford Beer, who founded management cybernetics with his viable system model, speaks of metasystems that apply metalanguages which are able to find means of making decisions when necessary improvements cannot be made. In computer science this is known as the halting problem. Here metalanguage works in a larger context than the language it describes and has more variety.\n\n"}
{"id": "19471895", "url": "https://en.wikipedia.org/wiki?curid=19471895", "title": "Negative affectivity", "text": "Negative affectivity\n\nNegative affectivity (NA), or negative affect, is a personality variable that involves the experience of negative emotions and poor self-concept. Negative affectivity subsumes a variety of negative emotions, including anger, contempt, disgust, guilt, fear, and nervousness. Low negative affectivity is characterized by frequent states of calmness and serenity, along with states of confidence, activeness, and great enthusiasm.\n\nIndividuals differ in negative emotional reactivity. Trait negative affectivity roughly corresponds to the dominant personality factor of anxiety/neuroticism that is found within the Big Five personality traits as emotional stability. The Big Five are characterized as openness, conscientiousness, extraversion, agreeableness, and neuroticism. Neuroticism can plague an individual with severe mood swings, frequent sadness, worry, and being easily disturbed, and predicts the development and onset of all \"common\" mental disorders. Research shows that negative affectivity relates to different classes of variables: Self-reported stress and (poor) coping skills, health complaints, and frequency of unpleasant events. Weight gain and mental health complaints are often experienced as well.\n\nPeople who express high negative affectivity view themselves and a variety of aspects of the world around them in generally negative terms. Negative affectivity is strongly related to life satisfaction. Individuals high in negative affect will exhibit, on average, higher levels of distress, anxiety, and dissatisfaction, and tend to focus on the unpleasant aspects of themselves, the world, the future, and other people, and also evoke more negative life events. The similarities between these affective traits and life satisfaction have led some researchers to view both positive and negative affect with life satisfaction as specific indicators of the broader construct of subjective well-being.\n\nNegative affect arousal mechanisms can induce negative affective states as evidenced by a study conducted by Stanley S. Seidner on negative arousal and white noise. The study quantified reactions from Mexican and Puerto Rican participants in response to the devaluation of speakers from other ethnic origins.\n\nThere are many instruments that can be used to measure negative affectivity, including measures of related concepts, such as neuroticism and trait anxiety. Two frequently used are:\n\nPANAS – The Positive and Negative Affect Schedule incorporates a 10-item negative affect scale. The PANAS-X is an expanded version of PANAS that incorporates negative affect subscales for Fear, Sadness, Guilt, Hostility, and Shyness.\n\nI-PANAS-SF – The International Positive and Negative Affect Schedule Short Form is an extensively validated brief, cross-culturally reliable 10-item version of the PANAS. Negative Affect items are Afraid, Ashamed, Hostile, Nervous and Upset. Internal consistency reliabilities between .72 and .76 are reported. The I-PANAS-SF was developed to eliminate redundant and ambiguous items and thereby derive an efficient measure for general use in research situations where either time or space are limited, or where international populations are of interest but where English may not be the mother tongue.\n\nRecent studies indicate that negative affect has important, beneficial impacts on cognition and behavior. These developments are a remarkable departure from past psychological research, which is characterized by a unilateral emphasis on the benefits of positive affect. Both states of affect influence mental processes and behavior. Negative affect is regularly recognized as a \"stable, heritable trait tendency to experience a broad range of negative feelings, such as worry, anxiety, self-criticisms, and a negative self-view\". This allows one to feel every type of emotion, which is regarded as a normal part of life and human nature. So, while the emotions themselves are viewed as negative, the individual experiencing them should not be classified as a negative person or depressed. They are going through a normal process and are feeling something that many individuals may not be able to feel or process due to differing problems.\n\nThese findings complement evolutionary psychology theories that affective states serve adaptive functions in promoting suitable cognitive strategies to deal with environmental challenges. Positive affect is associated with assimilative, top-down processing used in response to familiar, benign environments. Negative affect is connected with accommodative, bottom-up processing in response to unfamiliar, or problematic environments. Thus, positive affectivity promotes simplistic heuristic approaches that rely on preexisting knowledge and assumptions. Conversely, negative affectivity promotes controlled, analytic approaches that rely on externally drawn information.\n\nBenefits of negative affect are present in areas of cognition including perception, judgment, memory and interpersonal personal relations. Since negative affect relies more on cautious processing than preexisting knowledge, people with negative affect tend to perform better in instances involving deception, manipulation, impression formation, and stereotyping. Negative affectivity's analytical and detailed processing of information leads to fewer reconstructive-memory errors, whereas positive mood relies on broader schematic to thematic information that ignores detail. Thus, information processing in negative moods reduces the misinformation effect and increases overall accuracy of details. People also exhibit less interfering responses to stimuli when given descriptions or performing any cognitive task.\n\nPeople are notoriously susceptible to forming inaccurate judgments based on biases and limited information. Evolutionary theories propose that negative affective states tend to increase skepticism and decrease reliance on preexisting knowledge. Consequently, judgmental accuracy is improved in areas such as impression formation, reducing fundamental attribution error, stereotyping, and gullibility. While sadness is normally associated with the hippocampus, it does not produce the same side effects that would be associated with feelings of pleasure or excitement. Sadness correlates with feeling blue or the creation of tears, while excitement may cause a spike in blood pressure and one's pulse. As far as judgment goes, most people think about how they themselves feel about a certain situation. They will jump right to their current mood when asked a question. However, some mistake this process when using their current mood to justify a reaction to a stimulus. If you're sad, yet only a little bit, chances are your reactions and input will be negative as a whole.\n\nFirst impressions are one of the most basic forms of judgments people make on a daily basis; yet judgment formation is a complex and fallible process. Negative affect is shown to decrease errors in forming impressions based on presuppositions. One common judgment error is the halo effect, or the tendency to form unfounded impressions of people based on known but irrelevant information. For instance, more attractive people are often attributed with more positive qualities. Research demonstrates that positive affect tends to increase the halo effect, whereas negative affect decreases it.\n\nA study involving undergraduate students demonstrated a halo effect in identifying a middle-aged man as more likely to be a philosopher than an unconventional, young woman. These halo effects were nearly eliminated when participants were in a negative affective state. In the study, researchers sorted participants into either happy or sad groups using an autobiographical mood induction task in which participants reminisced on sad or happy memories. Then, participants read a philosophical essay by a fake academic who was identified as either a middle-aged, bespectacled man or as a young, unorthodox-looking woman. The fake writer was evaluated on intelligence and competence. The positive affect group exhibited a strong halo effect, rating the male writer significantly higher than the female writer in competence. The negative affect group exhibited almost no halo effects rating the two equally. Researchers concluded that impression formation is improved by negative affect. Their findings support theories that negative affect results in more elaborate processing based upon external, available information.\n\nThe systematic, attentive approach caused by negative affect reduces fundamental attribution error, the tendency to inaccurately attribute behavior to a person's internal character without taking external, situational factors into account. The fundamental attribution error (FAE) is connected with positive affect since it occurs when people use top-down cognitive processing based on inferences. Negative affect stimulates bottom-up, systematic analysis that reduces fundamental attribution error.\n\nThis effect is documented in FAE research in which students evaluated a fake debater on attitude and likability based on an essay the \"debater\" wrote. After being sorted into positive or negative affect groups, participants read one of two possible essays arguing for one side or another on a highly controversial topic. Participants were informed that the debater was assigned a stance to take in the essay that did not necessarily reflect his views. Still, the positive affect groups rated debaters who argued unpopular views as holding the same attitude expressed in the essay. They were also rated as unlikeable compared to debaters with popular stances, thus, demonstrating FAE. In contrast, the data for the negative affect group displayed no significant difference in ratings for debaters with popular stance and debaters with unpopular stances. These results indicate that positive affect assimilation styles promote fundamental attribution error, and negative affect accommodation styles minimize the error in respect to judging people.\n\nNegative affect benefits judgment in diminishing the implicit use of stereotypes by promoting closer attention to stimuli. In one study, participants were less likely to discriminate against targets that appeared Muslim when in a negative affective state. After organizing participants into positive and negative affect groups, researchers had them play a computer game. Participants had to make rapid decisions to shoot only at targets carrying a gun. Some of the targets wore turbans making them appear Muslim. As expected, there was a significant bias against Muslim targets resulting in a tendency to shoot at them. However, this tendency decreased with subjects in negative affective states. Positive affect groups developed more aggressive tendencies toward Muslims. Researchers concluded that negative affect leads to less reliance on internal stereotypes, thus decreasing judgmental bias.\n\nMultiple studies have shown that negative affectivity has a beneficial role in increasing skepticism and decreasing gullibility. Because negative affective states increase external analysis and attention to details, people in negative states are better able to detect deception.\n\nResearchers have presented findings in which students in negative affective states had improved lie detection compared to students in positive affective states. In a study, students watched video clips of everyday people either lying or telling the truth. First, music was used to induce positive, negative, or neutral affect in participants. Then, experimenters played 14 video messages that had to be identified by participants as true or false. As expected, the negative affect group performed better in veracity judgments than the positive affect group who performed no better than chance. Researchers believe that the negative affect groups detected deception more successfully because they attended to stimulus details and systematically built inferences from those details.\n\nMemory has been found to have many failures that effect the accuracy of recalled memories. This has been especially pragmatic in criminal settings as eyewitness memories have been found to be less reliable than one would hope. However, the externally focused and accommodative processing of negative affect has a positive effect on the overall improvement of memory. This evidenced by reduction of the misinformation effect and the number of false memories reported. The knowledge implies that negative affect can be used to enhance eyewitness memory; however, additional research suggests that the extent to which memory is improved by negative affect does not sufficiently improve eyewitness testimonies to significantly reduce its error.\n\nNegative affect has been shown to decrease susceptibility of incorporating misleading information, which is related to the misinformation effect. The misinformation effect refers to the finding that misleading information presented between the encoding of an event and its subsequent recall influences a witness's memory. This corresponds to two types of memory failure:\n\nNegative mood is shown to decrease suggestibility error. This is seen through reduced amounts of incorporation of false memories when misleading information is present. On the other hand, positive affect has shown to increase susceptibility to misleading information. An experiment with undergraduate students supported these results. Participants began the study in a lecture hall and witnessed what they thought was an unexpected five-minute belligerent encounter between an intruder and the lecturer. A week later, these participants watched a 10-minute-long video that generated either a positive, negative or neutral mood. They then completed a brief questionnaire about the previous incident between the intruder and lecturer that they witnessed the week earlier. In this questionnaire half of the participants received questions with misleading information and the other half received questions without any misleading information. This manipulation was used to determine if participants were susceptible to suggestibility failure. After 45 minutes of unrelated distractors participants were given a set of true or false questions which tested for false memories. Participants experiencing negative moods reported fewer numbers of false memories, whereas those experiencing positive moods reported a greater amount of false memories. This implies that positive affect promotes integration of misleading details and negative affect reduces the misinformation effect.\n\nPeople who experience negative affectivity following an event report fewer reconstructive false memories. This was evidenced by two studies conducted around public events. The first surrounded the events of the televised O.J. Simpson trial. Participants were asked to fill out questionnaires three times: one week, two months and a year after the televised verdict. These questionnaires measured participant emotion towards the verdict and the accuracy of their recalled memory of what occurred during the trial. Overall the study found that although participant response to the event outcome did not affect the quantity of remembered information, it did influence the likelihood of false memory. Participants who were pleased with the verdict of the O.J. Simpson trial were more likely to falsely believe something occurred during the trial than those who were displeased with the verdict. Another experiment found the same findings with Red Sox fans and Yankees fans in their overall memory of events that occurred in the final game of a 2004 playoff series in which the Red Sox defeated the Yankees. The study found that the Yankees fans had better memory of events that occurred than the Red Sox fans. The results from both of these experiments are consistent with the findings that negative emotion can lead to fewer memory errors and thus increased memory accuracy of events.\n\nAlthough negative affect has been shown to decrease the misinformation effect, the degree to which memory is improved is not enough to make a significant effect on witness testimony. In fact, emotions, including negative affect, are shown to reduce accuracy in identifying perpetrators from photographic lineups. Researchers demonstrated this effect in an experiment in which participants watched a video that induced either negative emotion or a neutral mood. The two videos were deliberately similar except for the action of interest, which was either a mugging (negative emotion) or a conversation (neutral emotion). After watching one of the two videos participants are shown perpetrator lineups, which either contained the target perpetrator from the video or a foil, a person that looked similar to the target. The results revealed that the participants who watched the emotion-induced video were more likely to incorrectly identify the innocent foil than to correctly identify the perpetrator. Neutral participants were more likely to correctly identify the perpetrator in comparison to their emotional counterparts. This demonstrates that emotional affect in forensic settings decreases accuracy of eyewitness memory. These findings are consistent with prior knowledge that stress and emotion greatly impair eyewitness ability to recognitive perpetrators.\n\nNegative affectivity can produce several interpersonal benefits. It can cause subjects to be more polite and considerate with others. Unlike positive mood, which causes less assertive approaches, negative affectivity can, in many ways, cause a person to be more polite and elaborate when making requests.\n\nNegative affectivity increases the accuracy of social perceptions and inferences. Specifically, high negative-affectivity people have more negative, but accurate, perceptions of the impression they make to others. People with low negative affectivity form overly-positive, potentially inaccurate impression of others that can lead to misplaced trust.\n\nA research conducted by Forgas J.P studied how affectivity can influence intergroup discrimination. He measured affectivity by how people allocate rewards to in-group and out-group members. In the procedure, participants had to describe their interpretations after looking at patterns of judgments about people. Afterwards, participants were exposed to a mood induction process, where they had to watch videotapes designed to elicit negative or positive affectivity. Results showed that participants with positive affectivity were more negative and discriminated more than participants with negative affectivity. Also, happy participants were more likely to discriminate between in-group and out-group members than sad participants. Negative affect is often associated with team selection. It is viewed as a trait that could make selecting individuals for a team irrelevant, thus preventing knowledge from becoming known or predicted for current issues that may arise.\n\nNegative affectivity subconsciously signals a challenging social environment.\nNegative mood may increase a tendency to conform to social norms.\n\nIn a study, college students where exposed to a mood induction process. After the mood induction process, participants were required to watch a show with positive and negative elements. After watching the show, they were asked to engage on a hypothetical conversation in which they \"describe the episode (they) just observed to a friend\". Their speech was recorded and transcribed during this task. Results showed that speakers in a negative mood had a better quality descriptions and greater amount of information and details. These results show that negative mood can improve people's communication skills.\n\nA negative mood is closely linked to better conversation because it makes use of the hippocampus and different regions of the brain. When someone is upset, that individual may see or hear things differently than an individual who is very upbeat and happy all the time. The small details the negative individual picks up may be something completely overlooked before. Anxiety disorders are often associated with over-thinking and ruminating on topics that would seem irrelevant and pointless to an individual without a disorder. OCD is one common anxiety trait that allows the affected individual a different insight on how things may appear to be. A person that makes use of his or her negative affect has a different view of the world and what goes on in it, thus making their conversations different and interesting to others.\n\nResults of one study show that participants with negative affectivity were more careful with the information they shared with others, being more cautious with who they could trust or not. Researchers found that negative mood not only decreases intimacy levels but also increases caution in placing trust in others.\n\n\n"}
{"id": "677516", "url": "https://en.wikipedia.org/wiki?curid=677516", "title": "Negative campaigning", "text": "Negative campaigning\n\nNegative campaigning or mudslinging is the process of deliberate spreading negative information about someone or something to worsen the public image of the described.\n\nDeliberate spreading of such information can be motivated either by honest desire of the campaigner to warn others against real dangers or deficiencies of the described, or by the campaigner's dishonest ideas on methods of winning in political, business or other spheres of competition against an honest rival.\n\nThe public image of an entity can be defined as reputation, esteem, respect, acceptance of the entity's appearance, values and behaviour by the general public of a given territory and/or a social group, possibly within time limits. As target groups of public and their values differ, so negativity or positivity of a public image is relative: e.g. while in most societies having an honest source of income is a positive value and stealing is discouraged, in the world of professional thieves honest work is frowned upon and stealing is encouraged. In polygamous societies monogamy is not viewed in the way it is valued in monogamous societies. Values of a society also change with time: e.g. homosexuality in Western culture was considered immoral and was criminally prosecuted until the sexual revolution of the second half of the 20 century.\nThus negative campaigning to be successful has to take into account current values of the group it addresses. The degree of strictness in practicing the group's values as opposed to its tolerance for violating the norms has also to be taken into consideration: e.g. while in the Old Testament and other traditional religious societies adultery and prostitution were outlawed and supposed to be punished by death, modern Western societies show much greater tolerance to these.\n\nIn United States politics, negative campaigning has been called \"as American as Mississippi mud\" and \"as American as apple pie\". Some research suggests negative campaigning is the norm in all political venues, mitigated only by the dynamics of a particular contest.\n\nThere are a number of techniques used in negative campaigning. Among the most effective is running advertisements attacking an opponent's personality, record, or opinion. There are two main types of ads used in negative campaigning: attack and contrast.\n\nAttack ads focus exclusively on the negative aspects of the opponent. There is no positive content in an attack ad, whether it is about the candidate or the opponent. Attack ads usually identify the risks associated with the opponent, often exploiting people’s fears to manipulate and lower the impression voters have of the opponent. Because attack ads have no positive content, they have the potential to be more influential than contrast ads in shaping voters’ views of the sponsoring candidate’s opponent.\n\nUnlike attack ads, contrast ads contain information about both the candidate and the opponent. The information about the candidate is positive, while the information about the opponent is negative. Contrast ads compare and contrast the candidate with the opponent, juxtaposing the positive information about the candidate with the negative information of the opponent. Because contrast ads must contain positive information, contrast ads are seen as less damaging to the political process than attack ads.\n\nOne of the most famous such ads was \"Daisy Girl\" by the campaign of Lyndon B. Johnson that successfully portrayed Republican Barry Goldwater as threatening nuclear war. Common negative campaign techniques include painting an opponent as soft on criminals, dishonest, corrupt, or a danger to the nation. One common negative campaigning tactic is attacking the other side for running a negative campaign.\n\nDirty tricks are also common in negative political campaigns. These generally involve secretly leaking damaging information to the media. This isolates a candidate from backlash and also does not cost any money. The material must be substantive enough to attract media interest, however, and if the truth is discovered it could severely damage a campaign. Other dirty tricks include trying to feed an opponent's team false information hoping they will use it and embarrass themselves.\n\nOften a campaign will use outside organizations, such as lobby groups, to launch attacks. These can be claimed to be coming from a neutral source and if the allegations turn out not to be true the attacking candidate will not be damaged if the links cannot be proven. Negative campaigning can be conducted by proxy. For instance, highly partisan ads were placed in the 2004 U.S. presidential election by allegedly independent bodies like MoveOn.org and Swift Boat Veterans for Truth.\n\nPush polls are attacks disguised as telephone polls. They might ask a question like \"How would you react if Candidate A was revealed to beat his wife?\", giving the impression that Candidate A might beat his wife. Members of the media and of the opposing party are deliberately not called making these tactics all but invisible and unprovable.\n\nG. Gordon Liddy played a major role in developing these tactics during the Nixon campaign playing an important advisory of rules that led to the campaign of 1972. James Carville, campaign manager of Bill Clinton's 1992 election, is also a major proponent of negative tactics. Lee Atwater, best known for being an advisor to presidents Ronald Reagan and George H.W. Bush, also pioneered many negative campaign techniques seen in political campaigns today.\n\nSponsors of overt negative campaigns often cite reasons to support mass communication of negative ideas. The Office of National Drug Control Policy uses negative campaigns to steer the public away from health risks. Similar negative campaigns have been used to rebut mass marketing by tobacco companies, or to discourage drunk driving. Those who conduct negative political campaigns sometimes say the public needs to know about the person he or she is voting for, even if it is bad. In other words, if a candidate’s opponent is a crook or a bad person, then he or she should be able to tell the public about it.\n\nMartin Wattenberg and Craig Brians, of the University of California, Irvine, considered in their study whether negative campaigning mobilizes or alienates voters. They concluded that data used by Stephen Ansolabehere in a 1994 American Political Science Review article to advance the hypothesis that negative campaigning demobilizes voters was flawed.\n\nA subsequent study done by Ansolabehere and Shanto Iyengar in 1995 corrected some of the previous study's flaws. This study concluded that negative advertising suppressed voter turnout, particularly for Independent voters. They speculated that campaigns tend to go negative only if the Independent vote is leaning toward the opponent. In doing so, they insure that the swing voters stay home, leaving the election up to base voters. They also found that negative ads have a greater impact on Democrats than on Republicans. According to them, base Republicans will vote no matter what (and will vote only for a Republican), but Democrats can be influenced to either stay home and not vote at all or to switch sides and vote for a Republican. This, combined with the effect negativity has on Independents, led them to conclude that Republicans benefit more from going negative than Democrats.\n\nOther researchers have found different, more positive outcomes from negative campaigns. Rick Farmer, PhD, an assistant professor of political science at the University of Akron found that negative ads are more memorable than positive ads when they reinforce a preexisting belief and are relevant to the central issues of a marketing campaign. Researchers at the University of Georgia found the impact of negative ads increases over time, while positive ads used to counteract negative ads lack the power of negative ads . Research also suggests negative campaigning introduces controversy and raises public awareness through additional news coverage .\n\nMost recently, Kyle Mattes and David P. Redlawsk in \"The Positive Case for Negative Campaigning\" show through surveys and experiments that negative campaigning provides informational benefits for voters. Without negativity, voters would not have full information about all of their choices, since no candidate will say anything bad about herself. They argue that candidates have to point out the flaws in their opponents for voters to be fully informed.\n\nSome strategists say that an effect of negative campaigning is that while it motivates the base of support it can alienate centrist and undecided voters from the political process, reducing voter turnout and radicalizing politics. \nIn a study done by Gina Garramone about how negative advertising affects the political process, it was found that a consequence of negative campaigning is greater image discrimination of the candidates and greater attitude polarization. While positive ads also contributed to the image discrimination and attitude polarization, Garramone found that negative campaigning played a more influential role in the discrimination and polarization than positive campaigning.\n\nNegative ads can produce a backlash. A disastrous ad was run by the Progressive Conservative Party of Canada in the 1993 Canadian federal election, apparently emphasizing Liberal Party of Canada leader Jean Chrétien's Bell's Palsy partial facial paralysis in a number of unflattering photos, with the subtext of criticizing his platforms. Chrétien took maximum advantage of the opportunity to gain the public's sympathy as a man who struggled with a physical disability and his party's subsequent overwhelming victory in the election helped reduce the governing Conservatives to two seats.\n\nA similar backlash happened to the Liberal Party in the 2006 federal election for running an attack ad that suggested that Conservative leader Stephen Harper would use Canadian soldiers to patrol Canadian cities, and impose some kind of martial law. The ad was only available from the Liberal Party's web site for a few hours prior to the release of the attack ads on television; nevertheless, it was picked up by the media and widely criticized for its absurdity, in particular the sentence \"we're not making this up; we're not allowed to make this stuff up\". Liberal MP Keith Martin expressed his disapproval of \"whoever the idiot who approved that ad was,\" shortly before Liberal leader Paul Martin (no relation) stated that he had personally approved them. The effect of the ads was to diminish the credibility of the party's other attack ads. It offended many Canadians, particularly those in the military, some of whom were fighting in Afghanistan at the time. (See Canadian federal election, 2006)\n\nMore recently, in the 2008 US Senate race in North Carolina, Republican incumbent Elizabeth Dole attempted an attack ad on Democratic challenger Kay Hagan, who had taken a small lead in polls, by tying her to atheists. Dole's campaign released an ad questioning Hagan's religion and it included a voice saying \"There is no God!\" over a picture of Kay Hagan's face. The voice was not Hagan's but it is believed the ad implied that it was. Initially, it was thought the ad would work as religion has historically been a very important issue to voters in the American south, but the ad produced a backlash across the state and Hagan responded forcefully with an ad saying that she was a Sunday school teacher and was a religious person. Hagan also claimed Dole was trying to change the subject from the economy (the ad appeared around the same time as the 2008 financial crisis). Hagan's lead in polls doubled and she won the race by a nine-point margin.\n\nBecause of the possible harm that can come from being seen as a negative campaigner, candidates often pledge to refrain from negative attacks. This pledge is usually abandoned when an opponent is perceived to be \"going negative,\" with the first retaliatory attack being, ironically, an accusation that the opponent is a negative campaigner.\n\nWhile some research has found advantages and other has found disadvantages, some studies find no difference between negative and positive approaches .\n\nResearch published in the Journal of Advertising found that negative political advertising makes the body want to turn away physically, but the mind remembers negative messages. The findings are based on research conducted by James Angelini, professor of communication at the University of Delaware, in collaboration with Samuel Bradley, assistant professor of advertising at Texas Tech University, and Sungkyoung Lee of Indiana University, which used ads that aired during the 2000 presidential election. During the study, the researchers placed electrodes under the eyes of willing participants and showed them a series of 30-second ads from both the George W. Bush and Al Gore campaigns. The electrodes picked up on the \"startle response,\" the automatic eye movement typically seen in response to snakes, spiders and other threats. Compared to positive or neutral messages, negative advertising prompted greater reflex reactions and a desire to move away.\n\n\n\n\n"}
{"id": "4639538", "url": "https://en.wikipedia.org/wiki?curid=4639538", "title": "Negative double", "text": "Negative double\n\nThe negative double is a form of takeout double in bridge. It is made by the responder after his right-hand opponent overcalls on the first round of bidding, and is used to show both support for the unbid suits as well as some values. It is treated as forcing, but not unconditionally so. In practice, the negative double is sometimes used as a sort of catch-all, made when no other call properly describes responder's hand. Therefore, a partnership might even treat the negative double as a wide-ranging call that merely shows some values.\n\nUsing the modern negative double convention, it is understood that a double over an initial overcall is conventional, and \"not\" for penalties (but see Playing for penalties). For example, using this convention, the following doubles would be regarded as negative, not for penalty:\n\n\nIn understandings regarding negative doubles, the emphasis is on major suit lengths. This is largely due to the special value that tournament play, especially the pairs game, places on major suits. Since the mid-1980s, the negative double has been used mainly to stand in for a bid in an unbid major suit.\n\nMost partnerships using the negative double agree that it applies only through a particular level of overcall. For example, they may agree that the double of an overcall through 3 is negative, and that beyond 3 a double is for penalties.\n\nAt rubber bridge many players are reluctant to give up the penalty double of an overcall, and so do not use the double as conventional.\n\nThe term \"negative double\" was initially employed to distinguish it from the \"penalty\", or \"business\", or \"positive\" double, and signified a double over an opponent's opening bid whose meaning was a request for partner to bid his best suit. Around 1930, the term \"informatory double\" replaced \"negative double\", and that term later gave way to \"takeout double\" as it is used at present; the original term \"negative double\" fell into disuse.\n\nIn 1957, Alvin Roth in his partnership with Tobias Stone appropriated the abandoned term \"negative double\" to denote a conventional double by responder over an overcall and gave it its current meaning. The bid was also briefly known as \"Sputnik\", because it was as new as the satellite of that name that the Soviet Union had recently launched. The term is still used sometimes in Europe.\n\nThe negative double is generally forcing, but opener might pass to convert the double to a penalty double. There is a special agreement called negative free bids, under which (after the overcall) the bid of a new suit by responder is not forcing. However, most negative doublers play that a new suit response (or free bid), whether at the one level or higher, is forcing.\n\nThe negative double loses even more definition when it can be made with a very broad range of strength, from roughly six HCP up to game forcing values. In a pinch, players use it to \"get by this round of bidding.\"\n\nThe negative double does not cause the partnership to completely lose the ability to penalize an overcall. There are two ways that the overcall can be doubled for penalties. For example:\n\n\nResponder makes a negative double, and opener passes for penalties. This position is analogous to one in which a player makes a takeout double and his partner passes the double, converting it to a penalty double.\n\n\nResponder passes the overcall, opener makes a re-opening double, and responder passes that double for penalties. This can be dangerous, because opener often doesn't know whether responder is simply too weak to make any call, or is hoping that opener can re-open with a double.\n\nThese situations are rare, though, and the more so because some five-card major partnerships play negative doubles \"over minor suit openings only.\" The rationale is that responder knows much more about opener's distribution after a major suit opening than after a minor suit opening, and can better judge whether to play in opener's major suit, to play for penalties by doubling, or to show a suit of his own.\n\nPartnerships have different understandings about the length in unbid suits that is shown by a negative double, and the understandings differ according both to which suits remain unbid and to the current level of the bidding. Nevertheless, the following are popular understandings:\n\n\n"}
{"id": "5205483", "url": "https://en.wikipedia.org/wiki?curid=5205483", "title": "Negative free bid", "text": "Negative free bid\n\nNegative free bid is a contract bridge treatment whereby a free bid by responder over an opponent's overcall shows a long suit in a weak hand and is not forcing. This is in contrast with standard treatment, where a free bid can show unlimited values and is unconditionally forcing. The treatment is a relatively recent invention, and has become quite popular, especially in expert circles.\n\nNegative free bids resolve relatively frequent situations where the responder holds a long suit with which he would like to compete for a partscore, but is deprived from bidding it by opponent's overcall.\n\nFor example, if South holds: , partner opens 1 and East overcalls 1, he couldn't bid 2 in standard methods, as it would show 10+ high-card points, and a negative double would be too off-shape. With NFB treatment in effect though, he can bid 2 which the partner may pass (unless he has extra values and support, or an excellent suit of its own without tolerance for hearts).\n\nHowever, as a corollary, negative free bids affect the scope of negative double; if the hand is suitable for \"standard\" forcing free bid (10-11+ points), a negative double has to be made first and the suit bid only in the next round. Thus, the negative double can be made with the following types of hand:\nThis can sometimes allow the opponents to preempt effectively. \nFor example, West, holding: , after this auction is in an awkward situation — he doesn't know whether partner has spades or not; whether South was bidding to make or to sacrifice — is it correct to double, bid 4 or pass?\n\n"}
{"id": "32108559", "url": "https://en.wikipedia.org/wiki?curid=32108559", "title": "Negative hyperconjugation", "text": "Negative hyperconjugation\n\nIn organic chemistry, negative hyperconjugation is the donation of electron density from a filled π- or p-orbital to a neighboring σ-orbital. This phenomenon, a type of resonance, can stabilize the molecule or transition state. It also causes an elongation of the σ-bond by adding electron density to its antibonding orbital.\n\nNegative hyperconjugation is most commonly observed when the σ-orbital is located on certain C–F or C–O bonds, and does not occur to an appreciable extent with normal C–H bonds.\n\nIn negative hyperconjugation, the electron density flows in the \"opposite\" direction (from π- or p-orbital to empty σ-orbital) than it does in the more common hyperconjugation (from σ-orbital to empty p-orbital).\n\n"}
{"id": "1762868", "url": "https://en.wikipedia.org/wiki?curid=1762868", "title": "Negative luminescence", "text": "Negative luminescence\n\nNegative luminescence is a physical phenomenon by which an electronic device emits less thermal radiation when an electric current is passed through it than it does in thermal equilibrium (current off). When viewed by a thermal camera, an operating negative luminescent device looks colder than its environment.\n\nNegative luminescence is most readily observed in semiconductors. Incoming infrared radiation is absorbed in the material by the creation of an electron–hole pair. An electric field is used to remove the electrons and holes from the region before they have a chance to recombine and re-emit thermal radiation. This effect occurs most efficiently in regions of low charge carrier density.\n\nNegative luminescence has also been observed in semiconductors in orthogonal electric and magnetic fields. In this case, the junction of a diode is not necessary and the effect can be observed in bulk material. A term that has been applied to this type of negative luminescence is \"galvanomagnetic luminescence\".\n\nNegative luminescence might appear to be a violation of Kirchhoff's law of thermal radiation. This is not true, as the law only applies in thermal equilibrium.\n\nAnother term that has been used to describe negative luminescent devices is \"Emissivity switch\", as an electric current changes the effective emissivity.\n\nThis effect was first seen by Russian physicists in the 1960s in A.F.Ioffe Physicotechnical Institute, Leningrad, Russia. Subsequently, it was studied in semiconductors such as indium antimonide (InSb), germanium (Ge) and indium arsenide (InAs) by workers in West Germany, Ukraine (Institute of Semiconductor Physics, Kiev), Japan (Chiba University) and the United States. It was first observed in the mid-infrared (3-5 µm wavelength) in the more convenient diode structures in InSb heterostructure diodes by workers at the Defence Research Agency, Great Malvern, UK (now QinetiQ). These British workers later demonstrated LWIR band (8-12 µm) negative luminescence using mercury cadmium telluride diodes.\n\nLater the Naval Research Laboratory, Washington DC, started work on negative luminescence in mercury cadmium telluride (HgCdTe). The phenomenon has since been observed by several university groups around the world.\n\n\n\n"}
{"id": "361356", "url": "https://en.wikipedia.org/wiki?curid=361356", "title": "Negentropy", "text": "Negentropy\n\nIn information theory and statistics, negentropy is used as a measure of distance to normality. The concept and phrase \"negative entropy\" was introduced by Erwin Schrödinger in his 1944 popular-science book \"What is Life?\" Later, Léon Brillouin shortened the phrase to \"negentropy\", to express it in a more \"positive\" way: a living system imports negentropy and stores it. In 1974, Albert Szent-Györgyi proposed replacing the term \"negentropy\" with \"syntropy\". That term may have originated in the 1940s with the Italian mathematician Luigi Fantappiè, who tried to construct a unified theory of biology and physics. Buckminster Fuller tried to popularize this usage, but \"negentropy\" remains common.\n\nIn a note to \"What is Life?\" Schrödinger explained his use of this phrase.\nIn 2009, Mahulikar & Herwig redefined negentropy of a dynamically ordered sub-system as the specific entropy deficit of the ordered sub-system relative to its surrounding chaos. Thus, negentropy has SI units of (J kg K) when defined based on specific entropy per unit mass, and (K) when defined based on specific entropy per unit energy. This definition enabled: \"i\") scale-invariant thermodynamic representation of dynamic order existence, \"ii\") formulation of physical principles exclusively for dynamic order existence and evolution, and \"iii\") mathematical interpretation of Schrödinger's negentropy debt.\n\nIn information theory and statistics, negentropy is used as a measure of distance to normality. Out of all distributions with a given mean and variance, the normal or Gaussian distribution is the one with the highest entropy. Negentropy measures the difference in entropy between a given distribution and the Gaussian distribution with the same mean and variance. Thus, negentropy is always nonnegative, is invariant by any linear invertible change of coordinates, and vanishes if and only if the signal is Gaussian.\n\nNegentropy is defined as\n\nwhere formula_2 is the differential entropy of the Gaussian density with the same mean and variance as formula_3 and formula_4 is the differential entropy of formula_3:\n\nNegentropy is used in statistics and signal processing. It is related to network entropy, which is used in independent component analysis.\n\nThere is a physical quantity closely linked to free energy (free enthalpy), with a unit of entropy and isomorphic to negentropy known in statistics and information theory. In 1873, Willard Gibbs created a diagram illustrating the concept of free energy corresponding to free enthalpy. On the diagram one can see the quantity called capacity for entropy. This quantity is the amount of entropy that may be increased without changing an internal energy or increasing its volume. In other words, it is a difference between maximum possible, under assumed conditions, entropy and its actual entropy. It corresponds exactly to the definition of negentropy adopted in statistics and information theory. A similar physical quantity was introduced in 1869 by Massieu for the isothermal process (both quantities differs just with a figure sign) and then Planck for the isothermal-isobaric process. More recently, the Massieu–Planck thermodynamic potential, known also as \"free entropy\", has been shown to play a great role in the so-called entropic formulation of statistical mechanics, applied among the others in molecular biology and thermodynamic non-equilibrium processes.\n\nIn 1953, Léon Brillouin derived a general equation stating that the changing of an information bit value requires at least kT ln(2) energy. This is the same energy as the work Leó Szilárd's engine produces in the idealistic case. In his book, he further explored this problem concluding that any cause of this bit value change (measurement, decision about a yes/no question, erasure, display, etc.) will require the same amount of energy.\n\n"}
{"id": "19777249", "url": "https://en.wikipedia.org/wiki?curid=19777249", "title": "Organizing principle", "text": "Organizing principle\n\nAn organizing principle is a core assumption from which everything else by proximity can derive a classification or a value. It is like a central reference point that allows all other objects to be located, often used in a conceptual framework. Having an organizing principle might help one simplify and get a handle on a particularly complicated domain or phenomenon. On the other hand, it might create a deceptive prism that colors one's judgment.\n\n\n\n"}
{"id": "3921784", "url": "https://en.wikipedia.org/wiki?curid=3921784", "title": "Original camera negative", "text": "Original camera negative\n\nThe original camera negative (OCN) is the film in a traditional film-based movie camera which captures the original image. This is the film from which all other copies will be made. It is known as raw stock prior to exposure.\n\nThe size of a roll varies depending on the film gauge and whether or not a new roll, re-can, or short end was used. One hundred or 400 foot rolls are common in 16mm, while 400 or 1,000 foot (ft) rolls are used in 35mm work. While these are the most common sizes, other lengths such as 200, 800, or 1,200 ft may be commercially available from film stock manufacturers, usually by special order. Rolls of 100 and 200 ft are generally wound on spools for daylight-loading, while longer lengths are only wound around a plastic core. Core-wound stock has no exposure protection outside its packaging, and therefore must be loaded into a camera magazine within a darkroom or changing bag/tent in order to prevent the film being fogged.\n\nOriginal camera negative is of great value, as if lost or damaged it cannot be re-created without re-shooting the scene, something which is often impossible. It also contains the highest-quality version of the original image available, before any analog resolution and dynamic range loss from copying. For these reasons, original camera negative is handled with great care, and only by specialized trained people in dedicated film laboratories.\n\nAfter the film is processed by the film lab, camera rolls are assembled into lab rolls of 1,200 to 1,500 ft. Work prints may be made for viewing dailies or editing the picture on film.\n\nOnce film editing is finalized, a negative cutter will conform the negative using the Keykode on the edge of the film as a reference, cutting the original camera negative and incorporating any opticals (titles, dissolves, fades, and special effects), and cementing it together into several rolls. \n\nThe edited original negative is then copied to create a safety positive which can be used as a backup to create a usable negative. At this point, an answer print will be created from the original camera negative, and upon its approval, interpositives (IPs) and internegatives (INs) are created, from which the release prints are made. Generally speaking, the original camera negative is considered too important and delicate to be used for any processes more than necessary, as each pass through a lab process carries the risk of further degrading the quality of the negative by scratching the emulsion. Once an answer print is approved, the interpositives and internegatives are regarded as the earliest generation of the finished and graded film, and are almost always used for transfers to video or new film restorations. The original camera negatives is usually regarded as a last resort in the event that all of the intermediate elements have been compromised or lost.\n\nThe more popular a film is, the higher the likelihood that the original negative is in a worse shape, due to the need to return to the original camera negative to strike new interpositives to replace the exhausted ones, and thus create more internegatives and release prints. Before 1969, 35mm prints were struck directly from the original negative, often running into hundreds of copies, and causing further wear on the original.\n\nPhysical film stock is still occasionally used in film-making, particularly in prestige productions where the director and cinematographer have the power to require the extra cost, but as of 2016, it is becoming increasingly rare.\n\nIn modern cinematography, the camera is usually a digital camera, and no physical negative exists. However, the concept of \"camera original material\" is still used to describe camera image data. Camera original material that has not yet been ingested, duplicated, and archived is in a similar precarious state to original camera negative in a film process. One of the jobs of the digital imaging technician is to ensure that digital camera original material is backed up as soon as possible.\n"}
{"id": "25262679", "url": "https://en.wikipedia.org/wiki?curid=25262679", "title": "Phonemic imagery", "text": "Phonemic imagery\n\nPhonemic imagery refers to the processing of thoughts as words rather than as symbols or other images. It is sometimes referred to as the equivalent of inner speech or covert speech, and sometimes considered as a third phenomenon, separate from but similar to these other forms of internal speech.\n\nPhonemic imagery is a part of the philosophy of consciousness rather than linguistics as it is considered an internal phenomenon of consciousness observed through reflection rather than amenable to empirical observation.\n"}
{"id": "831689", "url": "https://en.wikipedia.org/wiki?curid=831689", "title": "Pontryagin's maximum principle", "text": "Pontryagin's maximum principle\n\nPontryagin's maximum (or minimum) principle is used in optimal control theory to find the best possible control for taking a dynamical system from one state to another, especially in the presence of constraints for the state or input controls. It was formulated in 1956 by the Russian mathematician Lev Pontryagin and his students. It has as a special case the Euler–Lagrange equation of the calculus of variations.\n\nThe principle states, informally, that the \"control Hamiltonian\" must take an extreme value over controls in the set of all permissible controls. Whether the extreme value is maximum or minimum depends both on the problem and on the sign convention used for defining the Hamiltonian. The normal convention, which is the one used in Hamiltonian, leads to a maximum hence \"maximum principle\" but the sign convention used in this article makes the extreme value a minimum.\n\nIf formula_1 is the set of values of permissible controls then the principle states that the optimal control formula_2 must satisfy:\nwhere formula_4 is the optimal state trajectory and formula_5 is the optimal costate trajectory.\n\nThe result was first successfully applied to minimum time problems where the input control is constrained, but it can also be useful in studying state-constrained problems.\n\nSpecial conditions for the Hamiltonian can also be derived. When the final time formula_6 is fixed and the Hamiltonian does not depend explicitly on time formula_7, then:\nand if the final time is free, then:\nMore general conditions on the optimal control are given below.\n\nWhen satisfied along a trajectory, Pontryagin's minimum principle is a necessary condition for an optimum. The Hamilton–Jacobi–Bellman equation provides a necessary and sufficient condition for an optimum, but this condition must be satisfied over the whole of the state space.\n\nWhile the Hamilton-Jacobi-Bellman equation admits a straightforward extension to stochastic optimal control problems, the minimum principle does not.\n\nThe principle was first known as \"Pontryagin's maximum principle\" and its proof is historically based on maximizing the Hamiltonian. The initial application of this principle was to the maximization of the terminal speed of a rocket. However, as it was subsequently mostly used for minimization of a performance index it has here been referred to as the \"minimum principle\". Pontryagin's book solved the problem of minimizing a performance index.\n\nIn what follows we will be making use of the following notation.\n\nHere the necessary conditions are shown for minimization of a functional. Take formula_15 to be the state of the dynamical system with input formula_16, such that\nwhere formula_1 is the set of admissible controls and formula_19 is the terminal (i.e., final) time of the system. The control formula_20 must be chosen for all formula_21 to minimize the objective functional formula_22 which is defined by the application and can be abstracted as\n\nThe constraints on the system dynamics can be adjoined to the Lagrangian formula_24 by introducing time-varying Lagrange multiplier vector formula_25, whose elements are called the costates of the system. This motivates the construction of the Hamiltonian formula_26 defined for all formula_21 by:\nwhere formula_29 is the transpose of formula_25.\n\nPontryagin's minimum principle states that the optimal state trajectory formula_31, optimal control formula_2, and corresponding Lagrange multiplier vector formula_33 must minimize the Hamiltonian formula_26 so that\n\nfor all time formula_21 and for all permissible control inputs formula_20. It must also be the case that\n\nAdditionally, the costate equations\n\nmust be satisfied. If the final state formula_40 is not fixed (i.e., its differential variation is not zero), it must also be that the terminal costates are such that\n\nThese four conditions in (1)-(4) are the necessary conditions for an optimal control. Note that (4) only applies when formula_40 is free. If it is fixed, then this condition is not necessary for an optimum.\n\n\n"}
{"id": "1005874", "url": "https://en.wikipedia.org/wiki?curid=1005874", "title": "Principle", "text": "Principle\n\nA principle is a concept or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 \"Principles of Operation\".\n\nExamples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.\n\nIn common English, it is a substantive and collective term referring to rule governance, the absence of which, being \"unprincipled\", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only \"in principle\" but not in fact.\n\nA principle represents values that orient and rule the conduct of persons in a particular society. To \"act on principle\" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.\n\nIt represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.\n\nArchimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the \"population principle\", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.\n\nThe principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\nHowever, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that \"the Earth is round\", corresponds to a direct relation between the subject and the predicate.\n\nAccording to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.\n\nThe principle of the excluding third or \"principium tertium exclusum\" is a principle of the traditional logic formulated canonically by Leibniz as: either \"A\" is \"B\" or \"A\" isn't \"B\". It is read the following way: either \"P\" is true, or its denial ¬\"P\" is.\nIt is also known as \"tertium non datur\" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).\n"}
{"id": "11585926", "url": "https://en.wikipedia.org/wiki?curid=11585926", "title": "Principle of humanity", "text": "Principle of humanity\n\nIn philosophy and rhetoric, the principle of humanity states that when interpreting another speaker we must assume that his or her beliefs and desires are connected to each other and to reality in some way, and attribute to him or her \"the propositional attitudes one supposes one would have oneself in those circumstances\". The principle of humanity was named by Richard Grandy (then an assistant professor of philosophy at Princeton University) who first expressed it in 1973.\n\n"}
{"id": "8924176", "url": "https://en.wikipedia.org/wiki?curid=8924176", "title": "Principle of nonvacuous contrast", "text": "Principle of nonvacuous contrast\n\nThe principle of nonvacuous contrast is a logical or methodological principle which requires that a genuine predicate never refer to everything, or to nothing, within its universe of discourse.\n"}
{"id": "39812836", "url": "https://en.wikipedia.org/wiki?curid=39812836", "title": "Process reference models", "text": "Process reference models\n\nA process reference model is a model that has generic functionality and can be used more than once in different models. The creator of a process model benefits from existing process reference models by not needing to reinvent the process model but only reusing it as a starting point in creating a process model for a specific purpose.\n\nDuring the identification of processes ideal for reuse, the designer needs to (1) Get approval (2) Provide Organization Scope Context and (3) Identify Process Standardization Opportunities.\n"}
{"id": "244629", "url": "https://en.wikipedia.org/wiki?curid=244629", "title": "Scientific law", "text": "Scientific law\n\nThe laws of science, also called scientific laws or scientific principles, are statements that describe or predict a range of natural phenomena. Each scientific law is a statement based on repeated experimental observations that describes some aspect of the Universe. The term \"law\" has diverse usage in many cases (approximate, accurate, broad, or narrow theories) across all fields of natural science (physics, chemistry, biology, geology, astronomy, etc.). Scientific laws summarize and explain a large collection of facts determined by experiment, and are tested based on their ability to predict the results of future experiments. They are developed either from facts or through mathematics, and are strongly supported by empirical evidence. It is generally understood that they reflect causal relationships fundamental to reality, and are discovered rather than invented.\n\nLaws reflect scientific knowledge that experiments have repeatedly verified (and never falsified). Their accuracy does not change when new theories are worked out, but rather the scope of application, since the equation (if any) representing the law does not change. As with other scientific knowledge, they do not have absolute certainty (as mathematical theorems or identities do), and it is always possible for a law to be overturned by future observations. A law can usually be formulated as one or several statements or equations, so that it can be used to predict the outcome of an experiment, given the circumstances of the processes taking place.\n\nLaws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. Hypotheses and postulates are not laws since they have not been verified to the same degree and may not be sufficiently general, although they may lead to the formulation of laws. A law is a more solidified and formal statement, distilled from repeated experiment. Laws are narrower in scope than scientific theories, which may contain one or several laws. Science distinguishes a law or theory from facts. Calling a law a fact is ambiguous, an overstatement, or an equivocation. Although the nature of a scientific law is a question in philosophy and although scientific laws describe nature mathematically, scientific laws are practical conclusions reached by the scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolutes.\n\nAccording to the unity of science thesis, \"all\" scientific laws follow fundamentally from physics. Laws which occur in other sciences ultimately follow from physical laws. Often, from mathematically fundamental viewpoints, universal constants emerge from a scientific law.\n\nA scientific law always applies under the same conditions, and implies that there is a causal relationship involving its elements. Factual and well-confirmed statements like \"Mercury is liquid at standard temperature and pressure\" are considered too specific to qualify as scientific laws. A central problem in the philosophy of science, going back to David Hume, is that of distinguishing causal relationships (such as those implied by laws) from principles that arise due to constant conjunction.\n\nLaws differ from scientific theories in that they do not posit a mechanism or explanation of phenomena: they are merely distillations of the results of repeated observation. As such, a law is limited in applicability to circumstances resembling those already observed, and may be found false when extrapolated. Ohm's law only applies to linear networks, Newton's law of universal gravitation only applies in weak gravitational fields, the early laws of aerodynamics such as Bernoulli's principle do not apply in case of compressible flow such as occurs in transonic and supersonic flight, Hooke's law only applies to strain below the elastic limit, etc. These laws remain useful, but only under the conditions where they apply.\n\nMany laws take mathematical forms, and thus can be stated as an equation; for example, the law of conservation of energy can be written as formula_1, where E is the total amount of energy in the universe. Similarly, the first law of thermodynamics can be written as formula_2.\n\nThe term \"scientific law\" is traditionally associated with the natural sciences, though the social sciences also contain laws. An example of a scientific law in social sciences is Zipf's law.\n\nLike theories and hypotheses, laws make predictions (specifically, they predict that new observations will conform to the law), and can be falsified if they are found in contradiction with new data.\n\nMost significant laws in science are conservation laws. These fundamental laws follow from homogeneity of space, time and phase, in other words \"symmetry\".\n\n\nConservation laws can be expressed using the general continuity equation (for a conserved quantity) can be written in differential form as:\n\nwhere ρ is some quantity per unit volume, J is the flux of that quantity (change in quantity per unit time per unit area). Intuitively, the divergence (denoted ∇•) of a vector field is a measure of flux diverging radially outwards from a point, so the negative is the amount piling up at a point, hence the rate of change of density in a region of space must be the amount of flux leaving or collecting in some region (see main article for details). In the table below, the fluxes, flows for various physical quantities in transport, and their associated continuity equations, are collected for comparison.\n\nMore general equations are the convection–diffusion equation and Boltzmann transport equation, which have their roots in the continuity equation.\n\nAll of classical mechanics, including Newton's laws, Lagrange's equations, Hamilton's equations, etc., can be derived from this very simple principle:\n\nwhere formula_5 is the action; the integral of the Lagrangian\n\nof the physical system between two times \"t\" and \"t\". The kinetic energy of the system is \"T\" (a function of the rate of change of the configuration of the system), and potential energy is \"V\" (a function of the configuration and its rate of change). The configuration of a system which has \"N\" degrees of freedom is defined by generalized coordinates q = (\"q\", \"q\", ... \"q\").\n\nThere are generalized momenta conjugate to these coordinates, p = (\"p\", \"p\", ..., \"p\"), where:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the generalized coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nThe action is a \"functional\" rather than a \"function\", since it depends on the Lagrangian, and the Lagrangian depends on the path q(\"t\"), so the action depends on the \"entire\" \"shape\" of the path for all times (in the time interval from \"t\" to \"t\"). Between two instants of time, there are infinitely many paths, but one for which the action is stationary (to the first order) is the true path. The stationary value for the \"entire continuum\" of Lagrangian values corresponding to some path, \"not just one value\" of the Lagrangian, is required (in other words it is \"not\" as simple as \"differentiating a function and setting it to zero, then solving the equations to find the points of maxima and minima etc\", rather this idea is applied to the entire \"shape\" of the function, see calculus of variations for more details on this procedure).\n\nNotice \"L\" is \"not\" the total energy \"E\" of the system due to the difference, rather than the sum:\n\nThe following general approaches to classical mechanics are summarized below in the order of establishment. They are equivalent formulations, Newton's is very commonly used due to simplicity, but Hamilton's and Lagrange's equations are more general, and their range can extend into other branches of physics with suitable modifications.\n\nFrom the above, any equation of motion in classical mechanics can be derived.\n\n\n\n\nEquations describing fluid flow in various situations can be derived, using the above classical equations of motion and often conservation of mass, energy and momentum. Some elementary examples follow.\n\n\n\nPostulates of special relativity are not \"laws\" in themselves, but assumptions of their nature in terms of \"relative motion\".\n\nOften two are stated as \"the laws of physics are the same in all inertial frames\" and \"the speed of light is constant\". However the second is redundant, since the speed of light is predicted by Maxwell's equations. Essentially there is only one.\n\nThe said posulate leads to the Lorentz transformations – the transformation law between two frame of references moving relative to each other. For any 4-vector\n\nthis replaces the Galilean transformation law from classical mechanics. The Lorentz transformations reduce to the Galilean transformations for low velocities much less than the speed of light \"c\".\n\nThe magnitudes of 4-vectors are invariants - \"not\" \"conserved\", but the same for all inertial frames (i.e. every observer in an inertial frame will agree on the same value), in particular if \"A\" is the four-momentum, the magnitude can derive the famous invariant equation for mass-energy and momentum conservation (see invariant mass):\n\nin which the (more famous) mass-energy equivalence \"E\" = \"mc\" is a special case.\n\n\nGeneral relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass-energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated.\n\n\nIn a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous \"gravitomagnetic field\". They are well established by the theory, and experimental tests form ongoing research.\n\nThese equations can be modified to include magnetic monopoles, and are consistent with our observations of monopoles either existing or not existing; if they do not exist, the generalized equations reduce to the ones above, if they do, the equations become fully symmetric in electric and magnetic charges and currents. Indeed, there is a duality transformation where electric and magnetic charges can be \"rotated into one another\", and still satisfy Maxwell's equations.\n\n\nThese laws were found before the formulation of Maxwell's equations. They are not fundamental, since they can be derived from Maxwell's Equations. Coulomb's Law can be found from Gauss' Law (electrostatic form) and the Biot–Savart Law can be deduced from Ampere's Law (magnetostatic form). Lenz' Law and Faraday's Law can be incorporated into the Maxwell-Faraday equation. Nonetheless they are still very effective for simple calculations.\n\n\n\n\nClassically, optics is based on a variational principle: light travels from one point in space to another in the shortest time.\n\n\nIn geometric optics laws are based on approximations in Euclidean geometry (such as the paraxial approximation).\n\n\nIn physical optics, laws are based on physical properties of materials.\n\n\nIn actuality, optical properties of matter are significantly more complex and require quantum mechanics.\n\nQuantum mechanics has its roots in postulates. This leads to results which are not usually called \"laws\", but hold the same status, in that all of quantum mechanics follows from them.\n\nOne postulate that a particle (or a system of many particles) is described by a wavefunction, and this satisfies a quantum wave equation: namely the Schrödinger equation (which can be written as a non-relativistic wave equation, or a relativistic wave equation). Solving this wave equation predicts the time-evolution of the system's behaviour, analogous to solving Newton's laws in classical mechanics.\n\nOther postulates change the idea of physical observables; using quantum operators; some measurements can't be made at the same instant of time (Uncertainty principles), particles are fundamentally indistinguishable. Another postulate; the wavefunction collapse postulate, counters the usual idea of a measurement in science.\n\nApplying electromagnetism, thermodynamics, and quantum mechanics, to atoms and molecules, some laws of electromagnetic radiation and light are as follows.\n\n\nChemical laws are those laws of nature relevant to chemistry. Historically, observations led to many empirical laws, though now it is known that chemistry has its foundations in quantum mechanics.\n\n\nThe most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics.\n\nAdditional laws of chemistry elaborate on the law of conservation of mass. Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important.\n\nDalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers (i.e. 1:2 for Oxygen:Hydrogen ratio in water); although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction.\n\nMore modern laws of chemistry define the relationship between energy and its transformations.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2907387", "url": "https://en.wikipedia.org/wiki?curid=2907387", "title": "Signed zero", "text": "Signed zero\n\nSigned zero is zero with an associated sign. In ordinary arithmetic, the number 0 does not have a sign, so that −0, +0 and 0 are identical. However, in computing, some number representations allow for the existence of two zeros, often denoted by −0 (negative zero) and +0 (positive zero), regarded as equal by the numerical comparison operations but with possible different behaviors in particular operations. This occurs in the \"sign and magnitude\" and \"ones' complement\" signed number representations for integers, and in most floating-point number representations. The number 0 is usually encoded as +0, but can be represented by either +0 or −0.\n\nThe IEEE 754 standard for floating-point arithmetic (presently used by most computers and programming languages that support floating point numbers) requires both +0 and −0. Real arithmetic with signed zeros can be considered a variant of the extended real number line such that 1/−0 = −∞ and 1/+0 = +∞; division is only undefined for ±0/±0 and ±∞/±∞.\n\nNegatively signed zero echoes the mathematical analysis concept of approaching 0 from below as a one-sided limit, which may be denoted by \"x\" → 0, \"x\" → 0−, or \"x\" → ↑0. The notation \"−0\" may be used informally to denote a small negative number that has been rounded to zero. The concept of negative zero also has some theoretical applications in statistical mechanics and other disciplines.\n\nIt is claimed that the inclusion of signed zero in IEEE 754 makes it much easier to achieve numerical accuracy in some critical problems, in particular when computing with complex elementary functions. On the other hand, the concept of signed zero runs contrary to the general assumption made in most mathematical fields that negative zero is the same thing as zero. Representations that allow negative zero can be a source of errors in programs, if software developers do not take into account that while the two zero representations behave as equal under numeric comparisons, they yield different results in some operations.\n\nThe widely used two's complement encoding does not allow a negative zero. In a 1+7-bit sign-and-magnitude representation for integers, negative zero is represented by the bit string . In an 8-bit one's complement representation, negative zero is represented by the bit string . In all three encodings, positive zero is represented by .\nIn IEEE 754 binary floating point numbers, zero values are represented by the biased exponent and significand both being zero. Negative zero has the sign bit set to one. One may obtain negative zero as the result of certain computations, for instance as the result of arithmetic underflow on a negative number, or codice_1, or simply as codice_2.\n\nIn IEEE 754 decimal floating point encoding, a negative zero is represented by an exponent being any valid exponent in the range for the encoding, the true significand being zero, and the sign bit being one.\n\nThe IEEE 754 floating point standard specifies the behavior of positive zero and negative zero under various operations. The outcome may depend on the current IEEE rounding mode settings.\n\nIn systems that include both signed and unsigned zeros, the notation formula_1 and formula_2 is sometimes used for signed zeros.\n\nAddition and multiplication are commutative, but there are some special rules that have to be followed, which mean the usual mathematical rules for algebraic simplification may not apply. The formula_3 sign below shows the signed result of the operations.\n\nThe usual rule for signs is always followed when multiplying or dividing:\n\n\nThere are special rules for adding or subtracting signed zero:\n\n\nBecause of negative zero (and also when the rounding mode is upward or downward), the expressions and , for floating-point variables \"x\" and \"y\", cannot be replaced by . However can be replaced by \"x\" with rounding to nearest (except when \"x\" can be a signaling NaN).\n\nSome other special rules:\n\n\nDivision of a non-zero number by zero sets the divide by zero flag, and an operation producing a NaN sets the invalid operation flag. An exception handler is called if enabled for the corresponding flag.\n\nAccording to the IEEE 754 standard, negative zero and positive zero should compare as equal with the usual (numerical) comparison operators, like the codice_3 operators of C and Java. In those languages, special programming tricks may be needed to distinguish the two values:\n\n\nNote: Casting to integral type will not always work, especially on two's complement systems.\n\nHowever, some programming languages may provide alternative comparison operators that do distinguish the two zeros. This is the case, for example, of the equals method in Java's codice_6 wrapper class.\n\nInformally, one may use the notation \"−0\" for a negative value that was rounded to zero. This notation may be useful when a negative sign is significant; for example, when tabulating Celsius temperatures, where a negative sign means \"below freezing\".\n\nIn statistical mechanics, one sometimes uses negative temperatures to describe systems with population inversion, which can be considered to have a temperature greater than positive infinity, because the coefficient of energy in the population distribution function is −1/Temperature. In this context, a temperature of −0 is a (theoretical) temperature larger than any other negative temperature, corresponding to the (theoretical) maximum conceivable extent of population inversion, the opposite extreme to +0.\n\n\n"}
{"id": "37673", "url": "https://en.wikipedia.org/wiki?curid=37673", "title": "Symbol", "text": "Symbol\n\nA symbol is a mark, sign or word that indicates, signifies, or is understood as representing an idea, object, or relationship. Symbols allow people to go beyond what is known or seen by creating linkages between otherwise very different concepts and experiences. All communication (and data processing) is achieved through the use of symbols. Symbols take the form of words, sounds, gestures, ideas or visual images and are used to convey other ideas and beliefs. For example, a red octagon may be a symbol for \"STOP\". On a map, a blue line might represent a river. Numerals are symbols for numbers. Alphabetic letters may be symbols for sounds. Personal names are symbols representing individuals. A red rose may symbolize love and compassion. The variable 'x', in a mathematical equation, may symbolize the position of a particle in space.\n\nIn cartography, an organized collection of symbols forms a legend for a map.\n\nThe word \"symbol\" derives from the Greek σύμβολον \"symbolon\", meaning \"token, watchword\" from σύν \"syn\" \"together\" and βάλλω \"bállō\" \" \"I throw, put.\" The sense evolution in Greek is from \"throwing things together\" to \"contrasting\" to \"comparing\" to \"token used in comparisons to determine if something is genuine.\" Hence, \"outward sign\" of something. The meaning \"something which stands for something else\" was first recorded in 1590, in Edmund Spenser's \"Faerie Queene\".\n\nIn considering the effect of a symbol on the psyche, in his seminal essay \"The Symbol without Meaning\" Joseph Campbell proposes the following definition:\n\"A symbol is an energy evoking, and directing, agent\".\n\nLater, expanding on what he means by this definition Campbell says:\n\nJared Elisha defined symbolism that is something that stands for another, it can be place, object, or a person\n\nHeinrich Zimmer gives a concise overview of the nature, and perennial relevance, of symbols.\n\nIn the book \"Signs and Symbols, \"it is stated that \"A symbol ... is a visual image or sign representing an idea -- a deeper indicator of a universal truth.\"\n\nSymbols are a means of complex communication that often can have multiple levels of meaning. This separates symbols from signs, as signs have only one meaning.\n\nHuman cultures use symbols to express specific ideologies and social structures and to represent aspects of their specific culture. Thus, symbols carry meanings that depend upon one’s cultural background; in other words, the meaning of a symbol is not inherent in the symbol itself but is culturally learned.\n\nSymbols are the basis of all human understanding and serve as vehicles of conception for all human knowledge. Symbols facilitate understanding of the world in which we live, thus serving as the grounds upon which we make judgments. In this way, people use symbols not only to make sense of the world around them, but also to identify and cooperate in society through constitutive rhetoric.\n\nSemiotics is the study of signs, symbols, and signification as communicative behavior. Semiotics studies focus on the relationship of the signifier and the signified, also taking into account interpretation of visual cues, body language, sound, and other contextual clues. Semiotics is linked with both linguistics and psychology. Semioticians thus not only study what a symbol implies, but also how it got its meaning and how it functions to make meaning in society. Symbols allow the human brain continuously to create meaning using sensory input and decode symbols through both denotation and connotation.\n\nSwiss psychoanalyst Carl Jung, who studied archetypes, proposed an alternative definition of symbol, distinguishing it from the term \"sign\". In Jung's view, a sign stands for something known, as a word stands for its referent. He contrasted this with \"symbol\", which he used to stand for something that is unknown and that cannot be made clear or precise. An example of a symbol in this sense is Christ as a symbol of the archetype called \"self\". For example, written languages are composed of a variety of different symbols that create words, p. . Through these written words humans communicate with each other. Kenneth Burke described \"Homo sapiens\" as a \"symbol-using, symbol making, and symbol misusing animal\" to suggest that a person creates symbols as well as misuses them. One example he uses to indicate what he means by the misuse of symbol is the story of a man who, when told that a particular food item was whale blubber, could barely keep from throwing it up. Later, his friend discovered it was actually just a dumpling. But the man's reaction was a direct consequence of the symbol of \"blubber\" representing something inedible in his mind. In addition, the symbol of \"blubber\" was created by the man through various kinds of learning.\n\nBurke goes on to describe symbols as also being derived from Sigmund Freud's work on condensation and displacement, further stating that symbols are not just relevant to the theory of dreams but also to \"normal symbol systems\". He says they are related through \"substitution\", where one word, phrase, or symbol is substituted for another in order to change the meaning. In other words, if one person does not understand a certain word or phrase, another person may substitute a synonym or symbol in order to get the meaning across. However, upon learning the new way of interpreting a specific symbol, the person may change his or her already-formed ideas to incorporate the new information.\n\nJean Dalby Clift says that people not only add their own interpretations to symbols, they also create personal symbols that represent their own understanding of their lives: what she calls \"core images\" of the person. She argues that symbolic work with these personal symbols or core images can be as useful as working with dream symbols in psychoanalysis or counseling.\n\nWilliam Indick suggests that the symbols that are commonly found in myth, legend, and fantasy fulfil psychological functions and hence are why archetypes such as \"the hero,\" \"the princess\" and \"the witch\" have remained popular for centuries.\n\nPaul Tillich argued that, while signs are invented and forgotten, symbols are born and die. There are, therefore, dead and living symbols. A living symbol can reveal to an individual hidden levels of meaning and transcendent or religious realities. For Tillich a symbol always \"points beyond itself\" to something that is unquantifiable and mysterious; symbols open up the \"depth dimension of reality itself\". Symbols are complex, and their meanings can evolve as the individual or culture evolves. When a symbol loses its meaning and power for an individual or culture, it becomes a dead symbol.\nWhen a symbol becomes identified with the deeper reality to which it refers, it becomes idolatrous as the \"symbol is taken for reality.\" The symbol itself is substituted for the deeper meaning it intends to convey. The unique nature of a symbol is that it gives access to deeper layers of reality which are otherwise inaccessible.\n\nA symbol's meaning may be modified by various factors including popular usage, history, and contextual intent.\n\nThe history of a symbol is one of many factors in determining a particular symbol's apparent meaning. Consequently, symbols with emotive power carry problems analogous to false etymologies.\n\nThe context of a symbol may change its meaning. Similar five-pointed stars might signify a law enforcement officer or a member of the armed services, depending upon the uniform.\n\nSymbols are used in cartography to communicate geographical information (generally as point, line, or area features). As with other symbols, visual variables such as size, shape, orientation, texture, and pattern provide meaning to the symbol.\n\nThe form, or shape, of a cartographic symbol is classified into one of three main groups:\n\nPictorial/Representational - a shape or image that clearly resembles the geographic feature being symbolized and can be interpreted without a legend.\n\nAssociative - a mixture of pictorial and geometric elements that produce an easily recognizable shape.\n\nAbstract/Geometric - completely arbitrary shapes chosen to represent a certain feature.\n\nA symbolic action is an action that has no, or little, practical effect but symbolizes, or signals, what the actor wants or believes. The action conveys meaning to the viewers.\n\nSymbolic action may overlap with symbolic speech, such as the use of flag burning to express hostility or saluting the flag to express patriotism.\n\nIn response to intense public criticism, businesses, organizations, and governments may take symbolic actions rather than, or in addition to, directly addressing the identified problems.\n\nSymbolic actions are sometimes derided as slacktivism.\n\n\n"}
{"id": "49605051", "url": "https://en.wikipedia.org/wiki?curid=49605051", "title": "The Circumplex Model of Group Tasks", "text": "The Circumplex Model of Group Tasks\n\nThe Circumplex Model is a graphical representation of emotional states. Fundamentally, it is a circle with pleasant on the left, unpleasant on the right, activation on the top, and deactivation on the bottom. All the other emotions are placed around the circle as combinations of these four basic states. It is based on the theory that people experience emotions as overlapping and ambiguous. Group dynamics are the distinctive behaviors and attitudes observed by people in groups, and the study thereof. It is of most interest in the business world, the workforce, or any other setting where the performance of a group is important. Joseph E McGrath enlarged the circumplex model to include group dynamics, based on the work of Shaw, Carter, Hackman, Steiner, Shiflett, Taylor, Lorge, Davis, Laughlin, and others. There are four quadrants in this model representing: generating a task, choosing correct procedure, conflict resolution, and execution, and again there are subtypes distributed around the circle. He used this model as a research tool to evaluate group task performance.\n\nGroup dynamics involve the influential actions, processes and changes that exist both within and between groups. Group dynamics also involve the scientific study of group processes. Through extensive research in the field of group dynamics, it is now well known that all groups, despite their innumerable differences, possess common properties and dynamics. Social psychological researchers have attempted to organize these commonalities, in order to further understand the genuine nature of group processes.\n\nFor instance, social psychological research indicates that there are numerous goal-related interactions and activities that groups of all sizes undertake . These interactions have been categorized by Robert F. Bales, who spent his entire life attempting to find an answer to the question, \"What do people do when they are in groups?\". To simplify the understanding of group interactions, Bales concluded that all interactions within groups could be categorized as either a \"relationship interaction\" (or socioemotional interaction) or a \"task interaction\".\n\nJust as Bales was determined to identify the basic types of interactions involved in groups, Joseph E. McGrath was determined to identify the various goal-related activities that are regularly displayed by groups. McGrath contributed greatly to the understanding of group dynamics through the development of his circumplex model of group tasks. As intended, McGrath's model effectively organizes all group-related activities by distinguishing between four basic group goals. These goals are referred to as the circumplex model of group task's four quadrants, which are categorized based on the dominant performance process involved in a group's task of interest.\n\nThe four quadrants are as follows: \n\nTo further differentiate the various goal-related group activities, McGrath further sub-divides these four categories, resulting in eight categories in total. The breakdown of these categories is as follows:\n\n1. \"Generating ideas or plans\"\n2. \"Choosing a solution\"\n3. \"Negotiating a solution to a conflict\" \n4. \"Executing a task\" \n\nAccording to McGrath and Kravitz (1982), the four most commonly represented tasks in the group dynamics literature are intellective tasks, decision-making tasks, cognitive conflict tasks and mixed-motive tasks.\n\nThe circumplex model of group tasks takes the organization of goal-related activities a step further by distinguishing between tasks that involve cooperation between group members, cooperation tasks (Types 1, 2, 3 and 8) and tasks that often lead to conflict between group members, conflict tasks (Types 4, 5, 6 and 7). Additionally, McGrath's circumplex model of group tasks also distinguishes between tasks that require action (behavioural tasks) and tasks that require conceptual review (conceptual tasks). 'Behavioural tasks' include Types 1, 6, 7 and 8, while 'conceptual tasks' include Types 2, 3, 4 and 5.\n\nThe circumplex model of group tasks is, evidently, a very detailed and complex model. To allow for a more thorough understanding of its properties, a visual representation of the model has been developed. (Need a diagram of the model)\n\nSince the circumplex model of group tasks is quite detailed and complex, numerous social psychological researchers have attempted to describe the model in various ways to ensure readers obtain an optimal understanding of the model. For instance, according to Stratus and McGrath (1994), the four quadrants and the various task types with which they contain all relate to one another within a two-dimensional space. More specifically, Stratus and McGrath (1994) states that the horizontal dimension of the circumplex model of group tasks visual representation reflect the extent to which a task entails cognitive versus behavioural performance requirements. Likewise, the vertical dimension of the circumplex model of group tasks visual representation reflects the extent and form of interdependence among members.\n"}
