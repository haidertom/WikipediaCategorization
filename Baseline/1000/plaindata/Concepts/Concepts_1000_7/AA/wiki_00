{"id": "6679056", "url": "https://en.wikipedia.org/wiki?curid=6679056", "title": "A priori and a posteriori", "text": "A priori and a posteriori\n\nThe Latin phrases a priori ( \"from the earlier\") and a posteriori ( \"from the later\") are philosophical terms of art popularized by Immanuel Kant's \"Critique of Pure Reason\" (first published in 1781, second edition in 1787), one of the most influential works in the history of philosophy. However, in their Latin forms they appear in Latin translations of Euclid's \"Elements\", of about 300 , a work widely considered during the early European modern period as the model for precise thinking.\n\nThese terms are used with respect to reasoning (epistemology) to distinguish \"necessary conclusions from first premises\" (i.e., what must come before sense observation) from \"conclusions based on sense observation\" which must follow it. Thus, the two kinds of knowledge, justification, or argument, may be glossed:\n\n\nThere are many points of view on these two types of knowledge, and their relationship gives rise to one of the oldest problems in modern philosophy.\n\nThe terms \"a priori\" and \"a posteriori\" are primarily used as adjectives to modify the noun \"knowledge\" (for example, \"\"a priori\" knowledge\"). However, \"a priori\" is sometimes used to modify other nouns, such as \"truth\". Philosophers also may use \"apriority\" and \"aprioricity\" as nouns to refer (approximately) to the quality of being \"a priori\".\n\nAlthough definitions and use of the terms have varied in the history of philosophy, they have consistently labeled two separate epistemological notions. See also the related distinctions: deductive/inductive, analytic/synthetic, necessary/contingent.\n\nThe intuitive distinction between \"a priori\" and \"a posteriori\" knowledge (or justification) is best seen via examples, as below:\n\n\nSeveral philosophers reacting to Kant sought to explain \"a priori\" knowledge without appealing to, as Paul Boghossian (MD) explains, \"a special faculty ... that has never been described in satisfactory terms.\" One theory, popular among the logical positivists of the early 20th century, is what Boghossian calls the \"analytic explanation of the a priori.\" The distinction between analytic and synthetic propositions was first introduced by Kant. While Kant's original distinction was primarily drawn in terms of conceptual containment, the contemporary version of the distinction primarily involves, as the American philosopher W. V. O. Quine put it, the notions of \"true by virtue of meanings and independently of fact.\" \"Analytic\" propositions are thought to be true in virtue of their meaning alone, while \"a posteriori analytic\" propositions are thought to be true in virtue of their meaning \"and\" certain facts about the world. According to the analytic explanation of the \"a priori\", all \"a priori\" knowledge is analytic; so \"a priori\" knowledge need not require a special faculty of pure intuition, since it can be accounted for simply by one's ability to understand the meaning of the proposition in question. In short, proponents of this explanation claimed to have reduced a dubious metaphysical faculty of pure reason to a legitimate linguistic notion of analyticity.\n\nHowever, the analytic explanation of \"a priori\" knowledge has undergone several criticisms. Most notably, Quine argued that the analytic–synthetic distinction is illegitimate. Quine states: \"But for all its a priori reasonableness, a boundary between analytic and synthetic statements simply has not been drawn. That there is such a distinction to be drawn at all is an unempirical dogma of empiricists, a metaphysical article of faith.\" While the soundness of Quine's critique is highly disputed, it had a powerful effect on the project of explaining the \"a priori\" in terms of the analytic.\n\nThe metaphysical distinction between necessary and contingent truths has also been related to \"a priori\" and \"a posteriori\" knowledge. A proposition that is \"necessarily true\" is one whose negation is self-contradictory (thus, it is said to be true in every possible world). Consider the proposition that all bachelors are unmarried. Its negation, the proposition that some bachelors are married, is incoherent, because the concept of being unmarried (or the meaning of the word \"unmarried\") is part of the concept of being a bachelor (or part of the definition of the word \"bachelor\"). To the extent that contradictions are impossible, self-contradictory propositions are necessarily false, because it is impossible for them to be true. Thus, the negation of a self-contradictory proposition is supposed to be necessarily true. By contrast, a proposition that is \"contingently true\" is one whose negation is not self-contradictory (thus, it is said that it is \"not\" true in every possible world). As Jason Baehr states, it seems plausible that all necessary propositions are known \"a priori\", because \"[s]ense experience can tell us only about the actual world and hence about what is the case; it can say nothing about what must or must not be the case.\"\n\nFollowing Kant, some philosophers have considered the relationship between aprioricity, analyticity, and necessity to be extremely close. According to Jerry Fodor, \"Positivism, in particular, took it for granted that \"a priori\" truths must be necessary...\" However, since Kant, the distinction between analytic and synthetic propositions had slightly changed. Analytic propositions were largely taken to be \"true by virtue of meanings and independently of fact\", while synthetic propositions were not—one must conduct some sort of empirical investigation, looking to the world, to determine the truth-value of synthetic propositions.\n\nAprioricity, analyticity, and necessity have since been more clearly separated from each other. The American philosopher Saul Kripke (1972), for example, provided strong arguments against this position. Kripke argued that there are necessary \"a posteriori\" truths, such as the proposition that water is HO (if it is true). According to Kripke, this statement is necessarily true (since water and HO are the same thing, they are identical in every possible world, and truths of identity are logically necessary) and \"a posteriori\" (since it is known only through empirical investigation). Following such considerations of Kripke and others (such as Hilary Putnam), philosophers tend to distinguish more clearly the notion of aprioricity from that of necessity and analyticity.\n\nKripke's definitions of these terms, however, diverge in subtle ways from those of Kant. Taking these differences into account, Kripke's controversial analysis of naming as contingent and \"a priori\" would, according to Stephen Palmquist, best fit into Kant's epistemological framework by calling it \"analytic a posteriori\". Aaron Sloman presented a brief defence of Kant's three distinctions (analytic/synthetic, apriori/empirical and necessary/contingent) in . It did not assume \"possible world semantics\" for the third distinction, merely that some part of \"this\" world might have been different.\n\nThus, the relationship between aprioricity, necessity, and analyticity is not easy to discern. However, most philosophers at least seem to agree that while the various distinctions may overlap, the notions are clearly not identical: the \"a priori\"/\"a posteriori\" distinction is epistemological, the analytic/synthetic distinction is linguistic, and the necessary/contingent distinction is metaphysical.\n\nThe phrases \"\"a priori\" and \"a posteriori\"\" are Latin for \"from what comes before\" and \"from what comes later\" (or, less literally, \"from first principles, before experience\" and \"after experience\"). They appear in Latin translations of Euclid's \"Elements\", of about 300 , a work widely considered during the early European modern period as the model for precise thinking.\n\nAn early philosophical use of what might be considered a notion of \"a priori\" knowledge (though not called by that name) is Plato's theory of recollection, related in the dialogue \"Meno\" (380 ), according to which something like \"a priori\" knowledge is knowledge inherent, intrinsic in the human mind.\n\nAlbert of Saxony, a 14th-century logician, wrote on both \"a priori\" and \"a posteriori\".\n\nG. W. Leibniz introduced a distinction between \"a priori\" and \"a posteriori\" criteria for the possibility of a notion in his (1684) short treatise \"Meditations on Knowledge, Truth, and Ideas\". \"A priori\" and \"a posteriori\" arguments for the existence of God appear in his \"Monadology\" (1714).\n\nGeorge Berkeley outlined the distinction in his 1710 work \"A Treatise Concerning the Principles of Human Knowledge\" (para. XXI).\n\nThe 18th-century German philosopher Immanuel Kant (1781) advocated a blend of rationalist and empiricist theories. Kant says, \"Although all our cognition begins with experience, it does not follow that it arises [is caused by] from experience\" According to Kant, \"a priori\" cognition is transcendental, or based on the \"form\" of all possible experience, while \"a posteriori\" cognition is empirical, based on the \"content\" of experience. Kant states, \"[…] it is quite possible that our empirical knowledge is a compound of that which we receive through impressions, and that which the faculty of cognition supplies from itself sensuous impressions [sense data] giving merely the \"occasion\" [opportunity for a cause to produce its effect].\" Contrary to contemporary usages of the term, Kant thinks that \"a priori\" knowledge is not entirely independent of the content of experience. And unlike the rationalists, Kant thinks that \"a priori\" cognition, in its pure form, that is without the admixture of any empirical content, is limited to the deduction of the conditions of possible experience. These \"a priori\", or transcendental conditions, are seated in one's cognitive faculties, and are not provided by experience in general or any experience in particular (although an argument exists that \"a priori\" intuitions can be \"triggered\" by experience). \n\nKant nominated and explored the possibility of a transcendental logic with which to consider the deduction of the \"a priori\" in its pure form. Space, time and causality are considered pure \"a priori\" intuitions. Kant reasoned that the pure \"a priori\" intuitions are established via his transcendental aesthetic and transcendental logic. He claimed that the human subject would not have the kind of experience that it has were these \"a priori\" forms not in some way constitutive of him as a human subject. For instance, a person would not experience the world as an orderly, rule-governed place unless time, space and causality were determinant functions in the form of perceptual faculties, i. e., there can be no experience in general without space, time or causality as particular determinants thereon. The claim is more formally known as Kant's transcendental deduction and it is the central argument of his major work, the \"Critique of Pure Reason\". The transcendental deduction argues that time, space and causality are ideal as much as real. In consideration of a possible logic of the \"a priori\", this most famous of Kant's deductions has made the successful attempt in the case for the fact of subjectivity, what constitutes subjectivity and what relation it holds with objectivity and the empirical.\n\nAfter Kant's death, a number of philosophers saw themselves as correcting and expanding his philosophy, leading to the various forms of German Idealism. One of these philosophers was Johann Fichte. His student (and critic), Arthur Schopenhauer, accused him of rejecting the distinction between \"a priori\" and \"a posteriori\" knowledge:\n\n\n\n\n"}
{"id": "2235265", "url": "https://en.wikipedia.org/wiki?curid=2235265", "title": "Anna Karenina principle", "text": "Anna Karenina principle\n\nThe Anna Karenina principle states that a deficiency in any one of a number of factors dooms an endeavor to failure. Consequently, a successful endeavor (subject to this principle) is one where every possible deficiency has been avoided.\n\nThe name of the principle derives from Leo Tolstoy's book \"Anna Karenina\", which begins:\n\nAll happy families are alike; each unhappy family is unhappy in its own way.In other words: in order to be happy, a family must be successful on \"each and every one\" of \"a\" \"range\" of criteria e.g.: sexual attraction, money issues, parenting, religion, in-laws. Failure on only \"one\" of these counts leads to \"un\"happiness. Thus there are more ways for a family to be unhappy than happy.\n\nIn statistics, the term \"Anna Karenina principle\" is used to describe significance tests: there are any number of ways in which a dataset may violate the null hypothesis and only one in which all the assumptions are satisfied.\n\nThe Anna Karenina principle was popularized by Jared Diamond in his book \"Guns, Germs and Steel\". Diamond uses this principle to illustrate why so few wild animals have been successfully domesticated throughout history, as a deficiency in any one of a great number of factors can render a species undomesticable. Therefore, all successfully domesticated species are not so because of a particular positive trait, but because of a lack of any number of possible negative traits. In chapter 9, six groups of reasons for failed domestication of animals are defined:\n\n\nMoore describes applications of the \"Anna Karenina principle\" in ecology:\n\nSuccessful ecological risk assessments are all alike; every unsuccessful ecological risk assessment fails in its own way. Tolstoy posited a similar analogy in his novel Anna Karenina : \"Happy families are all alike; every unhappy family is unhappy in its own way.\" By that, Tolstoy meant that for a marriage to be happy, it had to succeed in several key aspects. Failure on even one of these aspects, and the marriage is doomed . . . the Anna Karenina principle also applies to ecological risk assessments involving multiple stressors.\n\nMuch earlier, \"Aristotle\" states the same principle in the \"Nicomachean Ethics\" (Book 2):\n\nAgain, it is possible to fail in many ways (for evil belongs to the class of the unlimited, as the Pythagoreans conjectured, and good to that of the limited), while to succeed is possible only in one way (for which reason also one is easy and the other difficult – to miss the mark easy, to hit it difficult); for these reasons also, then, excess and defect are characteristic of vice, and the mean of virtue; For men are good in but one way, but bad in many.\n\nMany experiments and observations of groups of humans, animals, trees, grassy plants, stockmarket prices, and changes in the banking sector proved the modified Anna Karenina principle.\n\nBy studying the dynamics of correlation and variance in many systems facing external, or environmental, factors, we can typically, even before obvious symptoms of crisis appear, predict when one might occur, as correlation between individuals increases, and, at the same time, variance (and volatility) goes up... All well-adapted systems are alike, all non-adapted systems experience maladaptation in their own way... But in the chaos of maladaptation, there is an order. It seems, paradoxically, that as systems become more different they actually become more correlated within limits.\n\nThis effect is proved for many systems: from the adaptation of healthy people to a change in climate conditions to the analysis of fatal outcomes in oncological and cardiological clinics. The same effect is found in the stock market. The applicability of these two statistical indicators of stress, simultaneous increase of variance and correlations, for diagnosis of social stress in large groups was examined in the prolonged stress period preceding the 2014 Ukrainian economic and political crisis. There was a simultaneous increase in the total correlation between the 19 major public fears in the Ukrainian society (by about 64%) and also in their statistical dispersion (by 29%) during the pre-crisis years.\n\nVladimir Arnold in his book \"Catastrophe Theory\" describes \"The Principle of Fragility of Good Things\" which in a sense supplements the Principle of Anna Karenina: good systems must meet simultaneously a number of requirements; therefore, they are more fragile:\n\n... for systems belonging to the singular part of the stability boundary a small change of the parameters is more likely to send the system into the unstable region than into the stable region. This is a manifestation of a general principle stating that all good things (e.g. stability) are more fragile than bad things. It seems that in good situations a number of requirements must hold simultaneously, while to call a situation bad even one failure suffices. \n"}
{"id": "5354120", "url": "https://en.wikipedia.org/wiki?curid=5354120", "title": "Apollonian and Dionysian", "text": "Apollonian and Dionysian\n\nThe Apollonian and Dionysian is a philosophical and literary concept, or also a dichotomy, based on Apollo and Dionysus in Greek mythology. Some Western philosophical and literary figures have invoked this dichotomy in critical and creative works, most notably Friedrich Nietzsche and later followers. \n\nIn Greek mythology, Apollo and Dionysus are both sons of Zeus. Apollo is the god of the sun, of rational thinking and order, and appeals to logic, prudence and purity. Dionysus is the god of wine and dance, of irrationality and chaos, and appeals to emotions and instincts. The Ancient Greeks did not consider the two gods to be opposites or rivals, although often the two deities were entwined by nature.\n\nAlthough the use of the concepts of the Apollonian and Dionysian is linked to Nietzsche's \"The Birth of Tragedy\", the terms were used before him in German culture. The poet Hölderlin spoke of them, while Winckelmann talked of Bacchus, the god of wine. After Nietzsche, others have continued to make use of the distinction. For example, Rudolf Steiner treated in depth the Apollonian and Dionysian and placed them in the general history and spiritual evolution of mankind.\n\nNietzsche's aesthetic usage of the concepts, which was later developed philosophically, first appeared in his book \"The Birth of Tragedy\", which was published in 1872. His major premise here was that the fusion of Dionysian and Apollonian \"Kunsttriebe\" (\"artistic impulses\") form dramatic arts, or tragedies. He goes on to argue that this fusion has not been achieved since the ancient Greek tragedians. Nietzsche is adamant that the works of Aeschylus and Sophocles represent the apex of artistic creation, the true realization of tragedy; it is with Euripides that tragedy begins its downfall (\"Untergang\"). Nietzsche objects to Euripides's use of Socratic rationalism (the dialectic) in his tragedies, claiming that the infusion of ethics and reason robs tragedy of its foundation, namely the fragile balance of the Dionysian and Apollonian.\n\nTo further the split, Nietzsche diagnoses the Socratic Dialectic as being diseased in the manner that it deals with looking at life. The scholarly dialectic is directly opposed to the concept of the Dionysian because it only seeks to negate life; it uses reason to always deflect, but never to create. Socrates rejects the intrinsic value of the senses and life for \"higher\" ideals. Nietzsche claims in \"The Gay Science\" that when Socrates drinks the hemlock, he sees the hemlock as the cure for life, proclaiming that he has been sick a long time. (Section 340.) In contrast, the Dionysian existence constantly seeks to affirm life. Whether in pain or pleasure, suffering or joy, the intoxicating revelry that Dionysus has for life itself overcomes the Socratic sickness and perpetuates the growth and flourishing of visceral life force—a great Dionysian 'Yes', to a Socratic 'No'. \nThe interplay between the Apollonian and Dionysian is apparent, Nietzsche claimed in \"The Birth of Tragedy\", from their use in Greek tragedy: the tragic hero of the drama, the main protagonist, struggles to make order of his unjust fate, though he dies unfulfilled in the end. For the audience of such a drama, Nietzsche claimed, this tragedy allows them to sense an underlying essence, what he called the \"Primordial Unity\", which revives our Dionysian nature—which is almost indescribably pleasurable. However, he later dropped this concept saying it was \"...burdened with all the errors of youth\" (Attempt at Self-Criticism, §2), the overarching theme was a sort of metaphysical solace or connection with the heart of creation.\n\nDifferent from Kant's idea of the sublime, the Dionysian is all-inclusive rather than alienating to the viewer as a sublimating experience. The sublime needs critical distance, while the Dionysian demands a closeness of experience. According to Nietzsche, the critical distance, which separates man from his closest emotions, originates in Apollonian ideals, which in turn separate him from his essential connection with self. The Dionysian embraces the chaotic nature of such experience as all-important; not just on its own, but as it is intimately connected with the Apollonian. The Dionysian magnifies man, but only so far as he realizes that he is one and the same with all ordered human experience. The godlike unity of the Dionysian experience is of utmost importance in viewing the Dionysian as it is related to the Apollonian, because it emphasizes the harmony that can be found within one's chaotic experience.\n\nNietzsche's idea has been interpreted as an expression of \"fragmented consciousness\" or existential instability by a variety of modern and post-modern writers, especially Martin Heidegger, Michel Foucault and Gilles Deleuze. According to Peter Sloterdijk, the Dionysian and the Apollonian form a dialectic; they are contrasting, but Nietzsche does not mean one to be valued more than the other. Truth being \"primordial pain\", our existential being is determined by the Dionysian/Apollonian dialectic.\n\nExtending the use of the Apollonian and Dionysian onto an argument on interaction between the mind and physical environment, Abraham Akkerman has pointed to masculine and feminine features of city form.\n\nAnthropologist Ruth Benedict used the terms to characterize cultures that value restraint and modesty (Apollonian) and ostentatiousness and excess (Dionysian). An example of an Apollonian culture in Benedict's analysis was the Zuñi people as opposed to the Dionysian Kwakiutl people. The theme was developed by Benedict in her main work \"Patterns of Culture\".\n\nAlbert Szent-Györgyi, who wrote that \"a discovery must be, by definition, at variance with existing knowledge\", divided scientists into two categories: the Apollonians and the Dionysians. He called scientific dissenters, who explored \"the fringes of knowledge\", Dionysians. He wrote, \"In science the Apollonian tends to develop established lines to perfection, while the Dionysian rather relies on intuition and is more likely to open new, unexpected alleys for research...The future of mankind depends on the progress of science, and the progress of science depends on the support it can find. Support mostly takes the form of grants, and the present methods of distributing grants unduly favor the Apollonian.\"\n\nAmerican humanities scholar Camille Paglia writes about the Apollonian and Dionysian in her 1990 bestseller \"Sexual Personae\". The broad outline of her concept has roots in Nietzschean discourse, an admitted influence, although Paglia's ideas diverge significantly.\n\nThe Apollonian and Dionysian concepts comprise a dichotomy that serves as the basis of Paglia's theory of art and culture. For Paglia, the Apollonian is light and structured while the Dionysian is dark and chthonic (she prefers \"Chthonic\" to Dionysian throughout the book, arguing that the latter concept has become all but synonymous with hedonism and is inadequate for her purposes, declaring that \"the Dionysian is no picnic.\"). The Chthonic is associated with females, wild/chaotic nature, and unconstrained sex/procreation. In contrast, the Apollonian is associated with males, clarity, celibacy and/or homosexuality, rationality/reason, and solidity, along with the goal of oriented progress: \"Everything great in western civilization comes from struggle against our origins.\"\n\nShe argues that there is a biological basis to the Apollonian/Dionysian dichotomy, writing: \"The quarrel between Apollo and Dionysus is the quarrel between the higher cortex and the older limbic and reptilian brains.\" Moreover, Paglia attributes all the progress of human civilization to masculinity revolting against the Chthonic forces of nature, and turning instead to the Apollonian trait of ordered creation. The Dionysian is a force of chaos and destruction, which is the overpowering and alluring chaotic state of wild nature. Rejection of – or combat with – Chthonianism by socially constructed Apollonian virtues accounts for the historical dominance of men (including asexual and homosexual men; and childless and/or lesbian-leaning women) in science, literature, arts, technology and politics. As an example, Paglia states: \"The male orientation of classical Athens was inseparable from its genius. Athens became great not despite but because of its misogyny.\"\n\n"}
{"id": "28231080", "url": "https://en.wikipedia.org/wiki?curid=28231080", "title": "Archiv für Begriffsgeschichte", "text": "Archiv für Begriffsgeschichte\n\nArchiv für Begriffsgeschichte ('Archive for Conceptual History') is a German peer-reviewed academic journal. It was founded by Erich Rothacker, and is published by Christian Bermes, Ulrich Dierse and Michael Erler. The editor is Annika Hand.\n\nThe journal publishes works on concepts of the history of philosophy and sciences, both from the European and from non-European traditions, on mythological and religious concepts, and on concepts of common parlance which have a characteristic significance for a special era or culture. The journal also embraces articles on revealing metaphors, on problems at translating concepts, as well as on theory and criticism of the method of conceptual history.\n\n\n"}
{"id": "41099486", "url": "https://en.wikipedia.org/wiki?curid=41099486", "title": "Argument-deduction-proof distinctions", "text": "Argument-deduction-proof distinctions\n\nArgument-deduction-proof distinctions originated with logic itself. Naturally, the terminology evolved. \nAn argument, more fully a premise-conclusion argument, is a two-part system composed of premises and conclusion. An argument is \"valid\" if and only if its conclusion is a consequence of its premises. Every premise set has infinitely many consequences each giving rise to a valid argument. Some consequences are obviously so but most are not: most are hidden consequences. Most valid arguments are not yet known to be valid. To determine validity in non-obvious cases deductive reasoning is required. There is no deductive reasoning in an argument \"per se\"; such must come from the outside. \n\nEvery argument's conclusion is a premise of other arguments. The word \"constituent\" may be used for either a premise or conclusion.In the context of this article and in most classical contexts, all candidates for consideration as argument constituents fall under the category of truth-bearer: propositions, statements, sentences, judgments, etc.\n\nA deduction is a three-part system composed of premises, a conclusion, and chain of intermediates — steps of reasoning showing that its conclusion is a consequence of its premises. The reasoning in a deduction is by definition cogent. Such reasoning itself, or the chain of intermediates representing it, has also been called an argument, more fully a deductive argument. In many cases, an argument can be known to be valid by means of a deduction of its conclusion from its premises but non-deductive methods such as Venn diagrams and other graphic procedures have been proposed.\n\nA proof is a deduction whose premises are known truths. A proof of the Pythagorean theorem is a deduction that might use several premises — axioms, postulates, and definitions — and contain dozens of intermediate steps. As Alfred Tarski famously emphasized in accord with Aristotle, truths can be known by proof but proofs presuppose truths not known by proof.\nPremise-conclusion arguments do not require or produce either knowledge of validity or knowledge of truth. Premise sets may be chosen arbitrarily and conclusions may be chosen arbitrarily. \nDeductions require knowing how to reason but they do not require knowledge of truth of their premises. Deductions produce knowledge of the validity of arguments but ordinarily they do not produce knowledge of the truth of their conclusions.\nProofs require knowledge of the truth of their premises, they require knowledge of deductive reasoning, and they produce knowledge of their conclusions.\nModern logicians disagree concerning the nature of argument constituents.Quine devotes the first chapter of \"Philosophy of Logic\" to this issue. Historians have not even been able to agree on what Aristotle took as constituents.\nArgument-deduction-proof distinctions are inseparable from what have been called the \"consequence-deducibility\" distinction and the \"truth-and-consequence conception of proof\". Variations among argument-deduction-proof distinctions are not all terminological.\n\nLogician Alonzo Church never used the word \"argument\" in the above sense and had no synonym. Moreover, Church never explained that deduction is the process of producing knowledge of consequence and it never used the common noun \"deduction\" for an application of the deduction process. His primary focus in discussing proof was “conviction” produced by generation of chains of logical truths—not the much more widely applicable and more familiar general process of demonstration as found in pre-Aristotelian geometry and discussed by Aristotle. He did discuss deductions in the above sense but not by that name: he called them awkwardly “proofs from premises” — an expression he coined for the purpose.\n\nThe absence of argument-deduction-proof distinctions is entirely consonant with Church's avowed Platonistic logicism. Following Dummett's insightful remarks about Frege, which — \"mutatis mutandis\" — apply even more to Church, it might be possible to explain the today-surprising absence.\n"}
{"id": "5370", "url": "https://en.wikipedia.org/wiki?curid=5370", "title": "Category of being", "text": "Category of being\n\nIn ontology, the different kinds or ways of being are called categories of being; or simply categories. To investigate the categories of being is to determine the most fundamental and the broadest classes of entities. A distinction between such categories, in making the categories or applying them, is called an ontological distinction.\n\nThe process of abstraction required to discover the number and names of the categories has been undertaken by many philosophers since Aristotle and involves the careful inspection of each concept to ensure that there is no higher category or categories under which that concept could be subsumed. The scholars of the twelfth and thirteenth centuries developed Aristotle's ideas, firstly, for example by Gilbert of Poitiers, dividing Aristotle's ten categories into two sets, primary and secondary, according to whether they inhere in the subject or not:\nSecondly, following Porphyry’s likening of the classificatory hierarchy to a tree, they concluded that the major classes could be subdivided to form subclasses, for example, Substance could be divided into Genus and Species, and Quality could be subdivided into Property and Accident, depending on whether the property was necessary or contingent. An alternative line of development was taken by Plotinus in the second century who by a process of abstraction reduced Aristotle’s list of ten categories to five: Substance, Relation, Quantity, Motion and Quality. Plotinus further suggested that the latter three categories of his list, namely Quantity, Motion and Quality correspond to three different kinds of relation and that these three categories could therefore be subsumed under the category of Relation. This was to lead to the supposition that there were only two categories at the top of the hierarchical tree, namely Substance and Relation, and if relations only exist in the mind as many supposed, to the two highest categories, Mind and Matter, reflected most clearly in the dualism of René Descartes.\n\nAn alternative conclusion however began to be formulated in the eighteenth century by Immanuel Kant who realised that we can say nothing about Substance except through the relation of the subject to other things. In the sentence \"This is a house\" the substantive subject \"house\" only gains meaning in relation to human use patterns or to other similar houses. The category of Substance disappears from Kant's tables, and under the heading of Relation, Kant lists \"inter alia\" the three relationship types of Disjunction, Causality and Inherence. The three older concepts of Quantity, Motion and Quality, as Peirce discovered, could be subsumed under these three broader headings in that Quantity relates to the subject through the relation of Disjunction; Motion relates to the subject through the relation of Causality; and Quality relates to the subject through the relation of Inherence. Sets of three continued to play an important part in the nineteenth century development of the categories, most notably in G.W.F. Hegel's extensive tabulation of categories, and in C.S. Peirce's categories set out in his work on the logic of relations. One of Peirce's contributions was to call the three primary categories Firstness, Secondness and Thirdness which both emphasises their general nature, and avoids the confusion of having the same name for both the category itself and for a concept within that category.\n\nIn a separate development, and building on the notion of primary and secondary categories introduced by the Scholastics, Kant introduced the idea that secondary or \"derivative\" categories could be derived from the primary categories through the combination of one primary category with another. This would result in the formation of three secondary categories: the first, \"Community\" was an example that Kant gave of such a derivative category; the second, \"Modality\", introduced by Kant, was a term which Hegel, in developing Kant's dialectical method, showed could also be seen as a derivative category; and the third, \"Spirit\" or \"Will\" were terms that Hegel and Schopenhauer were developing separately for use in their own systems. Karl Jaspers in the twentieth century, in his development of existential categories, brought the three together, allowing for differences in terminology, as Substantiality, Communication and Will. This pattern of three primary and three secondary categories was used most notably in the nineteenth century by Peter Mark Roget to form the six headings of his Thesaurus of English Words and Phrases. The headings used were the three objective categories of Abstract Relation, Space (including Motion) and Matter and the three subjective categories of Intellect, Feeling and Volition, and he found that under these six headings all the words of the English language, and hence any possible predicate, could be assembled.\n\nIn the twentieth century the primacy of the division between the subjective and the objective, or between mind and matter, was disputed by, among others, Bertrand Russell and Gilbert Ryle. Philosophy began to move away from the metaphysics of categorisation towards the linguistic problem of trying to differentiate between, and define, the words being used. Ludwig Wittgenstein’s conclusion was that there were no clear definitions which we can give to words and categories but only a \"halo\" or \"corona\" of related meanings radiating around each term. Gilbert Ryle thought the problem could be seen in terms of dealing with \"a galaxy of ideas\" rather than a single idea, and suggested that category mistakes are made when a concept (e.g. \"university\"), understood as falling under one category (e.g. abstract idea), is used as though it falls under another (e.g. physical object). With regard to the visual analogies being used, Peirce and Lewis, just like Plotinus earlier, likened the terms of propositions to points, and the relations between the terms to lines. Peirce, taking this further, talked of univalent, bivalent and trivalent relations linking predicates to their subject and it is just the number and types of relation linking subject and predicate that determine the category into which a predicate might fall. Primary categories contain concepts where there is one dominant kind of relation to the subject. Secondary categories contain concepts where there are two dominant kinds of relation. Examples of the latter were given by Heidegger in his two propositions \"the house is on the creek\" where the two dominant relations are spatial location (Disjunction) and cultural association (Inherence), and \"the house is eighteenth century\" where the two relations are temporal location (Causality) and cultural quality (Inherence). A third example may be inferred from Kant in the proposition \"the house is impressive or sublime\" where the two relations are spatial or mathematical disposition (Disjunction) and dynamic or motive power (Causality). Both Peirce and Wittgenstein introduced the analogy of colour theory in order to illustrate the shades of meanings of words. Primary categories, like primary colours, are analytical representing the furthest we can go in terms of analysis and abstraction and include Quantity, Motion and Quality. Secondary categories, like secondary colours, are synthetic and include concepts such as Substance, Community and Spirit.\n\nOne of Aristotle’s early interests lay in the classification of the natural world, how for example the genus \"animal\" could be first divided into \"two-footed animal\" and then into \"wingless, two-footed animal\". He realised that the distinctions were being made according to the qualities the animal possesses, the quantity of its parts and the kind of motion that it exhibits. To fully complete the proposition \"this animal is…\" Aristotle stated in his work on the Categories that there were ten kinds of predicate where...\n\n\"…each signifies either substance or quantity or quality or relation or where or when or being-in-a-position or having or acting or being acted upon\".\n\nHe realised that predicates could be simple or complex. The simple kinds consist of a subject and a predicate linked together by the \"categorical\" or inherent type of relation. For Aristotle the more complex kinds were limited to propositions where the predicate is compounded of two of the above categories for example \"this is a horse running\". More complex kinds of proposition were only discovered after Aristotle by the Stoic, Chrysippus, who developed the \"hypothetical\" and \"disjunctive\" types of syllogism and these were terms which were to be developed through the Middle Ages and were to reappear in Kant's system of categories.\n\n\"Category\" came into use with Aristotle's essay \"Categories\", in which he discussed univocal and equivocal terms, predication, and ten categories:\n\nPlotinus in writing his \"Enneads\" around AD 250 recorded that \"philosophy at a very early age investigated the number and character of the existents… some found ten, others less…. to some the genera were the first principles, to others only a generic classification of existents\". He realised that some categories were reducible to others saying \"why are not Beauty, Goodness and the virtues, Knowledge and Intelligence included among the primary genera?\" He concluded that such transcendental categories and even the categories of Aristotle were in some way posterior to the three Eleatic categories first recorded in Plato's dialogue \"Parmenides\" and which comprised the following three coupled terms: \n\nPlotinus called these \"the hearth of reality\" deriving from them not only the three categories of Quantity, Motion and Quality but also what came to be known as \"the three moments of the Neoplatonic world process\":\n\nPlotinus likened the three to the centre, the radii and the circumference of a circle, and clearly thought that the principles underlying the categories were the first principles of creation. \"From a single root all being multiplies\". Similar ideas were to be introduced into Early Christian thought by, for example, Gregory of Nazianzus who summed it up saying \"Therefore Unity, having from all eternity arrived by motion at duality, came to rest in trinity\".\n\nIn the \"Critique of Pure Reason\" (1781), Immanuel Kant argued that the categories are part of our own mental structure and consist of a set of \"a priori\" concepts through which we interpret the world around us. These concepts correspond to twelve logical functions of the understanding which we use to make judgements and there are therefore two tables given in the \"Critique\", one of the Judgements and a corresponding one for the Categories. To give an example, the logical function behind our reasoning from ground to consequence (based on the Hypothetical relation) underlies our understanding of the world in terms of cause and effect (the Causal relation). In each table the number twelve arises from, firstly, an initial division into two: the Mathematical and the Dynamical; a second division of each of these headings into a further two: Quantity and Quality, and Relation and Modality respectively; and, thirdly, each of these then divides into a further three subheadings as follows.\nTable of Judgements\n\nMathematical\nDynamical\n\nTable of Categories\n\nMathematical\nDynamical\n\nCriticism of Kant's system followed, firstly, by Arthur Schopenhauer, who amongst other things was unhappy with the term \"Community\", and declared that the tables \"do open violence to truth, treating it as nature was treated by old-fashioned gardeners\", and secondly, by W.T.Stace who in his book \"The Philosophy of Hegel\" suggested that in order to make Kant's structure completely symmetrical a third category would need to be added to the Mathematical and the Dynamical. This, he said, Hegel was to do with his category of Notion.\n\nG.W.F. Hegel in his \"Science of Logic\" (1812) attempted to provide a more comprehensive system of categories than Kant and developed a structure that was almost entirely triadic. So important were the categories to Hegel that he claimed \"the first principle of the world, the Absolute, is a system of categories… the categories must be the reason of which the world is a consequent\".\n\nUsing his own logical method of combination, later to be called the Hegelian dialectic, of arguing from thesis through antithesis to synthesis, he arrived, as shown in W.T.Stace's work cited, at a hierarchy of some 270 categories. The three very highest categories were Logic, Nature and Spirit. The three highest categories of Logic, however, he called Being, Essence and Notion which he explained as follows:\nSchopenhauer's category that corresponded with Notion was that of Idea, which in his \"Four-Fold Root of Sufficient Reason\" he complemented with the category of the Will. The title of his major work was \"The World as Will and Idea\". The two other complementary categories, reflecting one of Hegel's initial divisions, were those of Being and Becoming. At around the same time, Goethe was developing his colour theories in the \"Farbenlehre\" of 1810, and introduced similar principles of combination and complementation, symbolising, for Goethe, \"the primordial relations which belong both to nature and vision\". Hegel in his \"Science of Logic\" accordingly asks us to see his system not as a tree but as a circle.\n\nCharles Sanders Peirce, who had read Kant and Hegel closely, and who also had some knowledge of Aristotle, proposed a system of merely three phenomenological categories: Firstness, Secondness, and Thirdness, which he repeatedly invoked in his subsequent writings. Like Hegel, C.S.Peirce attempted to develop a system of categories from a single indisputable principle, in Peirce's case the notion that in the first instance he could only be aware of his own ideas. \n\nAlthough Peirce's three categories correspond to the three concepts of relation given in Kant's tables, the sequence is now reversed and follows that given by Hegel, and indeed before Hegel of the three moments of the world-process given by Plotinus. Later, Peirce gave a mathematical reason for there being three categories in that although monadic, dyadic and triadic nodes are irreducible, every node of a higher valency is reducible to a \"compound of triadic relations\". Ferdinand de Saussure, who was developing \"semiology\" in France just as Peirce was developing \"semiotics\" in the US, likened each term of a proposition to \"the centre of a constellation, the point where other coordinate terms, the sum of which is indefinite, converge\".\n\nEdmund Husserl (1962, 2000) wrote extensively about categorial systems as part of his phenomenology.\n\nFor Gilbert Ryle (1949), a category (in particular a \"category mistake\") is an important semantic concept, but one having only loose affinities to an ontological category.\n\nContemporary systems of categories have been proposed by John G. Bennett (The Dramatic Universe, 4 vols., 1956–65), Wilfrid Sellars (1974), Reinhardt Grossmann (1983, 1992), Johansson (1989), Hoffman and Rosenkrantz (1994), Roderick Chisholm (1996), Barry Smith (ontologist) (2003), and Jonathan Lowe (2006).\n\n\n\n"}
{"id": "33346439", "url": "https://en.wikipedia.org/wiki?curid=33346439", "title": "Conceptual design", "text": "Conceptual design\n\nConceptual Design is an early phase of the design process, in which the broad outlines of function and form of something are articulated. It includes the design of interactions, experiences, processes and strategies. It involves an understanding of people's needs - and how to meet them with products, services, & processes. Common artifacts of conceptual design are concept sketches and models.\n\n"}
{"id": "555650", "url": "https://en.wikipedia.org/wiki?curid=555650", "title": "Conceptual framework", "text": "Conceptual framework\n\nA conceptual framework is an analytical tool with several variations and contexts. It can be applied in different categories of work where an overall picture is needed. It is used to make conceptual distinctions and organize ideas. Strong conceptual frameworks capture something real and do this in a way that is easy to remember and apply.\n\nIsaiah Berlin used the metaphor of a \"fox\" and a \"hedgehog\" to make conceptual distinctions in how important philosophers and authors view the world. Berlin describes hedgehogs as those who use a single idea or organizing principle to view the world (such as Dante Alighieri, Blaise Pascal, Fyodor Dostoyevsky, Plato, Henrik Ibsen and Georg Wilhelm Friedrich Hegel). Foxes, on the other hand, incorporate a type of pluralism and view the world through multiple, sometimes conflicting, lenses (examples include Johann Wolfgang von Goethe, James Joyce, William Shakespeare, Aristotle, Herodotus, Molière, and Honoré de Balzac).\n\nEconomists use the conceptual framework of \"supply\" and \"demand\" to distinguish between the behavior and incentive systems of firms and consumers. Like many conceptual frameworks, supply and demand can be presented through visual or graphical representations (see demand curve). Both political Science and economics use principal agent theory as a conceptual framework. The politics-administration dichotomy is a long standing conceptual framework used in public administration. All three of these cases are examples of a macro level conceptual framework.\n\nThe use of the term \"conceptual framework\" crosses both scale (large and small theories) and contexts (social science, marketing, applied science, art etc.). Its explicit definition and application can therefore vary.\n\nConceptual frameworks are particularly useful as organizing devices in empirical research. One set of scholars has applied the notion of conceptual framework to deductive, empirical research at the micro- or individual study level. They employ American football plays as a useful metaphor to clarify the meaning of \"conceptual framework\" (used in the context of a deductive empirical study).\n\nLikewise, conceptual frameworks are abstract representations, connected to the research project's goal that direct the collection and analysis of data (on the plane of observation – the ground). Critically, a football play is a \"plan of action\" tied to a particular, timely, purpose, usually summarized as long or short yardage. Shields and Rangarajan (2013) argue that it is this tie to \"purpose\" that make American football plays such a good metaphor. They define a conceptual framework as \"the way ideas are organized to achieve a research project's purpose\". Like football plays, conceptual frameworks are connected to a research purpose or aim. Explanation is the most common type of research purpose employed in empirical research. The formal hypothesis of a scientific investigation is the framework associated with explanation.\n\nExplanatory research usually focuses on \"why\" or \"what caused\" a phenomenon to occur. Formal hypotheses posit possible explanations (answers to the why question) that are tested by collecting data and assessing the evidence (usually quantitative using statistical tests). For example, Kai Huang wanted to determine what factors contributed to residential fires in U.S. cities. Three factors were posited to influence residential fires. These factors (environment, population and building characteristics) became the hypotheses or conceptual framework he used to achieve his purpose – explain factors that influenced home fires in U.S. cities.\n\nSeveral types of conceptual frameworks have been identified, and line up with a research purpose in the following ways:\n\nNote that Shields and Rangarajan (2013) do not claim that the above are the only framework-purpose pairing. Nor do they claim the system is applicable to inductive forms of empirical research. Rather, the conceptual framework-research purpose pairings they propose are useful and provide new scholars a point of departure to develop their own research design.\n\nFrameworks have also been used to explain conflict theory and the balance necessary to reach what amounts to resolution. Within these conflict frameworks, visible and invisible variables function under concepts of relevance. Boundaries form and within these boundaries, tensions regarding laws and chaos (or freedom) are mitigated. These frameworks often function like cells, with sub-frameworks, stasis, evolution and revolution. Anomalies may exist without adequate \"lenses\" or \"filters\" to see them and may become visible only when the tools exist to define them.\n\n\n"}
{"id": "2381958", "url": "https://en.wikipedia.org/wiki?curid=2381958", "title": "Conceptual model", "text": "Conceptual model\n\nA conceptual model is a representation of a system, made of the composition of concepts which are used to help people know, understand, or simulate a subject the model represents. It is also a set of concepts. Some models are physical objects; for example, a toy model which may be assembled, and may be made to work like the object it represents.\n\nThe term \"conceptual model\" may be used to refer to models which are formed after a conceptualization or generalization process. Conceptual models are often abstractions of things in the real world whether physical or social. Semantic studies are relevant to various stages of concept formation. Semantics is basically about concepts, the meaning that thinking beings give to various elements of their experience.\n\nThe term \"conceptual model\" is normal. It could mean \"a model of concept\" or it could mean \"a model that is conceptual.\" A distinction can be made between \"what models are\" and \"what models are made of\". With the exception of iconic models, such as a scale model of Winchester Cathedral, most models are concepts. But they are, mostly, intended to be models of real world states of affairs. The value of a model is usually directly proportional to how well it corresponds to a past, present, future, actual or potential state of affairs. A model of a concept is quite different because in order to be a good model it need not have this real world correspondence. In artificial intelligence conceptual models and conceptual graphs are used for building expert systems and knowledge-based systems; here the analysts are concerned to represent expert opinion on what is true not their own ideas on what is true.\n\nConceptual models (models that are conceptual) range in type from the more concrete, such as the mental image of a familiar physical object, to the formal generality and abstractness of mathematical models which do not appear to the mind as an image. Conceptual models also range in terms of the scope of the subject matter that they are taken to represent. A model may, for instance, represent a single thing (e.g. the \"Statue of Liberty\"), whole classes of things (e.g. \"the electron\"), and even very vast domains of subject matter such as \"the physical universe.\" The variety and scope of conceptual models is due to then variety of purposes had by the people using them.\nConceptual modeling is the activity of formally describing some aspects of the physical and social world around us for the purposes of understanding and communication.\"\n\nA conceptual model's primary objective is to convey the fundamental principles and basic functionality of the system which it represents. Also, a conceptual model must be developed in such a way as to provide an easily understood system interpretation for the models users. A conceptual model, when implemented properly, should satisfy four fundamental objectives.\n\n\nThe conceptual model plays an important role in the overall system development life cycle. Figure 1 below, depicts the role of the conceptual model in a typical system development scheme. It is clear that if the conceptual model is not fully developed, the execution of fundamental system properties may not be implemented properly, giving way to future problems or system shortfalls. These failures do occur in the industry and have been linked to; lack of user input, incomplete or unclear requirements, and changing requirements. Those weak links in the system design and development process can be traced to improper execution of the fundamental objectives of conceptual modeling. The importance of conceptual modeling is evident when such systemic failures are mitigated by thorough system development and adherence to proven development objectives/techniques.\n\nAs systems have become increasingly complex, the role of conceptual modelling has dramatically expanded. With that expanded presence, the effectiveness of conceptual modeling at capturing the fundamentals of a system is being realized. Building on that realization, numerous conceptual modeling techniques have been created. These techniques can be applied across multiple disciplines to increase the users understanding of the system to be modeled. A few techniques are briefly described in the following text, however, many more exist or are being developed. Some commonly used conceptual modeling techniques and methods include: workflow modeling, workforce modeling, rapid application development, object-role modeling, and the Unified Modeling Language (UML).\n\nData flow modeling (DFM) is a basic conceptual modeling technique that graphically represents elements of a system. DFM is a fairly simple technique, however, like many conceptual modeling techniques, it is possible to construct higher and lower level representative diagrams. The data flow diagram usually does not convey complex system details such as parallel development considerations or timing information, but rather works to bring the major system functions into context. Data flow modeling is a central technique used in systems development that utilizes the structured systems analysis and design method (SSADM).\n\nEntity-relationship modeling (ERM) is a conceptual modeling technique used primarily for software system representation. Entity-relationship diagrams, which are a product of executing the ERM technique, are normally used to represent database models and information systems. The main components of the diagram are the entities and relationships. The entities can represent independent functions, objects, or events. The relationships are responsible for relating the entities to one another. To form a system process, the relationships are combined with the entities and any attributes needed to further describe the process. Multiple diagramming conventions exist for this technique; IDEF1X, Bachman, and EXPRESS, to name a few. These conventions are just different ways of viewing and organizing the data to represent different system aspects.\n\nThe event-driven process chain (EPC) is a conceptual modeling technique which is mainly used to systematically improve business process flows. Like most conceptual modeling techniques, the event driven process chain consists of entities/elements and functions that allow relationships to be developed and processed. More specifically, the EPC is made up of events which define what state a process is in or the rules by which it operates. In order to progress through events, a function/ active event must be executed. Depending on the process flow, the function has the ability to transform event states or link to other event driven process chains. Other elements exist within an EPC, all of which work together to define how and by what rules the system operates. The EPC technique can be applied to business practices such as resource planning, process improvement, and logistics.\n\nThe dynamic systems development method uses a specific process called JEFFF to conceptually model a systems life cycle. JEFFF is intended to focus more on the higher level development planning that precedes a projects initialization. The JAD process calls for a series of workshops in which the participants work to identify, define, and generally map a successful project from conception to completion. This method has been found to not work well for large scale applications, however smaller applications usually report some net gain in efficiency.\n\nAlso known as Petri nets, this conceptual modeling technique allows a system to be constructed with elements that can be described by direct mathematical means. The petri net, because of its nondeterministic execution properties and well defined mathematical theory, is a useful technique for modeling concurrent system behavior, i.e. simultaneous process executions.\n\nState transition modeling makes use of state transition diagrams to describe system behavior. These state transition diagrams use distinct states to define system behavior and changes. Most current modeling tools contain some kind of ability to represent state transition modeling. The use of state transition models can be most easily recognized as logic state diagrams and directed graphs for finite-state machines.\n\nBecause the conceptual modeling method can sometimes be purposefully vague to account for a broad area of use, the actual application of concept modeling can become difficult. To alleviate this issue, and shed some light on what to consider when selecting an appropriate conceptual modeling technique, the framework proposed by Gemino and Wand will be discussed in the following text. However, before evaluating the effectiveness of a conceptual modeling technique for a particular application, an important concept must be understood; Comparing conceptual models by way of specifically focusing on their graphical or top level representations is shortsighted. Gemino and Wand make a good point when arguing that the emphasis should be placed on a conceptual modeling language when choosing an appropriate technique. In general, a conceptual model is developed using some form of conceptual modeling technique. That technique will utilize a conceptual modeling language that determines the rules for how the model is arrived at. Understanding the capabilities of the specific language used is inherent to properly evaluating a conceptual modeling technique, as the language reflects the techniques descriptive ability. Also, the conceptual modeling language will directly influence the depth at which the system is capable of being represented, whether it be complex or simple.\n\nBuilding on some of their earlier work, Gemino and Wand acknowledge some main points to consider when studying the affecting factors: the content that the conceptual model must represent, the method in which the model will be presented, the characteristics of the models users, and the conceptual model languages specific task. The conceptual models content should be considered in order to select a technique that would allow relevant information to be presented. The presentation method for selection purposes would focus on the techniques ability to represent the model at the intended level of depth and detail. The characteristics of the models users or participants is an important aspect to consider. A participant's background and experience should coincide with the conceptual models complexity, else misrepresentation of the system or misunderstanding of key system concepts could lead to problems in that systems realization. The conceptual model language task will further allow an appropriate technique to be chosen. The difference between creating a system conceptual model to convey system functionality and creating a system conceptual model to interpret that functionality could involve to completely different types of conceptual modeling languages.\n\nGemino and Wand go on to expand the affected variable content of their proposed framework by considering the focus of observation and the criterion for comparison. The focus of observation considers whether the conceptual modeling technique will create a \"new product\", or whether the technique will only bring about a more intimate understanding of the system being modeled. The criterion for comparison would weigh the ability of the conceptual modeling technique to be efficient or effective. A conceptual modeling technique that allows for development of a system model which takes all system variables into account at a high level may make the process of understanding the system functionality more efficient, but the technique lacks the necessary information to explain the internal processes, rendering the model less effective.\n\nWhen deciding which conceptual technique to use, the recommendations of Gemino and Wand can be applied in order to properly evaluate the scope of the conceptual model in question. Understanding the conceptual models scope will lead to a more informed selection of a technique that properly addresses that particular model. In summary, when deciding between modeling techniques, answering the following questions would allow one to address some important conceptual modeling considerations.\n\n\nAnother function of the simulation conceptual model is to provide a rational and factual basis for assessment of simulation application appropriateness.\n\nIn cognitive psychology and philosophy of mind, a mental model is a representation of something in the mind, but a mental model may also refer to a nonphysical external model of the mind itself.\n\nA metaphysical model is a type of conceptual model which is distinguished from other conceptual models by its proposed scope; a metaphysical model intends to represent reality in the broadest possible way. This is to say that it explains the answers to fundamental questions such as whether matter and mind are one or two substances; or whether or not humans have free will.\n\nAn epistemological model is a type of conceptual model whose proposed scope is the known and the knowable, and the believed and the believable.\n\nIn logic, a model is a type of interpretation under which a particular statement is true. Logical models can be broadly divided into ones which only attempt to represent concepts, such as mathematical models; and ones which attempt to represent physical objects, and factual relationships, among which are scientific models.\n\nModel theory is the study of (classes of) mathematical structures such as groups, fields, graphs, or even universes of set theory, using tools from mathematical logic. A system that gives meaning to the sentences of a formal language is called a model for the language. If a model for a language moreover satisfies a particular sentence or theory (set of sentences), it is called a model of the sentence or theory. Model theory has close ties to algebra and universal algebra.\n\nMathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures.\n\nA more comprehensive type of mathematical model uses a linguistic version of category theory to model a given situation. Akin to entity-relationship models, custom categories or sketches can be directly translated into database schemas. The difference is that logic is replaced by category theory, which brings powerful theorems to bear on the subject of modeling, especially useful for translating between disparate models (as functors between categories).\n\nA scientific model is a simplified abstract view of a complex reality. A scientific model represents empirical objects, phenomena, and physical processes in a logical way. Attempts to formalize the principles of the empirical sciences use an interpretation to model reality, in the same way logicians axiomatize the principles of logic. The aim of these attempts is to construct a formal system for which reality is the only interpretation. The world is an interpretation (or model) of these sciences, only insofar as these sciences are true.\n\nA statistical model is a probability distribution function proposed as generating data. In a parametric model, the probability distribution function has variable parameters, such as the mean and variance in a normal distribution, or the coefficients for the various exponents of the independent variable in linear regression. A nonparametric model has a distribution function without parameters, such as in bootstrapping, and is only loosely confined by assumptions. Model selection is a statistical method for selecting a distribution function within a class of them; e.g., in linear regression where the dependent variable is a polynomial of the independent variable with parametric coefficients, model selection is selecting the highest exponent, and may be done with nonparametric means, such as with cross validation.\n\nIn statistics there can be models of mental events as well as models of physical events. For example, a statistical model of customer behavior is a model that is conceptual (because behavior is physical), but a statistical model of customer satisfaction is a model of a concept (because satisfaction is a mental not a physical event).\n\nIn economics, a model is a theoretical construct that represents economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified framework designed to illustrate complex processes, often but not always using mathematical techniques. Frequently, economic models use structural parameters. Structural parameters are underlying parameters in a model or class of models. A model may have various parameters and those parameters may change to create various properties.\n\nA system model is the conceptual model that describes and represents the structure, behavior, and more views of a system. A system model can represent multiple views of a system by using two different approaches. The first one is the non-architectural approach and the second one is the architectural approach. The non-architectural approach respectively picks a model for each view. The architectural approach, also known as system architecture, instead of picking many heterogeneous and unrelated models, will use only one integrated architectural model.\n\nIn business process modelling the enterprise process model is often referred to as the \"business process model\". Process models are core concepts in the discipline of process engineering. Process models are:\nThe same process model is used repeatedly for the development of many applications and thus, has many instantiations.\n\nOne possible use of a process model is to prescribe how things must/should/could be done in contrast to the process itself which is really what happens. A process model is roughly an anticipation of what the process will look like. What the process shall be will be determined during actual system development.\n\nConceptual models of human activity systems are used in soft systems methodology (SSM), which is a method of systems analysis concerned with the structuring of problems in management. These models are models of concepts; the authors specifically state that they are not intended to represent a state of affairs in the physical world. They are also used in information requirements analysis (IRA) which is a variant of SSM developed for information system design and software engineering.\n\nLogico-linguistic modeling is another variant of SSM that uses conceptual models. However, this method combines models of concepts with models of putative real world objects and events. It is a graphical representation of modal logic in which modal operators are used to distinguish statement about concepts from statements about real world objects and events.\n\nIn software engineering, an entity-relationship model (ERM) is an abstract and conceptual representation of data. Entity-relationship modeling is a database modeling method, used to produce a type of conceptual schema or semantic data model of a system, often a relational database, and its requirements in a top-down fashion. Diagrams created by this process are called entity-relationship diagrams, ER diagrams, or ERDs.\n\nEntity-relationship models have had wide application in the building of information systems intended to support activities involving objects and events in the real world. In these cases they are models that are conceptual. However, this modeling method can be used to build computer games or a family tree of the Greek Gods, in these cases it would be used to model concepts.\n\nA domain model is a type of conceptual model used to depict the structural elements and their conceptual constraints within a domain of interest (sometimes called the problem domain). A domain model includes the various entities, their attributes and relationships, plus the constraints governing the conceptual integrity of the structural model elements comprising that problem domain. A domain model may also include a number of conceptual views, where each view is pertinent to a particular subject area of the domain or to a particular subset of the domain model which is of interest to a stakeholder of the domain model.\n\nLike entity-relationship models, domain models can be used to model concepts or to model real world objects and events.\n\n\n"}
{"id": "47792266", "url": "https://en.wikipedia.org/wiki?curid=47792266", "title": "Construction of Concept Map", "text": "Construction of Concept Map\n\nConcept is usually perceived as a regularity in events or objects or in their records. While constructing a concept map, it is essential to keep in mind that the concept be built with reference to a focus question. Hence, initially, the focus question to which we seek to answer is carefully chosen because learners usually tend to deviate from this question relating only to domains and thus, fails to answer the question.\n\nWith the selected domain and the focus question, the next step is to identify the key concepts which apply to this domain.About 15 to 25 concepts is sufficient which is usually ordered in a rank ordered list. Such list should be established from the most general and inclusive concept for the particular chosen problem. This list will assist in at least in the beginning of the construction of the concept map. The list is referred to as Parking Lot since the list is constantly shuffled to build up the required network of the concept. Two or more concepts are connected to each other using linking words or phrases which can only then complete a meaningful sentence.Another important characteristics of a concept map is the cross-links. This cross link acts as the relationship between two different domains used in the concept map.This helps in a clear representation of the knowledge contained in the concept and also gives a clear background with specific examples. \n"}
{"id": "1845675", "url": "https://en.wikipedia.org/wiki?curid=1845675", "title": "Deductive-nomological model", "text": "Deductive-nomological model\n\nThe deductive-nomological model (DN model), also known as Hempel's model, the Hempel–Oppenheim model, the Popper–Hempel model, or the covering law model, is a formal view of scientifically answering questions asking, \"Why...?\". The DN model poses scientific explanation as a deductive structure—that is, one where truth of its premises entails truth of its conclusion—hinged on accurate prediction or postdiction of the phenomenon to be explained.\n\nBecause of problems concerning humans' ability to define, discover, and know causality, it was omitted in initial formulations of the DN model. Causality was thought to be incidentally approximated by realistic selection of premises that \"derive\" the phenomenon of interest from observed starting conditions plus general laws. Still, DN model formally permitted causally irrelevant factors. Also, derivability from observations and laws sometimes yielded absurd answers.\n\nWhen logical empiricism fell out of favor in the 1960s, the DN model was widely seen as a flawed or greatly incomplete model of scientific explanation. Nonetheless, it remained an idealized version of scientific explanation, and one that was rather accurate when applied to modern physics. In the early 1980s, revision to DN model emphasized \"maximal specificity\" for relevance of the conditions and axioms stated. Together with Hempel's inductive-statistical model, the DN model forms scientific explanation's covering law model, which is also termed, from critical angle, subsumption theory.\n\nThe term \"deductive\" distinguishes the DN model's intended determinism from the probabilism of inductive inferences. The term \"nomological\" is derived from the Greek word \"νόμος\" or \"nomos\", meaning \"law\". The DN model holds to a view of scientific explanation whose \"conditions of adequacy\" (CA)—semiformal but stated classically—are \"derivability\" (CA1), \"lawlikeness\" (CA2), \"empirical content\" (CA3), and \"truth\" (CA4).\n\nIn the DN model, a law axiomatizes an unrestricted generalization from antecedent \"A\" to consequent \"B\" by conditional proposition—\"If A, then B\"—and has empirical content testable. A law differs from mere true regularity—for instance, \"George always carries only $1 bills in his wallet\"—by supporting counterfactual claims and thus suggesting what \"must\" be true, while following from a scientific theory's axiomatic structure.\n\nThe phenomenon to be explained is the explanandum—an event, law, or theory—whereas the premises to explain it are explanans, true or highly confirmed, containing at least one universal law, and entailing the explanandum. Thus, given the explanans as initial, specific conditions \"C, C . . . C\" plus general laws \"L, L . . . L\", the phenomenon \"E\" as explanandum is a deductive consequence, thereby scientifically explained.\n\nAristotle's scientific explanation in \"Physics\" resembles the DN model, an idealized form of scientific explanation. The framework of Aristotelian physics—Aristotelian metaphysics—reflected the perspective of this principally biologist, who, amid living entities' undeniable purposiveness, formalized vitalism and teleology, an intrinsic morality in nature. With emergence of Copernicanism, however, Descartes introduced mechanical philosophy, then Newton rigorously posed lawlike explanation, both Descartes and especially Newton shunning teleology within natural philosophy. At 1740, David Hume staked Hume's fork, highlighted the problem of induction, and found humans ignorant of either necessary or sufficient causality. Hume also highlighted the fact/value gap, as what \"is\" does not itself reveal what \"ought\".\n\nNear 1780, countering Hume's ostensibly radical empiricism, Immanuel Kant highlighted extreme rationalism—as by Descartes or Spinoza—and sought middle ground. Inferring the mind to arrange experience of the world into \"substance\", \"space\", and \"time\", Kant placed the mind as part of the causal constellation of experience and thereby found Newton's theory of motion universally true, yet knowledge of things in themselves impossible. Safeguarding science, then, Kant paradoxically stripped it of scientific realism. Aborting Francis Bacon's inductivist mission to dissolve the veil of appearance to uncover the \"noumena\"—metaphysical view of nature's ultimate truths—Kant's transcendental idealism tasked science with simply modeling patterns of \"phenomena\". Safeguarding metaphysics, too, it found the mind's constants holding also universal moral truths, and launched German idealism, increasingly speculative.\n\nAuguste Comte found the problem of induction rather irrelevant since enumerative induction is grounded on the empiricism available, while science's point is not metaphysical truth. Comte found human knowledge had evolved from theological to metaphysical to scientific—the ultimate stage—rejecting both theology and metaphysics as asking questions unanswerable and posing answers unverifiable. Comte in the 1830s expounded positivism—the first modern philosophy of science and simultaneously a political philosophy—rejecting conjectures about unobservables, thus rejecting search for \"causes\". Positivism predicts observations, confirms the predictions, and states a \"law\", thereupon applied to benefit human society. From late 19th century into the early 20th century, the influence of positivism spanned the globe. Meanwhile, evolutionary theory's natural selection brought the Copernican Revolution into biology and eventuated in the first conceptual alternative to vitalism and teleology.\n\nWhereas Comtean positivism posed science as \"description\", logical positivism emerged in the late 1920s and posed science as \"explanation\", perhaps to better unify empirical sciences by covering not only fundamental science—that is, fundamental physics—but special sciences, too, such as biology, psychology, economics, and anthropology. After defeat of National Socialism with World War II's close in 1945, logical positivism shifted to a milder variant, \"logical empiricism\". All variants of the movement, which lasted until 1965, are neopositivism, sharing the quest of verificationism.\n\nNeopositivists led emergence of the philosophy subdiscipline philosophy of science, researching such questions and aspects of scientific theory and knowledge. Scientific realism takes scientific theory's statements at face value, thus accorded either falsity or truth—probable or approximate or actual. Neopositivists held scientific antirealism as instrumentalism, holding scientific theory as simply a device to predict observations and their course, while statements on nature's unobservable aspects are elliptical at or metaphorical of its observable aspects, rather.\n\nDN model received its most detailed, influential statement by Carl G Hempel, first in his 1942 article \"The function of general laws in history\", and more explicitly with Paul Oppenheim in their 1948 article \"Studies in the logic of explanation\". Leading logical empiricist, Hempel embraced the Humean empiricist view that humans observe sequence of sensory events, not cause and effect, as causal relations and casual mechanisms are unobservables. DN model bypasses causality beyond mere constant conjunction: first an event like \"A\", then always an event like \"B\".\n\nHempel held natural laws—empirically confirmed regularities—as satisfactory, and if included realistically to approximate causality. In later articles, Hempel defended DN model and proposed probabilistic explanation by \"inductive-statistical model\" (IS model). DN model and IS model—whereby the probability must be high, such as at least 50%—together form \"covering law model\", as named by a critic, William Dray. Derivation of statistical laws from other statistical laws goes to the \"deductive-statistical model\" (DS model). Georg Henrik von Wright, another critic, named the totality \"subsumption theory\".\n\nAmid failure of neopositivism's fundamental tenets, Hempel in 1965 abandoned verificationism, signaling neopositivism's demise. From 1930 onward, Karl Popper had refuted any positivism by asserting falsificationism, which Popper claimed had killed positivism, although, paradoxically, Popper was commonly mistaken for a positivist. Even Popper's 1934 book embraces DN model, widely accepted as the model of scientific explanation for as long as physics remained the model of science examined by philosophers of science.\n\nIn the 1940s, filling the vast observational gap between cytology and biochemistry, cell biology arose and established existence of cell organelles besides the nucleus. Launched in the late 1930s, the molecular biology research program cracked a genetic code in the early 1960s and then converged with cell biology as \"cell and molecular biology\", its breakthroughs and discoveries defying DN model by arriving in quest not of lawlike explanation but of causal mechanisms. Biology became a new model of science, while special sciences were no longer thought defective by lacking universal laws, as borne by physics.\n\nIn 1948, when explicating DN model and stating scientific explanation's semiformal \"conditions of adequacy\", Hempel and Oppenheim acknowledged redundancy of the third, \"empirical content\", implied by the other three—\"derivability\", \"lawlikeness\", and \"truth\". In the early 1980s, upon widespread view that causality ensures the explanans' relevance, Wesley Salmon called for returning \"cause\" to \"because\", and along with James Fetzer helped replace CA3 \"empirical content\" with CA3' \"strict maximal specificity\".\n\nSalmon introduced \"causal mechanical\" explanation, never clarifying how it proceeds, yet reviving philosophers' interest in such. Via shortcomings of Hempel's inductive-statistical model (IS model), Salmon introduced \"statistical-relevance model\" (SR model). Although DN model remained an idealized form of scientific explanation, especially in applied sciences, most philosophers of science consider DN model flawed by excluding many types of explanations generally accepted as scientific.\n\nAs theory of knowledge, epistemology differs from ontology, which is a subbranch of metaphysics, theory of reality. Ontology poses which categories of being—what sorts of things exist—and so, although a scientific theory's ontological commitment can be modified in light of experience, an ontological commitment inevitably precedes empirical inquiry.\n\nNatural laws, so called, are statements of humans' observations, thus are epistemological—concerning human knowledge—the \"epistemic\". Causal mechanisms and structures existing putatively independently of minds exist, or would exist, in the natural world's structure itself, and thus are ontological, the \"ontic\". Blurring epistemic with ontic—as by incautiously presuming a natural law to refer to a causal mechanism, or to trace structures realistically during unobserved transitions, or to be true regularities always unvarying—tends to generate a \"category mistake\".\n\nDiscarding ontic commitments, including causality \"per se\", DN model permits a theory's laws to be reduced to—that is, subsumed by—a more fundamental theory's laws. The higher theory's laws are explained in DN model by the lower theory's laws. Thus, the epistemic success of Newtonian theory's law of universal gravitation is reduced to—thus explained by—Einstein's general theory of relativity, although Einstein's discards Newton's ontic claim that universal gravitation's epistemic success predicting Kepler's laws of planetary motion is through a causal mechanism of a straightly attractive force instantly traversing absolute space despite absolute time.\n\nCovering law model reflects neopositivism's vision of empirical science, a vision interpreting or presuming unity of science, whereby all empirical sciences are either fundamental science—that is, fundamental physics—or are special sciences, whether astrophysics, chemistry, biology, geology, psychology, economics, and so on. All special sciences would network via covering law model. And by stating \"boundary conditions\" while supplying \"bridge laws\", any special law would reduce to a lower special law, ultimately reducing—theoretically although generally not practically—to fundamental science. (\"Boundary conditions\" are specified conditions whereby the phenomena of interest occur. \"Bridge laws\" translate terms in one science to terms in another science.)\n\nBy DN model, if one asks, \"Why is that shadow 20 feet long?\", another can answer, \"Because that flagpole is 15 feet tall, the Sun is at \"x\" angle, and laws of electromagnetism\". Yet by problem of symmetry, if one instead asked, \"Why is that flagpole 15 feet tall?\", another could answer, \"Because that shadow is 20 feet long, the Sun is at \"x\" angle, and laws of electromagnetism\", likewise a deduction from observed conditions and scientific laws, but an answer clearly incorrect. By the problem of irrelevance, if one asks, \"Why did that man not get pregnant?\", one could in part answer, among the explanans, \"Because he took birth control pills\"—if he factually took them, and the law of their preventing pregnancy—as covering law model poses no restriction to bar that observation from the explanans.\n\nMany philosophers have concluded that causality is integral to scientific explanation. DN model offers a necessary condition of a causal explanation—successful prediction—but not sufficient conditions of causal explanation, as a universal regularity can include spurious relations or simple correlations, for instance \"Z\" always following \"Y\", but not \"Z\" because of \"Y\", instead \"Y\" and then \"Z\" as an effect of \"X\". By relating temperature, pressure, and volume of gas within a container, Boyle's law permits prediction of an unknown variable—volume, pressure, or temperature—but does not explain \"why\" to expect that unless one adds, perhaps, the kinetic theory of gases.\n\nScientific explanations increasingly pose not determinism's universal laws, but probabilism's chance, \"ceteris paribus\" laws. Smoking's contribution to lung cancer fails even the inductive-statistical model (IS model), requiring probability over 0.5 (50%). (Probability standardly ranges from 0 (0%) to 1 (100%).) An applied science that applies statistics seeking associations between events, epidemiology cannot show causality, but consistently found higher incidence of lung cancer in smokers versus otherwise similar nonsmokers, although the proportion of smokers who develop lung cancer is modest. Versus nonsmokers, however, smokers as a group showed over 20 times the risk of lung cancer, and in conjunction with basic research, consensus followed that smoking had been scientifically explained as \"a\" cause of lung cancer, responsible for some cases that without smoking would not have occurred, a probabilistic counterfactual causality.\n\nThrough lawlike explanation, fundamental physics—often perceived as fundamental science—has proceeded through intertheory relation and theory reduction, thereby resolving experimental paradoxes to great historical success, resembling covering law model. In early 20th century, Ernst Mach as well as Wilhelm Ostwald had resisted Ludwig Boltzmann's reduction of thermodynamics—and thereby Boyle's law—to statistical mechanics partly \"because\" it rested on kinetic theory of gas, hinging on atomic/molecular theory of matter. Mach as well as Ostwald viewed matter as a variant of energy, and molecules as mathematical illusions, as even Boltzmann thought possible.\n\nIn 1905, via statistical mechanics, Albert Einstein predicted the phenomenon Brownian motion—unexplained since reported in 1827 by botanist Robert Brown. Soon, most physicists accepted that atoms and molecules were unobservable yet real. Also in 1905, Einstein explained the electromagnetic field's energy as distributed in \"particles\", doubted until this helped resolve atomic theory in the 1910s and 1920s. Meanwhile, all known physical phenomena were gravitational or electromagnetic, whose two theories misaligned. Yet belief in aether as the source of all physical phenomena was virtually unanimous. At experimental paradoxes, physicists modified the aether's hypothetical properties.\n\nFinding the luminiferous aether a useless hypothesis, Einstein in 1905 \"a priori\" unified all inertial reference frames to state special \"principle\" of relativity, which, by omitting aether, converted space and time into \"relative\" phenomena whose relativity aligned electrodynamics with the Newtonian principle Galilean relativity or invariance. Originally epistemic or instrumental, this was interpreted as ontic or realist—that is, a causal mechanical explanation—and the \"principle\" became a \"theory\", refuting Newtonian gravitation. By predictive success in 1919, general relativity apparently overthrew Newton's theory, a revolution in science resisted by many yet fulfilled around 1930.\n\nIn 1925, Werner Heisenberg as well as Erwin Schrödinger independently formalized quantum mechanics (QM). Despite clashing explanations, the two theories made identical predictions. Paul Dirac's 1928 model of the electron was set to special relativity, launching QM into the first quantum field theory (QFT), quantum electrodynamics (QED). From it, Dirac interpreted and predicted the electron's antiparticle, soon discovered and termed \"positron\", but the QED failed electrodynamics at high energies. Elsewhere and otherwise, strong nuclear force and weak nuclear force were discovered.\n\nIn 1941, Richard Feynman introduced QM's path integral formalism, which if taken toward \"interpretation\" as a causal mechanical model clashes with Heisenberg's matrix formalism and with Schrödinger's wave formalism, although all three are empirically identical, sharing predictions. Next, working on QED, Feynman sought to model particles without fields and find the vacuum truly empty. As each known fundamental force is apparently an effect of a field, Feynman failed. Louis de Broglie's waveparticle duality had rendered atomism—indivisible particles in a void—untenable, and highlighted the very notion of discontinuous particles as selfcontradictory.\n\nMeeting in 1947, Freeman Dyson, Richard Feynman, Julian Schwinger, and Sin-Itiro Tomonaga soon introduced \"renormalization\", a procedure converting QED to physics' most predictively precise theory, subsuming chemistry, optics, and statistical mechanics. QED thus won physicists' general acceptance. Paul Dirac criticized its need for renormalization as showing its unnaturalness, and called for an aether. In 1947, Willis Lamb had found unexpected motion of electron orbitals, shifted since the vacuum is not truly empty. Yet \"emptiness\" was catchy, abolishing aether conceptually, and physics proceeded ostensibly without it, even suppressing it. Meanwhile, \"sickened by untidy math, most philosophers of physics tend to neglect QED\".\n\nPhysicists have feared even mentioning \"aether\", renamed \"vacuum\", which—as such—is nonexistent. General philosophers of science commonly believe that aether, rather, is fictitious, \"relegated to the dustbin of scientific history ever since\" 1905 brought special relativity. Einstein was noncommittal to aether's nonexistence, simply said it superfluous. Abolishing Newtonian motion for electrodynamic primacy, however, Einstein inadvertently reinforced aether, and to explain motion was led back to aether in general relativity. Yet resistance to relativity theory became associated with earlier theories of aether, whose word and concept became taboo. Einstein explained special relativity's compatibility with an aether, but Einstein aether, too, was opposed. Objects became conceived as pinned directly on space and time by abstract geometric relations lacking ghostly or fluid medium.\n\nBy 1970, QED along with weak nuclear field was reduced to electroweak theory (EWT), and the strong nuclear field was modeled as quantum chromodynamics (QCD). Comprised by EWT, QCD, and Higgs field, this Standard Model of particle physics is an \"effective theory\", not truly fundamental. As QCD's particles are considered nonexistent in the everyday world, QCD especially suggests an aether, routinely found by physics experiments to exist and to exhibit relativistic symmetry. Confirmation of the Higgs particle, modeled as a condensation within the Higgs field, corroborates aether, although physics need not state or even include aether. Organizing regularities of \"observations\"—as in the covering law model—physicists find superfluous the quest to discover \"aether\".\n\nIn 1905, from special relativity, Einstein deduced mass–energy equivalence, particles being variant forms of distributed energy, how particles colliding at vast speed experience that energy's transformation into mass, producing heavier particles, although physicists' talk promotes confusion. As \"the contemporary locus of metaphysical research\", QFTs pose particles not as existing individually, yet as \"excitation modes\" of fields, the particles and their masses being states of aether, apparently unifying all physical phenomena as the more fundamental causal reality, as long ago foreseen. Yet a \"quantum\" field is an intricate abstraction—a \"mathematical\" field—virtually inconceivable as a \"classical\" field's physical properties. Nature's deeper aspects, still unknown, might elude any possible field theory.\n\nThough discovery of causality is popularly thought science's aim, search for it was shunned by the Newtonian research program, even more Newtonian than was Isaac Newton. By now, most theoretical physicists infer that the four, known fundamental interactions would reduce to superstring theory, whereby atoms and molecules, after all, are energy vibrations holding mathematical, geometric forms. Given uncertainties of scientific realism, some conclude that the concept \"causality\" raises comprehensibility of scientific explanation and thus is key folk science, but compromises precision of scientific explanation and is dropped as a science matures. Even epidemiology is maturing to heed the severe difficulties with presumptions about causality. Covering law model is among Carl G Hempel's admired contributions to philosophy of science.\n\nTypes of inference\n\nRelated subjects\n\n\n"}
{"id": "2286327", "url": "https://en.wikipedia.org/wiki?curid=2286327", "title": "Distinction without a difference", "text": "Distinction without a difference\n\nA distinction without a difference is a type of logical fallacy where an author or speaker attempts to describe a distinction between two things where no discernible difference exists. It is particularly used when a word or phrase has connotations associated with it that one party to an argument prefers to avoid.\n\n\n"}
{"id": "152902", "url": "https://en.wikipedia.org/wiki?curid=152902", "title": "Dormant Commerce Clause", "text": "Dormant Commerce Clause\n\nThe Dormant Commerce Clause, or Negative Commerce Clause, in American constitutional law, is a legal doctrine that courts in the United States have inferred from the Commerce Clause in Article I of the US Constitution. The Dormant Commerce Clause is used to prohibit state legislation that discriminates against interstate or international commerce.\n\nFor example, it is lawful for Michigan to require food labels that specifically identify certain animal parts, if they are present in the product, because the state law applies to food produced in Michigan as well as food imported from other states and foreign countries; the state law would violate the Commerce Clause if it applied only to imported food or if it was otherwise found to favor domestic over imported products. Likewise, California law requires milk sold to contain a certain percentage of milk solids that federal law does not require, which is allowed under the Dormant Commerce Clause doctrine because California's stricter requirements apply equally to California-produced milk and imported milk and so does not discriminate against or inappropriately burden interstate commerce.\n\nThe idea that regulation of interstate commerce may to some extent be an exclusive Federal power was discussed even before adoption of the Constitution, but the framers did not use the word \"dormant\". On September 15, 1787, the Framers of the Constitution debated in Philadelphia whether to guarantee states the ability to lay duties of tonnage without Congressional interference so that the states could finance the clearing of harbors and the building of lighthouses. James Madison believed that the mere existence of the Commerce Clause would bar states from imposing any duty of tonnage: \"He was more and more convinced that the regulation of Commerce was in its nature indivisible and ought to be wholly under one authority.\"\n\nRoger Sherman disagreed: \"The power of the United States to regulate trade being supreme can control interferences of the State regulations when such interferences happen; so that there is no danger to be apprehended from a concurrent jurisdiction.\" Sherman saw the commerce power as similar to the tax power, the latter being one of the concurrent powers shared by the federal and state governments. Ultimately, the Philadelphia Convention decided upon the present language about duties of tonnage in , which says: \"No state shall, without the consent of Congress, lay any duty of tonnage ...\"\n\nThe word \"dormant,\" in connection with the Commerce Clause, originated in dicta of Chief Justice John Marshall. For example, in the case of \"Gibbons v. Ogden\", , he wrote that the power to regulate interstate commerce \"can never be exercised by the people themselves, but must be placed in the hands of agents, or lie dormant.\" Concurring Justice William Johnson was even more emphatic that the Constitution is \"altogether in favor of the exclusive grants to Congress of power over commerce.\"\n\nLater, in the case of \"Willson v. Black-Bird Creek Marsh Co.\", , Marshall wrote: \"We do not think that the [state] act empowering the Black Bird Creek Marsh Company to place a dam across the creek, can, under all the circumstances of the case, be considered as repugnant to the power to regulate commerce in its dormant state, or as being in conflict with any law passed on the subject.\"\n\nIf Marshall was suggesting that the power over interstate commerce is an exclusive federal power, the Dormant Commerce Clause doctrine eventually developed very differently: it treats regulation that does not discriminate against or unduly burden interstate commerce as a concurrent power, rather than an exclusive federal power, and it treats regulation that does so as an exclusive federal power. Thus, the modern doctrine says that congressional power over interstate commerce is somewhat exclusive but \"not absolutely exclusive\". The approach began in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court: \"Either absolutely to affirm, or deny that the nature of this [commerce] power requires exclusive legislation by Congress, is to lose sight of the nature of the subjects of this power, and to assert concerning all of them, what is really applicable but to a part.\" The first clear holding of the Supreme Court striking down a state law under the Dormant Commerce Clause came in 1873.\n\nJustice Anthony Kennedy has written that: \"The central rationale for the rule against discrimination is to prohibit state or municipal laws whose object is local economic protectionism, laws that would excite those jealousies and retaliatory measures the Constitution was designed to prevent.\" In order to determine whether a law violates a so-called \"dormant\" aspect of the Commerce Clause, the court first asks whether it discriminates on its face against interstate commerce. In this context, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter.\n\nThus, in a dormant Commerce Clause case, a court is initially concerned with whether the law facially discriminates against out-of-state actors or has the effect of favoring in-state economic interests over out-of-state interests. Discriminatory laws motivated by \"simple economic protectionism\" are subject to a \"virtually per se rule of invalidity\", \"City of Philadelphia v. New Jersey\" 437 U.S. 617 (1978), \"Dean Milk Co. v. City of Madison, Wisconsin\", 340 U.S. 349 (1951), \"Hunt v. Washington State Apple Advertising Comm.\", 432 U.S. 333 (1977) which can only be overcome by a showing that the State has no other means to advance a legitimate local purpose, \"Maine v. Taylor\", 477 U.S. 131(1986). See also \"Brown-Forman Distillers v. New York State Liquor Authority\", .\n\nOn the other hand, when a law is \"directed to legitimate local concerns, with effects upon interstate commerce that are only incidental\" (United Haulers Association, Inc.), that is, where other legislative objectives are credibly advanced and there is no patent discrimination against interstate trade, the Court has adopted a much more flexible approach, the general contours of which were outlined in \"Pike v. Bruce Church, Inc.\", 397 U.S. 137, 142 (1970) and \"City of Philadelphia v. New Jersey\", 437 U.S. at 624. If the law is not outright or intentionally discriminatory or protectionist, but still has some impact on interstate commerce, the court will evaluate the law using a balancing test. The Court determines whether the interstate burden imposed by a law outweighs the local benefits. If such is the case, the law is usually deemed unconstitutional. See \"Pike v. Bruce Church, Inc.\", . In the Pike case, the Court explained that a state regulation having only \"incidental\" effects on interstate commerce \"will be upheld unless the burden imposed on such commerce is clearly excessive in relation to the putative local benefits\". 397 U.S. at 142, 90 S.Ct. at 847. When weighing burdens against benefits, a court should consider both \"the nature of the local interest involved, and ... whether it could be promoted as well with a lesser impact on interstate activities\". Id. Thus regulation designed to implement public health and safety, or serve other legitimate state interests, but impact interstate commerce as an incident to that purpose, are subject to a test akin to the rational basis test, a minimum level of scrutiny. See \"Bibb v. Navajo Freight Lines, Inc.\" In USA Recycling, Inc. v. Town of Babylon, 66 F.3d 1272, 1281 (C.A.2 (N.Y.), 1995), the court explained:\n\nIf the state activity constitutes \"regulation\" of interstate commerce, then the court must proceed to a second inquiry: whether the activity regulates evenhandedly with only \"incidental\" effects on interstate commerce, or discriminates against interstate commerce. As we use the term here, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter. The party challenging the validity of a state statute or municipal ordinance bears the burden of showing that it discriminates against, or places some burden on, interstate commerce. \"Hughes v. Oklahoma\", 441 U.S. 322, 336, 99 S.Ct. 1727, 1736, 60 L.Ed.2d 250 (1979). If discrimination is established, the burden shifts to the state or local government to show that the local benefits of the statute outweigh its discriminatory effects, and that the state or municipality lacked a nondiscriminatory alternative that could have adequately protected the relevant local interests. If the challenging party cannot show that the statute is discriminatory, then it must demonstrate that the statute places a burden on interstate commerce that \"is clearly excessive in relation to the putative local benefits.\" \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456, 471(1981) (quoting Pike, 397 U.S. at 142, 90 S.Ct. at 847).\n\nOver the years, the Supreme Court has consistently held that the language of the Commerce Clause contains a further, negative command prohibiting certain state taxation even when Congress has failed to legislate on the subject. Examples of such cases are \"Quill Corp. v. North Dakota\", 504 U.S. 298 (1992); \"Northwestern States Portland Cement Co. v. Minnesota\", 358 U.S. 450, 458 (1959) and \"H.P. Hood & Sons, Inc. v. Du Mond\", 336 U.S. 525 (1949).\n\nMore recently, in the 2015 case of \"Comptroller of Treasury of MD. v. Wynne\", the Court addressed Maryland's unusual practice of taxing personal income earned in Maryland, and taxing personal income of its citizens earned outside Maryland, \"without\" any tax credit for income tax paid to other states. The Court held this sort of double-taxation to be a violation of the dormant Commerce Clause. The Court faulted Justice Antonin Scalia's criticism of the dormant Commerce Clause doctrine by saying that he failed to \"explain why, under his interpretation of the Constitution, the Import-Export Clause \nwould not lead to the same result that we reach under the dormant Commerce Clause\".\n\nApplication of the dormant commerce clause to state taxation is another manifestation of the Court's holdings that the Commerce Clause prevents a State from retreating into economic isolation or jeopardizing the welfare of the Nation as a whole, as it would do if it were free to place burdens on the flow of commerce across its borders that commerce wholly within those borders would not bear. The Court's taxation decisions thus \"reflected a central concern of the Framers that was an immediate reason for calling the Constitutional Convention: the conviction that in order to succeed, the new Union would have to avoid the tendencies toward economic Balkanization that had plagued relations among the Colonies and later among the States under the Articles of Confederation.\" \"Wardair Canada, Inc. v. Florida Dept. of Revenue\", 477 U.S. 1 (1986); \"Hughes v. Oklahoma\", 441 U.S. 322 (1979); \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995).\n\nAs with the Court's application of the dormant commerce clause to discriminatory regulation, the pre-New Deal Court attempted to apply a formalistic approach to state taxation alleged to interfere with interstate commerce. The history is described in \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995):\n\nThe command has been stated more easily than its object has been attained, however, and the Court's understanding of the dormant Commerce Clause has taken some turns. In its early stages, the Court held the view that interstate commerce was wholly immune from state taxation \"in any form\", \"even though the same amount of tax should be laid on (intrastate) commerce\". This position gave way in time to a less uncompromising but formal approach, according to which, for example, the Court would invalidate a state tax levied on gross receipts from interstate commerce, or upon the \"freight carried\" in interstate commerce, but would allow a tax merely measured by gross receipts from interstate commerce as long as the tax was formally imposed upon franchises, or \"'in lieu of all taxes upon (the taxpayer's) property,'\" Dissenting from this formal approach in 1927, Justice Stone remarked that it was \"too mechanical, too uncertain in its application, and too remote from actualities, to be of value.\" \n\nAccompanying the revolution in approach in the Court's Congressional powers jurisprudence, the New Deal Court began to change its approach to state taxation as well. The Jefferson Lines decision continues:\n\nIn 1938, the old formalism began to give way with Justice Stone's opinion in \"Western Live Stock v. Bureau of Revenue\", 303 U.S. 250, which examined New Mexico's franchise tax, measured by gross receipts, as applied to receipts from out-of-state advertisers in a journal produced by taxpayers in New Mexico but circulated both inside and outside the State. Although the assessment could have been sustained solely on prior precedent, Justice Stone added a dash of the pragmatism that, with a brief interlude, has since become our aspiration in this quarter of the law. ... The Court explained that \"[i]t was not the purpose of the commerce clause to relieve those engaged in interstate commerce from their just share of state tax burden even though it increases the cost of doing the business.\" \n\nDuring the transition period, some taxes were upheld based on a careful review of the actual economic impact of the tax, and other taxes were reviewed based on the kind of tax involved, whether the tax had a nefarious impact on commerce or not. Under this formalistic approach, a tax might be struck down, and then re-passed with exactly the same economic incidence, but under another name, and then withstand review.\n\nThe absurdity of this approach was made manifest in the two Railway Express cases. In the first, a tax imposed by the state of Virginia on American business concerns operating within the state was struck down because it was a business privilege tax imposed on the privilege of doing business in interstate commerce. But then, in the second, Virginia revised the wording of its statute to impose a \"franchise tax\" on \"intangible property\" in the form of \"going concern\" value as measured by gross receipts.\n\nThe Court upheld the reworded statute as not violative of the prohibition on privilege taxes, even though the impact of the old tax and new were essentially identical. There was no real economic difference between the statutes in Railway Express I and Railway Express II. The Court long since had recognized that interstate commerce may be made to pay its way. Yet under the Spector rule, the economic realities in Railway Express I became irrelevant. The Spector rule (against privilege taxes) had come to operate only as a rule of draftsmanship, and served only to distract the courts and parties from their inquiry into whether the challenged tax produced results forbidden by the Commerce Clause.\n\nThe death knell of formalism occurred in \"Complete Auto Transit, Inc v. Brady\", 430 U.S. 274 (1977), which approved a Mississippi privilege tax upon a Michigan company engaged in the business of shipping automobiles to Mississippi dealers. The Court there explained:\n\nAppellant's attack is based solely on decisions of this Court holding that a tax on the \"privilege\" of engaging in an activity in the State may not be applied to an activity that is part of interstate commerce. See, e. g., \"Spector Motor Service v. O'Connor\", 340 U.S. 602 (1951); \"Freeman v. Hewit\", 329 U.S. 249 (1946). This rule looks only to the fact that the incidence of the tax is the \"privilege of doing business\"; it deems irrelevant any consideration of the practical effect of the tax. The rule reflects an underlying philosophy that interstate commerce should enjoy a sort of \"free trade\" immunity from state taxation. \n\nComplete Auto Transit is the last in a line of cases that gradually rejected a per se approach to state taxation challenges under the commerce clause. In overruling prior decisions which struck down privilege taxes per se, the Court noted the following, in what has become a central component of commerce clause state taxation jurisprudence:\n\nWe note again that no claim is made that the activity is not sufficiently connected to the State to justify a tax, or that the tax is not fairly related to benefits provided the taxpayer, or that the tax discriminates against interstate commerce, or that the tax is not fairly apportioned.\n\nThese four factors, nexus, relationship to benefits, discrimination, and apportionment, have come to be regarded as the four Complete Auto Transit factors applied repeatedly in subsequent cases. Complete Auto Transit must be recognized as the culmination of the Court's emerging commerce clause approach, not just in taxation, but in all of its aspects. Application of Complete Auto Transit to State taxation remains a highly technical and specialized venture, requiring the application of commerce clause principles to an understanding of specialized tax law.\n\nIn addition to satisfying the four-prong test in \"Complete Auto Transit\", the Supreme Court has held state taxes which burden international commerce cannot create a substantial risk of multiple taxations and must not prevent the federal government from \"speaking with one voice when regulating commercial relations with foreign governments\". \"Japan Lines, Ltd. v. County of Los Angeles\", 441 U.S. 434 (1979).\n\nIn \"Kraft Gen. Foods, Inc. v. Iowa Dept. of Revenue and Finance\", 505 U.S. 71 (1992), the Supreme Court considered a case in which Iowa taxed dividends from foreign subsidiaries, without allowing a credit for taxes paid to foreign governments, but not dividends from domestic subsidiaries operating outside Iowa. This differential treatment arose from Iowa's adoption of the definition of \"net income\" used by the Internal Revenue Service. For federal income tax purposes, dividends from domestic subsidiaries are allowed to be exempted from the parent corporations income to avoid double taxation. The Iowa Supreme Court rejected a Commerce Clause claim because Kraft failed to show \"that Iowa businesses receive a commercial advantage over foreign commerce due to Iowa's taxing scheme.\" Considering an Equal Protection Clause challenge, the Iowa Supreme Court held that the use of the federal government's definitions of income were convenient for the state and was \"rationally related to the goal of administrative efficiency\". The Supreme Court rejected the notion that administrative convenience was a sufficient defense for subjecting foreign commerce to a higher tax burden than interstate commerce. The Supreme Court held that \"a State's preference for domestic commerce over foreign commerce is inconsistent with the Commerce Clause even if the State's own economy is not a direct beneficiary of the discrimination.\"\n\nDiscrimination in the flow of interstate commerce has arisen in a variety of contexts. A line of important cases has dealt with local processing requirements. Under the local processing requirement, a municipality seeks to force the local processing of raw materials before they are shipped in interstate commerce.\n\nThe basic idea of the local processing ordinance was to provide favored access to local processors of locally produced raw materials. Examples of Supreme Court decisions in this vein are set out in its Carbone decision. They include \"Minnesota v. Barber\", 136 U.S. 313, (1890) (striking down a Minnesota statute that required any meat sold within the State, whether originating within or without the State, to be examined by an inspector within the State); \"Foster-Fountain Packing Co. v. Haydel\", 278 U.S. 1 (1928) (striking down a Louisiana statute that forbade shrimp to be exported unless the heads and hulls had first been removed within the State); \"Johnson v. Haydel\", 278 U.S. 16 (1928) (striking down analogous Louisiana statute for oysters); \"Toomer v. Witsell\", 334 U.S. 385 (1948) (striking down South Carolina statute that required shrimp fishermen to unload, pack, and stamp their catch before shipping it to another State); \"Pike v. Bruce Church, Inc.\", supra (striking down Arizona statute that required all Arizona-grown cantaloupes to be packaged within the State prior to export); \"South-Central Timber Development, Inc. v. Wunnicke\", 467 U.S. 82 (1984) (striking down an Alaska regulation that required all Alaska timber to be processed within the State prior to export). The Court has defined \"protectionist\" state legislation as \"regulatory measures designed to benefit in-state economic interests by burdening out-of-state competitors\". \"New Energy Co. of Indiana v. Limbach\", 486 U.S. 269, 273–74 (1988).\n\nIn the 1980s, spurred by RCRA's emphasis on comprehensive local planning, many states and municipalities sought to promote investment in more costly disposal technologies, such as waste-to-energy incinerators, state-of-the-art landfills, composting and recycling. Some states and localities sought to promote private investment in these costly technologies by guaranteeing a longterm supply of customers. See Phillip Weinberg, Congress, the Courts, and Solid Waste Transport: Good Fences Don't Always Make Good Neighbors, 25 Envtl. L. 57 (1995); Atlantic Coast Demolition & Recycling, Inc., 112 F.3d 652, 657 (3d Cir. 1997). For about a decade, the use of regulation to channel private commerce to designated private disposal sites was greatly restricted as the result of the Carbone decision discussed below.\n\nFlow control laws typically came in various designs. One common theme was the decision to fund local infrastructure by guaranteeing a minimum volume of business for privately constructed landfills, incinerators, composters or other costly disposal sites. In some locales, choice of the flow control device was driven by state bonding laws, or municipal finance concerns. If a county or other municipality issued general obligation bonds for construction of a costly incinerator, for example, state laws might require a special approval process. If approval could be obtained, the bonds themselves would be counted against governmental credit limitations, or might impact the governmental body's credit rating: in either instance the ability to bond for other purposes might be impaired. But by guaranteeing customers for a privately constructed and financed facility, a private entity could issue its own bonds, privately, on the strength of the public's waste assurance.\n\nThe private character of flow control regimens can thus be explained in part by the desire to utilize particular kinds of public financing devices. It can also be explained by significant encouragement at the national level, in national legislation as well as in federal executive policy to achieve environmental objectives utilizing private resources. Ironically, these public-private efforts often took the form of local processing requirements which ultimately ran afoul of the commerce clause.\n\nThe Town of Clarkstown had decided that it wanted to promote waste assurance through a local private transfer station. The transfer station would process waste and then forward the waste to the disposal site designated by the Town. The ordinance had the following features:\n\nWaste hauling in the Town of Clarkstown was accomplished by private haulers, subject to local regulation. The scheme had the following aspects: (A) The Town promoted the financing of a privately owned transfer station through a waste assurance agreement with the private company. Thus the designated facility was a private company. (B) The Town of Clarkstown forced private haulers to bring their solid waste for local processing at the designated transfer station, even if the ultimate destination of solid waste was an out-of-state disposal site. (C) The primary rationale for forcing in-state waste into the designated private transfer station was financial; it was seen as a device to raise revenue to finance the transfer station.\n\nThe Town of Clarkstown's ordinance was designed and written right in the teeth of the long line of Supreme Court cases which had historically struck down local processing requirements. In short, it was as if the authors of the ordinance had gone to a treatise on the commerce clause and intentionally chosen a device which had been traditionally prohibited. A long line of Supreme Court case law had struck down local processing requirements when applied to goods or services in interstate commerce. As the Court in Carbone wrote:\n\nWe consider a so-called flow control ordinance, which requires all solid waste to be processed at a designated transfer station before leaving the municipality. The avowed purpose of the ordinance is to retain the processing fees charged at the transfer station to amortize the cost of the facility. Because it attains this goal by depriving competitors, including out-of-state firms, of access to a local market, we hold that the flow control ordinance violates the Commerce Clause.\n\nThe Court plainly regarded the decision as a relatively unremarkable decision, not a bold stroke. As the Court wrote: \"The case decided today, while perhaps a small new chapter in that course of decisions, rests nevertheless upon well-settled principles of our Commerce Clause jurisprudence.\" And, the Court made it plain, that the problem with Clarkstown's ordinance was that it created a local processing requirement protective of a local private processing company:\n\nIn this light, the flow control ordinance is just one more instance of local processing requirements that we long have held invalid ... The essential vice in laws of this sort is that they bar the import of the processing service. Out-of-state meat inspectors, or shrimp hullers, or milk pasteurizers, are deprived of access to local demand for their services. Put another way, the offending local laws hoard a local resource—be it meat, shrimp, or milk—for the benefit of local businesses that treat it. 511 U.S. at 392–393.\n\nThe Court's 2007 decision in \"United Haulers Association v. Oneida-Herkimer Solid Waste Management Authority\" starkly illustrates the difference in result when the Court finds that local regulation is not discriminatory. The Court dealt with a flow control regimen quite similar to that considered in Carbone. The \"only salient difference is that the laws at issue here require haulers to bring waste to facilities owned and operated by a state-created public benefit corporation.\" The Court decided that the balancing test should apply, because the regulatory scheme favored the government owned facility, but treated all private facilities equally.\n\nCompelling reasons justify treating these laws differently from laws favoring particular private businesses over their competitors. \"Conceptually, of course, any notion of discrimination assumes a comparison of substantially similar entities.\" \"General Motors Corp. v. Tracy\", 519 U.S. 278 (1997). But States and municipalities are not private businesses—far from it. Unlike private enterprise, government is vested with the responsibility of protecting the health, safety, and welfare of its citizens. See \"Metropolitan Life Ins. Co. v. Massachusetts\", 471 U.S. 724 (1985) ... These important responsibilities set state and local government apart from a typical private business.\n\nThe Court's United Haulers decision demonstrates an understanding of the regulatory justifications for flow control starkly missing in the Carbone decision:\n\nBy the 1980s, the Counties confronted what they could credibly call a solid waste \" 'crisis.' \"... Many local landfills were operating without permits and in violation of state regulations. Sixteen were ordered to close and remediate the surrounding environment, costing the public tens of millions of dollars. These environmental problems culminated in a federal clean-up action against a landfill in Oneida County; the defendants in that case named over local businesses and several municipalities and school districts as third-party defendants The \"crisis\" extended beyond health and safety concerns. The Counties had an uneasy relationship with local waste management companies, enduring price fixing, pervasive overcharging, and the influence of organized crime. Dramatic price hikes were not uncommon: In 1986, for example, a county contractor doubled its waste disposal rate on six weeks' notice\n\nThe Court would not interfere with local government's efforts to solve an important public and safety problem.\n\nThe contrary approach of treating public and private entities the same under the dormant Commerce Clause would lead to unprecedented and unbounded interference by the courts with state and local government. The dormant Commerce Clause is not a roving license for federal courts to decide what activities are appropriate for state and local government to undertake, and what activities must be the province of private market competition. In this case, the citizens of Oneida and Herkimer Counties have chosen the government to provide waste management services, with a limited role for the private sector in arranging for transport of waste from the curb to the public facilities. The citizens could have left the entire matter for the private sector, in which case any regulation they undertook could not discriminate against interstate commerce. But it was also open to them to vest responsibility for the matter with their government, and to adopt flow control ordinances to support the government effort. It is not the office of the Commerce Clause to control the decision of the voters on whether government or the private sector should provide waste management services. \"The Commerce Clause significantly limits the ability of States and localities to regulate or otherwise burden the flow of interstate commerce, but it does not elevate free trade above all other values.\"\n\nThe history of commerce clause jurisprudence evidences a distinct difference in approach where the state is seeking to exercise its public health and safety powers, on the one hand, as opposed to attempting to regulate the flow of commerce. The exact dividing line between the two interests, the right of states to exercise regulatory control over their public health and safety, and the interest of the national government in unfettered interstate commerce is not always easy to discern. One Court has written as follows:\n\nNot surprisingly, the Court's effort to preserve a national market has, on numerous occasions, come into conflict with the states' traditional power to \"legislat[e] on all subjects relating to the health, life, and safety of their citizens.\" \"Huron Portland Cement Co. v. City of Detroit\", 362 U.S. 440, 443 (1960). On these occasions, the Supreme Court has \"struggled (to put it nicely) to develop a set of rules by which we may preserve a national market without needlessly intruding upon the States' police powers, each exercise of which no doubt has some effect on the commerce of the Nation.\" \"Camps Newfound/Owatonna v. Town of Harrison\", 520 U.S. 564, 596 (1997) (Scalia, J., dissenting) (citing \"Okla. Tax Comm'n v. Jefferson Lines\", 514 U.S. 175, 180–83 (1995)); see generally Boris I. Bittker, Regulation of Interstate and Foreign Commerce § 6.01[A], at 6–5 (\"[T]he boundaries of the [State's] off-limits area are, and always have been, enveloped in a haze.\"). Those rules are \"simply stated, if not simply applied.\" Camps Newfound/Owatonna, 520 U.S. at 596 (Scalia, J., dissenting).\n\nA frequently cited example of the deference afforded to the powers of state and local government may be found in \"Exxon Corp. v. Maryland\", 437 U.S. 117 (1978), where the State of Maryland barred producers of petroleum products from operating retail service stations in the state. It is difficult to imagine a regimen which might have greater impact on the way in which markets are organized. Yet, the Court found the legislation constitutionally permitted: \"The fact that the burden of a state regulation falls on some interstate companies does not, by itself establish a claim of discrimination against interstate commerce,\" the Court wrote. The \"Clause protects interstate market, not particular interstate firms, from prohibitive or burdensome regulations.\"\n\nSimilarly, in \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456 (1981) the Court upheld a state law that banned nonreturnable milk containers made of plastic but permitted other nonreturnable milk containers. The Court found that the existence of a burden on out-of-state plastic industry was not 'clearly excessive' in comparison to the state's interest in promoting conservation. And the court continued:\n\nIn Exxon, the Court stressed that the Commerce Clause protects the interstate market, not particular interstate firms, from prohibitive or burdensome regulations. A nondiscriminatory regulation serving substantial state purpose is not invalid simply because it causes some business to shift from a predominantly out-of-state industry to a predominantly in-state industry. Only if the burden on interstate commerce clearly outweighs the State's legitimate purpose does such a regulation violate the commerce clause. When a state statute regarding safety matters applies equally to interstate and intrastate commerce, the courts are generally reluctant to invalidate it even if it may have some impact on interstate commerce. In \"Bibb v. Navajo Freight Lines\" 359 U.S. 520, 524 (1959), the United States Supreme Court stated: 'These safety measures carry a strong presumption of validity when challenged in court. If there are alternative ways of solving a problem, we do not sit to determine which of them is best suited to achieve a valid state objective. Policy decisions are for the state legislature, absent federal entry into the field. Unless we can conclude on the whole record that \"the total effect of the law as a safety measure in reducing accidents and casualties is so slight or problematical as not to outweigh the national interest in keeping interstate commerce free from interferences which seriously impede it\" we must uphold the statute.\n\nThere are two notable exceptions to the dormant Commerce Clause doctrine that can permit state laws or actions that otherwise violate the Dormant Commerce Clause to survive court challenges.\n\nThe first exception occurs when Congress has legislated on the matter. See \"Western & Southern Life Ins. v. State Board of California\", . In this case the Dormant Commerce Clause is no longer \"dormant\" and the issue is a Commerce Clause issue, requiring a determination of whether Congress has approved, preempted, or left untouched the state law at issue.\n\nThe second exception is \"market participation exception\". This occurs when the state is acting \"in the market\", like a business or customer, rather than as a \"market regulator\". For example, when a state is contracting for the construction of a building or selling maps to state parks, rather than passing laws governing construction or dictating the price of state park maps, it is acting \"in the market\". Like any other business in such cases, a state may favor or shun certain customers or suppliers.\n\nThe Supreme Court introduced the market participant doctrine in \"Hughes v. Alexandria Scrap Corp.\", 426 U.S. 794 (1976), which upheld a Maryland program that offered bounties to scrap processors to destroy abandoned automobile hulks. See also \"Wisconsin Dep't of Indus., Labor & Human Relations v. Gould Inc.\", 475 U.S. 282, 289 (1986); \"Reeves, Inc. v. Stake\", 447 U.S. 429, 437 (1980). Because Maryland required out-of-state processors, but not in-state processors, to submit burdensome documentation to claim their bounties, the state effectively favored in-state processors over out-of-state processors. The Court held that because the state was merely attaching conditions to its expenditure of state funds, the Maryland program affected the market no differently than if Maryland were a private company bidding up the price of auto hulks. Because the state was not \"regulating\" the market, its economic activity was not subject to the anti-discrimination principles underlying the dormant Commerce Clause—and the state could impose different paperwork burdens on out-of-state processors. \"Nothing in the purposes animating the Commerce Clause prohibits a State, in the absence of congressional action, from participating in the market and exercising the right to favor its own citizens over others.\"\n\nAnother important case is \"White v. Massachusetts Council of Constr. Employers, Inc.\", in which the Supreme Court held that the City of Boston could require its building contractors to hire at least fifty percent of their workforce from among Boston residents. 460 U.S. at 214–15. Because all of the employees covered by that mandate were \"in a substantial if informal sense, 'working for the city,' \" Boston was considered to be simply favoring its own residents through the expenditures of municipal funds. The Supreme Court stated, \"when a state or local government enters the market as a participant it is not subject to the restraints of the Commerce Clause.\" Id. at 208. Nothing in the Constitution precludes a local government from hiring a local company precisely because it is local.\n\nOther important cases enunciating the market participation exception principle are \"Reeves, Inc. v. Stake\", and \"South-Central Timber Development, Inc. v. Wunnicke\", . The \"Reeves\" case outlines the market participation exception test. In this case state-run cement co-ops were allowed to make restrictive rules (e.g. rules not to sell out-of-state). Here, this government-sponsored business was acting restrictively like an individually owned business and this action was held to be constitutional. \"South-Central Timber\" is important because it limits the market exception. \"South-Central Timber\" holds that the market-participant doctrine is limited in allowing a State to impose burdens on commerce within the market in which it is a participant, but allows it to go no further. The State may not impose conditions that have a substantial regulatory effect outside of that particular market.\n\nThe \"market participation exception\" to the dormant Commerce Clause does not give states unlimited authority to favor local interests, because limits from other laws and Constitutional limits still apply. In \"United Building & Construction Trades Council v. Camden\", , the city of Camden, New Jersey had passed an ordinance requiring that at least forty percent of the employees of contractors and subcontractors on city projects be Camden residents. The Supreme Court found that while the law was not infirm because of the Dormant Commerce Clause, it violated the Privileges and Immunities Clause of Article IV of the Constitution. Justice Rehnquist's opinion distinguishes the market-participant doctrine from the privileges and immunities doctrine. Similarly, Congress has the power itself under the Commerce Clause to regulate and sanction states acting as \"market participants\", but it lacks power to legislate in ways that violate Article IV.\n\nIn the 21st century, the dormant Commerce Clause has been a frequent legal issue in cases arising under state laws regulating some aspects of Internet activity. Because of the interstate, and often international, nature of Internet communications, state laws addressing internet-related subjects such as spam, online sales or online pornography can often trigger Dormant Commerce Clause issues.\n\nA \"negative\" or \"dormant\" component to the Commerce Clause has been the subject of scholarly discussion for many decades. Both Supreme Court Justices Antonin Scalia and Clarence Thomas have rejected the notion of a Dormant Commerce Clause. They believe that such a doctrine is inconsistent with an originalist interpretation of the Constitution—so much so that they believe the doctrine is a \"judicial fraud\".\n\nA number of earlier Supreme Court justices also expressed dissatisfaction with the dormant Commerce Clause doctrine. For example, Chief Justice Taney said this in 1847:\n\nIf it was intended to forbid the States from making any regulations of commerce, it is difficult to account for the omission to prohibit it, when that prohibition has been so carefully and distinctly inserted in relation to other powers ... [T]he legislation of Congress and the States has conformed to this construction from the foundation of the government ... The decisions of this court will also, in my opinion, when carefully examined, be found to sanction the construction I am maintaining.\n\nHowever, that statement by Taney in 1847 was before the doctrine morphed in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court that the Commerce Clause does not always require \"exclusive legislation by Congress\".\n\nIn \"Trailer Marine Transport Corp. v. Rivera Vázquez\", 977 F.2d 1, 7-8 (1st Cir. 1992), the First Circuit held that the dormant Commerce Clause applies to Puerto Rico.\n\n\n"}
{"id": "638834", "url": "https://en.wikipedia.org/wiki?curid=638834", "title": "Economic model", "text": "Economic model\n\nIn economics, a model is a theoretical construct representing economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified, often mathematical, framework designed to illustrate complex processes. Frequently, economic models posit structural parameters. A model may have various exogenous variables, and those variables may change to create various responses by economic variables. Methodological uses of models include investigation, theorizing, and fitting theories to the world.\n\nIn general terms, economic models have two functions: first as a simplification of and abstraction from observed data, and second as a means of selection of data based on a paradigm of econometric study.\n\n\"Simplification\" is particularly important for economics given the enormous complexity of economic processes. This complexity can be attributed to the diversity of factors that determine economic activity; these factors include: individual and cooperative decision processes, resource limitations, environmental and geographical constraints, institutional and legal requirements and purely random fluctuations. Economists therefore must make a reasoned choice of which variables and which relationships between these variables are relevant and which ways of analyzing and presenting this information are useful.\n\n\"Selection\" is important because the nature of an economic model will often determine what facts will be looked at, and how they will be compiled. For example, inflation is a general economic concept, but to measure inflation requires a model of behavior, so that an economist can differentiate between changes in relative prices and changes in price that are to be attributed to inflation.\n\nIn addition to their professional academic interest, the use of models include:\n\nA model establishes an \"argumentative framework\" for applying logic and mathematics that can be independently discussed and tested and that can be applied in various instances. Policies and arguments that rely on economic models have a clear basis for soundness, namely the validity of the supporting model.\n\nEconomic models in current use do not pretend to be \"theories of everything economic\"; any such pretensions would immediately be thwarted by computational infeasibility and the incompleteness or lack of theories for various types of economic behavior. Therefore, conclusions drawn from models will be approximate representations of economic facts. However, properly constructed models can remove extraneous information and isolate useful approximations of key relationships. In this way more can be understood about the relationships in question than by trying to understand the entire economic process.\n\nThe details of model construction vary with type of model and its application, but a generic process can be identified. Generally any modelling process has two steps: generating a model, then checking the model for accuracy (sometimes called diagnostics). The diagnostic step is important because a model is only useful to the extent that it accurately mirrors the relationships that it purports to describe. Creating and diagnosing a model is frequently an iterative process in which the model is modified (and hopefully improved) with each iteration of diagnosis and respecification. Once a satisfactory model is found, it should be double checked by applying it to a different data set.\n\nAccording to whether all the model variables are deterministic, economic models can be classified as stochastic or non-stochastic models; according to whether all the variables are quantitative, economic models are classified as discrete or continuous choice model; according to the model's intended purpose/function, it can be classified as\nquantitative or qualitative; according to the model's ambit, it can be classified as a general equilibrium model, a partial equilibrium model, or even a non-equilibrium model; according to the economic agent's characteristics, models can be classified as rational agent models, representative agent models etc.\n\n\nAt a more practical level, quantitative modelling is applied to many areas of economics and several methodologies have evolved more or less independently of each other. As a result, no overall model taxonomy is naturally available. We can nonetheless provide a few examples that illustrate some particularly relevant points of model construction.\n\n\n\nMost economic models rest on a number of assumptions that are not entirely realistic. For example, agents are often assumed to have perfect information, and markets are often assumed to clear without friction. Or, the model may omit issues that are important to the question being considered, such as externalities. Any analysis of the results of an economic model must therefore consider the extent to which these results may be compromised by inaccuracies in these assumptions, and a large literature has grown up discussing problems with economic models, or at least asserting that their results are unreliable.\n\nOne of the major problems addressed by economic models has been understanding economic growth. An early attempt to provide a technique to approach this came from the French physiocratic school in the Eighteenth century. Among these economists, François Quesnay should be noted, particularly for his development and use of tables he called \"Tableaux économiques\". These tables have in fact been interpreted in more modern terminology as a Leontiev model, see the Phillips reference below.\n\nAll through the 18th century (that is, well before the founding of modern political economy, conventionally marked by Adam Smith's 1776 Wealth of Nations) simple probabilistic models were used to understand the economics of insurance. This was a natural extrapolation of the theory of gambling, and played an important role both in the development of probability theory itself and in the development of actuarial science. Many of the giants of 18th century mathematics contributed to this field. Around 1730, De Moivre addressed some of these problems in the 3rd edition of \"The Doctrine of Chances\". Even earlier (1709), Nicolas Bernoulli studies problems related to savings and interest in the Ars Conjectandi. In 1730, Daniel Bernoulli studied \"moral probability\" in his book Mensura Sortis, where he introduced what would today be called \"logarithmic utility of money\" and applied it to gambling and insurance problems, including a solution of the paradoxical Saint Petersburg problem. All of these developments were summarized by Laplace in his Analytical Theory of Probabilities (1812). Clearly, by the time David Ricardo came along he had a lot of well-established math to draw from.\n\nIn the late 1980s the Brookings Institution compared 12 leading macroeconomic models available at the time. They compared the models' predictions for how the economy would respond to specific economic shocks (allowing the models to control for all the variability in the real world; this was a test of model vs. model, not a test against the actual outcome). Although the models simplified the world and started from a stable, known common parameters the various models gave significantly different answers. For instance, in calculating the impact of a monetary loosening on output some models estimated a 3% change in GDP after one year, and one gave almost no change, with the rest spread between.\n\nPartly as a result of such experiments, modern central bankers no longer have as much confidence that it is possible to 'fine-tune' the economy as they had in the 1960s and early 1970s. Modern policy makers tend to use a less activist approach, explicitly because they lack confidence that their models will actually predict where the economy is going, or the effect of any shock upon it. The new, more humble, approach sees danger in dramatic policy changes based on model predictions, because of several practical and theoretical limitations in current macroeconomic models; in addition to the theoretical pitfalls, (listed above) some problems specific to aggregate modelling are:\n\nComplex systems specialist and mathematician David Orrell wrote on this issue in his book Apollo's Arrow and explained that the weather, human health and economics use similar methods of prediction (mathematical models). Their systems—the atmosphere, the human body and the economy—also have similar levels of complexity. He found that forecasts fail because the models suffer from two problems : (i) they cannot capture the full detail of the underlying system, so rely on approximate equations; (ii) they are sensitive to small changes in the exact form of these equations. This is because complex systems like the economy or the climate consist of a delicate balance of opposing forces, so a slight imbalance in their representation has big effects. Thus, predictions of things like economic recessions are still highly inaccurate, despite the use of enormous models running on fast computers.\n\nEconomic and meteorological simulations may share a fundamental limit to their predictive powers: chaos. Although the modern mathematical work on chaotic systems began in the 1970s the danger of chaos had been identified and defined in \"Econometrica\" as early as 1958:\n\nIt is straightforward to design economic models susceptible to butterfly effects of initial-condition sensitivity.\n\nHowever, the econometric research program to identify which variables are chaotic (if any) has largely concluded that aggregate macroeconomic variables probably do not behave chaotically. This would mean that refinements to the models could ultimately produce reliable long-term forecasts. However the validity of this conclusion has generated two challenges:\n\nMore recently, chaos (or the butterfly effect) has been identified as less significant than previously thought to explain prediction errors. Rather, the predictive power of economics and meteorology would mostly be limited by the models themselves and the nature of their underlying systems (see Comparison with models in other sciences above).\n\nA key strand of free market economic thinking is that the market's invisible hand guides an economy to prosperity more efficiently than central planning using an economic model. One reason, emphasized by Friedrich Hayek, is the claim that many of the true forces shaping the economy can never be captured in a single plan. This is an argument that cannot be made through a conventional (mathematical) economic model, because it says that there are critical systemic-elements that will always be omitted from any top-down analysis of the economy.\n\n\n\n\n"}
{"id": "36188092", "url": "https://en.wikipedia.org/wiki?curid=36188092", "title": "Fa (concept)", "text": "Fa (concept)\n\nFa (;) is a concept in Chinese philosophy that covers ethics, logic, and law. It can be translated as \"law\" in some contexts, but more often as \"model\" or \"standard.\" First gaining importance in the Mohist school of thought, the concept was principally elaborated in Legalism. In Han Fei's philosophy, the king is the sole source of \"fa\" (law), taught to the common people so that there would be a harmonious society free of chance occurrences, disorder, and \"appeal to privilege\". High officials were not to be held above \"fa\" (law or protocol), nor were they to be allowed to independently create their own \"fa\", uniting both executive fiat and rule of law.\n\nXunzi, a philosopher that would end up being foundational in Han dynasty Confucianism, also took up \"fa\", suggesting that it could only be properly assessed by the Confucian sage (ruler), and that the most important \"fa\" were the very rituals that Mozi had ridiculed for their ostentatious waste and lack of benefit for the people at large.\n\nThe concept of \"fa\" first gained importance in the Mohist school of thought. To Mozi, a standard must stand \"three tests\" in order to determine its efficacy and morality. The first of these tests was its origin; if the standard had precedence in the actions or thought of the semi-mythological sage kings of the Xia dynasty whose examples are frequently cited in classical Chinese philosophy. The second test was one of validity; does the model stand up to evidence in the estimation of the people? The third and final test was one of applicability; this final one is a utilitarian estimation of the net good that, if implemented, the standard would have on both the people and the state.\n\nThe third test speaks to the fact that to the Mohists, a \"fa\" was not simply an abstract model, but an active tool. The real-world use and practical application of \"fa\" were vital. Yet \"fa\" as models were also used in later Mohist logic as principles used in deductive reasoning. As classical Chinese philosophical logic was based on analogy rather than syllogism, \"fa\" were used as benchmarks to determine the validity of logical claims through comparison. There were three \"fa\" in particular that were used by these later Mohists (or \"Logicians\") to assess such claims, which were mentioned earlier. The first was considered a \"root\" standard, a concern for precedence and origin. The second, a \"source\", a concern for empiricism. The third, a \"use\", a concern for the consequence and pragmatic utility of a standard. These three \"fa\" were used by the Mohists to both promote social welfare and denounce ostentation or wasteful spending.\n\n"}
{"id": "14162696", "url": "https://en.wikipedia.org/wiki?curid=14162696", "title": "Fluid Concepts and Creative Analogies", "text": "Fluid Concepts and Creative Analogies\n\nFluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought is a 1995 book by Douglas Hofstadter and other members of the Fluid Analogies Research Group exploring the mechanisms of intelligence through computer modeling. It contends that the notions of analogy and fluidity are fundamental to explain how the human mind solves problems and to create computer programs that show intelligent behavior. It analyzes several computer programs that members of the group have created over the years to solve problems that require intelligence.\n\nIt was the first book ever sold by Amazon.com.\n\nThe book is a collection of revised articles that appeared in precedence, each preceded by an introduction by Hofstadter.\nThey describe the scientific work by him and his collaborators in the 1980s and 1990s.\nThe project started in the late 1970s at Indiana University.\nIn 1983 he took a sabbatical year at MIT, working in Marvin Minsky's Artificial Intelligence Lab.\nThere he met and collaborated with Melanie Mitchell, who then became his doctoral student.\nSubsequently, Hofstadter moved to the University of Michigan, where the FARG (Fluid Analogies Research Group) was founded.\nEventually he returned to Indiana University in 1988, continuing the FARG research there.\nThe book was written during a sabbatical year at the Istituto per la Ricerca Scientifica e Tecnologica in Trento, Italy.\n\nUpon publication, Jon Udell, a BYTE senior technical editor-at-large said:\nFifteen years ago, \"Gödel, Escher, Bach: An Eternal Golden Braid\" exploded on the literary scene, earning its author a Pulitzer prize and a monthly column in \"Scientific American\". Douglas Hofstadter's exuberant synthesis of math, music, and art, and his inspired thought experiments with \"tangled hierarchy,\" recursion, pattern recognition, figure/ground reversal, and self-reference, delighted armchair philosophers and AI theorists. But in the end, many people believed that these intellectual games yielded no useful model of cognition on which to base future AI research. Now \"Fluid Concepts and Creative Analogies\" presents that model, along with the computer programs Hofstadter and his associates have designed to test it. These programs work in stripped-down yet surprisingly rich microdomains.\n\nOn April 3, 1995, \"Fluid Concepts and Creative Analogies\" became the first book ordered online by an Amazon.com customer.\n\n\nThe first AI project by Hofstadter stemmed from his teenage fascination with number sequences.\nWhen he was 17, he studied the way that triangular and square numbers interleave, and eventually found a recursive relation describing it.\nIn his first course on AI, he set to the students and to himself the task of writing a program that could extrapolate the rule by which a numeric sequence is generated.\nHe discusses breadth-first and depth-first techniques, but eventually concludes that the results represent expert systems that incarnate a lot of technical knowledge but don't shine much light on the mental processes that humans use to solve such puzzles.\n\nInstead he devised a simplified version of the problem, called SeekWhence, where sequences are based on very simple basic rules not requiring advanced mathematical knowledge.\nHe argues that pattern recognition, analogy, and fluid working hypotheses are fundamental to understand how humans tackle such problems.\n\nJumbo is a program to solve jumbles, word puzzles consisting in five or six scrambled letters that need to be anagrammed to form an English word.\nThe resulting word does not need to be a real one but just to a plausible, that is, to consists of a sequence of letters that is normal in English.\n\nThe constituent elements of Jumbo are the following:\nA \"temperature\" is associated to the present state of the cytoplasm; it determines how probable it is that a destructive codelet is executed.\nThere is a \"freezing\" temperature at which no destruction can occur anymore: a solution has been found.\n\nNumbo is a program by Daniel Defays that tries to solve numerical problems similar to those used in the French game \"Le compte est bon\". The game consists in combining some numbers called \"bricks\", using the operations of multiplication, addition, and subtraction, to obtain a given result.\n\nThe program is modeled on Jumbo and Copycat and uses a permanent network of known mathematical facts, a working memory in the form of a cytoplasm, and a coderack containing codelets to produce free associations of bricks in order to arrive at the result.\n\nThe chapter subtitle \"A Critique of Artificial-intelligence Methodology\" indicates that this is a polemical article, in which David Chalmers, Robert French, and Hofstadter criticize most of the research going on at that time (the early '80s) as exaggerating results and missing the central features of human intelligence.\n\nSome of these AI projects, like the structure mapping engine (SME), claimed to model high faculties of the human mind and to be able to understand literary analogies and to rediscover important scientific breakthroughs.\nIn the introduction, Hofstadter warns about the Eliza effect that leads people to attribute understanding to a computer program that only uses a few stock phrases.\nThe authors claim that the input data for such impressive results are already heavily structured in the direction of the intended discovery and only a simple matching task is left to the computer.\n\nTheir main claim is that it is impossible to model high-level cognition without at the same time modeling low-level perception.\nWhile cognition is necessarily based on perception, they argue that it in turn influences perception itself.\nTherefore, a sound AI project should try to model the two together.\nIn a slogan repeated several times throughout the book: \"cognition is recognition\".\n\nSince human perception is too complex to be modelled by available technology, they favor the restriction of AI projects to limited domains like the one used for the Copycat project.\n\nThis chapter presents, as stated in the full title, \"A Model of Mental Fluidity and Analogy-making\".\nIt is a description of the architecture of the Copycat program, developed by Hofstadter and Melanie Mitchell.\nThe field of application of the program is a domain of short alphabetic sequences.\nA typical puzzle is: \"If abc were changed to abd, how would you change ijk in the same way?\".\nThe program tries to find an answer using a strategy supposedly similar to the way the human mind tackles the question.\n\nCopycat has three major components:\nThe resulting software displays emergent properties.\nIt works according to a \"parallel terraced scan\" that runs several possible processes at the same time.\nIt shows mental fluidity in that concepts may \"slip\" into similar ones.\nIt emulates human behavior in tending to find the most obvious solutions most of the time but being more satisfied (as witnessed by low temperature) by more clever and deep answers that it finds more rarely.\n\nThis chapter compares Copycat with other recent (at the time) work in artificial intelligence.\nSpecifically, it matches it with the claimed results from the structure mapping engine SME and the Analogical Constraint Mapping Engine (ACME).\nThe authors' judgment is that those programs suffer from two defects: Their input is pre-structured by the developers to highlight the analogies that the software is supposed to find; and the general architecture of the programs is serial and deterministic rather than parallel and stochastic like Copycat's, which they consider psychologically more plausible.\n\nSevere criticism is put on the claim that these tools can solve \"real-life\" problems.\nIn fact, only the terms used in the example suggest that the input to the programs comes from a concrete situation.\nThe logical structures don't actually imply any meaning for the term.\n\nFinally a more positive assessment is given to two other projects: Indurkhya' PAN model and Kokinov's AMBR system.\n\nThis chapter looks at those aspects of human creativity that are not yet modeled by Copycat and lays down a research plan for a future extension of the software.\nThe main missing element is the mind's ability to observe itself and reflect on its own thinking process.\nAlso important is the ability to learn and to remember the results of the mental activity.\n\nThe creativity displayed in finding analogies should be applicable at ever higher levels: making analogies between analogies (expression inspired by the title of a book by Stanislaw Ulam), analogies between these second-order analogies, and so on.\n\nAnother of Hofstadter's students, Robert French, was assigned the task of applying the architecture of Copycat to a different domain, consisting in analogies between objects lying on a table in a coffeehouse.\nThe resulting program was named Tabletop.\n\nThe authors present a different and vaster domain to justify the relevance of attacking such a trivial-seeming project.\nThe alternative domain is called Ob-Platte and consists in discovering analogies between geographical locations in different regions or countries.\n\nOnce again arguments are offered against a brute-force approach, which would work on the small Tabletop domain but would become unfeasible on the larger Ob-Platte domain.\nInstead a parallel non-deterministic architecture is used, similar to the one adopted by the Copycat project.\n\nIn the premise to the chapter, title \"The Knotty Problem of Evaluating Research\", Hofstadter considers the question of how research in AI should be assessed.\nHe argues against a strict adherence to a match between the results of an AI program with the average answer of human test subjects.\nHe gives two reasons for his rejection: the AI program is supposed to emulate creativity, while an average of human responses will delete any original insight by any of the single subjects; and the architecture of the program should be more important that its mere functional description.\n\nIn the main article, the architecture of Tabletop is described: it is strongly inspired by that of Copycat and consists of a Slipnet, a Workspace, and a Corerack.\n\nThis last chapter is about a more ambitious project that Hofstadter started with student Gary McGraw.\nThe microdomain used is that of grid fonts: typographic alphabets constructed using a rigid system of small rigid components.\nThe goal is to construct a program that, given only a few or just one letter from the grid font, can generate the whole alphabet \"in the same style\".\nThe difficulty lies in the ambiguity and undefinability of \"style\".\nThe projected program would have a structure very similar to that of Jumble, Numble, Copycat, and Tabletop.\n\nIn the concluding part of the book, Hofstadter analyses some AI projects with a critical eye.\nHe finds that today's AI is missing the gist of human creativity and is making exaggerated claims.\nThe project under scrutiny are the following.\n\nAARON, a computer artist that can draw images of people in outdoor settings in a distinctive style reminiscent of that of a human artist; criticism: the program doesn't have any understanding of the objects it draws, it just uses some graphical algorithms with some randomness thrown in to generate different scenes at every run and to give the style a more natural feel.\n\nRacter, a computer author that wrote a book entitled \"The Policeman's Beard Is Half Constructed\".\nAlthough some of the prose generated by the program is quite impressive, due in part to the Eliza effect, the computer does not have any notion of plot or of the meaning of the words it uses. Furthermore, the book is made up of selected texts from thousands produced by the computer over several years.\n\nAM, a computer mathematician that generates new mathematical concepts. It managed to produce by itself the notion of prime number and the Goldbach conjecture. As with Racter, the question is how much the programmer filtered the output of the program, keeping only the occasional interesting output.\nAlso, mathematics being a very specialized domain, it is doubtful whether the techniques used can be abstracted to general cognition.\n\nAnother mathematical program, called Geometry, was celebrated for making an insightful discovery of an original proof that an isosceles triangle has equal base angles. The proof is based on seeing the triangle in two different ways. However, the program generates all possible ways of seeing the triangle, not even knowing that it is the same triangle.\n\nHofstadter concludes with some methodological remarks on the Turing Test.\nIn his opinion it is still a good definition and he argues that by interacting with a program, a human may be able to have insight not just on its behaviour but also on its structure.\nHowever, he criticises the use that is made of it at present: it encourages the development of fancy natural-language interfaces instead of the investigation of deep cognitive faculties.\n"}
{"id": "12656", "url": "https://en.wikipedia.org/wiki?curid=12656", "title": "Godwin's law", "text": "Godwin's law\n\nGodwin's law (or Godwin's rule of Hitler analogies) is an Internet adage asserting that \"As an online discussion grows longer, the probability of a comparison involving Nazis or Hitler approaches 1\"; that is, if an online discussion (regardless of topic or scope) goes on long enough, sooner or later someone will compare someone or something to Adolf Hitler or his deeds, the point at which effectively the discussion or thread often ends. Promulgated by the American attorney and author Mike Godwin in 1990, Godwin's law originally referred specifically to Usenet newsgroup discussions. It is now applied to any threaded online discussion, such as Internet forums, chat rooms, and comment threads, as well as to speeches, articles, and other rhetoric where \"reductio ad Hitlerum\" occurs.\n\nThere are many corollaries to Godwin's law, some considered more canonical (by being adopted by Godwin himself) than others. For example, there is a tradition in many newsgroups and other Internet discussion forums that, when a Hitler comparison is made, the thread is finished and whoever made the comparison loses whatever debate is in progress. This principle is itself frequently referred to as Godwin's law.\n\nGodwin's law itself can be abused as a distraction, diversion or even as censorship, fallaciously miscasting an opponent's argument as hyperbole when the comparisons made by the argument are actually appropriate. Similar criticisms of the \"law\" (or \"at least the distorted version which purports to prohibit all comparisons to German crimes\") have been made by the American lawyer, journalist, and author Glenn Greenwald.\n\nGodwin's law does not claim to articulate a fallacy; it is instead framed as a memetic tool to reduce the incidence of inappropriate hyperbolic comparisons. \"Although deliberately framed as if it were a law of nature or of mathematics,\" Godwin wrote, \"its purpose has always been rhetorical and pedagogical: I wanted folks who glibly compared someone else to Hitler to think a bit harder about the Holocaust.\"\n\nGodwin has stated that he introduced Godwin's law in 1990 as an experiment in memetics.\n\nIn 2012, \"Godwin's law\" became an entry in the third edition of the \"Oxford English Dictionary\".\n\nIn December 2015, Godwin commented on the Nazi and fascist comparisons being made by several articles about Republican presidential candidate Donald Trump, saying: \"If you're thoughtful about it and show some real awareness of history, go ahead and refer to Hitler when you talk about Trump, or any other politician.\" In August 2017, Godwin made similar remarks on social networking websites Facebook and Twitter with respect to the two previous days' Unite the Right rally in Charlottesville, Virginia, endorsing and encouraging efforts to compare its alt-right organizers to Nazis.\n\nIn October 2018, Godwin made a similar statement when someone asked him, via Twitter, if it would be OK to call Brazilian presidential candidate Jair Bolsonaro a \"nazi\", answering a direct question (\"So, just to be clear, is it OK to call Bolsonaro a nazi?\") with the portuguese word \"Sim!\" (meaning \"yes\" in English). \n\n\n"}
{"id": "7949372", "url": "https://en.wikipedia.org/wiki?curid=7949372", "title": "Humanitarian principles", "text": "Humanitarian principles\n\nThere are a number of meanings for the term humanitarian. Here humanitarian pertains to the practice of saving lives and alleviating suffering. It is usually related to emergency response (also called humanitarian response) whether in the case of a natural disaster or a man-made disaster such as war or other armed conflict. Humanitarian principles govern the way humanitarian response is carried out.\n\nThe principle of humanity means that all humankind shall be treated humanely and equally in all circumstances by saving lives and alleviating suffering, while ensuring respect for the individual. It is the fundamental principle of humanitarian response.\n\nThe Code of Conduct for the International Red Cross and Red Crescent Movement and NGOs in Disaster Relief (RC/NGO Code) introduces the concept of the humanitarian imperative which expands the principle of humanity to include the right to receive and to give humanitarian assistance. It states the obligation of the international community \"to provide humanitarian assistance wherever it is needed.\"\n\nProvision of humanitarian assistance must be impartial and not based on nationality, race, religion, or political point of view. It must be based on need alone.\n\nFor most non-governmental humanitarian agencies (NGHAs), the principle of impartiality is unambiguous even if it is sometimes difficult to apply, especially in rapidly changing situations. However, it is no longer clear which organizations can claim to be humanitarian. For example, companies like PADCO, a USAID subcontractor, is sometimes seen as a humanitarian NGO. However, for the UN agencies, particularly where the UN is involved in peace keeping activities as the result of a Security Council resolution, it is not clear if the UN is in position to act in an impartial manner if one of the parties is in violation of terms of the UN Charter.\n\nHumanitarian agencies must formulate and implement their own policies independently of government policies or actions.\n\nProblems may arise because most NGHAs rely in varying degrees on government donors. Thus for some organizations it is difficult to maintain independence from their donors and not be confused in the field with governments who may be involved in the hostilities. The ICRC, has set the example for maintaining its independence (and neutrality) by raising its funds from governments through the use of separate annual appeals for headquarters costs and field operations.\n\nThe core principles are defining characteristics, the necessary conditions for humanitarian response. Organizations such as military forces and for-profit companies may deliver assistance to communities affected by disaster in order to save lives and alleviate suffering, but they are not considered by the humanitarian sector as humanitarian agencies as their response is not based on the core principles.\n\nIn addition to the core principles, there are other principles that govern humanitarian response for specific types of humanitarian agencies such as UN agencies, the Red Cross and Red Crescent Movement, and NGOs.\n\nThe International Red Cross and Red Crescent Movement follows, in addition to the above core principles, the principle of neutrality. For the Red Cross, neutrality means not to take sides in hostilities or engage at any time in controversies of a political, racial, religious or ideological nature.\n\nThe principle of neutrality was specifically addressed to the Red Cross Movement to prevent it from not only taking sides in a conflict, but not to \"engage at any time in controversies of a political, racial, religious or ideological nature.\" The principle of neutrality was left out of the Red Cross/NGO code because some of the NGHAs, while committed to giving impartial assistance, were not ready to forgo their lobbying on justice issues related to political and ideological questions.\n\nUnited Nations General Assembly Resolution 46/182 lists the principle of neutrality, alongside the principles of humanity and impartiality in its annex as a guide to the provision of humanitarian assistance. The resolution is designed to strengthen human response of the UN system, and it clearly applies to the UN agencies.\n\nNeutrality can also apply to humanitarian actions of a state. \"Neutrality remains closely linked with the definition which introduced the concept into international law to designate the status of a State which decided to stand apart from an armed conflict. Consequently, its applications under positive law still depend on the criteria of abstention and impartiality which have characterized neutrality from the outset.\"\n\nThe application of the word neutrality to humanitarian aid delivered by UN agencies or even governments can be confusing. GA Resolution 46/182 proclaims the principle of neutrality, yet as an inter-governmental political organization, the UN is often engaged in controversies of a political nature. According to this interpretation, the UN agency or a government can provide neutral humanitarian aid as long as it does it impartially, based upon need alone.\n\nToday, the word neutrality is widely used within the humanitarian community, usually to mean the provision of humanitarian aid in an impartial and independent manner, based on need alone. Few international NGOs have curtailed work on justice or human rights issues because of their commitment to neutrality.\n\nThe provision of aid must not exploit the vulnerability of victims and be used to further political or religious creeds. All of the major non-governmental humanitarian agencies (NGHAs) by signing up to the RC/NGO Code of Conduct have committed themselves not to use humanitarian response to further political or religious creeds.\n\nAll of the above principles are important requirements for effective field operations. They are based on widespread field experience of agencies engaged in humanitarian response. In conflict situations, their breach may drastically affect the ability of agencies to respond to the needs of the victims.\n\nIf a warring party believes, for example, that an agency is favoring the other side, or that it is an agent of the enemy, access to the victims may be blocked and the lives of humanitarian workers may be put in danger. If one of the parties perceives that an agency is trying to spread another religious faith, there may be a hostile reaction to their activities.\n\nThe core principles, found in the Red Cross/NGO Code of Conduct and in GA Resolution 46/182 are derived from the Fundamental Principles of the Red Cross, particularly principles I (humanity), II (impartiality), III (neutrality—in the case of the UN), and IV (independence).\n\nAccountability has been defined as: \"the processes through which an organisation makes a \ncommitment to respond to and balance the needs of stakeholders in its decision making processes and activities, and delivers against this commitment.\" Humanitarian Accountability Partnership International adds: \"Accountability is about using power responsibly.\"\n\nArticle 9 of the Code of Conduct for the International Red Cross and Red Crescent Movement and NGOs in Disaster Relief states:\n\"We hold ourselves accountable to both those we seek to assist and those from whom we accept resources;\" and thus identifies the two major stake holders: donors and beneficiaries. However, traditionally humanitarian agencies have tended to practice mainly \"upward accountability\", i.e. to their donors.\n\nThe experience of many humanitarian agencies during the Rwandan Genocide, led to a number of initiatives designed to improve humanitarian assistance and accountability, particularly with respect to the beneficiaries. Examples include the Sphere Project, ALNAP, Compas, the People In Aid Code of Good Practice, and the Humanitarian Accountability Partnership International, which runs a \"global quality insurance scheme for humanitarian agencies.\"\n\nThe RC/NGO Code also lists a number of more aspirational principles which are derived from experience with development assistance.\n\nThe Sphere Project Humanitarian Charter uses the language of human rights to remind that the right to life which is proclaimed in both the Universal Declaration of Human Rights and the International Convention on Civil and Political Rights is related to human dignity.\n\nHumanitarian principles are mainly focused on the behavior of organizations. However a humane response implies that humanitarian workers are not to take advantage of the vulnerabilities of those affected by war and violence. Agencies have the responsibility for developing rules of staff conduct which prevent abuse of the beneficiaries.\n\nOne of the most problematic areas is related to the issue of sexual exploitation and abuse of beneficiaries by humanitarian workers. In an emergency where victims have lost everything, women and girls are particularly vulnerable to sexual abuse.\n\nA number of reports which identified the sexual exploitation of refugees in west Africa prodded the humanitarian community to work together in examining the problem and to take measures to prevent abuses. In July 2002, the UN's Interagency Standing Committee (IASC) adopted a plan of action which stated: Sexual exploitation and abuse by humanitarian workers constitute acts of gross misconduct and are therefore grounds for termination of employment. The plan explicitly prohibited the \"Exchange of money, employment, goods, or services for sex, including sexual favours or other forms of humiliating, degrading or exploitative behaviour.\" The major NGHAs as well the UN agencies engaged in humanitarian response committed themselves to setting up internal structures to prevent sexual exploitation and abuse of beneficiaries.\n\nSubstantial efforts have been made in the humanitarian sector to monitor compliance with humanitarian principles. Such efforts include The People In Aid Code of Good Practice, an internationally recognised management tool that helps humanitarian and development organisations enhance the quality of their human resources management. The NGO, Humanitarian Accountability Partnership International, is also working to make humanitarian organizations more accountable, especially to the beneficiaries.\n\nStructures internal to the Red Cross Movement monitor compliance to the Fundamental Principles of the Red Cross.\n\nThe RC/NGO Code is self-enforcing. The SCHR carries out peer reviews among its members which look in part at the issue of compliance with principles set out in the RC/NGO Code\n\n"}
{"id": "369116", "url": "https://en.wikipedia.org/wiki?curid=369116", "title": "Hume's fork", "text": "Hume's fork\n\nHume's fork is an explanation, developed by later philosophers, of David Hume's 1730s division of \"relations of ideas\" from \"matters of fact and real existence\". A distinction is made between necessary versus contingent (concerning reality), versus (concerning knowledge), and analytic versus synthetic (concerning language). Relations of abstract ideas align on one side (necessary, \"a priori\", analytic), whereas concrete truths align on the other (contingent, \"a posteriori\", synthetic).\n\nThe \"necessary\" is generally true in all possible worlds—usually by mere logical validity—whereas the \"contingent\" hinges on the way the real world is. The \"a priori\" is knowable before or without, whereas the \"a posteriori\" is knowable only after or through, experience in an area of interest. The \"analytic\" is a statement true by virtue of its terms' meanings, and therefore a tautology—necessarily true but uninformative—whereas the \"synthetic\" is true by its terms' meanings in relation to a state of facts. In other words, analytic propositions are true by virtue of their meaning, while synthetic propositions are true by how their meaning relates to the world. Philosophers have used the terms differently, and there is debate over whether there is a legitimate distinction. \n\nHume's strong empiricism, as in Hume's fork as well as Hume's problem of induction, was taken as a threat to Newton's theory of motion. Immanuel Kant responded with rationalism in his 1781 \"Critique of Pure Reason\", where Kant attributed to the mind a causal role in sensory experience by the mind's aligning the environmental input by arranging those sense data into the experience of space and time. Kant thus reasoned existence of the synthetic \"a priori\"—combining meanings of terms with states of facts, yet known true without experience of the particular instance—replacing the two prongs of Hume's fork with a three-pronged-fork thesis (Kant's pitchfork) and thus saving Newton's law of universal gravitation.\n\nIn 1919, Newton's theory fell to Einstein's general theory of relativity. In the late 1920s, the logical positivists rejected Kant's synthetic \"a priori\" and asserted Hume's fork, so called, while hinging it at language—the analytic/synthetic division—while presuming that by holding to analyticity, they could develop a logical syntax entailing, as a consequence of Hume's fork, both necessity and aprioricity, thus restricting science to claims verifiable as either false or true. In the early 1950s, Willard Van Orman Quine undermined the analytic/synthetic division by explicating ontological relativity, as every term in any statement has its meaning contingent on a vast network of knowledge and belief, the speaker's conception of the entire world. By the early 1970s, Saul Kripke established the necessary \"a posteriori\", since if the Morning Star and the Evening Star are the same star, they are the same star by necessity, but this is known true by a human only through relevant experience.\n\nHume's fork remains basic in Anglo-American philosophy. Many deceptions and confusions are foisted by surreptitious or unwitting conversion of a synthetic claim to an analytic claim, rendered true by necessity but merely a tautology, for instance the \"No true Scotsman\" move. Simply put, Hume's fork has limitations. Related concerns are Hume's distinction of demonstrative versus probable reasoning and Hume's law. Hume makes other, important two-category distinctions, such as beliefs versus desires and as impressions versus ideas.\n\nThe first distinction is between two different areas of human study:\n\nHume's fork is often stated in such a way that statements are divided up into two types:\n\n\nIn modern terminology, members of the first group are known as analytic propositions and members of the latter as synthetic propositions. This terminology comes from Kant (Introduction to \"Critique of Pure Reason\", Section IV).\n\nInto the first class fall statements such as \"all bodies are extended\", \"all bachelors are unmarried\", and truths of mathematics and logic. Into the second class fall statements like \"the sun rises in the morning\", and \"all bodies have mass\".\n\nHume wants to prove that certainty does not exist in science. First, Hume notes that statements of the second type can never be entirely certain, due to the fallibility of our senses, the possibility of deception (see e.g. the modern brain in a vat theory) and other arguments made by philosophical skeptics. It is always logically possible that any given statement about the world is false.\n\nSecond, Hume claims that our belief in cause-and-effect relationships between events is not grounded on reason, but rather arises merely by habit or custom. Suppose one states: \"Whenever someone on earth lets go of a stone it falls.\" While we can grant that in every instance thus far when a rock was dropped on Earth it went down, this does not make it logically necessary that in the future rocks will fall when in the same circumstances. Things of this nature rely upon the future conforming to the same principles which governed the past. But that isn't something that we can know based on past experience—all past experience could tell us is that in the past, the future has resembled the past.\n\nThird, Hume notes that relations of ideas can be used only to prove other relations of ideas, and mean nothing outside of the context of how they relate to each other, and therefore tell us nothing about the world. Take the statement \"An equilateral triangle has three sides of equal length.\" While some earlier philosophers (most notably Plato and Descartes) held that logical statements such as these contained the most formal reality, since they are always true and unchanging, Hume held that, while true, they contain no formal reality, because the truth of the statements rests on the definitions of the words involved, and not on actual things in the world, since there is no such thing as a true triangle or exact equality of length in the world. So for this reason, relations of ideas cannot be used to prove matters of fact.\n\nThe results claimed by Hume as consequences of his fork are drastic. According to him, relations of ideas can be proved with certainty (by using other relations of ideas), however, they don't really mean anything about the world. Since they don't mean anything about the world, relations of ideas cannot be used to prove matters of fact. Because of this, matters of fact have no certainty and therefore cannot be used to prove anything. Only certain things can be used to prove other things for certain, but only things about the world can be used to prove other things about the world. But since we can't cross the fork, nothing is both certain and about the world, only one or the other, and so it is impossible to prove something about the world with certainty.\n\nIf accepted, Hume's fork makes it pointless to try to prove the existence of God (for example) as a matter of fact. If God is not literally made up of physical matter, and does not have an observable effect on the world, making a statement about God is not a matter of fact. Therefore, a statement about God must be a relation of ideas. In this case if we prove the statement \"God exists,\" it doesn't really tell us anything about the world; it is just playing with words. It is easy to see how Hume's fork voids the causal argument and the ontological argument for the existence of a non-observable God. However, this does not mean that the validity of Hume's fork would imply that God definitely does not exist, only that it would imply that the existence of God cannot be proven as a matter of fact without worldly evidence. \n\nHume rejected the idea of any meaningful statement that did not fall into this schema, saying:\nIf we take in our hand any volume; of divinity or school metaphysics, for instance; let us ask, Does it contain any abstract reasoning concerning quantity or number? No. Does it contain any experimental reasoning concerning matter of fact and existence? No. Commit it then to the flames: for it can contain nothing but sophistry and illusion. — \"An Enquiry Concerning Human Understanding\"\n"}
{"id": "41627505", "url": "https://en.wikipedia.org/wiki?curid=41627505", "title": "Imago Universi", "text": "Imago Universi\n\nAndreas Cellarius, German mathematician and cartographer (1596–1665), conceived an Atlas of the Universe, published in 1660, under the title of \"Harmonia Macrocosmica\". Numerous illustrations of the solar system appear in this atlas by different authors known at that time. Referring to Ptolemy, Cellarius called the representation of this Ptolemaic conception of heaven as \"Imago universi secundum Ptolaeum\"\n\n\"Imago\" is a word in Latin which means\" 'image\"' or even \"representation\". Therefore, the title expresses the \"Picture of the Universe according to Ptolemy.\" The Latin expression was used in the Middle Ages to express the representation and size of the known world at that time.\n\n\"Imago Universi\" is also the title, in Latin, of a cosmographic treatise, written in 2013 by the Spanish scientist Gabriel Barceló.\n\nAfter analyzing the history of cosmology, the treatise delves into the prevailing scientific lack of explanation of the rotation of the heavenly bodies in the laws of dynamic behaviour of the sidereal system. The author proposes the application of the Theory of Dynamic Interactions (TID) to astrophysics, in particular, the dynamics of stellar systems and galaxies. This theory allows new comprehension of the dynamics of nature and understands the dynamic equilibrium of the universe, always subjected to rotational accelerations, but repetitive and persistent. The author also highlights that the orbiting always coincides with the intrinsic rotation of celestial bodies. Paradox incorporating the book, noting that this had not been found to date.\n\n\n1. Einstein, Albert: The Origins of the General Theory of Relativity, lecture given at the George A. Foundation Gibson, University of Glasgow, 20 June 1933. Published by Jackson, Wylie and co, Glasgow, 1933.\n\n"}
{"id": "30679665", "url": "https://en.wikipedia.org/wiki?curid=30679665", "title": "Inherent bad faith model", "text": "Inherent bad faith model\n\nThe inherent bad faith model of information processing is a theory in political psychology that was first put forth by Ole Holsti to explain the relationship between John Foster Dulles' beliefs and his model of information processing. \n\nIt is the most widely studied model of one's opponent. A state is presumed to be implacably hostile, and contra-indicators of this are ignored. They are dismissed as propaganda ploys or signs of weakness. An example is John Foster Dulles' position regarding the Soviet Union.\n\n"}
{"id": "34074829", "url": "https://en.wikipedia.org/wiki?curid=34074829", "title": "Lexical entrainment", "text": "Lexical entrainment\n\nLexical entrainment is the phenomenon in conversational linguistics of the process of the subject adopting the reference terms of their interlocutor. In practice, it acts as a mechanism of the cooperative principle in which both parties to the conversation employ lexical entrainment as a progressive system to develop \"conceptual pacts\" (a working temporary conversational terminology) to ensure maximum clarity of reference in the communication between the parties; this process is necessary to overcome the ambiguity inherent in the multitude of synonyms that exist in language. \n\nLexical entrainment arises by two cooperative mechanisms: \n\nOnce lexical entrainment has come to determine the phrasing for a referent, both parties will use that terminology for the referent for the duration, even if it proceeds to violate the Gricean maxim of quantity. For example, if one wants to refer to a brown loafer out of a set of shoes that consist of: the loafer, a sneaker, and a high-heeled shoe, they will not use \"the shoe\" to describe the object as this phrasing does not unambiguously describe one item in the set under consideration. They will also not call the object \"the brown loafer\" which would violate Grice's maxim of quantity. The speaker will settle on using the term \"the loafer\" as it is just informative enough without giving too much information. \n\nAnother important factor is lexical availability; the ease of conceptualizing a referent in a certain way and then retrieving and producing a label for it. For many objects the most available labels are basic nouns; for example, the word \"dog\". Instead of saying \"animal\" or \"husky\" for the referent, most subjects will default to \"dog\".\nIf in a set of objects one is to refer to either a husky, a table, and a poster, people are still most likely to use the word \"dog.\" This is technically a violation of Grice's maxim of quantity, as using the term \"animal\" is ideal.\n\nLexical entrainment has applications in natural language processing in computers as well as human–human interaction. Currently, the adaptability of computers to modify their referencing to the terms of their human interlocutor is limited, so the entrainment adaptation falls to the human operator; this phenomenon is readily demonstrated in Brennan's 1996 experiment.\n"}
{"id": "29594530", "url": "https://en.wikipedia.org/wiki?curid=29594530", "title": "Logical hexagon", "text": "Logical hexagon\n\nThe logical hexagon (also called the hexagon of opposition) is a conceptual model of the relationships between the truth values of six statements. It is an extension of Aristotle's square of opposition. It was discovered independently by both Augustin Sesmat and Robert Blanché.\n\nThis extension consists in introducing two statements U and Y. Whereas U is the disjunction of A and E, Y is the conjunction of the two traditional particulars I and O.\n\nThe traditional square of opposition demonstrates two sets of contradictories A and O, and E and I (i.e. they cannot both be true and cannot both be false), two contraries A and E (i.e. they can both be false, but cannot both be true), and two subcontraries I and O (i.e. they can both be true, but cannot both be false) according to Aristotle’s definitions. However, the logical hexagon provides that U and Y are also contradictory.\n\nThe logical hexagon may be interpreted in various ways, including as a model of traditional logic, quantifications, modal logic, order theory, or paraconsistent logic.\n\nFor instance, the statement A may be interpreted as \"Whatever x may be, if x is a man, then x is white.\"\n\nThe statement E may be interpreted as \"Whatever x may be, if x is a man, then x is non-white.\"\n\nThe statement I may be interpreted as \"There exists at least one x that is both a man and white.\"\n\nThe statement O may be interpreted as \"There exists at least one x that is both a man and non-white\"\n\nThe statement Y may be interpreted as \"There exists at least one x that is both a man and white and there exists at least one x that is both a man and non-white\"\n\nThe statement U may be interpreted as \"One of two things, either whatever x may be, if x is a man, then x is white or whatever x may be, if x is a man, then x is non-white.\"\n\nThe logical hexagon may be interpreted in various ways, including as a model of traditional logic, quantifications, modal logic, order theory, or paraconsistent logic.\n\nThe logical hexagon may be interpreted as a model of modal logic such that\n\n\nIt has been proven that both the square and the hexagon, followed by a “logical cube”, belong to a regular series of n-dimensional objects called “logical bi-simplexes of dimension n.” The pattern also goes even beyond this.\n\n\n"}
{"id": "21582679", "url": "https://en.wikipedia.org/wiki?curid=21582679", "title": "Marginal product of labor", "text": "Marginal product of labor\n\nIn economics, the marginal product of labor (MP) is the change in output that results from employing an added unit of labor.\n\nThe marginal product of a factor of production is generally defined as the change in output associated with a change in that factor, holding other inputs into production constant.\n\nThe marginal product of labor is then the change in output (\"Y\") per unit change in labor (\"L\"). In discrete terms the marginal product of labor is:\n\nIn continuous terms, the \"MP\" is the first derivative of the production function:\n\nGraphically, the \"MP\" is the slope of the production function.\n\nThere is a factory which produces toys. When there are no workers in the factory, no toys are produced. When there is one worker in the factory, six toys are produced per hour. When there are two workers in the factory, eleven toys are produced per hour. There is a marginal product of labor of five when there are two workers in the factory compared to one. When the marginal product of labor is increasing, this is called increasing marginal returns. However, as the number of workers increases, the marginal product of labor may not increase indefinitely. When not scaled properly, the marginal product of labor may go down when the number of employees goes up, creating a situation known as diminishing marginal returns. When the marginal product of labor becomes negative, it is known as negative marginal returns.\n\nThe marginal product of labor is directly related to costs of production. Costs are divided between fixed and variable costs. Fixed costs are costs that relate to the fixed input, capital, or \"rK\", where \"r\" is the rental cost of capital and \"K\" is the quantity of capital. Variable costs (VC) are the costs of the variable input, labor, or \"wL\", where \"w\" is the wage rate and \"L\" is the amount of labor employed. Thus, VC = wL . Marginal cost (MC) is the change in total cost per unit change in output or ∆C/∆Q. In the short run, production can be varied only by changing the variable input. Thus only variable costs change as output increases: ∆C = ∆VC = ∆(wL). Marginal cost is ∆(Lw)/∆Q. Now, ∆L/∆Q is the reciprocal of the marginal product of labor (∆Q/∆L). Therefore, marginal cost is simply the wage rate w divided by the marginal product of labor\n\nThus if the marginal product of labor is rising then marginal costs will be falling and if the marginal product of labor is falling marginal costs will be rising (assuming a constant wage rate).\n\nThe average product of labor is the total product of labor divided by the number of units of labor employed, or \"Q/L\". The average product of labor is a common measure of labor productivity. The AP curve is shaped like an inverted “u”. At low production levels the AP tends to increase as additional labor is added. The primary reason for the increase is specialization and division of labor. At the point the AP reaches its maximum value AP equals the MP. Beyond this point the AP falls.\n\nDuring the early stages of production MP is greater than AP. When the MP is above the AP the AP will increase. Eventually the \"MP\" reaches it maximum value at the point of diminishing returns. Beyond this point MP will decrease. However, at the point of diminishing returns the MP is still above the AP and AP will continue to increase until MP equals AP. When MP is below AP, AP will decrease.\n\nGraphically, the \"AP\" curve can be derived from the total product curve by drawing secants from the origin that intersect (cut) the total product curve. The slope of the secant line equals the average product of labor, where the slope = dQ/dL. The slope of the curve at each intersection marks a point on the average product curve. The slope increases until the line reaches a point of tangency with the total product curve. This point marks the maximum average product of labor. It also marks the point where MP (which is the slope of the total product curve) equals the AP (the slope of the secant). Beyond this point the slope of the secants become progressively smaller as \"AP\" declines. The MP curve intersects the AP curve from above at the maximum point of the AP curve. Thereafter, the MP curve is below the AP curve.\n\nThe falling MP is due to the law of diminishing marginal returns. The law states, \"as units of one input are added (with all other inputs held constant) a point will be reached where the resulting additions to output will begin to decrease; that is marginal product will decline.\" The law of diminishing marginal returns applies regardless of whether the production function exhibits increasing, decreasing or constant returns to scale. The key factor is that the variable input is being changed while all other factors of production are being held constant. Under such circumstances diminishing marginal returns are inevitable at some level of production.\n\nDiminishing marginal returns differs from diminishing returns. Diminishing marginal returns means that the marginal product of the variable input is falling. Diminishing returns occur when the marginal product of the variable input is negative. That is when a unit increase in the variable input causes total product to fall. At the point that diminishing returns begin the MP is zero.\n\nThe general rule is that a firm maximizes profit by producing that quantity of output where marginal revenue equals marginal costs. The profit maximization issue can also be approached from the input side. That is, what is the profit maximizing usage of the variable input? To maximize profits the firm should increase usage \"up to the point where the input’s marginal revenue product equals its marginal costs\". So, mathematically the profit maximizing rule is MRP = MC. The marginal profit per unit of labor equals the marginal revenue product of labor minus the marginal cost of labor or Mπ = MRP − MCA firm maximizes profits where Mπ = 0.\n\nThe marginal revenue product is the change in total revenue per unit change in the variable input assume labor. That is, MRP = ∆TR/∆L. MRP is the product of marginal revenue and the marginal product of labor or MRP = MR × MP.\n\n\n • formula_4\n\n • Output price is $40 per unit.\n\n\nIn the aftermath of the marginal revolution in economics, a number of economists including John Bates Clark and Thomas Nixon Carver sought to derive an ethical theory of income distribution based on the idea that workers were morally entitled to receive a wage exactly equal to their marginal product. In the 20th century, marginal productivity ethics found few supporters among economists, being criticised not only by egalitarians but by economists associated with the Chicago school such as Frank Knight (in \"The Ethics of Competition\") and the Austrian School, such as Leland Yeager. However, marginal productivity ethics were defended by George Stigler.\n\n\n"}
{"id": "315426", "url": "https://en.wikipedia.org/wiki?curid=315426", "title": "Mediocrity principle", "text": "Mediocrity principle\n\nThe mediocrity principle is the philosophical notion that \"if an item is drawn at random from one of several sets or categories, it's likelier to come from the most numerous category than from any one of the less numerous categories\". The principle has been taken to suggest that there is nothing very unusual about the evolution of the Solar System, Earth's history, the evolution of biological complexity, human evolution, or any one nation. It is a heuristic in the vein of the Copernican principle, and is sometimes used as a philosophical statement about the place of humanity. The idea is to assume mediocrity, rather than starting with the assumption that a phenomenon is special, privileged, exceptional, or even superior.\n\nThe mediocrity principle suggests, given the existence of life on Earth, that life typically exists on Earth-like planets throughout the universe.\n\nThe mediocrity principle is in contrast with the anthropic principle, which asserts that the presence of an intelligent observer (humans) limits the circumstances to bounds under which intelligent life can be observed to exist, no matter how improbable. Both stand in contrast to the fine-tuning hypothesis, which asserts that the natural conditions for intelligent life are implausibly rare.\n\nThe mediocrity principle implies that Earth-like environments are necessarily common, based in part on the evidence of any happening at all, whereas the anthropic principle suggests that no assertion can be made about the probability of intelligent life based on a sample set of one (self-described) example, who are necessarily capable of making such an assertion about themselves.\n\nIt is also possible to handle the Mediocrity Principle as a statistical problem, a case of a single Data point statistics, also present in the German tank problem.\n\nDavid Deutsch argues that the mediocrity principle is incorrect from a physical point of view, in reference to either humanity's part of the universe or to its species. Deutsch refers to Stephen Hawking's quote: \"The human race is just a chemical scum on a moderate-sized planet, orbiting around a very average star in the outer suburb of one among a hundred billion galaxies\". Deutsch wrote that Earth's neighborhood in the universe is not typical (80% of the universe's matter is dark matter) and that a concentration of mass such as the Solar System is an \"isolated, uncommon phenomenon\". He also disagrees with Richard Dawkins, who considers that humans, because of natural evolution, are limited to the capabilities of their species. Deutsch responds that even though evolution did not give humans the ability to detect neutrinos, scientists can currently detect them, which significantly expands their capabilities beyond what is available as a result of evolution.\n\n"}
{"id": "4718632", "url": "https://en.wikipedia.org/wiki?curid=4718632", "title": "Mental representation", "text": "Mental representation\n\nA mental representation (or cognitive representation), in philosophy of mind, cognitive psychology, neuroscience, and cognitive science, is a hypothetical internal cognitive symbol that represents external reality, or else a mental process that makes use of such a symbol: \"a formal system for making explicit certain entities or types of information, together with a specification of how the system does this\".\n\nMental representation is the mental imagery of things that are not actually present to the senses. In contemporary philosophy, specifically in fields of metaphysics such as philosophy of mind and ontology, a mental representation is one of the prevailing ways of explaining and describing the nature of ideas and concepts.\n\nMental representations (or mental imagery) enable representing things that have never been experienced as well as things that do not exist. Think of yourself traveling to a place you have never visited before, or having a third arm. These things have either never happened or are impossible and do not exist, yet our brain and mental imagery allows us to imagine them. Although visual imagery is more likely to be recalled, mental imagery may involve representations in any of the sensory modalities, such as hearing, smell, or taste. Stephen Kosslyn proposes that images are used to help solve certain types of problems. We are able to visualize the objects in question and mentally represent the images to solve it.\n\nMental representations also allow people to experience things right in front of them—though the process of how the brain interprets the representational content is debated.\n\nRepresentationalism (also known as indirect realism) is the view that representations are the main way we access external reality. Another major prevailing philosophical theory posits that concepts are entirely abstract objects.\n\nThe representational theory of mind attempts to explain the nature of ideas, concepts and other mental content in contemporary philosophy of mind, cognitive science and experimental psychology. In contrast to theories of naive or direct realism, the representational theory of mind postulates the actual existence of mental representations which act as intermediaries between the observing subject and the objects, processes or other entities observed in the external world. These intermediaries stand for or represent to the mind the objects of that world.\n\nFor example, when someone arrives at the belief that his or her floor needs sweeping, the representational theory of mind states that he or she forms a mental representation that represents the floor and its state of cleanliness.\n\nThe original or \"classical\" representational theory probably can be traced back to Thomas Hobbes and was a dominant theme in classical empiricism in general. According to this version of the theory, the mental representations were images (often called \"ideas\") of the objects or states of affairs represented. For modern adherents, such as Jerry Fodor, Steven Pinker and many others, the representational system consists rather of an internal language of thought (i.e., mentalese). The contents of thoughts are represented in symbolic structures (the formulas of Mentalese) which, analogously to natural languages but on a much more abstract level, possess a syntax and semantics very much like those of natural languages. For the Spanish logician and cognitive scientist Luis M. Augusto, at this abstract, formal level, the syntax of thought is the set of symbol rules (i.e., operations, processes, etc. on and with symbol structures) and the semantics of thought is the set of symbol structures (concepts and propositions). Content (i.e., thought) emerges from the meaningful co-occurrence of both sets of symbols. For instance, \"8 x 9\" is a meaningful co-occurrence, whereas \"CAT x §\" is not; \"x\" is a symbol rule called for by symbol structures such as \"8\" and \"9\", but not by \"CAT\" and \"§\".\n\nThere are two types of representationalism, strong and weak. Strong representationalism attempts to reduce phenomenal character to intentional content. On the other hand, weak representationalism claims only that phenomenal character supervenes on intentional content. Strong representationalism aims to provide a theory about the nature of phenomenal character, and offers a solution to the hard problem of consciousness. In contrast to this, weak representationalism does not aim to provide a theory of consciousness, nor does it offer a solution to the hard problem of consciousness.\n\nStrong representationalism can be further broken down into restricted and unrestricted versions. The restricted version deals only with certain kinds of phenomenal states e.g. visual perception. Most representationalists endorse an unrestricted version of representationalism. According to the unrestricted version, for any state with phenomenal character that state’s phenomenal character reduces to its intentional content. Only this unrestricted version of representationalism is able to provide a general theory about the nature of phenomenal character, as well as offer a potential solution to the hard problem of consciousness. The successful reduction of the phenomenal character of a state to its intentional content would provide a solution to the hard problem of consciousness once a physicalist account of intentionality is worked out.\n\nWhen arguing against the unrestricted version of representationalism people will often bring up phenomenal mental states that appear to lack intentional content. The unrestricted version seeks to account for all phenomenal states. Thus, for it to be true, all states with phenomenal character must have intentional content to which that character is reduced. Phenomenal states without intentional content therefore serve as a counterexample to the unrestricted version. If the state has no intentional content its phenomenal character will not be reducible to that state’s intentional content, for it has none to begin with.\n\nA common example of this kind of state are moods. Moods are states with phenomenal character that are generally thought to not be directed at anything in particular. Moods are thought to lack directedness, unlike emotions, which are typically thought to be directed at particular things e.g. you are mad \"at\" your sibling, you are afraid \"of\" a dangerous animal. People conclude that because moods are undirected they are also nonintentional i.e. they lack intentionality or aboutness. Because they are not directed at anything they are not about anything. Because they lack intentionality they will lack any intentional content. Lacking intentional content their phenomenal character will not be reducible to intentional content, refuting the representational doctrine.\n\nThough emotions are typically considered as having directedness and intentionality this idea has also been called into question. One might point to emotions a person all of a sudden experiences that do not appear to be directed at or about anything in particular. Emotions elicited by listening to music are another potential example of undirected, nonintentional emotions. Emotions aroused in this way do not seem to necessarily be about anything, including the music that arouses them.\n\nIn response to this objection a proponent of representationalism might reject the undirected nonintentionality of moods, and attempt to identify some intentional content they might plausibly be thought to possess. The proponent of representationalism might also reject the narrow conception of intentionality as being directed at a particular thing, arguing instead for a broader kind of intentionality.\n\nThere are three alternative kinds of directedness/intentionality one might posit for moods. \nIn the case of outward directedness moods might be directed at either the world as a whole, a changing series of objects in the world, or unbound emotion properties projected by people onto things in the world. In the case of inward directedness moods are directed at the overall state of a person’s body. In the case of hybrid directedness moods are directed at some combination of inward and outward things.\n\nEven if one can identify some possible intentional content for moods we might still question whether that content is able to sufficiently capture the phenomenal character of the mood states they are a part of. Amy Kind contends that in the case of all the previously mentioned kinds of directedness (outward, inward, and hybrid) the intentional content supplied to the mood state is not capable of sufficiently capturing the phenomenal aspects of the mood states. In the case of inward directedness, the phenomenology of the mood does not seem tied to the state of one’s body, and even if one’s mood is reflected by the overall state of one’s body that person will not necessarily be aware of it, demonstrating the insufficiency of the intentional content to adequately capture the phenomenal aspects of the mood. In the case of outward directedness, the phenomenology of the mood and its intentional content do not seem to share the corresponding relation they should given that the phenomenal character is supposed to reduce to the intentional content. Hybrid directedness, if it can even get off the ground, faces the same objection.\n\nThere is a wide debate on what kinds of representations exist. There are several philosophers who bring about different aspects of the debate. Such philosophers include Alex Morgan, Gualtiero Piccinini, and Uriah Kriegel—though this is not an exhaustive list.\n\nThere are \"job description\" representations. That is representations that (1) represent something—have intentionality, (2) have a special relation—the represented object does not need to exist, and (3) content plays a causal role in what gets represented: e.g. saying \"hello\" to a friend, giving a glare to an enemy.\n\nStructural representations are also important. These types of representations are basically mental maps that we have in our minds that correspond exactly to those objects in the world (the intentional content). According to Morgan, structural representations are not the same as mental representations—there is nothing mental about them: plants can have structural representations.\n\nThere are also internal representations. These types of representations include those that involve future decisions, episodic memories, or any type of projection into the future.\n\nIn Gualtiero Piccinini's forthcoming work, he discusses topics on natural and nonnatural mental representations. He relies on the natural definition of mental representations given by Grice (1957) where \"P entails that P\". e.g. Those spots mean measles, entails that the patient has measles. Then there are nonnatural representations: \"P does not entail P\". e.g. The 3 rings on the bell of a bus mean the bus is full—the rings on the bell are independent of the fullness of the bus—we could have assigned something else (just as arbitrary) to signify that the bus is full.\n\nThere are also objective and subjective mental representations. Objective representations are closest to tracking theories—where the brain simply tracks what is in the environment. If there is a blue bird outside my window, the objective representation is that of the blue bird. Subjective representations can vary person-to-person. For example, if I am colorblind, that blue bird outside my window will not \"appear\" blue to me since I cannot represent the blueness of blue (i.e. I cannot see the color blue). The relationship between these two types of representation can vary.\n\nEliminativists think that subjective representations don't exist. Reductivists think subjective representations are reducible to objective. Non-reductivists think that subjective representations are real and distinct.\n\n\n"}
{"id": "3558732", "url": "https://en.wikipedia.org/wiki?curid=3558732", "title": "Negative and positive rights", "text": "Negative and positive rights\n\nNegative and positive rights are rights that oblige either action (\"positive rights\") or inaction (\"negative rights\"). These obligations may be of either a legal or moral character. The notion of positive and negative rights may also be applied to liberty rights.\n\nTo take an example involving two parties in a court of law: Adrian has a \"negative right to x\" against Clay if and only if Clay is \"prohibited\" from acting upon Adrian in some way regarding \"x\". In contrast, Adrian has a \"positive right to x\" against Clay if and only if Clay is obliged to act upon Adrian in some way regarding \"x\". A case in point, if Adrian has a \"negative right to life\" against Clay, then Clay is required to refrain from killing Adrian; while if Adrian has a \"positive right to life\" against Clay, then Clay is required to act as necessary to preserve the life of Adrian.\n\nRights considered \"negative rights\" may include civil and political rights such as freedom of speech, life, private property, freedom from violent crime, freedom of religion, \"habeas corpus\", a fair trial, and freedom from slavery.\n\nRights considered \"positive rights\", as initially proposed in 1979 by the Czech jurist Karel Vasak, may include other civil and political rights such as police protection of person and property and the right to counsel, as well as economic, social and cultural rights such as food, housing, public education, employment, national security, military, health care, social security, internet access, and a minimum standard of living. In the \"three generations\" account of human rights, negative rights are often associated with the first generation of rights, while positive rights are associated with the second and third generations.\n\nSome philosophers (see criticisms) disagree that the negative-positive rights distinction is useful or valid.\n\nUnder the theory of positive and negative rights, a negative right is a right \"not to be\" subjected to an action of another person or group—a government, for example—usually in the form of abuse or coercion. As such, negative rights exist unless someone acts to \"negate\" them. A positive right is a right \"to be\" subjected to an action of another person or group. In other words, for a positive right to be exercised, someone else's actions must be \"added\" to the equation. In theory, a negative right forbids others from acting against the right holder, while a positive right obligates others to act with respect to the right holder. In the framework of the Kantian categorical imperative, negative rights can be associated with perfect duties while positive rights can be connected to imperfect duties.\n\nBelief in a distinction between positive and negative rights is usually maintained, or emphasized, by libertarians, who believe that positive rights do not exist until they are created by contract. The United Nations Universal Declaration of Human Rights lists both positive and negative rights (but does not identify them as such). The constitutions of most liberal democracies guarantee negative rights, but not all include positive rights. Nevertheless, positive rights are often guaranteed by other laws, and the majority of liberal democracies provide their citizens with publicly funded education, health care, social security and unemployment benefits.\n\nRights are often spoken of as inalienable and sometimes even absolute. However, in practice this is often taken as graded absolutism; rights are ranked by degree of importance, and violations of lesser ones are accepted in the course of preventing violations of greater ones. Thus, even if the right not to be killed is inalienable, the corresponding obligation on others to refrain from killing is generally understood to have at least one exception: self-defense. Certain widely accepted negative obligations (such as the obligations to refrain from theft, murder, etc.) are often considered prima facie, meaning that the legitimacy of the obligation is accepted \"on its face\"; but even if not questioned, such obligations may still be ranked for ethical analysis.\n\nThus a thief may have a negative obligation not to steal, and a police officer may have a negative obligation not to tackle people—but a police officer tackling the thief easily meets the burden of proof that he acted justifiably, since his was a breach of a lesser obligation and negated the breach of a greater obligation. Likewise a shopkeeper or other passerby may also meet this burden of proof when tackling the thief. But if any of those individuals pulled a gun and shot the (unarmed) thief for stealing, most modern societies would not accept that the burden of proof had been met. The obligation not to kill—being universally regarded as one of the highest, if not the highest obligation—is so much greater than the obligation not to steal that a breach of the latter does not justify a breach of the former. Most modern societies insist that other, very serious ethical questions need come into play before stealing could justify killing.\n\nPositive obligations confer duty. But as we see with the police officer, exercising a duty may violate negative obligations (e.g. not to overreact and kill). For this reason, in ethics positive obligations are almost never considered \"prima facie\". The greatest negative obligation may have just one exception—one higher obligation of self-defense—but even the greatest positive obligations generally require more complex ethical analysis. For example, one could easily justify failing to help, not just one, but a great many injured children quite ethically in the case of triage after a disaster. This consideration has led ethicists to agree in a general way that positive obligations are usually junior to negative obligations because they are not reliably \"prima facie\". Some critics of positive rights implicitly suggest that because positive obligations are not reliably \"prima facie\" they must always be agreed to through contract.\n\nNineteenth-century philosopher Frédéric Bastiat summarized the conflict between these negative and positive rights by saying:\nAccording to Jan Narveson, the view of some that there is no distinction between negative and positive rights on the ground that negative rights require police and courts for their enforcement is \"mistaken\". He says that the question between what one has a right to do and who if anybody enforces it are separate issues. If rights are only negative then it simply means no one has a duty to enforce them, although individuals have a right to use any non-forcible means to gain the cooperation of others in protecting those rights. Therefore, he says \"the distinction between negative and positive is quite robust.\" Libertarians hold that positive rights, which would include a right to be protected, do not exist until they are created by contract. However, those who hold this view do not mean that police, for example, are not obligated to protect the rights of citizens. Since they contract with their employers to defend citizens from violence, then they have created that obligation to their employer. A negative right to life allows an individual to defend his life from others trying to kill him, or obtain voluntary assistance from others to defend his life—but he may not force others to defend him, because he has no natural right to be provided with defense. To force a person to defend one's own negative rights, or the negative rights of a third party, would be to violate that person's negative rights.\n\nOther advocates of the view that there is a distinction between negative and positive rights argue that the presence of a police force or army is not due to any positive right to these services that citizens claim, but rather because they are natural monopolies or public goods—features of any human society that arise naturally, even while adhering to the concept of negative rights only. Robert Nozick discusses this idea at length in his book \"Anarchy, State, and Utopia\".\n\nIn the field of medicine, positive rights of patients often conflict with negative rights of physicians. In controversial areas such as abortion and assisted suicide, medical professionals may not wish to offer certain services for moral or philosophical reasons. If enough practitioners opt out as a result of conscience, a right granted by conscience clause statutes in many jurisdictions, patients may not have any means of having their own positive rights fulfilled. Such was the case of Janet Murdock, a Montana woman who could not find any physician to assist her suicide in 2009. This controversy over positive and negative rights in medicine has become a focal point in the ongoing public debate between conservative ethicist Wesley J. Smith and bioethicist Jacob M. Appel. In discussing \"Baxter v. Montana\", Appel has written:\nSmith replies that this is \"taking the duty to die and transforming it into a duty to kill\", which he argues \"reflects a profound misunderstanding of the government’s role\".\n\nPresumably, if a person has positive rights it implies that other people have positive duties (to take certain actions); whereas negative rights imply that others have negative duties (to avoid certain other actions). Philosopher Henry Shue is skeptical; he believes that all rights (regardless of whether they seem more \"negative\" or \"positive\") requires both kinds of duties at once. In other words, Shue says that honouring a right will require avoidance (a \"negative\" duty) but also protective or reparative actions (\"positive\" duties). The negative positive distinction may be a matter of emphasis; it is therefore unhelpful to describe \"any right\" as though it requires only one of the two types of duties.\n\nTo Shue, rights can always be understood as confronting \"standard threats\" against humanity. Dealing with standard threats requires all kinds of duties, which may be divided across time (e.g. \"if avoiding the harmful behaviour fails, begin to repair the damages\"), but also divided across people. The point is that every right provokes all 3 types of behaviour (avoidance, protection, repair) to some degree. Dealing with a threat like murder, for instance, will require one individual to practice avoidance (e.g. the potential murderer must stay calm), others to protect (e.g. the police officer, who must stop the attack, or the bystander, who may be obligated to call the police), and others to repair (e.g. the doctor who must resuscitate a person who has been attacked). Thus, even the negative right not to be killed can only be guaranteed with the help of some positive duties. Shue goes further, and maintains that the negative and positive rights distinction can be harmful, because it may result in the neglect of necessary duties.\n\nJames P. Sterba makes similar criticisms. He holds that any right can be made to appear either positive or negative depending on the language used to define it. He writes:\n\nSterba has rephrased the traditional \"positive right\" to provisions, and put it in the form of a sort of \"negative right\" \"not to be prevented\" from taking the resources on their own.. Thus, all rights may not only require both \"positive\" and \"negative\" duties, but it seems that rights that do not involve forced labor can be phrased positively or negatively at will. The distinction between positive and negative may not be very useful, or justified, as rights requiring the provision of labor can be rephrased from \"right to education\" or \"right to health care\" to \"right to take surplus money to pay teachers\" or \"right to take surplus money to pay doctors\".\n\n\n\n"}
{"id": "548558", "url": "https://en.wikipedia.org/wiki?curid=548558", "title": "Negative pressure", "text": "Negative pressure\n\nNegative pressure may refer to:\n\n"}
{"id": "26502557", "url": "https://en.wikipedia.org/wiki?curid=26502557", "title": "Negative room pressure", "text": "Negative room pressure\n\nNegative room pressure is an isolation technique used in hospitals and medical centers to prevent cross-contaminations from room to room. It includes a ventilation system that generates negative pressure to allow air to flow into the isolation room but not escape from the room, as air will naturally flow from areas with higher pressure to areas with lower pressure, thereby preventing contaminated air from escaping the room. This technique is used to isolate patients with airborne contagious diseases such as tuberculosis, measles, or chickenpox.\n\nNegative pressure is generated and maintained by a ventilation system that removes more exhaust air from the room than air is allowed into the room. Air is allowed into the room through a gap under the door (typically about one half-inch high). Except for this gap, the room should be as airtight as possible, allowing no air in through cracks and gaps, such as those around windows, light fixtures and electrical outlets. Leakage from these sources can compromise or eliminate room negative pressure.\n\nA smoke test can help determine whether a room is under negative pressure. A tube containing smoke is held near the bottom of the negative pressure room door, about 2 inches in front of the door. The smoke tube is held parallel to the door, and a small amount of smoke is then generated by gently squeezing the bulb. Care is taken to release the smoke from the tube slowly to ensure the velocity of the smoke from the tube does not overpower the air velocity. If the room is at negative pressure, the smoke will travel under the door and into the room. If the room is not a negative pressure, the smoke will be blown outward or will stay stationary.\n\n"}
{"id": "4094117", "url": "https://en.wikipedia.org/wiki?curid=4094117", "title": "Negative stain", "text": "Negative stain\n\nNegative staining is an established method, often used in diagnostic microscopy, for contrasting a thin specimen with an optically opaque fluid. In this technique, the background is stained, leaving the actual specimen untouched, and thus visible. This contrasts with 'positive staining', in which the actual specimen is stained.\n\nFor bright field microscopy, negative staining is typically performed using a black ink fluid such as nigrosin and India ink. The specimen, such as a wet bacterial culture spread on a glass slide, is mixed with the negative stain and allowed to dry. When viewed with the microscope the bacterial cells, and perhaps their spores, appear light against the dark surrounding background. An alternative method has been developed using an ordinary waterproof marking pen to deliver the negative stain.\n\nIn the case of transmission electron microscopy, opaqueness to electrons is related to the atomic number, i.e., the number of protons. Some suitable negative stains include ammonium molybdate, uranyl acetate, uranyl formate, phosphotungstic acid, osmium tetroxide, osmium ferricyanide and auroglucothionate. These have been chosen because they scatter electrons strongly and also adsorb to biological matter well. The structures which can be negatively stained are much smaller than those studied with the light microscope. Here, the method is used to view viruses, bacteria, bacterial flagella, biological membrane structures and proteins or protein aggregates, which all have a low electron-scattering power. Some stains, such as osmium tetroxide and osmium ferricyanide, are very chemically active. As strong oxidants, they cross-link lipids mainly by reacting with unsaturated carbon-carbon bonds, and thereby both fix biological membranes in place in tissue samples and simultaneously stain them.\n\nThe choice of negative stain in electron microscopy can be very important. An early study of plant viruses using negatively stained leaf dips from a diseased plant showed only spherical viruses with one stain and only rod-shaped viruses with another. The verified conclusion was that this plant suffered from a mixed infection by two separate viruses. \nNegative staining at both light microscope and electron microscope level should never be performed with infectious organisms unless stringent safety precautions are followed. Negative staining is usually a very mild preparation method and thus does not reduce the possibility of operator infection.\n\nNegative staining transmission electron microscopy has also been successfully employed for study and identification of aqueous lipid aggregates like lamellar liposomes (le), inverted spherical micelles (M) and inverted hexagonal HII cylindrical (H) phases (see figure above).\n\n"}
{"id": "362728", "url": "https://en.wikipedia.org/wiki?curid=362728", "title": "Negative temperature", "text": "Negative temperature\n\nIn quantum thermodynamics, certain systems can achieve negative temperature; that is, their temperature can be expressed as a negative quantity on the Kelvin or Rankine scales.\n\nA system with a truly negative temperature on the Kelvin scale is \"hotter\" than any system with a positive temperature. If a negative-temperature system and a positive-temperature system come in contact, heat will flow from the negative- to the positive-temperature system. A standard example of such a system is population inversion in laser physics.\n\nTemperature is loosely interpreted as the average kinetic energy of the system's particles. The existence of negative temperature, let alone negative temperature representing \"hotter\" systems than positive temperature, would seem paradoxical in this interpretation. \nThe paradox is resolved by considering the more rigorous definition of thermodynamic temperature as the tradeoff between internal energy and entropy\ncontained in the system, with \"coldness\", the \"reciprocal\" of temperature, being the more fundamental quantity. \nSystems with a positive temperature will increase in entropy as one adds energy to the system, while \nsystems with a negative temperature will decrease in entropy as one adds energy to the system.\n\nClassical thermodynamic systems cannot achieve negative temperatures: adding heat always increases their entropy. \nThe possibility of a decrease in entropy as energy increases requires the system to \"saturate\" in entropy. This is only possible if the number of high energy states is limited. In classical Boltzmann statistics, the number of high energy states is unlimited (particle speeds can in principle be increased indefinitely).\nSystems bounded by a maximum amount of energy are generally forbidden in classical mechanics, and the phenomenon of negative temperature is strictly a \nquantum mechanical phenomenon. Some systems, however (see the examples below), have a maximum amount of energy that they can hold, and as they approach that maximum energy their entropy actually begins to decrease.\n\nThe definition of thermodynamic temperature as a function of the change in the system's entropy under reversible heat transfer \nformula_1:\nEntropy being a state function, the integral of over any cyclical process is zero.\nFor a system in which the entropy is purely a function of the system's energy , the temperature can be defined as:\n\nEquivalently, thermodynamic beta, or \"coldness\", is defined as \nwhere is the Boltzmann constant.\n\nNote that in classical thermodynamics, is defined in terms of temperature. This is reversed here, is the statistical entropy, a function of the possibly microstates of the system, and temperature conveys information on the distribution of energy levels among the possible microstates.\nFor systems with many degrees of freedom, the statistical and thermodynamic definitions of entropy are generally consistent with each other. \nHowever, for small systems and systems where the number of states decreases with energy, the definitions of statistical entropy and thermodynamic entropy are not necessarily consistent, and the temperatures derived from these entropies are different. \n\nSome theorists have proposed using an alternate definition of entropy originally proposed by Gibbs as a way to resolve these inconsistencies, although this new definition would create other inconsistencies.\n\nNegative temperatures can only exist in a system where there are a limited number of energy states (see below). As the temperature is increased on such a system, particles move into higher and higher energy states, and as the temperature increases, the number of particles in the lower energy states and in the higher energy states approaches equality. (This is a consequence of the definition of temperature in statistical mechanics for systems with limited states.) By injecting energy into these systems in the right fashion, it is possible to create a system in which there are more particles in the higher energy states than in the lower ones. The system can then be characterised as having a negative temperature.\n\nA substance with a negative temperature is not colder than absolute zero, but rather it is hotter than infinite temperature. As Kittel and Kroemer (p. 462) put it, \"The temperature scale from cold to hot runs:\n\nThe corresponding inverse temperature scale, for the quantity \"β\" = 1/\"kT\" (where \"k\" is Boltzmann's constant), runs continuously from low energy to high as +∞, … , 0, …, −∞. Because it avoids the abrupt jump from +∞ to −∞, β is considered more natural than \"T\". Although a system can have multiple negative temperature regions and thus have -∞ to +∞ discontinuities.\n\nIn many familiar physical systems, temperature is associated to the kinetic energy of atoms. Since there is no upper bound on the momentum of an atom, there is no upper bound to the number of energy states available when more energy is added, and therefore no way to get to a negative temperature. However, in statistical mechanics, temperature can correspond to other degrees of freedom than just kinetic energy (see below).\n\nThe distribution of energy among the various translational, vibrational, rotational, electronic, and nuclear modes of a system determines the macroscopic temperature. In a \"normal\" system, thermal energy is constantly being exchanged between the various modes.\n\nHowever, in some situations, it is possible to isolate one or more of the modes. In practice, the isolated modes still exchange energy with the other modes, but the time scale of this exchange is much slower than for the exchanges within the isolated mode. One example is the case of nuclear spins in a strong external magnetic field. In this case, energy flows fairly rapidly among the spin states of interacting atoms, but energy transfer between the nuclear spins and other modes is relatively slow. Since the energy flow is predominantly within the spin system, it makes sense to think of a spin temperature that is distinct from the temperature associated to other modes.\n\nA definition of temperature can be based on the relationship:\n\nThe relationship suggests that a \"positive temperature\" corresponds to the condition where entropy, \"S\", increases as thermal energy, \"q\", is added to the system. This is the \"normal\" condition in the macroscopic world, and is always the case for the translational, vibrational, rotational, and non-spin related electronic and nuclear modes. The reason for this is that there are an infinite number of these types of modes, and adding more heat to the system increases the number of modes that are energetically accessible, and thus increases the entropy.\n\nThe simplest example, albeit a rather nonphysical one, is to consider a system of \"N\" particles, each of which can take an energy of either \"+ε\" or \"-ε\" but are otherwise noninteracting. This can be understood as a limit of the Ising model in which the interaction term becomes negligible. The total energy of the system is\n\nwhere \"σ\" is the sign of the \"i\"th particle and \"j\" is the number of particles with positive energy minus the number of particles with negative energy. From elementary combinatorics, the total number of microstates with this amount of energy is a binomial coefficient:\n\nBy the fundamental assumption of statistical mechanics, the entropy of this microcanonical ensemble is\n\nWe can solve for thermodynamic beta (β = 1/kT) by considering it as a central difference without taking the continuum limit:\n\nhence the temperature\n\nThis entire proof assumes the microcanonical ensemble with energy fixed and temperature being the emergent property. In the canonical ensemble, the temperature is fixed and energy is the emergent property. This leads to (formula_14 refers to microstates):\nFollowing the previous example, we choose a state with two levels and two particles. This leads to microstates formula_18, formula_19, formula_20, and formula_21.\nThe resulting values for formula_28, formula_29, and formula_30 are all increasing with formula_31 and never need to enter a negative temperature regime.\n\nThe previous example is approximately realized by a system of nuclear spins in an external magnetic field. This allows the experiment to be run as a variation of nuclear magnetic resonance spectroscopy. In the case of electronic and nuclear spin systems, there are only a finite number of modes available, often just two, corresponding to spin up and spin down. In the absence of a magnetic field, these spin states are \"degenerate\", meaning that they correspond to the same energy. When an external magnetic field is applied, the energy levels are split, since those spin states that are aligned with the magnetic field will have a different energy from those that are anti-parallel to it.\n\nIn the absence of a magnetic field, such a two-spin system would have maximum entropy when half the atoms are in the spin-up state and half are in the spin-down state, and so one would expect to find the system with close to an equal distribution of spins. Upon application of a magnetic field, some of the atoms will tend to align so as to minimize the energy of the system, thus slightly more atoms should be in the lower-energy state (for the purposes of this example we will assume the spin-down state is the lower-energy state). It is possible to add energy to the spin system using radio frequency (RF) techniques. This causes atoms to \"flip\" from spin-down to spin-up.\n\nSince we started with over half the atoms in the spin-down state, this initially drives the system towards a 50/50 mixture, so the entropy is increasing, corresponding to a positive temperature. However, at some point, more than half of the spins are in the spin-up position. In this case, adding additional energy reduces the entropy, since it moves the system further from a 50/50 mixture. This reduction in entropy with the addition of energy corresponds to a negative temperature. In NMR spectroscopy, this corresponds to pulses with a pulse width of over 180° (for a given spin). While relaxation is fast in solids, it can take several seconds in solutions and even longer in gases and in ultracold systems; several hours were reported for silver and rhodium at picokelvin temperatures. It is still important to understand that the temperature is negative only with respect to nuclear spins. Other degrees of freedom, such as molecular vibrational, electronic and electron spin levels are at a positive temperature, so the object still has positive sensible heat. Relaxation actually happens by exchange of energy between the nuclear spin states and other states (e.g. through the nuclear Overhauser effect with other spins).\n\nThis phenomenon can also be observed in many lasing systems, wherein a large fraction of the system's atoms (for chemical and gas lasers) or electrons (in semiconductor lasers) are in excited states. This is referred to as a population inversion.\n\nThe Hamiltonian for a single mode of a luminescent radiation field at frequency \"ν\" is\nThe density operator in the grand canonical ensemble is\nFor the system to have a ground state, the trace to converge, and the density operator to be generally meaningful, \"βH\" must be positive semidefinite. So if \"hν\" < \"μ\", and \"H\" is negative semidefinite, then \"β\" must itself be negative, implying a negative temperature.\n\nThe two-dimensional systems can exist in negative temperature states.\n\nNegative temperatures have also been achieved in motional degrees of freedom. Using an optical lattice, upper bounds were placed on the kinetic energy, interaction energy and potential energy of cold atoms. This was done by tuning the interactions of the atoms from repulsive to attractive using a Feshbach resonance and changing the overall harmonic potential from trapping to anti-trapping, thus transforming the Bose-Hubbard Hamiltonian from formula_34. Performing this transformation adiabatically while keeping the atoms in the Mott insulator regime, it is possible to go from a low entropy positive temperature state to a low entropy negative temperature state. In the negative temperature state, the atoms macroscopically occupy the maximum momentum state of the lattice. The negative temperature ensembles equilibrated and showed long lifetimes in an anti-trapping harmonic potential.\n\n\n"}
{"id": "47880066", "url": "https://en.wikipedia.org/wiki?curid=47880066", "title": "Negative utilitarianism", "text": "Negative utilitarianism\n\nNegative utilitarianism is a version of the ethical theory utilitarianism that gives greater priority to reducing suffering (negative utility or 'disutility') than to increasing happiness (positive utility). This differs from classical utilitarianism, which does not claim that reducing suffering is intrinsically more important than increasing happiness. Both versions of utilitarianism hold that morally right and morally wrong actions depend solely on the consequences for overall well-being. 'Well-being' refers to the state of the individual. The term 'negative utilitarianism' is used by some authors to denote the theory that reducing negative well-being is the \"only\" thing that ultimately matters morally. Others distinguish between 'strong' and 'weak' versions of negative utilitarianism, where strong versions are \"only\" concerned with reducing negative well-being, and weak versions say that \"both\" positive and negative well-being matter but that negative well-being matters more.\n\nOther versions of negative utilitarianism differ in how much weight they give to negative well-being ('disutility') compared to positive well-being (positive utility), as well as the different conceptions of what well-being (utility) is. For example, negative preference utilitarianism says that the well-being in an outcome depends on frustrated preferences. Negative hedonistic utilitarianism thinks of well-being in terms of pleasant and unpleasant experiences. There are many other variations on how negative utilitarianism can be specified.\n\nThe term \"negative utilitarianism\" was introduced by R. Ninian Smart in 1958 in his reply to Karl Popper's \"The Open Society and Its Enemies\". Smart also presented the most famous argument against negative utilitarianism: that negative utilitarianism would entail that a ruler who is able to instantly and painlessly destroy the human race would have a duty to do so. Furthermore, every human being would have a moral responsibility to commit suicide, thereby preventing future suffering. Many authors have endorsed versions of this argument, and some have presented counterarguments against it.\n\nThe term ‘negative utilitarianism’ was introduced by R. N. Smart in his 1958 reply to Karl Popper's book \"The Open Society and Its Enemies\", published in 1945. In the book, Popper emphasizes the importance of preventing suffering in public policy. The ideas in negative utilitarianism have similarities with ancient traditions such as Jainism and Buddhism. Ancient Greek philosopher Hegesias of Cyrene has been said to be “one of the earliest exponents of NU [Negative Utilitarianism].” In more recent times, ideas similar to negative utilitarianism can be found in the works of 19th century psychologist Edmund Gurney who wrote:\n\nLike other kinds of utilitarianism, negative utilitarianism can take many forms depending on what specific claims are taken to constitute the theory. For example, negative preference utilitarianism says that the utility of an outcome depends on frustrated and satisfied preferences. Negative hedonistic utilitarianism thinks of utility in terms of hedonic mental states such as suffering and unpleasantness. Versions of (negative) utilitarianism can also differ based on whether the \"actual\" or \"expected\" consequences matter, and whether the aim is stated in terms of the \"average\" outcome among individuals or the \"total\" net utility (or lack of disutility) among them. Negative utilitarianism can aim either to \"optimize\" the value of the outcome or it can be a \"satisficing\" negative utilitarianism, according to which an action ought to be taken if and only if the outcome would be \"sufficiently\" valuable (or have sufficiently low disvalue). A key way in which negative utilitarianisms can differ from one another is with respect to how much weight they give to negative well-being (disutility) compared to positive well-being (positive utility). This is a key area of variation because the key difference between negative utilitarianism and non-negative kinds of utilitarianism is that negative utilitarianism gives more weight to negative well-being.\n\nPhilosophers Gustaf Arrhenius and Krister Bykvist develop a taxonomy of negative utilitarian views based on how the views weigh disutility against positive utility. In total, they distinguish among 16 kinds of negative utilitarianism. They first distinguish between \"strong negativism\" and \"weak negativism\". Strong negativism \"give all weight to disutility\" and weak negativism \"give some weight to positive utility, but more weight to disutility.\" The most commonly discussed subtypes are probably two versions of weak negative utilitarianism called 'lexical' and 'lexical threshold' negative utilitarianism. According to 'lexical' negative utilitarianism, positive utility gets weight only when outcomes are equal with respect to disutility. That is, positive utility functions as a tiebreaker in that it determines which outcome is better (or less bad) when the outcomes considered have equal disutility. 'Lexical threshold' negative utilitarianism says that there is some disutility, for instance some extreme suffering, such that no positive utility can counterbalance it. 'Consent-based' negative utilitarianism is a specification of lexical threshold negative utilitarianism, which specifies where the threshold should be located. It says that if an individual is suffering and would at that moment not \"agree to continue the suffering in order to obtain something else in the future\" then the suffering cannot be outweighed by any happiness.\n\nThomas Metzinger proposes the 'principle of negative utilitarianism,' which is the broad idea that suffering should be minimized when possible. Mario Bunge writes about negative utilitarianism in his \"Treatise on Basic Philosophy\" but in a different sense than most others. In Bunge's sense, negative utilitarianism is about not harming. In contrast, most other discussion of negative utilitarianism takes it to imply a duty both not to harm and to help (at least in the sense of reducing negative well-being).\n\nIn the 1958 article where R. N. Smart introduced the term ‘negative utilitarianism’ he argued against it, stating that negative utilitarianism would entail that a ruler who is able to instantly and painlessly destroy the human race, \"a benevolent world-exploder,\" would have a duty to do so. This is the most famous argument against negative utilitarianism, and it is directed against sufficiently strong versions of negative utilitarianism. Many authors have endorsed this argument, and some have presented counterarguments against it. Below are replies to this argument that have been presented and discussed.\n\nOne possible reply to this argument is that only a naive interpretation of negative utilitarianism would endorse world destruction. The conclusion can be mitigated by pointing out the importance of cooperation between different value systems. There are good consequentialist reasons why one should be cooperative towards other value systems and it is particularly important to avoid doing something harmful to other value systems. The destruction of the world would strongly violate many other value systems and endorsing it would therefore be uncooperative. Since there are many ways to reduce suffering which do not infringe on other value systems, it makes sense for negative utilitarians to focus on these options. In an extended interpretation of negative utilitarianism, cooperation with other value systems is considered and the conclusion is that it is better to reduce suffering without violating other value systems.\n\nAnother reply to the benevolent world-exploder argument is that it does not distinguish between eliminating and reducing negative well-being, and that negative utilitarianism should plausibly be formulated in terms of reducing and not eliminating. A counterargument to that reply is that elimination is a form of reduction, similar to how zero is a number.\n\nSeveral philosophers have argued that to try to destroy the world (or to kill many people) would be counterproductive from a negative utilitarian perspective. One such argument is provided by David Pearce, who says that \"planning and implementing the extinction of all sentient life couldn't be undertaken painlessly. Even contemplating such an enterprise would provoke distress. Thus a negative utilitarian is not compelled to argue for the apocalyptic solution.\" Instead, Pearce advocates the use of biotechnology to phase out the biology of suffering throughout the living world, and he says that \"life-long happiness can be genetically pre-programmed.\" A similar reply to the similar claim that negative utilitarianism would imply that we should kill off the miserable and needy is that we rarely face policy choices and that \"anyway there are excellent utilitarian reasons for avoiding such a policy, since people would find out about it and become even more miserable and fearful.\" The Negative Utilitarianism FAQ's answer to question \"3.2 Should NUs try to increase extinction risk?\" begins with \"No, that would very bad even by NU standards.\"\n\nSome replies to the benevolent world exploder-argument take the form that even if the world were destroyed, that would or might be bad from a negative utilitarian perspective. One such reply provided by John W. N. Watkins is that even if life were destroyed, life could evolve again, perhaps in a worse way. So the world-exploder would need to destroy the possibility of life, but that is in principle beyond human power. To this, J. J. C. Smart replies,\n\nAnother related reply to the world-exploder argument is that getting killed would be a great evil. Erich Kadlec defends negative utilitarianism and replies to the benevolent world-exploder argument (in part) as follows: \"He [R. N. Smart] also dispenses with the generally known fact that all people (with a few exceptions in extreme situations) like to live and would consider being killed not a benefit but as the greatest evil done to them.\"\n\nNegative preference utilitarianism has a preferentialist conception of well-being. That is, it is bad for an individual to get his aversions fulfilled (or preferences frustrated), and depending on version of negative utilitarianism, it may also be good for him to get his preferences satisfied. A negative utilitarian with such a conception of well-being, or whose conception of well-being includes such a preferentialist component, could reply to the benevolent world-exploder argument by saying that the explosion would be bad because it would fulfill many individuals' aversions. Arrhenius and Bykvist provide two criticisms of this reply. First, it could be claimed that frustrated preferences require that someone exists who has the frustrated preference. But if everyone is dead there are no preferences and hence no badness. Second, even if a world-explosion would involve frustrated preferences that would be bad from a negative preference utilitarian perspective, such a negative utilitarian should still favor it as the lesser of two evils compared to all the frustrated preferences that would likely exist if the world continued to exist.\n\nThe Negative Utilitarianism FAQ suggests two replies to Arrhenius and Bykvist's first type of criticism (the criticism that if no one exists anymore then there are no frustrated preferences anymore): The first reply is that past preferences count, even if the individual who held them no longer exists. The second is that \"instead of counting past preferences, one could look at the matter in terms of life-goals. The earlier the death of a person who wants to go on living, the more unfulfilled her life-goal.\" The Negative Utilitarianism FAQ also replies to Arrhenius and Bykvist's second type of criticism. The reply is (in part) that the criticism relies on the empirical premise that there would be more frustrated preferences in the future if the world continued to exist than if the world was destroyed. But that negative preference utilitarianism would say that extinction would be better (in theory), assuming that premise, should not count substantially against the theory, because for any view on population ethics that assigns disvalue to something, one can imagine future scenarios such that extinction would be better according to the given view.\n\nA part of Clark Wolf's response to the benevolent world-exploder objection is that negative utilitarianism can be combined with a theory of rights. He says,\n\nNegative utilitarianism can be combined, in particular, with Rawls' theory of justice. Rawls knew Popper’s normative claims and may have been influenced by his concern for the worst-off.\n\nToby Ord provides a critique of negative utilitarianism in his essay \"Why I'm Not a Negative Utilitarian,\" to which David Pearce and Bruno Contestabile have replied. Other critical views of negative utilitarianism are provided by Thaddeus Metz, Christopher Belshaw, and Ingmar Persson. On the other hand, Joseph Mendola develops a modification of utilitarianism, and he says that his principle\n\n\n"}
{"id": "39447080", "url": "https://en.wikipedia.org/wiki?curid=39447080", "title": "Non-extensive self-consistent thermodynamical theory", "text": "Non-extensive self-consistent thermodynamical theory\n\nIn experimental physics, researchers have proposed Non-extensive self-consistent thermodynamic theory to describe phenomena observed in the Large Hadron Collider (LHC). This theory investigates a fireball for high-energy particle collisions, while using Tsallis non-extensive thermodynamics. Fireballs lead to the bootstrap idea, or self-consistency principle, just as in the Boltzmann statistics used by Rolf Hagedorn. Assuming the distribution function gets variations, due to possible symmetrical change, Abdel Nasser Tawfik applied the non-extensive concepts of high-energy particle production.\n\nThe motivation to use the non-extensive statistics from Tsallis comes from the results obtained by Bediaga et al. They showed that with the substitution of the Boltzmann factor in Hagedorn's theory by the q-exponential function, it was possible to recover good agreement between calculation and experiment, even at energies as high as those achieved at the LHC, with q>1.\n\nThe starting point of the theory is entropy for a non-extensive quantum gas of bosons and fermions, as proposed by Conroy, Miller and Plastino, which is given by formula_1 where formula_2 is the non-extended version of the Fermi–Dirac entropy and formula_3 is the non-extended version of the Bose–Einstein entropy.\n\nThat group and also Clemens and Worku, the entropy just defined leads to occupation number formulas that reduce to Bediaga's. C. Beck, shows the power-like tails present in the distributions found in high energy physics experiments.\n\nUsing the entropy defined above, the partition function results are\nSince experiments have shown that formula_5, this restriction is adopted.\n\nAnother way to write the non-extensive partition function for a fireball is\nwhere formula_7 is the density of states of the fireballs.\n\nSelf-consistency implies that both forms of partition functions must be asymptotically equivalent and that the mass spectrum and the density of states must be related to each other by\nin the limit of formula_9 sufficiently large.\n\nThe self-consistency can be asymptotically achieved by choosing\nand\nwhere formula_12 is a constant and formula_13. Here, formula_14 are arbitrary constants. For formula_15 the two expressions above approach the corresponding expressions in Hagedorn's theory.\n\nWith the mass spectrum and density of states given above, the asymptotic form of the partition function is\nwhere\nwith\n\nOne immediate consequence of the expression for the partition function is the existence of a limiting temperature formula_19. This result is equivalent to Hagedorn's result. With these results, it is expected that at sufficiently high energy, the fireball presents a constant temperature and constant entropic factor.\n\nExperimental evidence of the existence of a limiting temperature and of a limiting entropic index can be found in J. Cleymans and collaborators, and I. Sena and A. Deppman.\n"}
{"id": "36797", "url": "https://en.wikipedia.org/wiki?curid=36797", "title": "Occam's razor", "text": "Occam's razor\n\nOccam's razor (also Ockham's razor or Ocham's razor; Latin: \"lex parsimoniae\" \"law of parsimony\") is the problem-solving principle that the simplest solution tends to be the correct one. When presented with competing hypotheses to solve a problem, one should select the solution with the fewest assumptions. The idea is attributed to English Franciscan friar William of Ockham (c. 1287–1347), a scholastic philosopher and theologian.\n\nIn science, Occam's razor is used as an abductive heuristic in the development of theoretical models, rather than as a rigorous arbiter between candidate models. In the scientific method, Occam's razor is not considered an irrefutable principle of logic or a scientific result; the preference for simplicity in the scientific method is based on the falsifiability criterion. For each accepted explanation of a phenomenon, there may be an extremely large, perhaps even incomprehensible, number of possible and more complex alternatives. Since one can always burden failing explanations with \"ad hoc\" hypotheses to prevent them from being falsified, simpler theories are preferable to more complex ones because they are more testable.\n\nThe term \"Occam's razor\" did not appear until a few centuries after William of Ockham's death in 1347. Libert Froidmont, in his \"On Christian Philosophy of the Soul\", takes credit for the phrase, speaking of \"novacula occami\". Ockham did not invent this principle, but the \"razor\"—and its association with him—may be due to the frequency and effectiveness with which he used it. Ockham stated the principle in various ways, but the most popular version, \"Entities are not to be multiplied without necessity\" () was formulated by the Irish Franciscan philosopher John Punch in his 1639 commentary on the works of Duns Scotus.\n\nThe origins of what has come to be known as Occam's razor are traceable to the works of earlier philosophers such as John Duns Scotus (1265–1308), Robert Grosseteste (1175–1253), Maimonides (Moses ben-Maimon, 1138–1204), and even Aristotle (384–322 BC). Aristotle writes in his \"Posterior Analytics\", \"We may assume the superiority [other things being equal] of the demonstration which derives from fewer postulates or hypotheses.\" Ptolemy () stated, \"We consider it a good principle to explain the phenomena by the simplest hypothesis possible.\"\n\nPhrases such as \"It is vain to do with more what can be done with fewer\" and \"A plurality is not to be posited without necessity\" were commonplace in 13th-century scholastic writing. Robert Grosseteste, in \"Commentary on\" [Aristotle's] \"the Posterior Analytics Books\" (\"Commentarius in Posteriorum Analyticorum Libros\") (c. 1217–1220), declares: \"That is better and more valuable which requires fewer, other circumstances being equal... For if one thing were demonstrated from many and another thing from fewer equally known premises, clearly that is better which is from fewer because it makes us know quickly, just as a universal demonstration is better than particular because it produces knowledge from fewer premises. Similarly in natural science, in moral science, and in metaphysics the best is that which needs no premises and the better that which needs the fewer, other circumstances being equal.\"\n\nThe \"Summa Theologica\" of Thomas Aquinas (1225–1274) states that \"it is superfluous to suppose that what can be accounted for by a few principles has been produced by many.\" Aquinas uses this principle to construct an objection to God's existence, an objection that he in turn answers and refutes generally (cf. \"quinque viae\"), and specifically, through an argument based on causality. Hence, Aquinas acknowledges the principle that today is known as Occam's razor, but prefers causal explanations to other simple explanations (cf. also Correlation does not imply causation).\n\nWilliam of Ockham (\"circa\" 1287–1347) was an English Franciscan friar and theologian, an influential medieval philosopher and a nominalist. His popular fame as a great logician rests chiefly on the maxim attributed to him and known as Occam's razor. The term \"razor\" refers to distinguishing between two hypotheses either by \"shaving away\" unnecessary assumptions or cutting apart two similar conclusions.\n\nWhile it has been claimed that Occam's razor is not found in any of William's writings, one can cite statements such as (\"Plurality must never be posited without necessity\"), which occurs in his theological work on the \"Sentences of Peter Lombard\" (\"Quaestiones et decisiones in quattuor libros Sententiarum Petri Lombardi\"; ed. Lugd., 1495, i, dist. 27, qu. 2, K).\n\nNevertheless, the precise words sometimes attributed to William of Ockham, (Entities must not be multiplied beyond necessity), are absent in his extant works; this particular phrasing comes from John Punch, who described the principle as a \"common axiom\" (\"axioma vulgare\") of the Scholastics. William of Ockham's contribution seems to restrict the operation of this principle in matters pertaining to miracles and God's power; so, in the Eucharist, a plurality of miracles is possible, simply because it pleases God.\n\nThis principle is sometimes phrased as (\"Plurality should not be posited without necessity\"). In his \"Summa Totius Logicae\", i. 12, William of Ockham cites the principle of economy, (\"It is futile to do with more things that which can be done with fewer\"; Thorburn, 1918, pp. 352–53; Kneale and Kneale, 1962, p. 243.)\n\nTo quote Isaac Newton, \"We are to admit no more causes of natural things than such as are both true and sufficient to explain their appearances. Therefore, to the same natural effects we must, as far as possible, assign the same causes.\"\n\nBertrand Russell offers a particular version of Occam's razor: \"Whenever possible, substitute constructions out of known entities for inferences to unknown entities.\"\n\nAround 1960, Ray Solomonoff founded the theory of universal inductive inference, the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable probability distribution. This theory is a mathematical formalization of Occam's razor.\n\nAnother technical approach to Occam's razor is ontological parsimony. Parsimony means spareness and is also referred to as the Rule of Simplicity. This is considered a strong version of Occam's razor. A variation used in medicine is called the \"Zebra\": a doctor should reject an exotic medical diagnosis when a more commonplace explanation is more likely, derived from Theodore Woodward's dictum \"When you hear hoofbeats, think of horses not zebras\".\n\nErnst Mach formulated the stronger version of Occam's razor into physics, which he called the Principle of Economy stating: \"Scientists must use the simplest means of arriving at their results and exclude everything not perceived by the senses.\"\n\nThis principle goes back at least as far as Aristotle, who wrote \"Nature operates in the shortest way possible.\" The idea of parsimony or simplicity in deciding between theories, though not the intent of the original expression of Occam's razor, has been assimilated into our culture as the widespread layman's formulation that \"the simplest explanation is usually the correct one.\"\n\nPrior to the 20th century, it was a commonly held belief that nature itself was simple and that simpler hypotheses about nature were thus more likely to be true. This notion was deeply rooted in the aesthetic value that simplicity holds for human thought and the justifications presented for it often drew from theology. Thomas Aquinas made this argument in the 13th century, writing, \"If a thing can be done adequately by means of one, it is superfluous to do it by means of several; for we observe that nature does not employ two instruments [if] one suffices.\"\n\nBeginning in the 20th century, epistemological justifications based on induction, logic, pragmatism, and especially probability theory have become more popular among philosophers.\n\nOccam's razor has gained strong empirical support in helping to converge on better theories (see \"Applications\" section below for some examples).\n\nIn the related concept of overfitting, excessively complex models are affected by statistical noise (a problem also known as the bias-variance trade-off), whereas simpler models may capture the underlying structure better and may thus have better predictive performance. It is, however, often difficult to deduce which part of the data is noise (cf. model selection, test set, minimum description length, Bayesian inference, etc.).\n\nThe razor's statement that \"other things being equal, simpler explanations are generally better than more complex ones\" is amenable to empirical testing. Another interpretation of the razor's statement would be that \"simpler hypotheses are generally better than the complex ones\". The procedure to test the former interpretation would compare the track records of simple and comparatively complex explanations. If one accepts the first interpretation, the validity of Occam's razor as a tool would then have to be rejected if the more complex explanations were more often correct than the less complex ones (while the converse would lend support to its use). If the latter interpretation is accepted, the validity of Occam's razor as a tool could possibly be accepted if the simpler hypotheses led to correct conclusions more often than not.\n\nSome increases in complexity are sometimes necessary. So there remains a justified general bias toward the simpler of two competing explanations. To understand why, consider that for each accepted explanation of a phenomenon, there is always an infinite number of possible, more complex, and ultimately incorrect, alternatives. This is so because one can always burden a failing explanation with an ad hoc hypothesis. Ad hoc hypotheses are justifications that prevent theories from being falsified. Even other empirical criteria, such as consilience, can never truly eliminate such explanations as competition. Each true explanation, then, may have had many alternatives that were simpler and false, but also an infinite number of alternatives that were more complex and false. But if an alternative ad hoc hypothesis were indeed justifiable, its implicit conclusions would be empirically verifiable. On a commonly accepted repeatability principle, these alternative theories have never been observed and continue to escape observation. In addition, one does not say an explanation is true if it has not withstood this principle.\n\nPut another way, any new, and even more complex, theory can still possibly be true. For example, if an individual makes supernatural claims that leprechauns were responsible for breaking a vase, the simpler explanation would be that he is mistaken, but ongoing ad hoc justifications (e.g. \"... and that's not me on the film; they tampered with that, too\") successfully prevent outright falsification. This endless supply of elaborate competing explanations, called saving hypotheses, cannot be ruled out—except by using Occam's razor. A study of the predictive validity of Occam's razor found 32 published papers that included 97 comparisons of economic forecasts from simple and complex forecasting methods. None of the papers provided a balance of evidence that complexity of method improved forecast accuracy. In the 25 papers with quantitative comparisons, complexity increased forecast errors by an average of 27 percent.\n\nOne justification of Occam's razor is a direct result of basic probability theory. By definition, all assumptions introduce possibilities for error; if an assumption does not improve the accuracy of a theory, its only effect is to increase the probability that the overall theory is wrong.\n\nThere have also been other attempts to derive Occam's razor from probability theory, including notable attempts made by Harold Jeffreys and E. T. Jaynes. The probabilistic (Bayesian) basis for Occam's razor is elaborated by David J. C. MacKay in chapter 28 of his book \"Information Theory, Inference, and Learning Algorithms\", where he emphasizes that a prior bias in favour of simpler models is not required.\n\nWilliam H. Jefferys and James O. Berger (1991) generalize and quantify the original formulation's \"assumptions\" concept as the degree to which a proposition is unnecessarily accommodating to possible observable data. They state, \"A hypothesis with fewer adjustable parameters will automatically have an enhanced posterior probability, due to the fact that the predictions it makes are sharp.\" The model they propose balances the precision of a theory's predictions against their sharpness—preferring theories that sharply make correct predictions over theories that accommodate a wide range of other possible results. This, again, reflects the mathematical relationship between key concepts in Bayesian inference (namely marginal probability, conditional probability, and posterior probability).\n\nThe bias–variance tradeoff is a framework that incorporates the Occam's razor principal in its balance between overfitting (i.e. variance minimization) and underfitting (i.e. bias minimization).\n\nKarl Popper argues that a preference for simple theories need not appeal to practical or aesthetic considerations. Our preference for simplicity may be justified by its falsifiability criterion: we prefer simpler theories to more complex ones \"because their empirical content is greater; and because they are better testable\" (Popper 1992). The idea here is that a simple theory applies to more cases than a more complex one, and is thus more easily falsifiable. This is again comparing a simple theory to a more complex theory where both explain the data equally well.\n\nThe philosopher of science Elliott Sober once argued along the same lines as Popper, tying simplicity with \"informativeness\": The simplest theory is the more informative, in the sense that it requires less information to a question. He has since rejected this account of simplicity, purportedly because it fails to provide an epistemic justification for simplicity. He now believes that simplicity considerations (and considerations of parsimony in particular) do not count unless they reflect something more fundamental. Philosophers, he suggests, may have made the error of hypostatizing simplicity (i.e., endowed it with a \"sui generis\" existence), when it has meaning only when embedded in a specific context (Sober 1992). If we fail to justify simplicity considerations on the basis of the context in which we use them, we may have no non-circular justification: \"Just as the question 'why be rational?' may have no non-circular answer, the same may be true of the question 'why should simplicity be considered in evaluating the plausibility of hypotheses?'\"\n\nRichard Swinburne argues for simplicity on logical grounds:\n\nAccording to Swinburne, since our choice of theory cannot be determined by data (see Underdetermination and Duhem-Quine thesis), we must rely on some criterion to determine which theory to use. Since it is absurd to have no logical method for settling on one hypothesis amongst an infinite number of equally data-compliant hypotheses, we should choose the simplest theory: \"Either science is irrational [in the way it judges theories and predictions probable] or the principle of simplicity is a fundamental synthetic a priori truth.\" (Swinburne 1997).\n\nFrom the \"Tractatus Logico-Philosophicus\":\n\n\nand on the related concept of \"simplicity\":\n\n\nIn science, Occam's razor is used as a heuristic to guide scientists in developing theoretical models rather than as an arbiter between published models. In physics, parsimony was an important heuristic in Albert Einstein's formulation of special relativity, in the development and application of the principle of least action by Pierre Louis Maupertuis and Leonhard Euler, and in the development of quantum mechanics by Max Planck, Werner Heisenberg and Louis de Broglie.\n\nIn chemistry, Occam's razor is often an important heuristic when developing a model of a reaction mechanism. Although it is useful as a heuristic in developing models of reaction mechanisms, it has been shown to fail as a criterion for selecting among some selected published models. In this context, Einstein himself expressed caution when he formulated Einstein's Constraint: \"It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience\". An often-quoted version of this constraint (which cannot be verified as posited by Einstein himself) says \"Everything should be kept as simple as possible, but not simpler.\"\n\nIn the scientific method, parsimony is an epistemological, metaphysical or heuristic preference, not an irrefutable principle of logic or a scientific result. As a logical principle, Occam's razor would demand that scientists accept the simplest possible theoretical explanation for existing data. However, science has shown repeatedly that future data often support more complex theories than do existing data. Science prefers the simplest explanation that is consistent with the data available at a given time, but the simplest explanation may be ruled out as new data become available. That is, science is open to the possibility that future experiments might support more complex theories than demanded by current data and is more interested in designing experiments to discriminate between competing theories than favoring one theory over another based merely on philosophical principles.\n\nWhen scientists use the idea of parsimony, it has meaning only in a very specific context of inquiry. Several background assumptions are required for parsimony to connect with plausibility in a particular research problem. The reasonableness of parsimony in one research context may have nothing to do with its reasonableness in another. It is a mistake to think that there is a single global principle that spans diverse subject matter.\n\nIt has been suggested that Occam's razor is a widely accepted example of extraevidential consideration, even though it is entirely a metaphysical assumption. There is little empirical evidence that the world is actually simple or that simple accounts are more likely to be true than complex ones.\n\nMost of the time, Occam's razor is a conservative tool, cutting out \"crazy, complicated constructions\" and assuring \"that hypotheses are grounded in the science of the day\", thus yielding \"normal\" science: models of explanation and prediction. There are, however, notable exceptions where Occam's razor turns a conservative scientist into a reluctant revolutionary. For example, Max Planck interpolated between the Wien and Jeans radiation laws and used Occam's razor logic to formulate the quantum hypothesis, even resisting that hypothesis as it became more obvious that it was correct.\n\nAppeals to simplicity were used to argue against the phenomena of meteorites, ball lightning, continental drift, and reverse transcriptase. One can argue for atomic building blocks for matter, because it provides a simpler explanation for the observed reversibility of both mixing and chemical reactions as simple separation and rearrangements of atomic building blocks. At the time, however, the atomic theory was considered more complex because it implied the existence of invisible particles that had not been directly detected. Ernst Mach and the logical positivists rejected John Dalton's atomic theory until the reality of atoms was more evident in Brownian motion, as shown by Albert Einstein.\n\nIn the same way, postulating the aether is more complex than transmission of light through a vacuum. At the time, however, all known waves propagated through a physical medium, and it seemed simpler to postulate the existence of a medium than to theorize about wave propagation without a medium. Likewise, Newton's idea of light particles seemed simpler than Christiaan Huygens's idea of waves, so many favored it. In this case, as it turned out, neither the wave—nor the particle—explanation alone suffices, as light behaves like waves and like particles.\n\nThree axioms presupposed by the scientific method are realism (the existence of objective reality), the existence of natural laws, and the constancy of natural law. Rather than depend on provability of these axioms, science depends on the fact that they have not been objectively falsified. Occam's razor and parsimony support, but do not prove, these axioms of science. The general principle of science is that theories (or models) of natural law must be consistent with repeatable experimental observations. This ultimate arbiter (selection criterion) rests upon the axioms mentioned above.\n\nThere are examples where Occam's razor would have favored the wrong theory given the available data. Simplicity principles are useful philosophical preferences for choosing a more likely theory from among several possibilities that are all consistent with available data. A single instance of Occam's razor favoring a wrong theory falsifies the razor as a general principle. Michael Lee and others provide cases in which a parsimonious approach does not guarantee a correct conclusion and, if based on incorrect working hypotheses or interpretations of incomplete data, may even strongly support a false conclusion.\n\nIf multiple models of natural law make exactly the same testable predictions, they are equivalent and there is no need for parsimony to choose a preferred one. For example, Newtonian, Hamiltonian and Lagrangian classical mechanics are equivalent. Physicists have no interest in using Occam's razor to say the other two are wrong. Likewise, there is no demand for simplicity principles to arbitrate between wave and matrix formulations of quantum mechanics. Science often does not demand arbitration or selection criteria between models that make the same testable predictions.\n\nBiologists or philosophers of biology use Occam's razor in either of two contexts both in evolutionary biology: the units of selection controversy and systematics. George C. Williams in his book \"Adaptation and Natural Selection\" (1966) argues that the best way to explain altruism among animals is based on low-level (i.e., individual) selection as opposed to high-level group selection. Altruism is defined by some evolutionary biologists (e.g., R. Alexander, 1987; W. D. Hamilton, 1964) as behavior that is beneficial to others (or to the group) at a cost to the individual, and many posit individual selection as the mechanism that explains altruism solely in terms of the behaviors of individual organisms acting in their own self-interest (or in the interest of their genes, via kin selection). Williams was arguing against the perspective of others who propose selection at the level of the group as an evolutionary mechanism that selects for altruistic traits (e.g., D. S. Wilson & E. O. Wilson, 2007). The basis for Williams' contention is that of the two, individual selection is the more parsimonious theory. In doing so he is invoking a variant of Occam's razor known as Morgan's Canon: \"In no case is an animal activity to be interpreted in terms of higher psychological processes, if it can be fairly interpreted in terms of processes which stand lower in the scale of psychological evolution and development.\" (Morgan 1903).\n\nHowever, more recent biological analyses, such as Richard Dawkins' \"The Selfish Gene\", have contended that Morgan's Canon is not the simplest and most basic explanation. Dawkins argues the way evolution works is that the genes propagated in most copies end up determining the development of that particular species, i.e., natural selection turns out to select specific genes, and this is really the fundamental underlying principle that automatically gives individual and group selection as emergent features of evolution.\n\nZoology provides an example. Muskoxen, when threatened by wolves, form a circle with the males on the outside and the females and young on the inside. This is an example of a behavior by the males that seems to be altruistic. The behavior is disadvantageous to them individually but beneficial to the group as a whole and was thus seen by some to support the group selection theory. Another interpretation is kin selection: if the males are protecting their offspring, they are protecting copies of their own alleles. Engaging in this behavior would be favored by individual selection if the cost to the male musk ox is less than half of the benefit received by his calf – which could easily be the case if wolves have an easier time killing calves than adult males. It could also be the case that male musk oxen would be individually less likely to be killed by wolves if they stood in a circle with their horns pointing out, regardless of whether they were protecting the females and offspring. That would be an example of regular natural selection – a phenomenon called \"the selfish herd\".\n\nSystematics is the branch of biology that attempts to establish patterns of genealogical relationship among biological taxa. It is also concerned with their classification. There are three primary camps in systematics: cladists, pheneticists, and evolutionary taxonomists. The cladists hold that genealogy alone should determine classification, pheneticists contend that overall similarity is the determining criterion, while evolutionary taxonomists say that both genealogy and similarity count in classification.\n\nIt is among the cladists that Occam's razor is to be found, although their term for it is \"cladistic parsimony\". Cladistic parsimony (or maximum parsimony) is a method of phylogenetic inference in the construction of types of phylogenetic trees (more specifically, cladograms). Cladograms are branching, tree-like structures used to represent hypotheses of relative degree of relationship, based on shared, derived character states. Cladistic parsimony is used to select as the preferred hypothesis of relationships the cladogram that requires the fewest implied character state transformations. Critics of the cladistic approach often observe that for some types of tree, parsimony consistently produces the wrong results, regardless of how much data is collected (this is called statistical inconsistency, or long branch attraction). However, this criticism is also potentially true for any type of phylogenetic inference, unless the model used to estimate the tree reflects the way that evolution actually happened. Because this information is not empirically accessible, the criticism of statistical inconsistency against parsimony holds no force. For a book-length treatment of cladistic parsimony, see Elliott Sober's \"Reconstructing the Past: Parsimony, Evolution, and Inference\" (1988). For a discussion of both uses of Occam's razor in biology, see Sober's article \"Let's Razor Ockham's Razor\" (1990).\n\nOther methods for inferring evolutionary relationships use parsimony in a more traditional way. Likelihood methods for phylogeny use parsimony as they do for all likelihood tests, with hypotheses requiring few differing parameters (i.e., numbers of different rates of character change or different frequencies of character state transitions) being treated as null hypotheses relative to hypotheses requiring many differing parameters. Thus, complex hypotheses must predict data much better than do simple hypotheses before researchers reject the simple hypotheses. Recent advances employ information theory, a close cousin of likelihood, which uses Occam's razor in the same way.\n\nFrancis Crick has commented on potential limitations of Occam's razor in biology. He advances the argument that because biological systems are the products of (an ongoing) natural selection, the mechanisms are not necessarily optimal in an obvious sense. He cautions: \"While Ockham's razor is a useful tool in the physical sciences, it can be a very dangerous implement in biology. It is thus very rash to use simplicity and elegance as a guide in biological research.\"\n\nIn biogeography, parsimony is used to infer ancient migrations of species or populations by observing the geographic distribution and relationships of existing organisms. Given the phylogenetic tree, ancestral migrations are inferred to be those that require the minimum amount of total movement.\n\nIn the philosophy of religion, Occam's razor is sometimes applied to the existence of God. William of Ockham himself was a Christian. He believed in God, and in the authority of Scripture; he writes that \"nothing ought to be posited without a reason given, unless it is self-evident (literally, known through itself) or known by experience or proved by the authority of Sacred Scripture.\" Ockham believed that an explanation has no sufficient basis in reality when it does not harmonize with reason, experience, or the Bible. However, unlike many theologians of his time, Ockham did not believe God could be logically proven with arguments. To Ockham, science was a matter of discovery, but theology was a matter of revelation and faith. He states: \"only faith gives us access to theological truths. The ways of God are not open to reason, for God has freely chosen to create a world and establish a way of salvation within it apart from any necessary laws that human logic or rationality can uncover.\"\n\nSt. Thomas Aquinas, in the \"Summa Theologica\", uses a formulation of Occam's razor to construct an objection to the idea that God exists, which he refutes directly with a counterargument:\n\nFurther, it is superfluous to suppose that what can be accounted for by a few principles has been produced by many. But it seems that everything we see in the world can be accounted for by other principles, supposing God did not exist. For all natural things can be reduced to one principle which is nature; and all voluntary things can be reduced to one principle which is human reason, or will. Therefore there is no need to suppose God's existence.\n\nIn turn, Aquinas answers this with the \"quinque viae\", and addresses the particular objection above with the following answer:\n\nSince nature works for a determinate end under the direction of a higher agent, whatever is done by nature must needs be traced back to God, as to its first cause. So also whatever is done voluntarily must also be traced back to some higher cause other than human reason or will, since these can change or fail; for all things that are changeable and capable of defect must be traced back to an immovable and self-necessary first principle, as was shown in the body of the Article.\n\nRather than argue for the necessity of a god, some theists base their belief upon grounds independent of, or prior to, reason, making Occam's razor irrelevant. This was the stance of Søren Kierkegaard, who viewed belief in God as a leap of faith that sometimes directly opposed reason. This is also the doctrine of Gordon Clark's presuppositional apologetics, with the exception that Clark never thought the leap of faith was contrary to reason (see also Fideism).\n\nVarious arguments in favor of God establish God as a useful or even necessary assumption. Contrastingly some anti-theists hold firmly to the belief that assuming the existence of God introduces unnecessary complexity (Schmitt 2005, e.g., the Ultimate Boeing 747 gambit).\n\nAnother application of the principle is to be found in the work of George Berkeley (1685–1753). Berkeley was an idealist who believed that all of reality could be explained in terms of the mind alone. He invoked Occam's razor against materialism, stating that matter was not required by his metaphysic and was thus eliminable. One potential problem with this belief is that it's possible, given Berkeley's position, to find solipsism itself more in line with the razor than a God-mediated world beyond a single thinker.\n\nOccam's razor may also be recognized in the apocryphal story about an exchange between Pierre-Simon Laplace and Napoleon. It is said that in praising Laplace for one of his recent publications, the emperor asked how it was that the name of God, which featured so frequently in the writings of Lagrange, appeared nowhere in Laplace's. At that, he is said to have replied, \"It's because I had no need of that hypothesis.\" Though some point to this story as illustrating Laplace's atheism, more careful consideration suggests that he may instead have intended merely to illustrate the power of methodological naturalism, or even simply that the fewer logical premises one assumes, the stronger is one's conclusion.\n\nIn his article \"Sensations and Brain Processes\" (1959), J. J. C. Smart invoked Occam's razor with the aim to justify his preference of the mind-brain identity theory over spirit-body dualism. Dualists state that there are two kinds of substances in the universe: physical (including the body) and spiritual, which is non-physical. In contrast, identity theorists state that everything is physical, including consciousness, and that there is nothing nonphysical. Though it is impossible to appreciate the spiritual when limiting oneself to the physical, Smart maintained that identity theory explains all phenomena by assuming only a physical reality. Subsequently, Smart has been severely criticized for his use (or misuse) of Occam's razor and ultimately retracted his advocacy of it in this context. Paul Churchland (1984) states that by itself Occam's razor is inconclusive regarding duality. In a similar way, Dale Jacquette (1994) stated that Occam's razor has been used in attempts to justify eliminativism and reductionism in the philosophy of mind. Eliminativism is the thesis that the ontology of folk psychology including such entities as \"pain\", \"joy\", \"desire\", \"fear\", etc., are eliminable in favor of an ontology of a completed neuroscience.\n\nIn penal theory and the philosophy of punishment, parsimony refers specifically to taking care in the distribution of punishment in order to avoid excessive punishment. In the utilitarian approach to the philosophy of punishment, Jeremy Bentham's \"parsimony principle\" states that any punishment greater than is required to achieve its end is unjust. The concept is related but not identical to the legal concept of proportionality. Parsimony is a key consideration of the modern restorative justice, and is a component of utilitarian approaches to punishment, as well as the prison abolition movement. Bentham believed that true parsimony would require punishment to be individualised to take account of the sensibility of the individual—an individual more sensitive to punishment should be given a proportionately lesser one, since otherwise needless pain would be inflicted. Later utilitarian writers have tended to abandon this idea, in large part due to the impracticality of determining each alleged criminal's relative sensitivity to specific punishments.\n\nMarcus Hutter's universal artificial intelligence builds upon Solomonoff's mathematical formalization of the razor to calculate the expected value of an action.\n\nThere are various papers in scholarly journals deriving formal versions of Occam's razor from probability theory, applying it in statistical inference, and using it to come up with criteria for penalizing complexity in statistical inference. Papers have suggested a connection between Occam's razor and Kolmogorov complexity.\n\nOne of the problems with the original formulation of the razor is that it only applies to models with the same explanatory power (i.e., it only tells us to prefer the simplest of equally good models). A more general form of the razor can be derived from Bayesian model comparison, which is based on Bayes factors and can be used to compare models that don't fit the observations equally well. These methods can sometimes optimally balance the complexity and power of a model. Generally, the exact Occam factor is intractable, but approximations such as Akaike information criterion, Bayesian information criterion, Variational Bayesian methods, false discovery rate, and Laplace's method are used. Many artificial intelligence researchers are now employing such techniques, for instance through work on Occam Learning or more generally on the Free energy principle.\n\nStatistical versions of Occam's razor have a more rigorous formulation than what philosophical discussions produce. In particular, they must have a specific definition of the term \"simplicity\", and that definition can vary. For example, in the Kolmogorov–Chaitin minimum description length approach, the subject must pick a Turing machine whose operations describe the basic operations \"believed\" to represent \"simplicity\" by the subject. However, one could always choose a Turing machine with a simple operation that happened to construct one's entire theory and would hence score highly under the razor. This has led to two opposing camps: one that believes Occam's razor is objective, and one that believes it is subjective.\n\nThe minimum instruction set of a universal Turing machine requires approximately the same length description across different formulations, and is small compared to the Kolmogorov complexity of most practical theories. Marcus Hutter has used this consistency to define a \"natural\" Turing machine of small size as the proper basis for excluding arbitrarily complex instruction sets in the formulation of razors. Describing the program for the universal program as the \"hypothesis\", and the representation of the evidence as program data, it has been formally proven under Zermelo–Fraenkel set theory that \"the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized.\" Interpreting this as minimising the total length of a two-part message encoding model followed by data given model gives us the minimum message length (MML) principle.\n\nOne possible conclusion from mixing the concepts of Kolmogorov complexity and Occam's razor is that an ideal data compressor would also be a scientific explanation/formulation generator. Some attempts have been made to re-derive known laws from considerations of simplicity or compressibility.\n\nAccording to Jürgen Schmidhuber, the appropriate mathematical theory of Occam's razor already exists, namely, Solomonoff's theory of optimal inductive inference and its extensions. See discussions in David L. Dowe's \"Foreword re C. S. Wallace\" for the subtle distinctions between the algorithmic probability work of Solomonoff and the MML work of Chris Wallace, and see Dowe's \"MML, hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness\" both for such discussions and for (in section 4) discussions of MML and Occam's razor. For a specific example of MML as Occam's razor in the problem of decision tree induction, see Dowe and Needham's \"Message Length as an Effective Ockham's Razor in Decision Tree Induction\".\n\nOccam's razor is not an embargo against the positing of any kind of entity, or a recommendation of the simplest theory come what may. Occam's razor is used to adjudicate between theories that have already passed \"theoretical scrutiny\" tests and are equally well-supported by evidence. Furthermore, it may be used to prioritize empirical testing between two equally plausible but unequally testable hypotheses; thereby minimizing costs and wastes while increasing chances of falsification of the simpler-to-test hypothesis.\n\nAnother contentious aspect of the razor is that a theory can become more complex in terms of its structure (or syntax), while its ontology (or semantics) becomes simpler, or vice versa. Quine, in a discussion on definition, referred to these two perspectives as \"economy of practical expression\" and \"economy in grammar and vocabulary\", respectively.\n\nGalileo Galilei lampooned the \"misuse\" of Occam's razor in his \"Dialogue\". The principle is represented in the dialogue by Simplicio. The telling point that Galileo presented ironically was that if one really wanted to start from a small number of entities, one could always consider the letters of the alphabet as the fundamental entities, since one could construct the whole of human knowledge out of them.\n\nOccam's razor has met some opposition from people who have considered it too extreme or rash. Walter Chatton (c. 1290–1343) was a contemporary of William of Ockham (c. 1287–1347) who took exception to Occam's razor and Ockham's use of it. In response he devised his own \"anti-razor:\" \"If three things are not enough to verify an affirmative proposition about things, a fourth must be added, and so on.\" Although there have been a number of philosophers who have formulated similar anti-razors since Chatton's time, no one anti-razor has perpetuated in as much notability as Chatton's anti-razor, although this could be the case of the Late Renaissance Italian motto of unknown attribution \"Se non è vero, è ben trovato\" (\"Even if it is not true, it is well conceived\") when referred to a particularly artful explanation.\n\nAnti-razors have also been created by Gottfried Wilhelm Leibniz (1646–1716), Immanuel Kant (1724–1804), and Karl Menger (1902–1985). Leibniz's version took the form of a principle of plenitude, as Arthur Lovejoy has called it: the idea being that God created the most varied and populous of possible worlds. Kant felt a need to moderate the effects of Occam's razor and thus created his own counter-razor: \"The variety of beings should not rashly be diminished.\"\n\nKarl Menger found mathematicians to be too parsimonious with regard to variables, so he formulated his Law Against Miserliness, which took one of two forms: \"Entities must not be reduced to the point of inadequacy\" and \"It is vain to do with fewer what requires more.\" A less serious but (some might say) even more extremist anti-razor is 'Pataphysics, the \"science of imaginary solutions\" developed by Alfred Jarry (1873–1907). Perhaps the ultimate in anti-reductionism, \"'Pataphysics seeks no less than to view each event in the universe as completely unique, subject to no laws but its own.\" Variations on this theme were subsequently explored by the Argentine writer Jorge Luis Borges in his story/mock-essay \"Tlön, Uqbar, Orbis Tertius\". There is also Crabtree's Bludgeon, which cynically states that \"[n]o set of mutually inconsistent observations can exist for which some human intellect cannot conceive a coherent explanation, however complicated.\"\n\n\n"}
{"id": "19777249", "url": "https://en.wikipedia.org/wiki?curid=19777249", "title": "Organizing principle", "text": "Organizing principle\n\nAn organizing principle is a core assumption from which everything else by proximity can derive a classification or a value. It is like a central reference point that allows all other objects to be located, often used in a conceptual framework. Having an organizing principle might help one simplify and get a handle on a particularly complicated domain or phenomenon. On the other hand, it might create a deceptive prism that colors one's judgment.\n\n\n\n"}
{"id": "25262679", "url": "https://en.wikipedia.org/wiki?curid=25262679", "title": "Phonemic imagery", "text": "Phonemic imagery\n\nPhonemic imagery refers to the processing of thoughts as words rather than as symbols or other images. It is sometimes referred to as the equivalent of inner speech or covert speech, and sometimes considered as a third phenomenon, separate from but similar to these other forms of internal speech.\n\nPhonemic imagery is a part of the philosophy of consciousness rather than linguistics as it is considered an internal phenomenon of consciousness observed through reflection rather than amenable to empirical observation.\n"}
{"id": "36606973", "url": "https://en.wikipedia.org/wiki?curid=36606973", "title": "Plastic Principle", "text": "Plastic Principle\n\nThe Plastic Principle is an idea introduced into Western thought by the English philosopher Ralph Cudworth (1617–1689) to explain the function of nature and life in the face of both the mechanism and materialism of the Enlightenment. It is a dynamic functional power that contains all of natural law, and is both sustentative and generative, organizing matter according to Platonic Ideas, that is, archetypes that lie beyond the physical realm coming from the Mind of God or Deity, the ground of Being.\n\nThe role of nature was one faced by philosophers in the Age of Reason or Enlightenment. The prevailing view was either that of the Church of a personal deity intervening in his creation, producing miracles, or an ancient pantheism (atheism relative to theism) – deity pervading all things and existing in all things. However, the \"ideas of an all-embracing providential care of the world and of one universal vital force capable of organizing the world from within.\" presented difficulties for philosophers of a spiritual as well as materialistic bent.\n\nThe Cartesian idea of nature as mechanical, and Hobbes' materialistic views were countered by the English philosopher, Ralph Cudworth (1617–1689), who, in his \"True intellectual system of the universe\" (1678), addressing the tension between theism and atheism, took both the Stoic idea of Divine Reason poured into the world, and the Platonic idea of the world soul (\"anima mundi\") to posit a power that was polaric – \"either as a ruling but separate mind or as an informing vital principle – either nous hypercosmios or nous enkosmios.\n\nCudworth was a member of the Cambridge Platonists, a group of English seventeenth-century thinkers associated with the University of Cambridge who were stimulated by Plato's teachings but also were aware or and influenced by Descartes, Hobbes, Bacon, Boyle and Spinoza. The other important philosopher of this group was Henry More (1614–1687). More held that spiritual substance or mind controlled inert matter. Out of his correspondence with Descartes, he developed the idea that everything, whether material or non, had extension, an example of the latter being space, which is infinite (Newton) and which then is correlative to the idea of God (set out in his Enchiridion metaphysicum 1667). In developing this idea, More also introduced a causal agent between God and substance, or Nature in his Hylarchic Principle, derived from Plato's \"anima mundi\" or world soul, and the Stoic's pneuma, which encapsulates the laws of nature, both for inert and vital nature, and involves a sympathetic resonance between soul (\"psyche\") and \"soma\".\n\nLike More, Cudworth put forward the idea of 'the Plastick Life of Nature', a formative principle that contains both substance and the laws of motion, as well as a nisus or direction that accounts for design and goal in the natural world. He was stimulated by the Cartesian idea of the mind as self-consciousness to see God as consciousness. He first analysed four forms of atheism from ancient times to present, and showed that all misunderstood the principle of life and knowledge, which involved unsentient activity and self-consciousness.\n\nAll of the atheistic approaches posted nature as unconscious, which for Cudworth was ontologically unsupportable, as a principle that was supposed to be the ultimate source of life and meaning could only be itself self-conscious and knowledgeable, that is, rational, otherwise creation or nature degenerates into inert matter set in motion by random external forces (Coleridge's 'chance whirlings of unproductive particles'). Cudworth saw nature as a vegetative power endowed with plastic (forming) and spermatic (generative) forces, but one with Mind, or a self-conscious knowledge. This idea would later emerge in the Romantic period in German science as Blumenbach's \"Bildungstreib\" (generative power) and the \"Lebenskraft\" (or \"Bildungskraft\").\n\nThe essence of atheism for Cudworth was the view that matter was self-active and self-sufficient, whereas for Cudworth the plastic power was unsentient and under the direct control of the universal Mind or \"Logos\". For him atheism, whether mechanical or material could not solve the \"phenomenon of nature.\" Henry More argued that atheism made each substance independent and self-acting such that it 'deified' matter. Cudworth argued that materialism/mechanism reduced \"substance to a corporeal entity, its activity to causal determinism, and each single thing to fleeting appearances in a system dominated by material necessity.\"\n\nCudworth had the idea of a general plastic nature of the world, containing natural laws to keep all of nature, inert and vital in orderly motion, and particular plastic natures in particular entities, which serve as 'Inward Principles' of growth and motion, but ascribes it to the Platonic tradition:\nFurther, Cudsworth's plastic principle was also a functional polarity. As he wrote:\n\nAs another historian notes in conclusion, \"Cudworth’s theory of plastic natures is offered as an alternative to the interpretation of all of nature as either governed by blind chance, or, on his understanding of the Malebranchean view, as micro-managed by God.\"\n\nCudworth's plastic principle also involves a theory of mind that is active, that is, God or the Supreme Mind is \"the spermatic reason\" which gives rise to individual mind and reason. Human mind can also create, and has access to spiritual or super-sensible 'Ideas' in the Platonic sense. Cudworth challenged Hobbesian determinism in arguing that will is not distinct from reason, but a power to act that is internal, and therefore, the voluntary will function involves self-determination, not external compulsion, though we have the power to act either in accordance with God's will or not. Cudworth's 'hegemonikon' (taken from Stoicism) is a function within the soul that combines the higher functions of the soul (voluntary will and reason) on the one hand with the lower animal functions (instinct), and also constitutes the whole person, thus bridging the Cartesian dualism of body and soul or \"psyche\" and \"soma\". This idea provided the basis for a concept of self-awareness and identity of an individual that is self-directed and autonomous, an idea that anticipates John Locke.\n\nLocke examined how man came to knowledge via stimulus (rather than seeing ideas as inherent), which approach led to his idea of the 'thinking' mind, which is both receptive and pro-active. The first involves receiving sensations ('simple ideas') and the second by reflection – \"observation of its own inner operations\" (inner sense which leads to complex ideas), with the second activity acting upon the first. Thought is set in motion by outer stimuli which 'simple ideas' are taken up by the mind's self-activity, an \"active power\" such that the outer world can only be real-ized as action (natural cause) by the activity of consciousness. Locke also took the issue of life as lying not in substance but in the capacity of the self for consciousness, to be able to organize (associate) disparate events, that is to participate life by means of the sense experiences, which have the capacity to produce every kind of experience in consciousness. These ideas of Locke were taken over by Fichte and influenced German Romantic science and medicine. (See Romantic medicine and Brunonian system of medicine).\nThomas Reid and his \"Common Sense\" philosophy, was also influenced by Cudworth, taking his influence into the Scottish Enlightenment.\n\nBerkeley later developed the idea of a plastic life principle with his idea of an 'aether' or 'aetherial medium' that causes 'vibrations' that animate all living beings. For Berkeley, it is the very nature of this medium that generates the 'attractions' of entities to each other.\n\nBerkeley meant this 'aether' to supplant Newton's gravity as the cause of motion (neither seeing the polarity involved between two forces, as Cudworth had in his plastic principle). However, in Berkeley's conception, aether is both the movement of spirit and the motion of nature.\n\nBoth Cudworth's views and those of Berkeley were taken up by Coleridge in his metaphor of the eolian harp in his 'Effusion XXXV' as one commentator noted: \"what we see in the first manuscript is the articulation of Cudworth’s principle of plastic nature, which is then transformed in the published version into a Berkeleyan expression of the causal agency of motion performed by God’s immanent activity.\"\n\nCudworth's idea of the plastic principle and that of mind will also be taken up in a new way in the idea of emergent evolution.\n"}
{"id": "1137736", "url": "https://en.wikipedia.org/wiki?curid=1137736", "title": "Principle of sufficient reason", "text": "Principle of sufficient reason\n\nThe principle of sufficient reason states that everything must have a reason or a cause. The modern formulation of the principle is usually attributed to Gottfried Leibniz, although the idea was conceived of and utilized by various philosophers who preceded him, including Anaximander, Parmenides, Archimedes, Plato and Aristotle, Cicero, Avicenna, Thomas Aquinas, and Spinoza. Some philosophers have associated the principle of sufficient reason with \"ex nihilo nihil fit\". Hamilton identified the laws of inference modus ponens with the \"law of Sufficient Reason, or of Reason and Consequent\" and modus tollens with its contrapositive expression.\n\nThe principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\n\nA sufficient explanation may be understood either in terms of \"reasons\" or \"causes,\" for like many philosophers of the period, Leibniz did not carefully distinguish between the two. The resulting principle is very different, however, depending on which interpretation is given.\n\nIt is an open question whether the principle of sufficient reason can be applied to axioms within a logic construction like a mathematical or a physical theory, because axioms are propositions accepted as having no justification possible within the system\nThe principle declares that all propositions considered to be true within a system should be deducible from the set axioms at the base of the construction (with some theoretical exceptions: see Gödel's theorem).\n\nLeibniz identified two kinds of truth, necessary and contingent truths. He believed necessary mathematical truths to be derived from the law of identity (and the principle of contradiction): \"Necessary truths are those that can be demonstrated through an analysis of terms, so that in the end they become identities, just as in Algebra an equation expressing an identity ultimately results from the substitution of values [for variables]. That is, necessary truths depend upon the principle of contradiction.\" Leibniz states that the sufficient reason for necessary truths is that their negation is a contradiction.\n\nLeibniz admitted contingent truths on the basis of infinitary reasons, to which God had access but humans did not:\nIn contingent truths, even though the predicate is in the subject, this can never be demonstrated, nor can a proposition ever be reduced to an equality or to an identity, but the resolution proceeds to infinity, God alone seeing, not the end of the resolution, of course, which does not exist, but the connection of the terms or the containment of the predicate in the subject, since he sees whatever is in the series.Without this qualification, the principle can be seen as a description of a certain notion of closed system, in which there is no 'outside' to provide unexplained events with causes. It is also in tension with the paradox of Buridan's ass. Leibniz denied that the paradox of Buridan's ass could ever occur, saying:\n\nLeibniz also used the principle of sufficient reason to refute the idea of absolute space:\n\nI say then, that if space is an absolute being, there would be something for which it would be impossible there should be a sufficient reason. Which is against my axiom. And I prove it thus. Space is something absolutely uniform; and without the things placed in it, one point in space does not absolutely differ in any respect whatsoever from another point of space. Now from hence it follows, (supposing space to be something in itself, beside the order of bodies among themselves,) that 'tis impossible that there should be a reason why God, preserving the same situation of bodies among themselves, should have placed them in space after one particular manner, and not otherwise; why everything was not placed the quite contrary way, for instance, by changing East into West.\n\nThe principle was one of the four recognised laws of thought, that held a place in European pedagogy of logic and reasoning (and, to some extent, philosophy in general) in the 18th and 19th centuries. It was influential in the thinking of Leo Tolstoy, amongst others, in the elevated form that history could not be accepted as random.\n\nA sufficient reason is sometimes described as the coincidence of every single thing that is needed for the occurrence of an effect (i.e. of the so-called \"necessary conditions\"). Such view could perhaps be also applied to indeterministic systems, as long as randomness is in a way incorporated in the preconditions.\n\nHere is how Hamilton, circa 1837–1838, expressed his \"fourth law\" in his LECT. V. LOGIC. 60–61:\n\nAccording to Schopenhauer's \"On the Fourfold Root of the Principle of Sufficient Reason\", there are four distinct forms of the principle.\n\nFirst Form: The Principle of Sufficient Reason of Becoming (principium rationis sufficientis fiendi); appears as the law of causality in the understanding.\n\nSecond Form: The Principle of Sufficient Reason of Knowing (principium rationis sufficientis cognoscendi); asserts that if a judgment is to express a piece of knowledge, it must have a sufficient ground or reason, in which case it receives the predicate true.\n\nThird Form: The Principle of Sufficient Reason of Being (principium rationis sufficientis essendi); the law whereby the parts of space and time determine one another as regards those relations. Example in arithmetic: Each number presupposes the preceding numbers as grounds or reasons of its being; \"I can reach ten only by going through all the preceding numbers; and only by virtue of this insight into the ground of being, do I know that where there are ten, so are there eight, six, four.\"\n\n\"Now just as the subjective correlative to the first class of representations is the understanding, that to the second the faculty of reason, and that to the third pure sensibility, so is the subjective correlative to this fourth class found to be the inner sense, or generally self-consciousness.\" \n\nFourth Form: The Principle of Sufficient Reason of Acting (principium rationis sufficientis agendi); briefly known as the law of motivation. \"Any judgment that does not follow its previously existing ground or reason\" or any state that cannot be explained away as falling under the three previous headings \"must be produced by an act of will which has a motive.\" As his proposition in 43 states, \"Motivation is causality seen from within.\"\n\nSeveral proofs have been prepared in order to demonstrate that the universe is at bottom causal, i.e. works in accord with the principle in question; perhaps not in every single case (randomness might still play a part here and there), but that causality must be the way it works at least \"in general\", in most of what we see; and that our minds are aware of the principle even before any experience. The two famous arguments or proofs were proposed by Immanuel Kant (from the form of Time, temporal ordering of events and \"directionality\" of time) and by Arthur Schopenhauer (by demonstrating how all perception depends on causality and the intellect).\n\nOnce it is agreed (e.g. from a kind of an \"arrow of time\") that causal interconnections, as a form of principle of sufficient reason, indeed must in general exist everywhere in the universe (at least in the large scale), \"backwards\" causality in general might then be precluded using a form of the paradox of free will (i.e. an event that has a future source might cause us to remove that source quick enough and thus causality would not work).\n\n\n"}
{"id": "21647661", "url": "https://en.wikipedia.org/wiki?curid=21647661", "title": "Self model", "text": "Self model\n\nThe self-model is the central concept in the theory of consciousness called the self-model theory of subjectivity (SMT). This concept comprises experiences of ownership, of first person perspective, and of a long-term unity of beliefs and attitudes. These features are instantiated in the prefrontal cortex. This theory is an interdisciplinary approach to understanding and explaining the phenomenology of consciousness and the self. This theory has two core contents, the phenomenal self-model (PSM) and the phenomenal model of the intentionality relation (PMIR). Thomas Metzinger advanced the theory in his 1993 book \"Subjekt und Selbstmodell\" (Subject and self-model).\n\nThe PSM is an entity that “actually exists, not only as a distinct theoretical entity but something that will be empirically discovered in the future- for instance, as a specific stage of the global neural dynamics in the human brain”. Involved in the PSM are three phenomenal properties that must occur in order to explain the concept of the self. The first is mineness, “a higher order property of particular forms of phenomenal content,” or the idea of ownership. The second is perspectivalness, which is “a global, structural property of phenomenal space as a whole”. More simply, it is what is commonly referred to as the ecological self, the immovable center of perception. The third phenomenal property is selfhood, which is “the phenomenal target property” or the idea of the self over time. It is the property of phenomenal selfhood that plays the most important role in creating the fictional self and the first person perspective. Metzinger defines the first person perspective as the “existence of single coherent and temporally stable model of reality which is representationally centered around or on a single coherent and temporally stable phenomenal subject”. The first-person perspective can be non-conceptual and is autonomously active due to the constant reception of perceptual information by the brain. The brain, specifically the brainstem and hypothalamus, processes this information into representational content, namely linguistic reflections. The PSM then uses this representational content to attribute phenomenal states to our perceived objects and ourselves. We are thus what Metzinger calls naïve realists, who believe we are perceiving reality directly when in actuality we are only perceiving representations of reality. The data structures and transport mechanisms of the data are “transparent” so that we can introspect on our representations of perceptions, but cannot introspect on the data or mechanisms themselves. These systemic representational experiences are then connected by subjective experience to generate the phenomenal property of selfhood. Subjective experience is the result of the Phenomenal Model of Intentionality Relationship (PMIR). The PMIR is a “conscious mental model, and its content is an ongoing, episodic subject-object relation”. The model is a result of the combination of our unique set of sensory receptors that acquire input, our unique set of experiences that shape connections within the brain, and our unique positions in space that give our perception perspectivalness.\n\nThe prefrontal cortex is implicated in all the functions of the human self model. The following functions all require communication with the prefrontal cortex; agency and association areas of the cortex; spatial perspectivity and the parietal lobes, unity and the temporal lobes.\n\nDisorders of the self model are implicated in several disorders including schizophrenia, autism, and depersonalization. According to this theory, long-term unity is impaired in autism, similar to theory of mind deficits and weak central coherence theory. Individuals with autism are thought to be impaired in assigning mental states to other people, an ability that probably codevelops with long-term unity of self. Weak central coherence, that is, the inability to assemble information into a cohesive whole, reflects the same problems with creating a unified sense of self and benific sense extreme in narcissism.\n\n"}
{"id": "55888", "url": "https://en.wikipedia.org/wiki?curid=55888", "title": "Trusted system", "text": "Trusted system\n\nIn the security engineering subspecialty of computer science, a trusted system is a system that is relied upon to a specified extent to enforce a specified security policy. This is equivalent to saying that a trusted system is one whose failure would break a security policy (if a policy exists that the trusted system is trusted to enforce).\n\nThe meaning of the word \"trust\" is critical, as it does not carry the meaning that might be expected in everyday usage. A system trusted by a user, is one that the user feels safe to use, and trusts to do tasks without secretly executing harmful or unauthorised programs; while trusted computing refers to whether programs can trust the platform to be unmodified from that expected, whether or not those programs are innocent, malicious or execute tasks that are undesired by the user.\n\nA subset of trusted systems (\"Division B\" and \"Division A\") implement mandatory access control (MAC) labels; as such, it is often assumed that they can be used for processing classified information. However, this is generally untrue. There are four modes in which one can operate a multilevel secure system: multilevel mode, compartmented mode, dedicated mode, and system-high mode. The National Computer Security Center's \"Yellow Book\" specifies that B3 and A1 systems can only be used for processing a strict subset of security labels, and only when operated according to a particularly strict configuration.\n\nCentral to the concept of U.S. Department of Defense-style \"trusted systems\" is the notion of a \"reference monitor\", which is an entity that occupies the logical heart of the system and is responsible for all access control decisions. Ideally, the reference monitor is (a) tamper-proof, (b) always invoked, and (c) small enough to be subject to independent testing, the completeness of which can be assured. Per the U.S. National Security Agency's 1983 Trusted Computer System Evaluation Criteria (TCSEC), or \"Orange Book\", a set of \"evaluation classes\" were defined that described the features and assurances that the user could expect from a trusted system.\n\nKey to the provision of the highest levels of assurance (B3 and A1) is the dedication of significant system engineering toward minimization of the complexity (not \"size\", as often cited) of the trusted computing base (TCB), defined as that combination of hardware, software, and firmware that is responsible for enforcing the system's security policy.\n\nAn inherent engineering conflict would appear to arise in higher-assurance systems in that, the smaller the TCB, the larger the set of hardware, software, and firmware that lies outside the TCB and is, therefore, untrusted. Although this may lead the more technically naive to sophists' arguments about the nature of trust, the argument confuses the issue of \"correctness\" with that of \"trustworthiness\".\n\nIn contrast to the TCSEC's precisely defined hierarchy of six evaluation classes—the highest of which, A1, is featurally identical to B3, differing only in documentation standards—the more recently introduced Common Criteria (CC)—which derive from a blend of more or less technically mature standards from various NATO countries—provide a more tenuous spectrum of seven \"evaluation classes\" that intermix features and assurances in an arguably non-hierarchical manner and lack the philosophic precision and mathematical stricture of the TCSEC. In particular, the CC tolerate very loose identification of the \"target of evaluation\" (TOE) and support—even encourage—an inter-mixture of security requirements culled from a variety of predefined \"protection profiles.\" While a strong case can be made that even the more seemingly arbitrary components of the TCSEC contribute to a \"chain of evidence\" that a fielded system properly enforces its advertised security policy, not even the highest (E7) level of the CC can truly provide analogous consistency and stricture of evidentiary reasoning.\n\nThe mathematical notions of trusted systems for the protection of classified information derive from two independent but interrelated corpora of work. In 1974, David Bell and Leonard LaPadula of MITRE, working under the close technical guidance and economic sponsorship of Maj. Roger Schell, Ph.D., of the U.S. Army Electronic Systems Command (Ft. Hanscom, MA), devised what is known as the Bell-LaPadula model, in which a more or less trustworthy computer system is modeled in terms of objects (passive repositories or destinations for data, such as files, disks, printers) and subjects (active entities—perhaps users, or system processes or threads operating on behalf of those users—that cause information to flow among objects). The entire operation of a computer system can indeed be regarded a \"history\" (in the serializability-theoretic sense) of pieces of information flowing from object to object in response to subjects' requests for such flows.\n\nAt the same time, Dorothy Denning at Purdue University was publishing her Ph.D. dissertation, which dealt with \"lattice-based information flows\" in computer systems. (A mathematical \"lattice\" is a partially ordered set, characterizable as a directed acyclic graph, in which the relationship between any two vertices is either \"dominates,\" \"is dominated by,\" or neither.) She defined a generalized notion of \"labels\"—corresponding more or less to the full security markings one encounters on classified military documents, \"e.g.\", TOP SECRET WNINTEL TK DUMBO—that are attached to entities. Bell and LaPadula integrated Denning's concept into their landmark MITRE technical report—entitled, \"Secure Computer System: Unified Exposition and Multics Interpretation\"—whereby labels attached to objects represented the sensitivity of data contained within the object (though there can be, and often is, a subtle semantic difference between the sensitivity of the data within the object and the sensitivity of the object itself), while labels attached to subjects represented the trustworthiness of the user executing the subject. The concepts are unified with two properties, the \"simple security property\" (a subject can only read from an object that it \"dominates\" [\"is greater than\" is a close enough—albeit mathematically imprecise—interpretation]) and the \"confinement property,\" or \"*-property\" (a subject can only write to an object that dominates it). (These properties are loosely referred to as \"no-read-up\" and \"no-write-down,\" respectively.) Jointly enforced, these properties ensure that information cannot flow \"downhill\" to a repository whence insufficiently trustworthy recipients may discover it. By extension, assuming that the labels assigned to subjects are truly representative of their trustworthiness, then the no-read-up and no-write-down rules rigidly enforced by the reference monitor are provably sufficient to constrain Trojan horses, one of the most general classes of attack (\"sciz.\", the popularly reported worms and viruses are specializations of the Trojan horse concept).\n\nThe Bell-LaPadula model technically only enforces \"confidentiality,\" or \"secrecy,\" controls, \"i.e.\", they address the problem of the sensitivity of objects and attendant trustworthiness of subjects to not inappropriately disclose it. The dual problem of \"integrity\"(i.e., the problem of accuracy, or even provenance of objects) and attendant trustworthiness of subjects to not inappropriately modify or destroy it, is addressed by mathematically affine models; the most important of which is named for its creator, K. J. Biba. Other integrity models include the Clark-Wilson model and Shockley and Schell's program integrity model, \"The SeaView Model\"\n\nAn important feature of MACs, is that they are entirely beyond the control of any user. The TCB automatically attaches labels to any subjects executed on behalf of users and files they access or modify. In contrast, an additional class of controls, termed discretionary access controls(DACs), are under the direct control of the system users. Familiar protection mechanisms such as permission bits (supported by UNIX since the late 1960s and—in a more flexible and powerful form—by Multics since earlier still) and access control lists (ACLs) are familiar examples of DACs.\n\nThe behavior of a trusted system is often characterized in terms of a mathematical model—which may be more or less rigorous depending upon applicable operational and administrative constraints—that takes the form of a finite state machine (FSM) with state criteria, state transition constraints, a set of \"operations\" that correspond to state transitions (usually, but not necessarily, one), and a descriptive top-level specification (DTLS) which entails a user-perceptible interface (\"e.g.\", an API, a set of system calls [in UNIX parlance] or system exits [in mainframe parlance]); each element of which engenders one or more model operations.\n\nThe Trusted Computing Group creates specifications that are meant to address particular requirements of trusted systems, including attestation of configuration and safe storage of sensitive information.\n\nTrusted systems in the context of national or homeland security, law enforcement, or social control policy are systems in which some conditional prediction about the behavior of people or objects within the system has been determined prior to authorizing access to system resources.\n\nFor example, trusted systems include the use of \"security envelopes\" in national security and counterterrorism applications, \"trusted computing\" initiatives in technical systems security, and the use of credit or identity scoring systems in financial and anti-fraud applications; in general, they include any system (i) in which probabilistic threat or risk analysis is used to assess \"trust\" for decision-making before authorizing access or for allocating resources against likely threats (including their use in the design of systems constraints to control behavior within the system), or (ii) in which deviation analysis or systems surveillance is used to ensure that behavior within systems complies with expected or authorized parameters.\n\nThe widespread adoption of these authorization-based security strategies (where the default state is DEFAULT=DENY) for counterterrorism, anti-fraud, and other purposes is helping accelerate the ongoing transformation of modern societies from a notional Beccarian model of criminal justice based on accountability for deviant actions after they occur – see Cesare Beccaria, On Crimes and Punishment (1764) – to a Foucauldian model based on authorization, preemption, and general social compliance through ubiquitous preventative surveillance and control through system constraints – see Michel Foucault, \"Discipline and Punish\" (1975, Alan Sheridan, tr., 1977, 1995).\n\nIn this emergent model, \"security\" is geared not towards policing but to risk management through surveillance, exchange of information, auditing, communication, and classification. These developments have led to general concerns about individual privacy and civil liberty and to a broader philosophical debate about the appropriate forms of social governance methodologies.\n\nTrusted systems in the context of information theory is based on the definition of trust as 'Trust is that which is essential to a communication channel but cannot be transferred from a source to a destination using that channel' by Ed Gerck.\n\nIn Information Theory, information has nothing to do with knowledge or meaning. In the context of Information Theory, information is simply that which is transferred from a source to a destination, using a communication channel. If, before transmission, the information is available at the destination then the transfer is zero. Information received by a party is that which the party does not expect—as measured by the uncertainty of the party as to what the message will be.\n\nLikewise, trust as defined by Gerck has nothing to do with friendship, acquaintances, employee-employer relationships, loyalty, betrayal and other overly-variable concepts. Trust is not taken in the purely subjective sense either, nor as a feeling or something purely personal or psychological—trust is understood as something potentially communicable. Further, this definition of trust is abstract, allowing different instances and observers in a trusted system to communicate based on a common idea of trust (otherwise communication would be isolated in domains), where all necessarily different subjective and intersubjective realizations of trust in each subsystem (man and machines) may coexist.\n\nTaken together in the model of Information Theory, information is what you do not expect and trust is what you know. Linking both concepts, trust is seen as qualified reliance on received information. In terms of trusted systems, an assertion of trust cannot be based on the record itself, but on information from other information channels. The deepening of these questions leads to complex conceptions of trust which have been thoroughly studied in the context of business relationships. It also leads to conceptions of information where the \"quality\" of information integrates trust or trustworthiness in the structure of the information itself and of the information system(s) in which it is conceived: higher quality in terms of particular definitions of accuracy and precision means higher trustworthiness.\n\nAn introduction to the calculus of trust (Example: 'If I connect two trusted systems, are they more or less trusted when taken together?') is given in.\n\nThe IBM Federal Software Group has suggested that provides the most useful definition of trust for application in an information technology environment, because it is related to other information theory concepts and provides a basis for measuring trust. In a network centric enterprise services environment, such notion of trust is considered to be requisite for achieving the desired collaborative, service-oriented architecture vision.\n\n\nSee also, The Trusted Systems Project, a part of the Global Information Society Project (GISP), a joint research project of the World Policy Institute (WPI) and the Center for Advanced Studies in Sci. & Tech. Policy (CAS).\n"}
