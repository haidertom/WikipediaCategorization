{"id": "509995", "url": "https://en.wikipedia.org/wiki?curid=509995", "title": "Abstract and concrete", "text": "Abstract and concrete\n\nAbstract and concrete are classifications that denote whether the object that a term describes has physical referents. Abstract objects have no physical referents, whereas concrete objects do. They are most commonly used in philosophy and semantics. Abstract objects are sometimes called abstracta (sing. abstractum) and concrete objects are sometimes called \"concreta\" (sing. \"concretum\"). An abstract object is an object that does not exist at any particular time or place, but rather exists as a type of thing—i.e., an idea, or abstraction. The term \"abstract object\" is said to have been coined by Willard Van Orman Quine. The study of abstract objects is called abstract object theory.\n\nThe type–token distinction identifies physical objects that are tokens of a particular type of thing. The \"type\" of which it is a part is in itself an abstract object. The abstract-concrete distinction is often introduced and initially understood in terms of paradigmatic examples of objects of each kind:\n\nAbstract objects have often garnered the interest of philosophers because they raise problems for popular theories. In ontology, abstract objects are considered problematic for physicalism and some forms of naturalism. Historically, the most important ontological dispute about abstract objects has been the problem of universals. In epistemology, abstract objects are considered problematic for empiricism. If abstracta lack causal powers or spatial location, how do we know about them? It is hard to say how they can affect our sensory experiences, and yet we seem to agree on a wide range of claims about them. \n\nSome, such as Edward Zalta and arguably, Plato in his Theory of Forms, have held that abstract objects constitute the defining subject matter of metaphysics or philosophical inquiry more broadly. To the extent that philosophy is independent of empirical research, and to the extent that empirical questions do not inform questions about abstracta, philosophy would seem especially suited to answering these latter questions. \n\nIn modern philosophy, the distinction between abstract and concrete was explored by Immanuel Kant and G. W. F. Hegel.\n\nGottlob Frege said that abstract objects, such as numbers, were members of a third realm, different from the external world or from internal consciousness. \n\nAnother popular proposal for drawing the abstract-concrete distinction contends that an object is abstract if it lacks any causal powers. A causal power has the ability to affect something causally. Thus, the empty set is abstract because it cannot act on other objects. One problem for this view is that it is not clear exactly what it is to have a causal power. For a more detailed exploration of the abstract-concrete distinction, follow the link below to the \"Stanford Encyclopedia\" article.\n\nJean Piaget uses the terms \"concrete\" and \"formal\" to describe the different types of learning. Concrete thinking involves facts and descriptions about everyday, tangible objects, while abstract (formal operational) thinking involves a mental process.\nRecently, there has been some philosophical interest in the development of a third category of objects known as the quasi-abstract. Quasi-abstract objects have drawn particular attention in the area of social ontology and documentality. Some argue that the over-adherence to the platonist duality of the concrete and the abstract has led to a large category of social objects having been overlooked or rejected as nonexisting because they exhibit characteristics that the traditional duality between concrete and abstract regards as incompatible. Specially, the ability to have temporal location, but not spatial location, and have causal agency (if only by acting through representatives). These characteristics are exhibited by a number of social objects, including states of the international legal system.\n\n"}
{"id": "26276222", "url": "https://en.wikipedia.org/wiki?curid=26276222", "title": "Bateman's principle", "text": "Bateman's principle\n\nBateman's principle, in evolutionary biology, is that in most species, variability in reproductive success (or reproductive variance) is greater in males than in females. It was first proposed by Angus John Bateman (1919–1996), an English geneticist. Bateman suggested that, since males are capable of producing millions of sperm cells with little effort, while females invest much higher levels of energy in order to nurture a relatively small number of eggs, the female plays a significantly larger role in their offspring's reproductive success. Bateman’s paradigm thus views females as the limiting factor of parental investment, over which males will compete in order to copulate successfully.\n\nAlthough Bateman's principle served as a cornerstone for the study of sexual selection for many decades, it has recently been subject to criticism. Attempts to reproduce Bateman's experiments in 2012 and 2013 were unable to support his conclusions. Some scientists have criticized Bateman's experimental and statistical methods, or pointed out conflicting evidence, while others have defended the veracity of the principle and cited evidence in support of it.\n\nTypically it is the females who have a relatively larger investment in producing each offspring. Bateman attributed the origin of the unequal investment to the differences in the production of gametes: sperm are cheaper than eggs. A single male can easily fertilize all females' eggs; she will not produce more offspring by mating with more than one male. A male is capable of fathering more offspring if he mates with several females. By and large, a male's potential reproductive success is limited by the number of females he mates with, whereas a female's potential reproductive success is limited by how many eggs she can produce. According to Bateman's principle, this results in sexual selection, in which males compete with each other, and females become choosy in which males to mate with. Thus, as a result of being anisogamous, males are fundamentally promiscuous, and females are fundamentally selective.\n\nBateman initially published his review in 1948. He was a botanist, contributing to the literature of sexual selection only once in his lifetime. Bateman initially saw his study on Drosophila to be a test of Charles Darwin’s doctrine of sexual selection. He saw Darwin’s theory of natural selection not as flawed, but as incomplete. He felt that if he were to provide a concrete demonstration of how sexual selection played a role in the reproductive success of certain species, he could explain the gap between Darwin’s ideas and sexual dimorphism.\n\nAlthough it is common to confuse Bateman's ideas with those of later scientists, his principle can be expressed in three simple statements. The first is that male reproductive success increases with the number of mates they attempt to copulate with, while female reproductive success does not. The second is that male reproductive success will show greater variance than female. The third is that sexual selection will have a greater effect on the sex with greater variance in reproductive success.\n\nThroughout his research, Bateman conducted experiments using fruit flies in order to observe their copulation and sexual behavior. A total of six series of experiments were conducted with the fruit fly \"Drosophila melanogaster\", using three to five individuals of each sex. Each trial ran for three or four days. Some ran to completion without the transfer of the Drosophila from one environment (bottle) to another. In the others, Bateman transferred the flies and their eggs to a new bottle every day. Bateman also varied the age of the flies depending on the experiment, with an age gap between one and six days total. He never watched the flies' copulations. The flies used were from several inbred strains, which meant they could be identified by their specific inbred strain. Therefore, he inferred the number of involved mates based on the number of offspring that were later found to have mutations from both a male and a female. The difficulty that arose was that if a female Drosophila had copulated with five males and only one larva survived, Bateman would not be able to account for the other four copulations.\n\nAnalysis of the data collected in sets one through four showed that the males' reproductive success, estimated as the number of sired offspring, increased at a steady rate until a total of three mates were reached. It is important to note that Bateman kept the sex ratio of males to females completely even throughout his trials. But after surpassing three mates, male reproductive success began to fall. Female reproductive success also increased with number of mates, but much more gradually than that of the males. The second series of data collected in sets five and six illustrated a dramatically different outcome. Male reproductive success increased at a steady and steep rate, never dropping. Female reproductive success, on the other hand, plateaued after a single mate. Bateman focused mainly on the second series of data when discussing his results. His main conclusion was that the reproductive success of females does not increase with an influx of mates, as one fit mate was enough to successfully complete fertilization. This is often referred to as Bateman’s Gradient.\n\nThroughout 2012 and 2013, Gowaty, Kim, and Anderson took it upon themselves to repeat Bateman's experiment in its entirety. While reproducing his tests, the same fly strains and mutations were used in order to maintain the same methodology. However, one of the 11 strains that Bateman used had gone extinct, and was thus replaced. (Tang-Martinez 2012)\n\nGowaty, Kim, and Anderson found that upon combining certain strains with one another, the offspring were unable to survive to adulthood. (Gowaty, Kim, & Anderson 2012) Thus, Bateman’s numbers regarding the number of individuals not having mated was higher than the actual number. Likewise, his estimate of those that mated with one or more mates was too low. This was valid for both the males and females of this species.\n\nGowaty desired to further explore the reasoning behind the premature death of the Drosophila. She began doing so by running monogamy trials between different strains of flies and found that 25% of the offspring died due to becoming double mutants. (Gowaty 2013) Bateman thought his work fit within the lines of Mendel’s laws of genetics, while Gowaty proved otherwise. The 1948 experiments inferred reproductive success based on the number of adults living by the end of the trial. In reality, many factors were left out of the equation when calculating reproductive success as a function of the number of mates, which had the ability to completely dislodge the accuracy behind Bateman's results. Gowaty was not able to confirm Bateman's conclusions and found no evidence for sexual selection in the experiment. (Gowaty 2013; Tang-Martinez 2012)\n\nNevertheless, some modern experiments between the relationship of number of mates and the reproductive success of males and females support Bateman's principle. Julie Collet conducted an experiment with a population of red jungle fowl. A total of thirteen replicate groups of three males and four females were monitored for ten days. In this experiment, the sex ratio was biased toward females. A male's reproductive success was calculated using the proportion of embryos fathered to the total number of embryos produced by all the females he mated with. The total sexual selection opportunity was calculated using the following formula.\n\nThe \"σ\" represents the variance in RS, while the is the square mean of reproductive success of members of one sex in a group.\n\nIn 2013, Fritzsche and Arnqvist tested Bateman's principle by estimating sexual selection between males and females in four seed beetles. They used a unique experimental design that showed sexual selection to be greater in males than in females. In contrast, sexual selection was also shown to be stronger for females in role-reversed species. They suggested that the Bateman gradient is typically the most accurate and informative measure of sexual selection between different sexes and species (Fritzsche, K. & Arnqvist, G).\n\nMore than 60 years later, Bateman's principle has received considerable attention. Sutherland argued that males' higher variance in reproductive success may result from random mating and coincidence. Hubbell and Johnson suggested that variance in reproductive success can be greatly influenced by the time and allocations of mating. In 2005, Gowaty and Hubbell suggested that mating tendencies are subject to change depending on certain strategies. They argued that there are cases in which males can be more selective than females, whereas Bateman suggested that his paradigm would be “almost universal” among sexually reproducing species. Critics proposed that females might be more subject to sexual selection than males, but not in all circumstances.\n\nExperimental and statistical criticisms followed. Until approximately a decade ago, critics of Bateman’s model focused on his experimental design. In recent years, they have shifted attention to the actual experimental and statistical calculations Bateman published throughout his trials. Birkhead wrote a 2000 review arguing that since Bateman’s experiments lasted only three to four days, the female fruit fly, \"Drosophila melanogaster\", may not have needed to mate repeatedly, as it can store sperm for up to four days; if Bateman had used a species in which females had to copulate more often to fertilize their eggs, the results might have been different. Snyder and Gowaty conducted the first in-depth analysis of the data in Bateman’s 1948 paper. They found sampling biases, mathematical errors, and selective presentation of data.\n\nA 2012 review by Zuleyma Tang-Martínez concluded that various empirical and theoretical studies, especially Gowaty's reproduction of Bateman's original experiment, pose a major challenge to Bateman's conclusions, and that Bateman's principle should be considered an unproven hypothesis in need of further reexamination. According to Tang-Martínez, \"modern data simply don't support most of Bateman's and Trivers's predictions and assumptions.\"\n\nA 2016 review confirmed Darwinian sex roles across the animal kingdom, concluding that \"sexual selection, as captured by standard Bateman metrics, is indeed stronger in males than in females and that it is evolutionarily tied to sex biases in parental care and sexual dimorphism.\"\n\nOne error source that have been shown to give an illusion of greater differential in reproductive success in males than in females genetically is that chromosome effects cause a greater percentage of mutations to be lethal before even reaching sexual maturity in males than in females.\n\nThe assumption that any differential in reproductive success between males and females among the individuals that do reach sexual maturity must be due to sexual selection in the current population is also subject to criticism, such as the possibility of remnants of sexually selected traits in a previous species from which a new species have evolved can be negatively selected due to costs in nutrients and weakened immune systems and that such negative selection would cause a higher difference in reproductive success in males than in females even without any still ongoing sexual selection. Since lower degrees of selection during times of stable environment allows genetic variation to build up by random mutations and allow some individuals in a population to survive environmental change while strong constant selection offsets the effect and increases the risk of the entire population dying out during catastrophic environmental change due to less genetic variation, constant loss of genetic variation caused by sexual selection have been suggested as a factor contributing to higher extinction rates in more sexually dimorphic species besides the nutrient, immunity and other costs of the ornaments themselves. While the ornament cost risk would only be removed when the ornaments have been eliminated by selection, the genetic variation model predicts that the species ability to survive would improve significantly even at an early stage of reduction of sexual dimorphism due to other adaptive mutations arising and surviving due to minimal selection during times of stable environment while the genes causing sexually dimorphic anatomy have only in small part been affected by the mutations. Applied to human evolution, this model can explain why early Homo sapiens display a significantly increased adaptability to environmental change already at its early divergence from Homo erectus that had a high muscular sexual dimorphism, as well as why human anatomy through the history of Homo sapiens show a diversification during times of stable climate and a selective loss of the more robust male forms during environmental change that does not recover during later stability, continuing through the loss of many robust characteristics in regional bottlenecks as recent as the end of the Ice Age and the time around the agricultural revolution. It also explains genetic evidence of human genetic diversity increasing during stable environmental periods and being reduced during bottlenecks related to changes in the environment.\n\nRecently, DNA testing has permitted more detailed investigation of mating behavior in numerous species. The results, in many cases, have been cited as evidence against Bateman's principle.\n\nUntil recently, most bird species were believed to be sexually monogamous. DNA paternity testing, however, has shown that in nearly 90% of bird species, females copulate with multiple males during each breeding season. The superb fairy wren is socially monogamous, but 95% of its clutches contain young fathered by extra-pair males. Up to 87% of tree swallow clutches, 75% of coal tit clutches, and 70% of reed bunting clutches contain young fathered by extra-pair males. Even female waved albatrosses, which typically mate for life, are sexually promiscuous, with 17% of young fathered by extra-pair males.\n\nIn many primate species, females solicit sex from males and may mate with more than one male in quick succession. Female lions may mate 100 times per day with different males while they are in estrus. Females of the pseudoscorpion species, \"Cordylochernes scorpioides\", have been shown to have higher reproductive success when mated with more than one male.\n\nThere are a number of claims about sex differences in humans said by evolutionary psychologists to be well supported that are not logically predicted by Bateman's principle. One example that have been pointed out is language ability that is said to be more developed in women than in men. It is argued that language, being a derived human trait, is not predicted by a model that assumes males to be more selected than females to be present to a higher degree in women than in men but the opposite. The objection about a trade-off between spatial ability and language is found lacking due to the existence of animals that outperform humans on spatial tasks such as birds of prey and bats while having much smaller and more rapidly maturing brains than humans, while great ape language is limited and ungrammatical at best despite great apes being much closer to humans in brain capacity than bats or birds of prey are, showing that the system requirements for spatial ability are much lower than those for language and therefore not a serious trade-off for them. The generalization of Bateman's principle to societies lacking modern birth control is also argued to be incompatible with such generalization of the \"nuns more successful at celibacy than monks\" clause of the female erotic plasticity model, as absence of modern contraceptives would mean that absence of reproduction for many men required that many men had no sexual relationships with women while male sexual exclusivity to a particular gender, if present, would have precluded same-sex relationships as a form of non-celibacy for men who could not find a female mate as well. It is also mentioned that the evolutionary psychologists themselves claim that women are sexually picky due to an evolutionary history without modern contraception, while it is pointed out that the notion of celibacy being impossible for men in general with only a small number of individual exceptions is incompatible with the notion of evolution making celibacy inevitable for a significant percentage of men.\n\nThe most well-known exceptions to Bateman's principle are the existence of sex-role reversed species such as pipefish (seahorses), phalaropes and jacanas in which the males perform the majority of the parental care, and are cryptic while the females are highly ornamented and territorially aggressive (; ; ). \n\nIn these species, however, the typical fundamental sex differences are reversed: females have a faster reproductive rate than males (and thus greater reproductive variance), and males have greater assurance of genetic parentage than do females . Consequently, reversals in sex roles and reproductive variance are consistent with Bateman's principle, and with Robert Trivers's parental investment theory.\n\n\n\n"}
{"id": "39105", "url": "https://en.wikipedia.org/wiki?curid=39105", "title": "Boehm system", "text": "Boehm system\n\nThe Boehm system is a system of keywork for the flute, created by inventor and flautist Theobald Boehm between 1831 and 1847. \n\nPrior to the development of the Boehm system, flutes were most commonly made of wood, with an inverse conical bore, eight keys, and tone holes (the openings where the fingers are placed to produce specific notes) that were small in size, and thus easily covered by the fingertips. Boehm's work was inspired by an 1831 concert in London, given by soloist Charles Nicholson who, with his father in the 1820s, had introduced a flute constructed with larger tone holes than were used in previous designs. This large-holed instrument could produce greater volume of sound than other flutes, and Boehm set out to produce his own large-holed design.\n\nIn addition to large holes, Boehm provided his flute with \"full venting\", meaning that all keys were normally open (previously, several keys were normally closed, and opened only when the key was operated). Boehm also wanted to locate tone holes at acoustically optimal points on the body of the instrument, rather than locations conveniently covered by the player's fingers. To achieve these goals, Boehm adapted a system of axle-mounted keys with a series of \"open rings\" (called \"brille\", German for \"eyeglasses\", as they resembled the type of eyeglass frames common during the 19th century) that were fitted around other tone holes, such that the closure of one tone hole by a finger would also close a key placed over a second hole.\n\nIn 1832 Boehm introduced a new conical-bore flute, which achieved a fair degree of success. Boehm, however, continued to look for ways to improve the instrument. Finding that an increased volume of air produced a stronger and clearer tone, he replaced the conical bore with a cylindrical bore, finding that a parabolic contraction of the bore near the embouchure hole improved the instrument's low register. He also found that optimal tone was produced when the tone holes were too large to be covered by the fingertips, and he developed a system of finger plates to cover the holes. These new flutes were at first made of silver, although Boehm later produced wooden versions. \n\nThe cylindrical Boehm flute was introduced in 1847, with the instrument gradually being adopted almost universally by professional and amateur players in Europe and around the world during the second half of the 19th century. The instrument was adopted for the performance of orchestral and chamber music, opera and theater, wind ensembles (e.g., military and civic bands), and most other music which might be loosely described as relating to \"Western classical music\" (including, for example, jazz). Many further refinements have been made, and countless design variations are common among flutes today (the \"offset G\" key, addition of the low B foot, etc.) The concepts of the Boehm system have been applied across the range of flutes available, including piccolos, alto flutes, bass flutes, and so on, as well as other wind instruments. The material of the instrument may vary (many piccolos are made of wood, some very large flutes are wooden or even made of PVC).\n\nThe flute is perhaps the oldest musical instrument, other than the human voice itself. There are very many flutes, both traversely blown and end-blown \"fipple\" flutes, currently produced which are not built on the Boehm model.\n\nThe fingering system for the saxophone closely resembles the Boehm system. A key system inspired by Boehm's for the clarinet family is also known as the \"Boehm system\", although it was developed by Hyacinthe Klosé and not Boehm himself. The Boehm system was also adapted for a small number of flageolets. Boehm did work on a system for the bassoon, and Boehm-inspired oboes have been made, but non-Boehm systems remain predominant for these instruments. The Albert system is another key system for the clarinet.\n\n\n"}
{"id": "10000937", "url": "https://en.wikipedia.org/wiki?curid=10000937", "title": "Category (Kant)", "text": "Category (Kant)\n\nIn Kant's philosophy, a category ( in the original or \"Kategorie\" in modern German) is a pure concept of the understanding (\"Verstand\"). A Kantian category is a characteristic of the appearance of any object in general, before it has been experienced. Kant wrote that \"They are concepts of an object in general….\" Kant also wrote that, \"…pure cоncepts [Categories] of the undеrstanding which apply to objects of intuition in general….\" Such a category is not a classificatory division, as the word is commonly used. It is, instead, the condition of the possibility of objects in general, that is, objects as such, any and all objects, not specific objects in particular.\n\nThe word comes from the Greek κατηγορία, \"katēgoria\", meaning \"that which can be said, predicated, or publicly declared and asserted, about something.\" A category is an attribute, property, quality, or characteristic that can be predicated of a thing. \"…I remark concerning the categories…that their logical employment consists in their use as predicates of objects.\" Kant called them \"ontological predicates.\"\n\nA category is that which can be said of everything in general, that is, of anything that is an object. John Stuart Mill wrote: \"The Categories, or Predicaments — the former a Greek word, the latter its literal translation in the Latin language — were believed to be an enumeration of all things capable of being named, an enumeration by the \"summa genera\" (highest kind), i.e., the most extensive classes into which things could be distributed, which, therefore, were so many highest Predicates, one or other of which was supposed capable of being affirmed with truth of every nameable thing whatsoever.\"\n\nAristotle had claimed that the following ten predicates or categories could be asserted of anything in general: substance, quantity, quality, relation, action, affection (passivity), place, time (date), position, and state. These are supposed to be the qualities or attributes that can be affirmed of each and every thing in experience. Any particular object that exists in thought must have been able to have the Categories attributed to it as possible predicates because the Categories are the properties, qualities, or characteristics of any possible object in general. The Categories of Aristotle and Kant are the general properties that belong to all things without expressing the peculiar nature of any particular thing. Kant appreciated Aristotle's effort, but said that his table was imperfect because \" … as he had no guiding principle, he merely picked them up as they occurred to him...\"\n\nThe Categories do not provide knowledge of individual, particular objects. Any object, however, must have Categories as its characteristics if it is to be an object of experience. It is presupposed or assumed that anything that is a specific object must possess Categories as its properties because Categories are predicates of an object in general. An object in general does not have all of the Categories as predicates at one time. For example, a general object cannot have the qualitative Categories of reality and negation at the same time. Similarly, an object in general cannot have both unity and plurality as quantitative predicates at once. The Categories of Modality exclude each other. Therefore, a general object cannot simultaneously have the Categories of possibility/impossibility and existence/non–existence as qualities.\n\nSince the Categories are a list of that which can be said of every object, they are related only to human language. In making a verbal statement about an object, a speaker makes a judgment. A general object, that is, every object, has attributes that are contained in Kant's list of Categories. In a judgment, or verbal statement, the Categories are the predicates that can be asserted of every object and all objects.\n\nKant believed that the ability of the human understanding (German: \"Verstand\", Greek: \"dianoia\" \"διάνοια\", Latin: \"ratio\") to think about and know an object is the same as the making of a spoken or written judgment about an object. According to him, \"Our ability to judge is equivalent to our ability to think.\"\nA judgment is the thought that a thing is known to have a certain quality or attribute. For example, the sentence \"The rose is red\" is a judgment. Kant created a table of the forms of such judgments as they relate to all objects in general.\n\nThis table of judgments was used by Kant as a model for the table of categories. Taken together, these twelvefold tables constitute the formal structure for Kant's architectonic conception of his philosophical system.\n\nCategories are entirely different from the appearances of objects. According to Kant, in order to relate to specific phenomena, categories must be \"applied\" through time. The way that this is done is called a schema.\n\nArthur Schopenhauer, in his criticism of the Kantian philosophy, found many errors in Kant's use of the Categories of Quality, Quantity, Relation, and Modality. Schopenhauer also noted that in accordance with Kant's claim, non-human animals would not be able to know objects. Animals would only know impressions on their sense organs, which Kant mistakenly calls perception.\n\n\n"}
{"id": "178942", "url": "https://en.wikipedia.org/wiki?curid=178942", "title": "Conceptual art", "text": "Conceptual art\n\nConceptual art, sometimes simply called conceptualism, is art in which the concept(s) or idea(s) involved in the work take precedence over traditional aesthetic, technical, and material concerns. Some works of conceptual art, sometimes called installations, may be constructed by anyone simply by following a set of written instructions. This method was fundamental to American artist Sol LeWitt's definition of Conceptual art, one of the first to appear in print:\n\nTony Godfrey, author of \"Conceptual Art (Art & Ideas)\" (1998), asserts that conceptual art questions the nature of art, a notion that Joseph Kosuth elevated to a definition of art itself in his seminal, early manifesto of conceptual art, \"Art after Philosophy\" (1969). The notion that art should examine its own nature was already a potent aspect of the influential art critic Clement Greenberg's vision of Modern art during the 1950s. With the emergence of an exclusively language-based art in the 1960s, however, conceptual artists such as Art & Language, Joseph Kosuth (who became the american editor of Art-Language), and Lawrence Weiner began a far more radical interrogation of art than was previously possible (see below). One of the first and most important things they questioned was the common assumption that the role of the artist was to create special kinds of material objects.\nThrough its association with the Young British Artists and the Turner Prize during the 1990s, in popular usage, particularly in the UK, \"conceptual art\" came to denote all contemporary art that does not practice the traditional skills of painting and sculpture. It could be said that one of the reasons why the term \"conceptual art\" has come to be associated with various contemporary practices far removed from its original aims and forms lies in the problem of defining the term itself. As the artist Mel Bochner suggested as early as 1970, in explaining why he does not like the epithet \"conceptual\", it is not always entirely clear what \"concept\" refers to, and it runs the risk of being confused with \"intention\". Thus, in describing or defining a work of art as conceptual it is important not to confuse what is referred to as \"conceptual\" with an artist's \"intention\".\n\nThe French artist Marcel Duchamp paved the way for the conceptualists, providing them with examples of prototypically conceptual works — the readymades, for instance. The most famous of Duchamp's readymades was \"Fountain\" (1917), a standard urinal-basin signed by the artist with the pseudonym \"R.Mutt\", and submitted for inclusion in the annual, un-juried exhibition of the Society of Independent Artists in New York (which rejected it). The artistic tradition does not see a commonplace object (such as a urinal) as art because it is not made by an artist or with any intention of being art, nor is it unique or hand-crafted. Duchamp's relevance and theoretical importance for future \"conceptualists\" was later acknowledged by US artist Joseph Kosuth in his 1969 essay, \"Art after Philosophy\", when he wrote: \"All art (after Duchamp) is conceptual (in nature) because art only exists conceptually\".\n\nIn 1956 the founder of Lettrism, Isidore Isou, developed the notion of a work of art which, by its very nature, could never be created in reality, but which could nevertheless provide aesthetic rewards by being contemplated intellectually. This concept, also called \"Art esthapériste\" (or \"infinite-aesthetics\"), derived from the infinitesimals of Gottfried Wilhelm Leibniz – quantities which could not actually exist except conceptually. The current incarnation () of the Isouian movement, Excoördism, self-defines as the art of the infinitely large and the infinitely small.\n\nIn 1961 the term \"concept art\", coined by the artist Henry Flynt in his article bearing the term as its title, appeared in a proto-Fluxus publication \"An Anthology of Chance Operations\". \nHowever, it assumed a different meaning when employed by Joseph Kosuth and by the English Art and Language group, who discarded the conventional art object in favour of a documented critical inquiry, that began in Art-Language The Journal of conceptual art in 1969, into the artist's social , philosophical and psychological status. By the mid-1970s they had produced publications, indices, performances, texts and paintings to this end. In 1970 \"Conceptual Art and Conceptual Aspects\", the first dedicated conceptual-art exhibition, took place at the New York Cultural Center.\n\nConceptual art emerged as a movement during the 1960s – in part as a reaction against formalism as then articulated by the influential New York art critic Clement Greenberg. According to Greenberg Modern art followed a process of progressive reduction and refinement toward the goal of defining the essential, formal nature of each medium. Those elements that ran counter to this nature were to be reduced. The task of painting, for example, was to define precisely what kind of object a painting truly is: what makes it a painting and nothing else. As it is of the nature of paintings to be flat objects with canvas surfaces onto which colored pigment is applied, such things as figuration, 3-D perspective illusion and references to external subject matter were all found to be extraneous to the essence of painting, and ought to be removed.\n\nSome have argued that conceptual art continued this \"dematerialization\" of art by removing the need for objects altogether,\nwhile others, including many of the artists themselves, saw conceptual art as a radical break with Greenberg's kind of formalist Modernism. Later artists continued to share a preference for art to be self-critical, as well as a distaste for illusion. However, by the end of the 1960s it was certainly clear that Greenberg's stipulations for art to continue within the confines of each medium and to exclude external subject matter no longer held traction.\nConceptual art also reacted against the commodification of art; it attempted a subversion of the gallery or museum as the location and determiner of art, and the art market as the owner and distributor of art. Lawrence Weiner said: \"Once you know about a work of mine you own it. There's no way I can climb inside somebody's head and remove it.\" Many conceptual artists' work can therefore only be known about through documentation which is manifested by it, e.g. photographs, written texts or displayed objects, which some might argue are not in themselves the art. It is sometimes (as in the work of Robert Barry, Yoko Ono, and Weiner himself) reduced to a set of written instructions describing a work, but stopping short of actually making it—emphasising the idea as more important than the artifact. This reveals an explicit preference for the \"art\" side of the ostensible dichotomy between art and craft, where art, unlike craft, takes place within and engages historical discourse: for example, Ono's \"written instructions\" make more sense alongside other conceptual art of the time.\n\nLanguage was a central concern for the first wave of conceptual artists of the 1960s and early 1970s. Although the utilisation of text in art was in no way novel, only in the 1960s did the artists Lawrence Weiner, Edward Ruscha, Joseph Kosuth, Robert Barry, and Art & Language begin to produce art by exclusively linguistic means. Where previously language was presented as one kind of visual element alongside others, and subordinate to an overarching composition (e.g. Synthetic Cubism), the conceptual artists used language in place of brush and canvas, and allowed it to signify in its own right. Of Lawrence Weiner's works Anne Rorimer writes, \"The thematic content of individual works derives solely from the import of the language employed, while presentational means and contextual placement play crucial, yet separate, roles.\"\n\nThe British philosopher and theorist of conceptual art Peter Osborne suggests that among the many factors that influenced the gravitation toward language-based art, a central role for conceptualism came from the turn to linguistic theories of meaning in both Anglo-American analytic philosophy, and structuralist and post structuralist Continental philosophy during the middle of the twentieth century. This linguistic turn \"reinforced and legitimized\" the direction the conceptual artists took. Osborne also notes that the early conceptualists were the first generation of artists to complete degree-based university training in art. Osborne later made the observation that contemporary art is \"post-conceptual\" in a public lecture delivered at the Fondazione Antonio Ratti, Villa Sucota in Como on July 9, 2010. It is a claim made at the level of the ontology of the work of art (rather than say at the descriptive level of style or movement).\n\nThe American art historian Edward A. Shanken points to the example of Roy Ascott who \"powerfully demonstrates the significant intersections between conceptual art and art-and-technology, exploding the conventional autonomy of these art-historical categories.\" Ascott, the British artist most closely associated with cybernetic art in England, was not included in Cybernetic Serendipity because his use of cybernetics was primarily conceptual and did not explicitly utilize technology. Conversely, although his essay on the application of cybernetics to art and art pedagogy, \"The Construction of Change\" (1964), was quoted on the dedication page (to Sol Lewitt) of Lucy R. Lippard's seminal \"Six Years: The Dematerialization of the Art Object from 1966 to 1972\", Ascott's anticipation of and contribution to the formation of conceptual art in Britain has received scant recognition, perhaps (and ironically) because his work was too closely allied with art-and-technology. Another vital intersection was explored in Ascott's use of the thesaurus in 1963 , which drew an explicit parallel between the taxonomic qualities of verbal and visual languages – a concept would be taken up in Joseph Kosuth's \"Second Investigation, Proposition 1\" (1968) and Mel Ramsden's \"Elements of an Incomplete Map\" (1968).\n\n\"By adopting language as their exclusive medium, Weiner, Barry, Wilson, Kosuth and Art & Language were able to sweep aside the vestiges of authorial presence manifested by formal invention and the handling of materials.\"\nAn important difference between conceptual art and more \"traditional\" forms of art-making goes to the question of artistic skill. Although skill in the handling of traditional media often plays little role in conceptual art, it is difficult to argue that no skill is required to make conceptual works, or that skill is always absent from them. John Baldessari, for instance, has presented realist pictures that he commissioned professional sign-writers to paint; and many conceptual performance artists (e.g. Stelarc, Marina Abramović) are technically accomplished performers and skilled manipulators of their own bodies. It is thus not so much an absence of skill or hostility toward tradition that defines conceptual art as an evident disregard for conventional, modern notions of authorial presence and of individual artistic expression.\n\nThe first wave of the \"conceptual art\" movement extended from approximately 1967 to 1978. Early \"concept\" artists like Henry Flynt, Robert Morris, and Ray Johnson influenced the later, widely accepted movement of conceptual art. Conceptual artists like Dan Graham, Hans Haacke, and Lawrence Weiner have proven very influential on subsequent artists, and well known contemporary artists such as Mike Kelley or Tracey Emin are sometimes labeled \"second- or third-generation\" conceptualists, or \"post-conceptual\" artists.\n\nMany of the concerns of the conceptual art movement have been taken up by contemporary artists. While they may or may not term themselves \"conceptual artists\", ideas such as anti-commodification, social and/or political critique, and ideas/information as medium continue to be aspects of contemporary art, especially among artists working with installation art, performance art, net.art and electronic/digital art.\n\n\n\nBooks\n\n\nEssays\n\n\nExhibition catalogues\n\n\n"}
{"id": "42415226", "url": "https://en.wikipedia.org/wiki?curid=42415226", "title": "Conceptual combination", "text": "Conceptual combination\n\nConceptual combination is a fundamental cognitive process by which two or more existing basic concepts are mentally synthesized to generate a composite, higher-order concept. The products of this process are sometimes referred to as \"complex concepts.\" Combining concepts allows individuals to use a finite number of concepts which they already understand to construct a potentially limitless quantity of new, related concepts. It is an essential component of many abilities, such as perception, language, synthetic reasoning, creative thought and abstraction.\n\nConceptual combination is an important concept in the fields of cognitive psychology and cognitive science.\n\nThe mechanism by which conceptual combination occurs is debatable, both on cognitive and neurological levels. As such, multiple models have been developed or applied to better define how the process occurs.\n\nCognitive models attempt to functionally outline the mental computation involved in conceptual combination. \n\nConstraint theory stipulates that the concept that results from an attempt at conceptual combination is controlled by three constraints known as diagnosticity, plausibility and informativeness. \"Diagnosticity\" refers to the a complex concept's possession of the defining properties of its component simple concepts. Because such properties are diagnostic of the component concepts, at least some of them should be diagnostic of the higher-order representations constructed from those concepts. \"Plausibility\" refers to consistency with existing knowledge and prior experience. It is based on the assumption that a complex concept should be reasonably relevant to the context in which it is used. This assumption makes the most sense in a practical, linguistic context, particularly when a speaker is catering to the understanding of the listener. \"Informativeness\" is the property of having more meaning or properties than any individual component. If a complex concept were not distinguishable from any given component, it would be identical to that component. Because nothing can logically be both a component of something and the totality of something simultaneously, a complex concept must at least be the sum of its parts. Many argue that the interaction among component concepts should allow a complex concept to be greater than that sum. If multiple possible ways to structure or interpret a complex concept exist, the one which best satisfies or most satisfies these constraints is the one which will be used. The paradigm upon which constraint theory is based is computational, and therefore views the mind as a processor which operates on the basis of standard problem-solving protocols (i.e. algorithms and heuristics).\n\nThe spreading activation model is a model in connectionist theory sometimes designed to represent how concepts are activated in relation to one another. Though it is typically applied to information search processes like recognition, brainstorming, and recall, it can be used to explain how concepts are combined as well as connected.\n\nSpreading activation models represent memory and knowledge as a network of interlinked concepts. Every concept manifests as a node within this network, with related nodes/concepts linked to one another with connections. Concepts that are more strongly associated with one another either in terms of content or an individual's past experience are correspondingly more strongly linked.\n\nWhen one concept is employed in working memory, the corresponding node is also activated. This activation spreads through the node's links, making it easier to activate nodes to which the activated node is connected. This spreading activation stimulates the linked nodes, pressuring them to activate to an extent proportional to the strength of the connection between the stimulating node and the stimulated node. If sufficient net stimulation is accrued through a stimulated node's links, it will also activate. Thus, being connected to an activated node makes it easier for an inactive node to become active as well; concepts become more readily accessed when individuals are stimulated with related concepts first. This increase in ease of access is known as \"priming.\"\n\nSpreading activation models tend to imply that processing concepts occurs in series; that is, each concept is processed one-at-a-time, one after the other. As such, individuals tend to combine concepts more readily, easily, and quickly if they are more closely linked within the network of concepts. This implication, however, has caused spreading activation to come under a great deal of criticism, particularly with respect to how the concept is employed in feature theories.\n\nThe features and properties of complex concepts are generally assumed to be derived from the simple concepts that compose them. One popularly discussed model involves a two-stage serial process. In the initial stage, features from each of the component simple concepts are retrieved from memory through spreading activation. This allows a complex concept to accrue features with existing relationships with its component simple concepts. During this stage, the basic definition of what the complex concept is and/or means is generates. In the second stage, knowledge and reasoning are employed upon the features accrued in the previous stage to generate further features. For example, one might reason that the complex concept \"white jacket,\" if worn in a blizzard, would make one difficult to see; it would follow that one should ascribe the property of \"good for winter camouflage,\" despite the fact that this property is not closely attached to the component concepts \"white\" nor \"jacket.\" This stage is especially useful when properties of complex concepts contradict those of their component concepts, such as the different colours of milk and chocolate milk.\n\nThis model, however, has come under criticism due to its inconsistency with empirical studies. If conceptual combination employed serial spreading activation, for instance, it should take longer to verify the properties of complex concepts, as they necessarily possess more concepts than their component simple concepts. Research has nonetheless shown that it takes less time to confirm complex concepts' properties than their components and about equal time to reject false properties for both. This occurred even when the properties of the complex concept contradicted those of the simple concepts. Likewise, when experiment participants are exposed to a set of features first, and then asked to verify whether or not they correspond to simple or complex concepts, the participants tend to provide correct verification answers for the complex concepts more quickly.\n\nThe neurological basis of conceptual combination has received considerably less attention than its cognitive basis. Nevertheless, research has revealed several specific brain regions that are intimately involved if not entirely responsible for neural processing involved in conceptual combination.\n\nOf particular relevance is the left anterior temporal lobe. Studies have previously demonstrated an additive effect for stimulation in this subsection of neural cortex tissue. When experiment participants were verbally presented with certain simple concepts, the processing of the information causes electrical stimulation in the region. When the same participants were verbally presented with a single complex concept formed from the combination of the aforementioned simple concepts, the stimulation recorded was equivalent to the sum of the stimulation that resulted from each individual component simple concept. In other words, the stimulation caused by a complex concept is equivalent to the total stimulation caused by its component concepts. More recent data contradicts those results by indicating a multiplicative effect in which the activation caused by a complex concept is the product of the activation levels caused by its component concepts, rather than the sum.\n\nFurther support for the role of the left anterior temporal lobe has been previously established through neuropsychological studies. Semantic dementia is a disorder in which conceptual manipulation, including conceptual combination, is hindered. These indicate that the neural damage associated with semantic dementia occurs within this brain region. Unfortunately, neuropsychological studies that attempt to replicate this pattern have failed, leading uncertainty as to whether initial results were valid.\n\nAs language is the means by which concepts are communicated and expressed, the processed involved in linguistic expression and interpretation are heavily intertwined with combined concepts. Many theories of concept combination mechanisms, including constraint theory were developed within the context of language, and therefore make more sense when applied in a linguistic context. Study into the linguistic aspects of concept combination as has generally been focused on the interpretation mechanism.\n\nA concept that can be expressed using a single word is called a \"lexical concept.\" A lexical concept is usually treated as a basic concept, although it can just as easily be a complex concept.\n\nTwo lexical concepts are often used together as phrases to represent a combined concept of greater specificity. This is most readily seen in the use of adjectives to modify nouns and the use of adverbs to modify verbs and adjectives. Consider, for example, phrases such as \"burnt toast,\" \"eat roughly,\" and \"readily loved.\" Multiple noun lexical concepts can also be used together in order to represent combined concepts. Through this process, a limited pool of nouns can be used to produce an exponentially larger pool of phrases such as \"sound wave,\" \"video game,\" and \"sleeping pill.\"\n\nIn addition to constraint theory, there are two principal theories surrounding the mechanism by which noun-noun combinations are interpreted. The first of these is \"dual-process theory.\" Dual-process theory proposed that there are two means by which people interpreted noun-noun phrases. \"Relational interpretation\" attempts to establish a relationship between the nouns and interprets the combined phrase in terms of that relationship. For example, one might relationally interpret the phrase \"snake mouse\" to refer to a mouse meant to be eaten by snakes, as the two concepts have a predatory relationship. \"Property interpretation\" identifies properties associated with the first noun and then applies them onto the second noun. In this case the phrase \"snake mouse\" might be interpreted as a mouse with poisonous fangs or an elongated body.\n\nThe second principal theory is known as the \"Competition in Relations among Nominals\" theory. It states that the assumed modification effect of a noun on its partner in a novel noun-noun combination is the one which it has been seen to employ most often in the past. For example, \"chocolate cat\" is usually interpreted as \"a cat made of chocolate\" rather than \"a chocolate-eating cat\" simply because the \"made of\" modifier is heavily conditioned to be associated with \"chocolate.\"\n\nExplanations of linguistic expression of complex concepts have been linked to spreading activation models. When an individual identifies a lexical concept through vision or hearing, the corresponding node in that individual's cognitive network is said to activate. This makes it easier for lexical concepts linked to the activated concept to be comprehended, as they are primed. This is consistent with current empirical data, which shows that when individuals are interpreting sentences, they process the linguistic content more quickly when several related words follow one another. In turn, it becomes easier for people to combine these related concepts together and understand them as a relationship, rather than two distinct entities. For example, consider the example, \"John spread butter on a bagel.\" In this sentence, the lexical concepts \"spread,\" \"butter,\" and \"bagel\" are associated with one another and easy to combine into a mental representation of a breakfast scenario. Conversely, consider the example, \"John baked a computer.\" Because \"baked\" and \"computer\" are not related lexical concepts, it takes more effort and time to build a mental representation of this unusual scenario.\n\nHowever, spreading activation models of conceptual combination have been criticized in light of how humans have been observed to combined languages. Those who claim that the theory provides an insufficient account of linguistic conceptual combination refer to the ability of humans to readily understand lexical concept combinations with seemingly no apparent connection with one another. One example of this would be the sentence \"John saw an elephant cloud.\" \"Elephant\" and \"cloud\" do not shared a close association, but it takes little effort to comprehend that the term \"elephant cloud\" refers to a cloud shaped like an elephant. This has led some to conclude that the combination of lexical concepts does not wholly rely on the simultaneous activation of linked lexical concepts alone. Rather, they claim that the process involves the use of existing nodes to generate entirely new concepts independent of their parent concepts.\n\nAlthough many theories of novel noun-noun combination interpretation ignore the effect of social environment, some theorists have attempted to account for any contingencies social context may cause.\n\nWhen lexical concept combinations are interpreted without the influence of social context, the interpretation carried out is termed \"sense generation.\" This includes all processes that would normally occur excepting those dependent on a conversation partner. The \"generation hypothesis\" accordingly states that the interpretation mechanism of a noun-noun combination is essentially the same regardless of context. This does not rule out the possibility that social context can affect sense generation in some way, but it does assert that the basic structure of the process is unaffected. As seen above, debate as to what sense generation entails and how many sub-processes into which it should be divided is a contentious matter in cognitive science.\n\nThe \"anaphor resolution hypothesis\" instead asserts that before sense generation occurs, interpreters first search their memory of recent communication to see if the combination refers to something previously discussed. This process is termed \"anaphor resolution'.' If a referent is identified, interpretation occurs without sense generation in light of that referent. Even if an explicit referent does not exist, anaphor resolution can help facilitate sense generation by providing more information that might hint at the combination's intended meaning.\n\nThe \"dual-process hypothesis\" not to be confused with dual-process theory, states that sense generation and anaphor resolution occur in parallel. Both processes begin to work once the noun-noun combination is presented. Proponents of this hypothesis disagree as to how the interpretation is eventually resolves. Some believe that whichever process reaches a conclusion first provides the answer. Others believe that both provide continuous input to a third, mediating process that eventually makes a decision based on input from both.\n\nCreativity necessitates the employment of existing concepts in novel ways, and therefore requires conceptual combination. Surprisingly, this contribution seems to be limited. Conceptual combination is a significant contributor to convergent thinking, but not divergent thinking. For example, practice with generating new concepts through combination does not improve brainstorming. It does, however, assist in devising creative problem solving methods.\n\nThe psychological community's growing understanding of how concepts are manipulated has allowed educators to teach new concepts more effectively. Tools that are developed based on conceptual combination theory attempt to teach individual tasks, and then challenge students to exercise them together in order to promote both base subject skills and the critical thinking needed to apply them simultaneously to solve new problems. Máder & Vajda, for instance, developed a three-dimensional grid with cells of adjustable height which has been successfully used in numerous activities capable of improving the effectiveness of high school mathematics education.\n"}
{"id": "21899301", "url": "https://en.wikipedia.org/wiki?curid=21899301", "title": "Disquotational principle", "text": "Disquotational principle\n\nThe disquotational principle is a philosophical theorem which holds that a rational speaker will accept \"p\" if and only if he or she believes \"p\". The quotes indicate that the statement \"p\" is being treated as a sentence, and not as a proposition. This principle is presupposed by claims that hold that substitution fails in certain intensional contexts.\n\nConsider the following argument:\n\nTo derive (3), we have to assume that when Sally accepts that \"Cicero was a famous orator\", she believes that Cicero was a famous orator. Then we can exchange Cicero for Tully, and derive (3). Bertrand Russell thought that this demonstrated the failure of substitutivity of identicals in intensional contexts.\n\nIn \"A Puzzle about Belief,\" Saul Kripke argues that the application of the disquotational theorem can yield a paradox on its own, without appeal to the substitution principle, and that this may show that the problem lies with the former, and not the latter. There are various formulations of this argument.\n\nSuppose that, Pierre, a Frenchman, comes to believe that (1) \"Londres est jolie\" (London is pretty), without ever having visited the city. Later in life, Pierre ends up living in London. He finds no French speakers there (he does not speak English yet), and everyone refers to the city as \"London,\" not \"Londres\". He finds this city decidedly unattractive, for the neighborhood he decides to live in is decidedly unattractive. Over time, he learns English, and formulates the belief that (2) \"London is not pretty\". Pierre never realizes that London is the English word for \"Londres\". Now with the disquotational principle, we can deduce from (1) that Pierre believes the proposition that \"Londres est jolie\". With a weak principle of translation (e.g., \"a proposition in language A is the same as a semantically identical proposition in language B\" [note that a proposition is not the same as a sentence]), we can now deduce that Pierre believes that London is pretty. But we can also deduce from (2) and the disquotational principle that Pierre believes that London is not pretty. These deductions can be made \"even though Pierre has made no logical blunders in forming his beliefs\". Without the disquotational principle, this contradiction could not be derived, because we would not be able to assume that (1) and (2) meant anything in particular.\n\nThis paradox can also be derived without appeal to another language. Suppose that Pierre assents to the proposition that \"Paderewski had musical talent\", perhaps having heard that this man was a famous pianist. With the disquotational principle, we can deduce that Pierre believes the proposition that Paderewski had musical talent. Now suppose that Pierre overhears a friend discussing the political exploits of a certain statesman, Paderewski, without knowing that the two Paderewskis are the same man. Pierre's background tells him that statesmen are generally not very gifted in music, and this leads him to the belief that Paderewski had no musical talent. The disquotation principle allows us to deduce that Pierre believes the proposition that Paderewski had no musical talent. Using this principle, we have now deduced that Pierre believes that Paderewski had musical talent, and does not believe that Paderewski had musical talent, \"even though Pierre's beliefs were formed logically\".\n\n"}
{"id": "194143", "url": "https://en.wikipedia.org/wiki?curid=194143", "title": "Double negative", "text": "Double negative\n\nA double negative is a grammatical construction occurring when two forms of negation are used in the same sentence. Multiple negation is the more general term referring to the occurrence of more than one negative in a clause. In some languages, double negatives cancel one another and produce an affirmative; in other languages, doubled negatives intensify the negation. Languages where multiple negatives affirm each other are said to have negative concord or emphatic negation. Portuguese, Persian, Russian, Spanish, Neapolitan, Italian, Japanese, Bulgarian, Czech, Polish, Afrikaans, Hebrew, and some dialects of English, such as African-American Vernacular English, are examples of negative-concord languages, while Latin and German do not have negative concord. It is cross-linguistically observed that negative-concord languages are more common than those without.\n\nLanguages without negative concord typically have negative polarity items that are used in place of additional negatives when another negating word already occurs. Examples are \"ever\", \"anything\" and \"anyone\" in the sentence \"I haven't ever owed anything to anyone\" (cf. \"I have\"n't\" \"never\" owed \"nothing\" to \"no one\"\" in negative-concord dialects of English, and \"\"Nunca\" devi \"nada\" a \"ninguém\"\" in Portuguese, lit. \"Never have I owed nothing to no one\", or \"\"Non\" ho \"mai\" dovuto \"nulla\" a \"nessuno\"\" in Italian). Note that negative polarity can be triggered not only by direct negatives such as \"not\" or \"never\", but also by words such as \"doubt\" or \"hardly\" (\"I doubt he has ever owed anything to anyone\" or \"He has hardly ever owed anything to anyone\").\n\nStylistically, in English, double negatives can sometimes be used for affirmation (e.g. \"I'm not feeling not good\"), an understatement of the positive (\"I'm feeling good\"). The rhetorical term for this is litotes.\n\nWhen two negatives are used in one independent clause, in standard English the negatives are understood to cancel one another and produce a weakened affirmative: this is known as litotes. However, depending on how such a sentence is constructed, in some dialects if a verb or adverb is in between two negatives then the latter negative is assumed to be intensifying the former thus adding weight or feeling to the negative clause of the sentence. For this reason, it is difficult to portray double negatives in writing as the level of intonation to add weight in one's speech is lost. A double negative intensifier does not necessarily require the prescribed steps, and can easily be ascertained by the mood or intonation of the speaker.\n\nvs.\n\nThese two sentences would be different in how they are communicated by speech. Any assumption would be correct, and the first sentence can be just as right or wrong in intensifying a negative as it is in cancelling it out; thereby rendering the sentence's meaning ambiguous. Since there is no adverb or verb to support the latter negative, the usage here is ambiguous and lies totally on the context behind the sentence. In light of punctuation, the second sentence can be viewed as the intensifier; and the former being a statement thus an admonishment.\n\nIn Standard English, two negatives are understood to resolve to a positive. This rule was observed as early as 1762, when Bishop Robert Lowth wrote \"A Short Introduction to English Grammar with Critical Notes\". For instance, \"I do not disagree\" could mean \"I certainly agree\", \"I agree\", \"I sort of agree\", \"I don't understand your point of view\", \"I have no opinion\", and so on; it is a form of \"weasel words\". Further statements are necessary to resolve which particular meaning was intended.\n\nThis is opposed to the single negative \"I do not agree\", which typically means \"I disagree\". However, the statement \"I do not completely disagree\" is a similar double negative to \"I do not disagree\" but needs little or no clarification.\n\nWith the meaning \"I completely agree\", Lowth would have been referring to litotes wherein two negatives simply cancel each other out. However, the usage of intensifying negatives and examples are presented in his work, which could also imply he wanted either usage of double negatives abolished. Because of this ambiguity, double negatives are frequently employed when making back-handed compliments. The phrase \"Mr. Jones was not incompetent.\" will seldom mean \"Mr. Jones was very competent\" since the speaker would have found a more flattering way to say so. Instead, some kind of problem is implied, though Mr. Jones possesses basic competence at his tasks.\n\nDiscussing English grammar, the term \"double negative\" is often though not universally applied to the non-standard use of a second negative as an intensifier to a negation.\n\nDouble negatives are usually associated with regional and ethnical dialects such as Southern American English, African American Vernacular English, and various British regional dialects. Indeed, they were used in Middle English. Historically, Chaucer made extensive use of double, triple, and even quadruple negatives in his \"Canterbury Tales\". About the Friar, he writes \"Ther nas no man no wher so vertuous\" (\"There never was no man nowhere so virtuous\"). About the Knight, \"He nevere yet no vileynye ne sayde / In all his lyf unto no maner wight\" (\"He never yet no vileness didn't say / In all his life to no manner of man\").\n\nFollowing the battle of Marston Moor, Oliver Cromwell quoted his nephew's dying words in a letter to the boy's father Valentine Walton: \"A little after, he said one thing lay upon his spirit. I asked him what it was. He told me it was that God had not suffered him to be no more the executioner of His enemies.\" Although this particular letter has often been reprinted, it is frequently changed to read \"not ... to be any more\" instead.\n\nWhereas some double negatives may resolve to a positive, in some dialects others resolve to intensify the negative clause within a sentence. For example:\n\nIn contrast, some double negatives become positives:\n\nThe key to understanding the former examples and knowing whether a double negative is intensive or negative is finding a verb between the two negatives. If a verb is present between the two, the latter negative becomes an intensifier which does not negate the former. In the first example, the verb \"to go\" separates the two negatives; therefore the latter negative does not negate the already negated verb. Indeed, the word 'nowhere' is thus being used as an adverb and does not negate the argument of the sentence. One interesting thing to note is that double negatives such as \"I don't want to know no more\" contrasts with Romance languages such as French in \"Je ne veux pas savoir.\" \n\nAn exception is when the second negative is stressed, as in \"I'm not doing ; I'm thinking.\" A sentence can otherwise usually only become positive through consecutive uses of negatives, such as those prescribed in the later examples, where a clause is void of a verb and lacks an adverb to intensify it. Two of them also use emphasis to make the meaning clearer. The last example is a popular example of a double negative that resolves to a positive. This is because the verb 'to doubt' has no intensifier which effectively resolves a sentence to a positive. Had we added an adverb thus:\n\nThen what happens is that the verb \"to doubt\" becomes intensified, which indeed deduces that the sentence is indeed false since nothing was resolved to a positive. The same applies to the third example, where the adverb 'more' merges with the prefix \"no-\" to become a negative word, which when combined with the sentence's former negative only acts as an intensifier to the verb \"hungry\". Where people think that the sentence \"I'm not hungry no more\" resolves to a positive is where the latter negative \"no\" becomes an adjective which only describes its suffix counterpart \"more\" which effectively becomes a noun, instead of an adverb. This is a valid argument since adjectives do indeed describe the nature of a noun; yet some fail to take into account that the phrase \"no more\" is only an adverb and simply serves as an intensifier. Another argument used to support the position double negatives aren't acceptable is a mathematical analogy: negating a negative number results in a positive one; e.g., ; therefore, it is argued, \"I did not go nowhere\" resolves to \"I went somewhere\".\n\nOther forms of double negatives, which are popular to this day and do strictly enhance the negative rather than destroying it, are described thus:\n\nPhilosophies aside, this form of double negative is still in use whereby the use of 'nor' enhances the negative clause by emphasizing what isn't to be. Opponents of double negatives would have preferred \"I'm not entirely familiar with Nihilism or Existentialism\"; however this renders the sentence somewhat empty of the negative clause being advanced in the sentence. This form of double negative along with others described are standard ways of intensifying as well as enhancing a negative. The use of 'nor' to emphasise the negative clause is still popular today, and has been popular in the past through works of Shakespeare and Milton:\n\nTo the common reader the negatives herein do not cancel each other out but simply emphasizes the negative clause.\nUp to the 18th century, double negatives were used to emphasize negation. \"Prescriptive grammarians\" recorded and codified a shift away from the double negative in the 1700s. Double negatives continue to be spoken by those of Vernacular English, such as those of Appalachian English and African American Vernacular English. To such speakers, they view double negatives as emphasizing the negative rather than cancelling out the negatives. Researchers have studied African American Vernacular English (AAVE) and trace its origins back to colonial English. This shows that double negatives were present in colonial English, and thus presumably English as a whole, and were acceptable at that time. English after the 18th century was changed to become more logical and double negatives became seen as canceling each other as in mathematics. The use of double negatives became associated with being uneducated and illogical.\n\nIn his \"Essay towards a practical English Grammar\" of 1711, James Greenwood first recorded the rule: \"Two Negatives, or two Adverbs of Denying do in English affirm\". Robert Lowth stated in his grammar textbook \"A Short Introduction to English Grammar\" (1762) that \"two negatives in English destroy one another, or are equivalent to an affirmative\". Grammarians have assumed that Latin was the model for Lowth and other early grammarians in prescribing against negative concord, as Latin does not feature it. Data indicates, however, that negative concord had already fallen into disuse in Standard English by the time of Lowth's grammar, and no evidence exists that the loss was driven by prescriptivism, which was well established by the time it appeared.\n\nDouble negatives have been employed in various films and television shows. In the film \"Mary Poppins\", the chimney sweep Bert employs a double negative when he says, \"If you don't want to go nowhere...\" Another is used by the bandits in the \"Stinking Badges\" scene of John Huston's \"The Treasure of the Sierra Madre\": \"Badges? We ain't got no badges. We don't need no badges!\".\n\nMore recently, the British television show \"EastEnders\" has received some publicity over the Estuary accent of character Dot Branning, who speaks with double and triple negatives (\"I ain't never heard of no licence.\").. In the Harry Enfield sketch \"Mr Cholmondley-Warner's Guide to the Working-Class\", a stereotypical Cockney employs a septuple-negative: \"Inside toilet? I ain't never not heard of one of them nor I ain't nor nothing.\"\n\nIn music, double negatives can be employed to similar effect (as in Pink Floyd's \"Another Brick in the Wall\", in which schoolchildren chant \"We don't need no education / We don't need no thought control\") or used to establish a frank and informal tone (as in The Rolling Stones' \"(I Can't Get No) Satisfaction\").\n\nDouble negation is uncommon in other West Germanic languages. A notable exception is Afrikaans, where it is mandatory (for example, \"He cannot speak Afrikaans\" becomes \"Hy kan nie Afrikaans praat nie\", \"He cannot Afrikaans speak not\"). Dialectal Dutch, French and San have been suggested as possible origins for this trait. Its proper use follows a set of fairly complex rules as in these examples provided by Bruce Donaldson:\n\nAnother point of view is that this construction is not really an example of a \"double negative\" but simply a grammatical template for negation. The second \"nie\" cannot be understood as a noun or adverb (as can, e.g., \"pas\" in French), and cannot be substituted by any part of speech other than itself with the sentence remaining grammatical. It is a grammatical particle with no independent meaning that happens to be spelled and pronounced the same as the embedded \"nie\", meaning \"not\", through historical accident.\n\nThe second \"nie\" is used if and only if the sentence or phrase doesn't already end with \"nie\" or another negating adverb.\n\nAfrikaans shares with English the property that two negatives make a positive. For example,\n\nWhile double negation is still found in the Low Franconian dialects of west Flanders (e.g., \"Ik ne willen da nie doen\", \"I do not want to do that\") and in some villages in the central Netherlands such as Garderen, it takes a different form than that found in Afrikaans. In Belgian Dutch dialects, however, there are still some widely used expressions like \"nooit niet\" (\"never not\") for \"never\".\n\nSimilar to some dialectal English, Bavarian employs both single and double negation, with the latter denoting special emphasis. For example, compare the Bavarian \"Des hob i no nia ned g'hört\" (\"This have I yet never not heard\") with the standard German \"Das habe ich noch nie gehört\". The German emphatic \"niemals!\" (roughly \"never ever\") corresponds to Bavarian \"(går) nia ned\" or even \"nie nicht\" in Standard German pronunciation.\n\nAnother exception is Yiddish. Due to Slavic influence, the double (and sometimes even triple) negative is quite common.\n\nA few examples would be:\n\nWhile in Latin a second negative word appearing along with \"non\" turns the meaning into a positive one: \"ullus\" means \"any\", \"nullus\" means \"no\", \"non...nullus\" (\"nonnullus\") means \"some\". In the same way, \"umquam\" means \"ever\", \"numquam\" means \"never\", \"non...numquam\" (\"nonnumquam\") means \"sometimes\", in many Romance languages a second term indicated a negative is required.\n\nIn French, the usual way to express negation is to employ two negatives, e.g. \"ne [verb] pas\", \"ne [verb] plus\", or \"ne [verb] jamais\", as in the sentences \"Je ne sais pas\" (\"I do not know\"), \"Il n'y a plus de baguettes\" (\"There aren't any more baguettes\"), and \"On ne sait jamais\" (\"one never knows\"). The second term was originally an emphatic; \"pas\", for example, derives from the Latin \"passus\", meaning \"step\", so that French \"Je ne marche pas\" and Catalan \"No camino pas\" originally meant \"I will not walk a single step.\" This initial usage spread so thoroughly that it became a necessary element of any negation in the modern French language and that, in fact, in contemporary French, the original actual negative \"ne\" is mostly left away in favour of \"pas\", as in \"Je sais pas\" \"I don't know\". In Northern Catalan, \"no\" may be omitted in colloquial language, and Occitan, which uses \"non\" only as a short answer to questions. In Venetian, the double negation \"no ... mìa\" can likewise lose the first particle and rely only on the second: \"magno mìa\" (\"I eat not\") and \"vegno mìa\" (\"I come not\"). These exemplify Jespersen's cycle.\n\nItalian, Portuguese and Romanian languages usually employ doubled negative correlatives. Portuguese \"Não vejo nada\", Romanian \"Nu văd nimic\" and Italian \"Non vedo niente\" (\"I do not see nothing\") are used to express \"No, I do not see anything\". In Italian, a second following negative particle \"non\" turns the phrase into a positive one, but with a slightly different meaning. For instance, while both \"Voglio mangiare\" (\"I want to eat\") and \"Non voglio non mangiare\" (\"I don't want not to eat\") mean \"I want to eat\", the latter phrase more precisely means \"I'd prefer to eat\".\n\nOther Romance languages employ double negatives less regularly. In Asturian, an extra negative particle is used with negative adverbs: \"Yo nunca nun lu viera\" (\"I had not never seen him\") means \"I have never seen him\" and \"A mi tampoco nun me presta\" (\"I neither do not like it\") means \"I do not like it either\". Standard Catalan and Galician also used to possess a tendency to double \"no\" with other negatives, so \"Jo tampoc no l'he vista\" or \"Eu tampouco non a vira\", respectively (\"I neither have not seen her\") meant \"I have not seen her either\". That practice is dying out.\n\nIn 1974, Italy held a referendum on whether to repeal a recent law that allowed divorce. Voters were said to have been confused in that in order to support divorce, they needed to vote 'no' on the referendum which was worded so that 'yes' would support repeal. And to reframe the fundamental underlying issue as being support/non-support of the continuation of marriage, then the vote was structured as a triple negative (with divorce as the negation of the continuation of marriage being the first negative). This referendum was defeated, and without this confusion, it was said that it would have been defeated more decisively.\n\nIn spoken Welsh, the word ddim (not) often occurs with a prefixed or mutated verb form that is negative in meaning: \"Dydy hi ddim yma\" (word-for-word, \"Not-is she not here\") expresses \"She is not here\" and \"Chaiff Aled ddim mynd\" (word-for-word, \"Not-will-get Aled not go\") expresses \"Aled is not allowed to go\".\n\nNegative correlatives can also occur with already negative verb forms. In literary Welsh, the mutated verb form is caused by an initial negative particle, ni or nid. The particle is usually omitted in speech but the mutation remains: \"[Ni] wyddai neb\" (word-for-word, \"[Not] not-knew nobody\") means \"Nobody knew\" and \"[Ni] chaiff Aled fawr o bres\" (word-for-word, \"[Not] not-will-get Aled lots of money\") means \"Aled will not get much money\". This is not usually regarded as three negative markers, however, because the negative mutation is really just an effect of the initial particle on the following word.\n\nDoubled negatives are perfectly correct in Ancient Greek. With few exceptions, a simple negative (οὐ or μή) following another negative (for example, οὐδείς, \"no one\") results in an affirmation: οὐδείς οὐκ ἔπασχε τι (\"No one was not suffering\") means more simply \"Everyone was suffering\". Meanwhile, a compound negative following a negative strengthens the negation: μὴ θορυβήσῃ μηδείς (\"Do not permit no one to raise an uproar\") means \"Let not a single one among them raise an uproar\".\n\nThose constructions apply only when the negatives all refer to the same word or expression. Otherwise, the negatives simply work independently of one another: οὐ διὰ τὸ μὴ ἀκοντίζειν οὐκ ἔβαλον αὐτόν means \"It was not on account of their not throwing that they did not hit him\", and one should not blame them for not trying.\n\nIn Modern Greek, negative concord is standard and more commonly used. For example, the sentence 'You (pl.) will not find anything' can be said in two ways: 'Δε θα βρείτε τίποτα' ('Not will find nothing') is more common than 'Δε θα βρείτε κάτι' ('Not will find something'). It depends simply on the mood of the speaker, and the latter being is considered slightly more polite. An exception to that rule is the (archaic) pronoun ουδείς, also meaning \"no one\", which does not allow negation of the verb that it governs.\n\nIn Slavic languages other than Slavonic, multiple negatives are grammatically correct ways to express negation, and a single negative is often incorrect. In complex sentences, every part that could be grammatically negated should be negative. For example, in the Serbo-Croatian, \"Ni(t)ko nikad(a) nigd(j)e ništa nije uradio\" (\"Nobody never did not do nothing nowhere\") means \"Nobody has ever done anything, anywhere\", and \"Nikad nisam tamo išao/išla\" (\"Never I did not go there\") means \"I have never been there\". In Czech it is also common to use three or more negations. For example, \"Nikdy jsem nikde nikoho neviděl\" (\"I have not never seen no one nowhere\"). In Russian, \"I know nothing\" is я ничего не знаю (\"ya nichevo nye znayu\"), lit. \"I nothing don't know.\"\n\nA single negation, while syntactically correct, may result in a very unusual meaning or make no sense at all. Saying \"I saw nobody\" in Polish (\"Widziałem nikogo\") instead of the more usual \"I did not see nobody\" (\"Nikogo nie widziałem\") might mean \"I saw an instance of nobody\" or \"I saw Mr. Nobody\" but it would not have its plain English meaning. Likewise, in Slovenian, saying \"I do not know anyone\" (') in place of \"I do not know no one\" (') has the connotation \"I do not know just \"anyone\"\": I know someone important or special.\n\nAs with most synthetic \"satem\" languages double negative is mandatory in Latvian and Lithuanian. Furthermore, all verbs and indefinite pronouns in a given statement must be negated, so it could be said that multiple negative is mandatory in Latvian.\n\nFor instance, a statement \"I have not ever owed anything to anyone\" would be rendered as \"es nekad nevienam neko neesmu bijis parādā\". The only alternative would be using a negating subordinate clause and subjunctive in the main clause, which could be approximated in English as \"there has not ever been an instance that I would have owed anything to anyone\" (\"nav bijis tā, ka es kādreiz būtu kādam bijis kaut ko parādā\"), where negative pronouns (\"nekad, neviens, nekas\") are replaced by indefinite pronouns (\"kādreiz, kāds, kaut kas\") more in line with the English \"ever, any\" indefinite pronoun structures.\n\nDouble or multiple negatives are grammatically required in Hungarian with negative pronouns: \"Nincs semmim\" (word for word: \"[doesn't-exists] [nothing-of-mine]\", and translates literally as \"I do not have nothing\") means \"I do not have anything\". Negative pronouns are constructed by means of adding the prefixes \"se-,\" \"sem-,\" and \"sen-\" to interrogative pronouns.\n\nSomething superficially resembling double negation is required also in Finnish, which uses the auxiliary verb \"ei\" to express negation. Negative pronouns are constructed by adding one of the suffixes \"-an,\" \"-än,\" \"-kaan,\" or \"-kään\" to interrogative pronouns: \"Kukaan ei soittanut minulle\" means \"No one called me\". These suffices are, however, never used alone, but always in connection with \"ei\". This phenomenon is commonplace in Finnish, where many words have alternatives that are required in negative expressions, for example \"edes\" for \"jopa\" (\"even\"), as in \"jopa niin paljon\" meaning \"even so much\", and \"ei edes niin paljoa\" meaning \"not even so much\".\n\nNegative verb forms are grammatically required in Turkish phrases with negative pronouns or adverbs that impart a negative meaning on the whole phrase. For example, \"Hiçbir şeyim yok\" (literally, word for word, \"Not-one thing-of-mine exists-not\") means \"I don't have anything\". Likewise, \"Asla memnun değilim\" (literally, \"Never satisfied not-I-am\") means \"I'm never satisfied\".\n\nJapanese employs litotes to phrase ideas in a more indirect and polite manner. Thus, one can indicate necessity by emphasizing that not doing something would not be proper. For instance, しなければならない (\"shinakereba naranai\", \"must\") literally means \"not doing [it] would not be proper\". しなければいけません (\"shinakereba ikemasen\", also \"must\") similarly means \"not doing [it] cannot go forward\".\n\nOf course, indirectness can also be employed to put an edge on one's rudeness as well. \"He has studied Japanese, so he should be able to write kanji\" can be phrased 彼は日本語を勉強したから漢字で書けないわけがありません (\"kare wa nihongo o benkyō shita kara kanji de kakenai wake ga arimasen\"), there is a rather harsher idea: \"As he has studied Japanese, the reasoning that he cannot write Kanji does not exist\".\n\nMandarin Chinese also employs litotes in a like manner. One common construction is 不得不 (Pinyin: \"bùdébù\", \"cannot not\"), which is used to express (or feign) a necessity more regretful and polite than that expressed by 必须 (\"bìxū\", \"must\"). Compared with \"我必须走\" (\"Wǒ bìxū zǒu\", \"I must go\"), \"我不得不走\" (\"Wǒ bùdébù zǒu\", \"I cannot not go\") tries to emphasize that the situation is out of the speaker's hands and that the speaker has no choice in the matter: \"Unfortunately, I have got to go\". Similarly, \"没有人不知道\" (\"Méiyǒu rén bù zhīdào\", \"There is not a person who does not know\") is a more emphatic way to express \"Everyone knows\".\n\nDouble negatives nearly always resolve to a positive meaning even in colloquial speech, while triple negatives resolve to a negative meaning. For example, \"我不相信没人不来\" (\"Wǒ bù xiāngxìn méi rén bù lái\", \"I do not believe no one will not come\") means \"I do not think everyone will come\". However, triple or multiple negatives are considered obscure and are typically avoided.\n\nMany languages, including all living Germanic languages, French, Welsh and some Berber and Arabic dialects, have gone through a process known as Jespersen's cycle, where an original negative particle is replaced by another, passing through an intermediate stage employing two particles (e.g. Old French \"jeo ne dis\" → Modern Standard French \"je ne dis pas\" → Modern Colloquial French \"je dis pas\" \"I don't say\").\n\nIn many cases, the original sense of the new negative particle is not negative \"per se\" (thus in French \"pas\" \"step\", originally \"not a step\" = \"not a bit\"), but in Germanic languages, such as English and German the intermediate stage was a case of double negation, as the current negatives \"not\" and \"nicht\" in these languages originally meant \"nothing\": e.g. Old English \"ic ne seah\" \"I didn't see\" » Middle English \"I ne saugh nawiht\", lit. \"I didn't see nothing\" » Early Modern English \"I saw not\".\n\nA similar development to a circumfix from double negation can be seen in non-Indo-European languages, too: for example, in Maltese, \"kiel\" \"he ate\" is negated as \"ma kielx\" \"he did not eat\", where the verb is preceded by a negative particle \"ma\"- \"not\" and followed by the particle -\"x\", which was originally a shortened form of \"xejn\" \"nothing\" - thus, \"he didn't eat nothing\".\n\n"}
{"id": "2892645", "url": "https://en.wikipedia.org/wiki?curid=2892645", "title": "Instantiation principle", "text": "Instantiation principle\n\nThe instantiation principle or principle of instantiation or principle of exemplification is the concept in metaphysics and logic (first put forward by David Malet Armstrong) that there can be no uninstantiated or unexemplified properties (or universals). In other words, it is impossible for a property to exist which is not had by some object.\n\nConsider a chair. Presumably chairs did not exist 150,000 years ago. Thus, according to the principle of instantiation, the property of being a chair did not exist 150,000 years ago either. Similarly, if all red objects were to suddenly go out of existence, then the property of being red would likewise go out of existence.\n\nTo make the principle more plausible in the light of these examples, the existence of properties or universals is not tied to their actual existence now, but to their existence in space-time considered as a whole. Thus, any property which \"is\", \"has been\", or \"will be\" instantiated exists. The property of being red would exist even if all red things were to be destroyed, because it has been instantiated. This broadens the range of properties which exist if the principle is true. \n\nThose who endorse the principle of instantiation are known as \"in re\" (in thing or in reality) realists or 'immanent realists'.\n\n"}
{"id": "3108937", "url": "https://en.wikipedia.org/wiki?curid=3108937", "title": "Landauer's principle", "text": "Landauer's principle\n\nLandauer's principle is a physical principle pertaining to the lower theoretical limit of energy consumption of computation. It holds that \"any logically irreversible manipulation of information, such as the erasure of a bit or the merging of two computation paths, must be accompanied by a corresponding entropy increase in non-information-bearing degrees of freedom of the information-processing apparatus or its environment\".\n\nAnother way of phrasing Landauer's principle is that if an observer loses information about a physical system, the observer loses the ability to extract work from that system.\n\nA so-called logically-reversible computation, in which no information is erased, may in principle be carried out without releasing any heat. This has led to considerable interest in the study of reversible computing. Indeed, without reversible computing, increases in the number of computations-per-joule-of-energy-dissipated must come to a halt by about 2050: because the limit implied by Landauer's principle will be reached by then, according to Koomey's law. \n\nAt 20 °C (room temperature, or 293.15 K), the Landauer limit represents an energy of approximately 0.0172 eV, or 2.75 zJ. Theoretically, roomtemperature computer memory operating at the Landauer limit could be changed at a rate of one billion bits per second with energy being converted to heat in the memory media at the rate of only 2.85 trillionths of a watt (that is, at a rate of only 2.85 pJ/s). Modern computers use millions of times as much energy per second.\n\nRolf Landauer first proposed the principle in 1961 while working at IBM. He rigorously justified and stated important limits to an earlier conjecture by John von Neumann. For this reason, it is sometimes referred to as being simply the Landauer bound or Landauer limit.\n\nIn 2011, the principle was generalized to show that while information erasure requires an increase in entropy, that increase could theoretically occur at no energy cost. Instead, the cost can be taken in another conserved quantity, such as angular momentum.\n\nIn a 2012 article published in \"Nature\", a team of physicists from the École normale supérieure de Lyon, University of Augsburg and the University of Kaiserslautern described that for the first time they have measured the tiny amount of heat released when an individual bit of data is erased.\n\nIn 2014, physical experiments tested Landauer's principle and confirmed its predictions.\n\nIn 2016, researchers used a laser probe to measure the amount of energy dissipation that resulted when a nanomagnetic bit flipped from off to on. Flipping the bit required 15 millielectron volts (3 zeptojoules).\n\nA 2018 article published in \"Nature Physics\" features a Landauer erasure performed at cryogenic temperatures (T = 1K) on an array of high-spin (S = 10) quantum molecular magnets. The array is made to act as a spin register where each nanomagnet encodes a single bit of information. The experiment has laid the foundations for the extension of the validity of the Landauer principle to the quantum realm. Owing to the fast dynamics and low \"inertia\" of the single spins used in the experiment, the researchers also showed how an erasure operation can be carried out at the lowest possible thermodynamic cost — that imposed by the Landauer principle — and at a high speed. \n\nLandauer's principle can be understood to be a simple logical consequence of the second law of thermodynamics—which states that the entropy of an isolated system cannot decrease—together with the definition of thermodynamic temperature. For, if the number of possible logical states of a computation were to decrease as the computation proceeded forward (logical irreversibility), this would constitute a forbidden decrease of entropy, unless the number of possible physical states corresponding to each logical state were to simultaneously increase by at least a compensating amount, so that the total number of possible physical states was no smaller than it was originally (i.e. total entropy has not decreased).\n\nYet, an increase in the number of physical states corresponding to each logical state means that, for an observer who is keeping track of the logical state of the system but not the physical state (for example an \"observer\" consisting of the computer itself), the number of possible physical states has increased; in other words, entropy has increased from the point of view of this observer.\n\nThe maximum entropy of a bounded physical system is finite. (If the holographic principle is correct, then physical systems with finite surface area have a finite maximum entropy; but regardless of the truth of the holographic principle, quantum field theory dictates that the entropy of systems with finite radius and energy is finite due to the Bekenstein bound.) To avoid reaching this maximum over the course of an extended computation, entropy must eventually be expelled to an outside environment.\n\nLandauer's principle asserts that there is a minimum possible amount of energy required to erase one bit of information, known as the \"Landauer limit\":\nwhere \"k\" is the Boltzmann constant (approximately 1.38×10 J/K), \"T\" is the temperature of the heat sink in kelvins, and ln 2 is the natural logarithm of 2 (approximately 0.69315).\n\nFor an environment at temperature \"T\", energy \"E\" = \"ST\" must be emitted into that environment if the amount of added entropy is \"S\". For a computational operation in which 1 bit of logical information is lost, the amount of entropy generated is at least \"k\" ln 2, and so, the energy that must eventually be emitted to the environment is \"E\" ≥ \"kT\" ln 2.\n\nThe principle is widely accepted as physical law, but in recent years it has been challenged for using circular reasoning and faulty assumptions, notably in Earman and Norton (1998), and subsequently in Shenker (2000) and Norton (2004, 2011), and defended by Bennett (2003) and Ladyman et al. (2007).\n\n\n"}
{"id": "34074829", "url": "https://en.wikipedia.org/wiki?curid=34074829", "title": "Lexical entrainment", "text": "Lexical entrainment\n\nLexical entrainment is the phenomenon in conversational linguistics of the process of the subject adopting the reference terms of their interlocutor. In practice, it acts as a mechanism of the cooperative principle in which both parties to the conversation employ lexical entrainment as a progressive system to develop \"conceptual pacts\" (a working temporary conversational terminology) to ensure maximum clarity of reference in the communication between the parties; this process is necessary to overcome the ambiguity inherent in the multitude of synonyms that exist in language. \n\nLexical entrainment arises by two cooperative mechanisms: \n\nOnce lexical entrainment has come to determine the phrasing for a referent, both parties will use that terminology for the referent for the duration, even if it proceeds to violate the Gricean maxim of quantity. For example, if one wants to refer to a brown loafer out of a set of shoes that consist of: the loafer, a sneaker, and a high-heeled shoe, they will not use \"the shoe\" to describe the object as this phrasing does not unambiguously describe one item in the set under consideration. They will also not call the object \"the brown loafer\" which would violate Grice's maxim of quantity. The speaker will settle on using the term \"the loafer\" as it is just informative enough without giving too much information. \n\nAnother important factor is lexical availability; the ease of conceptualizing a referent in a certain way and then retrieving and producing a label for it. For many objects the most available labels are basic nouns; for example, the word \"dog\". Instead of saying \"animal\" or \"husky\" for the referent, most subjects will default to \"dog\".\nIf in a set of objects one is to refer to either a husky, a table, and a poster, people are still most likely to use the word \"dog.\" This is technically a violation of Grice's maxim of quantity, as using the term \"animal\" is ideal.\n\nLexical entrainment has applications in natural language processing in computers as well as human–human interaction. Currently, the adaptability of computers to modify their referencing to the terms of their human interlocutor is limited, so the entrainment adaptation falls to the human operator; this phenomenon is readily demonstrated in Brennan's 1996 experiment.\n"}
{"id": "29594530", "url": "https://en.wikipedia.org/wiki?curid=29594530", "title": "Logical hexagon", "text": "Logical hexagon\n\nThe logical hexagon (also called the hexagon of opposition) is a conceptual model of the relationships between the truth values of six statements. It is an extension of Aristotle's square of opposition. It was discovered independently by both Augustin Sesmat and Robert Blanché.\n\nThis extension consists in introducing two statements U and Y. Whereas U is the disjunction of A and E, Y is the conjunction of the two traditional particulars I and O.\n\nThe traditional square of opposition demonstrates two sets of contradictories A and O, and E and I (i.e. they cannot both be true and cannot both be false), two contraries A and E (i.e. they can both be false, but cannot both be true), and two subcontraries I and O (i.e. they can both be true, but cannot both be false) according to Aristotle’s definitions. However, the logical hexagon provides that U and Y are also contradictory.\n\nThe logical hexagon may be interpreted in various ways, including as a model of traditional logic, quantifications, modal logic, order theory, or paraconsistent logic.\n\nFor instance, the statement A may be interpreted as \"Whatever x may be, if x is a man, then x is white.\"\n\nThe statement E may be interpreted as \"Whatever x may be, if x is a man, then x is non-white.\"\n\nThe statement I may be interpreted as \"There exists at least one x that is both a man and white.\"\n\nThe statement O may be interpreted as \"There exists at least one x that is both a man and non-white\"\n\nThe statement Y may be interpreted as \"There exists at least one x that is both a man and white and there exists at least one x that is both a man and non-white\"\n\nThe statement U may be interpreted as \"One of two things, either whatever x may be, if x is a man, then x is white or whatever x may be, if x is a man, then x is non-white.\"\n\nThe logical hexagon may be interpreted in various ways, including as a model of traditional logic, quantifications, modal logic, order theory, or paraconsistent logic.\n\nThe logical hexagon may be interpreted as a model of modal logic such that\n\n\nIt has been proven that both the square and the hexagon, followed by a “logical cube”, belong to a regular series of n-dimensional objects called “logical bi-simplexes of dimension n.” The pattern also goes even beyond this.\n\n\n"}
{"id": "274078", "url": "https://en.wikipedia.org/wiki?curid=274078", "title": "Marginal demand", "text": "Marginal demand\n\nMarginal demand in economics is the change in demand for a product or service in response to a specific change in its price.\n\nNormally, as prices for goods or service rise, demand falls, and conversely, as prices for goods or services fall, demand rises.\n\nA product or service where price changes cause a relatively big change in demand is said to have \"elastic\" demand. A product or service where price changes cause a relatively small change in demand is said to have \"inelastic\" demand. See Elasticity of demand.\n\n"}
{"id": "762043", "url": "https://en.wikipedia.org/wiki?curid=762043", "title": "Marginal product", "text": "Marginal product\n\nIn economics and in particular neoclassical economics, the marginal product or marginal physical productivity of an input (factor of production) is the change in output resulting from employing one more unit of a particular input (for instance, the change in output when a firm's labor is increased from five to six units), assuming that the quantities of other inputs are kept constant.\n\nThe marginal product of a given input can be expressed \nas:\n\nwhere formula_2 is the change in the firm's use of the input (conventionally a one-unit change) and formula_3 is the change in quantity of output produced (resulting from the change in the input). Note that the quantity formula_4 of the \"product\" is typically defined ignoring external costs and benefits.\n\nIf the output and the input are infinitely divisible, so the marginal \"units\" are infinitesimal, the marginal product is the mathematical derivative of the production function with respect to that input. Suppose a firm's output \"Y\" is given by the production function:\n\nwhere \"K\" and \"L\" are inputs to production (say, capital and labor). Then the marginal product of capital (\"MPK\") and marginal product of labor (\"MPL\") are given by:\n\nIn the \"law\" of diminishing marginal returns, the marginal product initially increases when more of an input (say labor) is employed, keeping the other input (say capital) constant. Here, labor is the variable input and capital is the fixed input (in a hypothetical two-inputs model). As more and more of variable input (labor) is employed, marginal product starts to fall. Finally, after a certain point, the marginal product becomes negative, implying that the additional unit of labor has \"decreased\" the output, rather than increasing it. The reason behind this is the diminishing marginal productivity of labor.\n\nThe marginal product of labor is the slope of the total product curve, which is the production function plotted against labor usage for a fixed level of usage of the capital input.\n\nIn the neoclassical theory of competitive markets, the marginal product of labor equals the real wage. In aggregate models of perfect competition, in which a single good is produced and that good is used both in consumption and as a capital good, the marginal product of capital equals its rate of return. As was shown in the Cambridge capital controversy, this proposition about the marginal product of capital cannot generally be sustained in multi-commodity models in which capital and consumption goods are distinguished.\n\nRelationship of marginal product (MPP) with the total product (TPP)\n\nThe relationship can be explained in three phases-\n(1) Initially, as the quantity of variable input is increased, TPP rises at an increasing rate. In this phase, MPP also rises.\n(2) As more and more quantities of the variable inputs are employed, TPP increases at a diminishing rate. In this phase, MPP starts to fall.\n(3) When the TPP reaches its maximum, MPP is zero. Beyond this point, TPP starts to fall and MPP becomes negative.\n\n"}
{"id": "432318", "url": "https://en.wikipedia.org/wiki?curid=432318", "title": "Marginal rate of substitution", "text": "Marginal rate of substitution\n\nIn economics, the marginal rate of substitution (MRS) is the rate at which a consumer can give up some amount of one good in exchange for another good while maintaining the same level of utility. At equilibrium consumption levels (assuming no externalities), marginal rates of substitution are identical.\n\nUnder the standard assumption of neoclassical economics that goods and services are continuously divisible, the marginal rates of substitution will be the same regardless of the direction of exchange, and will correspond to the slope of an indifference curve (more precisely, to the slope multiplied by −1) passing through the consumption bundle in question, at that point: mathematically, it is the implicit derivative. MRS of X for Y is the amount of Y which a consumer can exchange for one unit of X locally. The MRS is different at each point along the indifference curve thus it is important to keep locus in the definition. Further on this assumption, or otherwise on the assumption that utility is quantified, the marginal rate of substitution of good or service Y for good or service X (MRS) is also equivalent to the marginal utility of X over the marginal utility of Y. Formally,\n\nIt is important to note that when comparing bundles of goods X and Y that give a constant utility (points along an indifference curve), the marginal utility of X is measured in terms of units of Y that is being given up.\n\nFor example, if the MRS = 2, the consumer will give up 2 units of Y to obtain 1 additional unit of X.\n\nAs one moves down a (standardly convex) indifference curve, the marginal rate of substitution decreases (as measured by the absolute value of the slope of the indifference curve, which decreases). This is known as the law of diminishing marginal rate of substitution.\n\nSince the indifference curve is convex with respect to the origin and we have defined the MRS as the negative slope of the indifference curve,\n\nAssume the consumer utility function is defined by formula_4, where \"U\" is consumer utility, \"x\" and \"y\" are goods. Then the marginal rate of substitution can be computed via partial differentiation, as follows.\n\nAlso, note that:\n\nwhere formula_7 is the marginal utility with respect to good \"x\" and formula_8 is the marginal utility with respect to good \"y\".\n\nBy taking the total differential of the utility function equation, we obtain the following results:\n\nThrough any point on the indifference curve, \"dU/dx\" = 0, because \"U\" = \"c\", where \"c\" is a constant. It follows from the above equation that:\n\nThe marginal rate of substitution is defined as the absolute value of the slope of the indifference curve at whichever commodity bundle quantities are of interest. That turns out to equal the ratio of the marginal utilities:\n\nWhen consumers maximize utility with respect to a budget constraint, the indifference curve is tangent to the budget line, therefore, with \"m\" representing slope:\n\nTherefore, when the consumer is choosing his utility maximized market basket on his budget line,\n\nThis important result tells us that utility is maximized when the consumer's budget is allocated so that the marginal utility per unit of money spent is equal for each good. If this equality did not hold, the consumer could increase his/her utility by cutting spending on the good with lower marginal utility per unit of money and increase spending on the other good. To decrease the marginal rate of substitution, the consumer must buy more of the good for which he/she wishes the marginal utility to fall for (due to the law of diminishing marginal utility).\n\n\n"}
{"id": "4675536", "url": "https://en.wikipedia.org/wiki?curid=4675536", "title": "Master–slave morality", "text": "Master–slave morality\n\nMaster–slave morality is a central theme of Friedrich Nietzsche's works, in particular the first essay of \"On the Genealogy of Morality\". Nietzsche argued that there were two fundamental types of morality: \"master morality\" and \"slave morality\". Master morality values pride and power, while slave morality values things like kindness, empathy, and sympathy. Master morality weighs actions on good or bad consequences (i. e., classical virtues and vices, consequentialism), unlike slave morality, which weighs actions on a scale of good or evil intentions (e. g., Christian virtues and vices, Kantian deontology).\n\nFor Nietzsche, a particular morality is inseparable from the formation of a particular culture, meaning that a culture's language, codes and practices, narratives, and institutions are informed by the struggle between these two moral structures (see valuation).\n\nNietzsche defined master morality as the morality of the strong-willed. Nietzsche criticizes the view (which he identifies with contemporary British ideology) that good is everything that is helpful, and bad is everything that is harmful. He argues proponents of this view have forgotten the origins of its values and it is based merely on a non-critical acceptance of habit: what is useful has always been defined as good, therefore usefulness is goodness as a value. He continues explaining that in the prehistoric state \"the value or non-value of an action was derived from its consequences\" but ultimately \"[t]here are no moral phenomena at all, only moral interpretations of phenomena.\" For strong-willed men, the \"good\" is the noble, strong, and powerful, while the \"bad\" is the weak, cowardly, timid, and petty. \n\nThe essence of master morality is \"nobility\". Other qualities that are often valued in master morality are open-mindedness, courage, truthfulness, trust, and an accurate sense of one's self-worth. Master morality begins in the \"noble man\", with a spontaneous idea of the good; then the idea of bad develops as what is not good. \"The noble type of man experiences \"itself\" as determining values; it does not need approval; it judges, \"what is harmful to me is harmful in itself\"; it knows itself to be that which first accords honour to things; it is \"value-creating\".\" In this sense, the master morality is the full recognition that \"oneself\" is the measure of all moral truths. Insofar as something is helpful to the strong-willed man, it is like what he values in himself; therefore, the strong-willed man values such things as good because they aid him in a life-long process of self-actualization through the will to power.\n\nAccording to Nietzsche, masters are creators of morality; slaves respond to master morality with their slave morality. Unlike master morality, which is sentiment, slave morality is based on re-sentiment—devaluing that which the master values and the slave does not have. As master morality originates in the strong, slave morality originates in the weak. Because slave morality is a reaction to oppression, it vilifies its oppressors. Slave morality is the inverse of master morality. As such, it is characterized by pessimism and cynicism. Slave morality is created in opposition to what master morality values as \"good\". \n\nSlave morality does not aim at exerting one's will by strength, but by careful subversion. It does not seek to transcend the masters, but to make them slaves as well. The essence of slave morality is \"utility\": The good is what is most useful for the whole community, not just the strong. Nietzsche saw this as a contradiction. Since the powerful are few in number, compared to the masses of the weak, the weak gain power by corrupting the strong into believing that the causes of slavery (viz., the will to power) are \"evil\", as are the qualities the weak originally could not choose because of their weakness. By saying humility is voluntary, slave morality avoids admitting that their humility was in the beginning forced upon them by a master. Biblical principles of humility, charity, and pity are the result of universalizing the plight of the slave onto all humankind, and thus enslaving the masters as well. \"The \"democratic\" movement is the heir to Christianity\"—the political manifestation of slave morality because of its obsession with freedom and equality.\n\nThis struggle between master and slave moralities recurs historically. According to Nietzsche, ancient Greek and Roman societies were grounded in master morality. The Homeric hero is the strong-willed man, and the classical roots of the \"Iliad\" and \"Odyssey\" exemplified Nietzsche's master morality. He calls the heroes \"men of a noble culture\", giving a substantive example of master morality. Historically, master morality was defeated, as the slave morality of Judaism and Christianity spread throughout the Roman Empire. \n\nThe essential struggle between cultures has always been between the Roman (master, strong) and the Judean (slave, weak). Nietzsche condemns the triumph of slave morality in the West, saying that the democratic movement is the \"collective degeneration of man\". He claimed that the nascent democratic movement of his time was essentially slavish and weak. Weakness conquered strength, slave conquered master, re-sentiment conquered sentiment. This ressentiment Nietzsche calls \"priestly vindictiveness\", which is based on the jealous weak seeking to enslave the strong, and thus erode the basis for power itself by pulling the powerful down. Such movements were, according to Nietzsche, inspired by \"the most intelligent revenge\" of the weak. Nietzsche saw democracy and Christianity as the same emasculating impulse which sought to make everyone equal by making everyone a slave.\n\nNietzsche did not necessarily believe that everyone should adopt master morality as the \"be-all, end-all\" behavior. He thought that the revaluation of morals would correct the inconsistencies in both master and slave moralities. But he asserted that for the individual, master morality was preferable to slave morality. Walter Kaufmann disagrees that Nietzsche actually preferred master morality to slave morality. He certainly gives slave morality a more thorough critique, but this is partly because he thought of slave morality as society's more imminent danger.\n\n\n"}
{"id": "27079770", "url": "https://en.wikipedia.org/wiki?curid=27079770", "title": "Mental model theory of reasoning", "text": "Mental model theory of reasoning\n\nThe mental model theory of reasoning was developed by Philip Johnson-Laird and Ruth M.J. Byrne (Johnson-Laird and Byrne, 1991). It has been applied to the main domains of deductive inference including relational inferences such as spatial and temporal deductions; propositional inferences, such as conditional, disjunctive and negation deductions; quantified inferences such as syllogisms; and meta-deductive inferences.\n\nOngoing research on mental models and reasoning has led the theory to be extended to account for probabilistic inference (e.g., Johnson-Laird, 2006) and counterfactual thinking (Byrne, 2005).\n\n"}
{"id": "10784136", "url": "https://en.wikipedia.org/wiki?curid=10784136", "title": "Negative base", "text": "Negative base\n\nA negative base (or negative radix) may be used to construct a non-standard positional numeral system. Like other place-value systems, each position holds multiples of the appropriate power of the system's base; but that base is negative—that is to say, the base is equal to for some natural number ().\n\nNegative-base systems can accommodate all the same numbers as standard place-value systems, but both positive and negative numbers are represented without the use of a minus sign (or, in computer representation, a sign bit); this advantage is countered by an increased complexity of arithmetic operations. The need to store the information normally contained by a negative sign often results in a negative-base number being one digit longer than its positive-base equivalent.\n\nThe common names for negative-base positional numeral systems are formed by prefixing \"nega-\" to the name of the corresponding positive-base system; for example, negadecimal (base −10) corresponds to decimal (base 10), negabinary (base −2) to binary (base 2), and negaternary (base −3) to ternary (base 3).\n\nConsider what is meant by the representation in the negadecimal system, whose base is −10:\n\nSince 10,000 + (−2,000) + 200 + (−40) + 3 = , the representation in negadecimal notation is equivalent to in decimal notation, while in decimal would be written in negadecimal.\n\nNegative numerical bases were first considered by Vittorio Grünwald in his work \"Giornale di Matematiche di Battaglini\", published in 1885. Grünwald gave algorithms for performing addition, subtraction, multiplication, division, root extraction, divisibility tests, and radix conversion. Negative bases were later independently rediscovered by A. J. Kempner in 1936 and Zdzisław Pawlak and A. Wakulicz in 1959.\n\nNegabinary was implemented in the early Polish computer BINEG (and UMC), built 1957–59, based on ideas by Z. Pawlak and A. Lazarkiewicz from the Mathematical Institute in Warsaw. Implementations since then have been rare.\n\nDenoting the base as , every integer can be written uniquely as\nwhere each digit is an integer from 0 to and the leading digit is (unless ). The base expansion of is then given by the string .\n\nNegative-base systems may thus be compared to signed-digit representations, such as balanced ternary, where the radix is positive but the digits are taken from a partially negative range. (In the table below the digit of value −1 is written as the single character T.)\n\nSome numbers have the same representation in base as in base . For example, the numbers from 100 to 109 have the same representations in decimal and negadecimal. Similarly,\nand is represented by 10001 in binary and 10001 in negabinary.\n\nSome numbers with their expansions in a number of positive and corresponding negative bases are:\n\nNote that the base expansions of negative integers have an even number of digits, while the base expansions of the non-negative integers have an odd number of digits.\n\nThe base expansion of a number can be found by repeated division by , recording the non-negative remainders of formula_3, and concatenating those remainders, starting with the last. Note that if , remainder , then and therefore . To arrive at the correct conversion, the value for must be chosen such that is non-negative and minimal. This is exemplified in the fourth line of the following example wherein –5 ÷ –3 must be chosen to equal 2 remainder 1 instead of 1 remainder –2.\n\nFor example, to convert 146 in decimal to negaternary:\nReading the remainders backward we obtain the negaternary representation of 146: 21102.\n\nNote that in most programming languages, the result (in integer arithmetic) of dividing a negative number by a negative number is rounded towards 0, usually leaving a negative remainder. In such a case we have . Because , is the positive remainder. Therefore, to get the correct result in such case, computer implementations of the above algorithm should add 1 and to the quotient and remainder respectively.\n\nPrivate Shared Function ToNegaternary(value As Integer) As String\n\nEnd Function\nfrom [-10 -2] interval:\n\n (if\n\nThe conversion from integer to some negative base:\n\nFunction toNegativeBase(Number As Integer , base As Integer) As System.Collections.Generic.List(Of Integer)\n\nend function\nThe conversion to \"negabinary\" (base −2; digits formula_5) allows a remarkable shortcut \n(C implementation):\nDue to D. Librik (Szudzik). The bitwise XOR portion is originally due to Schroeppel (1972).\n\nJavaScript port for the same shortcut calculation: \nThe conversion to \"negaquaternary\" (base −4; digits formula_6) allows a similar shortcut (C implementation):\nJavaScript port for the same shortcut calculation: \nThe following describes the arithmetic operations for negabinary; calculations in larger bases are similar.\n\nAdding negabinary numbers proceeds bitwise, starting from the least significant bits; the bits from each addend are summed with the (balanced ternary) carry from the previous bit (0 at the LSB). This sum is then decomposed into an output bit and carry for the next iteration as show in the table:\n\nThe second row of this table, for instance, expresses the fact that −1 = 1 + 1 × −2; the fifth row says 2 = 0 + −1 × −2; etc.\n\nAs an example, to add 1010101 (1 + 4 + 16 + 64 = 85) and 1110100 (4 + 16 − 32 + 64 = 52),\n\nso the result is 110011001 (1 − 8 + 16 − 128 + 256 = 137).\n\nWhile adding two negabinary numbers, every time a carry is generated an extra carry should be propagated to next bit. Consider same example as above\n\nA full adder circuit can be designed to add numbers in negabinary. The following logic is used to calculate the sum and carries:\n\nIncrementing a negabinary number can be done by using the following formula:\n\nTo subtract, multiply each bit of the second number by −1, and add the numbers, using the same table as above.\n\nAs an example, to compute 1101001 (1 − 8 − 32 + 64 = 25) minus 1110100 (4 + 16 − 32 + 64 = 52),\n\nso the result is 100101 (1 + 4 −32 = −27).\n\nUnary negation, , can be computed as binary subtraction from zero, .\n\nShifting to the left multiplies by −2, shifting to the right divides by −2.\n\nTo multiply, multiply like normal decimal or binary numbers, but using the negabinary rules for adding the carry, when adding the numbers.\n\nFor each column, add \"carry\" to \"number\", and divide the sum by −2, to get the new \"carry\", and the resulting bit as the remainder.\n\nIt is possible to compare negabinary numbers by slightly adjusting a normal unsigned binary comparator. When comparing the numbers formula_11 and formula_12, invert each odd positioned bit of both numbers.\nAfter this, compare formula_11 and formula_12 using a standard unsigned comparator.\nBase representation may of course be carried beyond the radix point, allowing the representation of non-integral numbers.\n\nAs with positive-base systems, terminating representations correspond to fractions where the denominator is a power of the base; repeating representations correspond to other rationals, and for the same reason.\n\nUnlike positive-base systems, where integers and terminating fractions have non-unique representations (for example, in decimal 0.999… = 1) in negative-base systems the integers have only a single representation. However, there do exist rationals with non-unique representations. For the digits {0, 1, …, t} with formula_15 the biggest digit and\nwe have\nSo every number formula_19 with a terminating fraction formula_20 added has two distinct representations.\n\nFor example, in negaternary, i.e. formula_21 and formula_22, there is\n\nSuch non-unique representations can be found by considering the largest and smallest possible representations with integral parts 0 and 1 respectively, and then noting that they are equal. (Indeed, this works with any integral-base system.) The rationals thus non-uniquely expressible are those of form\nwith formula_25\n\nJust as using a negative base allows the representation of negative numbers without an explicit negative sign, using an imaginary base allows the representation of Gaussian integers. Donald Knuth proposed the quater-imaginary base (base 2i) in 1955.\n\n\n"}
{"id": "236895", "url": "https://en.wikipedia.org/wiki?curid=236895", "title": "Negative capability", "text": "Negative capability\n\nNegative capability was a phrase first used by Romantic poet John Keats in 1817 to characterise the capacity of the greatest writers (particularly Shakespeare) to pursue a vision of artistic beauty even when it leads them into intellectual confusion and uncertainty, as opposed to a preference for philosophical certainty over artistic beauty. The term has been used by poets and philosophers to describe the ability of the individual to perceive, think, and operate beyond any presupposition of a predetermined capacity of the human being.\n\nKeats used the phrase only briefly in a private letter, and it became known only after his correspondence was collected and published. In a letter to his brothers, George and Thomas, on 21 December 1817, Keats described a conversation he had been engaged in a few days previously: \nI had not a dispute but a disquisition with Dilke, upon various subjects; several things dove-tailed in my mind, and at once it struck me what quality went to form a Man of Achievement, especially in Literature, and which Shakespeare possessed so enormously—I mean Negative Capability, that is, when a man is capable of being in uncertainties, mysteries, doubts, without any irritable reaching after fact and reason—Coleridge, for instance, would let go by a fine isolated verisimilitude caught from the Penetralium of mystery, from being incapable of remaining content with half-knowledge. This pursued through volumes would perhaps take us no further than this, that with a great poet the sense of Beauty overcomes every other consideration, or rather obliterates all consideration.\nSamuel Taylor Coleridge was, by 1817, a frequent target of criticism by the younger poets of Keats's generation, often ridiculed for his infatuation with German idealistic philosophy. Against Coleridge's obsession with philosophical truth, Keats sets up the model of Shakespeare, whose poetry articulated various points of view and never advocated a particular vision of truth.\n\nKeats's ideas here, as was usually the case in his letters, were expressed tersely with no effort to fully expound what he meant, but passages from other letters enlarge on the same theme. In a letter to J.H. Reynolds in February, 1818, he wrote: \nWe hate poetry that has a palpable design upon us—and if we do not agree, seems to put its hand in its breeches pocket. Poetry should be great & unobtrusive, a thing which enters into one's soul, and does not startle it or amaze it with itself but with its subject.\nIn another letter to Reynolds the following May, he contrived the metaphor of 'the chamber of maiden thought' and the notion of the 'burden of mystery', which together express much the same idea as that of negative capability:\nI compare human life to a large Mansion of Many Apartments, two of which I can only describe, the doors of the rest being as yet shut upon me—The first we step into we call the infant or thoughtless Chamber, in which we remain as long as we do not think—We remain there a long while, and notwithstanding the doors of the second Chamber remain wide open, showing a bright appearance, we care not to hasten to it; but are at length imperceptibly impelled by the awakening of the thinking principle—within us—we no sooner get into the second Chamber, which I shall call the Chamber of Maiden-Thought, than we become intoxicated with the light and the atmosphere, we see nothing but pleasant wonders, and think of delaying there for ever in delight: However among the effects this breathing is father of is that tremendous one of sharpening one's vision into the heart and nature of Man—of convincing ones nerves that the World is full of Misery and Heartbreak, Pain, Sickness, and oppression—whereby This Chamber of Maiden Thought becomes gradually darken'd and at the same time on all sides of it many doors are set open—but all dark—all leading to dark passages—We see not the balance of good and evil. We are in a Mist—We are now in that state—We feel the 'burden of the Mystery,' To this point was Wordsworth come, as far as I can conceive when he wrote 'Tintern Abbey' and it seems to me that his Genius is explorative of those dark Passages. Now if we live, and go on thinking, we too shall explore them. he is a Genius and superior to us, in so far as he can, more than we, make discoveries, and shed a light in them—Here I must think Wordsworth is deeper than Milton[.]\nKeats understood Coleridge as searching for a single, higher-order truth or solution to the mysteries of the natural world. He went on to find the same fault in Dilke and Wordsworth. All these poets, he claimed, lacked objectivity and universality in their view of the human condition and the natural world. In each case, Keats found a mind which was a narrow private path, not a \"thoroughfare for all thoughts\". Lacking for Keats were the central and indispensable qualities requisite for flexibility and openness to the world, or what he referred to as negative capability.\n\nThis concept of Negative Capability is precisely a rejection of set philosophies and preconceived systems of nature. He demanded that the poet be receptive rather than searching for fact or reason, and to not seek absolute knowledge of every truth, mystery, or doubt.\n\nIt is not known why Keats settled on the phrase 'negative capability', but some scholars have hypothesized that Keats was influenced in his studies of medicine and chemistry, and that it refers to the negative pole of an electric current which is passive and receptive. In the same way that the negative pole receives the current from the positive pole, the poet receives impulses from a world that is full of mystery and doubt, which cannot be explained but which the poet can translate into art.\n\nRoberto Unger appropriated Keats' term in order to explain resistance to rigid social divisions and hierarchies. For Unger, \"negative capability\" is the \"denial of whatever in our contexts delivers us over to a fixed scheme of division and hierarchy and to an enforced choice between routine and rebellion.\" It is thus through \"negative capability\" that we can further empower ourselves against social and institutional constraints, and loosen the bonds that entrap us in a certain social station.\n\nAn example of negative capability can be seen at work in industrial innovation. In order to create an innovator's advantage and develop new forms of economic enterprise, the modern industrialist could not just become more efficient with surplus extraction based on pre-existing work roles, but rather needed to invent new styles of flexible labor, expertise, and capital management. The industrialist needed to bring people together in new and innovative ways and redefine work roles and workplace organization. The modern factory had to at once stabilize its productive environment by inventing new restraints upon labor, such as length of the work day and division of tasks, but at the same time could not be too severe or risk being at a disadvantage to competitors, e.g. not being able to shift production tasks or capacity. Those industrialists and managers who were able to break old forms of organizational arrangements exercised negative capability.\n\nThis thesis of \"negative capability\" is a key component in Unger's theory of false necessity and formative context. The theory of false necessity claims that our social worlds are the artifact of our own human endeavors. There is no pre-set institutional arrangement that our societies adhere to, and there is no necessary historical mold of development that they will follow. Rather we are free to choose and develop the forms and the paths that our societies will take through a process of conflicts and resolutions. However, there are groups of institutional arrangements that work together to bring out certain institutional forms, liberal democracy, for example. These forms are the basis of a social structure, and which Unger calls formative contexts. In order to explain how we move from one formative context to another without the conventional social theory constraints of historical necessity (e.g. feudalism to capitalism), and to do so while remaining true to the key insight of individual human empowerment and anti-necessitarian social thought, Unger recognized that there are an infinite number of ways of resisting social and institutional constraints, which can lead to an infinite number of outcomes. This variety of forms of resistance and empowerment (i.e. negative capability) make change possible.\n\nThis thesis of \"negative capability\" addresses the problem of agency in relation to structure. It recognizes the constraints of structure and its molding influence upon the individual, but at the same time finds the individual able to resist, deny, and transcend their context. Unlike other theories of structure and agency, \"negative capability\" does not reduce the individual to a simple actor possessing only the dual capacity of compliance or rebellion, but rather sees him as able to partake in a variety of activities of self empowerment.\n\nThe twentieth-century British psychoanalyst Wilfred Bion elaborated on Keats's term to illustrate an attitude of openness of mind which he considered of central importance, not only in the psychoanalytic session, but in life itself. For Bion, negative capability was the ability to tolerate the pain and confusion of not knowing, rather than imposing ready-made or omnipotent certainties upon an ambiguous situation or emotional challenge. His idea has been taken up more widely in the British Independent School, as well as elsewhere in psychoanalysis and psychotherapy.\n\nThe notion of negative capability has been associated with the Zen philosophy. Keats' man of negative capability had qualities that enabled him to \"lose his self-identity, his 'imaginative identification' with and submission to things, and his power to achieve a unity with life\". The Zen concept of satori is the outcome of passivity and receptivity, culminating in \"sudden insight into the character of the real\". Satori is reached without deliberate striving. The antecedent stages to satori: quest, search, ripening and explosion. The \"quest\" stage is accompanied by a strong feeling of uneasiness, resembling the capacity to practice negative capability while the mind is in a state of \"uncertainties, mysteries and doubts\". In the explosive stage (akin to Keats' 'chief intensity'), a man of negative capability effects a \"fellowship with essence\".\n\nStanley Fish has expressed strong reservations about the attempt to apply the concept of negative capability to social contexts. He has written in critique of Unger's early work as being unable to chart a route for the idea to pass into reality, which leaves history closed and the individual holding onto the concept while kicking against air. Fish finds the capability Unger invokes in his early works unimaginable and unmanufacturable that can only be expressed outright in blatant speech, or obliquely in concept. More generally, Fish finds the idea of radical culture as an oppositional ideal in which context is continuously refined or rejected impracticable at best, and impossible at worst. Unger has addressed these criticisms by developing a full theory of historical process in which negative capability is employed.\n\n"}
{"id": "6476061", "url": "https://en.wikipedia.org/wiki?curid=6476061", "title": "Negative pledge", "text": "Negative pledge\n\nNegative pledge is a provision in a contract which prohibits a party to the contract from creating any security interests over certain property specified in the provision.\n\nNegative pledges often appear in security documents, where they operate to prohibit the person who is granting the security interest from creating any other security interests over the same property, which might compete with (or rank \"pari passu\" with) the security of the first secured creditor under the security document in which the negative pledge appears.\n\nIn Australia, negative pledge lending took off after a substantial deal by Pioneer Concrete in 1978. It was a new way of lending, which allowed the banks to lend to corporations, something previously the domain of life insurers.\n\nNegative pledge clauses are almost universal in modern unsecured commercial loan documents. The purpose is to ensure that a borrower, having taken out an unsecured loan, cannot subsequently take out another loan with a different lender, securing the subsequent loan on the specified assets. If the borrower could do this, the original lender would be disadvantaged because the subsequent lender would have first call on the assets in an event of default.\n\n"}
{"id": "6475900", "url": "https://en.wikipedia.org/wiki?curid=6475900", "title": "Negative pregnant", "text": "Negative pregnant\n\nA negative pregnant (sometimes called a pregnant denial) refers to a denial which implies its affirmative opposite by seeming to deny only a qualification of the allegation and not the allegation itself. For example, \"I deny that I owe the plaintiff five hundred dollars\" might imply that the person making the statement owes some other sum of money, and was only denying that they owe that particular amount. \n\nA negative pregnant which appears in pleadings will often elicit a request for further and better particulars, or an interrogatory. In order to avoid a negative pregnant in the above example, one might instead say, \"I deny that I owe the plaintiff five hundred dollars, or any other sum of money.\"\n\nThe issue can also arise in the context of statutory interpretation. For instance, Justice Thurgood Marshall argues in his dissent to \"EEOC v. Aramco\" that the presumption against extraterritoriality is rebutted by a negative inference from the alien-exemption provision of Title VII of the Civil Rights Act of 1964, which states that Title VII \"shall not apply to an employer with respect to the employment of aliens outside any State.\" Marshall concludes that \"Absent an intention that Title VII \"apply\" 'outside any State,' Congress would have had no reason to craft this extraterritorial exemption. And because only discrimination against aliens is exempted, employers remain accountable for discrimination against United States citizens abroad.\" \n\n"}
{"id": "548558", "url": "https://en.wikipedia.org/wiki?curid=548558", "title": "Negative pressure", "text": "Negative pressure\n\nNegative pressure may refer to:\n\n"}
{"id": "8499571", "url": "https://en.wikipedia.org/wiki?curid=8499571", "title": "Negative probability", "text": "Negative probability\n\nThe probability of the outcome of an experiment is never negative, although a quasiprobability distribution allows a negative probability, or quasiprobability for some events. These distributions may apply to unobservable events or conditional probabilities.\n\nIn 1942, Paul Dirac wrote a paper \"The Physical Interpretation of Quantum Mechanics\" where he introduced the concept of negative energies and negative probabilities:\n\nThe idea of negative probabilities later received increased attention in physics and particularly in quantum mechanics. Richard Feynman argued that no one objects to using negative numbers in calculations: although \"minus three apples\" is not a valid concept in real life, negative money is valid. Similarly he argued how negative probabilities as well as probabilities above unity possibly could be useful in probability calculations.\n\nMark Burgin gives another example:\nNegative probabilities have later been suggested to solve several problems and paradoxes. \"Half-coins\" provide simple examples for negative probabilities. These strange coins were introduced in 2005 by Gábor J. Székely. Half-coins have infinitely many sides numbered with 0,1,2... and the positive even numbers are taken with negative probabilities. Two half-coins make a complete coin in the sense that if we flip two half-coins then the sum of the outcomes is 0 or 1 with probability 1/2 as if we simply flipped a fair coin.\n\nIn \"Convolution quotients of nonnegative definite functions\" and \"Algebraic Probability Theory\" Imre Z. Ruzsa and Gábor J. Székely proved that if a random variable X has a signed or quasi distribution where some of the probabilities are negative then one can always find two random variables, Y and Z, with ordinary (not signed / not quasi) distributions such that X, Y are independent and X + Y = Z in distribution. Thus X can always be interpreted as the \"difference\" of two ordinary random variables, Z and Y. If Y is interpreted as a measurement error of X and the observed value is Z then the negative regions of the distribution of X are masked / shielded by the error Y.\n\nAnother example known as the Wigner distribution in phase space, introduced by Eugene Wigner in 1932 to study quantum corrections, often leads to negative probabilities. For this reason, it has later been better known as the Wigner quasiprobability distribution. In 1945, M. S. Bartlett worked out the mathematical and logical consistency of such negative valuedness. The Wigner distribution function is routinely used in physics nowadays, and provides the cornerstone of phase-space quantization. Its negative features are an asset to the formalism, and often indicate quantum interference. The negative regions of the distribution are shielded from direct observation by the quantum uncertainty principle: typically, the moments of such a non-positive-semidefinite quasiprobability distribution are highly constrained, and prevent \"direct measurability\" of the negative regions of the distribution. But these regions contribute negatively and crucially to the expected values of observable quantities computed through such distributions, nevertheless.\n\nConsider a double slit experiment with photons. The two waves exiting each slit can be written as:\n\nformula_1\n\nand\n\nformula_2\n\nwhere \"d\" is the distance to the detection screen, \"a\" is the separation between the two slits, \"x\" the distance to the center of the screen, \"λ\" the wavelength and \"dN/dt\" is the number of photons emitted per unit time at the source. The amplitude of measuring a photon at distance \"x\" from the center of the screen is the sum of these two amplitudes coming out of each hole, and therefore the probability that a photon is detected at position \"x\" will be given by the square of this sum:\n\nformula_3,\n\nThis should strike you as the well-known probability rule:\n\nformula_4\n\nwhatever the last term means. Indeed, if one closes either one of the holes forcing the photon to go through the other slit, the two corresponding intensities are\n\nformula_5 and formula_6.\n\nBut now, if one does interpret each of these terms in this way, the joint probability takes negative values roughly every formula_7 !formula_8\n\nHowever, these negative probabilities are never observed as one can't isolate the cases in which the photon \"goes through both slits\", but can hint at the existence of anti-particles.\n\nNegative probabilities have more recently been applied to mathematical finance. In quantitative finance most probabilities are not real probabilities but pseudo probabilities, often what is known as risk neutral probabilities. These are not real probabilities, but theoretical \"probabilities\" under a series of assumptions that helps simplify calculations by allowing such pseudo probabilities to be negative in certain cases as first pointed out by Espen Gaarder Haug in 2004.\n\nA rigorous mathematical definition of negative probabilities and their properties was recently derived by Mark Burgin and Gunter Meissner (2011). The authors also show how negative probabilities can be applied to financial option pricing.\n\nThe concept of negative probabilities have also been proposed for reliable facility location models where facilities are subject to negatively correlated disruption risks when facility locations, customer allocation, and backup service plans are determined simultaneously. Li et al. proposed a virtual station structure that transforms a facility network with positively correlated disruptions into an equivalent one with added virtual supporting stations, and these virtual stations were subject to independent disruptions. This approach reduces a problem from one with correlated disruptions to one without. Xie et al. later showed how negatively correlated disruptions can also be addressed by the same modeling framework, except that a supporting station now may be disrupted with a “failure propensity” which\n\n“... inherits all mathematical characteristics and properties of a failure probability except that we allow it to be larger than 1...”\n\nThis finding paves ways for using compact mixed-integer mathematical programs to optimally design reliable location of service facilities under site-dependent and positive/negative/mixed disruption correlations.\n\nThe proposed “propensity” concept in Xie et al. turns out to be what Feynman and others referred to as “quasi-probability.” Note that when a quasi-probability is larger than 1, then 1 minus this value gives a negative probability. The truly physically verifiable observation is the facility disruption states, and there is no direct information on the station states or their corresponding probabilities. Hence the failure probability of the stations, interpreted as “probabilities of imagined intermediary states,” could exceed unity.\n\n"}
{"id": "237770", "url": "https://en.wikipedia.org/wiki?curid=237770", "title": "Negative resistance", "text": "Negative resistance\n\nIn electronics, negative resistance (NR) is a property of some electrical circuits and devices in which an increase in voltage across the device's terminals results in a decrease in electric current through it.\n\nThis is in contrast to an ordinary resistor in which an increase of applied voltage causes a proportional increase in current due to Ohm's law, resulting in a positive resistance. While a positive resistance consumes power from current passing through it, a negative resistance produces power. Under certain conditions it can increase the power of an electrical signal, amplifying it.\n\nNegative resistance is an uncommon property which occurs in a few nonlinear electronic components. In a nonlinear device, two types of resistance can be defined: 'static' or 'absolute resistance', the ratio of voltage to current formula_1, and \"differential resistance\", the ratio of a change in voltage to the resulting change in current formula_2. The term negative resistance means negative differential resistance (NDR), formula_3. In general, a negative differential resistance is a two-terminal component which can amplify, converting DC power applied to its terminals to AC output power to amplify an AC signal applied to the same terminals. They are used in electronic oscillators and amplifiers, particularly at microwave frequencies. Most microwave energy is produced with negative differential resistance devices. They can also have hysteresis and be bistable, and so are used in switching and memory circuits. Examples of devices with negative differential resistance are tunnel diodes, Gunn diodes, and gas discharge tubes such as neon lamps. In addition, circuits containing amplifying devices such as transistors and op amps with positive feedback can have negative differential resistance. These are used in oscillators and active filters.\n\nBecause they are nonlinear, negative resistance devices have a more complicated behavior than the positive \"ohmic\" resistances usually encountered in electric circuits. Unlike most positive resistances, negative resistance varies depending on the voltage or current applied to the device, and negative resistance devices can have negative resistance over only a limited portion of their voltage or current range. Therefore, there is no real \"negative resistor\" analogous to a positive resistor, which has a constant negative resistance over an arbitrarily wide range of current.\n\nThe resistance between two terminals of an electrical device or circuit is determined by its current–voltage (\"I–V\") curve (characteristic curve), giving the current formula_4 through it for any given voltage formula_5 across it. Most materials, including the ordinary (positive) resistances encountered in electrical circuits, obey Ohm's law; the current through them is proportional to the voltage over a wide range. So the \"I–V\" curve of an ohmic resistance is a straight line through the origin with positive slope. The resistance is the ratio of voltage to current, the inverse slope of the line (in \"I–V\" graphs where the voltage formula_5 is the independent variable) and is constant.\n\nNegative resistance occurs in a few nonlinear (nonohmic) devices. In a nonlinear component the \"I–V\" curve is not a straight line, so it does not obey Ohm's law. Resistance can still be defined, but the resistance is not constant; it varies with the voltage or current through the device. The resistance of such a nonlinear device can be defined in two ways, which are equal for ohmic resistances:\n\n\n\nNegative resistance, like positive resistance, is measured in ohms.\n\nConductance is the reciprocal of resistance. It is measured in siemens (formerly \"mho\") which is the conductance of a resistor with a resistance of one ohm. Each type of resistance defined above has a corresponding conductance\nIt can be seen that the conductance has the same sign as its corresponding resistance: a negative resistance will have a negative conductance while a positive resistance will have a positive conductance.\n\nOne way in which the different types of resistance can be distinguished is in the directions of current and electric power between a circuit and an electronic component. The illustrations below, with a rectangle representing the component attached to a circuit, summarize how the different types work:\nIn an electronic device, the differential resistance formula_14, the static resistance formula_15, or both, can be negative, so there are three categories of devices \"(fig. 2–4 above, and table)\" which could be called \"negative resistances\".\n\nThe term \"negative resistance\" almost always means negative \"differential\" resistance Negative differential resistance devices have unique capabilities: they can act as \"one-port amplifiers\", increasing the power of a time-varying signal applied to their port (terminals), or excite oscillations in a tuned circuit to make an oscillator. They can also have hysteresis. It is not possible for a device to have negative differential resistance without a power source, and these devices can be divided into two categories depending on whether they get their power from an internal source or from their port:\n\n\n\nOccasionally ordinary power sources are referred to as \"negative resistances\" (fig. 3 above). Although the \"static\" or \"absolute\" resistance formula_15 of active devices (power sources) can be considered negative (see Negative static resistance section below) most ordinary power sources (AC or DC), such as batteries, generators, and (non positive feedback) amplifiers, have positive \"differential\" resistance (their source resistance). Therefore, these devices cannot function as one-port amplifiers or have the other capabilities of negative differential resistances.\n\nElectronic components with negative differential resistance include these devices:\n\nElectric discharges through gases also exhibit negative differential resistance, including these devices\n\n\nIn addition, active circuits with negative differential resistance can also be built with amplifying devices like transistors and op amps, using feedback. A number of new experimental negative differential resistance materials and devices have been discovered in recent years. The physical processes which cause negative resistance are diverse, and each type of device has its own negative resistance characteristics, specified by its current–voltage curve.\n\nA point of some confusion is whether ordinary resistance (\"static\" or \"absolute\" resistance, formula_17) can be negative. In electronics, the term \"resistance\" is customarily applied only to passive materials and components – such as wires, resistors and diodes. These cannot have formula_18 as shown by Joule's law formula_19. A passive device consumes electric power, so from the passive sign convention formula_20. Therefore, from Joule's law formula_21. In other words, no material can conduct electric current better than a \"perfect\" conductor with zero resistance. For a passive device to have formula_22 would violate either conservation of energy or the second law of thermodynamics, \"(diagram)\". Therefore, some authors state that static resistance can never be negative.\n\nHowever it is easily shown that the ratio of voltage to current v/i at the terminals of any power source (AC or DC) is negative. For electric power (potential energy) to flow out of a device into the circuit, charge must flow through the device in the direction of increasing potential energy, conventional current (positive charge) must move from the negative to the positive terminal. So the direction of the instantaneous current is \"out\" of the positive terminal. This is opposite to the direction of current in a passive device defined by the passive sign convention so the current and voltage have opposite signs, and their ratio is negative\nThis can also be proved from Joule's law\nThis shows that power can flow out of a device into the circuit if and only if formula_18. Whether or not this quantity is referred to as \"resistance\" when negative is a matter of convention. The absolute resistance of power sources is negative, but this is not to be regarded as \"resistance\" in the same sense as positive resistances. The negative static resistance of a power source is a rather abstract and not very useful quantity, because it varies with the load. Due to conservation of energy it is always simply equal to the negative of the static resistance of the attached circuit \"(right)\".\n\nWork must be done on the charges by some source of energy in the device, to make them move toward the positive terminal against the electric field, so conservation of energy requires that negative static resistances have a source of power. The power may come from an internal source which converts some other form of energy to electric power as in a battery or generator, or from a separate connection to an external power supply circuit as in an amplifying device like a transistor, vacuum tube, or op amp.\n\nA circuit cannot have negative static resistance (be active) over an infinite voltage or current range, because it would have to be able to produce infinite power. Any active circuit or device with a finite power source is \"eventually passive\". This property means if a large enough external voltage or current of either polarity is applied to it, its static resistance becomes positive and it consumes power\n\nTherefore, the ends of the \"I–V\" curve will eventually turn and enter the 1st and 3rd quadrants. Thus the range of the curve having negative static resistance is limited, confined to a region around the origin. For example, applying a voltage to a generator or battery \"(graph, above)\" greater than its open-circuit voltage will reverse the direction of current flow, making its static resistance positive so it consumes power. Similarly, applying a voltage to the negative impedance converter below greater than its power supply voltage \"V\" will cause the amplifier to saturate, also making its resistance positive.\n\nIn a device or circuit with negative differential resistance (NDR), in some part of the \"I–V\" curve the current decreases as the voltage increases:\nThe \"I–V\" curve is nonmonotonic (having peaks and troughs) with regions of negative slope representing negative differential resistance.\n\nPassive negative differential resistances have positive \"static\" resistance; they consume net power. Therefore, the \"I–V\" curve is confined to the 1st and 3rd quadrants of the graph, and passes through the origin. This requirement means (excluding some asymptotic cases) that the region(s) of negative resistance must be limited, and surrounded by regions of positive resistance, and cannot include the origin.\n\nNegative differential resistances can be classified into two types:\n\n\nMost devices have a single negative resistance region. However devices with multiple separate negative resistance regions can also be fabricated. These can have more than two stable states, and are of interest for use in digital circuits to implement multivalued logic.\n\nAn intrinsic parameter used to compare different devices is the \"peak-to-valley current ratio\" (PVR), the ratio of the current at the top of the negative resistance region to the current at the bottom \"(see graphs, above)\":\nThe larger this is, the larger the potential AC output for a given DC bias current, and therefore the greater the efficiency\n\nA negative differential resistance device can amplify an AC signal applied to it if the signal is biased with a DC voltage or current to lie within the negative resistance region of its \"I–V\" curve.\n\nThe tunnel diode circuit \"(see diagram)\" is an example. The tunnel diode \"TD\" has voltage controlled negative differential resistance. The battery formula_30 adds a constant voltage (bias) across the diode so it operates in its negative resistance range, and provides power to amplify the signal. Suppose the negative resistance at the bias point is formula_31. For stability formula_32 must be less than formula_33. Using the formula for a voltage divider, the AC output voltage is\nIn a normal voltage divider, the resistance of each branch is less than the resistance of the whole, so the output voltage is less than the input. Here, due to the negative resistance, the total AC resistance formula_36 is less than the resistance of the diode alone formula_33 so the AC output voltage formula_38 is greater than the input formula_39. The voltage gain formula_40 is greater than one, and increases without limit as formula_32 approaches formula_33.\n\nThe diagrams illustrate how a biased negative differential resistance device can increase the power of a signal applied to it, amplifying it, although it only has two terminals. Due to the superposition principle the voltage and current at the device's terminals can be divided into a DC bias component and an AC component .\nSince a positive change in voltage formula_45 causes a \"negative\" change in current formula_46, the AC current and voltage in the device are 180° out of phase. This means in the AC equivalent circuit \"(right)\", the instantaneous AC current Δ\"i\" flows through the device in the direction of \"increasing\" AC potential Δ\"v\", as it would in a generator. Therefore, the AC power dissipation is \"negative\"; AC power is produced by the device and flows into the external circuit.\nWith the proper external circuit, the device can increase the AC signal power delivered to a load, serving as an amplifier, or excite oscillations in a resonant circuit to make an oscillator. Unlike in a two port amplifying device such as a transistor or op amp, the amplified signal leaves the device through the same two terminals (port) as the input signal enters.\n\nIn a passive device, the AC power produced comes from the input DC bias current, the device absorbs DC power, some of which is converted to AC power by the nonlinearity of the device, amplifying the applied signal. Therefore, the output power is limited by the bias power\nThe negative differential resistance region cannot include the origin, because it would then be able to amplify a signal with no applied DC bias current, producing AC power with no power input. The device also dissipates some power as heat, equal to the difference between the DC power in and the AC power out.\n\nThe device may also have reactance and therefore the phase difference between current and voltage may differ from 180° and may vary with frequency. As long as the real component of the impedance is negative (phase angle between 90° and 270°), the device will have negative resistance and can amplify.\n\nThe maximum AC output power is limited by size of the negative resistance region (formula_49 in graphs above)\n\nThe reason that the output signal can leave a negative resistance through the same port that the input signal enters is that from transmission line theory, the AC voltage or current at the terminals of a component can be divided into two oppositely moving waves, the \"incident wave\" formula_51, which travels toward the device, and the \"reflected wave\" formula_52, which travels away from the device. A negative differential resistance in a circuit can amplify if the magnitude of its reflection coefficient formula_53, the ratio of the reflected wave to the incident wave, is greater than one.\nThe \"reflected\" (output) signal has larger amplitude than the incident; the device has \"reflection gain\". The reflection coefficient is determined by the AC impedance of the negative resistance device, formula_56, and the impedance of the circuit attached to it, formula_57. If formula_58 and formula_59 then formula_60 and the device will amplify. On the Smith chart, a graphical aide widely used in the design of high frequency circuits, negative differential resistance corresponds to points outside the unit circle formula_61, the boundary of the conventional chart, so special \"expanded\" charts must be used.\n\nBecause it is nonlinear, a circuit with negative differential resistance can have multiple equilibrium points (possible DC operating points), which lie on the \"I–V\" curve. An equilibrium point will be stable, so the circuit converges to it within some neighborhood of the point, if its poles are in the left half of the s plane (LHP), while a point is unstable, causing the circuit to oscillate or \"latch up\" (converge to another point), if its poles are on the \"jω\" axis or right half plane (RHP), respectively. In contrast, a linear circuit has a single equilibrium point that may be stable or unstable. The equilibrium points are determined by the DC bias circuit, and their stability is determined by the AC impedance formula_62 of the external circuit.\nHowever, because of the different shapes of the curves, the condition for stability is different for VCNR and CCNR types of negative resistance:\n\n\nFor general negative resistance circuits with reactance, the stability must be determined by standard tests like the Nyquist stability criterion. Alternatively, in high frequency circuit design, the values of formula_70 for which the circuit is stable are determined by a graphical technique using \"stability circles\" on a Smith chart.\n\nFor simple nonreactive negative resistance devices with formula_71 and formula_72 the different operating regions of the device can be illustrated by load lines on the \"I–V\" curve \"(see graphs)\".\n\nThe DC load line (DCL) is a straight line determined by the DC bias circuit, with equation\nwhere formula_74 is the DC bias supply voltage and R is the resistance of the supply. The possible DC operating point(s) (Q points) occur where the DC load line intersects the \"I–V\" curve. For stability\nThe AC load line (\"L\" − \"L\") is a straight line through the Q point whose slope is the differential (AC) resistance formula_75 facing the device. Increasing formula_75 rotates the load line counterclockwise. The circuit operates in one of three possible regions \"(see diagrams)\", depending on formula_75.\n\nIn addition to the passive devices with intrinsic negative differential resistance above, circuits with amplifying devices like transistors or op amps can have negative resistance at their ports. The input or output impedance of an amplifier with enough positive feedback applied to it can be negative. If formula_84 is the input resistance of the amplifier without feedback, formula_85 is the amplifier gain, and formula_86 is the transfer function of the feedback path, the input resistance with positive shunt feedback is\nSo if the loop gain formula_88 is greater than one, formula_89 will be negative. The circuit acts like a \"negative linear resistor\" over a limited range, with \"I–V\" curve having a straight line segment through the origin with negative slope \"(see graphs)\". It has both negative differential resistance and is active\nand thus obeys Ohm's law as if it had a negative value of resistance \"−R\", over its linear range (such amplifiers can also have more complicated negative resistance \"I–V\" curves that do not pass through the origin).\n\nIn circuit theory these are called \"active resistors\". Applying a voltage across the terminals causes a proportional current \"out\" of the positive terminal, the opposite of an ordinary resistor. For example, connecting a battery to the terminals would cause the battery to charge rather than discharge.\n\nConsidered as one-port devices, these circuits function similarly to the passive negative differential resistance components above, and like them can be used to make one-port amplifiers and oscillators with the advantages that:\nThe \"I–V\" curve can have voltage-controlled (\"N\" type) or current-controlled (\"S\" type) negative resistance, depending on whether the feedback loop is connected in \"shunt\" or \"series\".\n\nNegative reactances \"(below)\" can also be created, so feedback circuits can be used to create \"active\" linear circuit elements, resistors, capacitors, and inductors, with negative values. They are widely used in active filters because they can create transfer functions that cannot be realized with positive circuit elements. Examples of circuits with this type of negative resistance are the negative impedance converter (NIC), gyrator, Deboo integrator, frequency dependent negative resistance (FDNR), and generalized immittance converter (GIC).\n\nIf an LC circuit is connected across the input of a positive feedback amplifier like that above, the negative differential input resistance formula_91 can cancel the positive loss resistance formula_92 inherent in the tuned circuit. If formula_93 this will create in effect a tuned circuit with zero AC resistance (poles on the \"jω\" axis). Spontaneous oscillation will be excited in the tuned circuit at its resonant frequency, sustained by the power from the amplifier. This is how feedback oscillators such as Hartley or Colpitts oscillators work. This negative resistance model is an alternate way of analyzing feedback oscillator operation. \"All\" linear oscillator circuits have negative resistance although in most feedback oscillators the tuned circuit is an integral part of the feedback network, so the circuit does not have negative resistance at all frequencies but only near the oscillation frequency.\n\nA tuned circuit connected to a negative resistance which cancels some but not all of its parasitic loss resistance (so formula_94) will not oscillate, but the negative resistance will decrease the damping in the circuit (moving its poles toward the \"jω\" axis), increasing its Q factor so it has a narrower bandwidth and more selectivity. Q enhancement, also called \"regeneration\", was first used in the regenerative radio receiver invented by Edwin Armstrong in 1912 and later in \"Q multipliers\". It is widely used in active filters. For example, RF integrated circuits use \"integrated inductors\" to save space, consisting of a spiral conductor fabricated on chip. These have high losses and low Q, so to create high Q tuned circuits their Q is increased by applying negative resistance.\n\nCircuits which exhibit chaotic behavior can be considered quasi-periodic or nonperiodic oscillators, and like all oscillators require a negative resistance in the circuit to provide power. Chua's circuit, a simple nonlinear circuit widely used as the standard example of a chaotic system, requires a nonlinear active resistor component, sometimes called Chua's diode. This is usually synthesized using a negative impedance converter circuit.\n\nA common example of an \"active resistance\" circuit is the negative impedance converter (NIC) shown in the diagram. The two resistors formula_95 and the op amp constitute a negative feedback non-inverting amplifier with gain of 2. The output voltage of the op-amp is\nSo if a voltage formula_5 is applied to the input, the same voltage is applied \"backwards\" across formula_98, causing current to flow through it out of the input. The current is\nSo the input impedance to the circuit is\nThe circuit converts the impedance formula_98 to its negative. If formula_98 is a resistor of value formula_103, within the linear range of the op amp formula_104 the input impedance acts like a linear \"negative resistor\" of value formula_105. The input port of the circuit is connected into another circuit as if it was a component. An NIC can cancel undesired positive resistance in another circuit, for example they were originally developed to cancel resistance in telephone cables, serving as repeaters.\n\nBy replacing formula_98 in the above circuit with a capacitor , negative capacitances and inductances can also be synthesized. A negative capacitance will have an \"I–V\" relation and an impedance formula_107 of\nwhere formula_109. Applying a positive current to a negative capacitance will cause it to \"discharge\"; its voltage will \"decrease\". Similarly, a negative inductance will have an \"I–V\" characteristic and impedance formula_110 of\nA circuit having negative capacitance or inductance can be used to cancel unwanted positive capacitance or inductance in another circuit. NIC circuits were used to cancel reactance on telephone cables.\n\nThere is also another way of looking at them. In a negative capacitance the current will be 180° opposite in phase to the current in a positive capacitance. Instead of leading the voltage by 90° it will lag the voltage by 90°, as in an inductor. Therefore, a negative capacitance acts like an inductance in which the impedance has a reverse dependence on frequency ω; decreasing instead of increasing like a real inductance Similarly a negative inductance acts like a capacitance that has an impedance which increases with frequency. Negative capacitances and inductances are \"non-Foster\" circuits which violate Foster's reactance theorem. One application being researched is to create an active matching network which could match an antenna to a transmission line over a broad range of frequencies, rather than just a single frequency as with current networks. This would allow the creation of small compact antennas that would have broad bandwidth, exceeding the Chu–Harrington limit.\n\nNegative differential resistance devices are widely used to make electronic oscillators. In a negative resistance oscillator, a negative differential resistance device such as an IMPATT diode, Gunn diode, or microwave vacuum tube is connected across an electrical resonator such as an LC circuit, a quartz crystal, dielectric resonator or cavity resonator with a DC source to bias the device into its negative resistance region and provide power. A resonator such as an LC circuit is \"almost\" an oscillator; it can store oscillating electrical energy, but because all resonators have internal resistance or other losses, the oscillations are damped and decay to zero. The negative resistance cancels the positive resistance of the resonator, creating in effect a lossless resonator, in which spontaneous continuous oscillations occur at the resonator's resonant frequency.\n\nNegative resistance oscillators are mainly used at high frequencies in the microwave range or above, since feedback oscillators function poorly at these frequencies. Microwave diodes are used in low- to medium-power oscillators for applications such as radar speed guns, and local oscillators for satellite receivers. They are a widely used source of microwave energy, and virtually the only solid-state source of millimeter wave and terahertz energy Negative resistance microwave vacuum tubes such as magnetrons produce higher power outputs, in such applications as radar transmitters and microwave ovens. Lower frequency relaxation oscillators can be made with UJTs and gas-discharge lamps such as neon lamps.\n\nThe negative resistance oscillator model is not limited to one-port devices like diodes but can also be applied to feedback oscillator circuits with two port devices such as transistors and tubes. In addition, in modern high frequency oscillators, transistors are increasingly used as one-port negative resistance devices like diodes. At microwave frequencies, transistors with certain loads applied to one port can become unstable due to internal feedback and show negative resistance at the other port. So high frequency transistor oscillators are designed by applying a reactive load to one port to give the transistor negative resistance, and connecting the other port across a resonator to make a negative resistance oscillator as described below.\n\nThe common Gunn diode oscillator \"(circuit diagrams)\" illustrates how negative resistance oscillators work. The diode \"D\" has voltage controlled (\"N\" type) negative resistance and the voltage source formula_112 biases it into its negative resistance region where its differential resistance is formula_113. The choke \"RFC\" prevents AC current from flowing through the bias source. formula_103 is the equivalent resistance due to damping and losses in the series tuned circuit formula_115, plus any load resistance. Analyzing the AC circuit with Kirchhoff's Voltage Law gives a differential equation for formula_116, the AC current\nSolving this equation gives a solution of the form\nThis shows that the current through the circuit, formula_116, varies with time about the DC Q point, formula_121. When started from a nonzero initial current formula_122 the current oscillates sinusoidally at the resonant frequency ω of the tuned circuit, with amplitude either constant, increasing, or decreasing exponentially, depending on the value of α. Whether the circuit can sustain steady oscillations depends on the balance between formula_103 and formula_124, the positive and negative resistance in the circuit:\n\nPractical oscillators are designed in region (3) above, with net negative resistance, to get oscillations started. A widely used rule of thumb is to make formula_129. When the power is turned on, electrical noise in the circuit provides a signal formula_130 to start spontaneous oscillations, which grow exponentially. However, the oscillations cannot grow forever; the nonlinearity of the diode eventually limits the amplitude.\n\nAt large amplitudes the circuit is nonlinear, so the linear analysis above does not strictly apply and differential resistance is undefined; but the circuit can be understood by considering formula_124 to be the \"average\" resistance over the cycle. As the amplitude of the sine wave exceeds the width of the negative resistance region and the voltage swing extends into regions of the curve with positive differential resistance, the average negative differential resistance formula_124 becomes smaller, and thus the total resistance formula_133 and the damping formula_134 becomes less negative and eventually turns positive. Therefore, the oscillations will stabilize at the amplitude at which the damping becomes zero, which is when formula_135.\n\nGunn diodes have negative resistance in the range −5 to −25 ohms. In oscillators where formula_103 is close to formula_124; just small enough to allow the oscillator to start, the voltage swing will be mostly limited to the linear portion of the \"I–V\" curve, the output waveform will be nearly sinusoidal and the frequency will be most stable. In circuits in which formula_103 is far below formula_124, the swing extends further into the nonlinear part of the curve, the clipping distortion of the output sine wave is more severe, and the frequency will be increasingly dependent on the supply voltage.\n\nNegative resistance oscillator circuits can be divided into two types, which are used with the two types of negative differential resistance – voltage controlled (VCNR), and current controlled (CCNR)\n\nMost oscillators are more complicated than the Gunn diode example, since both the active device and the load may have reactance (\"X\") as well as resistance (\"R\"). Modern negative resistance oscillators are designed by a frequency domain technique due to K. Kurokawa. The circuit diagram is imagined to be divided by a \"reference plane\" \"(red)\" which separates the negative resistance part, the active device, from the positive resistance part, the resonant circuit and output load \"(right)\". The complex impedance of the negative resistance part formula_140 depends on frequency \"ω\" but is also nonlinear, in general declining with the amplitude of the AC oscillation current \"I\"; while the resonator part formula_141 is linear, depending only on frequency. The circuit equation is formula_142 so it will only oscillate (have nonzero \"I\") at the frequency \"ω\" and amplitude \"I\" for which the total impedance formula_143 is zero. This means the magnitude of the negative and positive resistances must be equal, and the reactances must be conjugate\nFor steady-state oscillation the equal sign applies. During startup the inequality applies, because the circuit must have excess negative resistance for oscillations to start.\n\nAlternately, the condition for oscillation can be expressed using the reflection coefficient. The voltage waveform at the reference plane can be divided into a component \"V\" travelling toward the negative resistance device and a component \"V\" travelling in the opposite direction, toward the resonator part. The reflection coefficient of the active device formula_146 is greater than one, while that of the resonator part formula_147 is less than one. During operation the waves are reflected back and forth in a round trip so the circuit will oscillate only if\nAs above, the equality gives the condition for steady oscillation, while the inequality is required during startup to provide excess negative resistance. The above conditions are analogous to the Barkhausen criterion for feedback oscillators; they are necessary but not sufficient, so there are some circuits that satisfy the equations but do not oscillate. Kurokawa also derived more complicated sufficient conditions, which are often used instead.\n\nNegative differential resistance devices such as Gunn and IMPATT diodes are also used to make amplifiers, particularly at microwave frequencies, but not as commonly as oscillators. Because negative resistance devices have only one \"port\" (two terminals), unlike two-port devices such as transistors, the outgoing amplified signal has to leave the device by the same terminals as the incoming signal enters it. Without some way of separating the two signals, a negative resistance amplifier is \"bilateral\"; it amplifies in both directions, so it suffers from sensitivity to load impedance and feedback problems. To separate the input and output signals, many negative resistance amplifiers use nonreciprocal devices such as isolators and directional couplers.\n\nOne widely used circuit is the \"reflection amplifier\" in which the separation is accomplished by a \"circulator\". A circulator is a nonreciprocal solid-state component with three ports (connectors) which transfers a signal applied to one port to the next in only one direction, port 1 to port 2, 2 to 3, and 3 to 1. In the reflection amplifier diagram the input signal is applied to port 1, a biased VCNR negative resistance diode \"N\" is attached through a filter \"F\" to port 2, and the output circuit is attached to port 3. The input signal is passed from port 1 to the diode at port 2, but the outgoing \"reflected\" amplified signal from the diode is routed to port 3, so there is little coupling from output to input. The characteristic impedance formula_149 of the input and output transmission lines, usually 50Ω, is matched to the port impedance of the circulator. The purpose of the filter \"F\" is to present the correct impedance to the diode to set the gain. At radio frequencies NR diodes are not pure resistive loads and have reactance, so a second purpose of the filter is to cancel the diode reactance with a conjugate reactance to prevent standing waves.\n\nThe filter has only reactive components and so does not absorb any power itself, so power is passed between the diode and the ports without loss. The input signal power to the diode is\nThe output power from the diode is\nSo the power gain formula_152 of the amplifier is the square of the reflection coefficient\n\nformula_156 is the negative resistance of the diode −r. Assuming the filter is matched to the diode so formula_157 then the gain is\nThe VCNR reflection amplifier above is stable for formula_159. while a CCNR amplifier is stable for formula_160. It can be seen that the reflection amplifier can have unlimited gain, approaching infinity as formula_95 approaches the point of oscillation at formula_124. This is a characteristic of all NR amplifiers, contrasting with the behavior of two-port amplifiers, which generally have limited gain but are often unconditionally stable. In practice the gain is limited by the backward \"leakage\" coupling between circulator ports.\n\nMasers and parametric amplifiers are extremely low noise NR amplifiers that are also implemented as reflection amplifiers; they are used in applications like radio telescopes.\n\nNegative differential resistance devices are also used in switching circuits in which the device operates nonlinearly, changing abruptly from one state to another, with hysteresis. The advantage of using a negative resistance device is that a relaxation oscillator, flip-flop or memory cell can be built with a single active device, whereas the standard logic circuit for these functions, the Eccles-Jordan multivibrator, requires two active devices (transistors). Three switching circuits built with negative resistances are\n\nSome instances of neurons display regions of negative slope conductances (RNSC) in voltage-clamp experiments. The negative resistance here is implied were one to consider the neuron a typical Hodgkin–Huxley style circuit model.\n\nNegative resistance was first recognized during investigations of electric arcs, which were used for lighting during the 19th century. In 1881 Alfred Niaudet had observed that the voltage across arc electrodes decreased temporarily as the arc current increased, but many researchers thought this was a secondary effect due to temperature. The term \"negative resistance\" was applied by some to this effect, but the term was controversial because it was known that the resistance of a passive device could not be negative. Beginning in 1895 Hertha Ayrton, extending her husband William's research with a series of meticulous experiments measuring the \"I–V\" curve of arcs, established that the curve had regions of negative slope, igniting controversy. Frith and Rodgers in 1896 with the support of the Ayrtons introduced the concept of \"differential\" resistance, \"dv/di\", and it was slowly accepted that arcs had negative differential resistance. In recognition of her research, Hertha Ayrton became the first woman voted for induction into the Institute of Electrical Engineers.\n\nGeorge Francis FitzGerald first realized in 1892 that if the damping resistance in a resonant circuit could be made zero or negative, it would produce continuous oscillations. In the same year Elihu Thomson built a negative resistance oscillator by connecting an LC circuit to the electrodes of an arc, perhaps the first example of an electronic oscillator. William Duddell, a student of Ayrton at London Central Technical College, brought Thomson's arc oscillator to public attention. Due to its negative resistance, the current through an arc was unstable, and arc lights would often produce hissing, humming, or even howling noises. In 1899, investigating this effect, Duddell connected an LC circuit across an arc and the negative resistance excited oscillations in the tuned circuit, producing a musical tone from the arc. To demonstrate his invention Duddell wired several tuned circuits to an arc and played a tune on it. Duddell's \"singing arc\" oscillator was limited to audio frequencies. However, in 1903 Danish engineers Valdemar Poulsen and P. O. Pederson increased the frequency into the radio range by operating the arc in a hydrogen atmosphere in a magnetic field, inventing the Poulsen arc radio transmitter, which was widely used until the 1920s.\n\nBy the early 20th century, although the physical causes of negative resistance were not understood, engineers knew it could generate oscillations and had begun to apply it. Heinrich Barkhausen in 1907 showed that oscillators must have negative resistance. Ernst Ruhmer and Adolf Pieper discovered that mercury vapor lamps could produce oscillations, and by 1912 AT&T had used them to build amplifying repeaters for telephone lines.\n\nIn 1918 Albert Hull at GE discovered that vacuum tubes could have negative resistance in parts of their operating ranges, due to a phenomenon called secondary emission. In a vacuum tube when electrons strike the plate electrode they can knock additional electrons out of the surface into the tube. This represents a current \"away\" from the plate, reducing the plate current. Under certain conditions increasing the plate voltage causes a \"decrease\" in plate current. By connecting an LC circuit to the tube Hull created an oscillator, the dynatron oscillator. Other negative resistance tube oscillators followed, such as the magnetron invented by Hull in 1920.\n\nThe negative impedance converter originated from work by Marius Latour around 1920. He was also one of the first to report negative capacitance and inductance. A decade later, vacuum tube NICs were developed as telephone line repeaters at Bell Labs by George Crisson and others, which made transcontinental telephone service possible. Transistor NICs, pioneered by Linvill in 1953, initiated a great increase in interest in NICs and many new circuits and applications developed.\n\nNegative differential resistance in semiconductors was observed around 1909 in the first point-contact junction diodes, called cat's whisker detectors, by researchers such as William Henry Eccles and G. W. Pickard. They noticed that when junctions were biased with a DC voltage to improve their sensitivity as radio detectors, they would sometimes break into spontaneous oscillations. However the effect was not pursued.\n\nThe first person to exploit negative resistance diodes practically was Russian radio researcher Oleg Losev, who in 1922 discovered negative differential resistance in biased zincite (zinc oxide) point contact junctions. He used these to build solid-state amplifiers, oscillators, and amplifying and regenerative radio receivers, 25 years before the invention of the transistor. Later he even built a superheterodyne receiver. However his achievements were overlooked because of the success of vacuum tube technology. After ten years he abandoned research into this technology (dubbed \"Crystodyne\" by Hugo Gernsback), and it was forgotten.\n\nThe first widely used solid-state negative resistance device was the tunnel diode, invented in 1957 by Japanese physicist Leo Esaki. Because they have lower parasitic capacitance than vacuum tubes due to their small junction size, diodes can function at higher frequencies, and tunnel diode oscillators proved able to produce power at microwave frequencies, above the range of ordinary vacuum tube oscillators. Its invention set off a search for other negative resistance semiconductor devices for use as microwave oscillators, resulting in the discovery of the IMPATT diode, Gunn diode, TRAPATT diode, and others. In 1969 Kurokawa derived conditions for stability in negative resistance circuits. Currently negative differential resistance diode oscillators are the most widely used sources of microwave energy, and many new negative resistance devices have been discovered in recent decades.\n\n"}
{"id": "244067", "url": "https://en.wikipedia.org/wiki?curid=244067", "title": "Negative sign (astrology)", "text": "Negative sign (astrology)\n\nIn astrology, a negative, ceptive, dispassive, yin, nocturnal or feminine sign refers to any of the six even-numbered signs of the zodiac: Taurus, Cancer, Virgo, Scorpio, Capricorn or Pisces.\n\nThese signs constitute the earth and water triplicities.\n\nIn astrology there are two groups: positive and negative. These two groups also\ninclude six individual signs that are called zodiac signs. The negative signs associated\nwith the zodiac are Taurus, Cancer, Virgo, Scorpio, Capricorn, and Pisces. The positive\nsigns associated with the zodiac are Aries, Gemini, Leo, Libra, Sagittarius, and\nAquarius. The twelve signs are divided into two\ngroups based upon one's exact time and place of birth. The negative and positive signs\nalternate, starting with Aries as positive and Taurus as negative and continuing this pattern through the list of zodiac signs.\n\nThe signs negative and positive are referred to as a negative-sunsign or a\npositive-sunsign. There are many terms used in astrology to differentiate the\ntwo groups. In Chinese astrology, the two groups are categorized as yin and yang, corresponding respectively to negative and positive. Standen explains that different astrologers may refer to signs by different names. For example, an astrologer may refer to positive signs as masculine and negative signs as feminine. Each sign is divided into two main\ntypes: active and passive. As a general rule, the active type will be called masculine and the passive type will be called feminine.\n\nZodiac signs associated with the negative are Taurus, Cancer, Virgo, Scorpio, Capricorn, and Pisces. Note that there is no value judgment attached to the terms negative or positive. They may be likened to polarities in a magnet: one side is positive and one side is negative. Neither side is \"good\" nor \"bad\"—they are merely different.\n\nThe sunsign effect is a pattern of alternating high and low extraversion-scores for\nthe 12 signs. Introvert is a person who gains energy when alone and spends it when with other people. Extravert is a person who gains energy from socialization, expending it when alone. Jan J.F. van Rooij did an experiment on Introversion-Extraversion: astrology\nversus psychology, to see if those in the negative sunsign were introverted and those in\nthe positive sunsign were negative. Van Rooijs did this experiment on those with\nastrological knowledge and on those with no astrological knowledge. His results showed\nthat negative sunsign people are not that responsive to the outer world and are\naccordingly not influenced that easily by astrological information. They rely more on\ntheir own ideas and feelings, thus proving his point that people who are born with the\nnegative sunsign are introverted, and those born with the positive sunsign are\nextroverted.\n\nEarth and Water are the elements attached to those who are in the negative sign.\nEarth is the element of Taurus, Virgo, and Capricorn. Water is the element of Cancer,\nScorpio, and Pisces. Elements are the basic traits of the signs. They reveal the\nfundamental aspects of the personality.\n“Water signs are attuned to waves of emotion, and often seem to have a built-in\nsonar for reading a mood. This gives them a special sensitivity in relationships, knowing\nwhen to show warmth and when to hold back. At their best, they are a healing force that\nbring people together -- at their worst, they are psychic vampires, able to manipulate\nand drain the life force of the closest to them”. Hall explains that water signs are\nmore in tune with their emotions and are comfortable showing them. Water signs bring a\ncertain presence to a situation; they seek out the problem and fix it.\n“Earth signs are sensual, meaning they engage with life through the five senses.\nIt takes time to sense the dense physical world, and earth signs can operate at a slower,\nmore thorough pace than the other elements. Theyʼre oriented toward whatʼs real, and\noften this makes them very productive, able to create tangible results.”\nEarth signs are described by Hall as earthy people. These signs focus on the things\nthat connect us to the earth: things which bring peace, as opposed to focusing on the material world.\n\n"}
{"id": "362728", "url": "https://en.wikipedia.org/wiki?curid=362728", "title": "Negative temperature", "text": "Negative temperature\n\nIn quantum thermodynamics, certain systems can achieve negative temperature; that is, their temperature can be expressed as a negative quantity on the Kelvin or Rankine scales.\n\nA system with a truly negative temperature on the Kelvin scale is \"hotter\" than any system with a positive temperature. If a negative-temperature system and a positive-temperature system come in contact, heat will flow from the negative- to the positive-temperature system. A standard example of such a system is population inversion in laser physics.\n\nTemperature is loosely interpreted as the average kinetic energy of the system's particles. The existence of negative temperature, let alone negative temperature representing \"hotter\" systems than positive temperature, would seem paradoxical in this interpretation. \nThe paradox is resolved by considering the more rigorous definition of thermodynamic temperature as the tradeoff between internal energy and entropy\ncontained in the system, with \"coldness\", the \"reciprocal\" of temperature, being the more fundamental quantity. \nSystems with a positive temperature will increase in entropy as one adds energy to the system, while \nsystems with a negative temperature will decrease in entropy as one adds energy to the system.\n\nClassical thermodynamic systems cannot achieve negative temperatures: adding heat always increases their entropy. \nThe possibility of a decrease in entropy as energy increases requires the system to \"saturate\" in entropy. This is only possible if the number of high energy states is limited. In classical Boltzmann statistics, the number of high energy states is unlimited (particle speeds can in principle be increased indefinitely).\nSystems bounded by a maximum amount of energy are generally forbidden in classical mechanics, and the phenomenon of negative temperature is strictly a \nquantum mechanical phenomenon. Some systems, however (see the examples below), have a maximum amount of energy that they can hold, and as they approach that maximum energy their entropy actually begins to decrease.\n\nThe definition of thermodynamic temperature as a function of the change in the system's entropy under reversible heat transfer \nformula_1:\nEntropy being a state function, the integral of over any cyclical process is zero.\nFor a system in which the entropy is purely a function of the system's energy , the temperature can be defined as:\n\nEquivalently, thermodynamic beta, or \"coldness\", is defined as \nwhere is the Boltzmann constant.\n\nNote that in classical thermodynamics, is defined in terms of temperature. This is reversed here, is the statistical entropy, a function of the possibly microstates of the system, and temperature conveys information on the distribution of energy levels among the possible microstates.\nFor systems with many degrees of freedom, the statistical and thermodynamic definitions of entropy are generally consistent with each other. \nHowever, for small systems and systems where the number of states decreases with energy, the definitions of statistical entropy and thermodynamic entropy are not necessarily consistent, and the temperatures derived from these entropies are different. \n\nSome theorists have proposed using an alternate definition of entropy originally proposed by Gibbs as a way to resolve these inconsistencies, although this new definition would create other inconsistencies.\n\nNegative temperatures can only exist in a system where there are a limited number of energy states (see below). As the temperature is increased on such a system, particles move into higher and higher energy states, and as the temperature increases, the number of particles in the lower energy states and in the higher energy states approaches equality. (This is a consequence of the definition of temperature in statistical mechanics for systems with limited states.) By injecting energy into these systems in the right fashion, it is possible to create a system in which there are more particles in the higher energy states than in the lower ones. The system can then be characterised as having a negative temperature.\n\nA substance with a negative temperature is not colder than absolute zero, but rather it is hotter than infinite temperature. As Kittel and Kroemer (p. 462) put it, \"The temperature scale from cold to hot runs:\n\nThe corresponding inverse temperature scale, for the quantity \"β\" = 1/\"kT\" (where \"k\" is Boltzmann's constant), runs continuously from low energy to high as +∞, … , 0, …, −∞. Because it avoids the abrupt jump from +∞ to −∞, β is considered more natural than \"T\". Although a system can have multiple negative temperature regions and thus have -∞ to +∞ discontinuities.\n\nIn many familiar physical systems, temperature is associated to the kinetic energy of atoms. Since there is no upper bound on the momentum of an atom, there is no upper bound to the number of energy states available when more energy is added, and therefore no way to get to a negative temperature. However, in statistical mechanics, temperature can correspond to other degrees of freedom than just kinetic energy (see below).\n\nThe distribution of energy among the various translational, vibrational, rotational, electronic, and nuclear modes of a system determines the macroscopic temperature. In a \"normal\" system, thermal energy is constantly being exchanged between the various modes.\n\nHowever, in some situations, it is possible to isolate one or more of the modes. In practice, the isolated modes still exchange energy with the other modes, but the time scale of this exchange is much slower than for the exchanges within the isolated mode. One example is the case of nuclear spins in a strong external magnetic field. In this case, energy flows fairly rapidly among the spin states of interacting atoms, but energy transfer between the nuclear spins and other modes is relatively slow. Since the energy flow is predominantly within the spin system, it makes sense to think of a spin temperature that is distinct from the temperature associated to other modes.\n\nA definition of temperature can be based on the relationship:\n\nThe relationship suggests that a \"positive temperature\" corresponds to the condition where entropy, \"S\", increases as thermal energy, \"q\", is added to the system. This is the \"normal\" condition in the macroscopic world, and is always the case for the translational, vibrational, rotational, and non-spin related electronic and nuclear modes. The reason for this is that there are an infinite number of these types of modes, and adding more heat to the system increases the number of modes that are energetically accessible, and thus increases the entropy.\n\nThe simplest example, albeit a rather nonphysical one, is to consider a system of \"N\" particles, each of which can take an energy of either \"+ε\" or \"-ε\" but are otherwise noninteracting. This can be understood as a limit of the Ising model in which the interaction term becomes negligible. The total energy of the system is\n\nwhere \"σ\" is the sign of the \"i\"th particle and \"j\" is the number of particles with positive energy minus the number of particles with negative energy. From elementary combinatorics, the total number of microstates with this amount of energy is a binomial coefficient:\n\nBy the fundamental assumption of statistical mechanics, the entropy of this microcanonical ensemble is\n\nWe can solve for thermodynamic beta (β = 1/kT) by considering it as a central difference without taking the continuum limit:\n\nhence the temperature\n\nThis entire proof assumes the microcanonical ensemble with energy fixed and temperature being the emergent property. In the canonical ensemble, the temperature is fixed and energy is the emergent property. This leads to (formula_14 refers to microstates):\nFollowing the previous example, we choose a state with two levels and two particles. This leads to microstates formula_18, formula_19, formula_20, and formula_21.\nThe resulting values for formula_28, formula_29, and formula_30 are all increasing with formula_31 and never need to enter a negative temperature regime.\n\nThe previous example is approximately realized by a system of nuclear spins in an external magnetic field. This allows the experiment to be run as a variation of nuclear magnetic resonance spectroscopy. In the case of electronic and nuclear spin systems, there are only a finite number of modes available, often just two, corresponding to spin up and spin down. In the absence of a magnetic field, these spin states are \"degenerate\", meaning that they correspond to the same energy. When an external magnetic field is applied, the energy levels are split, since those spin states that are aligned with the magnetic field will have a different energy from those that are anti-parallel to it.\n\nIn the absence of a magnetic field, such a two-spin system would have maximum entropy when half the atoms are in the spin-up state and half are in the spin-down state, and so one would expect to find the system with close to an equal distribution of spins. Upon application of a magnetic field, some of the atoms will tend to align so as to minimize the energy of the system, thus slightly more atoms should be in the lower-energy state (for the purposes of this example we will assume the spin-down state is the lower-energy state). It is possible to add energy to the spin system using radio frequency (RF) techniques. This causes atoms to \"flip\" from spin-down to spin-up.\n\nSince we started with over half the atoms in the spin-down state, this initially drives the system towards a 50/50 mixture, so the entropy is increasing, corresponding to a positive temperature. However, at some point, more than half of the spins are in the spin-up position. In this case, adding additional energy reduces the entropy, since it moves the system further from a 50/50 mixture. This reduction in entropy with the addition of energy corresponds to a negative temperature. In NMR spectroscopy, this corresponds to pulses with a pulse width of over 180° (for a given spin). While relaxation is fast in solids, it can take several seconds in solutions and even longer in gases and in ultracold systems; several hours were reported for silver and rhodium at picokelvin temperatures. It is still important to understand that the temperature is negative only with respect to nuclear spins. Other degrees of freedom, such as molecular vibrational, electronic and electron spin levels are at a positive temperature, so the object still has positive sensible heat. Relaxation actually happens by exchange of energy between the nuclear spin states and other states (e.g. through the nuclear Overhauser effect with other spins).\n\nThis phenomenon can also be observed in many lasing systems, wherein a large fraction of the system's atoms (for chemical and gas lasers) or electrons (in semiconductor lasers) are in excited states. This is referred to as a population inversion.\n\nThe Hamiltonian for a single mode of a luminescent radiation field at frequency \"ν\" is\nThe density operator in the grand canonical ensemble is\nFor the system to have a ground state, the trace to converge, and the density operator to be generally meaningful, \"βH\" must be positive semidefinite. So if \"hν\" < \"μ\", and \"H\" is negative semidefinite, then \"β\" must itself be negative, implying a negative temperature.\n\nThe two-dimensional systems can exist in negative temperature states.\n\nNegative temperatures have also been achieved in motional degrees of freedom. Using an optical lattice, upper bounds were placed on the kinetic energy, interaction energy and potential energy of cold atoms. This was done by tuning the interactions of the atoms from repulsive to attractive using a Feshbach resonance and changing the overall harmonic potential from trapping to anti-trapping, thus transforming the Bose-Hubbard Hamiltonian from formula_34. Performing this transformation adiabatically while keeping the atoms in the Mott insulator regime, it is possible to go from a low entropy positive temperature state to a low entropy negative temperature state. In the negative temperature state, the atoms macroscopically occupy the maximum momentum state of the lattice. The negative temperature ensembles equilibrated and showed long lifetimes in an anti-trapping harmonic potential.\n\n\n"}
{"id": "19777249", "url": "https://en.wikipedia.org/wiki?curid=19777249", "title": "Organizing principle", "text": "Organizing principle\n\nAn organizing principle is a core assumption from which everything else by proximity can derive a classification or a value. It is like a central reference point that allows all other objects to be located, often used in a conceptual framework. Having an organizing principle might help one simplify and get a handle on a particularly complicated domain or phenomenon. On the other hand, it might create a deceptive prism that colors one's judgment.\n\n\n\n"}
{"id": "4595966", "url": "https://en.wikipedia.org/wiki?curid=4595966", "title": "Pollyanna principle", "text": "Pollyanna principle\n\nThe Pollyanna principle (also called Pollyannaism or positivity bias) is the tendency for people to remember pleasant items more accurately than unpleasant ones. Research indicates that at the subconscious level, the mind has a tendency to focus on the optimistic; while at the conscious level, it has a tendency to focus on the negative. This subconscious bias towards the positive is often described as the Pollyanna principle and is similar to the Forer effect.\n\nThe name derives from the 1913 novel \"Pollyanna\" by Eleanor H. Porter describing a girl who plays the \"glad game\"—trying to find something to be glad about in every situation. The novel has been adapted to film several times, most famously in 1920 and 1960. An early use of the name \"Pollyanna\" in psychological literature was in 1969 by Boucher and Osgood who described a \"Pollyanna hypothesis\" as a universal human tendency to use evaluatively positive words more frequently and diversely than evaluatively negative words in communicating. Empirical evidence for this tendency has been provided by computational analyses of large corpora of text.\n\nThe \"Pollyanna principle\" was described by Matlin and Stang in 1978 using the archetype of Pollyanna more specifically as a psychological principle which portrays the positive bias people have when thinking of the past. According to the Pollyanna Principle, the brain processes information that is pleasing and agreeable in a more precise and exact manner as compared to unpleasant information. We actually tend to remember past experiences as more rosy than they actually occurred.\n\nResearchers Margaret Matlin and David Stang provided substantial evidence of the Pollyanna Principle. They found that people expose themselves to positive stimuli and avoid negative stimuli, they take longer to recognize what is unpleasant or threatening than what is pleasant and safe, and they report that they encounter positive stimuli more frequently than they actually do. Matlin and Stang also determined that selective recall was a more likely occurrence when recall was delayed: the longer the delay, the more selective recall that occurred.\n\nThe Pollyanna principle has been observed on online social networks as well. For example, Twitter users preferentially share more, and are emotionally affected more frequently by, positive information.\n\nHowever, the Pollyanna principle does not always apply to individuals suffering from depression or anxiety, who tend to either have more depressive realism or a negative bias.\n\n\n"}
{"id": "780566", "url": "https://en.wikipedia.org/wiki?curid=780566", "title": "Possible world", "text": "Possible world\n\nIn philosophy and logic, the concept of a possible world is used to express modal claims. The concept of possible worlds is common in contemporary philosophical discourse but has been disputed.\n\nThose theorists who use the concept of possible worlds consider the \"actual\" world to be one of the many possible worlds. For each distinct way the world could have been, there is said to be a distinct possible world; the actual world is the one we in fact live in. Among such theorists there is disagreement about the nature of possible worlds; their precise ontological status is disputed, and especially the difference, if any, in ontological status between the actual world and all the other possible worlds. One position on these matters is set forth in David Lewis's modal realism (see below). There is a close relation between propositions and possible worlds. We note that every proposition is either true or false at any given possible world; then the \"modal status\" of a proposition is understood in terms of the \"worlds in which it is true\" and \"worlds in which it is false\". The following are among the assertions we may now usefully make:\n\n\nThe idea of possible worlds is most commonly attributed to Gottfried Leibniz, who spoke of possible worlds as ideas in the mind of God and used the notion to argue that our actually created world must be \"the best of all possible worlds\". Arthur Schopenhauer argued that on the contrary our world must be the worst of all possible worlds, because if it were only a little worse it could not continue to exist.\n\nScholars have found implicit earlier traces of the idea of possible worlds in the works of René Descartes, a major influence on Leibniz, Al-Ghazali (\"The Incoherence of the Philosophers\"), Averroes (\"The Incoherence of the Incoherence\"), Fakhr al-Din al-Razi (\"Matalib al-'Aliya\") and John Duns Scotus. The modern philosophical use of the notion was pioneered by David Lewis and Saul Kripke.\n\nA semantics for modal logic was first introduced in the late-1950s work of Saul Kripke and his colleagues. A statement in modal logic that is \"possible\" is said to be true in at least one possible world; a statement that is \"necessary\" is said to be true in all possible worlds.\n\nFrom this groundwork, the theory of possible worlds became a central part of many philosophical developments, from the 1960s onwards – including, most famously, the analysis of counterfactual conditionals in terms of \"nearby possible worlds\" developed by David Lewis and Robert Stalnaker. On this analysis, when we discuss what \"would\" happen \"if\" some set of conditions \"were\" the case, the truth of our claims is determined by what is true at the nearest possible world (or the \"set\" of nearest possible worlds) where the conditions obtain. (A possible world W is said to be near to another possible world W in respect of R to the degree that the same things happen in W and W in respect of R; the more different something happens in two possible worlds in a certain respect, the \"further\" they are from one another in that respect.) Consider this conditional sentence: \"If George W. Bush hadn't become president of the U.S. in 2001, Al Gore would have.\" The sentence would be taken to express a claim that could be reformulated as follows: \"In all nearest worlds to our actual world (nearest in relevant respects) where George W. Bush didn't become president of the U.S. in 2001, Al Gore became president of the U.S. then instead.\" And on this interpretation of the sentence, if there is or are some nearest worlds to the actual world (nearest in relevant respects) where George W. Bush didn't become president but Al Gore didn't either, then the claim expressed by this counterfactual would be false.\n\nToday, possible worlds play a central role in many debates in philosophy, including especially debates over the Zombie Argument, and physicalism and supervenience in the philosophy of mind. Many debates in the philosophy of religion have been reawakened by the use of possible worlds. Intense debate has also emerged over the ontological status of possible worlds, provoked especially by David Lewis's defense of modal realism, the doctrine that talk about \"possible worlds\" is best explained in terms of innumerable, \"really existing\" worlds beyond the one we live in. The fundamental question here is: \"given\" that modal logic works, and that some possible-worlds semantics for modal logic is correct, \"what has to be true\" of the world, and just what \"are\" these possible worlds that we range over in our interpretation of modal statements? Lewis argued that what we range over are real, concrete \"worlds\" that exist just as unequivocally as our actual world exists, but that are distinguished from the actual world simply by standing in no spatial, temporal, or causal relations with the actual world. (On Lewis's account, the only \"special\" property that the \"actual\" world has is a relational one: that \"we\" are in it. This doctrine is called \"the indexicality of actuality\": \"actual\" is a merely indexical term, like \"now\" and \"here\".) Others, such as Robert Adams and William Lycan, reject Lewis's picture as metaphysically extravagant, and suggest in its place an interpretation of possible worlds as consistent, maximally complete sets of descriptions of or propositions about the world, so that a \"possible world\" is conceived of as a complete \"description\" of \"a way the world could be\" – rather than a \"world that is that way\". (Lewis describes their position, and similar positions such as those advocated by Alvin Plantinga and Peter Forrest, as \"\"ersatz\" modal realism\", arguing that such theories try to get the benefits of possible worlds semantics for modal logic \"on the cheap\", but that they ultimately fail to provide an adequate explanation.) Saul Kripke, in \"Naming and Necessity\", took explicit issue with Lewis's use of possible worlds semantics, and defended a \"stipulative\" account of possible worlds as purely \"formal\" (logical) entities rather than either really existent worlds or as some set of propositions or descriptions.\n\nPossible worlds theory in literary studies uses concepts from possible-world logic and applies them to worlds that are created by fictional texts, fictional universe. In particular, possible-world theory provides a useful vocabulary and conceptual framework with which to describe such worlds. However, a literary world is a specific type of possible world, quite distinct from the possible worlds in logic. This is because a literary text houses its own system of modality, consisting of actual worlds (actual events) and possible worlds (possible events). In fiction, the principle of simultaneity, it extends to cover the dimensional aspect, when it is contemplated that two or more physical objects, realities, perceptions and objects non-physical, can coexist in the same space-time. Thus, a literary universe is granted autonomy in much the same way as the actual universe.\n\nLiterary critics, such as Marie-Laure Ryan, Lubomír Doležel, and Thomas Pavel, have used possible-worlds theory to address notions of literary truth, the nature of fictionality, and the relationship between fictional worlds and reality. Taxonomies of fictional possibilities have also been proposed where the likelihood of a fictional world is assessed. Possible-world theory is also used within narratology to divide a specific text into its constituent worlds, possible and actual. In this approach, the modal structure of the fictional text is analysed in relation to its narrative and thematic concerns. Rein Raud has extended this approach onto \"cultural\" worlds, comparing possible worlds to the particular constructions of reality of different cultures. However, the metaphor of the \"cultural possible worlds\" relates to the framework of cultural relativism and, depending on the ontological status ascribed to possible worlds, warrants different, often controversial claims ranging from ethnocentrism to cultural imperialism.\n\n\n\n"}
{"id": "7295638", "url": "https://en.wikipedia.org/wiki?curid=7295638", "title": "Primary/secondary quality distinction", "text": "Primary/secondary quality distinction\n\nThe primary/secondary quality distinction is a conceptual distinction in epistemology and metaphysics, concerning the nature of reality. It is most explicitly articulated by John Locke in his \"Essay concerning Human Understanding\", but earlier thinkers such as Galileo and Descartes made similar distinctions.\n\nPrimary qualities are thought to be properties of objects that are independent of any observer, such as solidity, extension, motion, number and figure. These characteristics convey facts. They exist in the thing itself, can be determined with certainty, and do not rely on subjective judgments. For example, if an object is spherical, no one can reasonably argue that it is triangular.\n\nSecondary qualities are thought to be properties that produce sensations in observers, such as color, taste, smell, and sound. They can be described as the effect things have on certain people. Knowledge that comes from secondary qualities does not provide objective facts about things.\n\nPrimary qualities are measurable aspects of physical reality. Secondary qualities are subjective.\n\n\nLeibniz was an early critic of the distinction, writing in his 1686 \"Discourse on Metaphysics\" that \"[i]t is even possible to demonstrate that the ideas of size, figure and motion are not so distinctive as is imagined, and that they stand for something imaginary relative to our perceptions as do, although to a greater extent, the ideas of color, heat, and the other similar qualities in regard to which we may doubt whether they are actually to be found in the nature of the things outside of us.\"\n\nGeorge Berkeley wrote his famous critique of this distinction in his book Three Dialogues between Hylas and Philonous. Berkeley maintained that the ideas created by sensations are all that people can know for sure. As a result, what is perceived as real consists only of ideas in the mind. The crux of Berkeley's argument is that once an object is stripped of all its secondary qualities, it becomes very problematic to assign any acceptable meaning to the idea that \"there is\" some object. Not that one cannot picture to oneself (in one's mind) that some object could exist apart from any perceiver — one clearly can do this — but rather, that one cannot give any \"content\" to this idea. Suppose that someone says that a particular mind-independent object (meaning, an object free of all secondary qualities) exists at some particular spatio-temporal location (in Newtonian terms, in some particular place and at some particular time). Now, none of this particularly means anything if one cannot specify a place and time. In that case it's still a purely imaginary, empty idea. This is not generally thought to be a problem because realists imagine that they can, in fact, specify a place and time for a 'mind-independent' object. What is overlooked is that they can only specify a place and time in place and time \"as we experience them\". Berkeley did not doubt that one can do this, but that it is objective. One has simply related ideas to experiences (the idea of an \"object\" to our \"experiences of space and time\"). In this case there is no space and time, and therefore no objectivity. Space and time as we experience them are always piecemeal (even when the piece of space is big, as in some astronomical photos), it is only in imagination that they are total and all-encompassing, which is how we definitely imagine (!) 'real' space and time as being. This is why Berkeley argued that the materialist has merely an \"idea\" of an unperceived object: because people typically do take our imagining or picturing, as guaranteeing an objective reality to the 'existence' of 'something'. In no adequate way has it been specified nor given any acceptable meaning. As such Berkeley comes to his conclusion that having a compelling image in the mind, one which connects to no specifiable thing external to us, does not guarantee an objective existence.\n\nKant, in his \"Prolegomena to Any Future Metaphysics That Will Be Able to Present Itself as a Science\", claimed that primary, as well as secondary, qualities are subjective. They are both mere appearances that are located in the mind of a knowing observer. In § 13, Remark II, he wrote: \"Long before Locke's time, but assuredly since him, it has been generally assumed and granted without detriment to the actual existence of external things, that many of their predicates may be said to belong not to the things in themselves, but to their appearances, and to have no proper existence outside our representation. Heat, color, and taste, for instance, are of this kind. Now, if I go farther, and for weighty reasons rank as mere appearances the remaining qualities of bodies also, which are called primary, such as extension, place, and in general space, with all that which belongs to it (impenetrability or materiality, space, etc.)—no one in the least can adduce the reason of its being inadmissible.\"\n\n"}
{"id": "1931801", "url": "https://en.wikipedia.org/wiki?curid=1931801", "title": "Principle of charity", "text": "Principle of charity\n\nIn philosophy and rhetoric, the principle of charity or charitable interpretation requires interpreting a speaker's statements in the most rational way possible and, in the case of any argument, considering its best, strongest possible interpretation. In its narrowest sense, the goal of this methodological principle is to avoid attributing irrationality, logical fallacies, or falsehoods to the others' statements, when a coherent, rational interpretation of the statements is available. According to Simon Blackburn \"it constrains the interpreter to maximize the truth or rationality in the subject's sayings.\"\n\nNeil L. Wilson gave the principle its name in 1958–59. Its main area of application, by his lights, is determining the referent of a proper name:\nHow should we set about discovering the significance which a person attaches to a given name? […] Let us suppose that somebody (whom I am calling \"Charles\") makes just the following five assertions containing the name \"Caesar.\" […]\n(1) Caesar conquered Gaul. (Gc) \n(2) Caesar crossed the Rubicon. (Rc) \n(3) Caesar was murdered on the Ides of March. (Mc) \n[…] And so we act on what might be called the Principle of Charity. We select as designatum that individual which will make the largest possible number of Charles' statements true. […] We might say the designatum is that individual which satisfies more of the asserted matrices containing the word \"Caesar\" than does any other individual. \nWillard Van Orman Quine and Donald Davidson provide other formulations of the principle of charity. Davidson sometimes referred to it as \"the principle of rational accommodation\". He summarized it: \"We make maximum sense of the words and thoughts of others when we interpret in a way that optimises agreement\". The principle may be invoked to make sense of a speaker's utterances when one is unsure of their meaning. In particular, Quine's use of the principle gives it this latter, wide domain.\n\nSince the time of Quine \"et al.\", other philosophers have formulated at least four versions of the principle of charity. These alternatives may conflict with one another, so which principle to use may depend on the goal of the conversation. The four principles are:\n\n\nA related principle is the principle of humanity, which states that we must assume that another speaker's beliefs and desires are connected to each other and to reality in some way, and attribute to him or her \"the propositional attitudes one supposes one would have oneself in those circumstances\" (Daniel Dennett, \"Mid-Term Examination,\" in \"The Intentional Stance\", p. 343).\n\n\n"}
{"id": "8924176", "url": "https://en.wikipedia.org/wiki?curid=8924176", "title": "Principle of nonvacuous contrast", "text": "Principle of nonvacuous contrast\n\nThe principle of nonvacuous contrast is a logical or methodological principle which requires that a genuine predicate never refer to everything, or to nothing, within its universe of discourse.\n"}
{"id": "1137736", "url": "https://en.wikipedia.org/wiki?curid=1137736", "title": "Principle of sufficient reason", "text": "Principle of sufficient reason\n\nThe principle of sufficient reason states that everything must have a reason or a cause. The modern formulation of the principle is usually attributed to Gottfried Leibniz, although the idea was conceived of and utilized by various philosophers who preceded him, including Anaximander, Parmenides, Archimedes, Plato and Aristotle, Cicero, Avicenna, Thomas Aquinas, and Spinoza. Some philosophers have associated the principle of sufficient reason with \"ex nihilo nihil fit\". Hamilton identified the laws of inference modus ponens with the \"law of Sufficient Reason, or of Reason and Consequent\" and modus tollens with its contrapositive expression.\n\nThe principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\n\nA sufficient explanation may be understood either in terms of \"reasons\" or \"causes,\" for like many philosophers of the period, Leibniz did not carefully distinguish between the two. The resulting principle is very different, however, depending on which interpretation is given.\n\nIt is an open question whether the principle of sufficient reason can be applied to axioms within a logic construction like a mathematical or a physical theory, because axioms are propositions accepted as having no justification possible within the system\nThe principle declares that all propositions considered to be true within a system should be deducible from the set axioms at the base of the construction (with some theoretical exceptions: see Gödel's theorem).\n\nLeibniz identified two kinds of truth, necessary and contingent truths. He believed necessary mathematical truths to be derived from the law of identity (and the principle of contradiction): \"Necessary truths are those that can be demonstrated through an analysis of terms, so that in the end they become identities, just as in Algebra an equation expressing an identity ultimately results from the substitution of values [for variables]. That is, necessary truths depend upon the principle of contradiction.\" Leibniz states that the sufficient reason for necessary truths is that their negation is a contradiction.\n\nLeibniz admitted contingent truths on the basis of infinitary reasons, to which God had access but humans did not:\nIn contingent truths, even though the predicate is in the subject, this can never be demonstrated, nor can a proposition ever be reduced to an equality or to an identity, but the resolution proceeds to infinity, God alone seeing, not the end of the resolution, of course, which does not exist, but the connection of the terms or the containment of the predicate in the subject, since he sees whatever is in the series.Without this qualification, the principle can be seen as a description of a certain notion of closed system, in which there is no 'outside' to provide unexplained events with causes. It is also in tension with the paradox of Buridan's ass. Leibniz denied that the paradox of Buridan's ass could ever occur, saying:\n\nLeibniz also used the principle of sufficient reason to refute the idea of absolute space:\n\nI say then, that if space is an absolute being, there would be something for which it would be impossible there should be a sufficient reason. Which is against my axiom. And I prove it thus. Space is something absolutely uniform; and without the things placed in it, one point in space does not absolutely differ in any respect whatsoever from another point of space. Now from hence it follows, (supposing space to be something in itself, beside the order of bodies among themselves,) that 'tis impossible that there should be a reason why God, preserving the same situation of bodies among themselves, should have placed them in space after one particular manner, and not otherwise; why everything was not placed the quite contrary way, for instance, by changing East into West.\n\nThe principle was one of the four recognised laws of thought, that held a place in European pedagogy of logic and reasoning (and, to some extent, philosophy in general) in the 18th and 19th centuries. It was influential in the thinking of Leo Tolstoy, amongst others, in the elevated form that history could not be accepted as random.\n\nA sufficient reason is sometimes described as the coincidence of every single thing that is needed for the occurrence of an effect (i.e. of the so-called \"necessary conditions\"). Such view could perhaps be also applied to indeterministic systems, as long as randomness is in a way incorporated in the preconditions.\n\nHere is how Hamilton, circa 1837–1838, expressed his \"fourth law\" in his LECT. V. LOGIC. 60–61:\n\nAccording to Schopenhauer's \"On the Fourfold Root of the Principle of Sufficient Reason\", there are four distinct forms of the principle.\n\nFirst Form: The Principle of Sufficient Reason of Becoming (principium rationis sufficientis fiendi); appears as the law of causality in the understanding.\n\nSecond Form: The Principle of Sufficient Reason of Knowing (principium rationis sufficientis cognoscendi); asserts that if a judgment is to express a piece of knowledge, it must have a sufficient ground or reason, in which case it receives the predicate true.\n\nThird Form: The Principle of Sufficient Reason of Being (principium rationis sufficientis essendi); the law whereby the parts of space and time determine one another as regards those relations. Example in arithmetic: Each number presupposes the preceding numbers as grounds or reasons of its being; \"I can reach ten only by going through all the preceding numbers; and only by virtue of this insight into the ground of being, do I know that where there are ten, so are there eight, six, four.\"\n\n\"Now just as the subjective correlative to the first class of representations is the understanding, that to the second the faculty of reason, and that to the third pure sensibility, so is the subjective correlative to this fourth class found to be the inner sense, or generally self-consciousness.\" \n\nFourth Form: The Principle of Sufficient Reason of Acting (principium rationis sufficientis agendi); briefly known as the law of motivation. \"Any judgment that does not follow its previously existing ground or reason\" or any state that cannot be explained away as falling under the three previous headings \"must be produced by an act of will which has a motive.\" As his proposition in 43 states, \"Motivation is causality seen from within.\"\n\nSeveral proofs have been prepared in order to demonstrate that the universe is at bottom causal, i.e. works in accord with the principle in question; perhaps not in every single case (randomness might still play a part here and there), but that causality must be the way it works at least \"in general\", in most of what we see; and that our minds are aware of the principle even before any experience. The two famous arguments or proofs were proposed by Immanuel Kant (from the form of Time, temporal ordering of events and \"directionality\" of time) and by Arthur Schopenhauer (by demonstrating how all perception depends on causality and the intellect).\n\nOnce it is agreed (e.g. from a kind of an \"arrow of time\") that causal interconnections, as a form of principle of sufficient reason, indeed must in general exist everywhere in the universe (at least in the large scale), \"backwards\" causality in general might then be precluded using a form of the paradox of free will (i.e. an event that has a future source might cause us to remove that source quick enough and thus causality would not work).\n\n\n"}
{"id": "39396833", "url": "https://en.wikipedia.org/wiki?curid=39396833", "title": "Self-consistency principle in high energy Physics", "text": "Self-consistency principle in high energy Physics\n\nThe self-consistency principle was established by Rolf Hagedorn in 1965 to explain the thermodynamics of fireballs in high energy physics collisions. A thermodynamical approach to the high energy collisions first proposed by E. Fermi.\n\nThe partition function of the fireballs can be written in two forms, one in terms of its density of states, formula_1, and the other in terms of its mass spectrum, formula_2.\n\nThe self-consistency principle says that both forms must be asymptotically equivalent for energies or masses sufficiently high (asymptotic limit). Also, the density of states and the mass spectrum must be asymptotically equivalent in the sense of the weak constraint proposed by Hagedorn as\n\nThese two conditions are known as the \"self-consistency principle\" or \"bootstrap-idea\". After a long mathematical analysis Hagedorn was able to prove that there is in fact formula_4 and formula_5 satisfying the above conditions, resulting in\n\nand\n\nwith formula_8 and formula_9 related by\n\nThen the asymptotic partition function is given by\n\nwhere a singularity is clearly observed for formula_12 →formula_13. This singularity determines the limiting temperature formula_14 in Hagedorn's theory, which is also known as Hagedorn temperature.\n\nHagedorn was able not only to give a simple explanation for the thermodynamical aspect of high energy particle production, but also worked out a formula for the hadronic mass spectrum and predicted the limiting temperature for hot hadronic systems.\n\nAfter some time this limiting temperature was shown by N. Cabibbo and G. Parisi to be related to a phase transition, which characterizes by the deconfinement of quarks at high energies. The mass spectrum was further analyzed by Steven Frautschi.\n\nThe Hagedorn theory was able to describe correctly the experimental data from collision with center-of-mass energies up to approximately 10 GeV, but above this region it failed. In 2000 I. Bediaga, E. M. F. Curado and J. M. de Miranda proposed a phenomenological generalization of Hagedorn's theory by replacing the exponential function that appears in the partition function by the q-exponential function from the Tsallis non-extensive statistics. With this modification the generalized theory was able again to describe the extended experimental data.\n\nIn 2012 A. Deppman proposed a non-extensive self-consistent thermodynamical theory that includes the self-consistency principle and the non-extensive statistics. This theory gives as result the same formula proposed by Bediaga et al., which describes correctly the high energy data, but also new formulas for the mass spectrum and density of states of fireball. It also predicts a new limiting temperature and a limiting entropic index.\n\n"}
{"id": "16280135", "url": "https://en.wikipedia.org/wiki?curid=16280135", "title": "Systems art", "text": "Systems art\n\nSystems art is art influenced by cybernetics, and systems theory, that reflects on natural systems, social systems and social signs of the art world itself.\n\nSystems art emerged as part of the first wave of the conceptual art movement extended in the 1960s and 1970s. Closely related and overlapping terms are \"Anti-form movement\", \"Cybernetic art\", \"Generative Systems\", \"Process art\", \"Systems aesthetic\", \"Systemic art\", \"Systemic painting\" and \"Systems sculptures\".\n\nBy the early 1960s Minimalism had emerged as an abstract movement in art (with roots in geometric abstraction via Malevich, the Bauhaus and Mondrian) which rejected the idea of relational, and subjective painting, the complexity of Abstract expressionist surfaces, and the emotional zeitgeist and polemics present in the arena of Action painting. Minimalism argued that extreme simplicity could capture all of the sublime representation needed in art. The term Systematic art was coined by Lawrence Alloway in 1966 as a description of the method artists, such as Kenneth Noland, Al Held and Frank Stella, were using for composing abstract paintings.\n\nAssociated with painters such as Frank Stella, minimalism in painting, as opposed to other areas, is a modernist movement. Depending on the context, minimalism might be construed as a precursor to the postmodern movement. Seen from the perspective of writers who sometimes classify it as a postmodern movement, early minimalism began and succeeded as a modernist movement to yield advanced works, but which partially abandoned this project when a few artists changed direction in favor of the anti-form movement.\n\nIn the late 1960s the term Postminimalism was coined by Robert Pincus-Witten to describe minimalist derived art which had content and contextual overtones which minimalism rejected, and was applied to the work of Eva Hesse, Keith Sonnier, Richard Serra and new work by former minimalists Robert Smithson, Robert Morris, Bruce Nauman, Sol LeWitt, and Barry Le Va, and others. Minimalists like Donald Judd, Dan Flavin, Carl Andre, Agnes Martin, John McCracken and others continued to produce their late modernist paintings and sculpture for the remainder of their careers.\n\nAudio feedback and the use of tape loops, sound synthesis and computer generated compositions reflected a cybernetic awareness of information, systems and cycles. Such techniques became widespread in the 1960s in the music industry. The visual effects of electronic feedback became a focus of artistic research in the late 1960s, when video equipment first reached the consumer market. Steina and Woody Vasulka, for example, used \"all manner and combination of audio and video signals to generate electronic feedback in their respective of corresponding media.\"\n\nWith related work by Edward Ihnatowicz, Wen-Ying Tsai and cybernetician Gordon Pask and the animist kinetics of Robert Breer and Jean Tinguely, the 1960s produced a strain of cybernetic art that was very much concerned with the shared circuits within and between the living and the technological. A line of cybernetic art theory also emerged during the late 1960s. Writers like Jonathan Benthall and Gene Youngblood drew on cybernetics and cybernetic. The most substantial contributors here were the British artist and theorist Roy Ascott with his essay \"Behaviourist Art and the Cybernetic Vision\" in the journal Cybernetica (1966–67), and the American critic and theorist Jack Burnham. In \"Beyond Modern Sculpture\" from 1968 Burnham builds cybernetic art into an extensive theory that centers on art's drive to imitate and ultimately reproduce life. Also in 1968, curator Jasia Reichardt organized the landmark exhibition, Cybernetic Serendipity, at the Institute of Contemporary Art in London.\n\nGenerative art is art that has been generated, composed, or constructed in an algorithmic manner through the use of systems defined by computer software algorithms, or similar mathematical or mechanical or randomised autonomous processes. Sonia Landy Sheridan established Generative Systems as a program at the School of the Art Institute of Chicago in 1970 in response to social change brought about in part by the computer-robot communications revolution. The program, which brought artists and scientists together, was an effort at turning the artist's passive role into an active one by promoting the investigation of contemporary scientific—technological systems and their relationship to art and life. Unlike copier art, which was a simple commercial spin-off, Generative Systems was actually involved in the development of elegant yet simple systems intended for creative use by the general population. Generative Systems artists attempted to bridge the gap between elite and novice by directing the line of communication between the two, thus bringing first generation information to greater numbers of people and bypassing the entrepreneur.\n\nProcess art is an artistic movement as well as a creative sentiment and world view where the end product of \"art\" and \"craft\", the \"objet d’art\", is not the principal focus. The 'process' in process art refers to the process of the formation of art: the gathering, sorting, collating, associating, and patterning. Process art is concerned with the actual \"doing\"; art as a rite, ritual, and performance. Process art often entails an inherent motivation, rationale, and intentionality. Therefore, art is viewed as a creative journey or process, rather than as a deliverable or end product.\nIn the artistic discourse the work of Jackson Pollock is hailed as an antecedent. Process art in its employment of serendipity has a marked correspondence with Dada. Change and transience are marked themes in the process art movement. The Guggenheim Museum states that Robert Morris in 1968 had a groundbreaking exhibition and essay defining the movement and the Museum Website states as \"Process artists were involved in issues attendant to the body, random occurrences, improvisation, and the liberating qualities of nontraditional materials such as wax, felt, and latex. Using these, they created eccentric forms in erratic or irregular arrangements produced by actions such as cutting, hanging, and dropping, or organic processes such as growth, condensation, freezing, or decomposition\".\n\nAccording to Chilvers (2004) \"earlier in 1966 the British art critic Lawrence Alloway had coined the term \"Systemic art\", to describe a type of abstract art characterized by the use of very simple standardized forms, usually geometric in character, either in a single concentrated image or repeated in a system arranged according to a clearly visible principle of organization. He considered the chevron paintings of Kenneth Noland as examples of Systemic art, and considered this as a branch of Minimal art\".\n\nJohn G. Harries considered a common ground in the ideas that underlie developments in 20th-century art such as Serial art, Systems Art, Constructivism and Kinetic art. These kind of arts often do not stem directly from observations of things visible in the external natural environment, but from the observation of depicted shapes and of the relationship between them. Systems art, according to Harries, represents a deliberate attempt by artists to develop a more flexible frame of reference. A style in which its frame of reference is taken as a model to be emulated rather than as a cognitive systems, that only leads to the institutionalization of the imposed model. But to transfer the meaning of a picture to its location within a systemic structure does not remove the need to define the constitutive elements of the system: if they are not defined, one will not know how to build the system.\nSystemic Painting, according to Auping (1989) \"was the title of an highly influential exhibition at the Guggenheim Museum in 1966 assembled and introduction written by Lawrence Alloway as curator. The show contained numerous works that many critics today would consider part of the Minimal art\". In the catalogue Alloway noted, that ... \"paintings, such as those in this exhibition are not, as has been often claimed, impersonal. The personal is not expunged by using a neat technique: anonymity is not a consequence of highly finishing a painting\". The term \"systemic painting\" later on has become the name for artists who employ systems make a number of aesthetic decisions before commencing \nto paint.\n\nAccording to Feldman (1987) \"serial art, serial painting, systems sculpture and ABC art, were art styles of the 1960s and 1970s in which simple geometric configurations are repeated with little or no variation. Sequences becomes important as in mathematics and linguistic context. These works rely on simple arrangements of basic volumes and voids, mechanically produced surfaces, and algebraic permutations of form. The impact on the viewer, however, is anything but simple\".\n\n\n"}
{"id": "28547570", "url": "https://en.wikipedia.org/wiki?curid=28547570", "title": "Terminology model", "text": "Terminology model\n\nA terminology model is a refinement of a concept system. Within a terminology model the concepts (object types) of a specific problem or subject area are defined by subject matter experts in terms of concept (object type) definitions and definitions of subordinated concepts or characteristics (properties). Besides object types, the terminology model allows defining hierarchical classifications, definitions for object type and property behavior and definition of casual relations.\n\nThe terminology model is a means for subject matter experts to express their knowledge about the subject in subject specific terms. Since the terminology model is structured rather similar to an object-oriented database schema, is can be transformed without loss of information into an object-oriented database schema. Thus, the terminology model is a method for problem analysis on the one side and a mean of defining database schema on the other side.\n\nSeveral terminology models have been developed and published in the field of statistics:\n\n\n"}
{"id": "30746", "url": "https://en.wikipedia.org/wiki?curid=30746", "title": "Theory", "text": "Theory\n\nA theory is a contemplative and rational type of abstract or generalizing thinking, or the results of such thinking. Depending on the context, the results might, for example, include generalized explanations of how nature works. The word has its roots in ancient Greek, but in modern use it has taken on several related meanings.\n\nTheories guide the enterprise of finding facts rather than of reaching goals, and are neutral concerning alternatives among values. A theory can be a body of knowledge, which may or may not be associated with particular explanatory models. To theorize is to develop this body of knowledge.\n\nAs already in Aristotle's definitions, theory is very often contrasted to \"practice\" (from Greek \"\", πρᾶξις) a Greek term for \"doing\", which is opposed to theory because pure theory involves no doing apart from itself. A classical example of the distinction between \"theoretical\" and \"practical\" uses the discipline of medicine: medical theory involves trying to understand the causes and nature of health and sickness, while the practical side of medicine is trying to make people healthy. These two things are related but can be independent, because it is possible to research health and sickness without curing specific patients, and it is possible to cure a patient without knowing how the cure worked.\n\nIn modern science, the term \"theory\" refers to scientific theories, a well-confirmed type of explanation of nature, made in a way consistent with scientific method, and fulfilling the criteria required by modern science. Such theories are described in such a way that scientific tests should be able to provide empirical support for, or empirically contradict (\"falsify\") it. Scientific theories are the most reliable, rigorous, and comprehensive form of scientific knowledge, in contrast to more common uses of the word \"theory\" that imply that something is unproven or speculative (which in formal terms is better characterized by the word \"hypothesis\"). Scientific theories are distinguished from hypotheses, which are individual empirically testable conjectures, and from scientific laws, which are descriptive accounts of how nature behaves under certain conditions.\n\nThe English word \"theory\" derives from a technical term in philosophy in Ancient Greek. As an everyday word, \"theoria\", , meant \"a looking at, viewing, beholding\", but in more technical contexts it came to refer to contemplative or speculative understandings of natural things, such as those of natural philosophers, as opposed to more practical ways of knowing things, like that of skilled orators or artisans. English-speakers have used the word \"theory\" since at least the late 16th century. Modern uses of the word \"theory\" derive from the original definition, but have taken on new shades of meaning, still based on the idea of a theory as a thoughtful and rational explanation of the general nature of things.\n\nAlthough it has more mundane meanings in Greek, the word apparently developed special uses early in the recorded history of the Greek language. In the book \"From Religion to Philosophy\", Francis Cornford suggests that the Orphics used the word \"theoria\" to mean \"passionate sympathetic contemplation\". Pythagoras changed the word to mean a passionate sympathetic contemplation of mathematical knowledge, because he considered this intellectual pursuit the way to reach the highest plane of existence. Pythagoras emphasized subduing emotions and bodily desires to help the intellect function at the higher plane of theory. Thus, it was Pythagoras who gave the word \"theory\" the specific meaning that led to the classical and modern concept of a distinction between theory (as uninvolved, neutral thinking) and practice.\n\nAristotle's terminology, as already mentioned, contrasts theory with \"praxis\" or practice, and this contrast remains today. For Aristotle, both practice and theory involve thinking, but the aims are different. Theoretical contemplation considers things humans do not move or change, such as nature, so it has no human aim apart from itself and the knowledge it helps create. On the other hand, \"praxis\" involves thinking, but always with an aim to desired actions, whereby humans cause change or movement themselves for their own ends. Any human movement that involves no conscious choice and thinking could not be an example of \"praxis\" or doing.\n\nTheories are analytical tools for understanding, explaining, and making predictions about a given subject matter. There are theories in many and varied fields of study, including the arts and sciences. A formal theory is syntactic in nature and is only meaningful when given a semantic component by applying it to some content (e.g., facts and relationships of the actual historical world as it is unfolding). Theories in various fields of study are expressed in natural language, but are always constructed in such a way that their general form is identical to a theory as it is expressed in the formal language of mathematical logic. Theories may be expressed mathematically, symbolically, or in common language, but are generally expected to follow principles of rational thought or logic.\n\nTheory is constructed of a set of sentences that are entirely true statements about the subject under consideration. However, the truth of any one of these statements is always relative to the whole theory. Therefore, the same statement may be true with respect to one theory, and not true with respect to another. This is, in ordinary language, where statements such as \"He is a terrible person\" cannot be judged as true or false without reference to some interpretation of who \"He\" is and for that matter what a \"terrible person\" is under the theory.\n\nSometimes two theories have exactly the same explanatory power because they make the same predictions. A pair of such theories is called indistinguishable or observationally equivalent, and the choice between them reduces to convenience or philosophical preference.\n\nThe form of theories is studied formally in mathematical logic, especially in model theory. When theories are studied in mathematics, they are usually expressed in some formal language and their statements are closed under application of certain procedures called rules of inference. A special case of this, an axiomatic theory, consists of axioms (or axiom schemata) and rules of inference. A theorem is a statement that can be derived from those axioms by application of these rules of inference. Theories used in applications are abstractions of observed phenomena and the resulting theorems provide solutions to real-world problems. Obvious examples include arithmetic (abstracting concepts of number), geometry (concepts of space), and probability (concepts of randomness and likelihood).\n\nGödel's incompleteness theorem shows that no consistent, recursively enumerable theory (that is, one whose theorems form a recursively enumerable set) in which the concept of natural numbers can be expressed, can include all true statements about them. As a result, some domains of knowledge cannot be formalized, accurately and completely, as mathematical theories. (Here, formalizing accurately and completely means that all true propositions—and only true propositions—are derivable within the mathematical system.) This limitation, however, in no way precludes the construction of mathematical theories that formalize large bodies of scientific knowledge.\n\nA theory is \"underdetermined\" (also called \"indeterminacy of data to theory\") if a rival, inconsistent theory is at least as consistent with the evidence. Underdetermination is an epistemological issue about the relation of evidence to conclusions.\n\nA theory that lacks supporting evidence is generally, more properly, referred to as a hypothesis.\n\nIf a new theory better explains and predicts a phenomenon than an old theory (i.e., it has more explanatory power), we are justified in believing that the newer theory describes reality more correctly. This is called an \"intertheoretic reduction\" because the terms of the old theory can be reduced to the terms of the new one. For instance, our historical understanding about \"sound\", \"light\" and \"heat\" have been reduced to \"wave compressions and rarefactions\", \"electromagnetic waves\", and \"molecular kinetic energy\", respectively. These terms, which are identified with each other, are called \"intertheoretic identities.\" When an old and new theory are parallel in this way, we can conclude that the new one describes the same reality, only more completely.\n\nWhen a new theory uses new terms that do not reduce to terms of an older theory, but rather replace them because they misrepresent reality, it is called an \"intertheoretic elimination.\" For instance, the obsolete scientific theory that put forward an understanding of heat transfer in terms of the movement of caloric fluid was eliminated when a theory of heat as energy replaced it. Also, the theory that phlogiston is a substance released from burning and rusting material was eliminated with the new understanding of the reactivity of oxygen.\n\nTheories are distinct from theorems. A \"theorem\" is derived deductively from axioms (basic assumptions) according to a formal system of rules, sometimes as an end in itself and sometimes as a first step toward being tested or applied in a concrete situation; theorems are said to be true in the sense that the conclusions of a theorem are logical consequences of the axioms. \"Theories\" are abstract and conceptual, and are supported or challenged by observations in the world. They are 'rigorously tentative', meaning that they are proposed as true and expected to satisfy careful examination to account for the possibility of faulty inference or incorrect observation. Sometimes theories are incorrect, meaning that an explicit set of observations contradicts some fundamental objection or application of the theory, but more often theories are corrected to conform to new observations, by restricting the class of phenomena the theory applies to or changing the assertions made. An example of the former is the restriction of classical mechanics to phenomena involving macroscopic length scales and particle speeds much lower than the speed of light.\n\nIn science, the term \"theory\" refers to \"a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment.\" Theories must also meet further requirements, such as the ability to make falsifiable predictions with consistent accuracy across a broad area of scientific inquiry, and production of strong evidence in favor of the theory from multiple independent sources (consilience).\n\nThe strength of a scientific theory is related to the diversity of phenomena it can explain, which is measured by its ability to make falsifiable predictions with respect to those phenomena. Theories are improved (or replaced by better theories) as more evidence is gathered, so that accuracy in prediction improves over time; this increased accuracy corresponds to an increase in scientific knowledge. Scientists use theories as a foundation to gain further scientific knowledge, as well as to accomplish goals such as inventing technology or curing disease.\n\nThe United States National Academy of Sciences defines scientific theories as follows:\nThe formal scientific definition of \"theory\" is quite different from the everyday meaning of the word. It refers to a comprehensive explanation of some aspect of nature that is supported by a vast body of evidence. Many scientific theories are so well established that no new evidence is likely to alter them substantially. For example, no new evidence will demonstrate that the Earth does not orbit around the sun (heliocentric theory), or that living things are not made of cells (cell theory), that matter is not composed of atoms, or that the surface of the Earth is not divided into solid plates that have moved over geological timescales (the theory of plate tectonics)...One of the most useful properties of scientific theories is that they can be used to make predictions about natural events or phenomena that have not yet been observed.\n\nFrom the American Association for the Advancement of Science:\nA scientific theory is a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment. Such fact-supported theories are not \"guesses\" but reliable accounts of the real world. The theory of biological evolution is more than \"just a theory.\" It is as factual an explanation of the universe as the atomic theory of matter or the germ theory of disease. Our understanding of gravity is still a work in progress. But the phenomenon of gravity, like evolution, is an accepted fact.\n\nNote that the term \"theory\" would not be appropriate for describing untested but intricate hypotheses or even scientific models.\n\nThe logical positivists thought of scientific theories as \"deductive theories\"—that a theory's content is based on some formal system of logic and on basic axioms. In a deductive theory, any sentence which is a logical consequence of one or more of the axioms is also a sentence of that theory. This is called the received view of theories.\n\nIn the semantic view of theories, which has largely replaced the received view, theories are viewed as scientific models. A model is a logical framework intended to represent reality (a \"model of reality\"), similar to the way that a map is a graphical model that represents the territory of a city or country. In this approach, theories are a specific category of models that fulfill the necessary criteria. (See Theories as models for further discussion.)\n\nIn physics the term \"theory\" is generally used for a mathematical framework—derived from a small set of basic postulates (usually symmetries, like equality of locations in space or in time, or identity of electrons, etc.)—which is capable of producing experimental predictions for a given category of physical systems. One good example is classical electromagnetism, which encompasses results derived from gauge symmetry (sometimes called gauge invariance) in a form of a few equations called Maxwell's equations. The specific mathematical aspects of classical electromagnetic theory are termed \"laws of electromagnetism\", reflecting the level of consistent and reproducible evidence that supports them. Within electromagnetic theory generally, there are numerous hypotheses about how electromagnetism applies to specific situations. Many of these hypotheses are already considered adequately tested, with new ones always in the making and perhaps untested.\n\nAcceptance of a theory does not require that all of its major predictions be tested, if it is already supported by sufficiently strong evidence. For example, certain tests may be infeasible or technically difficult. As a result, theories may make predictions that have not yet been confirmed or proven incorrect; in this case, the predicted results may be described informally using the term \"theoretical.\" These predictions can be tested at a later time, and if they are incorrect, this may lead to revision, invalidation, or rejection of the theory.\n\nA theory can be either \"descriptive\" as in science, or \"prescriptive\" (normative) as in philosophy. The latter are those whose subject matter consists not of empirical data, but rather of ideas. At least some of the elementary theorems of a philosophical theory are statements whose truth cannot necessarily be scientifically tested through empirical observation.\n\nA field of study is sometimes named a \"theory\" because its basis is some initial set of assumptions describing the field's approach to the subject. These assumptions are the elementary theorems of the particular theory, and can be thought of as the axioms of that field. Some commonly known examples include set theory and number theory; however literary theory, critical theory, and music theory are also of the same form.\n\nOne form of philosophical theory is a \"metatheory\" or \"meta-theory\". A metatheory is a theory whose subject matter is some other theory or set of theories. In other words, it is a theory about theories. Statements made in the metatheory about the theory are called metatheorems.\n\nA political theory is an ethical theory about the law and government. Often the term \"political theory\" refers to a general view, or specific ethic, political belief or attitude, about politics.\n\nIn social science, jurisprudence is the philosophical theory of law. Contemporary philosophy of law addresses problems internal to law and legal systems, and problems of law as a particular social institution.\n\nMost of the following are scientific theories; some are not, but rather encompass a body of knowledge or art, such as Music theory and Visual Arts Theories.\n\n\n\n"}
{"id": "49535", "url": "https://en.wikipedia.org/wiki?curid=49535", "title": "Thought experiment", "text": "Thought experiment\n\nA thought experiment (, \"Gedanken-Experiment\", or \"Gedankenerfahrung\",) considers some hypothesis, theory, or principle for the purpose of thinking through its consequences. Given the structure of the experiment, it may not be possible to perform it, and even if it could be performed, there need not be an intention to perform it.\n\nThe common goal of a thought experiment is to explore the potential consequences of the principle in question:\n\nExamples of thought experiments include Schrödinger's cat, illustrating quantum indeterminacy through the manipulation of a perfectly sealed environment and a tiny bit of radioactive substance, and Maxwell's demon, which attempts to demonstrate the ability of a hypothetical finite being to violate the 2nd law of thermodynamics.\n\nThe ancient Greek δείκνυμι \"(transl.: deiknymi)\", or thought experiment, \"was the most ancient pattern of mathematical proof\", and existed before Euclidean mathematics, where the emphasis was on the conceptual, rather than on the experimental part of a thought-experiment. Perhaps the key experiment in the history of modern science is Galileo's demonstration that falling objects must fall at the same rate regardless of their masses. This is widely thought to have been a straightforward physical demonstration, involving climbing up the Leaning Tower of Pisa and dropping two heavy weights off it, whereas in fact, it was a logical demonstration, using the 'thought experiment' technique. The 'experiment' is described by Galileo in \"Discorsi e dimostrazioni matematiche\" (1638) (literally, 'Discourses and Mathematical Demonstrations') thus:\n\nAlthough the extract does not convey the elegance and power of the 'demonstration' terribly well, it is clear that it is a 'thought' experiment, rather than a practical one. Strange then, as Cohen says, that philosophers and scientists alike refuse to acknowledge either Galileo in particular, or the thought experiment technique in general for its pivotal role in both science and philosophy. (The exception proves the rule — the iconoclastic philosopher of science, Paul Feyerabend, has also observed this methodological prejudice.)\n\nInstead, many philosophers prefer to consider 'Thought Experiments' to be merely the use of a hypothetical scenario to help understand the way things are.\n\nThought experiments have been used in a variety of fields, including philosophy, law, physics, and mathematics. In philosophy, they have been used at least since classical antiquity, some pre-dating Socrates. In law, they were well-known to Roman lawyers quoted in the Digest. In physics and other sciences, notable thought experiments date from the 19th and especially the 20th century, but examples can be found at least as early as Galileo.\n\nJohann Witt-Hansen established that Hans Christian Ørsted was the first to use the Latin-German mixed term \"Gedankenexperiment\" (lit. thought experiment) circa 1812. Ørsted was also the first to use its entirely German equivalent, \"Gedankenversuch\", in 1820.\n\nMuch later, Ernst Mach used the term \"Gedankenexperiment\" in a different way, to denote exclusively the \"imaginary\" conduct of a \"real\" experiment that would be subsequently performed as a \"real physical experiment\" by his students. Physical and mental experimentation could then be contrasted: Mach asked his students to provide him with explanations whenever the results from their subsequent, real, physical experiment differed from those of their prior, imaginary experiment.\n\nThe English term \"thought experiment\" was coined (as a calque) from Mach's \"Gedankenexperiment\", and it first appeared in the 1897 English translation of one of Mach’s papers. Prior to its emergence, the activity of posing hypothetical questions that employed subjunctive reasoning had existed for a very long time (for both scientists and philosophers). However, people had no way of categorizing it or speaking about it. This helps to explain the extremely wide and diverse range of the application of the term \"thought experiment\" once it had been introduced into English.\n\nThought experiments, which are well-structured, well-defined hypothetical questions that employ subjunctive reasoning (irrealis moods) – \"What might happen (or, what might have happened) if . . . \" – have been used to pose questions in philosophy at least since Greek antiquity, some pre-dating Socrates. In physics and other sciences many thought experiments date from the 19th and especially the 20th Century, but examples can be found at least as early as Galileo.\n\nIn thought experiments we gain new information by rearranging or reorganizing already known empirical data in a new way and drawing new (a priori) inferences from them or by looking at these data from a different and unusual perspective. In Galileo’s thought experiment, for example, the rearrangement of empirical experience consists in the original idea of combining bodies of different weight.\n\nThought experiments have been used in philosophy (especially ethics), physics, and other fields (such as cognitive psychology, history, political science, economics, social psychology, law, organizational studies, marketing, and epidemiology). In law, the synonym \"hypothetical\" is frequently used for such experiments.\n\nRegardless of their intended goal, all thought experiments display a patterned way of thinking that is designed to allow us to explain, predict and control events in a better and more productive way.\n\nIn terms of their theoretical consequences, thought experiments generally:\n\nThought experiments can produce some very important and different outlooks on previously unknown or unaccepted theories. However, they may make those theories themselves irrelevant, and could possibly create new problems that are just as difficult, or possibly more difficult to resolve.\n\nIn terms of their practical application, thought experiments are generally created to:\n\nScientists tend to use thought experiments as imaginary, \"proxy\" experiments prior to a real, \"physical\" experiment (Ernst Mach always argued that these gedankenexperiments were \"a necessary precondition for physical experiment\"). In these cases, the result of the \"proxy\" experiment will often be so clear that there will be no need to conduct a physical experiment at all.\n\nScientists also use thought experiments when particular physical experiments are impossible to conduct (Carl Gustav Hempel labeled these sorts of experiment \"theoretical experiments-in-imagination\"), such as Einstein's thought experiment of chasing a light beam, leading to special relativity. This is a unique use of a scientific thought experiment, in that it was never carried out, but led to a successful theory, proven by other empirical means.\n\nThe relation to real experiments can be quite complex, as can be seen again from an example going back to Albert Einstein. In 1935, with two coworkers, he published a paper on a newly created subject called later the EPR effect (EPR paradox). In this paper, starting from certain philosophical assumptions, on the basis of a rigorous analysis of a certain, complicated, but in the meantime assertedly realizable model, he came to the conclusion that \"quantum mechanics should be described as \"incomplete\"\". Niels Bohr asserted a refutation of Einstein's analysis immediately, and his view prevailed. After some decades, it was asserted that feasible experiments could prove the error of the EPR paper. These experiments tested the Bell inequalities published in 1964 in a purely theoretical paper. The above-mentioned EPR philosophical starting assumptions were considered to be falsified by empirical fact (e.g. by the optical \"real experiments\" of Alain Aspect).\n\nThus \"thought experiments\" belong to a theoretical discipline, usually to theoretical physics, but often to theoretical philosophy. In any case, it must be distinguished from a real experiment, which belongs naturally to the experimental discipline and has \"the final decision on \"true\" or \"not true\"\", at least in physics.\n\nThe first characteristic pattern that thought experiments display is their orientation\nin time. They are either:\n\nThe second characteristic pattern is their movement in time in relation to “the present\nmoment standpoint” of the individual performing the experiment; namely, in terms of:\n\nGenerally speaking, there are seven types of thought experiments in which one reasons from causes to effects, or effects to causes:\n\n\"Prefactual (before the fact) thought experiments\" — the term prefactual was coined by Lawrence J. Sanna in 1998 — speculate on possible future outcomes, given the present, and ask \"What will be the outcome if event E occurs?\"\n\n\"Counterfactual (contrary to established fact) thought experiments\" — the term \"counterfactual\" was coined by Nelson Goodman in 1947, extending Roderick Chisholm's (1946) notion of a \"contrary-to-fact conditional\" — speculate on the possible outcomes of a different past; and ask \"What might have happened if A had happened instead of B?\" (e.g., \"If Isaac Newton and Gottfried Leibniz had cooperated with each other, what would mathematics look like today?\").\n\nThe study of counterfactual speculation has increasingly engaged the interest of scholars in a wide range of domains such as philosophy, psychology, cognitive psychology, history, political science, economics, social psychology, law, organizational theory, marketing, and epidemiology.\n\n\"Semifactual thought experiments\" — the term \"semifactual\" was coined by Nelson Goodman in 1947 — speculate on the extent to which things might have remained the same, despite there being a different past; and asks the question Even though X happened instead of E, would Y have still occurred? (e.g., Even if the goalie had moved left, rather than right, could he have intercepted a ball that was traveling at such a speed?).\n\nSemifactual speculations are an important part of clinical medicine.\n\nThe activity of prediction attempts to project the circumstances of the present into the future. According to David Sarewitz and Roger Pielke (1999, p123), scientific prediction takes two forms:\n\nAlthough they perform different social and scientific functions, the only difference between the qualitatively identical activities of \"predicting\", \"forecasting,\" and \"nowcasting\" is the distance of the speculated future from the present moment occupied by the user. Whilst the activity of nowcasting, defined as “a detailed description of the current weather along with forecasts obtained by extrapolation up to 2 hours ahead”, is essentially concerned with describing the current state of affairs, it is common practice to extend the term “to cover very-short-range forecasting up to 12 hours ahead” (Browning, 1982, p.ix).\n\nThe activity of hindcasting involves running a forecast model after an event has happened in order to test whether the model's simulation is valid.\n\nIn 2003, Dake Chen and his colleagues “trained” a computer using the data of the surface temperature of the oceans from the last 20 years. Then, using data that had been collected on the surface temperature of the oceans for the period 1857 to 2003, they went through a hindcasting exercise and discovered that their simulation not only accurately predicted every El Niño event for the last 148 years, it also identified the (up to 2 years) looming foreshadow of every single one of those El Niño events.\n\nThe activity of \"retrodiction\" (or \"postdiction\") involves moving backwards in time, step-by-step, in as many stages as are considered necessary, from the present into the speculated past to establish the ultimate cause of a specific event (e.g., reverse engineering and forensics).\n\nGiven that retrodiction is a process in which \"past observations, events and data are used as evidence to infer the process(es) the produced them\" and that diagnosis \"involve[s] going from visible effects such as symptoms, signs and the like to their prior causes\", the essential balance between prediction and retrodiction could be characterized as:\nregardless of whether the prognosis is of the course of the disease in the absence of treatment, or of the application of a specific treatment regimen to a specific disorder in a particular patient.\n\nThe activity of \"backcasting\" — the term \"backcasting\" was coined by John Robinson in 1982 — involves establishing the description of a very definite and very specific future situation. It then involves an imaginary moving backwards in time, step-by-step, in as many stages as are considered necessary, from the future to the present to reveal the mechanism through which that particular specified future could be attained from the present.\n\nBackcasting is not concerned with predicting the future:\n\nAccording to Jansen (1994, p. 503:\n\nIn philosophy, a thought experiment typically presents an imagined scenario with the intention of eliciting an intuitive or reasoned response about the way things are in the thought experiment. (Philosophers might also supplement their thought experiments with theoretical reasoning designed to support the desired intuitive response.) The scenario will typically be designed to target a particular philosophical notion, such as morality, or the nature of the mind or linguistic reference. The response to the imagined scenario is supposed to tell us about the nature of that notion in any scenario, real or imagined.\n\nFor example, a thought experiment might present a situation in which an agent intentionally kills an innocent for the benefit of others. Here, the relevant question is not whether the action is moral or not, but more broadly whether a moral theory is correct that says morality is determined solely by an action's consequences (See Consequentialism). John Searle imagines a man in a locked room who receives written sentences in Chinese, and returns written sentences in Chinese, according to a sophisticated instruction manual. Here, the relevant question is not whether or not the man understands Chinese, but more broadly, whether a functionalist theory of mind is correct.\n\nIt is generally hoped that there is universal agreement about the intuitions that a thought experiment elicits. (Hence, in assessing their own thought experiments, philosophers may appeal to \"what we should say,\" or some such locution.) A successful thought experiment will be one in which intuitions about it are widely shared. But often, philosophers differ in their intuitions about the scenario.\n\nOther philosophical uses of imagined scenarios arguably are thought experiments also. In one use of scenarios, philosophers might imagine persons in a particular situation (maybe ourselves), and ask what they would do.\n\nFor example, in the veil of ignorance, John Rawls asks us to imagine a group of persons in a situation where they know nothing about themselves, and are charged with devising a social or political organization. The use of the state of nature to imagine the origins of government, as by Thomas Hobbes and John Locke, may also be considered a thought experiment. Søren Kierkegaard explored the possible ethical and religious implications of Abraham's binding of Isaac in \"Fear and Trembling\" Similarly, Friedrich Nietzsche, in \"On the Genealogy of Morals\", speculated about the historical development of Judeo-Christian morality, with the intent of questioning its legitimacy.\n\nAn early written thought experiment was Plato's allegory of the cave. Another historic thought experiment was Avicenna's \"Floating Man\" thought experiment in the 11th century. He asked his readers to imagine themselves suspended in the air isolated from all in order to demonstrate human self-awareness and self-consciousness, and the substantiality of the soul.\n\nIn many thought experiments, the scenario would be nomologically possible, or possible according to the laws of nature. John Searle's Chinese room is nomologically possible.\n\nSome thought experiments present scenarios that are not nomologically possible. In his Twin Earth thought experiment, Hilary Putnam asks us to imagine a scenario in which there is a substance with all of the observable properties of water (e.g., taste, color, boiling point), but is chemically different from water. It has been argued that this thought experiment is not nomologically possible, although it may be possible in some other sense, such as metaphysical possibility. It is debatable whether the nomological impossibility of a thought experiment renders intuitions about it moot.\n\nIn some cases, the hypothetical scenario might be considered metaphysically impossible, or impossible in any sense at all. David Chalmers says that we can imagine that there are zombies, or persons who are physically identical to us in every way but who lack consciousness. This is supposed to show that physicalism is false. However, some argue that zombies are inconceivable: we can no more imagine a zombie than we can imagine that 1+1=3. Others have claimed that the conceivability of a scenario may not entail its possibility.\n\nThe philosophical work of Stefano Gualeni focuses on the use of virtual worlds to materialize thought experiments and to playfully negotiate philosophical ideas. His arguments were originally presented in his 2015 book \"Virtual Worlds as Philosophical Tools\".\n\nGualeni's argument is that the history of philosophy has, until recently, merely been the history of written thought, and digital media can complement and enrich the limited and almost exclusively linguistic approach to philosophical thought. He considers virtual worlds to be philosophically viable and advantageous in contexts like those of thought experiments, when the recipients of a certain philosophical notion or perspective are expected to objectively test and evaluate different possible courses of action, or in cases where they are confronted with interrogatives concerning non-actual or non-human phenomenologies .\n\nAmong the most visible thought experiments designed by Stefano Gualeni:\n\nOther examples of playful, interactive thought experiments:\n\n\n\n\n\n\n\n"}
{"id": "491097", "url": "https://en.wikipedia.org/wiki?curid=491097", "title": "Variational principle", "text": "Variational principle\n\nA variational principle is a scientific principle used within the calculus of variations, which develops general methods for finding functions which extremize the value of quantities that depend upon those functions. For example, to answer this question: \"What is the shape of a chain suspended at both ends?\" we can use the variational principle that the shape must minimize the gravitational potential energy.\n\nAny physical law which can be expressed as a variational principle describes a self-adjoint operator. These expressions are also called Hermitian. Such an expression describes an invariant under a Hermitian transformation.\n\nFelix Klein's Erlangen program attempted to identify such invariants under a group of transformations. In what is referred to in physics as Noether's theorem, the Poincaré group of transformations (what is now called a gauge group) for general relativity defines symmetries under a group of transformations which depend on a variational principle, or action principle.\n\n\n"}
{"id": "24709966", "url": "https://en.wikipedia.org/wiki?curid=24709966", "title": "Vicious circle principle", "text": "Vicious circle principle\n\nThe vicious circle principle is a principle that was endorsed by many predicativist mathematicians in the early 20th century to prevent contradictions. The principle states that no object or property may be introduced by a definition that depends on that object or property itself. In addition to ruling out definitions that are explicitly circular (like \"an object has property P iff it is not next to anything that has property P\"), this principle rules out definitions that quantify over domains which include the entity being defined. Thus, it blocks Russell's paradox, which defines a set S that contains all sets that don't contain themselves. This definition is blocked because it defines a new set in terms of the totality of all sets, of which this new set would itself be a member.\n\nHowever, it also blocks one standard definition of the natural numbers. First, we define a property as being \"hereditary\" if, whenever a number \"n\" has the property, so does \"n\" + 1. Then we say that \"x\" has the property of being a natural number if and \"only\" if it has every hereditary property that 0 has. This definition is blocked, because it defines \"natural number\" in terms of the totality of all hereditary properties, but \"natural number\" itself would be such a hereditary property, so the definition is circular in this sense.\n\nMost modern mathematicians and philosophers of mathematics think that this particular definition is not circular in any problematic sense, and thus they reject the vicious circle principle. But it was endorsed by many early 20th century researchers including Bertrand Russell and Henri Poincaré. On the other hand, Frank P. Ramsey and Rudolf Carnap accepted the ban on explicit circularity, but argued against the ban on circular quantification. After all, the definition \"let T be the tallest man in the room\" defines T by means of quantification over a domain (men in the room) of which T is a member. But this is not problematic, they suggest, because the definition doesn't actually create the person, but merely shows how to pick him out of the totality. Similarly, they suggest, definitions don't actually create sets or properties or objects, but rather just give one way of picking out the already existing entity from the collection of which it is a part. Thus, this sort of circularity in terms of quantification can't cause any problems.\n\nThis principle was the reason for Russell's development of the ramified theory of types rather than the theory of simple types. (See \"Ramified Hierarchy and Impredicative Principles\".)\nAn analysis of the paradoxes to be avoided shows that they all result from a kind of vicious circle. The vicious circles in question arise from supposing that a collection of objects may contain members which can only be defined by means of the collection as a whole. Thus, for example, the collection of propositions will be supposed to contain a proposition stating that “all propositions are either true or false.” It would seem, however, that such a statement could not be legitimate unless “all propositions” referred to some already definite collection, which it cannot do if new propositions are created by statements about “all propositions.” We shall, therefore, have to say that statements about “all propositions” are meaningless.… The principle which enables us to avoid illegitimate totalities may be stated as follows: “Whatever involves all of a collection must not be one of the collection”; or, conversely: “If, provided a certain collection had a total, it would have members only definable in terms of that total, then the said collection has no total.” We shall call this the “vicious-circle principle,” because it enables us to avoid the vicious circles involved in the assumption of illegitimate totalities. (Whitehead and Russell 1910, 37) (quoted in the Stanford Encyclopedia of Philosophy entry on Russell's Paradox)\n\n\n"}
