{"id": "38235255", "url": "https://en.wikipedia.org/wiki?curid=38235255", "title": "Affirmation and negation", "text": "Affirmation and negation\n\nIn linguistics and grammar, affirmation and negation (abbreviated respectively ' and ') are the ways that grammar encode negative and positive polarity in verb phrases, clauses, or other utterances. Essentially an affirmative (positive) form is used to express the validity or truth of a basic assertion, while a negative form expresses its falsity. Examples are the sentences \"Jane is here\" and \"Jane is not here\"; the first is affirmative, while the second is negative.\n\nThe grammatical category associated and with affirmative and negative is called polarity. This means that a sentence, verb phrase, etc. may be said to have either affirmative or negative polarity (its polarity may be either affirmative or negative). Affirmative is typically the unmarked polarity, whereas a negative statement is marked in some way, whether by a negating word or particle such as English \"not\", an affix such as Japanese -\"nai\", or by other means, which reverses the meaning of the predicate. The process of converting affirmative to negative is called negation – the grammatical rules for negation vary from language to language, and a given language may have more than one method of doing so.\n\nAffirmative and negative responses (especially, though not exclusively, to questions) are often expressed using particles or words such as \"yes\" and \"no\", where \"yes\" is the affirmative and \"no\" the negative particle.\n\nSpecial affirmative and negative words (particles) are often found in responses to questions, and sometimes to other assertions by way of agreement or disagreement. In English, these are \"yes\" and \"no\" respectively, in French \"oui\", \"si\" and \"non\", in Swedish \"ja\", \"jo\" and \"nej\", and so on. Not all languages make such common use of particles of this type; in some (such as Welsh) it is more common to repeat the verb or another part of the predicate, with or without negation accordingly.\n\nComplications sometimes arise in the case of responses to negative statements or questions; in some cases the response that confirms a negative statement is the negative particle (as in English: \"You're not going out? No.\"), but in some languages this is reversed. Some languages have a distinct form to answer a negative question, such as French \"si\" and Swedish \"jo\" (these serve to contradict the negative statement suggested by the first speaker).\n\nLanguages have a variety of grammatical rules for converting affirmative verb phrases or clauses into negative ones.\n\nIn many languages, an affirmative is made negative by the addition of a particle, meaning \"not\". This may be added before the verb phrase, as with the Spanish \"no\":\nOther examples of negating particles preceding the verb phrase include Italian \"non\", Russian не \"nye\" and Polish \"nie\" (they can also be found in constructed languages: \"ne\" in Esperanto and \"non\" in Interlingua). In some other languages the negating particle follows the verb or verb phrase, as in Dutch:\nParticles following the verb in this way include \"not\" in archaic and dialectal English (\"you remember not\"), \"nicht\" in German (\"ich schlafe nicht\", \"I am not sleeping\"), and \"inte\" in Swedish (\"han hoppade inte\", \"he did not jump\").\n\nIn French, particles are added both before the verb phrase (\"ne\") and after the verb (\"pas\"):\nHowever, in colloquial French the first particle is often omitted: \"Je sais pas\". Similar use of two negating particles can also be found in Afrikaans: \"Hy kan nie Afrikaans praat nie\" (\"He cannot speak Afrikaans\").\n\nIn standard Modern English, negation is achieved by adding \"not\" after an auxiliary verb (which here means one of a special grammatical class of verbs that also includes forms of the copula \"be\"; see English auxiliaries). If no such verb is present then the dummy auxiliary \"do\" (\"does\", \"did\") is introduced – see \"do\"-support. For example:\nDifferent rules apply in subjunctive, imperative and non-finite clauses. For more details see . (In Middle English, the particle \"not\" could follow any verb, e.g. \"I see not the horse.\")\n\nIn some languages, like Welsh, verbs have special inflections to be used in negative clauses. (In some language families, this may lead to reference to a negative mood.) An example is Japanese, which conjugates verbs in the negative after adding the suffix \"-nai\" (indicating negation), e.g. \"taberu\" (\"eat\") and \"tabenai\" (\"do not eat\"). It could be argued that English has joined the ranks of these languages, since negation requires the use of an auxiliary verb and a distinct syntax in most cases; the form of the basic verb can change on negation, as in \"he sings\" vs. \"he doesn't sing\". Zwicky and Pullum have shown that \"n't\" is an inflectional suffix, not a clitic or a derivational suffix.\n\nComplex rules for negation also apply in Finnish; see . In some languages negation may also affect the dependents of the verb; for example in some Slavic languages, such as Russian, the case of a direct object often changes from accusative to genitive when the verb is negated.\n\nNegation can be applied not just to whole verb phrases, clauses or sentences, but also to specific elements (such as adjectives and noun phrases) within sentences. Ways in which this can be done again depend on the grammar of the language in question. English generally places \"not\" before the negated element, as in \"I witnessed not a debate, but a war.\" There are also negating affixes, such as the English prefixes \"non-\", \"un-\", \"in-\", etc. Such elements are called privatives.\n\nThere also exist elements which carry a specialized negative meaning, including pronouns such as \"nobody\", \"none\" and \"nothing\", determiners such as \"no\" (as in \"no apples\"), and adverbs such as \"never\", \"no longer\" and \"nowhere\".\n\nAlthough such elements themselves have negative force, in some languages a clause in which they appear is additionally marked for ordinary negation. For example, in Russian, \"I see nobody\" is expressed as я никого́ не ви́жу \"ja nikovó nye vízhu\", literally \"I nobody not see\" – the ordinary negating particle не \"nye\" (\"not\") is used in addition to the negative pronoun никого́ \"nikovó\" (\"nobody\"). Italian behaves in a similar way: \"Non ti vede nessuno\", \"nobody can see you\", although \"Nessuno ti vede\" is also a possible clause with exactly the same meaning.\n\nIn Russian, all of the elements (\"not\", \"never\", \"nobody\", \"nowhere\") would appear together in the sentence in their negative form. In Italian, a clause works much as in Russian, but \"non\" does not have to be there, and can be there only before the verb if it precedes all other negative elements: \"Tu non porti mai nessuno da nessuna parte\". \"Nobody ever brings you anything here\", however, could be translated \"Nessuno qui ti porta mai niente\" or \"Qui non ti porta mai niente nessuno\". In French, where simple negation is performed using \"ne ... pas\" (see above), specialized negatives appear in combination with the first particle (\"ne\"), but \"pas\" is omitted:\nIn Ancient Greek, a simple negative (οὐ \"ou\" \"not\" or μή \"mḗ\" \"not (modal)\") following another simple or compound negative (e.g. οὐδείς \"oudeís\" \"nobody\") results in an affirmation, whereas a compound negative following a simple or compound negative strengthens the negation:\n\nSimple grammatical negation of a clause in principle has the effect of converting a proposition to its logical negation – replacing an assertion that something is the case by an assertion that it is not the case.\n\nIn some cases, however, particularly when a particular modality is expressed, the semantic effect of negation may be somewhat different. For example, in English, the meaning of \"you must not go\" is not in fact the exact negation of that of \"you must go\" – this would be expressed as \"you don't have to go\" or \"you needn't go\". The negation \"must not\" has a stronger meaning (the effect is to apply the logical negation to the following infinitive rather than to the full clause with \"must\"). For more details and other similar cases, see the relevant sections of English modal verbs.\n\nIn some cases, by way of irony, an affirmative statement may be intended to have the meaning of the corresponding negative, or vice versa. For examples see antiphrasis and sarcasm.\n\nFor the use of double negations or similar as understatements (\"not unappealing\", \"not bad\", etc.) see litotes.\n\n\n\n"}
{"id": "2235265", "url": "https://en.wikipedia.org/wiki?curid=2235265", "title": "Anna Karenina principle", "text": "Anna Karenina principle\n\nThe Anna Karenina principle states that a deficiency in any one of a number of factors dooms an endeavor to failure. Consequently, a successful endeavor (subject to this principle) is one where every possible deficiency has been avoided.\n\nThe name of the principle derives from Leo Tolstoy's book \"Anna Karenina\", which begins:\n\nAll happy families are alike; each unhappy family is unhappy in its own way.In other words: in order to be happy, a family must be successful on \"each and every one\" of \"a\" \"range\" of criteria e.g.: sexual attraction, money issues, parenting, religion, in-laws. Failure on only \"one\" of these counts leads to \"un\"happiness. Thus there are more ways for a family to be unhappy than happy.\n\nIn statistics, the term \"Anna Karenina principle\" is used to describe significance tests: there are any number of ways in which a dataset may violate the null hypothesis and only one in which all the assumptions are satisfied.\n\nThe Anna Karenina principle was popularized by Jared Diamond in his book \"Guns, Germs and Steel\". Diamond uses this principle to illustrate why so few wild animals have been successfully domesticated throughout history, as a deficiency in any one of a great number of factors can render a species undomesticable. Therefore, all successfully domesticated species are not so because of a particular positive trait, but because of a lack of any number of possible negative traits. In chapter 9, six groups of reasons for failed domestication of animals are defined:\n\n\nMoore describes applications of the \"Anna Karenina principle\" in ecology:\n\nSuccessful ecological risk assessments are all alike; every unsuccessful ecological risk assessment fails in its own way. Tolstoy posited a similar analogy in his novel Anna Karenina : \"Happy families are all alike; every unhappy family is unhappy in its own way.\" By that, Tolstoy meant that for a marriage to be happy, it had to succeed in several key aspects. Failure on even one of these aspects, and the marriage is doomed . . . the Anna Karenina principle also applies to ecological risk assessments involving multiple stressors.\n\nMuch earlier, \"Aristotle\" states the same principle in the \"Nicomachean Ethics\" (Book 2):\n\nAgain, it is possible to fail in many ways (for evil belongs to the class of the unlimited, as the Pythagoreans conjectured, and good to that of the limited), while to succeed is possible only in one way (for which reason also one is easy and the other difficult – to miss the mark easy, to hit it difficult); for these reasons also, then, excess and defect are characteristic of vice, and the mean of virtue; For men are good in but one way, but bad in many.\n\nMany experiments and observations of groups of humans, animals, trees, grassy plants, stockmarket prices, and changes in the banking sector proved the modified Anna Karenina principle.\n\nBy studying the dynamics of correlation and variance in many systems facing external, or environmental, factors, we can typically, even before obvious symptoms of crisis appear, predict when one might occur, as correlation between individuals increases, and, at the same time, variance (and volatility) goes up... All well-adapted systems are alike, all non-adapted systems experience maladaptation in their own way... But in the chaos of maladaptation, there is an order. It seems, paradoxically, that as systems become more different they actually become more correlated within limits.\n\nThis effect is proved for many systems: from the adaptation of healthy people to a change in climate conditions to the analysis of fatal outcomes in oncological and cardiological clinics. The same effect is found in the stock market. The applicability of these two statistical indicators of stress, simultaneous increase of variance and correlations, for diagnosis of social stress in large groups was examined in the prolonged stress period preceding the 2014 Ukrainian economic and political crisis. There was a simultaneous increase in the total correlation between the 19 major public fears in the Ukrainian society (by about 64%) and also in their statistical dispersion (by 29%) during the pre-crisis years.\n\nVladimir Arnold in his book \"Catastrophe Theory\" describes \"The Principle of Fragility of Good Things\" which in a sense supplements the Principle of Anna Karenina: good systems must meet simultaneously a number of requirements; therefore, they are more fragile:\n\n... for systems belonging to the singular part of the stability boundary a small change of the parameters is more likely to send the system into the unstable region than into the stable region. This is a manifestation of a general principle stating that all good things (e.g. stability) are more fragile than bad things. It seems that in good situations a number of requirements must hold simultaneously, while to call a situation bad even one failure suffices. \n"}
{"id": "26276222", "url": "https://en.wikipedia.org/wiki?curid=26276222", "title": "Bateman's principle", "text": "Bateman's principle\n\nBateman's principle, in evolutionary biology, is that in most species, variability in reproductive success (or reproductive variance) is greater in males than in females. It was first proposed by Angus John Bateman (1919–1996), an English geneticist. Bateman suggested that, since males are capable of producing millions of sperm cells with little effort, while females invest much higher levels of energy in order to nurture a relatively small number of eggs, the female plays a significantly larger role in their offspring's reproductive success. Bateman’s paradigm thus views females as the limiting factor of parental investment, over which males will compete in order to copulate successfully.\n\nAlthough Bateman's principle served as a cornerstone for the study of sexual selection for many decades, it has recently been subject to criticism. Attempts to reproduce Bateman's experiments in 2012 and 2013 were unable to support his conclusions. Some scientists have criticized Bateman's experimental and statistical methods, or pointed out conflicting evidence, while others have defended the veracity of the principle and cited evidence in support of it.\n\nTypically it is the females who have a relatively larger investment in producing each offspring. Bateman attributed the origin of the unequal investment to the differences in the production of gametes: sperm are cheaper than eggs. A single male can easily fertilize all females' eggs; she will not produce more offspring by mating with more than one male. A male is capable of fathering more offspring if he mates with several females. By and large, a male's potential reproductive success is limited by the number of females he mates with, whereas a female's potential reproductive success is limited by how many eggs she can produce. According to Bateman's principle, this results in sexual selection, in which males compete with each other, and females become choosy in which males to mate with. Thus, as a result of being anisogamous, males are fundamentally promiscuous, and females are fundamentally selective.\n\nBateman initially published his review in 1948. He was a botanist, contributing to the literature of sexual selection only once in his lifetime. Bateman initially saw his study on Drosophila to be a test of Charles Darwin’s doctrine of sexual selection. He saw Darwin’s theory of natural selection not as flawed, but as incomplete. He felt that if he were to provide a concrete demonstration of how sexual selection played a role in the reproductive success of certain species, he could explain the gap between Darwin’s ideas and sexual dimorphism.\n\nAlthough it is common to confuse Bateman's ideas with those of later scientists, his principle can be expressed in three simple statements. The first is that male reproductive success increases with the number of mates they attempt to copulate with, while female reproductive success does not. The second is that male reproductive success will show greater variance than female. The third is that sexual selection will have a greater effect on the sex with greater variance in reproductive success.\n\nThroughout his research, Bateman conducted experiments using fruit flies in order to observe their copulation and sexual behavior. A total of six series of experiments were conducted with the fruit fly \"Drosophila melanogaster\", using three to five individuals of each sex. Each trial ran for three or four days. Some ran to completion without the transfer of the Drosophila from one environment (bottle) to another. In the others, Bateman transferred the flies and their eggs to a new bottle every day. Bateman also varied the age of the flies depending on the experiment, with an age gap between one and six days total. He never watched the flies' copulations. The flies used were from several inbred strains, which meant they could be identified by their specific inbred strain. Therefore, he inferred the number of involved mates based on the number of offspring that were later found to have mutations from both a male and a female. The difficulty that arose was that if a female Drosophila had copulated with five males and only one larva survived, Bateman would not be able to account for the other four copulations.\n\nAnalysis of the data collected in sets one through four showed that the males' reproductive success, estimated as the number of sired offspring, increased at a steady rate until a total of three mates were reached. It is important to note that Bateman kept the sex ratio of males to females completely even throughout his trials. But after surpassing three mates, male reproductive success began to fall. Female reproductive success also increased with number of mates, but much more gradually than that of the males. The second series of data collected in sets five and six illustrated a dramatically different outcome. Male reproductive success increased at a steady and steep rate, never dropping. Female reproductive success, on the other hand, plateaued after a single mate. Bateman focused mainly on the second series of data when discussing his results. His main conclusion was that the reproductive success of females does not increase with an influx of mates, as one fit mate was enough to successfully complete fertilization. This is often referred to as Bateman’s Gradient.\n\nThroughout 2012 and 2013, Gowaty, Kim, and Anderson took it upon themselves to repeat Bateman's experiment in its entirety. While reproducing his tests, the same fly strains and mutations were used in order to maintain the same methodology. However, one of the 11 strains that Bateman used had gone extinct, and was thus replaced. (Tang-Martinez 2012)\n\nGowaty, Kim, and Anderson found that upon combining certain strains with one another, the offspring were unable to survive to adulthood. (Gowaty, Kim, & Anderson 2012) Thus, Bateman’s numbers regarding the number of individuals not having mated was higher than the actual number. Likewise, his estimate of those that mated with one or more mates was too low. This was valid for both the males and females of this species.\n\nGowaty desired to further explore the reasoning behind the premature death of the Drosophila. She began doing so by running monogamy trials between different strains of flies and found that 25% of the offspring died due to becoming double mutants. (Gowaty 2013) Bateman thought his work fit within the lines of Mendel’s laws of genetics, while Gowaty proved otherwise. The 1948 experiments inferred reproductive success based on the number of adults living by the end of the trial. In reality, many factors were left out of the equation when calculating reproductive success as a function of the number of mates, which had the ability to completely dislodge the accuracy behind Bateman's results. Gowaty was not able to confirm Bateman's conclusions and found no evidence for sexual selection in the experiment. (Gowaty 2013; Tang-Martinez 2012)\n\nNevertheless, some modern experiments between the relationship of number of mates and the reproductive success of males and females support Bateman's principle. Julie Collet conducted an experiment with a population of red jungle fowl. A total of thirteen replicate groups of three males and four females were monitored for ten days. In this experiment, the sex ratio was biased toward females. A male's reproductive success was calculated using the proportion of embryos fathered to the total number of embryos produced by all the females he mated with. The total sexual selection opportunity was calculated using the following formula.\n\nThe \"σ\" represents the variance in RS, while the is the square mean of reproductive success of members of one sex in a group.\n\nIn 2013, Fritzsche and Arnqvist tested Bateman's principle by estimating sexual selection between males and females in four seed beetles. They used a unique experimental design that showed sexual selection to be greater in males than in females. In contrast, sexual selection was also shown to be stronger for females in role-reversed species. They suggested that the Bateman gradient is typically the most accurate and informative measure of sexual selection between different sexes and species (Fritzsche, K. & Arnqvist, G).\n\nMore than 60 years later, Bateman's principle has received considerable attention. Sutherland argued that males' higher variance in reproductive success may result from random mating and coincidence. Hubbell and Johnson suggested that variance in reproductive success can be greatly influenced by the time and allocations of mating. In 2005, Gowaty and Hubbell suggested that mating tendencies are subject to change depending on certain strategies. They argued that there are cases in which males can be more selective than females, whereas Bateman suggested that his paradigm would be “almost universal” among sexually reproducing species. Critics proposed that females might be more subject to sexual selection than males, but not in all circumstances.\n\nExperimental and statistical criticisms followed. Until approximately a decade ago, critics of Bateman’s model focused on his experimental design. In recent years, they have shifted attention to the actual experimental and statistical calculations Bateman published throughout his trials. Birkhead wrote a 2000 review arguing that since Bateman’s experiments lasted only three to four days, the female fruit fly, \"Drosophila melanogaster\", may not have needed to mate repeatedly, as it can store sperm for up to four days; if Bateman had used a species in which females had to copulate more often to fertilize their eggs, the results might have been different. Snyder and Gowaty conducted the first in-depth analysis of the data in Bateman’s 1948 paper. They found sampling biases, mathematical errors, and selective presentation of data.\n\nA 2012 review by Zuleyma Tang-Martínez concluded that various empirical and theoretical studies, especially Gowaty's reproduction of Bateman's original experiment, pose a major challenge to Bateman's conclusions, and that Bateman's principle should be considered an unproven hypothesis in need of further reexamination. According to Tang-Martínez, \"modern data simply don't support most of Bateman's and Trivers's predictions and assumptions.\"\n\nA 2016 review confirmed Darwinian sex roles across the animal kingdom, concluding that \"sexual selection, as captured by standard Bateman metrics, is indeed stronger in males than in females and that it is evolutionarily tied to sex biases in parental care and sexual dimorphism.\"\n\nOne error source that have been shown to give an illusion of greater differential in reproductive success in males than in females genetically is that chromosome effects cause a greater percentage of mutations to be lethal before even reaching sexual maturity in males than in females.\n\nThe assumption that any differential in reproductive success between males and females among the individuals that do reach sexual maturity must be due to sexual selection in the current population is also subject to criticism, such as the possibility of remnants of sexually selected traits in a previous species from which a new species have evolved can be negatively selected due to costs in nutrients and weakened immune systems and that such negative selection would cause a higher difference in reproductive success in males than in females even without any still ongoing sexual selection. Since lower degrees of selection during times of stable environment allows genetic variation to build up by random mutations and allow some individuals in a population to survive environmental change while strong constant selection offsets the effect and increases the risk of the entire population dying out during catastrophic environmental change due to less genetic variation, constant loss of genetic variation caused by sexual selection have been suggested as a factor contributing to higher extinction rates in more sexually dimorphic species besides the nutrient, immunity and other costs of the ornaments themselves. While the ornament cost risk would only be removed when the ornaments have been eliminated by selection, the genetic variation model predicts that the species ability to survive would improve significantly even at an early stage of reduction of sexual dimorphism due to other adaptive mutations arising and surviving due to minimal selection during times of stable environment while the genes causing sexually dimorphic anatomy have only in small part been affected by the mutations. Applied to human evolution, this model can explain why early Homo sapiens display a significantly increased adaptability to environmental change already at its early divergence from Homo erectus that had a high muscular sexual dimorphism, as well as why human anatomy through the history of Homo sapiens show a diversification during times of stable climate and a selective loss of the more robust male forms during environmental change that does not recover during later stability, continuing through the loss of many robust characteristics in regional bottlenecks as recent as the end of the Ice Age and the time around the agricultural revolution. It also explains genetic evidence of human genetic diversity increasing during stable environmental periods and being reduced during bottlenecks related to changes in the environment.\n\nRecently, DNA testing has permitted more detailed investigation of mating behavior in numerous species. The results, in many cases, have been cited as evidence against Bateman's principle.\n\nUntil recently, most bird species were believed to be sexually monogamous. DNA paternity testing, however, has shown that in nearly 90% of bird species, females copulate with multiple males during each breeding season. The superb fairy wren is socially monogamous, but 95% of its clutches contain young fathered by extra-pair males. Up to 87% of tree swallow clutches, 75% of coal tit clutches, and 70% of reed bunting clutches contain young fathered by extra-pair males. Even female waved albatrosses, which typically mate for life, are sexually promiscuous, with 17% of young fathered by extra-pair males.\n\nIn many primate species, females solicit sex from males and may mate with more than one male in quick succession. Female lions may mate 100 times per day with different males while they are in estrus. Females of the pseudoscorpion species, \"Cordylochernes scorpioides\", have been shown to have higher reproductive success when mated with more than one male.\n\nThere are a number of claims about sex differences in humans said by evolutionary psychologists to be well supported that are not logically predicted by Bateman's principle. One example that have been pointed out is language ability that is said to be more developed in women than in men. It is argued that language, being a derived human trait, is not predicted by a model that assumes males to be more selected than females to be present to a higher degree in women than in men but the opposite. The objection about a trade-off between spatial ability and language is found lacking due to the existence of animals that outperform humans on spatial tasks such as birds of prey and bats while having much smaller and more rapidly maturing brains than humans, while great ape language is limited and ungrammatical at best despite great apes being much closer to humans in brain capacity than bats or birds of prey are, showing that the system requirements for spatial ability are much lower than those for language and therefore not a serious trade-off for them. The generalization of Bateman's principle to societies lacking modern birth control is also argued to be incompatible with such generalization of the \"nuns more successful at celibacy than monks\" clause of the female erotic plasticity model, as absence of modern contraceptives would mean that absence of reproduction for many men required that many men had no sexual relationships with women while male sexual exclusivity to a particular gender, if present, would have precluded same-sex relationships as a form of non-celibacy for men who could not find a female mate as well. It is also mentioned that the evolutionary psychologists themselves claim that women are sexually picky due to an evolutionary history without modern contraception, while it is pointed out that the notion of celibacy being impossible for men in general with only a small number of individual exceptions is incompatible with the notion of evolution making celibacy inevitable for a significant percentage of men.\n\nThe most well-known exceptions to Bateman's principle are the existence of sex-role reversed species such as pipefish (seahorses), phalaropes and jacanas in which the males perform the majority of the parental care, and are cryptic while the females are highly ornamented and territorially aggressive (; ; ). \n\nIn these species, however, the typical fundamental sex differences are reversed: females have a faster reproductive rate than males (and thus greater reproductive variance), and males have greater assurance of genetic parentage than do females . Consequently, reversals in sex roles and reproductive variance are consistent with Bateman's principle, and with Robert Trivers's parental investment theory.\n\n\n\n"}
{"id": "10000937", "url": "https://en.wikipedia.org/wiki?curid=10000937", "title": "Category (Kant)", "text": "Category (Kant)\n\nIn Kant's philosophy, a category ( in the original or \"Kategorie\" in modern German) is a pure concept of the understanding (\"Verstand\"). A Kantian category is a characteristic of the appearance of any object in general, before it has been experienced. Kant wrote that \"They are concepts of an object in general….\" Kant also wrote that, \"…pure cоncepts [Categories] of the undеrstanding which apply to objects of intuition in general….\" Such a category is not a classificatory division, as the word is commonly used. It is, instead, the condition of the possibility of objects in general, that is, objects as such, any and all objects, not specific objects in particular.\n\nThe word comes from the Greek κατηγορία, \"katēgoria\", meaning \"that which can be said, predicated, or publicly declared and asserted, about something.\" A category is an attribute, property, quality, or characteristic that can be predicated of a thing. \"…I remark concerning the categories…that their logical employment consists in their use as predicates of objects.\" Kant called them \"ontological predicates.\"\n\nA category is that which can be said of everything in general, that is, of anything that is an object. John Stuart Mill wrote: \"The Categories, or Predicaments — the former a Greek word, the latter its literal translation in the Latin language — were believed to be an enumeration of all things capable of being named, an enumeration by the \"summa genera\" (highest kind), i.e., the most extensive classes into which things could be distributed, which, therefore, were so many highest Predicates, one or other of which was supposed capable of being affirmed with truth of every nameable thing whatsoever.\"\n\nAristotle had claimed that the following ten predicates or categories could be asserted of anything in general: substance, quantity, quality, relation, action, affection (passivity), place, time (date), position, and state. These are supposed to be the qualities or attributes that can be affirmed of each and every thing in experience. Any particular object that exists in thought must have been able to have the Categories attributed to it as possible predicates because the Categories are the properties, qualities, or characteristics of any possible object in general. The Categories of Aristotle and Kant are the general properties that belong to all things without expressing the peculiar nature of any particular thing. Kant appreciated Aristotle's effort, but said that his table was imperfect because \" … as he had no guiding principle, he merely picked them up as they occurred to him...\"\n\nThe Categories do not provide knowledge of individual, particular objects. Any object, however, must have Categories as its characteristics if it is to be an object of experience. It is presupposed or assumed that anything that is a specific object must possess Categories as its properties because Categories are predicates of an object in general. An object in general does not have all of the Categories as predicates at one time. For example, a general object cannot have the qualitative Categories of reality and negation at the same time. Similarly, an object in general cannot have both unity and plurality as quantitative predicates at once. The Categories of Modality exclude each other. Therefore, a general object cannot simultaneously have the Categories of possibility/impossibility and existence/non–existence as qualities.\n\nSince the Categories are a list of that which can be said of every object, they are related only to human language. In making a verbal statement about an object, a speaker makes a judgment. A general object, that is, every object, has attributes that are contained in Kant's list of Categories. In a judgment, or verbal statement, the Categories are the predicates that can be asserted of every object and all objects.\n\nKant believed that the ability of the human understanding (German: \"Verstand\", Greek: \"dianoia\" \"διάνοια\", Latin: \"ratio\") to think about and know an object is the same as the making of a spoken or written judgment about an object. According to him, \"Our ability to judge is equivalent to our ability to think.\"\nA judgment is the thought that a thing is known to have a certain quality or attribute. For example, the sentence \"The rose is red\" is a judgment. Kant created a table of the forms of such judgments as they relate to all objects in general.\n\nThis table of judgments was used by Kant as a model for the table of categories. Taken together, these twelvefold tables constitute the formal structure for Kant's architectonic conception of his philosophical system.\n\nCategories are entirely different from the appearances of objects. According to Kant, in order to relate to specific phenomena, categories must be \"applied\" through time. The way that this is done is called a schema.\n\nArthur Schopenhauer, in his criticism of the Kantian philosophy, found many errors in Kant's use of the Categories of Quality, Quantity, Relation, and Modality. Schopenhauer also noted that in accordance with Kant's claim, non-human animals would not be able to know objects. Animals would only know impressions on their sense organs, which Kant mistakenly calls perception.\n\n\n"}
{"id": "714069", "url": "https://en.wikipedia.org/wiki?curid=714069", "title": "Church–Turing–Deutsch principle", "text": "Church–Turing–Deutsch principle\n\nIn computer science and quantum physics, the Church–Turing–Deutsch principle (CTD principle) is a stronger, physical form of the Church–Turing thesis formulated by David Deutsch in 1985.\n\nThe principle states that a universal computing device can simulate every physical process.\n\nThe principle was stated by Deutsch in 1985 with respect to finitary machines and processes. He observed that classical physics, which makes use of the concept of real numbers, cannot be simulated by a Turing machine, which can only represent computable reals. Deutsch proposed that quantum computers may actually obey the CTD principle, assuming that the laws of quantum physics can completely describe every physical process.\n\nAn earlier version of this thesis for classical computers was stated by Alan Turing's friend and student Robin Gandy in 1980.\n\n\n"}
{"id": "5653", "url": "https://en.wikipedia.org/wiki?curid=5653", "title": "Clarke's three laws", "text": "Clarke's three laws\n\nBritish science fiction writer Arthur C. Clarke formulated three adages that are known as Clarke's three laws, of which the third law is the best known and most widely cited. They were part of his ideas in his extensive writings about the future. These so-called laws include:\n\n\nOne account claimed that Clarke's \"laws\" were developed after the editor of his works in French started numbering the author's assertions. All three laws appear in Clarke's essay \"Hazards of Prophecy: The Failure of Imagination\", first published in \"Profiles of the Future\" (1962). However, they were not published at the same time. Clarke's first law was proposed in the 1962 edition of the essay, as \"Clarke's Law\" in \"Profiles of the Future\".\n\nThe second law is offered as a simple observation in the same essay but its status as Clarke's second law was conferred by others. It was initially a derivative of the first law and formally became Clarke's second law where the author proposed the third law in the 1973 revision of \"Profiles of the Future,\" which included an acknowledgement\".\" It was also here that Clarke wrote about the third law in these words: \"As three laws were good enough for Newton, I have modestly decided to stop there\".\n\nThe third law, despite being latest stated by a decade, is the best known and most widely cited. It appears only in the 1973 revision of the \"Hazards of Prophecy\" essay. It echoes a statement in a 1942 story by Leigh Brackett: \"Witchcraft to the ignorant, … simple science to the learned\". Earlier examples of this sentiment may be found in \"Wild Talents\" (1932) by Charles Fort: \"...a performance that may some day be considered understandable, but that, in these primitive times, so transcends what is said to be the known that it is what I mean by magic,\" and in the short story \"The Hound of Death\" (1933) by Agatha Christie: \"The supernatural is only the natural of which the laws are not yet understood.\"\n\nClarke gave an example of the third law when he said that while he \"would have believed anyone who told him back in 1962 that there would one day exist a book-sized object capable of holding the content of an entire library, he would never have accepted that the same device could find a page or word in a second and then convert it into any typeface and size from Albertus Extra Bold to Zurich Calligraphic\", referring to his memory of \"seeing and hearing Linotype machines which slowly converted ‘molten lead into front pages that required two men to lift them’\".\n\nA fourth law has been proposed for the canon, despite Clarke's declared intention of stopping at three laws. Geoff Holder quotes: \"For every expert, there is an equal and opposite expert,\" which is part of American economist Thomas Sowell's \"For every expert, there is an equal and opposite expert, but for every fact there is not necessarily an equal and opposite fact\", from his 1995 book \"The Vision of the Anointed\".\n\nThe third law has inspired many snowclones and other variations:\n\n\nA of the third law is\n\n\nThe third law has been:\n\n\n\n"}
{"id": "2381958", "url": "https://en.wikipedia.org/wiki?curid=2381958", "title": "Conceptual model", "text": "Conceptual model\n\nA conceptual model is a representation of a system, made of the composition of concepts which are used to help people know, understand, or simulate a subject the model represents. It is also a set of concepts. Some models are physical objects; for example, a toy model which may be assembled, and may be made to work like the object it represents.\n\nThe term \"conceptual model\" may be used to refer to models which are formed after a conceptualization or generalization process. Conceptual models are often abstractions of things in the real world whether physical or social. Semantic studies are relevant to various stages of concept formation. Semantics is basically about concepts, the meaning that thinking beings give to various elements of their experience.\n\nThe term \"conceptual model\" is normal. It could mean \"a model of concept\" or it could mean \"a model that is conceptual.\" A distinction can be made between \"what models are\" and \"what models are made of\". With the exception of iconic models, such as a scale model of Winchester Cathedral, most models are concepts. But they are, mostly, intended to be models of real world states of affairs. The value of a model is usually directly proportional to how well it corresponds to a past, present, future, actual or potential state of affairs. A model of a concept is quite different because in order to be a good model it need not have this real world correspondence. In artificial intelligence conceptual models and conceptual graphs are used for building expert systems and knowledge-based systems; here the analysts are concerned to represent expert opinion on what is true not their own ideas on what is true.\n\nConceptual models (models that are conceptual) range in type from the more concrete, such as the mental image of a familiar physical object, to the formal generality and abstractness of mathematical models which do not appear to the mind as an image. Conceptual models also range in terms of the scope of the subject matter that they are taken to represent. A model may, for instance, represent a single thing (e.g. the \"Statue of Liberty\"), whole classes of things (e.g. \"the electron\"), and even very vast domains of subject matter such as \"the physical universe.\" The variety and scope of conceptual models is due to then variety of purposes had by the people using them.\nConceptual modeling is the activity of formally describing some aspects of the physical and social world around us for the purposes of understanding and communication.\"\n\nA conceptual model's primary objective is to convey the fundamental principles and basic functionality of the system which it represents. Also, a conceptual model must be developed in such a way as to provide an easily understood system interpretation for the models users. A conceptual model, when implemented properly, should satisfy four fundamental objectives.\n\n\nThe conceptual model plays an important role in the overall system development life cycle. Figure 1 below, depicts the role of the conceptual model in a typical system development scheme. It is clear that if the conceptual model is not fully developed, the execution of fundamental system properties may not be implemented properly, giving way to future problems or system shortfalls. These failures do occur in the industry and have been linked to; lack of user input, incomplete or unclear requirements, and changing requirements. Those weak links in the system design and development process can be traced to improper execution of the fundamental objectives of conceptual modeling. The importance of conceptual modeling is evident when such systemic failures are mitigated by thorough system development and adherence to proven development objectives/techniques.\n\nAs systems have become increasingly complex, the role of conceptual modelling has dramatically expanded. With that expanded presence, the effectiveness of conceptual modeling at capturing the fundamentals of a system is being realized. Building on that realization, numerous conceptual modeling techniques have been created. These techniques can be applied across multiple disciplines to increase the users understanding of the system to be modeled. A few techniques are briefly described in the following text, however, many more exist or are being developed. Some commonly used conceptual modeling techniques and methods include: workflow modeling, workforce modeling, rapid application development, object-role modeling, and the Unified Modeling Language (UML).\n\nData flow modeling (DFM) is a basic conceptual modeling technique that graphically represents elements of a system. DFM is a fairly simple technique, however, like many conceptual modeling techniques, it is possible to construct higher and lower level representative diagrams. The data flow diagram usually does not convey complex system details such as parallel development considerations or timing information, but rather works to bring the major system functions into context. Data flow modeling is a central technique used in systems development that utilizes the structured systems analysis and design method (SSADM).\n\nEntity-relationship modeling (ERM) is a conceptual modeling technique used primarily for software system representation. Entity-relationship diagrams, which are a product of executing the ERM technique, are normally used to represent database models and information systems. The main components of the diagram are the entities and relationships. The entities can represent independent functions, objects, or events. The relationships are responsible for relating the entities to one another. To form a system process, the relationships are combined with the entities and any attributes needed to further describe the process. Multiple diagramming conventions exist for this technique; IDEF1X, Bachman, and EXPRESS, to name a few. These conventions are just different ways of viewing and organizing the data to represent different system aspects.\n\nThe event-driven process chain (EPC) is a conceptual modeling technique which is mainly used to systematically improve business process flows. Like most conceptual modeling techniques, the event driven process chain consists of entities/elements and functions that allow relationships to be developed and processed. More specifically, the EPC is made up of events which define what state a process is in or the rules by which it operates. In order to progress through events, a function/ active event must be executed. Depending on the process flow, the function has the ability to transform event states or link to other event driven process chains. Other elements exist within an EPC, all of which work together to define how and by what rules the system operates. The EPC technique can be applied to business practices such as resource planning, process improvement, and logistics.\n\nThe dynamic systems development method uses a specific process called JEFFF to conceptually model a systems life cycle. JEFFF is intended to focus more on the higher level development planning that precedes a projects initialization. The JAD process calls for a series of workshops in which the participants work to identify, define, and generally map a successful project from conception to completion. This method has been found to not work well for large scale applications, however smaller applications usually report some net gain in efficiency.\n\nAlso known as Petri nets, this conceptual modeling technique allows a system to be constructed with elements that can be described by direct mathematical means. The petri net, because of its nondeterministic execution properties and well defined mathematical theory, is a useful technique for modeling concurrent system behavior, i.e. simultaneous process executions.\n\nState transition modeling makes use of state transition diagrams to describe system behavior. These state transition diagrams use distinct states to define system behavior and changes. Most current modeling tools contain some kind of ability to represent state transition modeling. The use of state transition models can be most easily recognized as logic state diagrams and directed graphs for finite-state machines.\n\nBecause the conceptual modeling method can sometimes be purposefully vague to account for a broad area of use, the actual application of concept modeling can become difficult. To alleviate this issue, and shed some light on what to consider when selecting an appropriate conceptual modeling technique, the framework proposed by Gemino and Wand will be discussed in the following text. However, before evaluating the effectiveness of a conceptual modeling technique for a particular application, an important concept must be understood; Comparing conceptual models by way of specifically focusing on their graphical or top level representations is shortsighted. Gemino and Wand make a good point when arguing that the emphasis should be placed on a conceptual modeling language when choosing an appropriate technique. In general, a conceptual model is developed using some form of conceptual modeling technique. That technique will utilize a conceptual modeling language that determines the rules for how the model is arrived at. Understanding the capabilities of the specific language used is inherent to properly evaluating a conceptual modeling technique, as the language reflects the techniques descriptive ability. Also, the conceptual modeling language will directly influence the depth at which the system is capable of being represented, whether it be complex or simple.\n\nBuilding on some of their earlier work, Gemino and Wand acknowledge some main points to consider when studying the affecting factors: the content that the conceptual model must represent, the method in which the model will be presented, the characteristics of the models users, and the conceptual model languages specific task. The conceptual models content should be considered in order to select a technique that would allow relevant information to be presented. The presentation method for selection purposes would focus on the techniques ability to represent the model at the intended level of depth and detail. The characteristics of the models users or participants is an important aspect to consider. A participant's background and experience should coincide with the conceptual models complexity, else misrepresentation of the system or misunderstanding of key system concepts could lead to problems in that systems realization. The conceptual model language task will further allow an appropriate technique to be chosen. The difference between creating a system conceptual model to convey system functionality and creating a system conceptual model to interpret that functionality could involve to completely different types of conceptual modeling languages.\n\nGemino and Wand go on to expand the affected variable content of their proposed framework by considering the focus of observation and the criterion for comparison. The focus of observation considers whether the conceptual modeling technique will create a \"new product\", or whether the technique will only bring about a more intimate understanding of the system being modeled. The criterion for comparison would weigh the ability of the conceptual modeling technique to be efficient or effective. A conceptual modeling technique that allows for development of a system model which takes all system variables into account at a high level may make the process of understanding the system functionality more efficient, but the technique lacks the necessary information to explain the internal processes, rendering the model less effective.\n\nWhen deciding which conceptual technique to use, the recommendations of Gemino and Wand can be applied in order to properly evaluate the scope of the conceptual model in question. Understanding the conceptual models scope will lead to a more informed selection of a technique that properly addresses that particular model. In summary, when deciding between modeling techniques, answering the following questions would allow one to address some important conceptual modeling considerations.\n\n\nAnother function of the simulation conceptual model is to provide a rational and factual basis for assessment of simulation application appropriateness.\n\nIn cognitive psychology and philosophy of mind, a mental model is a representation of something in the mind, but a mental model may also refer to a nonphysical external model of the mind itself.\n\nA metaphysical model is a type of conceptual model which is distinguished from other conceptual models by its proposed scope; a metaphysical model intends to represent reality in the broadest possible way. This is to say that it explains the answers to fundamental questions such as whether matter and mind are one or two substances; or whether or not humans have free will.\n\nAn epistemological model is a type of conceptual model whose proposed scope is the known and the knowable, and the believed and the believable.\n\nIn logic, a model is a type of interpretation under which a particular statement is true. Logical models can be broadly divided into ones which only attempt to represent concepts, such as mathematical models; and ones which attempt to represent physical objects, and factual relationships, among which are scientific models.\n\nModel theory is the study of (classes of) mathematical structures such as groups, fields, graphs, or even universes of set theory, using tools from mathematical logic. A system that gives meaning to the sentences of a formal language is called a model for the language. If a model for a language moreover satisfies a particular sentence or theory (set of sentences), it is called a model of the sentence or theory. Model theory has close ties to algebra and universal algebra.\n\nMathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures.\n\nA more comprehensive type of mathematical model uses a linguistic version of category theory to model a given situation. Akin to entity-relationship models, custom categories or sketches can be directly translated into database schemas. The difference is that logic is replaced by category theory, which brings powerful theorems to bear on the subject of modeling, especially useful for translating between disparate models (as functors between categories).\n\nA scientific model is a simplified abstract view of a complex reality. A scientific model represents empirical objects, phenomena, and physical processes in a logical way. Attempts to formalize the principles of the empirical sciences use an interpretation to model reality, in the same way logicians axiomatize the principles of logic. The aim of these attempts is to construct a formal system for which reality is the only interpretation. The world is an interpretation (or model) of these sciences, only insofar as these sciences are true.\n\nA statistical model is a probability distribution function proposed as generating data. In a parametric model, the probability distribution function has variable parameters, such as the mean and variance in a normal distribution, or the coefficients for the various exponents of the independent variable in linear regression. A nonparametric model has a distribution function without parameters, such as in bootstrapping, and is only loosely confined by assumptions. Model selection is a statistical method for selecting a distribution function within a class of them; e.g., in linear regression where the dependent variable is a polynomial of the independent variable with parametric coefficients, model selection is selecting the highest exponent, and may be done with nonparametric means, such as with cross validation.\n\nIn statistics there can be models of mental events as well as models of physical events. For example, a statistical model of customer behavior is a model that is conceptual (because behavior is physical), but a statistical model of customer satisfaction is a model of a concept (because satisfaction is a mental not a physical event).\n\nIn economics, a model is a theoretical construct that represents economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified framework designed to illustrate complex processes, often but not always using mathematical techniques. Frequently, economic models use structural parameters. Structural parameters are underlying parameters in a model or class of models. A model may have various parameters and those parameters may change to create various properties.\n\nA system model is the conceptual model that describes and represents the structure, behavior, and more views of a system. A system model can represent multiple views of a system by using two different approaches. The first one is the non-architectural approach and the second one is the architectural approach. The non-architectural approach respectively picks a model for each view. The architectural approach, also known as system architecture, instead of picking many heterogeneous and unrelated models, will use only one integrated architectural model.\n\nIn business process modelling the enterprise process model is often referred to as the \"business process model\". Process models are core concepts in the discipline of process engineering. Process models are:\nThe same process model is used repeatedly for the development of many applications and thus, has many instantiations.\n\nOne possible use of a process model is to prescribe how things must/should/could be done in contrast to the process itself which is really what happens. A process model is roughly an anticipation of what the process will look like. What the process shall be will be determined during actual system development.\n\nConceptual models of human activity systems are used in soft systems methodology (SSM), which is a method of systems analysis concerned with the structuring of problems in management. These models are models of concepts; the authors specifically state that they are not intended to represent a state of affairs in the physical world. They are also used in information requirements analysis (IRA) which is a variant of SSM developed for information system design and software engineering.\n\nLogico-linguistic modeling is another variant of SSM that uses conceptual models. However, this method combines models of concepts with models of putative real world objects and events. It is a graphical representation of modal logic in which modal operators are used to distinguish statement about concepts from statements about real world objects and events.\n\nIn software engineering, an entity-relationship model (ERM) is an abstract and conceptual representation of data. Entity-relationship modeling is a database modeling method, used to produce a type of conceptual schema or semantic data model of a system, often a relational database, and its requirements in a top-down fashion. Diagrams created by this process are called entity-relationship diagrams, ER diagrams, or ERDs.\n\nEntity-relationship models have had wide application in the building of information systems intended to support activities involving objects and events in the real world. In these cases they are models that are conceptual. However, this modeling method can be used to build computer games or a family tree of the Greek Gods, in these cases it would be used to model concepts.\n\nA domain model is a type of conceptual model used to depict the structural elements and their conceptual constraints within a domain of interest (sometimes called the problem domain). A domain model includes the various entities, their attributes and relationships, plus the constraints governing the conceptual integrity of the structural model elements comprising that problem domain. A domain model may also include a number of conceptual views, where each view is pertinent to a particular subject area of the domain or to a particular subset of the domain model which is of interest to a stakeholder of the domain model.\n\nLike entity-relationship models, domain models can be used to model concepts or to model real world objects and events.\n\n\n"}
{"id": "633037", "url": "https://en.wikipedia.org/wiki?curid=633037", "title": "Conceptualism", "text": "Conceptualism\n\nIn metaphysics, conceptualism is a theory that explains universality of particulars as conceptualized frameworks situated within the thinking mind. Intermediate between nominalism and realism, the conceptualist view approaches the metaphysical concept of universals from a perspective that denies their presence in particulars outside the mind's perception of them. Conceptualism is anti-realist about abstract objects, just like immanent realism is (their difference being that immanent realism does not deny the mind-independence of universals, like conceptualism does).\n\nThe evolution of late scholastic terminology has led to the emergence of conceptualism, which stemmed from doctrines that were previously considered to be nominalistic. The terminological distinction was made in order to stress the difference between the claim that universal mental acts correspond with universal intentional objects and the perspective that dismissed the existence of universals outside the mind. The former perspective of rejection of objective universality was distinctly defined as conceptualism.\n\nPeter Abélard was a medieval thinker whose work is currently classified as having the most potential in representing the roots of conceptualism. Abélard’s view denied the existence of determinate universals within things. William of Ockham was another famous late medieval thinker who had a strictly conceptualist solution to the metaphysical problem of universals. He argued that abstract concepts have no \"fundamentum\" outside the mind.\n\nIn the 17th century conceptualism gained favour for some decades especially among the Jesuits: Hurtado de Mendoza, Rodrigo de Arriaga and Francisco Oviedo are the main figures. Although the order soon returned to the more realist philosophy of Francisco Suárez, the ideas of these Jesuits had a great impact on the early modern philosophy.\n\nConceptualism was either explicitly or implicitly embraced by most of the early modern thinkers, including René Descartes, John Locke, Baruch Spinoza, Gottfried Wilhelm Leibniz, George Berkeley, and David Hume – often in a quite simplified form if compared with the elaborate scholastic theories.\n\nSometimes the term is applied even to the radically different philosophy of Immanuel Kant, who holds that universals have no connection with external things because they are exclusively produced by our \"a priori\" mental structures and functions.\n\nIn late modern philosophy, conceptualist views were held by G. W. F. Hegel.\n\nEdmund Husserl's philosophy of mathematics has been construed as a form of conceptualism.\n\nConceptualist realism (a view put forward by David Wiggins in 1980) states that our conceptual framework maps reality.\n\nThough separate from the historical debate regarding the status of universals, there has been significant debate regarding the conceptual character of experience since the release of \"Mind and World\" by John McDowell in 1994. McDowell's touchstone is the famous refutation that Wilfrid Sellars provided for what he called the \"Myth of the Given\"—the notion that all empirical knowledge is based on certain assumed or 'given' items, such as sense data. Thus, in rejecting the Myth of the Given, McDowell argues for perceptual conceptualism, according to which perceptual content is conceptual \"from the ground up\", that is, all perceptual experience is a form of conceptual experience. McDowell's philosophy of justification is considered a form of foundationalism: it is a form of foundationalism because it allows that certain judgements are warranted by experience and it is a coherent form of this view because it maintains that experience can warrant certain judgements because experience is irreducibly conceptual.\n\nA clear motivation of contemporary conceptualism is that the kind of perception that rational creatures like humans enjoy is unique in the fact that it has conceptual character. McDowell explains his position:\n\nI have urged that our perceptual relation to the world is conceptual all the way out to the world’s impacts on our receptive capacities. The idea of the conceptual that I mean to be invoking is to be understood in close connection with the idea of rationality, in the sense that is in play in the traditional separation of mature human beings, as rational animals, from the rest of the animal kingdom. Conceptual capacities are capacities that belong to their subject’s rationality. So another way of putting my claim is to say that our perceptual experience is permeated with rationality. I have also suggested, in passing, that something parallel should be said about our agency.\n\nMcDowell's conceptualism, though rather distinct (philosophically and historically) from conceptualism's genesis, shares the view that universals are not \"given\" in perception from outside the sphere of reason. Particular objects are perceived, as it were, already infused with conceptuality stemming from the spontaneity of the rational subject herself.\n\nThe application of the term \"perceptual conceptualism\" to Kant's philosophy of perception is debatable. Other scholars have argued for a rival interpretation of Kant's work termed perceptual non-conceptualism.\n\n"}
{"id": "730906", "url": "https://en.wikipedia.org/wiki?curid=730906", "title": "Exclusion principle (philosophy)", "text": "Exclusion principle (philosophy)\n\nThe exclusion principle is a philosophical principle that states:\n\nThe exclusion principle is most commonly applied when one poses this scenario; One usually considers that the desire to lift one’s arm as a mental event, and the lifting on one's arm, a physical event. According to the exclusion principle, there must be no event that does not supervene on \"e\" while causing \"e*\". To show this better, substitute \"\"the desire to lift one's arm\" for \"e\", and \"one to lift their arm\" for \"e*\"\".\n\nThis is interpreted as meaning that mental events supervene upon the physical. However, some philosophers do not accept this principle, and accept epiphenomenalism, which states that mental events are caused by physical events, but physical events are not caused by mental events (called \"causal impotence\"). However, If \"e#\" does not cause \"e\", then there is no way to verify that \"e*\" exists. Yet, this debate has not been settled in the philosophical community.\n\n"}
{"id": "33920395", "url": "https://en.wikipedia.org/wiki?curid=33920395", "title": "General Group Problem Solving (GGPS) Model", "text": "General Group Problem Solving (GGPS) Model\n\nThe General Group Problem Solving (GGPS) Model is a problem solving methodology, in which a group of individuals will define the desired outcome, identify the gap between the current state and the target and generate ideas for closing the gap by brainstorming. The end result is list of actions needed to achieve the desired results.\n\nSally Fuller and Ramon Aldag argue that group decision-making models have been operating under too narrow of a focus due to the overemphasis of the groupthink phenomenon. In addition, according to them, group decision-making has often been framed in relative isolation, ignoring context and real-world circumstances, which is a likely consequence of testing group decision-making in laboratory studies. They claim that the groupthink model is overly deterministic and an unrealistically restrictive depiction of the group problem-solving process.” To address these problems, they propose a new model that incorporates elements of group decision-making processes from a broader, more comprehensive perspective, offering a more general and generalizable framework for future research. The model includes elements of Irving Janis's original model (1977), but only those that have been consistently supported by the literature. To understand the differences between the two models, we briefly summarize both Janis's model and the GGPS-model first.\n\nJanis defines groupthink as “the mode of thinking that persons engage in when concurrence-seeking becomes so dominant in a cohesive in-group that it tends to over-ride realistic appraisals of alternative courses of action.” In a subsequent article, he elaborates on this by saying: “I use the term \"groupthink\" as a quick and easy way to refer to a mode of thinking that people engage in when they are deeply involved in a cohesive in-group, when the members' strivings for unanimity override their motivation to realistically appraise alternative courses of action. Groupthink refers to a deterioration of mental efficiency, reality testing, and moral judgment that results from in-group pressures.”\nAll this suggests that the original groupthink model was proposed for a rather specific situation, and Janis states that we can only call a phenomenon groupthink if all the warning signs are present (see groupthink symptoms).\n\nThe GGPS-model (developed by Ramon Aldag and Sally Fuller) broadens the perspectives, incorporating elements of the original groupthink model, in a fashion that creates a more widely applicable schematic.\nTwo key differences should be noted in comparison to Janis’ model:\n\n\nThree sets of antecedents are proposed by GGPS: decision characteristics, group structure and decision-making context.\n\nElements belonging here are the importance of the decision, time pressure, structure, procedural requirements, and task characteristics.\n\n\"Examples\": whether the task is simple or complex will make a substantial difference in required member input, as well as in whether a directive leader is necessary. Group interaction is also altered if, given a task, a single correct answer exists and if it becomes obvious to any of the members, since subsequent group interaction will likely be reduced.\n\nElements are cohesiveness, members’ homogeneity, insulation of the group, leader impartiality, leader power, history of the group, probability of future interaction, stage of group development and type of group.\n\n\"Examples\": whether group members anticipate to work together again in the future can have a major impact on to what degree can political motives influence the process. If it’s unlikely that the group will come together again, political influence can be lessened. Stage of group development is important because members of mature group with a long history may feel more comfortable challenging each other’s ideas, thus cohesiveness results in quality decision making and positive outcomes.\n\nElements are organizational political norms, member political motives, prior discussion of issue, prior goal attainment, goal definition, and degree of stress from external threat.\n\n\"Examples\": whether group members identified and pursue a unitary goal or they have multiple, discrepant goals influences the rationality of the decision-making process. Members’ political motives also make a tremendous difference, if individuals have a vested interest in certain outcomes, or there are one or more coalitions present, behavior in the decision making process could be altered.\n\nThe model differentiates two categories of emergent group characteristics: group perceptions and processes.\n\nThese include members’ perceptions of the group’s vulnerability, the inherent morality of the group, member unanimity, and views of opposing groups.\n\nThese include the group’s response to negative feedback, treatment of dissenters, self-censorship, and use of mindguards.\n\nDecision process characteristics are grouped in terms of the first three stages of group problem-solving processes: problem identification, alternative generation and evaluation and choice. Implementation and control stages are not included because they follow the actual decision, but some variables preparing for those stages are indeed included (e.g. development of contingency plans and gathering of control-related information).\n\nElements of this stage are predecisional information search, survey of objectives, and explicit problem definition.\n\n\"Example\": if members of the group fail to explicitly or correctly define the problem, there is a chance that they will solve the wrong problem.\n\nElements are number of alternatives and quality of alternatives.\n\n\"Example\": in generating alternatives, it’s important to differentiate quality and quantity of alternative ideas generated. Some group processes, such as brainstorming, are directed towards generating large numbers of ideas, with the assumption that it will lead to a superior alternative. Defective processes, on the other hand, might lead to large numbers of low quality ideas.\n\nElements are information processing quality, the source of the initial selection of a preferred alternative, emergence of preferred alternative, group decision rule, timing of convergence, reexamination of preferred and rejected alternatives, source of the final solution, development of contingency plans, and gathering of control-related information.\n\n\"Example\": whether the group decides based on a majority rule or a consensus has to be reached, makes a great difference in the process. If a consensus is to be reached, dissent could be discouraged, because dissenters could elongate and jeopardize the process. With a majority rule, dissent is more acceptable.\n\nThe GGPS model includes an array of decision, political and affective outcomes.\n\nDecision outcomes: include acceptance of the decision by those affected by it and/or those who have to implement it, adherence to the decision, implementation success, and decision quality.\n\n\"Example\": if the leader of the group is not satisfied with the decision, he/she might unilaterally reverse it.\n\nPolitical outcomes: include future motivation of the leader, future motivation of the group and future use of the group.\n\n\"Example\": if the outcome did not satisfy the political agenda of the leader, he/she might use the group less or not at all in the future.\n\nAffective outcomes: include satisfaction with the leader, satisfaction with the group process and satisfaction with the decision.\n\n\"Example\": whether members are content with the fairness of the group process, whether trust was developed and preserved, or whether commitment to the decision is strong will greatly influence the group’s future functioning.\n"}
{"id": "25200737", "url": "https://en.wikipedia.org/wiki?curid=25200737", "title": "Knowledge arena", "text": "Knowledge arena\n\nA knowledge arena is a virtual space where individuals can manipulate concepts and relationships to form a concept map. Individuals using a computer with appropriate software can represent concepts and the relationships between concepts in a node-relationship-node formalism. The process of thinking about the concepts and making associations between them has been called \"off-loading\" by McAleese.\n\nThe concept map is a form of a semantic network or semantic graph. It is formally based on graph theory. In the concept map, concepts are represented by nodes. The relationship between nodes are represented by \"typed links\" or \"edges\". (See Graph theory.) In creating a map or graphic representation of what is known an individual intentionally interacts with the graphical interface or map and through a reflective process adds nodes (concepts) and /or adds relationships (edges or typed links) or modifies existing node-relationship-node instances. It is likely that the process of engaging with concepts and relationships between concepts brings about the creation of understandings as well as making the understandings explicit.\n\nMany different claims have been made for the utility of the concept map. The interactive and reflective nature of map creation is highlighted by the use of the description Knowledge Arena. Although maps may represent what an individual knows at a point in time; it is likely that by interacting with the concepts and relationships in the knowledge arena individual continues to create and modify what that individual \"knows\".\n\nSee also \n"}
{"id": "34074829", "url": "https://en.wikipedia.org/wiki?curid=34074829", "title": "Lexical entrainment", "text": "Lexical entrainment\n\nLexical entrainment is the phenomenon in conversational linguistics of the process of the subject adopting the reference terms of their interlocutor. In practice, it acts as a mechanism of the cooperative principle in which both parties to the conversation employ lexical entrainment as a progressive system to develop \"conceptual pacts\" (a working temporary conversational terminology) to ensure maximum clarity of reference in the communication between the parties; this process is necessary to overcome the ambiguity inherent in the multitude of synonyms that exist in language. \n\nLexical entrainment arises by two cooperative mechanisms: \n\nOnce lexical entrainment has come to determine the phrasing for a referent, both parties will use that terminology for the referent for the duration, even if it proceeds to violate the Gricean maxim of quantity. For example, if one wants to refer to a brown loafer out of a set of shoes that consist of: the loafer, a sneaker, and a high-heeled shoe, they will not use \"the shoe\" to describe the object as this phrasing does not unambiguously describe one item in the set under consideration. They will also not call the object \"the brown loafer\" which would violate Grice's maxim of quantity. The speaker will settle on using the term \"the loafer\" as it is just informative enough without giving too much information. \n\nAnother important factor is lexical availability; the ease of conceptualizing a referent in a certain way and then retrieving and producing a label for it. For many objects the most available labels are basic nouns; for example, the word \"dog\". Instead of saying \"animal\" or \"husky\" for the referent, most subjects will default to \"dog\".\nIf in a set of objects one is to refer to either a husky, a table, and a poster, people are still most likely to use the word \"dog.\" This is technically a violation of Grice's maxim of quantity, as using the term \"animal\" is ideal.\n\nLexical entrainment has applications in natural language processing in computers as well as human–human interaction. Currently, the adaptability of computers to modify their referencing to the terms of their human interlocutor is limited, so the entrainment adaptation falls to the human operator; this phenomenon is readily demonstrated in Brennan's 1996 experiment.\n"}
{"id": "160970", "url": "https://en.wikipedia.org/wiki?curid=160970", "title": "Literal and figurative language", "text": "Literal and figurative language\n\nLiteral and figurative language is a distinction within some fields of language analysis, in particular stylistics, rhetoric, and semantics. \n\nLiteral usage confers meaning to words, in the sense of the meaning they have by themselves, outside any figure of speech. It maintains a consistent meaning regardless of the context, with \"the intended meaning corresponding exactly to the meaning\" of the individual words. Figurative use of language is the use of words or phrases that \"implies a non-literal meaning which does make sense or that could [also] be true\".\n\nAristotle and later the Roman Quintilian were among the early analysts of rhetoric who expounded on the differences between literal and figurative language.\n\nIn 1769, Frances Brooke's novel \"The History of Emily Montague\" was used in the earliest \"Oxford English Dictionary\" citation for the figurative sense of \"literally\"; the sentence from the novel used was, \"He is a fortunate man to be introduced to such a party of fine women at his arrival; it is literally \"to feed among the lilies\".\" This citation was also used in the OED's 2011 revision.\n\nWithin literary analysis, such terms are still used; but within the fields of cognition and linguistics, the basis for identifying such a distinction is no longer used.\n\nFigurative language can take multiple forms, such as simile or metaphor. \"Merriam-Webster's Encyclopedia Of Literature\" says that figurative language can be classified in five categories: resemblance or relationship, emphasis or understatement, figures of sound, verbal games, and errors. \n\nA simile is a comparison of two things, indicated by some connective, usually \"like\", \"as\", \"than\", or a verb such as \"resembles\" to show how they are similar.\n\nA metaphor is a figure of speech in which two \"essentially unlike things\" are shown to have a type of resemblance or create a new image. The similarities between the objects being compared may be implied rather than directly stated.\n\nAn extended metaphor is a metaphor that is continued over multiple sentences.\n\nOnomatopoeia is a word designed to be an imitation of a sound.\n\nPersonification is the attribution of a personal nature or character to inanimate objects or abstract notions, especially as a rhetorical figure.\n\nAn oxymoron is a figure of speech in which a pair of opposite or contradictory terms is used together for emphasis.\n\nA paradox is a statement or proposition which is self-contradictory, unreasonable, or illogical.\n\nHyperbole is a figure of speech which uses an extravagant or exaggerated statement to express strong feelings.\n\nAllusion is a reference to a famous character or event.\n\nAn idiom is an expression that has a figurative meaning unrelated to the literal meaning of the phrase.\n\nA pun is an expression intended for a humorous or rhetorical effect by exploiting different meanings of words.\n\nPrior to the 1980s, the \"standard pragmatic\" model of comprehension was widely believed. In that model, it was thought the recipient would first attempt to comprehend the meaning as if literal, but when an appropriate literal inference could not be made, the recipient would shift to look for a figurative interpretation that would allow comprehension. Since then, research has cast doubt on the model. In tests, figurative language was found to be comprehended at the same speed as literal language; and so the premise that the recipient was first attempting to process a literal meaning and discarding it before attempting to process a figurative meaning appears to be false.\n\nBeginning with the work of Michael Reddy in his 1979 work \"The Conduit Metaphor\", many linguists now reject that there is a valid way to distinguish between a \"literal\" and \"figurative\" mode of language.\n\n"}
{"id": "27940157", "url": "https://en.wikipedia.org/wiki?curid=27940157", "title": "Marginal factor cost", "text": "Marginal factor cost\n\nIn microeconomics, the marginal factor cost (MFC) is the increment to total costs paid for a factor of production resulting from a one-unit increase in the amount of the factor employed. It is expressed in currency units per incremental unit of a factor of production (input), such as labor, per unit of time. In the case of the labor input, for example, if the wage rate paid is unaffected by the number of units of labor hired, the marginal factor cost is identical to the wage rate. However, if hiring another unit of labor drives up the wage rate that must be paid to all existing units of labor employed, then the marginal cost of the labor factor is higher than the wage rate paid to the last unit because it also includes the increment to the rates paid to the other units.\n\nThus for any factor the MFC is the change in total amount paid for all units of that factor divided by the change in the quantity of that factor employed.\n\nA firm that wants to optimize its profits hires each factor up to the point at which its marginal factor cost equals its marginal revenue product (MFC=MRP).\n"}
{"id": "21582679", "url": "https://en.wikipedia.org/wiki?curid=21582679", "title": "Marginal product of labor", "text": "Marginal product of labor\n\nIn economics, the marginal product of labor (MP) is the change in output that results from employing an added unit of labor.\n\nThe marginal product of a factor of production is generally defined as the change in output associated with a change in that factor, holding other inputs into production constant.\n\nThe marginal product of labor is then the change in output (\"Y\") per unit change in labor (\"L\"). In discrete terms the marginal product of labor is:\n\nIn continuous terms, the \"MP\" is the first derivative of the production function:\n\nGraphically, the \"MP\" is the slope of the production function.\n\nThere is a factory which produces toys. When there are no workers in the factory, no toys are produced. When there is one worker in the factory, six toys are produced per hour. When there are two workers in the factory, eleven toys are produced per hour. There is a marginal product of labor of five when there are two workers in the factory compared to one. When the marginal product of labor is increasing, this is called increasing marginal returns. However, as the number of workers increases, the marginal product of labor may not increase indefinitely. When not scaled properly, the marginal product of labor may go down when the number of employees goes up, creating a situation known as diminishing marginal returns. When the marginal product of labor becomes negative, it is known as negative marginal returns.\n\nThe marginal product of labor is directly related to costs of production. Costs are divided between fixed and variable costs. Fixed costs are costs that relate to the fixed input, capital, or \"rK\", where \"r\" is the rental cost of capital and \"K\" is the quantity of capital. Variable costs (VC) are the costs of the variable input, labor, or \"wL\", where \"w\" is the wage rate and \"L\" is the amount of labor employed. Thus, VC = wL . Marginal cost (MC) is the change in total cost per unit change in output or ∆C/∆Q. In the short run, production can be varied only by changing the variable input. Thus only variable costs change as output increases: ∆C = ∆VC = ∆(wL). Marginal cost is ∆(Lw)/∆Q. Now, ∆L/∆Q is the reciprocal of the marginal product of labor (∆Q/∆L). Therefore, marginal cost is simply the wage rate w divided by the marginal product of labor\n\nThus if the marginal product of labor is rising then marginal costs will be falling and if the marginal product of labor is falling marginal costs will be rising (assuming a constant wage rate).\n\nThe average product of labor is the total product of labor divided by the number of units of labor employed, or \"Q/L\". The average product of labor is a common measure of labor productivity. The AP curve is shaped like an inverted “u”. At low production levels the AP tends to increase as additional labor is added. The primary reason for the increase is specialization and division of labor. At the point the AP reaches its maximum value AP equals the MP. Beyond this point the AP falls.\n\nDuring the early stages of production MP is greater than AP. When the MP is above the AP the AP will increase. Eventually the \"MP\" reaches it maximum value at the point of diminishing returns. Beyond this point MP will decrease. However, at the point of diminishing returns the MP is still above the AP and AP will continue to increase until MP equals AP. When MP is below AP, AP will decrease.\n\nGraphically, the \"AP\" curve can be derived from the total product curve by drawing secants from the origin that intersect (cut) the total product curve. The slope of the secant line equals the average product of labor, where the slope = dQ/dL. The slope of the curve at each intersection marks a point on the average product curve. The slope increases until the line reaches a point of tangency with the total product curve. This point marks the maximum average product of labor. It also marks the point where MP (which is the slope of the total product curve) equals the AP (the slope of the secant). Beyond this point the slope of the secants become progressively smaller as \"AP\" declines. The MP curve intersects the AP curve from above at the maximum point of the AP curve. Thereafter, the MP curve is below the AP curve.\n\nThe falling MP is due to the law of diminishing marginal returns. The law states, \"as units of one input are added (with all other inputs held constant) a point will be reached where the resulting additions to output will begin to decrease; that is marginal product will decline.\" The law of diminishing marginal returns applies regardless of whether the production function exhibits increasing, decreasing or constant returns to scale. The key factor is that the variable input is being changed while all other factors of production are being held constant. Under such circumstances diminishing marginal returns are inevitable at some level of production.\n\nDiminishing marginal returns differs from diminishing returns. Diminishing marginal returns means that the marginal product of the variable input is falling. Diminishing returns occur when the marginal product of the variable input is negative. That is when a unit increase in the variable input causes total product to fall. At the point that diminishing returns begin the MP is zero.\n\nThe general rule is that a firm maximizes profit by producing that quantity of output where marginal revenue equals marginal costs. The profit maximization issue can also be approached from the input side. That is, what is the profit maximizing usage of the variable input? To maximize profits the firm should increase usage \"up to the point where the input’s marginal revenue product equals its marginal costs\". So, mathematically the profit maximizing rule is MRP = MC. The marginal profit per unit of labor equals the marginal revenue product of labor minus the marginal cost of labor or Mπ = MRP − MCA firm maximizes profits where Mπ = 0.\n\nThe marginal revenue product is the change in total revenue per unit change in the variable input assume labor. That is, MRP = ∆TR/∆L. MRP is the product of marginal revenue and the marginal product of labor or MRP = MR × MP.\n\n\n • formula_4\n\n • Output price is $40 per unit.\n\n\nIn the aftermath of the marginal revolution in economics, a number of economists including John Bates Clark and Thomas Nixon Carver sought to derive an ethical theory of income distribution based on the idea that workers were morally entitled to receive a wage exactly equal to their marginal product. In the 20th century, marginal productivity ethics found few supporters among economists, being criticised not only by egalitarians but by economists associated with the Chicago school such as Frank Knight (in \"The Ethics of Competition\") and the Austrian School, such as Leland Yeager. However, marginal productivity ethics were defended by George Stigler.\n\n\n"}
{"id": "19466946", "url": "https://en.wikipedia.org/wiki?curid=19466946", "title": "Marginal profit", "text": "Marginal profit\n\nIn microeconomics, marginal profit is the difference between the marginal revenue and the marginal cost. Under the marginal approach to profit maximization, to maximize profits, a firm should continue to produce a good or service up to the point where marginal profit is zero.\n\nThe most simple formula of Marginal profit is: Marginal revenue - Marginal cost. The derivate of the profit f(x) is in fact MP. In other words: p(x)=R(x)-C(x)."}
{"id": "2695183", "url": "https://en.wikipedia.org/wiki?curid=2695183", "title": "Marginal propensity to import", "text": "Marginal propensity to import\n\nThe marginal propensity to import (MPI) is the fractional change in import expenditure that occurs with a change in disposable income (income after taxes and transfers). For example, if a household earns one extra dollar of disposable income, and the marginal propensity to import is 0.2, then the household will spend 20 cents of that dollar on imported goods and services.\n\nMathematically, the marginal propensity to import (MPM) function is expressed as the derivative of the import (I) function with respect to disposable income (Y).formula_1In other words, the marginal propensity to import is measured as the ratio of the change in imports to the change in income, thus giving us a figure between 0 and 1. \n\nImports are also considered to be automatic stabilisers that work to lessen fluctuations in real GDP.\n\nThe UK government assumes that UK citizens have a high marginal propensity to import and thus will use a decrease in disposable income as a tool to control the current account on the balance of payments.\n\n"}
{"id": "1402030", "url": "https://en.wikipedia.org/wiki?curid=1402030", "title": "Marginal revenue productivity theory of wages", "text": "Marginal revenue productivity theory of wages\n\nThe marginal revenue productivity theory of wages is a theory in neoclassical economics stating that wages are paid at a level equal to the marginal revenue product of labor, MRP (the value of the marginal product of labor), which is the increment to revenues caused by the increment to output produced by the last laborer employed. In a model, this is justified by an assumption that the firm is profit-maximizing and thus would employ labor only up to the point that marginal labor costs equal the marginal revenue generated for the firm. \n\nThe marginal revenue product (MRP) of a worker is equal to the product of the marginal product of labour (MP) (the increment to output from an increment to labor used) and the marginal revenue (MR) (the increment to sales revenue from an increment to output): MRP = MP × MR. The theory states that workers will be hired up to the point when the marginal revenue product is equal to the wage rate. If the marginal revenue brought by the worker is less than the wage rate, then employing that laborer would cause a decrease in profit.\n\nThe idea that payments to factors of production equal their marginal productivity had been laid out by John Bates Clark and Knut Wicksell, in simpler models. Much of the MRP theory stems from Wicksell's model.\n\nThe marginal revenue product of labour MRP is the increase in revenue per unit increase in the variable input = ∆TR/∆L \n\nThe change in output is not limited to that directly attributable to the additional worker. Assuming that the firm is operating with diminishing marginal returns then the addition of an extra worker reduces the average productivity of every other worker (and every other worker affects the marginal productivity of the additional worker).\n\nAs above noted the firm will continue to add units of labor until the MRP equals the wage rate \"w\"—mathematically until\n\nUnder perfect competition, marginal revenue product is equal to marginal physical product (extra unit produced as a result of a new employment) multiplied by price.\n\nThis is because the firm in perfect competition is a price taker. It does not have to lower the price in order to sell additional units of the good.\n\nFirms operating as monopolies or in imperfect competition face downward-sloping demand curves. To sell extra units of output, they would have to lower their output's price. Under such market conditions, marginal revenue product will not equal MPP×Price. This is because the firm is not able to sell output at a fixed price per unit. Thus the MRP curve of a firm in monopoly or in imperfect competition will slope downwards, when plotted against labor usage, at a faster rate than in perfect specific competition.\n"}
{"id": "4675536", "url": "https://en.wikipedia.org/wiki?curid=4675536", "title": "Master–slave morality", "text": "Master–slave morality\n\nMaster–slave morality is a central theme of Friedrich Nietzsche's works, in particular the first essay of \"On the Genealogy of Morality\". Nietzsche argued that there were two fundamental types of morality: \"master morality\" and \"slave morality\". Master morality values pride and power, while slave morality values things like kindness, empathy, and sympathy. Master morality weighs actions on good or bad consequences (i. e., classical virtues and vices, consequentialism), unlike slave morality, which weighs actions on a scale of good or evil intentions (e. g., Christian virtues and vices, Kantian deontology).\n\nFor Nietzsche, a particular morality is inseparable from the formation of a particular culture, meaning that a culture's language, codes and practices, narratives, and institutions are informed by the struggle between these two moral structures (see valuation).\n\nNietzsche defined master morality as the morality of the strong-willed. Nietzsche criticizes the view (which he identifies with contemporary British ideology) that good is everything that is helpful, and bad is everything that is harmful. He argues proponents of this view have forgotten the origins of its values and it is based merely on a non-critical acceptance of habit: what is useful has always been defined as good, therefore usefulness is goodness as a value. He continues explaining that in the prehistoric state \"the value or non-value of an action was derived from its consequences\" but ultimately \"[t]here are no moral phenomena at all, only moral interpretations of phenomena.\" For strong-willed men, the \"good\" is the noble, strong, and powerful, while the \"bad\" is the weak, cowardly, timid, and petty. \n\nThe essence of master morality is \"nobility\". Other qualities that are often valued in master morality are open-mindedness, courage, truthfulness, trust, and an accurate sense of one's self-worth. Master morality begins in the \"noble man\", with a spontaneous idea of the good; then the idea of bad develops as what is not good. \"The noble type of man experiences \"itself\" as determining values; it does not need approval; it judges, \"what is harmful to me is harmful in itself\"; it knows itself to be that which first accords honour to things; it is \"value-creating\".\" In this sense, the master morality is the full recognition that \"oneself\" is the measure of all moral truths. Insofar as something is helpful to the strong-willed man, it is like what he values in himself; therefore, the strong-willed man values such things as good because they aid him in a life-long process of self-actualization through the will to power.\n\nAccording to Nietzsche, masters are creators of morality; slaves respond to master morality with their slave morality. Unlike master morality, which is sentiment, slave morality is based on re-sentiment—devaluing that which the master values and the slave does not have. As master morality originates in the strong, slave morality originates in the weak. Because slave morality is a reaction to oppression, it vilifies its oppressors. Slave morality is the inverse of master morality. As such, it is characterized by pessimism and cynicism. Slave morality is created in opposition to what master morality values as \"good\". \n\nSlave morality does not aim at exerting one's will by strength, but by careful subversion. It does not seek to transcend the masters, but to make them slaves as well. The essence of slave morality is \"utility\": The good is what is most useful for the whole community, not just the strong. Nietzsche saw this as a contradiction. Since the powerful are few in number, compared to the masses of the weak, the weak gain power by corrupting the strong into believing that the causes of slavery (viz., the will to power) are \"evil\", as are the qualities the weak originally could not choose because of their weakness. By saying humility is voluntary, slave morality avoids admitting that their humility was in the beginning forced upon them by a master. Biblical principles of humility, charity, and pity are the result of universalizing the plight of the slave onto all humankind, and thus enslaving the masters as well. \"The \"democratic\" movement is the heir to Christianity\"—the political manifestation of slave morality because of its obsession with freedom and equality.\n\nThis struggle between master and slave moralities recurs historically. According to Nietzsche, ancient Greek and Roman societies were grounded in master morality. The Homeric hero is the strong-willed man, and the classical roots of the \"Iliad\" and \"Odyssey\" exemplified Nietzsche's master morality. He calls the heroes \"men of a noble culture\", giving a substantive example of master morality. Historically, master morality was defeated, as the slave morality of Judaism and Christianity spread throughout the Roman Empire. \n\nThe essential struggle between cultures has always been between the Roman (master, strong) and the Judean (slave, weak). Nietzsche condemns the triumph of slave morality in the West, saying that the democratic movement is the \"collective degeneration of man\". He claimed that the nascent democratic movement of his time was essentially slavish and weak. Weakness conquered strength, slave conquered master, re-sentiment conquered sentiment. This ressentiment Nietzsche calls \"priestly vindictiveness\", which is based on the jealous weak seeking to enslave the strong, and thus erode the basis for power itself by pulling the powerful down. Such movements were, according to Nietzsche, inspired by \"the most intelligent revenge\" of the weak. Nietzsche saw democracy and Christianity as the same emasculating impulse which sought to make everyone equal by making everyone a slave.\n\nNietzsche did not necessarily believe that everyone should adopt master morality as the \"be-all, end-all\" behavior. He thought that the revaluation of morals would correct the inconsistencies in both master and slave moralities. But he asserted that for the individual, master morality was preferable to slave morality. Walter Kaufmann disagrees that Nietzsche actually preferred master morality to slave morality. He certainly gives slave morality a more thorough critique, but this is partly because he thought of slave morality as society's more imminent danger.\n\n\n"}
{"id": "3595285", "url": "https://en.wikipedia.org/wiki?curid=3595285", "title": "Maximum power principle", "text": "Maximum power principle\n\nThe maximum power principle or Lotka's principle has been proposed as the fourth principle of energetics in open system thermodynamics, where an example of an open system is a biological cell. According to Howard T. Odum, \"The maximum power principle can be stated: During self-organization, system designs develop and prevail that maximize power intake, energy transformation, and those uses that reinforce production and efficiency.\"\n\nChen (2006) has located the origin of the statement of maximum power as a formal principle in a tentative proposal by Alfred J. Lotka (1922a, b). Lotka's statement sought to explain the Darwinian notion of evolution with reference to a physical principle. Lotka's work was subsequently developed by the systems ecologist Howard T. Odum in collaboration with the Chemical Engineer Richard C. Pinkerton, and later advanced by the Engineer Myron Tribus.\n\nWhile Lotka's work may have been a first attempt to formalise evolutionary thought in mathematical terms, it followed similar observations made by Leibniz and Volterra and Ludwig Boltzmann, for example, throughout the sometimes controversial history of natural philosophy. In contemporary literature it is most commonly associated with the work of Howard T. Odum.\n\nThe significance of Odum's approach was given greater support during the 1970s, amid times of oil crisis, where, as Gilliland (1978, pp. 100) observed, there was an emerging need for a new method of analysing the importance and value of energy resources to economic and environmental production. A field known as energy analysis, itself associated with net energy and EROEI, arose to fulfill this analytic need. However, in energy analysis intractable theoretical and practical difficulties arose when using the energy unit to understand, a) the conversion among concentrated fuel types (or energy types), b) the contribution of labour, and c) the contribution of the environment.\n\nLotka said (1922b: 151): \nGilliland noted that these difficulties in analysis in turn required some new theory to adequately explain the interactions and transactions of these different energies (different concentrations of fuels, labour and environmental forces). Gilliland (Gilliland 1978, p. 101) suggested that Odum's statement of the maximum power principle (H.T.Odum 1978, pp. 54–87) was, perhaps, an adequate expression of the requisite theory:\nThis theory Odum called maximum power theory. In order to formulate maximum power theory Gilliland observed that Odum had added another law (the maximum power principle) to the already well established laws of thermodynamics. In 1978 Gilliland wrote that Odum's new law had not yet been validated (Gilliland 1978, p. 101). Gilliland stated that in maximum power theory the second law efficiency of thermodynamics required an additional physical concept: \"the concept of second law efficiency under maximum power\" (Gilliland 1978, p. 101):\nIn this way the concept of maximum power was being used as a principle to quantitatively describe the selective law of biological evolution. Perhaps H.T.Odum's most concise statement of this view was (1970, p. 62):\n\nThe Odum–Pinkerton approach to Lotka's proposal was to apply Ohm's law – and the associated maximum power theorem (a result in electrical power systems) – to ecological systems. Odum and Pinkerton defined \"power\" in electronic terms as the rate of work, where Work is understood as a \"useful energy transformation\". The concept of maximum power can therefore be defined as the \"maximum rate of useful energy transformation\". Hence the underlying philosophy aims to unify the theories and associated laws of electronic and thermodynamic systems with biological systems. This approach presupposed an analogical view which sees the world as an ecological-electronic-economic engine.\n\nOdum et al. viewed the maximum power theorem as a principle of power-efficiency reciprocity selection with wider application than just electronics. For example, Odum saw it in open systems operating on solar energy, like both photovoltaics and photosynthesis (1963, p. 438). Like the maximum power theorem, Odum's statement of the maximum power principle relies on the notion of 'matching', such that high-quality energy maximizes power by matching and amplifying energy (1994, pp. 262, 541): \"in surviving designs a matching of high-quality energy with larger amounts of low-quality energy is likely to occur\" (1994, p. 260). As with electronic circuits, the resultant rate of energy transformation will be at a maximum at an intermediate power efficiency. In 2006, T.T. Cai, C.L. Montague and J.S. Davis said that, \"The maximum power principle is a potential guide to understanding the patterns and processes of ecosystem development and sustainability. The principle predicts the selective persistence of ecosystem designs that capture a previously untapped energy source.\" (2006, p. 317). In several texts H.T. Odum gave the Atwood machine as a practical example of the 'principle' of maximum power.\n\nThe mathematical definition given by H.T. Odum is formally analogous to the definition provided on the maximum power theorem article. (For a brief explanation of Odum's approach to the relationship between ecology and electronics see Ecological Analog of Ohm's Law)\n\nWhether or not the principle of maximum power efficiency can be considered the fourth law of thermodynamics and the fourth principle of energetics is moot. Nevertheless, H.T. Odum also proposed a corollary of maximum power as the organisational principle of evolution, describing the evolution of microbiological systems, economic systems, planetary systems, and astrophysical systems. He called this corollary the maximum empower principle. This was suggested because, as S.E. Jorgensen, M.T. Brown, H.T. Odum (2004) note,\n\nC. Giannantoni may have confused matters when he wrote \"The \"Maximum Em-Power Principle\" (Lotka–Odum) is generally considered the \"Fourth Thermodynamic Principle\" (mainly) because of its practical validity for a very wide class of physical and biological systems\" (C. Giannantoni 2002, § 13, p. 155). Nevertheless, Giannantoni has proposed the Maximum Em-Power Principle as the fourth principle of thermodynamics (Giannantoni 2006).\n\nThe preceding discussion is incomplete. The \"maximum power\" was discovered several times independently, in physics and engineering, see: Novikov (1957), El-Wakil (1962), and Curzon and Ahlborn (1975). The incorrectness of this analysis and design evolution conclusions was demonstrated by Gyftopoulos (2002).\n\n\n"}
{"id": "50154587", "url": "https://en.wikipedia.org/wiki?curid=50154587", "title": "Menthor Editor", "text": "Menthor Editor\n\nMenthor Editor is a free ontology engineering tool for dealing with OntoUML models. It also includes OntoUML syntax validation, Alloy simulation, Anti-Pattern verification, and MDA transformations from OntoUML to OWL, SBVR and Natural Language (Brazilian Portuguese).\n\nMenthor Editor emerged from OLED. OLED was developed at the Ontology & Conceptual Modeling Research Group (NEMO) located at the Federal University of Espírito Santo (UFES) in Vitória city, state of Espírito Santo, Brazil\n\nMenthor Editor is being developed by Menthor using Java. Menthor Editor is available in English and it is a multiplaform software, i.e., it is compatible with Windows, Linux and OS X.\n\n"}
{"id": "161019", "url": "https://en.wikipedia.org/wiki?curid=161019", "title": "Negation", "text": "Negation\n\nIn logic, negation, also called the logical complement, is an operation that takes a proposition formula_1 to another proposition \"not formula_1\", written formula_3 (¬P), which is interpreted intuitively as being true when formula_1 is false, and false when formula_1 is true. Negation is thus a unary (single-argument) logical connective. It may be applied as an operation on notions, propositions, truth values, or semantic values more generally. In classical logic, negation is normally identified with the truth function that takes \"truth\" to \"falsity\" and vice versa. In intuitionistic logic, according to the Brouwer–Heyting–Kolmogorov interpretation, the negation of a proposition formula_1 is the proposition whose proofs are the refutations of formula_1.\n\nNo agreement exists as to the possibility of defining negation, as to its logical status, function, and meaning, as to its field of applicability..., and as to the interpretation of the negative judgment, (F.H. Heinemann 1944).\n\n\"Classical negation\" is an operation on one logical value, typically the value of a proposition, that produces a value of \"true\" when its operand is false and a value of \"false\" when its operand is true. So, if statement formula_1 is true, then formula_3 (pronounced \"not P\") would therefore be false; and conversely, if formula_3 is false, then formula_1 would be true.\n\nThe truth table of formula_3 is as follows:\n\nNegation can be defined in terms of other logical operations. For example, formula_3 can be defined as formula_14 (where formula_15 is logical consequence and formula_16 is absolute falsehood). Conversely, one can define formula_16 as formula_18 for any proposition formula_19 (where formula_20 is logical conjunction). The idea here is that any contradiction is false. While these ideas work in both classical and intuitionistic logic, they do not work in paraconsistent logic, where contradictions are not necessarily false. In classical logic, we also get a further identity, formula_21 can be defined as formula_22, where formula_23 is logical disjunction.\n\nAlgebraically, classical negation corresponds to complementation in a Boolean algebra, and intuitionistic negation to pseudocomplementation in a Heyting algebra. These algebras provide a semantics for classical and intuitionistic logic respectively.\n\nThe negation of a proposition formula_1 is notated in different ways in various contexts of discussion and fields of application. Among these variants are the following:\n\nThe notation N\"p\" is Łukasiewicz notation.\n\nIn set theory formula_25 is also used to indicate 'not member of': formula_26 is the set of all members of formula_27 that are not members of formula_28.\n\nNo matter how it is notated or symbolized, the negation formula_3 can be read as \"it is not the case that formula_1\", \"not that formula_1\", or usually more simply as \"not formula_1\".\n\nWithin a system of classical logic, double negation, that is, the negation of the negation of a proposition formula_1, is logically equivalent to formula_1. Expressed in symbolic terms, formula_35. In intuitionistic logic, a proposition implies its double negation but not conversely. This marks one important difference between classical and intuitionistic negation. Algebraically, classical negation is called an involution of period two.\n\nHowever, in intuitionistic logic we do have the equivalence of formula_36. Moreover, in the propositional case, a sentence is classically provable if its double negation is intuitionistically provable. This result is known as Glivenko's theorem.\n\nDe Morgan's laws provide a way of distributing negation over disjunction and conjunction :\n\nLet formula_39 denote the logical xor operation. In Boolean algebra, a linear function is one such that:\n\nIf there exists formula_40,\nformula_41,\nfor all formula_42.\n\nAnother way to express this is that each variable always makes a difference in the truth-value of the operation or it never makes a difference. Negation is a linear logical operator.\n\nIn Boolean algebra a self dual function is one such that:\n\nformula_43 for all\nformula_44.\nNegation is a self dual logical operator.\n\nThere are a number of equivalent ways to formulate rules for negation. One usual way to formulate classical negation in a natural deduction setting is to take as primitive rules of inference \"negation introduction\" (from a derivation of formula_1 to both formula_19 and formula_47, infer formula_3; this rule also being called \"reductio ad absurdum\"), \"negation elimination\" (from formula_1 and formula_3 infer formula_19; this rule also being called \"ex falso quodlibet\"), and \"double negation elimination\" (from formula_52 infer formula_1). One obtains the rules for intuitionistic negation the same way but by excluding double negation elimination.\n\nNegation introduction states that if an absurdity can be drawn as conclusion from formula_1 then formula_1 must not be the case (i.e. formula_1 is false (classically) or refutable (intuitionistically) or etc.). Negation elimination states that anything follows from an absurdity. Sometimes negation elimination is formulated using a primitive absurdity sign formula_16. In this case the rule says that from formula_1 and formula_3 follows an absurdity. Together with double negation elimination one may infer our originally formulated rule, namely that anything follows from an absurdity.\n\nTypically the intuitionistic negation formula_3 of formula_1 is defined as formula_14. Then negation introduction and elimination are just special cases of implication introduction (conditional proof) and elimination (modus ponens). In this case one must also add as a primitive rule \"ex falso quodlibet\".\n\nAs in mathematics, negation is used in computer science to construct logical statements.\n\nThe \"codice_1\" signifies logical NOT in B, C, and languages with a C-inspired syntax such as C++, Java, JavaScript, Perl, and PHP. \"codice_2\" is the operator used in ALGOL 60, BASIC, and languages with an ALGOL- or BASIC-inspired syntax such as Pascal, Ada, Eiffel and Seed7. Some languages (C++, Perl, etc.) provide more than one operator for negation. A few languages like PL/I and Ratfor use codice_3 for negation. Some modern computers and operating systems will display codice_3 as codice_1 on files encoded in ASCII. Most modern languages allow the above statement to be shortened from codice_6 to codice_7, which allows sometimes, when the compiler/interpreter is not able to optimize it, faster programs.\n\nIn computer science there is also \"bitwise negation\". This takes the value given and switches all the binary 1s to 0s and 0s to 1s. See bitwise operation. This is often used to create ones' complement or \"codice_8\" in C or C++ and two's complement (just simplified to \"codice_9\" or the negative sign since this is equivalent to taking the arithmetic negative value of the number) as it basically creates the opposite (negative value equivalent) or mathematical complement of the value (where both values are added together they create a whole).\n\nTo get the absolute (positive equivalent) value of a given integer the following would work as the \"codice_9\" changes it from negative to positive (it is negative because \"codice_11\" yields true)\n\nTo demonstrate logical negation:\n\nInverting the condition and reversing the outcomes produces code that is logically equivalent to the original code, i.e. will have identical results for any input (note that depending on the compiler used, the actual instructions performed by the computer may differ).\n\nThis convention occasionally surfaces in written speech, as computer-related slang for \"not\". The phrase codice_12, for example, means \"not voting\".\n\nIn Kripke semantics where the semantic values of formulae are sets of possible worlds, negation can be taken to mean set-theoretic complementation. (See also possible world semantics.)\n\n\n"}
{"id": "2059959", "url": "https://en.wikipedia.org/wiki?curid=2059959", "title": "Negative amortization", "text": "Negative amortization\n\nIn finance, negative amortization (also known as NegAm, deferred interest or graduated payment mortgage) occurs whenever the loan payment for any period is less than the interest charged over that period so that the outstanding balance of the loan increases. As an amortization method the shorted amount (difference between interest and repayment) is then added to the total amount owed to the lender. Such a practice would have to be agreed upon before shorting the payment so as to avoid default on payment. This method is generally used in an introductory period before loan payments exceed interest and the loan becomes self-amortizing. The term is most often used for mortgage loans; corporate loans with negative amortization are called PIK loans.\n\nAmortization refers to the process of paying off a debt (often from a loan or mortgage) through regular payments. A portion of each payment is for interest while the remaining amount is applied towards the principal balance. The percentage of interest versus principal in each payment is determined in an amortization schedule.\n\nNegative amortization only occurs in loans in which the periodic payment does not cover the amount of interest due for that loan period. The unpaid accrued interest is then capitalized monthly into the outstanding principal balance. The result of this is that the loan balance (or principal) increases by the amount of the unpaid interest on a monthly basis. The purpose of such a feature is most often for advanced cash management and/or more simply payment flexibility, but not to increase overall affordability.\n\nNeg-Ams also have what is called a recast period, and the recast principal balance cap is in the U.S. based on Federal and State legislation. The recast period is usually 60 months (5 years). The recast principal balance cap (also known as the \"neg am limit\") is usually up to a 25% increase of the amortized loan balance over the original loan amount. States and lenders can offer products with lesser recast periods and principal balance caps; but cannot issue loans that exceed their state and federal legislated requirements under penalty of law.\n\nA newer loan option has been introduced which allows for a 40-year loan term. This makes the minimum payment even lower than a comparable 30-year term.\n\n\nAll NegAM home loans eventually require full repayment of principal and interest according to the original term of the mortgage and note signed by the borrower. Most loans only allow NegAM to happen for no more than 5 years, and have terms to \"Recast\" (see below) the payment to a fully amortizing schedule if the borrower allows the principal balance to rise to a pre-specified amount.\n\nThis loan is written often in high cost areas, because the monthly mortgage payments will be lower than any other type of financing instrument.\n\nNegative amortization loans can be high risk loans for inexperienced investors. These loans tend to be safer in a falling rate market and riskier in a rising rate market.\n\nStart rates on negative amortization or minimum payment option loans can be as low as 1%. This is the payment rate, not the actual interest rate. The payment rate is used to calculate the minimum payment. Other minimum payment options include 1.95% or more.\n\nNegAM loans today are mostly straight adjustable rate mortgages (ARMs), meaning that they are fixed for a certain period and adjust every time that period has elapsed; e.g., one month fixed, adjusting every month. The NegAm loan, like all adjustable rate mortgages, is tied to a specific financial index which is used to determine the interest rate based on the current index and the margin (the markup the lender charges). Most NegAm loans today are tied to the Monthly Treasury Average, in keeping with the monthly adjustments of this loan. There are also Hybrid ARM loans in which there is a period of fixed payments for months or years, followed by an increased change cycle, such as six months fixed, then monthly adjustable.\n\nThe graduated payment mortgage is a \"fixed rate\" NegAm loan, but since the payment increases over time, it has aspects of the ARM loan until amortizing payments are required.\n\nThe most notable differences between the traditional payment option ARM and the hybrid payment option ARM are in the start rate, also known as the \"minimum payment\" rate. On a Traditional Payment Option Arm, the minimum payment is based on a principal and interest calculation of 1% - 2.5% on average.\n\nThe start rate on a hybrid payment option ARM is higher, yet still extremely competitive payment wise.\n\nOn a hybrid payment option ARM, the minimum payment is derived using the \"interest only\" calculation of the start rate. The start rate on the hybrid payment option ARM typically is calculated by taking the fully indexed rate (actual note rate), then subtracting 3%, which will give you the start rate.\n\nExample: 7.5% fully indexed rate − 3% = 4.5% (4.5% would be the start rate on a hybrid pay option ARM)\n\nThis guideline can vary among lenders.\n\nAliases the payment option ARM loans are known by:\n\n(In general Author is using time references that are relative to a time frame that is not defined. 'Today' which is?; 'last 5 years' from when, etc.)\n\nNegative-amortization loans, being relatively popular only in the last decade, have attracted a variety of criticisms:\n\nIn a very hot real estate market a buyer may use a negative-amortizing mortgage to purchase a property with the plan to sell the property at a higher price before the end of the \"negam\" period. Therefore, an informed investor could purchase several properties with minimal monthly obligations and make a great profit over a five-year plan in a rising real-estate market.\n\nHowever, if the property values decrease, it is likely that the borrower will owe more on the property than it is worth, known colloquially in the mortgage industry as \"being underwater\". In this situation, if the property owner cannot make the new monthly payment, he or she may be faced with foreclosure or having to refinance with a very high loan-to-value ratio, requiring additional monthly obligations, such as mortgage insurance, and higher rates and payments due to the adversity of a high loan-to-value ratio.\n\nIt is very easy for borrowers to ignore or misunderstand the complications of this product when being presented with minimal monthly obligations that could be from one half to one third what other, more predictable, mortgage products require.\n"}
{"id": "677516", "url": "https://en.wikipedia.org/wiki?curid=677516", "title": "Negative campaigning", "text": "Negative campaigning\n\nNegative campaigning or mudslinging is the process of deliberate spreading negative information about someone or something to worsen the public image of the described.\n\nDeliberate spreading of such information can be motivated either by honest desire of the campaigner to warn others against real dangers or deficiencies of the described, or by the campaigner's dishonest ideas on methods of winning in political, business or other spheres of competition against an honest rival.\n\nThe public image of an entity can be defined as reputation, esteem, respect, acceptance of the entity's appearance, values and behaviour by the general public of a given territory and/or a social group, possibly within time limits. As target groups of public and their values differ, so negativity or positivity of a public image is relative: e.g. while in most societies having an honest source of income is a positive value and stealing is discouraged, in the world of professional thieves honest work is frowned upon and stealing is encouraged. In polygamous societies monogamy is not viewed in the way it is valued in monogamous societies. Values of a society also change with time: e.g. homosexuality in Western culture was considered immoral and was criminally prosecuted until the sexual revolution of the second half of the 20 century.\nThus negative campaigning to be successful has to take into account current values of the group it addresses. The degree of strictness in practicing the group's values as opposed to its tolerance for violating the norms has also to be taken into consideration: e.g. while in the Old Testament and other traditional religious societies adultery and prostitution were outlawed and supposed to be punished by death, modern Western societies show much greater tolerance to these.\n\nIn United States politics, negative campaigning has been called \"as American as Mississippi mud\" and \"as American as apple pie\". Some research suggests negative campaigning is the norm in all political venues, mitigated only by the dynamics of a particular contest.\n\nThere are a number of techniques used in negative campaigning. Among the most effective is running advertisements attacking an opponent's personality, record, or opinion. There are two main types of ads used in negative campaigning: attack and contrast.\n\nAttack ads focus exclusively on the negative aspects of the opponent. There is no positive content in an attack ad, whether it is about the candidate or the opponent. Attack ads usually identify the risks associated with the opponent, often exploiting people’s fears to manipulate and lower the impression voters have of the opponent. Because attack ads have no positive content, they have the potential to be more influential than contrast ads in shaping voters’ views of the sponsoring candidate’s opponent.\n\nUnlike attack ads, contrast ads contain information about both the candidate and the opponent. The information about the candidate is positive, while the information about the opponent is negative. Contrast ads compare and contrast the candidate with the opponent, juxtaposing the positive information about the candidate with the negative information of the opponent. Because contrast ads must contain positive information, contrast ads are seen as less damaging to the political process than attack ads.\n\nOne of the most famous such ads was \"Daisy Girl\" by the campaign of Lyndon B. Johnson that successfully portrayed Republican Barry Goldwater as threatening nuclear war. Common negative campaign techniques include painting an opponent as soft on criminals, dishonest, corrupt, or a danger to the nation. One common negative campaigning tactic is attacking the other side for running a negative campaign.\n\nDirty tricks are also common in negative political campaigns. These generally involve secretly leaking damaging information to the media. This isolates a candidate from backlash and also does not cost any money. The material must be substantive enough to attract media interest, however, and if the truth is discovered it could severely damage a campaign. Other dirty tricks include trying to feed an opponent's team false information hoping they will use it and embarrass themselves.\n\nOften a campaign will use outside organizations, such as lobby groups, to launch attacks. These can be claimed to be coming from a neutral source and if the allegations turn out not to be true the attacking candidate will not be damaged if the links cannot be proven. Negative campaigning can be conducted by proxy. For instance, highly partisan ads were placed in the 2004 U.S. presidential election by allegedly independent bodies like MoveOn.org and Swift Boat Veterans for Truth.\n\nPush polls are attacks disguised as telephone polls. They might ask a question like \"How would you react if Candidate A was revealed to beat his wife?\", giving the impression that Candidate A might beat his wife. Members of the media and of the opposing party are deliberately not called making these tactics all but invisible and unprovable.\n\nG. Gordon Liddy played a major role in developing these tactics during the Nixon campaign playing an important advisory of rules that led to the campaign of 1972. James Carville, campaign manager of Bill Clinton's 1992 election, is also a major proponent of negative tactics. Lee Atwater, best known for being an advisor to presidents Ronald Reagan and George H.W. Bush, also pioneered many negative campaign techniques seen in political campaigns today.\n\nSponsors of overt negative campaigns often cite reasons to support mass communication of negative ideas. The Office of National Drug Control Policy uses negative campaigns to steer the public away from health risks. Similar negative campaigns have been used to rebut mass marketing by tobacco companies, or to discourage drunk driving. Those who conduct negative political campaigns sometimes say the public needs to know about the person he or she is voting for, even if it is bad. In other words, if a candidate’s opponent is a crook or a bad person, then he or she should be able to tell the public about it.\n\nMartin Wattenberg and Craig Brians, of the University of California, Irvine, considered in their study whether negative campaigning mobilizes or alienates voters. They concluded that data used by Stephen Ansolabehere in a 1994 American Political Science Review article to advance the hypothesis that negative campaigning demobilizes voters was flawed.\n\nA subsequent study done by Ansolabehere and Shanto Iyengar in 1995 corrected some of the previous study's flaws. This study concluded that negative advertising suppressed voter turnout, particularly for Independent voters. They speculated that campaigns tend to go negative only if the Independent vote is leaning toward the opponent. In doing so, they insure that the swing voters stay home, leaving the election up to base voters. They also found that negative ads have a greater impact on Democrats than on Republicans. According to them, base Republicans will vote no matter what (and will vote only for a Republican), but Democrats can be influenced to either stay home and not vote at all or to switch sides and vote for a Republican. This, combined with the effect negativity has on Independents, led them to conclude that Republicans benefit more from going negative than Democrats.\n\nOther researchers have found different, more positive outcomes from negative campaigns. Rick Farmer, PhD, an assistant professor of political science at the University of Akron found that negative ads are more memorable than positive ads when they reinforce a preexisting belief and are relevant to the central issues of a marketing campaign. Researchers at the University of Georgia found the impact of negative ads increases over time, while positive ads used to counteract negative ads lack the power of negative ads . Research also suggests negative campaigning introduces controversy and raises public awareness through additional news coverage .\n\nMost recently, Kyle Mattes and David P. Redlawsk in \"The Positive Case for Negative Campaigning\" show through surveys and experiments that negative campaigning provides informational benefits for voters. Without negativity, voters would not have full information about all of their choices, since no candidate will say anything bad about herself. They argue that candidates have to point out the flaws in their opponents for voters to be fully informed.\n\nSome strategists say that an effect of negative campaigning is that while it motivates the base of support it can alienate centrist and undecided voters from the political process, reducing voter turnout and radicalizing politics. \nIn a study done by Gina Garramone about how negative advertising affects the political process, it was found that a consequence of negative campaigning is greater image discrimination of the candidates and greater attitude polarization. While positive ads also contributed to the image discrimination and attitude polarization, Garramone found that negative campaigning played a more influential role in the discrimination and polarization than positive campaigning.\n\nNegative ads can produce a backlash. A disastrous ad was run by the Progressive Conservative Party of Canada in the 1993 Canadian federal election, apparently emphasizing Liberal Party of Canada leader Jean Chrétien's Bell's Palsy partial facial paralysis in a number of unflattering photos, with the subtext of criticizing his platforms. Chrétien took maximum advantage of the opportunity to gain the public's sympathy as a man who struggled with a physical disability and his party's subsequent overwhelming victory in the election helped reduce the governing Conservatives to two seats.\n\nA similar backlash happened to the Liberal Party in the 2006 federal election for running an attack ad that suggested that Conservative leader Stephen Harper would use Canadian soldiers to patrol Canadian cities, and impose some kind of martial law. The ad was only available from the Liberal Party's web site for a few hours prior to the release of the attack ads on television; nevertheless, it was picked up by the media and widely criticized for its absurdity, in particular the sentence \"we're not making this up; we're not allowed to make this stuff up\". Liberal MP Keith Martin expressed his disapproval of \"whoever the idiot who approved that ad was,\" shortly before Liberal leader Paul Martin (no relation) stated that he had personally approved them. The effect of the ads was to diminish the credibility of the party's other attack ads. It offended many Canadians, particularly those in the military, some of whom were fighting in Afghanistan at the time. (See Canadian federal election, 2006)\n\nMore recently, in the 2008 US Senate race in North Carolina, Republican incumbent Elizabeth Dole attempted an attack ad on Democratic challenger Kay Hagan, who had taken a small lead in polls, by tying her to atheists. Dole's campaign released an ad questioning Hagan's religion and it included a voice saying \"There is no God!\" over a picture of Kay Hagan's face. The voice was not Hagan's but it is believed the ad implied that it was. Initially, it was thought the ad would work as religion has historically been a very important issue to voters in the American south, but the ad produced a backlash across the state and Hagan responded forcefully with an ad saying that she was a Sunday school teacher and was a religious person. Hagan also claimed Dole was trying to change the subject from the economy (the ad appeared around the same time as the 2008 financial crisis). Hagan's lead in polls doubled and she won the race by a nine-point margin.\n\nBecause of the possible harm that can come from being seen as a negative campaigner, candidates often pledge to refrain from negative attacks. This pledge is usually abandoned when an opponent is perceived to be \"going negative,\" with the first retaliatory attack being, ironically, an accusation that the opponent is a negative campaigner.\n\nWhile some research has found advantages and other has found disadvantages, some studies find no difference between negative and positive approaches .\n\nResearch published in the Journal of Advertising found that negative political advertising makes the body want to turn away physically, but the mind remembers negative messages. The findings are based on research conducted by James Angelini, professor of communication at the University of Delaware, in collaboration with Samuel Bradley, assistant professor of advertising at Texas Tech University, and Sungkyoung Lee of Indiana University, which used ads that aired during the 2000 presidential election. During the study, the researchers placed electrodes under the eyes of willing participants and showed them a series of 30-second ads from both the George W. Bush and Al Gore campaigns. The electrodes picked up on the \"startle response,\" the automatic eye movement typically seen in response to snakes, spiders and other threats. Compared to positive or neutral messages, negative advertising prompted greater reflex reactions and a desire to move away.\n\n\n\n\n"}
{"id": "2795942", "url": "https://en.wikipedia.org/wiki?curid=2795942", "title": "Negative cutting", "text": "Negative cutting\n\nNegative cutting (also known as negative matching and negative conforming) is the process of cutting motion picture negative to match precisely the final edit as specified by the film editor. Original camera negative (OCN) is cut with scissors and joined using a film splicer and film cement. Negative cutting is part of the post-production process and occurs after editing and prior to striking internegatives and release prints. The process of negative cutting has changed little since the beginning of cinema in the early 20th century. In the early 1980s computer software was first used to aid the cutting process. Kodak introduced barcode on motion picture negative in the mid-1990s. This enabled negative cutters to more easily track shots and identify film sections based on keykode.\n\nToward the late 1990s and early 2000s negative cutting changed due to the advent of digital cinema technologies such as digital intermediate (DI), digital projection and high-definition television. In some countries, due to the high cost of online suites, negative cutting is still used for commercials by reducing footage. Increasingly feature films are bypassing the negative cutting process altogether and are being scanned directly from the uncut rushes.\n\nThe existence of digital intermediates (DI) has created a new demand for negative cutters to extract selected takes which are cut from the rushes and re-spliced into new rolls (in edit order) to reduce the volume of footage for scanning.\n\nAfter a film shoot, the original camera negative (OCN) is sent to a film laboratory for processing. Two or three camera rolls are spliced together to create a lab roll approximately long. After developing the lab roll, it is put through a telecine to create a rushes transfer tape. This rushes transfer tape is of lower quality than film and is used for editing purposes only.\n\nThe rushes tape is sent to the Editor who loads it into an offline edit suite. The lab rolls are sent to the negative cutter for logging and storage.\n\nAfter the Editor finishes the Edit it is exported to an offline EDL list and the EDL list is sent to the negative cutter. The negative cutter will translate the Timecode in the EDL list to edge numbers (keykode) using specially designed negative cutting software to find which shot is needed from the rushes negative.\n\nTraditionally a negative cutter would then fine cut the negative to match the Editor's final edit frame accurately. Negative would be spliced together to create rolls less than which would then be sent to the film laboratory to print release prints.\n\nToday most feature films are extracted full takes (as selected takes) and scanned digitally as a digital intermediate. Television series and commercials shot on film follow the same extraction process but are sent for telecine. Each required shot is extracted from the lab roll as a full take and respliced together to create a new selected roll of negative. This reduces the negative required by up to 1/10 of the footage shot, saving considerable time during scanning or telecine. The negative cutter will create a new Online EDL list replacing the rushes roll timecode with the new selected roll timecode.\n\nIn the case of feature films the selected roll and Online EDL are sent to a post production facility for scanning as a digital intermediate. For television commercials or series the selected takes and EDL are sent to a post production facility for re-telecine and compiled in an Online Suite for final grading.\n\nThere have been a number of dedicated software systems that have been developed for and by negative cutters to manage the process of cutting motion picture negative. A number of individual proprietary software systems have been developed starting in the early 1980s. Stan Sztaba developed a system for World Cinevision Services Inc (New York) in 1983 using Apple II DOS and then ProDOS, this system is still used today. Elliott Gamson of Immaculate Matching (New York) developed a system using MS-DOS. Computamatch was one of the first MS-DOS-based systems developed and is still in use today in several countries.\n\nThe first commercially available software product was OSC/R (pronounced \"Oscar\"), a DOS-based application developed in Toronto, Canada by The Adelaide Works. OSC/R was very widely used and at the time was the only negative cutting software on the market until Adelaide Works ceased operation in 1993. OSC/R is still used today in some negative cutting facilities but has been mostly replaced by newer and more advanced systems. Excalibur was a later Windows 98 based product developed by FilmLab Engineering in Britain. Film Fusion is one of the most recent developments and is a Windows XP and Vista based system developed in Sydney, Australia by Popsoft IT.\n\nNegative cutters use various hardware tools such as film synchronizers, re-winders, film splicers, scissors, film cement and film keykode readers. DigiSync, a purpose built keykode reader is used by most negative cutters in conjunction with software for logging the keykode from film. DigiSync was developed by Research In Motion and in 1998 it won a Technical Achievement Academy Award for the design and development of the DigiSync Film Keykode Reader. Research In Motion later moved on to bigger things and invented the BlackBerry Wireless Email Phone and is now a publicly listed company. Other brands of barcode scanners are also in use.\n\n"}
{"id": "262606", "url": "https://en.wikipedia.org/wiki?curid=262606", "title": "Negative mass", "text": "Negative mass\n\nIn theoretical physics, negative mass is matter whose mass is of opposite sign to the mass of normal matter, e.g. −1 kg. Such matter would violate one or more energy conditions and show some strange properties, stemming from the ambiguity as to whether attraction should refer to force or the oppositely oriented acceleration for negative mass. It is used in certain speculative hypothesis, such as on the construction of traversable wormholes and the Alcubierre drive. Initially, the closest known real representative of such exotic matter is a region of negative pressure density produced by the Casimir effect.\n\nGeneral relativity describes gravity and the laws of motion for both positive and negative energy particles, hence negative mass, but does not include the other fundamental forces. On the other hand, the Standard Model describes elementary particles and the other fundamental forces, but it does not include gravity. A unified theory that explicitly includes gravity along with the other fundamental forces may be needed for a better understanding of the concept of negative mass.\n\nNegative mass is any region of space in which for some observers the mass density is measured to be negative. This could occur due to a region of space in which the stress component of the Einstein stress–energy tensor is larger in magnitude than the mass density. All of these are violations of one or another variant of the positive energy condition of Einstein's general theory of relativity; however, the positive energy condition is not a required condition for the mathematical consistency of the theory.\n\nEver since Newton first formulated his theory of gravity, there have been at least three conceptually distinct quantities called mass:\n\nEinstein’s equivalence principle postulates that inertial mass must equal passive gravitational mass. The law of conservation of momentum requires that active and passive gravitational mass be identical. All experimental evidence to date has found these are, indeed, always the same. In considering negative mass, it is important to consider which of these concepts of mass are negative. In most analyses of negative mass, it is assumed that the equivalence principle and conservation of momentum continue to apply, and therefore all three forms of mass are still the same.\n\nIn his 4th-prize essay for the 1951 Gravity Research Foundation competition, Joaquin Mazdak Luttinger considered the possibility of negative mass and how it would behave under gravitational and other forces.\n\nIn 1957, following Luttinger's idea, Hermann Bondi suggested in a paper in \"Reviews of Modern Physics\" that mass might be negative as well as positive. He pointed out that this does not entail a logical contradiction, as long as all three forms of mass are negative, but that the assumption of negative mass involves some counter-intuitive form of motion. For example, an object with negative inertial mass would be expected to accelerate in the opposite direction to that in which it was pushed (non-gravitationally).\n\nThere have been several other analyses of negative mass, such as the studies conducted by R. M. Price, however none addressed the question of what kind of energy and momentum would be necessary to describe non-singular negative mass. Indeed, the Schwarzschild solution for negative mass parameter has a naked singularity at a fixed spatial position. The question that immediately comes up is, would it not be possible to smooth out the singularity with some kind of negative mass density. The answer is yes, but not with energy and momentum that satisfies the dominant energy condition. This is because if the energy and momentum satisfies the dominant energy condition within a spacetime that is asymptotically flat, which would be the case of smoothing out the singular negative mass Schwarzschild solution, then it must satisfy the positive energy theorem, i.e. its ADM mass must be positive, which is of course not the case. However, it was noticed by Belletête and Paranjape that since the positive energy theorem does not apply to asymptotic de Sitter spacetime, it would actually be possible to smooth out, with energy–momentum that does satisfy the dominant energy condition, the singularity of the corresponding exact solution of negative mass Schwarzschild–de Sitter, which is the singular, exact solution of Einstein's equations with cosmological constant. In a subsequent article, Mbarek and Paranjape showed that it is in fact possible to obtain the required deformation through the introduction of the energy–momentum of a perfect fluid.\n\nAlthough no particles are known to have negative mass, physicists (primarily Hermann Bondi in 1957, William B. Bonnor in 1989, then Robert L. Forward) have been able to describe some of the anticipated properties such particles may have. Assuming that all three concepts of mass are equivalent the gravitational interactions between masses of arbitrary sign can be explored, based on the Einstein field equations and the equivalence principle:\n\n\nFor two positive masses, nothing changes and there is a gravitational pull on each other causing an attraction. Two negative masses would repel because of their negative inertial masses. For different signs however, there is a push that repels the positive mass from the negative mass, and a pull that attracts the negative mass towards the positive one at the same time.\n\nHence Bondi pointed out that two objects of equal and opposite mass would produce a constant acceleration of the system towards the positive-mass object, an effect called \"runaway motion\" by Bonnor who disregarded its physical existence, stating: \nSuch a couple of objects would accelerate without limit (except relativistic one); however, the total mass, momentum and energy of the system would remain 0.\n\nThis behavior is completely inconsistent with a common-sense approach and the expected behaviour of 'normal' matter; but is completely mathematically consistent and introduces no violation of conservation of momentum or energy. If the masses are equal in magnitude but opposite in sign, then the momentum of the system remains zero if they both travel together and accelerate together, no matter what their speed:\n\nAnd equivalently for the kinetic energy:\n\nHowever, this is perhaps not exactly valid if the energy in the gravitational field is taken into account.\n\nForward extended Bondi's analysis to additional cases, and showed that even if the two masses and are not the same, the conservation laws remain unbroken. This is true even when relativistic effects are considered, so long as inertial mass, not rest mass, is equal to gravitational mass.\n\nThis behaviour can produce bizarre results: for instance, a gas containing a mixture of positive and negative matter particles will have the positive matter portion increase in temperature without bound. However, the negative matter portion gains negative temperature at the same rate, again balancing out. Geoffrey A. Landis pointed out other implications of Forward's analysis, including noting that although negative mass particles would repel each other gravitationally, the electrostatic force would be attractive for like charges and repulsive for opposite charges.\n\nForward used the properties of negative-mass matter to create the concept of diametric drive, a design for spacecraft propulsion using negative mass that requires no energy input and no reaction mass to achieve arbitrarily high acceleration.\n\nForward also coined a term, \"nullification\", to describe what happens when ordinary matter and negative matter meet: they are expected to be able to cancel out or nullify each other's existence. An interaction between equal quantities of positive mass matter (hence of positive energy ) and negative mass matter (of negative energy ) would release no energy, but because the only configuration of such particles that has zero momentum (both particles moving with the same velocity in the same direction) does not produce a collision, all such interactions would leave a surplus of momentum, which is classically forbidden. So once this runaway phenomenon has been revealed, the scientific community considered negative mass could not exist in the universe.\n\nIn 1970, Jean-Marie Souriau demonstrated, through the complete Poincaré group of dynamic group theory, that reversing the energy of a particle (hence its mass, if the particle has one) is equal to reversing its arrow of time.\n\nThe universe according to general relativity is a Riemannian manifold associated to a metric tensor solution of Einstein’s field equations. In such a framework, the runaway motion prevents the existence of negative matter.\n\nSome bimetric theories of the universe propose that two parallel universes instead of one may exist with an opposite arrow of time, linked together by the Big Bang and interacting only through gravitation. The universe is then described as a manifold associated to two Riemannian metrics (one with positive mass matter and the other with negative mass matter). According to group theory, the matter of the conjugated metric would appear to the matter of the other metric as having opposite mass and arrow of time (though its proper time would remain positive). The coupled metrics have their own geodesics and are solutions of two coupled field equations:\n\nThe Newtonian approximation then provides the following interaction laws:\nThose laws are different to the laws described by Bondi and Bonnor, and solve the runaway paradox. The negative matter of the coupled metric, interacting with the matter of the other metric via gravity, could be an alternative candidate for the explanation of dark matter, dark energy, cosmic inflation and accelerating universe.\n\nIn electromagnetism one can derive the energy density of a field from Gauss's law, assuming the curl of the field is 0. Performing the same calculation using Gauss's law for gravity produces a negative energy density for a gravitational field.\n\nThe overwhelming consensus among physicists is that antimatter has positive mass and should be affected by gravity just like normal matter. Direct experiments on neutral antihydrogen have not been sensitive enough to detect any difference between the gravitational interaction of antimatter, compared to normal matter.\n\nBubble chamber experiments provide further evidence that antiparticles have the same inertial mass as their normal counterparts. In these experiments, the chamber is subjected to a constant magnetic field that causes charged particles to travel in helical paths, the radius and direction of which correspond to the ratio of electric charge to inertial mass. Particle–antiparticle pairs are seen to travel in helices with opposite directions but identical radii, implying that the ratios differ only in sign; but this does not indicate whether it is the charge or the inertial mass that is inverted. However, particle–antiparticle pairs are observed to electrically attract one another. This behavior implies that both have positive inertial mass and opposite charges; if the reverse were true, then the particle with positive inertial mass would be repelled from its antiparticle partner.\n\nPhysicist Peter Engels and a team of colleagues at Washington State University claimed to have observed negative mass behavior in rubidium atoms. On 10 April 2017 Engels team created negative \"effective\" mass by reducing the temperature of rubidium atoms to near absolute zero, generating a Bose–Einstein condensate. By using a laser-trap, the team were able to reverse the spin of some of the rubidium atoms in this state, and observed that once released from the trap, the atoms expanded and displayed properties of negative mass, in particular accelerating towards a pushing force instead of away from it.\nThis kind of negative effective mass is analogous to the well-known apparent negative effective mass of electrons in the upper part of the dispersion bands in solids. However, neither case is negative mass for the purposes of the stress–energy tensor.\n\nSome recent work with metamaterials suggests that some as-yet-undiscovered composite of superconductors, metamaterials and normal matter could exhibit signs of negative effective mass in much the same way as low temperature alloys melt at below the melting point of their components or some semiconductors have negative differential resistance.\nIn 1928, Paul Dirac's theory of elementary particles, now part of the Standard Model, already included negative solutions. The Standard Model is a generalization of quantum electrodynamics (QED) and negative mass is already built into the theory.\n\nMorris, Thorne and Yurtsever pointed out that the quantum mechanics of the Casimir effect can be used to produce a locally mass-negative region of space–time. In this article, and subsequent work by others, they showed that negative matter could be used to stabilize a wormhole. Cramer \"et al.\" argue that such wormholes might have been created in the early universe, stabilized by negative-mass loops of cosmic string. Stephen Hawking has proved that negative energy is a necessary condition for the creation of a closed timelike curve by manipulation of gravitational fields within a finite region of space; this proves, for example, that a finite Tipler cylinder cannot be used as a time machine.\n\nFor energy eigenstates of the Schrödinger equation, the wavefunction is wavelike wherever the particle's energy is greater than the local potential, and exponential-like (evanescent) wherever it is less. Naively, this would imply kinetic energy is negative in evanescent regions (to cancel the local potential). However, kinetic energy is an operator in quantum mechanics, and its expectation value is always positive, summing with the expectation value of the potential energy to yield the energy eigenvalue.\n\nFor wavefunctions of particles with zero rest mass (such as photons), this means that any evanescent portions of the wavefunction would be associated with a local negative mass–energy. However, the Schrödinger equation does not apply to massless particles; instead the Klein–Gordon equation is required.\n\nOne can achieve a negative mass independent of negative energy. According to mass–energy equivalence, mass is in proportion to energy and the coefficient of proportionality is . Actually, is still equivalent to although the coefficient is another constant such as . In this case, it is unnecessary to introduce a negative energy because the mass can be negative although the energy is positive. That is to say,\n\nUnder the circumstances,\n\nand so,\n\nWhen ,\n\nConsequently,\n\nwhere is invariant mass and invariant energy equals . The squared mass is still positive and the particle can be stable.\n\nFrom the above relation,\n\nThe negative momentum is applied to explain negative refraction, the inverse Doppler effect and the reverse Cherenkov effect observed in a negative index metamaterial. The radiation pressure in the metamaterial is also negative because the force is defined as . Negative pressure exists in dark energy too. Using these above equations, the energy–momentum relation should be\n\nSubstituting the Planck–Einstein relation and de Broglie's , we obtain the following dispersion relation \n\nwhen the wave consists of a stream of particles whose energy–momentum relation is formula_11 (wave–particle duality) and can be excited in a negative index metamaterial. The velocity of such a particle is equal to\n\nand range is from zero to infinity\n\nMoreover, the kinetic energy is also negative\n\nIn fact, negative kinetic energy exists in some models to describe dark energy (phantom energy) whose pressure is negative. In this way, the negative mass of exotic matter is now associated with negative momentum, negative pressure, negative kinetic energy and faster-than-light phenomena.\n\n"}
{"id": "3778432", "url": "https://en.wikipedia.org/wiki?curid=3778432", "title": "Negative repetition", "text": "Negative repetition\n\nA negative repetition (negative rep) is the repetition of a technique in weight lifting in which the lifter performs the eccentric phase of a lift. Instead of pressing the weight up slowly, in proper form, a spotter generally aids in the concentric, or lifting, portion of the repetition while the lifter slowly performs the eccentric phase for 3–6 seconds. Negative reps are used to improve both muscular strength and power in subjects, this is commonly known as hypertrophy training.\n\nDue to its mechanical properties, this form of training can be used for both healthy individuals and individuals who are in rehabilitation. Studies have shown that negative repetitions or \"eccentric phase training\" combines a high amount of force on the muscle with a lower energy cost than normal concentric training, which requires 4–5 times the amount of energy. This justifies why this type of training is more beneficial and less of a risk to subjects rehabilitating or with a limited exercise capacity.\n\nEccentric training is often associated with the terms \"muscles soreness\" and \"muscle damage\". In 1902, Theodore Hough discovered and developed the term DOMS (delayed onset muscle soreness), after he found that exercises containing negative repetitions caused athletes to have sore muscles. Hough believed this was causing a rupture within the muscle; when he looked further into the subject, he found that when performing eccentric exercise that exhibited soreness, the muscle \"quickly adapts and becomes accustomed to the increase in applied stress\". The result of this was that the muscles' soreness not only decreased, but the muscular damage did too.\n\nIt has been proven that eccentric resistance training improves the functional mobility of older adults. Studies have shown that eccentric training of the lower body, in particular the , are essential in preventing falls in older adults and helping them maintain their independence. A study conducted focusing on eccentric training for the age group 65–87 years of age showed that, over 12 weeks, they had strengthened their knee extensors by up to 26%. With this evidence, it is reasonable to suggest that negative repetitions can help improve the health of older adults.\n\nStudies have shown that eccentric training may be successful in the treatment of certain tendonitis. Studies have shown that the use of eccentric training for twelve weeks may be an alternative to therapy for people suffering from Patellar Tendinopathy (Jumper's Knee). Eccentric training has also been proven successful in the treatment of chronic Achilles tendonitis, using a twelve-week eccentric calf muscle program various studies have shown the ability for people to return to normal pre-tendonitis levels. The reasoning behind the benefits of eccentric training for tendinopathy is still unclear.\n\n"}
{"id": "362728", "url": "https://en.wikipedia.org/wiki?curid=362728", "title": "Negative temperature", "text": "Negative temperature\n\nIn quantum thermodynamics, certain systems can achieve negative temperature; that is, their temperature can be expressed as a negative quantity on the Kelvin or Rankine scales.\n\nA system with a truly negative temperature on the Kelvin scale is \"hotter\" than any system with a positive temperature. If a negative-temperature system and a positive-temperature system come in contact, heat will flow from the negative- to the positive-temperature system. A standard example of such a system is population inversion in laser physics.\n\nTemperature is loosely interpreted as the average kinetic energy of the system's particles. The existence of negative temperature, let alone negative temperature representing \"hotter\" systems than positive temperature, would seem paradoxical in this interpretation. \nThe paradox is resolved by considering the more rigorous definition of thermodynamic temperature as the tradeoff between internal energy and entropy\ncontained in the system, with \"coldness\", the \"reciprocal\" of temperature, being the more fundamental quantity. \nSystems with a positive temperature will increase in entropy as one adds energy to the system, while \nsystems with a negative temperature will decrease in entropy as one adds energy to the system.\n\nClassical thermodynamic systems cannot achieve negative temperatures: adding heat always increases their entropy. \nThe possibility of a decrease in entropy as energy increases requires the system to \"saturate\" in entropy. This is only possible if the number of high energy states is limited. In classical Boltzmann statistics, the number of high energy states is unlimited (particle speeds can in principle be increased indefinitely).\nSystems bounded by a maximum amount of energy are generally forbidden in classical mechanics, and the phenomenon of negative temperature is strictly a \nquantum mechanical phenomenon. Some systems, however (see the examples below), have a maximum amount of energy that they can hold, and as they approach that maximum energy their entropy actually begins to decrease.\n\nThe definition of thermodynamic temperature as a function of the change in the system's entropy under reversible heat transfer \nformula_1:\nEntropy being a state function, the integral of over any cyclical process is zero.\nFor a system in which the entropy is purely a function of the system's energy , the temperature can be defined as:\n\nEquivalently, thermodynamic beta, or \"coldness\", is defined as \nwhere is the Boltzmann constant.\n\nNote that in classical thermodynamics, is defined in terms of temperature. This is reversed here, is the statistical entropy, a function of the possibly microstates of the system, and temperature conveys information on the distribution of energy levels among the possible microstates.\nFor systems with many degrees of freedom, the statistical and thermodynamic definitions of entropy are generally consistent with each other. \nHowever, for small systems and systems where the number of states decreases with energy, the definitions of statistical entropy and thermodynamic entropy are not necessarily consistent, and the temperatures derived from these entropies are different. \n\nSome theorists have proposed using an alternate definition of entropy originally proposed by Gibbs as a way to resolve these inconsistencies, although this new definition would create other inconsistencies.\n\nNegative temperatures can only exist in a system where there are a limited number of energy states (see below). As the temperature is increased on such a system, particles move into higher and higher energy states, and as the temperature increases, the number of particles in the lower energy states and in the higher energy states approaches equality. (This is a consequence of the definition of temperature in statistical mechanics for systems with limited states.) By injecting energy into these systems in the right fashion, it is possible to create a system in which there are more particles in the higher energy states than in the lower ones. The system can then be characterised as having a negative temperature.\n\nA substance with a negative temperature is not colder than absolute zero, but rather it is hotter than infinite temperature. As Kittel and Kroemer (p. 462) put it, \"The temperature scale from cold to hot runs:\n\nThe corresponding inverse temperature scale, for the quantity \"β\" = 1/\"kT\" (where \"k\" is Boltzmann's constant), runs continuously from low energy to high as +∞, … , 0, …, −∞. Because it avoids the abrupt jump from +∞ to −∞, β is considered more natural than \"T\". Although a system can have multiple negative temperature regions and thus have -∞ to +∞ discontinuities.\n\nIn many familiar physical systems, temperature is associated to the kinetic energy of atoms. Since there is no upper bound on the momentum of an atom, there is no upper bound to the number of energy states available when more energy is added, and therefore no way to get to a negative temperature. However, in statistical mechanics, temperature can correspond to other degrees of freedom than just kinetic energy (see below).\n\nThe distribution of energy among the various translational, vibrational, rotational, electronic, and nuclear modes of a system determines the macroscopic temperature. In a \"normal\" system, thermal energy is constantly being exchanged between the various modes.\n\nHowever, in some situations, it is possible to isolate one or more of the modes. In practice, the isolated modes still exchange energy with the other modes, but the time scale of this exchange is much slower than for the exchanges within the isolated mode. One example is the case of nuclear spins in a strong external magnetic field. In this case, energy flows fairly rapidly among the spin states of interacting atoms, but energy transfer between the nuclear spins and other modes is relatively slow. Since the energy flow is predominantly within the spin system, it makes sense to think of a spin temperature that is distinct from the temperature associated to other modes.\n\nA definition of temperature can be based on the relationship:\n\nThe relationship suggests that a \"positive temperature\" corresponds to the condition where entropy, \"S\", increases as thermal energy, \"q\", is added to the system. This is the \"normal\" condition in the macroscopic world, and is always the case for the translational, vibrational, rotational, and non-spin related electronic and nuclear modes. The reason for this is that there are an infinite number of these types of modes, and adding more heat to the system increases the number of modes that are energetically accessible, and thus increases the entropy.\n\nThe simplest example, albeit a rather nonphysical one, is to consider a system of \"N\" particles, each of which can take an energy of either \"+ε\" or \"-ε\" but are otherwise noninteracting. This can be understood as a limit of the Ising model in which the interaction term becomes negligible. The total energy of the system is\n\nwhere \"σ\" is the sign of the \"i\"th particle and \"j\" is the number of particles with positive energy minus the number of particles with negative energy. From elementary combinatorics, the total number of microstates with this amount of energy is a binomial coefficient:\n\nBy the fundamental assumption of statistical mechanics, the entropy of this microcanonical ensemble is\n\nWe can solve for thermodynamic beta (β = 1/kT) by considering it as a central difference without taking the continuum limit:\n\nhence the temperature\n\nThis entire proof assumes the microcanonical ensemble with energy fixed and temperature being the emergent property. In the canonical ensemble, the temperature is fixed and energy is the emergent property. This leads to (formula_14 refers to microstates):\nFollowing the previous example, we choose a state with two levels and two particles. This leads to microstates formula_18, formula_19, formula_20, and formula_21.\nThe resulting values for formula_28, formula_29, and formula_30 are all increasing with formula_31 and never need to enter a negative temperature regime.\n\nThe previous example is approximately realized by a system of nuclear spins in an external magnetic field. This allows the experiment to be run as a variation of nuclear magnetic resonance spectroscopy. In the case of electronic and nuclear spin systems, there are only a finite number of modes available, often just two, corresponding to spin up and spin down. In the absence of a magnetic field, these spin states are \"degenerate\", meaning that they correspond to the same energy. When an external magnetic field is applied, the energy levels are split, since those spin states that are aligned with the magnetic field will have a different energy from those that are anti-parallel to it.\n\nIn the absence of a magnetic field, such a two-spin system would have maximum entropy when half the atoms are in the spin-up state and half are in the spin-down state, and so one would expect to find the system with close to an equal distribution of spins. Upon application of a magnetic field, some of the atoms will tend to align so as to minimize the energy of the system, thus slightly more atoms should be in the lower-energy state (for the purposes of this example we will assume the spin-down state is the lower-energy state). It is possible to add energy to the spin system using radio frequency (RF) techniques. This causes atoms to \"flip\" from spin-down to spin-up.\n\nSince we started with over half the atoms in the spin-down state, this initially drives the system towards a 50/50 mixture, so the entropy is increasing, corresponding to a positive temperature. However, at some point, more than half of the spins are in the spin-up position. In this case, adding additional energy reduces the entropy, since it moves the system further from a 50/50 mixture. This reduction in entropy with the addition of energy corresponds to a negative temperature. In NMR spectroscopy, this corresponds to pulses with a pulse width of over 180° (for a given spin). While relaxation is fast in solids, it can take several seconds in solutions and even longer in gases and in ultracold systems; several hours were reported for silver and rhodium at picokelvin temperatures. It is still important to understand that the temperature is negative only with respect to nuclear spins. Other degrees of freedom, such as molecular vibrational, electronic and electron spin levels are at a positive temperature, so the object still has positive sensible heat. Relaxation actually happens by exchange of energy between the nuclear spin states and other states (e.g. through the nuclear Overhauser effect with other spins).\n\nThis phenomenon can also be observed in many lasing systems, wherein a large fraction of the system's atoms (for chemical and gas lasers) or electrons (in semiconductor lasers) are in excited states. This is referred to as a population inversion.\n\nThe Hamiltonian for a single mode of a luminescent radiation field at frequency \"ν\" is\nThe density operator in the grand canonical ensemble is\nFor the system to have a ground state, the trace to converge, and the density operator to be generally meaningful, \"βH\" must be positive semidefinite. So if \"hν\" < \"μ\", and \"H\" is negative semidefinite, then \"β\" must itself be negative, implying a negative temperature.\n\nThe two-dimensional systems can exist in negative temperature states.\n\nNegative temperatures have also been achieved in motional degrees of freedom. Using an optical lattice, upper bounds were placed on the kinetic energy, interaction energy and potential energy of cold atoms. This was done by tuning the interactions of the atoms from repulsive to attractive using a Feshbach resonance and changing the overall harmonic potential from trapping to anti-trapping, thus transforming the Bose-Hubbard Hamiltonian from formula_34. Performing this transformation adiabatically while keeping the atoms in the Mott insulator regime, it is possible to go from a low entropy positive temperature state to a low entropy negative temperature state. In the negative temperature state, the atoms macroscopically occupy the maximum momentum state of the lattice. The negative temperature ensembles equilibrated and showed long lifetimes in an anti-trapping harmonic potential.\n\n\n"}
{"id": "4003593", "url": "https://en.wikipedia.org/wiki?curid=4003593", "title": "Negative volume index", "text": "Negative volume index\n\nNearly 78 years have passed since Paul L. Dysart, Jr. invented the Negative Volume Index and Positive Volume Index indicators. The indicators remain useful to identify primary market trends and reversals.\n\nIn 1936, Paul L. Dysart, Jr. began accumulating two series of advances and declines distinguished by whether volume was greater or lesser than the prior day's volume. He called the cumulative series for the days when volume had been greater than the prior day's volume the Positive Volume Index (PVI), and the series for the days when volume had been lesser the Negative Volume Index (NVI).\n\nA native of Iowa, Dysart worked in Chicago's LaSalle Street during the 1920s. After giving up his Chicago Board of Trade membership, he published an advisory letter geared to short-term trading using advance-decline data. In 1933, he launched the \"Trendway\" weekly stock market letter and published it until 1969 when he died. Dysart also developed the 25-day Plurality Index, the 25-day total of the absolute difference between the number of advancing issues and the number of declining issues, and was a pioneer in using several types of volume of trading studies. Richard Russell, editor of Dow Theory Letters, in his January 7, 1976 letter called Dysart \"one of the most brilliant of the pioneer market technicians.\"\n\nThe daily volume of the New York Stock Exchange and the NYSE Composite Index's advances and declines drove Dysart's indicators. Dysart believed that “volume is the driving force in the market.” He began studying market breadth numbers in 1931, and was familiar with the work of Leonard P. Ayres and James F. Hughes, who pioneered the tabulation of advances and declines to interpret stock market movements.\n\nDysart calculated NVI as follows: 1) if today's volume is less than yesterday's volume, subtract declines from advances, 2) add the difference to the cumulative NVI beginning at zero, and 3) retain the current NVI reading for the days when volume is greater than the prior day's volume. He calculated PVI in the same manner but for the days when volume was greater than the prior day's volume. NVI and PVI can be calculated daily or weekly.\n\nInitially, Dysart believed that PVI would be the more useful series, but in 1967, he wrote that NVI had “proved to be the most valuable of all the breadth indexes.” He relied most on NVI, naming it AMOMET, the acronym of “A Measure Of Major Economic Trend.”\n\nDysart's theory, expressed in his 1967 Barron's article, was that “if volume advances and prices move up or down in accordance [with volume], the move is assumed to be a good movement - if it is sustained when the volume subsides.” In other words, after prices have moved up on positive volume days, \"if prices stay up when the volume subsides for a number of days, we can say that such a move is 'good'.\" If the market “holds its own on negative volume days after advancing on positive volume, the market is in a strong position.”\n\nHe called PVI the “majority” curve. Dysart distinguished between the actions of the “majority” and those of the “minority.” The majority tends to emulate the minority, but its timing is not as sharp as that of the minority. When the majority showed an appetite for stocks, the PVI was usually “into new high ground” as happened in 1961.\n\nIt is said that the two indicators assume that \"smart\" money is traded on quiet days (low volume) and that the crowd trades on very active days. Therefore, the negative volume index picks out days when the volume is lower than on the previous day, and the positive index picks out days with a higher volume.\n\nBesides an article he wrote for Barron's in 1967, not many of Dysart's writings are available. What can be interpreted about Dysart's NVI is that whenever it rises above a prior high, and the DJIA is trending up, a “Bull Market Signal” is given. When the NVI falls below a prior low, and the DJIA is trending down, a “Bear Market Signal” is given. The PVI is interpreted in reverse.\nHowever, not all movements above or below a prior NVI or PVI level generate signals, as Dysart also designated “bullish” and “bearish penetrations.” These penetrations could occur before or after a Bull or Bear Market Signal, and at times were called “reaffirmations” of a signal. In 1969, he articulated one rule: “signals are most authentic when the NVI has moved sideways for a number of months in a relatively narrow range.” Dysart cautioned that “there is no mathematical system devoid of judgment which will continuously work without error in the stock market.”\n\nAccording to Dysart, between 1946 and 1967, the NVI “rendered 17 significant signals,” of which 14 proved to be right (an average of 4.32% from the final high or low) and 3 wrong (average loss of 6.33%). However, NVI “seriously erred” in 1963-1964 and in 1968, which concerned him. In 1969, Dysart reduced the weight he had previously given to the NVI in his analyses because NVI was no longer a “decisive” indicator of the primary trend, although it retained an “excellent ability to give us ‘leading’ indications of short-term trend reversals.”\n\nA probable reason for the NVI losing its efficacy during the mid-1960s may have been the steadily higher NYSE daily volume due to the dramatic increase in the number of issues traded so that prices rose on declining volume. Dysart’s NVI topped out in 1955 and trended down until at least 1968, although the DJIA moved higher during that period.\nNorman G. Fosback has attributed the “long term increase in the number of issues traded” as a reason for a downward bias in a cumulative advance-decline line. Fosback was the next influential technician in the story of NVI and PVI.\n\nFosback studied NVI and PVI and in 1976 reported his findings in his classic Stock Market Logic. He did not elucidate on the indicators’ background or mentioned Dysart except for saying that “in the past Negative Volume Indexes have always [his emphasis] been constructed using advance-decline data….” He posited, “There is no good reason for this fixation on the A/D Line. In truth, a Negative Volume Index can be calculated with any market index - the Dow Jones Industrial Average, the S&P 500, or even ‘unweighted’ market measures... Somehow this point has escaped the attention of technicians to date.”\n\nThe point had not been lost on Dysart, who wrote in Barron’s, “we prefer to use the issues-traded data [advances and declines] rather than the price data of any average because it is more all-encompassing, and more truly represents what’s happening in the entire market.” Dysart was a staunch proponent of using advances and declines.\n\nFosback made three variations to NVI and PVI:\n\n1. He cumulated the daily percent change in the market index rather than the difference between advances and declines. On negative volume days, he calculated the price change in the index from the prior day and added it to the most recent NVI. His calculations are as follows:\n\nIf C and C denote the closing prices of today and yesterday, respectively, the NVI for today is calculated by\n\n\nand the PVI is calculated by:\n\n\n2. He suggested starting the cumulative count at a base index level such as 100.\n\n3. He derived buy or sell signals by whether the NVI or PVI was above or below its one-year moving average.\n\nFosback's versions of NVI and PVI are what are popularly described in books and posted on Internet financial sites. Often reported are his findings that whenever NVI is above its one-year moving average there is a 96% (PVI - 79%) probability that a bull market is in progress, and when it is below its one-year moving average, there is a 53% (PVI - 67%) probability that a bear market is in place. These results were derived using a 1941-1975 test period. Modern tests might reveal different probabilities.\n\nToday, NVI and PVI are commonly associated with Fosback's versions, and Dysart, their inventor, is forgotten. It cannot be said that one version is better than the other. While Fosback provided a more objective interpretation of these indicators, Dysart's versions offer value to identify primary trends and short-term trend reversals.\n\nAlthough some traders use Fosback's NVI and PVI to analyze individual stocks, the indicators were created to track, and have been tested, on major market indexes. NVI was Dysart's most invaluable breadth index, and Fosback found that his version of “the Negative Volume Index is an excellent indicator of the primary market trend.” Traders can benefit from both innovations.\n\n"}
{"id": "19777249", "url": "https://en.wikipedia.org/wiki?curid=19777249", "title": "Organizing principle", "text": "Organizing principle\n\nAn organizing principle is a core assumption from which everything else by proximity can derive a classification or a value. It is like a central reference point that allows all other objects to be located, often used in a conceptual framework. Having an organizing principle might help one simplify and get a handle on a particularly complicated domain or phenomenon. On the other hand, it might create a deceptive prism that colors one's judgment.\n\n\n\n"}
{"id": "30226192", "url": "https://en.wikipedia.org/wiki?curid=30226192", "title": "Polar concept argument", "text": "Polar concept argument\n\nA polar concept argument is a type of argument that posits the understanding of one concept, from the mere understanding of its polar opposite. A well-known instance of a polar concept argument is Gilbert Ryle's argument against scepticism (1960). According to Anthony Grayling's characterisation, Ryle's argument can be stated as follows:\n\nAccording to Ryle's polar concept argument, counterfeit and genuine coins come in pairs, and one cannot conceive of counterfeit coins without also capturing the essence of the genuine coins at the same time. When one grasps the essence of one polar concept, one also grasps immediately the essence of its polar opposite. Ryle's original argument (1960) runs as follows:\n\nA polar concept argument bears on some more or less strong version of dialectical monism, a philosophical doctrine that views reality as a unified whole, due to the complementarity of polar concepts.\n\n"}
{"id": "51837768", "url": "https://en.wikipedia.org/wiki?curid=51837768", "title": "Positive and Negative Affect Schedule", "text": "Positive and Negative Affect Schedule\n\nThe Positive and Negative Affect Schedule (PANAS) is a self-report questionnaire that consists of two 10-item scales to measure both positive and negative affect. Each item is rated on a 5-point scale of 1 \"(not at all)\" to 5 \"(very much)\". The measure has been used mainly as a research tool in group studies, but can be utilized within clinical and non-clinical populations as well. Shortened, elongated, and children's versions of the PANAS have been developed, taking approximately 5–10 minutes to complete. Clinical and non-clinical studies have found the PANAS to be a reliable and valid instrument in the assessment of positive and negative affect.\n\nThe PANAS was developed in 1988 by researchers from the University of Minnesota and Southern Methodist University. Previous mood measures have shown correlations of variable strength between positive and negative affect, and these same measures have questionable reliability and validity. Watson, Clark, and Tellegen developed the PANAS in an attempt to provide a better, purer measure of each of these dimensions.\n\nThe researchers extracted 60 terms from the factor analyses of Michael Zevon and Tellegen shown to be relatively accurate markers of either positive or negative affect, but not both. They chose terms that met a strong correlation to one corresponding dimension but exhibited a weak correlation to the other. Through multiple rounds of elimination and preliminary analyses with a test population, the researchers arrived at 10 terms for each of the two scales, as follows:\n\nThe PANAS for Children (PANAS-C) was developed in an attempt to differentiate the affective expressions of anxiety and depression in children. The tripartite model on which this measure is based suggests that high levels of negative affect is present in those with anxiety and depression, but high levels of positive affect is not shared between the two. Previous mood scales for children have been shown to reliably capture the former relationship but not the latter; the PANAS-C was created as a tool with better discriminant validity for child assessment. Similar to the development of the original PANAS, the PANAS-C drew from terms of the PANAS-X and eliminated several terms with insufficient correlations between the term and the affective construct after preliminary analyses with a non-clinical sample of children. The final version of the measure consists of 27 items: 12 positive affect terms and 15 negative affect terms. Despite the purpose of its development, however, the measure’s discriminant validity is still wanting.\n\nThe PANAS-SF, comprises 10 items that were determined through the highest factor loadings on the exploratory factor analysis reported by Watson et al. (1988) in his original PANAS. Previous mood scales, such that of Bradburn, had low reliabilities and high correlations between subscales. Watson was able to address these concerns in his study of the original PANAS; however, his participants consisted mostly of student populations. The purpose of the PANAS-SF was not only to provide a shorter and more concise form of the PANAS, but to be able to apply the schedules to older clinical populations. Overall, it was reported that this modified model was consistent with Watson’s.\n\nSeparate from the PANAS-SF, Edmund Thompson created the international PANAS short form (I-PANAS-SF) in order to make a 10 item mood scale that can be implemented effectively on an international level, provide more clarity on the content of the items, reduce ambiguities, address the limitations of the original and the previous short form of the PANAS, and also to provide a shorter, yet dependable and valid scale. To determine the 10 items of the 20 original items, two focus groups were utilized to evaluate all of the original 20 PANAS items. They found that while some items were easily understood by the participant, certains items had different meanings or were too ambiguous. Items that had too much ambiguity were eliminated from the modified form. Researchers found that the I-PANAS-SF had high correlations with the original PANAS. Through multiple tests and studies, they were able to determine that the I-PANAS-SF was on par with the original scale and can be used as a reliable, valid, brief, and efficient instrument on an international scale.\n\nIn 1994, Watson and Clark developed an expanded form of the PANAS, called the PANAS-X, that consists of 60 items that can be completed in 10 minutes or less. The PANAS-X incorporates the original, higher order dimensions specified in the PANAS in addition to the measures of 11 lower order emotional states. These measures are broken down into three main categories: basic negative emotion scales consisting of fear, hostility, guilt, and sadness; basic positive emotion scales consisting of joviality, self-assurance, and attentiveness; and other affective states consisting of shyness, fatigue, serenity, and surprise. Through extensive analyses, all eleven affective states, with the exception of surprise, were shown to be stable, valid measures that assess how an individual’s emotional states fluctuate over time.\n\nReliability refers to whether the scores are reproducible. Unless otherwise specified, the reliability scores and values come from studies done with a United States population sample.\n\nMany forms of the PANAS (PANAS-C, PANAS-X, I-PANAS-SF, and among others) have shown that the PANAS has been widely employed. Recent studies have also shown that the PANAS can be administered in a large general adult population, as well as other populations. However, to date, the PANAS is mostly used as a research tool in group studies, but it has the potential to be utilized in clinical work with individuals. Furthermore, the PANAS has the potential to be used to evaluate mental illnesses, as shown in an experiment conducted by Dyck, Jolly, and Kramer, which demonstrated its effectiveness in distinguishing between depression and anxiety in clinical samples.  \n\nSince the PANAS is a self-report questionnaire, it can be difficult to assess people’s mood accurately, as people can overstate or understate their experience of their moods. In addition, the original PANAS had a limited sample size of college students, which concerns with wide applicability to other samples. Furthermore, some studies claim that the PANAS is too long or that its items are redundant. The PANAS does not encompass higher order mood states.\n\n"}
{"id": "952842", "url": "https://en.wikipedia.org/wiki?curid=952842", "title": "Principle of compositionality", "text": "Principle of compositionality\n\nIn mathematics, semantics, and philosophy of language, the principle of compositionality is the principle that the meaning of a complex expression is determined by the meanings of its constituent expressions and the rules used to combine them. This principle is also called Frege's principle, because Gottlob Frege is widely credited for the first modern formulation of it. However, the idea appears already among Indian philosophers of grammar such as Yāska, and also in Plato's work such as in \"Theaetetus\".\nBesides, the principle was never explicitly stated by Frege, and it was arguably already assumed by George Boole decades before Frege’s work.\n\nThe principle of compositionality states that in a meaningful sentence, if the lexical parts are taken out of the sentence, what remains will be the rules of composition. Take, for example, the sentence \"Socrates was a man\". Once the meaningful lexical items are taken away—\"Socrates\" and \"man\"—what is left is the pseudo-sentence, \"S was a M\". The task becomes a matter of describing what the connection is between S and M.\n\nIt is frequently taken to mean that every operation of the syntax should be associated with an operation of the semantics that acts on the meanings of the constituents combined by the syntactic operation. As a guideline for constructing semantic theories, this is generally taken, as in the influential work on the philosophy of language by Donald Davidson, to mean that every construct of the syntax should be associated by a clause of the T-schema with an operator in the semantics that specifies how the meaning of the whole expression is built from constituents combined by the syntactic rule. In some general mathematical theories (especially those in the tradition of Montague grammar), this guideline is taken to mean that the interpretation of a language is essentially given by a homomorphism between an algebra of syntactic representations and an algebra of semantic objects.\n\nThe principle of compositionality also exists in a similar form in the compositionality of programming languages.\n\nThe principle of compositionality has been the subject of intense debate. Indeed, there is no general agreement as to how the principle is to be interpreted, although there have been several attempts to provide formal definitions of it. (Szabó, 2012)\n\nScholars are also divided as to whether the principle should be regarded as a factual claim, open to empirical testing; an analytic truth, obvious from the nature of language and meaning; or a methodological principle to guide the development of theories of syntax and semantics. The Principle of Compositionality has been attacked in all three spheres, although so far none of the criticisms brought against it have been generally regarded as compelling. Most proponents of the principle, however, make certain exceptions for idiomatic expressions in natural language. (Szabó, 2012)\n\nFurther, in the context of the philosophy of language, the principle of compositionality does not explain all of meaning. For example, you cannot infer sarcasm purely on the basis of words and their composition, yet a phrase used sarcastically means something completely different from the same phrase uttered straightforwardly. Thus, some theorists argue that the principle has to be revised to take into account linguistic and extralinguistic context, which includes the tone of voice used, common ground between the speakers, the intentions of the speaker, and so on. (Szabó, 2012)\n\n\n"}
{"id": "1166245", "url": "https://en.wikipedia.org/wiki?curid=1166245", "title": "Principle of distributivity", "text": "Principle of distributivity\n\nThe principle of distributivity states that the algebraic distributive law is valid for classical logic, where both logical conjunction and logical disjunction are distributive over each other so that for any propositions \"A\", \"B\" and \"C\" the equivalences\nand\nhold.\n\nThe principle of distributivity is valid in classical logic, but invalid in quantum logic.\n\nThe article \"Is Logic Empirical?\" discusses the case that quantum logic is the correct, empirical logic, on the grounds that the principle of distributivity is inconsistent with a reasonable interpretation of quantum phenomena.\n"}
{"id": "11585926", "url": "https://en.wikipedia.org/wiki?curid=11585926", "title": "Principle of humanity", "text": "Principle of humanity\n\nIn philosophy and rhetoric, the principle of humanity states that when interpreting another speaker we must assume that his or her beliefs and desires are connected to each other and to reality in some way, and attribute to him or her \"the propositional attitudes one supposes one would have oneself in those circumstances\". The principle of humanity was named by Richard Grandy (then an assistant professor of philosophy at Princeton University) who first expressed it in 1973.\n\n"}
{"id": "1593030", "url": "https://en.wikipedia.org/wiki?curid=1593030", "title": "Product/process distinction", "text": "Product/process distinction\n\nThe product/process distinction is the distinction between the product information and the process information of a consumer good. Product information is information that pertains to a consumer good, namely to its price, quality, and safety (its proximate attributes). Process information is information that pertains to the means by which the consumer good is made i.e. the working conditions under which it comes into being, as well as the treatment of animals involved in its production chain (its peripheral attributes).\n\nThe product/process distinction is used by the World Trade Organization (WTO) as a way to determine whether or not a complaint filed by an importing nation is valid and warrants trade barriers against the exporting nation. Under WTO rules, an importing nation can lodge a complaint with the WTO that the exporting nation uses methods for obtaining or producing the good in question that the importing nation finds to be immoral or unethical. If the independent World Trade Organization Advisory Board, made up of a panel of international law and trade experts, finds that the importing nation has a legitimate complaint, enforces said ethical standards for domestic production, and isn't trying to merely skirt its free trade obligations, then the Board will rule that trade barriers are justified. Despite what World Trade Organization officials have said, in practice the World Trade Organization finds these complaints illegitimate the vast majority of the time.\n\nFor example, if the European Union (EU) wants to ban imports of cosmetics that were tested on laboratory animals on grounds that such testing is unethical, it can file a complaint with the World Trade Organization and, in theory, the WTO would allow the EU to enact trade barriers provided that the EU bans its own domestic cosmetic producers from testing on laboratory animals. \nIn these cases, however, the World Trade Organization has consistently ruled that such barriers are illegal because only the process is different, while the final product itself is not. Therefore, the WTO has made the product/process distinction an important factor in determining whether trade barriers are justified.\n\nThe World Trade Organization has stated that if nations were able to enact barriers merely because the importing nation's standards differ from their own, control could be lost and barriers could be enacted around the world for frivolous reasons. However, many complain that these rulings go against the stated intentions of the World Trade Organization, and prove that the organization often puts commercial interests above environmental, ethical, and human rights issues.\n"}
{"id": "33112023", "url": "https://en.wikipedia.org/wiki?curid=33112023", "title": "Rationes seminales", "text": "Rationes seminales\n\nRationes seminales (Latin, from the Greek \"λόγοι σπερματικοὶ\" or \"logoi spermatikoi\"), translated variously as germinal or causal principles, primordial reasons, original factors, seminal reasons or virtues, or seedlike principles, is a theological theory on the origin of species. It is the doctrine that God created the world in seed form, with certain potentialities, which then developed or unfolded accordingly over time; what appears to be change is simply the realization of the preexisting potentialities. The theory is a metaphor of the growth of a plant: much like a planted seed eventually develops into a tree, so when God created the world he planted \"rationes seminales\", from which all life sprung. It is intended to reconcile the belief that God created all things, with the evident fact that new things are constantly developing.\n\nThe roots of this idea can be found within the Greek philosophy of the Stoics and Neoplatonism\nThe idea was incorporated into Christian thought through the writings of authors such as Athenagoras of Athens, Tertullian, Gregory of Nyssa, Augustine of Hippo, Bonaventure, Albertus Magnus, and Roger Bacon, until mostly rejected in the modern period. Evolution, though now it is seen to be compatible with evolution theories (cf \"Man incarnate spirit\" by Ramon Lucas Lucas). The idea of \"rationes seminales\" was also used as an explanation for spontaneous generation.\n\n"}
{"id": "685179", "url": "https://en.wikipedia.org/wiki?curid=685179", "title": "Schwinger's quantum action principle", "text": "Schwinger's quantum action principle\n\nThe Schwinger's quantum action principle is a variational approach to quantum mechanics and quantum field theory. This theory was introduced by Julian Schwinger. In this approach, the quantum action is an operator. Although it is superficially different from the path integral formulation where the action is a classical function, the modern formulation of\nthe two formalisms are identical.\n\nSuppose we have two states defined by the values of a complete set of commuting operators at two times. Let the early and late states be formula_1 and formula_2, respectively. Suppose that there is a parameter in the Lagrangian which can be varied, usually a source for a field. The main equation of Schwinger's quantum action principle is:\n\nwhere the derivative is with respect to small changes in the parameter.\n\nIn the path integral formulation, the transition amplitude is represented by the sum\nover all histories of formula_4, with appropriate boundary conditions representing the states formula_1 and formula_2. The infinitesimal change in the amplitude is clearly given by Schwinger's formula. Conversely, starting from Schwinger's formula, it is easy to show that the fields obey canonical commutation relations and the classical equations\nof motion, and so have a path integral representation. Schwinger's formulation was most significant because it could treat fermionic anticommuting fields with the same formalism as bose fields, thus implicitly introducing differentiation and integration\nwith respect to anti-commuting coordinates.\n\n"}
{"id": "244629", "url": "https://en.wikipedia.org/wiki?curid=244629", "title": "Scientific law", "text": "Scientific law\n\nThe laws of science, also called scientific laws or scientific principles, are statements that describe or predict a range of natural phenomena. Each scientific law is a statement based on repeated experimental observations that describes some aspect of the Universe. The term \"law\" has diverse usage in many cases (approximate, accurate, broad, or narrow theories) across all fields of natural science (physics, chemistry, biology, geology, astronomy, etc.). Scientific laws summarize and explain a large collection of facts determined by experiment, and are tested based on their ability to predict the results of future experiments. They are developed either from facts or through mathematics, and are strongly supported by empirical evidence. It is generally understood that they reflect causal relationships fundamental to reality, and are discovered rather than invented.\n\nLaws reflect scientific knowledge that experiments have repeatedly verified (and never falsified). Their accuracy does not change when new theories are worked out, but rather the scope of application, since the equation (if any) representing the law does not change. As with other scientific knowledge, they do not have absolute certainty (as mathematical theorems or identities do), and it is always possible for a law to be overturned by future observations. A law can usually be formulated as one or several statements or equations, so that it can be used to predict the outcome of an experiment, given the circumstances of the processes taking place.\n\nLaws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. Hypotheses and postulates are not laws since they have not been verified to the same degree and may not be sufficiently general, although they may lead to the formulation of laws. A law is a more solidified and formal statement, distilled from repeated experiment. Laws are narrower in scope than scientific theories, which may contain one or several laws. Science distinguishes a law or theory from facts. Calling a law a fact is ambiguous, an overstatement, or an equivocation. Although the nature of a scientific law is a question in philosophy and although scientific laws describe nature mathematically, scientific laws are practical conclusions reached by the scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolutes.\n\nAccording to the unity of science thesis, \"all\" scientific laws follow fundamentally from physics. Laws which occur in other sciences ultimately follow from physical laws. Often, from mathematically fundamental viewpoints, universal constants emerge from a scientific law.\n\nA scientific law always applies under the same conditions, and implies that there is a causal relationship involving its elements. Factual and well-confirmed statements like \"Mercury is liquid at standard temperature and pressure\" are considered too specific to qualify as scientific laws. A central problem in the philosophy of science, going back to David Hume, is that of distinguishing causal relationships (such as those implied by laws) from principles that arise due to constant conjunction.\n\nLaws differ from scientific theories in that they do not posit a mechanism or explanation of phenomena: they are merely distillations of the results of repeated observation. As such, a law is limited in applicability to circumstances resembling those already observed, and may be found false when extrapolated. Ohm's law only applies to linear networks, Newton's law of universal gravitation only applies in weak gravitational fields, the early laws of aerodynamics such as Bernoulli's principle do not apply in case of compressible flow such as occurs in transonic and supersonic flight, Hooke's law only applies to strain below the elastic limit, etc. These laws remain useful, but only under the conditions where they apply.\n\nMany laws take mathematical forms, and thus can be stated as an equation; for example, the law of conservation of energy can be written as formula_1, where E is the total amount of energy in the universe. Similarly, the first law of thermodynamics can be written as formula_2.\n\nThe term \"scientific law\" is traditionally associated with the natural sciences, though the social sciences also contain laws. An example of a scientific law in social sciences is Zipf's law.\n\nLike theories and hypotheses, laws make predictions (specifically, they predict that new observations will conform to the law), and can be falsified if they are found in contradiction with new data.\n\nMost significant laws in science are conservation laws. These fundamental laws follow from homogeneity of space, time and phase, in other words \"symmetry\".\n\n\nConservation laws can be expressed using the general continuity equation (for a conserved quantity) can be written in differential form as:\n\nwhere ρ is some quantity per unit volume, J is the flux of that quantity (change in quantity per unit time per unit area). Intuitively, the divergence (denoted ∇•) of a vector field is a measure of flux diverging radially outwards from a point, so the negative is the amount piling up at a point, hence the rate of change of density in a region of space must be the amount of flux leaving or collecting in some region (see main article for details). In the table below, the fluxes, flows for various physical quantities in transport, and their associated continuity equations, are collected for comparison.\n\nMore general equations are the convection–diffusion equation and Boltzmann transport equation, which have their roots in the continuity equation.\n\nAll of classical mechanics, including Newton's laws, Lagrange's equations, Hamilton's equations, etc., can be derived from this very simple principle:\n\nwhere formula_5 is the action; the integral of the Lagrangian\n\nof the physical system between two times \"t\" and \"t\". The kinetic energy of the system is \"T\" (a function of the rate of change of the configuration of the system), and potential energy is \"V\" (a function of the configuration and its rate of change). The configuration of a system which has \"N\" degrees of freedom is defined by generalized coordinates q = (\"q\", \"q\", ... \"q\").\n\nThere are generalized momenta conjugate to these coordinates, p = (\"p\", \"p\", ..., \"p\"), where:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the generalized coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nThe action is a \"functional\" rather than a \"function\", since it depends on the Lagrangian, and the Lagrangian depends on the path q(\"t\"), so the action depends on the \"entire\" \"shape\" of the path for all times (in the time interval from \"t\" to \"t\"). Between two instants of time, there are infinitely many paths, but one for which the action is stationary (to the first order) is the true path. The stationary value for the \"entire continuum\" of Lagrangian values corresponding to some path, \"not just one value\" of the Lagrangian, is required (in other words it is \"not\" as simple as \"differentiating a function and setting it to zero, then solving the equations to find the points of maxima and minima etc\", rather this idea is applied to the entire \"shape\" of the function, see calculus of variations for more details on this procedure).\n\nNotice \"L\" is \"not\" the total energy \"E\" of the system due to the difference, rather than the sum:\n\nThe following general approaches to classical mechanics are summarized below in the order of establishment. They are equivalent formulations, Newton's is very commonly used due to simplicity, but Hamilton's and Lagrange's equations are more general, and their range can extend into other branches of physics with suitable modifications.\n\nFrom the above, any equation of motion in classical mechanics can be derived.\n\n\n\n\nEquations describing fluid flow in various situations can be derived, using the above classical equations of motion and often conservation of mass, energy and momentum. Some elementary examples follow.\n\n\n\nPostulates of special relativity are not \"laws\" in themselves, but assumptions of their nature in terms of \"relative motion\".\n\nOften two are stated as \"the laws of physics are the same in all inertial frames\" and \"the speed of light is constant\". However the second is redundant, since the speed of light is predicted by Maxwell's equations. Essentially there is only one.\n\nThe said posulate leads to the Lorentz transformations – the transformation law between two frame of references moving relative to each other. For any 4-vector\n\nthis replaces the Galilean transformation law from classical mechanics. The Lorentz transformations reduce to the Galilean transformations for low velocities much less than the speed of light \"c\".\n\nThe magnitudes of 4-vectors are invariants - \"not\" \"conserved\", but the same for all inertial frames (i.e. every observer in an inertial frame will agree on the same value), in particular if \"A\" is the four-momentum, the magnitude can derive the famous invariant equation for mass-energy and momentum conservation (see invariant mass):\n\nin which the (more famous) mass-energy equivalence \"E\" = \"mc\" is a special case.\n\n\nGeneral relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass-energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated.\n\n\nIn a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous \"gravitomagnetic field\". They are well established by the theory, and experimental tests form ongoing research.\n\nThese equations can be modified to include magnetic monopoles, and are consistent with our observations of monopoles either existing or not existing; if they do not exist, the generalized equations reduce to the ones above, if they do, the equations become fully symmetric in electric and magnetic charges and currents. Indeed, there is a duality transformation where electric and magnetic charges can be \"rotated into one another\", and still satisfy Maxwell's equations.\n\n\nThese laws were found before the formulation of Maxwell's equations. They are not fundamental, since they can be derived from Maxwell's Equations. Coulomb's Law can be found from Gauss' Law (electrostatic form) and the Biot–Savart Law can be deduced from Ampere's Law (magnetostatic form). Lenz' Law and Faraday's Law can be incorporated into the Maxwell-Faraday equation. Nonetheless they are still very effective for simple calculations.\n\n\n\n\nClassically, optics is based on a variational principle: light travels from one point in space to another in the shortest time.\n\n\nIn geometric optics laws are based on approximations in Euclidean geometry (such as the paraxial approximation).\n\n\nIn physical optics, laws are based on physical properties of materials.\n\n\nIn actuality, optical properties of matter are significantly more complex and require quantum mechanics.\n\nQuantum mechanics has its roots in postulates. This leads to results which are not usually called \"laws\", but hold the same status, in that all of quantum mechanics follows from them.\n\nOne postulate that a particle (or a system of many particles) is described by a wavefunction, and this satisfies a quantum wave equation: namely the Schrödinger equation (which can be written as a non-relativistic wave equation, or a relativistic wave equation). Solving this wave equation predicts the time-evolution of the system's behaviour, analogous to solving Newton's laws in classical mechanics.\n\nOther postulates change the idea of physical observables; using quantum operators; some measurements can't be made at the same instant of time (Uncertainty principles), particles are fundamentally indistinguishable. Another postulate; the wavefunction collapse postulate, counters the usual idea of a measurement in science.\n\nApplying electromagnetism, thermodynamics, and quantum mechanics, to atoms and molecules, some laws of electromagnetic radiation and light are as follows.\n\n\nChemical laws are those laws of nature relevant to chemistry. Historically, observations led to many empirical laws, though now it is known that chemistry has its foundations in quantum mechanics.\n\n\nThe most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics.\n\nAdditional laws of chemistry elaborate on the law of conservation of mass. Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important.\n\nDalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers (i.e. 1:2 for Oxygen:Hydrogen ratio in water); although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction.\n\nMore modern laws of chemistry define the relationship between energy and its transformations.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "25512250", "url": "https://en.wikipedia.org/wiki?curid=25512250", "title": "Truth table", "text": "Truth table\n\nA truth table is a mathematical table used in logic—specifically in connection with Boolean algebra, boolean functions, and propositional calculus—which sets out the functional values of logical expressions on each of their functional arguments, that is, for each combination of values taken by their logical variables (Enderton, 2001). In particular, truth tables can be used to show whether a propositional expression is true for all legitimate input values, that is, logically valid.\n\nA truth table has one column for each input variable (for example, P and Q), and one final column showing all of the possible results of the logical operation that the table represents (for example, P XOR Q). Each row of the truth table contains one possible configuration of the input variables (for instance, P=true Q=false), and the result of the operation for those values. See the examples below for further clarification. Ludwig Wittgenstein is often credited with inventing the truth table in his \"Tractatus Logico-Philosophicus\", though it appeared at least a year earlier in a paper on propositional logic by Emil Leon Post.\n\nThere are 4 unary operations:\n\nThe output value is always true, regardless of the input value of p\n\nThe output value is never true: that is, always false, regardless of the input value of p\nLogical identity is an operation on one logical value p, for which the output value remains p.\n\nThe truth table for the logical identity operator is as follows:\n\nLogical negation is an operation on one logical value, typically the value of a proposition, that produces a value of \"true\" if its operand is false and a value of \"false\" if its operand is true.\n\nThe truth table for NOT p (also written as ¬p, Np, Fpq, or ~p) is as follows:\n\nThere are 16 possible truth functions of two binary variables:\n\nHere is an extended truth table giving definitions of all possible truth functions of two Boolean variables P and Q:\n\nwhere\n\nThe four combinations of input values for p, q, are read by row from the table above.\nThe output function for each p, q combination, can be read, by row, from the table.\n\nKey:\n\nThe following table is oriented by column, rather than by row. There are four columns rather than four rows, to display the four combinations of p, q, as input. \n\np: T T F F<br>\nq: T F T F\n\nThere are 16 rows in this key, one row for each binary function of the two binary variables, p, q. For example, in row 2 of this Key, the value of Converse nonimplication ('formula_1') is solely T, for the column denoted by the unique combination p=F, q=T; while in row 2, the value of that 'formula_1' operation is F for the three remaining columns of p, q. The output row for formula_1 is thus\n\n2: F F T F\n\nand the 16-row key is\n\nLogical operators can also be visualized using Venn diagrams.\n\nLogical conjunction is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if both of its operands are true.\n\nThe truth table for p AND q (also written as p ∧ q, Kpq, p & q, or p formula_4 q) is as follows:\n\nIn ordinary language terms, if both \"p\" and \"q\" are true, then the conjunction \"p\" ∧ \"q\" is true. For all other assignments of logical values to \"p\" and to \"q\" the conjunction \"p\" ∧ \"q\" is false.\n\nIt can also be said that if \"p\", then \"p\" ∧ \"q\" is \"q\", otherwise \"p\" ∧ \"q\" is \"p\".\n\nLogical disjunction is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if at least one of its operands is true.\n\nThe truth table for p OR q (also written as p ∨ q, Apq, p || q, or p + q) is as follows:\n\nStated in English, if \"p\", then \"p\" ∨ \"q\" is \"p\", otherwise \"p\" ∨ \"q\" is \"q\".\n\nLogical implication and the material conditional are both associated with an operation on two logical values, typically the values of two propositions, which produces a value of \"false\" if the first operand is true and the second operand is false, and a value of \"true\" otherwise.\n\nThe truth table associated with the logical implication p implies q (symbolized as p ⇒ q, or more rarely Cpq) is as follows:\n\nThe truth table associated with the material conditional if p then q (symbolized as p → q) is as follows:\n\nIt may also be useful to note that p ⇒ q and p → q are equivalent to ¬p ∨ q.\n\nLogical equality (also known as biconditional) is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if both operands are false or both operands are true.\n\nThe truth table for p XNOR q (also written as p ↔ q, Epq, p = q, or p ≡ q) is as follows:\n\nSo p EQ q is true if p and q have the same truth value (both true or both false), and false if they have different truth values.\n\nExclusive disjunction is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if one but not both of its operands is true.\n\nThe truth table for p XOR q (also written as p ⊕ q, Jpq, p ≠ q, or p ↮ q) is as follows:\n\nFor two propositions, XOR can also be written as (p ∧ ¬q) ∨ (¬p ∧ q).\n\nThe logical NAND is an operation on two logical values, typically the values of two propositions, that produces a value of \"false\" if both of its operands are true. In other words, it produces a value of \"true\" if at least one of its operands is false.\n\nThe truth table for p NAND q (also written as p ↑ q, Dpq, or p | q) is as follows:\n\nIt is frequently useful to express a logical operation as a compound operation, that is, as an operation that is built up or composed from other operations. Many such compositions are possible, depending on the operations that are taken as basic or \"primitive\" and the operations that are taken as composite or \"derivative\".\n\nIn the case of logical NAND, it is clearly expressible as a compound of NOT and AND.\n\nThe negation of a conjunction: ¬(\"p\" ∧ \"q\"), and the disjunction of negations: (¬\"p\") ∨ (¬\"q\") can be tabulated as follows:\n\nThe logical NOR is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if both of its operands are false. In other words, it produces a value of \"false\" if at least one of its operands is true. ↓ is also known as the Peirce arrow after its inventor, Charles Sanders Peirce, and is a Sole sufficient operator.\n\nThe truth table for p NOR q (also written as p ↓ q, or Xpq) is as follows:\n\nThe negation of a disjunction ¬(\"p\" ∨ \"q\"), and the conjunction of negations (¬\"p\") ∧ (¬\"q\") can be tabulated as follows:\n\nInspection of the tabular derivations for NAND and NOR, under each assignment of logical values to the functional arguments \"p\" and \"q\", produces the identical patterns of functional values for ¬(\"p\" ∧ \"q\") as for (¬\"p\") ∨ (¬\"q\"), and for ¬(\"p\" ∨ \"q\") as for (¬\"p\") ∧ (¬\"q\"). Thus the first and second expressions in each pair are logically equivalent, and may be substituted for each other in all contexts that pertain solely to their logical values.\n\nThis equivalence is one of De Morgan's laws.\n\nTruth tables can be used to prove many other logical equivalences. For example, consider the following truth table:\n\nThis demonstrates the fact that formula_5 is logically equivalent to formula_6.\n\nHere is a truth table that gives definitions of the 6 most commonly used out of the 16 possible truth functions of two Boolean variables P and Q:\n\nwhere\n\nFor binary operators, a condensed form of truth table is also used, where the row headings and the column headings specify the operands and the table cells specify the result. For example, Boolean logic uses this condensed truth table notation:\n\nThis notation is useful especially if the operations are commutative, although one can additionally specify that the rows are the first operand and the columns are the second operand. This condensed notation is particularly useful in discussing multi-valued extensions of logic, as it significantly cuts down on combinatoric explosion of the number of rows otherwise needed. It also provides for quickly recognizable characteristic \"shape\" of the distribution of the values in the table which can assist the reader in grasping the rules more quickly.\n\nTruth tables are also used to specify the function of hardware look-up tables (LUTs) in digital logic circuitry. For an n-input LUT, the truth table will have 2^\"n\" values (or rows in the above tabular format), completely specifying a boolean function for the LUT. By representing each boolean value as a bit in a binary number, truth table values can be efficiently encoded as integer values in electronic design automation (EDA) software. For example, a 32-bit integer can encode the truth table for a LUT with up to 5 inputs.\n\nWhen using an integer representation of a truth table, the output value of the LUT can be obtained by calculating a bit index \"k\" based on the input values of the LUT, in which case the LUT's output value is the \"k\"th bit of the integer. For example, to evaluate the output value of a LUT given an array of \"n\" boolean input values, the bit index of the truth table's output value can be computed as follows: if the \"i\"th input is true, let formula_14, else let formula_15. Then the \"k\"th bit of the binary representation of the truth table is the LUT's output value, where formula_16.\n\nTruth tables are a simple and straightforward way to encode boolean functions, however given the exponential growth in size as the number of inputs increase, they are not suitable for functions with a large number of inputs. Other representations which are more memory efficient are text equations and binary decision diagrams.\n\nIn digital electronics and computer science (fields of applied logic engineering and mathematics), truth tables can be used to reduce basic boolean operations to simple correlations of inputs to outputs, without the use of logic gates or code. For example, a binary addition can be represented with the truth table:\n\nThis truth table is read left to right:\n\nNote that this table does not describe the logic operations necessary to implement this operation, rather it simply specifies the function of inputs to output values.\n\nWith respect to the result, this example may be arithmetically viewed as modulo 2 binary addition, and as logically equivalent to the exclusive-or (exclusive disjunction) binary logic operation.\n\nIn this case it can be used for only very simple inputs and outputs, such as 1s and 0s. However, if the number of types of values one can have on the inputs increases, the size of the truth table will increase.\n\nFor instance, in an addition operation, one needs two operands, A and B. Each can have one of two values, zero or one. The number of combinations of these two values is 2×2, or four. So the result is four possible outputs of C and R. If one were to use base 3, the size would increase to 3×3, or nine possible outputs.\n\nThe first \"addition\" example above is called a half-adder. A full-adder is when the carry from the previous operation is provided as input to the next adder. Thus, a truth table of eight rows would be needed to describe a full adder's logic:\n\nIrving Anellis has done the research to show that C.S. Peirce appears to be the earliest logician (in 1893) to devise a truth table matrix. From the summary of his paper:\n\n\n"}
{"id": "449568", "url": "https://en.wikipedia.org/wiki?curid=449568", "title": "−1", "text": "−1\n\nIn mathematics, −1 is the additive inverse of 1, that is, the number that when added to 1 gives the additive identity element, 0. It is the negative integer greater than negative two (−2) and less than 0.\n\nNegative one bears relation to Euler's identity since \"e\" = −1.\n\nIn software development, −1 is a common initial value for integers and is also used to show that a variable contains no useful information.\n\nIn programming languages, −1 can be used to index the last (or 2nd last) item of an array, depending on whether 0 or 1 represents the first item.\n\nNegative one has some similar but slightly different properties to positive one.\n\nMultiplying a number by −1 is equivalent to changing the sign on the number. This can be proved using the distributive law and the axiom that 1 is the multiplicative identity: for \"x\" real, we have\n\nwhere we used the fact that any real \"x\" times 0 equals 0, implied by cancellation from the equation\n\nIn other words,\n\nso (−1) · \"x\", or −\"x\", is the arithmetic inverse of \"x\".\n\nThe square of −1, i.e. −1 multiplied by −1, equals 1. As a consequence, a product of two negative real numbers is positive.\n\nFor an algebraic proof of this result, start with the equation\n\nThe first equality follows from the above result. The second follows from the definition of −1 as additive inverse of 1: it is precisely that number that when added to 1 gives 0. Now, using the distributive law, we see that\n\nThe second equality follows from the fact that 1 is a multiplicative identity. But now adding 1 to both sides of this last equation implies\n\nThe above arguments hold in any ring, a concept of abstract algebra generalizing integers and real numbers.\n\nAlthough there are no real square roots of -1, the complex number \"i\" satisfies \"i\" = −1, and as such can be considered as a square root of −1. The only other complex number who's square is 1 is −\"i\". In the algebra of quaternions, which contain the complex plane, the equation \"x\" = −1 has infinite solutions.\n\nExponentiation of a non-zero real number can be extended to negative integers. We make the definition that \"x\" = , meaning that we define raising a number to the power −1 to have the same effect as taking its reciprocal. This definition is then extended to negative integers preserves the exponential law \"x\"\"x\" = \"x\" for real numbers \"a\" and \"b\".\n\nExponentiation to negative integers can be extended to invertible elements of a ring, by defining \"x\" as the multiplicative inverse of \"x\".\n\n−1 that appears next to functions or matrices does not mean raising them to the power −1 but their inverse functions or inverse matrices. For example, \"f\"(\"x\") is the inverse of \"f\"(\"x\"), or sin(\"x\") is a notation of arcsine function.\n\nMost computer systems represent negative integers using two's complement. In such systems, −1 is represented using a bit pattern of all ones. For example, an 8-bit signed integer using two's complement would represent −1 as the bitstring \"11111111\", or \"FF\" in hexadecimal (base 16). If interpreted as an unsigned integer, the same bitstring of \"n\" ones represents 2 − 1, the largest possible value that \"n\" bits can hold. For example, the 8-bit string \"11111111\" above represents 2 − 1 = 255.\n\nIn some programming languages, when used to index some data types (such as an array), then -1 can be used to identify the very last (or 2nd last) item, depending on whether 0 or 1 represents the first item.\nIf the first item is indexed by 0, then -1 identifies the last item.\nIf the first item is indexed by 1, then -1 identifies the second-to-last item.\n"}
