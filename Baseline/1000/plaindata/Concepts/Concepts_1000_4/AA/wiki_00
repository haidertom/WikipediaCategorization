{"id": "34460605", "url": "https://en.wikipedia.org/wiki?curid=34460605", "title": "Andersen healthcare utilization model", "text": "Andersen healthcare utilization model\n\nThe Andersen Healthcare Utilization Model - is a conceptual model aimed at demonstrating the factors that lead to the use of health services. According to the model, usage of health services (including inpatient care, physician visits, dental care etc.) is determined by three dynamics: predisposing factors, enabling factors, and need. Predisposing factors can be characteristics such as race, age, and health beliefs. For instance, an individual who believes health services are an effective treatment for an ailment is more likely to seek care. Examples of enabling factors could be family support, access to health insurance, one's community etc. Need represents both perceived and actual need for health care services. The original model was developed by Ronald M. Andersen, a health services professor at UCLA, in 1968. The original model was expanded through numerous iterations and its most recent form models past the use of services to end at health outcomes and includes feedback loops.\n\nA major motivation for the development of the model was to offer measures of access. Andersen discusses four concepts within access that can be viewed through the conceptual framework. Potential access is the presence of enabling resources, allowing the individual to seek care if needed. Realized access is the actual use of care, shown as the outcome of interest in the earlier models. The Andersen framework also makes a distinction between equitable and inequitable access. Equitable access is driven by demographic characteristics and need whereas inequitable access is a result of social structure, health beliefs, and enabling resources.\n\nAndersen also introduces the concept of mutability of his factors. The idea here being that if a concept has a high degree of mutability (can be easily changed) perhaps policy would be justified in using its resources to do rather than a factor with low mutability. Characteristics that fall under demographics are quite difficult to change, however, enabling resources is assigned a high degree of mutability as the individual, community, or national policy can take steps to alter the level of enabling resources for an individual. For example, if the government decides to expand the Medicaid program an individual may experience an increase in enabling resources, which in turn may beget an increase in health services usage. The RAND Health Insurance Experiment (HIE) changed a highly mutable factor, out-of-pocket costs, which greatly changed individual rates of health services usage.\n\nThe initial behavior model was an attempt to study of why a family uses health services. However, due to the heterogeneity of family members the model focused on the individual rather than the family as the unit of analysis. Andersen also states that the model functions both to predict and explain use of health services.\n\nA second model was developed in the 1970s in conjunction with Aday and colleagues at the University of Chicago. This iteration includes systematic concepts of health care such as current policy, resources, and organization. The second generation model also extends the outcome of interest beyond utilization to consumer satisfaction.\n\nThe next generation of the model builds upon this idea by including health status (both perceived and evaluated) as outcomes alongside consumer satisfaction. Furthermore, this model include personal health practices as an antecedent to outcomes, acknowledging that it not solely use of health services that drives health and satisfaction. This model emphasizes a more public health approach of prevention, as advocated by Evans and Stoddart wherein personal health practices (i.e. smoking, diet, exercise) are included as a driving force towards health outcomes.\n\nThe 6th iteration of Andersen’s conceptual framework focuses on the individual as the unit of analysis and goes beyond health care utilization, adopting health outcomes as the endpoint of interest. This model is further differentiated from its predecessors by using a feedback loop to illustrate that health outcomes may affect aspects such as health beliefs, and need. It added genetic susceptibility as a predisposing determinant and quality of life as an outcome By using the framework’s relationships we can determine the directionality of the effect following a change in an individual’s characteristics or environment. For example, if one experiences an increase in need as a result of an infection, the Andersen model predicts this will lead to an increased use of services (all else equal). One potential change for a future iteration of this model is to add genetic information under predisposing characteristics. As genetic information becomes more readily available it seems likely this could impact health services usage, as well as health outcomes, beyond what is already accounted for in the current model.\n\nThe model has been criticized for not paying enough attention to culture and social interaction but Andersen argues this social structure is included in the \"predisposing characteristics\" component. Another criticism was the overemphasis of need and at the expense of health beliefs and social structure. However, Andersen argues need itself is a social construct. This is why need is split into perceived and evaluated. Where evaluated need represents a more measurable/objective need, perceived need is partly determined by health beliefs, such as whether or not people think their condition is serious enough to seek health services. Another limitation of the model is its emphasis on health care utilization or adopting health outcomes as a dichotomous factor, present or not present. Other help-seeking models also consider the type of help source, including informal sources. More recent work has taken help-seeking behaviors further, and more real-world, by including online and other non-face-to-face sources.\n"}
{"id": "66975", "url": "https://en.wikipedia.org/wiki?curid=66975", "title": "Apophatic theology", "text": "Apophatic theology\n\nApophatic theology, also known as negative theology, is a form of theological thinking and religious practice which attempts to approach God, the Divine, by negation, to speak only in terms of what may not be said about the perfect goodness that is God. It forms a pair together with cataphatic theology, which approaches God or the Divine by affirmations or positive statements about what God \"is\".\n\nThe apophatic tradition is often, though not always, allied with the approach of mysticism, which aims at the vision of God, the perception of the divine reality beyond the realm of ordinary perception.\n\n\"Apophatic\", (adjective); from ἀπόφημι \"apophēmi\", meaning \"to deny\". From \"Online Etymology Dictionary\": \n\"Via negativa\" or \"via negationis\" (Latin), \"negative way\" or \"by way of denial\". The negative way forms a pair together with the \"kataphatic\" or positive way. According to Deirdre Carabine,\nAccording to Fagenblat, \"negative theology is as old as philosophy itself;\" elements of it can be found in Plato's \"unwritten doctrines,\" while it is also present in Neo-Platonic, Gnostic and early Christian writers. A tendency to apophatic thought can also be found in Philo of Alexandria.\n\nAccording to Carabine, \"apophasis proper\" in Greek thought starts with Neo-Platonism, with its speculations about the nature of the One, culminating in the works of Proclus. According to Carabine, there are two major points in the development of apophatic theology, namely the fusion of the Jewish tradition with Platonic philosophy in the writings of Philo, and the works of Dionysius the Pseudo-Areopagite, who infused Christian thought with Neo-Platonic ideas.\n\nThe Early Church Fathers were influenced by Philo, and Meredith even states that Philo \"is the real founder of the apophatic tradition.\" Yet, it was with Pseudo-Dionysius the Areopagite and Maximus the Confessor, whose writings shaped both Hesychasm, the contemplative tradition of the Eastern Orthodox Churches, and the mystical traditions of western Europe, that apophatic theology became a central element of Christian theology and contemplative practice.\n\nFor the ancient Greeks, knowledge of the gods was essential for proper worship. Poets had an important responsibility in this regard, and a central question was how knowledge of the Divine forms can be attained. Epiphany played an essential role in attaining this knowledge. Xenophanes (c. 570 – c. 475 BC) noted that the knowledge of the Divine forms is restrained by the human imagination, and Greek philosophers realized that this knowledge can only be mediated through myth and visual representations, which are culture-dependent.\n\nAccording to Herodotus (484–425 BCE), Homer and Hesiod (between 750 and 650 BC) taught the Greek the knowledge of the Divine bodies of the Gods. The ancient Greek poet Hesiod (between 750 and 650 BC) describes in his \"Theogony\" the birth of the gods and creation of the world, which became an \"ur-text for programmatic, first-person epiphanic narratives in Greek literature,\" but also \"explores the necessary limitations placed on human access to the divine.\" According to Platt, the statement of the Muses who grant Hesiod knowledge of the Gods \"actually accords better with the logic of apophatic religious thought.\"\n\nParmenides (fl. late sixth or early fifth century BC), in his poem \"On Nature\", gives an account of a revelation on two ways of inquiry. \"The way of conviction\" explores Being, true reality (\"what-is\"), which is \"What is ungenerated and deathless,/whole and uniform, and still and perfect.\" \"The way of opinion\" is the world of appearances, in which one's sensory faculties lead to conceptions which are false and deceitful. His distinction between unchanging Truth and shifting opinion is reflected in Plato's allegory of the Cave. Together with the Biblical story of Moses's ascent of Mount Sinai, it is used by Gregory of Nyssa and Pseudo-Dionysius the Areopagite to give a Christian account of the ascent of the soul toward God. Cook notes that Parmenides poem is a religious account of a mystical journey, akin to the mystery cults, giving a philosophical form to a religious outlook. Cook further notes that the philosopher's task is to \"attempt through 'negative' thinking to tear themselves loose from all that frustrates their pursuit of wisdom.\"\n\nPlato (428/427 or 424/423 – 348/347 BCE), \"deciding for Parmenides against Heraclitus\" and his theory of eternal change, had a strong influence on the development of apophatic thought.\n\nPlato further explored Parmenides's idea of timeless truth in his dialogue \"Parmenides\", which is a treatment of the eternal forms, \"Truth, Beauty and Goodness\", which are the real aims for knowledge. The Theory of Forms is Plato's answer to the problem \"how one unchanging reality or essential being can admit of many changing phenomena (and not just by dismissing them as being mere illusion).\"\n\nIn \"The Republic\", Plato argues that the \"real objects of knowledge are not the changing objects of the senses, but the immutable Forms,\" stating that the \"Form of the Good\" is the highest object of knowledge. His argument culminates in the Allegory of the Cave, in which he argues that humans are like prisoners in a cave, who can only see shadows of the Real, the \"Form of the Good\". Humans are to be educated to search for knowledge, by turning away from their bodily desires toward higher contemplation, culminating in an intellectual understanding or apprehension of the Forms, c.q. the \"first principles of all knowledge.\"\n\nAccording to Cook, the \"Theory of Forms\" has a theological flavour, and had a strong influence on the ideas of his Neo-Platonist interpreters Proclus and Plotinus. The pursuit of \"Truth, Beauty and Goodness\" became a central element in the apophatic tradition, but nevertheless, according to Carabine \"Plato himself cannot be regarded as the founder of the negative way.\" Carabine warns not to read later Neo-Platonic and Christian understandings into Plato, and notes that Plato did not identify his Forms with \"one transcendent source,\" an identification which his later interpreters made.\n\nMiddle Platonism (1st century BCE - 3rd century CE) further investigated Plato's \"Unwritten Doctrines,\" which drew on Pythagoras' first principles of the Monad and the Dyad (matter). Middle Platonism proposed a hierarchy of being, with God as its first principle at its top, identifying it with Plato's \"Form of the Good\". An influential proponent of Middle Platonism was Philo (c.25 BCE–c. 50 CE), who employed Middle Platonic philosophy in his interpretation of the Hebrew scriptures, and asserted a strong influence on early Christianity. According to Craig D. Allert, \"Philo made a monumental contribution to the creation of a vocabulary for use in negative statements about God.\" For Philo, God is undescribable, and he uses terms which emphasize God's transcendence.\n\nNeo-Platonism was a mystical or contemplative form of Platonism, which \"developed outside the mainstream of Academic Platonism.\" It started with the writings of Plotinus (204/5–270), and ended with the closing of the Platonic Academy by Emperor Justinian in 529 CE, when the pagan traditions were ousted. It is a product of Hellenistic syncretism, which developed due to the crossover between Greek thought and the Jewish scriptures, and also gave birth to Gnosticism. Proclus was the last head of the Platonic Academy; his student Pseudo-Dinosysius had a far-stretching Neo-Platonic influence on Christianity and Christian mysticism.\n\nPlotinus (204/5–270) was the founder of Neo-Platonism. In the Neo-Platonic philosophy of Plotinus and Proclus, the first principle became even more elevated as a radical unity, which was presented as an unknowable Absolute. For Plotinus, \"the One\" is the first principle, from which everything else emanates. He took it from Plato's writings, identifying the Good of the \"Republic\", as the cause of the other Forms, with \"the One\" of the first hypothesis of the second part of the \"Parmenides\". For Plotinus, \"the One\" precedes the Forms, and \"is beyond Mind and indeed beyond Being.\" From \"the One\" comes the Intellect, which contains all the Forms. \"The One\" is the principle of Being, while the Forms are the principle of the essence of beings, and the intelligibility which can recognize them as such. Plotinus's third principle is Soul, the desire for objects external to the person. The highest satisfaction of desire is the contemplation of \"the One\", which unites all existents \"as a single, all-pervasive reality.\"\n\n\"The One\" is radically simple, and does not even have self-knowledge, since self-knowledge would imply multiplicity. Nevertheless, Plotinus does urge for a search for the Absolute, turning inward and becoming aware of the \"presence of the intellect in the human soul,\" initiating an ascent of the soul by abstraction or \"taking away,\" culminating in a sudden appearance of \"the One\". In the \"Enneads\" Plotinus writes: \nCarabine notes that Plotinus' apophasis is not just a mental exercise, an acknowledgement of the unknowability of \"the One\", but a means to \"extasis\" and an ascent to \"the unapproachable light that is God.\" Pao-Shen Ho, investigating what are Plotinus' methods for reaching \"henosis\", concludes that \"Plotinus' mystical teaching is made up of two practices only, namely philosophy and negative theology.\" According to Moore, Plotinus appeals to the \"non-discursive, intuitive faculty of the soul,\" by \"calling for a sort of prayer, an invocation of the deity, that will permit the soul to lift itself up to the unmediated, direct, and intimate contemplation of that which exceeds it (V.1.6).\" Pao-Shen Ho further notes that \"for Plotinus, mystical experience is irreducible to philosophical arguments.\" The argumentation about \"henosis\" is preceded by the actual experience of it, and can only be understood when \"henosis\" has been attained. Ho further notes that Plotinus's writings have a didactic flavour, aiming to \"bring his own soul and \"the souls of others\" by way of Intellect to union with the One.\" As such, the \"Enneads\" as a spiritual or ascetic teaching device, akin to \"The Cloud of Unknowing\", demonstrating the methods of philosophical and apophatic inquiry. Ultimately, this leads to silence and the abandonment of all intellectual inquiry, leaving contemplation and unity.\n\nProclus (412-485) introduced the terminology which is being used in apophatic and cataphatic theology. He did this in the second book of his \"Platonic Theology\", arguing that Plato states that \"the One\" can be revealed \"through analogy,\" and that \"through negations [\"dia ton apophaseon\"] its transcendence over everything can be shown.\" For Proclus, apophatic and cataphonic theology form a contemplatory pair, with the apophatic approach corresponding to the manifestation of the world from \"the One\", and cataphonic theology corresponding to the return to \"the One\". The analogies are affirmations which direct us toward \"the One\", while the negations underlie the confirmations, being closer to \"the One\". According to Luz, Proclus also attracted students from other faiths, including the Samaritan Marinus. Luz notes that \"Marinus' Samaritan origins with its Abrahamic notion of a single ineffable Name of God () should also have been in many ways compatible with the school's ineffable and apophatic divine principle.\"\nThe Book of Revelation 8:1 mentions \"the silence of the perpetual choir in heaven.\" According to Dan Merkur,\nThe Early Church Fathers were influenced by Philo (c. 25 BCE – c. 50 CE), who saw Moses as \"the model of human virtue and Sinai as the archetype of man's ascent into the \"luminous darkness\" of God.\" His interpretation of Moses was followed by Clement of Alexandria, Origen, the Cappadocian Fathers, Pseudo-Dionysius, and Maximus the Confessor.\n\nGod's appearance to Moses in the burning bush was often elaborated on by the Early Church Fathers, especially Gregory of Nyssa (c. 335 – c. 395), realizing the fundamental unknowability of God; an exegesis which continued in the medieval mystical tradition. Their response is that, although God is unknowable, Jesus as person can be followed, since \"following Christ is the human way of seeing God.\"\n\nClement of Alexandria (c. 150 – c. 215) was an early proponent of apophatic theology. According to R.A. Baker, in Clement's writings the term \"theoria\" develops further from a mere intellectual \"seeing\" toward a spirutal form of contemplation. Clement's apophatic theology or philosophy is closely related to this kind of \"theoria\" and the \"mystic vision of the soul.\" For Clement, God is transcendent and immanent. According to Baker, Clement's apophaticism is mainly driven by Biblical texts, but by the Platonic tradition. His conception of an ineffable God is a synthesis of Plato and Philo, as seen from a Biblical perspective. According to Osborne, it is a synthesis in a Biblical framework; according to Baker, while the Platonic tradition accounts for the negative approach, the Biblical tradition accounts for the positive approach. \"Theoria\" and abstraction is the means to conceive of this ineffable God; it is preceded by dispassion.\n\nAccording to Tertullian (c. 155 – c. 240),\nSaint Cyril of Jerusalem (313-386), in his \"Catechetical Homilies\", states: \nAugustine of Hippo (354-430) defined God \"aliud, aliud valde\", meaning \"other, completely other\", in \"Confessions\" 7.10.16.\n\nApophatic theology found its most influential expression in the works of Pseudo-Dionysius the Areopagite (late 5th to early 6th century), a student of Proclus (412-485), combining a Christian worldview with Neo-Platonic ideas. He is a constant factor in the contemplative tradition of the eastern Orthodox Churches, and from the 9th century onwards his writings also had a strong impact on western mysticism.\n\nDionysius the Areopagite was a pseudonym, taken from Acts of the Apostles chapter 17, in which Paul gives a missionary speech to the court of the Areopagus in Athens. In Paul makes a reference to an altar-inscription, dedicated to the Unknown God, \"a safety measure honoring foreign gods still unknown to the Hellenistic world.\" For Paul, Jesus Christ is this unknown God, and as a result of Paul's speech Dionysius the Areopagite converts to Christianity. Yet, according to Stang, for Pseudo-Dionysius the Areopagite Athens is also the place of Neo-Platonic wisdom, and the term \"unknown God\" is a reversal of Paul's preaching toward an integration of Christianity with Neo-Platonism, and the union with the \"unknown God.\"\n\nAccording to Corrigan and Harrington, \"Dionysius' central concern is how a triune God, ... who is utterly unknowable, unrestricted being, beyond individual substances, beyond even goodness, can become manifest to, in, and through the whole of creation in order to bring back all things to the hidden darkness of their source.\" Drawing on Neo-Platonism, Pseudo-Dionysius described humans ascend to divinity as a process of purgation, illumination and union. Another Neo-Platonic influence was his description of the cosmos as a series of hierarchies, which overcome the distance between God and humans.\n\nIn Orthodox Christianity apophatic theology is taught as superior to cataphatic theology. The fourth-century Cappadocian Fathers stated a belief in the existence of God, but an existence unlike that of everything else: everything else that exists was created, but the Creator transcends this existence, is uncreated. The essence of God is completely unknowable; mankind can know God only through His energies. Gregory of Nyssa (c.335-c.395), John Chrysostom (c. 349 – 407), and Basil the Great (329-379) emphasized the importance of negative theology to an orthodox understanding of God. John of Damascus (c.675/676–749) employed negative theology when he wrote that positive statements about God reveal \"not the nature, but the things around the nature.\"\n\nMaximus the Confessor (580-622) took over Pseudo-Dionysius' ideas, and had a strong influence on the theology and contemplative practices of the Eastern Orthodox Churches. Gregory Palamas (1296–1359) formulated the definite theology of Hesychasm, the Orthodox practices of contemplative prayer and theosis, \"deification.\"\n\nInfluential modern Eastern Orthodox theologians are Vladimir Lossky, John Meyendorff, John S. Romanides and Georges Florovsky. Lossky argues, based on his reading of Dionysius and Maximus Confessor, that positive theology is always inferior to negative theology which is a step along the way to the superior knowledge attained by negation. This is expressed in the idea that mysticism is the expression of dogmatic theology \"par excellence\".\n\nAccording to Lossky, outside of directly revealed knowledge through Scripture and Sacred Tradition, such as the Trinitarian nature of God, God in His essence is beyond the limits of what human beings (or even angels) can understand. He is transcendent in essence (\"ousia\"). Further knowledge must be sought in a direct experience of God or His indestructible energies through \"theoria\" (vision of God). According to Aristotle Papanikolaou, in Eastern Christianity, God is immanent in his hypostasis or existences.\n\nNegative theology has a place in the Western Christian tradition as well. The 9th-century theologian John Scotus Erigena wrote: \n\nWhen he says \"\"He is not anything\" and \"God is not\"\", Scotus does not mean that there is no God, but that God cannot be said to exist in the way that creation exists, i.e. that God is uncreated. He is using apophatic language to emphasise that God is \"other\".\n\nTheologians like Meister Eckhart and Saint John of the Cross (San Juan de la Cruz) exemplify some aspects of or tendencies towards the apophatic tradition in the West. The medieval work, \"The Cloud of Unknowing\" and Saint John's \"Dark Night of the Soul\" are particularly well known. In 1215 apophatism became the official position of the Catholic Church, which, on the basis of Scripture and church tradition, during the Fourth Lateran Council formulated the following dogma:\nThomas Aquinas was born ten years later (1225-1274) and, although in his \"Summa Theologica\" he quotes Pseudo-Dionysius 1,760 times, his reading in a neo-Aristotelian key of the conciliar declaration overthrew its meaning inaugurating the \"analogical way\" as \"tertium\" between \"via negativa\" and \"via positiva\": the \"via eminentiae\" (see also \"analogia entis\"). According to Adrian Langdon,\nAccording to \"Catholic Encyclopedia\", the \"Doctor Angelicus\" and the scholastici declare [that] \nSince then Thomism has played a decisive role in resizing the negative or apophatic tradition of the magisterium.\n\nApophatic statements are still crucial to many modern theologians, restarting in 1800s by Søren Kierkegaard (see his concept of the infinite qualitative distinction) up to Rudolf Otto and Karl Barth (see their idea of \"Wholly Other\", i.e. \"ganz Andere\" or \"totaliter aliter\").\n\nC. S. Lewis, in his book \"Miracles\" (1947), advocates the use of negative theology when first thinking about God, in order to cleanse our minds of misconceptions. He goes on to say we must then refill our minds with the truth about God, untainted by mythology, bad analogies or false mind-pictures.\n\nThe mid-20th century Dutch philosopher Herman Dooyeweerd, who is often associated with a neo-Calvinistic tradition, provides a philosophical foundation for understanding why we can never absolutely know God, and yet, paradoxically, truly know something of God. Dooyeweerd made a sharp distinction between theoretical and pre-theoretical attitudes of thought. Most of the discussion of knowledge of God presupposes theoretical knowledge, in which we reflect and try to define and discuss. Theoretical knowing, by its very nature, is never absolute, always depends on religious presuppositions, and cannot grasp either God or the law side. Pre-theoretical knowing, on the other hand, is intimate engagement, and exhibits a diverse range of aspects. Pre-theoretical intuition, on the other hand, can grasp at least the law side. Knowledge of God, as God wishes to reveal it, is pre-theoretical, immediate and intuitive, never theoretical in nature. The philosopher Leo Strauss considered that the Bible, for example, should be treated as pre-theoretical (everyday) rather than theoretical in what it contains.\n\nIvan Illich (1926-2002), the historian and social critic, can be read as an apophatic theologian, according to a longtime collaborator, Lee Hoinacki, in a paper presented in memory of Illich, called \"Why Philia?\"\n\nAccording to Deirdre Carabine, negative theology has become a hot topic since the 1990s, resulting from a broad effort in the 19 and 20th century to portray Plato as a mysticist, which revived the interest in Neoplatonism and negative theology.\n\nKaren Armstrong, in her book \"The Case for God\" (2009), notices a recovery of apophatic theology in postmodern theology.\n\nThe Arabic term for \"negative theology\" is \"lahoot salbi\", which is a \"system of theology\" or \"nizaam al lahoot\" in Arabic. Different traditions/doctrine schools in Islam called Kalam schools (see Islamic schools and branches) use different theological approaches or \"nizaam al lahoot\" in approaching God in Islam (\"Allah\", Arabic الله) or the ultimate reality. The \"lahoot salbi\" or \"negative theology\" involves the use of \"ta'til\", which means \"negation,\" and the followers of the Mu'tazili school of Kalam, founded by Imam Wasil ibn Ata, are often called the \"Mu'attili\", because they are frequent users of the \"ta'tili\" methodology.\n\nRajab ʿAlī Tabrīzī, an Iranian and Shiat philosopher and mystic of the 17th century. instilled a radical apophatic theology in a generation of philosophers and theologians whose influence extended into the Qajar period. Mulla Rajab affirmed the completely unknowable,\nunqualifiable, and attributeless nature of God and upheld a general view concerning God’s attributes which can only be negatively ‘affirmed’, by means of the via\nnegativa.\n\nShia Islam adopted \"negative theology\". In the words of the Persian Ismaili missionary, Abu Yaqub al-Sijistani: \"There does not exist a tanzíh [\"transcendence\"] more brilliant and more splendid than that by which we establish the absolute transcendence of our Originator through the use of these phrases in which a negative and a negative of a negative apply to the thing denied.\" Early Sunni scholars who held to a literal reading of the Quran and hadith rejected this view, adhering to its opposite, believing that the Attributes of God such as \"Hand\", \"Foot\" etc... should be taken literally and that, therefore, God is like a human being. Today, most Sunnis, like the Ash'ari and Maturidi, adhere to a middle path between negation and anthropomorphism.\n\nMaimonides (1135/1138-1204) was \"the most influential medieval Jewish exponent of the \"via negativa\".\" Maimonides, but also Samuel ibn Tibbon, draw on Bahya ibn Paquda, who shows that our inability to describe God is related to the fact of His absolute unity. God, as the entity which is \"truly One\" (האחד האמת), must be free of properties and is thus unlike anything else and indescribable. According to Rabbi Yosef Wineberg, Maimonides stated that \"[God] is knowledge,\" and saw His Essence, Being and knowledge as completely one, \"a perfect unity and not a composite at all.\" Wineberg quotes Maimonides as stating\nIn \"The Guide for the Perplexed\" Maimonides stated:\nAccording to Fagenblat, it is only in the modern period that negative theology really gains importance in Jewish thought. Yeshayahu Leibowitz (1903-1994) was a prominent modern exponent of Jewish negative theology. According to Leibowitz, a person's faith is his commitment to obey God, meaning God's commandments, and this has nothing to do with a person’s image of God. This must be so because Leibowitz thought that God cannot be described, that God's understanding is not man's understanding, and thus all the questions asked of God are out of place.\n\nThere are interesting parallels in Indian thought, which developed largely separate from Western thought. Early Indian philosophical works which have apophatic themes include the Principal Upanishads (800 BCE to the start of common era) and the Brahma Sutras (from 450 BCE and 200 CE). An expression of negative theology is found in the Brihadaranyaka Upanishad, where Brahman is described as \"neti neti\" or \"neither this, nor that\". Further use of apophatic theology is found in the Brahma Sutras, which state:\n\nBuddhist philosophy has also strongly advocated the way of negation, beginning with the Buddha's own theory of anatta (not-atman, not-self) which denies any truly existent and unchanging essence of a person. Madhyamaka is a Buddhist philosophical school founded by Nagarjuna (2nd-3rd century CE), which is based on a fourfold negation of all assertions and concepts and promotes the theory of emptiness (shunyata). Apophatic assertions are also an important feature of Mahayana sutras, especially the prajñaparamita genre. These currents of negative theology are visible in all forms of Buddhism.\n\nApophatic movements in medieval Hindu philosophy are visible in the works of Shankara (8th century), a philosopher of Advaita Vedanta (non-dualism), and Bhartṛhari (5th century), a grammarian. While Shankara holds that the transcendent noumenon, Brahman, is realized by the means of negation of every phenomenon including language, Bhartṛhari theorizes that language has both phenomenal and noumenal dimensions, the latter of which manifests Brahman.\n\nIn Advaita, Brahman is defined as being Nirguna or without qualities. Anything imaginable or conceivable is not deemed to be the ultimate reality. The Taittiriya hymn speaks of Brahman as \"one where the mind does not reach\". Yet the Hindu scriptures often speak of Brahman's positive aspect. For instance, Brahman is often equated with bliss. These contradictory descriptions of Brahman are used to show that the attributes of Brahman are similar to ones experienced by mortals, but not the same. \n\nNegative theology also figures in the Buddhist and Hindu polemics. The arguments go something like this – Is Brahman an object of experience? If so, how do you convey this experience to others who have not had a similar experience? The only way possible is to relate this unique experience to common experiences while explicitly negating their sameness.\n\nEven though the \"via negativa\" essentially rejects theological understanding in and of itself as a path to God, some have sought to make it into an intellectual exercise, by describing God only in terms of what God is not. One problem noted with this approach is that there seems to be no fixed basis on deciding what God is not, unless the Divine is understood as an abstract experience of full aliveness unique to each individual consciousness, and universally, the perfect goodness applicable to the whole field of reality. Apophatic theology is often accused of being a version of atheism or agnosticism, since it cannot say truly that God exists. \"The comparison is crude, however, for conventional atheism treats the existence of God as a predicate that can be denied (“God is nonexistent”), whereas negative theology denies that God has predicates\". \"God or the Divine is\" without being able to attribute qualities about \"what He is\" would be the prerequisite of positive theology in negative theology that distinguishes theism from atheism. \"Negative theology is a complement to, not the enemy of, positive theology\". Since religious experience—or consciousness of the holy or sacred, is not reducible to other kinds of human experience, an abstract understanding of religious experience cannot be used as evidence or proof that religious discourse or praxis can have no meaning or value. In apophatic theology, the negation of theisms in the \"via negativa\" also requires the negation of their correlative atheisms if the dialectical method it employs is to maintain integrity.\n\n\n\n\n\n\n\n\n\n \n\n\n"}
{"id": "28231080", "url": "https://en.wikipedia.org/wiki?curid=28231080", "title": "Archiv für Begriffsgeschichte", "text": "Archiv für Begriffsgeschichte\n\nArchiv für Begriffsgeschichte ('Archive for Conceptual History') is a German peer-reviewed academic journal. It was founded by Erich Rothacker, and is published by Christian Bermes, Ulrich Dierse and Michael Erler. The editor is Annika Hand.\n\nThe journal publishes works on concepts of the history of philosophy and sciences, both from the European and from non-European traditions, on mythological and religious concepts, and on concepts of common parlance which have a characteristic significance for a special era or culture. The journal also embraces articles on revealing metaphors, on problems at translating concepts, as well as on theory and criticism of the method of conceptual history.\n\n\n"}
{"id": "188401", "url": "https://en.wikipedia.org/wiki?curid=188401", "title": "Axiomatic system", "text": "Axiomatic system\n\nIn mathematics, an axiomatic system is any set of axioms from which some or all axioms can be used in conjunction to logically derive theorems. A theory consists of an axiomatic system and all its derived theorems. An axiomatic system that is completely described is a special kind of formal system. A formal theory typically means an axiomatic system, for example formulated within model theory. A formal proof is a complete rendition of a mathematical proof within a formal system.\n\nAn axiomatic system is said to be \"consistent\" if it lacks contradiction, i.e. the ability to derive both a statement and its denial from the system's axioms.\n\nIn an axiomatic system, an axiom is called \"independent\" if it is not a theorem that can be derived from other axioms in the system. A system will be called independent if each of its underlying axioms is independent. Although independence is not a necessary requirement for a system, consistency usually is, but see neutrosophic logic.\n\nAn axiomatic system will be called \"complete\" if for every statement, either itself or its negation is derivable.\n\nBeyond consistency, relative consistency is also the mark of a worthwhile axiom system. This is when the undefined terms of a first axiom system are provided definitions from a second, such that the axioms of the first are theorems of the second.\n\nA good example is the relative consistency of neutral geometry or absolute geometry with respect to the theory of the real number system. Lines and points are undefined terms in absolute geometry, but assigned meanings in the theory of real numbers in a way that is consistent with both axiom systems.\n\nA model for an axiomatic system is a well-defined set, which assigns meaning for the undefined terms presented in the system, in a manner that is correct with the relations defined in the system. The existence of a concrete model proves the consistency of a system. A model is called concrete if the meanings assigned are objects and relations from the real world}, as opposed to an abstract model which is based on other axiomatic systems.\n\nModels can also be used to show the independence of an axiom in the system. By constructing a valid model for a subsystem without a specific axiom, we show that the omitted axiom is independent if its correctness does not necessarily follow from the subsystem.\n\nTwo models are said to be isomorphic if a one-to-one correspondence can be found between their elements, in a manner that preserves their relationship. An axiomatic system for which every model is isomorphic to another is called categorial (sometimes categorical), and the property of categoriality (categoricity) ensures the completeness of a system.\n\nStating definitions and propositions in a way such that each new term can be formally eliminated by the priorly introduced terms requires primitive notions (axioms) to avoid infinite regress. This way of doing mathematics is called the axiomatic method.\n\nA common attitude towards the axiomatic method is logicism. In their book \"Principia Mathematica\", Alfred North Whitehead and Bertrand Russell attempted to show that all mathematical theory could be reduced to some collection of axioms. More generally, the reduction of a body of propositions to a particular collection of axioms underlies the mathematician's research program. This was very prominent in the mathematics of the twentieth century, in particular in subjects based around homological algebra.\n\nThe explication of the particular axioms used in a theory can help to clarify a suitable level of abstraction that the mathematician would like to work with. For example, mathematicians opted that rings need not be commutative, which differed from Emmy Noether's original formulation. Mathematicians decided to consider topological spaces more generally without the separation axiom which Felix Hausdorff originally formulated.\n\nThe Zermelo-Fraenkel axioms, the result of the axiomatic method applied to set theory, allowed the \"proper\" formulation of set-theory problems and helped to avoid the paradoxes of naïve set theory. One such problem was the Continuum hypothesis. Zermelo–Fraenkel set theory with the historically controversial axiom of choice included is commonly abbreviated ZFC, where C stands for choice. Many authors use ZF to refer to the axioms of Zermelo–Fraenkel set theory with the axiom of choice excluded. Today ZFC is the standard form of axiomatic set theory and as such is the most common foundation of mathematics.\n\nMathematical methods developed to some degree of sophistication in ancient Egypt, Babylon, India, and China, apparently without employing the axiomatic method.\n\nEuclid of Alexandria authored the earliest extant axiomatic presentation of Euclidean geometry and number theory. Many axiomatic systems were developed in the nineteenth century, including non-Euclidean geometry, the foundations of real analysis, Cantor's set theory, Frege's work on foundations, and Hilbert's 'new' use of axiomatic method as a research tool. For example, group theory was first put on an axiomatic basis towards the end of that century. Once the axioms were clarified (that inverse elements should be required, for example), the subject could proceed autonomously, without reference to the transformation group origins of those studies.\n\nNot every consistent body of propositions can be captured by a describable collection of axioms. Call a collection of axioms recursive if a computer program can recognize whether a given proposition in the language is an axiom. Gödel's First Incompleteness Theorem then tells us that there are certain consistent bodies of propositions with no recursive axiomatization. Typically, the computer can recognize the axioms and logical rules for deriving theorems, and the computer can recognize whether a proof is valid, but to determine whether a proof exists for a statement is only soluble by \"waiting\" for the proof or disproof to be generated. The result is that one will not know which propositions are theorems and the axiomatic method breaks down. An example of such a body of propositions is the theory of the natural numbers. The Peano Axioms (described below) thus only partially axiomatize this theory.\n\nIn practice, not every proof is traced back to the axioms. At times, it is not clear which collection of axioms a proof appeals to. For example, a number-theoretic statement might be expressible in the language of arithmetic (i.e. the language of the Peano Axioms) and a proof might be given that appeals to topology or complex analysis. It might not be immediately clear whether another proof can be found that derives itself solely from the Peano Axioms.\n\nAny more-or-less arbitrarily chosen system of axioms is the basis of some mathematical theory, but such an arbitrary axiomatic system will not necessarily be free of contradictions, and even if it is, it is not likely to shed light on anything. Philosophers of mathematics sometimes assert that mathematicians choose axioms \"arbitrarily\", but it is possible that although they may appear arbitrary when viewed only from the point of view of the canons of deductive logic, that appearance is due to a limitation on the purposes that deductive logic serves.\n\nThe mathematical system of natural numbers 0, 1, 2, 3, 4, ... is based on an axiomatic system first written down by the mathematician Peano in 1889. He chose the axioms, in the language of a single unary function symbol \"S\" (short for \"successor\"), for the set of natural numbers to be:\n\n\nIn mathematics, axiomatization is the formulation of a system of statements (i.e. axioms) that relate a number of primitive terms in order that a consistent body of propositions may be derived deductively from these statements. Thereafter, the proof of any proposition should be, in principle, traceable back to these axioms.\n\n\n"}
{"id": "31799202", "url": "https://en.wikipedia.org/wiki?curid=31799202", "title": "Comstock–Needham system", "text": "Comstock–Needham system\n\nThe Comstock–Needham system is a naming system for insect wing veins, devised by John Comstock and George Needham in 1898. It was an important step in showing the homology of all insect wings. This system was based on Needham's \"pretracheation theory\" that was later discredited by Frederic Charles Fraser in 1938.\n\nThe Comstock and Needham system attributes different names to the veins on an insect's wing. From the anterior (leading) edge of the wing towards the posterior (rear), the major longitudinal veins are named:\n\nApart from the costal and the anal veins, each vein can be branched, in which case the branches are numbered from anterior to posterior. For example, the two branches of the subcostal vein will be called Sc and Sc.\n\nThe radius typically branches once near the base, producing anteriorly the R and posteriorly the \"radial sector\" Rs. The radial sector may fork twice.\n\nThe media may also fork twice, therefore having four branches reaching the wing margin.\n\nAccording to the Comstock–Needham system, the cubitus forks once, producing the cubital veins Cu and Cu. \nAccording to some other authorities, Cu may fork again, producing the Cu and Cu.\n\nAs there are several anal veins, they are called A1, A2, and so on. They are usually unforked.\n\nCrossveins link the longitudinal veins, and are named accordingly (for example, the medio-cubital crossvein is termed m-cu). Some crossveins have their own name, like the humeral crossvein h and the sectoral crossvein s.\n\nThe cells are named after the vein on the anterior side; for instance, the cell between Sc and R is called Sc.\n\nIn the case where two cells are separated by a crossvein but have the same anterior longitudinal vein, they should have the same name. To avoid this, they are attributed a number. For example, the R cell is divided in two by the radial cross vein: the basal cell is termed \"first R\", and the distal cell \"second R\".\n\nIf a cell is bordered anteriorly by a forking vein, such as R and R, the cell is named after the distal vein, in this case R.\n\n"}
{"id": "38191512", "url": "https://en.wikipedia.org/wiki?curid=38191512", "title": "Concept-driven strategy", "text": "Concept-driven strategy\n\nA concept-driven strategy is a process for formulating strategy that draws on the explanation of how humans inquire provided by linguistic pragmatic philosophy. This argues that thinking starts by selecting (explicitly or implicitly) a set of concepts (frames, patterns, lens, principles, etc.) gained from our past experiences. These are used to reflect on whatever happens, or is done, in the future.\n\nConcept-driven strategy therefore starts from agreeing and enacting a set of strategic concepts (organizing principles) that \"works best\" for an organisation. For example, a hospital might set its strategy as intending to be Caring, World Class, Local, Evidence Based, and Team Based. A University might set its strategy as intending to be Ranked, Problem Solving, Online, Equis, and Offering Pathways. A commercial corporation might set its strategy as intending to be Innovative, Global, Have Visible Supply Chains, Agile and Market Share Dominant. These strategic concepts make up its \"Statement of Intent\" (or Purpose).\n\nMuch of the strategic management literature mutates Peter Drucker's call for corporations to start the strategic management process by producing a statement of purpose, mission and objectives. This has been mutated into a call to start with a vision, mission and objectives statement. There is an alternative approach which focuses on the Statement of Purpose or Intent. Drucker's example for this statement for a commercial corporation was to state that the corporation's purpose was to create customers. That is, it was going to use the concept of 'customer creation' to coordinate and organise the cognition or mindset of those that worked for the organisation. This was why the corporation existed. Having one concept is now thought to be insufficient. George Armitage Miller's modified The Magical Number Seven, Plus or Minus Two and dialectic suggests a handful of concepts under tension would be preferable.\n\nThe Statement of Purpose, Statement of Intent or concept-driven approach to strategy formulation therefore focuses on setting and enacting a set strategic concepts. If a participatory approach is being used these concepts will be acquired through a process of collaboration with stakeholders. Once agreed the strategic concepts can be used to coordinate activities and act as a set of decision making criteria. The set of concepts that make up the Statement of Intent is then used to make sense of an unpredictable future across an organisation in a co-ordinated manner.\n\nLinguistic pragmatism argues that our prior conceptions interpret our perception (sensory inputs). These conceptions are represented by concepts like running, smiling, justice, reasoning and agility. They are patterns of activity, experienced in our past and remembered. They can be named by those with language and so shared.\n\nBagginni explains pragmatic concepts using the classic example of whether the earth is flat or round.\n\nAnother example would be that we can think of the war in Iraqi differently by reflecting off the concepts of oil security, Imperialism, aggressive capitalism, liberation or democracy. \nThe concept-driven approach to strategy formulation involves setting and using a set of linguistic pragmatic concepts.\n\nThe steps to formulating a participatory concept-driven strategy are:\n\n\nConcept-driven strategy is the name given to a number of similar strategic thinking approaches.\n\nGenerally, the term 'concept-driven' is used to encourage a focus on the 'concepts' being used. See Concept learning or Management Concepts.\n\nSome organisations produce a 'statement of intent' with little thought as to the concepts it contains. However, if it is a short list of concepts, high level objectives, principles, priorities or frames, then concept-driven strategy offers a philosophical basis for these statements.\n\nSome organisations produce a 'strategic principles' statement which again is similar to a statement of intent and the same applies about the concepts approach offering a philosophical basis. The term 'strategic priorities' or 'strategic values' are often used in the same way as strategic principles.\n\nThe literature about 'corporate purpose' is also similar to that of strategic intent. Sometimes, purpose refers to present actions and intent to future ones. If purpose is expressed as a set of concepts, then the concepts approach again provides some philosophical basis.\n\nThere is a connection between 'systems thinking' and concept-driven strategy. The Churchman/Ackoff stream of systems thinking was interested in a developing generic system of concepts for thinking about problems. Rather than a generic set of concepts, the concept-driven approach uses whatever concepts stakeholders think work best for the future of their organisation.\n\nThere is a military planning approach called 'concept-led'. The military-like leadership seems to have moved the concepts from being drivers to be leaders. There seems to be very little difference otherwise.\n\nIn turbulent environments, concepts are thought 'more flexible than objectives' (goals, targets) as they provide why certain actions are preferable. The purpose and intent literature likes to distinguish itself from the objectives literature by saying purpose and intent provide the reasons for (why change), the driver for change. Objectives are where you end up. In complex dynamic situations, there may be many acceptable end points, many of which cannot be anticipated by planners. Arguably the only objective is to survive. How is explained in the statement of intent.\n\nPerhaps strangely, there is a connection between 'metaphor', metaphoric criticism, or conceptual metaphor and concept-driven strategy. Pragmatic concepts are not images but most concepts relate to metaphors. For example, to say an organisation is like a machine, with cogs, or like an adaptive organism, is to use the concepts of machine and organism to reflect on organisations. Much of what has been written about the usefulness of metaphors in planning applies to concepts.\n\nThe term 'strategic frames' is not common given the extensive literature on frame analysis but frames and pragmatic concepts seem to be very similar. Amos Tversky defines a frame as a conception of outcomes.\n\nThe system of strategic concepts listed in a statement of intent, purpose, principles, frames or conceptual metaphor are organizing principle(s).\n\nAlso, as Karl Weick explains sensemaking as the process of conceptualising problems, concept-driven strategy might be thought of as a pragmatic means of sensemaking a strategy.\n\n\n"}
{"id": "178942", "url": "https://en.wikipedia.org/wiki?curid=178942", "title": "Conceptual art", "text": "Conceptual art\n\nConceptual art, sometimes simply called conceptualism, is art in which the concept(s) or idea(s) involved in the work take precedence over traditional aesthetic, technical, and material concerns. Some works of conceptual art, sometimes called installations, may be constructed by anyone simply by following a set of written instructions. This method was fundamental to American artist Sol LeWitt's definition of Conceptual art, one of the first to appear in print:\n\nTony Godfrey, author of \"Conceptual Art (Art & Ideas)\" (1998), asserts that conceptual art questions the nature of art, a notion that Joseph Kosuth elevated to a definition of art itself in his seminal, early manifesto of conceptual art, \"Art after Philosophy\" (1969). The notion that art should examine its own nature was already a potent aspect of the influential art critic Clement Greenberg's vision of Modern art during the 1950s. With the emergence of an exclusively language-based art in the 1960s, however, conceptual artists such as Art & Language, Joseph Kosuth (who became the american editor of Art-Language), and Lawrence Weiner began a far more radical interrogation of art than was previously possible (see below). One of the first and most important things they questioned was the common assumption that the role of the artist was to create special kinds of material objects.\nThrough its association with the Young British Artists and the Turner Prize during the 1990s, in popular usage, particularly in the UK, \"conceptual art\" came to denote all contemporary art that does not practice the traditional skills of painting and sculpture. It could be said that one of the reasons why the term \"conceptual art\" has come to be associated with various contemporary practices far removed from its original aims and forms lies in the problem of defining the term itself. As the artist Mel Bochner suggested as early as 1970, in explaining why he does not like the epithet \"conceptual\", it is not always entirely clear what \"concept\" refers to, and it runs the risk of being confused with \"intention\". Thus, in describing or defining a work of art as conceptual it is important not to confuse what is referred to as \"conceptual\" with an artist's \"intention\".\n\nThe French artist Marcel Duchamp paved the way for the conceptualists, providing them with examples of prototypically conceptual works — the readymades, for instance. The most famous of Duchamp's readymades was \"Fountain\" (1917), a standard urinal-basin signed by the artist with the pseudonym \"R.Mutt\", and submitted for inclusion in the annual, un-juried exhibition of the Society of Independent Artists in New York (which rejected it). The artistic tradition does not see a commonplace object (such as a urinal) as art because it is not made by an artist or with any intention of being art, nor is it unique or hand-crafted. Duchamp's relevance and theoretical importance for future \"conceptualists\" was later acknowledged by US artist Joseph Kosuth in his 1969 essay, \"Art after Philosophy\", when he wrote: \"All art (after Duchamp) is conceptual (in nature) because art only exists conceptually\".\n\nIn 1956 the founder of Lettrism, Isidore Isou, developed the notion of a work of art which, by its very nature, could never be created in reality, but which could nevertheless provide aesthetic rewards by being contemplated intellectually. This concept, also called \"Art esthapériste\" (or \"infinite-aesthetics\"), derived from the infinitesimals of Gottfried Wilhelm Leibniz – quantities which could not actually exist except conceptually. The current incarnation () of the Isouian movement, Excoördism, self-defines as the art of the infinitely large and the infinitely small.\n\nIn 1961 the term \"concept art\", coined by the artist Henry Flynt in his article bearing the term as its title, appeared in a proto-Fluxus publication \"An Anthology of Chance Operations\". \nHowever, it assumed a different meaning when employed by Joseph Kosuth and by the English Art and Language group, who discarded the conventional art object in favour of a documented critical inquiry, that began in Art-Language The Journal of conceptual art in 1969, into the artist's social , philosophical and psychological status. By the mid-1970s they had produced publications, indices, performances, texts and paintings to this end. In 1970 \"Conceptual Art and Conceptual Aspects\", the first dedicated conceptual-art exhibition, took place at the New York Cultural Center.\n\nConceptual art emerged as a movement during the 1960s – in part as a reaction against formalism as then articulated by the influential New York art critic Clement Greenberg. According to Greenberg Modern art followed a process of progressive reduction and refinement toward the goal of defining the essential, formal nature of each medium. Those elements that ran counter to this nature were to be reduced. The task of painting, for example, was to define precisely what kind of object a painting truly is: what makes it a painting and nothing else. As it is of the nature of paintings to be flat objects with canvas surfaces onto which colored pigment is applied, such things as figuration, 3-D perspective illusion and references to external subject matter were all found to be extraneous to the essence of painting, and ought to be removed.\n\nSome have argued that conceptual art continued this \"dematerialization\" of art by removing the need for objects altogether,\nwhile others, including many of the artists themselves, saw conceptual art as a radical break with Greenberg's kind of formalist Modernism. Later artists continued to share a preference for art to be self-critical, as well as a distaste for illusion. However, by the end of the 1960s it was certainly clear that Greenberg's stipulations for art to continue within the confines of each medium and to exclude external subject matter no longer held traction.\nConceptual art also reacted against the commodification of art; it attempted a subversion of the gallery or museum as the location and determiner of art, and the art market as the owner and distributor of art. Lawrence Weiner said: \"Once you know about a work of mine you own it. There's no way I can climb inside somebody's head and remove it.\" Many conceptual artists' work can therefore only be known about through documentation which is manifested by it, e.g. photographs, written texts or displayed objects, which some might argue are not in themselves the art. It is sometimes (as in the work of Robert Barry, Yoko Ono, and Weiner himself) reduced to a set of written instructions describing a work, but stopping short of actually making it—emphasising the idea as more important than the artifact. This reveals an explicit preference for the \"art\" side of the ostensible dichotomy between art and craft, where art, unlike craft, takes place within and engages historical discourse: for example, Ono's \"written instructions\" make more sense alongside other conceptual art of the time.\n\nLanguage was a central concern for the first wave of conceptual artists of the 1960s and early 1970s. Although the utilisation of text in art was in no way novel, only in the 1960s did the artists Lawrence Weiner, Edward Ruscha, Joseph Kosuth, Robert Barry, and Art & Language begin to produce art by exclusively linguistic means. Where previously language was presented as one kind of visual element alongside others, and subordinate to an overarching composition (e.g. Synthetic Cubism), the conceptual artists used language in place of brush and canvas, and allowed it to signify in its own right. Of Lawrence Weiner's works Anne Rorimer writes, \"The thematic content of individual works derives solely from the import of the language employed, while presentational means and contextual placement play crucial, yet separate, roles.\"\n\nThe British philosopher and theorist of conceptual art Peter Osborne suggests that among the many factors that influenced the gravitation toward language-based art, a central role for conceptualism came from the turn to linguistic theories of meaning in both Anglo-American analytic philosophy, and structuralist and post structuralist Continental philosophy during the middle of the twentieth century. This linguistic turn \"reinforced and legitimized\" the direction the conceptual artists took. Osborne also notes that the early conceptualists were the first generation of artists to complete degree-based university training in art. Osborne later made the observation that contemporary art is \"post-conceptual\" in a public lecture delivered at the Fondazione Antonio Ratti, Villa Sucota in Como on July 9, 2010. It is a claim made at the level of the ontology of the work of art (rather than say at the descriptive level of style or movement).\n\nThe American art historian Edward A. Shanken points to the example of Roy Ascott who \"powerfully demonstrates the significant intersections between conceptual art and art-and-technology, exploding the conventional autonomy of these art-historical categories.\" Ascott, the British artist most closely associated with cybernetic art in England, was not included in Cybernetic Serendipity because his use of cybernetics was primarily conceptual and did not explicitly utilize technology. Conversely, although his essay on the application of cybernetics to art and art pedagogy, \"The Construction of Change\" (1964), was quoted on the dedication page (to Sol Lewitt) of Lucy R. Lippard's seminal \"Six Years: The Dematerialization of the Art Object from 1966 to 1972\", Ascott's anticipation of and contribution to the formation of conceptual art in Britain has received scant recognition, perhaps (and ironically) because his work was too closely allied with art-and-technology. Another vital intersection was explored in Ascott's use of the thesaurus in 1963 , which drew an explicit parallel between the taxonomic qualities of verbal and visual languages – a concept would be taken up in Joseph Kosuth's \"Second Investigation, Proposition 1\" (1968) and Mel Ramsden's \"Elements of an Incomplete Map\" (1968).\n\n\"By adopting language as their exclusive medium, Weiner, Barry, Wilson, Kosuth and Art & Language were able to sweep aside the vestiges of authorial presence manifested by formal invention and the handling of materials.\"\nAn important difference between conceptual art and more \"traditional\" forms of art-making goes to the question of artistic skill. Although skill in the handling of traditional media often plays little role in conceptual art, it is difficult to argue that no skill is required to make conceptual works, or that skill is always absent from them. John Baldessari, for instance, has presented realist pictures that he commissioned professional sign-writers to paint; and many conceptual performance artists (e.g. Stelarc, Marina Abramović) are technically accomplished performers and skilled manipulators of their own bodies. It is thus not so much an absence of skill or hostility toward tradition that defines conceptual art as an evident disregard for conventional, modern notions of authorial presence and of individual artistic expression.\n\nThe first wave of the \"conceptual art\" movement extended from approximately 1967 to 1978. Early \"concept\" artists like Henry Flynt, Robert Morris, and Ray Johnson influenced the later, widely accepted movement of conceptual art. Conceptual artists like Dan Graham, Hans Haacke, and Lawrence Weiner have proven very influential on subsequent artists, and well known contemporary artists such as Mike Kelley or Tracey Emin are sometimes labeled \"second- or third-generation\" conceptualists, or \"post-conceptual\" artists.\n\nMany of the concerns of the conceptual art movement have been taken up by contemporary artists. While they may or may not term themselves \"conceptual artists\", ideas such as anti-commodification, social and/or political critique, and ideas/information as medium continue to be aspects of contemporary art, especially among artists working with installation art, performance art, net.art and electronic/digital art.\n\n\n\nBooks\n\n\nEssays\n\n\nExhibition catalogues\n\n\n"}
{"id": "42415226", "url": "https://en.wikipedia.org/wiki?curid=42415226", "title": "Conceptual combination", "text": "Conceptual combination\n\nConceptual combination is a fundamental cognitive process by which two or more existing basic concepts are mentally synthesized to generate a composite, higher-order concept. The products of this process are sometimes referred to as \"complex concepts.\" Combining concepts allows individuals to use a finite number of concepts which they already understand to construct a potentially limitless quantity of new, related concepts. It is an essential component of many abilities, such as perception, language, synthetic reasoning, creative thought and abstraction.\n\nConceptual combination is an important concept in the fields of cognitive psychology and cognitive science.\n\nThe mechanism by which conceptual combination occurs is debatable, both on cognitive and neurological levels. As such, multiple models have been developed or applied to better define how the process occurs.\n\nCognitive models attempt to functionally outline the mental computation involved in conceptual combination. \n\nConstraint theory stipulates that the concept that results from an attempt at conceptual combination is controlled by three constraints known as diagnosticity, plausibility and informativeness. \"Diagnosticity\" refers to the a complex concept's possession of the defining properties of its component simple concepts. Because such properties are diagnostic of the component concepts, at least some of them should be diagnostic of the higher-order representations constructed from those concepts. \"Plausibility\" refers to consistency with existing knowledge and prior experience. It is based on the assumption that a complex concept should be reasonably relevant to the context in which it is used. This assumption makes the most sense in a practical, linguistic context, particularly when a speaker is catering to the understanding of the listener. \"Informativeness\" is the property of having more meaning or properties than any individual component. If a complex concept were not distinguishable from any given component, it would be identical to that component. Because nothing can logically be both a component of something and the totality of something simultaneously, a complex concept must at least be the sum of its parts. Many argue that the interaction among component concepts should allow a complex concept to be greater than that sum. If multiple possible ways to structure or interpret a complex concept exist, the one which best satisfies or most satisfies these constraints is the one which will be used. The paradigm upon which constraint theory is based is computational, and therefore views the mind as a processor which operates on the basis of standard problem-solving protocols (i.e. algorithms and heuristics).\n\nThe spreading activation model is a model in connectionist theory sometimes designed to represent how concepts are activated in relation to one another. Though it is typically applied to information search processes like recognition, brainstorming, and recall, it can be used to explain how concepts are combined as well as connected.\n\nSpreading activation models represent memory and knowledge as a network of interlinked concepts. Every concept manifests as a node within this network, with related nodes/concepts linked to one another with connections. Concepts that are more strongly associated with one another either in terms of content or an individual's past experience are correspondingly more strongly linked.\n\nWhen one concept is employed in working memory, the corresponding node is also activated. This activation spreads through the node's links, making it easier to activate nodes to which the activated node is connected. This spreading activation stimulates the linked nodes, pressuring them to activate to an extent proportional to the strength of the connection between the stimulating node and the stimulated node. If sufficient net stimulation is accrued through a stimulated node's links, it will also activate. Thus, being connected to an activated node makes it easier for an inactive node to become active as well; concepts become more readily accessed when individuals are stimulated with related concepts first. This increase in ease of access is known as \"priming.\"\n\nSpreading activation models tend to imply that processing concepts occurs in series; that is, each concept is processed one-at-a-time, one after the other. As such, individuals tend to combine concepts more readily, easily, and quickly if they are more closely linked within the network of concepts. This implication, however, has caused spreading activation to come under a great deal of criticism, particularly with respect to how the concept is employed in feature theories.\n\nThe features and properties of complex concepts are generally assumed to be derived from the simple concepts that compose them. One popularly discussed model involves a two-stage serial process. In the initial stage, features from each of the component simple concepts are retrieved from memory through spreading activation. This allows a complex concept to accrue features with existing relationships with its component simple concepts. During this stage, the basic definition of what the complex concept is and/or means is generates. In the second stage, knowledge and reasoning are employed upon the features accrued in the previous stage to generate further features. For example, one might reason that the complex concept \"white jacket,\" if worn in a blizzard, would make one difficult to see; it would follow that one should ascribe the property of \"good for winter camouflage,\" despite the fact that this property is not closely attached to the component concepts \"white\" nor \"jacket.\" This stage is especially useful when properties of complex concepts contradict those of their component concepts, such as the different colours of milk and chocolate milk.\n\nThis model, however, has come under criticism due to its inconsistency with empirical studies. If conceptual combination employed serial spreading activation, for instance, it should take longer to verify the properties of complex concepts, as they necessarily possess more concepts than their component simple concepts. Research has nonetheless shown that it takes less time to confirm complex concepts' properties than their components and about equal time to reject false properties for both. This occurred even when the properties of the complex concept contradicted those of the simple concepts. Likewise, when experiment participants are exposed to a set of features first, and then asked to verify whether or not they correspond to simple or complex concepts, the participants tend to provide correct verification answers for the complex concepts more quickly.\n\nThe neurological basis of conceptual combination has received considerably less attention than its cognitive basis. Nevertheless, research has revealed several specific brain regions that are intimately involved if not entirely responsible for neural processing involved in conceptual combination.\n\nOf particular relevance is the left anterior temporal lobe. Studies have previously demonstrated an additive effect for stimulation in this subsection of neural cortex tissue. When experiment participants were verbally presented with certain simple concepts, the processing of the information causes electrical stimulation in the region. When the same participants were verbally presented with a single complex concept formed from the combination of the aforementioned simple concepts, the stimulation recorded was equivalent to the sum of the stimulation that resulted from each individual component simple concept. In other words, the stimulation caused by a complex concept is equivalent to the total stimulation caused by its component concepts. More recent data contradicts those results by indicating a multiplicative effect in which the activation caused by a complex concept is the product of the activation levels caused by its component concepts, rather than the sum.\n\nFurther support for the role of the left anterior temporal lobe has been previously established through neuropsychological studies. Semantic dementia is a disorder in which conceptual manipulation, including conceptual combination, is hindered. These indicate that the neural damage associated with semantic dementia occurs within this brain region. Unfortunately, neuropsychological studies that attempt to replicate this pattern have failed, leading uncertainty as to whether initial results were valid.\n\nAs language is the means by which concepts are communicated and expressed, the processed involved in linguistic expression and interpretation are heavily intertwined with combined concepts. Many theories of concept combination mechanisms, including constraint theory were developed within the context of language, and therefore make more sense when applied in a linguistic context. Study into the linguistic aspects of concept combination as has generally been focused on the interpretation mechanism.\n\nA concept that can be expressed using a single word is called a \"lexical concept.\" A lexical concept is usually treated as a basic concept, although it can just as easily be a complex concept.\n\nTwo lexical concepts are often used together as phrases to represent a combined concept of greater specificity. This is most readily seen in the use of adjectives to modify nouns and the use of adverbs to modify verbs and adjectives. Consider, for example, phrases such as \"burnt toast,\" \"eat roughly,\" and \"readily loved.\" Multiple noun lexical concepts can also be used together in order to represent combined concepts. Through this process, a limited pool of nouns can be used to produce an exponentially larger pool of phrases such as \"sound wave,\" \"video game,\" and \"sleeping pill.\"\n\nIn addition to constraint theory, there are two principal theories surrounding the mechanism by which noun-noun combinations are interpreted. The first of these is \"dual-process theory.\" Dual-process theory proposed that there are two means by which people interpreted noun-noun phrases. \"Relational interpretation\" attempts to establish a relationship between the nouns and interprets the combined phrase in terms of that relationship. For example, one might relationally interpret the phrase \"snake mouse\" to refer to a mouse meant to be eaten by snakes, as the two concepts have a predatory relationship. \"Property interpretation\" identifies properties associated with the first noun and then applies them onto the second noun. In this case the phrase \"snake mouse\" might be interpreted as a mouse with poisonous fangs or an elongated body.\n\nThe second principal theory is known as the \"Competition in Relations among Nominals\" theory. It states that the assumed modification effect of a noun on its partner in a novel noun-noun combination is the one which it has been seen to employ most often in the past. For example, \"chocolate cat\" is usually interpreted as \"a cat made of chocolate\" rather than \"a chocolate-eating cat\" simply because the \"made of\" modifier is heavily conditioned to be associated with \"chocolate.\"\n\nExplanations of linguistic expression of complex concepts have been linked to spreading activation models. When an individual identifies a lexical concept through vision or hearing, the corresponding node in that individual's cognitive network is said to activate. This makes it easier for lexical concepts linked to the activated concept to be comprehended, as they are primed. This is consistent with current empirical data, which shows that when individuals are interpreting sentences, they process the linguistic content more quickly when several related words follow one another. In turn, it becomes easier for people to combine these related concepts together and understand them as a relationship, rather than two distinct entities. For example, consider the example, \"John spread butter on a bagel.\" In this sentence, the lexical concepts \"spread,\" \"butter,\" and \"bagel\" are associated with one another and easy to combine into a mental representation of a breakfast scenario. Conversely, consider the example, \"John baked a computer.\" Because \"baked\" and \"computer\" are not related lexical concepts, it takes more effort and time to build a mental representation of this unusual scenario.\n\nHowever, spreading activation models of conceptual combination have been criticized in light of how humans have been observed to combined languages. Those who claim that the theory provides an insufficient account of linguistic conceptual combination refer to the ability of humans to readily understand lexical concept combinations with seemingly no apparent connection with one another. One example of this would be the sentence \"John saw an elephant cloud.\" \"Elephant\" and \"cloud\" do not shared a close association, but it takes little effort to comprehend that the term \"elephant cloud\" refers to a cloud shaped like an elephant. This has led some to conclude that the combination of lexical concepts does not wholly rely on the simultaneous activation of linked lexical concepts alone. Rather, they claim that the process involves the use of existing nodes to generate entirely new concepts independent of their parent concepts.\n\nAlthough many theories of novel noun-noun combination interpretation ignore the effect of social environment, some theorists have attempted to account for any contingencies social context may cause.\n\nWhen lexical concept combinations are interpreted without the influence of social context, the interpretation carried out is termed \"sense generation.\" This includes all processes that would normally occur excepting those dependent on a conversation partner. The \"generation hypothesis\" accordingly states that the interpretation mechanism of a noun-noun combination is essentially the same regardless of context. This does not rule out the possibility that social context can affect sense generation in some way, but it does assert that the basic structure of the process is unaffected. As seen above, debate as to what sense generation entails and how many sub-processes into which it should be divided is a contentious matter in cognitive science.\n\nThe \"anaphor resolution hypothesis\" instead asserts that before sense generation occurs, interpreters first search their memory of recent communication to see if the combination refers to something previously discussed. This process is termed \"anaphor resolution'.' If a referent is identified, interpretation occurs without sense generation in light of that referent. Even if an explicit referent does not exist, anaphor resolution can help facilitate sense generation by providing more information that might hint at the combination's intended meaning.\n\nThe \"dual-process hypothesis\" not to be confused with dual-process theory, states that sense generation and anaphor resolution occur in parallel. Both processes begin to work once the noun-noun combination is presented. Proponents of this hypothesis disagree as to how the interpretation is eventually resolves. Some believe that whichever process reaches a conclusion first provides the answer. Others believe that both provide continuous input to a third, mediating process that eventually makes a decision based on input from both.\n\nCreativity necessitates the employment of existing concepts in novel ways, and therefore requires conceptual combination. Surprisingly, this contribution seems to be limited. Conceptual combination is a significant contributor to convergent thinking, but not divergent thinking. For example, practice with generating new concepts through combination does not improve brainstorming. It does, however, assist in devising creative problem solving methods.\n\nThe psychological community's growing understanding of how concepts are manipulated has allowed educators to teach new concepts more effectively. Tools that are developed based on conceptual combination theory attempt to teach individual tasks, and then challenge students to exercise them together in order to promote both base subject skills and the critical thinking needed to apply them simultaneously to solve new problems. Máder & Vajda, for instance, developed a three-dimensional grid with cells of adjustable height which has been successfully used in numerous activities capable of improving the effectiveness of high school mathematics education.\n"}
{"id": "58267", "url": "https://en.wikipedia.org/wiki?curid=58267", "title": "Conceptual schema", "text": "Conceptual schema\n\nA 'conceptual schema' is a high-level description of a business's informational needs. It typically includes only the main concepts and the main relationships among them. Typically this is a first-cut model, with insufficient detail to build an actual database. This level describes the structure of the whole database for a group of users. The conceptual model is also known as the data model that can be used to describe the conceptual schema when a database system is implemented. It hides the internal details of physical storage and targets on describing entities, datatype, relationships and constraints.\n\nA conceptual schema or conceptual data model is a map of concepts and their relationships used for databases. This describes the semantics of an organization and represents a series of assertions about its nature. Specifically, it describes the things of significance to an organization (\"entity classes\"), about which it is inclined to collect information, and characteristics of (\"attributes\") and associations between pairs of those things of significance (\"relationships\").\n\nBecause a conceptual schema represents the semantics of an organization, and not a database design, it may exist on various levels of abstraction. The original ANSI four-schema architecture began with the set of \"external schema\" that each represent one person's view of the world around him or her. These are consolidated into a single \"conceptual schema\" that is the superset of all of those external views. A data model can be as concrete as each person's perspective, but this tends to make it inflexible. If that person's world changes, the model must change. Conceptual data models take a more abstract perspective, identifying the fundamental things, of which the things an individual deals with are just examples.\n\nThe model does allow for what is called inheritance in object oriented terms. The set of instances of an entity class may be subdivided into entity classes in their own right. Thus, each instance of a \"sub-type\" entity class is also an instance of the entity class's \"super-type\". Each instance of the super-type entity class, then is also an instance of one of the sub-type entity classes.\n\nSuper-type/sub-type relationships may be \"exclusive\" or not. A methodology may require that each instance of a super-type may \"only\" be an instance of \"one\" sub-type. Similarly, a super-type/sub-type relationship may be \"exhaustive\" or not. It is exhaustive if the methodology requires that each instance of a super-type \"must be\" an instance of a sub-type. A sub-type named other is often necessary.\n\n\nA data structure diagram (DSD) is a data model or diagram used to describe conceptual data models by providing graphical notations which document entities and their relationships, and the constraints that bind them.\n\n\n\n"}
{"id": "633037", "url": "https://en.wikipedia.org/wiki?curid=633037", "title": "Conceptualism", "text": "Conceptualism\n\nIn metaphysics, conceptualism is a theory that explains universality of particulars as conceptualized frameworks situated within the thinking mind. Intermediate between nominalism and realism, the conceptualist view approaches the metaphysical concept of universals from a perspective that denies their presence in particulars outside the mind's perception of them. Conceptualism is anti-realist about abstract objects, just like immanent realism is (their difference being that immanent realism does not deny the mind-independence of universals, like conceptualism does).\n\nThe evolution of late scholastic terminology has led to the emergence of conceptualism, which stemmed from doctrines that were previously considered to be nominalistic. The terminological distinction was made in order to stress the difference between the claim that universal mental acts correspond with universal intentional objects and the perspective that dismissed the existence of universals outside the mind. The former perspective of rejection of objective universality was distinctly defined as conceptualism.\n\nPeter Abélard was a medieval thinker whose work is currently classified as having the most potential in representing the roots of conceptualism. Abélard’s view denied the existence of determinate universals within things. William of Ockham was another famous late medieval thinker who had a strictly conceptualist solution to the metaphysical problem of universals. He argued that abstract concepts have no \"fundamentum\" outside the mind.\n\nIn the 17th century conceptualism gained favour for some decades especially among the Jesuits: Hurtado de Mendoza, Rodrigo de Arriaga and Francisco Oviedo are the main figures. Although the order soon returned to the more realist philosophy of Francisco Suárez, the ideas of these Jesuits had a great impact on the early modern philosophy.\n\nConceptualism was either explicitly or implicitly embraced by most of the early modern thinkers, including René Descartes, John Locke, Baruch Spinoza, Gottfried Wilhelm Leibniz, George Berkeley, and David Hume – often in a quite simplified form if compared with the elaborate scholastic theories.\n\nSometimes the term is applied even to the radically different philosophy of Immanuel Kant, who holds that universals have no connection with external things because they are exclusively produced by our \"a priori\" mental structures and functions.\n\nIn late modern philosophy, conceptualist views were held by G. W. F. Hegel.\n\nEdmund Husserl's philosophy of mathematics has been construed as a form of conceptualism.\n\nConceptualist realism (a view put forward by David Wiggins in 1980) states that our conceptual framework maps reality.\n\nThough separate from the historical debate regarding the status of universals, there has been significant debate regarding the conceptual character of experience since the release of \"Mind and World\" by John McDowell in 1994. McDowell's touchstone is the famous refutation that Wilfrid Sellars provided for what he called the \"Myth of the Given\"—the notion that all empirical knowledge is based on certain assumed or 'given' items, such as sense data. Thus, in rejecting the Myth of the Given, McDowell argues for perceptual conceptualism, according to which perceptual content is conceptual \"from the ground up\", that is, all perceptual experience is a form of conceptual experience. McDowell's philosophy of justification is considered a form of foundationalism: it is a form of foundationalism because it allows that certain judgements are warranted by experience and it is a coherent form of this view because it maintains that experience can warrant certain judgements because experience is irreducibly conceptual.\n\nA clear motivation of contemporary conceptualism is that the kind of perception that rational creatures like humans enjoy is unique in the fact that it has conceptual character. McDowell explains his position:\n\nI have urged that our perceptual relation to the world is conceptual all the way out to the world’s impacts on our receptive capacities. The idea of the conceptual that I mean to be invoking is to be understood in close connection with the idea of rationality, in the sense that is in play in the traditional separation of mature human beings, as rational animals, from the rest of the animal kingdom. Conceptual capacities are capacities that belong to their subject’s rationality. So another way of putting my claim is to say that our perceptual experience is permeated with rationality. I have also suggested, in passing, that something parallel should be said about our agency.\n\nMcDowell's conceptualism, though rather distinct (philosophically and historically) from conceptualism's genesis, shares the view that universals are not \"given\" in perception from outside the sphere of reason. Particular objects are perceived, as it were, already infused with conceptuality stemming from the spontaneity of the rational subject herself.\n\nThe application of the term \"perceptual conceptualism\" to Kant's philosophy of perception is debatable. Other scholars have argued for a rival interpretation of Kant's work termed perceptual non-conceptualism.\n\n"}
{"id": "194143", "url": "https://en.wikipedia.org/wiki?curid=194143", "title": "Double negative", "text": "Double negative\n\nA double negative is a grammatical construction occurring when two forms of negation are used in the same sentence. Multiple negation is the more general term referring to the occurrence of more than one negative in a clause. In some languages, double negatives cancel one another and produce an affirmative; in other languages, doubled negatives intensify the negation. Languages where multiple negatives affirm each other are said to have negative concord or emphatic negation. Portuguese, Persian, Russian, Spanish, Neapolitan, Italian, Japanese, Bulgarian, Czech, Polish, Afrikaans, Hebrew, and some dialects of English, such as African-American Vernacular English, are examples of negative-concord languages, while Latin and German do not have negative concord. It is cross-linguistically observed that negative-concord languages are more common than those without.\n\nLanguages without negative concord typically have negative polarity items that are used in place of additional negatives when another negating word already occurs. Examples are \"ever\", \"anything\" and \"anyone\" in the sentence \"I haven't ever owed anything to anyone\" (cf. \"I have\"n't\" \"never\" owed \"nothing\" to \"no one\"\" in negative-concord dialects of English, and \"\"Nunca\" devi \"nada\" a \"ninguém\"\" in Portuguese, lit. \"Never have I owed nothing to no one\", or \"\"Non\" ho \"mai\" dovuto \"nulla\" a \"nessuno\"\" in Italian). Note that negative polarity can be triggered not only by direct negatives such as \"not\" or \"never\", but also by words such as \"doubt\" or \"hardly\" (\"I doubt he has ever owed anything to anyone\" or \"He has hardly ever owed anything to anyone\").\n\nStylistically, in English, double negatives can sometimes be used for affirmation (e.g. \"I'm not feeling not good\"), an understatement of the positive (\"I'm feeling good\"). The rhetorical term for this is litotes.\n\nWhen two negatives are used in one independent clause, in standard English the negatives are understood to cancel one another and produce a weakened affirmative: this is known as litotes. However, depending on how such a sentence is constructed, in some dialects if a verb or adverb is in between two negatives then the latter negative is assumed to be intensifying the former thus adding weight or feeling to the negative clause of the sentence. For this reason, it is difficult to portray double negatives in writing as the level of intonation to add weight in one's speech is lost. A double negative intensifier does not necessarily require the prescribed steps, and can easily be ascertained by the mood or intonation of the speaker.\n\nvs.\n\nThese two sentences would be different in how they are communicated by speech. Any assumption would be correct, and the first sentence can be just as right or wrong in intensifying a negative as it is in cancelling it out; thereby rendering the sentence's meaning ambiguous. Since there is no adverb or verb to support the latter negative, the usage here is ambiguous and lies totally on the context behind the sentence. In light of punctuation, the second sentence can be viewed as the intensifier; and the former being a statement thus an admonishment.\n\nIn Standard English, two negatives are understood to resolve to a positive. This rule was observed as early as 1762, when Bishop Robert Lowth wrote \"A Short Introduction to English Grammar with Critical Notes\". For instance, \"I do not disagree\" could mean \"I certainly agree\", \"I agree\", \"I sort of agree\", \"I don't understand your point of view\", \"I have no opinion\", and so on; it is a form of \"weasel words\". Further statements are necessary to resolve which particular meaning was intended.\n\nThis is opposed to the single negative \"I do not agree\", which typically means \"I disagree\". However, the statement \"I do not completely disagree\" is a similar double negative to \"I do not disagree\" but needs little or no clarification.\n\nWith the meaning \"I completely agree\", Lowth would have been referring to litotes wherein two negatives simply cancel each other out. However, the usage of intensifying negatives and examples are presented in his work, which could also imply he wanted either usage of double negatives abolished. Because of this ambiguity, double negatives are frequently employed when making back-handed compliments. The phrase \"Mr. Jones was not incompetent.\" will seldom mean \"Mr. Jones was very competent\" since the speaker would have found a more flattering way to say so. Instead, some kind of problem is implied, though Mr. Jones possesses basic competence at his tasks.\n\nDiscussing English grammar, the term \"double negative\" is often though not universally applied to the non-standard use of a second negative as an intensifier to a negation.\n\nDouble negatives are usually associated with regional and ethnical dialects such as Southern American English, African American Vernacular English, and various British regional dialects. Indeed, they were used in Middle English. Historically, Chaucer made extensive use of double, triple, and even quadruple negatives in his \"Canterbury Tales\". About the Friar, he writes \"Ther nas no man no wher so vertuous\" (\"There never was no man nowhere so virtuous\"). About the Knight, \"He nevere yet no vileynye ne sayde / In all his lyf unto no maner wight\" (\"He never yet no vileness didn't say / In all his life to no manner of man\").\n\nFollowing the battle of Marston Moor, Oliver Cromwell quoted his nephew's dying words in a letter to the boy's father Valentine Walton: \"A little after, he said one thing lay upon his spirit. I asked him what it was. He told me it was that God had not suffered him to be no more the executioner of His enemies.\" Although this particular letter has often been reprinted, it is frequently changed to read \"not ... to be any more\" instead.\n\nWhereas some double negatives may resolve to a positive, in some dialects others resolve to intensify the negative clause within a sentence. For example:\n\nIn contrast, some double negatives become positives:\n\nThe key to understanding the former examples and knowing whether a double negative is intensive or negative is finding a verb between the two negatives. If a verb is present between the two, the latter negative becomes an intensifier which does not negate the former. In the first example, the verb \"to go\" separates the two negatives; therefore the latter negative does not negate the already negated verb. Indeed, the word 'nowhere' is thus being used as an adverb and does not negate the argument of the sentence. One interesting thing to note is that double negatives such as \"I don't want to know no more\" contrasts with Romance languages such as French in \"Je ne veux pas savoir.\" \n\nAn exception is when the second negative is stressed, as in \"I'm not doing ; I'm thinking.\" A sentence can otherwise usually only become positive through consecutive uses of negatives, such as those prescribed in the later examples, where a clause is void of a verb and lacks an adverb to intensify it. Two of them also use emphasis to make the meaning clearer. The last example is a popular example of a double negative that resolves to a positive. This is because the verb 'to doubt' has no intensifier which effectively resolves a sentence to a positive. Had we added an adverb thus:\n\nThen what happens is that the verb \"to doubt\" becomes intensified, which indeed deduces that the sentence is indeed false since nothing was resolved to a positive. The same applies to the third example, where the adverb 'more' merges with the prefix \"no-\" to become a negative word, which when combined with the sentence's former negative only acts as an intensifier to the verb \"hungry\". Where people think that the sentence \"I'm not hungry no more\" resolves to a positive is where the latter negative \"no\" becomes an adjective which only describes its suffix counterpart \"more\" which effectively becomes a noun, instead of an adverb. This is a valid argument since adjectives do indeed describe the nature of a noun; yet some fail to take into account that the phrase \"no more\" is only an adverb and simply serves as an intensifier. Another argument used to support the position double negatives aren't acceptable is a mathematical analogy: negating a negative number results in a positive one; e.g., ; therefore, it is argued, \"I did not go nowhere\" resolves to \"I went somewhere\".\n\nOther forms of double negatives, which are popular to this day and do strictly enhance the negative rather than destroying it, are described thus:\n\nPhilosophies aside, this form of double negative is still in use whereby the use of 'nor' enhances the negative clause by emphasizing what isn't to be. Opponents of double negatives would have preferred \"I'm not entirely familiar with Nihilism or Existentialism\"; however this renders the sentence somewhat empty of the negative clause being advanced in the sentence. This form of double negative along with others described are standard ways of intensifying as well as enhancing a negative. The use of 'nor' to emphasise the negative clause is still popular today, and has been popular in the past through works of Shakespeare and Milton:\n\nTo the common reader the negatives herein do not cancel each other out but simply emphasizes the negative clause.\nUp to the 18th century, double negatives were used to emphasize negation. \"Prescriptive grammarians\" recorded and codified a shift away from the double negative in the 1700s. Double negatives continue to be spoken by those of Vernacular English, such as those of Appalachian English and African American Vernacular English. To such speakers, they view double negatives as emphasizing the negative rather than cancelling out the negatives. Researchers have studied African American Vernacular English (AAVE) and trace its origins back to colonial English. This shows that double negatives were present in colonial English, and thus presumably English as a whole, and were acceptable at that time. English after the 18th century was changed to become more logical and double negatives became seen as canceling each other as in mathematics. The use of double negatives became associated with being uneducated and illogical.\n\nIn his \"Essay towards a practical English Grammar\" of 1711, James Greenwood first recorded the rule: \"Two Negatives, or two Adverbs of Denying do in English affirm\". Robert Lowth stated in his grammar textbook \"A Short Introduction to English Grammar\" (1762) that \"two negatives in English destroy one another, or are equivalent to an affirmative\". Grammarians have assumed that Latin was the model for Lowth and other early grammarians in prescribing against negative concord, as Latin does not feature it. Data indicates, however, that negative concord had already fallen into disuse in Standard English by the time of Lowth's grammar, and no evidence exists that the loss was driven by prescriptivism, which was well established by the time it appeared.\n\nDouble negatives have been employed in various films and television shows. In the film \"Mary Poppins\", the chimney sweep Bert employs a double negative when he says, \"If you don't want to go nowhere...\" Another is used by the bandits in the \"Stinking Badges\" scene of John Huston's \"The Treasure of the Sierra Madre\": \"Badges? We ain't got no badges. We don't need no badges!\".\n\nMore recently, the British television show \"EastEnders\" has received some publicity over the Estuary accent of character Dot Branning, who speaks with double and triple negatives (\"I ain't never heard of no licence.\").. In the Harry Enfield sketch \"Mr Cholmondley-Warner's Guide to the Working-Class\", a stereotypical Cockney employs a septuple-negative: \"Inside toilet? I ain't never not heard of one of them nor I ain't nor nothing.\"\n\nIn music, double negatives can be employed to similar effect (as in Pink Floyd's \"Another Brick in the Wall\", in which schoolchildren chant \"We don't need no education / We don't need no thought control\") or used to establish a frank and informal tone (as in The Rolling Stones' \"(I Can't Get No) Satisfaction\").\n\nDouble negation is uncommon in other West Germanic languages. A notable exception is Afrikaans, where it is mandatory (for example, \"He cannot speak Afrikaans\" becomes \"Hy kan nie Afrikaans praat nie\", \"He cannot Afrikaans speak not\"). Dialectal Dutch, French and San have been suggested as possible origins for this trait. Its proper use follows a set of fairly complex rules as in these examples provided by Bruce Donaldson:\n\nAnother point of view is that this construction is not really an example of a \"double negative\" but simply a grammatical template for negation. The second \"nie\" cannot be understood as a noun or adverb (as can, e.g., \"pas\" in French), and cannot be substituted by any part of speech other than itself with the sentence remaining grammatical. It is a grammatical particle with no independent meaning that happens to be spelled and pronounced the same as the embedded \"nie\", meaning \"not\", through historical accident.\n\nThe second \"nie\" is used if and only if the sentence or phrase doesn't already end with \"nie\" or another negating adverb.\n\nAfrikaans shares with English the property that two negatives make a positive. For example,\n\nWhile double negation is still found in the Low Franconian dialects of west Flanders (e.g., \"Ik ne willen da nie doen\", \"I do not want to do that\") and in some villages in the central Netherlands such as Garderen, it takes a different form than that found in Afrikaans. In Belgian Dutch dialects, however, there are still some widely used expressions like \"nooit niet\" (\"never not\") for \"never\".\n\nSimilar to some dialectal English, Bavarian employs both single and double negation, with the latter denoting special emphasis. For example, compare the Bavarian \"Des hob i no nia ned g'hört\" (\"This have I yet never not heard\") with the standard German \"Das habe ich noch nie gehört\". The German emphatic \"niemals!\" (roughly \"never ever\") corresponds to Bavarian \"(går) nia ned\" or even \"nie nicht\" in Standard German pronunciation.\n\nAnother exception is Yiddish. Due to Slavic influence, the double (and sometimes even triple) negative is quite common.\n\nA few examples would be:\n\nWhile in Latin a second negative word appearing along with \"non\" turns the meaning into a positive one: \"ullus\" means \"any\", \"nullus\" means \"no\", \"non...nullus\" (\"nonnullus\") means \"some\". In the same way, \"umquam\" means \"ever\", \"numquam\" means \"never\", \"non...numquam\" (\"nonnumquam\") means \"sometimes\", in many Romance languages a second term indicated a negative is required.\n\nIn French, the usual way to express negation is to employ two negatives, e.g. \"ne [verb] pas\", \"ne [verb] plus\", or \"ne [verb] jamais\", as in the sentences \"Je ne sais pas\" (\"I do not know\"), \"Il n'y a plus de baguettes\" (\"There aren't any more baguettes\"), and \"On ne sait jamais\" (\"one never knows\"). The second term was originally an emphatic; \"pas\", for example, derives from the Latin \"passus\", meaning \"step\", so that French \"Je ne marche pas\" and Catalan \"No camino pas\" originally meant \"I will not walk a single step.\" This initial usage spread so thoroughly that it became a necessary element of any negation in the modern French language and that, in fact, in contemporary French, the original actual negative \"ne\" is mostly left away in favour of \"pas\", as in \"Je sais pas\" \"I don't know\". In Northern Catalan, \"no\" may be omitted in colloquial language, and Occitan, which uses \"non\" only as a short answer to questions. In Venetian, the double negation \"no ... mìa\" can likewise lose the first particle and rely only on the second: \"magno mìa\" (\"I eat not\") and \"vegno mìa\" (\"I come not\"). These exemplify Jespersen's cycle.\n\nItalian, Portuguese and Romanian languages usually employ doubled negative correlatives. Portuguese \"Não vejo nada\", Romanian \"Nu văd nimic\" and Italian \"Non vedo niente\" (\"I do not see nothing\") are used to express \"No, I do not see anything\". In Italian, a second following negative particle \"non\" turns the phrase into a positive one, but with a slightly different meaning. For instance, while both \"Voglio mangiare\" (\"I want to eat\") and \"Non voglio non mangiare\" (\"I don't want not to eat\") mean \"I want to eat\", the latter phrase more precisely means \"I'd prefer to eat\".\n\nOther Romance languages employ double negatives less regularly. In Asturian, an extra negative particle is used with negative adverbs: \"Yo nunca nun lu viera\" (\"I had not never seen him\") means \"I have never seen him\" and \"A mi tampoco nun me presta\" (\"I neither do not like it\") means \"I do not like it either\". Standard Catalan and Galician also used to possess a tendency to double \"no\" with other negatives, so \"Jo tampoc no l'he vista\" or \"Eu tampouco non a vira\", respectively (\"I neither have not seen her\") meant \"I have not seen her either\". That practice is dying out.\n\nIn 1974, Italy held a referendum on whether to repeal a recent law that allowed divorce. Voters were said to have been confused in that in order to support divorce, they needed to vote 'no' on the referendum which was worded so that 'yes' would support repeal. And to reframe the fundamental underlying issue as being support/non-support of the continuation of marriage, then the vote was structured as a triple negative (with divorce as the negation of the continuation of marriage being the first negative). This referendum was defeated, and without this confusion, it was said that it would have been defeated more decisively.\n\nIn spoken Welsh, the word ddim (not) often occurs with a prefixed or mutated verb form that is negative in meaning: \"Dydy hi ddim yma\" (word-for-word, \"Not-is she not here\") expresses \"She is not here\" and \"Chaiff Aled ddim mynd\" (word-for-word, \"Not-will-get Aled not go\") expresses \"Aled is not allowed to go\".\n\nNegative correlatives can also occur with already negative verb forms. In literary Welsh, the mutated verb form is caused by an initial negative particle, ni or nid. The particle is usually omitted in speech but the mutation remains: \"[Ni] wyddai neb\" (word-for-word, \"[Not] not-knew nobody\") means \"Nobody knew\" and \"[Ni] chaiff Aled fawr o bres\" (word-for-word, \"[Not] not-will-get Aled lots of money\") means \"Aled will not get much money\". This is not usually regarded as three negative markers, however, because the negative mutation is really just an effect of the initial particle on the following word.\n\nDoubled negatives are perfectly correct in Ancient Greek. With few exceptions, a simple negative (οὐ or μή) following another negative (for example, οὐδείς, \"no one\") results in an affirmation: οὐδείς οὐκ ἔπασχε τι (\"No one was not suffering\") means more simply \"Everyone was suffering\". Meanwhile, a compound negative following a negative strengthens the negation: μὴ θορυβήσῃ μηδείς (\"Do not permit no one to raise an uproar\") means \"Let not a single one among them raise an uproar\".\n\nThose constructions apply only when the negatives all refer to the same word or expression. Otherwise, the negatives simply work independently of one another: οὐ διὰ τὸ μὴ ἀκοντίζειν οὐκ ἔβαλον αὐτόν means \"It was not on account of their not throwing that they did not hit him\", and one should not blame them for not trying.\n\nIn Modern Greek, negative concord is standard and more commonly used. For example, the sentence 'You (pl.) will not find anything' can be said in two ways: 'Δε θα βρείτε τίποτα' ('Not will find nothing') is more common than 'Δε θα βρείτε κάτι' ('Not will find something'). It depends simply on the mood of the speaker, and the latter being is considered slightly more polite. An exception to that rule is the (archaic) pronoun ουδείς, also meaning \"no one\", which does not allow negation of the verb that it governs.\n\nIn Slavic languages other than Slavonic, multiple negatives are grammatically correct ways to express negation, and a single negative is often incorrect. In complex sentences, every part that could be grammatically negated should be negative. For example, in the Serbo-Croatian, \"Ni(t)ko nikad(a) nigd(j)e ništa nije uradio\" (\"Nobody never did not do nothing nowhere\") means \"Nobody has ever done anything, anywhere\", and \"Nikad nisam tamo išao/išla\" (\"Never I did not go there\") means \"I have never been there\". In Czech it is also common to use three or more negations. For example, \"Nikdy jsem nikde nikoho neviděl\" (\"I have not never seen no one nowhere\"). In Russian, \"I know nothing\" is я ничего не знаю (\"ya nichevo nye znayu\"), lit. \"I nothing don't know.\"\n\nA single negation, while syntactically correct, may result in a very unusual meaning or make no sense at all. Saying \"I saw nobody\" in Polish (\"Widziałem nikogo\") instead of the more usual \"I did not see nobody\" (\"Nikogo nie widziałem\") might mean \"I saw an instance of nobody\" or \"I saw Mr. Nobody\" but it would not have its plain English meaning. Likewise, in Slovenian, saying \"I do not know anyone\" (') in place of \"I do not know no one\" (') has the connotation \"I do not know just \"anyone\"\": I know someone important or special.\n\nAs with most synthetic \"satem\" languages double negative is mandatory in Latvian and Lithuanian. Furthermore, all verbs and indefinite pronouns in a given statement must be negated, so it could be said that multiple negative is mandatory in Latvian.\n\nFor instance, a statement \"I have not ever owed anything to anyone\" would be rendered as \"es nekad nevienam neko neesmu bijis parādā\". The only alternative would be using a negating subordinate clause and subjunctive in the main clause, which could be approximated in English as \"there has not ever been an instance that I would have owed anything to anyone\" (\"nav bijis tā, ka es kādreiz būtu kādam bijis kaut ko parādā\"), where negative pronouns (\"nekad, neviens, nekas\") are replaced by indefinite pronouns (\"kādreiz, kāds, kaut kas\") more in line with the English \"ever, any\" indefinite pronoun structures.\n\nDouble or multiple negatives are grammatically required in Hungarian with negative pronouns: \"Nincs semmim\" (word for word: \"[doesn't-exists] [nothing-of-mine]\", and translates literally as \"I do not have nothing\") means \"I do not have anything\". Negative pronouns are constructed by means of adding the prefixes \"se-,\" \"sem-,\" and \"sen-\" to interrogative pronouns.\n\nSomething superficially resembling double negation is required also in Finnish, which uses the auxiliary verb \"ei\" to express negation. Negative pronouns are constructed by adding one of the suffixes \"-an,\" \"-än,\" \"-kaan,\" or \"-kään\" to interrogative pronouns: \"Kukaan ei soittanut minulle\" means \"No one called me\". These suffices are, however, never used alone, but always in connection with \"ei\". This phenomenon is commonplace in Finnish, where many words have alternatives that are required in negative expressions, for example \"edes\" for \"jopa\" (\"even\"), as in \"jopa niin paljon\" meaning \"even so much\", and \"ei edes niin paljoa\" meaning \"not even so much\".\n\nNegative verb forms are grammatically required in Turkish phrases with negative pronouns or adverbs that impart a negative meaning on the whole phrase. For example, \"Hiçbir şeyim yok\" (literally, word for word, \"Not-one thing-of-mine exists-not\") means \"I don't have anything\". Likewise, \"Asla memnun değilim\" (literally, \"Never satisfied not-I-am\") means \"I'm never satisfied\".\n\nJapanese employs litotes to phrase ideas in a more indirect and polite manner. Thus, one can indicate necessity by emphasizing that not doing something would not be proper. For instance, しなければならない (\"shinakereba naranai\", \"must\") literally means \"not doing [it] would not be proper\". しなければいけません (\"shinakereba ikemasen\", also \"must\") similarly means \"not doing [it] cannot go forward\".\n\nOf course, indirectness can also be employed to put an edge on one's rudeness as well. \"He has studied Japanese, so he should be able to write kanji\" can be phrased 彼は日本語を勉強したから漢字で書けないわけがありません (\"kare wa nihongo o benkyō shita kara kanji de kakenai wake ga arimasen\"), there is a rather harsher idea: \"As he has studied Japanese, the reasoning that he cannot write Kanji does not exist\".\n\nMandarin Chinese also employs litotes in a like manner. One common construction is 不得不 (Pinyin: \"bùdébù\", \"cannot not\"), which is used to express (or feign) a necessity more regretful and polite than that expressed by 必须 (\"bìxū\", \"must\"). Compared with \"我必须走\" (\"Wǒ bìxū zǒu\", \"I must go\"), \"我不得不走\" (\"Wǒ bùdébù zǒu\", \"I cannot not go\") tries to emphasize that the situation is out of the speaker's hands and that the speaker has no choice in the matter: \"Unfortunately, I have got to go\". Similarly, \"没有人不知道\" (\"Méiyǒu rén bù zhīdào\", \"There is not a person who does not know\") is a more emphatic way to express \"Everyone knows\".\n\nDouble negatives nearly always resolve to a positive meaning even in colloquial speech, while triple negatives resolve to a negative meaning. For example, \"我不相信没人不来\" (\"Wǒ bù xiāngxìn méi rén bù lái\", \"I do not believe no one will not come\") means \"I do not think everyone will come\". However, triple or multiple negatives are considered obscure and are typically avoided.\n\nMany languages, including all living Germanic languages, French, Welsh and some Berber and Arabic dialects, have gone through a process known as Jespersen's cycle, where an original negative particle is replaced by another, passing through an intermediate stage employing two particles (e.g. Old French \"jeo ne dis\" → Modern Standard French \"je ne dis pas\" → Modern Colloquial French \"je dis pas\" \"I don't say\").\n\nIn many cases, the original sense of the new negative particle is not negative \"per se\" (thus in French \"pas\" \"step\", originally \"not a step\" = \"not a bit\"), but in Germanic languages, such as English and German the intermediate stage was a case of double negation, as the current negatives \"not\" and \"nicht\" in these languages originally meant \"nothing\": e.g. Old English \"ic ne seah\" \"I didn't see\" » Middle English \"I ne saugh nawiht\", lit. \"I didn't see nothing\" » Early Modern English \"I saw not\".\n\nA similar development to a circumfix from double negation can be seen in non-Indo-European languages, too: for example, in Maltese, \"kiel\" \"he ate\" is negated as \"ma kielx\" \"he did not eat\", where the verb is preceded by a negative particle \"ma\"- \"not\" and followed by the particle -\"x\", which was originally a shortened form of \"xejn\" \"nothing\" - thus, \"he didn't eat nothing\".\n\n"}
{"id": "335910", "url": "https://en.wikipedia.org/wiki?curid=335910", "title": "Eight-circuit model of consciousness", "text": "Eight-circuit model of consciousness\n\nThe Eight-Circuit Model of Consciousness is a hypothesis by Timothy Leary, and later expanded on by Robert Anton Wilson and Antero Alli, that \"suggested eight periods [circuits] and twenty-four stages of neurological evolution\". The eight circuits, or eight \"brains\" as referred by other authors, operate within the human nervous system, each corresponding to its own imprint and direct experience of reality. Leary and Alli include three stages for each circuit that details developmental points for each level of consciousness.\nThe first four circuits deal with life on earth, and survival of the species. The last four circuits are post-terrestrial, and deal with the evolution of the species, altered states of consciousness, enlightenment, mystical experiences, psychedelic states of mind, and psychic abilities. The proposal suggests that these altered states of consciousness are recently realized, but not widely utilized. Leary describes the first four as \"larval circuits\", necessary for surviving and functioning in a terrestrial human society, and proposed that the post terrestrial circuits will be useful for future humans who, through a predetermined script, continue to act on their urge to migrate to outer space and live extra-terrestrially. Leary, Wilson, and Alli have written about the idea in depth, and have explored and attempted to define how each circuit operates, both in the lives of individual people and in societies and civilization.\n\nThe term \"circuit\" is equated to a metaphor of the brain being computer hardware, and that the wiring of the brain as circuitry.\n\nLeary uses the eight circuits along with recapitulation theory to explain the evolution of the human species, the personal development of an individual, and the biological evolution of all life.\n\nEach circuit listed has each name from Leary's book \"Exo-Psychology\" after the preface, and Wilson's book \"Quantum Psychology\" pgs.196-201. \"Note:In other books from Leary, Wilson, and Alli, the eight circuits have different names due to different interpretations and findings of each author. Please reference bibliography section for other works on labeling of each circuit.\"\n\nThis circuit is concerned with nourishment, physical safety, comfort and survival, suckling, cuddling, etc. It begins with one spatial dimension, forward/back.\n\nThis circuit is imprinted early in infancy. The imprint will normally last for life unless it is re-imprinted by a powerful experience. Depending on the nature of the imprint, the organism will tend towards one of two basic attitudes:\n\nThis circuit is said to have appeared in the earliest evolution of the invertebrate brain and corresponds to the reptilian brain of triune brain theory. This circuit operates in essentially the same way across mammals, reptiles, fish, primates and humans. \n\nRobert Anton Wilson equated this circuit with the oral stage in the Freudian theory of psychosexual development, and proposed that this circuit is activated in adults by strong opioids.\n\nThe emotional-territorial circuit is imprinted in the toddler stage. It is concerned with domination and submission, territoriality, etc.\n\nThe imprint on this circuit will trigger one of two states:\n\nThis circuit is activated by depressant drugs such as alcohol, barbiturates, and benzodiazepines. This circuit appeared first in territorial vertebrate animals and is preserved across all mammals. It corresponds to the mammalian brain of triune brain theory. Robert Anton Wilson equated this circuit with the anal stage in the Freudian theory of psycho-sexual development. This circuit introduces a 2nd spatial dimension; up/down.\n\nThe first and second circuits both imprint in a binary fashion: trust/suspicion and dominance/submission. Thus there are four possible ways of imprinting the first two circuits:\n\n\nThis circuit is imprinted by human symbol systems. It is concerned with language, handling the environment, invention, calculation, prediction, building a mental \"map\" of the universe, physical dexterity, etc.\n\nThis circuit is activated by stimulant drugs such as amphetamines, cathinones, cocaine, and caffeine. This circuit supposedly appeared first when hominids started differentiating from the rest of the primates.\n\nRobert Anton Wilson, being heavily influenced by General Semantics, writes of this circuit as the 'time-binding circuit'. This means that this circuit's contents – including human know-how, technology, science etc. - are preserved memetically and passed on from generation to generation, constantly mutating and increasing in sophistication.\n\nThis fourth circuit is imprinted by the first orgasm-mating experiences and tribal \"morals\". It is concerned with sexual pleasure (instead of sexual reproduction), local definitions of \"moral\" and \"immoral\", reproduction, rearing of the young, etc. The fourth circuit concerns itself with cultural values and operating within social networks. This circuit is said to have first appeared with the development of tribes. Some have pointed out that entactogens such as MDMA seem to meet some of the requirements needed to activate this circuit.\n\nThis is concerned with neurological-somatic feedbacks, feeling high and blissful, somatic reprogramming, etc. It may be called the rapture circuit.\n\nWhen this circuit is activated, a non-conceptual feeling of well-being arises. This has a beneficial effect on the health of the physical body.\n\nThe fifth circuit is consciousness of the body. There is a marked shift from linear visual space to an all-encompassing aesthetic sensory space. Perceptions are judged not so much for their meaning and utility, but for their aesthetic qualities. Experience of this circuit often accompanies an hedonistic turn-on, a rapturous amusement, a detachment from the previously compulsive mechanism of the first four circuits.\n\nThis circuit is activated by ecstatic experiences via physiological effects of cannabis, Hatha Yoga, tantra and Zen meditation. Robert Anton Wilson writes, \"Tantra yoga is concerned with shifting consciousness entirely into this circuit\" and that \"Prolonged sexual play without orgasm always triggers some Circuit V consciousness\".\n\nLeary describes that this circuit first appeared in the upper classes, with the development of leisure-class civilizations around 2000 BC.\n\n\"Note: Timothy Leary lists this circuit as the sixth, and the neurogenetic circuit as the seventh. In \"Prometheus Rising\", Robert Anton Wilson reversed the order of these two circuits, describing the neurogenetic circuit as the sixth circuit, and the metaprogramming circuit as the seventh. In the subsequently published \"Quantum Psychology\", he reverted this back to the order proposed by Leary.\"\n\nThis circuit is concerned with re-imprinting and re-programming all earlier circuits and the relativity of \"realities\" perceived. The sixth circuit consists of the nervous system becoming aware of itself. Leary says this circuit enables telepathic communication and is activated by low-to-moderate doses of LSD (50-150 µg), moderate doses of peyote, psilocybin mushrooms and meditation/chanting especially when used in a group or ritual setting. This circuit is traced by Leary back to 500 BC.\n\nThis circuit is the connection of the individual's mind to the whole sweep of evolution and life as a whole. It is the part of consciousness that echoes the experiences of the previous generations that have brought the individual's brain-mind to its present level.\n\nIt deals with ancestral, societal and scientific DNA-RNA-brain feedbacks. Those who achieve this mutation may speak of past lives, reincarnation, immortality etc. It corresponds to the collective unconscious in the models of Carl Jung where archetypes reside.\n\nActivation of this circuit may be equated with consciousness of the Great God Pan in his aspect as Life as a whole, or with consciousness of Gaia, the biosphere considered as a single organism.\n\nThis circuit is activated by higher doses of LSD (200-500 µg), higher doses of peyote, higher doses of psilocybin mushrooms, yoga and meditation.\n\nThe circuit first appeared among the Hindus in the early first millennium and later reappeared among the Sufi sects.\n\nThe eighth circuit is concerned with quantum consciousness, non-local awareness (information from beyond ordinary space-time awareness which is limited by the speed of light), illumination. Some of the ways this circuit can get activated are: the awakening of kundalini, shock, a near-death experience, DMT, high doses of LSD and according to Robert Anton Wilson almost any dose of ketamine. This circuit has even been compared to the Buddhist concept of Indra's net from the Avatamsaka Sutra.\n\nLeary stated \"They[The theories presented in \"Info-Psychology\"] are scientific in that they are based on empirical findings from physics, physiology, pharmacology, genetics, astronomy, behavioral psychology, information science, and most importantly, neurology.\" \n\nLeary called his book \"science faction\" or \"psi-phy\" and noted he had written it \"in various prisons to which the author had been sentenced for dangerous ideology and violations of Newtonian and religious laws\".\n\nAlthough Leary propounded the basic premise of eight \"brains\" or brain circuits, he was inspired by sources such as the Hindu \"chakra\" system.\n\nLeary claimed that among other things this model explained the social conflict in the 1960s, where the mainstream was said to be those with four circuits active and characterized by Leary as tribal moralists and clashed with the counter-culturists, who were then said to be those with the fifth circuit active and characterized as individualists and hedonists. \n\nLeary's first book on the subject, \"Neurologic\", only included seven circuits when it was published in 1973. \"Exo-Psychology\", published in 1977, expanded the number of circuits to eight and clarified the subject. In it, he puts forward the theory that the later four circuits are \"post terrestrial;\" intended to develop as we migrate off this planet and colonize others. Once we begin space migration, according to Leary, we will have more ready access to these higher circuits. \"Exo-Psychology\" was re-published as revised by Timothy Leary with additional material in 1989 under the title \"Info-Psychology\" (New Falcon Publishing).\n\nLeary's ideas heavily influenced the work of Robert Anton Wilson. Wilson's book \"Prometheus Rising\" is an in-depth work documenting Leary's eight-circuit model of consciousness. Wilson's published screenplay \"Reality Is What You Can Get Away With\" uses and explains the model. Wilson, like Leary, wrote about the distinction between terrestrial and post-terrestrial life.\n\n\"Angel Tech\" by Antero Alli, is structured around the Eight-circuit model of consciousness. Alli defines the word angel as \"a being of light\" and tech from the word \"techne\" meaning \"art\". The title is defined as \"the art of being light\". It includes suggested activities such as meditations and construction of tarot-card collages associated with each circuit and imprint.\n\nThe model is fairly prominent in chaos magic. This concept has been detailed in \"Chaotopia!\" by Dave Lee, a leading member of the magic society Illuminates of Thanateros. Leary and Wilson were also members of the society. \n\nRolf Von Eckartsberg also appears to have been influenced by the model.\n\n\n\n"}
{"id": "857235", "url": "https://en.wikipedia.org/wiki?curid=857235", "title": "Equivalence principle", "text": "Equivalence principle\n\nIn the theory of general relativity, the equivalence principle is the equivalence of gravitational and inertial mass, and Albert Einstein's observation that the gravitational \"force\" as experienced locally while standing on a massive body (such as the Earth) is the same as the \"pseudo-force\" experienced by an observer in a non-inertial (accelerated) frame of reference.\n\nSomething like the equivalence principle emerged in the early 17th century, when Galileo expressed experimentally that the acceleration of a test mass due to gravitation is independent of the amount of mass being accelerated.\n\nKepler, using Galileo's discoveries, showed knowledge of the equivalence principle by accurately describing what would occur if the moon were stopped in its orbit and dropped towards Earth. This can be deduced without knowing if or in what manner gravity decreases with distance, but requires assuming the equivalency between gravity and inertia.\n\nThe 1/54 ratio is Kepler's estimate of the Moon–Earth mass ratio, based on their diameters. The accuracy of his statement can be deduced by using Newton's inertia law F=ma and Galileo's gravitational observation that distance formula_1. Setting these accelerations equal for a mass is the equivalence principle. Noting the time to collision for each mass is the same gives Kepler's statement that D/D=M/M, without knowing the time to collision or how or if the acceleration force from gravity is a function of distance.\n\nNewton's gravitational theory simplified and formalized Galileo's and Kepler's ideas by recognizing Kepler's \"animal force or some other equivalent\" beyond gravity and inertia were not needed, deducing from Kepler's planetary laws how gravity reduces with distance.\n\nThe equivalence principle was properly introduced by Albert Einstein in 1907, when he observed that the acceleration of bodies towards the center of the Earth at a rate of 1\"\"g\"\" (\"g\" = 9.81 m/s being a standard reference of gravitational acceleration at the Earth's surface) is equivalent to the acceleration of an inertially moving body that would be observed on a rocket in free space being accelerated at a rate of 1\"g\". Einstein stated it thus:\n\nThat is, being on the surface of the Earth is equivalent to being inside a spaceship (far from any sources of gravity) that is being accelerated by its engines. The direction or vector of acceleration equivalence on the surface of the earth is \"up\" or directly opposite the center of the planet while the vector of acceleration in a spaceship is directly opposite from the mass ejected by its thrusters. From this principle, Einstein deduced that free-fall is inertial motion. Objects in free-fall do not experience being accelerated downward (e.g. toward the earth or other massive body) but rather weightlessness and no acceleration. In an inertial frame of reference bodies (and photons, or light) obey Newton's first law, moving at constant velocity in straight lines. Analogously, in a curved spacetime the world line of an inertial particle or pulse of light is \"as straight as possible\" (in space \"and\" time). Such a world line is called a geodesic and from the point of view of the inertial frame is a straight line. This is why an accelerometer in free-fall doesn't register any acceleration; there isn't any.\n\nAs an example: an inertial body moving along a geodesic through space can be trapped into an orbit around a large gravitational mass without ever experiencing acceleration. This is possible because spacetime is radically curved in close vicinity to a large gravitational mass. In such a situation the geodesic lines bend inward around the center of the mass and a free-floating (weightless) inertial body will simply follow those curved geodesics into an elliptical orbit. An accelerometer on-board would never record any acceleration.\n\nBy contrast, in Newtonian mechanics, gravity is assumed to be a force. This force draws objects having mass towards the center of any massive body. At the Earth's surface, the force of gravity is counteracted by the mechanical (physical) resistance of the Earth's surface. So in Newtonian physics, a person at rest on the surface of a (non-rotating) massive object is in an inertial frame of reference. These considerations suggest the following corollary to the equivalence principle, which Einstein formulated precisely in 1911:\n\nEinstein also referred to two reference frames, K and K'. K is a uniform gravitational field, whereas K' has no gravitational field but is uniformly accelerated such that objects in the two frames experience identical forces:\n\nThis observation was the start of a process that culminated in general relativity. Einstein suggested that it should be elevated to the status of a general principle, which he called the \"principle of equivalence\" when constructing his theory of relativity:\n\nEinstein combined (postulated) the equivalence principle with special relativity to predict that clocks run at different rates in a gravitational potential, and light rays bend in a gravitational field, even before he developed the concept of curved spacetime.\n\nSo the original equivalence principle, as described by Einstein, concluded that free-fall and inertial motion were physically equivalent. This form of the equivalence principle can be stated as follows. An observer in a windowless room cannot distinguish between being on the surface of the Earth, and being in a spaceship in deep space accelerating at 1g. This is not strictly true, because massive bodies give rise to tidal effects (caused by variations in the strength and direction of the gravitational field) which are absent from an accelerating spaceship in deep space. The room, therefore, should be small enough that tidal effects can be neglected.\n\nAlthough the equivalence principle guided the development of general relativity, it is not a founding principle of relativity but rather a simple consequence of the \"geometrical\" nature of the theory. In general relativity, objects in free-fall follow geodesics of spacetime, and what we perceive as the force of gravity is instead a result of our being unable to follow those geodesics of spacetime, because the mechanical resistance of matter prevents us from doing so.\n\nSince Einstein developed general relativity, there was a need to develop a framework to test the theory against other possible theories of gravity compatible with special relativity. This was developed by Robert Dicke as part of his program to test general relativity. Two new principles were suggested, the so-called Einstein equivalence principle and the strong equivalence principle, each of which assumes the weak equivalence principle as a starting point. They only differ in whether or not they apply to gravitational experiments.\n\nAnother clarification needed is that the equivalence principle assumes a constant acceleration of 1g without considering the mechanics of generating 1g. If we do consider the mechanics of it, then we must assume the aforementioned windowless room has a fixed mass. Accelerating it at 1g means there is a constant force being applied, which = m*g where m is the mass of the windowless room along with its contents (including the observer). Now, if the observer jumps inside the room, an object lying freely on the floor will decrease in weight momentarily because the acceleration is going to decrease momentarily due to the observer pushing back against the floor in order to jump. The object will then gain weight while the observer is in the air and the resulting decreased mass of the windowless room allows greater acceleration; it will lose weight again when the observer lands and pushes once more against the floor; and it will finally return to its initial weight afterwards. To make all these effects equal those we would measure on a planet producing 1g, the windowless room must be assumed to have the same mass as that planet. Additionally, the windowless room must not cause its own gravity, otherwise the scenario changes even further. These are technicalities, clearly, but practical ones if we wish the experiment to demonstrate more or less precisely the equivalence of 1g gravity and 1g acceleration.\n\nThree forms of the equivalence principle are in current use: weak (Galilean), Einsteinian, and strong.\n\nThe weak equivalence principle, also known as the universality of free fall or the Galilean equivalence principle can be stated in many ways. The strong EP includes (astronomic) bodies with gravitational binding energy (e.g., 1.74 solar-mass pulsar PSR J1903+0327, 15.3% of whose separated mass is absent as gravitational binding energy). The weak EP assumes falling bodies are bound by non-gravitational forces only. Either way:\n\nLocality eliminates measurable tidal forces originating from a radial divergent gravitational field (e.g., the Earth) upon finite sized physical bodies. The \"falling\" equivalence principle embraces Galileo's, Newton's, and Einstein's conceptualization. The equivalence principle does not deny the existence of measurable effects caused by a \"rotating\" gravitating mass (frame dragging), or bear on the measurements of light deflection and gravitational time delay made by non-local observers.\n\nBy definition of active and passive gravitational mass, the force on formula_2 due to the gravitational field of formula_3 is:\n\nLikewise the force on a second object of arbitrary mass due to the gravitational field of mass is:\n\nBy definition of inertial mass:\n\nIf formula_7 and formula_8 are the same distance formula_9 from formula_10 then, by the weak equivalence principle, they fall at the same rate (i.e. their accelerations are the same)\n\nHence:\n\nTherefore:\n\nIn other words, passive gravitational mass must be proportional to inertial mass for all objects.\n\nFurthermore, by Newton's third law of motion:\n\nmust be equal and opposite to\n\nIt follows that:\n\nIn other words, passive gravitational mass must be proportional to active gravitational mass for all objects.\n\nThe dimensionless Eötvös-parameter formula_17 is the difference of the ratios of gravitational and inertial masses divided by their average for the two sets of test masses \"A\" and \"B.\"\n\nTests of the weak equivalence principle are those that verify the equivalence of gravitational mass and inertial mass. An obvious test is dropping different objects, ideally in a vacuum environment, e.g., inside the Fallturm Bremen drop tower.\n\nSee:\n\nExperiments are still being performed at the University of Washington which have placed limits on the differential acceleration of objects towards the Earth, the Sun and towards dark matter in the galactic center. Future satellite experiments – STEP (Satellite Test of the Equivalence Principle), Galileo Galilei, and MICROSCOPE (MICROSatellite à traînée Compensée pour l'Observation du Principe d'Équivalence) – will test the weak equivalence principle in space, to much higher accuracy.\n\nWith the first successful production of antimatter, in particular anti-hydrogen, a new approach to test the weak equivalence principle has been proposed. Experiments to compare the gravitational behavior of matter and antimatter are currently being developed.\n\nProposals that may lead to a quantum theory of gravity such as string theory and loop quantum gravity predict violations of the weak equivalence principle because they contain many light scalar fields with long Compton wavelengths, which should generate fifth forces and variation of the fundamental constants. Heuristic arguments suggest that the magnitude of these equivalence principle violations could be in the 10 to 10 range. Currently envisioned tests of the weak equivalence principle are approaching a degree of sensitivity such that \"non-discovery\" of a violation would be just as profound a result as discovery of a violation. Non-discovery of equivalence principle violation in this range would suggest that gravity is so fundamentally different from other forces as to require a major reevaluation of current attempts to unify gravity with the other forces of nature. A positive detection, on the other hand, would provide a major guidepost towards unification.\n\nWhat is now called the \"Einstein equivalence principle\" states that the weak equivalence principle holds, and that:\nHere \"local\" has a very special meaning: not only must the experiment not look outside the laboratory, but it must also be small compared to variations in the gravitational field, tidal forces, so that the entire laboratory is freely falling. It also implies the absence of interactions with \"external\" fields \"other than the gravitational field\".\n\nThe principle of relativity implies that the outcome of local experiments must be independent of the velocity of the apparatus, so the most important consequence of this principle is the Copernican idea that dimensionless physical values such as the fine-structure constant and electron-to-proton mass ratio must not depend on where in space or time we measure them. Many physicists believe that any Lorentz invariant theory that satisfies the weak equivalence principle also satisfies the Einstein equivalence principle.\n\n\"Schiff's conjecture\" suggests that the weak equivalence principle implies the Einstein equivalence principle, but it has not been proven. Nonetheless, the two principles are tested with very different kinds of experiments. The Einstein equivalence principle has been criticized as imprecise, because there is no universally accepted way to distinguish gravitational from non-gravitational experiments (see for instance Hadley and Durand).\n\nIn addition to the tests of the weak equivalence principle, the Einstein equivalence principle can be tested by searching for variation of dimensionless constants and mass ratios. The present best limits on the variation of the fundamental constants have mainly been set by studying the naturally occurring Oklo natural nuclear fission reactor, where nuclear reactions similar to ones we observe today have been shown to have occurred underground approximately two billion years ago. These reactions are extremely sensitive to the values of the fundamental constants.\n\nThere have been a number of controversial attempts to constrain the variation of the strong interaction constant. There have been several suggestions that \"constants\" do vary on cosmological scales. The best known is the reported detection of variation (at the 10 level) of the fine-structure constant from measurements of distant quasars, see Webb et al. Other researchers dispute these findings. Other tests of the Einstein equivalence principle are gravitational redshift experiments, such as the Pound–Rebka experiment which test the position independence of experiments.\n\nThe strong equivalence principle suggests the laws of gravitation are independent of velocity and location. In particular,\nand\nThe first part is a version of the weak equivalence principle that applies to objects that exert a gravitational force on themselves, such as stars, planets, black holes or Cavendish experiments. The second part is the Einstein equivalence principle (with the same definition of \"local\"), restated to allow gravitational experiments and self-gravitating bodies. The freely-falling object or laboratory, however, must still be small, so that tidal forces may be neglected (hence \"local experiment\").\n\nThis is the only form of the equivalence principle that applies to self-gravitating objects (such as stars), which have substantial internal gravitational interactions. It requires that the gravitational constant be the same everywhere in the universe and is incompatible with a fifth force. It is much more restrictive than the Einstein equivalence principle.\n\nThe strong equivalence principle suggests that gravity is entirely geometrical by nature (that is, the metric alone determines the effect of gravity) and does not have any extra fields associated with it. If an observer measures a patch of space to be flat, then the strong equivalence principle suggests that it is absolutely equivalent to any other patch of flat space elsewhere in the universe. Einstein's theory of general relativity (including the cosmological constant) is thought to be the only theory of gravity that satisfies the strong equivalence principle. A number of alternative theories, such as Brans–Dicke theory, satisfy only the Einstein equivalence principle.\n\nThe strong equivalence principle can be tested by searching for a variation of Newton's gravitational constant \"G\" over the life of the universe, or equivalently, variation in the masses of the fundamental particles. A number of independent constraints, from orbits in the solar system and studies of big bang nucleosynthesis have shown that \"G\" cannot have varied by more than 10%.\n\nThus, the strong equivalence principle can be tested by searching for fifth forces (deviations from the gravitational force-law predicted by general relativity). These experiments typically look for failures of the inverse-square law (specifically Yukawa forces or failures of Birkhoff's theorem) behavior of gravity in the laboratory. The most accurate tests over short distances have been performed by the Eöt–Wash group. A future satellite experiment, SEE (Satellite Energy Exchange), will search for fifth forces in space and should be able to further constrain violations of the strong equivalence principle. Other limits, looking for much longer-range forces, have been placed by searching for the Nordtvedt effect, a \"polarization\" of solar system orbits that would be caused by gravitational self-energy accelerating at a different rate from normal matter. This effect has been sensitively tested by the Lunar Laser Ranging Experiment. Other tests include studying the deflection of radiation from distant radio sources by the sun, which can be accurately measured by very long baseline interferometry. Another sensitive test comes from measurements of the frequency shift of signals to and from the Cassini spacecraft. Together, these measurements have put tight limits on Brans–Dicke theory and other alternative theories of gravity.\n\nIn 2014, astronomers discovered a stellar triple system including a millisecond pulsar PSR J0337+1715 and two white dwarfs orbiting it. The system provided them a chance to test the strong equivalence principle in a strong gravitational field with high accuracy.\n\nOne challenge to the equivalence principle is the Brans–Dicke theory. Self-creation cosmology is a modification of the Brans–Dicke theory. The Fredkin Finite Nature Hypothesis is an even more radical challenge to the equivalence principle and has even fewer supporters.\n\nIn August 2010, researchers from the University of New South Wales, Swinburne University of Technology, and Cambridge University published a paper titled \"Evidence for spatial variation of the fine structure constant\", whose tentative conclusion is that, \"qualitatively, [the] results suggest a violation of the Einstein Equivalence Principle, and could infer a very large or infinite universe, within which our 'local' Hubble volume represents a tiny fraction.\"\n\nIn his book \"Einstein's Mistakes\", pages 226–227, Hans C. Ohanian describes several situations which falsify Einstein's Equivalence Principle. Inertial accelerative effects are analogous to, but not equivalent to, gravitational effects. Ohanian cites Ehrenfest for this same opinion.\n\nDutch physicist and string theorist Erik Verlinde has generated a self-contained, logical derivation of the equivalence principle based on the starting assumption of a holographic universe. Given this situation, gravity would not be a true fundamental force as is currently thought but instead an \"emergent property\" related to entropy. Verlinde's entropic gravity theory apparently leads naturally to the correct observed strength of dark energy; previous failures to explain its incredibly small magnitude have been called by such people as cosmologist Michael Turner (who is credited as having coined the term \"dark energy\") as \"the greatest embarrassment in the history of theoretical physics\". However, it should be noted that these ideas are far from settled and still very controversial.\n\n\n\n"}
{"id": "24856902", "url": "https://en.wikipedia.org/wiki?curid=24856902", "title": "Evidential existentiality", "text": "Evidential existentiality\n\nThe principle of evidential existentiality in philosophy is a principle that explains and gives value to the existence of entities. The principle states that the reality of an entity's existence gives greater value to prove its existence than would be given through any outward studies. The principle has become a backbone of the God argument, stating that because God is a self-evident entity, His existence can only be shared by humans, thus proof of God is unnecessary and moot.\nIt appears that the existence is primarily evident to the self only. The God or Supreme self is perceivable to the self. So evidentially self perception is followed by God perception and so on.\n\n"}
{"id": "15285866", "url": "https://en.wikipedia.org/wiki?curid=15285866", "title": "Experimental system", "text": "Experimental system\n\nIn scientific research, an experimental system is the physical, technical and procedural basis for an experiment or series of experiments. Historian of science Hans-Jörg Rheinberger defines an experimental system as: \"A basic unit of experimental activity combining local, technical, instrumental, institutional, social, and epistemic aspects.\" Scientists (particularly laboratory biologists) and historians and philosophers of biology have pointed to the development and spread of successful experimental systems, such as those based on popular model organism or scientific apparatus, as key elements in the history of science, particularly since the early 20th century. The choice of an appropriate experimental system is often seen as critical for a scientist's long-term success, as experimental systems can be very productive for some kinds of questions and less productive for others, acquiring a sort of momentum that takes research in unpredicted directions.\n\nA successful experimental system must be stable and reproducible enough for scientists to make sense of the system's behavior, but variable and unpredictable enough that it can produce useful results. In many cases, a well-understood experimental system can be \"black-boxed\" as a standard technique, which can then be a component of other experimental systems. Rheinberger divides experimental systems into two parts: the part under investigation (\"epistemic things\") and the well-understood part that provides a stable context for experimentation (\"technical objects\").\n\nThe development of experimental systems in biology often requires the \"domestication\" of a particular organism for the laboratory environment, including the creation of relatively homogeneous lines or strains and the tailoring of conditions to highlight the variable aspects that scientists are interested in. Scientific technologies, similarly, often require the development of a full experimental system to go from a viable concept to a technique that works in practice on a usefully consistent basis. For example, the invention of the polymerase chain reaction (PCR) is generally attributed to Kary Mullis, who came up with the concept in 1983, but the process of development of PCR into the revolutionary technology it became by the early 1990s took years of work by others at Cetus Corporation—and the basic components of the system had been known since the 1960s DNA synthesis work of Har Gobind Khorana—making \"who invented PCR?\" a complicated question.\n\n"}
{"id": "7949372", "url": "https://en.wikipedia.org/wiki?curid=7949372", "title": "Humanitarian principles", "text": "Humanitarian principles\n\nThere are a number of meanings for the term humanitarian. Here humanitarian pertains to the practice of saving lives and alleviating suffering. It is usually related to emergency response (also called humanitarian response) whether in the case of a natural disaster or a man-made disaster such as war or other armed conflict. Humanitarian principles govern the way humanitarian response is carried out.\n\nThe principle of humanity means that all humankind shall be treated humanely and equally in all circumstances by saving lives and alleviating suffering, while ensuring respect for the individual. It is the fundamental principle of humanitarian response.\n\nThe Code of Conduct for the International Red Cross and Red Crescent Movement and NGOs in Disaster Relief (RC/NGO Code) introduces the concept of the humanitarian imperative which expands the principle of humanity to include the right to receive and to give humanitarian assistance. It states the obligation of the international community \"to provide humanitarian assistance wherever it is needed.\"\n\nProvision of humanitarian assistance must be impartial and not based on nationality, race, religion, or political point of view. It must be based on need alone.\n\nFor most non-governmental humanitarian agencies (NGHAs), the principle of impartiality is unambiguous even if it is sometimes difficult to apply, especially in rapidly changing situations. However, it is no longer clear which organizations can claim to be humanitarian. For example, companies like PADCO, a USAID subcontractor, is sometimes seen as a humanitarian NGO. However, for the UN agencies, particularly where the UN is involved in peace keeping activities as the result of a Security Council resolution, it is not clear if the UN is in position to act in an impartial manner if one of the parties is in violation of terms of the UN Charter.\n\nHumanitarian agencies must formulate and implement their own policies independently of government policies or actions.\n\nProblems may arise because most NGHAs rely in varying degrees on government donors. Thus for some organizations it is difficult to maintain independence from their donors and not be confused in the field with governments who may be involved in the hostilities. The ICRC, has set the example for maintaining its independence (and neutrality) by raising its funds from governments through the use of separate annual appeals for headquarters costs and field operations.\n\nThe core principles are defining characteristics, the necessary conditions for humanitarian response. Organizations such as military forces and for-profit companies may deliver assistance to communities affected by disaster in order to save lives and alleviate suffering, but they are not considered by the humanitarian sector as humanitarian agencies as their response is not based on the core principles.\n\nIn addition to the core principles, there are other principles that govern humanitarian response for specific types of humanitarian agencies such as UN agencies, the Red Cross and Red Crescent Movement, and NGOs.\n\nThe International Red Cross and Red Crescent Movement follows, in addition to the above core principles, the principle of neutrality. For the Red Cross, neutrality means not to take sides in hostilities or engage at any time in controversies of a political, racial, religious or ideological nature.\n\nThe principle of neutrality was specifically addressed to the Red Cross Movement to prevent it from not only taking sides in a conflict, but not to \"engage at any time in controversies of a political, racial, religious or ideological nature.\" The principle of neutrality was left out of the Red Cross/NGO code because some of the NGHAs, while committed to giving impartial assistance, were not ready to forgo their lobbying on justice issues related to political and ideological questions.\n\nUnited Nations General Assembly Resolution 46/182 lists the principle of neutrality, alongside the principles of humanity and impartiality in its annex as a guide to the provision of humanitarian assistance. The resolution is designed to strengthen human response of the UN system, and it clearly applies to the UN agencies.\n\nNeutrality can also apply to humanitarian actions of a state. \"Neutrality remains closely linked with the definition which introduced the concept into international law to designate the status of a State which decided to stand apart from an armed conflict. Consequently, its applications under positive law still depend on the criteria of abstention and impartiality which have characterized neutrality from the outset.\"\n\nThe application of the word neutrality to humanitarian aid delivered by UN agencies or even governments can be confusing. GA Resolution 46/182 proclaims the principle of neutrality, yet as an inter-governmental political organization, the UN is often engaged in controversies of a political nature. According to this interpretation, the UN agency or a government can provide neutral humanitarian aid as long as it does it impartially, based upon need alone.\n\nToday, the word neutrality is widely used within the humanitarian community, usually to mean the provision of humanitarian aid in an impartial and independent manner, based on need alone. Few international NGOs have curtailed work on justice or human rights issues because of their commitment to neutrality.\n\nThe provision of aid must not exploit the vulnerability of victims and be used to further political or religious creeds. All of the major non-governmental humanitarian agencies (NGHAs) by signing up to the RC/NGO Code of Conduct have committed themselves not to use humanitarian response to further political or religious creeds.\n\nAll of the above principles are important requirements for effective field operations. They are based on widespread field experience of agencies engaged in humanitarian response. In conflict situations, their breach may drastically affect the ability of agencies to respond to the needs of the victims.\n\nIf a warring party believes, for example, that an agency is favoring the other side, or that it is an agent of the enemy, access to the victims may be blocked and the lives of humanitarian workers may be put in danger. If one of the parties perceives that an agency is trying to spread another religious faith, there may be a hostile reaction to their activities.\n\nThe core principles, found in the Red Cross/NGO Code of Conduct and in GA Resolution 46/182 are derived from the Fundamental Principles of the Red Cross, particularly principles I (humanity), II (impartiality), III (neutrality—in the case of the UN), and IV (independence).\n\nAccountability has been defined as: \"the processes through which an organisation makes a \ncommitment to respond to and balance the needs of stakeholders in its decision making processes and activities, and delivers against this commitment.\" Humanitarian Accountability Partnership International adds: \"Accountability is about using power responsibly.\"\n\nArticle 9 of the Code of Conduct for the International Red Cross and Red Crescent Movement and NGOs in Disaster Relief states:\n\"We hold ourselves accountable to both those we seek to assist and those from whom we accept resources;\" and thus identifies the two major stake holders: donors and beneficiaries. However, traditionally humanitarian agencies have tended to practice mainly \"upward accountability\", i.e. to their donors.\n\nThe experience of many humanitarian agencies during the Rwandan Genocide, led to a number of initiatives designed to improve humanitarian assistance and accountability, particularly with respect to the beneficiaries. Examples include the Sphere Project, ALNAP, Compas, the People In Aid Code of Good Practice, and the Humanitarian Accountability Partnership International, which runs a \"global quality insurance scheme for humanitarian agencies.\"\n\nThe RC/NGO Code also lists a number of more aspirational principles which are derived from experience with development assistance.\n\nThe Sphere Project Humanitarian Charter uses the language of human rights to remind that the right to life which is proclaimed in both the Universal Declaration of Human Rights and the International Convention on Civil and Political Rights is related to human dignity.\n\nHumanitarian principles are mainly focused on the behavior of organizations. However a humane response implies that humanitarian workers are not to take advantage of the vulnerabilities of those affected by war and violence. Agencies have the responsibility for developing rules of staff conduct which prevent abuse of the beneficiaries.\n\nOne of the most problematic areas is related to the issue of sexual exploitation and abuse of beneficiaries by humanitarian workers. In an emergency where victims have lost everything, women and girls are particularly vulnerable to sexual abuse.\n\nA number of reports which identified the sexual exploitation of refugees in west Africa prodded the humanitarian community to work together in examining the problem and to take measures to prevent abuses. In July 2002, the UN's Interagency Standing Committee (IASC) adopted a plan of action which stated: Sexual exploitation and abuse by humanitarian workers constitute acts of gross misconduct and are therefore grounds for termination of employment. The plan explicitly prohibited the \"Exchange of money, employment, goods, or services for sex, including sexual favours or other forms of humiliating, degrading or exploitative behaviour.\" The major NGHAs as well the UN agencies engaged in humanitarian response committed themselves to setting up internal structures to prevent sexual exploitation and abuse of beneficiaries.\n\nSubstantial efforts have been made in the humanitarian sector to monitor compliance with humanitarian principles. Such efforts include The People In Aid Code of Good Practice, an internationally recognised management tool that helps humanitarian and development organisations enhance the quality of their human resources management. The NGO, Humanitarian Accountability Partnership International, is also working to make humanitarian organizations more accountable, especially to the beneficiaries.\n\nStructures internal to the Red Cross Movement monitor compliance to the Fundamental Principles of the Red Cross.\n\nThe RC/NGO Code is self-enforcing. The SCHR carries out peer reviews among its members which look in part at the issue of compliance with principles set out in the RC/NGO Code\n\n"}
{"id": "161999", "url": "https://en.wikipedia.org/wiki?curid=161999", "title": "Idea", "text": "Idea\n\nIn philosophy, ideas are usually taken as mental representational images of some object. Ideas can also be abstract concepts that do not present as mental images. Many philosophers have considered ideas to be a fundamental ontological category of being. The capacity to create and understand the meaning of ideas is considered to be an essential and defining feature of human beings. In a popular sense, an idea arises in a reflexive, spontaneous manner, even without thinking or serious reflection, for example, when we talk about the \"idea\" of a person or a place. A new or original idea can often lead to innovation.\n\nThe word \"idea\" comes from Greek ἰδέα \"idea\" \"form, pattern,\" from the root of ἰδεῖν \"idein\", \"to see.\" \n\nOne view on the nature of ideas is that there exist some ideas (called \"innate ideas\") which are so general and abstract that they could not have arisen as a representation of an object of our perception but rather were in some sense always present. These are distinguished from \"adventitious ideas\" which are images or concepts which are accompanied by the judgment that they are caused or occasioned by an external object.\n\nAnother view holds that we only discover ideas in the same way that we discover the real world, from personal experiences. The view that humans acquire all or almost all their behavioral traits from nurture (life experiences) is known as \"tabula rasa\" (\"blank slate\"). Most of the confusions in the way ideas arise is at least in part due to the use of the term \"idea\" to cover both the representation perceptics and the object of conceptual thought. This can be always illustrated in terms of the scientific doctrines of innate ideas, \"concrete ideas versus abstract ideas\", as well as \"simple ideas versus complex ideas\".\n\nPlato in Ancient Greece was one of the earliest philosophers to provide a detailed discussion of ideas and of the thinking process (it must be noted that in Plato's Greek the word \"idea\" carries a rather different sense from our modern English term). Plato argued in dialogues such as the \"Phaedo\", \"Symposium\", \"Republic\", and \"Timaeus\" that there is a realm of ideas or forms (\"eidei\"), which exist independently of anyone who may have thoughts on these ideas, and it is the ideas which distinguish mere opinion from knowledge, for unlike material things which are transient and liable to contrary properties, ideas are unchanging and nothing but just what they are. Consequently, Plato seems to assert forcefully that material things can only be the objects of opinion; real knowledge can only be had of unchanging ideas. Furthermore, ideas for Plato appear to serve as universals; consider the following passage from the \"Republic\":\nDescartes often wrote of the meaning of \"idea\" as an image or representation, often but not necessarily \"in the mind\", which was well known in the vernacular. Despite that Descartes is usually credited with the invention of the non-Platonic use of the term, he at first followed this vernacular use. In his \"Meditations on First Philosophy\" he says, \"Some of my thoughts are like images of things, and it is to these alone that the name 'idea' properly belongs.\" He sometimes maintained that ideas were innate and uses of the term \"idea\" diverge from the original primary scholastic use. He provides multiple non-equivalent definitions of the term, uses it to refer to as many as six distinct kinds of entities, and divides \"ideas\" inconsistently into various genetic categories. For him knowledge took the form of ideas and philosophical investigation is the deep consideration of these entities.\n\nIn striking contrast to Plato's use of idea is that of John Locke. In his Introduction to An Essay Concerning Human Understanding, Locke defines \"idea\" as \"that term which, I think, serves best to stand for whatsoever is the object of the understanding when a man thinks, I have used it to express whatever is meant by phantasm, notion, species, or whatever it is which the mind can be employed about in thinking; and I could not avoid frequently using it.\" He said he regarded the book necessary to examine our own abilities and see what objects our understandings were, or were not, fitted to deal with. In his philosophy other outstanding figures followed in his footsteps — Hume and Kant in the 18th century, Arthur Schopenhauer in the 19th century, and Bertrand Russell, Ludwig Wittgenstein, and Karl Popper in the 20th century. Locke always believed in \"good sense\" — not pushing things to extremes and on taking fully into account the plain facts of the matter. He considered his common-sense ideas \"good-tempered, moderate, and down-to-earth.\"\n\nAs John Locke studied humans in his work “An Essay Concerning Human Understanding” he continually referenced Descartes for ideas as he asked this fundamental question: “When we are concerned with something about which we have no certain knowledge, what rules or standards should guide how confident we allow ourselves to be that our opinions are right?” A simpler way of putting it is how do humans know ideas, and what are the different types of ideas. An idea to Locke “can simply mean some sort of brute experience.” He shows that there are “No innate principles in the mind.”. Thus, he concludes that “our ideas are all experiential in nature.” An experience can either be a sensation or a reflection: “consider whether there are any innate ideas in the mind before any are brought in by the impression from sensation or reflection.” Therefore, an idea was an experience in which the human mind apprehended something.\n\nIn a Lockean view, there are really two types of ideas: complex and simple. Simple ideas are the building blocks for much more complex ideas, and “While the mind is wholly passive in the reception of simple ideas, it is very active in the building of complex ideas…” Complex ideas, therefore, can either be modes, substances, or relations. Modes are when ideas are combined in order to convey new information. For instance, David Banach gives the example of beauty as a mode. He says that it is the combination of color and form. Substances, however, is different. Substances are certain objects, that can either be dogs, cats, or tables. And relations represent the relationship between two or more ideas. In this way, Locke did, in fact, answer his own questions about ideas and humans.\n\nHume differs from Locke by limiting \"idea\" to the more or less vague mental reconstructions of perceptions, the perceptual process being described as an \"impression.\" Hume shared with Locke the basic empiricist premise that it is only from life experiences (whether their own or others') that humans' knowledge of the existence of anything outside of themselves can be ultimately derived, that they shall carry on doing what they are prompted to do by their emotional drives of varying kinds. In choosing the means to those ends, they shall follow their accustomed associations of ideas. Hume has contended and defended the notion that \"reason alone is merely the 'slave of the passions'.\" \n\nImmanuel Kant defines an \"idea\" as opposed to a \"concept\". \"Regulative ideas\" are ideals that one must tend towards, but by definition may not be completely realized. Liberty, according to Kant, is an idea. The autonomy of the rational and universal subject is opposed to the determinism of the empirical subject. Kant felt that it is precisely in knowing its limits that philosophy exists. The business of philosophy he thought was not to give rules, but to analyze the private judgements of good common sense.\n\nWhereas Kant declares limits to knowledge (\"we can never know the thing in itself\"), in his epistemological work, Rudolf Steiner sees \"ideas\" as \"objects of experience\" which the mind apprehends, much as the eye apprehends light. In \"Goethean Science\" (1883), he declares, \"Thinking ... is no more and no less an organ of perception than the eye or ear. Just as the eye perceives colors and the ear sounds, so thinking perceives ideas.\" He holds this to be the premise upon which Goethe made his natural-scientific observations.\n\nWundt widens the term from Kant's usage to include \"conscious representation of some object or process of the external world\". In so doing, he includes not only ideas of memory and imagination, but also perceptual processes, whereas other psychologists confine the term to the first two groups. One of Wundt's main concerns was to investigate conscious processes in their own context by experiment and introspection. He regarded both of these as \"exact methods\", interrelated in that experimentation created optimal conditions for introspection. Where the experimental method failed, he turned to other \"objectively valuable aids\", specifically to \"those products of cultural communal life which lead one to infer particular mental motives. Outstanding among these are speech, myth, and social custom.\" Wundt designed the basic mental activity apperception — a unifying function which should be understood as an activity of the will. Many aspects of his empirical physiological psychology are used today. One is his principles of mutually enhanced contrasts and of assimilation and dissimilation (i.e. in color and form perception and his advocacy of \"objective\" methods of expression and of recording results, especially in language. Another is the principle of heterogony of ends — that multiply motivated acts lead to unintended side effects which in turn become motives for new actions.\n\nC. S. Peirce published the first full statement of pragmatism in his important works \"\" (1878) and \"\" (1877). In \"How to Make Our Ideas Clear\" he proposed that a \"clear idea\" (in his study he uses concept and \"idea\" as synonymic) is defined as one, when it is apprehended such as it will be recognized wherever it is met, and no other will be mistaken for it. If it fails of this clearness, it is said to be obscure. He argued that to understand an idea clearly we should ask ourselves what difference its application would make to our evaluation of a proposed solution to the problem at hand. Pragmatism (a term he appropriated for use in this context), he defended, was a method for ascertaining the meaning of terms (as a theory of meaning). The originality of his ideas is in their rejection of what was accepted as a view and understanding of knowledge by scientists for some 250 years, i.e. that, he pointed, knowledge was an impersonal fact. Peirce contended that we acquire knowledge as \"participants\", not as \"spectators\". He felt \"the real\", sooner or later, is information acquired through ideas and knowledge with the application of logical reasoning would finally result in. He also published many papers on logic in relation to \"ideas\".\n\nG. F. Stout and J. M. Baldwin, in the \"Dictionary of Philosophy and Psychology\", define \"idea\" as \"the reproduction with a more or less adequate image, of an object not actually present to the senses.\" They point out that an idea and a perception are by various authorities contrasted in various ways. \"Difference in degree of intensity\", \"comparative absence of bodily movement on the part of the subject\", \"comparative dependence on mental activity\", are suggested by psychologists as characteristic of an idea as compared with a perception.\n\nIt should be observed that an idea, in the narrower and generally accepted sense of a mental reproduction, is frequently composite. That is, as in the example given above of the idea of a chair, a great many objects, differing materially in detail, all call a single idea. When a man, for example, has obtained an idea of chairs in general by comparison with which he can say \"This is a chair, that is a stool\", he has what is known as an \"abstract idea\" distinct from the reproduction in his mind of any particular chair (see abstraction). Furthermore, a complex idea may not have any corresponding physical object, though its particular constituent elements may severally be the reproductions of actual perceptions. Thus the idea of a centaur is a complex mental picture composed of the ideas of man and horse, that of a mermaid of a woman and a fish.\n\nDiffusion studies explore the spread of ideas from culture to culture. Some anthropological theories hold that all cultures imitate ideas from one or a few original cultures, the Adam of the Bible, or several cultural circles that overlap. Evolutionary diffusion theory holds that cultures are influenced by one another but that similar ideas can be developed in isolation.\n\nIn the mid-20th century, social scientists began to study how and why ideas spread from one person or culture to another. Everett Rogers pioneered diffusion of innovations studies, using research to prove factors in adoption and profiles of adopters of ideas. In 1976, in his book \"The Selfish Gene\", Richard Dawkins suggested applying biological evolutionary theories to the spread of ideas. He coined the term \"meme\" to describe an abstract unit of selection, equivalent to the gene in evolutionary biology.\n\nJames Boswell recorded Samuel Johnson's opinion about ideas. Johnson claimed that they are mental images or internal visual pictures. As such, they have no relation to words or the concepts which are designated by verbal names.\n\nTo protect the cause of invention and innovation, the legal constructions of Copyrights and Patents were established. Patent law regulates various aspects related to the functional manifestation of inventions based on new ideas or incremental improvements to existing ones. Thus, patents have a direct relationship to ideas.\n\nIn some cases, authors can be granted limited legal monopolies on the manner in which certain works are expressed. This is known colloquially as copyright, although the term intellectual property is used mistakenly in place of \"copyright\". Copyright law regulating the aforementioned monopolies generally does not cover the actual ideas. The law does not bestow the legal status of property upon ideas per se. Instead, laws purport to regulate events related to the usage, copying, production, sale and other forms of exploitation of the fundamental expression of a work, that may or may not carry ideas. Copyright law is fundamentally different from patent law in this respect: patents do grant monopolies on ideas (more on this below).\n\nA copyright is meant to regulate some aspects of the usage of expressions of a work, \"not\" an idea. Thus, copyrights have a negative relationship to ideas.\n\nWork means a tangible medium of expression. It may be an original or derivative work of art, be it literary, dramatic, musical recitation, artistic, related to sound recording, etc. In (at least) countries adhering to the Berne Convention, copyright automatically starts covering the work upon the original creation and fixation thereof, without any extra steps. While creation usually involves an idea, the idea in itself does not suffice for the purposes of claiming copyright. \nConfidentiality and nondisclosure agreements are legal instruments that assist corporations and individuals in keeping ideas from escaping to the general public. Generally, these instruments are covered by contract law.\n\n\n"}
{"id": "7051566", "url": "https://en.wikipedia.org/wiki?curid=7051566", "title": "Idios kosmos", "text": "Idios kosmos\n\nIdios kosmos comes from Greek and means private world. It exists with, and is opposite to, koinos kosmos (shared world). \"Idios kosmos\" is the view of the world that is developed from personal experience and knowledge and is therefore unique; however, it can be difficult to tell the difference between it and \"koinos kosmos\".\n\nThe two phrases come from the Diels-Kranz fragment B89 of Heraclitus: ὁ Ἡράκλειτός φησι τοῖς ἐγρηγορόσιν ἕνα καὶ κοινὸν κόσμον εἶναι τῶν δὲ κοιμωμένων ἕκαστον εἰς ἴδιον ἀποστρέφεσθαι (\"Heraclitus said that the waking have one common world, but the sleeping turn aside each into a world of his own.\")\n\nThe idea of \"idios kosmos\" is an important part of Philip K. Dick's views on schizophrenia, as expressed in his 1964 essay \"Schizophrenia & 'The Book of Changes'\", drawing on personal experience with the I Ching.\n\n\n"}
{"id": "23276654", "url": "https://en.wikipedia.org/wiki?curid=23276654", "title": "Infinite qualitative distinction", "text": "Infinite qualitative distinction\n\nThe infinite qualitative distinction (; ), sometimes translated as infinite qualitative difference, is a concept coined by the Danish philosopher Søren Kierkegaard. The distinction emphasizes the very different attributes of finite and temporal men and the infinite and eternal qualities of a supreme being. This concept fits into the apophatic theology tradition and therefore is fundamentally at odds with theological theories which posit a supreme being able to be fully understood by man. The theologian Karl Barth made the concept of infinite qualitative distinction a cornerstone of his theology.\n\nFor Kierkegaard, direct communication with God is impossible, as the idea of God and man are infinitely different. He argues that indirect communication with God is the only way of communication. For example, in Christian belief, the Incarnation posits that Jesus Christ is God incarnate. The infinite qualitative distinction is opposed to rational theology in the sense that, whereas the latter argues one can prove empirically Jesus is God incarnate, the former argues that empirical evidence is ultimately insufficient in making that conclusion. The paradoxical nature of the Incarnation, that God is embodied in a man, is offensive to reason, and can only be comprehended indirectly, through faith.\n\nBarth's book \"The Epistle to the Romans\" also emphasizes such a gulf. In the preface to the Second Edition of his commentary, Barth writes, \"if I have a system, it is limited to a recognition of what Kierkegaard called the 'infinite qualitative distinction' between time and eternity, and to my regarding this as possessing negative as well as positive significance: 'God is in heaven, and thou art on earth'. The relation between such a God and such a man, and the relation between such a man and such a God, is for me the theme of the Bible and the essence of philosophy.\"\n\nKierkegaard doesn't believe God is so objective toward human beings but rather that he is the absolute subjective being. He put it this way in 1846: \n\n\n\n"}
{"id": "30679665", "url": "https://en.wikipedia.org/wiki?curid=30679665", "title": "Inherent bad faith model", "text": "Inherent bad faith model\n\nThe inherent bad faith model of information processing is a theory in political psychology that was first put forth by Ole Holsti to explain the relationship between John Foster Dulles' beliefs and his model of information processing. \n\nIt is the most widely studied model of one's opponent. A state is presumed to be implacably hostile, and contra-indicators of this are ignored. They are dismissed as propaganda ploys or signs of weakness. An example is John Foster Dulles' position regarding the Soviet Union.\n\n"}
{"id": "182727", "url": "https://en.wikipedia.org/wiki?curid=182727", "title": "Mach's principle", "text": "Mach's principle\n\nIn theoretical physics, particularly in discussions of , Mach's principle (or Mach's conjecture) is the name given by Einstein to an imprecise hypothesis often credited to the physicist and philosopher Ernst Mach. The idea is that the existence of absolute rotation (the distinction of local inertial frames vs. rotating reference frames) is determined by the large-scale distribution of matter, as exemplified by this anecdote:\n\nYou are standing in a field looking at the stars. Your arms are resting freely at your side, and you see that the distant stars are not moving. Now start spinning. The stars are whirling around you and your arms are pulled away from your body. Why should your arms be pulled away when the stars are whirling? Why should they be dangling freely when the stars don't move?\nMach's principle says that this is not a coincidence—that there is a physical law that relates the motion of the distant stars to the local inertial frame. If you see all the stars whirling around you, Mach suggests that there is some physical law which would make it so you would feel a centrifugal force. There are a number of rival formulations of the principle. It is often stated in vague ways, like \"mass out there influences inertia here\". A very general statement of Mach's principle is \"local physical laws are determined by the large-scale structure of the universe\".\n\nThis concept was a guiding factor in Einstein's development of the general theory of relativity. Einstein realized that the overall distribution of matter would determine the metric tensor, which tells you which frame is rotationally stationary. Frame-dragging and conservation of gravitational angular momentum makes this into a true statement in the general theory in certain solutions. But because the principle is so vague, many distinct statements can be (and have been) made that would qualify as a Mach principle, and some of these are false. The Gödel rotating universe is a solution of the field equations that is designed to disobey Mach's principle in the worst possible way. In this example, the distant stars seem to be revolving faster and faster as one moves further away. This example doesn't completely settle the question, because it has closed timelike curves.\n\nThe basic idea also appears before Mach's time, in the writings of George Berkeley. The book \"Absolute or Relative Motion?\" (1896) by Benedict Friedländer and his brother Immanuel contained ideas similar to Mach's principle.\n\nThere is a fundamental issue in relativity theory. If all motion is relative, how can we measure the inertia of a body? We must measure the inertia with respect to something else. But what if we imagine a particle completely on its own in the universe? We might hope to still have some notion of its state of motion. Mach's principle is sometimes interpreted as the statement that such a particle's state of motion has no meaning in that case.\n\nIn Mach's words, the principle is embodied as follows:\n\nAlbert Einstein seemed to view Mach's principle as something along the lines of:\n\nIn this sense, at least some of Mach's principles are related to philosophical holism. Mach's suggestion can be taken as the injunction that gravitation theories should be relational theories. Einstein brought the principle into mainstream physics while working on general relativity. Indeed, it was Einstein who first coined the phrase \"Mach's principle\". There is much debate as to whether Mach really intended to suggest a new physical law since he never states it explicitly.\n\nThe writing in which Einstein found inspiration from Mach was \"The Science of Mechanics\", where the philosopher criticized Newton's idea of absolute space, in particular the argument that Newton gave sustaining the existence of an advantaged reference system: what is commonly called \"Newton's bucket argument\".\n\nIn his \"Philosophiae Naturalis Principia Mathematica\", Newton tried to demonstrate that:\n\nMach, in his book, says that:\n\nThis same thought had been expressed by the philosopher George Berkeley in his \"De Motu\". It is then not clear, in the passages from Mach just mentioned, if the philosopher intended to formulate a new kind of physical action between heavy bodies. This physical mechanism should determine the inertia of bodies, in a way that the heavy and distant bodies of our universe should contribute the most to the inertial forces. More likely, Mach only suggested a mere \"redescription of motion in space as experiences that do not invoke the term \"space\"\". What is certain is that Einstein interpreted Mach's passage in the former way, originating a long-lasting debate.\n\nMost physicists believe Mach's principle was never developed into a quantitative physical theory that would explain a mechanism by which the stars can have such an effect. It was never made clear by Mach himself exactly what his principle was. Although Einstein was intrigued and inspired by Mach's principle, Einstein's formulation of the principle is not a fundamental assumption of general relativity.\n\nBecause intuitive notions of distance and time no longer apply, what exactly is meant by \"Mach's principle\" in general relativity is even less clear than in Newtonian physics and at least 21 formulations of Mach's principle are possible, some being considered more strongly Machian than others. A relatively weak formulation is the assertion that the motion of matter in one place should affect which frames are inertial in another.\n\nEinstein, before completing his development of the general theory of relativity, found an effect which he interpreted as being evidence of Mach's principle. We assume a fixed background for conceptual simplicity, construct a large spherical shell of mass, and set it spinning in that background. The reference frame in the interior of this shell will precess with respect to the fixed background. This effect is known as the Lense–Thirring effect. Einstein was so satisfied with this manifestation of Mach's principle that he wrote a letter to Mach expressing this:\nThe Lense–Thirring effect certainly satisfies the very basic and broad notion that \"matter there influences inertia here\". The plane of the pendulum would not be dragged around if the shell of matter were not present, or if it were not spinning. As for the statement that \"inertia originates in a kind of interaction between bodies\", this too could be interpreted as true in the context of the effect.\n\nMore fundamental to the problem, however, is the very existence of a fixed background, which Einstein describes as \"the fixed stars\". Modern relativists see the imprints of Mach's principle in the initial-value problem. Essentially, we humans seem to wish to separate spacetime into slices of constant time. When we do this, Einstein's equations can be decomposed into one set of equations, which must be satisfied on each slice, and another set, which describe how to move between slices. The equations for an individual slice are elliptic partial differential equations. In general, this means that only part of the geometry of the slice can be given by the scientist, while the geometry everywhere else will then be dictated by Einstein's equations on the slice.\n\nIn the context of an asymptotically flat spacetime, the boundary conditions are given at infinity. Heuristically, the boundary conditions for an asymptotically flat universe define a frame with respect to which inertia has meaning. By performing a Lorentz transformation on the distant universe, of course, this inertia can also be transformed.\n\nA stronger form of Mach's principle applies in Wheeler–Mach–Einstein spacetimes, which require spacetime to be spatially compact and globally hyperbolic. In such universes Mach's principle can be stated as \"the distribution of matter and field energy-momentum (and possibly other information) at a particular moment in the universe determines the inertial frame at each point in the universe\" (where \"a particular moment in the universe\" refers to a chosen Cauchy surface).\n\nThere have been other attempts to formulate a theory that is more fully Machian, such as the Brans–Dicke theory and the Hoyle–Narlikar theory of gravity, but most physicists argue that none have been fully successful. At an exit poll of experts, held in Tübingen in 1993, when asked the question \"Is general relativity perfectly Machian?\", 3 respondents replied \"yes\", and 22 replied \"no\". To the question \"Is general relativity with appropriate boundary conditions of closure of some kind very Machian?\" the result was 14 \"yes\" and 7 \"no\".\n\nHowever, Einstein was convinced that a valid theory of gravity would necessarily have to include the relativity of inertia:\nThe broad notion that \"mass there influences inertia here\" has been expressed in several forms.\nHermann Bondi and Joseph Samuel have listed eleven distinct statements that can be called Mach principles, labelled by \"Mach0\" through \"Mach10\". Though their list is not necessarily exhaustive, it does give a flavor for the variety possible.\n\n\n\n"}
{"id": "27940157", "url": "https://en.wikipedia.org/wiki?curid=27940157", "title": "Marginal factor cost", "text": "Marginal factor cost\n\nIn microeconomics, the marginal factor cost (MFC) is the increment to total costs paid for a factor of production resulting from a one-unit increase in the amount of the factor employed. It is expressed in currency units per incremental unit of a factor of production (input), such as labor, per unit of time. In the case of the labor input, for example, if the wage rate paid is unaffected by the number of units of labor hired, the marginal factor cost is identical to the wage rate. However, if hiring another unit of labor drives up the wage rate that must be paid to all existing units of labor employed, then the marginal cost of the labor factor is higher than the wage rate paid to the last unit because it also includes the increment to the rates paid to the other units.\n\nThus for any factor the MFC is the change in total amount paid for all units of that factor divided by the change in the quantity of that factor employed.\n\nA firm that wants to optimize its profits hires each factor up to the point at which its marginal factor cost equals its marginal revenue product (MFC=MRP).\n"}
{"id": "19466946", "url": "https://en.wikipedia.org/wiki?curid=19466946", "title": "Marginal profit", "text": "Marginal profit\n\nIn microeconomics, marginal profit is the difference between the marginal revenue and the marginal cost. Under the marginal approach to profit maximization, to maximize profits, a firm should continue to produce a good or service up to the point where marginal profit is zero.\n\nThe most simple formula of Marginal profit is: Marginal revenue - Marginal cost. The derivate of the profit f(x) is in fact MP. In other words: p(x)=R(x)-C(x)."}
{"id": "795103", "url": "https://en.wikipedia.org/wiki?curid=795103", "title": "Multiple drafts model", "text": "Multiple drafts model\n\nDaniel Dennett's multiple drafts model of consciousness is a physicalist theory of consciousness based upon cognitivism, which views the mind in terms of information processing. The theory is described in depth in his book, \"Consciousness Explained\", published in 1991. As the title states, the book proposes a high-level explanation of consciousness which is consistent with support for the possibility of strong AI.\n\nDennett describes the theory as \"first-person operationalism\". As he states it:\nDennett's thesis is that our modern understanding of consciousness is unduly influenced by the ideas of René Descartes. To show why, he starts with a description of the phi illusion. In this experiment, two different coloured lights, with an angular separation of a few degrees at the eye, are flashed in succession. If the interval between the flashes is less than a second or so, the first light that is flashed appears to move across to the position of the second light. Furthermore, the light seems to change colour as it moves across the visual field. A green light will appear to turn red as it seems to move across to the position of a red light. Dennett asks how we could see the light change colour \"before\" the second light is observed.\n\nDennett claims that conventional explanations of the colour change boil down to either \"Orwellian\" or \"Stalinesque\" hypotheses, which he says are the result of Descartes' continued influence on our vision of the mind. In an Orwellian hypothesis, the subject comes to one conclusion, then goes back and changes that memory in light of subsequent events. This is akin to George Orwell's \"Nineteen Eighty-Four\", where records of the past are routinely altered. In a Stalinesque hypothesis, the two events would be reconciled prior to entering the subject's consciousness, with the final result presented as fully resolved. This is akin to Joseph Stalin's show trials, where the verdict has been decided in advance and the trial is just a rote presentation.\nDennett argues that there is no principled basis for picking one of these theories over the other, because they share a common error in supposing that there is a special time and place where unconscious processing becomes consciously experienced, entering into what Dennett calls the \"Cartesian theatre\". Both theories require us to cleanly divide a sequence of perceptions and reactions into before and after the instant that they reach the seat of consciousness, but he denies that there is any such moment, as it would lead to infinite regress. Instead, he asserts that there is no privileged place in the brain where consciousness happens. Dennett states that, \"[t]here does not exist ... a process such as 'recruitment of consciousness' (into what?), nor any place where the 'vehicle's arrival' is recognized (by whom?)\"\n\nWith no theatre, there is no screen, hence no reason to re-present data after it has already been analysed. Dennett says that, \"the Multiple Drafts model goes on to claim that the brain does not bother 'constructing' any representations that go to the trouble of 'filling in' the blanks. That would be a waste of time and (shall we say?) paint. The judgement is already in so we can get on with other tasks!\"\n\nAccording to the model, there are a variety of sensory inputs from a given event and also a variety of interpretations of these inputs. The sensory inputs arrive in the brain and are interpreted at different times, so a given event can give rise to a succession of discriminations, constituting the equivalent of multiple drafts of a story. As soon as each discrimination is accomplished, it becomes available for eliciting a behaviour; it does not have to wait to be presented at the theatre.\n\nLike a number of other theories, the Multiple Drafts model understands conscious experience as taking time to occur, such that percepts do not instantaneously arise in the mind in their full richness. The distinction is that Dennett's theory denies any clear and unambiguous boundary separating conscious experiences from all other processing. According to Dennett, consciousness is to be found in the actions and flows of information from place to place, rather than some singular view containing our experience. There is no central experiencer who confers a durable stamp of approval on any particular draft.\n\nDifferent parts of the neural processing assert more or less control at different times. For something to reach consciousness is akin to becoming famous, in that it must leave behind consequences by which it is remembered. To put it another way, consciousness is the property of having enough influence to affect what the mouth will say and the hands will do. Which inputs are \"edited\" into our drafts is not an exogenous act of supervision, but part of the self-organizing functioning of the network, and at the same level as the circuitry that conveys information bottom-up.\n\nThe conscious self is taken to exist as an abstraction visible at the level of the intentional stance, akin to a body of mass having a \"centre of gravity\". Analogously, Dennett refers to the self as the \"centre of narrative gravity\", a story we tell ourselves about our experiences. Consciousness exists, but not independently of behaviour and behavioural disposition, which can be studied through heterophenomenology.\n\nThe origin of this operationalist approach can be found in Dennett's immediately preceding work. Dennett (1988) explains consciousness in terms of \"access consciousness\" alone, denying the independent existence of what Ned Block has labeled \"phenomenal consciousness\". He argues that \"Everything real has properties, and since I don't deny the reality of conscious experience, I grant that conscious experience has properties\". Having related all consciousness to properties, he concludes that they cannot be meaningfully distinguished from our judgements about them. He writes: \nIn other words, once we've explained a perception fully in terms of how it affects us, there is nothing left to explain. In particular, there is no such thing as a perception which may be considered in and of itself (a quale). Instead, the subject's honest reports of how things seem to them are inherently authoritative on how things seem to them, but not on the matter of how things actually are.\nThe key to the multiple drafts model is that, after removing qualia, explaining consciousness boils down to explaining the behaviour we recognise as conscious. Consciousness is as consciousness does.\n\nSome of the criticism of Dennett's theory is due to the perceived tone of his presentation. As one grudging supporter admits, \"there is much in this book that is disputable. And Dennett is at times aggravatingly smug and confident about the merits of his arguments ... All in all Dennett's book is annoying, frustrating, insightful, provocative and above all annoying\" (Korb 1993).\n\nBogen (1992) points out that the brain is bilaterally symmetrical. That being the case, if Cartesian materialism is true, there might be \"two\" Cartesian theatres, so arguments against only one are flawed. Velmans (1992) argues that the phi effect and the cutaneous rabbit illusion demonstrate that there is a delay whilst modelling occurs and that this delay was discovered by Libet.\n\nIt has also been claimed that the argument in the multiple drafts model does not support its conclusion.\n\nMuch of the criticism asserts that Dennett's theory attacks the wrong target, failing to explain what it claims to. Chalmers (1996) maintains that Dennett has produced no more than a theory of how subjects report events. Some even parody the title of the book as \"Consciousness Explained Away\", accusing him of greedy reductionism. Another line of criticism disputes the accuracy of Dennett's characterisations of existing theories:\nMultiple drafts is also attacked for making a claim to novelty. It may be the case, however, that such attacks mistake which features Dennett is claiming as novel. Korb states that, \"I believe that the central thesis will be relatively uncontentious for most cognitive scientists, but that its use as a cleaning solvent for messy puzzles will be viewed less happily in most quarters.\" (Korb 1993) In this way, Dennett uses uncontroversial ideas towards more controversial ends, leaving him open to claims of unoriginality when uncontroversial parts are focused upon.\n\nEven the notion of consciousness as drafts is not unique to Dennett. According to Hankins, Dieter Teichert suggests that Paul Ricoeur's theories agree with Dennett's on the notion that \"the self is basically a narrative entity, and that any attempt to give it a free-floating independent status is misguided.\" [Hankins] Others see Derrida's (1982) representationalism as consistent with the notion of a mind that has perceptually changing content without a definitive present instant.\n\nTo those who believe that consciousness entails something more than behaving in all ways conscious, Dennett's view is seen as eliminativist, since it denies the existence of qualia and the possibility of philosophical zombies. However, Dennett is not denying the existence of the mind or of consciousness, only what he considers a naive view of them. The point of contention is whether Dennett's own definitions are indeed more accurate: whether what we think of when we speak of perceptions and consciousness can be understood in terms of nothing more than their effect on behaviour.\n\nThe role of information processing in consciousness has been criticised by John Searle who, in his Chinese room argument, states that he cannot find anything that could be recognised as conscious experience in a system that relies solely on motions of things from place to place. Dennett sees this argument as misleading, arguing that consciousness is not to be found in a specific part of the system, but in the actions of the whole. In essence, he denies that consciousness requires something in addition to capacity for behaviour, saying that philosophers such as Searle, \"just can't imagine how understanding could be a property that emerges from lots of distributed quasi-understanding in a large system\" (p. 439).\n\n\n\n\n"}
{"id": "5205483", "url": "https://en.wikipedia.org/wiki?curid=5205483", "title": "Negative free bid", "text": "Negative free bid\n\nNegative free bid is a contract bridge treatment whereby a free bid by responder over an opponent's overcall shows a long suit in a weak hand and is not forcing. This is in contrast with standard treatment, where a free bid can show unlimited values and is unconditionally forcing. The treatment is a relatively recent invention, and has become quite popular, especially in expert circles.\n\nNegative free bids resolve relatively frequent situations where the responder holds a long suit with which he would like to compete for a partscore, but is deprived from bidding it by opponent's overcall.\n\nFor example, if South holds: , partner opens 1 and East overcalls 1, he couldn't bid 2 in standard methods, as it would show 10+ high-card points, and a negative double would be too off-shape. With NFB treatment in effect though, he can bid 2 which the partner may pass (unless he has extra values and support, or an excellent suit of its own without tolerance for hearts).\n\nHowever, as a corollary, negative free bids affect the scope of negative double; if the hand is suitable for \"standard\" forcing free bid (10-11+ points), a negative double has to be made first and the suit bid only in the next round. Thus, the negative double can be made with the following types of hand:\nThis can sometimes allow the opponents to preempt effectively. \nFor example, West, holding: , after this auction is in an awkward situation — he doesn't know whether partner has spades or not; whether South was bidding to make or to sacrifice — is it correct to double, bid 4 or pass?\n\n"}
{"id": "1092282", "url": "https://en.wikipedia.org/wiki?curid=1092282", "title": "Negative frequency", "text": "Negative frequency\n\nThe concept of negative and positive frequency can be as simple as a wheel rotating one way or the other way: a \"signed value\" of frequency can indicate both the rate and direction of rotation. The rate is expressed in units such as revolutions (a.k.a. \"cycles\") per second (hertz) or radian/second (where 1 cycle corresponds to 2\"π\" radians).\n\nLet \"ω\" be a nonnegative parameter with units of radians/sec. Then the angular function (angle vs. time) , has slope −\"ω\", which is called a negative frequency. But when the function is used as the argument of a cosine operator, the result is indistinguishable from .  Similarly, is indistinguishable from . Thus any sinusoids can be represented in terms of positive frequencies. The sign of the underlying phase slope is ambiguous.\n\nThe ambiguity is resolved when the cosine and sine operators can be observed simultaneously, because leads by 1/4 cycle (= \"π\"/2 radians) when , and lags by 1/4 cycle when .  Similarly, a vector, , rotates counter-clockwise at 1 radian/sec, and completes a circle every 2π seconds, and the vector rotates in the other direction.\n\nThe sign of \"ω\" is also preserved in the complex-valued function:\n\nsince R(\"t\") and I(\"t\") can be separately extracted and compared. Although formula_1  clearly contains more information than either of its components, a common interpretation is that it is a simpler function, because:\nwhich gives rise to the interpretation that cos(\"ωt\") comprises \"both\" positive and negative frequencies.  But the sum is actually a cancellation that contains less, not more, information. Any measure that indicates both frequencies includes a false positive, because \"ω\" can have only one sign.  The Fourier transform, for instance, merely tells us that cos(\"ωt\") correlates equally well with both and with .\n\nPerhaps the most well-known application of negative frequency is the calculation:\n\nwhich is a measure of the amount of frequency ω in the function \"x\"(\"t\") over the interval . When evaluated as a continuous function of \"ω\" for the theoretical interval , it is known as the Fourier transform of \"x\"(\"t\"). A brief explanation is that the product of two complex sinusoids is also a complex sinusoid whose frequency is the sum of the original frequencies. So when \"ω\" is positive, formula_4 causes all the frequencies of \"x\"(\"t\") to be reduced by amount \"ω\". Whatever part of \"x\"(\"t\") that was at frequency \"ω\" is changed to frequency zero, which is just a constant whose amplitude level is a measure of the strength of the original \"ω\" content. And whatever part of \"x\"(\"t\") that was at frequency zero is changed to a sinusoid at frequency −\"ω\". Similarly, all other frequencies are changed to non-zero values. As the interval increases, the contribution of the constant term grows in proportion. But the contributions of the sinusoidal terms only oscillate around zero. So \"X\"(\"ω\") improves as a relative measure of the amount of frequency \"ω\" in the function \"x\"(\"t\").\n\nThe Fourier transform of  formula_1  produces a non-zero response only at frequency \"ω\". The transform of formula_2 has responses at both \"ω\" and −\"ω\", as anticipated by .\n\n"}
{"id": "548558", "url": "https://en.wikipedia.org/wiki?curid=548558", "title": "Negative pressure", "text": "Negative pressure\n\nNegative pressure may refer to:\n\n"}
{"id": "237770", "url": "https://en.wikipedia.org/wiki?curid=237770", "title": "Negative resistance", "text": "Negative resistance\n\nIn electronics, negative resistance (NR) is a property of some electrical circuits and devices in which an increase in voltage across the device's terminals results in a decrease in electric current through it.\n\nThis is in contrast to an ordinary resistor in which an increase of applied voltage causes a proportional increase in current due to Ohm's law, resulting in a positive resistance. While a positive resistance consumes power from current passing through it, a negative resistance produces power. Under certain conditions it can increase the power of an electrical signal, amplifying it.\n\nNegative resistance is an uncommon property which occurs in a few nonlinear electronic components. In a nonlinear device, two types of resistance can be defined: 'static' or 'absolute resistance', the ratio of voltage to current formula_1, and \"differential resistance\", the ratio of a change in voltage to the resulting change in current formula_2. The term negative resistance means negative differential resistance (NDR), formula_3. In general, a negative differential resistance is a two-terminal component which can amplify, converting DC power applied to its terminals to AC output power to amplify an AC signal applied to the same terminals. They are used in electronic oscillators and amplifiers, particularly at microwave frequencies. Most microwave energy is produced with negative differential resistance devices. They can also have hysteresis and be bistable, and so are used in switching and memory circuits. Examples of devices with negative differential resistance are tunnel diodes, Gunn diodes, and gas discharge tubes such as neon lamps. In addition, circuits containing amplifying devices such as transistors and op amps with positive feedback can have negative differential resistance. These are used in oscillators and active filters.\n\nBecause they are nonlinear, negative resistance devices have a more complicated behavior than the positive \"ohmic\" resistances usually encountered in electric circuits. Unlike most positive resistances, negative resistance varies depending on the voltage or current applied to the device, and negative resistance devices can have negative resistance over only a limited portion of their voltage or current range. Therefore, there is no real \"negative resistor\" analogous to a positive resistor, which has a constant negative resistance over an arbitrarily wide range of current.\n\nThe resistance between two terminals of an electrical device or circuit is determined by its current–voltage (\"I–V\") curve (characteristic curve), giving the current formula_4 through it for any given voltage formula_5 across it. Most materials, including the ordinary (positive) resistances encountered in electrical circuits, obey Ohm's law; the current through them is proportional to the voltage over a wide range. So the \"I–V\" curve of an ohmic resistance is a straight line through the origin with positive slope. The resistance is the ratio of voltage to current, the inverse slope of the line (in \"I–V\" graphs where the voltage formula_5 is the independent variable) and is constant.\n\nNegative resistance occurs in a few nonlinear (nonohmic) devices. In a nonlinear component the \"I–V\" curve is not a straight line, so it does not obey Ohm's law. Resistance can still be defined, but the resistance is not constant; it varies with the voltage or current through the device. The resistance of such a nonlinear device can be defined in two ways, which are equal for ohmic resistances:\n\n\n\nNegative resistance, like positive resistance, is measured in ohms.\n\nConductance is the reciprocal of resistance. It is measured in siemens (formerly \"mho\") which is the conductance of a resistor with a resistance of one ohm. Each type of resistance defined above has a corresponding conductance\nIt can be seen that the conductance has the same sign as its corresponding resistance: a negative resistance will have a negative conductance while a positive resistance will have a positive conductance.\n\nOne way in which the different types of resistance can be distinguished is in the directions of current and electric power between a circuit and an electronic component. The illustrations below, with a rectangle representing the component attached to a circuit, summarize how the different types work:\nIn an electronic device, the differential resistance formula_14, the static resistance formula_15, or both, can be negative, so there are three categories of devices \"(fig. 2–4 above, and table)\" which could be called \"negative resistances\".\n\nThe term \"negative resistance\" almost always means negative \"differential\" resistance Negative differential resistance devices have unique capabilities: they can act as \"one-port amplifiers\", increasing the power of a time-varying signal applied to their port (terminals), or excite oscillations in a tuned circuit to make an oscillator. They can also have hysteresis. It is not possible for a device to have negative differential resistance without a power source, and these devices can be divided into two categories depending on whether they get their power from an internal source or from their port:\n\n\n\nOccasionally ordinary power sources are referred to as \"negative resistances\" (fig. 3 above). Although the \"static\" or \"absolute\" resistance formula_15 of active devices (power sources) can be considered negative (see Negative static resistance section below) most ordinary power sources (AC or DC), such as batteries, generators, and (non positive feedback) amplifiers, have positive \"differential\" resistance (their source resistance). Therefore, these devices cannot function as one-port amplifiers or have the other capabilities of negative differential resistances.\n\nElectronic components with negative differential resistance include these devices:\n\nElectric discharges through gases also exhibit negative differential resistance, including these devices\n\n\nIn addition, active circuits with negative differential resistance can also be built with amplifying devices like transistors and op amps, using feedback. A number of new experimental negative differential resistance materials and devices have been discovered in recent years. The physical processes which cause negative resistance are diverse, and each type of device has its own negative resistance characteristics, specified by its current–voltage curve.\n\nA point of some confusion is whether ordinary resistance (\"static\" or \"absolute\" resistance, formula_17) can be negative. In electronics, the term \"resistance\" is customarily applied only to passive materials and components – such as wires, resistors and diodes. These cannot have formula_18 as shown by Joule's law formula_19. A passive device consumes electric power, so from the passive sign convention formula_20. Therefore, from Joule's law formula_21. In other words, no material can conduct electric current better than a \"perfect\" conductor with zero resistance. For a passive device to have formula_22 would violate either conservation of energy or the second law of thermodynamics, \"(diagram)\". Therefore, some authors state that static resistance can never be negative.\n\nHowever it is easily shown that the ratio of voltage to current v/i at the terminals of any power source (AC or DC) is negative. For electric power (potential energy) to flow out of a device into the circuit, charge must flow through the device in the direction of increasing potential energy, conventional current (positive charge) must move from the negative to the positive terminal. So the direction of the instantaneous current is \"out\" of the positive terminal. This is opposite to the direction of current in a passive device defined by the passive sign convention so the current and voltage have opposite signs, and their ratio is negative\nThis can also be proved from Joule's law\nThis shows that power can flow out of a device into the circuit if and only if formula_18. Whether or not this quantity is referred to as \"resistance\" when negative is a matter of convention. The absolute resistance of power sources is negative, but this is not to be regarded as \"resistance\" in the same sense as positive resistances. The negative static resistance of a power source is a rather abstract and not very useful quantity, because it varies with the load. Due to conservation of energy it is always simply equal to the negative of the static resistance of the attached circuit \"(right)\".\n\nWork must be done on the charges by some source of energy in the device, to make them move toward the positive terminal against the electric field, so conservation of energy requires that negative static resistances have a source of power. The power may come from an internal source which converts some other form of energy to electric power as in a battery or generator, or from a separate connection to an external power supply circuit as in an amplifying device like a transistor, vacuum tube, or op amp.\n\nA circuit cannot have negative static resistance (be active) over an infinite voltage or current range, because it would have to be able to produce infinite power. Any active circuit or device with a finite power source is \"eventually passive\". This property means if a large enough external voltage or current of either polarity is applied to it, its static resistance becomes positive and it consumes power\n\nTherefore, the ends of the \"I–V\" curve will eventually turn and enter the 1st and 3rd quadrants. Thus the range of the curve having negative static resistance is limited, confined to a region around the origin. For example, applying a voltage to a generator or battery \"(graph, above)\" greater than its open-circuit voltage will reverse the direction of current flow, making its static resistance positive so it consumes power. Similarly, applying a voltage to the negative impedance converter below greater than its power supply voltage \"V\" will cause the amplifier to saturate, also making its resistance positive.\n\nIn a device or circuit with negative differential resistance (NDR), in some part of the \"I–V\" curve the current decreases as the voltage increases:\nThe \"I–V\" curve is nonmonotonic (having peaks and troughs) with regions of negative slope representing negative differential resistance.\n\nPassive negative differential resistances have positive \"static\" resistance; they consume net power. Therefore, the \"I–V\" curve is confined to the 1st and 3rd quadrants of the graph, and passes through the origin. This requirement means (excluding some asymptotic cases) that the region(s) of negative resistance must be limited, and surrounded by regions of positive resistance, and cannot include the origin.\n\nNegative differential resistances can be classified into two types:\n\n\nMost devices have a single negative resistance region. However devices with multiple separate negative resistance regions can also be fabricated. These can have more than two stable states, and are of interest for use in digital circuits to implement multivalued logic.\n\nAn intrinsic parameter used to compare different devices is the \"peak-to-valley current ratio\" (PVR), the ratio of the current at the top of the negative resistance region to the current at the bottom \"(see graphs, above)\":\nThe larger this is, the larger the potential AC output for a given DC bias current, and therefore the greater the efficiency\n\nA negative differential resistance device can amplify an AC signal applied to it if the signal is biased with a DC voltage or current to lie within the negative resistance region of its \"I–V\" curve.\n\nThe tunnel diode circuit \"(see diagram)\" is an example. The tunnel diode \"TD\" has voltage controlled negative differential resistance. The battery formula_30 adds a constant voltage (bias) across the diode so it operates in its negative resistance range, and provides power to amplify the signal. Suppose the negative resistance at the bias point is formula_31. For stability formula_32 must be less than formula_33. Using the formula for a voltage divider, the AC output voltage is\nIn a normal voltage divider, the resistance of each branch is less than the resistance of the whole, so the output voltage is less than the input. Here, due to the negative resistance, the total AC resistance formula_36 is less than the resistance of the diode alone formula_33 so the AC output voltage formula_38 is greater than the input formula_39. The voltage gain formula_40 is greater than one, and increases without limit as formula_32 approaches formula_33.\n\nThe diagrams illustrate how a biased negative differential resistance device can increase the power of a signal applied to it, amplifying it, although it only has two terminals. Due to the superposition principle the voltage and current at the device's terminals can be divided into a DC bias component and an AC component .\nSince a positive change in voltage formula_45 causes a \"negative\" change in current formula_46, the AC current and voltage in the device are 180° out of phase. This means in the AC equivalent circuit \"(right)\", the instantaneous AC current Δ\"i\" flows through the device in the direction of \"increasing\" AC potential Δ\"v\", as it would in a generator. Therefore, the AC power dissipation is \"negative\"; AC power is produced by the device and flows into the external circuit.\nWith the proper external circuit, the device can increase the AC signal power delivered to a load, serving as an amplifier, or excite oscillations in a resonant circuit to make an oscillator. Unlike in a two port amplifying device such as a transistor or op amp, the amplified signal leaves the device through the same two terminals (port) as the input signal enters.\n\nIn a passive device, the AC power produced comes from the input DC bias current, the device absorbs DC power, some of which is converted to AC power by the nonlinearity of the device, amplifying the applied signal. Therefore, the output power is limited by the bias power\nThe negative differential resistance region cannot include the origin, because it would then be able to amplify a signal with no applied DC bias current, producing AC power with no power input. The device also dissipates some power as heat, equal to the difference between the DC power in and the AC power out.\n\nThe device may also have reactance and therefore the phase difference between current and voltage may differ from 180° and may vary with frequency. As long as the real component of the impedance is negative (phase angle between 90° and 270°), the device will have negative resistance and can amplify.\n\nThe maximum AC output power is limited by size of the negative resistance region (formula_49 in graphs above)\n\nThe reason that the output signal can leave a negative resistance through the same port that the input signal enters is that from transmission line theory, the AC voltage or current at the terminals of a component can be divided into two oppositely moving waves, the \"incident wave\" formula_51, which travels toward the device, and the \"reflected wave\" formula_52, which travels away from the device. A negative differential resistance in a circuit can amplify if the magnitude of its reflection coefficient formula_53, the ratio of the reflected wave to the incident wave, is greater than one.\nThe \"reflected\" (output) signal has larger amplitude than the incident; the device has \"reflection gain\". The reflection coefficient is determined by the AC impedance of the negative resistance device, formula_56, and the impedance of the circuit attached to it, formula_57. If formula_58 and formula_59 then formula_60 and the device will amplify. On the Smith chart, a graphical aide widely used in the design of high frequency circuits, negative differential resistance corresponds to points outside the unit circle formula_61, the boundary of the conventional chart, so special \"expanded\" charts must be used.\n\nBecause it is nonlinear, a circuit with negative differential resistance can have multiple equilibrium points (possible DC operating points), which lie on the \"I–V\" curve. An equilibrium point will be stable, so the circuit converges to it within some neighborhood of the point, if its poles are in the left half of the s plane (LHP), while a point is unstable, causing the circuit to oscillate or \"latch up\" (converge to another point), if its poles are on the \"jω\" axis or right half plane (RHP), respectively. In contrast, a linear circuit has a single equilibrium point that may be stable or unstable. The equilibrium points are determined by the DC bias circuit, and their stability is determined by the AC impedance formula_62 of the external circuit.\nHowever, because of the different shapes of the curves, the condition for stability is different for VCNR and CCNR types of negative resistance:\n\n\nFor general negative resistance circuits with reactance, the stability must be determined by standard tests like the Nyquist stability criterion. Alternatively, in high frequency circuit design, the values of formula_70 for which the circuit is stable are determined by a graphical technique using \"stability circles\" on a Smith chart.\n\nFor simple nonreactive negative resistance devices with formula_71 and formula_72 the different operating regions of the device can be illustrated by load lines on the \"I–V\" curve \"(see graphs)\".\n\nThe DC load line (DCL) is a straight line determined by the DC bias circuit, with equation\nwhere formula_74 is the DC bias supply voltage and R is the resistance of the supply. The possible DC operating point(s) (Q points) occur where the DC load line intersects the \"I–V\" curve. For stability\nThe AC load line (\"L\" − \"L\") is a straight line through the Q point whose slope is the differential (AC) resistance formula_75 facing the device. Increasing formula_75 rotates the load line counterclockwise. The circuit operates in one of three possible regions \"(see diagrams)\", depending on formula_75.\n\nIn addition to the passive devices with intrinsic negative differential resistance above, circuits with amplifying devices like transistors or op amps can have negative resistance at their ports. The input or output impedance of an amplifier with enough positive feedback applied to it can be negative. If formula_84 is the input resistance of the amplifier without feedback, formula_85 is the amplifier gain, and formula_86 is the transfer function of the feedback path, the input resistance with positive shunt feedback is\nSo if the loop gain formula_88 is greater than one, formula_89 will be negative. The circuit acts like a \"negative linear resistor\" over a limited range, with \"I–V\" curve having a straight line segment through the origin with negative slope \"(see graphs)\". It has both negative differential resistance and is active\nand thus obeys Ohm's law as if it had a negative value of resistance \"−R\", over its linear range (such amplifiers can also have more complicated negative resistance \"I–V\" curves that do not pass through the origin).\n\nIn circuit theory these are called \"active resistors\". Applying a voltage across the terminals causes a proportional current \"out\" of the positive terminal, the opposite of an ordinary resistor. For example, connecting a battery to the terminals would cause the battery to charge rather than discharge.\n\nConsidered as one-port devices, these circuits function similarly to the passive negative differential resistance components above, and like them can be used to make one-port amplifiers and oscillators with the advantages that:\nThe \"I–V\" curve can have voltage-controlled (\"N\" type) or current-controlled (\"S\" type) negative resistance, depending on whether the feedback loop is connected in \"shunt\" or \"series\".\n\nNegative reactances \"(below)\" can also be created, so feedback circuits can be used to create \"active\" linear circuit elements, resistors, capacitors, and inductors, with negative values. They are widely used in active filters because they can create transfer functions that cannot be realized with positive circuit elements. Examples of circuits with this type of negative resistance are the negative impedance converter (NIC), gyrator, Deboo integrator, frequency dependent negative resistance (FDNR), and generalized immittance converter (GIC).\n\nIf an LC circuit is connected across the input of a positive feedback amplifier like that above, the negative differential input resistance formula_91 can cancel the positive loss resistance formula_92 inherent in the tuned circuit. If formula_93 this will create in effect a tuned circuit with zero AC resistance (poles on the \"jω\" axis). Spontaneous oscillation will be excited in the tuned circuit at its resonant frequency, sustained by the power from the amplifier. This is how feedback oscillators such as Hartley or Colpitts oscillators work. This negative resistance model is an alternate way of analyzing feedback oscillator operation. \"All\" linear oscillator circuits have negative resistance although in most feedback oscillators the tuned circuit is an integral part of the feedback network, so the circuit does not have negative resistance at all frequencies but only near the oscillation frequency.\n\nA tuned circuit connected to a negative resistance which cancels some but not all of its parasitic loss resistance (so formula_94) will not oscillate, but the negative resistance will decrease the damping in the circuit (moving its poles toward the \"jω\" axis), increasing its Q factor so it has a narrower bandwidth and more selectivity. Q enhancement, also called \"regeneration\", was first used in the regenerative radio receiver invented by Edwin Armstrong in 1912 and later in \"Q multipliers\". It is widely used in active filters. For example, RF integrated circuits use \"integrated inductors\" to save space, consisting of a spiral conductor fabricated on chip. These have high losses and low Q, so to create high Q tuned circuits their Q is increased by applying negative resistance.\n\nCircuits which exhibit chaotic behavior can be considered quasi-periodic or nonperiodic oscillators, and like all oscillators require a negative resistance in the circuit to provide power. Chua's circuit, a simple nonlinear circuit widely used as the standard example of a chaotic system, requires a nonlinear active resistor component, sometimes called Chua's diode. This is usually synthesized using a negative impedance converter circuit.\n\nA common example of an \"active resistance\" circuit is the negative impedance converter (NIC) shown in the diagram. The two resistors formula_95 and the op amp constitute a negative feedback non-inverting amplifier with gain of 2. The output voltage of the op-amp is\nSo if a voltage formula_5 is applied to the input, the same voltage is applied \"backwards\" across formula_98, causing current to flow through it out of the input. The current is\nSo the input impedance to the circuit is\nThe circuit converts the impedance formula_98 to its negative. If formula_98 is a resistor of value formula_103, within the linear range of the op amp formula_104 the input impedance acts like a linear \"negative resistor\" of value formula_105. The input port of the circuit is connected into another circuit as if it was a component. An NIC can cancel undesired positive resistance in another circuit, for example they were originally developed to cancel resistance in telephone cables, serving as repeaters.\n\nBy replacing formula_98 in the above circuit with a capacitor , negative capacitances and inductances can also be synthesized. A negative capacitance will have an \"I–V\" relation and an impedance formula_107 of\nwhere formula_109. Applying a positive current to a negative capacitance will cause it to \"discharge\"; its voltage will \"decrease\". Similarly, a negative inductance will have an \"I–V\" characteristic and impedance formula_110 of\nA circuit having negative capacitance or inductance can be used to cancel unwanted positive capacitance or inductance in another circuit. NIC circuits were used to cancel reactance on telephone cables.\n\nThere is also another way of looking at them. In a negative capacitance the current will be 180° opposite in phase to the current in a positive capacitance. Instead of leading the voltage by 90° it will lag the voltage by 90°, as in an inductor. Therefore, a negative capacitance acts like an inductance in which the impedance has a reverse dependence on frequency ω; decreasing instead of increasing like a real inductance Similarly a negative inductance acts like a capacitance that has an impedance which increases with frequency. Negative capacitances and inductances are \"non-Foster\" circuits which violate Foster's reactance theorem. One application being researched is to create an active matching network which could match an antenna to a transmission line over a broad range of frequencies, rather than just a single frequency as with current networks. This would allow the creation of small compact antennas that would have broad bandwidth, exceeding the Chu–Harrington limit.\n\nNegative differential resistance devices are widely used to make electronic oscillators. In a negative resistance oscillator, a negative differential resistance device such as an IMPATT diode, Gunn diode, or microwave vacuum tube is connected across an electrical resonator such as an LC circuit, a quartz crystal, dielectric resonator or cavity resonator with a DC source to bias the device into its negative resistance region and provide power. A resonator such as an LC circuit is \"almost\" an oscillator; it can store oscillating electrical energy, but because all resonators have internal resistance or other losses, the oscillations are damped and decay to zero. The negative resistance cancels the positive resistance of the resonator, creating in effect a lossless resonator, in which spontaneous continuous oscillations occur at the resonator's resonant frequency.\n\nNegative resistance oscillators are mainly used at high frequencies in the microwave range or above, since feedback oscillators function poorly at these frequencies. Microwave diodes are used in low- to medium-power oscillators for applications such as radar speed guns, and local oscillators for satellite receivers. They are a widely used source of microwave energy, and virtually the only solid-state source of millimeter wave and terahertz energy Negative resistance microwave vacuum tubes such as magnetrons produce higher power outputs, in such applications as radar transmitters and microwave ovens. Lower frequency relaxation oscillators can be made with UJTs and gas-discharge lamps such as neon lamps.\n\nThe negative resistance oscillator model is not limited to one-port devices like diodes but can also be applied to feedback oscillator circuits with two port devices such as transistors and tubes. In addition, in modern high frequency oscillators, transistors are increasingly used as one-port negative resistance devices like diodes. At microwave frequencies, transistors with certain loads applied to one port can become unstable due to internal feedback and show negative resistance at the other port. So high frequency transistor oscillators are designed by applying a reactive load to one port to give the transistor negative resistance, and connecting the other port across a resonator to make a negative resistance oscillator as described below.\n\nThe common Gunn diode oscillator \"(circuit diagrams)\" illustrates how negative resistance oscillators work. The diode \"D\" has voltage controlled (\"N\" type) negative resistance and the voltage source formula_112 biases it into its negative resistance region where its differential resistance is formula_113. The choke \"RFC\" prevents AC current from flowing through the bias source. formula_103 is the equivalent resistance due to damping and losses in the series tuned circuit formula_115, plus any load resistance. Analyzing the AC circuit with Kirchhoff's Voltage Law gives a differential equation for formula_116, the AC current\nSolving this equation gives a solution of the form\nThis shows that the current through the circuit, formula_116, varies with time about the DC Q point, formula_121. When started from a nonzero initial current formula_122 the current oscillates sinusoidally at the resonant frequency ω of the tuned circuit, with amplitude either constant, increasing, or decreasing exponentially, depending on the value of α. Whether the circuit can sustain steady oscillations depends on the balance between formula_103 and formula_124, the positive and negative resistance in the circuit:\n\nPractical oscillators are designed in region (3) above, with net negative resistance, to get oscillations started. A widely used rule of thumb is to make formula_129. When the power is turned on, electrical noise in the circuit provides a signal formula_130 to start spontaneous oscillations, which grow exponentially. However, the oscillations cannot grow forever; the nonlinearity of the diode eventually limits the amplitude.\n\nAt large amplitudes the circuit is nonlinear, so the linear analysis above does not strictly apply and differential resistance is undefined; but the circuit can be understood by considering formula_124 to be the \"average\" resistance over the cycle. As the amplitude of the sine wave exceeds the width of the negative resistance region and the voltage swing extends into regions of the curve with positive differential resistance, the average negative differential resistance formula_124 becomes smaller, and thus the total resistance formula_133 and the damping formula_134 becomes less negative and eventually turns positive. Therefore, the oscillations will stabilize at the amplitude at which the damping becomes zero, which is when formula_135.\n\nGunn diodes have negative resistance in the range −5 to −25 ohms. In oscillators where formula_103 is close to formula_124; just small enough to allow the oscillator to start, the voltage swing will be mostly limited to the linear portion of the \"I–V\" curve, the output waveform will be nearly sinusoidal and the frequency will be most stable. In circuits in which formula_103 is far below formula_124, the swing extends further into the nonlinear part of the curve, the clipping distortion of the output sine wave is more severe, and the frequency will be increasingly dependent on the supply voltage.\n\nNegative resistance oscillator circuits can be divided into two types, which are used with the two types of negative differential resistance – voltage controlled (VCNR), and current controlled (CCNR)\n\nMost oscillators are more complicated than the Gunn diode example, since both the active device and the load may have reactance (\"X\") as well as resistance (\"R\"). Modern negative resistance oscillators are designed by a frequency domain technique due to K. Kurokawa. The circuit diagram is imagined to be divided by a \"reference plane\" \"(red)\" which separates the negative resistance part, the active device, from the positive resistance part, the resonant circuit and output load \"(right)\". The complex impedance of the negative resistance part formula_140 depends on frequency \"ω\" but is also nonlinear, in general declining with the amplitude of the AC oscillation current \"I\"; while the resonator part formula_141 is linear, depending only on frequency. The circuit equation is formula_142 so it will only oscillate (have nonzero \"I\") at the frequency \"ω\" and amplitude \"I\" for which the total impedance formula_143 is zero. This means the magnitude of the negative and positive resistances must be equal, and the reactances must be conjugate\nFor steady-state oscillation the equal sign applies. During startup the inequality applies, because the circuit must have excess negative resistance for oscillations to start.\n\nAlternately, the condition for oscillation can be expressed using the reflection coefficient. The voltage waveform at the reference plane can be divided into a component \"V\" travelling toward the negative resistance device and a component \"V\" travelling in the opposite direction, toward the resonator part. The reflection coefficient of the active device formula_146 is greater than one, while that of the resonator part formula_147 is less than one. During operation the waves are reflected back and forth in a round trip so the circuit will oscillate only if\nAs above, the equality gives the condition for steady oscillation, while the inequality is required during startup to provide excess negative resistance. The above conditions are analogous to the Barkhausen criterion for feedback oscillators; they are necessary but not sufficient, so there are some circuits that satisfy the equations but do not oscillate. Kurokawa also derived more complicated sufficient conditions, which are often used instead.\n\nNegative differential resistance devices such as Gunn and IMPATT diodes are also used to make amplifiers, particularly at microwave frequencies, but not as commonly as oscillators. Because negative resistance devices have only one \"port\" (two terminals), unlike two-port devices such as transistors, the outgoing amplified signal has to leave the device by the same terminals as the incoming signal enters it. Without some way of separating the two signals, a negative resistance amplifier is \"bilateral\"; it amplifies in both directions, so it suffers from sensitivity to load impedance and feedback problems. To separate the input and output signals, many negative resistance amplifiers use nonreciprocal devices such as isolators and directional couplers.\n\nOne widely used circuit is the \"reflection amplifier\" in which the separation is accomplished by a \"circulator\". A circulator is a nonreciprocal solid-state component with three ports (connectors) which transfers a signal applied to one port to the next in only one direction, port 1 to port 2, 2 to 3, and 3 to 1. In the reflection amplifier diagram the input signal is applied to port 1, a biased VCNR negative resistance diode \"N\" is attached through a filter \"F\" to port 2, and the output circuit is attached to port 3. The input signal is passed from port 1 to the diode at port 2, but the outgoing \"reflected\" amplified signal from the diode is routed to port 3, so there is little coupling from output to input. The characteristic impedance formula_149 of the input and output transmission lines, usually 50Ω, is matched to the port impedance of the circulator. The purpose of the filter \"F\" is to present the correct impedance to the diode to set the gain. At radio frequencies NR diodes are not pure resistive loads and have reactance, so a second purpose of the filter is to cancel the diode reactance with a conjugate reactance to prevent standing waves.\n\nThe filter has only reactive components and so does not absorb any power itself, so power is passed between the diode and the ports without loss. The input signal power to the diode is\nThe output power from the diode is\nSo the power gain formula_152 of the amplifier is the square of the reflection coefficient\n\nformula_156 is the negative resistance of the diode −r. Assuming the filter is matched to the diode so formula_157 then the gain is\nThe VCNR reflection amplifier above is stable for formula_159. while a CCNR amplifier is stable for formula_160. It can be seen that the reflection amplifier can have unlimited gain, approaching infinity as formula_95 approaches the point of oscillation at formula_124. This is a characteristic of all NR amplifiers, contrasting with the behavior of two-port amplifiers, which generally have limited gain but are often unconditionally stable. In practice the gain is limited by the backward \"leakage\" coupling between circulator ports.\n\nMasers and parametric amplifiers are extremely low noise NR amplifiers that are also implemented as reflection amplifiers; they are used in applications like radio telescopes.\n\nNegative differential resistance devices are also used in switching circuits in which the device operates nonlinearly, changing abruptly from one state to another, with hysteresis. The advantage of using a negative resistance device is that a relaxation oscillator, flip-flop or memory cell can be built with a single active device, whereas the standard logic circuit for these functions, the Eccles-Jordan multivibrator, requires two active devices (transistors). Three switching circuits built with negative resistances are\n\nSome instances of neurons display regions of negative slope conductances (RNSC) in voltage-clamp experiments. The negative resistance here is implied were one to consider the neuron a typical Hodgkin–Huxley style circuit model.\n\nNegative resistance was first recognized during investigations of electric arcs, which were used for lighting during the 19th century. In 1881 Alfred Niaudet had observed that the voltage across arc electrodes decreased temporarily as the arc current increased, but many researchers thought this was a secondary effect due to temperature. The term \"negative resistance\" was applied by some to this effect, but the term was controversial because it was known that the resistance of a passive device could not be negative. Beginning in 1895 Hertha Ayrton, extending her husband William's research with a series of meticulous experiments measuring the \"I–V\" curve of arcs, established that the curve had regions of negative slope, igniting controversy. Frith and Rodgers in 1896 with the support of the Ayrtons introduced the concept of \"differential\" resistance, \"dv/di\", and it was slowly accepted that arcs had negative differential resistance. In recognition of her research, Hertha Ayrton became the first woman voted for induction into the Institute of Electrical Engineers.\n\nGeorge Francis FitzGerald first realized in 1892 that if the damping resistance in a resonant circuit could be made zero or negative, it would produce continuous oscillations. In the same year Elihu Thomson built a negative resistance oscillator by connecting an LC circuit to the electrodes of an arc, perhaps the first example of an electronic oscillator. William Duddell, a student of Ayrton at London Central Technical College, brought Thomson's arc oscillator to public attention. Due to its negative resistance, the current through an arc was unstable, and arc lights would often produce hissing, humming, or even howling noises. In 1899, investigating this effect, Duddell connected an LC circuit across an arc and the negative resistance excited oscillations in the tuned circuit, producing a musical tone from the arc. To demonstrate his invention Duddell wired several tuned circuits to an arc and played a tune on it. Duddell's \"singing arc\" oscillator was limited to audio frequencies. However, in 1903 Danish engineers Valdemar Poulsen and P. O. Pederson increased the frequency into the radio range by operating the arc in a hydrogen atmosphere in a magnetic field, inventing the Poulsen arc radio transmitter, which was widely used until the 1920s.\n\nBy the early 20th century, although the physical causes of negative resistance were not understood, engineers knew it could generate oscillations and had begun to apply it. Heinrich Barkhausen in 1907 showed that oscillators must have negative resistance. Ernst Ruhmer and Adolf Pieper discovered that mercury vapor lamps could produce oscillations, and by 1912 AT&T had used them to build amplifying repeaters for telephone lines.\n\nIn 1918 Albert Hull at GE discovered that vacuum tubes could have negative resistance in parts of their operating ranges, due to a phenomenon called secondary emission. In a vacuum tube when electrons strike the plate electrode they can knock additional electrons out of the surface into the tube. This represents a current \"away\" from the plate, reducing the plate current. Under certain conditions increasing the plate voltage causes a \"decrease\" in plate current. By connecting an LC circuit to the tube Hull created an oscillator, the dynatron oscillator. Other negative resistance tube oscillators followed, such as the magnetron invented by Hull in 1920.\n\nThe negative impedance converter originated from work by Marius Latour around 1920. He was also one of the first to report negative capacitance and inductance. A decade later, vacuum tube NICs were developed as telephone line repeaters at Bell Labs by George Crisson and others, which made transcontinental telephone service possible. Transistor NICs, pioneered by Linvill in 1953, initiated a great increase in interest in NICs and many new circuits and applications developed.\n\nNegative differential resistance in semiconductors was observed around 1909 in the first point-contact junction diodes, called cat's whisker detectors, by researchers such as William Henry Eccles and G. W. Pickard. They noticed that when junctions were biased with a DC voltage to improve their sensitivity as radio detectors, they would sometimes break into spontaneous oscillations. However the effect was not pursued.\n\nThe first person to exploit negative resistance diodes practically was Russian radio researcher Oleg Losev, who in 1922 discovered negative differential resistance in biased zincite (zinc oxide) point contact junctions. He used these to build solid-state amplifiers, oscillators, and amplifying and regenerative radio receivers, 25 years before the invention of the transistor. Later he even built a superheterodyne receiver. However his achievements were overlooked because of the success of vacuum tube technology. After ten years he abandoned research into this technology (dubbed \"Crystodyne\" by Hugo Gernsback), and it was forgotten.\n\nThe first widely used solid-state negative resistance device was the tunnel diode, invented in 1957 by Japanese physicist Leo Esaki. Because they have lower parasitic capacitance than vacuum tubes due to their small junction size, diodes can function at higher frequencies, and tunnel diode oscillators proved able to produce power at microwave frequencies, above the range of ordinary vacuum tube oscillators. Its invention set off a search for other negative resistance semiconductor devices for use as microwave oscillators, resulting in the discovery of the IMPATT diode, Gunn diode, TRAPATT diode, and others. In 1969 Kurokawa derived conditions for stability in negative resistance circuits. Currently negative differential resistance diode oscillators are the most widely used sources of microwave energy, and many new negative resistance devices have been discovered in recent decades.\n\n"}
{"id": "7108905", "url": "https://en.wikipedia.org/wiki?curid=7108905", "title": "Negative return (finance)", "text": "Negative return (finance)\n\nThe term negative return is used in business or finance to describe a loss, i.e., a negative return on investment. By extension the term is also used for a project that is not worthwhile, even in a non-economic sense.\n"}
{"id": "362728", "url": "https://en.wikipedia.org/wiki?curid=362728", "title": "Negative temperature", "text": "Negative temperature\n\nIn quantum thermodynamics, certain systems can achieve negative temperature; that is, their temperature can be expressed as a negative quantity on the Kelvin or Rankine scales.\n\nA system with a truly negative temperature on the Kelvin scale is \"hotter\" than any system with a positive temperature. If a negative-temperature system and a positive-temperature system come in contact, heat will flow from the negative- to the positive-temperature system. A standard example of such a system is population inversion in laser physics.\n\nTemperature is loosely interpreted as the average kinetic energy of the system's particles. The existence of negative temperature, let alone negative temperature representing \"hotter\" systems than positive temperature, would seem paradoxical in this interpretation. \nThe paradox is resolved by considering the more rigorous definition of thermodynamic temperature as the tradeoff between internal energy and entropy\ncontained in the system, with \"coldness\", the \"reciprocal\" of temperature, being the more fundamental quantity. \nSystems with a positive temperature will increase in entropy as one adds energy to the system, while \nsystems with a negative temperature will decrease in entropy as one adds energy to the system.\n\nClassical thermodynamic systems cannot achieve negative temperatures: adding heat always increases their entropy. \nThe possibility of a decrease in entropy as energy increases requires the system to \"saturate\" in entropy. This is only possible if the number of high energy states is limited. In classical Boltzmann statistics, the number of high energy states is unlimited (particle speeds can in principle be increased indefinitely).\nSystems bounded by a maximum amount of energy are generally forbidden in classical mechanics, and the phenomenon of negative temperature is strictly a \nquantum mechanical phenomenon. Some systems, however (see the examples below), have a maximum amount of energy that they can hold, and as they approach that maximum energy their entropy actually begins to decrease.\n\nThe definition of thermodynamic temperature as a function of the change in the system's entropy under reversible heat transfer \nformula_1:\nEntropy being a state function, the integral of over any cyclical process is zero.\nFor a system in which the entropy is purely a function of the system's energy , the temperature can be defined as:\n\nEquivalently, thermodynamic beta, or \"coldness\", is defined as \nwhere is the Boltzmann constant.\n\nNote that in classical thermodynamics, is defined in terms of temperature. This is reversed here, is the statistical entropy, a function of the possibly microstates of the system, and temperature conveys information on the distribution of energy levels among the possible microstates.\nFor systems with many degrees of freedom, the statistical and thermodynamic definitions of entropy are generally consistent with each other. \nHowever, for small systems and systems where the number of states decreases with energy, the definitions of statistical entropy and thermodynamic entropy are not necessarily consistent, and the temperatures derived from these entropies are different. \n\nSome theorists have proposed using an alternate definition of entropy originally proposed by Gibbs as a way to resolve these inconsistencies, although this new definition would create other inconsistencies.\n\nNegative temperatures can only exist in a system where there are a limited number of energy states (see below). As the temperature is increased on such a system, particles move into higher and higher energy states, and as the temperature increases, the number of particles in the lower energy states and in the higher energy states approaches equality. (This is a consequence of the definition of temperature in statistical mechanics for systems with limited states.) By injecting energy into these systems in the right fashion, it is possible to create a system in which there are more particles in the higher energy states than in the lower ones. The system can then be characterised as having a negative temperature.\n\nA substance with a negative temperature is not colder than absolute zero, but rather it is hotter than infinite temperature. As Kittel and Kroemer (p. 462) put it, \"The temperature scale from cold to hot runs:\n\nThe corresponding inverse temperature scale, for the quantity \"β\" = 1/\"kT\" (where \"k\" is Boltzmann's constant), runs continuously from low energy to high as +∞, … , 0, …, −∞. Because it avoids the abrupt jump from +∞ to −∞, β is considered more natural than \"T\". Although a system can have multiple negative temperature regions and thus have -∞ to +∞ discontinuities.\n\nIn many familiar physical systems, temperature is associated to the kinetic energy of atoms. Since there is no upper bound on the momentum of an atom, there is no upper bound to the number of energy states available when more energy is added, and therefore no way to get to a negative temperature. However, in statistical mechanics, temperature can correspond to other degrees of freedom than just kinetic energy (see below).\n\nThe distribution of energy among the various translational, vibrational, rotational, electronic, and nuclear modes of a system determines the macroscopic temperature. In a \"normal\" system, thermal energy is constantly being exchanged between the various modes.\n\nHowever, in some situations, it is possible to isolate one or more of the modes. In practice, the isolated modes still exchange energy with the other modes, but the time scale of this exchange is much slower than for the exchanges within the isolated mode. One example is the case of nuclear spins in a strong external magnetic field. In this case, energy flows fairly rapidly among the spin states of interacting atoms, but energy transfer between the nuclear spins and other modes is relatively slow. Since the energy flow is predominantly within the spin system, it makes sense to think of a spin temperature that is distinct from the temperature associated to other modes.\n\nA definition of temperature can be based on the relationship:\n\nThe relationship suggests that a \"positive temperature\" corresponds to the condition where entropy, \"S\", increases as thermal energy, \"q\", is added to the system. This is the \"normal\" condition in the macroscopic world, and is always the case for the translational, vibrational, rotational, and non-spin related electronic and nuclear modes. The reason for this is that there are an infinite number of these types of modes, and adding more heat to the system increases the number of modes that are energetically accessible, and thus increases the entropy.\n\nThe simplest example, albeit a rather nonphysical one, is to consider a system of \"N\" particles, each of which can take an energy of either \"+ε\" or \"-ε\" but are otherwise noninteracting. This can be understood as a limit of the Ising model in which the interaction term becomes negligible. The total energy of the system is\n\nwhere \"σ\" is the sign of the \"i\"th particle and \"j\" is the number of particles with positive energy minus the number of particles with negative energy. From elementary combinatorics, the total number of microstates with this amount of energy is a binomial coefficient:\n\nBy the fundamental assumption of statistical mechanics, the entropy of this microcanonical ensemble is\n\nWe can solve for thermodynamic beta (β = 1/kT) by considering it as a central difference without taking the continuum limit:\n\nhence the temperature\n\nThis entire proof assumes the microcanonical ensemble with energy fixed and temperature being the emergent property. In the canonical ensemble, the temperature is fixed and energy is the emergent property. This leads to (formula_14 refers to microstates):\nFollowing the previous example, we choose a state with two levels and two particles. This leads to microstates formula_18, formula_19, formula_20, and formula_21.\nThe resulting values for formula_28, formula_29, and formula_30 are all increasing with formula_31 and never need to enter a negative temperature regime.\n\nThe previous example is approximately realized by a system of nuclear spins in an external magnetic field. This allows the experiment to be run as a variation of nuclear magnetic resonance spectroscopy. In the case of electronic and nuclear spin systems, there are only a finite number of modes available, often just two, corresponding to spin up and spin down. In the absence of a magnetic field, these spin states are \"degenerate\", meaning that they correspond to the same energy. When an external magnetic field is applied, the energy levels are split, since those spin states that are aligned with the magnetic field will have a different energy from those that are anti-parallel to it.\n\nIn the absence of a magnetic field, such a two-spin system would have maximum entropy when half the atoms are in the spin-up state and half are in the spin-down state, and so one would expect to find the system with close to an equal distribution of spins. Upon application of a magnetic field, some of the atoms will tend to align so as to minimize the energy of the system, thus slightly more atoms should be in the lower-energy state (for the purposes of this example we will assume the spin-down state is the lower-energy state). It is possible to add energy to the spin system using radio frequency (RF) techniques. This causes atoms to \"flip\" from spin-down to spin-up.\n\nSince we started with over half the atoms in the spin-down state, this initially drives the system towards a 50/50 mixture, so the entropy is increasing, corresponding to a positive temperature. However, at some point, more than half of the spins are in the spin-up position. In this case, adding additional energy reduces the entropy, since it moves the system further from a 50/50 mixture. This reduction in entropy with the addition of energy corresponds to a negative temperature. In NMR spectroscopy, this corresponds to pulses with a pulse width of over 180° (for a given spin). While relaxation is fast in solids, it can take several seconds in solutions and even longer in gases and in ultracold systems; several hours were reported for silver and rhodium at picokelvin temperatures. It is still important to understand that the temperature is negative only with respect to nuclear spins. Other degrees of freedom, such as molecular vibrational, electronic and electron spin levels are at a positive temperature, so the object still has positive sensible heat. Relaxation actually happens by exchange of energy between the nuclear spin states and other states (e.g. through the nuclear Overhauser effect with other spins).\n\nThis phenomenon can also be observed in many lasing systems, wherein a large fraction of the system's atoms (for chemical and gas lasers) or electrons (in semiconductor lasers) are in excited states. This is referred to as a population inversion.\n\nThe Hamiltonian for a single mode of a luminescent radiation field at frequency \"ν\" is\nThe density operator in the grand canonical ensemble is\nFor the system to have a ground state, the trace to converge, and the density operator to be generally meaningful, \"βH\" must be positive semidefinite. So if \"hν\" < \"μ\", and \"H\" is negative semidefinite, then \"β\" must itself be negative, implying a negative temperature.\n\nThe two-dimensional systems can exist in negative temperature states.\n\nNegative temperatures have also been achieved in motional degrees of freedom. Using an optical lattice, upper bounds were placed on the kinetic energy, interaction energy and potential energy of cold atoms. This was done by tuning the interactions of the atoms from repulsive to attractive using a Feshbach resonance and changing the overall harmonic potential from trapping to anti-trapping, thus transforming the Bose-Hubbard Hamiltonian from formula_34. Performing this transformation adiabatically while keeping the atoms in the Mott insulator regime, it is possible to go from a low entropy positive temperature state to a low entropy negative temperature state. In the negative temperature state, the atoms macroscopically occupy the maximum momentum state of the lattice. The negative temperature ensembles equilibrated and showed long lifetimes in an anti-trapping harmonic potential.\n\n\n"}
{"id": "577248", "url": "https://en.wikipedia.org/wiki?curid=577248", "title": "New riddle of induction", "text": "New riddle of induction\n\nGrue and bleen are examples of logical predicates coined by Nelson Goodman in \"Fact, Fiction, and Forecast\" to illustrate the \"new riddle of induction\". These predicates are unusual because their application is time-dependent; many have tried to solve the new riddle on those terms, but Hilary Putnam and others have argued such time-dependency depends on the language adopted, and in some languages it is equally true for natural-sounding predicates such as \"green.\" For Goodman they illustrate the problem of projectible predicates and ultimately, which empirical generalizations are law-like and which are not.\nGoodman's construction and use of \"grue\" and \"bleen\" illustrates how philosophers use simple examples in conceptual analysis.\n\nGoodman defined grue relative to an arbitrary but fixed time \"t\" as follows: An object is grue if and only if it is observed before \"t\" and is green, or else is not so observed and is blue. An object is bleen if and only if it is observed before \"t\" and is blue, or else is not so observed and is green.\n\nTo understand the problem Goodman posed, it is helpful to imagine some arbitrary future time \"t\", say January 1, 10. For all green things we observe up to time \"t\", such as emeralds and well-watered grass, both the predicates \"green\" and \"grue\" apply. Likewise for all blue things we observe up to time \"t\", such as bluebirds or blue flowers, both the predicates \"blue\" and \"bleen\" apply. On January 2, 10, however, emeralds and well-watered grass are \"bleen\" and bluebirds or blue flowers are \"grue\". Clearly, the predicates \"grue\" and \"bleen\" are not the kinds of predicates we use in everyday life or in science, but the problem is that they apply in just the same way as the predicates \"green\" and \"blue\" up until some future time \"t\". From our current perspective (i.e., before time \"t\"), how can we say which predicates are more projectible into the future: \"green\" and \"blue\" or \"grue\" and \"bleen\"?\n\nIn this section, Goodman's new riddle of induction is outlined in order to set the context for his introduction of the predicates \"grue\" and \"bleen\" and thereby illustrate their philosophical importance.\n\nGoodman poses Hume's problem of induction as a problem of the validity of the predictions we make. Since predictions are about what has yet to be observed and because there is no necessary connection between what has been observed and what will be observed, what is the justification for the predictions we make? We cannot use deductive logic to infer predictions about future observations based on past observations because there are no valid rules of deductive logic for such inferences. Hume's answer was that our observations of one kind of event following another kind of event result in our minds forming habits of regularity (i.e., associating one kind of event with another kind). The predictions we make are then based on these regularities or habits of mind we have formed.\n\nGoodman takes Hume's answer to be a serious one. He rejects other philosophers' objection that Hume is merely explaining the origin of our predictions and not their justification. His view is that Hume has identified something deeper. To illustrate this, Goodman turns to the problem of justifying a system of rules of deduction. For Goodman, the validity of a deductive system is justified by its conformity to good deductive practice. The justification of rules of a deductive system depends on our judgements about whether to reject or accept specific deductive inferences. Thus, for Goodman, the problem of induction dissolves into the same problem as justifying a deductive system and while, according to Goodman, Hume was on the right track with habits of mind, the problem is more complex than Hume realized.\n\nIn the context of justifying rules of induction, this becomes the problem of confirmation of generalizations for Goodman. However, the confirmation is not a problem of justification but instead it is a problem of precisely defining how evidence confirms generalizations. It is with this turn that \"grue\" and \"bleen\" have their philosophical role in Goodman's view of induction.\n\nThe new riddle of induction, for Goodman, rests on our ability to distinguish \"lawlike\" from \"non-lawlike\" generalizations. \"Lawlike\" generalizations are capable of confirmation while \"non-lawlike\" generalizations are not. \"Lawlike\" generalizations are required for making predictions. Using examples from Goodman, the generalization that all copper conducts electricity is capable of confirmation by a particular piece of copper whereas the generalization that all men in a given room are third sons is not \"lawlike\" but accidental. The generalization that all copper conducts electricity is a basis for predicting that this piece of copper will conduct electricity. The generalization that all men in a given room are third sons, however, is not a basis for predicting that a given man in that room is a third son.\n\nWhat then makes some generalizations \"lawlike\" and others accidental? This, for Goodman, becomes a problem of determining which predicates are projectible (i.e., can be used in \"lawlike\" generalizations that serve as predictions) and which are not. Goodman argues that this is where the fundamental problem lies. This problem, known as \"Goodman's paradox\", is as follows. Consider the evidence that all emeralds examined thus far have been green. This leads us to conclude (by induction) that all future emeralds will be green. However, whether this prediction is \"lawlike\" or not depends on the predicates used in this prediction. Goodman observed that (assuming \"t\" has yet to pass) it is equally true that every emerald that has been observed is \"grue\". Thus, by the same evidence we can conclude that all future emeralds will be \"grue\". The new problem of induction becomes one of distinguishing projectible predicates such as \"green\" and \"blue\" from non-projectible predicates such as \"grue\" and \"bleen\".\n\nHume, Goodman argues, missed this problem. We do not, by habit, form generalizations from all associations of events we have observed but only some of them. All past observed emeralds were green, and we formed a habit of thinking the next emerald will be green, but they were equally grue, and we do not form habits concerning grueness. \"Lawlike\" predictions (or projections) ultimately are distinguishable by the predicates we use. Goodman's solution is to argue that \"lawlike\" predictions are based on projectible predicates such as \"green\" and \"blue\" and not on non-projectible predicates such as \"grue\" and \"bleen\" and what makes predicates projectible is their \"entrenchment\", which depends on their successful past projections. Thus, \"grue\" and \"bleen\" function in Goodman's arguments to both illustrate the new riddle of induction and to illustrate the distinction between projectible and non-projectible predicates via their relative entrenchment.\n\nThe most obvious response is to point to the artificially disjunctive definition of grue. The notion of predicate \"entrenchment\" is not required. Goodman, however, noted that this move will not work. If we take \"grue\" and \"bleen\" as primitive predicates, we can define green as \"\"grue\" if first observed before \"t\" and \"bleen\" otherwise\", and likewise for blue. To deny the acceptability of this disjunctive definition of green would be to beg the question.\n\nAnother proposed resolution of the paradox (which Goodman addresses and rejects) that does not require predicate \"entrenchment\" is that \"\"x\" is grue\" is not solely a predicate of \"x\", but of \"x\" and a time \"t\"—we can know that an object is green without knowing the time \"t\", but we cannot know that it is grue. If this is the case, we should not expect \"\"x\" is grue\" to remain true when the time changes. However, one might ask why \"\"x\" is green\" is \"not\" considered a predicate of a particular time \"t\"—the more common definition of \"green\" does not require any mention of a time \"t\", but the definition \"grue\" does. As we have just seen, this response also begs the question because \"blue\" can be defined in terms of \"grue\" and \"bleen\", which explicitly refer to time.\n\nRichard Swinburne gets past the objection that green may be redefined in terms of \"grue\" and \"bleen\" by making a distinction based on how we test for the applicability of a predicate in a particular case. He distinguishes between qualitative and locational predicates. Qualitative predicates, like green, \"can\" be assessed without knowing the spatial or temporal relation of \"x\" to a particular time, place or event. Locational predicates, like \"grue\", \"cannot\" be assessed without knowing the spatial or temporal relation of \"x\" to a particular time, place or event, in this case whether \"x\" is being observed before or after time \"t\". Although green can be given a definition in terms of the locational predicates \"grue\" and \"bleen\", this is irrelevant to the fact that green meets the criterion for being a qualitative predicate whereas \"grue\" is merely locational. He concludes that if some \"x\"'s under examination—like emeralds—satisfy both a qualitative and a locational predicate, but projecting these two predicates yields conflicting predictions, namely, whether emeralds examined after time \"t\" shall appear blue or green, we should project the qualitative predicate, in this case green.\n\nRudolf Carnap responded to Goodman's 1946 article. Carnap's approach to inductive logic is based on the notion of \"degree of confirmation\" \"c\"(\"h\",\"e\") of a given hypothesis \"h\" by a given evidence \"e\". Both \"h\" and \"e\" are logical formulas expressed in a simple language \"L\" which allows for\nThe universe of discourse consists of denumerably many individuals, each of which is designated by its own constant symbol; such individuals are meant to be regarded as positions (\"like space-time points in our actual world\") rather than extended physical bodies. A state description is a (usually infinite) conjunction containing every possible ground atomic sentence, either negated or unnegated; such a conjunction describes a possible state of the whole universe. Carnap requires the following semantic properties:\nCarnap distinguishes three kinds of properties:\nTo illuminate this taxonomy, let \"x\" be a variable and \"a\" a constant symbol; then an example of 1. could be \"\"x\" is blue or \"x\" is non-warm\", an example of 2. \"\"x\" = \"a\", and an example of 3. \"x\" is red and not \"x\" = \"a\"\".\n\nBased on his theory of inductive logic sketched above, Carnap formalizes Goodman's notion of projectibility of a property \"W\" as follows: the higher the relative frequency of \"W\" in an observed sample, the higher is the probability that a non-observed individual has the property \"W\". Carnap suggests \"as a tentative answer\" to Goodman, that all purely qualitative properties are projectible, all purely positional properties are non-projectible, and mixed properties require further investigation.\n\nWillard Van Orman Quine discusses an approach to consider only \"natural kinds\" as projectible predicates.\nHe first relates Goodman's grue paradox to Hempel's raven paradox by defining two predicates \"F\" and \"G\" to be (simultaneously) projectible if all their shared instances count toward confirmation of the claim \"each \"F\" is a \"G\"\". Then Hempel's paradox just shows that the complements of projectible predicates (such as \"is a raven\", and \"is black\") need not be projectible, while Goodman's paradox shows that \"is green\" is projectible, but \"is grue\" is not.\n\nNext, Quine reduces projectibility to the subjective notion of \"similarity\". Two green emeralds are usually considered more similar than two grue ones if only one of them is green. Observing a green emerald makes us expect a similar observation (i.e., a green emerald) next time. Green emeralds are a \"natural kind\", but grue emeralds are not. Quine investigates \"the dubious scientific standing of a general notion of similarity, or of kind\". Both are basic to thought and language, like the logical notions of e.g. identity, negation, disjunction. However, it remains unclear how to relate the logical notions to \"similarity\" or \"kind\"; Quine therefore tries to relate at least the latter two notions to each other.\n\nRelation between similarity and kind\n\nAssuming finitely many \"kinds\" only, the notion of \"similarity\" can be defined by that of \"kind\": an object \"A\" is more similar to \"B\" than to \"C\" if \"A\" and \"B\" belong jointly to more kinds than \"A\" and \"C\" do.\n\nVice versa, it remains again unclear how to define \"kind\" by \"similarity\". Defining e.g. the kind of red things as the set of all things that are more similar to a fixed \"paradigmatical\" red object than this is to another fixed \"foil\" non-red object (cf. left picture) isn't satisfactory, since the degree of overall similarity, including e.g. shape, weight, will afford little evidence of degree of redness. (In the picture, the yellow paprika might be considered more similar to the red one than the orange.)\n\nAn alternative approach inspired by Carnap defines a natural kind to be a set whose members are more similar to each other than each non-member is to at least one member. \nHowever, Goodman argued, that this definition would make the set of all red round things, red wooden things, and round wooden things (cf. right picture) meet the proposed definition of a natural kind, while \"surely it is not what anyone means by a kind\".\n\nWhile neither of the notions of similarity and kind can be defined by the other, they at least vary together: if \"A\" is reassessed to be more similar to \"C\" than to \"B\" rather than the other way around, the assignment of \"A\", \"B\", \"C\" to kinds will be permuted correspondingly; and conversely.\n\nBasic importance of similarity and kind\n\nIn language, every general term owes its generality to some resemblance of the things referred to. Learning to use a word depends on a double resemblance, viz. between the present and past circumstances in which the word was used, and between the present and past phonetic utterances of the word.\n\nEvery reasonable expectation depends on resemblance of circumstances, together with our tendency to expect similar causes to have similar effects. This includes any scientific experiment, since it can be reproduced only under similar, but not under completely identical, circumstances. Already Heraclitus' famous saying \"No man ever steps in the same river twice\" highlighted the distinction between similar and identical circumstances.\n\nGenesis of similarity and kind\n\nIn a behavioral sense, humans and other animals have an innate standard of similarity. It is part of our animal birthright, and characteristically animal in its lack of intellectual status, e.g. its alieness to mathematics and logic, cf. bird example.\n\nInduction itself is essentially animal expectation or habit formation.\nOstensive learning\nis a case of induction, and a curiously comfortable one, since each man's spacing of qualities and kind is enough like his neighbor's.\nIn contrast, the \"brute irrationality of our sense of similarity\" offers little reason to expect it being somehow in tune with the unanimated nature, which we never made.\nWhy inductively obtained theories about it should be trusted is the perennial philosophical problem of induction. Quine, following Watanabe,\nsuggests Darwin's theory as an explanation: if people's innate spacing of qualities is a gene-linked trait, then the spacing that has made for the most successful inductions will have tended to predominate through natural selection.\nHowever, this cannot account for the human ability to dynamically refine one's spacing of qualities in the course of getting acquainted with a new area.\n\nIn his book \"Wittgenstein on Rules and Private Language\", Saul Kripke proposed a related argument that leads to skepticism about meaning rather than skepticism about induction, as part of his personal interpretation (nicknamed \"Kripkenstein\" by some) of the private language argument. He proposed a new form of addition, which he called \"quus\", which is identical with \"+\" in all cases except those in which either of the numbers added are equal to or greater than 57; in which case the answer would be 5, i.e.:\n\nHe then asks how, given certain obvious circumstances, anyone could know that previously when I thought I had meant \"+\", I had not actually meant \"quus\". Kripke then argues for an interpretation of Wittgenstein as holding that the meanings of words are not individually contained mental entities.\n\n\n"}
{"id": "4849201", "url": "https://en.wikipedia.org/wiki?curid=4849201", "title": "Object of the mind", "text": "Object of the mind\n\nAn object of the mind is an object that exists in the imagination, but which, in the real world, can only be represented or modeled. Some such objects are abstractions, literary concepts, or fictional scenarios.\n\nClosely related are intentional objects, which are what thoughts and feelings are about, even if they are not about anything real (such as thoughts \nabout unicorns, or feelings of apprehension about a dental appointment which is subsequently cancelled). However, intentional objects may coincide with real objects (as in thoughts about horses, or a feeling of regret about a missed appointment).\n\nMathematics and geometry describe abstract objects that sometimes correspond to familiar shapes, and sometimes do not. Circles, triangles, rectangles, and so forth describe two-dimensional shapes that are often found in the real world. However, mathematical formulas do not describe individual physical circles, triangles, or rectangles. They describe ideal shapes that are objects of the mind. The incredible precision of mathematical expression permits a vast applicability of mental abstractions to real life situations.\n\nMany more mathematical formulas describe shapes that are unfamiliar, or do not necessarily correspond to objects in the real world. For example, the Klein bottle is a one-sided, sealed surface with no inside or outside (in other words, it is the three-dimensional equivalent of the Möbius strip). Such objects can be represented by twisting and cutting or taping pieces of paper together, as well as by computer simulations. To hold them in the imagination, abstractions such as extra or fewer dimensions are necessary.\n\nIf-then arguments posit logical sequences that sometimes include objects of the mind. For example, a counterfactual argument proposes a hypothetical or subjunctive possibility which \"could\" or \"would\" be true, but \"might not\" be false. Conditional sequences involving subjunctives use intensional language, which is studied by modal logic, whereas classical logic studies the extensional language of necessary and sufficient conditions.\n\nIn general, a logical antecedent is a sufficient condition, and a logical consequent is a necessary condition(or the contingency) in a logical conditional. But logical conditionals accounting only for necessity and sufficiency do not always reflect every day if-then reasoning, and for this reason they are sometimes known as material conditionals. In contrast, indicative conditionals, sometimes known as non-material conditionals, attempt to describe if-then reasoning involving hypotheticals, fictions, or counterfactuals.\n\nTruth tables for if-then statements identify four unique combinations of premises and conclusions: true premises and true conclusions; false premises and true conclusions; true premises and false conclusions; false premises and false conclusions. Strict conditionals assign a positive truth-value to every case except the case of a true premise and a false conclusion. This is sometimes regarded as counterintuitive, but makes more sense when false conditions are understood as objects of the mind.\n\nA false antecedent is a premise known to be false, fictional, imaginary, or unnecessary. In a conditional sequence, a false antecedent may be the basis for any consequence, true or false.\n\nThe subjects of literature are sometimes false antecedents. For example, the contents of false documents, the origins of stand-alone phenomena, or the implications of loaded words. Also, artificial sources, personalities, events, and histories. False antecedents are sometimes referred to as \"nothing\", or \"nonexistent\", whereas nonexistent referents are not referred to.\n\nArt and acting often portray scenarios without any antecedent except an artist's imagination. For example, mythical heroes, legendary creatures, gods, and goddesses.\n\nA false consequent, in contrast, is a conclusion known to be false, fictional, imaginary, or insufficient. In a conditional statement, a fictional conclusion is known as a non sequitur, which literally means \"out of sequence\". A conclusion that is out of sequence is not contingent on any premises that precede it, and it does not follow from them, so such a sequence is not conditional. A conditional sequence is a connected series of statements. A false consequent cannot follow from true premises in a connected sequence. But, on the other hand, a false consequent can follow from a false antecedent.\n\nAs an example, the name of a team, a genre, or a nation is a collective term applied ex post facto to a group of distinct individuals. None of the individuals on a sports team is the team itself, nor is any musical chord a genre, nor any person America. The name is an identity for a collection that is connected by consensus or reference, but not by sequence. A different name could equally follow, but it would have different social or political significance.\n\nIn philosophy, mind-body dualism is the doctrine that mental activities exist apart from the physical body, notably posited by René Descartes in \"Meditations on First Philosophy\".\n\nMany objects in fiction follow the example of false antecedents or false consequents. For example, \"The Lord of the Rings\" by J.R.R. Tolkien is based on an imaginary book. In the \"Appendices\" to \"The Lord of the Rings\", Tolkien's characters name the \"Red Book of Westmarch\" as the source material for \"The Lord of the Rings\", which they describe as a translation. But the \"Red Book of Westmarch\" is a fictional document that chronicles events in an imaginary world. One might imagine a different translation, by another author.\n\nSocial reality is composed of many standards and inventions that facilitate communication, but which are ultimately objects of the mind. For example, money is an object of the mind which currency represents. Similarly, languages signify ideas and thoughts.\n\nObjects of the mind are frequently involved in the roles that people play. For example, acting is a profession which predicates real jobs on fictional premises. Charades is a game people play by guessing imaginary objects from short play-acts.\n\nImaginary personalities and histories are sometimes invented to enhance the verisimilitude of fictional universes, and/or the immersion of role-playing games. In the sense that they exist independently of extant personalities and histories, they are believed to be fictional characters and fictional time frames.\n\nScience fiction is abundant with future times, alternate times, and past times that are objects of the mind. For example, in the novel \"Nineteen Eighty-Four\" by George Orwell, the number 1984 represented a year that had not yet passed.\n\nCalendar dates also represent objects of the mind, specifically, past and future times. In \"\", which was released in 1986, the narration opens with the statement, \"It is the year 2005.\" In 1986, that statement was futuristic. During the year 2005, that reference to the year 2005 was factual. Now, \"The Transformers: The Movie\" is retro-futuristic. The number 2005 did not change, but the object of the mind that it represents did change.\n\nDeliberate invention also may reference an object of the mind. The intentional invention of fiction for the purpose of deception is usually referred to as lying, in contrast to invention for entertainment or art. Invention is also often applied to problem solving. In this sense the physical invention of materials is associated with the mental invention of fictions.\n\nConvenient fictions also occur in science.\n\nThe theoretical posits of one era's scientific theories may be demoted to mere objects of the mind by subsequent discoveries: some standard examples include phlogiston and ptolemaic epicycles.\n\nThis raises questions, in the debate between scientific realism and instrumentalism about the status of current posits, such as black holes and quarks. Are they still merely intentional, even if the theory is correct?\n\nThe situation is further complicated by the existence in scientific practice of entities which are explicitly held not to be real, but which nonetheless serve a purpose—convenient fictions. Examples include field lines, centers of gravity, and electron holes in semiconductor theory.\n\nA reference that names an imaginary source is in some sense also a self-reference. A self-reference automatically makes a comment about itself. Premises that name themselves as premises are premises by self-reference; conclusions that name themselves as conclusions are conclusions by self-reference.\n\nIn their respective imaginary worlds the \"Necronomicon\", \"The Hitchhiker's Guide to the Galaxy\", and the \"Red Book of Westmarch\" are realities, but only because they are referred to as real. Authors use this technique to invite readers to pretend or to make-believe that their imaginary world is real. In the sense that the stories that quote these books are true, the quoted books exist; in the sense that the stories are fiction, the quoted books do not exist.\n\nAustrian philosopher Alexius Meinong (1853–1920) advanced nonexistent objects in the 19th and 20th century within a “theory of objects”. He was interested in intentional states which are directed at nonexistent objects. Starting with the “principle of intentionality”, mental phenomena are intentionally directed towards an object. People may imagine, desire or fear something that does not exist. Other philosophers concluded that intentionality is not a real relation and therefore does not require the existence of an object, while Meinong concluded there is an object for every mental state whatsoever—if not an existent then at least a nonexistent one.\n\n\n"}
{"id": "19777249", "url": "https://en.wikipedia.org/wiki?curid=19777249", "title": "Organizing principle", "text": "Organizing principle\n\nAn organizing principle is a core assumption from which everything else by proximity can derive a classification or a value. It is like a central reference point that allows all other objects to be located, often used in a conceptual framework. Having an organizing principle might help one simplify and get a handle on a particularly complicated domain or phenomenon. On the other hand, it might create a deceptive prism that colors one's judgment.\n\n\n\n"}
{"id": "1005874", "url": "https://en.wikipedia.org/wiki?curid=1005874", "title": "Principle", "text": "Principle\n\nA principle is a concept or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 \"Principles of Operation\".\n\nExamples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.\n\nIn common English, it is a substantive and collective term referring to rule governance, the absence of which, being \"unprincipled\", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only \"in principle\" but not in fact.\n\nA principle represents values that orient and rule the conduct of persons in a particular society. To \"act on principle\" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.\n\nIt represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.\n\nArchimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the \"population principle\", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.\n\nThe principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\nHowever, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that \"the Earth is round\", corresponds to a direct relation between the subject and the predicate.\n\nAccording to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.\n\nThe principle of the excluding third or \"principium tertium exclusum\" is a principle of the traditional logic formulated canonically by Leibniz as: either \"A\" is \"B\" or \"A\" isn't \"B\". It is read the following way: either \"P\" is true, or its denial ¬\"P\" is.\nIt is also known as \"tertium non datur\" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).\n"}
{"id": "952842", "url": "https://en.wikipedia.org/wiki?curid=952842", "title": "Principle of compositionality", "text": "Principle of compositionality\n\nIn mathematics, semantics, and philosophy of language, the principle of compositionality is the principle that the meaning of a complex expression is determined by the meanings of its constituent expressions and the rules used to combine them. This principle is also called Frege's principle, because Gottlob Frege is widely credited for the first modern formulation of it. However, the idea appears already among Indian philosophers of grammar such as Yāska, and also in Plato's work such as in \"Theaetetus\".\nBesides, the principle was never explicitly stated by Frege, and it was arguably already assumed by George Boole decades before Frege’s work.\n\nThe principle of compositionality states that in a meaningful sentence, if the lexical parts are taken out of the sentence, what remains will be the rules of composition. Take, for example, the sentence \"Socrates was a man\". Once the meaningful lexical items are taken away—\"Socrates\" and \"man\"—what is left is the pseudo-sentence, \"S was a M\". The task becomes a matter of describing what the connection is between S and M.\n\nIt is frequently taken to mean that every operation of the syntax should be associated with an operation of the semantics that acts on the meanings of the constituents combined by the syntactic operation. As a guideline for constructing semantic theories, this is generally taken, as in the influential work on the philosophy of language by Donald Davidson, to mean that every construct of the syntax should be associated by a clause of the T-schema with an operator in the semantics that specifies how the meaning of the whole expression is built from constituents combined by the syntactic rule. In some general mathematical theories (especially those in the tradition of Montague grammar), this guideline is taken to mean that the interpretation of a language is essentially given by a homomorphism between an algebra of syntactic representations and an algebra of semantic objects.\n\nThe principle of compositionality also exists in a similar form in the compositionality of programming languages.\n\nThe principle of compositionality has been the subject of intense debate. Indeed, there is no general agreement as to how the principle is to be interpreted, although there have been several attempts to provide formal definitions of it. (Szabó, 2012)\n\nScholars are also divided as to whether the principle should be regarded as a factual claim, open to empirical testing; an analytic truth, obvious from the nature of language and meaning; or a methodological principle to guide the development of theories of syntax and semantics. The Principle of Compositionality has been attacked in all three spheres, although so far none of the criticisms brought against it have been generally regarded as compelling. Most proponents of the principle, however, make certain exceptions for idiomatic expressions in natural language. (Szabó, 2012)\n\nFurther, in the context of the philosophy of language, the principle of compositionality does not explain all of meaning. For example, you cannot infer sarcasm purely on the basis of words and their composition, yet a phrase used sarcastically means something completely different from the same phrase uttered straightforwardly. Thus, some theorists argue that the principle has to be revised to take into account linguistic and extralinguistic context, which includes the tone of voice used, common ground between the speakers, the intentions of the speaker, and so on. (Szabó, 2012)\n\n\n"}
{"id": "591394", "url": "https://en.wikipedia.org/wiki?curid=591394", "title": "Principle of explosion", "text": "Principle of explosion\n\nThe principle of explosion (Latin: \"ex falso (sequitur) quodlibet\" (EFQ), \"from falsehood, anything (follows)\", or \"ex contradictione (sequitur) quodlibet\" (ECQ), \"from contradiction, anything (follows)\"), or the principle of Pseudo-Scotus, is the law of classical logic, intuitionistic logic and similar logical systems, according to which any statement can be proven from a contradiction. That is, once a contradiction has been asserted, any proposition (including their negations) can be inferred from it. This is known as deductive explosion. The proof of this principle was first given by 12th century French philosopher William of Soissons.\n\nAs a demonstration of the principle, consider two contradictory statements – \"All lemons are yellow\" and \"Not all lemons are yellow\", and suppose (for the sake of argument) that both are simultaneously true. If that is the case, anything can be proven, e.g. \"unicorns exist\", by using the following argument:\n\nDue to the principle of explosion, the existence of a contradiction (inconsistency) in a formal axiomatic system is disastrous; since any statement can be proved true it trivializes the concepts of truth and falsity. Around the turn of the 20th century, the discovery of contradictions such as Russell's paradox at the foundations of mathematics thus threatened the entire structure of mathematics. Mathematicians such as Gottlob Frege, Ernst Zermelo, Abraham Fraenkel, and Thoralf Skolem put much effort into revising set theory to eliminate these contradictions, resulting in the modern Zermelo–Fraenkel set theory. \n\nIn a different solution to these problems, a few mathematicians have devised alternate theories of logic called paraconsistent logics, which eliminate the principle of explosion. These allow some contradictory statements to be proved without affecting other proofs.\n\nIn symbolic logic, the principle of explosion can be expressed in the following way\n\nBelow is a formal proof of the principle using symbolic logic\n\nThis is just the symbolic version of the informal argument given in the introduction, with formula_3 standing for \"all lemons are yellow\" and formula_6 standing for \"Unicorns exist\". From \"all lemons are yellow and not all lemons are yellow\" (1), we infer \"all lemons are yellow\" (2) and \"not all lemons are yellow\" (3); from \"all lemons are yellow\" (2), we infer \"all lemons are yellow or unicorns exist\" (4); and from \"not all lemons are yellow\" (3) and \"all lemons are yellow or unicorns exist\" (4), we infer \"unicorns exist\" (5). Hence, if all lemons are yellow and not all lemons are yellow, then unicorns exist.\n\nAn alternate argument for the principle stems from model theory. A sentence formula_3 is a \"semantic consequence\" of a set of sentences formula_11 only if every model of formula_11 is a model of formula_3. But there is no model of the contradictory set formula_14. A fortiori, there is no model of formula_14 that is not a model of formula_6. Thus, vacuously, every model of formula_14 is a model of formula_6. Thus formula_6 is a semantic consequence of formula_14.\n\nParaconsistent logics have been developed that allow for sub-contrary forming operators. Model-theoretic paraconsistent logicians often deny the assumption that there can be no model of formula_21 and devise semantical systems in which there are such models. Alternatively, they reject the idea that propositions can be classified as true or false. Proof-theoretic paraconsistent logics usually deny the validity of one of the steps necessary for deriving an explosion, typically including disjunctive syllogism, disjunction introduction, and reductio ad absurdum.\n\nThe metamathematical value of the principle of explosion is that for any logical system where this principle holds, any derived theory which proves ⊥ (or an equivalent form, formula_22) is worthless because \"all\" its statements would become theorems, making it impossible to distinguish truth from falsehood. That is to say, the principle of explosion is an argument for the law of non-contradiction in classical logic, because without it all truth statements become meaningless.\n\n"}
{"id": "11585926", "url": "https://en.wikipedia.org/wiki?curid=11585926", "title": "Principle of humanity", "text": "Principle of humanity\n\nIn philosophy and rhetoric, the principle of humanity states that when interpreting another speaker we must assume that his or her beliefs and desires are connected to each other and to reality in some way, and attribute to him or her \"the propositional attitudes one supposes one would have oneself in those circumstances\". The principle of humanity was named by Richard Grandy (then an assistant professor of philosophy at Princeton University) who first expressed it in 1973.\n\n"}
{"id": "8924176", "url": "https://en.wikipedia.org/wiki?curid=8924176", "title": "Principle of nonvacuous contrast", "text": "Principle of nonvacuous contrast\n\nThe principle of nonvacuous contrast is a logical or methodological principle which requires that a genuine predicate never refer to everything, or to nothing, within its universe of discourse.\n"}
{"id": "244755", "url": "https://en.wikipedia.org/wiki?curid=244755", "title": "Sense and reference", "text": "Sense and reference\n\nIn the philosophy of language, the distinction between sense and reference was an innovation of the German philosopher and mathematician Gottlob Frege in 1892 (in his paper \"On Sense and Reference\"; German: \"Über Sinn und Bedeutung\"), reflecting the two ways he believed a singular term may have meaning.\n\nThe reference (or \"referent\"; \"Bedeutung\") of a proper name is the object it means or indicates (\"bedeuten\"), its sense (\"Sinn\") is what the name expresses. The reference of a sentence is its truth value, its sense is the thought that it expresses. Frege justified the distinction in a number of ways.\n\nMuch of analytic philosophy is traceable to Frege's philosophy of language. Frege's views on logic (i.e., his idea that some parts of speech are complete by themselves, and are analogous to the arguments of a mathematical function) led to his views on a theory of reference.\n\nFrege developed his original theory of meaning in early works like \"Begriffsschrift\" ('concept script') of 1879 and \"Grundlagen\" ('foundations of arithmetic') of 1884. On this theory, the meaning of a complete sentence consists in its being true or false, and the meaning of each significant expression in the sentence is an extralinguistic entity which Frege called its \"Bedeutung\", literally 'meaning' or 'significance', but rendered by Frege's translators as 'reference', 'referent', \"'M\"eaning', 'nominatum', etc. Frege supposed that some parts of speech are complete by themselves, and are analogous to the arguments of a mathematical function, but that other parts are incomplete, and contain an empty place, by analogy with the function itself. Thus 'Caesar conquered Gaul' divides into the complete term 'Caesar', whose reference is Caesar himself, and the incomplete term '—conquered Gaul', whose reference is a Concept. Only when the empty place is filled by a proper name does the reference of the completed sentence – its truth value – appear. This early theory of meaning explains how the significance or reference of a sentence (its truth value) depends on the significance or reference of its parts.\n\nFrege introduced the notion of \"sense\" (German: \"Sinn\") to accommodate difficulties in his early theory of meaning.\n\nFirst, if the entire significance of a sentence consists of its truth value, it follows that the sentence will have the same significance if we replace a word of the sentence with one having an identical reference, as this will not change its truth value. The reference of the whole is determined by the reference of the parts. If \"the evening star\" has the same reference as \"the morning star\", it follows that \"the evening star is a body illuminated by the Sun\" has the same truth value as \"the morning star is a body illuminated by the Sun\". But it is possible for someone to think that the first sentence is true while also thinking that the second is false. Therefore, the thought corresponding to each sentence cannot be its reference, but something else, which Frege called its \"sense\".\n\nSecond, sentences that contain proper names with no reference cannot have a truth value at all. Yet the sentence 'Odysseus was set ashore at Ithaca while sound asleep' obviously has a sense, even though 'Odysseus' has no reference. The thought remains the same whether or not 'Odysseus' has a reference. Furthermore, a thought cannot contain the objects that it is about. For example, Mont Blanc, 'with its snowfields', cannot be a component of the thought that Mont Blanc is more than 4,000 metres high. Nor can a thought about Etna contain lumps of solidified lava.\n\nFrege's notion of sense is somewhat obscure, and neo-Fregeans have come up with different candidates for its role. Accounts based on the work of Carnap and Church treat sense as an intension, or a function from possible worlds to extensions. For example, the intension of ‘number of planets’ is a function that maps any possible world to the number of planets in that world. John McDowell supplies cognitive and reference-determining roles. Devitt treats senses as causal-historical chains connecting names to referents.\n\nIn his theory of descriptions, Bertrand Russell held the view that most proper names in ordinary language are in fact disguised definite descriptions. For example, 'Aristotle' can be understood as \"The pupil of Plato and teacher of Alexander,\" or by some other uniquely applying description. This is known as the descriptivist theory of names. Because Frege used definite descriptions in many of his examples, he is often taken to have endorsed the descriptivist theory. Thus Russell's theory of descriptions was conflated with Frege's theory of sense, and for most of the twentieth century this 'Frege-Russell' view was the orthodox view of proper name semantics. However, Saul Kripke argued compellingly against the descriptivist theory. According to Kripke, proper names are rigid designators which designate the same object in every possible world. Descriptions such as 'the President of the U.S. in 1970' do not designate the same in every possible world. For example, someone other than Richard Nixon, e.g. Hubert Humphrey, might have been the President in 1970. Hence a description (or cluster of descriptions) cannot be a rigid designator, and thus a proper name cannot \"mean\" the same as a description.\n\nHowever, the Russellian descriptivist reading of Frege has been rejected by many scholars, in particular by Gareth Evans in \"The Varieties of Reference\" and by John McDowell in \"The Sense and Reference of a Proper Name,\" following Michael Dummett, who argued that Frege's notion of sense should not be equated with a description. Evans further developed this line, arguing that a sense without a referent was not possible. He and McDowell both take the line that Frege's discussion of empty names, and of the idea of sense without reference, are inconsistent, and that his apparent endorsement of descriptivism rests only on a small number of imprecise and perhaps offhand remarks. And both point to the power that the sense-reference distinction \"does\" have (i.e., to solve at least the first two problems), even if it is not given a descriptivist reading.\n\nAs noted above, translators of Frege have rendered the German \"Bedeutung\" in various ways. The term 'reference' has been the most widely adopted, but this fails to capture the meaning of the original German ('meaning' or 'significance'), and does not reflect the decision to standardise key terms across different editions of Frege's works published by Blackwell. The decision was based on the principle of exegetical neutrality, namely that 'if at any point in a text there is a passage that raises for the native speaker legitimate questions of exegesis, then, if at all possible, a translator should strive to confront the reader of his version with the same questions of exegesis and not produce a version which in his mind resolves those questions'. The term 'meaning' best captures the standard German meaning of \"Bedeutung\", and Frege's own use of the term sounds as odd when translated into English as it does in German. Moreover, 'meaning' captures Frege's early use of \"Bedeutung\" well, and it would be problematic to translate Frege's early use as 'meaning' and his later use as 'reference', suggesting a change in terminology not evident in the original German.\n\nThe Greek philosopher Antisthenes, a pupil of Socrates, apparently distinguished \"a general object that can be aligned with the meaning of the utterance” from “a particular object of extensional reference.\" This \"suggests that he makes a distinction between sense and reference.\" \nThe principal basis of this claim is a quotation in Alexander of Aphrodisias's “Comments on Aristotle's 'Topics'” with a three-way distinction: \n\nThe sense-reference distinction is commonly confused with that between connotation and denotation, which originates with John Stuart Mill. According to Mill, a common term like 'white' \"denotes\" all white things, as snow, paper. But according to Frege, a common term does not refer to any individual white thing, but rather to an abstract Concept (\"Begriff\"). We must distinguish between the relation of reference, which holds between a proper name and the object it refers to, such as between the name 'Earth', and the planet Earth, and the relation of 'falling under', such as when the Earth falls under the concept \"planet\". The relation of a proper name to the object it designates is direct, whereas a word like 'planet' has no such direct relation at all to the Earth at all, but only to a concept that the Earth falls under. Moreover, judging \"of\" anything that it falls under this concept is not in any way part of our knowledge of what the word 'planet' means. The distinction between connotation and denotation is closer to that between Concept and Object, than to that between 'sense' and 'reference'.\n\n"}
{"id": "55888", "url": "https://en.wikipedia.org/wiki?curid=55888", "title": "Trusted system", "text": "Trusted system\n\nIn the security engineering subspecialty of computer science, a trusted system is a system that is relied upon to a specified extent to enforce a specified security policy. This is equivalent to saying that a trusted system is one whose failure would break a security policy (if a policy exists that the trusted system is trusted to enforce).\n\nThe meaning of the word \"trust\" is critical, as it does not carry the meaning that might be expected in everyday usage. A system trusted by a user, is one that the user feels safe to use, and trusts to do tasks without secretly executing harmful or unauthorised programs; while trusted computing refers to whether programs can trust the platform to be unmodified from that expected, whether or not those programs are innocent, malicious or execute tasks that are undesired by the user.\n\nA subset of trusted systems (\"Division B\" and \"Division A\") implement mandatory access control (MAC) labels; as such, it is often assumed that they can be used for processing classified information. However, this is generally untrue. There are four modes in which one can operate a multilevel secure system: multilevel mode, compartmented mode, dedicated mode, and system-high mode. The National Computer Security Center's \"Yellow Book\" specifies that B3 and A1 systems can only be used for processing a strict subset of security labels, and only when operated according to a particularly strict configuration.\n\nCentral to the concept of U.S. Department of Defense-style \"trusted systems\" is the notion of a \"reference monitor\", which is an entity that occupies the logical heart of the system and is responsible for all access control decisions. Ideally, the reference monitor is (a) tamper-proof, (b) always invoked, and (c) small enough to be subject to independent testing, the completeness of which can be assured. Per the U.S. National Security Agency's 1983 Trusted Computer System Evaluation Criteria (TCSEC), or \"Orange Book\", a set of \"evaluation classes\" were defined that described the features and assurances that the user could expect from a trusted system.\n\nKey to the provision of the highest levels of assurance (B3 and A1) is the dedication of significant system engineering toward minimization of the complexity (not \"size\", as often cited) of the trusted computing base (TCB), defined as that combination of hardware, software, and firmware that is responsible for enforcing the system's security policy.\n\nAn inherent engineering conflict would appear to arise in higher-assurance systems in that, the smaller the TCB, the larger the set of hardware, software, and firmware that lies outside the TCB and is, therefore, untrusted. Although this may lead the more technically naive to sophists' arguments about the nature of trust, the argument confuses the issue of \"correctness\" with that of \"trustworthiness\".\n\nIn contrast to the TCSEC's precisely defined hierarchy of six evaluation classes—the highest of which, A1, is featurally identical to B3, differing only in documentation standards—the more recently introduced Common Criteria (CC)—which derive from a blend of more or less technically mature standards from various NATO countries—provide a more tenuous spectrum of seven \"evaluation classes\" that intermix features and assurances in an arguably non-hierarchical manner and lack the philosophic precision and mathematical stricture of the TCSEC. In particular, the CC tolerate very loose identification of the \"target of evaluation\" (TOE) and support—even encourage—an inter-mixture of security requirements culled from a variety of predefined \"protection profiles.\" While a strong case can be made that even the more seemingly arbitrary components of the TCSEC contribute to a \"chain of evidence\" that a fielded system properly enforces its advertised security policy, not even the highest (E7) level of the CC can truly provide analogous consistency and stricture of evidentiary reasoning.\n\nThe mathematical notions of trusted systems for the protection of classified information derive from two independent but interrelated corpora of work. In 1974, David Bell and Leonard LaPadula of MITRE, working under the close technical guidance and economic sponsorship of Maj. Roger Schell, Ph.D., of the U.S. Army Electronic Systems Command (Ft. Hanscom, MA), devised what is known as the Bell-LaPadula model, in which a more or less trustworthy computer system is modeled in terms of objects (passive repositories or destinations for data, such as files, disks, printers) and subjects (active entities—perhaps users, or system processes or threads operating on behalf of those users—that cause information to flow among objects). The entire operation of a computer system can indeed be regarded a \"history\" (in the serializability-theoretic sense) of pieces of information flowing from object to object in response to subjects' requests for such flows.\n\nAt the same time, Dorothy Denning at Purdue University was publishing her Ph.D. dissertation, which dealt with \"lattice-based information flows\" in computer systems. (A mathematical \"lattice\" is a partially ordered set, characterizable as a directed acyclic graph, in which the relationship between any two vertices is either \"dominates,\" \"is dominated by,\" or neither.) She defined a generalized notion of \"labels\"—corresponding more or less to the full security markings one encounters on classified military documents, \"e.g.\", TOP SECRET WNINTEL TK DUMBO—that are attached to entities. Bell and LaPadula integrated Denning's concept into their landmark MITRE technical report—entitled, \"Secure Computer System: Unified Exposition and Multics Interpretation\"—whereby labels attached to objects represented the sensitivity of data contained within the object (though there can be, and often is, a subtle semantic difference between the sensitivity of the data within the object and the sensitivity of the object itself), while labels attached to subjects represented the trustworthiness of the user executing the subject. The concepts are unified with two properties, the \"simple security property\" (a subject can only read from an object that it \"dominates\" [\"is greater than\" is a close enough—albeit mathematically imprecise—interpretation]) and the \"confinement property,\" or \"*-property\" (a subject can only write to an object that dominates it). (These properties are loosely referred to as \"no-read-up\" and \"no-write-down,\" respectively.) Jointly enforced, these properties ensure that information cannot flow \"downhill\" to a repository whence insufficiently trustworthy recipients may discover it. By extension, assuming that the labels assigned to subjects are truly representative of their trustworthiness, then the no-read-up and no-write-down rules rigidly enforced by the reference monitor are provably sufficient to constrain Trojan horses, one of the most general classes of attack (\"sciz.\", the popularly reported worms and viruses are specializations of the Trojan horse concept).\n\nThe Bell-LaPadula model technically only enforces \"confidentiality,\" or \"secrecy,\" controls, \"i.e.\", they address the problem of the sensitivity of objects and attendant trustworthiness of subjects to not inappropriately disclose it. The dual problem of \"integrity\"(i.e., the problem of accuracy, or even provenance of objects) and attendant trustworthiness of subjects to not inappropriately modify or destroy it, is addressed by mathematically affine models; the most important of which is named for its creator, K. J. Biba. Other integrity models include the Clark-Wilson model and Shockley and Schell's program integrity model, \"The SeaView Model\"\n\nAn important feature of MACs, is that they are entirely beyond the control of any user. The TCB automatically attaches labels to any subjects executed on behalf of users and files they access or modify. In contrast, an additional class of controls, termed discretionary access controls(DACs), are under the direct control of the system users. Familiar protection mechanisms such as permission bits (supported by UNIX since the late 1960s and—in a more flexible and powerful form—by Multics since earlier still) and access control lists (ACLs) are familiar examples of DACs.\n\nThe behavior of a trusted system is often characterized in terms of a mathematical model—which may be more or less rigorous depending upon applicable operational and administrative constraints—that takes the form of a finite state machine (FSM) with state criteria, state transition constraints, a set of \"operations\" that correspond to state transitions (usually, but not necessarily, one), and a descriptive top-level specification (DTLS) which entails a user-perceptible interface (\"e.g.\", an API, a set of system calls [in UNIX parlance] or system exits [in mainframe parlance]); each element of which engenders one or more model operations.\n\nThe Trusted Computing Group creates specifications that are meant to address particular requirements of trusted systems, including attestation of configuration and safe storage of sensitive information.\n\nTrusted systems in the context of national or homeland security, law enforcement, or social control policy are systems in which some conditional prediction about the behavior of people or objects within the system has been determined prior to authorizing access to system resources.\n\nFor example, trusted systems include the use of \"security envelopes\" in national security and counterterrorism applications, \"trusted computing\" initiatives in technical systems security, and the use of credit or identity scoring systems in financial and anti-fraud applications; in general, they include any system (i) in which probabilistic threat or risk analysis is used to assess \"trust\" for decision-making before authorizing access or for allocating resources against likely threats (including their use in the design of systems constraints to control behavior within the system), or (ii) in which deviation analysis or systems surveillance is used to ensure that behavior within systems complies with expected or authorized parameters.\n\nThe widespread adoption of these authorization-based security strategies (where the default state is DEFAULT=DENY) for counterterrorism, anti-fraud, and other purposes is helping accelerate the ongoing transformation of modern societies from a notional Beccarian model of criminal justice based on accountability for deviant actions after they occur – see Cesare Beccaria, On Crimes and Punishment (1764) – to a Foucauldian model based on authorization, preemption, and general social compliance through ubiquitous preventative surveillance and control through system constraints – see Michel Foucault, \"Discipline and Punish\" (1975, Alan Sheridan, tr., 1977, 1995).\n\nIn this emergent model, \"security\" is geared not towards policing but to risk management through surveillance, exchange of information, auditing, communication, and classification. These developments have led to general concerns about individual privacy and civil liberty and to a broader philosophical debate about the appropriate forms of social governance methodologies.\n\nTrusted systems in the context of information theory is based on the definition of trust as 'Trust is that which is essential to a communication channel but cannot be transferred from a source to a destination using that channel' by Ed Gerck.\n\nIn Information Theory, information has nothing to do with knowledge or meaning. In the context of Information Theory, information is simply that which is transferred from a source to a destination, using a communication channel. If, before transmission, the information is available at the destination then the transfer is zero. Information received by a party is that which the party does not expect—as measured by the uncertainty of the party as to what the message will be.\n\nLikewise, trust as defined by Gerck has nothing to do with friendship, acquaintances, employee-employer relationships, loyalty, betrayal and other overly-variable concepts. Trust is not taken in the purely subjective sense either, nor as a feeling or something purely personal or psychological—trust is understood as something potentially communicable. Further, this definition of trust is abstract, allowing different instances and observers in a trusted system to communicate based on a common idea of trust (otherwise communication would be isolated in domains), where all necessarily different subjective and intersubjective realizations of trust in each subsystem (man and machines) may coexist.\n\nTaken together in the model of Information Theory, information is what you do not expect and trust is what you know. Linking both concepts, trust is seen as qualified reliance on received information. In terms of trusted systems, an assertion of trust cannot be based on the record itself, but on information from other information channels. The deepening of these questions leads to complex conceptions of trust which have been thoroughly studied in the context of business relationships. It also leads to conceptions of information where the \"quality\" of information integrates trust or trustworthiness in the structure of the information itself and of the information system(s) in which it is conceived: higher quality in terms of particular definitions of accuracy and precision means higher trustworthiness.\n\nAn introduction to the calculus of trust (Example: 'If I connect two trusted systems, are they more or less trusted when taken together?') is given in.\n\nThe IBM Federal Software Group has suggested that provides the most useful definition of trust for application in an information technology environment, because it is related to other information theory concepts and provides a basis for measuring trust. In a network centric enterprise services environment, such notion of trust is considered to be requisite for achieving the desired collaborative, service-oriented architecture vision.\n\n\nSee also, The Trusted Systems Project, a part of the Global Information Society Project (GISP), a joint research project of the World Policy Institute (WPI) and the Center for Advanced Studies in Sci. & Tech. Policy (CAS).\n"}
{"id": "449568", "url": "https://en.wikipedia.org/wiki?curid=449568", "title": "−1", "text": "−1\n\nIn mathematics, −1 is the additive inverse of 1, that is, the number that when added to 1 gives the additive identity element, 0. It is the negative integer greater than negative two (−2) and less than 0.\n\nNegative one bears relation to Euler's identity since \"e\" = −1.\n\nIn software development, −1 is a common initial value for integers and is also used to show that a variable contains no useful information.\n\nIn programming languages, −1 can be used to index the last (or 2nd last) item of an array, depending on whether 0 or 1 represents the first item.\n\nNegative one has some similar but slightly different properties to positive one.\n\nMultiplying a number by −1 is equivalent to changing the sign on the number. This can be proved using the distributive law and the axiom that 1 is the multiplicative identity: for \"x\" real, we have\n\nwhere we used the fact that any real \"x\" times 0 equals 0, implied by cancellation from the equation\n\nIn other words,\n\nso (−1) · \"x\", or −\"x\", is the arithmetic inverse of \"x\".\n\nThe square of −1, i.e. −1 multiplied by −1, equals 1. As a consequence, a product of two negative real numbers is positive.\n\nFor an algebraic proof of this result, start with the equation\n\nThe first equality follows from the above result. The second follows from the definition of −1 as additive inverse of 1: it is precisely that number that when added to 1 gives 0. Now, using the distributive law, we see that\n\nThe second equality follows from the fact that 1 is a multiplicative identity. But now adding 1 to both sides of this last equation implies\n\nThe above arguments hold in any ring, a concept of abstract algebra generalizing integers and real numbers.\n\nAlthough there are no real square roots of -1, the complex number \"i\" satisfies \"i\" = −1, and as such can be considered as a square root of −1. The only other complex number who's square is 1 is −\"i\". In the algebra of quaternions, which contain the complex plane, the equation \"x\" = −1 has infinite solutions.\n\nExponentiation of a non-zero real number can be extended to negative integers. We make the definition that \"x\" = , meaning that we define raising a number to the power −1 to have the same effect as taking its reciprocal. This definition is then extended to negative integers preserves the exponential law \"x\"\"x\" = \"x\" for real numbers \"a\" and \"b\".\n\nExponentiation to negative integers can be extended to invertible elements of a ring, by defining \"x\" as the multiplicative inverse of \"x\".\n\n−1 that appears next to functions or matrices does not mean raising them to the power −1 but their inverse functions or inverse matrices. For example, \"f\"(\"x\") is the inverse of \"f\"(\"x\"), or sin(\"x\") is a notation of arcsine function.\n\nMost computer systems represent negative integers using two's complement. In such systems, −1 is represented using a bit pattern of all ones. For example, an 8-bit signed integer using two's complement would represent −1 as the bitstring \"11111111\", or \"FF\" in hexadecimal (base 16). If interpreted as an unsigned integer, the same bitstring of \"n\" ones represents 2 − 1, the largest possible value that \"n\" bits can hold. For example, the 8-bit string \"11111111\" above represents 2 − 1 = 255.\n\nIn some programming languages, when used to index some data types (such as an array), then -1 can be used to identify the very last (or 2nd last) item, depending on whether 0 or 1 represents the first item.\nIf the first item is indexed by 0, then -1 identifies the last item.\nIf the first item is indexed by 1, then -1 identifies the second-to-last item.\n"}
