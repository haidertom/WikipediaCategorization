{"id": "509995", "url": "https://en.wikipedia.org/wiki?curid=509995", "title": "Abstract and concrete", "text": "Abstract and concrete\n\nAbstract and concrete are classifications that denote whether the object that a term describes has physical referents. Abstract objects have no physical referents, whereas concrete objects do. They are most commonly used in philosophy and semantics. Abstract objects are sometimes called abstracta (sing. abstractum) and concrete objects are sometimes called \"concreta\" (sing. \"concretum\"). An abstract object is an object that does not exist at any particular time or place, but rather exists as a type of thing—i.e., an idea, or abstraction. The term \"abstract object\" is said to have been coined by Willard Van Orman Quine. The study of abstract objects is called abstract object theory.\n\nThe type–token distinction identifies physical objects that are tokens of a particular type of thing. The \"type\" of which it is a part is in itself an abstract object. The abstract-concrete distinction is often introduced and initially understood in terms of paradigmatic examples of objects of each kind:\n\nAbstract objects have often garnered the interest of philosophers because they raise problems for popular theories. In ontology, abstract objects are considered problematic for physicalism and some forms of naturalism. Historically, the most important ontological dispute about abstract objects has been the problem of universals. In epistemology, abstract objects are considered problematic for empiricism. If abstracta lack causal powers or spatial location, how do we know about them? It is hard to say how they can affect our sensory experiences, and yet we seem to agree on a wide range of claims about them. \n\nSome, such as Edward Zalta and arguably, Plato in his Theory of Forms, have held that abstract objects constitute the defining subject matter of metaphysics or philosophical inquiry more broadly. To the extent that philosophy is independent of empirical research, and to the extent that empirical questions do not inform questions about abstracta, philosophy would seem especially suited to answering these latter questions. \n\nIn modern philosophy, the distinction between abstract and concrete was explored by Immanuel Kant and G. W. F. Hegel.\n\nGottlob Frege said that abstract objects, such as numbers, were members of a third realm, different from the external world or from internal consciousness. \n\nAnother popular proposal for drawing the abstract-concrete distinction contends that an object is abstract if it lacks any causal powers. A causal power has the ability to affect something causally. Thus, the empty set is abstract because it cannot act on other objects. One problem for this view is that it is not clear exactly what it is to have a causal power. For a more detailed exploration of the abstract-concrete distinction, follow the link below to the \"Stanford Encyclopedia\" article.\n\nJean Piaget uses the terms \"concrete\" and \"formal\" to describe the different types of learning. Concrete thinking involves facts and descriptions about everyday, tangible objects, while abstract (formal operational) thinking involves a mental process.\nRecently, there has been some philosophical interest in the development of a third category of objects known as the quasi-abstract. Quasi-abstract objects have drawn particular attention in the area of social ontology and documentality. Some argue that the over-adherence to the platonist duality of the concrete and the abstract has led to a large category of social objects having been overlooked or rejected as nonexisting because they exhibit characteristics that the traditional duality between concrete and abstract regards as incompatible. Specially, the ability to have temporal location, but not spatial location, and have causal agency (if only by acting through representatives). These characteristics are exhibited by a number of social objects, including states of the international legal system.\n\n"}
{"id": "26276222", "url": "https://en.wikipedia.org/wiki?curid=26276222", "title": "Bateman's principle", "text": "Bateman's principle\n\nBateman's principle, in evolutionary biology, is that in most species, variability in reproductive success (or reproductive variance) is greater in males than in females. It was first proposed by Angus John Bateman (1919–1996), an English geneticist. Bateman suggested that, since males are capable of producing millions of sperm cells with little effort, while females invest much higher levels of energy in order to nurture a relatively small number of eggs, the female plays a significantly larger role in their offspring's reproductive success. Bateman’s paradigm thus views females as the limiting factor of parental investment, over which males will compete in order to copulate successfully.\n\nAlthough Bateman's principle served as a cornerstone for the study of sexual selection for many decades, it has recently been subject to criticism. Attempts to reproduce Bateman's experiments in 2012 and 2013 were unable to support his conclusions. Some scientists have criticized Bateman's experimental and statistical methods, or pointed out conflicting evidence, while others have defended the veracity of the principle and cited evidence in support of it.\n\nTypically it is the females who have a relatively larger investment in producing each offspring. Bateman attributed the origin of the unequal investment to the differences in the production of gametes: sperm are cheaper than eggs. A single male can easily fertilize all females' eggs; she will not produce more offspring by mating with more than one male. A male is capable of fathering more offspring if he mates with several females. By and large, a male's potential reproductive success is limited by the number of females he mates with, whereas a female's potential reproductive success is limited by how many eggs she can produce. According to Bateman's principle, this results in sexual selection, in which males compete with each other, and females become choosy in which males to mate with. Thus, as a result of being anisogamous, males are fundamentally promiscuous, and females are fundamentally selective.\n\nBateman initially published his review in 1948. He was a botanist, contributing to the literature of sexual selection only once in his lifetime. Bateman initially saw his study on Drosophila to be a test of Charles Darwin’s doctrine of sexual selection. He saw Darwin’s theory of natural selection not as flawed, but as incomplete. He felt that if he were to provide a concrete demonstration of how sexual selection played a role in the reproductive success of certain species, he could explain the gap between Darwin’s ideas and sexual dimorphism.\n\nAlthough it is common to confuse Bateman's ideas with those of later scientists, his principle can be expressed in three simple statements. The first is that male reproductive success increases with the number of mates they attempt to copulate with, while female reproductive success does not. The second is that male reproductive success will show greater variance than female. The third is that sexual selection will have a greater effect on the sex with greater variance in reproductive success.\n\nThroughout his research, Bateman conducted experiments using fruit flies in order to observe their copulation and sexual behavior. A total of six series of experiments were conducted with the fruit fly \"Drosophila melanogaster\", using three to five individuals of each sex. Each trial ran for three or four days. Some ran to completion without the transfer of the Drosophila from one environment (bottle) to another. In the others, Bateman transferred the flies and their eggs to a new bottle every day. Bateman also varied the age of the flies depending on the experiment, with an age gap between one and six days total. He never watched the flies' copulations. The flies used were from several inbred strains, which meant they could be identified by their specific inbred strain. Therefore, he inferred the number of involved mates based on the number of offspring that were later found to have mutations from both a male and a female. The difficulty that arose was that if a female Drosophila had copulated with five males and only one larva survived, Bateman would not be able to account for the other four copulations.\n\nAnalysis of the data collected in sets one through four showed that the males' reproductive success, estimated as the number of sired offspring, increased at a steady rate until a total of three mates were reached. It is important to note that Bateman kept the sex ratio of males to females completely even throughout his trials. But after surpassing three mates, male reproductive success began to fall. Female reproductive success also increased with number of mates, but much more gradually than that of the males. The second series of data collected in sets five and six illustrated a dramatically different outcome. Male reproductive success increased at a steady and steep rate, never dropping. Female reproductive success, on the other hand, plateaued after a single mate. Bateman focused mainly on the second series of data when discussing his results. His main conclusion was that the reproductive success of females does not increase with an influx of mates, as one fit mate was enough to successfully complete fertilization. This is often referred to as Bateman’s Gradient.\n\nThroughout 2012 and 2013, Gowaty, Kim, and Anderson took it upon themselves to repeat Bateman's experiment in its entirety. While reproducing his tests, the same fly strains and mutations were used in order to maintain the same methodology. However, one of the 11 strains that Bateman used had gone extinct, and was thus replaced. (Tang-Martinez 2012)\n\nGowaty, Kim, and Anderson found that upon combining certain strains with one another, the offspring were unable to survive to adulthood. (Gowaty, Kim, & Anderson 2012) Thus, Bateman’s numbers regarding the number of individuals not having mated was higher than the actual number. Likewise, his estimate of those that mated with one or more mates was too low. This was valid for both the males and females of this species.\n\nGowaty desired to further explore the reasoning behind the premature death of the Drosophila. She began doing so by running monogamy trials between different strains of flies and found that 25% of the offspring died due to becoming double mutants. (Gowaty 2013) Bateman thought his work fit within the lines of Mendel’s laws of genetics, while Gowaty proved otherwise. The 1948 experiments inferred reproductive success based on the number of adults living by the end of the trial. In reality, many factors were left out of the equation when calculating reproductive success as a function of the number of mates, which had the ability to completely dislodge the accuracy behind Bateman's results. Gowaty was not able to confirm Bateman's conclusions and found no evidence for sexual selection in the experiment. (Gowaty 2013; Tang-Martinez 2012)\n\nNevertheless, some modern experiments between the relationship of number of mates and the reproductive success of males and females support Bateman's principle. Julie Collet conducted an experiment with a population of red jungle fowl. A total of thirteen replicate groups of three males and four females were monitored for ten days. In this experiment, the sex ratio was biased toward females. A male's reproductive success was calculated using the proportion of embryos fathered to the total number of embryos produced by all the females he mated with. The total sexual selection opportunity was calculated using the following formula.\n\nThe \"σ\" represents the variance in RS, while the is the square mean of reproductive success of members of one sex in a group.\n\nIn 2013, Fritzsche and Arnqvist tested Bateman's principle by estimating sexual selection between males and females in four seed beetles. They used a unique experimental design that showed sexual selection to be greater in males than in females. In contrast, sexual selection was also shown to be stronger for females in role-reversed species. They suggested that the Bateman gradient is typically the most accurate and informative measure of sexual selection between different sexes and species (Fritzsche, K. & Arnqvist, G).\n\nMore than 60 years later, Bateman's principle has received considerable attention. Sutherland argued that males' higher variance in reproductive success may result from random mating and coincidence. Hubbell and Johnson suggested that variance in reproductive success can be greatly influenced by the time and allocations of mating. In 2005, Gowaty and Hubbell suggested that mating tendencies are subject to change depending on certain strategies. They argued that there are cases in which males can be more selective than females, whereas Bateman suggested that his paradigm would be “almost universal” among sexually reproducing species. Critics proposed that females might be more subject to sexual selection than males, but not in all circumstances.\n\nExperimental and statistical criticisms followed. Until approximately a decade ago, critics of Bateman’s model focused on his experimental design. In recent years, they have shifted attention to the actual experimental and statistical calculations Bateman published throughout his trials. Birkhead wrote a 2000 review arguing that since Bateman’s experiments lasted only three to four days, the female fruit fly, \"Drosophila melanogaster\", may not have needed to mate repeatedly, as it can store sperm for up to four days; if Bateman had used a species in which females had to copulate more often to fertilize their eggs, the results might have been different. Snyder and Gowaty conducted the first in-depth analysis of the data in Bateman’s 1948 paper. They found sampling biases, mathematical errors, and selective presentation of data.\n\nA 2012 review by Zuleyma Tang-Martínez concluded that various empirical and theoretical studies, especially Gowaty's reproduction of Bateman's original experiment, pose a major challenge to Bateman's conclusions, and that Bateman's principle should be considered an unproven hypothesis in need of further reexamination. According to Tang-Martínez, \"modern data simply don't support most of Bateman's and Trivers's predictions and assumptions.\"\n\nA 2016 review confirmed Darwinian sex roles across the animal kingdom, concluding that \"sexual selection, as captured by standard Bateman metrics, is indeed stronger in males than in females and that it is evolutionarily tied to sex biases in parental care and sexual dimorphism.\"\n\nOne error source that have been shown to give an illusion of greater differential in reproductive success in males than in females genetically is that chromosome effects cause a greater percentage of mutations to be lethal before even reaching sexual maturity in males than in females.\n\nThe assumption that any differential in reproductive success between males and females among the individuals that do reach sexual maturity must be due to sexual selection in the current population is also subject to criticism, such as the possibility of remnants of sexually selected traits in a previous species from which a new species have evolved can be negatively selected due to costs in nutrients and weakened immune systems and that such negative selection would cause a higher difference in reproductive success in males than in females even without any still ongoing sexual selection. Since lower degrees of selection during times of stable environment allows genetic variation to build up by random mutations and allow some individuals in a population to survive environmental change while strong constant selection offsets the effect and increases the risk of the entire population dying out during catastrophic environmental change due to less genetic variation, constant loss of genetic variation caused by sexual selection have been suggested as a factor contributing to higher extinction rates in more sexually dimorphic species besides the nutrient, immunity and other costs of the ornaments themselves. While the ornament cost risk would only be removed when the ornaments have been eliminated by selection, the genetic variation model predicts that the species ability to survive would improve significantly even at an early stage of reduction of sexual dimorphism due to other adaptive mutations arising and surviving due to minimal selection during times of stable environment while the genes causing sexually dimorphic anatomy have only in small part been affected by the mutations. Applied to human evolution, this model can explain why early Homo sapiens display a significantly increased adaptability to environmental change already at its early divergence from Homo erectus that had a high muscular sexual dimorphism, as well as why human anatomy through the history of Homo sapiens show a diversification during times of stable climate and a selective loss of the more robust male forms during environmental change that does not recover during later stability, continuing through the loss of many robust characteristics in regional bottlenecks as recent as the end of the Ice Age and the time around the agricultural revolution. It also explains genetic evidence of human genetic diversity increasing during stable environmental periods and being reduced during bottlenecks related to changes in the environment.\n\nRecently, DNA testing has permitted more detailed investigation of mating behavior in numerous species. The results, in many cases, have been cited as evidence against Bateman's principle.\n\nUntil recently, most bird species were believed to be sexually monogamous. DNA paternity testing, however, has shown that in nearly 90% of bird species, females copulate with multiple males during each breeding season. The superb fairy wren is socially monogamous, but 95% of its clutches contain young fathered by extra-pair males. Up to 87% of tree swallow clutches, 75% of coal tit clutches, and 70% of reed bunting clutches contain young fathered by extra-pair males. Even female waved albatrosses, which typically mate for life, are sexually promiscuous, with 17% of young fathered by extra-pair males.\n\nIn many primate species, females solicit sex from males and may mate with more than one male in quick succession. Female lions may mate 100 times per day with different males while they are in estrus. Females of the pseudoscorpion species, \"Cordylochernes scorpioides\", have been shown to have higher reproductive success when mated with more than one male.\n\nThere are a number of claims about sex differences in humans said by evolutionary psychologists to be well supported that are not logically predicted by Bateman's principle. One example that have been pointed out is language ability that is said to be more developed in women than in men. It is argued that language, being a derived human trait, is not predicted by a model that assumes males to be more selected than females to be present to a higher degree in women than in men but the opposite. The objection about a trade-off between spatial ability and language is found lacking due to the existence of animals that outperform humans on spatial tasks such as birds of prey and bats while having much smaller and more rapidly maturing brains than humans, while great ape language is limited and ungrammatical at best despite great apes being much closer to humans in brain capacity than bats or birds of prey are, showing that the system requirements for spatial ability are much lower than those for language and therefore not a serious trade-off for them. The generalization of Bateman's principle to societies lacking modern birth control is also argued to be incompatible with such generalization of the \"nuns more successful at celibacy than monks\" clause of the female erotic plasticity model, as absence of modern contraceptives would mean that absence of reproduction for many men required that many men had no sexual relationships with women while male sexual exclusivity to a particular gender, if present, would have precluded same-sex relationships as a form of non-celibacy for men who could not find a female mate as well. It is also mentioned that the evolutionary psychologists themselves claim that women are sexually picky due to an evolutionary history without modern contraception, while it is pointed out that the notion of celibacy being impossible for men in general with only a small number of individual exceptions is incompatible with the notion of evolution making celibacy inevitable for a significant percentage of men.\n\nThe most well-known exceptions to Bateman's principle are the existence of sex-role reversed species such as pipefish (seahorses), phalaropes and jacanas in which the males perform the majority of the parental care, and are cryptic while the females are highly ornamented and territorially aggressive (; ; ). \n\nIn these species, however, the typical fundamental sex differences are reversed: females have a faster reproductive rate than males (and thus greater reproductive variance), and males have greater assurance of genetic parentage than do females . Consequently, reversals in sex roles and reproductive variance are consistent with Bateman's principle, and with Robert Trivers's parental investment theory.\n\n\n\n"}
{"id": "10000937", "url": "https://en.wikipedia.org/wiki?curid=10000937", "title": "Category (Kant)", "text": "Category (Kant)\n\nIn Kant's philosophy, a category ( in the original or \"Kategorie\" in modern German) is a pure concept of the understanding (\"Verstand\"). A Kantian category is a characteristic of the appearance of any object in general, before it has been experienced. Kant wrote that \"They are concepts of an object in general….\" Kant also wrote that, \"…pure cоncepts [Categories] of the undеrstanding which apply to objects of intuition in general….\" Such a category is not a classificatory division, as the word is commonly used. It is, instead, the condition of the possibility of objects in general, that is, objects as such, any and all objects, not specific objects in particular.\n\nThe word comes from the Greek κατηγορία, \"katēgoria\", meaning \"that which can be said, predicated, or publicly declared and asserted, about something.\" A category is an attribute, property, quality, or characteristic that can be predicated of a thing. \"…I remark concerning the categories…that their logical employment consists in their use as predicates of objects.\" Kant called them \"ontological predicates.\"\n\nA category is that which can be said of everything in general, that is, of anything that is an object. John Stuart Mill wrote: \"The Categories, or Predicaments — the former a Greek word, the latter its literal translation in the Latin language — were believed to be an enumeration of all things capable of being named, an enumeration by the \"summa genera\" (highest kind), i.e., the most extensive classes into which things could be distributed, which, therefore, were so many highest Predicates, one or other of which was supposed capable of being affirmed with truth of every nameable thing whatsoever.\"\n\nAristotle had claimed that the following ten predicates or categories could be asserted of anything in general: substance, quantity, quality, relation, action, affection (passivity), place, time (date), position, and state. These are supposed to be the qualities or attributes that can be affirmed of each and every thing in experience. Any particular object that exists in thought must have been able to have the Categories attributed to it as possible predicates because the Categories are the properties, qualities, or characteristics of any possible object in general. The Categories of Aristotle and Kant are the general properties that belong to all things without expressing the peculiar nature of any particular thing. Kant appreciated Aristotle's effort, but said that his table was imperfect because \" … as he had no guiding principle, he merely picked them up as they occurred to him...\"\n\nThe Categories do not provide knowledge of individual, particular objects. Any object, however, must have Categories as its characteristics if it is to be an object of experience. It is presupposed or assumed that anything that is a specific object must possess Categories as its properties because Categories are predicates of an object in general. An object in general does not have all of the Categories as predicates at one time. For example, a general object cannot have the qualitative Categories of reality and negation at the same time. Similarly, an object in general cannot have both unity and plurality as quantitative predicates at once. The Categories of Modality exclude each other. Therefore, a general object cannot simultaneously have the Categories of possibility/impossibility and existence/non–existence as qualities.\n\nSince the Categories are a list of that which can be said of every object, they are related only to human language. In making a verbal statement about an object, a speaker makes a judgment. A general object, that is, every object, has attributes that are contained in Kant's list of Categories. In a judgment, or verbal statement, the Categories are the predicates that can be asserted of every object and all objects.\n\nKant believed that the ability of the human understanding (German: \"Verstand\", Greek: \"dianoia\" \"διάνοια\", Latin: \"ratio\") to think about and know an object is the same as the making of a spoken or written judgment about an object. According to him, \"Our ability to judge is equivalent to our ability to think.\"\nA judgment is the thought that a thing is known to have a certain quality or attribute. For example, the sentence \"The rose is red\" is a judgment. Kant created a table of the forms of such judgments as they relate to all objects in general.\n\nThis table of judgments was used by Kant as a model for the table of categories. Taken together, these twelvefold tables constitute the formal structure for Kant's architectonic conception of his philosophical system.\n\nCategories are entirely different from the appearances of objects. According to Kant, in order to relate to specific phenomena, categories must be \"applied\" through time. The way that this is done is called a schema.\n\nArthur Schopenhauer, in his criticism of the Kantian philosophy, found many errors in Kant's use of the Categories of Quality, Quantity, Relation, and Modality. Schopenhauer also noted that in accordance with Kant's claim, non-human animals would not be able to know objects. Animals would only know impressions on their sense organs, which Kant mistakenly calls perception.\n\n\n"}
{"id": "5653", "url": "https://en.wikipedia.org/wiki?curid=5653", "title": "Clarke's three laws", "text": "Clarke's three laws\n\nBritish science fiction writer Arthur C. Clarke formulated three adages that are known as Clarke's three laws, of which the third law is the best known and most widely cited. They were part of his ideas in his extensive writings about the future. These so-called laws include:\n\n\nOne account claimed that Clarke's \"laws\" were developed after the editor of his works in French started numbering the author's assertions. All three laws appear in Clarke's essay \"Hazards of Prophecy: The Failure of Imagination\", first published in \"Profiles of the Future\" (1962). However, they were not published at the same time. Clarke's first law was proposed in the 1962 edition of the essay, as \"Clarke's Law\" in \"Profiles of the Future\".\n\nThe second law is offered as a simple observation in the same essay but its status as Clarke's second law was conferred by others. It was initially a derivative of the first law and formally became Clarke's second law where the author proposed the third law in the 1973 revision of \"Profiles of the Future,\" which included an acknowledgement\".\" It was also here that Clarke wrote about the third law in these words: \"As three laws were good enough for Newton, I have modestly decided to stop there\".\n\nThe third law, despite being latest stated by a decade, is the best known and most widely cited. It appears only in the 1973 revision of the \"Hazards of Prophecy\" essay. It echoes a statement in a 1942 story by Leigh Brackett: \"Witchcraft to the ignorant, … simple science to the learned\". Earlier examples of this sentiment may be found in \"Wild Talents\" (1932) by Charles Fort: \"...a performance that may some day be considered understandable, but that, in these primitive times, so transcends what is said to be the known that it is what I mean by magic,\" and in the short story \"The Hound of Death\" (1933) by Agatha Christie: \"The supernatural is only the natural of which the laws are not yet understood.\"\n\nClarke gave an example of the third law when he said that while he \"would have believed anyone who told him back in 1962 that there would one day exist a book-sized object capable of holding the content of an entire library, he would never have accepted that the same device could find a page or word in a second and then convert it into any typeface and size from Albertus Extra Bold to Zurich Calligraphic\", referring to his memory of \"seeing and hearing Linotype machines which slowly converted ‘molten lead into front pages that required two men to lift them’\".\n\nA fourth law has been proposed for the canon, despite Clarke's declared intention of stopping at three laws. Geoff Holder quotes: \"For every expert, there is an equal and opposite expert,\" which is part of American economist Thomas Sowell's \"For every expert, there is an equal and opposite expert, but for every fact there is not necessarily an equal and opposite fact\", from his 1995 book \"The Vision of the Anointed\".\n\nThe third law has inspired many snowclones and other variations:\n\n\nA of the third law is\n\n\nThe third law has been:\n\n\n\n"}
{"id": "38191512", "url": "https://en.wikipedia.org/wiki?curid=38191512", "title": "Concept-driven strategy", "text": "Concept-driven strategy\n\nA concept-driven strategy is a process for formulating strategy that draws on the explanation of how humans inquire provided by linguistic pragmatic philosophy. This argues that thinking starts by selecting (explicitly or implicitly) a set of concepts (frames, patterns, lens, principles, etc.) gained from our past experiences. These are used to reflect on whatever happens, or is done, in the future.\n\nConcept-driven strategy therefore starts from agreeing and enacting a set of strategic concepts (organizing principles) that \"works best\" for an organisation. For example, a hospital might set its strategy as intending to be Caring, World Class, Local, Evidence Based, and Team Based. A University might set its strategy as intending to be Ranked, Problem Solving, Online, Equis, and Offering Pathways. A commercial corporation might set its strategy as intending to be Innovative, Global, Have Visible Supply Chains, Agile and Market Share Dominant. These strategic concepts make up its \"Statement of Intent\" (or Purpose).\n\nMuch of the strategic management literature mutates Peter Drucker's call for corporations to start the strategic management process by producing a statement of purpose, mission and objectives. This has been mutated into a call to start with a vision, mission and objectives statement. There is an alternative approach which focuses on the Statement of Purpose or Intent. Drucker's example for this statement for a commercial corporation was to state that the corporation's purpose was to create customers. That is, it was going to use the concept of 'customer creation' to coordinate and organise the cognition or mindset of those that worked for the organisation. This was why the corporation existed. Having one concept is now thought to be insufficient. George Armitage Miller's modified The Magical Number Seven, Plus or Minus Two and dialectic suggests a handful of concepts under tension would be preferable.\n\nThe Statement of Purpose, Statement of Intent or concept-driven approach to strategy formulation therefore focuses on setting and enacting a set strategic concepts. If a participatory approach is being used these concepts will be acquired through a process of collaboration with stakeholders. Once agreed the strategic concepts can be used to coordinate activities and act as a set of decision making criteria. The set of concepts that make up the Statement of Intent is then used to make sense of an unpredictable future across an organisation in a co-ordinated manner.\n\nLinguistic pragmatism argues that our prior conceptions interpret our perception (sensory inputs). These conceptions are represented by concepts like running, smiling, justice, reasoning and agility. They are patterns of activity, experienced in our past and remembered. They can be named by those with language and so shared.\n\nBagginni explains pragmatic concepts using the classic example of whether the earth is flat or round.\n\nAnother example would be that we can think of the war in Iraqi differently by reflecting off the concepts of oil security, Imperialism, aggressive capitalism, liberation or democracy. \nThe concept-driven approach to strategy formulation involves setting and using a set of linguistic pragmatic concepts.\n\nThe steps to formulating a participatory concept-driven strategy are:\n\n\nConcept-driven strategy is the name given to a number of similar strategic thinking approaches.\n\nGenerally, the term 'concept-driven' is used to encourage a focus on the 'concepts' being used. See Concept learning or Management Concepts.\n\nSome organisations produce a 'statement of intent' with little thought as to the concepts it contains. However, if it is a short list of concepts, high level objectives, principles, priorities or frames, then concept-driven strategy offers a philosophical basis for these statements.\n\nSome organisations produce a 'strategic principles' statement which again is similar to a statement of intent and the same applies about the concepts approach offering a philosophical basis. The term 'strategic priorities' or 'strategic values' are often used in the same way as strategic principles.\n\nThe literature about 'corporate purpose' is also similar to that of strategic intent. Sometimes, purpose refers to present actions and intent to future ones. If purpose is expressed as a set of concepts, then the concepts approach again provides some philosophical basis.\n\nThere is a connection between 'systems thinking' and concept-driven strategy. The Churchman/Ackoff stream of systems thinking was interested in a developing generic system of concepts for thinking about problems. Rather than a generic set of concepts, the concept-driven approach uses whatever concepts stakeholders think work best for the future of their organisation.\n\nThere is a military planning approach called 'concept-led'. The military-like leadership seems to have moved the concepts from being drivers to be leaders. There seems to be very little difference otherwise.\n\nIn turbulent environments, concepts are thought 'more flexible than objectives' (goals, targets) as they provide why certain actions are preferable. The purpose and intent literature likes to distinguish itself from the objectives literature by saying purpose and intent provide the reasons for (why change), the driver for change. Objectives are where you end up. In complex dynamic situations, there may be many acceptable end points, many of which cannot be anticipated by planners. Arguably the only objective is to survive. How is explained in the statement of intent.\n\nPerhaps strangely, there is a connection between 'metaphor', metaphoric criticism, or conceptual metaphor and concept-driven strategy. Pragmatic concepts are not images but most concepts relate to metaphors. For example, to say an organisation is like a machine, with cogs, or like an adaptive organism, is to use the concepts of machine and organism to reflect on organisations. Much of what has been written about the usefulness of metaphors in planning applies to concepts.\n\nThe term 'strategic frames' is not common given the extensive literature on frame analysis but frames and pragmatic concepts seem to be very similar. Amos Tversky defines a frame as a conception of outcomes.\n\nThe system of strategic concepts listed in a statement of intent, purpose, principles, frames or conceptual metaphor are organizing principle(s).\n\nAlso, as Karl Weick explains sensemaking as the process of conceptualising problems, concept-driven strategy might be thought of as a pragmatic means of sensemaking a strategy.\n\n\n"}
{"id": "25707018", "url": "https://en.wikipedia.org/wiki?curid=25707018", "title": "Concision", "text": "Concision\n\nConcision (alternatively brevity, laconicism, terseness, or conciseness) is the cutting out of unnecessary words while conveying an idea. It aims to enhance communication by eliminating redundancy without omitting important information. Concision has been described as one of the elementary principles of writing. The related concept of succinctness is the opposite of verbosity.\n\nConcision means to be economical with words, expressing what's needed using the fewest words necessary. That may involve removing redundant or unnecessary phrases or replacing them with shorter ones. It is described in \"The Elements of Style\" by Strunk and White as follows:\n\nConcision has also been described as \"eliminat[ing] words that take up space without saying much.\" Simple examples include replacing \"\" with \"because\" or \"at this point in time\" with \"now\" or \"currently.\"\n\nAn example sentence, with explanation:\n\nThe following example is taken from:\n\nThe source suggests this replacement:\n\nIn the second quote, the same information is communicated in less than half the length. However, it could be more concisely rewritten and communicate the same information:\n\nConcise expression, particularly in writing, is considered one of the basic goals of teaching the English language. Techniques to achieve concise writing are taught for students at all levels, from the introduction to writing to the preparation of PhD dissertations, and legal writing for law students.\n\nIt has been argued that although \"in expository prose English places a high value on conciseness... [t]he value placed on conciseness... is not shared by all cultures\", with, for example, the Thai culture as one where redundancy is prized as an opportunity to use additional words to demonstrate the writer's command of the language. This may lead to a tendency for people from those cultures to use repetitive or redundant phrasing when learning English.\n\nThe related concept of succinctness is a characteristic of speech, writing, data structure, algorithmic games, and thought in general, exhibiting both clarity and brevity. It is the opposite of verbosity, in which there is an excess of words.\n\nBrevity in succinctness is not achieved by shortening original material by coding or compressing it, but rather by omitting redundant material from it.\n\n\n"}
{"id": "1845675", "url": "https://en.wikipedia.org/wiki?curid=1845675", "title": "Deductive-nomological model", "text": "Deductive-nomological model\n\nThe deductive-nomological model (DN model), also known as Hempel's model, the Hempel–Oppenheim model, the Popper–Hempel model, or the covering law model, is a formal view of scientifically answering questions asking, \"Why...?\". The DN model poses scientific explanation as a deductive structure—that is, one where truth of its premises entails truth of its conclusion—hinged on accurate prediction or postdiction of the phenomenon to be explained.\n\nBecause of problems concerning humans' ability to define, discover, and know causality, it was omitted in initial formulations of the DN model. Causality was thought to be incidentally approximated by realistic selection of premises that \"derive\" the phenomenon of interest from observed starting conditions plus general laws. Still, DN model formally permitted causally irrelevant factors. Also, derivability from observations and laws sometimes yielded absurd answers.\n\nWhen logical empiricism fell out of favor in the 1960s, the DN model was widely seen as a flawed or greatly incomplete model of scientific explanation. Nonetheless, it remained an idealized version of scientific explanation, and one that was rather accurate when applied to modern physics. In the early 1980s, revision to DN model emphasized \"maximal specificity\" for relevance of the conditions and axioms stated. Together with Hempel's inductive-statistical model, the DN model forms scientific explanation's covering law model, which is also termed, from critical angle, subsumption theory.\n\nThe term \"deductive\" distinguishes the DN model's intended determinism from the probabilism of inductive inferences. The term \"nomological\" is derived from the Greek word \"νόμος\" or \"nomos\", meaning \"law\". The DN model holds to a view of scientific explanation whose \"conditions of adequacy\" (CA)—semiformal but stated classically—are \"derivability\" (CA1), \"lawlikeness\" (CA2), \"empirical content\" (CA3), and \"truth\" (CA4).\n\nIn the DN model, a law axiomatizes an unrestricted generalization from antecedent \"A\" to consequent \"B\" by conditional proposition—\"If A, then B\"—and has empirical content testable. A law differs from mere true regularity—for instance, \"George always carries only $1 bills in his wallet\"—by supporting counterfactual claims and thus suggesting what \"must\" be true, while following from a scientific theory's axiomatic structure.\n\nThe phenomenon to be explained is the explanandum—an event, law, or theory—whereas the premises to explain it are explanans, true or highly confirmed, containing at least one universal law, and entailing the explanandum. Thus, given the explanans as initial, specific conditions \"C, C . . . C\" plus general laws \"L, L . . . L\", the phenomenon \"E\" as explanandum is a deductive consequence, thereby scientifically explained.\n\nAristotle's scientific explanation in \"Physics\" resembles the DN model, an idealized form of scientific explanation. The framework of Aristotelian physics—Aristotelian metaphysics—reflected the perspective of this principally biologist, who, amid living entities' undeniable purposiveness, formalized vitalism and teleology, an intrinsic morality in nature. With emergence of Copernicanism, however, Descartes introduced mechanical philosophy, then Newton rigorously posed lawlike explanation, both Descartes and especially Newton shunning teleology within natural philosophy. At 1740, David Hume staked Hume's fork, highlighted the problem of induction, and found humans ignorant of either necessary or sufficient causality. Hume also highlighted the fact/value gap, as what \"is\" does not itself reveal what \"ought\".\n\nNear 1780, countering Hume's ostensibly radical empiricism, Immanuel Kant highlighted extreme rationalism—as by Descartes or Spinoza—and sought middle ground. Inferring the mind to arrange experience of the world into \"substance\", \"space\", and \"time\", Kant placed the mind as part of the causal constellation of experience and thereby found Newton's theory of motion universally true, yet knowledge of things in themselves impossible. Safeguarding science, then, Kant paradoxically stripped it of scientific realism. Aborting Francis Bacon's inductivist mission to dissolve the veil of appearance to uncover the \"noumena\"—metaphysical view of nature's ultimate truths—Kant's transcendental idealism tasked science with simply modeling patterns of \"phenomena\". Safeguarding metaphysics, too, it found the mind's constants holding also universal moral truths, and launched German idealism, increasingly speculative.\n\nAuguste Comte found the problem of induction rather irrelevant since enumerative induction is grounded on the empiricism available, while science's point is not metaphysical truth. Comte found human knowledge had evolved from theological to metaphysical to scientific—the ultimate stage—rejecting both theology and metaphysics as asking questions unanswerable and posing answers unverifiable. Comte in the 1830s expounded positivism—the first modern philosophy of science and simultaneously a political philosophy—rejecting conjectures about unobservables, thus rejecting search for \"causes\". Positivism predicts observations, confirms the predictions, and states a \"law\", thereupon applied to benefit human society. From late 19th century into the early 20th century, the influence of positivism spanned the globe. Meanwhile, evolutionary theory's natural selection brought the Copernican Revolution into biology and eventuated in the first conceptual alternative to vitalism and teleology.\n\nWhereas Comtean positivism posed science as \"description\", logical positivism emerged in the late 1920s and posed science as \"explanation\", perhaps to better unify empirical sciences by covering not only fundamental science—that is, fundamental physics—but special sciences, too, such as biology, psychology, economics, and anthropology. After defeat of National Socialism with World War II's close in 1945, logical positivism shifted to a milder variant, \"logical empiricism\". All variants of the movement, which lasted until 1965, are neopositivism, sharing the quest of verificationism.\n\nNeopositivists led emergence of the philosophy subdiscipline philosophy of science, researching such questions and aspects of scientific theory and knowledge. Scientific realism takes scientific theory's statements at face value, thus accorded either falsity or truth—probable or approximate or actual. Neopositivists held scientific antirealism as instrumentalism, holding scientific theory as simply a device to predict observations and their course, while statements on nature's unobservable aspects are elliptical at or metaphorical of its observable aspects, rather.\n\nDN model received its most detailed, influential statement by Carl G Hempel, first in his 1942 article \"The function of general laws in history\", and more explicitly with Paul Oppenheim in their 1948 article \"Studies in the logic of explanation\". Leading logical empiricist, Hempel embraced the Humean empiricist view that humans observe sequence of sensory events, not cause and effect, as causal relations and casual mechanisms are unobservables. DN model bypasses causality beyond mere constant conjunction: first an event like \"A\", then always an event like \"B\".\n\nHempel held natural laws—empirically confirmed regularities—as satisfactory, and if included realistically to approximate causality. In later articles, Hempel defended DN model and proposed probabilistic explanation by \"inductive-statistical model\" (IS model). DN model and IS model—whereby the probability must be high, such as at least 50%—together form \"covering law model\", as named by a critic, William Dray. Derivation of statistical laws from other statistical laws goes to the \"deductive-statistical model\" (DS model). Georg Henrik von Wright, another critic, named the totality \"subsumption theory\".\n\nAmid failure of neopositivism's fundamental tenets, Hempel in 1965 abandoned verificationism, signaling neopositivism's demise. From 1930 onward, Karl Popper had refuted any positivism by asserting falsificationism, which Popper claimed had killed positivism, although, paradoxically, Popper was commonly mistaken for a positivist. Even Popper's 1934 book embraces DN model, widely accepted as the model of scientific explanation for as long as physics remained the model of science examined by philosophers of science.\n\nIn the 1940s, filling the vast observational gap between cytology and biochemistry, cell biology arose and established existence of cell organelles besides the nucleus. Launched in the late 1930s, the molecular biology research program cracked a genetic code in the early 1960s and then converged with cell biology as \"cell and molecular biology\", its breakthroughs and discoveries defying DN model by arriving in quest not of lawlike explanation but of causal mechanisms. Biology became a new model of science, while special sciences were no longer thought defective by lacking universal laws, as borne by physics.\n\nIn 1948, when explicating DN model and stating scientific explanation's semiformal \"conditions of adequacy\", Hempel and Oppenheim acknowledged redundancy of the third, \"empirical content\", implied by the other three—\"derivability\", \"lawlikeness\", and \"truth\". In the early 1980s, upon widespread view that causality ensures the explanans' relevance, Wesley Salmon called for returning \"cause\" to \"because\", and along with James Fetzer helped replace CA3 \"empirical content\" with CA3' \"strict maximal specificity\".\n\nSalmon introduced \"causal mechanical\" explanation, never clarifying how it proceeds, yet reviving philosophers' interest in such. Via shortcomings of Hempel's inductive-statistical model (IS model), Salmon introduced \"statistical-relevance model\" (SR model). Although DN model remained an idealized form of scientific explanation, especially in applied sciences, most philosophers of science consider DN model flawed by excluding many types of explanations generally accepted as scientific.\n\nAs theory of knowledge, epistemology differs from ontology, which is a subbranch of metaphysics, theory of reality. Ontology poses which categories of being—what sorts of things exist—and so, although a scientific theory's ontological commitment can be modified in light of experience, an ontological commitment inevitably precedes empirical inquiry.\n\nNatural laws, so called, are statements of humans' observations, thus are epistemological—concerning human knowledge—the \"epistemic\". Causal mechanisms and structures existing putatively independently of minds exist, or would exist, in the natural world's structure itself, and thus are ontological, the \"ontic\". Blurring epistemic with ontic—as by incautiously presuming a natural law to refer to a causal mechanism, or to trace structures realistically during unobserved transitions, or to be true regularities always unvarying—tends to generate a \"category mistake\".\n\nDiscarding ontic commitments, including causality \"per se\", DN model permits a theory's laws to be reduced to—that is, subsumed by—a more fundamental theory's laws. The higher theory's laws are explained in DN model by the lower theory's laws. Thus, the epistemic success of Newtonian theory's law of universal gravitation is reduced to—thus explained by—Einstein's general theory of relativity, although Einstein's discards Newton's ontic claim that universal gravitation's epistemic success predicting Kepler's laws of planetary motion is through a causal mechanism of a straightly attractive force instantly traversing absolute space despite absolute time.\n\nCovering law model reflects neopositivism's vision of empirical science, a vision interpreting or presuming unity of science, whereby all empirical sciences are either fundamental science—that is, fundamental physics—or are special sciences, whether astrophysics, chemistry, biology, geology, psychology, economics, and so on. All special sciences would network via covering law model. And by stating \"boundary conditions\" while supplying \"bridge laws\", any special law would reduce to a lower special law, ultimately reducing—theoretically although generally not practically—to fundamental science. (\"Boundary conditions\" are specified conditions whereby the phenomena of interest occur. \"Bridge laws\" translate terms in one science to terms in another science.)\n\nBy DN model, if one asks, \"Why is that shadow 20 feet long?\", another can answer, \"Because that flagpole is 15 feet tall, the Sun is at \"x\" angle, and laws of electromagnetism\". Yet by problem of symmetry, if one instead asked, \"Why is that flagpole 15 feet tall?\", another could answer, \"Because that shadow is 20 feet long, the Sun is at \"x\" angle, and laws of electromagnetism\", likewise a deduction from observed conditions and scientific laws, but an answer clearly incorrect. By the problem of irrelevance, if one asks, \"Why did that man not get pregnant?\", one could in part answer, among the explanans, \"Because he took birth control pills\"—if he factually took them, and the law of their preventing pregnancy—as covering law model poses no restriction to bar that observation from the explanans.\n\nMany philosophers have concluded that causality is integral to scientific explanation. DN model offers a necessary condition of a causal explanation—successful prediction—but not sufficient conditions of causal explanation, as a universal regularity can include spurious relations or simple correlations, for instance \"Z\" always following \"Y\", but not \"Z\" because of \"Y\", instead \"Y\" and then \"Z\" as an effect of \"X\". By relating temperature, pressure, and volume of gas within a container, Boyle's law permits prediction of an unknown variable—volume, pressure, or temperature—but does not explain \"why\" to expect that unless one adds, perhaps, the kinetic theory of gases.\n\nScientific explanations increasingly pose not determinism's universal laws, but probabilism's chance, \"ceteris paribus\" laws. Smoking's contribution to lung cancer fails even the inductive-statistical model (IS model), requiring probability over 0.5 (50%). (Probability standardly ranges from 0 (0%) to 1 (100%).) An applied science that applies statistics seeking associations between events, epidemiology cannot show causality, but consistently found higher incidence of lung cancer in smokers versus otherwise similar nonsmokers, although the proportion of smokers who develop lung cancer is modest. Versus nonsmokers, however, smokers as a group showed over 20 times the risk of lung cancer, and in conjunction with basic research, consensus followed that smoking had been scientifically explained as \"a\" cause of lung cancer, responsible for some cases that without smoking would not have occurred, a probabilistic counterfactual causality.\n\nThrough lawlike explanation, fundamental physics—often perceived as fundamental science—has proceeded through intertheory relation and theory reduction, thereby resolving experimental paradoxes to great historical success, resembling covering law model. In early 20th century, Ernst Mach as well as Wilhelm Ostwald had resisted Ludwig Boltzmann's reduction of thermodynamics—and thereby Boyle's law—to statistical mechanics partly \"because\" it rested on kinetic theory of gas, hinging on atomic/molecular theory of matter. Mach as well as Ostwald viewed matter as a variant of energy, and molecules as mathematical illusions, as even Boltzmann thought possible.\n\nIn 1905, via statistical mechanics, Albert Einstein predicted the phenomenon Brownian motion—unexplained since reported in 1827 by botanist Robert Brown. Soon, most physicists accepted that atoms and molecules were unobservable yet real. Also in 1905, Einstein explained the electromagnetic field's energy as distributed in \"particles\", doubted until this helped resolve atomic theory in the 1910s and 1920s. Meanwhile, all known physical phenomena were gravitational or electromagnetic, whose two theories misaligned. Yet belief in aether as the source of all physical phenomena was virtually unanimous. At experimental paradoxes, physicists modified the aether's hypothetical properties.\n\nFinding the luminiferous aether a useless hypothesis, Einstein in 1905 \"a priori\" unified all inertial reference frames to state special \"principle\" of relativity, which, by omitting aether, converted space and time into \"relative\" phenomena whose relativity aligned electrodynamics with the Newtonian principle Galilean relativity or invariance. Originally epistemic or instrumental, this was interpreted as ontic or realist—that is, a causal mechanical explanation—and the \"principle\" became a \"theory\", refuting Newtonian gravitation. By predictive success in 1919, general relativity apparently overthrew Newton's theory, a revolution in science resisted by many yet fulfilled around 1930.\n\nIn 1925, Werner Heisenberg as well as Erwin Schrödinger independently formalized quantum mechanics (QM). Despite clashing explanations, the two theories made identical predictions. Paul Dirac's 1928 model of the electron was set to special relativity, launching QM into the first quantum field theory (QFT), quantum electrodynamics (QED). From it, Dirac interpreted and predicted the electron's antiparticle, soon discovered and termed \"positron\", but the QED failed electrodynamics at high energies. Elsewhere and otherwise, strong nuclear force and weak nuclear force were discovered.\n\nIn 1941, Richard Feynman introduced QM's path integral formalism, which if taken toward \"interpretation\" as a causal mechanical model clashes with Heisenberg's matrix formalism and with Schrödinger's wave formalism, although all three are empirically identical, sharing predictions. Next, working on QED, Feynman sought to model particles without fields and find the vacuum truly empty. As each known fundamental force is apparently an effect of a field, Feynman failed. Louis de Broglie's waveparticle duality had rendered atomism—indivisible particles in a void—untenable, and highlighted the very notion of discontinuous particles as selfcontradictory.\n\nMeeting in 1947, Freeman Dyson, Richard Feynman, Julian Schwinger, and Sin-Itiro Tomonaga soon introduced \"renormalization\", a procedure converting QED to physics' most predictively precise theory, subsuming chemistry, optics, and statistical mechanics. QED thus won physicists' general acceptance. Paul Dirac criticized its need for renormalization as showing its unnaturalness, and called for an aether. In 1947, Willis Lamb had found unexpected motion of electron orbitals, shifted since the vacuum is not truly empty. Yet \"emptiness\" was catchy, abolishing aether conceptually, and physics proceeded ostensibly without it, even suppressing it. Meanwhile, \"sickened by untidy math, most philosophers of physics tend to neglect QED\".\n\nPhysicists have feared even mentioning \"aether\", renamed \"vacuum\", which—as such—is nonexistent. General philosophers of science commonly believe that aether, rather, is fictitious, \"relegated to the dustbin of scientific history ever since\" 1905 brought special relativity. Einstein was noncommittal to aether's nonexistence, simply said it superfluous. Abolishing Newtonian motion for electrodynamic primacy, however, Einstein inadvertently reinforced aether, and to explain motion was led back to aether in general relativity. Yet resistance to relativity theory became associated with earlier theories of aether, whose word and concept became taboo. Einstein explained special relativity's compatibility with an aether, but Einstein aether, too, was opposed. Objects became conceived as pinned directly on space and time by abstract geometric relations lacking ghostly or fluid medium.\n\nBy 1970, QED along with weak nuclear field was reduced to electroweak theory (EWT), and the strong nuclear field was modeled as quantum chromodynamics (QCD). Comprised by EWT, QCD, and Higgs field, this Standard Model of particle physics is an \"effective theory\", not truly fundamental. As QCD's particles are considered nonexistent in the everyday world, QCD especially suggests an aether, routinely found by physics experiments to exist and to exhibit relativistic symmetry. Confirmation of the Higgs particle, modeled as a condensation within the Higgs field, corroborates aether, although physics need not state or even include aether. Organizing regularities of \"observations\"—as in the covering law model—physicists find superfluous the quest to discover \"aether\".\n\nIn 1905, from special relativity, Einstein deduced mass–energy equivalence, particles being variant forms of distributed energy, how particles colliding at vast speed experience that energy's transformation into mass, producing heavier particles, although physicists' talk promotes confusion. As \"the contemporary locus of metaphysical research\", QFTs pose particles not as existing individually, yet as \"excitation modes\" of fields, the particles and their masses being states of aether, apparently unifying all physical phenomena as the more fundamental causal reality, as long ago foreseen. Yet a \"quantum\" field is an intricate abstraction—a \"mathematical\" field—virtually inconceivable as a \"classical\" field's physical properties. Nature's deeper aspects, still unknown, might elude any possible field theory.\n\nThough discovery of causality is popularly thought science's aim, search for it was shunned by the Newtonian research program, even more Newtonian than was Isaac Newton. By now, most theoretical physicists infer that the four, known fundamental interactions would reduce to superstring theory, whereby atoms and molecules, after all, are energy vibrations holding mathematical, geometric forms. Given uncertainties of scientific realism, some conclude that the concept \"causality\" raises comprehensibility of scientific explanation and thus is key folk science, but compromises precision of scientific explanation and is dropped as a science matures. Even epidemiology is maturing to heed the severe difficulties with presumptions about causality. Covering law model is among Carl G Hempel's admired contributions to philosophy of science.\n\nTypes of inference\n\nRelated subjects\n\n\n"}
{"id": "33548913", "url": "https://en.wikipedia.org/wiki?curid=33548913", "title": "Dehaene–Changeux model", "text": "Dehaene–Changeux model\n\nThe Dehaene–Changeux model (DCM), also known as the global neuronal workspace or the global cognitive workspace model is a part of Bernard Baars's \"global workspace model\" for consciousness.\n\nIt is a computer model of the neural correlates of consciousness programmed as a neural network. It attempts to reproduce the swarm behaviour of the brain's \"higher cognitive functions\" such as consciousness, decision-making and the central executive functions. It was developed by cognitive neuroscientists Stanislas Dehaene and Jean-Pierre Changeux beginning in 1986. It has been used to provide a predictive framework to the study of inattentional blindness and the solving of the Tower of London test.\n\nThe Dehaene–Changeux model was initially established as a spin glass neural network attempting to represent learning and to then provide a stepping stone towards artificial learning among other objectives. It would later be used to predict observable reaction times within the priming paradigm and in inattentional blindness.\n\nThe Dehaene–Changeux model is a meta neural network (i.e. a network of neural networks) composed of a very large number of integrate-and-fire neurons programmed in either a stochastic or deterministic way. The neurons are organised in complex thalamo-cortical columns with long-range connexions and a critical role played by the interaction between von Economo's areas. Each thalamo-cortical column is composed of pyramidal cells and inhibitory interneurons receiving a long-distance excitatory neuromodulation which could represent noradrenergic input.\n\nAmong others Cohen & Hudson (2002) had already used \"\"Meta neural networks as intelligent agents for diagnosis \" Similarly to Cohen & Hudson, Dehaene & Changeux have established their model as an interaction of meta-neural networks (thalamocortical columns) themselves programmed in the manner of a \"hierarchy of neural networks that together act as an intelligent agent\"\", in order to use them as a system composed of a large scale of inter-connected intelligent agents for predicting the self-organized behaviour of the neural correlates of consciousness. It may also be noted that Jain et al. (2002) had already clearly identified spiking neurons as intelligent agents since the lower bound for computational power of networks of spiking neurons is the capacity to simulate in real-time for boolean-valued inputs any Turing machine. The DCM being composed of a very large number of interacting sub-networks which are themselves intelligent agents, it is formally a Multi-agent system programmed as a Swarm or neural networks and \"a fortiori\" of spiking neurons.\n\nThe DCM exhibits several surcritical emergent behaviors such as multistability and a Hopf bifurcation between two very different regimes which may represent either sleep or arousal with a various all-or-none behaviors which Dehaene et al. use to determine a testable taxonomy between different states of consciousness. \n\nThe Dehaene-Changeux Model contributed to the study of nonlinearity and self-organized criticality in particular as an explanatory model of the brain's emergent behaviors, including consciousness. Studying the brain's phase-locking and large-scale synchronization, Kitzbichler et al. (2011a) confirmed that criticality is a property of human brain functional network organization at all frequency intervals in the brain's physiological bandwidth.\n\nFurthermore, exploring the neural dynamics of cognitive efforts after, \"inter alia\", the Dehaene-Changeux Model, Kitzbichler et al. (2011b) demonstrated how cognitive effort breaks the modularity of mind to make human brain functional networks transiently adopt a more efficient but less economical configuration. Werner (2007a) used the Dehaene-Changeux Global Neuronal Workspace to defend the use of statistical physics approaches for exploring phase transitions, scaling and universality properties of the so-called \"Dynamic Core\" of the brain, with relevance to the macroscopic electrical activity in EEG and EMG. Furthermore, building from the Dehaene-Changeux Model, Werner (2007b) proposed that the application of the twin concepts of scaling and universality of the theory of non-equilibrium phase transitions can serve as an informative approach for elucidating the nature of underlying neural-mechanisms, with emphasis on the dynamics of recursively reentrant activity flow in intracortical and cortico-subcortical neuronal loops. Friston (2000) also claimed that \"the nonlinear nature of asynchronous coupling enables the rich, context-sensitive interactions that characterize real brain dynamics, suggesting that it plays a role in functional integration that may be as important as synchronous interactions\".\n\nIt contributed to the study of phase transition in the brain under sedation, and notably GABA-ergic sedation such as that induced by propofol (Murphy et al. 2011, Stamatakis et al. 2010). The Dehaene-Changeux Model was contrasted and cited in the study of collective consciousness and its pathologies (Wallace et al. 2007). Boly et al. (2007) used the model for a reverse somatotopic study, demonstrating a correlation between baseline brain activity and somatosensory perception in humans. Boly et al. (2008) also used the DCM in a study of the baseline state of consciousness of the human brain's default network.\n\n\n"}
{"id": "17910574", "url": "https://en.wikipedia.org/wiki?curid=17910574", "title": "Digital ecosystem", "text": "Digital ecosystem\n\nA digital ecosystem is a distributed, adaptive, open socio-technical system with properties of self-organisation, scalability and sustainability inspired from natural ecosystems. Digital ecosystem models are informed by knowledge of natural ecosystems, especially for aspects related to competition and collaboration among diverse entities. The term is used in the computer industry, the entertainment industry, and the World Economic Forum.\n\nThe concept of Digital Business Ecosystem was put forward in 2002 by a group of European researchers and practitioners, including Francesco Nachira, Paolo Dini and Andrea Nicolai, who applied the general notion of digital ecosystems to model the process of adoption and development of ICT-based products and services in competitive, highly fragmented markets like the European one\n. Elizabeth Chang, Ernesto Damiani and Tharam Dillon started in 2007 the IEEE Digital EcoSystems and Technologies Conference (IEEE DEST). Richard Chbeir, Youakim Badr, Dominique Laurent, and Hiroshi Ishikawa started in 2009 the ACM Conference on Management of Digital EcoSystems (MEDES)\n\nThe digital ecosystem metaphor and models have been applied to a number of business areas related to the production and distribution of knowledge-intensive products and services, including higher education. The perspective of this research is providing methods and tools to achieve a set of objectives of the ecosystem (e.g. sustainability, fairness, bounded information asymmetry, risk control and gracious failure). These objectives are seen as desirable properties whose emergence should be fostered by the digital ecosystem self-organization, rather than as explicit design goals like in conventional IT.\n\n\n"}
{"id": "2286327", "url": "https://en.wikipedia.org/wiki?curid=2286327", "title": "Distinction without a difference", "text": "Distinction without a difference\n\nA distinction without a difference is a type of logical fallacy where an author or speaker attempts to describe a distinction between two things where no discernible difference exists. It is particularly used when a word or phrase has connotations associated with it that one party to an argument prefers to avoid.\n\n\n"}
{"id": "638834", "url": "https://en.wikipedia.org/wiki?curid=638834", "title": "Economic model", "text": "Economic model\n\nIn economics, a model is a theoretical construct representing economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified, often mathematical, framework designed to illustrate complex processes. Frequently, economic models posit structural parameters. A model may have various exogenous variables, and those variables may change to create various responses by economic variables. Methodological uses of models include investigation, theorizing, and fitting theories to the world.\n\nIn general terms, economic models have two functions: first as a simplification of and abstraction from observed data, and second as a means of selection of data based on a paradigm of econometric study.\n\n\"Simplification\" is particularly important for economics given the enormous complexity of economic processes. This complexity can be attributed to the diversity of factors that determine economic activity; these factors include: individual and cooperative decision processes, resource limitations, environmental and geographical constraints, institutional and legal requirements and purely random fluctuations. Economists therefore must make a reasoned choice of which variables and which relationships between these variables are relevant and which ways of analyzing and presenting this information are useful.\n\n\"Selection\" is important because the nature of an economic model will often determine what facts will be looked at, and how they will be compiled. For example, inflation is a general economic concept, but to measure inflation requires a model of behavior, so that an economist can differentiate between changes in relative prices and changes in price that are to be attributed to inflation.\n\nIn addition to their professional academic interest, the use of models include:\n\nA model establishes an \"argumentative framework\" for applying logic and mathematics that can be independently discussed and tested and that can be applied in various instances. Policies and arguments that rely on economic models have a clear basis for soundness, namely the validity of the supporting model.\n\nEconomic models in current use do not pretend to be \"theories of everything economic\"; any such pretensions would immediately be thwarted by computational infeasibility and the incompleteness or lack of theories for various types of economic behavior. Therefore, conclusions drawn from models will be approximate representations of economic facts. However, properly constructed models can remove extraneous information and isolate useful approximations of key relationships. In this way more can be understood about the relationships in question than by trying to understand the entire economic process.\n\nThe details of model construction vary with type of model and its application, but a generic process can be identified. Generally any modelling process has two steps: generating a model, then checking the model for accuracy (sometimes called diagnostics). The diagnostic step is important because a model is only useful to the extent that it accurately mirrors the relationships that it purports to describe. Creating and diagnosing a model is frequently an iterative process in which the model is modified (and hopefully improved) with each iteration of diagnosis and respecification. Once a satisfactory model is found, it should be double checked by applying it to a different data set.\n\nAccording to whether all the model variables are deterministic, economic models can be classified as stochastic or non-stochastic models; according to whether all the variables are quantitative, economic models are classified as discrete or continuous choice model; according to the model's intended purpose/function, it can be classified as\nquantitative or qualitative; according to the model's ambit, it can be classified as a general equilibrium model, a partial equilibrium model, or even a non-equilibrium model; according to the economic agent's characteristics, models can be classified as rational agent models, representative agent models etc.\n\n\nAt a more practical level, quantitative modelling is applied to many areas of economics and several methodologies have evolved more or less independently of each other. As a result, no overall model taxonomy is naturally available. We can nonetheless provide a few examples that illustrate some particularly relevant points of model construction.\n\n\n\nMost economic models rest on a number of assumptions that are not entirely realistic. For example, agents are often assumed to have perfect information, and markets are often assumed to clear without friction. Or, the model may omit issues that are important to the question being considered, such as externalities. Any analysis of the results of an economic model must therefore consider the extent to which these results may be compromised by inaccuracies in these assumptions, and a large literature has grown up discussing problems with economic models, or at least asserting that their results are unreliable.\n\nOne of the major problems addressed by economic models has been understanding economic growth. An early attempt to provide a technique to approach this came from the French physiocratic school in the Eighteenth century. Among these economists, François Quesnay should be noted, particularly for his development and use of tables he called \"Tableaux économiques\". These tables have in fact been interpreted in more modern terminology as a Leontiev model, see the Phillips reference below.\n\nAll through the 18th century (that is, well before the founding of modern political economy, conventionally marked by Adam Smith's 1776 Wealth of Nations) simple probabilistic models were used to understand the economics of insurance. This was a natural extrapolation of the theory of gambling, and played an important role both in the development of probability theory itself and in the development of actuarial science. Many of the giants of 18th century mathematics contributed to this field. Around 1730, De Moivre addressed some of these problems in the 3rd edition of \"The Doctrine of Chances\". Even earlier (1709), Nicolas Bernoulli studies problems related to savings and interest in the Ars Conjectandi. In 1730, Daniel Bernoulli studied \"moral probability\" in his book Mensura Sortis, where he introduced what would today be called \"logarithmic utility of money\" and applied it to gambling and insurance problems, including a solution of the paradoxical Saint Petersburg problem. All of these developments were summarized by Laplace in his Analytical Theory of Probabilities (1812). Clearly, by the time David Ricardo came along he had a lot of well-established math to draw from.\n\nIn the late 1980s the Brookings Institution compared 12 leading macroeconomic models available at the time. They compared the models' predictions for how the economy would respond to specific economic shocks (allowing the models to control for all the variability in the real world; this was a test of model vs. model, not a test against the actual outcome). Although the models simplified the world and started from a stable, known common parameters the various models gave significantly different answers. For instance, in calculating the impact of a monetary loosening on output some models estimated a 3% change in GDP after one year, and one gave almost no change, with the rest spread between.\n\nPartly as a result of such experiments, modern central bankers no longer have as much confidence that it is possible to 'fine-tune' the economy as they had in the 1960s and early 1970s. Modern policy makers tend to use a less activist approach, explicitly because they lack confidence that their models will actually predict where the economy is going, or the effect of any shock upon it. The new, more humble, approach sees danger in dramatic policy changes based on model predictions, because of several practical and theoretical limitations in current macroeconomic models; in addition to the theoretical pitfalls, (listed above) some problems specific to aggregate modelling are:\n\nComplex systems specialist and mathematician David Orrell wrote on this issue in his book Apollo's Arrow and explained that the weather, human health and economics use similar methods of prediction (mathematical models). Their systems—the atmosphere, the human body and the economy—also have similar levels of complexity. He found that forecasts fail because the models suffer from two problems : (i) they cannot capture the full detail of the underlying system, so rely on approximate equations; (ii) they are sensitive to small changes in the exact form of these equations. This is because complex systems like the economy or the climate consist of a delicate balance of opposing forces, so a slight imbalance in their representation has big effects. Thus, predictions of things like economic recessions are still highly inaccurate, despite the use of enormous models running on fast computers.\n\nEconomic and meteorological simulations may share a fundamental limit to their predictive powers: chaos. Although the modern mathematical work on chaotic systems began in the 1970s the danger of chaos had been identified and defined in \"Econometrica\" as early as 1958:\n\nIt is straightforward to design economic models susceptible to butterfly effects of initial-condition sensitivity.\n\nHowever, the econometric research program to identify which variables are chaotic (if any) has largely concluded that aggregate macroeconomic variables probably do not behave chaotically. This would mean that refinements to the models could ultimately produce reliable long-term forecasts. However the validity of this conclusion has generated two challenges:\n\nMore recently, chaos (or the butterfly effect) has been identified as less significant than previously thought to explain prediction errors. Rather, the predictive power of economics and meteorology would mostly be limited by the models themselves and the nature of their underlying systems (see Comparison with models in other sciences above).\n\nA key strand of free market economic thinking is that the market's invisible hand guides an economy to prosperity more efficiently than central planning using an economic model. One reason, emphasized by Friedrich Hayek, is the claim that many of the true forces shaping the economy can never be captured in a single plan. This is an argument that cannot be made through a conventional (mathematical) economic model, because it says that there are critical systemic-elements that will always be omitted from any top-down analysis of the economy.\n\n\n\n\n"}
{"id": "39228396", "url": "https://en.wikipedia.org/wiki?curid=39228396", "title": "Equivalence principle (geometric)", "text": "Equivalence principle (geometric)\n\nThe equivalence principle is one of the corner-stones of gravitation theory. Different formulations of the equivalence principle are labeled \"weakest\", \"weak\", \"middle-strong\" and \"strong.\" All of these formulations are based on the empirical equality of inertial mass, gravitational active and passive charges.\n\nThe \"weakest\" equivalence principle is restricted to the motion law of a probe point mass in a uniform gravitational field. Its localization is the \"weak\" equivalence principle that states the existence of a desired local inertial frame at a given world point. This is the case of equations depending on a gravitational field and its first order derivatives, e. g., the equations of mechanics of probe point masses, and the equations of electromagnetic and Dirac fermion fields. The \"middle-strong\" equivalence principle is concerned with any matter, except a gravitational field, while the \"strong\" one is applied to all physical laws.\n\nThe above-mentioned variants of the equivalence principle aim to guarantee the transition of General Relativity to Special Relativity in a certain reference frame. However, only the particular \"weakest\" and \"weak\" equivalence principles are true. \nTo overcome this difficulty, the equivalence principle can be formulated in geometric terms as follows.\n\nIn the spirit of Felix Klein's Erlanger program, Special Relativity can be characterized as the Klein geometry of Lorentz group invariants. Then the geometric equivalence principle is formulated to require the existence of Lorentz invariants on a world manifold formula_1. This requirement holds if the tangent bundle formula_2 of formula_1 admits an atlas with Lorentz transition functions, i.e., a structure group of the associated frame bundle formula_4 of linear tangent frames in formula_2 is reduced to the Lorentz group formula_6. By virtue of the well known theorem on structure group reduction, this reduction takes place if and only if the quotient bundle formula_7 possesses a global section, which is a pseudo-Riemannian metric on formula_1.\n\nThus the geometric equivalence principle provides the necessary and sufficient conditions of the existence of a pseudo-Riemannian metric, i.e., a gravitational field on a world manifold.\n\nBased on the geometric equivalence principle, gravitation theory is formulated as gauge theory where a gravitational field is described as a classical Higgs field responsible for spontaneous breakdown of space-time symmetries.\n\n\n"}
{"id": "15285866", "url": "https://en.wikipedia.org/wiki?curid=15285866", "title": "Experimental system", "text": "Experimental system\n\nIn scientific research, an experimental system is the physical, technical and procedural basis for an experiment or series of experiments. Historian of science Hans-Jörg Rheinberger defines an experimental system as: \"A basic unit of experimental activity combining local, technical, instrumental, institutional, social, and epistemic aspects.\" Scientists (particularly laboratory biologists) and historians and philosophers of biology have pointed to the development and spread of successful experimental systems, such as those based on popular model organism or scientific apparatus, as key elements in the history of science, particularly since the early 20th century. The choice of an appropriate experimental system is often seen as critical for a scientist's long-term success, as experimental systems can be very productive for some kinds of questions and less productive for others, acquiring a sort of momentum that takes research in unpredicted directions.\n\nA successful experimental system must be stable and reproducible enough for scientists to make sense of the system's behavior, but variable and unpredictable enough that it can produce useful results. In many cases, a well-understood experimental system can be \"black-boxed\" as a standard technique, which can then be a component of other experimental systems. Rheinberger divides experimental systems into two parts: the part under investigation (\"epistemic things\") and the well-understood part that provides a stable context for experimentation (\"technical objects\").\n\nThe development of experimental systems in biology often requires the \"domestication\" of a particular organism for the laboratory environment, including the creation of relatively homogeneous lines or strains and the tailoring of conditions to highlight the variable aspects that scientists are interested in. Scientific technologies, similarly, often require the development of a full experimental system to go from a viable concept to a technique that works in practice on a usefully consistent basis. For example, the invention of the polymerase chain reaction (PCR) is generally attributed to Kary Mullis, who came up with the concept in 1983, but the process of development of PCR into the revolutionary technology it became by the early 1990s took years of work by others at Cetus Corporation—and the basic components of the system had been known since the 1960s DNA synthesis work of Har Gobind Khorana—making \"who invented PCR?\" a complicated question.\n\n"}
{"id": "37232", "url": "https://en.wikipedia.org/wiki?curid=37232", "title": "Fermat's principle", "text": "Fermat's principle\n\nIn optics, Fermat's principle or the principle of least time, named after French mathematician Pierre de Fermat, is the principle that the path taken between two points by a ray of light is the path that can be traversed in the least time. This principle is sometimes taken as the definition of a ray of light. However, this version of the principle is not general; a more modern statement of the principle is that rays of light traverse the path of stationary optical length with respect to variations of the path. In other words, a ray of light prefers the path such that there are other paths, arbitrarily nearby on either side, along which the ray would take almost exactly the same time to traverse.\n\nFermat's principle can be used to describe the properties of light rays reflected off mirrors, refracted through different media, or undergoing total internal reflection. It follows mathematically from Huygens' principle (at the limit of small wavelength). Fermat's text \"Analyse des réfractions\" exploits the technique of adequality to derive Snell's law of refraction and the law of reflection.\n\nFermat's principle has the same form as Hamilton's principle and it is the basis of Hamiltonian optics.\n\nThe time T a point of the electromagnetic wave needs to cover a path between the points A and B is given by:\n\n\"c\" is the speed of light in vacuum, \"ds\" an infinitesimal displacement along the ray, \"v\" = \"ds\"/\"dt\" the speed of light in a medium and \"n\" = \"c\"/\"v\" the refractive index of that medium, formula_2 is the starting time (the wave front is in A), formula_3 is the arrival time at B. The optical path length of a ray from a point A to a point B is defined by:\n\nand it is related to the travel time by \"S\" = \"cT\". The optical path length is a purely geometrical quantity since time is not considered in its calculation. An extremum in the light travel time between two points A and B is equivalent to an extremum of the optical path length between those two points. The historical form proposed by Fermat is incomplete. A complete modern statement of the variational Fermat principle is that In the context of calculus of variations this can be written as\n\nIn general, the refractive index is a scalar field of position in space, that is, formula_6 in 3D euclidean space. Assuming now that light has a component that travels along the \"x\" axis, the path of a light ray may be parametrized as formula_7 and\n\nwhere formula_9. The principle of Fermat can now be written as\n\nwhich has the same form as Hamilton's principle but in which \"x\" takes the role of time in classical mechanics. Function formula_12 is the optical Lagrangian from which the Lagrangian and Hamiltonian (as in Hamiltonian mechanics) formulations of geometrical optics may be derived.\n\nClassically, Fermat's principle can be considered as a mathematical consequence of Huygens' principle. Indeed, of all secondary waves (along all possible paths) the waves with the extremal (stationary) paths contribute most due to constructive interference. Suppose that light waves propagate from A to B by all possible routes AB, unrestricted initially by rules of geometrical or physical optics. The various optical paths AB will vary by amounts greatly in excess of one wavelength, and so the waves arriving at B will have a large range of phases and will tend to interfere destructively. But if there is a shortest route AB, and the optical path varies smoothly through it, then a considerable number of neighboring routes close to AB will have optical paths differing from AB by second-order amounts only and will therefore interfere constructively. Waves along and close to this shortest route will thus dominate and AB will be the route along which the light is seen to travel.\n\nFermat's principle is the main principle of quantum electrodynamics which states that any particle (e.g. a photon or an electron) propagates over all available, unobstructed paths and that the interference, or superposition, of its wavefunction over all those paths at the point of observation gives the probability of detecting the particle at this point. Thus, because the extremal paths (shortest, longest, or stationary) cannot be completely canceled out, they contribute most to this interference. In humans, for example, Fermat's principle can be demonstrated in a situation when a lifeguard has to find the fastest way to traverse both beach and water in order to reach a drowning swimmer. The principle has been tested in studies with ants, in which the ants' nest is on one end of a container and food is on the opposite end, but the ants choose to follow the path of least time, rather than the most direct path.\n\nIn the classic mechanics of waves, Fermat's principle follows from the extremum principle of mechanics (see variational principle).\n\nEuclid, c. 320 BCE in his Catoptrics (on mirrors, including spherical mirrors) and Optics, laid the foundations for reflection, which was repeated by Ptolemy, and then in his more detailed books that have surfaced, Hero of Alexandria (Heron) (c. 60) described the principle of reflection, which stated that a ray of light that goes from point A to point B, suffering any number of reflections on flat mirrors in the same medium, has a smaller path length than any nearby path.\n\nIbn al-Haytham (Alhacen), in his \"Book of Optics\" (1021), expanded the principle to both reflection and refraction, and expressed an early version of the principle of least time. His experiments were based on earlier works on refraction carried out by the Greek scientist Ptolemy.\n\nThe generalized principle of least time in its modern form was stated by Fermat in a letter dated January 1, 1662, to Cureau de la Chambre. It was met with objections by Claude Clerselier in May 1662, an expert in optics and leading spokesman for the Cartesians at the time. Amongst his objections, Clerselier states:\n\"... The principle which you take as the basis for your proof, namely that Nature always acts by using the simplest and shortest paths, is merely a moral, and not a physical one. It is not, and cannot be, the cause of any effect in Nature.\n\nThe original French, from Mahoney, is as follows:\n\"Le principe que vous prenez pour fondement de votre démonstration, à savoir que la nature agit toujours par les voies les plus courtes et les plus simples, n’est qu’un principe moral et non point physique, qui n’est point et qui ne peut être la cause d’aucun effet de la nature.\"\nAlthough Fermat's principle does not hold standing alone, we now know it can be derived from earlier principles such as Huygens' principle.\n\nHistorically, Fermat's principle has served as a guiding principle in the formulation of physical laws with the use of variational calculus (see Principle of least action).\n\n"}
{"id": "386398", "url": "https://en.wikipedia.org/wiki?curid=386398", "title": "Hypothetico-deductive model", "text": "Hypothetico-deductive model\n\nThe hypothetico-deductive model or method is a proposed description of scientific method. According to it, scientific inquiry proceeds by formulating a hypothesis in a form that can be falsifiable, using a test on observable data where the outcome is not yet known. A test outcome that could have and does run contrary to predictions of the hypothesis is taken as a falsification of the hypothesis. A test outcome that could have, but does not run contrary to the hypothesis corroborates the theory. It is then proposed to compare the explanatory value of competing hypotheses by testing how stringently they are corroborated by their predictions.\n\nOne example of an algorithmic statement of the hypothetico-deductive method is as follows:\n\nOne possible sequence in this model would be 1, 2, 3, 4. If the outcome of 4 holds, and 3 is not yet disproven, you may continue with 3, 4, 1, and so forth; but if the outcome of 4 shows 3 to be false, you will have to go back to 2 and try to invent a new 2, deduce a new 3, look for 4, and so forth.\n\nNote that this method can never absolutely verify (prove the truth of) 2. It can only falsify 2. (This is what Einstein meant when he said, \"No amount of experimentation can ever prove me right; a single experiment can prove me wrong.\")\n\nAdditionally, as pointed out by Carl Hempel (1905–1997), this simple view of the scientific method is incomplete; a conjecture can also incorporate probabilities, e.g., the drug is effective about 70% of the time. Tests, in this case, must be repeated to substantiate the conjecture (in particular, the probabilities). In this and other cases, we can quantify a probability for our confidence in the conjecture itself and then apply a Bayesian analysis, with each experimental result shifting the probability either up or down. Bayes' theorem shows that the probability will never reach exactly 0 or 100% (no absolute certainty in either direction), but it can still get very close to either extreme. See also confirmation holism.\n\nQualification of corroborating evidence is sometimes raised as philosophically problematic. The raven paradox is a famous example. The hypothesis that 'all ravens are black' would appear to be corroborated by observations of only black ravens. However, 'all ravens are black' is logically equivalent to 'all non-black things are non-ravens' (this is the contraposition form of the original implication). 'This is a green tree' is an observation of a non-black thing that is a non-raven and therefore corroborates 'all non-black things are non-ravens'. It appears to follow that the observation 'this is a green tree' is corroborating evidence for the hypothesis 'all ravens are black'. Attempted resolutions may distinguish:\n\nEvidence contrary to a hypothesis is itself philosophically problematic. Such evidence is called a falsification of the hypothesis. However, under the theory of confirmation holism it is always possible to save a given hypothesis from falsification. This is so because any falsifying observation is embedded in a theoretical background, which can be modified in order to save the hypothesis. Popper acknowledged this but maintained that a critical approach respecting methodological rules that avoided such \"immunizing stratagems\" is conducive to the progress of science.\n\nPhysicist Sean Carroll claims the model ignores underdetermination.\nThe hypothetico-deductive model (or approach) versus other research models\n\nThe hypothetico-deductive approach contrasts with other research models such as the inductive approach or grounded theory. In the data percolation methodology, \nthe hypothetico-deductive approach is included in a paradigm of pragmatism by which four types of relations between the variables can exist: descriptive, of influence, longitudinal or causal. The variables are classified in two groups, structural and functional, a classification that drives the formulation of hypotheses and the statistical tests to be performed on the data so as to increase the efficiency of the research. \n\n"}
{"id": "4021739", "url": "https://en.wikipedia.org/wiki?curid=4021739", "title": "LaSalle's invariance principle", "text": "LaSalle's invariance principle\n\nLaSalle's invariance principle (also known as the invariance principle, Barbashin-Krasovskii-LaSalle principle, or Krasovskii-LaSalle principle ) is a criterion for the asymptotic stability of an autonomous (possibly nonlinear) dynamical system.\n\nSuppose a system is represented as\n\nwhere formula_2 is the vector of variables, with\n\nIf a formula_4 function formula_5 can be found such that\n\nthen the set of accumulation points of any trajectory is contained in formula_8 where formula_8 is the union of complete trajectories contained entirely in the set formula_10. \n\nIf we additionally have that the function formula_11 is positive definite, i.e.\n\nand if formula_8 contains no trajectory of the system except the trivial trajectory formula_16 for formula_17, then the origin is asymptotically stable.\n\nFurthermore, if formula_11 is radially unbounded, i.e.\n\nthen the origin is globally asymptotically stable.\n\nIf \n\nhold only for formula_24 in some neighborhood formula_25 of the origin, and the set\n\ndoes not contain any trajectories of the system besides the trajectory formula_27, then the local version of the invariance principle states that the origin is locally asymptotically stable.\n\nIf formula_28 is negative definite, the global asymptotic stability of the origin is a consequence of Lyapunov's second theorem. The invariance principle gives a criterion for asymptotic stability in the case when formula_29 is only negative semidefinite.\n\nThis section will apply the invariance principle to establish the local asymptotic stability of a simple system, the pendulum with friction. This system can be modeled with the differential equation \n\nwhere formula_31 is the angle the pendulum makes with the vertical normal, formula_32 is the mass of the pendulum, formula_33 is the length of the pendulum, formula_34 is the friction coefficient, and \"g\" is acceleration due to gravity.\n\nThis, in turn, can be written as the system of equations\n\nUsing the invariance principle, it can be shown that all trajectories which begin in a ball of certain size around the origin formula_37 asymptotically converge to the origin. We define formula_38 as\n\nThis formula_38 is simply the scaled energy of the system Clearly, formula_41 is positive definite in an open ball of radius formula_42 around the origin. Computing the derivative,\n\nObserve that formula_44. If it were true that formula_45, we could conclude that every trajectory approaches the origin by Lyapunov's second theorem. Unfortunately, formula_46 and formula_47 is only negative semidefinite. However, the set\n\nwhich is simply the set\n\ndoes not contain any trajectory of the system, except the trivial trajectory x = 0. Indeed, if at some time formula_50, formula_51, then because \nformula_52 must be less than formula_42 away from the origin, formula_54 and formula_55. As a result, the trajectory will not stay in the set formula_56.\n\nAll the conditions of the local version of the invariance principle are satisfied, and we can conclude that every trajectory that begins in some neighborhood of the origin will converge to the origin as formula_57 .\n\nThe general result was independently discovered by J.P. LaSalle (then at RIAS) and N.N. Krasovskii, who published in 1960 and 1959 respectively. While LaSalle was the first author in the West to publish the general theorem in 1960, a special case of the theorem was in communicated in 1952 by Barbashin and Krasovskii, followed by a publication of the general result in 1959 by Krasovskii .\n\n\n\n\n\n"}
{"id": "3108937", "url": "https://en.wikipedia.org/wiki?curid=3108937", "title": "Landauer's principle", "text": "Landauer's principle\n\nLandauer's principle is a physical principle pertaining to the lower theoretical limit of energy consumption of computation. It holds that \"any logically irreversible manipulation of information, such as the erasure of a bit or the merging of two computation paths, must be accompanied by a corresponding entropy increase in non-information-bearing degrees of freedom of the information-processing apparatus or its environment\".\n\nAnother way of phrasing Landauer's principle is that if an observer loses information about a physical system, the observer loses the ability to extract work from that system.\n\nA so-called logically-reversible computation, in which no information is erased, may in principle be carried out without releasing any heat. This has led to considerable interest in the study of reversible computing. Indeed, without reversible computing, increases in the number of computations-per-joule-of-energy-dissipated must come to a halt by about 2050: because the limit implied by Landauer's principle will be reached by then, according to Koomey's law. \n\nAt 20 °C (room temperature, or 293.15 K), the Landauer limit represents an energy of approximately 0.0172 eV, or 2.75 zJ. Theoretically, roomtemperature computer memory operating at the Landauer limit could be changed at a rate of one billion bits per second with energy being converted to heat in the memory media at the rate of only 2.85 trillionths of a watt (that is, at a rate of only 2.85 pJ/s). Modern computers use millions of times as much energy per second.\n\nRolf Landauer first proposed the principle in 1961 while working at IBM. He rigorously justified and stated important limits to an earlier conjecture by John von Neumann. For this reason, it is sometimes referred to as being simply the Landauer bound or Landauer limit.\n\nIn 2011, the principle was generalized to show that while information erasure requires an increase in entropy, that increase could theoretically occur at no energy cost. Instead, the cost can be taken in another conserved quantity, such as angular momentum.\n\nIn a 2012 article published in \"Nature\", a team of physicists from the École normale supérieure de Lyon, University of Augsburg and the University of Kaiserslautern described that for the first time they have measured the tiny amount of heat released when an individual bit of data is erased.\n\nIn 2014, physical experiments tested Landauer's principle and confirmed its predictions.\n\nIn 2016, researchers used a laser probe to measure the amount of energy dissipation that resulted when a nanomagnetic bit flipped from off to on. Flipping the bit required 15 millielectron volts (3 zeptojoules).\n\nA 2018 article published in \"Nature Physics\" features a Landauer erasure performed at cryogenic temperatures (T = 1K) on an array of high-spin (S = 10) quantum molecular magnets. The array is made to act as a spin register where each nanomagnet encodes a single bit of information. The experiment has laid the foundations for the extension of the validity of the Landauer principle to the quantum realm. Owing to the fast dynamics and low \"inertia\" of the single spins used in the experiment, the researchers also showed how an erasure operation can be carried out at the lowest possible thermodynamic cost — that imposed by the Landauer principle — and at a high speed. \n\nLandauer's principle can be understood to be a simple logical consequence of the second law of thermodynamics—which states that the entropy of an isolated system cannot decrease—together with the definition of thermodynamic temperature. For, if the number of possible logical states of a computation were to decrease as the computation proceeded forward (logical irreversibility), this would constitute a forbidden decrease of entropy, unless the number of possible physical states corresponding to each logical state were to simultaneously increase by at least a compensating amount, so that the total number of possible physical states was no smaller than it was originally (i.e. total entropy has not decreased).\n\nYet, an increase in the number of physical states corresponding to each logical state means that, for an observer who is keeping track of the logical state of the system but not the physical state (for example an \"observer\" consisting of the computer itself), the number of possible physical states has increased; in other words, entropy has increased from the point of view of this observer.\n\nThe maximum entropy of a bounded physical system is finite. (If the holographic principle is correct, then physical systems with finite surface area have a finite maximum entropy; but regardless of the truth of the holographic principle, quantum field theory dictates that the entropy of systems with finite radius and energy is finite due to the Bekenstein bound.) To avoid reaching this maximum over the course of an extended computation, entropy must eventually be expelled to an outside environment.\n\nLandauer's principle asserts that there is a minimum possible amount of energy required to erase one bit of information, known as the \"Landauer limit\":\nwhere \"k\" is the Boltzmann constant (approximately 1.38×10 J/K), \"T\" is the temperature of the heat sink in kelvins, and ln 2 is the natural logarithm of 2 (approximately 0.69315).\n\nFor an environment at temperature \"T\", energy \"E\" = \"ST\" must be emitted into that environment if the amount of added entropy is \"S\". For a computational operation in which 1 bit of logical information is lost, the amount of entropy generated is at least \"k\" ln 2, and so, the energy that must eventually be emitted to the environment is \"E\" ≥ \"kT\" ln 2.\n\nThe principle is widely accepted as physical law, but in recent years it has been challenged for using circular reasoning and faulty assumptions, notably in Earman and Norton (1998), and subsequently in Shenker (2000) and Norton (2004, 2011), and defended by Bennett (2003) and Ladyman et al. (2007).\n\n\n"}
{"id": "26127533", "url": "https://en.wikipedia.org/wiki?curid=26127533", "title": "Marginal abatement cost", "text": "Marginal abatement cost\n\nAbatement cost is the cost of reducing environmental negatives such as pollution. Marginal cost is an economic concept that measures the cost of an additional unit. The marginal abatement cost (MAC), in general, measures the cost of reducing one more unit of pollution.\n\nAlthough marginal abatement costs can be negative, such as when the low carbon option is cheaper than the business-as-usual option, MACs often rise steeply as more pollution is reduced.\n\nMarginal abatement costs are typically used on a marginal abatement cost curve (MACC) or MAC curve, which shows the marginal cost of additional reductions in pollution.\n\nCarbon traders use MAC curves to derive the supply function for modelling carbon price fundamentals. Power companies may employ MAC curves to guide their decisions about long-term capital investment strategies to select among a variety of efficiency and generation options. Economists have used MAC curves to explain the economics of interregional carbon trading. Policy-makers use MAC curves as merit order curves, to analyze how much abatement can be done in an economy at what cost, and where policy should be directed to achieve the emission reductions.\n\nHowever, MAC curves should not be used as abatement supply curves (or merit order curves) to decide which measures to implement in order to achieve a given emission-reduction target. Indeed, the options they list would take decades to implement, and it may be optimal to implement expensive but high-potential measures before introducing cheaper measures.\n\nThe way that MAC curves are usually built has been criticized for lack of transparency and the poor treatment it makes of uncertainty, inter-temporal dynamics, interactions between sectors and ancillary benefits. There is also concern regarding the biased ranking that occurs if some included options have negative costs. \n\nVarious economists, research organizations, and consultancies have produced MAC curves. Bloomberg New Energy Finance and McKinsey & Company have produced economy wide analyses on greenhouse gas emissions reductions for the United States. ICF International produced a California specific curve following AB-32 legislation as have Sweeney and Weyant.\n\nThe Wuppertal Institute for Climate, Environment and Energy produced several marginal abatement cost curves for Germany (also called Cost Potential Curves), depending on the perspective (end-user, utilities, society).\n\nThe US Environmental Protection Agency has done work on a MAC curve for non carbon dioxide emissions such as methane, NO, and HFCs. Enerdata and LEPII-CNRS (France) produce MAC curves with the Prospective Outlook on Long-term Energy Systems (POLES) model for the 6 Kyoto Protocol gases. These curves have been used for various public and private actors either to assess carbon policies or through the use of a carbon market analysis tool.\n\nThe World Bank 2013 low-carbon energy development plan for Nigeria, prepared jointly with the World Bank, ulitizes MAC curves created in Analytica.\n\n"}
{"id": "1148564", "url": "https://en.wikipedia.org/wiki?curid=1148564", "title": "Marginal concepts", "text": "Marginal concepts\n\nIn economics, marginal concepts are associated with a \"specific change\" in the quantity used of a good or service, as opposed to some notion of the over-all significance of that class of good or service, or of some total quantity thereof.\n\nConstraints are conceptualized as a \"border\" or \"margin\". The location of the margin for any individual corresponds to his or her \"endowment\", broadly conceived to include opportunities. This endowment is determined by many things including physical laws (which constrain how forms of energy and matter may be transformed), accidents of nature (which determine the presence of natural resources), and the outcomes of past decisions made both by others and by the individual himself or herself.\n\nA value that holds true given particular constraints is a \"marginal\" value. A change that would be affected as or by a specific loosening or tightening of those constraints is a \"marginal\" change, as large as the smallest relevant division of that good or service. For reasons of tractability, it is often assumed in neoclassical analysis that goods and services are continuously divisible. In such context, a marginal change may be an infinitesimal change or a limit. However, strictly speaking, the smallest relevant division may be quite large.\n\nThe marginal use of a good or service is the specific use to which an agent would put a given increase, or the specific use of the good or service that would be abandoned in response to a given decrease.\n\nThe marginal utility of a good or service is the utility of the specific use to which an agent would put a given increase in that good or service, or of the specific use that would be abandoned in response to a given decrease. In other words, marginal utility is the utility of the marginal use.\n\nThe marginal rate of substitution is the rate of substitution is the least favorable rate, at the margin, at which an agent is willing to exchange units of one good or service for units of another.\n\nA marginal benefit is a benefit (howsoever ranked or measured) associated with a marginal change.\n\nThe term “marginal cost” may refer to an opportunity cost at the margin, or to marginal \"pecuniary\" cost — that is to say marginal cost measured by forgone money.\n\nOther marginal concepts include (but are not limited to):\n\nMarginalism is the use of marginal concepts to explain economic phenomena.\n\nThe related concept of elasticity is the ratio of the incremental percentage change in one variable with respect to an incremental percentage change in another variable.\n"}
{"id": "21530142", "url": "https://en.wikipedia.org/wiki?curid=21530142", "title": "Marginal efficiency of capital", "text": "Marginal efficiency of capital\n\nThe marginal efficiency of capital (MEC) is that rate of discount which would equate the price of a fixed capital asset with its present discounted value of expected income.\n\nThe term “marginal efficiency of capital” was introduced by John Maynard Keynes in his \"General Theory\", and defined as “the rate of discount which would make the present value of the series of annuities given by the returns expected from the capital asset during its life just equal its supply price”.\n\nThe MEC is the net rate of return that is expected from the purchase of additional capital. It is calculated as the profit that a firm is expected to earn considering the cost of inputs and the depreciation of capital. \nIt is influenced by expectations about future input costs and demand. \nThe MEC and capital outlays are the elements that a firm takes into account when deciding about an investment project.\n\nThe MEC needs to be higher than the rate of interest, \"r\", for investment to take place. This is because the present value PV of future returns to capital needs to be higher than the cost of capital, C.\nThese variables can be expressed as follows:\nHence, for investment to take place, it is necessary that PV > C; that is, MEC > r.\nAs a consequence, an inverse relationship between the rate of interest and investment is found (i.e.: a higher rate of interest generates less investment).\n\nWith the European Commission according to its data bank \"AMECO\" (Annual Macro-Economic Data) the Marginal efficiency of capital is defined as \"Change in GDP at constant market prices of year T per unit of gross fixed capital formation at constant prices of year T-.5 [that is, lagged by half a year].\n"}
{"id": "19466946", "url": "https://en.wikipedia.org/wiki?curid=19466946", "title": "Marginal profit", "text": "Marginal profit\n\nIn microeconomics, marginal profit is the difference between the marginal revenue and the marginal cost. Under the marginal approach to profit maximization, to maximize profits, a firm should continue to produce a good or service up to the point where marginal profit is zero.\n\nThe most simple formula of Marginal profit is: Marginal revenue - Marginal cost. The derivate of the profit f(x) is in fact MP. In other words: p(x)=R(x)-C(x)."}
{"id": "432318", "url": "https://en.wikipedia.org/wiki?curid=432318", "title": "Marginal rate of substitution", "text": "Marginal rate of substitution\n\nIn economics, the marginal rate of substitution (MRS) is the rate at which a consumer can give up some amount of one good in exchange for another good while maintaining the same level of utility. At equilibrium consumption levels (assuming no externalities), marginal rates of substitution are identical.\n\nUnder the standard assumption of neoclassical economics that goods and services are continuously divisible, the marginal rates of substitution will be the same regardless of the direction of exchange, and will correspond to the slope of an indifference curve (more precisely, to the slope multiplied by −1) passing through the consumption bundle in question, at that point: mathematically, it is the implicit derivative. MRS of X for Y is the amount of Y which a consumer can exchange for one unit of X locally. The MRS is different at each point along the indifference curve thus it is important to keep locus in the definition. Further on this assumption, or otherwise on the assumption that utility is quantified, the marginal rate of substitution of good or service Y for good or service X (MRS) is also equivalent to the marginal utility of X over the marginal utility of Y. Formally,\n\nIt is important to note that when comparing bundles of goods X and Y that give a constant utility (points along an indifference curve), the marginal utility of X is measured in terms of units of Y that is being given up.\n\nFor example, if the MRS = 2, the consumer will give up 2 units of Y to obtain 1 additional unit of X.\n\nAs one moves down a (standardly convex) indifference curve, the marginal rate of substitution decreases (as measured by the absolute value of the slope of the indifference curve, which decreases). This is known as the law of diminishing marginal rate of substitution.\n\nSince the indifference curve is convex with respect to the origin and we have defined the MRS as the negative slope of the indifference curve,\n\nAssume the consumer utility function is defined by formula_4, where \"U\" is consumer utility, \"x\" and \"y\" are goods. Then the marginal rate of substitution can be computed via partial differentiation, as follows.\n\nAlso, note that:\n\nwhere formula_7 is the marginal utility with respect to good \"x\" and formula_8 is the marginal utility with respect to good \"y\".\n\nBy taking the total differential of the utility function equation, we obtain the following results:\n\nThrough any point on the indifference curve, \"dU/dx\" = 0, because \"U\" = \"c\", where \"c\" is a constant. It follows from the above equation that:\n\nThe marginal rate of substitution is defined as the absolute value of the slope of the indifference curve at whichever commodity bundle quantities are of interest. That turns out to equal the ratio of the marginal utilities:\n\nWhen consumers maximize utility with respect to a budget constraint, the indifference curve is tangent to the budget line, therefore, with \"m\" representing slope:\n\nTherefore, when the consumer is choosing his utility maximized market basket on his budget line,\n\nThis important result tells us that utility is maximized when the consumer's budget is allocated so that the marginal utility per unit of money spent is equal for each good. If this equality did not hold, the consumer could increase his/her utility by cutting spending on the good with lower marginal utility per unit of money and increase spending on the other good. To decrease the marginal rate of substitution, the consumer must buy more of the good for which he/she wishes the marginal utility to fall for (due to the law of diminishing marginal utility).\n\n\n"}
{"id": "23014670", "url": "https://en.wikipedia.org/wiki?curid=23014670", "title": "Marginal utility", "text": "Marginal utility\n\nIn economics, utility is the satisfaction or benefit derived by consuming a product; thus the marginal utility of a good or service is the change in the utility from an increase in the consumption of that good or service. \n\nIn the context of cardinal utility, economists sometimes speak of a law of diminishing marginal utility, meaning that the first unit of consumption of a good or service yields more utility than the second and subsequent units, with a continuing reduction for greater amounts. Therefore, the fall in marginal utility as consumption increases is known as diminishing marginal utility. Mathematically:\n\nThe term \"marginal\" refers to a small change, starting from some baseline level. As Philip Wicksteed explained the term,\n\nMarginal considerations are considerations which concern a slight increase or diminution of the stock of anything which we possess or are considering\n\nFrequently the marginal change is assumed to start from the endowment, meaning the total resources available for consumption (see Budget constraint). This endowment is determined by many things including physical laws (which constrain how forms of energy and matter may be transformed), accidents of nature (which determine the presence of natural resources), and the outcomes of past decisions made by the individual himself or herself and by others.\n\nFor reasons of tractability, it is often assumed in neoclassical analysis that goods and services are continuously divisible. Under this assumption, marginal concepts, including marginal utility, may be expressed in terms of differential calculus. Marginal utility can then be defined as the first derivative of total utility—the total satisfaction obtained from consumption of a good or service—with respect to the amount of consumption of that good or service.\n\nIn practice the smallest relevant division may be quite large. Sometimes economic analysis concerns the marginal values associated with a change of one unit of a discrete good or service, such as a motor vehicle or a haircut. For a motor vehicle, the total number of motor vehicles produced is large enough for a continuous assumption to be reasonable: this may not be true for, say, an aircraft carrier.\n\nDepending on which theory of \"utility\" is used, the interpretation of marginal utility can be meaningful or not. Economists have commonly described utility as if it were \"quantifiable\", that is, as if different levels of utility could be compared along a numerical scale. This has affected the development and reception of theories of marginal utility. Quantitative concepts of utility allow familiar arithmetic operations, and further assumptions of continuity and differentiability greatly increase tractability.\n\nContemporary mainstream economic theory frequently defers metaphysical questions, and merely notes or assumes that preference structures conforming to certain rules can be usefully \"proxied\" by associating goods, services, or their uses with quantities, and \"defines\" \"utility\" as such a quantification.\n\nAnother conception is Benthamite philosophy, which equated usefulness with the production of pleasure and avoidance of pain, assumed subject to arithmetic operation. British economists, under the influence of this philosophy (especially by way of John Stuart Mill), viewed utility as \"the feelings of pleasure and pain\" and further as a \"\"quantity\" of feeling\" (emphasis added).\n\nThough generally pursued outside of the mainstream methods, there are conceptions of utility that do not rely on quantification.\nFor example, the Austrian school generally attributes value to \"the satisfaction of wants\", and sometimes rejects even the \"possibility\" of quantification.\nIt has been argued that the Austrian framework makes it possible to consider rational preferences that would otherwise be excluded.\n\nIn any standard framework, the same object may have different marginal utilities for different people, reflecting different preferences or individual circumstances.\n\nThe concept in cardinal utility theory that marginal utilities diminish across the ranges relevant to decision-making is called the \"law of diminishing marginal utility\" (and is also known as Gossen's First Law). This refers to the increase in utility an individual gains from increasing their consumption of a particular good. \"The law of diminishing marginal utility is at the heart of the explanation of numerous economic phenomena, including time preference and the value of goods ... The law says, first, that the marginal utility of each homogenous unit decreases as the supply of units increases (and vice versa); second, that the marginal utility of a larger-sized unit is greater than the marginal utility of a smaller-sized unit (and vice versa). The first law denotes the law of diminishing marginal utility, the second law denotes the law of increasing total utility.\"\n\nIn modern economics, choice under conditions of certainty at a single point in time is modeled via ordinal utility, in which the numbers assigned to the utility of a particular circumstance of the individual have no meaning by themselves, but which of two alternative circumstances has higher utility \"is\" meaningful. With ordinal utility, a person's preferences have no unique marginal utility, and thus whether or not marginal utility is diminishing is not meaningful. In contrast, the concept of diminishing marginal utility is meaningful in the context of cardinal utility, which in modern economics is used in analyzing intertemporal choice, choice under uncertainty, and social welfare.\n\nThe law of diminishing marginal utility is similar to the law of diminishing returns which states that as the amount of one factor of production increases as all other factors of production are held the same, the marginal return (extra output gained by adding an extra unit) decreases.\n\nAs the rate of commodity acquisition increases, \"marginal\" utility decreases. If commodity consumption continues to rise, marginal utility at some point may fall to zero, reaching maximum total utility. Further increase in consumption of units of commodities causes marginal utility to become negative; this signifies dissatisfaction. For example,\n\nDiminishing marginal utility is traditionally a microeconomic concept and often holds for an individual, although the marginal utility of a good or service might be \"increasing\" as well. For example:\n\nAs suggested elsewhere in this article, occasionally one may come across a situation in which marginal utility increases even at a macroeconomic level. For example, the provision of a service may only be viable if it accessible to most or all of the population, and the marginal utility of a raw material required to provide such a service will increase at the \"tipping point\" at which this occurs. This is similar to the position with very large items such as aircraft carriers: the numbers of these items involved are so small that marginal utility is no longer a helpful concept, as there is merely a simple \"yes\" or \"no\" decision.\n\nMarginalism explains choice with the hypothesis that people decide whether to effect any given change based on the marginal utility of that change, with rival alternatives being chosen based upon which has the greatest marginal utility.\n\nIf an individual possesses a good or service whose marginal utility to him is less than that of some other good or service for which he could trade it, then it is in his interest to effect that trade. Of course, as one thing is sold and another is bought, the respective marginal gains or losses from further trades will change. If the marginal utility of one thing is diminishing, and the other is not increasing, all else being equal, an individual will demand an increasing ratio of that which is acquired to that which is sacrificed. (One important way in which all else might not be equal is when the use of the one good or service complements that of the other. In such cases, exchange ratios might be constant.) If any trader can better his position by offering a trade more favorable to complementary traders, then he will do so.\n\nIn an economy with money, the marginal utility of a quantity is simply that of the best good or service that it could purchase. In this way it is useful for explaining supply and demand, as well as essential aspects of models of imperfect competition.\n\nThe \"paradox of water and diamonds\", usually most commonly associated with Adam Smith, though recognized by earlier thinkers, is the apparent contradiction that water possesses a value far lower than diamonds, even though water is far more vital to a human being.\n\nPrice is determined by both marginal utility and marginal cost, and here the key to the \"paradox\" is that the marginal cost of water is far lower than that of diamonds.\n\nThat is not to say that the price of any good or service is simply a function of the marginal utility that it has for any one individual nor for some ostensibly typical individual. Rather, individuals are willing to trade based upon the respective marginal utilities of the goods that they have or desire (with these marginal utilities being distinct for each potential trader), and prices thus develop constrained by these marginal utilities.\n\nUnder the special case in which usefulness can be quantified, the change in utility of moving from state formula_1 to state formula_2 is\nMoreover, if formula_1 and formula_2 are distinguishable by values of just one variable formula_6 which is itself quantified, then it becomes possible to speak of the ratio of the marginal utility of the change in formula_6 to the size of that change:\n\n(where “c.p.” indicates that the \"only\" independent variable to change is formula_6).\n\nMainstream neoclassical economics will typically assume that the limit\nexists, and use “marginal utility” to refer to the partial derivative\nAccordingly, diminishing marginal utility corresponds to the condition\n\nThe concept of marginal utility grew out of attempts by economists to explain the determination of price. The term “marginal utility”, credited to the Austrian economist Friedrich von Wieser by Alfred Marshall, was a translation of Wieser's term “Grenznutzen” (\"border-use\").\n\nPerhaps the essence of a notion of diminishing marginal utility can be found in Aristotle's \"Politics\", wherein he writes (There has been marked disagreement about the development and role of marginal considerations in Aristotle's value theory.)\n\nA great variety of economists have concluded that there is \"some\" sort of interrelationship between utility and rarity that affects economic decisions, and in turn informs the determination of prices. Diamonds are priced higher than water because their marginal utility is higher than water .\n\nEighteenth-century Italian mercantilists, such as Antonio Genovesi, Giammaria Ortes, Pietro Verri, Marchese Cesare di Beccaria, and Count Giovanni Rinaldo Carli, held that value was explained in terms of the general utility and of scarcity, though they did not typically work-out a theory of how these interacted. In \"Della moneta\" (1751), Abbé Ferdinando Galiani, a pupil of Genovesi, attempted to explain value as a ratio of two ratios, \"utility\" and \"scarcity\", with the latter component ratio being the ratio of quantity to use.\n\nAnne Robert Jacques Turgot, in \"Réflexions sur la formation et la distribution de richesse\" (1769), held that value derived from the general utility of the class to which a good belonged, from comparison of present and future wants, and from anticipated difficulties in procurement.\n\nLike the Italian mercantists, Étienne Bonnot, Abbé de Condillac, saw value as determined by utility associated with the class to which the good belong, and by estimated scarcity. In \"De commerce et le gouvernement\" (1776), Condillac emphasized that value is not based upon cost but that costs were paid because of value.\n\nThis last point was famously restated by the Nineteenth Century proto-marginalist, Richard Whately, who in \"Introductory Lectures on Political Economy\" (1832) wrote (Whatley's student Senior is noted below as an early marginalist.)\n\nThe first unambiguous published statement of any sort of theory of marginal utility was by Daniel Bernoulli, in “Specimen theoriae novae de mensura sortis”. This paper appeared in 1738, but a draft had been written in 1731 or in 1732. In 1728, Gabriel Cramer had produced fundamentally the same theory in a private letter. Each had sought to resolve the St. Petersburg paradox, and had concluded that the marginal desirability of money decreased as it was accumulated, more specifically such that the desirability of a sum were the natural logarithm (Bernoulli) or square root (Cramer) thereof. However, the more general implications of this hypothesis were not explicated, and the work fell into obscurity.\n\nIn “A Lecture on the Notion of Value as Distinguished Not Only from Utility, but also from Value in Exchange”, delivered in 1833 and included in \"Lectures on Population, Value, Poor Laws and Rent\" (1837), William Forster Lloyd explicitly offered a general marginal utility theory, but did not offer its derivation nor elaborate its implications. The importance of his statement seems to have been lost on everyone (including Lloyd) until the early 20th century, by which time others had independently developed and popularized the same insight.\n\nIn \"An Outline of the Science of Political Economy\" (1836), Nassau William Senior asserted that marginal utilities were the ultimate determinant of demand, yet apparently did not pursue implications, though some interpret his work as indeed doing just that.\n\nIn “De la mesure de l’utilité des travaux publics” (1844), Jules Dupuit applied a conception of marginal utility to the problem of determining bridge tolls.\n\nIn 1854, Hermann Heinrich Gossen published \"Die Entwicklung der Gesetze des menschlichen Verkehrs und der daraus fließenden Regeln für menschliches Handeln\", which presented a marginal utility theory and to a very large extent worked-out its implications for the behavior of a market economy. However, Gossen's work was not well received in the Germany of his time, most copies were destroyed unsold, and he was virtually forgotten until rediscovered after the so-called Marginal Revolution.\n\nMarginalism eventually found a foothold by way of the work of three economists, Jevons in England, Menger in Austria, and Walras in Switzerland.\n\nWilliam Stanley Jevons first proposed the theory in “A General Mathematical Theory of Political Economy” (PDF), a paper presented in 1862 and published in 1863, followed by a series of works culminating in his book \"The Theory of Political Economy\" in 1871 that established his reputation as a leading political economist and logician of the time. Jevons' conception of utility was in the utilitarian tradition of Jeremy Bentham and of John Stuart Mill, but he differed from his classical predecessors in emphasizing that \"value depends entirely upon utility\", in particular, on \"final utility upon which the theory of Economics will be found to turn.\" He later qualified this in deriving the result that in a model of exchange equilibrium, price ratios would be proportional not only to ratios of \"final degrees of utility,\" but also to costs of production.\n\nCarl Menger presented the theory in \"Grundsätze der Volkswirtschaftslehre\" (translated as \"Principles of Economics\") in 1871. Menger's presentation is peculiarly notable on two points. First, he took special pains to explain \"why\" individuals should be expected to rank possible uses and then to use marginal utility to decide amongst trade-offs. (For this reason, Menger and his followers are sometimes called “the Psychological School”, though they are more frequently known as “the Austrian School” or as “the Vienna School”.) Second, while his illustrative examples present utility as quantified, his essential assumptions do not. (Menger in fact crossed-out the numerical tables in his own copy of the published \"Grundsätze\".) Menger also developed the law of diminishing marginal utility. Menger's work found a significant and appreciative audience.\n\nMarie-Esprit-Léon Walras introduced the theory in \"Éléments d'économie politique pure\", the first part of which was published in 1874 in a relatively mathematical exposition. Walras's work found relatively few readers at the time but was recognized and incorporated two decades later in the work of Pareto and Barone.\n\nAn American, John Bates Clark, is sometimes also mentioned. But, while Clark independently arrived at a marginal utility theory, he did little to advance it until it was clear that the followers of Jevons, Menger, and Walras were revolutionizing economics. Nonetheless, his contributions thereafter were profound.\n\nAlthough the Marginal Revolution flowed from the work of Jevons, Menger, and Walras, their work might have failed to enter the mainstream were it not for a second generation of economists. In England, the second generation were exemplified by Philip Henry Wicksteed, by William Smart, and by Alfred Marshall; in Austria by Eugen von Böhm-Bawerk and by Friedrich von Wieser; in Switzerland by Vilfredo Pareto; and in America by Herbert Joseph Davenport and by Frank A. Fetter.\n\nThere were significant, distinguishing features amongst the approaches of Jevons, Menger, and Walras, but the second generation did not maintain distinctions along national or linguistic lines. The work of von Wieser was heavily influenced by that of Walras. Wicksteed was heavily influenced by Menger. Fetter referred to himself and Davenport as part of “the American Psychological School”, named in imitation of the Austrian “Psychological School”. (And Clark's work from this period onward similarly shows heavy influence by Menger.) William Smart began as a conveyor of Austrian School theory to English-language readers, though he fell increasingly under the influence of Marshall.\n\nBöhm-Bawerk was perhaps the most able expositor of Menger's conception. He was further noted for producing a theory of interest and of profit in equilibrium based upon the interaction of diminishing marginal utility with diminishing marginal productivity of time and with time preference. (This theory was adopted in full and then further developed by Knut Wicksell and, with modifications including formal disregard for time-preference, by Wicksell's American rival Irving Fisher.)\n\nMarshall was the second-generation marginalist whose work on marginal utility came most to inform the mainstream of neoclassical economics, especially by way of his \"Principles of Economics\", the first volume of which was published in 1890. Marshall constructed the demand curve with the aid of assumptions that utility was quantified, and that the marginal utility of money was constant (or nearly so). Like Jevons, Marshall did not see an explanation for supply in the theory of marginal utility, so he synthesized an explanation of demand thus explained with supply explained in a more classical manner, determined by costs which were taken to be objectively determined. (Marshall later actively mischaracterized the criticism that these costs were themselves ultimately determined by marginal utilities.)\n\nKarl Marx acknowledged that \"nothing can have value, without being an object of utility\", but, in his analysis, \"use-value as such lies outside the sphere of investigation of political economy\", with labor being the principal determinant of value under capitalism.\n\nThe doctrines of marginalism and the Marginal Revolution are often interpreted as somehow a response to Marxist economics. However the first volume of \"Das Kapital\" was not published until July 1867, after the works of Jevons, Menger, and Walras were written or well under way (In 1874 Walras published Éléments d'économie politique pure and Carl Menger published Principles of Economics in 1871) ; and Marx was still a relatively minor figure when these works were completed. It is unlikely that any of them knew anything of him. (On the other hand, Hayek or Bartley has suggested that Marx, voraciously reading at the British Museum, may have come across the works of one or more of these figures, and that his inability to formulate a viable critique may account for his failure to complete any further volumes of \"Kapital\" before his death.)\n\nNonetheless, it is not unreasonable to suggest that the generation who followed the preceptors of the Revolution succeeded partly because they could formulate straightforward responses to Marxist economic theory. The most famous of these was that of Böhm-Bawerk, \"Zum Abschluss des Marxschen Systems\" (1896), but the first was Wicksteed's \"The Marxian Theory of Value. \"Das Kapital\": a criticism\" (1884, followed by \"The Jevonian criticism of Marx: a rejoinder\" in 1885). Initially there were only a few Marxist responses to marginalism, of which the most famous were Rudolf Hilferding's \"Böhm-Bawerks Marx-Kritik\" (1904) and \"Politicheskoy ekonomni rante\" (1914) by Никола́й Ива́нович Буха́рин (Nikolai Bukharin). However, over the course of the 20th century a considerable literature developed on the conflict between marginalism and the labour theory of value, with the work of the neo-Ricardian economist Piero Sraffa providing an important critique of marginalism.\n\nIt might also be noted that some followers of Henry George similarly consider marginalism and neoclassical economics a reaction to \"Progress and Poverty\", which was published in 1879.\n\nIn the 1980s John Roemer and other analytical Marxists have worked to rebuild Marxian theses on a marginalist foundation.\n\nIn his 1881 work \"Mathematical Psychics\", Francis Ysidro Edgeworth presented the indifference curve, deriving its properties from marginalist theory which assumed utility to be a differentiable function of quantified goods and services. Later work attempted to generalize to the indifference curve formulations of utility and marginal utility in avoiding unobservable measures of utility.\n\nIn 1915, Eugen Slutsky derived a theory of consumer choice solely from properties of indifference curves. Because of the World War, the Bolshevik Revolution, and his own subsequent loss of interest, Slutsky's work drew almost no notice, but similar work in 1934 by John Richard Hicks and R. G. D. Allen derived much the same results and found a significant audience. (Allen subsequently drew attention to Slutsky's earlier accomplishment.)\n\nAlthough some of the third generation of Austrian School economists had by 1911 rejected the quantification of utility while continuing to think in terms of marginal utility, most economists presumed that utility must be a sort of quantity. Indifference curve analysis seemed to represent a way to dispense with presumptions of quantification, albeit that a seemingly arbitrary assumption (admitted by Hicks to be a \"rabbit out of a hat\") about decreasing marginal rates of substitution would then have to be introduced to have convexity of indifference curves.\n\nFor those who accepted that indifference curve analysis superseded earlier marginal utility analysis, the latter became at best perhaps pedagogically useful, but \"old fashioned\" and observationally unnecessary.\n\nWhen Cramer and Bernoulli introduced the notion of diminishing marginal utility, it had been to address a paradox of gambling, rather than the paradox of value. The marginalists of the revolution, however, had been formally concerned with problems in which there was neither risk nor uncertainty. So too with the indifference curve analysis of Slutsky, Hicks, and Allen.\n\nThe expected utility hypothesis of Bernoulli and others was revived by various 20th century thinkers, with early contributions by Ramsey (1926), von Neumann and Morgenstern (1944), and Savage (1954). Although this hypothesis remains controversial, it brings not only utility, but a quantified conception of utility (cardinal utility), back into the mainstream of economic thought.\n\nA major reason why quantified models of utility are influential today is that risk and uncertainty have been recognized as central topics in contemporary economic theory. Quantified utility models simplify the analysis of risky decisions because, under quantified utility, diminishing marginal utility implies risk aversion. In fact, many contemporary analyses of saving and portfolio choice require stronger assumptions than diminishing marginal utility, such as the assumption of prudence, which means convex marginal utility.\n\nMeanwhile, the Austrian School continued to develop its ordinalist notions of marginal utility analysis, formally demonstrating that from them proceed the decreasing marginal rates of substitution of indifference curves.\n\n"}
{"id": "4675536", "url": "https://en.wikipedia.org/wiki?curid=4675536", "title": "Master–slave morality", "text": "Master–slave morality\n\nMaster–slave morality is a central theme of Friedrich Nietzsche's works, in particular the first essay of \"On the Genealogy of Morality\". Nietzsche argued that there were two fundamental types of morality: \"master morality\" and \"slave morality\". Master morality values pride and power, while slave morality values things like kindness, empathy, and sympathy. Master morality weighs actions on good or bad consequences (i. e., classical virtues and vices, consequentialism), unlike slave morality, which weighs actions on a scale of good or evil intentions (e. g., Christian virtues and vices, Kantian deontology).\n\nFor Nietzsche, a particular morality is inseparable from the formation of a particular culture, meaning that a culture's language, codes and practices, narratives, and institutions are informed by the struggle between these two moral structures (see valuation).\n\nNietzsche defined master morality as the morality of the strong-willed. Nietzsche criticizes the view (which he identifies with contemporary British ideology) that good is everything that is helpful, and bad is everything that is harmful. He argues proponents of this view have forgotten the origins of its values and it is based merely on a non-critical acceptance of habit: what is useful has always been defined as good, therefore usefulness is goodness as a value. He continues explaining that in the prehistoric state \"the value or non-value of an action was derived from its consequences\" but ultimately \"[t]here are no moral phenomena at all, only moral interpretations of phenomena.\" For strong-willed men, the \"good\" is the noble, strong, and powerful, while the \"bad\" is the weak, cowardly, timid, and petty. \n\nThe essence of master morality is \"nobility\". Other qualities that are often valued in master morality are open-mindedness, courage, truthfulness, trust, and an accurate sense of one's self-worth. Master morality begins in the \"noble man\", with a spontaneous idea of the good; then the idea of bad develops as what is not good. \"The noble type of man experiences \"itself\" as determining values; it does not need approval; it judges, \"what is harmful to me is harmful in itself\"; it knows itself to be that which first accords honour to things; it is \"value-creating\".\" In this sense, the master morality is the full recognition that \"oneself\" is the measure of all moral truths. Insofar as something is helpful to the strong-willed man, it is like what he values in himself; therefore, the strong-willed man values such things as good because they aid him in a life-long process of self-actualization through the will to power.\n\nAccording to Nietzsche, masters are creators of morality; slaves respond to master morality with their slave morality. Unlike master morality, which is sentiment, slave morality is based on re-sentiment—devaluing that which the master values and the slave does not have. As master morality originates in the strong, slave morality originates in the weak. Because slave morality is a reaction to oppression, it vilifies its oppressors. Slave morality is the inverse of master morality. As such, it is characterized by pessimism and cynicism. Slave morality is created in opposition to what master morality values as \"good\". \n\nSlave morality does not aim at exerting one's will by strength, but by careful subversion. It does not seek to transcend the masters, but to make them slaves as well. The essence of slave morality is \"utility\": The good is what is most useful for the whole community, not just the strong. Nietzsche saw this as a contradiction. Since the powerful are few in number, compared to the masses of the weak, the weak gain power by corrupting the strong into believing that the causes of slavery (viz., the will to power) are \"evil\", as are the qualities the weak originally could not choose because of their weakness. By saying humility is voluntary, slave morality avoids admitting that their humility was in the beginning forced upon them by a master. Biblical principles of humility, charity, and pity are the result of universalizing the plight of the slave onto all humankind, and thus enslaving the masters as well. \"The \"democratic\" movement is the heir to Christianity\"—the political manifestation of slave morality because of its obsession with freedom and equality.\n\nThis struggle between master and slave moralities recurs historically. According to Nietzsche, ancient Greek and Roman societies were grounded in master morality. The Homeric hero is the strong-willed man, and the classical roots of the \"Iliad\" and \"Odyssey\" exemplified Nietzsche's master morality. He calls the heroes \"men of a noble culture\", giving a substantive example of master morality. Historically, master morality was defeated, as the slave morality of Judaism and Christianity spread throughout the Roman Empire. \n\nThe essential struggle between cultures has always been between the Roman (master, strong) and the Judean (slave, weak). Nietzsche condemns the triumph of slave morality in the West, saying that the democratic movement is the \"collective degeneration of man\". He claimed that the nascent democratic movement of his time was essentially slavish and weak. Weakness conquered strength, slave conquered master, re-sentiment conquered sentiment. This ressentiment Nietzsche calls \"priestly vindictiveness\", which is based on the jealous weak seeking to enslave the strong, and thus erode the basis for power itself by pulling the powerful down. Such movements were, according to Nietzsche, inspired by \"the most intelligent revenge\" of the weak. Nietzsche saw democracy and Christianity as the same emasculating impulse which sought to make everyone equal by making everyone a slave.\n\nNietzsche did not necessarily believe that everyone should adopt master morality as the \"be-all, end-all\" behavior. He thought that the revaluation of morals would correct the inconsistencies in both master and slave moralities. But he asserted that for the individual, master morality was preferable to slave morality. Walter Kaufmann disagrees that Nietzsche actually preferred master morality to slave morality. He certainly gives slave morality a more thorough critique, but this is partly because he thought of slave morality as society's more imminent danger.\n\n\n"}
{"id": "50154587", "url": "https://en.wikipedia.org/wiki?curid=50154587", "title": "Menthor Editor", "text": "Menthor Editor\n\nMenthor Editor is a free ontology engineering tool for dealing with OntoUML models. It also includes OntoUML syntax validation, Alloy simulation, Anti-Pattern verification, and MDA transformations from OntoUML to OWL, SBVR and Natural Language (Brazilian Portuguese).\n\nMenthor Editor emerged from OLED. OLED was developed at the Ontology & Conceptual Modeling Research Group (NEMO) located at the Federal University of Espírito Santo (UFES) in Vitória city, state of Espírito Santo, Brazil\n\nMenthor Editor is being developed by Menthor using Java. Menthor Editor is available in English and it is a multiplaform software, i.e., it is compatible with Windows, Linux and OS X.\n\n"}
{"id": "348044", "url": "https://en.wikipedia.org/wiki?curid=348044", "title": "Meta-system", "text": "Meta-system\n\nMeta-systems have several definitions. In general, they link the concepts \"system\" and \"meta-\". A \"meta-system\" is about other systems, such as describing, generalizing, modelling, or analyzing the other system(s).\n\nAccording to Turchin and Joslyn (1997), this \"natural\" systemic definition is not sufficient for their Theory of Meta-system Transition, it also is not equivalent to the definition of \"system of systems\" in Autopoietic Systems Theory.\n\nIn economics, meta-systems are like what Bataille calls general economies as opposed to the restricted economies of systems.\n\nA book about the difference between general and restricted economies is \"Complementarity\" by Arshad Naim. In this case \"meta\" is defined as what is beyond: the meta-system is what lies beyond the system.\n\nIn mathematics, biology and psychology, many variables have occurred within structures and systems that determined the results, discoveries, rates and value(s) of sets, systems, and developments within systems, structures, systems within structures and sets of structures.\n\nA mathematical-modelling rule system for a domain D is an example of a meta-system in mathematics and science, for similar and consistency of concrete or frequency found in models within a domain. \nThese are all modes or models; where commonalities are more consistent with consecutive scores or values within a ranged order and are good indicators for gauging probabilities, traits (psychology) or properties (biology).\n\nMeta-systems in cultural studies and sociology refer to contexts, milieux, situations, ecosystems, environments and the biological process with the use of commonalities in behavioral traits and human developments found surrounding a social or scientific system which the system must interact with in order to remain viable. Meta-systems have different structures and also are complementary to other structures of such systems. Without this complementarity in the values, bondings, or tact, the systems could not remain productive, viable or operational.\n\nThe term \"meta-system\" (or \"metasystem\") in cybernetics is synonymous with management system or control system. Stafford Beer, who founded management cybernetics with his viable system model, speaks of metasystems that apply metalanguages which are able to find means of making decisions when necessary improvements cannot be made. In computer science this is known as the halting problem. Here metalanguage works in a larger context than the language it describes and has more variety.\n\n"}
{"id": "1092282", "url": "https://en.wikipedia.org/wiki?curid=1092282", "title": "Negative frequency", "text": "Negative frequency\n\nThe concept of negative and positive frequency can be as simple as a wheel rotating one way or the other way: a \"signed value\" of frequency can indicate both the rate and direction of rotation. The rate is expressed in units such as revolutions (a.k.a. \"cycles\") per second (hertz) or radian/second (where 1 cycle corresponds to 2\"π\" radians).\n\nLet \"ω\" be a nonnegative parameter with units of radians/sec. Then the angular function (angle vs. time) , has slope −\"ω\", which is called a negative frequency. But when the function is used as the argument of a cosine operator, the result is indistinguishable from .  Similarly, is indistinguishable from . Thus any sinusoids can be represented in terms of positive frequencies. The sign of the underlying phase slope is ambiguous.\n\nThe ambiguity is resolved when the cosine and sine operators can be observed simultaneously, because leads by 1/4 cycle (= \"π\"/2 radians) when , and lags by 1/4 cycle when .  Similarly, a vector, , rotates counter-clockwise at 1 radian/sec, and completes a circle every 2π seconds, and the vector rotates in the other direction.\n\nThe sign of \"ω\" is also preserved in the complex-valued function:\n\nsince R(\"t\") and I(\"t\") can be separately extracted and compared. Although formula_1  clearly contains more information than either of its components, a common interpretation is that it is a simpler function, because:\nwhich gives rise to the interpretation that cos(\"ωt\") comprises \"both\" positive and negative frequencies.  But the sum is actually a cancellation that contains less, not more, information. Any measure that indicates both frequencies includes a false positive, because \"ω\" can have only one sign.  The Fourier transform, for instance, merely tells us that cos(\"ωt\") correlates equally well with both and with .\n\nPerhaps the most well-known application of negative frequency is the calculation:\n\nwhich is a measure of the amount of frequency ω in the function \"x\"(\"t\") over the interval . When evaluated as a continuous function of \"ω\" for the theoretical interval , it is known as the Fourier transform of \"x\"(\"t\"). A brief explanation is that the product of two complex sinusoids is also a complex sinusoid whose frequency is the sum of the original frequencies. So when \"ω\" is positive, formula_4 causes all the frequencies of \"x\"(\"t\") to be reduced by amount \"ω\". Whatever part of \"x\"(\"t\") that was at frequency \"ω\" is changed to frequency zero, which is just a constant whose amplitude level is a measure of the strength of the original \"ω\" content. And whatever part of \"x\"(\"t\") that was at frequency zero is changed to a sinusoid at frequency −\"ω\". Similarly, all other frequencies are changed to non-zero values. As the interval increases, the contribution of the constant term grows in proportion. But the contributions of the sinusoidal terms only oscillate around zero. So \"X\"(\"ω\") improves as a relative measure of the amount of frequency \"ω\" in the function \"x\"(\"t\").\n\nThe Fourier transform of  formula_1  produces a non-zero response only at frequency \"ω\". The transform of formula_2 has responses at both \"ω\" and −\"ω\", as anticipated by .\n\n"}
{"id": "561582", "url": "https://en.wikipedia.org/wiki?curid=561582", "title": "Negative gearing", "text": "Negative gearing\n\nNegative gearing is a form of financial leverage whereby an investor borrows money to acquire an income-producing investment and the gross income generated by the investment (at least in the short term) is less than the cost of owning and managing the investment, including depreciation and interest charged on the loan (but excluding capital repayments). The investor may enter into a negatively geared investment expecting tax benefits or the capital gain on the investment, when the investment is ultimately disposed of, to exceed the accumulated losses of holding the investment. The investor would take into account the tax treatment of negative gearing, which may generate additional benefits to the investor in the form of tax benefits if the loss on a negatively geared investment is tax-deductible against the investor's other taxable income and if the capital gain on the sale is given a favourable tax treatment. \nNegative gearing is often discussed with regard to real estate, where rental income is less than mortgage interest costs, but may also apply to shares whose dividend income falls short of interest costs on a margin loan. The tax treatment may or may not be the same between the two.\n\nPositive gearing occurs when one borrows to invest in an income-producing asset and the returns (income) from that asset exceed the cost of borrowing. From then on, the investor must pay tax on the rental income profit until the asset is sold, when point the investor must pay capital gains tax on any profit.\n\nWhen the income generated covers the interest, it is simply a geared investment, which creates passive income. A negative gearing strategy makes a profit under any of the following circumstances:\n\n\nThe investor must be able to fund any shortfall until the asset is sold or until the investment becomes positively geared (income > interest). The different tax treatment of planned ongoing losses and possible future capital gains affects the investor's final return. In countries that tax capital gains at a lower rate than income, it is possible for an investor to make a loss overall before taxation but a small gain after taxpayer subsidies.\n\nSome countries, including Australia, Japan, and New Zealand, allow unrestricted use of negative gearing losses to offset income from other sources. Several other OECD countries, including the US, Germany, Sweden, Canada, and France, allow loss offsetting with some restrictions. Applying tax deductions from negatively geared investment housing to other income is not permitted in the UK or the Netherlands. With respect to investment decisions and market prices, other taxes such as stamp duties and capital gains tax may be more or less onerous in those countries, increasing or decreasing the attractiveness of residential property as an investment.\n\nA negatively-geared investment property will generally remain negatively geared for several years, when the rental income will have increased with inflation to the point that the investment is positively geared (the rental income is greater than the interest cost).\n\nThe tax treatment of negative gearing (also termed \"rental loss offset against other income\") varies. For example:\n\nAustralia allows the application of property losses arising from negative gearing against other types of income, such as wage or business income, with only a few limits or restrictions. Negative gearing by property investors reduced personal income tax revenue in Australia by $600 million in the 2001/02 tax year, $3.9 billion in 2004/05 and $13.2 billion in 2010/11.\n\nNegative gearing is a controversial political issue in Australia and was a major issue during the 2016 federal election, during which the Labor Party proposed restricting (but not eliminating) negative gearing and to halve the capital gains tax discount to 25%. Analysis found that negative gearing in Australia provides a greater benefit to wealthier Australians than the less wealthy. Then Federal Treasurer Scott Morrison, in defence of negative gearing, cited tax data that shows that numerous middle income groups (he mentioned teachers, nurses, and electricians) benefit in larger numbers from negative gearing than finance managers. (The raw numbers do not show the extent to which different professional groups benefit.)\n\nTraditionally, Australian taxpayers have been allowed to negatively gear their investment properties, in the strict sense of investing in property at an initial loss. Negative gearing was restricted by a prohibition on the transfer of contingent property income and the property losses could not offset income from labour. It is assumed this applied to losses as well as income, but this is unclear in the Income Tax Assessment Act 1936. \n\nA common method of bypassing the restrictions on property losses offsetting income from labour was to convert such income into another form through the use of partnerships and other legal mechanisms. As a result, this restriction may not have been significant. A partnership, trust or similar legal mechaninsm allowed an individual, or individuals, to pool their incomes and losses, thus allowing them to offset property losses against other incomes. This option was not available to individuals who derived their income from wages. \n\nIn 1983, the Victorian Deputy Commissioner of Taxation briefly denied Victorian property investors the deduction for interest in excess of the rental income, so losses could not be transferred nor moved to a future tax year. That ruling was quickly over-ruled by the federal tax commissioner.\n\nin 1985, under the Australian Labor Party Hawke government, negative gearing rules were changed so that property losses were no longer quarantined and instead could offset wage income. Six months later, following the tax summit in July 1985, the Hawke government undid this change, once more quarantining negative gearing interest expenses on new transactions so that they could be claimed only against rental income, not other income. (Any excess loss could be carried forward to offset property income in later years.) That ensured that at personal level and, more importantly, at a national level, property losses would not be subsidised by income from personal exertion. In applying the formula, all previous governments thereby isolated and consequently discouraged capital speculation being subsidised from the general income tax receipts pool.\n\nIn addition, a capital gains tax (CGT) was introduced in Australia on 20 September 1985. While a separate tax, it is often associated with negative gearing.\n\nThe Hawke government's reversion to the earlier system in which property losses could not offset income from labour was unpopular with property investors. These investors claimed this reversion had caused investment in rental accommodation to dry up and rents to rise substantially. This was unsupported by evidence other than localised increases in real rents in both Perth and Sydney, which also had the lowest vacancy rates of all capital cities at the time. \n\nHowever, in July 1987, after lobbying by the property industry, the Federal Labor government with Paul Keating as its Treasurer, reversed its decision once more, allowing negative gearing losses to be applied against income from labour.\n\nAustralian tax treatment of negative gearing is as follows:\n\n\nThe tax treatment of negative gearing and capital gains may benefit investors in a number of ways, including:\n\nHowever, in certain situations the tax rate applied to the capital gain may be higher than the rate of tax saving because of initial deductions such as for investors who have a low marginal tax rate while they make deductions but a high marginal rate in the year the capital gain is realised.\n\nIn contrast, the tax treatment of real estate by owner-occupiers differs from investment properties. Mortgage interest and upkeep expenses on a private property are not deductible, but any capital gain (or loss) made on disposal of a primary residence is tax-free. (Special rules apply on a change from private use to renting or vice versa and for what is considered a main residence.)\n\nThe economic and social effects of negative gearing in Australia are a matter of ongoing debate. Those in favour of negative gearing argue:\n\n\nOpponents of negative gearing argue:\n\n\nThe view that the temporary removal of negative gearing between 1985 and 1987 caused rents to rise has been challenged by Chief Economist of the ANZ Saul Eslake, who has been quoted as saying: It's true, according to Real Estate Institute data, that rents went up in Sydney and Perth. But the same data doesn't show any discernible increase in the other state capitals. I would say that, if negative gearing had been responsible for a surge in rents, then you should have observed it everywhere, not just two capitals. In fact, if you dig into other parts of the REI database, what you find is that vacancy rates were unusually low at that time before negative gearing was abolished.Eslake is referring to changes in inflation-adjusted rents (i.e., when CPI inflation is subtracted from the nominal rent increases). These are also known as real rent changes. Nominal rents nationally rose by over 25% during the two years that negative gearing was quarantined. They rose strongly in every Australian capital city, according to the official ABS CPI data. However, as nominal changes include inflation, they provide a less clear picture of how rents changed in effect, and of how changes such as disallowing property losses to offset other types of income affect rent.\n\nAdditionally, it is difficult to assess the impact no longer allowing such deductions had during those two years, given that these deductions had only been allowed for six months prior to their disallowance in July 1985.\n\nAn ABC Fact check report, posted on the 3rd of March 2016 and titled \"Fact check: Did abolishing negative gearing push up rents?\" provided the following, after inflation, rental cost changes, during the period negative gearing was abolished. (Rounded to whole numbers and listed by size of city). Sydney +2% to +4%, Melbourne +2% to 0%, Brisbane -3% to -4%, Perth 0% to +6% and Adelaide +1% to -3%. All values have the effects of inflation removed, which in 1986 was 9.18%. \n\nAs a comparison with house pricing, the medium house price in Sydney rose from $98000, in 1985, to $120025, in 1986, or after inflation, by 12.18%. In Brisbane the medium house price moved from $61550 to $63000, in the same period, or an after inflation, a change of -6.25%. The medium unit price in Sydney moved from $70500 to $72300, in the same period. An after inflation change of -6.07%. In brisbane the after inflation change for units was -0.05%.\n\nIn 2003, the Reserve Bank of Australia (RBA) stated in its submission to the Productivity Commission First Home Ownership Inquiry: there are no specific aspects of current tax arrangements designed to encourage investment in property relative to other investments in the Australian tax system. Nor is there any recent tax policy initiative we can point to that accounts for the rapid growth in geared property investment. But the fact is that when we observe the results, resources and finance are being disproportionately channelled into this area, and property promoters use tax effectiveness as an important selling point.They went on to say that \"the most sensible area to look for moderation of demand is among investors\", and that: the taxation treatment in Australia is more favourable to investors than is the case in other countries. In particular, the following areas appear worthy of further study by the Productivity Commission:i. ability to negatively gear an investment property when there is little prospect of the property being cash-flow positive for many years;ii. the benefit that investors receive by virtue of the fact that when property depreciation allowances are \"clawed back\" through the capital gains tax, the rate of tax is lower than the rate that applied when depreciation was allowed in the first place.iii. the general treatment of property depreciation, including the ability to claim depreciation on loss-making investments.\n\nIn 2008, the report of the Senate Select Committee on Housing Affordability in Australia echoed the findings of the 2004 Productivity Commission report. One recommendation to the enquiry suggested that negative gearing should be capped: \"There should not be unlimited access. Millionaires and billionaires should not be able to access it, and you should not be able to access it on your 20th investment property. There should be limits to it.\"\n\nA 2015 report from the Senate Economics References Committee argues that, while negative gearing has an influence on housing affordability, the primary issue is a mismatch between supply and demand. A submission to this committee from the Department of Social Services stated that:[while] demand for housing has increased significantly over the last 30 years, the supply of new dwellings has not responded, with average annual completions of new dwellings remaining around 150,000 since the mid-1980s.The effect of negative gearing on the supply side of dwelling construction is difficult to pin down. Commentary from Eslake and others has highlighted the preponderance of negatively-geared purchases in established suburbs where the probability of a lightly-taxed capital gain exists, challenging the idea that negative gearing leads to substantial amounts of new construction. Many economists have commented extensively on the tax subsidy being made available to speculative buyers in competition against homebuyers, who have no such tax subsidy, leading to significant social dislocation.\n\nAdditionally, the tax subsidy feeding into higher home prices adds to the wealth of those taking advantage of negative gearing. The process that crowds out domestic home owners by pushing up the price of housing also makes the successful user of negative gearing more asset rich due to the increase in land value. This allows these people to borrow further funds against equity in the previously acquired properties, resulting in further acquisitions under tax subsidy. This process can raise prices and thereby make it harder for people who wish to buy a house as an owner-occupier.\n\nWhile allowing for negative gearing in its basic form, the United Kingdom does not allow the transfer of one type of income (or loss) to another type of income. This is due to its schedular system of taxation. In this type of taxation system, the tax paid is dependent on income source. Therefore, an individual who received an income from labour and from land would pay two separate tax rates for the two relevant income sources.\n\nBetween 1997 and 2007, the Tax Law Rewrite Project changed this system by simplifying the schedules. As with the previous system, people would not be allowed to transfer incomes (or losses).\n\nA UK government online resource on renting out property in England and Wales outlines how to offset losses. It states that losses can be offset against \"future profits by carrying it forward to a later year\" or against \"profits from other properties (if you have them)\".\n\nNew Zealand allows negative gearing and the transfer of losses to other income streams, with some restrictions.\n\nThe Rental Income Guide states a loss can only be deducted against other incomes if the rental income is at market rate.\n\nThe Opposition Labour Party attempted to raise negative gearing in the 2011 election, but after their failure to win government the issue reduced in significance.\n\nIn principle, Canada does not allow the transfer of income streams. However, the most current Canadian tax form indicates this can occur in some circumstances. According to \"Line 221 - Carrying charges and interest expenses\", interest payments from an investment designed to generate an income can be deducted:Claim the following carrying charges and interest you paid to earn income from investments: [...] Most interest you pay on money you borrow for investment purposes, but generally only if you use it to try to earn investment income, including interest and dividends. However, if the only earnings your investment can produce are capital gains, you cannot claim the interest you paid.Other sources indicate the deduction must be reasonable and that people should contact the Canada Revenue Agency for more information. The \"Rental Income Includes Form T776\" states people can deduce rental losses from other sources of income: \"You have a rental loss if your rental expenses are more than your gross rental income. If you incur the expenses to earn income, you can deduct your rental loss against your other sources of income.\" However, there is a caveat: the rental loss must be reasonable. What is reasonable is not defined in the \"Rental Income Includes Form T776\" Guide.\n\nBased on these sources, claiming rental losses against other incomes in a given year is allowed as long as a profit is made over the life of the investment, excluding the effects of capital gains.\n\nIt should be noted that Canada has a Federal and Provincial income tax, and the above only relates to Federal income tax.\n\nIn principle, the US federal tax does not allow the transfer of income streams. In general, taxpayers can only deduct expenses of renting property from their rental income, as renting property out is usually considered a passive activity. However, if renters are considered to have actively participated in the activities, they can claim deductions from rental losses against their other \"nonpassive income\". A definition of \"active participation\" is outlined in the \"Reporting Rental Income, Expenses, and Losses\" guide:You actively participated in a rental real estate activity if you (and your spouse) owned at least 10% of the rental property and you made management decisions or arranged for others to provide services (such as repairs) in a significant and \"bona fide\" sense. Management decisions that may count as active participation include approving new tenants, deciding on rental terms, approving expenditures, and other similar decisions.It is possible deduct any loss against other incomes, depending on a range of factors.\n\nJapan allows tax payers to offset rental losses against other income.\n\nIndividuals can claim losses against rental loss with minimal restrictions, but if the property was owned through a partnership or trust there are restrictions.\n\nThere are a number of additional rules, such as restricting claims of losses due to Bad Debt. Additional information can be found in the Japan Tax Site.\n\nThe German tax system is complex, but within the bounds of standard federal income tax, Germany does not allow the transfer of income. Rental losses can only be offset against rental income. As stated on the Global Property Guide site, \"Owners can deduct any expenses from the gross receipts, which were incurred to produce, maintain and safeguard that income.\"\n\nGermany recognizes seven sources of income:\n\n\nThe income from each of these sources is calculated separately.\n\nRental income is taxed as income and is subject to the progressive tax rate. Interest on loans provided to finance real estate, expenses, and property-related cost (e.g., management fees, insurance) can be deducted from the taxable rental income.\n\nIn principle, the Dutch tax system does not allow the transfer of income. Most citizens calculate tax, separately, in 3 income groups:\n\nHowever, I am unable to identify a clear definition where rental income fits in these three categories in the government taxation laws. \n\nDutch resident and non-resident companies and partnerships owning Dutch property are in principle allowed to deduct interest expenses on loans from banks or affiliated companies, and property-related costs from their taxable income.\n\n\n"}
{"id": "2219887", "url": "https://en.wikipedia.org/wiki?curid=2219887", "title": "Negative refraction", "text": "Negative refraction\n\nNegative refraction is the name for an electromagnetic phenomenon where light rays are refracted at an interface in the reverse sense to that normally expected. Such an effect can be obtained using a metamaterial which has been designed to achieve a negative value for both (electric) permittivity ε and (magnetic) permeability μ, as in such cases the material can be assigned a negative refractive index. Such materials are sometimes called \"double negative\" materials.\n\nNegative refraction occurs at interfaces between materials at which one has an ordinary positive phase velocity (i.e. a positive refractive index), and the other has the more exotic negative phase velocity (a negative refractive index).\n\nNegative phase velocity (NPV) is a property of light propagation in a medium. There are different definitions of NPV, the most common being Veselago's original proposal of opposition of wavevector and (Abraham) Poynting vector, i.e. E×H; other common choices are opposition of wavevector to group velocity, or to energy velocity. The use of \"phase velocity\" in the naming convention, as opposed to the perhaps more appropriate \"wave vector\", follows since phase velocity has the same sign as the wavevector.\n\nA typical criterion used to determine Veselago NPV is that the dot product of the Poynting vector and wavevector is negative (i.e. that formula_1); however this definition is not covariant. Whilst this restriction is rarely of practical significance, the criterion has nevertheless been generalized into a covariant form. For plane waves propagating in a Veselago NPV medium, the electric field, magnetic field and wave vector follow a left-hand rule, rather than the usual right-hand rule. This gives rise to the name \"left-handed (meta)materials\". However, the terms left-handed and right-handed can also arise in the study of chiral media, so this terminology is best avoided.\n\nWe can choose to avoid directly considering the Poynting vector and wavevector or a propagating light field, and consider instead the response of the materials directly: that is, we consider what values of permittivity ε and permeability µ result in negative phase velocity (NPV). Since both ε and µ are in general complex, their imaginary parts do not have to be negative for a passive (i.e. lossy) material to display negative refraction. The most general Veselago criterion applying to ε and µ is that of Depine and Lakhtakia, although other less general forms exist. The Depine-Lakhtakia criterion for negative phase velocity is\n\nwhere formula_3 are the real valued parts of ε and µ, respectively. However, negative refraction (negative refractive index) and negative phase velocity can be distinct from each other, even in passive materials, but also in active materials.\n\nTypically, the refractive index \"n\" is determined using formula_4, where by convention the positive square root is chosen for \"n\". However, in NPV materials, we reverse that convention and pick the negative sign to mimic the fact that the wavevector (and hence phase velocity) are likewise reversed. Strictly speaking, the refractive index is a derived quantity telling us how the wavevector is related to the optical frequency and propagation direction of the light, thus the sign of \"n\" must be chosen to match the physical situation.\n\nThe principal symptom of negative refraction is just that – light rays are \nrefracted on the \"same\" side of the normal on entering the material, as indicated in the diagram, and by a suitably general form of Snell's law.\n\n\n"}
{"id": "244067", "url": "https://en.wikipedia.org/wiki?curid=244067", "title": "Negative sign (astrology)", "text": "Negative sign (astrology)\n\nIn astrology, a negative, ceptive, dispassive, yin, nocturnal or feminine sign refers to any of the six even-numbered signs of the zodiac: Taurus, Cancer, Virgo, Scorpio, Capricorn or Pisces.\n\nThese signs constitute the earth and water triplicities.\n\nIn astrology there are two groups: positive and negative. These two groups also\ninclude six individual signs that are called zodiac signs. The negative signs associated\nwith the zodiac are Taurus, Cancer, Virgo, Scorpio, Capricorn, and Pisces. The positive\nsigns associated with the zodiac are Aries, Gemini, Leo, Libra, Sagittarius, and\nAquarius. The twelve signs are divided into two\ngroups based upon one's exact time and place of birth. The negative and positive signs\nalternate, starting with Aries as positive and Taurus as negative and continuing this pattern through the list of zodiac signs.\n\nThe signs negative and positive are referred to as a negative-sunsign or a\npositive-sunsign. There are many terms used in astrology to differentiate the\ntwo groups. In Chinese astrology, the two groups are categorized as yin and yang, corresponding respectively to negative and positive. Standen explains that different astrologers may refer to signs by different names. For example, an astrologer may refer to positive signs as masculine and negative signs as feminine. Each sign is divided into two main\ntypes: active and passive. As a general rule, the active type will be called masculine and the passive type will be called feminine.\n\nZodiac signs associated with the negative are Taurus, Cancer, Virgo, Scorpio, Capricorn, and Pisces. Note that there is no value judgment attached to the terms negative or positive. They may be likened to polarities in a magnet: one side is positive and one side is negative. Neither side is \"good\" nor \"bad\"—they are merely different.\n\nThe sunsign effect is a pattern of alternating high and low extraversion-scores for\nthe 12 signs. Introvert is a person who gains energy when alone and spends it when with other people. Extravert is a person who gains energy from socialization, expending it when alone. Jan J.F. van Rooij did an experiment on Introversion-Extraversion: astrology\nversus psychology, to see if those in the negative sunsign were introverted and those in\nthe positive sunsign were negative. Van Rooijs did this experiment on those with\nastrological knowledge and on those with no astrological knowledge. His results showed\nthat negative sunsign people are not that responsive to the outer world and are\naccordingly not influenced that easily by astrological information. They rely more on\ntheir own ideas and feelings, thus proving his point that people who are born with the\nnegative sunsign are introverted, and those born with the positive sunsign are\nextroverted.\n\nEarth and Water are the elements attached to those who are in the negative sign.\nEarth is the element of Taurus, Virgo, and Capricorn. Water is the element of Cancer,\nScorpio, and Pisces. Elements are the basic traits of the signs. They reveal the\nfundamental aspects of the personality.\n“Water signs are attuned to waves of emotion, and often seem to have a built-in\nsonar for reading a mood. This gives them a special sensitivity in relationships, knowing\nwhen to show warmth and when to hold back. At their best, they are a healing force that\nbring people together -- at their worst, they are psychic vampires, able to manipulate\nand drain the life force of the closest to them”. Hall explains that water signs are\nmore in tune with their emotions and are comfortable showing them. Water signs bring a\ncertain presence to a situation; they seek out the problem and fix it.\n“Earth signs are sensual, meaning they engage with life through the five senses.\nIt takes time to sense the dense physical world, and earth signs can operate at a slower,\nmore thorough pace than the other elements. Theyʼre oriented toward whatʼs real, and\noften this makes them very productive, able to create tangible results.”\nEarth signs are described by Hall as earthy people. These signs focus on the things\nthat connect us to the earth: things which bring peace, as opposed to focusing on the material world.\n\n"}
{"id": "47880066", "url": "https://en.wikipedia.org/wiki?curid=47880066", "title": "Negative utilitarianism", "text": "Negative utilitarianism\n\nNegative utilitarianism is a version of the ethical theory utilitarianism that gives greater priority to reducing suffering (negative utility or 'disutility') than to increasing happiness (positive utility). This differs from classical utilitarianism, which does not claim that reducing suffering is intrinsically more important than increasing happiness. Both versions of utilitarianism hold that morally right and morally wrong actions depend solely on the consequences for overall well-being. 'Well-being' refers to the state of the individual. The term 'negative utilitarianism' is used by some authors to denote the theory that reducing negative well-being is the \"only\" thing that ultimately matters morally. Others distinguish between 'strong' and 'weak' versions of negative utilitarianism, where strong versions are \"only\" concerned with reducing negative well-being, and weak versions say that \"both\" positive and negative well-being matter but that negative well-being matters more.\n\nOther versions of negative utilitarianism differ in how much weight they give to negative well-being ('disutility') compared to positive well-being (positive utility), as well as the different conceptions of what well-being (utility) is. For example, negative preference utilitarianism says that the well-being in an outcome depends on frustrated preferences. Negative hedonistic utilitarianism thinks of well-being in terms of pleasant and unpleasant experiences. There are many other variations on how negative utilitarianism can be specified.\n\nThe term \"negative utilitarianism\" was introduced by R. Ninian Smart in 1958 in his reply to Karl Popper's \"The Open Society and Its Enemies\". Smart also presented the most famous argument against negative utilitarianism: that negative utilitarianism would entail that a ruler who is able to instantly and painlessly destroy the human race would have a duty to do so. Furthermore, every human being would have a moral responsibility to commit suicide, thereby preventing future suffering. Many authors have endorsed versions of this argument, and some have presented counterarguments against it.\n\nThe term ‘negative utilitarianism’ was introduced by R. N. Smart in his 1958 reply to Karl Popper's book \"The Open Society and Its Enemies\", published in 1945. In the book, Popper emphasizes the importance of preventing suffering in public policy. The ideas in negative utilitarianism have similarities with ancient traditions such as Jainism and Buddhism. Ancient Greek philosopher Hegesias of Cyrene has been said to be “one of the earliest exponents of NU [Negative Utilitarianism].” In more recent times, ideas similar to negative utilitarianism can be found in the works of 19th century psychologist Edmund Gurney who wrote:\n\nLike other kinds of utilitarianism, negative utilitarianism can take many forms depending on what specific claims are taken to constitute the theory. For example, negative preference utilitarianism says that the utility of an outcome depends on frustrated and satisfied preferences. Negative hedonistic utilitarianism thinks of utility in terms of hedonic mental states such as suffering and unpleasantness. Versions of (negative) utilitarianism can also differ based on whether the \"actual\" or \"expected\" consequences matter, and whether the aim is stated in terms of the \"average\" outcome among individuals or the \"total\" net utility (or lack of disutility) among them. Negative utilitarianism can aim either to \"optimize\" the value of the outcome or it can be a \"satisficing\" negative utilitarianism, according to which an action ought to be taken if and only if the outcome would be \"sufficiently\" valuable (or have sufficiently low disvalue). A key way in which negative utilitarianisms can differ from one another is with respect to how much weight they give to negative well-being (disutility) compared to positive well-being (positive utility). This is a key area of variation because the key difference between negative utilitarianism and non-negative kinds of utilitarianism is that negative utilitarianism gives more weight to negative well-being.\n\nPhilosophers Gustaf Arrhenius and Krister Bykvist develop a taxonomy of negative utilitarian views based on how the views weigh disutility against positive utility. In total, they distinguish among 16 kinds of negative utilitarianism. They first distinguish between \"strong negativism\" and \"weak negativism\". Strong negativism \"give all weight to disutility\" and weak negativism \"give some weight to positive utility, but more weight to disutility.\" The most commonly discussed subtypes are probably two versions of weak negative utilitarianism called 'lexical' and 'lexical threshold' negative utilitarianism. According to 'lexical' negative utilitarianism, positive utility gets weight only when outcomes are equal with respect to disutility. That is, positive utility functions as a tiebreaker in that it determines which outcome is better (or less bad) when the outcomes considered have equal disutility. 'Lexical threshold' negative utilitarianism says that there is some disutility, for instance some extreme suffering, such that no positive utility can counterbalance it. 'Consent-based' negative utilitarianism is a specification of lexical threshold negative utilitarianism, which specifies where the threshold should be located. It says that if an individual is suffering and would at that moment not \"agree to continue the suffering in order to obtain something else in the future\" then the suffering cannot be outweighed by any happiness.\n\nThomas Metzinger proposes the 'principle of negative utilitarianism,' which is the broad idea that suffering should be minimized when possible. Mario Bunge writes about negative utilitarianism in his \"Treatise on Basic Philosophy\" but in a different sense than most others. In Bunge's sense, negative utilitarianism is about not harming. In contrast, most other discussion of negative utilitarianism takes it to imply a duty both not to harm and to help (at least in the sense of reducing negative well-being).\n\nIn the 1958 article where R. N. Smart introduced the term ‘negative utilitarianism’ he argued against it, stating that negative utilitarianism would entail that a ruler who is able to instantly and painlessly destroy the human race, \"a benevolent world-exploder,\" would have a duty to do so. This is the most famous argument against negative utilitarianism, and it is directed against sufficiently strong versions of negative utilitarianism. Many authors have endorsed this argument, and some have presented counterarguments against it. Below are replies to this argument that have been presented and discussed.\n\nOne possible reply to this argument is that only a naive interpretation of negative utilitarianism would endorse world destruction. The conclusion can be mitigated by pointing out the importance of cooperation between different value systems. There are good consequentialist reasons why one should be cooperative towards other value systems and it is particularly important to avoid doing something harmful to other value systems. The destruction of the world would strongly violate many other value systems and endorsing it would therefore be uncooperative. Since there are many ways to reduce suffering which do not infringe on other value systems, it makes sense for negative utilitarians to focus on these options. In an extended interpretation of negative utilitarianism, cooperation with other value systems is considered and the conclusion is that it is better to reduce suffering without violating other value systems.\n\nAnother reply to the benevolent world-exploder argument is that it does not distinguish between eliminating and reducing negative well-being, and that negative utilitarianism should plausibly be formulated in terms of reducing and not eliminating. A counterargument to that reply is that elimination is a form of reduction, similar to how zero is a number.\n\nSeveral philosophers have argued that to try to destroy the world (or to kill many people) would be counterproductive from a negative utilitarian perspective. One such argument is provided by David Pearce, who says that \"planning and implementing the extinction of all sentient life couldn't be undertaken painlessly. Even contemplating such an enterprise would provoke distress. Thus a negative utilitarian is not compelled to argue for the apocalyptic solution.\" Instead, Pearce advocates the use of biotechnology to phase out the biology of suffering throughout the living world, and he says that \"life-long happiness can be genetically pre-programmed.\" A similar reply to the similar claim that negative utilitarianism would imply that we should kill off the miserable and needy is that we rarely face policy choices and that \"anyway there are excellent utilitarian reasons for avoiding such a policy, since people would find out about it and become even more miserable and fearful.\" The Negative Utilitarianism FAQ's answer to question \"3.2 Should NUs try to increase extinction risk?\" begins with \"No, that would very bad even by NU standards.\"\n\nSome replies to the benevolent world exploder-argument take the form that even if the world were destroyed, that would or might be bad from a negative utilitarian perspective. One such reply provided by John W. N. Watkins is that even if life were destroyed, life could evolve again, perhaps in a worse way. So the world-exploder would need to destroy the possibility of life, but that is in principle beyond human power. To this, J. J. C. Smart replies,\n\nAnother related reply to the world-exploder argument is that getting killed would be a great evil. Erich Kadlec defends negative utilitarianism and replies to the benevolent world-exploder argument (in part) as follows: \"He [R. N. Smart] also dispenses with the generally known fact that all people (with a few exceptions in extreme situations) like to live and would consider being killed not a benefit but as the greatest evil done to them.\"\n\nNegative preference utilitarianism has a preferentialist conception of well-being. That is, it is bad for an individual to get his aversions fulfilled (or preferences frustrated), and depending on version of negative utilitarianism, it may also be good for him to get his preferences satisfied. A negative utilitarian with such a conception of well-being, or whose conception of well-being includes such a preferentialist component, could reply to the benevolent world-exploder argument by saying that the explosion would be bad because it would fulfill many individuals' aversions. Arrhenius and Bykvist provide two criticisms of this reply. First, it could be claimed that frustrated preferences require that someone exists who has the frustrated preference. But if everyone is dead there are no preferences and hence no badness. Second, even if a world-explosion would involve frustrated preferences that would be bad from a negative preference utilitarian perspective, such a negative utilitarian should still favor it as the lesser of two evils compared to all the frustrated preferences that would likely exist if the world continued to exist.\n\nThe Negative Utilitarianism FAQ suggests two replies to Arrhenius and Bykvist's first type of criticism (the criticism that if no one exists anymore then there are no frustrated preferences anymore): The first reply is that past preferences count, even if the individual who held them no longer exists. The second is that \"instead of counting past preferences, one could look at the matter in terms of life-goals. The earlier the death of a person who wants to go on living, the more unfulfilled her life-goal.\" The Negative Utilitarianism FAQ also replies to Arrhenius and Bykvist's second type of criticism. The reply is (in part) that the criticism relies on the empirical premise that there would be more frustrated preferences in the future if the world continued to exist than if the world was destroyed. But that negative preference utilitarianism would say that extinction would be better (in theory), assuming that premise, should not count substantially against the theory, because for any view on population ethics that assigns disvalue to something, one can imagine future scenarios such that extinction would be better according to the given view.\n\nA part of Clark Wolf's response to the benevolent world-exploder objection is that negative utilitarianism can be combined with a theory of rights. He says,\n\nNegative utilitarianism can be combined, in particular, with Rawls' theory of justice. Rawls knew Popper’s normative claims and may have been influenced by his concern for the worst-off.\n\nToby Ord provides a critique of negative utilitarianism in his essay \"Why I'm Not a Negative Utilitarian,\" to which David Pearce and Bruno Contestabile have replied. Other critical views of negative utilitarianism are provided by Thaddeus Metz, Christopher Belshaw, and Ingmar Persson. On the other hand, Joseph Mendola develops a modification of utilitarianism, and he says that his principle\n\n\n"}
{"id": "36797", "url": "https://en.wikipedia.org/wiki?curid=36797", "title": "Occam's razor", "text": "Occam's razor\n\nOccam's razor (also Ockham's razor or Ocham's razor; Latin: \"lex parsimoniae\" \"law of parsimony\") is the problem-solving principle that the simplest solution tends to be the correct one. When presented with competing hypotheses to solve a problem, one should select the solution with the fewest assumptions. The idea is attributed to English Franciscan friar William of Ockham (c. 1287–1347), a scholastic philosopher and theologian.\n\nIn science, Occam's razor is used as an abductive heuristic in the development of theoretical models, rather than as a rigorous arbiter between candidate models. In the scientific method, Occam's razor is not considered an irrefutable principle of logic or a scientific result; the preference for simplicity in the scientific method is based on the falsifiability criterion. For each accepted explanation of a phenomenon, there may be an extremely large, perhaps even incomprehensible, number of possible and more complex alternatives. Since one can always burden failing explanations with \"ad hoc\" hypotheses to prevent them from being falsified, simpler theories are preferable to more complex ones because they are more testable.\n\nThe term \"Occam's razor\" did not appear until a few centuries after William of Ockham's death in 1347. Libert Froidmont, in his \"On Christian Philosophy of the Soul\", takes credit for the phrase, speaking of \"novacula occami\". Ockham did not invent this principle, but the \"razor\"—and its association with him—may be due to the frequency and effectiveness with which he used it. Ockham stated the principle in various ways, but the most popular version, \"Entities are not to be multiplied without necessity\" () was formulated by the Irish Franciscan philosopher John Punch in his 1639 commentary on the works of Duns Scotus.\n\nThe origins of what has come to be known as Occam's razor are traceable to the works of earlier philosophers such as John Duns Scotus (1265–1308), Robert Grosseteste (1175–1253), Maimonides (Moses ben-Maimon, 1138–1204), and even Aristotle (384–322 BC). Aristotle writes in his \"Posterior Analytics\", \"We may assume the superiority [other things being equal] of the demonstration which derives from fewer postulates or hypotheses.\" Ptolemy () stated, \"We consider it a good principle to explain the phenomena by the simplest hypothesis possible.\"\n\nPhrases such as \"It is vain to do with more what can be done with fewer\" and \"A plurality is not to be posited without necessity\" were commonplace in 13th-century scholastic writing. Robert Grosseteste, in \"Commentary on\" [Aristotle's] \"the Posterior Analytics Books\" (\"Commentarius in Posteriorum Analyticorum Libros\") (c. 1217–1220), declares: \"That is better and more valuable which requires fewer, other circumstances being equal... For if one thing were demonstrated from many and another thing from fewer equally known premises, clearly that is better which is from fewer because it makes us know quickly, just as a universal demonstration is better than particular because it produces knowledge from fewer premises. Similarly in natural science, in moral science, and in metaphysics the best is that which needs no premises and the better that which needs the fewer, other circumstances being equal.\"\n\nThe \"Summa Theologica\" of Thomas Aquinas (1225–1274) states that \"it is superfluous to suppose that what can be accounted for by a few principles has been produced by many.\" Aquinas uses this principle to construct an objection to God's existence, an objection that he in turn answers and refutes generally (cf. \"quinque viae\"), and specifically, through an argument based on causality. Hence, Aquinas acknowledges the principle that today is known as Occam's razor, but prefers causal explanations to other simple explanations (cf. also Correlation does not imply causation).\n\nWilliam of Ockham (\"circa\" 1287–1347) was an English Franciscan friar and theologian, an influential medieval philosopher and a nominalist. His popular fame as a great logician rests chiefly on the maxim attributed to him and known as Occam's razor. The term \"razor\" refers to distinguishing between two hypotheses either by \"shaving away\" unnecessary assumptions or cutting apart two similar conclusions.\n\nWhile it has been claimed that Occam's razor is not found in any of William's writings, one can cite statements such as (\"Plurality must never be posited without necessity\"), which occurs in his theological work on the \"Sentences of Peter Lombard\" (\"Quaestiones et decisiones in quattuor libros Sententiarum Petri Lombardi\"; ed. Lugd., 1495, i, dist. 27, qu. 2, K).\n\nNevertheless, the precise words sometimes attributed to William of Ockham, (Entities must not be multiplied beyond necessity), are absent in his extant works; this particular phrasing comes from John Punch, who described the principle as a \"common axiom\" (\"axioma vulgare\") of the Scholastics. William of Ockham's contribution seems to restrict the operation of this principle in matters pertaining to miracles and God's power; so, in the Eucharist, a plurality of miracles is possible, simply because it pleases God.\n\nThis principle is sometimes phrased as (\"Plurality should not be posited without necessity\"). In his \"Summa Totius Logicae\", i. 12, William of Ockham cites the principle of economy, (\"It is futile to do with more things that which can be done with fewer\"; Thorburn, 1918, pp. 352–53; Kneale and Kneale, 1962, p. 243.)\n\nTo quote Isaac Newton, \"We are to admit no more causes of natural things than such as are both true and sufficient to explain their appearances. Therefore, to the same natural effects we must, as far as possible, assign the same causes.\"\n\nBertrand Russell offers a particular version of Occam's razor: \"Whenever possible, substitute constructions out of known entities for inferences to unknown entities.\"\n\nAround 1960, Ray Solomonoff founded the theory of universal inductive inference, the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable probability distribution. This theory is a mathematical formalization of Occam's razor.\n\nAnother technical approach to Occam's razor is ontological parsimony. Parsimony means spareness and is also referred to as the Rule of Simplicity. This is considered a strong version of Occam's razor. A variation used in medicine is called the \"Zebra\": a doctor should reject an exotic medical diagnosis when a more commonplace explanation is more likely, derived from Theodore Woodward's dictum \"When you hear hoofbeats, think of horses not zebras\".\n\nErnst Mach formulated the stronger version of Occam's razor into physics, which he called the Principle of Economy stating: \"Scientists must use the simplest means of arriving at their results and exclude everything not perceived by the senses.\"\n\nThis principle goes back at least as far as Aristotle, who wrote \"Nature operates in the shortest way possible.\" The idea of parsimony or simplicity in deciding between theories, though not the intent of the original expression of Occam's razor, has been assimilated into our culture as the widespread layman's formulation that \"the simplest explanation is usually the correct one.\"\n\nPrior to the 20th century, it was a commonly held belief that nature itself was simple and that simpler hypotheses about nature were thus more likely to be true. This notion was deeply rooted in the aesthetic value that simplicity holds for human thought and the justifications presented for it often drew from theology. Thomas Aquinas made this argument in the 13th century, writing, \"If a thing can be done adequately by means of one, it is superfluous to do it by means of several; for we observe that nature does not employ two instruments [if] one suffices.\"\n\nBeginning in the 20th century, epistemological justifications based on induction, logic, pragmatism, and especially probability theory have become more popular among philosophers.\n\nOccam's razor has gained strong empirical support in helping to converge on better theories (see \"Applications\" section below for some examples).\n\nIn the related concept of overfitting, excessively complex models are affected by statistical noise (a problem also known as the bias-variance trade-off), whereas simpler models may capture the underlying structure better and may thus have better predictive performance. It is, however, often difficult to deduce which part of the data is noise (cf. model selection, test set, minimum description length, Bayesian inference, etc.).\n\nThe razor's statement that \"other things being equal, simpler explanations are generally better than more complex ones\" is amenable to empirical testing. Another interpretation of the razor's statement would be that \"simpler hypotheses are generally better than the complex ones\". The procedure to test the former interpretation would compare the track records of simple and comparatively complex explanations. If one accepts the first interpretation, the validity of Occam's razor as a tool would then have to be rejected if the more complex explanations were more often correct than the less complex ones (while the converse would lend support to its use). If the latter interpretation is accepted, the validity of Occam's razor as a tool could possibly be accepted if the simpler hypotheses led to correct conclusions more often than not.\n\nSome increases in complexity are sometimes necessary. So there remains a justified general bias toward the simpler of two competing explanations. To understand why, consider that for each accepted explanation of a phenomenon, there is always an infinite number of possible, more complex, and ultimately incorrect, alternatives. This is so because one can always burden a failing explanation with an ad hoc hypothesis. Ad hoc hypotheses are justifications that prevent theories from being falsified. Even other empirical criteria, such as consilience, can never truly eliminate such explanations as competition. Each true explanation, then, may have had many alternatives that were simpler and false, but also an infinite number of alternatives that were more complex and false. But if an alternative ad hoc hypothesis were indeed justifiable, its implicit conclusions would be empirically verifiable. On a commonly accepted repeatability principle, these alternative theories have never been observed and continue to escape observation. In addition, one does not say an explanation is true if it has not withstood this principle.\n\nPut another way, any new, and even more complex, theory can still possibly be true. For example, if an individual makes supernatural claims that leprechauns were responsible for breaking a vase, the simpler explanation would be that he is mistaken, but ongoing ad hoc justifications (e.g. \"... and that's not me on the film; they tampered with that, too\") successfully prevent outright falsification. This endless supply of elaborate competing explanations, called saving hypotheses, cannot be ruled out—except by using Occam's razor. A study of the predictive validity of Occam's razor found 32 published papers that included 97 comparisons of economic forecasts from simple and complex forecasting methods. None of the papers provided a balance of evidence that complexity of method improved forecast accuracy. In the 25 papers with quantitative comparisons, complexity increased forecast errors by an average of 27 percent.\n\nOne justification of Occam's razor is a direct result of basic probability theory. By definition, all assumptions introduce possibilities for error; if an assumption does not improve the accuracy of a theory, its only effect is to increase the probability that the overall theory is wrong.\n\nThere have also been other attempts to derive Occam's razor from probability theory, including notable attempts made by Harold Jeffreys and E. T. Jaynes. The probabilistic (Bayesian) basis for Occam's razor is elaborated by David J. C. MacKay in chapter 28 of his book \"Information Theory, Inference, and Learning Algorithms\", where he emphasizes that a prior bias in favour of simpler models is not required.\n\nWilliam H. Jefferys and James O. Berger (1991) generalize and quantify the original formulation's \"assumptions\" concept as the degree to which a proposition is unnecessarily accommodating to possible observable data. They state, \"A hypothesis with fewer adjustable parameters will automatically have an enhanced posterior probability, due to the fact that the predictions it makes are sharp.\" The model they propose balances the precision of a theory's predictions against their sharpness—preferring theories that sharply make correct predictions over theories that accommodate a wide range of other possible results. This, again, reflects the mathematical relationship between key concepts in Bayesian inference (namely marginal probability, conditional probability, and posterior probability).\n\nThe bias–variance tradeoff is a framework that incorporates the Occam's razor principal in its balance between overfitting (i.e. variance minimization) and underfitting (i.e. bias minimization).\n\nKarl Popper argues that a preference for simple theories need not appeal to practical or aesthetic considerations. Our preference for simplicity may be justified by its falsifiability criterion: we prefer simpler theories to more complex ones \"because their empirical content is greater; and because they are better testable\" (Popper 1992). The idea here is that a simple theory applies to more cases than a more complex one, and is thus more easily falsifiable. This is again comparing a simple theory to a more complex theory where both explain the data equally well.\n\nThe philosopher of science Elliott Sober once argued along the same lines as Popper, tying simplicity with \"informativeness\": The simplest theory is the more informative, in the sense that it requires less information to a question. He has since rejected this account of simplicity, purportedly because it fails to provide an epistemic justification for simplicity. He now believes that simplicity considerations (and considerations of parsimony in particular) do not count unless they reflect something more fundamental. Philosophers, he suggests, may have made the error of hypostatizing simplicity (i.e., endowed it with a \"sui generis\" existence), when it has meaning only when embedded in a specific context (Sober 1992). If we fail to justify simplicity considerations on the basis of the context in which we use them, we may have no non-circular justification: \"Just as the question 'why be rational?' may have no non-circular answer, the same may be true of the question 'why should simplicity be considered in evaluating the plausibility of hypotheses?'\"\n\nRichard Swinburne argues for simplicity on logical grounds:\n\nAccording to Swinburne, since our choice of theory cannot be determined by data (see Underdetermination and Duhem-Quine thesis), we must rely on some criterion to determine which theory to use. Since it is absurd to have no logical method for settling on one hypothesis amongst an infinite number of equally data-compliant hypotheses, we should choose the simplest theory: \"Either science is irrational [in the way it judges theories and predictions probable] or the principle of simplicity is a fundamental synthetic a priori truth.\" (Swinburne 1997).\n\nFrom the \"Tractatus Logico-Philosophicus\":\n\n\nand on the related concept of \"simplicity\":\n\n\nIn science, Occam's razor is used as a heuristic to guide scientists in developing theoretical models rather than as an arbiter between published models. In physics, parsimony was an important heuristic in Albert Einstein's formulation of special relativity, in the development and application of the principle of least action by Pierre Louis Maupertuis and Leonhard Euler, and in the development of quantum mechanics by Max Planck, Werner Heisenberg and Louis de Broglie.\n\nIn chemistry, Occam's razor is often an important heuristic when developing a model of a reaction mechanism. Although it is useful as a heuristic in developing models of reaction mechanisms, it has been shown to fail as a criterion for selecting among some selected published models. In this context, Einstein himself expressed caution when he formulated Einstein's Constraint: \"It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience\". An often-quoted version of this constraint (which cannot be verified as posited by Einstein himself) says \"Everything should be kept as simple as possible, but not simpler.\"\n\nIn the scientific method, parsimony is an epistemological, metaphysical or heuristic preference, not an irrefutable principle of logic or a scientific result. As a logical principle, Occam's razor would demand that scientists accept the simplest possible theoretical explanation for existing data. However, science has shown repeatedly that future data often support more complex theories than do existing data. Science prefers the simplest explanation that is consistent with the data available at a given time, but the simplest explanation may be ruled out as new data become available. That is, science is open to the possibility that future experiments might support more complex theories than demanded by current data and is more interested in designing experiments to discriminate between competing theories than favoring one theory over another based merely on philosophical principles.\n\nWhen scientists use the idea of parsimony, it has meaning only in a very specific context of inquiry. Several background assumptions are required for parsimony to connect with plausibility in a particular research problem. The reasonableness of parsimony in one research context may have nothing to do with its reasonableness in another. It is a mistake to think that there is a single global principle that spans diverse subject matter.\n\nIt has been suggested that Occam's razor is a widely accepted example of extraevidential consideration, even though it is entirely a metaphysical assumption. There is little empirical evidence that the world is actually simple or that simple accounts are more likely to be true than complex ones.\n\nMost of the time, Occam's razor is a conservative tool, cutting out \"crazy, complicated constructions\" and assuring \"that hypotheses are grounded in the science of the day\", thus yielding \"normal\" science: models of explanation and prediction. There are, however, notable exceptions where Occam's razor turns a conservative scientist into a reluctant revolutionary. For example, Max Planck interpolated between the Wien and Jeans radiation laws and used Occam's razor logic to formulate the quantum hypothesis, even resisting that hypothesis as it became more obvious that it was correct.\n\nAppeals to simplicity were used to argue against the phenomena of meteorites, ball lightning, continental drift, and reverse transcriptase. One can argue for atomic building blocks for matter, because it provides a simpler explanation for the observed reversibility of both mixing and chemical reactions as simple separation and rearrangements of atomic building blocks. At the time, however, the atomic theory was considered more complex because it implied the existence of invisible particles that had not been directly detected. Ernst Mach and the logical positivists rejected John Dalton's atomic theory until the reality of atoms was more evident in Brownian motion, as shown by Albert Einstein.\n\nIn the same way, postulating the aether is more complex than transmission of light through a vacuum. At the time, however, all known waves propagated through a physical medium, and it seemed simpler to postulate the existence of a medium than to theorize about wave propagation without a medium. Likewise, Newton's idea of light particles seemed simpler than Christiaan Huygens's idea of waves, so many favored it. In this case, as it turned out, neither the wave—nor the particle—explanation alone suffices, as light behaves like waves and like particles.\n\nThree axioms presupposed by the scientific method are realism (the existence of objective reality), the existence of natural laws, and the constancy of natural law. Rather than depend on provability of these axioms, science depends on the fact that they have not been objectively falsified. Occam's razor and parsimony support, but do not prove, these axioms of science. The general principle of science is that theories (or models) of natural law must be consistent with repeatable experimental observations. This ultimate arbiter (selection criterion) rests upon the axioms mentioned above.\n\nThere are examples where Occam's razor would have favored the wrong theory given the available data. Simplicity principles are useful philosophical preferences for choosing a more likely theory from among several possibilities that are all consistent with available data. A single instance of Occam's razor favoring a wrong theory falsifies the razor as a general principle. Michael Lee and others provide cases in which a parsimonious approach does not guarantee a correct conclusion and, if based on incorrect working hypotheses or interpretations of incomplete data, may even strongly support a false conclusion.\n\nIf multiple models of natural law make exactly the same testable predictions, they are equivalent and there is no need for parsimony to choose a preferred one. For example, Newtonian, Hamiltonian and Lagrangian classical mechanics are equivalent. Physicists have no interest in using Occam's razor to say the other two are wrong. Likewise, there is no demand for simplicity principles to arbitrate between wave and matrix formulations of quantum mechanics. Science often does not demand arbitration or selection criteria between models that make the same testable predictions.\n\nBiologists or philosophers of biology use Occam's razor in either of two contexts both in evolutionary biology: the units of selection controversy and systematics. George C. Williams in his book \"Adaptation and Natural Selection\" (1966) argues that the best way to explain altruism among animals is based on low-level (i.e., individual) selection as opposed to high-level group selection. Altruism is defined by some evolutionary biologists (e.g., R. Alexander, 1987; W. D. Hamilton, 1964) as behavior that is beneficial to others (or to the group) at a cost to the individual, and many posit individual selection as the mechanism that explains altruism solely in terms of the behaviors of individual organisms acting in their own self-interest (or in the interest of their genes, via kin selection). Williams was arguing against the perspective of others who propose selection at the level of the group as an evolutionary mechanism that selects for altruistic traits (e.g., D. S. Wilson & E. O. Wilson, 2007). The basis for Williams' contention is that of the two, individual selection is the more parsimonious theory. In doing so he is invoking a variant of Occam's razor known as Morgan's Canon: \"In no case is an animal activity to be interpreted in terms of higher psychological processes, if it can be fairly interpreted in terms of processes which stand lower in the scale of psychological evolution and development.\" (Morgan 1903).\n\nHowever, more recent biological analyses, such as Richard Dawkins' \"The Selfish Gene\", have contended that Morgan's Canon is not the simplest and most basic explanation. Dawkins argues the way evolution works is that the genes propagated in most copies end up determining the development of that particular species, i.e., natural selection turns out to select specific genes, and this is really the fundamental underlying principle that automatically gives individual and group selection as emergent features of evolution.\n\nZoology provides an example. Muskoxen, when threatened by wolves, form a circle with the males on the outside and the females and young on the inside. This is an example of a behavior by the males that seems to be altruistic. The behavior is disadvantageous to them individually but beneficial to the group as a whole and was thus seen by some to support the group selection theory. Another interpretation is kin selection: if the males are protecting their offspring, they are protecting copies of their own alleles. Engaging in this behavior would be favored by individual selection if the cost to the male musk ox is less than half of the benefit received by his calf – which could easily be the case if wolves have an easier time killing calves than adult males. It could also be the case that male musk oxen would be individually less likely to be killed by wolves if they stood in a circle with their horns pointing out, regardless of whether they were protecting the females and offspring. That would be an example of regular natural selection – a phenomenon called \"the selfish herd\".\n\nSystematics is the branch of biology that attempts to establish patterns of genealogical relationship among biological taxa. It is also concerned with their classification. There are three primary camps in systematics: cladists, pheneticists, and evolutionary taxonomists. The cladists hold that genealogy alone should determine classification, pheneticists contend that overall similarity is the determining criterion, while evolutionary taxonomists say that both genealogy and similarity count in classification.\n\nIt is among the cladists that Occam's razor is to be found, although their term for it is \"cladistic parsimony\". Cladistic parsimony (or maximum parsimony) is a method of phylogenetic inference in the construction of types of phylogenetic trees (more specifically, cladograms). Cladograms are branching, tree-like structures used to represent hypotheses of relative degree of relationship, based on shared, derived character states. Cladistic parsimony is used to select as the preferred hypothesis of relationships the cladogram that requires the fewest implied character state transformations. Critics of the cladistic approach often observe that for some types of tree, parsimony consistently produces the wrong results, regardless of how much data is collected (this is called statistical inconsistency, or long branch attraction). However, this criticism is also potentially true for any type of phylogenetic inference, unless the model used to estimate the tree reflects the way that evolution actually happened. Because this information is not empirically accessible, the criticism of statistical inconsistency against parsimony holds no force. For a book-length treatment of cladistic parsimony, see Elliott Sober's \"Reconstructing the Past: Parsimony, Evolution, and Inference\" (1988). For a discussion of both uses of Occam's razor in biology, see Sober's article \"Let's Razor Ockham's Razor\" (1990).\n\nOther methods for inferring evolutionary relationships use parsimony in a more traditional way. Likelihood methods for phylogeny use parsimony as they do for all likelihood tests, with hypotheses requiring few differing parameters (i.e., numbers of different rates of character change or different frequencies of character state transitions) being treated as null hypotheses relative to hypotheses requiring many differing parameters. Thus, complex hypotheses must predict data much better than do simple hypotheses before researchers reject the simple hypotheses. Recent advances employ information theory, a close cousin of likelihood, which uses Occam's razor in the same way.\n\nFrancis Crick has commented on potential limitations of Occam's razor in biology. He advances the argument that because biological systems are the products of (an ongoing) natural selection, the mechanisms are not necessarily optimal in an obvious sense. He cautions: \"While Ockham's razor is a useful tool in the physical sciences, it can be a very dangerous implement in biology. It is thus very rash to use simplicity and elegance as a guide in biological research.\"\n\nIn biogeography, parsimony is used to infer ancient migrations of species or populations by observing the geographic distribution and relationships of existing organisms. Given the phylogenetic tree, ancestral migrations are inferred to be those that require the minimum amount of total movement.\n\nIn the philosophy of religion, Occam's razor is sometimes applied to the existence of God. William of Ockham himself was a Christian. He believed in God, and in the authority of Scripture; he writes that \"nothing ought to be posited without a reason given, unless it is self-evident (literally, known through itself) or known by experience or proved by the authority of Sacred Scripture.\" Ockham believed that an explanation has no sufficient basis in reality when it does not harmonize with reason, experience, or the Bible. However, unlike many theologians of his time, Ockham did not believe God could be logically proven with arguments. To Ockham, science was a matter of discovery, but theology was a matter of revelation and faith. He states: \"only faith gives us access to theological truths. The ways of God are not open to reason, for God has freely chosen to create a world and establish a way of salvation within it apart from any necessary laws that human logic or rationality can uncover.\"\n\nSt. Thomas Aquinas, in the \"Summa Theologica\", uses a formulation of Occam's razor to construct an objection to the idea that God exists, which he refutes directly with a counterargument:\n\nFurther, it is superfluous to suppose that what can be accounted for by a few principles has been produced by many. But it seems that everything we see in the world can be accounted for by other principles, supposing God did not exist. For all natural things can be reduced to one principle which is nature; and all voluntary things can be reduced to one principle which is human reason, or will. Therefore there is no need to suppose God's existence.\n\nIn turn, Aquinas answers this with the \"quinque viae\", and addresses the particular objection above with the following answer:\n\nSince nature works for a determinate end under the direction of a higher agent, whatever is done by nature must needs be traced back to God, as to its first cause. So also whatever is done voluntarily must also be traced back to some higher cause other than human reason or will, since these can change or fail; for all things that are changeable and capable of defect must be traced back to an immovable and self-necessary first principle, as was shown in the body of the Article.\n\nRather than argue for the necessity of a god, some theists base their belief upon grounds independent of, or prior to, reason, making Occam's razor irrelevant. This was the stance of Søren Kierkegaard, who viewed belief in God as a leap of faith that sometimes directly opposed reason. This is also the doctrine of Gordon Clark's presuppositional apologetics, with the exception that Clark never thought the leap of faith was contrary to reason (see also Fideism).\n\nVarious arguments in favor of God establish God as a useful or even necessary assumption. Contrastingly some anti-theists hold firmly to the belief that assuming the existence of God introduces unnecessary complexity (Schmitt 2005, e.g., the Ultimate Boeing 747 gambit).\n\nAnother application of the principle is to be found in the work of George Berkeley (1685–1753). Berkeley was an idealist who believed that all of reality could be explained in terms of the mind alone. He invoked Occam's razor against materialism, stating that matter was not required by his metaphysic and was thus eliminable. One potential problem with this belief is that it's possible, given Berkeley's position, to find solipsism itself more in line with the razor than a God-mediated world beyond a single thinker.\n\nOccam's razor may also be recognized in the apocryphal story about an exchange between Pierre-Simon Laplace and Napoleon. It is said that in praising Laplace for one of his recent publications, the emperor asked how it was that the name of God, which featured so frequently in the writings of Lagrange, appeared nowhere in Laplace's. At that, he is said to have replied, \"It's because I had no need of that hypothesis.\" Though some point to this story as illustrating Laplace's atheism, more careful consideration suggests that he may instead have intended merely to illustrate the power of methodological naturalism, or even simply that the fewer logical premises one assumes, the stronger is one's conclusion.\n\nIn his article \"Sensations and Brain Processes\" (1959), J. J. C. Smart invoked Occam's razor with the aim to justify his preference of the mind-brain identity theory over spirit-body dualism. Dualists state that there are two kinds of substances in the universe: physical (including the body) and spiritual, which is non-physical. In contrast, identity theorists state that everything is physical, including consciousness, and that there is nothing nonphysical. Though it is impossible to appreciate the spiritual when limiting oneself to the physical, Smart maintained that identity theory explains all phenomena by assuming only a physical reality. Subsequently, Smart has been severely criticized for his use (or misuse) of Occam's razor and ultimately retracted his advocacy of it in this context. Paul Churchland (1984) states that by itself Occam's razor is inconclusive regarding duality. In a similar way, Dale Jacquette (1994) stated that Occam's razor has been used in attempts to justify eliminativism and reductionism in the philosophy of mind. Eliminativism is the thesis that the ontology of folk psychology including such entities as \"pain\", \"joy\", \"desire\", \"fear\", etc., are eliminable in favor of an ontology of a completed neuroscience.\n\nIn penal theory and the philosophy of punishment, parsimony refers specifically to taking care in the distribution of punishment in order to avoid excessive punishment. In the utilitarian approach to the philosophy of punishment, Jeremy Bentham's \"parsimony principle\" states that any punishment greater than is required to achieve its end is unjust. The concept is related but not identical to the legal concept of proportionality. Parsimony is a key consideration of the modern restorative justice, and is a component of utilitarian approaches to punishment, as well as the prison abolition movement. Bentham believed that true parsimony would require punishment to be individualised to take account of the sensibility of the individual—an individual more sensitive to punishment should be given a proportionately lesser one, since otherwise needless pain would be inflicted. Later utilitarian writers have tended to abandon this idea, in large part due to the impracticality of determining each alleged criminal's relative sensitivity to specific punishments.\n\nMarcus Hutter's universal artificial intelligence builds upon Solomonoff's mathematical formalization of the razor to calculate the expected value of an action.\n\nThere are various papers in scholarly journals deriving formal versions of Occam's razor from probability theory, applying it in statistical inference, and using it to come up with criteria for penalizing complexity in statistical inference. Papers have suggested a connection between Occam's razor and Kolmogorov complexity.\n\nOne of the problems with the original formulation of the razor is that it only applies to models with the same explanatory power (i.e., it only tells us to prefer the simplest of equally good models). A more general form of the razor can be derived from Bayesian model comparison, which is based on Bayes factors and can be used to compare models that don't fit the observations equally well. These methods can sometimes optimally balance the complexity and power of a model. Generally, the exact Occam factor is intractable, but approximations such as Akaike information criterion, Bayesian information criterion, Variational Bayesian methods, false discovery rate, and Laplace's method are used. Many artificial intelligence researchers are now employing such techniques, for instance through work on Occam Learning or more generally on the Free energy principle.\n\nStatistical versions of Occam's razor have a more rigorous formulation than what philosophical discussions produce. In particular, they must have a specific definition of the term \"simplicity\", and that definition can vary. For example, in the Kolmogorov–Chaitin minimum description length approach, the subject must pick a Turing machine whose operations describe the basic operations \"believed\" to represent \"simplicity\" by the subject. However, one could always choose a Turing machine with a simple operation that happened to construct one's entire theory and would hence score highly under the razor. This has led to two opposing camps: one that believes Occam's razor is objective, and one that believes it is subjective.\n\nThe minimum instruction set of a universal Turing machine requires approximately the same length description across different formulations, and is small compared to the Kolmogorov complexity of most practical theories. Marcus Hutter has used this consistency to define a \"natural\" Turing machine of small size as the proper basis for excluding arbitrarily complex instruction sets in the formulation of razors. Describing the program for the universal program as the \"hypothesis\", and the representation of the evidence as program data, it has been formally proven under Zermelo–Fraenkel set theory that \"the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized.\" Interpreting this as minimising the total length of a two-part message encoding model followed by data given model gives us the minimum message length (MML) principle.\n\nOne possible conclusion from mixing the concepts of Kolmogorov complexity and Occam's razor is that an ideal data compressor would also be a scientific explanation/formulation generator. Some attempts have been made to re-derive known laws from considerations of simplicity or compressibility.\n\nAccording to Jürgen Schmidhuber, the appropriate mathematical theory of Occam's razor already exists, namely, Solomonoff's theory of optimal inductive inference and its extensions. See discussions in David L. Dowe's \"Foreword re C. S. Wallace\" for the subtle distinctions between the algorithmic probability work of Solomonoff and the MML work of Chris Wallace, and see Dowe's \"MML, hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness\" both for such discussions and for (in section 4) discussions of MML and Occam's razor. For a specific example of MML as Occam's razor in the problem of decision tree induction, see Dowe and Needham's \"Message Length as an Effective Ockham's Razor in Decision Tree Induction\".\n\nOccam's razor is not an embargo against the positing of any kind of entity, or a recommendation of the simplest theory come what may. Occam's razor is used to adjudicate between theories that have already passed \"theoretical scrutiny\" tests and are equally well-supported by evidence. Furthermore, it may be used to prioritize empirical testing between two equally plausible but unequally testable hypotheses; thereby minimizing costs and wastes while increasing chances of falsification of the simpler-to-test hypothesis.\n\nAnother contentious aspect of the razor is that a theory can become more complex in terms of its structure (or syntax), while its ontology (or semantics) becomes simpler, or vice versa. Quine, in a discussion on definition, referred to these two perspectives as \"economy of practical expression\" and \"economy in grammar and vocabulary\", respectively.\n\nGalileo Galilei lampooned the \"misuse\" of Occam's razor in his \"Dialogue\". The principle is represented in the dialogue by Simplicio. The telling point that Galileo presented ironically was that if one really wanted to start from a small number of entities, one could always consider the letters of the alphabet as the fundamental entities, since one could construct the whole of human knowledge out of them.\n\nOccam's razor has met some opposition from people who have considered it too extreme or rash. Walter Chatton (c. 1290–1343) was a contemporary of William of Ockham (c. 1287–1347) who took exception to Occam's razor and Ockham's use of it. In response he devised his own \"anti-razor:\" \"If three things are not enough to verify an affirmative proposition about things, a fourth must be added, and so on.\" Although there have been a number of philosophers who have formulated similar anti-razors since Chatton's time, no one anti-razor has perpetuated in as much notability as Chatton's anti-razor, although this could be the case of the Late Renaissance Italian motto of unknown attribution \"Se non è vero, è ben trovato\" (\"Even if it is not true, it is well conceived\") when referred to a particularly artful explanation.\n\nAnti-razors have also been created by Gottfried Wilhelm Leibniz (1646–1716), Immanuel Kant (1724–1804), and Karl Menger (1902–1985). Leibniz's version took the form of a principle of plenitude, as Arthur Lovejoy has called it: the idea being that God created the most varied and populous of possible worlds. Kant felt a need to moderate the effects of Occam's razor and thus created his own counter-razor: \"The variety of beings should not rashly be diminished.\"\n\nKarl Menger found mathematicians to be too parsimonious with regard to variables, so he formulated his Law Against Miserliness, which took one of two forms: \"Entities must not be reduced to the point of inadequacy\" and \"It is vain to do with fewer what requires more.\" A less serious but (some might say) even more extremist anti-razor is 'Pataphysics, the \"science of imaginary solutions\" developed by Alfred Jarry (1873–1907). Perhaps the ultimate in anti-reductionism, \"'Pataphysics seeks no less than to view each event in the universe as completely unique, subject to no laws but its own.\" Variations on this theme were subsequently explored by the Argentine writer Jorge Luis Borges in his story/mock-essay \"Tlön, Uqbar, Orbis Tertius\". There is also Crabtree's Bludgeon, which cynically states that \"[n]o set of mutually inconsistent observations can exist for which some human intellect cannot conceive a coherent explanation, however complicated.\"\n\n\n"}
{"id": "3921784", "url": "https://en.wikipedia.org/wiki?curid=3921784", "title": "Original camera negative", "text": "Original camera negative\n\nThe original camera negative (OCN) is the film in a traditional film-based movie camera which captures the original image. This is the film from which all other copies will be made. It is known as raw stock prior to exposure.\n\nThe size of a roll varies depending on the film gauge and whether or not a new roll, re-can, or short end was used. One hundred or 400 foot rolls are common in 16mm, while 400 or 1,000 foot (ft) rolls are used in 35mm work. While these are the most common sizes, other lengths such as 200, 800, or 1,200 ft may be commercially available from film stock manufacturers, usually by special order. Rolls of 100 and 200 ft are generally wound on spools for daylight-loading, while longer lengths are only wound around a plastic core. Core-wound stock has no exposure protection outside its packaging, and therefore must be loaded into a camera magazine within a darkroom or changing bag/tent in order to prevent the film being fogged.\n\nOriginal camera negative is of great value, as if lost or damaged it cannot be re-created without re-shooting the scene, something which is often impossible. It also contains the highest-quality version of the original image available, before any analog resolution and dynamic range loss from copying. For these reasons, original camera negative is handled with great care, and only by specialized trained people in dedicated film laboratories.\n\nAfter the film is processed by the film lab, camera rolls are assembled into lab rolls of 1,200 to 1,500 ft. Work prints may be made for viewing dailies or editing the picture on film.\n\nOnce film editing is finalized, a negative cutter will conform the negative using the Keykode on the edge of the film as a reference, cutting the original camera negative and incorporating any opticals (titles, dissolves, fades, and special effects), and cementing it together into several rolls. \n\nThe edited original negative is then copied to create a safety positive which can be used as a backup to create a usable negative. At this point, an answer print will be created from the original camera negative, and upon its approval, interpositives (IPs) and internegatives (INs) are created, from which the release prints are made. Generally speaking, the original camera negative is considered too important and delicate to be used for any processes more than necessary, as each pass through a lab process carries the risk of further degrading the quality of the negative by scratching the emulsion. Once an answer print is approved, the interpositives and internegatives are regarded as the earliest generation of the finished and graded film, and are almost always used for transfers to video or new film restorations. The original camera negatives is usually regarded as a last resort in the event that all of the intermediate elements have been compromised or lost.\n\nThe more popular a film is, the higher the likelihood that the original negative is in a worse shape, due to the need to return to the original camera negative to strike new interpositives to replace the exhausted ones, and thus create more internegatives and release prints. Before 1969, 35mm prints were struck directly from the original negative, often running into hundreds of copies, and causing further wear on the original.\n\nPhysical film stock is still occasionally used in film-making, particularly in prestige productions where the director and cinematographer have the power to require the extra cost, but as of 2016, it is becoming increasingly rare.\n\nIn modern cinematography, the camera is usually a digital camera, and no physical negative exists. However, the concept of \"camera original material\" is still used to describe camera image data. Camera original material that has not yet been ingested, duplicated, and archived is in a similar precarious state to original camera negative in a film process. One of the jobs of the digital imaging technician is to ensure that digital camera original material is backed up as soon as possible.\n"}
{"id": "4595966", "url": "https://en.wikipedia.org/wiki?curid=4595966", "title": "Pollyanna principle", "text": "Pollyanna principle\n\nThe Pollyanna principle (also called Pollyannaism or positivity bias) is the tendency for people to remember pleasant items more accurately than unpleasant ones. Research indicates that at the subconscious level, the mind has a tendency to focus on the optimistic; while at the conscious level, it has a tendency to focus on the negative. This subconscious bias towards the positive is often described as the Pollyanna principle and is similar to the Forer effect.\n\nThe name derives from the 1913 novel \"Pollyanna\" by Eleanor H. Porter describing a girl who plays the \"glad game\"—trying to find something to be glad about in every situation. The novel has been adapted to film several times, most famously in 1920 and 1960. An early use of the name \"Pollyanna\" in psychological literature was in 1969 by Boucher and Osgood who described a \"Pollyanna hypothesis\" as a universal human tendency to use evaluatively positive words more frequently and diversely than evaluatively negative words in communicating. Empirical evidence for this tendency has been provided by computational analyses of large corpora of text.\n\nThe \"Pollyanna principle\" was described by Matlin and Stang in 1978 using the archetype of Pollyanna more specifically as a psychological principle which portrays the positive bias people have when thinking of the past. According to the Pollyanna Principle, the brain processes information that is pleasing and agreeable in a more precise and exact manner as compared to unpleasant information. We actually tend to remember past experiences as more rosy than they actually occurred.\n\nResearchers Margaret Matlin and David Stang provided substantial evidence of the Pollyanna Principle. They found that people expose themselves to positive stimuli and avoid negative stimuli, they take longer to recognize what is unpleasant or threatening than what is pleasant and safe, and they report that they encounter positive stimuli more frequently than they actually do. Matlin and Stang also determined that selective recall was a more likely occurrence when recall was delayed: the longer the delay, the more selective recall that occurred.\n\nThe Pollyanna principle has been observed on online social networks as well. For example, Twitter users preferentially share more, and are emotionally affected more frequently by, positive information.\n\nHowever, the Pollyanna principle does not always apply to individuals suffering from depression or anxiety, who tend to either have more depressive realism or a negative bias.\n\n\n"}
{"id": "31014293", "url": "https://en.wikipedia.org/wiki?curid=31014293", "title": "Principle of transformation groups", "text": "Principle of transformation groups\n\nThe principle of transformation groups is a rule for assigning \"epistemic\" probabilities in a statistical inference problem. It was first suggested by Edwin T. Jaynes and can be seen as a generalisation of the principle of indifference.\n\nThis can be seen as a method to create \"objective ignorance probabilities\" in the sense that two people who apply the principle and are confronted with the same information will assign the same probabilities.\n\nThe method is motivated by the following normative principle, or desideratum:\n\n\"In two problems where we have the same prior information we should assign the same prior probabilities\"\n\nThe method then comes about from \"transforming\" a given problem into an equivalent one. This method has close connections with group theory, and to a large extent is about finding symmetry in a given problem, and then exploiting this symmetry to assign prior probabilities.\n\nIn problems with discrete variables (e.g. dice, cards, categorical data) the principle reduces to the principle of indifference, as the \"symmetry\" in the discrete case is a permutation of the labels, that is the permutation group is the relevant transformation group for this problem.\n\nIn problems with continuous variables, this method generally reduces to solving a differential equation. Given that differential equations do not always lead to unique solutions, this method cannot be guaranteed to produce a unique solution. However, in a large class of the most common types of parameters it does lead to unique solutions (see the examples below)\n\nConsider a problem where all you are told is that there is a coin, and it has a head (H) and a tail (T). Denote this information by \"I\". You are then asked \"what is the probability of Heads?\". Call this \"problem 1\" and denote the probability \"P(H|I)\". Consider another question \"what is the probability of Tails?\". Call this \"problem 2\" and denote this probability by \"P(T|I)\".\n\nNow from the information which was actually in the question, there is no distinction between heads and tails. The whole paragraph above could be re-written with \"Heads\" and \"Tails\" interchanged, and \"H\" and \"T\" interchanged, and the problem statement would not be any different. Using the desideratum then demands that\n\nformula_1\n\nThe probabilities must add to 1, this means that\n\nformula_2.\n\nThus we have a unique solution. This argument easily extents to \"N\" categories, to give the \"flat\" prior probability \"1/N\".\nThis provides a \"consistency\" based argument to the principle of indifference which goes as follows: \"if someone is truly ignorant about a discrete/countable set of outcomes apart from their potential existence, but does not assign them equal prior probabilities, then they are assigning different probabilities when given the same information\".\n\nThis can be alternatively phrased as: \"a person who does not use the principle of indifference to assign prior probabilities to discrete variables, is either not ignorant about them, or reasoning inconsistently\".\n\nThis is the easiest example for continuous variables. It is given by stating one is \"ignorant\" of the location parameter in a given problem. The statement that a parameter is a \"location parameter\" is that the sampling distribution, or likelihood of an observation \"X\" depends on a parameter formula_3 only through the difference\n\nformula_4\n\nfor some normalised, but otherwise arbitrary distribution \"f(.)\". Examples of location parameters include mean parameter of normal distribution with known variance and median parameter of Cauchy distribution with known inter-quartile range.\nThe two \"equivalent problems\" in this case, given ones knowledge of the sampling distribution formula_4, but no other knowledge about formula_3, is simply given by a \"shift\" of equal magnitude in \"X\" and formula_3. This is because of the relation:\n\nformula_8\n\nSo simply \"shifting\" all quantities up by some number \"b\" and solving in the \"shifted space\" and then \"shifting\" back to the original one should give exactly the same answer as if we just worked on the original space. Making the transformation from formula_3 to formula_10 has a Jacobian of simply 1, and so the prior probability formula_11 must satisfy the functional equation:\n\nformula_12\n\nAnd the only function which satisfies this equation is the \"constant prior\":\n\nformula_13\n\nThus the uniform prior is justified for expressing complete ignorance of a location parameter.\n\nAs in the above argument, a statement that formula_14 is a scale parameter means that the sampling distribution has the functional form:\n\nformula_15\n\nWhere, as before \"f(.)\" is a normalised probability density function. The requirement that probabilities be finite and positive forces the condition formula_16. Examples include the standard deviation of a normal distribution with known mean, the gamma distribution. The \"symmetry\" in this problem is found by noting that\n\nformula_17\n\nBut, unlike in the location parameter case, the Jacobian of this transformation in the sample space and the parameter space is \"a\", not 1. so the sampling probability changes to:\n\nformula_18\n\nWhich is invariant (i.e. has the same form before and after the transformation), and the prior probability changes to:\n\nformula_19\n\nWhich has the unique solution (up to a proportionality constant):\n\nformula_20\n\nWhich is the well-known Jeffreys prior for scale parameters, which is \"flat\" on the log scale, although it should be noted that it is derived using a different argument to that here, based on the Fisher information function. The fact that these two methods give the same results in this case does not imply it in general.\n\nEdwin Jaynes used this principle to provide a resolution to Bertrand's Paradox\nby stating his ignorance about the exact position of the circle. The details are available in the reference or in the link.\n\nThis argument depends crucially on \"I\"; changing the information may result in a different probability assignment. It is just as crucial as changing axioms in deductive logic - small changes in the information can lead to large changes in the probability assignments allowed by \"consistent reasoning\".\n\nTo illustrate suppose that the coin flipping example also states as part of the information that the coin has a side (S) (i.e. it is a \"real coin\"). Denote this new information by \"N\". The same argument using \"complete ignorance\", or more precisely, the information actually described, gives:\n\nformula_21\n\nBut this seems absurd to most people - intuition tells us that we should have P(S) very close to zero. This is because most people's intuition do not see \"symmetry\" between a coin landing on its side compared to landing on heads. Our intuition says that the particular \"labels\" actually carry some information about the problem. A simple argument could be used to make this more formal mathematically (e.g. the physics of the problem make it difficult for a flipped coin to land on its side) - we make a distinction between \"thick\" coins and \"thin\" coins [here thickness is measured relative to the coin's diameter]. It could reasonably be assumed that:\n\nformula_22\n\nNote that this new information probably wouldn't break the symmetry between \"heads\" and \"tails\", so \"that\" permutation would still apply in describing \"equivalent problems\", and we would require:\n\nformula_23\n\nThis is a good example of how the principle of transformation groups can be used to \"flesh out\" personal opinions. All of the information used in the derivation is explicitly stated. If a prior probability assignment doesn't \"seem right\" according to what your intuition tells you, then there must be some \"background information\" which has not been put into the problem. It is then the task to try and work out what that information is. In some sense, by combining the method of transformation groups with one's intuition can be used to \"weed out\" the actual assumptions one has. This makes it a very powerful tool for prior elicitation.\n\nIntroducing the thickness of the coin is permissible because it was not specified in the problem, so this is still only using information in the question. Introducing a \"nuisance parameter\" and then making the answer invariant to this parameter is a very useful technique for solving supposedly \"ill-posed\" problems like Bertrand's Paradox. This has been called \"the well-posing strategy\" by some.\n\nThe real power of this principle lies in its application to continuous parameters, where the notion of \"complete ignorance\" is not so well defined as in the discrete case. However, if applied with infinite limits, it often gives improper prior distributions. Note that the discrete case for a countably infinite set, such as (0,1,2...) also produces an improper discrete prior. For most cases where the likelihood is sufficiently \"steep\" this does not present a problem. However, in order to be absolutely sure to avoid incoherent results and paradoxes, the prior distribution should be approached via a well defined and well behaved limiting process. One such process is the use of a sequence of priors with increasing range, such as formula_24 where the limit formula_25 is to be taken \"at the end of the calculation\" i.e. after the normalisation of the posterior distribution. What this effectively is doing, is ensuring that one is taking the limit of the ratio, and not the ratio of two limits. See Limit of a function#Properties for details on limits and why this order of operations is important.\n\nIf the limit of the ratio does not exist or diverges, then this gives an improper posterior (i.e. a posterior which does not integrate to one). This indicates that the data are so uninformative about the parameters that the prior probability of arbitrarily large values still matters in the final answer. In some sense, an improper posterior means that the information contained in the data has not \"ruled out\" arbitrarily large values. Looking at the improper priors this way, it seems to make some sense that \"complete ignorance\" priors should be improper, because the information used to derive them is so meager that it cannot rule out absurd values on its own. From a state of complete ignorance, only the data or some other form of additional information can rule out such absurdities.\n\n"}
{"id": "48000439", "url": "https://en.wikipedia.org/wiki?curid=48000439", "title": "Records Continuum Model", "text": "Records Continuum Model\n\nThe Records Continuum Model (RCM) was created in the 1990s by Monash University academic Frank Upward with input from colleagues Sue McKemmish and Livia Iacovino as a response to evolving discussions about the challenges of managing digital records and archives in the discipline of Archival Science. The RCM was first published in Upward’s 1996 paper \"Structuring the Records Continuum – Part One: Postcustodial principles and properties\". Upward describes the RCM within the broad context of a continuum where activities and interactions transform documents into records, evidence and memory that are used for multiple purposes over time. Upward places the RCM within a post-custodial, postmodern and structuration conceptual framework. Australian academics and practitioners continue to explore, develop and extend the RCM and records continuum theory, along with international collaborators, via the Records Continuum Research Group (RCRG) at Monash University.\n\nThe RCM is an abstract conceptual model that helps to understand and explore recordkeeping activities (as interaction) in relation to multiple contexts over space and time (spacetime). Recordkeeping activities take place from before the records are created by identifying recordkeeping requirements in policies, systems, organizations, processes, laws, social mandates that impact on what is created and how it is managed over spacetime. In a continuum, recordkeeping processes, such as adding metadata, fix documents so that they can be managed as evidence. Those records deemed as having continuing value are retained and managed as an archive. The implication of an RCM approach to records and archives is that systems and processes can be designed and put in place before records are even created. A continuum approach therefore highlights that records are both current and archival at the point of creation.\n\nThe RCM is represented as a series of concentric rings (dimensions of \"Create\", \"Capture\", \"Organize\" and \"Pluralize\") and crossed axes (transactionality, evidentiality, recordkeeping and identity) with each axis labelled with a description of the activity or interaction that occurs at that intersection. \"Create\", \"Capture\", \"Organize\" and \"Pluralize\" represent recordkeeping activities that occur within spacetime. Activities that occur in these dimensions across the axes are explained in the table below:\n\nThe value of the RCM is that it can help to map where on a continuum recordkeeping activities are or can be placed. The RCM can then be used to explore the conceptual and practical assumptions that underpin the practice, in particular the dualisms inherent in the usage and practice of the terms \"records\" and \"archives\". This definition lends itself to a linear reading of the RCM – starting at \"Create\" as the initiating phase and working outwards towards \"Pluralization\" of recorded information. Another linear reading is to consider design first – the role that systems of \"Pluralization\" and \"Organization\" play in designing, planning and implementing recordkeeping and then considering the implications for \"Create\" and \"Capture\". However, these are just two of many ways to interpret the model as the dimensions and axes represent multiple realities that occur within spacetime, any of which can occur simultaneously, concurrently and sequentially in electronic or digital environments, and/or physical spaces.\n\nBy representing multiple realities, the RCM articulates the numerous and diverse perspectives that contribute to records and archives including individual, group, community, organizational, institutional and societal. These contexts reveal the need to take into account various stakeholders and co-contributors in relation to use, access and appraisal of records and archives. Over the lifespan of a record multiple decisions are made by various stakeholders of the records that include, but are not limited to records managers and archivists. Other stakeholders can be identified at various dimensions of interaction, including those involved in providing information (not only the person or organization who produced or captured it), as well as their family and community. Records are therefore not simply physical or digital representations of physical objects held and managed in an archive or repository, but are evidence of multiple perspectives, narratives and contexts that contributed to their formation.\n\nThe RCM is often described as being in contrast or at odds with the lifecycle records model. While the RCM is inclusive of multiple ways of conceptualizing and performing recordkeeping, including a lifecycle approach, there are some significant differences. Firstly, where the lifecycle approach shows clearly demarcated phases in the management of records, a continuum approach conceptualizes elements as continuous with no discernable parts. Secondly, the lifecycle approach identifies clear conceptual and procedural boundaries between active or current records and inactive or historical records, but a continuum approach sees records processes as more integrated across spacetime. In the continuum it is recordkeeping processes that carry records forward through spacetime to enable their use for multiple purposes. What this means is that records are always \"in a state of always becoming...\", and able to contribute new contexts via the recordkeeping processes that occur with them. Archival records are therefore not just historical, but are able to be re-interpreted, re-created, and re-contextualized according to their place and use in spacetime. In this way, archival institutions are nodes in the network of recorded information and its contexts, rather than the end point in a lifecycle stage for records that are managed as \"relics\".\n\nThe RCM is a representation of what is commonly referred to as records continuum theory, as well as Australian continuum thinking and/or approaches. These ideas were evolved as part of an Australian approach to archival management espoused by Ian Maclean, Chief Archivist of the Commonwealth Archives Office in Australia in the 1950s and 1960s. Maclean, whose ideas and practices were the subject of the first RCRG publication in 1994, referred in a 1959 \"American Archivist\" article to a \"continuum of (public) records administration\" from administrative efficiency through recordkeeping to the safe keeping of a \"cultural end-product\". Maclean’s vision challenged the divide between current recordkeeping and archival practice. Fellow contemporary at the Commonwealth Archives Office Peter Scott is also included as a core influence on Australian records continuum theory with his development of the Australian Series System, a registry system that helped identify and document the complex and multiple \"social, functional, provenancial, and documentary relationships\" involved in managing records and recordkeeping processes over spacetime.\n\nFurther influences on the RCRG group include archival professionals and researchers like David Bearman and his work on transactionality and systems thinking, and Terry Cook's ideas about postcustodialism and macroappraisal. Wider influencing ideas include those from philosophers and social theorists Jacques Lacan, Michel Foucault, Jacques Derrida, and Jean-François Lyotard, as well as sociologist Anthony Giddens, with structuration theory being a core component of understanding social interaction over spacetime. Canadian archivist Jay Atherton's critique of the division between records managers and archivists in the 1980s and use of the term \"records continuum\" re-commenced the conversation MacLean began during his career and helped to bring his ideas and this term to Australian records continuum thinking. Atherton's use of the term records continuum has several significant differences in conception, application and heritage when compared to Australian records continuum thinking.\n\nPost-custodiality as an archival concept plays a major role in how the RCM was conceived. This term was born from an identified and urgent need to address the complexities of computer technologies on records creation and management over time and space. Post-custodiality is discussed by Frank Upward and Sue McKemmish in 1994 as part of an exploration of changes in archival discourse commencing in the 1980s by Gerald Ham and expanded on by Terry Cook as part of a \"post-custodial paradigm shift\". Post-custodiality in relation to the RCM is explored by Upward and McKemmish as an entry point into a wider conversation about records and recordkeeping being part of a process in which archival institutions have a part to play beyond that of the archival authority handling, appraisal, describing and arranging physical objects in their custody.\n\nDrawing from the above theoretical foundations, the RCM as a framework acknowledges the central role that recordkeeping activities have on the creation, capture, organization and ongoing management of records over time and throughout spaces such as organizations and institutional archives. Recordkeeping is a practice and a concept clearly defined in the archival and records literature by continuum writers as \"a broad and inclusive concept of integrated recordkeeping and archiving processes for current, regulatory, and historical recordkeeping purposes\". Recordkeeping refers to the activities performed on records that add new contexts such as capturing a record into a system, adding metadata, or selecting it for an archive. In the RCM records are therefore not defined according to their status as objects. Rather, records are understood as being part of a continuum of activity related to known (as well as potentially unknown) contexts. A record (as well as records, collections and archives) are therefore part of larger social, cultural, political, legal and archival processes. It is these contexts that are vital to understanding the role, value and evidential qualities of records in and across spacetime (past, present and potential future).\n\nThe RCM is the most well-known of all the continuum models created, but does not exist in isolation. Several other complementary models have been created by RCM creator Frank Upward, and there are others created by continuum researchers that offer enhanced or alternative ways of understanding the continuum.\n\nThe series of continuum models created by Frank Upward include:\n\nModels created in collaboration:\n\nOther models:\n\n"}
{"id": "21647661", "url": "https://en.wikipedia.org/wiki?curid=21647661", "title": "Self model", "text": "Self model\n\nThe self-model is the central concept in the theory of consciousness called the self-model theory of subjectivity (SMT). This concept comprises experiences of ownership, of first person perspective, and of a long-term unity of beliefs and attitudes. These features are instantiated in the prefrontal cortex. This theory is an interdisciplinary approach to understanding and explaining the phenomenology of consciousness and the self. This theory has two core contents, the phenomenal self-model (PSM) and the phenomenal model of the intentionality relation (PMIR). Thomas Metzinger advanced the theory in his 1993 book \"Subjekt und Selbstmodell\" (Subject and self-model).\n\nThe PSM is an entity that “actually exists, not only as a distinct theoretical entity but something that will be empirically discovered in the future- for instance, as a specific stage of the global neural dynamics in the human brain”. Involved in the PSM are three phenomenal properties that must occur in order to explain the concept of the self. The first is mineness, “a higher order property of particular forms of phenomenal content,” or the idea of ownership. The second is perspectivalness, which is “a global, structural property of phenomenal space as a whole”. More simply, it is what is commonly referred to as the ecological self, the immovable center of perception. The third phenomenal property is selfhood, which is “the phenomenal target property” or the idea of the self over time. It is the property of phenomenal selfhood that plays the most important role in creating the fictional self and the first person perspective. Metzinger defines the first person perspective as the “existence of single coherent and temporally stable model of reality which is representationally centered around or on a single coherent and temporally stable phenomenal subject”. The first-person perspective can be non-conceptual and is autonomously active due to the constant reception of perceptual information by the brain. The brain, specifically the brainstem and hypothalamus, processes this information into representational content, namely linguistic reflections. The PSM then uses this representational content to attribute phenomenal states to our perceived objects and ourselves. We are thus what Metzinger calls naïve realists, who believe we are perceiving reality directly when in actuality we are only perceiving representations of reality. The data structures and transport mechanisms of the data are “transparent” so that we can introspect on our representations of perceptions, but cannot introspect on the data or mechanisms themselves. These systemic representational experiences are then connected by subjective experience to generate the phenomenal property of selfhood. Subjective experience is the result of the Phenomenal Model of Intentionality Relationship (PMIR). The PMIR is a “conscious mental model, and its content is an ongoing, episodic subject-object relation”. The model is a result of the combination of our unique set of sensory receptors that acquire input, our unique set of experiences that shape connections within the brain, and our unique positions in space that give our perception perspectivalness.\n\nThe prefrontal cortex is implicated in all the functions of the human self model. The following functions all require communication with the prefrontal cortex; agency and association areas of the cortex; spatial perspectivity and the parietal lobes, unity and the temporal lobes.\n\nDisorders of the self model are implicated in several disorders including schizophrenia, autism, and depersonalization. According to this theory, long-term unity is impaired in autism, similar to theory of mind deficits and weak central coherence theory. Individuals with autism are thought to be impaired in assigning mental states to other people, an ability that probably codevelops with long-term unity of self. Weak central coherence, that is, the inability to assemble information into a cohesive whole, reflects the same problems with creating a unified sense of self and benific sense extreme in narcissism.\n\n"}
{"id": "244755", "url": "https://en.wikipedia.org/wiki?curid=244755", "title": "Sense and reference", "text": "Sense and reference\n\nIn the philosophy of language, the distinction between sense and reference was an innovation of the German philosopher and mathematician Gottlob Frege in 1892 (in his paper \"On Sense and Reference\"; German: \"Über Sinn und Bedeutung\"), reflecting the two ways he believed a singular term may have meaning.\n\nThe reference (or \"referent\"; \"Bedeutung\") of a proper name is the object it means or indicates (\"bedeuten\"), its sense (\"Sinn\") is what the name expresses. The reference of a sentence is its truth value, its sense is the thought that it expresses. Frege justified the distinction in a number of ways.\n\nMuch of analytic philosophy is traceable to Frege's philosophy of language. Frege's views on logic (i.e., his idea that some parts of speech are complete by themselves, and are analogous to the arguments of a mathematical function) led to his views on a theory of reference.\n\nFrege developed his original theory of meaning in early works like \"Begriffsschrift\" ('concept script') of 1879 and \"Grundlagen\" ('foundations of arithmetic') of 1884. On this theory, the meaning of a complete sentence consists in its being true or false, and the meaning of each significant expression in the sentence is an extralinguistic entity which Frege called its \"Bedeutung\", literally 'meaning' or 'significance', but rendered by Frege's translators as 'reference', 'referent', \"'M\"eaning', 'nominatum', etc. Frege supposed that some parts of speech are complete by themselves, and are analogous to the arguments of a mathematical function, but that other parts are incomplete, and contain an empty place, by analogy with the function itself. Thus 'Caesar conquered Gaul' divides into the complete term 'Caesar', whose reference is Caesar himself, and the incomplete term '—conquered Gaul', whose reference is a Concept. Only when the empty place is filled by a proper name does the reference of the completed sentence – its truth value – appear. This early theory of meaning explains how the significance or reference of a sentence (its truth value) depends on the significance or reference of its parts.\n\nFrege introduced the notion of \"sense\" (German: \"Sinn\") to accommodate difficulties in his early theory of meaning.\n\nFirst, if the entire significance of a sentence consists of its truth value, it follows that the sentence will have the same significance if we replace a word of the sentence with one having an identical reference, as this will not change its truth value. The reference of the whole is determined by the reference of the parts. If \"the evening star\" has the same reference as \"the morning star\", it follows that \"the evening star is a body illuminated by the Sun\" has the same truth value as \"the morning star is a body illuminated by the Sun\". But it is possible for someone to think that the first sentence is true while also thinking that the second is false. Therefore, the thought corresponding to each sentence cannot be its reference, but something else, which Frege called its \"sense\".\n\nSecond, sentences that contain proper names with no reference cannot have a truth value at all. Yet the sentence 'Odysseus was set ashore at Ithaca while sound asleep' obviously has a sense, even though 'Odysseus' has no reference. The thought remains the same whether or not 'Odysseus' has a reference. Furthermore, a thought cannot contain the objects that it is about. For example, Mont Blanc, 'with its snowfields', cannot be a component of the thought that Mont Blanc is more than 4,000 metres high. Nor can a thought about Etna contain lumps of solidified lava.\n\nFrege's notion of sense is somewhat obscure, and neo-Fregeans have come up with different candidates for its role. Accounts based on the work of Carnap and Church treat sense as an intension, or a function from possible worlds to extensions. For example, the intension of ‘number of planets’ is a function that maps any possible world to the number of planets in that world. John McDowell supplies cognitive and reference-determining roles. Devitt treats senses as causal-historical chains connecting names to referents.\n\nIn his theory of descriptions, Bertrand Russell held the view that most proper names in ordinary language are in fact disguised definite descriptions. For example, 'Aristotle' can be understood as \"The pupil of Plato and teacher of Alexander,\" or by some other uniquely applying description. This is known as the descriptivist theory of names. Because Frege used definite descriptions in many of his examples, he is often taken to have endorsed the descriptivist theory. Thus Russell's theory of descriptions was conflated with Frege's theory of sense, and for most of the twentieth century this 'Frege-Russell' view was the orthodox view of proper name semantics. However, Saul Kripke argued compellingly against the descriptivist theory. According to Kripke, proper names are rigid designators which designate the same object in every possible world. Descriptions such as 'the President of the U.S. in 1970' do not designate the same in every possible world. For example, someone other than Richard Nixon, e.g. Hubert Humphrey, might have been the President in 1970. Hence a description (or cluster of descriptions) cannot be a rigid designator, and thus a proper name cannot \"mean\" the same as a description.\n\nHowever, the Russellian descriptivist reading of Frege has been rejected by many scholars, in particular by Gareth Evans in \"The Varieties of Reference\" and by John McDowell in \"The Sense and Reference of a Proper Name,\" following Michael Dummett, who argued that Frege's notion of sense should not be equated with a description. Evans further developed this line, arguing that a sense without a referent was not possible. He and McDowell both take the line that Frege's discussion of empty names, and of the idea of sense without reference, are inconsistent, and that his apparent endorsement of descriptivism rests only on a small number of imprecise and perhaps offhand remarks. And both point to the power that the sense-reference distinction \"does\" have (i.e., to solve at least the first two problems), even if it is not given a descriptivist reading.\n\nAs noted above, translators of Frege have rendered the German \"Bedeutung\" in various ways. The term 'reference' has been the most widely adopted, but this fails to capture the meaning of the original German ('meaning' or 'significance'), and does not reflect the decision to standardise key terms across different editions of Frege's works published by Blackwell. The decision was based on the principle of exegetical neutrality, namely that 'if at any point in a text there is a passage that raises for the native speaker legitimate questions of exegesis, then, if at all possible, a translator should strive to confront the reader of his version with the same questions of exegesis and not produce a version which in his mind resolves those questions'. The term 'meaning' best captures the standard German meaning of \"Bedeutung\", and Frege's own use of the term sounds as odd when translated into English as it does in German. Moreover, 'meaning' captures Frege's early use of \"Bedeutung\" well, and it would be problematic to translate Frege's early use as 'meaning' and his later use as 'reference', suggesting a change in terminology not evident in the original German.\n\nThe Greek philosopher Antisthenes, a pupil of Socrates, apparently distinguished \"a general object that can be aligned with the meaning of the utterance” from “a particular object of extensional reference.\" This \"suggests that he makes a distinction between sense and reference.\" \nThe principal basis of this claim is a quotation in Alexander of Aphrodisias's “Comments on Aristotle's 'Topics'” with a three-way distinction: \n\nThe sense-reference distinction is commonly confused with that between connotation and denotation, which originates with John Stuart Mill. According to Mill, a common term like 'white' \"denotes\" all white things, as snow, paper. But according to Frege, a common term does not refer to any individual white thing, but rather to an abstract Concept (\"Begriff\"). We must distinguish between the relation of reference, which holds between a proper name and the object it refers to, such as between the name 'Earth', and the planet Earth, and the relation of 'falling under', such as when the Earth falls under the concept \"planet\". The relation of a proper name to the object it designates is direct, whereas a word like 'planet' has no such direct relation at all to the Earth at all, but only to a concept that the Earth falls under. Moreover, judging \"of\" anything that it falls under this concept is not in any way part of our knowledge of what the word 'planet' means. The distinction between connotation and denotation is closer to that between Concept and Object, than to that between 'sense' and 'reference'.\n\n"}
{"id": "53256704", "url": "https://en.wikipedia.org/wiki?curid=53256704", "title": "Threatcasting", "text": "Threatcasting\n\nThreatcasting is a conceptual framework used to help multidisciplinary groups envision future scenarios. It is also a process that enables systematic planning against threats ten years in the future. Utilizing the threatcasting process, groups explore possible future threats and how to transform the future they desire into reality while avoiding undesired futures. Threatcasting is a continuous, multiple-step process with inputs from social science, technical research, cultural history, economics, trends, expert interviews, and science fiction storytelling. These inputs inform the exploration of potential visions of the future.\n\nOnce inputs are explored for impact and application, participants create a science fiction story (Science Fiction Prototyping) based ten years in the future to add context around human activity. Science Fiction Prototyping consists of a future story about a person in a place doing a thing. The threatcasting process results in creation of many potential futures scenarios - some futures are desirable while others are not. Identifying both types of futures (desirable and undesirable) will help the participant recognize which future to aim toward, and which to avoid. Utilizing the scenarios, participants plot actions necessary in the present and at various intervals working toward the ten year future scenario. These actions will help participants understand how to empower or disrupt the target future scenario. Flags (warning events) are also determined in order to map societal indicators onto the recommended path toward the targeted future. When identified flags appear in society, threatcasting participants map these back to the original forecast to see whether or not they are on track toward the target future scenario.\n\nThe notion of threatcasting can be traced back to Brian David Johnson, an applied futurist, who first began using threatcasting, also referred to as futurecasting, in 2011 and to George Hemingway of the Stratalis Group, who pioneered notion of futurecasting for corporate strategy and innovation industrial markets, including mining in the same year. Early adopters of threatcasting include the United States Air Force Academy, the Government of California, and the Army Cyber Institute at West Point Military Academy. Official use of the term threatcasting is attributed to Brian David Johnson in a 2014 Gazette article “Drones, smart hydrants considered by experts looking at future of firefighting.”\n\nThreatcasting is fundamentally different from traditional strategic planning and scenario building processes due to the identification of specific actions, indicators and concrete steps that can be taken today to disrupt, mitigate and recover from future threats.\n\nThe Army Cyber Institute at West Point in conjunction with Arizona State University's Global Securities Initiative and the School for the Future of Innovation in Society have established a Threatcasting Lab to host and manage a Cyber Threatcasting Project which looks to envision future cyber threats ten years in the future. The first session of this collaborative group was held at West Point, NY in August 2016.\n\n\n"}
{"id": "24709966", "url": "https://en.wikipedia.org/wiki?curid=24709966", "title": "Vicious circle principle", "text": "Vicious circle principle\n\nThe vicious circle principle is a principle that was endorsed by many predicativist mathematicians in the early 20th century to prevent contradictions. The principle states that no object or property may be introduced by a definition that depends on that object or property itself. In addition to ruling out definitions that are explicitly circular (like \"an object has property P iff it is not next to anything that has property P\"), this principle rules out definitions that quantify over domains which include the entity being defined. Thus, it blocks Russell's paradox, which defines a set S that contains all sets that don't contain themselves. This definition is blocked because it defines a new set in terms of the totality of all sets, of which this new set would itself be a member.\n\nHowever, it also blocks one standard definition of the natural numbers. First, we define a property as being \"hereditary\" if, whenever a number \"n\" has the property, so does \"n\" + 1. Then we say that \"x\" has the property of being a natural number if and \"only\" if it has every hereditary property that 0 has. This definition is blocked, because it defines \"natural number\" in terms of the totality of all hereditary properties, but \"natural number\" itself would be such a hereditary property, so the definition is circular in this sense.\n\nMost modern mathematicians and philosophers of mathematics think that this particular definition is not circular in any problematic sense, and thus they reject the vicious circle principle. But it was endorsed by many early 20th century researchers including Bertrand Russell and Henri Poincaré. On the other hand, Frank P. Ramsey and Rudolf Carnap accepted the ban on explicit circularity, but argued against the ban on circular quantification. After all, the definition \"let T be the tallest man in the room\" defines T by means of quantification over a domain (men in the room) of which T is a member. But this is not problematic, they suggest, because the definition doesn't actually create the person, but merely shows how to pick him out of the totality. Similarly, they suggest, definitions don't actually create sets or properties or objects, but rather just give one way of picking out the already existing entity from the collection of which it is a part. Thus, this sort of circularity in terms of quantification can't cause any problems.\n\nThis principle was the reason for Russell's development of the ramified theory of types rather than the theory of simple types. (See \"Ramified Hierarchy and Impredicative Principles\".)\nAn analysis of the paradoxes to be avoided shows that they all result from a kind of vicious circle. The vicious circles in question arise from supposing that a collection of objects may contain members which can only be defined by means of the collection as a whole. Thus, for example, the collection of propositions will be supposed to contain a proposition stating that “all propositions are either true or false.” It would seem, however, that such a statement could not be legitimate unless “all propositions” referred to some already definite collection, which it cannot do if new propositions are created by statements about “all propositions.” We shall, therefore, have to say that statements about “all propositions” are meaningless.… The principle which enables us to avoid illegitimate totalities may be stated as follows: “Whatever involves all of a collection must not be one of the collection”; or, conversely: “If, provided a certain collection had a total, it would have members only definable in terms of that total, then the said collection has no total.” We shall call this the “vicious-circle principle,” because it enables us to avoid the vicious circles involved in the assumption of illegitimate totalities. (Whitehead and Russell 1910, 37) (quoted in the Stanford Encyclopedia of Philosophy entry on Russell's Paradox)\n\n\n"}
{"id": "213446", "url": "https://en.wikipedia.org/wiki?curid=213446", "title": "World view", "text": "World view\n\nA world view or worldview is the fundamental cognitive orientation of an individual or society encompassing the whole of the individual's or society's knowledge and point of view. A world view can include natural philosophy; fundamental, existential, and normative postulates; or themes, values, emotions, and ethics. The term is a calque of the German word Weltanschauung , composed of \"Welt\" ('world') and \"Anschauung\" ('view' or 'outlook'). The German word is also used in English.\n\nIt is a concept fundamental to German philosophy and epistemology and refers to a \"wide world perception\". Additionally, it refers to the framework of ideas and beliefs forming a global description through which an individual, group or culture watches and interprets the world and interacts with it.\n\nWorldview remains a confused and confusing concept in English, used very differently by linguists and sociologists. It is for this reason that James W. Underhill suggests five subcategories: world-perceiving, world-conceiving, cultural mindset, personal world, and perspective.\n\nWorldviews are often taken to operate at a conscious level, directly accessible to articulation and discussion, as opposed to existing at a deeper, pre-conscious level, such as the idea of \"ground\" in Gestalt psychology and media analysis. However, core worldview beliefs are often deeply rooted, and so are only rarely reflected on by individuals, and are brought to the surface only in moments of crises of faith.\n\nThe Prussian philologist Wilhelm von Humboldt (1767–1835) originated the idea that language and worldview are inextricable. Humboldt saw language as part of the creative adventure of mankind. Culture, language and linguistic communities developed simultaneously and could not do so without one another. In stark contrast to linguistic determinism, which invites us to consider language as a constraint, a framework or a prison house, Humboldt maintained that speech is inherently and implicitly creative. Human beings take their place in speech and continue to modify language and thought by their creative exchanges. \n\nEdward Sapir (1884–1939) also gives an account of the relationship between thinking and speaking in English.\n\nThe linguistic relativity hypothesis of Benjamin Lee Whorf (1897–1941) describes how the syntactic-semantic structure of a language becomes an underlying structure for the world view or \"Weltanschauung\" of a people through the organization of the causal perception of the world and the linguistic categorization of entities. As linguistic categorization emerges as a representation of worldview and causality, it further modifies social perception and thereby leads to a continual interaction between language and perception.\n\nWhorf's hypothesis became influential in the late 1940s, but declined in prominence after a decade. In the 1990s, new research gave further support for the linguistic relativity theory in the works of Stephen Levinson (1947–) and his team at the Max Planck institute for psycholinguistics at Nijmegen, Netherlands.\nThe theory has also gained attention through the work of Lera Boroditsky at Stanford University.\n\nOne of the most important concepts in cognitive philosophy and cognitive sciences is the German concept of \"Weltanschauung\". This expression has often been used to refer to the \"wide worldview\" or \"wide world perception\" of a people, family, or person. The \"Weltanschauung\" of a people originates from the unique world experience of a people, which they experience over several millennia.The language of a people reflects the \"Weltanschauung\" of that people in the form of its syntactic structures and untranslatable connotations and its denotations.\n\nThe term \"Weltanschauung\" is often wrongly attributed to Wilhelm von Humboldt, the founder of German ethnolinguistics. However, as Jürgen Trabant points out, and as James W. Underhill reminds us, Humboldt's key concept was \"Weltansicht\". \"Weltansicht\" was used by Humboldt to refer to the overarching conceptual and sensorial apprehension of reality shared by a linguistic community (Nation). On the other hand, \"Weltanschauung\", first used by Kant and later popularized by Hegel, was always used in German and later in English to refer more to philosophies, ideologies and cultural or religious perspectives, than to linguistic communities and their mode of apprehending reality.\n\nA worldview can be expressed as the \"fundamental cognitive, affective, and evaluative presuppositions a group of people make about the nature of things, and which they use to order their lives.\"\n\nIf it were possible to draw a map of the world on the basis of \"Weltanschauung\", it would probably be seen to cross political borders—\"Weltanschauung\" is the product of political borders and common experiences of a people from a geographical region, environmental-climatic conditions, the economic resources available, socio-cultural systems, and the language family. (The work of the population geneticist Luigi Luca Cavalli-Sforza aims to show the gene-linguistic co-evolution of people).\n\nIf the Sapir–Whorf hypothesis is correct, the worldview map of the world would be similar to the linguistic map of the world. However, it would also almost coincide with a map of the world drawn on the basis of music across people.\n\nAs natural language becomes manifestations of world perception, the literature of a people with common \"Weltanschauung\" emerges as holistic representations of the wide world perception of the people. Thus the extent and commonality between world folk-epics becomes a manifestation of the commonality and extent of a worldview.\n\nEpic poems are shared often by people across political borders and across generations. Examples of such epics include the Nibelungenlied of the Germanic people, the Iliad for the Ancient Greeks and Hellenized societies, the Silappadhikaram of the Tamil people, the Ramayana and Mahabharata of the Hindus, the Gilgamesh of the Mesopotamian-Sumerian civilization and the people of the Fertile Crescent at large, The Book of One Thousand and One Nights (Arabian nights) of the Arab world and the Sundiata epic of the Mandé people.\n\nA worldview, according to terror management theory (TMT), serves as a buffer against death anxiety. It is theorised that living up to the ideals of one's worldview provides a sense of self-esteem which provides a sense of transcending the limits of human life (e.g. literally, as in religious belief in immortality, symbolically, as in art works or children to live on after one's death, or in contributions to one's culture). Evidence in support of terror management theory includes a series of experiments by Jeff Schimel and colleagues in which a group of Canadians found to score highly on a measure of patriotism were asked to read an essay attacking the dominant Canadian worldview.\n\nUsing a test of death-thought accessibility (DTA), involving an ambiguous word completion test (e.g. \"COFF__\" could either be completed as either \"COFFEE\" or \"COFFIN\" or \"COFFER\"), participants who had read the essay attacking their worldview were found to have a significantly higher level of DTA than the control group, who read a similar essay attacking Australian cultural values. Mood was also measured following the worldview threat, to test whether the increase in death thoughts following worldview threat were due to other causes, for example, anger at the attack on one's cultural worldview. No significant changes on mood scales were found immediately following the worldview threat.\n\nTo test the generalisability of these findings to groups and worldviews other than those of nationalistic Canadians, Schimel \"et al\" conducted a similar experiment on a group of religious individuals whose worldview included that of creationism. Participants were asked to read an essay which argued in support of the theory of evolution, following which the same measure of DTA was taken as for the Canadian group. Religious participants with a creationist worldview were found to have a significantly higher level of death-thought accessibility than those of the control group.\n\nGoldenberg \"et al\" found that highlighting the similarities between humans and other animals increases death-thought accessibility, as does attention to the physical rather than meaningful qualities of sex.\n\nThe term World View denotes a comprehensive set of opinions, seen as an organic unity, about the world as the medium and exercise of human existence. World View serves as a framework for generating various dimensions of human perception and experience like knowledge, politics, economics, religion, culture, science and ethics. For example, worldview of causality as \"uni-directional\", \"cyclic\", or \"spiral\" generates a framework of the world that reflects these systems of causality.\n\nAn unidirectional view of causality is present in some monotheistic views of the world with a beginning and an end and a single great force with a single end (e.g., Christianity and Islam), while a cyclic worldview of causality is present in religious traditions which are cyclic and seasonal and wherein events and experiences recur in systematic patterns (e.g., Zoroastrianism, Mithraism and Hinduism). These worldviews of causality not only underlie religious traditions but also other aspects of thought like the purpose of history, political and economic theories, and systems like democracy, authoritarianism, anarchism, capitalism, socialism and communism.\n\nThe worldview of a linear and non-linear causality generates various related/conflicting disciplines and approaches in scientific thinking. The \"Weltanschauung\" of the temporal contiguity of act and event leads to underlying diversifications like \"determinism\" vs. \"free will\". A worldview of free will leads to disciplines that are governed by simple laws that remain constant and are static and empirical in scientific method, while a worldview of determinism generates disciplines that are governed with generative systems and rationalistic in scientific method.\n\nSome forms of philosophical naturalism and materialism reject the validity of entities inaccessible to natural science. They view the scientific method as the most reliable model for building an understanding of the world.\n\nNishida Kitaro wrote extensively on \"the Religious Worldview\" in exploring the philosophical significance of Eastern religions.\n\nAccording to Neo-Calvinist David Naugle's \"World view: The History of a Concept\", \"Conceiving of Christianity as a worldview has been one of the most significant developments in the recent history of the church.\"\n\nThe Christian thinker James W. Sire defines a worldview as \"a commitment, a fundamental orientation of the heart, that can be expressed as a story or in a set of presuppositions (assumptions which may be true, partially true, or entirely false) which we hold (consciously or subconsciously, consistently or inconsistently) about the basic construction of reality, and that provides the foundation on which we live and move and have our being.\" He suggests that \"we should all think in terms of worldviews, that is, with a consciousness not only of our own way of thought but also that of other people, so that we can first understand and then genuinely communicate with others in our pluralistic society.\"\n\nThe commitment mentioned by James W. Sire can be extended further. The worldview increases the commitment to serve the world. With the change of a person's view towards the world, he/she can be motivated to serve the world. This serving attitude has been illustrated by Tareq M Zayed as the 'Emancipatory Worldview' in his writing \"History of emancipatory worldview of Muslim learners\".\n\nThe question mentioned above - on whether the super-smart machines, that is, any \"superintelligences\", as expected by some, could have worldviews - is interesting in this context and this would influence human worldviews. \n\nThe philosophical importance of worldviews became increasingly clear during the 20th century for a number of reasons, such as increasing contact between cultures, and the failure of some aspects of the Enlightenment project, such as the rationalist project of attaining all truth by reason alone. Mathematical logic showed that fundamental choices of axioms were essential in deductive reasoning and that, even having chosen axioms not everything that was true in a given logical system could be proven. Some philosophers believe the problems extend to \"the inconsistencies and failures which plagued the Enlightenment attempt to identify universal moral and rational principles\"; although Enlightenment principles such as universal suffrage and the universal declaration of human rights are accepted, if not taken for granted, by many.\n\nThe theory of relativity offers a Weltanschauung that is revolting to absolute space and time, yet provides a context for modern theories of electromagnetism and gravity. In a book review for a new undergraduate textbook on relativity by Wolfgang Rindler, Kenneth Jacobs noted that \"during the post-Sputnik era, special relativity began to take its rightful place in the undergraduate curriculum\". On the adoption of the Weltanschauung, he notes, \"The historical impact of any world picture is ... partly attributable to the zeal of the promulgators and to the efficacy of their teachings.\"\n\nPhilosophers also distinguish the manifest image from the scientific image. These phrases are due to the American 20th century philosopher Wilfrid Sellars. This is one angle on the ancient philosophical distinction between appearance and reality which is particularly pertinent to everyday contemporary living. Indeed, many believe that the scientific image, with its reductionist methodology, will undermine our sense of individual freedom and responsibility. So, many worry that as science advances, particularly cognitive neuroscience, we will be dehumanized. This certainly has powerful Nietzschean undertones. When our immediately given, manifest (sc. obvious) self-conception is shaken, what is lost for the individual and society? And does it have to be that way? Some questions well worth working on, then, are those concerning the refinement of the manifest view of such centrally important concepts such as free will, the self and individuality, and the possibility of real or lived meaning.\n\nWhile Leo Apostel and his followers clearly hold that individuals can construct worldviews, other writers regard worldviews as operating at a community level, or in an unconscious way. For instance, if one's worldview is fixed by one's language, as according to a strong version of the Sapir–Whorf hypothesis, one would have to learn or invent a new language in order to construct a new worldview.\n\nAccording to Apostel, a worldview is an ontology, or a descriptive model of the world. It should comprise these six elements:\n\nFrom across the world across all of the cultures, Roland Muller has suggested that cultural world views can be broken down into three separate world views. It is not simple enough to say that each person is one of these three cultures. Instead, each individual is a mix of the three. For example, a person may be raised in a Power–Fear society, in an Honor–Shame family, and go to school under a Guilt–Innocence system.\n\nIn a Guilt–Innocence focused culture, schools focus on deductive reasoning, cause and effect, good questions, and process. Issues are often seen as black and white. Written contracts are paramount. Communication is direct, and can be blunt.\n\nSocieties with a predominantly Honor–Shame worldview teach children to make honorable choices according to the situations they find themselves in. Communication, interpersonal interaction, and business dealings are very relationship-driven, with every interaction having an effect on the Honor–Shame status of the participants. In an Honor–Shame society the crucial objective is to avoid shame and to be viewed honorably by other people. The Honor–Shame paradigm is especially strong in most regions of Asia.\n\nSome cultures can be seen very clearly in operating under a Power–Fear worldview. In these cultures it is very important to assess the people around you and know where they fall in line according to their level of power. This can be used for good or for bad. A benevolent king rules with power and his citizens fully support him wielding that power. On the converse, a ruthless dictator can use his power to create a culture of fear where his citizens are oppressed.\n\nAccording to Michael Lind, \"a worldview is a more or less coherent understanding of the nature of reality, which permits its holders to interpret new information in light of their preconceptions. Clashes among worldviews cannot be ended by a simple appeal to facts. Even if rival sides agree on the facts, people may disagree on conclusions because of their different premises.\" This is why politicians often seem to talk past one another, or ascribe different meanings to the same events. Tribal or national wars are often the result of incompatible worldviews. Lind has organized American political worldviews into five categories:\nLind argues that even though not all people will fit neatly into only one category or the other, their core worldview shape how they frame their arguments.\n\nOne can think of a worldview as comprising a number of basic beliefs which are philosophically equivalent to the axioms of the worldview considered as a logical theory. These basic beliefs cannot, by definition, be proven (in the logical sense) within the worldview precisely because they are axioms, and are typically argued \"from\" rather than argued \"for\". However their coherence can be explored philosophically and logically.\n\nIf two different worldviews have sufficient common beliefs it may be possible to have a constructive dialogue between them.\n\nOn the other hand, if different worldviews are held to be basically incommensurate and irreconcilable, then the situation is one of cultural relativism and would therefore incur the standard criticisms from philosophical realists.\nAdditionally, religious believers might not wish to see their beliefs relativized into something that is only \"true for them\".\nSubjective logic is a belief-reasoning formalism where beliefs explicitly are subjectively held by individuals but where a consensus between different worldviews can be achieved.\n\nA third alternative sees the worldview approach as only a methodological relativism, as a suspension judgment about the truth of various belief systems but not a declaration that there is no global truth. For instance, the religious philosopher Ninian Smart begins his \"Worldviews: Cross-cultural Explorations of Human Beliefs\" with \"Exploring Religions and Analysing Worldviews\" and argues for \"the neutral, dispassionate study of different religious and secular systems—a process I call worldview analysis.\"\n\nThe comparison of religious, philosophical or scientific worldviews is a delicate endeavor, because such worldviews start from different presuppositions and cognitive values. Clément Vidal has proposed metaphilosophical criteria for the comparison of worldviews, classifying them in three broad categories:\n\n\nDavid Bell has raised interesting questions on worldviews for the designers of superintelligences – machines much smarter than humans.  'Would they need worldviews, where would they get their worldviews and what would they be like?'. The answers would have to relate to, for example, Christian worldviews. Some of the people who consider features of superintelligences say they will have characteristics that are often associated with divinity, raising big open questions for Christian believers. For example, very advanced machines could, perhaps,   ultimately engender in people a terrified reverence and mystical awe in the light of, say, an artificial agent's impressive understanding of the human condition. And perhaps some humans might even be induced to 'worship and serve the creature rather than the Creator'? On the other hand what would the agent's relationship to God be? Anyone attempting to accommodate concepts such as an omnipotent, personal creator's sacrificial, emotional, spiritual and attitudinal demands being made of any man-made entity, superintelligent or not, could be said to have strayed into \"terra prohibita\" theologically, of course. And how would the worldviews of any superintelligences handle the relationships with what it might regard as its \"human\" 'creator'? \n\n"}
