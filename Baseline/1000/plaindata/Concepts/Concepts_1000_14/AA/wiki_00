{"id": "27658583", "url": "https://en.wikipedia.org/wiki?curid=27658583", "title": "AeroVironment Global Observer", "text": "AeroVironment Global Observer\n\nThe AeroVironment Global Observer is a concept for a high-altitude, long endurance unmanned aerial vehicle, designed by AeroVironment (AV) to operate as a stratospheric geosynchronous satellite system with regional coverage.\n\nTwo Global Observer aircraft, each flying for up to a week at an altitude of , could alternate coverage over any area on the earth, providing a platform for communications relays, remote sensing, or long-term surveillance. In addition to flying above weather and above other conventional aircraft, operation at this altitude permits communications and sensor payloads on the aircraft to service an area on the surface of the earth up to in diameter, equivalent to more than of coverage. Global Observer may offer greater flexibility than a satellite and longer duration than conventional manned and unmanned aircraft.\n\nThe Global Observer Joint Capabilities Technology Demonstration (JCTD) program had the goal of helping solve the capability gap in persistent ISR and communications relay for the US military and homeland security. The Global Observer JCTD demonstrated a new stratospheric, extreme endurance UAS that could be transitioned for post-JCTD development, extended user evaluation, and fielding. The program was a joint effort with the U.S. Department of Defense, Department of Homeland Security, and AeroVironment that started in September 2007, to culminate in a Joint Operational Utility Assessment (JOUA) in 2011.\n\nThe program provided for the system development, production of two aircraft, development flight testing, and JOUA with ISR and communications relay payload. The flight testing and JOUA was conducted at the Air Force Flight Test Center at Edwards Air Force Base, California. The primary objectives of the Global Observer JCTD Program were:\n\n\n\n\nHigh-altitude, long endurance unmanned aerial vehicles, such as Global Observer, may enable several capabilities that enable rapid and effective actions or countermeasures:\n\nA Global Observer prototype, called \"Odyssey,\" flew in May 2005. It had a , one-third the size of the planned full-sized version, and ran solely on hydrogen fuel-cells powering electric motors that drove eight propellers, flying the aircraft for several hours. The JCTD started in September 2007. In August 2010, Aerovironment announced that the full-sized Global Observer wing had passed wing load testing. The 53 m (175 ft) all-composite wing, which comes in five sections and is designed to maximize wing strength while minimizing weight, had loads applied to it that approximated the maximum loads it is designed to withstand during normal flight, turbulence and maneuvers. In its third year of testing, the demonstrator had also undergone ground and taxi tests as well as taken a \"short hop\" lifting off the ground briefly during taxiing.\n\nThe Global Observer performed its first flight on 5 August 2010, taking off from Edwards AFB and reaching an altitude of for one hour. The flight was performed using battery power.The aircraft completed initial flight testing, consisting of multiple low-altitude flights, at Edwards AFB in August and September 2010. This phase used batteries to power the hybrid-electric aircraft and approximate full aircraft weight and center of gravity for flight control, performance, and responsiveness evaluation. Following this, the program team installed and ground tested the aircraft's hydrogen-fueled generator and liquid hydrogen fuel tanks which will power it for up to a week in the stratosphere.\n\nThe first flight of the Global Observer using hydrogen fuel occurred on 11 January 2011, reaching an altitude of for four hours. On 1 April 2011, Global Observer-1 (GO-1), the first aircraft to be completed, crashed 18 hours into its 9th test flight. AeroVironment said it was undergoing flight test envelope expansion and had been operating for nearly twice the endurance and at a higher altitude than previous flights when the crash occurred. At the time, the second aircraft developed as part of the JCTD program was nearing completion at a company facility; the $140 million program was originally scheduled for completion in late 2011, but the crash delayed this by a year. AeroVironment was looking for sources of incremental funding to provide a bridge between the demonstration and a future procurement program.\n\nIn December 2012, the Pentagon closed the development contract for the Global Observer, the reason being the crash in April 2011. The Global Observer was used as a technology demonstration, not a program for a functioning aircraft. In April 2013, the Pentagon stated that no service or defense agency had advocated for it to be a program. AeroVironment is currently in possession of the second prototype Global Observer. On 6 February 2014, AeroVironment announced that it had teamed with Lockheed Martin to sell the Global Observer to international customers. The partnership is focused around building \"atmospheric satellite systems\" around the UAV. The Global Observer may compete for orders with the Boeing Phantom Eye liquid hydrogen-powered long endurance UAV.\n\n\n\n"}
{"id": "6395956", "url": "https://en.wikipedia.org/wiki?curid=6395956", "title": "Analytic–synthetic distinction", "text": "Analytic–synthetic distinction\n\nThe analytic–synthetic distinction (also called the analytic–synthetic dichotomy) is a semantic distinction, used primarily in philosophy to distinguish propositions (in particular, statements that are affirmative subject–predicate judgments) into two types: analytic propositions and synthetic propositions. Analytic propositions are true by virtue of their meaning, while synthetic propositions are true by how their meaning relates to the world. However, philosophers have used the terms in very different ways. Furthermore, philosophers have debated whether there is a legitimate distinction.\n\nThe philosopher Immanuel Kant uses the terms \"analytic\" and \"synthetic\" to divide propositions into two types. Kant introduces the analytic–synthetic distinction in the Introduction to his \"Critique of Pure Reason\" (1781/1998, A6–7/B10–11). There, he restricts his attention to statements that are affirmative subject-predicate judgments and defines \"analytic proposition\" and \"synthetic proposition\" as follows:\n\nExamples of analytic propositions, on Kant's definition, include:\n\nKant's own example is:\n\nEach of these statements is an affirmative subject-predicate judgment, and, in each, the predicate concept is \"contained\" within the subject concept. The concept \"bachelor\" contains the concept \"unmarried\"; the concept \"unmarried\" is part of the definition of the concept \"bachelor\". Likewise, for \"triangle\" and \"has three sides\", and so on.\n\nExamples of synthetic propositions, on Kant's definition, include:\n\nKant's own example is:\n\nAs with the previous examples classified as analytic propositions, each of these new statements is an affirmative subject–predicate judgment. However, in none of these cases does the subject concept contain the predicate concept. The concept \"bachelor\" does not contain the concept \"alone\"; \"alone\" is not a part of the \"definition\" of \"bachelor\". The same is true for \"creatures with hearts\" and \"have kidneys\"; even if every creature with a heart also has kidneys, the concept \"creature with a heart\" does not contain the concept \"has kidneys\".\n\nIn the Introduction to the \"Critique of Pure Reason\", Kant contrasts his distinction between analytic and synthetic propositions with another distinction, the distinction between \"a priori\" and \"a posteriori\" propositions. He defines these terms as follows:\n\nExamples of \"a priori\" propositions include:\n\nThe justification of these propositions does not depend upon experience: one need not consult experience to determine whether all bachelors are unmarried, nor whether . (Of course, as Kant would grant, experience is required to understand the concepts \"bachelor\", \"unmarried\", \"7\", \"+\" and so forth. However, the \"a priori\" / \"a posteriori\" distinction as employed here by Kant refers not to the \"origins\" of the concepts but to the \"justification\" of the propositions. Once we have the concepts, experience is no longer necessary.)\n\nExamples of \"a posteriori\" propositions include:\n\nBoth of these propositions are \"a posteriori\": any justification of them would require one's experience.\n\nThe analytic/synthetic distinction and the \"a priori\" / \"a posteriori\" distinction together yield four types of propositions:\n\nKant posits the third type as obviously self-contradictory. Ruling it out, he discusses only the remaining three types as components of his epistemological frameworkeach, for brevity's sake, becoming, respectively, \"analytic\", \"synthetic a priori\", and \"empirical\" or \"a posteriori\" propositions. This triad will account for all propositions possible.\n\nPart of Kant's argument in the Introduction to the \"Critique of Pure Reason\" involves arguing that there is no problem figuring out how knowledge of analytic propositions is possible. To know an analytic proposition, Kant argued, one need not consult experience. Instead, one needs merely to take the subject and \"extract from it, in accordance with the principle of contradiction, the required predicate\" (A7/B12). In analytic propositions, the predicate concept is contained in the subject concept. Thus, to know an analytic proposition is true, one need merely examine the concept of the subject. If one finds the predicate contained in the subject, the judgment is true.\n\nThus, for example, one need not consult experience to determine whether \"All bachelors are unmarried\" is true. One need merely examine the subject concept (\"bachelors\") and see if the predicate concept \"unmarried\" is contained in it. And in fact, it is: \"unmarried\" is part of the definition of \"bachelor\" and so is contained within it. Thus the proposition \"All bachelors are unmarried\" can be known to be true without consulting experience.\n\nIt follows from this, Kant argued, first: All analytic propositions are \"a priori\"; there are no \"a posteriori\" analytic propositions. It follows, second: There is no problem understanding how we can know analytic propositions; we can know them because we only need to consult our concepts in order to determine that they are true.\n\nAfter ruling out the possibility of analytic \"a posteriori\" propositions, and explaining how we can obtain knowledge of analytic \"a priori\" propositions, Kant also explains how we can obtain knowledge of synthetic \"a posteriori\" propositions. That leaves only the question of how knowledge of synthetic \"a priori\" propositions is possible. This question is exceedingly important, Kant maintains, because all important metaphysical knowledge is of synthetic \"a priori\" propositions. If it is impossible to determine which synthetic \"a priori\" propositions are true, he argues, then metaphysics as a discipline is impossible. The remainder of the \"Critique of Pure Reason\" is devoted to examining whether and how knowledge of synthetic \"a priori\" propositions is possible.\n\nOver a hundred years later, a group of philosophers took interest in Kant and his distinction between analytic and synthetic propositions: the logical positivists.\n\nPart of Kant's examination of the possibility of synthetic \"a priori\" knowledge involved the examination of mathematical propositions, such as\n\nKant maintained that mathematical propositions such as these are synthetic \"a priori\" propositions, and that we know them. That they are synthetic, he thought, is obvious: the concept \"equal to 12\" is not contained within the concept \"7 + 5\"; and the concept \"straight line\" is not contained within the concept \"the shortest distance between two points\". From this, Kant concluded that we have knowledge of synthetic \"a priori\" propositions.\n\nGottlob Frege's notion of analyticity included a number of logical properties and relations beyond containment: symmetry, transitivity, antonymy, or negation and so on. He had a strong emphasis on formality, in particular formal definition, and also emphasized the idea of substitution of synonymous terms. \"All bachelors are unmarried\" can be expanded out with the formal definition of bachelor as \"unmarried man\" to form \"All unmarried men are unmarried\", which is recognizable as tautologous and therefore analytic from its logical form: any statement of the form \"All \"X\" that are (\"F\" and \"G\") are \"F\"\". Using this particular expanded idea of analyticity, Frege concluded that Kant's examples of arithmetical truths are analytical \"a priori\" truths and \"not\" synthetic \"a priori\" truths.\n\nThe logical positivists agreed with Kant that we have knowledge of mathematical truths, and further that mathematical propositions are \"a priori\". However, they did not believe that any complex metaphysics, such as the type Kant supplied, are necessary to explain our knowledge of mathematical truths. Instead, the logical positivists maintained that our knowledge of judgments like \"all bachelors are unmarried\" and our knowledge of mathematics (and logic) are in the basic sense the same: all proceeded from our knowledge of the meanings of terms or the conventions of language.\n\nThus the logical positivists drew a new distinction, and, inheriting the terms from Kant, named it the \"analytic/synthetic distinction\". They provided many different definitions, such as the following:\n\nSynthetic propositions were then defined as:\n\nThese definitions applied to all propositions, regardless of whether they were of subject–predicate form. Thus, under these definitions, the proposition \"It is raining or it is not raining\" was classified as analytic, while for Kant it was analytic by virtue of its logical form. And the proposition \"\" was classified as analytic, while under Kant's definitions it was synthetic.\n\nTwo-dimensionalism is an approach to semantics in analytic philosophy. It is a theory of how to determine the sense and reference of a word and the truth-value of a sentence. It is intended to resolve a puzzle that has plagued philosophy for some time, namely: How is it possible to discover empirically that a necessary truth is true? Two-dimensionalism provides an analysis of the semantics of words and sentences that makes sense of this possibility. The theory was first developed by Robert Stalnaker, but it has been advocated by numerous philosophers since, including David Chalmers and Berit Brogaard.\n\nAny given sentence, for example, the words,\n\nis taken to express two distinct propositions, often referred to as a \"primary intension\" and a \"secondary intension\", which together compose its meaning.\n\nThe primary intension of a word or sentence is its sense, i.e., is the idea or method by which we find its referent. The primary intension of \"water\" might be a description, such as \"watery stuff\". The thing picked out by the primary intension of \"water\" could have been otherwise. For example, on some other world where the inhabitants take \"water\" to mean \"watery stuff\", but, where the chemical make-up of watery stuff is not HO, it is not the case that water is HO for that world.\n\nThe \"secondary intension\" of \"water\" is whatever thing \"water\" happens to pick out in \"this\" world, whatever that world happens to be. So if we assign \"water\" the primary intension \"watery stuff\" then the secondary intension of \"water\" is HO, since HO is \"watery stuff\" in this world. The secondary intension of \"water\" in our world is HO, which is HO in every world because unlike \"watery stuff\" it is impossible for HO to be other than HO. When considered according to its secondary intension, \"Water is HO\" is true in every world.\n\nIf two-dimensionalism is workable it solves some very important problems in the philosophy of language. Saul Kripke has argued that \"Water is HO\" is an example of the \"necessary a posteriori\", since we had to discover that water was HO, but given that it is true, it cannot be false. It would be absurd to claim that something that is water is not HO, for these are known to be \"identical\".\n\nRudolf Carnap was a strong proponent of the distinction between what he called \"internal questions\", questions entertained within a \"framework\" (like a mathematical theory), and \"external questions\", questions posed outside any framework – posed before the adoption of any framework. The \"internal\" questions could be of two types: \"logical\" (or analytic, or logically true) and \"factual\" (empirical, that is, matters of observation interpreted using terms from a framework). The \"external\" questions were also of two types: those that were confused pseudo-questions (\"one disguised in the form of a theoretical question\") and those that could be re-interpreted as practical, pragmatic questions about whether a framework under consideration was \"more or less expedient, fruitful, conducive to the aim for which the language is intended\". The adjective \"synthetic\" was not used by Carnap in his 1950 work \"Empiricism, Semantics, and Ontology\". Carnap did define a \"synthetic truth\" in his work \"Meaning and Necessity\": a sentence that is true, but not simply because \"the semantical rules of the system suffice for establishing its truth\".\n\nThe notion of a synthetic truth is of something that is true both because of what it means and because of the way the world is, whereas analytic truths are true in virtue of meaning alone. Thus, what Carnap calls internal \"factual\" statements (as opposed to internal \"logical\" statements) could be taken as being also synthetic truths because they require \"observations\", but some external statements also could be \"synthetic\" statements and Carnap would be doubtful about their status. The analytic–synthetic argument therefore is not identical with the internal–external distinction.\n\nIn 1951, Willard Van Orman Quine published the essay \"Two Dogmas of Empiricism\" in which he argued that the analytic–synthetic distinction is untenable. The argument at bottom is that there are no \"analytic\" truths, but all truths involve an empirical aspect. In the first paragraph, Quine takes the distinction to be the following:\n\nQuine's position denying the analytic-synthetic distinction is summarized as follows:\nTo summarize Quine's argument, the notion of an analytic proposition requires a notion of synonymy, but establishing synonymy inevitably leads to matters of fact – synthetic propositions. Thus, there is no non-circular (and so no tenable) way to ground the notion of analytic propositions.\n\nWhile Quine's rejection of the analytic–synthetic distinction is widely known, the precise argument for the rejection and its status is highly debated in contemporary philosophy. However, some (for example, Boghossian) argue that Quine's rejection of the distinction is still widely accepted among philosophers, even if for poor reasons.\n\nPaul Grice and P. F. Strawson criticized \"Two Dogmas\" in their 1956 article \"In Defense of a Dogma\". Among other things, they argue that Quine's skepticism about synonyms leads to a skepticism about meaning. If statements can have meanings, then it would make sense to ask \"What does it mean?\". If it makes sense to ask \"What does it mean?\", then synonymy can be defined as follows: Two sentences are synonymous if and only if the true answer of the question \"What does it mean?\" asked of one of them is the true answer to the same question asked of the other. They also draw the conclusion that discussion about correct or incorrect translations would be impossible given Quine's argument. Four years after Grice and Strawson published their paper, Quine's book \"Word and Object\" was released. In the book Quine presented his theory of indeterminacy of translation.\n\nIn \"Speech Acts\", John Searle argues that from the difficulties encountered in trying to explicate analyticity by appeal to specific criteria, it does not follow that the notion itself is void. Considering the way which we would test any proposed list of criteria, which is by comparing their extension to the set of analytic statements, it would follow that any explication of what analyticity means presupposes that we already have at our disposal a working notion of analyticity.\n\nIn \"'Two Dogmas' Revisited\", Hilary Putnam argues that Quine is attacking two different notions:\nAnalytic truth defined as a true statement derivable from a tautology by putting synonyms for synonyms is near Kant's account of analytic truth as a truth whose negation is a contradiction. Analytic truth defined as a truth confirmed no matter what, however, is closer to one of the traditional accounts of \"a priori\". While the first four sections of Quine's paper concern analyticity, the last two concern a priority. Putnam considers the argument in the two last sections as independent of the first four, and at the same time as Putnam criticizes Quine, he also emphasizes his historical importance as the first top rank philosopher to both reject the notion of a priority and sketch a methodology without it.\n\nJerrold Katz, a one-time associate of Noam Chomsky, countered the arguments of \"Two Dogmas\" directly by trying to define analyticity non-circularly on the syntactical features of sentences.\n\nIn \"Philosophical Analysis in the Twentieth Century, Volume 1 : The Dawn of Analysis\", Scott Soames has pointed out that Quine's circularity argument needs two of the logical positivists' central theses to be effective:\n\nIt is only when these two theses are accepted that Quine's argument holds. It is not a problem that the notion of necessity is presupposed by the notion of analyticity if necessity can be explained without analyticity. According to Soames, both theses were accepted by most philosophers when Quine published \"Two Dogmas\". Today, however, Soames holds both statements to be antiquated. He says: \"Very few philosophers today would accept either [of these assertions], both of which now seem decidedly antique.\"\n\nPhilosopher Leonard Peikoff, in his essay \"The Analytic-Synthetic Dichotomy\", expands upon Ayn Rand's analysis. He posits that:\n\nThe theory of the analytic-synthetic dichotomy presents men with the following choice: If your statement is proved, it says nothing about that which exists; if it is about existents, it cannot be proved. If it is demonstrated by logical argument, it represents a subjective convention; if it asserts a fact, logic cannot establish it. If you validate it by an appeal to the meanings of your \"concepts\", then it is cut off from reality; if you validate it by an appeal to your \"percepts\", then you cannot be certain of it.\n\nTo Peikoff, the critical question is: What is included in the meaning of a concept? He rejects the idea that some of the characteristics of a concept's referents are excluded from the concept. Applying Rand's theory that a concept is a \"mental integration\" of similar existents, treated as \"units\", he argues that concepts stand for and mean the actual existents, including all their characteristics, not just those used to pick out the referents or define the concept. He states,\n\nSince a concept is an integration of units, it has no content or meaning apart from its units. The meaning of a concept consists of the units — the existents — which it integrates, including all the characteristics of these units... The fact that certain characteristics are, at a given time, unknown to man, does not indicate that these characteristics are excluded from the entity — or from the concept.\n\nFurthermore, he argues that there is no valid distinction between \"necessary\" and \"contingent\" facts, and that all truths are learned and validated by the same process: the application of logic to perceptual data. Associated with the analytic-synthetic dichotomy are a cluster of other divisions that Objectivism also regards as false and artificial, such as logical truth vs. factual truth, logically possible vs. empirically possible, and a priori vs. the a posteriori.\n\n\n\n"}
{"id": "3280462", "url": "https://en.wikipedia.org/wiki?curid=3280462", "title": "Belief–desire–intention model", "text": "Belief–desire–intention model\n\nThe belief–desire–intention (BDI) model of human practical reasoning was developed by Michael Bratman as a way of explaining future-directed intention.\n\nBDI is fundamentally reliant on folk psychology (the 'theory theory'), which is the notion that our mental models of the world are theories. It was used as a basis for developing the belief–desire–intention software model.\n\nBDI was part of the inspiration behind the BDI software architecture, which Bratman was also involved in developing. Here, the notion of intention was seen as a way of limiting time spent on deliberating about what to do, by eliminating choices inconsistent with current intentions.\n\nBDI has also aroused some interest in psychology. BDI formed the basis for a computational model of childlike reasoning CRIBB. It has been proposed that autistic children do not recognise other people as folk-psychological agents (i.e., agents with their own beliefs, etc.). BDI has been used to develop a rehabilitation strategy to teach autistic children to reason about other people.\n\n"}
{"id": "6978", "url": "https://en.wikipedia.org/wiki?curid=6978", "title": "Concept", "text": "Concept\n\nConcepts are mental representations, abstract objects or abilities that make up the fundamental building blocks of thoughts and beliefs. They play an important role in all aspects of cognition.\n\nIn contemporary philosophy, there are at least three prevailing ways to understand what a concept is:\n\n\nConcepts can be organized into a hierarchy, higher levels of which are termed \"superordinate\" and lower levels termed \"subordinate\". Additionally, there is the \"basic\" or \"middle\" level at which people will most readily categorize a concept. For example, a basic-level concept would be \"chair\", with its superordinate, \"furniture\", and its subordinate, \"easy chair\".\n\nA concept is instantiated (reified) by all of its actual or potential instances, whether these are things in the real world or other ideas.\n\nConcepts are studied as components of human cognition in the cognitive science disciplines of linguistics, psychology and philosophy, where an ongoing debate asks whether all cognition must occur through concepts. Concepts are used as formal tools or models in mathematics, computer science, databases and artificial intelligence where they are sometimes called classes, schema or categories. In informal use the word \"concept\" often just means any idea.\n\nWithin the framework of the representational theory of mind, the structural position of concepts can be understood as follows: Concepts serve as the building blocks of what are called \"mental representations\" (colloquially understood as \"ideas in the mind\"). Mental representations, in turn, are the building blocks of what are called \"propositional attitudes\" (colloquially understood as the stances or perspectives we take towards ideas, be it \"believing\", \"doubting\", \"wondering\", \"accepting\", etc.). And these propositional attitudes, in turn, are the building blocks of our understanding of thoughts that populate everyday life, as well as folk psychology. In this way, we have an analysis that ties our common everyday understanding of thoughts down to the scientific and philosophical understanding of concepts.\n\nA central question in the study of concepts is the question of what concepts \"are\". Philosophers construe this question as one about the ontology of concepts – what they are really like. The ontology of concepts determines the answer to other questions, such as how to integrate concepts into a wider theory of the mind, what functions are allowed or disallowed by a concept's ontology, etc. There are two main views of the ontology of concepts: (1) Concepts are abstract objects, and (2) concepts are mental representations.\n\nPlatonist views of the mind construe concepts as abstract objects,\n\nThere is debate as to the relationship between concepts and natural language. However, it is necessary at least to begin by understanding that the concept \"dog\" is philosophically distinct from the things in the world grouped by this concept – or the reference class or extension. Concepts that can be equated to a single word are called \"lexical concepts\".\n\nStudy of concepts and conceptual structure falls into the disciplines of linguistics, philosophy, psychology, and cognitive science.\n\nIn the simplest terms, a concept is a name or label that regards or treats an abstraction as if it had concrete or material existence, such as a person, a place, or a thing. It may represent a natural object that exists in the real world like a tree, an animal, a stone, etc. It may also name an artificial (man-made) object like a chair, computer, house, etc. Abstract ideas and knowledge domains such as freedom, equality, science, happiness, etc., are also symbolized by concepts. It is important to realize that a concept is merely a symbol, a representation of the abstraction. The word is not to be mistaken for the thing. For example, the word \"moon\" (a concept) is not the large, bright, shape-changing object up in the sky, but only \"represents\" that celestial object. Concepts are created (named) to describe, explain and capture reality as it is known and understood.\n\nKant declared that human minds possess pure or \"a priori\" concepts. Instead of being abstracted from individual perceptions, like empirical concepts, they originate in the mind itself. He called these concepts categories, in the sense of the word that means predicate, attribute, characteristic, or quality. But these pure categories are predicates of things \"in general\", not of a particular thing. According to Kant, there are twelve categories that constitute the understanding of phenomenal objects. Each category is that one predicate which is common to multiple empirical concepts. In order to explain how an \"a priori\" concept can relate to individual phenomena, in a manner analogous to an \"a posteriori\" concept, Kant employed the technical concept of the schema. He held that the account of the concept as an abstraction of experience is only partly correct. He called those concepts that result from abstraction \"a posteriori concepts\" (meaning concepts that arise out of experience). An empirical or an \"a posteriori\" concept is a general representation (\"Vorstellung\") or non-specific thought of that which is common to several specific perceived objects (Logic, I, 1., §1, Note 1)\n\nA concept is a common feature or characteristic. Kant investigated the way that empirical \"a posteriori\" concepts are created.\nIn cognitive linguistics, abstract concepts are transformations of concrete concepts derived from embodied experience. The mechanism of transformation is structural mapping, in which properties of two or more source domains are selectively mapped onto a blended space (Fauconnier & Turner, 1995; see conceptual blending). A common class of blends are metaphors. This theory contrasts with the rationalist view that concepts are perceptions (or \"recollections\", in Plato's term) of an independently existing world of ideas, in that it denies the existence of any such realm. It also contrasts with the empiricist view that concepts are abstract generalizations of individual experiences, because the contingent and bodily experience is preserved in a concept, and not abstracted away. While the perspective is compatible with Jamesian pragmatism, the notion of the transformation of embodied concepts through structural mapping makes a distinct contribution to the problem of concept formation.\n\nPlato was the starkest proponent of the realist thesis of universal concepts. By his view, concepts (and ideas in general) are innate ideas that were instantiations of a transcendental world of pure forms that lay behind the veil of the physical world. In this way, universals were explained as transcendent objects. Needless to say this form of realism was tied deeply with Plato's ontological projects. This remark on Plato is not of merely historical interest. For example, the view that numbers are Platonic objects was revived by Kurt Gödel as a result of certain puzzles that he took to arise from the phenomenological accounts.\n\nGottlob Frege, founder of the analytic tradition in philosophy, famously argued for the analysis of language in terms of sense and reference. For him, the sense of an expression in language describes a certain state of affairs in the world, namely, the way that some object is presented. Since many commentators view the notion of sense as identical to the notion of concept, and Frege regards senses as the linguistic representations of states of affairs in the world, it seems to follow that we may understand concepts as the manner in which we grasp the world. Accordingly, concepts (as senses) have an ontological status (Morgolis:7).\n\nAccording to Carl Benjamin Boyer, in the introduction to his \"The History of the Calculus and its Conceptual Development\", concepts in calculus do not refer to perceptions. As long as the concepts are useful and mutually compatible, they are accepted on their own. For example, the concepts of the derivative and the integral are not considered to refer to spatial or temporal perceptions of the external world of experience. Neither are they related in any way to mysterious limits in which quantities are on the verge of nascence or evanescence, that is, coming into or going out of existence. The abstract concepts are now considered to be totally autonomous, even though they originated from the process of abstracting or taking away qualities from perceptions until only the common, essential attributes remained.\n\nIn a physicalist theory of mind, a concept is a mental representation, which the brain uses to denote a class of things in the world. This is to say that it is literally, a symbol or group of symbols together made from the physical material of the brain. Concepts are mental representations that allow us to draw appropriate inferences about the type of entities we encounter in our everyday lives. Concepts do not encompass all mental representations, but are merely a subset of them. The use of concepts is necessary to cognitive processes such as categorization, memory, decision making, learning, and inference.\n\nConcepts are thought to be stored in long term cortical memory, in contrast to episodic memory of the particular objects and events which they abstract, which are stored in hippocampus. Evidence for this separation comes from hippocampal damaged patients such as patient HM. The abstraction from the day's hippocampal events and objects into cortical concepts is often considered to be the computation underlying (some stages of) sleep and dreaming. Many people (beginning with Aristotle) report memories of dreams which appear to mix the day's events with analogous or related historical concepts and memories, and suggest that they were being sorted or organised into more abstract concepts. (\"Sort\" is itself another word for concept, and \"sorting\" thus means to organise into concepts.)\n\nThe classical theory of concepts, also referred to as the empiricist theory of concepts, is the oldest theory about the structure of concepts (it can be traced back to Aristotle), and was prominently held until the 1970s. The classical theory of concepts says that concepts have a definitional structure. Adequate definitions of the kind required by this theory usually take the form of a list of features. These features must have two important qualities to provide a comprehensive definition. Features entailed by the definition of a concept must be both \"necessary\" and \"sufficient\" for membership in the class of things covered by a particular concept. A feature is considered necessary if every member of the denoted class has that feature. A feature is considered sufficient if something has all the parts required by the definition. For example, the classic example \"bachelor\" is said to be defined by \"unmarried\" and \"man\". An entity is a bachelor (by this definition) if and only if it is both unmarried and a man. To check whether something is a member of the class, you compare its qualities to the features in the definition. Another key part of this theory is that it obeys the \"law of the excluded middle\", which means that there are no partial members of a class, you are either in or out.\n\nThe classical theory persisted for so long unquestioned because it seemed intuitively correct and has great explanatory power. It can explain how concepts would be acquired, how we use them to categorize and how we use the structure of a concept to determine its referent class. In fact, for many years it was one of the major activities in philosophy – concept analysis. Concept analysis is the act of trying to articulate the necessary and sufficient conditions for the membership in the referent class of a concept. For example, Shoemaker's classic \"Time Without Change\" explored whether the concept of the flow of time can include flows where no changes take place, though change is usually taken as a definition of time.\n\nGiven that most later theories of concepts were born out of the rejection of some or all of the classical theory, it seems appropriate to give an account of what might be wrong with this theory. In the 20th century, philosophers such as Wittgenstein and Rosch argued against the classical theory. There are six primary arguments summarized as follows:\n\nPrototype theory came out of problems with the classical view of conceptual structure. Prototype theory says that concepts specify properties that members of a class tend to possess, rather than must possess. Wittgenstein, Rosch, Mervis, Berlin, Anglin, and Posner are a few of the key proponents and creators of this theory. Wittgenstein describes the relationship between members of a class as \"family resemblances\". There are not necessarily any necessary conditions for membership, a dog can still be a dog with only three legs. This view is particularly supported by psychological experimental evidence for prototypicality effects. Participants willingly and consistently rate objects in categories like 'vegetable' or 'furniture' as more or less typical of that class. It seems that our categories are fuzzy psychologically, and so this structure has explanatory power. We can judge an item's membership to the referent class of a concept by comparing it to the typical member – the most central member of the concept. If it is similar enough in the relevant ways, it will be cognitively admitted as a member of the relevant class of entities. Rosch suggests that every category is represented by a central exemplar which embodies all or the maximum possible number of features of a given category. According to Lech, Gunturkun, and Suchan explain that categorization involves many areas of the brain, some of these are; visual association areas, prefrontal cortex, basal ganglia, and temporal lobe.\n\nTheory-theory is a reaction to the previous two theories and develops them further. This theory postulates that categorization by concepts is something like scientific theorizing. Concepts are not learned in isolation, but rather are learned as a part of our experiences with the world around us. In this sense, concepts' structure relies on their relationships to other concepts as mandated by a particular mental theory about the state of the world. How this is supposed to work is a little less clear than in the previous two theories, but is still a prominent and notable theory. This is supposed to explain some of the issues of ignorance and error that come up in prototype and classical theories as concepts that are structured around each other seem to account for errors such as whale as a fish (this misconception came from an incorrect theory about what a whale is like, combining with our theory of what a fish is). When we learn that a whale is not a fish, we are recognizing that whales don't in fact fit the theory we had about what makes something a fish. In this sense, the Theory–Theory of concepts is responding to some of the issues of prototype theory and classic theory.\n\nAccording to the theory of ideasthesia (or \"sensing concepts\"), activation of a concept may be the main mechanism responsible for creation of phenomenal experiences. Therefore, understanding how the brain processes concepts may be central to solving the mystery of how conscious experiences (or qualia) emerge within a physical system e.g., the sourness of the sour taste of lemon. This question is also known as the hard problem of consciousness. Research on ideasthesia emerged from research on synesthesia where it was noted that a synesthetic experience requires first an activation of a concept of the inducer. Later research expanded these results into everyday perception.\n\nThere is a lot of discussion on the most effective theory in concepts. Another theory is semantic pointers, which use perceptual and motor representations and these representations are like symbols.\n\nThe term \"concept\" is traced back to 1554–60 (Latin \"\" – \"something conceived\").\n\n"}
{"id": "42415226", "url": "https://en.wikipedia.org/wiki?curid=42415226", "title": "Conceptual combination", "text": "Conceptual combination\n\nConceptual combination is a fundamental cognitive process by which two or more existing basic concepts are mentally synthesized to generate a composite, higher-order concept. The products of this process are sometimes referred to as \"complex concepts.\" Combining concepts allows individuals to use a finite number of concepts which they already understand to construct a potentially limitless quantity of new, related concepts. It is an essential component of many abilities, such as perception, language, synthetic reasoning, creative thought and abstraction.\n\nConceptual combination is an important concept in the fields of cognitive psychology and cognitive science.\n\nThe mechanism by which conceptual combination occurs is debatable, both on cognitive and neurological levels. As such, multiple models have been developed or applied to better define how the process occurs.\n\nCognitive models attempt to functionally outline the mental computation involved in conceptual combination. \n\nConstraint theory stipulates that the concept that results from an attempt at conceptual combination is controlled by three constraints known as diagnosticity, plausibility and informativeness. \"Diagnosticity\" refers to the a complex concept's possession of the defining properties of its component simple concepts. Because such properties are diagnostic of the component concepts, at least some of them should be diagnostic of the higher-order representations constructed from those concepts. \"Plausibility\" refers to consistency with existing knowledge and prior experience. It is based on the assumption that a complex concept should be reasonably relevant to the context in which it is used. This assumption makes the most sense in a practical, linguistic context, particularly when a speaker is catering to the understanding of the listener. \"Informativeness\" is the property of having more meaning or properties than any individual component. If a complex concept were not distinguishable from any given component, it would be identical to that component. Because nothing can logically be both a component of something and the totality of something simultaneously, a complex concept must at least be the sum of its parts. Many argue that the interaction among component concepts should allow a complex concept to be greater than that sum. If multiple possible ways to structure or interpret a complex concept exist, the one which best satisfies or most satisfies these constraints is the one which will be used. The paradigm upon which constraint theory is based is computational, and therefore views the mind as a processor which operates on the basis of standard problem-solving protocols (i.e. algorithms and heuristics).\n\nThe spreading activation model is a model in connectionist theory sometimes designed to represent how concepts are activated in relation to one another. Though it is typically applied to information search processes like recognition, brainstorming, and recall, it can be used to explain how concepts are combined as well as connected.\n\nSpreading activation models represent memory and knowledge as a network of interlinked concepts. Every concept manifests as a node within this network, with related nodes/concepts linked to one another with connections. Concepts that are more strongly associated with one another either in terms of content or an individual's past experience are correspondingly more strongly linked.\n\nWhen one concept is employed in working memory, the corresponding node is also activated. This activation spreads through the node's links, making it easier to activate nodes to which the activated node is connected. This spreading activation stimulates the linked nodes, pressuring them to activate to an extent proportional to the strength of the connection between the stimulating node and the stimulated node. If sufficient net stimulation is accrued through a stimulated node's links, it will also activate. Thus, being connected to an activated node makes it easier for an inactive node to become active as well; concepts become more readily accessed when individuals are stimulated with related concepts first. This increase in ease of access is known as \"priming.\"\n\nSpreading activation models tend to imply that processing concepts occurs in series; that is, each concept is processed one-at-a-time, one after the other. As such, individuals tend to combine concepts more readily, easily, and quickly if they are more closely linked within the network of concepts. This implication, however, has caused spreading activation to come under a great deal of criticism, particularly with respect to how the concept is employed in feature theories.\n\nThe features and properties of complex concepts are generally assumed to be derived from the simple concepts that compose them. One popularly discussed model involves a two-stage serial process. In the initial stage, features from each of the component simple concepts are retrieved from memory through spreading activation. This allows a complex concept to accrue features with existing relationships with its component simple concepts. During this stage, the basic definition of what the complex concept is and/or means is generates. In the second stage, knowledge and reasoning are employed upon the features accrued in the previous stage to generate further features. For example, one might reason that the complex concept \"white jacket,\" if worn in a blizzard, would make one difficult to see; it would follow that one should ascribe the property of \"good for winter camouflage,\" despite the fact that this property is not closely attached to the component concepts \"white\" nor \"jacket.\" This stage is especially useful when properties of complex concepts contradict those of their component concepts, such as the different colours of milk and chocolate milk.\n\nThis model, however, has come under criticism due to its inconsistency with empirical studies. If conceptual combination employed serial spreading activation, for instance, it should take longer to verify the properties of complex concepts, as they necessarily possess more concepts than their component simple concepts. Research has nonetheless shown that it takes less time to confirm complex concepts' properties than their components and about equal time to reject false properties for both. This occurred even when the properties of the complex concept contradicted those of the simple concepts. Likewise, when experiment participants are exposed to a set of features first, and then asked to verify whether or not they correspond to simple or complex concepts, the participants tend to provide correct verification answers for the complex concepts more quickly.\n\nThe neurological basis of conceptual combination has received considerably less attention than its cognitive basis. Nevertheless, research has revealed several specific brain regions that are intimately involved if not entirely responsible for neural processing involved in conceptual combination.\n\nOf particular relevance is the left anterior temporal lobe. Studies have previously demonstrated an additive effect for stimulation in this subsection of neural cortex tissue. When experiment participants were verbally presented with certain simple concepts, the processing of the information causes electrical stimulation in the region. When the same participants were verbally presented with a single complex concept formed from the combination of the aforementioned simple concepts, the stimulation recorded was equivalent to the sum of the stimulation that resulted from each individual component simple concept. In other words, the stimulation caused by a complex concept is equivalent to the total stimulation caused by its component concepts. More recent data contradicts those results by indicating a multiplicative effect in which the activation caused by a complex concept is the product of the activation levels caused by its component concepts, rather than the sum.\n\nFurther support for the role of the left anterior temporal lobe has been previously established through neuropsychological studies. Semantic dementia is a disorder in which conceptual manipulation, including conceptual combination, is hindered. These indicate that the neural damage associated with semantic dementia occurs within this brain region. Unfortunately, neuropsychological studies that attempt to replicate this pattern have failed, leading uncertainty as to whether initial results were valid.\n\nAs language is the means by which concepts are communicated and expressed, the processed involved in linguistic expression and interpretation are heavily intertwined with combined concepts. Many theories of concept combination mechanisms, including constraint theory were developed within the context of language, and therefore make more sense when applied in a linguistic context. Study into the linguistic aspects of concept combination as has generally been focused on the interpretation mechanism.\n\nA concept that can be expressed using a single word is called a \"lexical concept.\" A lexical concept is usually treated as a basic concept, although it can just as easily be a complex concept.\n\nTwo lexical concepts are often used together as phrases to represent a combined concept of greater specificity. This is most readily seen in the use of adjectives to modify nouns and the use of adverbs to modify verbs and adjectives. Consider, for example, phrases such as \"burnt toast,\" \"eat roughly,\" and \"readily loved.\" Multiple noun lexical concepts can also be used together in order to represent combined concepts. Through this process, a limited pool of nouns can be used to produce an exponentially larger pool of phrases such as \"sound wave,\" \"video game,\" and \"sleeping pill.\"\n\nIn addition to constraint theory, there are two principal theories surrounding the mechanism by which noun-noun combinations are interpreted. The first of these is \"dual-process theory.\" Dual-process theory proposed that there are two means by which people interpreted noun-noun phrases. \"Relational interpretation\" attempts to establish a relationship between the nouns and interprets the combined phrase in terms of that relationship. For example, one might relationally interpret the phrase \"snake mouse\" to refer to a mouse meant to be eaten by snakes, as the two concepts have a predatory relationship. \"Property interpretation\" identifies properties associated with the first noun and then applies them onto the second noun. In this case the phrase \"snake mouse\" might be interpreted as a mouse with poisonous fangs or an elongated body.\n\nThe second principal theory is known as the \"Competition in Relations among Nominals\" theory. It states that the assumed modification effect of a noun on its partner in a novel noun-noun combination is the one which it has been seen to employ most often in the past. For example, \"chocolate cat\" is usually interpreted as \"a cat made of chocolate\" rather than \"a chocolate-eating cat\" simply because the \"made of\" modifier is heavily conditioned to be associated with \"chocolate.\"\n\nExplanations of linguistic expression of complex concepts have been linked to spreading activation models. When an individual identifies a lexical concept through vision or hearing, the corresponding node in that individual's cognitive network is said to activate. This makes it easier for lexical concepts linked to the activated concept to be comprehended, as they are primed. This is consistent with current empirical data, which shows that when individuals are interpreting sentences, they process the linguistic content more quickly when several related words follow one another. In turn, it becomes easier for people to combine these related concepts together and understand them as a relationship, rather than two distinct entities. For example, consider the example, \"John spread butter on a bagel.\" In this sentence, the lexical concepts \"spread,\" \"butter,\" and \"bagel\" are associated with one another and easy to combine into a mental representation of a breakfast scenario. Conversely, consider the example, \"John baked a computer.\" Because \"baked\" and \"computer\" are not related lexical concepts, it takes more effort and time to build a mental representation of this unusual scenario.\n\nHowever, spreading activation models of conceptual combination have been criticized in light of how humans have been observed to combined languages. Those who claim that the theory provides an insufficient account of linguistic conceptual combination refer to the ability of humans to readily understand lexical concept combinations with seemingly no apparent connection with one another. One example of this would be the sentence \"John saw an elephant cloud.\" \"Elephant\" and \"cloud\" do not shared a close association, but it takes little effort to comprehend that the term \"elephant cloud\" refers to a cloud shaped like an elephant. This has led some to conclude that the combination of lexical concepts does not wholly rely on the simultaneous activation of linked lexical concepts alone. Rather, they claim that the process involves the use of existing nodes to generate entirely new concepts independent of their parent concepts.\n\nAlthough many theories of novel noun-noun combination interpretation ignore the effect of social environment, some theorists have attempted to account for any contingencies social context may cause.\n\nWhen lexical concept combinations are interpreted without the influence of social context, the interpretation carried out is termed \"sense generation.\" This includes all processes that would normally occur excepting those dependent on a conversation partner. The \"generation hypothesis\" accordingly states that the interpretation mechanism of a noun-noun combination is essentially the same regardless of context. This does not rule out the possibility that social context can affect sense generation in some way, but it does assert that the basic structure of the process is unaffected. As seen above, debate as to what sense generation entails and how many sub-processes into which it should be divided is a contentious matter in cognitive science.\n\nThe \"anaphor resolution hypothesis\" instead asserts that before sense generation occurs, interpreters first search their memory of recent communication to see if the combination refers to something previously discussed. This process is termed \"anaphor resolution'.' If a referent is identified, interpretation occurs without sense generation in light of that referent. Even if an explicit referent does not exist, anaphor resolution can help facilitate sense generation by providing more information that might hint at the combination's intended meaning.\n\nThe \"dual-process hypothesis\" not to be confused with dual-process theory, states that sense generation and anaphor resolution occur in parallel. Both processes begin to work once the noun-noun combination is presented. Proponents of this hypothesis disagree as to how the interpretation is eventually resolves. Some believe that whichever process reaches a conclusion first provides the answer. Others believe that both provide continuous input to a third, mediating process that eventually makes a decision based on input from both.\n\nCreativity necessitates the employment of existing concepts in novel ways, and therefore requires conceptual combination. Surprisingly, this contribution seems to be limited. Conceptual combination is a significant contributor to convergent thinking, but not divergent thinking. For example, practice with generating new concepts through combination does not improve brainstorming. It does, however, assist in devising creative problem solving methods.\n\nThe psychological community's growing understanding of how concepts are manipulated has allowed educators to teach new concepts more effectively. Tools that are developed based on conceptual combination theory attempt to teach individual tasks, and then challenge students to exercise them together in order to promote both base subject skills and the critical thinking needed to apply them simultaneously to solve new problems. Máder & Vajda, for instance, developed a three-dimensional grid with cells of adjustable height which has been successfully used in numerous activities capable of improving the effectiveness of high school mathematics education.\n"}
{"id": "555650", "url": "https://en.wikipedia.org/wiki?curid=555650", "title": "Conceptual framework", "text": "Conceptual framework\n\nA conceptual framework is an analytical tool with several variations and contexts. It can be applied in different categories of work where an overall picture is needed. It is used to make conceptual distinctions and organize ideas. Strong conceptual frameworks capture something real and do this in a way that is easy to remember and apply.\n\nIsaiah Berlin used the metaphor of a \"fox\" and a \"hedgehog\" to make conceptual distinctions in how important philosophers and authors view the world. Berlin describes hedgehogs as those who use a single idea or organizing principle to view the world (such as Dante Alighieri, Blaise Pascal, Fyodor Dostoyevsky, Plato, Henrik Ibsen and Georg Wilhelm Friedrich Hegel). Foxes, on the other hand, incorporate a type of pluralism and view the world through multiple, sometimes conflicting, lenses (examples include Johann Wolfgang von Goethe, James Joyce, William Shakespeare, Aristotle, Herodotus, Molière, and Honoré de Balzac).\n\nEconomists use the conceptual framework of \"supply\" and \"demand\" to distinguish between the behavior and incentive systems of firms and consumers. Like many conceptual frameworks, supply and demand can be presented through visual or graphical representations (see demand curve). Both political Science and economics use principal agent theory as a conceptual framework. The politics-administration dichotomy is a long standing conceptual framework used in public administration. All three of these cases are examples of a macro level conceptual framework.\n\nThe use of the term \"conceptual framework\" crosses both scale (large and small theories) and contexts (social science, marketing, applied science, art etc.). Its explicit definition and application can therefore vary.\n\nConceptual frameworks are particularly useful as organizing devices in empirical research. One set of scholars has applied the notion of conceptual framework to deductive, empirical research at the micro- or individual study level. They employ American football plays as a useful metaphor to clarify the meaning of \"conceptual framework\" (used in the context of a deductive empirical study).\n\nLikewise, conceptual frameworks are abstract representations, connected to the research project's goal that direct the collection and analysis of data (on the plane of observation – the ground). Critically, a football play is a \"plan of action\" tied to a particular, timely, purpose, usually summarized as long or short yardage. Shields and Rangarajan (2013) argue that it is this tie to \"purpose\" that make American football plays such a good metaphor. They define a conceptual framework as \"the way ideas are organized to achieve a research project's purpose\". Like football plays, conceptual frameworks are connected to a research purpose or aim. Explanation is the most common type of research purpose employed in empirical research. The formal hypothesis of a scientific investigation is the framework associated with explanation.\n\nExplanatory research usually focuses on \"why\" or \"what caused\" a phenomenon to occur. Formal hypotheses posit possible explanations (answers to the why question) that are tested by collecting data and assessing the evidence (usually quantitative using statistical tests). For example, Kai Huang wanted to determine what factors contributed to residential fires in U.S. cities. Three factors were posited to influence residential fires. These factors (environment, population and building characteristics) became the hypotheses or conceptual framework he used to achieve his purpose – explain factors that influenced home fires in U.S. cities.\n\nSeveral types of conceptual frameworks have been identified, and line up with a research purpose in the following ways:\n\nNote that Shields and Rangarajan (2013) do not claim that the above are the only framework-purpose pairing. Nor do they claim the system is applicable to inductive forms of empirical research. Rather, the conceptual framework-research purpose pairings they propose are useful and provide new scholars a point of departure to develop their own research design.\n\nFrameworks have also been used to explain conflict theory and the balance necessary to reach what amounts to resolution. Within these conflict frameworks, visible and invisible variables function under concepts of relevance. Boundaries form and within these boundaries, tensions regarding laws and chaos (or freedom) are mitigated. These frameworks often function like cells, with sub-frameworks, stasis, evolution and revolution. Anomalies may exist without adequate \"lenses\" or \"filters\" to see them and may become visible only when the tools exist to define them.\n\n\n"}
{"id": "499429", "url": "https://en.wikipedia.org/wiki?curid=499429", "title": "D'Alembert's principle", "text": "D'Alembert's principle\n\nD'Alembert's principle, also known as the Lagrange–d'Alembert principle, is a statement of the fundamental classical laws of motion. It is named after its discoverer, the French physicist and mathematician Jean le Rond d'Alembert. It is the dynamic analogue to the \"principle of virtual work for applied forces\" in a static system and in fact is more general than Hamilton's principle, avoiding restriction to holonomic systems. A holonomic constraint depends only on the coordinates and time. It does not depend on the velocities. If the negative terms in accelerations are recognized as \"inertial forces\", the statement of d'Alembert's principle becomes \"The total virtual work of the impressed forces plus the inertial forces vanishes for reversible displacements\". The principle does not apply for irreversible displacements, such as sliding friction, and more general specification of the irreversibility is required.\n\nThe principle states that the sum of the differences between the forces acting on a system of mass particles and the time derivatives of the momenta of the system itself projected onto any virtual displacement consistent with the constraints of the system is zero. Thus, in symbols d'Alembert's principle is written as following,\n\nwhere :\n\nThis above equation is often called d'Alembert's principle, but it was first written in this variational form by Joseph Louis Lagrange. D'Alembert's contribution was to demonstrate that in the totality of a dynamic system the forces of constraint vanish. That is to say that the generalized forces formula_2 need not include constraint forces. It is equivalent to the somewhat more cumbersome Gauss's principle of least constraint.\n\nThe general statement of d'Alembert's principle mentions \"the time derivatives of the momenta of the system\". The momentum of the \"i\"-th mass is the product of its mass and velocity:\n\nand its time derivative is\n\nIn many applications, the masses are constant and this equation reduces to\n\nwhich appears in the formula given above. However, some applications involve changing masses (for example, chains being rolled up or being unrolled) and in those cases both terms formula_6 and formula_7 have to remain present, giving\n\nTo date, nobody has shown that D'Alembert's principle is equivalent to Newton's Second Law. D'Alembert's principle is a more general case . And it is true only for some very special cases e.g. rigid body constraints. However, an approximate solution to this problem does exist.\n\nConsider Newton's law for a system of particles, i. The total force on each particle is\n\nwhere\n\nMoving the inertial forces to the left gives an expression that can be considered to represent quasi-static equilibrium, but which is really just a small algebraic manipulation of Newton's law:\n\nConsidering the virtual work, formula_11, done by the total and inertial forces together through an arbitrary virtual displacement, formula_12, of the system leads to a zero identity, since the forces involved sum to zero for each particle.\n\nThe original vector equation could be recovered by recognizing that the work expression must hold for arbitrary displacements. Separating the total forces into applied forces, formula_14, and constraint forces, formula_15, yields\n\nIf arbitrary virtual displacements are assumed to be in directions that are orthogonal to the constraint forces (which is not usually the case, so this derivation works only for special cases), the constraint forces do no work. Such displacements are said to be \"consistent\" with the constraints. This leads to the formulation of \"d'Alembert's principle\", which states that the difference of applied forces and inertial forces for a dynamic system does no virtual work:.\n\nThere is also a corresponding principle for static systems called the principle of virtual work for applied forces.\n\nD'Alembert showed that one can transform an accelerating rigid body into an equivalent static system by adding the so-called \"inertial force\" and \"inertial torque\" or moment. The inertial force must act through the center of mass and the inertial torque can act anywhere. The system can then be analyzed exactly as a static system subjected to this \"inertial force and moment\" and the external forces. The advantage is that, in the equivalent static system one can take moments about any point (not just the center of mass). This often leads to simpler calculations because any force (in turn) can be eliminated from the moment equations by choosing the appropriate point about which to apply the moment equation (sum of moments = zero). Even in the course of Fundamentals of Dynamics and Kinematics of machines, this principle helps in analyzing the forces that act on a link of a mechanism when it is in motion. In textbooks of engineering dynamics this is sometimes referred to as \"d'Alembert's principle\".\n\nTo illustrate the concept of \"d'Alembert's principle\", let's use a simple model with a weight formula_18, suspended from a wire. The weight is subjected to a gravitational force, formula_19, and a tension force formula_20 in the wire. The mass accelerates upward with an acceleration formula_21. Newton's Second Law becomes formula_22 or formula_23. As an observer with feet planted firmly on the ground, we see that the force formula_20 accelerates the weight, formula_18, but, if we are moving with the wire we don’t see the acceleration, we feel it. The tension in the wire seems to counteract an acceleration “force” formula_26 or formula_27.\nFor a planar rigid body, moving in the plane of the body (the \"x\"–\"y\" plane), and subjected to forces and torques causing rotation only in this plane, the inertial force is\n\nwhere formula_29 is the position vector of the centre of mass of the body, and formula_30 is the mass of the body. The inertial torque (or moment) is\n\nwhere formula_32 is the moment of inertia of the body. If, in addition to the external forces and torques acting on the body, the inertia force acting through the center of mass is added and the inertial torque is added (acting around the centre of mass is as good as anywhere) the system is equivalent to one in static equilibrium. Thus the equations of static equilibrium\n\nhold. The important thing is that formula_34 is the sum of torques (or moments, including the inertial moment and the moment of the inertial force) taken about \"any\" point. The direct application of Newton's laws requires that the angular acceleration equation be applied \"only\" about the center of mass.\n\nD'Alembert's form of the principle of virtual work states that a system of rigid bodies is in dynamic equilibrium when the virtual work of the sum of the applied forces and the inertial forces is zero for any virtual displacement of the system. Thus, dynamic equilibrium of a system of n rigid bodies with m generalized coordinates requires that is to be\nfor any set of virtual displacements δq. This condition yields m equations,\nwhich can also be written as\nThe result is a set of m equations of motion that define the dynamics of the rigid body system.\n"}
{"id": "34802284", "url": "https://en.wikipedia.org/wiki?curid=34802284", "title": "Gauss's Pythagorean right triangle proposal", "text": "Gauss's Pythagorean right triangle proposal\n\nGauss's Pythagorean right triangle proposal is an idea attributed to Carl Friedrich Gauss for a method to signal extraterrestrial beings by constructing an immense right triangle and three squares on the surface of the Earth. The shapes would be a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars.\n\nAlthough credited in numerous sources as originating with Gauss, with exact details of the proposal set out, the specificity of detail, and even whether Gauss made the proposal, have been called into question. Many of the earliest sources do not actually name Gauss as the originator, instead crediting a \"German Astronomer\" or using other nonspecific descriptors, and in some cases naming a different author entirely. The details of the proposal also change significantly upon different retellings. Nevertheless, Gauss's writings reveal a belief and interest in finding a method to contact extraterrestrial life, and that he did, at the least, propose using amplified light using a heliotrope, his own 1818 invention, to signal supposed inhabitants of the Moon.\n\nCarl Friedrich Gauss is credited with an 1820 proposal for a method to signal extraterrestrial beings in the form of drawing an immense right triangle and three squares on the surface of the Earth, intended as a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars. Details vary between sources, but typically the \"drawing\" was to be constructed on the Siberian tundra, and made up of vast strips of pine forest forming the right triangle's borders, with the interior of the drawing and exterior squares composed of fields of wheat. Gauss is said to have been convinced that Mars harbored intelligent life and that this geometric figure, invoking the Pythagorean theorem through the squares on the outside borders (sometimes called a \"windmill diagram\", as originated by Euclid), would demonstrate to such alien observers the reciprocal existence of intelligent life on Earth and its grounding in mathematics. Wheat was said to be chosen by Gauss for contrast with the pine tree borders \"because of its uniform color\".\n\nThe specificity of the proposal's details as it appears in most later sources—even its attribution to Gauss—is called into question in University of Notre Dame Professor Michael J. Crowe's 1986 book, \"The Extraterrestrial Life Debate, 1750–1900\", in which he surveys the origins of the Gauss proposal and observes that:The history of this proposal ... can be traced through two dozen or more pluralist writings reaching back to the first half of the nineteenth century. When this is done, however, it turns out that the story exists in almost as many forms as its retellings. Furthermore, these versions share one characteristic: Never is reference supplied to where in the writings of Gauss ... the [proposal] appear[s]!Some early sources explored by Crowe for the attribution and form of Gauss's proposal include Austrian astronomer, Joseph Johann Littrow's statement in \"Wunder des Himmels\" that \"one of our most distinguished geometers\" proposed that a geometric figure \"for example the well known so-called square of the hypotenuse, be laid out on a large scale, say on a particular broad plain of the earth\". and Patrick Scott's \"Love in the Moon\", in which a \"learned man\" is described as proposing a signal formed by a \"great plantation of tree\" in the form of \"47th Proposition of Euclid\" in \"the great African dessert \". In \"Chambers's Edinburgh Journal\" it was written that a Russian savant had proposed to \"communicate with the moon by cutting a large figure of the forty-seventh proposition of Euclid on the plains of Siberia, which, he said, any fool would understand\".\n\nIn the writings of astronomers Asaph Hall and of Norman Lockyer, each refer separately to a \"German Astronomer\" who proposed the method of contact be by \"fire signals\" from Siberia. Writing in 1902, Simon Newcomb placed the origin of a Siberian triangle \"several hundred miles in extent\" not with Gauss, but at the feet of German astronomer Franz Xaver von Zach. In lectures presented by François Arago at the Paris Observatory, he named Siberia as the location of an extraterrestrial signaling project advanced by an unnamed \"German geometer\", but that the signaling method was to be through the use of mirrors, rather than any large symbol drawn upon the Earth. Despite this version's departure from a geometric figure, the appearance of mirrors as a signaling device has a connection with Gauss's background. Gauss invented the Heliotrope in 1818, an instrument that uses a mirror to reflect sunlight in a manner allowing a square mirror to be seen away even in sunny weather.\n\nGauss wrote of the heliotrope's potential as a celestial signaling device in a March 25, 1822 letter to Heinrich Olbers, by which he reveals a belief and interest in finding a method to contact extraterrestrial life: \"With 100 separate mirrors, each of 16 square feet, used conjointly, one would be able to send good heliotrope-light to the moon ... This would be a discovery even greater than that of America, if we could get in touch with our neighbors on the moon.\" Finally, in the October 1826 issue of the \"Edinburgh New Philosophical Journal\" an unnamed author wrote that in a conversation with Franz von Gruithuisen, Gauss stated words to the effect that \"the plan of erecting a geometrical figure on the plains of Siberia corresponded with his opinion, because, according to his view a correspondence with the inhabitants of the moon could only be begun by means of such mathematical contemplations and ideas, which we and they have in common.\" Crowe concluded in sum that his review of earliest sources failed to confirm the detail of the proposal and Gauss as its author, but that his origination of the idea was not unlikely given the existing evidence.\n"}
{"id": "33920395", "url": "https://en.wikipedia.org/wiki?curid=33920395", "title": "General Group Problem Solving (GGPS) Model", "text": "General Group Problem Solving (GGPS) Model\n\nThe General Group Problem Solving (GGPS) Model is a problem solving methodology, in which a group of individuals will define the desired outcome, identify the gap between the current state and the target and generate ideas for closing the gap by brainstorming. The end result is list of actions needed to achieve the desired results.\n\nSally Fuller and Ramon Aldag argue that group decision-making models have been operating under too narrow of a focus due to the overemphasis of the groupthink phenomenon. In addition, according to them, group decision-making has often been framed in relative isolation, ignoring context and real-world circumstances, which is a likely consequence of testing group decision-making in laboratory studies. They claim that the groupthink model is overly deterministic and an unrealistically restrictive depiction of the group problem-solving process.” To address these problems, they propose a new model that incorporates elements of group decision-making processes from a broader, more comprehensive perspective, offering a more general and generalizable framework for future research. The model includes elements of Irving Janis's original model (1977), but only those that have been consistently supported by the literature. To understand the differences between the two models, we briefly summarize both Janis's model and the GGPS-model first.\n\nJanis defines groupthink as “the mode of thinking that persons engage in when concurrence-seeking becomes so dominant in a cohesive in-group that it tends to over-ride realistic appraisals of alternative courses of action.” In a subsequent article, he elaborates on this by saying: “I use the term \"groupthink\" as a quick and easy way to refer to a mode of thinking that people engage in when they are deeply involved in a cohesive in-group, when the members' strivings for unanimity override their motivation to realistically appraise alternative courses of action. Groupthink refers to a deterioration of mental efficiency, reality testing, and moral judgment that results from in-group pressures.”\nAll this suggests that the original groupthink model was proposed for a rather specific situation, and Janis states that we can only call a phenomenon groupthink if all the warning signs are present (see groupthink symptoms).\n\nThe GGPS-model (developed by Ramon Aldag and Sally Fuller) broadens the perspectives, incorporating elements of the original groupthink model, in a fashion that creates a more widely applicable schematic.\nTwo key differences should be noted in comparison to Janis’ model:\n\n\nThree sets of antecedents are proposed by GGPS: decision characteristics, group structure and decision-making context.\n\nElements belonging here are the importance of the decision, time pressure, structure, procedural requirements, and task characteristics.\n\n\"Examples\": whether the task is simple or complex will make a substantial difference in required member input, as well as in whether a directive leader is necessary. Group interaction is also altered if, given a task, a single correct answer exists and if it becomes obvious to any of the members, since subsequent group interaction will likely be reduced.\n\nElements are cohesiveness, members’ homogeneity, insulation of the group, leader impartiality, leader power, history of the group, probability of future interaction, stage of group development and type of group.\n\n\"Examples\": whether group members anticipate to work together again in the future can have a major impact on to what degree can political motives influence the process. If it’s unlikely that the group will come together again, political influence can be lessened. Stage of group development is important because members of mature group with a long history may feel more comfortable challenging each other’s ideas, thus cohesiveness results in quality decision making and positive outcomes.\n\nElements are organizational political norms, member political motives, prior discussion of issue, prior goal attainment, goal definition, and degree of stress from external threat.\n\n\"Examples\": whether group members identified and pursue a unitary goal or they have multiple, discrepant goals influences the rationality of the decision-making process. Members’ political motives also make a tremendous difference, if individuals have a vested interest in certain outcomes, or there are one or more coalitions present, behavior in the decision making process could be altered.\n\nThe model differentiates two categories of emergent group characteristics: group perceptions and processes.\n\nThese include members’ perceptions of the group’s vulnerability, the inherent morality of the group, member unanimity, and views of opposing groups.\n\nThese include the group’s response to negative feedback, treatment of dissenters, self-censorship, and use of mindguards.\n\nDecision process characteristics are grouped in terms of the first three stages of group problem-solving processes: problem identification, alternative generation and evaluation and choice. Implementation and control stages are not included because they follow the actual decision, but some variables preparing for those stages are indeed included (e.g. development of contingency plans and gathering of control-related information).\n\nElements of this stage are predecisional information search, survey of objectives, and explicit problem definition.\n\n\"Example\": if members of the group fail to explicitly or correctly define the problem, there is a chance that they will solve the wrong problem.\n\nElements are number of alternatives and quality of alternatives.\n\n\"Example\": in generating alternatives, it’s important to differentiate quality and quantity of alternative ideas generated. Some group processes, such as brainstorming, are directed towards generating large numbers of ideas, with the assumption that it will lead to a superior alternative. Defective processes, on the other hand, might lead to large numbers of low quality ideas.\n\nElements are information processing quality, the source of the initial selection of a preferred alternative, emergence of preferred alternative, group decision rule, timing of convergence, reexamination of preferred and rejected alternatives, source of the final solution, development of contingency plans, and gathering of control-related information.\n\n\"Example\": whether the group decides based on a majority rule or a consensus has to be reached, makes a great difference in the process. If a consensus is to be reached, dissent could be discouraged, because dissenters could elongate and jeopardize the process. With a majority rule, dissent is more acceptable.\n\nThe GGPS model includes an array of decision, political and affective outcomes.\n\nDecision outcomes: include acceptance of the decision by those affected by it and/or those who have to implement it, adherence to the decision, implementation success, and decision quality.\n\n\"Example\": if the leader of the group is not satisfied with the decision, he/she might unilaterally reverse it.\n\nPolitical outcomes: include future motivation of the leader, future motivation of the group and future use of the group.\n\n\"Example\": if the outcome did not satisfy the political agenda of the leader, he/she might use the group less or not at all in the future.\n\nAffective outcomes: include satisfaction with the leader, satisfaction with the group process and satisfaction with the decision.\n\n\"Example\": whether members are content with the fairness of the group process, whether trust was developed and preserved, or whether commitment to the decision is strong will greatly influence the group’s future functioning.\n"}
{"id": "18562346", "url": "https://en.wikipedia.org/wiki?curid=18562346", "title": "Gossen's laws", "text": "Gossen's laws\n\nGossen's laws, named for Hermann Heinrich Gossen (1810 – 1858), are three laws of economics:\n\nThe citation referenced is the translation by Nicholas Georgescu-Roegen in which the traslator names only two laws: 1) ”If an enjoyment is experienced uninterruptedly, the corresponding intensity of pleasure decreases continuously until satiety is ultimately reached, at which point the intensity becomes nil.\" and, 2) \"A similar decrease of the intensity of pleasure takes place if a previous enjoyment of the same kind of pleasure is repeated. Not only does the initial intensity of pleasure become smaller but also the duration of the enjoyment becomes shorter, so that satiety is reached sooner. Moreover, the sooner the repetition, the smaller becomes the initial intensity as well as the duration of the enjoyment.\" (p.lxxx)\n\n\n"}
{"id": "161999", "url": "https://en.wikipedia.org/wiki?curid=161999", "title": "Idea", "text": "Idea\n\nIn philosophy, ideas are usually taken as mental representational images of some object. Ideas can also be abstract concepts that do not present as mental images. Many philosophers have considered ideas to be a fundamental ontological category of being. The capacity to create and understand the meaning of ideas is considered to be an essential and defining feature of human beings. In a popular sense, an idea arises in a reflexive, spontaneous manner, even without thinking or serious reflection, for example, when we talk about the \"idea\" of a person or a place. A new or original idea can often lead to innovation.\n\nThe word \"idea\" comes from Greek ἰδέα \"idea\" \"form, pattern,\" from the root of ἰδεῖν \"idein\", \"to see.\" \n\nOne view on the nature of ideas is that there exist some ideas (called \"innate ideas\") which are so general and abstract that they could not have arisen as a representation of an object of our perception but rather were in some sense always present. These are distinguished from \"adventitious ideas\" which are images or concepts which are accompanied by the judgment that they are caused or occasioned by an external object.\n\nAnother view holds that we only discover ideas in the same way that we discover the real world, from personal experiences. The view that humans acquire all or almost all their behavioral traits from nurture (life experiences) is known as \"tabula rasa\" (\"blank slate\"). Most of the confusions in the way ideas arise is at least in part due to the use of the term \"idea\" to cover both the representation perceptics and the object of conceptual thought. This can be always illustrated in terms of the scientific doctrines of innate ideas, \"concrete ideas versus abstract ideas\", as well as \"simple ideas versus complex ideas\".\n\nPlato in Ancient Greece was one of the earliest philosophers to provide a detailed discussion of ideas and of the thinking process (it must be noted that in Plato's Greek the word \"idea\" carries a rather different sense from our modern English term). Plato argued in dialogues such as the \"Phaedo\", \"Symposium\", \"Republic\", and \"Timaeus\" that there is a realm of ideas or forms (\"eidei\"), which exist independently of anyone who may have thoughts on these ideas, and it is the ideas which distinguish mere opinion from knowledge, for unlike material things which are transient and liable to contrary properties, ideas are unchanging and nothing but just what they are. Consequently, Plato seems to assert forcefully that material things can only be the objects of opinion; real knowledge can only be had of unchanging ideas. Furthermore, ideas for Plato appear to serve as universals; consider the following passage from the \"Republic\":\nDescartes often wrote of the meaning of \"idea\" as an image or representation, often but not necessarily \"in the mind\", which was well known in the vernacular. Despite that Descartes is usually credited with the invention of the non-Platonic use of the term, he at first followed this vernacular use. In his \"Meditations on First Philosophy\" he says, \"Some of my thoughts are like images of things, and it is to these alone that the name 'idea' properly belongs.\" He sometimes maintained that ideas were innate and uses of the term \"idea\" diverge from the original primary scholastic use. He provides multiple non-equivalent definitions of the term, uses it to refer to as many as six distinct kinds of entities, and divides \"ideas\" inconsistently into various genetic categories. For him knowledge took the form of ideas and philosophical investigation is the deep consideration of these entities.\n\nIn striking contrast to Plato's use of idea is that of John Locke. In his Introduction to An Essay Concerning Human Understanding, Locke defines \"idea\" as \"that term which, I think, serves best to stand for whatsoever is the object of the understanding when a man thinks, I have used it to express whatever is meant by phantasm, notion, species, or whatever it is which the mind can be employed about in thinking; and I could not avoid frequently using it.\" He said he regarded the book necessary to examine our own abilities and see what objects our understandings were, or were not, fitted to deal with. In his philosophy other outstanding figures followed in his footsteps — Hume and Kant in the 18th century, Arthur Schopenhauer in the 19th century, and Bertrand Russell, Ludwig Wittgenstein, and Karl Popper in the 20th century. Locke always believed in \"good sense\" — not pushing things to extremes and on taking fully into account the plain facts of the matter. He considered his common-sense ideas \"good-tempered, moderate, and down-to-earth.\"\n\nAs John Locke studied humans in his work “An Essay Concerning Human Understanding” he continually referenced Descartes for ideas as he asked this fundamental question: “When we are concerned with something about which we have no certain knowledge, what rules or standards should guide how confident we allow ourselves to be that our opinions are right?” A simpler way of putting it is how do humans know ideas, and what are the different types of ideas. An idea to Locke “can simply mean some sort of brute experience.” He shows that there are “No innate principles in the mind.”. Thus, he concludes that “our ideas are all experiential in nature.” An experience can either be a sensation or a reflection: “consider whether there are any innate ideas in the mind before any are brought in by the impression from sensation or reflection.” Therefore, an idea was an experience in which the human mind apprehended something.\n\nIn a Lockean view, there are really two types of ideas: complex and simple. Simple ideas are the building blocks for much more complex ideas, and “While the mind is wholly passive in the reception of simple ideas, it is very active in the building of complex ideas…” Complex ideas, therefore, can either be modes, substances, or relations. Modes are when ideas are combined in order to convey new information. For instance, David Banach gives the example of beauty as a mode. He says that it is the combination of color and form. Substances, however, is different. Substances are certain objects, that can either be dogs, cats, or tables. And relations represent the relationship between two or more ideas. In this way, Locke did, in fact, answer his own questions about ideas and humans.\n\nHume differs from Locke by limiting \"idea\" to the more or less vague mental reconstructions of perceptions, the perceptual process being described as an \"impression.\" Hume shared with Locke the basic empiricist premise that it is only from life experiences (whether their own or others') that humans' knowledge of the existence of anything outside of themselves can be ultimately derived, that they shall carry on doing what they are prompted to do by their emotional drives of varying kinds. In choosing the means to those ends, they shall follow their accustomed associations of ideas. Hume has contended and defended the notion that \"reason alone is merely the 'slave of the passions'.\" \n\nImmanuel Kant defines an \"idea\" as opposed to a \"concept\". \"Regulative ideas\" are ideals that one must tend towards, but by definition may not be completely realized. Liberty, according to Kant, is an idea. The autonomy of the rational and universal subject is opposed to the determinism of the empirical subject. Kant felt that it is precisely in knowing its limits that philosophy exists. The business of philosophy he thought was not to give rules, but to analyze the private judgements of good common sense.\n\nWhereas Kant declares limits to knowledge (\"we can never know the thing in itself\"), in his epistemological work, Rudolf Steiner sees \"ideas\" as \"objects of experience\" which the mind apprehends, much as the eye apprehends light. In \"Goethean Science\" (1883), he declares, \"Thinking ... is no more and no less an organ of perception than the eye or ear. Just as the eye perceives colors and the ear sounds, so thinking perceives ideas.\" He holds this to be the premise upon which Goethe made his natural-scientific observations.\n\nWundt widens the term from Kant's usage to include \"conscious representation of some object or process of the external world\". In so doing, he includes not only ideas of memory and imagination, but also perceptual processes, whereas other psychologists confine the term to the first two groups. One of Wundt's main concerns was to investigate conscious processes in their own context by experiment and introspection. He regarded both of these as \"exact methods\", interrelated in that experimentation created optimal conditions for introspection. Where the experimental method failed, he turned to other \"objectively valuable aids\", specifically to \"those products of cultural communal life which lead one to infer particular mental motives. Outstanding among these are speech, myth, and social custom.\" Wundt designed the basic mental activity apperception — a unifying function which should be understood as an activity of the will. Many aspects of his empirical physiological psychology are used today. One is his principles of mutually enhanced contrasts and of assimilation and dissimilation (i.e. in color and form perception and his advocacy of \"objective\" methods of expression and of recording results, especially in language. Another is the principle of heterogony of ends — that multiply motivated acts lead to unintended side effects which in turn become motives for new actions.\n\nC. S. Peirce published the first full statement of pragmatism in his important works \"\" (1878) and \"\" (1877). In \"How to Make Our Ideas Clear\" he proposed that a \"clear idea\" (in his study he uses concept and \"idea\" as synonymic) is defined as one, when it is apprehended such as it will be recognized wherever it is met, and no other will be mistaken for it. If it fails of this clearness, it is said to be obscure. He argued that to understand an idea clearly we should ask ourselves what difference its application would make to our evaluation of a proposed solution to the problem at hand. Pragmatism (a term he appropriated for use in this context), he defended, was a method for ascertaining the meaning of terms (as a theory of meaning). The originality of his ideas is in their rejection of what was accepted as a view and understanding of knowledge by scientists for some 250 years, i.e. that, he pointed, knowledge was an impersonal fact. Peirce contended that we acquire knowledge as \"participants\", not as \"spectators\". He felt \"the real\", sooner or later, is information acquired through ideas and knowledge with the application of logical reasoning would finally result in. He also published many papers on logic in relation to \"ideas\".\n\nG. F. Stout and J. M. Baldwin, in the \"Dictionary of Philosophy and Psychology\", define \"idea\" as \"the reproduction with a more or less adequate image, of an object not actually present to the senses.\" They point out that an idea and a perception are by various authorities contrasted in various ways. \"Difference in degree of intensity\", \"comparative absence of bodily movement on the part of the subject\", \"comparative dependence on mental activity\", are suggested by psychologists as characteristic of an idea as compared with a perception.\n\nIt should be observed that an idea, in the narrower and generally accepted sense of a mental reproduction, is frequently composite. That is, as in the example given above of the idea of a chair, a great many objects, differing materially in detail, all call a single idea. When a man, for example, has obtained an idea of chairs in general by comparison with which he can say \"This is a chair, that is a stool\", he has what is known as an \"abstract idea\" distinct from the reproduction in his mind of any particular chair (see abstraction). Furthermore, a complex idea may not have any corresponding physical object, though its particular constituent elements may severally be the reproductions of actual perceptions. Thus the idea of a centaur is a complex mental picture composed of the ideas of man and horse, that of a mermaid of a woman and a fish.\n\nDiffusion studies explore the spread of ideas from culture to culture. Some anthropological theories hold that all cultures imitate ideas from one or a few original cultures, the Adam of the Bible, or several cultural circles that overlap. Evolutionary diffusion theory holds that cultures are influenced by one another but that similar ideas can be developed in isolation.\n\nIn the mid-20th century, social scientists began to study how and why ideas spread from one person or culture to another. Everett Rogers pioneered diffusion of innovations studies, using research to prove factors in adoption and profiles of adopters of ideas. In 1976, in his book \"The Selfish Gene\", Richard Dawkins suggested applying biological evolutionary theories to the spread of ideas. He coined the term \"meme\" to describe an abstract unit of selection, equivalent to the gene in evolutionary biology.\n\nJames Boswell recorded Samuel Johnson's opinion about ideas. Johnson claimed that they are mental images or internal visual pictures. As such, they have no relation to words or the concepts which are designated by verbal names.\n\nTo protect the cause of invention and innovation, the legal constructions of Copyrights and Patents were established. Patent law regulates various aspects related to the functional manifestation of inventions based on new ideas or incremental improvements to existing ones. Thus, patents have a direct relationship to ideas.\n\nIn some cases, authors can be granted limited legal monopolies on the manner in which certain works are expressed. This is known colloquially as copyright, although the term intellectual property is used mistakenly in place of \"copyright\". Copyright law regulating the aforementioned monopolies generally does not cover the actual ideas. The law does not bestow the legal status of property upon ideas per se. Instead, laws purport to regulate events related to the usage, copying, production, sale and other forms of exploitation of the fundamental expression of a work, that may or may not carry ideas. Copyright law is fundamentally different from patent law in this respect: patents do grant monopolies on ideas (more on this below).\n\nA copyright is meant to regulate some aspects of the usage of expressions of a work, \"not\" an idea. Thus, copyrights have a negative relationship to ideas.\n\nWork means a tangible medium of expression. It may be an original or derivative work of art, be it literary, dramatic, musical recitation, artistic, related to sound recording, etc. In (at least) countries adhering to the Berne Convention, copyright automatically starts covering the work upon the original creation and fixation thereof, without any extra steps. While creation usually involves an idea, the idea in itself does not suffice for the purposes of claiming copyright. \nConfidentiality and nondisclosure agreements are legal instruments that assist corporations and individuals in keeping ideas from escaping to the general public. Generally, these instruments are covered by contract law.\n\n\n"}
{"id": "2892645", "url": "https://en.wikipedia.org/wiki?curid=2892645", "title": "Instantiation principle", "text": "Instantiation principle\n\nThe instantiation principle or principle of instantiation or principle of exemplification is the concept in metaphysics and logic (first put forward by David Malet Armstrong) that there can be no uninstantiated or unexemplified properties (or universals). In other words, it is impossible for a property to exist which is not had by some object.\n\nConsider a chair. Presumably chairs did not exist 150,000 years ago. Thus, according to the principle of instantiation, the property of being a chair did not exist 150,000 years ago either. Similarly, if all red objects were to suddenly go out of existence, then the property of being red would likewise go out of existence.\n\nTo make the principle more plausible in the light of these examples, the existence of properties or universals is not tied to their actual existence now, but to their existence in space-time considered as a whole. Thus, any property which \"is\", \"has been\", or \"will be\" instantiated exists. The property of being red would exist even if all red things were to be destroyed, because it has been instantiated. This broadens the range of properties which exist if the principle is true. \n\nThose who endorse the principle of instantiation are known as \"in re\" (in thing or in reality) realists or 'immanent realists'.\n\n"}
{"id": "34644725", "url": "https://en.wikipedia.org/wiki?curid=34644725", "title": "Jurisprudence of concepts", "text": "Jurisprudence of concepts\n\nThe jurisprudence of concepts was the first \"sub-school\" of legal positivism, according to which, the written law must reflect concepts, when interpreted. Its main representatives were Ihering, Savigny and Puchta.\n\nThis school was, thus, the preceding trigger of the idea that law comes from a dogmatic source, imposition from man over man and not a \"natural\" consequence of other sciences or of metaphysical faith.\n\nAmong the main characters of the \"jurisprudence of concepts\" are:\n\nSo, according to this school, law should have prevailing sources based upon the legislative process, although needing to be proven by more inclusive ideas of a social sense.\n\n"}
{"id": "4021739", "url": "https://en.wikipedia.org/wiki?curid=4021739", "title": "LaSalle's invariance principle", "text": "LaSalle's invariance principle\n\nLaSalle's invariance principle (also known as the invariance principle, Barbashin-Krasovskii-LaSalle principle, or Krasovskii-LaSalle principle ) is a criterion for the asymptotic stability of an autonomous (possibly nonlinear) dynamical system.\n\nSuppose a system is represented as\n\nwhere formula_2 is the vector of variables, with\n\nIf a formula_4 function formula_5 can be found such that\n\nthen the set of accumulation points of any trajectory is contained in formula_8 where formula_8 is the union of complete trajectories contained entirely in the set formula_10. \n\nIf we additionally have that the function formula_11 is positive definite, i.e.\n\nand if formula_8 contains no trajectory of the system except the trivial trajectory formula_16 for formula_17, then the origin is asymptotically stable.\n\nFurthermore, if formula_11 is radially unbounded, i.e.\n\nthen the origin is globally asymptotically stable.\n\nIf \n\nhold only for formula_24 in some neighborhood formula_25 of the origin, and the set\n\ndoes not contain any trajectories of the system besides the trajectory formula_27, then the local version of the invariance principle states that the origin is locally asymptotically stable.\n\nIf formula_28 is negative definite, the global asymptotic stability of the origin is a consequence of Lyapunov's second theorem. The invariance principle gives a criterion for asymptotic stability in the case when formula_29 is only negative semidefinite.\n\nThis section will apply the invariance principle to establish the local asymptotic stability of a simple system, the pendulum with friction. This system can be modeled with the differential equation \n\nwhere formula_31 is the angle the pendulum makes with the vertical normal, formula_32 is the mass of the pendulum, formula_33 is the length of the pendulum, formula_34 is the friction coefficient, and \"g\" is acceleration due to gravity.\n\nThis, in turn, can be written as the system of equations\n\nUsing the invariance principle, it can be shown that all trajectories which begin in a ball of certain size around the origin formula_37 asymptotically converge to the origin. We define formula_38 as\n\nThis formula_38 is simply the scaled energy of the system Clearly, formula_41 is positive definite in an open ball of radius formula_42 around the origin. Computing the derivative,\n\nObserve that formula_44. If it were true that formula_45, we could conclude that every trajectory approaches the origin by Lyapunov's second theorem. Unfortunately, formula_46 and formula_47 is only negative semidefinite. However, the set\n\nwhich is simply the set\n\ndoes not contain any trajectory of the system, except the trivial trajectory x = 0. Indeed, if at some time formula_50, formula_51, then because \nformula_52 must be less than formula_42 away from the origin, formula_54 and formula_55. As a result, the trajectory will not stay in the set formula_56.\n\nAll the conditions of the local version of the invariance principle are satisfied, and we can conclude that every trajectory that begins in some neighborhood of the origin will converge to the origin as formula_57 .\n\nThe general result was independently discovered by J.P. LaSalle (then at RIAS) and N.N. Krasovskii, who published in 1960 and 1959 respectively. While LaSalle was the first author in the West to publish the general theorem in 1960, a special case of the theorem was in communicated in 1952 by Barbashin and Krasovskii, followed by a publication of the general result in 1959 by Krasovskii .\n\n\n\n\n\n"}
{"id": "1722373", "url": "https://en.wikipedia.org/wiki?curid=1722373", "title": "Locard's exchange principle", "text": "Locard's exchange principle\n\nIn forensic science, Locard's exchange principle holds that the perpetrator of a crime will bring something into the crime scene and leave with something from it, and that both can be used as forensic evidence. Dr. Edmond Locard (13 December 1877 – 4 May 1966) was a pioneer in forensic science who became known as the Sherlock Holmes of France. He formulated the basic principle of forensic science as: \"Every contact leaves a trace\". Paul L. Kirk expressed the principle as follows:\n\nFragmentary or trace evidence is any type of material left at (or taken from) a crime scene, or the result of contact between two surfaces, such as shoes and the floor covering or soil, or fibers from where someone sat on an upholstered chair.\n\nWhen a crime is committed, fragmentary (or trace) evidence needs to be collected from the scene. A team of specialized police technicians goes to the scene of the crime and seals it off. They record video and take photographs of the crime scene, victim/s (if there are any) and items of evidence. If necessary, they undertake ballistics examinations. They check for foot, shoe, and tire mark impressions, plus hair as well as examine any vehicles and check for fingerprints - whole or partial.\n\nThe case studies below show how prevalent Locard's Exchange Principle is in each and every crime. The examples using Locard's Principle show not only how the transfer of trace evidence can tell the tale of what happened, but also how much care is required when collecting and evaluating trace evidence.\n\nKarola and Melanie Weimar, aged 5 and 7, lived with their parents, Reinhard and Monika, in Germany. They were reported missing on 4 August 1986. Their bodies were found on 7 August. They had been murdered.\n\nMonika first said the children had breakfast, then went to a playground. Three weeks later she said they were already dead when she returned home the previous night: Reinhard was sitting on the edge of Karola's bed, weeping and confused; he then disposed of the bodies.\n\nBoth parents were suspected, but Monika was having an affair, and was seen where Melanie's body was later found. She was convicted, but after serving her sentence, was released in 2006.\n\nInvestigators determined what clothes Monika was wearing on 3 and 4 August, but not Reinhard's clothes, so only fibers from her clothing were identified on the children's bodies, yet they were also constantly in contact with him.\n\nThe bedding contained 14 fibers from Karola's T-shirt. Frictionless tests, simulating a dead child, matched that figure better than the friction tests, simulating a live child, so Karola could have lain lifelessly in bed wearing her T-shirt, as stated by her mother.\n\n35 fibers from Monika's blouse were found on the back of Melanie's T-shirt, but only one on her bed sheet. In tests, between 6 and 10 fibers remained on the sheet. These higher numbers were thought to disprove Monika's claim that she gave her child a goodbye hug the previous day. However, there are several likely explanations. For example, the bedding was put in one bag, so fibers from the sheet could have been transferred to the cover and pillow. Only the central area of the top of the sheet was taped: it might have originally contained more than one blouse fiber, the others could have been transferred to the back or sides while in the bag.\n\nThe blouse fibers on Melanie's clothing were distributed evenly, not the clusters expected from carrying the body.\n\n265 fibers from the family car’s rear seat covers were found on Melanie's panties and the inside of her trousers, but only a small number of fibers from the front seats was found on the children. This helped disprove the theory that they were killed on the front seats.\n\nMelanie's clothes and hair were covered in 375 clinging fruits of goosegrass. As some of these itchy things were on the inside of her trousers and on her panties, the trousers must have been put on her after death.\n\nNo sand was found on the bodies or clothing (including socks and sandals) of either child, making the morning playground story unlikely.\n\nDanielle van Dam, aged 7, lived with her parents and brothers in San Diego, California. She was reported missing on 2 February 2002; her body was discovered on 27 February. Neighbor David Westerfield was almost immediately suspected, as he had gone camping in his RV, and he was convicted of her kidnapping and murder.\n\nHairs consistent with the van Dams’ dog were found in his RV, also carpet fibers consistent with Danielle's bedroom carpet. Danielle's nightly ritual was to wrestle with the dog after getting into her pajamas. The prosecution argued that those hairs and fibers got onto her pajamas through that contact, and were then carried on the pajamas to first Westerfield's house and then to his RV, when he kidnapped her from her bed. The alternative scenario is that they got onto her daytime clothes, and those of her mother and younger brother, and were carried to his house when they visited him earlier that week selling cookies. He said his laundry was out during that visit, so trace evidence from them could have got on it, and then been transferred to his bedroom and his RV (secondary Locard transfer). Also, his RV was often parked, sometimes unlocked, in the neighborhood streets, so Danielle could have sneaked inside, leaving behind that evidence.\n\nNo trace of Westerfield was found in the van Dam house.\n\n14 hairs consistent with Danielle's were found in his environment. All but one were compared on only mitochondrial DNA, so they might have come from her mother or a sibling. Most (21) of the hairs were in a dryer lint ball in his trash can, so they might have got in his laundry before the kidnapping.\n\nThere were 5 carpet fibers in his RV, but none in his house, suggesting those were deposited by someone going directly from her house to his RV, or they may have come from another house in that development.\n\nNo Danielle pajama or bedding fibers were reported in his environment. There was no trace evidence in his SUV (which casts doubt on the belief that she was transported from his house to his RV in his SUV). He vacuumed his RV after the kidnapping, but no trace evidence was in the vacuum cleaner.\n\nOne orange fiber with her body was consistent with about 200 in his house and 20 in his SUV (none in his RV), while 21 blue fibers with her body were consistent with 10 in his house and 46 in his RV (none in his SUV). Contrary to media reports, only a few items from her house were tested so that can’t be excluded as the source. In particular, the clothes of Danielle and her family during the cookie sale were not determined and eliminated. There were apparently two different types of the orange fibers, dull and very bright (so the number which matched might have been much less than 200). There were red fibers with her fingernails, and many other fibers with her body, which could not be matched to his environment. The only non-Danielle hair found with her body wasn’t his, nor was any desert sand reported with the body, and no soil or vegetation from the dump site was reported on his shoes, laundry, shovel or RV.\n\nTo explain why so much expected evidence was missing, the prosecution argued that he went on a cleaning frenzy, and tossed out evidence.\n\nIt is also mentioned in an episode of \"Hawaii Five-O\"\n\n"}
{"id": "182727", "url": "https://en.wikipedia.org/wiki?curid=182727", "title": "Mach's principle", "text": "Mach's principle\n\nIn theoretical physics, particularly in discussions of , Mach's principle (or Mach's conjecture) is the name given by Einstein to an imprecise hypothesis often credited to the physicist and philosopher Ernst Mach. The idea is that the existence of absolute rotation (the distinction of local inertial frames vs. rotating reference frames) is determined by the large-scale distribution of matter, as exemplified by this anecdote:\n\nYou are standing in a field looking at the stars. Your arms are resting freely at your side, and you see that the distant stars are not moving. Now start spinning. The stars are whirling around you and your arms are pulled away from your body. Why should your arms be pulled away when the stars are whirling? Why should they be dangling freely when the stars don't move?\nMach's principle says that this is not a coincidence—that there is a physical law that relates the motion of the distant stars to the local inertial frame. If you see all the stars whirling around you, Mach suggests that there is some physical law which would make it so you would feel a centrifugal force. There are a number of rival formulations of the principle. It is often stated in vague ways, like \"mass out there influences inertia here\". A very general statement of Mach's principle is \"local physical laws are determined by the large-scale structure of the universe\".\n\nThis concept was a guiding factor in Einstein's development of the general theory of relativity. Einstein realized that the overall distribution of matter would determine the metric tensor, which tells you which frame is rotationally stationary. Frame-dragging and conservation of gravitational angular momentum makes this into a true statement in the general theory in certain solutions. But because the principle is so vague, many distinct statements can be (and have been) made that would qualify as a Mach principle, and some of these are false. The Gödel rotating universe is a solution of the field equations that is designed to disobey Mach's principle in the worst possible way. In this example, the distant stars seem to be revolving faster and faster as one moves further away. This example doesn't completely settle the question, because it has closed timelike curves.\n\nThe basic idea also appears before Mach's time, in the writings of George Berkeley. The book \"Absolute or Relative Motion?\" (1896) by Benedict Friedländer and his brother Immanuel contained ideas similar to Mach's principle.\n\nThere is a fundamental issue in relativity theory. If all motion is relative, how can we measure the inertia of a body? We must measure the inertia with respect to something else. But what if we imagine a particle completely on its own in the universe? We might hope to still have some notion of its state of motion. Mach's principle is sometimes interpreted as the statement that such a particle's state of motion has no meaning in that case.\n\nIn Mach's words, the principle is embodied as follows:\n\nAlbert Einstein seemed to view Mach's principle as something along the lines of:\n\nIn this sense, at least some of Mach's principles are related to philosophical holism. Mach's suggestion can be taken as the injunction that gravitation theories should be relational theories. Einstein brought the principle into mainstream physics while working on general relativity. Indeed, it was Einstein who first coined the phrase \"Mach's principle\". There is much debate as to whether Mach really intended to suggest a new physical law since he never states it explicitly.\n\nThe writing in which Einstein found inspiration from Mach was \"The Science of Mechanics\", where the philosopher criticized Newton's idea of absolute space, in particular the argument that Newton gave sustaining the existence of an advantaged reference system: what is commonly called \"Newton's bucket argument\".\n\nIn his \"Philosophiae Naturalis Principia Mathematica\", Newton tried to demonstrate that:\n\nMach, in his book, says that:\n\nThis same thought had been expressed by the philosopher George Berkeley in his \"De Motu\". It is then not clear, in the passages from Mach just mentioned, if the philosopher intended to formulate a new kind of physical action between heavy bodies. This physical mechanism should determine the inertia of bodies, in a way that the heavy and distant bodies of our universe should contribute the most to the inertial forces. More likely, Mach only suggested a mere \"redescription of motion in space as experiences that do not invoke the term \"space\"\". What is certain is that Einstein interpreted Mach's passage in the former way, originating a long-lasting debate.\n\nMost physicists believe Mach's principle was never developed into a quantitative physical theory that would explain a mechanism by which the stars can have such an effect. It was never made clear by Mach himself exactly what his principle was. Although Einstein was intrigued and inspired by Mach's principle, Einstein's formulation of the principle is not a fundamental assumption of general relativity.\n\nBecause intuitive notions of distance and time no longer apply, what exactly is meant by \"Mach's principle\" in general relativity is even less clear than in Newtonian physics and at least 21 formulations of Mach's principle are possible, some being considered more strongly Machian than others. A relatively weak formulation is the assertion that the motion of matter in one place should affect which frames are inertial in another.\n\nEinstein, before completing his development of the general theory of relativity, found an effect which he interpreted as being evidence of Mach's principle. We assume a fixed background for conceptual simplicity, construct a large spherical shell of mass, and set it spinning in that background. The reference frame in the interior of this shell will precess with respect to the fixed background. This effect is known as the Lense–Thirring effect. Einstein was so satisfied with this manifestation of Mach's principle that he wrote a letter to Mach expressing this:\nThe Lense–Thirring effect certainly satisfies the very basic and broad notion that \"matter there influences inertia here\". The plane of the pendulum would not be dragged around if the shell of matter were not present, or if it were not spinning. As for the statement that \"inertia originates in a kind of interaction between bodies\", this too could be interpreted as true in the context of the effect.\n\nMore fundamental to the problem, however, is the very existence of a fixed background, which Einstein describes as \"the fixed stars\". Modern relativists see the imprints of Mach's principle in the initial-value problem. Essentially, we humans seem to wish to separate spacetime into slices of constant time. When we do this, Einstein's equations can be decomposed into one set of equations, which must be satisfied on each slice, and another set, which describe how to move between slices. The equations for an individual slice are elliptic partial differential equations. In general, this means that only part of the geometry of the slice can be given by the scientist, while the geometry everywhere else will then be dictated by Einstein's equations on the slice.\n\nIn the context of an asymptotically flat spacetime, the boundary conditions are given at infinity. Heuristically, the boundary conditions for an asymptotically flat universe define a frame with respect to which inertia has meaning. By performing a Lorentz transformation on the distant universe, of course, this inertia can also be transformed.\n\nA stronger form of Mach's principle applies in Wheeler–Mach–Einstein spacetimes, which require spacetime to be spatially compact and globally hyperbolic. In such universes Mach's principle can be stated as \"the distribution of matter and field energy-momentum (and possibly other information) at a particular moment in the universe determines the inertial frame at each point in the universe\" (where \"a particular moment in the universe\" refers to a chosen Cauchy surface).\n\nThere have been other attempts to formulate a theory that is more fully Machian, such as the Brans–Dicke theory and the Hoyle–Narlikar theory of gravity, but most physicists argue that none have been fully successful. At an exit poll of experts, held in Tübingen in 1993, when asked the question \"Is general relativity perfectly Machian?\", 3 respondents replied \"yes\", and 22 replied \"no\". To the question \"Is general relativity with appropriate boundary conditions of closure of some kind very Machian?\" the result was 14 \"yes\" and 7 \"no\".\n\nHowever, Einstein was convinced that a valid theory of gravity would necessarily have to include the relativity of inertia:\nThe broad notion that \"mass there influences inertia here\" has been expressed in several forms.\nHermann Bondi and Joseph Samuel have listed eleven distinct statements that can be called Mach principles, labelled by \"Mach0\" through \"Mach10\". Though their list is not necessarily exhaustive, it does give a flavor for the variety possible.\n\n\n\n"}
{"id": "221419", "url": "https://en.wikipedia.org/wiki?curid=221419", "title": "Marginalism", "text": "Marginalism\n\nMarginalism is a theory of economics that attempts to explain the discrepancy in the value of goods and services by reference to their secondary, or marginal, utility. The reason why the price of diamonds is higher than that of water, for example, owes to the greater additional satisfaction of the diamonds over the water. Thus, while the water has greater total utility, the diamond has greater marginal utility.\n\nAlthough the central concept of marginalism is that of marginal utility, marginalists, following the lead of Alfred Marshall, drew upon the idea of marginal physical productivity in explanation of cost. The neoclassical tradition that emerged from British marginalism abandoned the concept of utility and gave marginal rates of substitution a more fundamental role in analysis. Marginalism is an integral part of mainstream economic theory.\n\nFor issues of marginality, constraints are conceptualized as a \"border\" or \"margin\". The location of the margin for any individual corresponds to his or her \"endowment\", broadly conceived to include opportunities. This endowment is determined by many things including physical laws (which constrain how forms of energy and matter may be transformed), accidents of nature (which determine the presence of natural resources), and the outcomes of past decisions made both by others and by the individual.\n\nA value that holds true given particular constraints is a \"marginal\" value. A change that would be affected as or by a specific loosening or tightening of those constraints is a \"marginal\" change.\n\nNeoclassical economics usually assumes that marginal changes are infinitesimals or limits. (Though this assumption makes the analysis less robust, it increases tractability.) One is therefore often told that \"marginal\" is synonymous with \"very small\", though in more general analysis this may not be operationally true (and would not in any case be literally true). Frequently, economic analysis concerns the marginal values associated with a change of one unit of a resource, because decisions are often made in terms of units; marginalism seeks to explain unit prices in terms of such marginal values.\n\nThe marginal use of a good or service is the specific use to which an agent would put a given increase, or the specific use of the good or service that would be abandoned in response to a given decrease.\n\nMarginalism assumes, for any given agent, economic rationality and an ordering of possible states-of-the-world, such that, for any given set of constraints, there is an attainable state which is best in the eyes of that agent. Descriptive marginalism asserts that choice amongst the specific means by which various anticipated specific states-of-the-world (outcomes) might be affected is governed only by the distinctions amongst those specific outcomes; prescriptive marginalism asserts that such choice \"ought\" to be so governed.\n\nOn such assumptions, each increase would be put to the specific, feasible, previously unrealized use of greatest priority, and each decrease would result in abandonment of the use of lowest priority amongst the uses to which the good or service had been put.\n\nThe marginal utility of a good or service is the utility of its marginal use. Under the assumption of economic rationality, it is the utility of its least urgent possible use \"from\" the best feasible combination of actions in which its use is included.\n\nIn 20th century mainstream economics, the term \"utility\" has come to be formally defined as a \"quantification\" capturing preferences by assigning greater quantities to states, goods, services, or applications that are of higher priority. But marginalism and the concept of marginal utility predate the establishment of this convention within economics. The more general conception of utility is that of \"use\" or \"usefulness\", and this conception is at the heart of marginalism; the term \"marginal utility\" arose from translation of the German \"Grenznutzen\", which literally means \"border use\", referring directly to the marginal use, and the more general formulations of marginal utility do not treat quantification as an \"essential\" feature. On the other hand, none of the early marginalists insisted that utility were \"not\" quantified, some indeed treated quantification as an essential feature, and those who did not still used an assumption of quantification for expository purposes. In this context, it is not surprising to find many presentations that fail to recognize a more general approach.\n\nUnder the special case in which usefulness can be quantified, the change in utility of moving from state formula_1 to state formula_2 is\nMoreover, if formula_1 and formula_2 are distinguishable by values of just one variable formula_6 which is itself quantified, then it becomes possible to speak of the ratio of the marginal utility of the change in formula_6 to the size of that change:\n(where “c.p.” indicates that the \"only\" independent variable to change is formula_6).\n\nMainstream neoclassical economics will typically assume that\nis well defined, and use “marginal utility” to refer to a partial derivative\n\nThe \"law\" of diminishing marginal utility (also known as a \"Gossen's First Law\") is that, \"ceteris paribus\", as additional amounts of a good or service are added to available resources, their marginal utilities are decreasing. This \"law\" is sometimes treated as a tautology, sometimes as something proven by introspection, or sometimes as a mere instrumental assumption, adopted only for its perceived predictive efficacy. Actually, it is not quite any of these things, though it may have aspects of each. The \"law\" does not hold under all circumstances, so it is neither a tautology nor otherwise proveable; but it has a basis in prior observation.\n\nAn individual will typically be able to partially order the potential uses of a good or service. If there is scarcity, then a rational agent will satisfy wants of highest possible priority, so that no want is avoidably sacrificed to satisfy a want of \"lower\" priority. In the absence of complementarity across the uses, this will imply that the priority of use of any additional amount will be lower than the priority of the established uses, as in this famous example:\n\nHowever, if there \"is\" a complementarity across uses, then an amount added can bring things past a desired tipping point, or an amount subtracted cause them to fall short. In such cases, the marginal utility of a good or service might actually be \"increasing\".\n\nWithout the presumption that utility is quantified, the \"diminishing\" of utility should not be taken to be itself an arithmetic subtraction. It is the movement from use of higher to lower priority, and may be no more than a purely ordinal change.\n\nWhen quantification of utility is assumed, diminishing marginal utility corresponds to a utility function whose \"slope\" is continually or continuously decreasing. In the latter case, if the function is also smooth, then the “law” may be expressed\nNeoclassical economics usually supplements or supplants discussion of marginal utility with indifference curves, which were originally derived as the level curves of utility functions, or can be produced without presumption of quantification, but are often simply treated as axiomatic. In the absence of complementarity of goods or services, diminishing marginal utility implies convexity of indifference curves (though such convexity would also follow from quasiconcavity of the utility function).\n\nThe \"rate of substitution\" is the \"least favorable\" rate at which an agent is willing to exchange units of one good or service for units of another. The marginal rate of substitution (\"MRS\") is the rate of substitution at the margin – in other words, given some constraint(s).\n\nWhen goods and services are discrete, the least favorable rate at which an agent would trade A for B will usually be different from that at which she would trade B for A:\nBut, when the goods and services are continuously divisible, in the limiting case\nand the marginal rate of substitution is the slope of the indifference curve (multiplied by formula_15).\n\nIf, for example, Lisa will not trade a goat for anything less than two sheep, then her\nAnd if she will not trade a sheep for anything less than two goats, then her\nBut if she would trade one gram of banana for one ounce of ice cream \"and vice versa\", then\n\nWhen indifference curves (which are essentially graphs of instantaneous rates of substitution) and the convexity of those curves are not taken as given, the \"law\" of diminishing marginal utility is invoked to explain diminishing marginal rates of substitution – a willingness to accept fewer units of good or service formula_19 in substitution for formula_20 as one's holdings of formula_19 grow relative to those of formula_20. If an individual has a stock or flow of a good or service whose marginal utility is less than would be that of some other good or service for which he or she could trade, then it is in his or her interest to effect that trade. Of course, as one thing is traded-away and another is acquired, the respective marginal gains or losses from further trades are now changed. On the assumption that the marginal utility of one is diminishing, and the other is not increasing, all else being equal, an individual will demand an increasing ratio of that which is acquired to that which is sacrificed. (One important way in which all else might not be equal is when the use of the one good or service complements that of the other. In such cases, exchange ratios might be constant.) If any trader can better his or her own marginal position by offering an exchange more favorable to other traders with desired goods or services, then he or she will do so.\n\nAt the highest level of generality, a marginal cost is a marginal opportunity cost. In most contexts, however, \"marginal cost\" will refer to marginal \"pecuniary\" cost – that is to say marginal cost measured by forgone money.\n\nA thorough-going marginalism sees marginal cost as increasing under the \"law\" of diminishing marginal utility, because applying resources to one application reduces their availability to other applications. Neoclassical economics tends to disregard this argument, but to see marginal costs as increasing in consequence of diminishing returns.\n\nMarginalism and neoclassical economics typically explain price formation broadly through the interaction of curves or schedules of supply and demand. In any case buyers are modelled as pursuing typically lower quantities, and sellers offering typically higher quantities, as price is increased, with each being willing to trade until the marginal value of what they would trade-away exceeds that of the thing for which they would trade.\n\nDemand curves are explained by marginalism in terms of marginal rates of substitution.\n\nAt any given price, a prospective buyer has some marginal rate of substitution of money for the good or service in question. Given the \"law\" of diminishing marginal utility, or otherwise given convex indifference curves, the rates are such that the willingness to forgo money for the good or service decreases as the buyer would have ever more of the good or service and ever less money. Hence, any given buyer has a demand schedule that generally decreases in response to price (at least until quantity demanded reaches zero). The aggregate quantity demanded by all buyers is, at any given price, just the sum of the quantities demanded by individual buyers, so it too decreases as price increases.\n\nBoth neoclassical economics and thorough-going marginalism could be said to explain supply curves in terms of marginal cost; however, there are marked differences in conceptions of that cost.\n\nMarginalists in the tradition of Marshall and neoclassical economists tend to represent the supply curve for any producer as a curve of marginal pecuniary costs objectively determined by physical processes, with an upward slope determined by diminishing returns.\n\nA more thorough-going marginalism represents the supply curve as a \"complementary demand curve\" – where the demand is \"for\" money and the purchase is made \"with\" a good or service. The shape of that curve is then determined by marginal rates of substitution of money for that good or service.\n\nBy confining themselves to limiting cases in which sellers or buyers are both \"price takers\" – so that demand functions ignore supply functions or \"vice versa\" – Marshallian marginalists and neoclassical economists produced tractable models of \"pure\" or \"perfect\" competition and of various forms of \"imperfect\" competition, which models are usually captured by relatively simple graphs. Other marginalists have sought to present what they thought of as more realistic explanations, but this work has been relatively uninfluential on the mainstream of economic thought.\n\nThe \"law\" of diminishing marginal utility is said to explain the \"paradox of water and diamonds\", most commonly associated with Adam Smith (though recognized by earlier thinkers). Human beings cannot even survive without water, whereas diamonds, in Smith's day, were ornamentation or engraving bits. Yet water had a very small price, and diamonds a very large price. Marginalists explained that it is the \"marginal\" usefulness of any given quantity that matters, rather than the usefulness of a \"class\" or of a \"totality\". For most people, water was sufficiently abundant that the loss or gain of a gallon would withdraw or add only some very minor use if any, whereas diamonds were in much more restricted supply, so that the loss or gain was much greater.\n\nThat is not to say that the price of any good or service is simply a function of the marginal utility that it has for any one individual nor for some ostensibly typical individual. Rather, individuals are willing to trade based upon the respective marginal utilities of the goods that they have or desire (with these marginal utilities being distinct for each potential trader), and prices thus develop constrained by these marginal utilities.\n\nPerhaps the essence of a notion of diminishing marginal utility can be found in Aristotle's \"Politics\", wherein he writes \n\nA great variety of economists concluded that there was \"some\" sort of inter-relationship between utility and rarity that effected economic decisions, and in turn informed the determination of prices.\n\nEighteenth-century Italian mercantilists, such as Antonio Genovesi, Giammaria Ortes, Pietro Verri, Cesare Beccaria, and Giovanni Rinaldo, held that value was explained in terms of the general utility and of scarcity, though they did not typically work-out a theory of how these interacted. In \"Della Moneta\" (1751), Abbé Ferdinando Galiani, a pupil of Genovesi, attempted to explain value as a ratio of two ratios, \"utility\" and \"scarcity\", with the latter component ratio being the ratio of quantity to use.\n\nAnne Robert Jacques Turgot, in \"Réflexions sur la formation et la distribution de richesse\" (1769), held that value derived from the general utility of the class to which a good belonged, from comparison of present and future wants, and from anticipated difficulties in procurement.\n\nLike the Italian mercantilists, Étienne Bonnot de Condillac saw value as determined by utility associated with the class to which the good belongs, and by estimated scarcity. In \"De commerce et le gouvernement\" (1776), Condillac emphasized that value is not based upon cost but that costs were paid because of value.\n\nThis last point was famously restated by the Nineteenth Century proto-marginalist, Richard Whately, who in \"Introductory Lectures on Political Economy\" (1832) wrote (Whately's student Nassau William Senior is noted below as an early marginalist.)\n\nFrédéric Bastiat in chapters V and XI of his \"Economic Harmonies\" (1850) also develops a theory of value as ratio between services that increment utility, rather than between total utility.\n\nThe first unambiguous published statement of any sort of theory of marginal utility was by Daniel Bernoulli, in \"Specimen theoriae novae de mensura sortis\". This paper appeared in 1738, but a draft had been written in 1731 or in 1732. In 1728, Gabriel Cramer produced fundamentally the same theory in a private letter. Each had sought to resolve the St. Petersburg paradox, and had concluded that the marginal desirability of money decreased as it was accumulated, more specifically such that the desirability of a sum were the natural logarithm (Bernoulli) or square root (Cramer) thereof. However, the more general implications of this hypothesis were not explicated, and the work fell into obscurity.\n\nIn \"A Lecture on the Notion of Value as Distinguished Not Only from Utility, but also from Value in Exchange\", delivered in 1833 and included in \"Lectures on Population, Value, Poor Laws and Rent\" (1837), William Forster Lloyd explicitly offered a general marginal utility theory, but did not offer its derivation nor elaborate its implications. The importance of his statement seems to have been lost on everyone (including Lloyd) until the early 20th century, by which time others had independently developed and popularized the same insight.\n\nIn \"An Outline of the Science of Political Economy\" (1836), Nassau William Senior asserted that marginal utilities were the ultimate determinant of demand, yet apparently did not pursue implications, though some interpret his work as indeed doing just that.\n\nIn \"De la mesure de l'utilité des travaux publics\" (1844), Jules Dupuit applied a conception of marginal utility to the problem of determining bridge tolls.\n\nIn 1854, Hermann Heinrich Gossen published \"Die Entwicklung der Gesetze des menschlichen Verkehrs und der daraus fließenden Regeln für menschliches Handeln\", which presented a marginal utility theory and to a very large extent worked-out its implications for the behavior of a market economy. However, Gossen's work was not well received in the Germany of his time, most copies were destroyed unsold, and he was virtually forgotten until rediscovered after the so-called Marginal Revolution.\n\nMarginalism as a formal theory can be attributed to the work of three economists, Jevons in England, Menger in Austria, and Walras in Switzerland. William Stanley Jevons first proposed the theory in articles in 1863 and 1871. Similarly, Carl Menger presented the theory in 1871. Menger explained why individuals use marginal utility to decide amongst trade-offs, but while his illustrative examples present utility as quantified, his essential assumptions do not.\nLéon Walras introduced the theory in \"Éléments d'économie politique pure\", the first part of which was published in 1874. (American John Bates Clark is also associated with the origins of Marginalism, but did little to advance the theory.\n\nAlthough the Marginal Revolution flowed from the work of Jevons, Menger, and Walras, their work might have failed to enter the mainstream were it not for a second generation of economists. In England, the second generation were exemplified by Philip Wicksteed, by William Smart, and by Alfred Marshall; in Austria by Eugen Böhm von Bawerk and by Friedrich von Wieser; in Switzerland by Vilfredo Pareto; and in America by Herbert Joseph Davenport and by Frank A. Fetter.\n\nThere were significant, distinguishing features amongst the approaches of Jevons, Menger, and Walras, but the second generation did not maintain distinctions along national or linguistic lines. The work of von Wieser was heavily influenced by that of Walras. Wicksteed was heavily influenced by Menger. Fetter referred to himself and Davenport as part of \"the American Psychological School\", named in imitation of the Austrian \"Psychological School\". (And Clark's work from this period onward similarly shows heavy influence by Menger.) William Smart began as a conveyor of Austrian School theory to English-language readers, though he fell increasingly under the influence of Marshall.\n\nBöhm-Bawerk was perhaps the most able expositor of Menger's conception. He was further noted for producing a theory of interest and of profit in equilibrium based upon the interaction of diminishing marginal utility with diminishing marginal productivity of time and with time preference. (This theory was adopted in full and then further developed by Knut Wicksell and, with modifications including formal disregard for time-preference, by Wicksell's American rival Irving Fisher.)\n\nMarshall was the second-generation marginalist whose work on marginal utility came most to inform the mainstream of neoclassical economics, especially by way of his \"Principles of Economics\", the first volume of which was published in 1890. Marshall constructed the demand curve with the aid of assumptions that utility was quantified, and that the marginal utility of money was constant (or nearly so). Like Jevons, Marshall did not see an explanation for supply in the theory of marginal utility, so he paired a marginal explanation of demand with a more classical explanation of supply, wherein costs were taken to be objectively determined. (Marshall later actively mischaracterized the criticism that these costs were themselves ultimately determined by marginal utilities.)\n\nThe doctrines of marginalism and the Marginal Revolution are often interpreted as a response to the rise of the worker's movement, Marxian economics and the earlier (Ricardian) socialist theories of the exploitation of labour. The first volume of \"Das Kapital\" was not published until July 1867, when marginalism was already developing, but before the advent of Marxian economics, proto-marginalist ideas such as those of Gossen had largely fallen on deaf ears. It was only in the 1880s, when Marxism had come to the fore as the main economic theory of the workers' movement, that Gossen found (posthumous) recognition.\n\nAside from the rise of Marxism, E. Screpanti and S. Zamagni point to a different 'external' reason for marginalism's success, which is its successful response to the Long Depression and the resurgence of class conflict in all developed capitalist economies after the 1848-1870 period of social peace. Marginalism, Screpanti and Zamagni argue, offered a theory of the free market as perfect, as performing optimal allocation of resources, while it allowed economists to blame any adverse effects of laissez-faire economics on the interference of workers' coalitions in the proper functioning of the market.\n\nScholars have suggested that the success of the generation who followed the preceptors of the Revolution was their ability to formulate straightforward responses to Marxist economic theory. The most famous of these was that of Böhm-Bawerk, “Zum Abschluss des Marxschen Systems” (1896), but the first was Wicksteed's “The Marxian Theory of Value. \"Das Kapital\": a criticism” (1884, followed by “The Jevonian criticism of Marx: a rejoinder” in 1885). The most famous early Marxist responses were Rudolf Hilferding's \"Böhm-Bawerks Marx-Kritik\" (1904) and \"The Economic Theory of the Leisure Class\" (1914) by Nikolai Bukharin.\n\nIn his 1881 work \"Mathematical Psychics\", Francis Ysidro Edgeworth presented the indifference curve, deriving its properties from marginalist theory which assumed utility to be a differentiable function of quantified goods and services. But it came to be seen that indifference curves could be considered as somehow \"given\", without bothering with notions of utility.\n\nIn 1915, Eugen Slutsky derived a theory of consumer choice solely from properties of indifference curves. Because of the World War, the Bolshevik Revolution, and his own subsequent loss of interest, Slutsky's work drew almost no notice, but similar work in 1934 by John Hicks and R. G. D. Allen derived much the same results and found a significant audience. (Allen subsequently drew attention to Slutsky's earlier accomplishment.)\n\nAlthough some of the third generation of Austrian School economists had by 1911 rejected the quantification of utility while continuing to think in terms of marginal utility, most economists presumed that utility must be a sort of quantity. Indifference curve analysis seemed to represent a way of dispensing with presumptions of quantification, albeït that a seemingly arbitrary assumption (admitted by Hicks to be a \"rabbit out of a hat\") about decreasing marginal rates of substitution would then have to be introduced to have convexity of indifference curves.\n\nFor those who accepted that superseded marginal utility analysis had been superseded by indifference curve analysis, the former became at best somewhat analogous to the Bohr model of the atom—perhaps pedagogically useful, but “old fashioned” and ultimately incorrect.\n\nWhen Cramer and Bernoulli introduced the notion of diminishing marginal utility, it had been to address a paradox of gambling, rather than the paradox of value. The marginalists of the revolution, however, had been formally concerned with problems in which there was neither risk nor uncertainty. So too with the indifference curve analysis of Slutsky, Hicks, and Allen.\n\nThe expected utility hypothesis of Bernoulli \"et alii\" was revived by various 20th century thinkers, including Frank Ramsey (1926), John von Neumann and Oskar Morgenstern (1944), and Leonard Savage (1954). Although this hypothesis remains controversial, it brings not merely utility but a quantified conception thereof back into the mainstream of economic thought, and would dispatch the Ockhamistic argument. (It should perhaps be noted that, in expected utility analysis, the “law” of diminishing marginal utility corresponds to what is called “risk aversion”.)\n\nKarl Marx died before marginalism became the interpretation of economic value accepted by mainstream economics. His theory was based on the labor theory of value, which distinguishes between exchange value and use value. In his \"Capital\" he rejected the explanation of long-term market values by supply and demand:\n\nIn his early response to marginalism, Nikolai Bukharin argued that \"the subjective evaluation from which price is to be derived really starts from this price\", concluding:\n\nSimilarly a later Marxist critic, Ernest Mandel, argued that marginalism was \"divorced from reality\", ignored the role of production, and that:\n\nMaurice Dobb argued that prices derived through marginalism depend on the distribution of income. The ability of consumers to express their preferences is dependent on their spending power. As the theory asserts that prices arise in the act of exchange, Dobb argues that it cannot explain how the distribution of income affects prices and consequently cannot explain prices.\n\nDobb also criticized the \"motives\" behind marginal utility theory. Jevons wrote, for example, \"so far as is consistent with the inequality of wealth in every community, all commodities are distributed by exchange so as to produce the maximum social benefit.\" (See Fundamental theorems of welfare economics.) Dobb contended that this statement indicated that marginalism is intended to insulate market economics from criticism by making prices the natural result of the given income distribution.\n\nSome economists strongly influenced by the Marxian tradition such as Oskar Lange, Włodzimierz Brus, and Michał Kalecki have attempted to integrate the insights of classical political economy, marginalism, and neoclassical economics. They believed that Marx lacked a sophisticated theory of prices, and neoclassical economics lacked a theory of the social frameworks of economic activity. Some other Marxists have also argued that on one level there is no conflict between marginalism and Marxism: one could employ a marginalist theory of supply and demand within the context of a “big picture” understanding of the Marxist notion that capitalists exploit surplus labor.\n\n\n"}
{"id": "599917", "url": "https://en.wikipedia.org/wiki?curid=599917", "title": "Mental image", "text": "Mental image\n\nA mental image or mental picture is the representation in a person's mind of the physical world outside that person. It is an experience that, on most occasions, significantly resembles the experience of perceiving some object, event, or scene, but occurs when the relevant object, event, or scene is not actually present to the senses. There are sometimes episodes, particularly on falling asleep (hypnagogic imagery) and waking up (hypnopompic), when the mental imagery, being of a rapid, phantasmagoric and involuntary character, defies perception, presenting a kaleidoscopic field, in which no distinct object can be discerned. Mental imagery can sometimes produce the same effects as would be produced by the behavior or experience imagined.\n\nThe nature of these experiences, what makes them possible, and their function (if any) have long been subjects of research and controversy in philosophy, psychology, cognitive science, and, more recently, neuroscience. As contemporary researchers use the expression, mental images or imagery can comprise information from any source of sensory input; one may experience auditory images, olfactory images, and so forth. However, the majority of philosophical and scientific investigations of the topic focus upon \"visual\" mental imagery. It has sometimes been assumed that, like humans, some types of animals are capable of experiencing mental images. Due to the fundamentally introspective nature of the phenomenon, there is little to no evidence either for or against this view.\n\nPhilosophers such as George Berkeley and David Hume, and early experimental psychologists such as Wilhelm Wundt and William James, understood ideas in general to be mental images. Today it is very widely believed that much imagery functions as mental representations (or mental models), playing an important role in memory and thinking. William Brant (2013, p. 12) traces the scientific use of the phrase \"mental images\" back to John Tyndall's 1870 speech called the \"Scientific Use of the Imagination\". Some have gone so far as to suggest that images are best understood to be, by definition, a form of inner, mental or neural representation; in the case of hypnagogic and hypnapompic imagery, it is not representational at all. Others reject the view that the image experience may be identical with (or directly caused by) any such representation in the mind or the brain, but do not take account of the non-representational forms of imagery.\n\nIn 2010, IBM applied for a patent on a method to extract mental images of human faces from the human brain. It uses a feedback loop based on brain measurements of the fusiform face area in the brain that activates proportionate with degree of facial recognition. It was issued in 2015.\n\nThe notion of a \"mind's eye\" goes back at least to Cicero's reference to mentis oculi during his discussion of the orator's appropriate use of simile.\n\nIn this discussion, Cicero observed that allusions to \"the Syrtis of his patrimony\" and \"the Charybdis of his possessions\" involved similes that were \"too far-fetched\"; and he advised the orator to, instead, just speak of \"the rock\" and \"the gulf\" (respectively)—on the grounds that \"the eyes of the mind are more easily directed to those objects which we have seen, than to those which we have only heard\".\n\nThe concept of \"the mind's eye\" first appeared in English in Chaucer's (c. 1387) Man of Law's Tale in his \"Canterbury Tales\", where he tells us that one of the three men dwelling in a castle was blind, and could only see with \"the eyes of his mind\"; namely, those eyes \"with which all men see after they have become blind\".\n\nThe biological foundation of the mind's eye is not fully understood. Studies using fMRI have shown that the lateral geniculate nucleus and the V1 area of the visual cortex are activated during mental imagery tasks. Ratey writes:\nThe visual pathway is not a one-way street. Higher areas of the brain can also send visual input back to neurons in lower areas of the visual cortex. [...] As humans, we have the ability to see with the mind's eye – to have a perceptual experience in the absence of visual input. For example, PET scans have shown that when subjects, seated in a room, imagine they are at their front door starting to walk either to the left or right, activation begins in the visual association cortex, the parietal cortex, and the prefrontal cortex - all higher cognitive processing centers of the brain.\n\nThe rudiments of a biological basis for the mind's eye is found in the deeper portions of the brain below the neocortex, or where the center of perception exists. The thalamus has been found to be discrete to other components in that it processes all forms of perceptional data relayed from both lower and higher components of the brain. Damage to this component can produce permanent perceptual damage, however when damage is inflicted upon the cerebral cortex, the brain adapts to neuroplasticity to amend any occlusions for perception. It can be thought that the neocortex is a sophisticated memory storage warehouse in which data received as an input from sensory systems are compartmentalized via the cerebral cortex. This would essentially allow for shapes to be identified, although given the lack of filtering input produced internally, one may as a consequence, hallucinate - essentially seeing something that isn't received as an input externally but rather internal (i.e. an error in the filtering of segmented sensory data from the cerebral cortex may result in one seeing, feeling, hearing or experiencing something that is inconsistent with reality).\n\nNot all people have the same internal perceptual ability. For many, when the eyes are closed, the perception of darkness prevails. However, some people are able to perceive colorful, dynamic imagery. The use of hallucinogenic drugs increases the subject's ability to consciously access visual (and auditory, and other sense) percepts.\n\nFurthermore, the pineal gland is a hypothetical candidate for producing a mind's eye; Rick Strassman and others have postulated that during near-death experiences (NDEs) and dreaming, the gland might secrete a hallucinogenic chemical \"N\",\"N\"-Dimethyltryptamine (DMT) to produce internal visuals when external sensory data is occluded. However, this hypothesis has yet to be fully supported with neurochemical evidence and plausible mechanism for DMT production.\n\nThe hypothesized condition where a person lacks a mind's eye is called aphantasia. The term was first suggested in a 2015 study.\n\nCommon examples of mental images include daydreaming and the mental visualization that occurs while reading a book. Another is of the pictures summoned by athletes during training or before a competition, outlining each step they will take to accomplish their goal. When a musician hears a song, he or she can sometimes \"see\" the song notes in their head, as well as hear them with all their tonal qualities. This is considered different from an after-effect, such as an after-image. Calling up an image in our minds can be a voluntary act, so it can be characterized as being under various degrees of conscious control.\n\nAccording to psychologist and cognitive scientist Steven Pinker, our experiences of the world are represented in our minds as mental images. These mental images can then be associated and compared with others, and can be used to synthesize completely new images. In this view, mental images allow us to form useful theories of how the world works by formulating likely sequences of mental images in our heads without having to directly experience that outcome. Whether other creatures have this capability is debatable.\n\nThere are several theories as to how mental images are formed in the mind. These include the dual-code theory, the propositional theory, and the functional-equivalency hypothesis. The dual-code theory, created by Allan Paivio in 1971, is the theory that we use two separate codes to represent information in our brains: image codes and verbal codes. Image codes are things like thinking of a picture of a dog when you are thinking of a dog, whereas a verbal code would be to think of the word \"dog\". Another example is the difference between thinking of abstract words such as \"justice\" or \"love\" and thinking of concrete words like \"elephant\" or \"chair.\" When abstract words are thought of, it is easier to think of them in terms of verbal codes—finding words that define them or describe them. With concrete words, it is often easier to use image codes and bring up a picture of a \"human\" or \"chair\" in your mind rather than words associated or descriptive of them.\n\nThe propositional theory involves storing images in the form of a generic propositional code that stores the meaning of the concept not the image itself. The propositional codes can either be descriptive of the image or symbolic. They are then transferred back into verbal and visual code to form the mental image.\n\nThe functional-equivalency hypothesis is that mental images are \"internal representations\" that work in the same way as the actual perception of physical objects. In other words, the picture of a dog brought to mind when the word \"dog\" is read is interpreted in the same way as if the person looking at an actual dog before them.\n\nResearch has occurred to designate a specific neural correlate of imagery; however, studies show a multitude of results. Most studies published before 2001 suggest neural correlates of visual imagery occur in brodmann area 17. Auditory performance imagery have been observed in the premotor areas, precunes, and medial brodmann area 40. Auditory imagery in general occurs across participants in the temporal voice area (TVA), which allows top-down imaging manipulations, processing, and storage of audition functions. Olfactory imagery research shows activation in the anterior piriform cortex and the posterior piriform cortex; experts in olfactory imagery have larger gray matter associated to olfactory areas. Tactile imagery is found to occur in the dorsolateral prefrontal area, inferior frontal gyrus, frontal gyrus, insula, precentral gyrus, and the medial frontal gyrus with basil ganglia activation in the ventral posteriomedial nucleus and putamen (hemisphere activation corresponds to the location of the imagined tactile stimulus). Research in gustatory imagery reveals activation in the anterior insular cortex, frontal operculum, and prefrontal cortex. Novices of a specific form of mental imagery show less gray matter than experts of mental imagery congruent to that form. A meta-analysis of neuroimagery studies revealed significant activation of the bilateral dorsal parietal, interior insula, and left inferior frontal regions of the brain.\n\nImagery has been thought to cooccur with perception; however, participants with damaged sense-modality receptors can sometimes perform imagery of said modality receptors. Neuroscience with imagery has been used to communicate with seemingly unconscious individuals through fMRI activation of different neural correlates of imagery, demanding further study into low quality consciousness. A study on one patient with one occipital lobe removed found the horizontal area of their visual mental image was reduced.\n\nVisual imagery is the ability to create mental representations of things, people, and places that are absent from an individual’s visual field. This ability is crucial to problem-solving tasks, memory, and spatial reasoning. Neuroscientists have found that imagery and perception share many of the same neural substrates, or areas of the brain that function similarly during both imagery and perception, such as the visual cortex and higher visual areas. Kosslyn and colleagues (1999) showed that the early visual cortex, Area 17 and Area 18/19, is activated during visual imagery. They found that inhibition of these areas through repetitive transcranial magnetic stimulation (rTMS) resulted in impaired visual perception and imagery. Furthermore, research conducted with lesioned patients has revealed that visual imagery and visual perception have the same representational organization. This has been concluded from patients in which impaired perception also experience visual imagery deficits at the same level of the mental representation.\n\nBehrmann and colleagues (1992) describe a patient C.K., who provided evidence challenging the view that visual imagery and visual perception rely on the same representational system. C.K. was a 33-year old man with visual object agnosia acquired after a vehicular accident. This deficit prevented him from being able to recognize objects and copy objects fluidly. Surprisingly, his ability to draw accurate objects from memory indicated his visual imagery was intact and normal. Furthermore, C.K. successfully performed other tasks requiring visual imagery for judgment of size, shape, color, and composition. These findings conflict with previous research as they suggest there is a partial dissociation between visual imagery and visual perception. C.K. exhibited a perceptual deficit that was not associated with a corresponding deficit in visual imagery, indicating that these two processes have systems for mental representations that may not be mediated entirely by the same neural substrates. \n\nSchlegel and colleagues (2013) conducted a functional MRI analysis of regions activated during manipulation of visual imagery. They identified 11 bilateral cortical and subcortical regions that exhibited increased activation when manipulating a visual image compared to when the visual image was just maintained. These regions included the occipital lobe and ventral stream areas, two parietal lobe regions, the posterior parietal cortex and the precuneus lobule, and three frontal lobe regions, the frontal eye fields, dorsolateral prefrontal cortex, and the prefrontal cortex. Due to their suspected involvement in working memory and attention, the authors propose that these parietal and prefrontal regions, and occipital regions, are part of a network involved in mediating the manipulation of visual imagery. These results suggest a top-down activation of visual areas in visual imagery.\n\nUsing Dynamic Causal Modeling (DCM) to determine the connectivity of cortical networks, Ishai et al. (2010) demonstrated that activation of the network mediating visual imagery is initiated by prefrontal cortex and posterior parietal cortex activity. Generation of objects from memory resulted in initial activation of the prefrontal and the posterior parietal areas, which then activate earlier visual areas through backward connectivity. Activation of the prefrontal cortex and posterior parietal cortex has also been found to be involved in retrieval of object representations from long-term memory, their maintenance in working memory, and attention during visual imagery. Thus, Ishai et al. suggest that the network mediating visual imagery is composed of attentional mechanisms arising from the posterior parietal cortex and the prefrontal cortex.\n\nVividness of visual imagery is a crucial component of an individual’s ability to perform cognitive tasks requiring imagery. Vividness of visual imagery varies not only between individuals but also within individuals. Dijkstra and colleagues (2017) found that the variation in vividness of visual imagery is dependent on the degree to which the neural substrates of visual imagery overlap with those of visual perception. They found that overlap between imagery and perception in the entire visual cortex, the parietal precuneus lobule, the right parietal cortex, and the medial frontal cortex predicted the vividness of a mental representation. The activated regions beyond the visual areas are believed to drive the imagery-specific processes rather than the visual processes shared with perception. It has been suggested that the precuneus contributes to vividness by selecting important details for imagery. The medial frontal cortex is suspected to be involved in the retrieval and integration of information from the parietal and visual areas during working memory and visual imagery. The right parietal cortex appears to be important in attention, visual inspection, and stabilization of mental representations. Thus, the neural substrates of visual imagery and perception overlap in areas beyond the visual cortex and the degree of this overlap in these areas correlates with the vividness of mental representations during imagery.\n\nMental images are an important topic in classical and modern philosophy, as they are central to the study of knowledge. In the \"Republic\", Book VII, Plato has Socrates present the Allegory of the Cave: a prisoner, bound and unable to move, sits with his back to a fire watching the shadows cast on the cave wall in front of him by people carrying objects behind his back. These people and the objects they carry are representations of real things in the world. Unenlightened man is like the prisoner, explains Socrates, a human being making mental images from the sense data that he experiences.\n\nThe eighteenth-century philosopher Bishop George Berkeley proposed similar ideas in his theory of idealism. Berkeley stated that reality is equivalent to mental images—our mental images are not a copy of another material reality but that reality itself. Berkeley, however, sharply distinguished between the images that he considered to constitute the external world, and the images of individual imagination. According to Berkeley, only the latter are considered \"mental imagery\" in the contemporary sense of the term.\n\nThe eighteenth century British writer Dr. Samuel Johnson criticized idealism. When asked what he thought about idealism, he is alleged to have replied \"I refute it thus!\" as he kicked a large rock and his leg rebounded. His point was that the idea that the rock is just another mental image and has no material existence of its own is a poor explanation of the painful sense data he had just experienced.\n\nDavid Deutsch addresses Johnson's objection to idealism in \"The Fabric of Reality\" when he states that, if we judge the value of our mental images of the world by the quality and quantity of the sense data that they can explain, then the most valuable mental image—or theory—that we currently have is that the world has a real independent existence and that humans have successfully evolved by building up and adapting patterns of mental images to explain it. This is an important idea in scientific thought.\n\nCritics of scientific realism ask how the inner perception of mental images actually occurs. This is sometimes called the \"homunculus problem\" (see also the mind's eye). The problem is similar to asking how the images you see on a computer screen exist in the memory of the computer. To scientific materialism, mental images and the perception of them must be brain-states. According to critics, scientific realists cannot explain where the images and their perceiver exist in the brain. To use the analogy of the computer screen, these critics argue that cognitive science and psychology have been unsuccessful in identifying either the component in the brain (i.e., \"hardware\") or the mental processes that store these images (i.e. \"software\").\n\nCognitive psychologists and (later) cognitive neuroscientists have empirically tested some of the philosophical questions related to whether and how the human brain uses mental imagery in cognition.\n\nOne theory of the mind that was examined in these experiments was the \"brain as serial computer\" philosophical metaphor of the 1970s. Psychologist Zenon Pylyshyn theorized that the human mind processes mental images by decomposing them into an underlying mathematical proposition. Roger Shepard and Jacqueline Metzler challenged that view by presenting subjects with 2D line drawings of groups of 3D block \"objects\" and asking them to determine whether that \"object\" is the same as a second figure, some of which rotations of the first \"object\". Shepard and Metzler proposed that if we decomposed and then mentally re-imaged the objects into basic mathematical propositions, as the then-dominant view of cognition \"as a serial digital computer\" assumed, then it would be expected that the time it took to determine whether the object is the same or not would be independent of how much the object had been rotated. Shepard and Metzler found the opposite: a linear relationship between the degree of rotation in the mental imagery task and the time it took participants to reach their answer.\n\nThis mental rotation finding implied that the human mind—and the human brain—maintains and manipulates mental images as topographic and topological wholes, an implication that was quickly put to test by psychologists. Stephen Kosslyn and colleagues showed in a series of neuroimaging experiments that the mental image of objects like the letter \"F\" are mapped, maintained and rotated as an image-like whole in areas of the human visual cortex. Moreover, Kosslyn's work showed that there are considerable similarities between the neural mappings for imagined stimuli and perceived stimuli. The authors of these studies concluded that, while the neural processes they studied rely on mathematical and computational underpinnings, the brain also seems optimized to handle the sort of mathematics that constantly computes a series of topologically-based images rather than calculating a mathematical model of an object.\n\nRecent studies in neurology and neuropsychology on mental imagery have further questioned the \"mind as serial computer\" theory, arguing instead that human mental imagery manifests both visually and kinesthetically. For example, several studies have provided evidence that people are slower at rotating line drawings of objects such as hands in directions incompatible with the joints of the human body, and that patients with painful, injured arms are slower at mentally rotating line drawings of the hand from the side of the injured arm.\n\nSome psychologists, including Kosslyn, have argued that such results occur because of interference in the brain between distinct systems in the brain that process the visual and motor mental imagery. Subsequent neuroimaging studies showed that the interference between the motor and visual imagery system could be induced by having participants physically handle actual 3D blocks glued together to form objects similar to those depicted in the line-drawings. Amorim et al. have shown that, when a cylindrical \"head\" was added to Shepard and Metzler's line drawings of 3D block figures, participants were quicker and more accurate at solving mental rotation problems. They argue that motoric embodiment is not just \"interference\" that inhibits visual mental imagery but is capable of facilitating mental imagery.\n\nAs cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain’s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain’s visual areas while subjects imagined visual objects and scenes.\n\nThe previously mentioned and numerous related studies have led to a relative consensus within cognitive science, psychology, neuroscience, and philosophy on the neural status of mental images. In general, researchers agree that, while there is no homunculus inside the head viewing these mental images, our brains do form and maintain mental images as image-like wholes. The problem of exactly how these images are stored and manipulated within the human brain, in particular within language and communication, remains a fertile area of study.\n\nOne of the longest-running research topics on the mental image has basis on the fact that people report large individual differences in the vividness of their images. Special questionnaires have been developed to assess such differences, including the Vividness of Visual Imagery Questionnaire (VVIQ) developed by David Marks. Laboratory studies have suggested that the subjectively reported variations in imagery vividness are associated with different neural states within the brain and also different cognitive competences such as the ability to accurately recall information presented in pictures Rodway, Gillies and Schepman used a novel long-term change detection task to determine whether participants with low and high vividness scores on the VVIQ2 showed any performance differences. Rodway et al. found that high vividness participants were significantly more accurate at detecting salient changes to pictures compared to low-vividness participants. This replicated an earlier study.\n\nRecent studies have found that individual differences in VVIQ scores can be used to predict changes in a person's brain while visualizing different activities. Functional magnetic resonance imaging (fMRI) was used to study the association between early visual cortex activity relative to the whole brain while participants visualized themselves or another person bench pressing or stair climbing. Reported image vividness correlates significantly with the relative fMRI signal in the visual cortex. Thus, individual differences in the vividness of visual imagery can be measured objectively.\n\nLogie, Pernet, Buonocore and Della Sala (2011) used behavioural and fMRI data for mental rotation from individuals reporting vivid and poor imagery on the VVIQ. Groups differed in brain activation patterns suggesting that the groups performed the same tasks in different ways. These findings help to explain the lack of association previously reported between VVIQ scores and mental rotation performance.\n\nSome educational theorists have drawn from the idea of mental imagery in their studies of learning styles. Proponents of these theories state that people often have learning processes that emphasize visual, auditory, and kinesthetic systems of experience. According to these theorists, teaching in multiple overlapping sensory systems benefits learning, and they encourage teachers to use content and media that integrates well with the visual, auditory, and kinesthetic systems whenever possible.\n\nEducational researchers have examined whether the experience of mental imagery affects the degree of learning. For example, imagining playing a 5-finger piano exercise (mental practice) resulted in a significant improvement in performance over no mental practice—though not as significant as that produced by physical practice. The authors of the study stated that \"mental practice alone seems to be sufficient to promote the modulation of neural circuits involved in the early stages of motor skill learning\".\n\nIn general, Vajrayana Buddhism, Bön, and Tantra utilize sophisticated visualization or \"imaginal\" (in the language of Jean Houston of Transpersonal Psychology) processes in the thoughtform construction of the yidam sadhana, kye-rim, and dzog-rim modes of meditation and in the yantra, thangka, and mandala traditions, where holding the fully realized form in the mind is a prerequisite prior to creating an 'authentic' new art work that will provide a sacred support or foundation for deity.\n\nMental imagery can act as a substitute for the imagined experience: Imagining an experience can evoke similar cognitive, physiological, and/or behavioral consequences as having the corresponding experience in reality. At least four classes of such effects have been documented.\n\n"}
{"id": "994704", "url": "https://en.wikipedia.org/wiki?curid=994704", "title": "Mental model", "text": "Mental model\n\nA mental model is an explanation of someone's thought process about how something works in the real world. It is a representation of the surrounding world, the relationships between its various parts and a person's intuitive perception about his or her own acts and their consequences. Mental models can help shape behaviour and set an approach to solving problems (similar to a personal algorithm) and doing tasks.\n\nA mental model is a kind of internal symbol or representation of external reality, hypothesized to play a major role in cognition, reasoning and decision-making. Kenneth Craik suggested in 1943 that the mind constructs \"small-scale models\" of reality that it uses to anticipate events.\n\nJay Wright Forrester defined general mental models as:\nThe image of the world around us, which we carry in our head, is just a model. Nobody in his head imagines all the world, government or country. He has only selected concepts, and relationships between them, and uses those to represent the real system (Forrester, 1971).\n\nIn psychology, the term \"mental models\" is sometimes used to refer to mental representations or mental simulation generally. At other times it is used to refer to and to the mental model theory of reasoning developed by Philip Johnson-Laird and Ruth M.J. Byrne.\n\nThe term \"mental model\" is believed to have originated with Kenneth Craik in his 1943 book \"The Nature of Explanation\". in \"Le dessin enfantin\" (Children's drawings), published in 1927 by Alcan, Paris, argued that children construct internal models, a view that influenced, among others, child psychologist Jean Piaget.\n\nPhilip Johnson-Laird published \"Mental Models: Towards a Cognitive Science of Language, Inference and Consciousness\" in 1983. In the same year, Dedre Gentner and Albert Stevens edited a collection of chapters in a book also titled \"Mental Models\". The first line of their book explains the idea further: \"One function of this chapter is to belabor the obvious; people's views of the world, of themselves, of their own capabilities, and of the tasks that they are asked to perform, or topics they are asked to learn, depend heavily on the conceptualizations that they bring to the task.\" (see the book: \"Mental Models\").\n\nSince then, there has been much discussion and use of the idea in human-computer interaction and usability by researchers including Donald Norman and Steve Krug (in his book \"Don't Make Me Think\"). Walter Kintsch and Teun A. van Dijk, using the term \"situation model\" (in their book \"Strategies of Discourse Comprehension\", 1983), showed the relevance of mental models for the production and comprehension of discourse.\n\nOne view of human reasoning is that it depends on mental models. In this view, mental models can be constructed from perception, imagination, or the comprehension of discourse (Johnson-Laird, 1983). Such mental models are similar to architects' models or to physicists' diagrams in that their structure is analogous to the structure of the situation that they represent, unlike, say, the structure of logical forms used in formal rule theories of reasoning. In this respect, they are a little like pictures in the picture theory of language described by philosopher Ludwig Wittgenstein in 1922. Philip Johnson-Laird and Ruth M.J. Byrne developed a theory of mental models which makes the assumption that reasoning depends, not on logical form, but on mental models (Johnson-Laird and Byrne, 1991).\n\nMental models are based on a small set of fundamental assumptions (axioms), which distinguish them from other proposed representations in the psychology of reasoning (Byrne and Johnson-Laird, 2009). Each mental model represents a possibility. A mental model represents one possibility, capturing what is common to all the different ways in which the possibility may occur (Johnson-Laird and Byrne, 2002). Mental models are iconic, i.e., each part of a model corresponds to each part of what it represents (Johnson-Laird, 2006). Mental models are based on a principle of truth: they typically represent only those situations that are possible, and each model of a possibility represents only what is true in that possibility according to the proposition. However, mental models can represent what is false, temporarily assumed to be true, for example, in the case of counterfactual conditionals and counterfactual thinking (Byrne, 2005).\n\nPeople infer that a conclusion is valid if it holds in all the possibilities. Procedures for reasoning with mental models rely on counter-examples to refute invalid inferences; they establish validity by ensuring that a conclusion holds over all the models of the premises. Reasoners focus on a subset of the possible models of multiple-model problems, often just a single model. The ease with which reasoners can make deductions is affected by many factors, including age and working memory (Barrouillet, et al., 2000). They reject a conclusion if they find a counterexample, i.e., a possibility in which the premises hold, but the conclusion does not (Schroyens, et al. 2003; Verschueren, et al., 2005).\n\nScientific debate continues about whether human reasoning is based on mental models, versus formal rules of inference (e.g., O'Brien, 2009), domain-specific rules of inference (e.g., Cheng & Holyoak, 2008; Cosmides, 2005), or probabilities (e.g., Oaksford and Chater, 2007). Many empirical comparisons of the different theories have been carried out (e.g., Oberauer, 2006).\n\nA mental model is generally:\n\nMental models are a fundamental way to understand organizational learning. Mental models, in popular science parlance, have been described as \"deeply held images of thinking and acting\". Mental models are so basic to understanding the world that people are hardly conscious of them.\n\nS.N. Groesser and M. Schaffernicht (2012) describe three basic methods which are typically used:\nThese methods allow showing a mental model of a dynamic system, as an explicit, written model about a certain system based on internal beliefs. Analyzing these graphical representations has been an increasing area of research across many social science fields. Additionally software tools that attempt to capture and analyze the structural and functional properties of individual mental models such as Mental Modeler, \"a participatory modeling tool based in fuzzy-logic cognitive mapping\", have recently been developed and used to collect/compare/combine mental model representations collected from individuals for use in social science research, collaborative decision-making, and natural resource planning.\n\nIn the simplification of reality, creating a model can find a sense of reality, seeking to overcome systemic thinking and system dynamics.\n\nThese two disciplines can help to construct a better coordination with the reality of mental models and simulate it accurately. They increase the probability that the consequences of how to decide and act in accordance with how to plan.\n\n\nAfter analyzing the basic characteristics, it is necessary to bring the process of changing the mental models, or the process of learning. Learning is a back-loop process, and feedback loops can be illustrated as: single-loop learning or double-loop learning.\n\nMental models affect the way that people work with information, and also how they determine the final decision. The decision itself changes, but the mental models remain the same. It is the predominant method of learning, because it is very convenient.\n\nDouble-loop learning (\"see diagram below\") is used when it is necessary to change the mental model on which a decision depends. Unlike single loops, this model includes a shift in understanding, from simple and static to broader and more dynamic, such as taking into account the changes in the surroundings and the need for expression changes in mental models.\n\n\n\n"}
{"id": "161019", "url": "https://en.wikipedia.org/wiki?curid=161019", "title": "Negation", "text": "Negation\n\nIn logic, negation, also called the logical complement, is an operation that takes a proposition formula_1 to another proposition \"not formula_1\", written formula_3 (¬P), which is interpreted intuitively as being true when formula_1 is false, and false when formula_1 is true. Negation is thus a unary (single-argument) logical connective. It may be applied as an operation on notions, propositions, truth values, or semantic values more generally. In classical logic, negation is normally identified with the truth function that takes \"truth\" to \"falsity\" and vice versa. In intuitionistic logic, according to the Brouwer–Heyting–Kolmogorov interpretation, the negation of a proposition formula_1 is the proposition whose proofs are the refutations of formula_1.\n\nNo agreement exists as to the possibility of defining negation, as to its logical status, function, and meaning, as to its field of applicability..., and as to the interpretation of the negative judgment, (F.H. Heinemann 1944).\n\n\"Classical negation\" is an operation on one logical value, typically the value of a proposition, that produces a value of \"true\" when its operand is false and a value of \"false\" when its operand is true. So, if statement formula_1 is true, then formula_3 (pronounced \"not P\") would therefore be false; and conversely, if formula_3 is false, then formula_1 would be true.\n\nThe truth table of formula_3 is as follows:\n\nNegation can be defined in terms of other logical operations. For example, formula_3 can be defined as formula_14 (where formula_15 is logical consequence and formula_16 is absolute falsehood). Conversely, one can define formula_16 as formula_18 for any proposition formula_19 (where formula_20 is logical conjunction). The idea here is that any contradiction is false. While these ideas work in both classical and intuitionistic logic, they do not work in paraconsistent logic, where contradictions are not necessarily false. In classical logic, we also get a further identity, formula_21 can be defined as formula_22, where formula_23 is logical disjunction.\n\nAlgebraically, classical negation corresponds to complementation in a Boolean algebra, and intuitionistic negation to pseudocomplementation in a Heyting algebra. These algebras provide a semantics for classical and intuitionistic logic respectively.\n\nThe negation of a proposition formula_1 is notated in different ways in various contexts of discussion and fields of application. Among these variants are the following:\n\nThe notation N\"p\" is Łukasiewicz notation.\n\nIn set theory formula_25 is also used to indicate 'not member of': formula_26 is the set of all members of formula_27 that are not members of formula_28.\n\nNo matter how it is notated or symbolized, the negation formula_3 can be read as \"it is not the case that formula_1\", \"not that formula_1\", or usually more simply as \"not formula_1\".\n\nWithin a system of classical logic, double negation, that is, the negation of the negation of a proposition formula_1, is logically equivalent to formula_1. Expressed in symbolic terms, formula_35. In intuitionistic logic, a proposition implies its double negation but not conversely. This marks one important difference between classical and intuitionistic negation. Algebraically, classical negation is called an involution of period two.\n\nHowever, in intuitionistic logic we do have the equivalence of formula_36. Moreover, in the propositional case, a sentence is classically provable if its double negation is intuitionistically provable. This result is known as Glivenko's theorem.\n\nDe Morgan's laws provide a way of distributing negation over disjunction and conjunction :\n\nLet formula_39 denote the logical xor operation. In Boolean algebra, a linear function is one such that:\n\nIf there exists formula_40,\nformula_41,\nfor all formula_42.\n\nAnother way to express this is that each variable always makes a difference in the truth-value of the operation or it never makes a difference. Negation is a linear logical operator.\n\nIn Boolean algebra a self dual function is one such that:\n\nformula_43 for all\nformula_44.\nNegation is a self dual logical operator.\n\nThere are a number of equivalent ways to formulate rules for negation. One usual way to formulate classical negation in a natural deduction setting is to take as primitive rules of inference \"negation introduction\" (from a derivation of formula_1 to both formula_19 and formula_47, infer formula_3; this rule also being called \"reductio ad absurdum\"), \"negation elimination\" (from formula_1 and formula_3 infer formula_19; this rule also being called \"ex falso quodlibet\"), and \"double negation elimination\" (from formula_52 infer formula_1). One obtains the rules for intuitionistic negation the same way but by excluding double negation elimination.\n\nNegation introduction states that if an absurdity can be drawn as conclusion from formula_1 then formula_1 must not be the case (i.e. formula_1 is false (classically) or refutable (intuitionistically) or etc.). Negation elimination states that anything follows from an absurdity. Sometimes negation elimination is formulated using a primitive absurdity sign formula_16. In this case the rule says that from formula_1 and formula_3 follows an absurdity. Together with double negation elimination one may infer our originally formulated rule, namely that anything follows from an absurdity.\n\nTypically the intuitionistic negation formula_3 of formula_1 is defined as formula_14. Then negation introduction and elimination are just special cases of implication introduction (conditional proof) and elimination (modus ponens). In this case one must also add as a primitive rule \"ex falso quodlibet\".\n\nAs in mathematics, negation is used in computer science to construct logical statements.\n\nThe \"codice_1\" signifies logical NOT in B, C, and languages with a C-inspired syntax such as C++, Java, JavaScript, Perl, and PHP. \"codice_2\" is the operator used in ALGOL 60, BASIC, and languages with an ALGOL- or BASIC-inspired syntax such as Pascal, Ada, Eiffel and Seed7. Some languages (C++, Perl, etc.) provide more than one operator for negation. A few languages like PL/I and Ratfor use codice_3 for negation. Some modern computers and operating systems will display codice_3 as codice_1 on files encoded in ASCII. Most modern languages allow the above statement to be shortened from codice_6 to codice_7, which allows sometimes, when the compiler/interpreter is not able to optimize it, faster programs.\n\nIn computer science there is also \"bitwise negation\". This takes the value given and switches all the binary 1s to 0s and 0s to 1s. See bitwise operation. This is often used to create ones' complement or \"codice_8\" in C or C++ and two's complement (just simplified to \"codice_9\" or the negative sign since this is equivalent to taking the arithmetic negative value of the number) as it basically creates the opposite (negative value equivalent) or mathematical complement of the value (where both values are added together they create a whole).\n\nTo get the absolute (positive equivalent) value of a given integer the following would work as the \"codice_9\" changes it from negative to positive (it is negative because \"codice_11\" yields true)\n\nTo demonstrate logical negation:\n\nInverting the condition and reversing the outcomes produces code that is logically equivalent to the original code, i.e. will have identical results for any input (note that depending on the compiler used, the actual instructions performed by the computer may differ).\n\nThis convention occasionally surfaces in written speech, as computer-related slang for \"not\". The phrase codice_12, for example, means \"not voting\".\n\nIn Kripke semantics where the semantic values of formulae are sets of possible worlds, negation can be taken to mean set-theoretic complementation. (See also possible world semantics.)\n\n\n"}
{"id": "3558732", "url": "https://en.wikipedia.org/wiki?curid=3558732", "title": "Negative and positive rights", "text": "Negative and positive rights\n\nNegative and positive rights are rights that oblige either action (\"positive rights\") or inaction (\"negative rights\"). These obligations may be of either a legal or moral character. The notion of positive and negative rights may also be applied to liberty rights.\n\nTo take an example involving two parties in a court of law: Adrian has a \"negative right to x\" against Clay if and only if Clay is \"prohibited\" from acting upon Adrian in some way regarding \"x\". In contrast, Adrian has a \"positive right to x\" against Clay if and only if Clay is obliged to act upon Adrian in some way regarding \"x\". A case in point, if Adrian has a \"negative right to life\" against Clay, then Clay is required to refrain from killing Adrian; while if Adrian has a \"positive right to life\" against Clay, then Clay is required to act as necessary to preserve the life of Adrian.\n\nRights considered \"negative rights\" may include civil and political rights such as freedom of speech, life, private property, freedom from violent crime, freedom of religion, \"habeas corpus\", a fair trial, and freedom from slavery.\n\nRights considered \"positive rights\", as initially proposed in 1979 by the Czech jurist Karel Vasak, may include other civil and political rights such as police protection of person and property and the right to counsel, as well as economic, social and cultural rights such as food, housing, public education, employment, national security, military, health care, social security, internet access, and a minimum standard of living. In the \"three generations\" account of human rights, negative rights are often associated with the first generation of rights, while positive rights are associated with the second and third generations.\n\nSome philosophers (see criticisms) disagree that the negative-positive rights distinction is useful or valid.\n\nUnder the theory of positive and negative rights, a negative right is a right \"not to be\" subjected to an action of another person or group—a government, for example—usually in the form of abuse or coercion. As such, negative rights exist unless someone acts to \"negate\" them. A positive right is a right \"to be\" subjected to an action of another person or group. In other words, for a positive right to be exercised, someone else's actions must be \"added\" to the equation. In theory, a negative right forbids others from acting against the right holder, while a positive right obligates others to act with respect to the right holder. In the framework of the Kantian categorical imperative, negative rights can be associated with perfect duties while positive rights can be connected to imperfect duties.\n\nBelief in a distinction between positive and negative rights is usually maintained, or emphasized, by libertarians, who believe that positive rights do not exist until they are created by contract. The United Nations Universal Declaration of Human Rights lists both positive and negative rights (but does not identify them as such). The constitutions of most liberal democracies guarantee negative rights, but not all include positive rights. Nevertheless, positive rights are often guaranteed by other laws, and the majority of liberal democracies provide their citizens with publicly funded education, health care, social security and unemployment benefits.\n\nRights are often spoken of as inalienable and sometimes even absolute. However, in practice this is often taken as graded absolutism; rights are ranked by degree of importance, and violations of lesser ones are accepted in the course of preventing violations of greater ones. Thus, even if the right not to be killed is inalienable, the corresponding obligation on others to refrain from killing is generally understood to have at least one exception: self-defense. Certain widely accepted negative obligations (such as the obligations to refrain from theft, murder, etc.) are often considered prima facie, meaning that the legitimacy of the obligation is accepted \"on its face\"; but even if not questioned, such obligations may still be ranked for ethical analysis.\n\nThus a thief may have a negative obligation not to steal, and a police officer may have a negative obligation not to tackle people—but a police officer tackling the thief easily meets the burden of proof that he acted justifiably, since his was a breach of a lesser obligation and negated the breach of a greater obligation. Likewise a shopkeeper or other passerby may also meet this burden of proof when tackling the thief. But if any of those individuals pulled a gun and shot the (unarmed) thief for stealing, most modern societies would not accept that the burden of proof had been met. The obligation not to kill—being universally regarded as one of the highest, if not the highest obligation—is so much greater than the obligation not to steal that a breach of the latter does not justify a breach of the former. Most modern societies insist that other, very serious ethical questions need come into play before stealing could justify killing.\n\nPositive obligations confer duty. But as we see with the police officer, exercising a duty may violate negative obligations (e.g. not to overreact and kill). For this reason, in ethics positive obligations are almost never considered \"prima facie\". The greatest negative obligation may have just one exception—one higher obligation of self-defense—but even the greatest positive obligations generally require more complex ethical analysis. For example, one could easily justify failing to help, not just one, but a great many injured children quite ethically in the case of triage after a disaster. This consideration has led ethicists to agree in a general way that positive obligations are usually junior to negative obligations because they are not reliably \"prima facie\". Some critics of positive rights implicitly suggest that because positive obligations are not reliably \"prima facie\" they must always be agreed to through contract.\n\nNineteenth-century philosopher Frédéric Bastiat summarized the conflict between these negative and positive rights by saying:\nAccording to Jan Narveson, the view of some that there is no distinction between negative and positive rights on the ground that negative rights require police and courts for their enforcement is \"mistaken\". He says that the question between what one has a right to do and who if anybody enforces it are separate issues. If rights are only negative then it simply means no one has a duty to enforce them, although individuals have a right to use any non-forcible means to gain the cooperation of others in protecting those rights. Therefore, he says \"the distinction between negative and positive is quite robust.\" Libertarians hold that positive rights, which would include a right to be protected, do not exist until they are created by contract. However, those who hold this view do not mean that police, for example, are not obligated to protect the rights of citizens. Since they contract with their employers to defend citizens from violence, then they have created that obligation to their employer. A negative right to life allows an individual to defend his life from others trying to kill him, or obtain voluntary assistance from others to defend his life—but he may not force others to defend him, because he has no natural right to be provided with defense. To force a person to defend one's own negative rights, or the negative rights of a third party, would be to violate that person's negative rights.\n\nOther advocates of the view that there is a distinction between negative and positive rights argue that the presence of a police force or army is not due to any positive right to these services that citizens claim, but rather because they are natural monopolies or public goods—features of any human society that arise naturally, even while adhering to the concept of negative rights only. Robert Nozick discusses this idea at length in his book \"Anarchy, State, and Utopia\".\n\nIn the field of medicine, positive rights of patients often conflict with negative rights of physicians. In controversial areas such as abortion and assisted suicide, medical professionals may not wish to offer certain services for moral or philosophical reasons. If enough practitioners opt out as a result of conscience, a right granted by conscience clause statutes in many jurisdictions, patients may not have any means of having their own positive rights fulfilled. Such was the case of Janet Murdock, a Montana woman who could not find any physician to assist her suicide in 2009. This controversy over positive and negative rights in medicine has become a focal point in the ongoing public debate between conservative ethicist Wesley J. Smith and bioethicist Jacob M. Appel. In discussing \"Baxter v. Montana\", Appel has written:\nSmith replies that this is \"taking the duty to die and transforming it into a duty to kill\", which he argues \"reflects a profound misunderstanding of the government’s role\".\n\nPresumably, if a person has positive rights it implies that other people have positive duties (to take certain actions); whereas negative rights imply that others have negative duties (to avoid certain other actions). Philosopher Henry Shue is skeptical; he believes that all rights (regardless of whether they seem more \"negative\" or \"positive\") requires both kinds of duties at once. In other words, Shue says that honouring a right will require avoidance (a \"negative\" duty) but also protective or reparative actions (\"positive\" duties). The negative positive distinction may be a matter of emphasis; it is therefore unhelpful to describe \"any right\" as though it requires only one of the two types of duties.\n\nTo Shue, rights can always be understood as confronting \"standard threats\" against humanity. Dealing with standard threats requires all kinds of duties, which may be divided across time (e.g. \"if avoiding the harmful behaviour fails, begin to repair the damages\"), but also divided across people. The point is that every right provokes all 3 types of behaviour (avoidance, protection, repair) to some degree. Dealing with a threat like murder, for instance, will require one individual to practice avoidance (e.g. the potential murderer must stay calm), others to protect (e.g. the police officer, who must stop the attack, or the bystander, who may be obligated to call the police), and others to repair (e.g. the doctor who must resuscitate a person who has been attacked). Thus, even the negative right not to be killed can only be guaranteed with the help of some positive duties. Shue goes further, and maintains that the negative and positive rights distinction can be harmful, because it may result in the neglect of necessary duties.\n\nJames P. Sterba makes similar criticisms. He holds that any right can be made to appear either positive or negative depending on the language used to define it. He writes:\n\nSterba has rephrased the traditional \"positive right\" to provisions, and put it in the form of a sort of \"negative right\" \"not to be prevented\" from taking the resources on their own.. Thus, all rights may not only require both \"positive\" and \"negative\" duties, but it seems that rights that do not involve forced labor can be phrased positively or negatively at will. The distinction between positive and negative may not be very useful, or justified, as rights requiring the provision of labor can be rephrased from \"right to education\" or \"right to health care\" to \"right to take surplus money to pay teachers\" or \"right to take surplus money to pay doctors\".\n\n\n\n"}
{"id": "1697560", "url": "https://en.wikipedia.org/wiki?curid=1697560", "title": "Negative equity", "text": "Negative equity\n\nNegative equity occurs when the value of an asset used to secure a loan is less than the outstanding balance on the loan. In the United States, assets (particularly real estate, whose loans are mortgages) with negative equity are often referred to as being \"underwater\", and loans and borrowers with negative equity are said to be \"upside down\". \n\nPeople and companies alike may have negative equity, as reflected on their balance sheets.\n\nIn the owner-occupied housing market, a fall in the market value of a mortgaged house or apartment/flat is the usual cause of negative equity. It may occur when the property owner obtains second-mortgage home-equity loans, causing the combined loans to exceed the home value, or simply because the original mortgage was too generous. If the borrower defaults, repossession and sale of the property by the lender will not raise enough cash to repay the amount outstanding, and the borrower will still be in debt as well as having lost the property. Some US states like California require lenders to choose between going after the borrower or taking repossession, but not both.\n\nThe term negative equity was widely used in the United Kingdom during the economic recession between 1991 and 1996, and in Hong Kong between 1998 and 2003. These recessions led to increased unemployment and a decline in property prices, which in turn led to an increase in repossessions by banks and building societies of properties worth less than the outstanding debt.\n\nIt is also common for negative equity to occur when the value of a property drops shortly after its purchase. This occurs frequently in automobile loans, where the market value of a car might drop by 20-30% as soon as the car is driven off the lot.\n\nWhile typically a result of fluctuating asset prices, negative equity can occur when the value of the asset stays fixed and the loan balance increases because loan payments are less than the interest, a situation known as negative amortization. The typical assets securing such loans are real property – commercial, office or residential. When the loan is nonrecourse, the lender can only look to the security, that is, the value of the property, when the borrower fails to repay the loan.\n\nSince 2007, those most exposed to negative equity are borrowers who obtained loans of a high percentage of the property value (such as 90% or even 100%). These were commonly available before the credit crunch. Such cases are of course the most at risk from falls in property value.\n\nA person who has negative equity can be said to have \"negative net worth\", where the person's liabilities exceed their assets. One might come to have negative equity as a result of taking out a substantial, unsecured loan. For example, one might use a student loan to pursue higher education. Although education increases the likelihood of higher future earnings, potential alone is not a financial asset. \n\nIn the United States, student loans are rarely dischargeable in bankruptcy, and typically lenders provide student loans without requiring security. This stands in contrast to lenders requiring borrowers to have an equity stake in a comparably-sized real estate loan, as described above, secured by both a down payment and a mortgage. An explanation for the willingness of creditors to provide unsecured student loans is that, in a practical sense, American student loans are secured by the borrower's future earnings. This is so since creditors may legally garnish wages when a borrower defaults.\n\nA home owner who is under water might be financially incapable of selling their current house and buying another one.\n\n\n"}
{"id": "1092282", "url": "https://en.wikipedia.org/wiki?curid=1092282", "title": "Negative frequency", "text": "Negative frequency\n\nThe concept of negative and positive frequency can be as simple as a wheel rotating one way or the other way: a \"signed value\" of frequency can indicate both the rate and direction of rotation. The rate is expressed in units such as revolutions (a.k.a. \"cycles\") per second (hertz) or radian/second (where 1 cycle corresponds to 2\"π\" radians).\n\nLet \"ω\" be a nonnegative parameter with units of radians/sec. Then the angular function (angle vs. time) , has slope −\"ω\", which is called a negative frequency. But when the function is used as the argument of a cosine operator, the result is indistinguishable from .  Similarly, is indistinguishable from . Thus any sinusoids can be represented in terms of positive frequencies. The sign of the underlying phase slope is ambiguous.\n\nThe ambiguity is resolved when the cosine and sine operators can be observed simultaneously, because leads by 1/4 cycle (= \"π\"/2 radians) when , and lags by 1/4 cycle when .  Similarly, a vector, , rotates counter-clockwise at 1 radian/sec, and completes a circle every 2π seconds, and the vector rotates in the other direction.\n\nThe sign of \"ω\" is also preserved in the complex-valued function:\n\nsince R(\"t\") and I(\"t\") can be separately extracted and compared. Although formula_1  clearly contains more information than either of its components, a common interpretation is that it is a simpler function, because:\nwhich gives rise to the interpretation that cos(\"ωt\") comprises \"both\" positive and negative frequencies.  But the sum is actually a cancellation that contains less, not more, information. Any measure that indicates both frequencies includes a false positive, because \"ω\" can have only one sign.  The Fourier transform, for instance, merely tells us that cos(\"ωt\") correlates equally well with both and with .\n\nPerhaps the most well-known application of negative frequency is the calculation:\n\nwhich is a measure of the amount of frequency ω in the function \"x\"(\"t\") over the interval . When evaluated as a continuous function of \"ω\" for the theoretical interval , it is known as the Fourier transform of \"x\"(\"t\"). A brief explanation is that the product of two complex sinusoids is also a complex sinusoid whose frequency is the sum of the original frequencies. So when \"ω\" is positive, formula_4 causes all the frequencies of \"x\"(\"t\") to be reduced by amount \"ω\". Whatever part of \"x\"(\"t\") that was at frequency \"ω\" is changed to frequency zero, which is just a constant whose amplitude level is a measure of the strength of the original \"ω\" content. And whatever part of \"x\"(\"t\") that was at frequency zero is changed to a sinusoid at frequency −\"ω\". Similarly, all other frequencies are changed to non-zero values. As the interval increases, the contribution of the constant term grows in proportion. But the contributions of the sinusoidal terms only oscillate around zero. So \"X\"(\"ω\") improves as a relative measure of the amount of frequency \"ω\" in the function \"x\"(\"t\").\n\nThe Fourier transform of  formula_1  produces a non-zero response only at frequency \"ω\". The transform of formula_2 has responses at both \"ω\" and −\"ω\", as anticipated by .\n\n"}
{"id": "237770", "url": "https://en.wikipedia.org/wiki?curid=237770", "title": "Negative resistance", "text": "Negative resistance\n\nIn electronics, negative resistance (NR) is a property of some electrical circuits and devices in which an increase in voltage across the device's terminals results in a decrease in electric current through it.\n\nThis is in contrast to an ordinary resistor in which an increase of applied voltage causes a proportional increase in current due to Ohm's law, resulting in a positive resistance. While a positive resistance consumes power from current passing through it, a negative resistance produces power. Under certain conditions it can increase the power of an electrical signal, amplifying it.\n\nNegative resistance is an uncommon property which occurs in a few nonlinear electronic components. In a nonlinear device, two types of resistance can be defined: 'static' or 'absolute resistance', the ratio of voltage to current formula_1, and \"differential resistance\", the ratio of a change in voltage to the resulting change in current formula_2. The term negative resistance means negative differential resistance (NDR), formula_3. In general, a negative differential resistance is a two-terminal component which can amplify, converting DC power applied to its terminals to AC output power to amplify an AC signal applied to the same terminals. They are used in electronic oscillators and amplifiers, particularly at microwave frequencies. Most microwave energy is produced with negative differential resistance devices. They can also have hysteresis and be bistable, and so are used in switching and memory circuits. Examples of devices with negative differential resistance are tunnel diodes, Gunn diodes, and gas discharge tubes such as neon lamps. In addition, circuits containing amplifying devices such as transistors and op amps with positive feedback can have negative differential resistance. These are used in oscillators and active filters.\n\nBecause they are nonlinear, negative resistance devices have a more complicated behavior than the positive \"ohmic\" resistances usually encountered in electric circuits. Unlike most positive resistances, negative resistance varies depending on the voltage or current applied to the device, and negative resistance devices can have negative resistance over only a limited portion of their voltage or current range. Therefore, there is no real \"negative resistor\" analogous to a positive resistor, which has a constant negative resistance over an arbitrarily wide range of current.\n\nThe resistance between two terminals of an electrical device or circuit is determined by its current–voltage (\"I–V\") curve (characteristic curve), giving the current formula_4 through it for any given voltage formula_5 across it. Most materials, including the ordinary (positive) resistances encountered in electrical circuits, obey Ohm's law; the current through them is proportional to the voltage over a wide range. So the \"I–V\" curve of an ohmic resistance is a straight line through the origin with positive slope. The resistance is the ratio of voltage to current, the inverse slope of the line (in \"I–V\" graphs where the voltage formula_5 is the independent variable) and is constant.\n\nNegative resistance occurs in a few nonlinear (nonohmic) devices. In a nonlinear component the \"I–V\" curve is not a straight line, so it does not obey Ohm's law. Resistance can still be defined, but the resistance is not constant; it varies with the voltage or current through the device. The resistance of such a nonlinear device can be defined in two ways, which are equal for ohmic resistances:\n\n\n\nNegative resistance, like positive resistance, is measured in ohms.\n\nConductance is the reciprocal of resistance. It is measured in siemens (formerly \"mho\") which is the conductance of a resistor with a resistance of one ohm. Each type of resistance defined above has a corresponding conductance\nIt can be seen that the conductance has the same sign as its corresponding resistance: a negative resistance will have a negative conductance while a positive resistance will have a positive conductance.\n\nOne way in which the different types of resistance can be distinguished is in the directions of current and electric power between a circuit and an electronic component. The illustrations below, with a rectangle representing the component attached to a circuit, summarize how the different types work:\nIn an electronic device, the differential resistance formula_14, the static resistance formula_15, or both, can be negative, so there are three categories of devices \"(fig. 2–4 above, and table)\" which could be called \"negative resistances\".\n\nThe term \"negative resistance\" almost always means negative \"differential\" resistance Negative differential resistance devices have unique capabilities: they can act as \"one-port amplifiers\", increasing the power of a time-varying signal applied to their port (terminals), or excite oscillations in a tuned circuit to make an oscillator. They can also have hysteresis. It is not possible for a device to have negative differential resistance without a power source, and these devices can be divided into two categories depending on whether they get their power from an internal source or from their port:\n\n\n\nOccasionally ordinary power sources are referred to as \"negative resistances\" (fig. 3 above). Although the \"static\" or \"absolute\" resistance formula_15 of active devices (power sources) can be considered negative (see Negative static resistance section below) most ordinary power sources (AC or DC), such as batteries, generators, and (non positive feedback) amplifiers, have positive \"differential\" resistance (their source resistance). Therefore, these devices cannot function as one-port amplifiers or have the other capabilities of negative differential resistances.\n\nElectronic components with negative differential resistance include these devices:\n\nElectric discharges through gases also exhibit negative differential resistance, including these devices\n\n\nIn addition, active circuits with negative differential resistance can also be built with amplifying devices like transistors and op amps, using feedback. A number of new experimental negative differential resistance materials and devices have been discovered in recent years. The physical processes which cause negative resistance are diverse, and each type of device has its own negative resistance characteristics, specified by its current–voltage curve.\n\nA point of some confusion is whether ordinary resistance (\"static\" or \"absolute\" resistance, formula_17) can be negative. In electronics, the term \"resistance\" is customarily applied only to passive materials and components – such as wires, resistors and diodes. These cannot have formula_18 as shown by Joule's law formula_19. A passive device consumes electric power, so from the passive sign convention formula_20. Therefore, from Joule's law formula_21. In other words, no material can conduct electric current better than a \"perfect\" conductor with zero resistance. For a passive device to have formula_22 would violate either conservation of energy or the second law of thermodynamics, \"(diagram)\". Therefore, some authors state that static resistance can never be negative.\n\nHowever it is easily shown that the ratio of voltage to current v/i at the terminals of any power source (AC or DC) is negative. For electric power (potential energy) to flow out of a device into the circuit, charge must flow through the device in the direction of increasing potential energy, conventional current (positive charge) must move from the negative to the positive terminal. So the direction of the instantaneous current is \"out\" of the positive terminal. This is opposite to the direction of current in a passive device defined by the passive sign convention so the current and voltage have opposite signs, and their ratio is negative\nThis can also be proved from Joule's law\nThis shows that power can flow out of a device into the circuit if and only if formula_18. Whether or not this quantity is referred to as \"resistance\" when negative is a matter of convention. The absolute resistance of power sources is negative, but this is not to be regarded as \"resistance\" in the same sense as positive resistances. The negative static resistance of a power source is a rather abstract and not very useful quantity, because it varies with the load. Due to conservation of energy it is always simply equal to the negative of the static resistance of the attached circuit \"(right)\".\n\nWork must be done on the charges by some source of energy in the device, to make them move toward the positive terminal against the electric field, so conservation of energy requires that negative static resistances have a source of power. The power may come from an internal source which converts some other form of energy to electric power as in a battery or generator, or from a separate connection to an external power supply circuit as in an amplifying device like a transistor, vacuum tube, or op amp.\n\nA circuit cannot have negative static resistance (be active) over an infinite voltage or current range, because it would have to be able to produce infinite power. Any active circuit or device with a finite power source is \"eventually passive\". This property means if a large enough external voltage or current of either polarity is applied to it, its static resistance becomes positive and it consumes power\n\nTherefore, the ends of the \"I–V\" curve will eventually turn and enter the 1st and 3rd quadrants. Thus the range of the curve having negative static resistance is limited, confined to a region around the origin. For example, applying a voltage to a generator or battery \"(graph, above)\" greater than its open-circuit voltage will reverse the direction of current flow, making its static resistance positive so it consumes power. Similarly, applying a voltage to the negative impedance converter below greater than its power supply voltage \"V\" will cause the amplifier to saturate, also making its resistance positive.\n\nIn a device or circuit with negative differential resistance (NDR), in some part of the \"I–V\" curve the current decreases as the voltage increases:\nThe \"I–V\" curve is nonmonotonic (having peaks and troughs) with regions of negative slope representing negative differential resistance.\n\nPassive negative differential resistances have positive \"static\" resistance; they consume net power. Therefore, the \"I–V\" curve is confined to the 1st and 3rd quadrants of the graph, and passes through the origin. This requirement means (excluding some asymptotic cases) that the region(s) of negative resistance must be limited, and surrounded by regions of positive resistance, and cannot include the origin.\n\nNegative differential resistances can be classified into two types:\n\n\nMost devices have a single negative resistance region. However devices with multiple separate negative resistance regions can also be fabricated. These can have more than two stable states, and are of interest for use in digital circuits to implement multivalued logic.\n\nAn intrinsic parameter used to compare different devices is the \"peak-to-valley current ratio\" (PVR), the ratio of the current at the top of the negative resistance region to the current at the bottom \"(see graphs, above)\":\nThe larger this is, the larger the potential AC output for a given DC bias current, and therefore the greater the efficiency\n\nA negative differential resistance device can amplify an AC signal applied to it if the signal is biased with a DC voltage or current to lie within the negative resistance region of its \"I–V\" curve.\n\nThe tunnel diode circuit \"(see diagram)\" is an example. The tunnel diode \"TD\" has voltage controlled negative differential resistance. The battery formula_30 adds a constant voltage (bias) across the diode so it operates in its negative resistance range, and provides power to amplify the signal. Suppose the negative resistance at the bias point is formula_31. For stability formula_32 must be less than formula_33. Using the formula for a voltage divider, the AC output voltage is\nIn a normal voltage divider, the resistance of each branch is less than the resistance of the whole, so the output voltage is less than the input. Here, due to the negative resistance, the total AC resistance formula_36 is less than the resistance of the diode alone formula_33 so the AC output voltage formula_38 is greater than the input formula_39. The voltage gain formula_40 is greater than one, and increases without limit as formula_32 approaches formula_33.\n\nThe diagrams illustrate how a biased negative differential resistance device can increase the power of a signal applied to it, amplifying it, although it only has two terminals. Due to the superposition principle the voltage and current at the device's terminals can be divided into a DC bias component and an AC component .\nSince a positive change in voltage formula_45 causes a \"negative\" change in current formula_46, the AC current and voltage in the device are 180° out of phase. This means in the AC equivalent circuit \"(right)\", the instantaneous AC current Δ\"i\" flows through the device in the direction of \"increasing\" AC potential Δ\"v\", as it would in a generator. Therefore, the AC power dissipation is \"negative\"; AC power is produced by the device and flows into the external circuit.\nWith the proper external circuit, the device can increase the AC signal power delivered to a load, serving as an amplifier, or excite oscillations in a resonant circuit to make an oscillator. Unlike in a two port amplifying device such as a transistor or op amp, the amplified signal leaves the device through the same two terminals (port) as the input signal enters.\n\nIn a passive device, the AC power produced comes from the input DC bias current, the device absorbs DC power, some of which is converted to AC power by the nonlinearity of the device, amplifying the applied signal. Therefore, the output power is limited by the bias power\nThe negative differential resistance region cannot include the origin, because it would then be able to amplify a signal with no applied DC bias current, producing AC power with no power input. The device also dissipates some power as heat, equal to the difference between the DC power in and the AC power out.\n\nThe device may also have reactance and therefore the phase difference between current and voltage may differ from 180° and may vary with frequency. As long as the real component of the impedance is negative (phase angle between 90° and 270°), the device will have negative resistance and can amplify.\n\nThe maximum AC output power is limited by size of the negative resistance region (formula_49 in graphs above)\n\nThe reason that the output signal can leave a negative resistance through the same port that the input signal enters is that from transmission line theory, the AC voltage or current at the terminals of a component can be divided into two oppositely moving waves, the \"incident wave\" formula_51, which travels toward the device, and the \"reflected wave\" formula_52, which travels away from the device. A negative differential resistance in a circuit can amplify if the magnitude of its reflection coefficient formula_53, the ratio of the reflected wave to the incident wave, is greater than one.\nThe \"reflected\" (output) signal has larger amplitude than the incident; the device has \"reflection gain\". The reflection coefficient is determined by the AC impedance of the negative resistance device, formula_56, and the impedance of the circuit attached to it, formula_57. If formula_58 and formula_59 then formula_60 and the device will amplify. On the Smith chart, a graphical aide widely used in the design of high frequency circuits, negative differential resistance corresponds to points outside the unit circle formula_61, the boundary of the conventional chart, so special \"expanded\" charts must be used.\n\nBecause it is nonlinear, a circuit with negative differential resistance can have multiple equilibrium points (possible DC operating points), which lie on the \"I–V\" curve. An equilibrium point will be stable, so the circuit converges to it within some neighborhood of the point, if its poles are in the left half of the s plane (LHP), while a point is unstable, causing the circuit to oscillate or \"latch up\" (converge to another point), if its poles are on the \"jω\" axis or right half plane (RHP), respectively. In contrast, a linear circuit has a single equilibrium point that may be stable or unstable. The equilibrium points are determined by the DC bias circuit, and their stability is determined by the AC impedance formula_62 of the external circuit.\nHowever, because of the different shapes of the curves, the condition for stability is different for VCNR and CCNR types of negative resistance:\n\n\nFor general negative resistance circuits with reactance, the stability must be determined by standard tests like the Nyquist stability criterion. Alternatively, in high frequency circuit design, the values of formula_70 for which the circuit is stable are determined by a graphical technique using \"stability circles\" on a Smith chart.\n\nFor simple nonreactive negative resistance devices with formula_71 and formula_72 the different operating regions of the device can be illustrated by load lines on the \"I–V\" curve \"(see graphs)\".\n\nThe DC load line (DCL) is a straight line determined by the DC bias circuit, with equation\nwhere formula_74 is the DC bias supply voltage and R is the resistance of the supply. The possible DC operating point(s) (Q points) occur where the DC load line intersects the \"I–V\" curve. For stability\nThe AC load line (\"L\" − \"L\") is a straight line through the Q point whose slope is the differential (AC) resistance formula_75 facing the device. Increasing formula_75 rotates the load line counterclockwise. The circuit operates in one of three possible regions \"(see diagrams)\", depending on formula_75.\n\nIn addition to the passive devices with intrinsic negative differential resistance above, circuits with amplifying devices like transistors or op amps can have negative resistance at their ports. The input or output impedance of an amplifier with enough positive feedback applied to it can be negative. If formula_84 is the input resistance of the amplifier without feedback, formula_85 is the amplifier gain, and formula_86 is the transfer function of the feedback path, the input resistance with positive shunt feedback is\nSo if the loop gain formula_88 is greater than one, formula_89 will be negative. The circuit acts like a \"negative linear resistor\" over a limited range, with \"I–V\" curve having a straight line segment through the origin with negative slope \"(see graphs)\". It has both negative differential resistance and is active\nand thus obeys Ohm's law as if it had a negative value of resistance \"−R\", over its linear range (such amplifiers can also have more complicated negative resistance \"I–V\" curves that do not pass through the origin).\n\nIn circuit theory these are called \"active resistors\". Applying a voltage across the terminals causes a proportional current \"out\" of the positive terminal, the opposite of an ordinary resistor. For example, connecting a battery to the terminals would cause the battery to charge rather than discharge.\n\nConsidered as one-port devices, these circuits function similarly to the passive negative differential resistance components above, and like them can be used to make one-port amplifiers and oscillators with the advantages that:\nThe \"I–V\" curve can have voltage-controlled (\"N\" type) or current-controlled (\"S\" type) negative resistance, depending on whether the feedback loop is connected in \"shunt\" or \"series\".\n\nNegative reactances \"(below)\" can also be created, so feedback circuits can be used to create \"active\" linear circuit elements, resistors, capacitors, and inductors, with negative values. They are widely used in active filters because they can create transfer functions that cannot be realized with positive circuit elements. Examples of circuits with this type of negative resistance are the negative impedance converter (NIC), gyrator, Deboo integrator, frequency dependent negative resistance (FDNR), and generalized immittance converter (GIC).\n\nIf an LC circuit is connected across the input of a positive feedback amplifier like that above, the negative differential input resistance formula_91 can cancel the positive loss resistance formula_92 inherent in the tuned circuit. If formula_93 this will create in effect a tuned circuit with zero AC resistance (poles on the \"jω\" axis). Spontaneous oscillation will be excited in the tuned circuit at its resonant frequency, sustained by the power from the amplifier. This is how feedback oscillators such as Hartley or Colpitts oscillators work. This negative resistance model is an alternate way of analyzing feedback oscillator operation. \"All\" linear oscillator circuits have negative resistance although in most feedback oscillators the tuned circuit is an integral part of the feedback network, so the circuit does not have negative resistance at all frequencies but only near the oscillation frequency.\n\nA tuned circuit connected to a negative resistance which cancels some but not all of its parasitic loss resistance (so formula_94) will not oscillate, but the negative resistance will decrease the damping in the circuit (moving its poles toward the \"jω\" axis), increasing its Q factor so it has a narrower bandwidth and more selectivity. Q enhancement, also called \"regeneration\", was first used in the regenerative radio receiver invented by Edwin Armstrong in 1912 and later in \"Q multipliers\". It is widely used in active filters. For example, RF integrated circuits use \"integrated inductors\" to save space, consisting of a spiral conductor fabricated on chip. These have high losses and low Q, so to create high Q tuned circuits their Q is increased by applying negative resistance.\n\nCircuits which exhibit chaotic behavior can be considered quasi-periodic or nonperiodic oscillators, and like all oscillators require a negative resistance in the circuit to provide power. Chua's circuit, a simple nonlinear circuit widely used as the standard example of a chaotic system, requires a nonlinear active resistor component, sometimes called Chua's diode. This is usually synthesized using a negative impedance converter circuit.\n\nA common example of an \"active resistance\" circuit is the negative impedance converter (NIC) shown in the diagram. The two resistors formula_95 and the op amp constitute a negative feedback non-inverting amplifier with gain of 2. The output voltage of the op-amp is\nSo if a voltage formula_5 is applied to the input, the same voltage is applied \"backwards\" across formula_98, causing current to flow through it out of the input. The current is\nSo the input impedance to the circuit is\nThe circuit converts the impedance formula_98 to its negative. If formula_98 is a resistor of value formula_103, within the linear range of the op amp formula_104 the input impedance acts like a linear \"negative resistor\" of value formula_105. The input port of the circuit is connected into another circuit as if it was a component. An NIC can cancel undesired positive resistance in another circuit, for example they were originally developed to cancel resistance in telephone cables, serving as repeaters.\n\nBy replacing formula_98 in the above circuit with a capacitor , negative capacitances and inductances can also be synthesized. A negative capacitance will have an \"I–V\" relation and an impedance formula_107 of\nwhere formula_109. Applying a positive current to a negative capacitance will cause it to \"discharge\"; its voltage will \"decrease\". Similarly, a negative inductance will have an \"I–V\" characteristic and impedance formula_110 of\nA circuit having negative capacitance or inductance can be used to cancel unwanted positive capacitance or inductance in another circuit. NIC circuits were used to cancel reactance on telephone cables.\n\nThere is also another way of looking at them. In a negative capacitance the current will be 180° opposite in phase to the current in a positive capacitance. Instead of leading the voltage by 90° it will lag the voltage by 90°, as in an inductor. Therefore, a negative capacitance acts like an inductance in which the impedance has a reverse dependence on frequency ω; decreasing instead of increasing like a real inductance Similarly a negative inductance acts like a capacitance that has an impedance which increases with frequency. Negative capacitances and inductances are \"non-Foster\" circuits which violate Foster's reactance theorem. One application being researched is to create an active matching network which could match an antenna to a transmission line over a broad range of frequencies, rather than just a single frequency as with current networks. This would allow the creation of small compact antennas that would have broad bandwidth, exceeding the Chu–Harrington limit.\n\nNegative differential resistance devices are widely used to make electronic oscillators. In a negative resistance oscillator, a negative differential resistance device such as an IMPATT diode, Gunn diode, or microwave vacuum tube is connected across an electrical resonator such as an LC circuit, a quartz crystal, dielectric resonator or cavity resonator with a DC source to bias the device into its negative resistance region and provide power. A resonator such as an LC circuit is \"almost\" an oscillator; it can store oscillating electrical energy, but because all resonators have internal resistance or other losses, the oscillations are damped and decay to zero. The negative resistance cancels the positive resistance of the resonator, creating in effect a lossless resonator, in which spontaneous continuous oscillations occur at the resonator's resonant frequency.\n\nNegative resistance oscillators are mainly used at high frequencies in the microwave range or above, since feedback oscillators function poorly at these frequencies. Microwave diodes are used in low- to medium-power oscillators for applications such as radar speed guns, and local oscillators for satellite receivers. They are a widely used source of microwave energy, and virtually the only solid-state source of millimeter wave and terahertz energy Negative resistance microwave vacuum tubes such as magnetrons produce higher power outputs, in such applications as radar transmitters and microwave ovens. Lower frequency relaxation oscillators can be made with UJTs and gas-discharge lamps such as neon lamps.\n\nThe negative resistance oscillator model is not limited to one-port devices like diodes but can also be applied to feedback oscillator circuits with two port devices such as transistors and tubes. In addition, in modern high frequency oscillators, transistors are increasingly used as one-port negative resistance devices like diodes. At microwave frequencies, transistors with certain loads applied to one port can become unstable due to internal feedback and show negative resistance at the other port. So high frequency transistor oscillators are designed by applying a reactive load to one port to give the transistor negative resistance, and connecting the other port across a resonator to make a negative resistance oscillator as described below.\n\nThe common Gunn diode oscillator \"(circuit diagrams)\" illustrates how negative resistance oscillators work. The diode \"D\" has voltage controlled (\"N\" type) negative resistance and the voltage source formula_112 biases it into its negative resistance region where its differential resistance is formula_113. The choke \"RFC\" prevents AC current from flowing through the bias source. formula_103 is the equivalent resistance due to damping and losses in the series tuned circuit formula_115, plus any load resistance. Analyzing the AC circuit with Kirchhoff's Voltage Law gives a differential equation for formula_116, the AC current\nSolving this equation gives a solution of the form\nThis shows that the current through the circuit, formula_116, varies with time about the DC Q point, formula_121. When started from a nonzero initial current formula_122 the current oscillates sinusoidally at the resonant frequency ω of the tuned circuit, with amplitude either constant, increasing, or decreasing exponentially, depending on the value of α. Whether the circuit can sustain steady oscillations depends on the balance between formula_103 and formula_124, the positive and negative resistance in the circuit:\n\nPractical oscillators are designed in region (3) above, with net negative resistance, to get oscillations started. A widely used rule of thumb is to make formula_129. When the power is turned on, electrical noise in the circuit provides a signal formula_130 to start spontaneous oscillations, which grow exponentially. However, the oscillations cannot grow forever; the nonlinearity of the diode eventually limits the amplitude.\n\nAt large amplitudes the circuit is nonlinear, so the linear analysis above does not strictly apply and differential resistance is undefined; but the circuit can be understood by considering formula_124 to be the \"average\" resistance over the cycle. As the amplitude of the sine wave exceeds the width of the negative resistance region and the voltage swing extends into regions of the curve with positive differential resistance, the average negative differential resistance formula_124 becomes smaller, and thus the total resistance formula_133 and the damping formula_134 becomes less negative and eventually turns positive. Therefore, the oscillations will stabilize at the amplitude at which the damping becomes zero, which is when formula_135.\n\nGunn diodes have negative resistance in the range −5 to −25 ohms. In oscillators where formula_103 is close to formula_124; just small enough to allow the oscillator to start, the voltage swing will be mostly limited to the linear portion of the \"I–V\" curve, the output waveform will be nearly sinusoidal and the frequency will be most stable. In circuits in which formula_103 is far below formula_124, the swing extends further into the nonlinear part of the curve, the clipping distortion of the output sine wave is more severe, and the frequency will be increasingly dependent on the supply voltage.\n\nNegative resistance oscillator circuits can be divided into two types, which are used with the two types of negative differential resistance – voltage controlled (VCNR), and current controlled (CCNR)\n\nMost oscillators are more complicated than the Gunn diode example, since both the active device and the load may have reactance (\"X\") as well as resistance (\"R\"). Modern negative resistance oscillators are designed by a frequency domain technique due to K. Kurokawa. The circuit diagram is imagined to be divided by a \"reference plane\" \"(red)\" which separates the negative resistance part, the active device, from the positive resistance part, the resonant circuit and output load \"(right)\". The complex impedance of the negative resistance part formula_140 depends on frequency \"ω\" but is also nonlinear, in general declining with the amplitude of the AC oscillation current \"I\"; while the resonator part formula_141 is linear, depending only on frequency. The circuit equation is formula_142 so it will only oscillate (have nonzero \"I\") at the frequency \"ω\" and amplitude \"I\" for which the total impedance formula_143 is zero. This means the magnitude of the negative and positive resistances must be equal, and the reactances must be conjugate\nFor steady-state oscillation the equal sign applies. During startup the inequality applies, because the circuit must have excess negative resistance for oscillations to start.\n\nAlternately, the condition for oscillation can be expressed using the reflection coefficient. The voltage waveform at the reference plane can be divided into a component \"V\" travelling toward the negative resistance device and a component \"V\" travelling in the opposite direction, toward the resonator part. The reflection coefficient of the active device formula_146 is greater than one, while that of the resonator part formula_147 is less than one. During operation the waves are reflected back and forth in a round trip so the circuit will oscillate only if\nAs above, the equality gives the condition for steady oscillation, while the inequality is required during startup to provide excess negative resistance. The above conditions are analogous to the Barkhausen criterion for feedback oscillators; they are necessary but not sufficient, so there are some circuits that satisfy the equations but do not oscillate. Kurokawa also derived more complicated sufficient conditions, which are often used instead.\n\nNegative differential resistance devices such as Gunn and IMPATT diodes are also used to make amplifiers, particularly at microwave frequencies, but not as commonly as oscillators. Because negative resistance devices have only one \"port\" (two terminals), unlike two-port devices such as transistors, the outgoing amplified signal has to leave the device by the same terminals as the incoming signal enters it. Without some way of separating the two signals, a negative resistance amplifier is \"bilateral\"; it amplifies in both directions, so it suffers from sensitivity to load impedance and feedback problems. To separate the input and output signals, many negative resistance amplifiers use nonreciprocal devices such as isolators and directional couplers.\n\nOne widely used circuit is the \"reflection amplifier\" in which the separation is accomplished by a \"circulator\". A circulator is a nonreciprocal solid-state component with three ports (connectors) which transfers a signal applied to one port to the next in only one direction, port 1 to port 2, 2 to 3, and 3 to 1. In the reflection amplifier diagram the input signal is applied to port 1, a biased VCNR negative resistance diode \"N\" is attached through a filter \"F\" to port 2, and the output circuit is attached to port 3. The input signal is passed from port 1 to the diode at port 2, but the outgoing \"reflected\" amplified signal from the diode is routed to port 3, so there is little coupling from output to input. The characteristic impedance formula_149 of the input and output transmission lines, usually 50Ω, is matched to the port impedance of the circulator. The purpose of the filter \"F\" is to present the correct impedance to the diode to set the gain. At radio frequencies NR diodes are not pure resistive loads and have reactance, so a second purpose of the filter is to cancel the diode reactance with a conjugate reactance to prevent standing waves.\n\nThe filter has only reactive components and so does not absorb any power itself, so power is passed between the diode and the ports without loss. The input signal power to the diode is\nThe output power from the diode is\nSo the power gain formula_152 of the amplifier is the square of the reflection coefficient\n\nformula_156 is the negative resistance of the diode −r. Assuming the filter is matched to the diode so formula_157 then the gain is\nThe VCNR reflection amplifier above is stable for formula_159. while a CCNR amplifier is stable for formula_160. It can be seen that the reflection amplifier can have unlimited gain, approaching infinity as formula_95 approaches the point of oscillation at formula_124. This is a characteristic of all NR amplifiers, contrasting with the behavior of two-port amplifiers, which generally have limited gain but are often unconditionally stable. In practice the gain is limited by the backward \"leakage\" coupling between circulator ports.\n\nMasers and parametric amplifiers are extremely low noise NR amplifiers that are also implemented as reflection amplifiers; they are used in applications like radio telescopes.\n\nNegative differential resistance devices are also used in switching circuits in which the device operates nonlinearly, changing abruptly from one state to another, with hysteresis. The advantage of using a negative resistance device is that a relaxation oscillator, flip-flop or memory cell can be built with a single active device, whereas the standard logic circuit for these functions, the Eccles-Jordan multivibrator, requires two active devices (transistors). Three switching circuits built with negative resistances are\n\nSome instances of neurons display regions of negative slope conductances (RNSC) in voltage-clamp experiments. The negative resistance here is implied were one to consider the neuron a typical Hodgkin–Huxley style circuit model.\n\nNegative resistance was first recognized during investigations of electric arcs, which were used for lighting during the 19th century. In 1881 Alfred Niaudet had observed that the voltage across arc electrodes decreased temporarily as the arc current increased, but many researchers thought this was a secondary effect due to temperature. The term \"negative resistance\" was applied by some to this effect, but the term was controversial because it was known that the resistance of a passive device could not be negative. Beginning in 1895 Hertha Ayrton, extending her husband William's research with a series of meticulous experiments measuring the \"I–V\" curve of arcs, established that the curve had regions of negative slope, igniting controversy. Frith and Rodgers in 1896 with the support of the Ayrtons introduced the concept of \"differential\" resistance, \"dv/di\", and it was slowly accepted that arcs had negative differential resistance. In recognition of her research, Hertha Ayrton became the first woman voted for induction into the Institute of Electrical Engineers.\n\nGeorge Francis FitzGerald first realized in 1892 that if the damping resistance in a resonant circuit could be made zero or negative, it would produce continuous oscillations. In the same year Elihu Thomson built a negative resistance oscillator by connecting an LC circuit to the electrodes of an arc, perhaps the first example of an electronic oscillator. William Duddell, a student of Ayrton at London Central Technical College, brought Thomson's arc oscillator to public attention. Due to its negative resistance, the current through an arc was unstable, and arc lights would often produce hissing, humming, or even howling noises. In 1899, investigating this effect, Duddell connected an LC circuit across an arc and the negative resistance excited oscillations in the tuned circuit, producing a musical tone from the arc. To demonstrate his invention Duddell wired several tuned circuits to an arc and played a tune on it. Duddell's \"singing arc\" oscillator was limited to audio frequencies. However, in 1903 Danish engineers Valdemar Poulsen and P. O. Pederson increased the frequency into the radio range by operating the arc in a hydrogen atmosphere in a magnetic field, inventing the Poulsen arc radio transmitter, which was widely used until the 1920s.\n\nBy the early 20th century, although the physical causes of negative resistance were not understood, engineers knew it could generate oscillations and had begun to apply it. Heinrich Barkhausen in 1907 showed that oscillators must have negative resistance. Ernst Ruhmer and Adolf Pieper discovered that mercury vapor lamps could produce oscillations, and by 1912 AT&T had used them to build amplifying repeaters for telephone lines.\n\nIn 1918 Albert Hull at GE discovered that vacuum tubes could have negative resistance in parts of their operating ranges, due to a phenomenon called secondary emission. In a vacuum tube when electrons strike the plate electrode they can knock additional electrons out of the surface into the tube. This represents a current \"away\" from the plate, reducing the plate current. Under certain conditions increasing the plate voltage causes a \"decrease\" in plate current. By connecting an LC circuit to the tube Hull created an oscillator, the dynatron oscillator. Other negative resistance tube oscillators followed, such as the magnetron invented by Hull in 1920.\n\nThe negative impedance converter originated from work by Marius Latour around 1920. He was also one of the first to report negative capacitance and inductance. A decade later, vacuum tube NICs were developed as telephone line repeaters at Bell Labs by George Crisson and others, which made transcontinental telephone service possible. Transistor NICs, pioneered by Linvill in 1953, initiated a great increase in interest in NICs and many new circuits and applications developed.\n\nNegative differential resistance in semiconductors was observed around 1909 in the first point-contact junction diodes, called cat's whisker detectors, by researchers such as William Henry Eccles and G. W. Pickard. They noticed that when junctions were biased with a DC voltage to improve their sensitivity as radio detectors, they would sometimes break into spontaneous oscillations. However the effect was not pursued.\n\nThe first person to exploit negative resistance diodes practically was Russian radio researcher Oleg Losev, who in 1922 discovered negative differential resistance in biased zincite (zinc oxide) point contact junctions. He used these to build solid-state amplifiers, oscillators, and amplifying and regenerative radio receivers, 25 years before the invention of the transistor. Later he even built a superheterodyne receiver. However his achievements were overlooked because of the success of vacuum tube technology. After ten years he abandoned research into this technology (dubbed \"Crystodyne\" by Hugo Gernsback), and it was forgotten.\n\nThe first widely used solid-state negative resistance device was the tunnel diode, invented in 1957 by Japanese physicist Leo Esaki. Because they have lower parasitic capacitance than vacuum tubes due to their small junction size, diodes can function at higher frequencies, and tunnel diode oscillators proved able to produce power at microwave frequencies, above the range of ordinary vacuum tube oscillators. Its invention set off a search for other negative resistance semiconductor devices for use as microwave oscillators, resulting in the discovery of the IMPATT diode, Gunn diode, TRAPATT diode, and others. In 1969 Kurokawa derived conditions for stability in negative resistance circuits. Currently negative differential resistance diode oscillators are the most widely used sources of microwave energy, and many new negative resistance devices have been discovered in recent decades.\n\n"}
{"id": "362728", "url": "https://en.wikipedia.org/wiki?curid=362728", "title": "Negative temperature", "text": "Negative temperature\n\nIn quantum thermodynamics, certain systems can achieve negative temperature; that is, their temperature can be expressed as a negative quantity on the Kelvin or Rankine scales.\n\nA system with a truly negative temperature on the Kelvin scale is \"hotter\" than any system with a positive temperature. If a negative-temperature system and a positive-temperature system come in contact, heat will flow from the negative- to the positive-temperature system. A standard example of such a system is population inversion in laser physics.\n\nTemperature is loosely interpreted as the average kinetic energy of the system's particles. The existence of negative temperature, let alone negative temperature representing \"hotter\" systems than positive temperature, would seem paradoxical in this interpretation. \nThe paradox is resolved by considering the more rigorous definition of thermodynamic temperature as the tradeoff between internal energy and entropy\ncontained in the system, with \"coldness\", the \"reciprocal\" of temperature, being the more fundamental quantity. \nSystems with a positive temperature will increase in entropy as one adds energy to the system, while \nsystems with a negative temperature will decrease in entropy as one adds energy to the system.\n\nClassical thermodynamic systems cannot achieve negative temperatures: adding heat always increases their entropy. \nThe possibility of a decrease in entropy as energy increases requires the system to \"saturate\" in entropy. This is only possible if the number of high energy states is limited. In classical Boltzmann statistics, the number of high energy states is unlimited (particle speeds can in principle be increased indefinitely).\nSystems bounded by a maximum amount of energy are generally forbidden in classical mechanics, and the phenomenon of negative temperature is strictly a \nquantum mechanical phenomenon. Some systems, however (see the examples below), have a maximum amount of energy that they can hold, and as they approach that maximum energy their entropy actually begins to decrease.\n\nThe definition of thermodynamic temperature as a function of the change in the system's entropy under reversible heat transfer \nformula_1:\nEntropy being a state function, the integral of over any cyclical process is zero.\nFor a system in which the entropy is purely a function of the system's energy , the temperature can be defined as:\n\nEquivalently, thermodynamic beta, or \"coldness\", is defined as \nwhere is the Boltzmann constant.\n\nNote that in classical thermodynamics, is defined in terms of temperature. This is reversed here, is the statistical entropy, a function of the possibly microstates of the system, and temperature conveys information on the distribution of energy levels among the possible microstates.\nFor systems with many degrees of freedom, the statistical and thermodynamic definitions of entropy are generally consistent with each other. \nHowever, for small systems and systems where the number of states decreases with energy, the definitions of statistical entropy and thermodynamic entropy are not necessarily consistent, and the temperatures derived from these entropies are different. \n\nSome theorists have proposed using an alternate definition of entropy originally proposed by Gibbs as a way to resolve these inconsistencies, although this new definition would create other inconsistencies.\n\nNegative temperatures can only exist in a system where there are a limited number of energy states (see below). As the temperature is increased on such a system, particles move into higher and higher energy states, and as the temperature increases, the number of particles in the lower energy states and in the higher energy states approaches equality. (This is a consequence of the definition of temperature in statistical mechanics for systems with limited states.) By injecting energy into these systems in the right fashion, it is possible to create a system in which there are more particles in the higher energy states than in the lower ones. The system can then be characterised as having a negative temperature.\n\nA substance with a negative temperature is not colder than absolute zero, but rather it is hotter than infinite temperature. As Kittel and Kroemer (p. 462) put it, \"The temperature scale from cold to hot runs:\n\nThe corresponding inverse temperature scale, for the quantity \"β\" = 1/\"kT\" (where \"k\" is Boltzmann's constant), runs continuously from low energy to high as +∞, … , 0, …, −∞. Because it avoids the abrupt jump from +∞ to −∞, β is considered more natural than \"T\". Although a system can have multiple negative temperature regions and thus have -∞ to +∞ discontinuities.\n\nIn many familiar physical systems, temperature is associated to the kinetic energy of atoms. Since there is no upper bound on the momentum of an atom, there is no upper bound to the number of energy states available when more energy is added, and therefore no way to get to a negative temperature. However, in statistical mechanics, temperature can correspond to other degrees of freedom than just kinetic energy (see below).\n\nThe distribution of energy among the various translational, vibrational, rotational, electronic, and nuclear modes of a system determines the macroscopic temperature. In a \"normal\" system, thermal energy is constantly being exchanged between the various modes.\n\nHowever, in some situations, it is possible to isolate one or more of the modes. In practice, the isolated modes still exchange energy with the other modes, but the time scale of this exchange is much slower than for the exchanges within the isolated mode. One example is the case of nuclear spins in a strong external magnetic field. In this case, energy flows fairly rapidly among the spin states of interacting atoms, but energy transfer between the nuclear spins and other modes is relatively slow. Since the energy flow is predominantly within the spin system, it makes sense to think of a spin temperature that is distinct from the temperature associated to other modes.\n\nA definition of temperature can be based on the relationship:\n\nThe relationship suggests that a \"positive temperature\" corresponds to the condition where entropy, \"S\", increases as thermal energy, \"q\", is added to the system. This is the \"normal\" condition in the macroscopic world, and is always the case for the translational, vibrational, rotational, and non-spin related electronic and nuclear modes. The reason for this is that there are an infinite number of these types of modes, and adding more heat to the system increases the number of modes that are energetically accessible, and thus increases the entropy.\n\nThe simplest example, albeit a rather nonphysical one, is to consider a system of \"N\" particles, each of which can take an energy of either \"+ε\" or \"-ε\" but are otherwise noninteracting. This can be understood as a limit of the Ising model in which the interaction term becomes negligible. The total energy of the system is\n\nwhere \"σ\" is the sign of the \"i\"th particle and \"j\" is the number of particles with positive energy minus the number of particles with negative energy. From elementary combinatorics, the total number of microstates with this amount of energy is a binomial coefficient:\n\nBy the fundamental assumption of statistical mechanics, the entropy of this microcanonical ensemble is\n\nWe can solve for thermodynamic beta (β = 1/kT) by considering it as a central difference without taking the continuum limit:\n\nhence the temperature\n\nThis entire proof assumes the microcanonical ensemble with energy fixed and temperature being the emergent property. In the canonical ensemble, the temperature is fixed and energy is the emergent property. This leads to (formula_14 refers to microstates):\nFollowing the previous example, we choose a state with two levels and two particles. This leads to microstates formula_18, formula_19, formula_20, and formula_21.\nThe resulting values for formula_28, formula_29, and formula_30 are all increasing with formula_31 and never need to enter a negative temperature regime.\n\nThe previous example is approximately realized by a system of nuclear spins in an external magnetic field. This allows the experiment to be run as a variation of nuclear magnetic resonance spectroscopy. In the case of electronic and nuclear spin systems, there are only a finite number of modes available, often just two, corresponding to spin up and spin down. In the absence of a magnetic field, these spin states are \"degenerate\", meaning that they correspond to the same energy. When an external magnetic field is applied, the energy levels are split, since those spin states that are aligned with the magnetic field will have a different energy from those that are anti-parallel to it.\n\nIn the absence of a magnetic field, such a two-spin system would have maximum entropy when half the atoms are in the spin-up state and half are in the spin-down state, and so one would expect to find the system with close to an equal distribution of spins. Upon application of a magnetic field, some of the atoms will tend to align so as to minimize the energy of the system, thus slightly more atoms should be in the lower-energy state (for the purposes of this example we will assume the spin-down state is the lower-energy state). It is possible to add energy to the spin system using radio frequency (RF) techniques. This causes atoms to \"flip\" from spin-down to spin-up.\n\nSince we started with over half the atoms in the spin-down state, this initially drives the system towards a 50/50 mixture, so the entropy is increasing, corresponding to a positive temperature. However, at some point, more than half of the spins are in the spin-up position. In this case, adding additional energy reduces the entropy, since it moves the system further from a 50/50 mixture. This reduction in entropy with the addition of energy corresponds to a negative temperature. In NMR spectroscopy, this corresponds to pulses with a pulse width of over 180° (for a given spin). While relaxation is fast in solids, it can take several seconds in solutions and even longer in gases and in ultracold systems; several hours were reported for silver and rhodium at picokelvin temperatures. It is still important to understand that the temperature is negative only with respect to nuclear spins. Other degrees of freedom, such as molecular vibrational, electronic and electron spin levels are at a positive temperature, so the object still has positive sensible heat. Relaxation actually happens by exchange of energy between the nuclear spin states and other states (e.g. through the nuclear Overhauser effect with other spins).\n\nThis phenomenon can also be observed in many lasing systems, wherein a large fraction of the system's atoms (for chemical and gas lasers) or electrons (in semiconductor lasers) are in excited states. This is referred to as a population inversion.\n\nThe Hamiltonian for a single mode of a luminescent radiation field at frequency \"ν\" is\nThe density operator in the grand canonical ensemble is\nFor the system to have a ground state, the trace to converge, and the density operator to be generally meaningful, \"βH\" must be positive semidefinite. So if \"hν\" < \"μ\", and \"H\" is negative semidefinite, then \"β\" must itself be negative, implying a negative temperature.\n\nThe two-dimensional systems can exist in negative temperature states.\n\nNegative temperatures have also been achieved in motional degrees of freedom. Using an optical lattice, upper bounds were placed on the kinetic energy, interaction energy and potential energy of cold atoms. This was done by tuning the interactions of the atoms from repulsive to attractive using a Feshbach resonance and changing the overall harmonic potential from trapping to anti-trapping, thus transforming the Bose-Hubbard Hamiltonian from formula_34. Performing this transformation adiabatically while keeping the atoms in the Mott insulator regime, it is possible to go from a low entropy positive temperature state to a low entropy negative temperature state. In the negative temperature state, the atoms macroscopically occupy the maximum momentum state of the lattice. The negative temperature ensembles equilibrated and showed long lifetimes in an anti-trapping harmonic potential.\n\n\n"}
{"id": "2288927", "url": "https://en.wikipedia.org/wiki?curid=2288927", "title": "Negative thermal expansion", "text": "Negative thermal expansion\n\nNegative thermal expansion (NTE) is an unusual physicochemical process in which some materials contract upon heating, rather than expand as most other materials do. Materials which undergo NTE have a range of potential engineering, photonic, electronic, and structural applications. For example, if one were to mix a negative thermal expansion material with a \"normal\" material which expands on heating, it could be possible to make a zero expansion composite material.\n\nThere are a number of physical processes which may cause contraction with increasing temperature, including transverse vibrational modes, Rigid Unit Modes and phase transitions.\n\nRecently, Liu et al. showed that the NTE phenomenon originates from the existence of high pressure, small volume configurations with higher entropy, with their configurations present in the stable phase matrix through thermal fluctuations. They were able to predict both the colossal positive thermal expansion (In cerium) and zero and infinite negative thermal expansion (in ) \n\nNegative thermal expansion is usually observed in non-close-packed systems with directional interactions (e.g. ice, graphene, etc.) and complex compounds (e.g. , , beta-quartz, some zeolites, etc.). However, in a paper, it was shown that negative thermal expansion (NTE) is also realized in single-component close-packed lattices with pair central force interactions. The following sufficient condition for potential giving rise to NTE behavior is proposed:\n\nformula_1\n\nwhere formula_2 is pair interatomic potential, formula_3 is the equilibrium distance. This condition is (i) necessary and sufficient in 1D and (ii) sufficient, but not necessary in 2D and 3D. \"An approximate\" necessary and sufficient condition is derived in a paper\n\nformula_4\n\nwhere formula_5 is the space dimensionality. Thus in 2D and 3D negative thermal expansion in close-packed systems with pair interactions is realized even when the third derivative of the potential is zero or even negative. Note that one-dimensional and multidimensional cases are qualitatively different. In 1D thermal expansion is cased by anharmonicity of interatomic potential only. Therefore, the sign of thermal expansion coefficient is determined by the sign of the third derivative of the potential. In multidimensional case the geometrical nonlinearity is also present, i.e. lattice vibrations are nonlinear even in the case of harmonic interatomic potential. This nonlinearity contributes to thermal expansion. Therefore, in multidimensional case both formula_6 and formula_7 are present in the condition for negative thermal expansion.\n\nThere are many potential applications for materials with controlled thermal expansion properties, as thermal expansion causes many problems in engineering, and indeed in everyday life. One simple example of a thermal expansion problem is the tendency of dental fillings to expand by an amount different from the teeth, for example when drinking a hot drink, causing toothache. If dental fillings were made of a composite material containing a mixture of materials with positive and negative thermal expansion then the overall expansion could be precisely tailored to that of tooth enamel.\n\nPerhaps one of the most studied materials to exhibit \"negative thermal expansion\" is zirconium tungstate (). This compound contracts continuously over a temperature range of 0.3 to 1050 K (at higher temperatures the material decomposes). Other materials that exhibit this behaviour include: other members of the family of materials (where A = or , M = or ) and . also is an example of controllable \"negative thermal expansion\".\n\nOrdinary ice shows NTE in its hexagonal and cubic phases at very low temperatures (below –200 °C). In its liquid form, pure water also displays negative thermal expansivity below 3.984 °C.\n\nRubber elasticity shows NTE at normal temperatures, but the reason for the effect is rather different from that in most other materials. Put simply, as the long polymer chains absorb energy, they adopt a more contorted configuration, reducing the volume of the material.\n\nQuartz () and a number of zeolites also show NTE over certain temperature ranges. Fairly pure silicon (Si) has a negative coefficient of thermal expansion for temperatures between about 18 K and 120 K.\nCubic Scandium trifluoride has this property which is explained by the quartic oscillation of the fluoride ions. The energy stored in the bending strain of the fluoride ion is proportional to the fourth power of the displacement angle, unlike most other materials where it is proportional to the square of the displacement. A fluorine atom is bound to two scandium atoms, and as temperature increases the fluorine oscillates more perpendicularly to its bonds. This draws the scandium atoms together throughout the material and it contracts. exhibits this property from 10 to 1100 K above which it shows the normal positive thermal expansion.. Shape memory alloys such as NiTi are a nascent class of materials that exhibits zero and negative thermal expansion [13].\n\n 13. ^ Ahadi, A.; Matsushita, Y.; Sawaguchi, T.; Sun, QP.; Tsuchiya, K. (2017). \"Origin of zero and negative thermal expansion in severely-deformed superelastic NiTi alloy\". Acta Materialia. 124, 79–92. doi:10.1107/S0108768194004933.\nhttps://doi.org/10.1016/j.actamat.2016.10.054\n\n\n"}
{"id": "1809875", "url": "https://en.wikipedia.org/wiki?curid=1809875", "title": "Notion (philosophy)", "text": "Notion (philosophy)\n\nA notion in philosophy is a reflection in the mind of real objects and phenomena in their essential features and relations. Notions are usually described in terms of scope and content. This is because notions are often created in response to empirical observations (or experiments) of covarying trends among variables.\n\n\"Notion\" is the common translation for \"Begriff\" as used by Hegel in his Science of Logic (1816).\n\nA primitive notion is used in logic or mathematics as an undefined term or concept at the foundation of an axiomatic system to be constructed. \n\nHowever, in philosophy the term \"primitive notion\" has historical content. For example, Gottfried Leibniz wrote \"De Alphabeto Cogitationum Humanarum\" (English: \"An Alphabet for Human Thought\"). Jaap Maat (2004) reviewed Leibniz for \"Philosophical Languages of the 17th Century\". According to Leibniz, \"The alphabet of human thought is a catalogue of primitive notions, or those we cannot render clearer by any definitions.\" Maat explains, \"a thing which is known without other intermediate notions can be considered to be primitive,\" and further, \"a primitive notion is said to be conceived through itself\".\n\nAnother example is in the \"Meditations\" of René Descartes:\n\n"}
{"id": "13065509", "url": "https://en.wikipedia.org/wiki?curid=13065509", "title": "Paired opposites", "text": "Paired opposites\n\nPaired opposites are an ancient, pre-Socratic method of establishing thesis, antithesis and synthesis in terms of a standard for what is right and proper in natural philosophy.\n\nScalar ranges and coordinate systems are paired opposites within sets. Incorporating dimensions of positive and negative numbers and exponents, or expanding x, y and z coordinates, by adding a fourth dimension of time allows a resolution of position relative to the standard of the scale which is often taken as 0,0,0,0 with additional dimensions added as referential scales are expanded from space and time to mass and energy.\n\nAncient systems frequently scaled their degree of opposition by rate of increase or rate of decrease. Linear increase was enhanced by doubling systems. An acceleration in the rate of increase or decrease could be analyzed arithmetrically, geometrically, or through a wide range of other numerical and physical analysis. Arithmetic and geometric series, and other methods of rating proportionate expansion or contraction could be thought of as convergent or divergent toward a position.\n\nThough unit quantities were first defined by spatial dimensions, and then expanded by adding coordinates of time, the weight or mass a given spatial dimension could contain was also considered and even in antiquity, conditions under which the standard would be established such as at a given temperature, distance from sea level, or density were added.\n\nRates of change over time were then considered as either indexes of production or depletion\n\nPaired opposites are used as poetic diction meaning \"everything\". Common phrases incorporated paired opposites in English include \"all creatures great and small,\" \"working for the man every night and day,\" \"more things in heaven and Earth\" \"searching high and low\" \"in sickness and in health\". In Greek literature, Homer uses the device when he lets Telemachus say, \"I know all things, the good and the evil\" (Od.20:309-10).\nThe same phrase is used in Hebrew in text of Genesis, referring to the Tree of the knowledge of good and evil.\n\nIn quantum mechanics, as well as some fields of mathematics, conjugate variables are a form of paired opposites, in which knowledge of one precludes knowledge of the other. A standard example is the relation between position (x) and momentum (p), which can be expressed in terms of the uncertainty principle as formula_1.\n"}
{"id": "6880483", "url": "https://en.wikipedia.org/wiki?curid=6880483", "title": "Philosophy of mind", "text": "Philosophy of mind\n\nPhilosophy of mind is a branch of philosophy that studies the nature of the mind. The mind–body problem is a paradigm issue in philosophy of mind, although other issues are addressed, such as the hard problem of consciousness, and the nature of particular mental states. Aspects of the mind that are studied include mental events, mental functions, mental properties, consciousness, the ontology of the mind, the nature of thought, and the relationship of the mind to the body.\n\nDualism and monism are the two central schools of thought on the mind–body problem, although nuanced views have arisen that do not fit one or the other category neatly. Dualism is found in both Eastern and Western traditions (in the Sankhya and Yoga schools of Hindu philosophy as well as Plato) but its entry into Western philosophy was thanks to René Descartes in the 17th century. Substance dualists like Descartes argue that the mind is an independently existing substance, whereas property dualists maintain that the mind is a group of independent properties that emerge from and cannot be reduced to the brain, but that it is not a distinct substance.\n\nMonism is the position that mind and body are not ontologically distinct entities (independent substances). This view was first advocated in Western philosophy by Parmenides in the 5th century BCE and was later espoused by the 17th century rationalist Baruch Spinoza. Physicalists argue that only entities postulated by physical theory exist, and that mental processes will eventually be explained in terms of these entities as physical theory continues to evolve. Physicalists maintain various positions on the prospects of reducing mental properties to physical properties (many of whom adopt compatible forms of property dualism), and the ontological status of such mental properties remains unclear. Idealists maintain that the mind is all that exists and that the external world is either mental itself, or an illusion created by the mind. Neutral monists such as Ernst Mach and William James argue that events in the world can be thought of as either mental (psychological) or physical depending on the network of relationships into which they enter, and dual-aspect monists such as Spinoza adhere to the position that there is some other, neutral substance, and that both matter and mind are properties of this unknown substance. The most common monisms in the 20th and 21st centuries have all been variations of physicalism; these positions include behaviorism, the type identity theory, anomalous monism and functionalism.\n\nMost modern philosophers of mind adopt either a reductive or non-reductive physicalist position, maintaining in their different ways that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, especially in the fields of sociobiology, computer science (specifically, artificial intelligence), evolutionary psychology and the various neurosciences. Reductive physicalists assert that all mental states and properties will eventually be explained by scientific accounts of physiological processes and states. Non-reductive physicalists argue that although the mind is not a separate substance, mental properties supervene on physical properties, or that the predicates and vocabulary used in mental descriptions and explanations are indispensable, and cannot be reduced to the language and lower-level explanations of physical science. Continued neuroscientific progress has helped to clarify some of these issues; however, they are far from being resolved. Modern philosophers of mind continue to ask how the subjective qualities and the intentionality of mental states and properties can be explained in naturalistic terms.\n\nThe mind–body problem concerns the explanation of the relationship that exists between minds, or mental processes, and bodily states or processes. The main aim of philosophers working in this area is to determine the nature of the mind and mental states/processes, and how—or even if—minds are affected by and can affect the body.\n\nOur perceptual experiences depend on stimuli that arrive at our various sensory organs from the external world, and these stimuli cause changes in our mental states, ultimately causing us to feel a sensation, which may be pleasant or unpleasant. Someone's desire for a slice of pizza, for example, will tend to cause that person to move his or her body in a specific manner and in a specific direction to obtain what he or she wants. The question, then, is how it can be possible for conscious experiences to arise out of a lump of gray matter endowed with nothing but electrochemical properties.\n\nA related problem is how someone's propositional attitudes (e.g. beliefs and desires) cause that individual's neurons to fire and muscles to contract. These comprise some of the puzzles that have confronted epistemologists and philosophers of mind from at least the time of René Descartes.\n\nDualism is a set of views about the relationship between mind and matter (or body). It begins with the claim that mental phenomena are, in some respects, non-physical. One of the earliest known formulations of mind–body dualism was expressed in the eastern Sankhya and Yoga schools of Hindu philosophy (c. 650 BCE), which divided the world into purusha (mind/spirit) and prakriti (material substance). Specifically, the Yoga Sutra of Patanjali presents an analytical approach to the nature of the mind.\n\nIn Western Philosophy, the earliest discussions of dualist ideas are in the writings of Plato who maintained that humans' \"intelligence\" (a faculty of the mind or soul) could not be identified with, or explained in terms of, their physical body. However, the best-known version of dualism is due to René Descartes (1641), and holds that the mind is a non-extended, non-physical substance, a \"res cogitans\". Descartes was the first to clearly identify the mind with consciousness and self-awareness, and to distinguish this from the brain, which was the seat of intelligence. He was therefore the first to formulate the mind–body problem in the form in which it still exists today.\n\nThe most frequently used argument in favor of dualism appeals to the common-sense intuition that conscious experience is distinct from inanimate matter. If asked what the mind is, the average person would usually respond by identifying it with their self, their personality, their soul, or some other such entity. They would almost certainly deny that the mind simply is the brain, or vice versa, finding the idea that there is just one ontological entity at play to be too mechanistic, or simply unintelligible. Many modern philosophers of mind think that these intuitions are misleading and that we should use our critical faculties, along with empirical evidence from the sciences, to examine these assumptions to determine whether there is any real basis to them.\n\nAnother important argument in favor of dualism is that the mental and the physical seem to have quite different, and perhaps irreconcilable, properties. Mental events have a subjective quality, whereas physical events do not. So, for example, one can reasonably ask what a burnt finger feels like, or what a blue sky looks like, or what nice music sounds like to a person. But it is meaningless, or at least odd, to ask what a surge in the uptake of glutamate in the dorsolateral portion of the prefrontal cortex feels like.\n\nPhilosophers of mind call the subjective aspects of mental events \"qualia\" or \"raw feels\". There is something that it is like to feel pain, to see a familiar shade of blue, and so on. There are qualia involved in these mental events that seem particularly difficult to reduce to anything physical. David Chalmers explains this argument by stating that we could conceivably know all the objective information about something, such as the brain states and wavelengths of light involved with seeing the color red, but still not know something fundamental about the situation – what it is like to see the color red.\n\nIf consciousness (the mind) can exist independently of physical reality (the brain), one must explain how physical memories are created concerning consciousness. Dualism must therefore explain how consciousness affects physical reality. One possible explanation is that of a miracle, proposed by Arnold Geulincx and Nicolas Malebranche, where all mind–body interactions require the direct intervention of God.\n\nAnother possible argument that has been proposed by C. S. Lewis is the Argument from Reason: if, as monism implies, all of our thoughts are the effects of physical causes, then we have no reason for assuming that they are also the consequent of a reasonable ground. Knowledge, however, is apprehended by reasoning from ground to consequent. Therefore, if monism is correct, there would be no way of knowing this—or anything else—we could not even suppose it, except by a fluke.\n\nThe zombie argument is based on a thought experiment proposed by Todd Moody, and developed by David Chalmers in his book \"The Conscious Mind\". The basic idea is that one can imagine one's body, and therefore conceive the existence of one's body, without any conscious states being associated with this body. Chalmers' argument is that it seems possible that such a being could exist because all that is needed is that all and only the things that the physical sciences describe about a zombie must be true of it. Since none of the concepts involved in these sciences make reference to consciousness or other mental phenomena, and any physical entity can be by definition described scientifically via physics, the move from conceivability to possibility is not such a large one. Others such as Dennett have argued that the notion of a philosophical zombie is an incoherent, or unlikely, concept. It has been argued under physicalism that one must either believe that anyone including oneself might be a zombie, or that no one can be a zombie—following from the assertion that one's own conviction about being (or not being) a zombie is a product of the physical world and is therefore no different from anyone else's. This argument has been expressed by Dennett who argues that \"Zombies think they are conscious, think they have qualia, think they suffer pains—they are just 'wrong' (according to this lamentable tradition) in ways that neither they nor we could ever discover!\"\nSee also the problem of other minds.\n\nInteractionist dualism, or simply interactionism, is the particular form of dualism first espoused by Descartes in the \"Meditations\". In the 20th century, its major defenders have been Karl Popper and John Carew Eccles. It is the view that mental states, such as beliefs and desires, causally interact with physical states.\n\nDescartes' famous argument for this position can be summarized as follows: Seth has a clear and distinct idea of his mind as a thinking thing that has no spatial extension (i.e., it cannot be measured in terms of length, weight, height, and so on). He also has a clear and distinct idea of his body as something that is spatially extended, subject to quantification and not able to think. It follows that mind and body are not identical because they have radically different properties.\n\nAt the same time, however, it is clear that Seth's mental states (desires, beliefs, etc.) have causal effects on his body and vice versa: A child touches a hot stove (physical event) which causes pain (mental event) and makes her yell (physical event), this in turn provokes a sense of fear and protectiveness in the caregiver (mental event), and so on.\n\nDescartes' argument crucially depends on the premise that what Seth believes to be \"clear and distinct\" ideas in his mind are necessarily true. Many contemporary philosophers doubt this. For example, Joseph Agassi suggests that several scientific discoveries made since the early 20th century have undermined the idea of privileged access to one's own ideas. Freud claimed that a psychologically-trained observer can understand a person's unconscious motivations better than the person himself does. Duhem has shown that a philosopher of science can know a person's methods of discovery better than that person herself does, while Malinowski has shown that an anthropologist can know a person's customs and habits better than the person whose customs and habits they are. He also asserts that modern psychological experiments that cause people to see things that are not there provide grounds for rejecting Descartes' argument, because scientists can describe a person's perceptions better than the person herself can.\n\nPsychophysical parallelism, or simply parallelism, is the view that mind and body, while having distinct ontological statuses, do not causally influence one another. Instead, they run along parallel paths (mind events causally interact with mind events and brain events causally interact with brain events) and only seem to influence each other. This view was most prominently defended by Gottfried Leibniz. Although Leibniz was an ontological monist who believed that only one type of substance, the monad, exists in the universe, and that everything is reducible to it, he nonetheless maintained that there was an important distinction between \"the mental\" and \"the physical\" in terms of causation. He held that God had arranged things in advance so that minds and bodies would be in harmony with each other. This is known as the doctrine of pre-established harmony.\n\nOccasionalism is the view espoused by Nicholas Malebranche as well as Islamic philosophers such as Abu Hamid Muhammad ibn Muhammad al-Ghazali that asserts that all supposedly causal relations between physical events, or between physical and mental events, are not really causal at all. While body and mind are different substances, causes (whether mental or physical) are related to their effects by an act of God's intervention on each specific occasion.\n\nProperty dualism is the view that the world is constituted of just one kind of substance – the physical kind – and there exist two distinct kinds of properties: physical properties and mental properties. In other words, it is the view that non-physical, mental properties (such as beliefs, desires and emotions) inhere in some physical bodies (at least, brains). How mental and physical properties relate causally depends on the variety of property dualism in question, and is not always a clear issue. Sub-varieties of property dualism include:\n\n\nDual aspect theory or dual-aspect monism is the view that the mental and the physical are two aspects of, or perspectives on, the same substance. (Thus it is a mixed position, which is monistic in some respects). In modern philosophical writings, the theory's relationship to neutral monism has become somewhat ill-defined, but one proffered distinction says that whereas neutral monism allows the context of a given group of neutral elements and the relationships into which they enter to determine whether the group can be thought of as mental, physical, both, or neither, dual-aspect theory suggests that the mental and the physical are manifestations (or aspects) of some underlying substance, entity or process that is itself neither mental nor physical as normally understood. Various formulations of dual-aspect monism also require the mental and the physical to be complementary, mutually irreducible and perhaps inseparable (though distinct).\n\nThis is a philosophy of mind that regards the degrees of freedom between mental and physical well-being as not necessarily synonymous thus implying an experiential dualism between body and mind. An example of these disparate degrees of freedom is given by Allan Wallace who notes that it is \"experientially apparent that one may be physically uncomfortable—for instance, while engaging in a strenuous physical workout—while mentally cheerful; conversely, one may be mentally distraught while experiencing physical comfort\". Experiential dualism notes that our subjective experience of merely seeing something in the physical world seems qualitatively different than mental processes like grief that comes from losing a loved one. This philosophy also is a proponent of causal dualism which is defined as the dual ability for mental states and physical states to affect one another. Mental states can cause changes in physical states and vice versa.\n\nHowever, unlike cartesian dualism or some other systems, experiential dualism does not posit two fundamental substances in reality: mind and matter. Rather, experiential dualism is to be understood as a conceptual framework that gives credence to the qualitative difference between the experience of mental and physical states. Experiential dualism is accepted as the conceptual framework of Madhyamaka Buddhism.\n\nMadhayamaka Buddhism goes even further, finding fault with the monist view of physicalist philosophies of mind as well in that these generally posit matter and energy as the fundamental substance of reality. Nonetheless, this does not imply that the cartesian dualist view is correct, rather Madhyamaka regards as error any affirming view of a fundamental substance to reality.In denying the independent self-existence of all the phenomena that make up the world of our experience, the Madhyamaka view departs from both the substance dualism of Descartes and the substance monism—namely, physicalism—that is characteristic of modern science. The physicalism propounded by many contemporary scientists seems to assert that the real world is composed of physical things-in-themselves, while all mental phenomena are regarded as mere appearances, devoid of any reality in and of themselves. Much is made of this difference between appearances and reality.\nIndeed, physicalism, or the idea that matter is the only fundamental substance of reality, is explicitly rejected by Buddhism.In the Madhyamaka view, mental events are no more or less real than physical events. In terms of our common-sense experience, differences of kind do exist between physical and mental phenomena. While the former commonly have mass, location, velocity, shape, size, and numerous other physical attributes, these are not generally characteristic of mental phenomena. For example, we do not commonly conceive of the feeling of affection for another person as having mass or location. These physical attributes are no more appropriate to other mental events such as sadness, a recalled image from one's childhood, the visual perception of a rose, or consciousness of any sort. Mental phenomena are, therefore, not regarded as being physical, for the simple reason that they lack many of the attributes that are uniquely characteristic of physical phenomena. Thus, Buddhism has never adopted the physicalist principle that regards only physical things as real.\n\nHylomorphism is a theory that originates with Aristotelian philosophy, which conceives being as a compound of matter and form. \"Hylomorphism\" is a 19th-century term formed from the Greek words ὕλη \"hyle\", \"wood, matter\", and μορφή, \"morphē\", \"form\".\n\nIn contrast to dualism, monism does not accept any fundamental divisions. The fundamentally disparate nature of reality has been central to forms of eastern philosophies for over two millennia. In Indian and Chinese philosophy, monism is integral to how experience is understood. Today, the most common forms of monism in Western philosophy are physicalist. Physicalistic monism asserts that the only existing substance is physical, in some sense of that term to be clarified by our best science. However, a variety of formulations (see below) are possible. Another form of monism, idealism, states that the only existing substance is mental. Although pure idealism, such as that of George Berkeley, is uncommon in contemporary Western philosophy, a more sophisticated variant called panpsychism, according to which mental experience and properties may be at the foundation of physical experience and properties, has been espoused by some philosophers such as Alfred North Whitehead and David Ray Griffin.\n\nPhenomenalism is the theory that representations (or sense data) of external objects are all that exist. Such a view was briefly adopted by Bertrand Russell and many of the logical positivists during the early 20th century. A third possibility is to accept the existence of a basic substance that is neither physical nor mental. The mental and physical would then both be properties of this neutral substance. Such a position was adopted by Baruch Spinoza and was popularized by Ernst Mach in the 19th century. This neutral monism, as it is called, resembles property dualism.\n\nBehaviorism dominated philosophy of mind for much of the 20th century, especially the first half. In psychology, behaviorism developed as a reaction to the inadequacies of introspectionism. Introspective reports on one's own interior mental life are not subject to careful examination for accuracy and cannot be used to form predictive generalizations. Without generalizability and the possibility of third-person examination, the behaviorists argued, psychology cannot be scientific. The way out, therefore, was to eliminate the idea of an interior mental life (and hence an ontologically independent mind) altogether and focus instead on the description of observable behavior.\n\nParallel to these developments in psychology, a philosophical behaviorism (sometimes called logical behaviorism) was developed. This is characterized by a strong verificationism, which generally considers unverifiable statements about interior mental life pointless. For the behaviorist, mental states are not interior states on which one can make introspective reports. They are just descriptions of behavior or dispositions to behave in certain ways, made by third parties to explain and predict another's behavior.\n\nPhilosophical behaviorism has fallen out of favor since the latter half of the 20th century, coinciding with the rise of cognitivism. Cognitivists reject behaviorism due to several perceived problems. For example, behaviorism could be said to be counterintuitive when it maintains that someone is talking about behavior in the event that a person is experiencing a painful headache.\n\nType physicalism (or type-identity theory) was developed by John Smart and Ullin Place as a direct reaction to the failure of behaviorism. These philosophers reasoned that, if mental states are something material, but not behavioral, then mental states are probably identical to internal states of the brain. In very simplified terms: a mental state \"M\" is nothing other than brain state \"B\". The mental state \"desire for a cup of coffee\" would thus be nothing more than the \"firing of certain neurons in certain brain regions\".\nDespite its initial plausibility, the identity theory faces a strong challenge in the form of the thesis of multiple realizability, first formulated by Hilary Putnam. For example, not only humans, but many different species of animals can experience pain. However, it seems highly unlikely that all of these diverse organisms with the same pain experience are in the identical brain state. And if this is the case, then pain cannot be identical to a specific brain state. The identity theory is thus empirically unfounded.\n\nOn the other hand, even granted the above, it does not follow that identity theories of all types must be abandoned. According to token identity theories, the fact that a certain brain state is connected with only one mental state of a person does not have to mean that there is an absolute correlation between types of mental state and types of brain state. The type–token distinction can be illustrated by a simple example: the word \"green\" contains four types of letters (g, r, e, n) with two tokens (occurrences) of the letter \"e\" along with one each of the others.\nThe idea of token identity is that only particular occurrences of mental events are identical with particular occurrences or tokenings of physical events. Anomalous monism (see below) and most other non-reductive physicalisms are token-identity theories. Despite these problems, there is a renewed interest in the type identity theory today, primarily due to the influence of Jaegwon Kim.\n\nFunctionalism was formulated by Hilary Putnam and Jerry Fodor as a reaction to the inadequacies of the identity theory. Putnam and Fodor saw mental states in terms of an empirical computational theory of the mind. At about the same time or slightly after, D.M. Armstrong and David Kellogg Lewis formulated a version of functionalism that analyzed the mental concepts of folk psychology in terms of functional roles. Finally, Wittgenstein's idea of meaning as use led to a version of functionalism as a theory of meaning, further developed by Wilfrid Sellars and Gilbert Harman. Another one, psychofunctionalism, is an approach adopted by the naturalistic philosophy of mind associated with Jerry Fodor and Zenon Pylyshyn.\n\nWhat all these different varieties of functionalism share in common is the thesis that mental states are characterized by their causal relations with other mental states and with sensory inputs and behavioral outputs. That is, functionalism abstracts away from the details of the physical implementation of a mental state by characterizing it in terms of non-mental functional properties. For example, a kidney is characterized scientifically by its functional role in filtering blood and maintaining certain chemical balances. From this point of view, it does not really matter whether the kidney be made up of organic tissue, plastic nanotubes or silicon chips: it is the role that it plays and its relations to other organs that define it as a kidney.\n\nNon-reductionist philosophers hold firmly to two essential convictions with regard to mind–body relations: 1) Physicalism is true and mental states must be physical states, but 2) All reductionist proposals are unsatisfactory: mental states cannot be reduced to behavior, brain states or functional states. Hence, the question arises whether there can still be a non-reductive physicalism. Donald Davidson's anomalous monism is an attempt to formulate such a physicalism. He \"thinks that when one runs across what are traditionally seen as absurdities of Reason, such as akrasia or self-deception, the personal psychology framework is not to be given up in favor of the subpersonal one, but rather must be enlarged or extended so that the rationality set out by the principle of charity can be found elsewhere.\"\n\nDavidson uses the thesis of supervenience: mental states supervene on physical states, but are not reducible to them. \"Supervenience\" therefore describes a functional dependence: there can be no change in the mental without some change in the physical–causal reducibility between the mental and physical without ontological reducibility.\n\nBecause non-reductive physicalist theories attempt to both retain the ontological distinction between mind and body and try to solve the \"surfeit of explanations puzzle\" in some way; critics often see this as a paradox and point out the similarities to epiphenomenalism, in that it is the brain that is seen as the root \"cause\" not the mind, and the mind seems to be rendered inert.\n\nEpiphenomenalism regards one or more mental states as the byproduct of physical brain states, having no influence on physical states. The interaction is one-way (solving the \"surfeit of explanations puzzle\") but leaving us with non-reducible mental states (as a byproduct of brain states) – causally reducible, but ontologically irreducible to physical states. Pain would be seen by epiphenomenaliasts as being caused by the brain state but as not having effects on other brain states, though it might have effects on other mental states (i.e. cause distress).\n\nWeak emergentism is a form of \"non-reductive physicalism\" that involves a layered view of nature, with the layers arranged in terms of increasing complexity and each corresponding to its own special science. Some philosophers hold that emergent properties causally interact with more fundamental levels, while others maintain that higher-order properties simply supervene over lower levels without direct causal interaction. The latter group therefore holds a less strict, or \"weaker\", definition of emergentism, which can be rigorously stated as follows: a property P of composite object O is emergent if it is metaphysically impossible for another object to lack property P if that object is composed of parts with intrinsic properties identical to those in O and has those parts in an identical configuration.\n\nSometimes emergentists use the example of water having a new property when Hydrogen H and Oxygen O combine to form HO (water). In this example there \"emerges\" a new property of a transparent liquid that would not have been predicted by understanding hydrogen and oxygen as gases. This is analogous to physical properties of the brain giving rise to a mental state. Emergentists try to solve the notorious mind–body gap this way. One problem for emergentism is the idea of \"causal closure\" in the world that does not allow for a mind-to-body causation.\n\nIf one is a materialist and believes that all aspects of our common-sense psychology will find reduction to a mature cognitive neuroscience, and that non-reductive materialism is mistaken, then one can adopt a final, more radical position: eliminative materialism.\n\nThere are several varieties of eliminative materialism, but all maintain that our common-sense \"folk psychology\" badly misrepresents the nature of some aspect of cognition. Eliminativists such as Patricia and Paul Churchland argue that while folk psychology treats cognition as fundamentally sentence-like, the non-linguistic vector/matrix model of neural network theory or connectionism will prove to be a much more accurate account of how the brain works.\n\nThe Churchlands often invoke the fate of other, erroneous popular theories and ontologies that have arisen in the course of history. For example, Ptolemaic astronomy served to explain and roughly predict the motions of the planets for centuries, but eventually this model of the solar system was eliminated in favor of the Copernican model. The Churchlands believe the same eliminative fate awaits the \"sentence-cruncher\" model of the mind in which thought and behavior are the result of manipulating sentence-like states called \"propositional attitudes\".\n\nIdealism is the form of monism that sees the world as consisting of minds, mental contents and or consciousness.\nIdealists are not faced with explaining how minds arise from bodies: rather, the world, bodies and objects are regarded as mere appearances held by minds. However, accounting for the mind–body problem is not usually the main motivation for idealism; rather, idealists tend to be motivated by skepticism, intentionality, and the unique nature of ideas.\nIdealism is prominent in Eastern religious and philosophical thought. It has gone through several cycles of popularity and neglect in the history of Western philosophy.\n\nDifferent varieties of idealism may hold that there are\n\nNeutral monism, in philosophy, is the metaphysical view that the mental and the physical are two ways of organizing or describing the same elements, which are themselves \"neutral\", that is, neither physical nor mental. This view denies that the mental and the physical are two fundamentally different things. Rather, neutral monism claims the universe consists of only one kind of stuff, in the form of neutral elements that are in themselves neither mental nor physical. These neutral elements might have the properties of color and shape, just as we experience those properties. But these shaped and colored elements do not exist in a mind (considered as a substantial entity, whether dualistically or physicalistically); they exist on their own.\n\nSome philosophers take an epistemic approach and argue that the mind–body problem is currently unsolvable, and perhaps will always remain unsolvable to human beings. This is usually termed New mysterianism. Colin McGinn holds that human beings are cognitively closed in regards to their own minds. According to McGinn human minds lack the concept-forming procedures to fully grasp how mental properties such as consciousness arise from their causal basis. An example would be how an elephant is cognitively closed in regards to particle physics.\n\nA more moderate conception has been expounded by Thomas Nagel, which holds that the mind–body problem is currently unsolvable at the present stage of scientific development and that it might take a future scientific paradigm shift or revolution to bridge the explanatory gap. Nagel posits that in the future a sort of \"objective phenomenology\" might be able to bridge the gap between subjective conscious experience and its physical basis.\n\nEach attempt to answer the mind–body problem encounters substantial problems. Some philosophers argue that this is because there is an underlying conceptual confusion. These philosophers, such as Ludwig Wittgenstein and his followers in the tradition of linguistic criticism, therefore reject the problem as illusory. They argue that it is an error to ask how mental and biological states fit together. Rather it should simply be accepted that human experience can be described in different ways—for instance, in a mental and in a biological vocabulary. Illusory problems arise if one tries to describe the one in terms of the other's vocabulary or if the mental vocabulary is used in the wrong contexts. This is the case, for instance, if one searches for mental states of the brain. The brain is simply the wrong context for the use of mental vocabulary—the search for mental states of the brain is therefore a category error or a sort of fallacy of reasoning.\n\nToday, such a position is often adopted by interpreters of Wittgenstein such as Peter Hacker. However, Hilary Putnam, the originator of functionalism, has also adopted the position that the mind–body problem is an illusory problem which should be dissolved according to the manner of Wittgenstein.\n\nWhere is the mind located? If the mind is a physical phenomenon of some kind, it has to be located somewhere. According to some, there are two possible options: either the mind is internal to the body (internalism) or the mind is external to it (externalism). More generally, either the mind depends only on events and properties taking place inside the subject's body or it depends also on factors external to it.\n\nProponents of internalism are committed to the view that neural activity is sufficient to produce the mind.\nProponents of externalism maintain that the surrounding world is in some sense constitutive of the mind.\n\nExternalism differentiates into several versions. The main ones are semantic externalism, cognitive externalism and phenomenal externalism. Each of these versions of externalism can further be divided into whether they refer only to the content or to the vehicles of the mind.\n\nSemantic externalism holds that the semantic content of the mind is totally or partially defined by a state of affairs external to the body of the subject. Hilary Putnam's Twin Earth thought experiment is a good example.\n\nCognitive externalism is a very broad collection of views that suggests the role of the environment, of tools, of development, and of the body in fleshing out cognition. Embodied cognition, the extended mind, and enactivism are good examples.\n\nPhenomenal externalism suggests that the phenomenal aspects of the mind are external to the body. Authors who addressed this possibility are Ted Honderich, Edwin Holt, Francois Tonneau, Kevin O'Regan, Riccardo Manzotti, Teed Rockwell and Max Velmans.\n\nThe thesis of physicalism is that the mind is part of the material (or physical) world. Such a position faces the problem that the mind has certain properties that no other material thing seems to possess. Physicalism must therefore explain how it is possible that these properties can nonetheless emerge from a material thing. The project of providing such an explanation is often referred to as the \"naturalization of the mental\". Some of the crucial problems that this project attempts to resolve include the existence of qualia and the nature of intentionality.\n\nMany mental states seem to be experienced subjectively in different ways by different individuals. And it is characteristic of a mental state that it has some experiential \"quality\", e.g. of pain, that it hurts. However, the sensation of pain between two individuals may not be identical, since no one has a perfect way to measure how much something hurts or of describing exactly how it feels to hurt. Philosophers and scientists therefore ask where these experiences come from. The existence of cerebral events, in and of themselves, cannot explain why they are accompanied by these corresponding qualitative experiences. The puzzle of why many cerebral processes occur with an accompanying experiential aspect in consciousness seems impossible to explain.\n\nYet it also seems to many that science will eventually have to explain such experiences. This follows from an assumption about the possibility of reductive explanations. According to this view, if an attempt can be successfully made to explain a phenomenon reductively (e.g., water), then it can be explained why the phenomenon has all of its properties (e.g., fluidity, transparency). In the case of mental states, this means that there needs to be an explanation of why they have the property of being experienced in a certain way.\n\nThe 20th-century German philosopher Martin Heidegger criticized the ontological assumptions underpinning such a reductive model, and claimed that it was impossible to make sense of experience in these terms. This is because, according to Heidegger, the nature of our subjective experience and its \"qualities\" is impossible to understand in terms of Cartesian \"substances\" that bear \"properties\". Another way to put this is that the very concept of qualitative experience is incoherent in terms of—or is semantically incommensurable with the concept of—substances that bear properties.\n\nThis problem of explaining introspective first-person aspects of mental states and consciousness in general in terms of third-person quantitative neuroscience is called the explanatory gap. There are several different views of the nature of this gap among contemporary philosophers of mind. David Chalmers and the early Frank Jackson interpret the gap as ontological in nature; that is, they maintain that qualia can never be explained by science because physicalism is false. There are two separate categories involved and one cannot be reduced to the other. An alternative view is taken by philosophers such as Thomas Nagel and Colin McGinn. According to them, the gap is epistemological in nature. For Nagel, science is not yet able to explain subjective experience because it has not yet arrived at the level or kind of knowledge that is required. We are not even able to formulate the problem coherently. For McGinn, on other hand, the problem is one of permanent and inherent biological limitations. We are not able to resolve the explanatory gap because the realm of subjective experiences is cognitively closed to us in the same manner that quantum physics is cognitively closed to elephants. Other philosophers liquidate the gap as purely a semantic problem. This semantic problem, of course, led to the famous \"Qualia Question\", which is: \"Does Red cause Redness\"?\n\nIntentionality is the capacity of mental states to be directed towards (\"about\") or be in relation with something in the external world. This property of mental states entails that they have contents and semantic referents and can therefore be assigned truth values. When one tries to reduce these states to natural processes there arises a problem: natural processes are not true or false, they simply happen. It would not make any sense to say that a natural process is true or false. But mental ideas or judgments are true or false, so how then can mental states (ideas or judgments) be natural processes? The possibility of assigning semantic value to ideas must mean that such ideas are about facts. Thus, for example, the idea that Herodotus was a historian refers to Herodotus and to the fact that he was a historian. If the fact is true, then the idea is true; otherwise, it is false. But where does this relation come from? In the brain, there are only electrochemical processes and these seem not to have anything to do with Herodotus.\n\nPhilosophy of perception is concerned with the nature of perceptual experience and the status of perceptual objects, in particular how perceptual experience relates to appearances and beliefs about the world. The main contemporary views within philosophy of perception include naive realism, enactivism and representational views.\n\nHumans are corporeal beings and, as such, they are subject to examination and description by the natural sciences. Since mental processes are intimately related to bodily processes, the descriptions that the natural sciences furnish of human beings play an important role in the philosophy of mind. There are many scientific disciplines that study processes related to the mental. The list of such sciences includes: biology, computer science, cognitive science, cybernetics, linguistics, medicine, pharmacology, and psychology.\n\nThe theoretical background of biology, as is the case with modern natural sciences in general, is fundamentally materialistic. The objects of study are, in the first place, physical processes, which are considered to be the foundations of mental activity and behavior. The increasing success of biology in the explanation of mental phenomena can be seen by the absence of any empirical refutation of its fundamental presupposition: \"there can be no change in the mental states of a person without a change in brain states.\"\n\nWithin the field of neurobiology, there are many subdisciplines that are concerned with the relations between mental and physical states and processes: Sensory neurophysiology investigates the relation between the processes of perception and stimulation. Cognitive neuroscience studies the correlations between mental processes and neural processes. Neuropsychology describes the dependence of mental faculties on specific anatomical regions of the brain. Lastly, evolutionary biology studies the origins and development of the human nervous system and, in as much as this is the basis of the mind, also describes the ontogenetic and phylogenetic development of mental phenomena beginning from their most primitive stages. Evolutionary biology furthermore places tight constraints on any philosophical theory of the mind, as the gene-based mechanism of natural selection does not allow any giant leaps in the development of neural complexity or neural software but only incremental steps over long time periods.\n\nThe methodological breakthroughs of the neurosciences, in particular the introduction of high-tech neuroimaging procedures, has propelled scientists toward the elaboration of increasingly ambitious research programs: one of the main goals is to describe and comprehend the neural processes which correspond to mental functions (see: neural correlate). Several groups are inspired by these advances.\n\nComputer science concerns itself with the automatic processing of information (or at least with physical systems of symbols to which information is assigned) by means of such things as computers. From the beginning, computer programmers have been able to develop programs that permit computers to carry out tasks for which organic beings need a mind. A simple example is multiplication. It is not clear whether computers could be said to have a mind. Could they, someday, come to have what we call a mind? This question has been propelled into the forefront of much philosophical debate because of investigations in the field of artificial intelligence (AI).\n\nWithin AI, it is common to distinguish between a modest research program and a more ambitious one: this distinction was coined by John Searle in terms of a weak AI and strong AI. The exclusive objective of \"weak AI\", according to Searle, is the successful simulation of mental states, with no attempt to make computers become conscious or aware, etc. The objective of strong AI, on the contrary, is a computer with consciousness similar to that of human beings. The program of strong AI goes back to one of the pioneers of computation Alan Turing. As an answer to the question \"Can computers think?\", he formulated the famous Turing test. Turing believed that a computer could be said to \"think\" when, if placed in a room by itself next to another room that contained a human being and with the same questions being asked of both the computer and the human being by a third party human being, the computer's responses turned out to be indistinguishable from those of the human. Essentially, Turing's view of machine intelligence followed the behaviourist model of the mind—intelligence is as intelligence does. The Turing test has received many criticisms, among which the most famous is probably the Chinese room thought experiment formulated by Searle.\n\nThe question about the possible sensitivity (qualia) of computers or robots still remains open. Some computer scientists believe that the specialty of AI can still make new contributions to the resolution of the \"mind–body problem\". They suggest that based on the reciprocal influences between software and hardware that takes place in all computers, it is possible that someday theories can be discovered that help us to understand the reciprocal influences between the human mind and the brain (wetware).\n\nPsychology is the science that investigates mental states directly. It uses generally empirical methods to investigate concrete mental states like joy, fear or obsessions. Psychology investigates the laws that bind these mental states to each other or with inputs and outputs to the human organism.\n\nAn example of this is the psychology of perception. Scientists working in this field have discovered general principles of the perception of forms. A law of the psychology of forms says that objects that move in the same direction are perceived as related to each other. This law describes a relation between visual input and mental perceptual states. However, it does not suggest anything about the nature of perceptual states. The laws discovered by psychology are compatible with all the answers to the mind–body problem already described.\n\nCognitive science is the interdisciplinary scientific study of the mind and its processes. It examines what cognition is, what it does, and how it works. It includes research on intelligence and behavior, especially focusing on how information is represented, processed, and transformed (in faculties such as perception, language, memory, reasoning, and emotion) within nervous systems (human or other animal) and machines (e.g. computers). Cognitive science consists of multiple research disciplines, including psychology, artificial intelligence, philosophy, neuroscience, linguistics, anthropology, sociology, and education. It spans many levels of analysis, from low-level learning and decision mechanisms to high-level logic and planning; from neural circuitry to modular brain organisation. Rowlands argues that cognition is enactive, embodied, embedded, affective and (potentially) extended. The position is taken that the \"classical sandwich\" of cognition sandwiched between perception and action is artificial; cognition has to be seen as a product of a strongly coupled interaction that cannot be divided this way.\n\nMost of the discussion in this article has focused on one style or tradition of philosophy in modern Western culture, usually called analytic philosophy (sometimes described as Anglo-American philosophy). Many other schools of thought exist, however, which are sometimes subsumed under the broad (and vague) label of continental philosophy. In any case, though topics and methods here are numerous, in relation to the philosophy of mind the various schools that fall under this label (phenomenology, existentialism, etc.) can globally be seen to differ from the analytic school in that they focus less on language and logical analysis alone but also take in other forms of understanding human existence and experience. With reference specifically to the discussion of the mind, this tends to translate into attempts to grasp the concepts of thought and perceptual experience in some sense that does not merely involve the analysis of linguistic forms.\n\nImmanuel Kant's \"Critique of Pure Reason\", first published in 1781 and presented again with major revisions in 1787, represents a significant intervention into what will later become known as the philosophy of mind. Kant's first critique is generally recognized as among the most significant works of modern philosophy in the West. Kant is a figure whose influence is marked in both continental and analytic/Anglo-American philosophy. Kant's work develops an in-depth study of transcendental consciousness, or the life of the mind as conceived through universal categories of consciousness.\n\nIn Georg Wilhelm Friedrich Hegel's \"Philosophy of Mind\" (frequently translated as \"Philosophy of Spirit\" or Geist), the third part of his \"Encyclopedia of the Philosophical Sciences\", Hegel discusses three distinct types of mind: the \"subjective mind/spirit\", the mind of an individual; the \"objective mind/spirit\", the mind of society and of the State; and the \"Absolute mind/spirit\", the position of religion, art, and philosophy. See also Hegel's \"The Phenomenology of Spirit\". Nonetheless, Hegel's work differs radically from the style of Anglo-American philosophy of mind.\n\nIn 1896, Henri Bergson made in \"Matter and Memory\" \"Essay on the relation of body and spirit\" a forceful case for the ontological difference of body and mind by reducing the problem to the more definite one of memory, thus allowing for a solution built on the \"empirical test case\" of aphasia.\n\nIn modern times, the two main schools that have developed in response or opposition to this Hegelian tradition are phenomenology and existentialism. Phenomenology, founded by Edmund Husserl, focuses on the contents of the human mind (see noema) and how processes shape our experiences. Existentialism, a school of thought founded upon the work of Søren Kierkegaard, focuses on Human predicament and how people deal with the situation of being alive. Existential-phenomenology represents a major branch of continental philosophy (they are not contradictory), rooted in the work of Husserl but expressed in its fullest forms in the work of Martin Heidegger, Jean-Paul Sartre, Simone de Beauvoir and Maurice Merleau-Ponty. See Heidegger's \"Being and Time\", Merleau-Ponty's \"Phenomenology of Perception\", Sartre's \"Being and Nothingness\", and Simone de Beauvoir's \"The Second Sex\".\n\nSubstance Dualism is a common feature of several orthodox Hindu schools including the Sāṅkhya, Nyāya, Yoga and Dvaita Vedanta. In these schools a clear difference is drawn between matter and a non-material soul, which is eternal and undergoes samsara, a cycle of death and rebirth. The Nyāya school argued that qualities such as cognition and desire are inherent qualities which are not possessed by anything solely material, and therefore by process of elimination must belong to a non-material self, the atman. Many of these schools see their spiritual goal as moksha, liberation from the cycle of reincarnation.\n\nIn the Advaita Vedanta of the 8th century Indian philosopher Śaṅkara, the mind, body and world are all held to be the same unchanging eternal conscious entity called Brahman. Advaita, which means non-dualism, holds the view that all that exists is pure absolute consciousness. The fact that the world seems to be made up of changing entities is an illusion, or Maya. The only thing that exists is Brahman, which is described as Satchitananda (Being, consciousness and bliss). Advaita Vedanta is best described by a verse which states \"Brahman is alone True, and this world of plurality is an error; the individual self is not different from Brahman.\"\n\nAnother form of monistic Vedanta is Vishishtadvaita (qualified non-dualism) as posited by the eleventh century philosopher Ramanuja. Ramanuja criticized Advaita Vedanta by arguing that consciousness is always intentional and that it is also always a property of something. Ramanuja's Brahman is defined by a multiplicity of qualities and properties in a single monistic entity. This doctrine is called \"samanadhikaranya\" (several things in a common substrate).\n\nArguably the first exposition of empirical materialism in the history of philosophy is in the Cārvāka school (also called Lokāyata). The Cārvāka school rejected the existence of anything but matter (which they defined as being made up of the four elements), including God and the soul. Therefore, they held that even consciousness was nothing but a construct made up of atoms. A section of the Cārvāka school believed in a material soul made up of air or breath, but since this also was a form of matter, it was not said to survive death.\n\nBuddhist teachings describe that the mind manifests moment-to-moment as sense impressions and mental phenomena that are continuously changing. The moment-by-moment manifestation of the mind-stream has been described as happening in every person all the time, even in a scientist who analyses various phenomena in the world, or analyses the material body including the organ brain. The manifestation of the mind-stream is also described as being influenced by physical laws, biological laws, psychological laws, volitional laws, and universal laws.\n\nA salient feature of Buddhist philosophy which sets it apart from Indian orthodoxy is the centrality of the doctrine of not-self (Pāli. anatta, Skt. anātman). The Buddha's not-self doctrine sees humans as an impermanent composite of five psychological and physical aspects instead of a single fixed self. In this sense, what is called ego or the self is merely a convenient fiction, an illusion that does not apply to anything real but to an erroneous way of looking at the ever-changing stream of five interconnected aggregate factors. The relationship between these aggregates is said to be one of dependent-arising (pratītyasamutpāda). This means that all things, including mental events, arise co-dependently from a plurality of other causes and conditions. This seems to reject both causal determinist and epiphenomenalist conceptions of mind.\n\nThree centuries after the death of the Buddha (c. 150 BCE) saw the growth of a large body of literature called the Abhidharma in several contending Buddhist schools. In the Abhidharmic analysis of mind, the ordinary thought is defined as prapañca ('conceptual proliferation'). According to this theory, perceptual experience is bound up in multiple conceptualizations (expectations, judgments and desires). This proliferation of conceptualizations form our illusory superimposition of concepts like self and other upon an ever-changing stream of aggregate phenomena.\nIn this conception of mind no strict distinction is made between the conscious faculty and the actual sense perception of various phenomena. Consciousness is instead said to be divided into six sense modalities, five for the five senses and sixth for perception of mental phenomena. The arising of cognitive awareness is said to depend on sense perception, awareness of the mental faculty itself which is termed mental or 'introspective awareness' (\"manovijñāna\") and attention (\"āvartana\"), the picking out of objects out of the constantly changing stream of sensory impressions.\n\nRejection of a permanent agent eventually led to the philosophical problems of the seeming continuity of mind and also of explaining how rebirth and karma continue to be relevant doctrines without an eternal mind. This challenge was met by the Theravāda school by introducing the concept of mind as a factor of existence. This \"life-stream\" (Bhavanga-sota) is an undercurrent forming the condition of being. The continuity of a karmic \"person\" is therefore assured in the form of a mindstream (citta-santana), a series of flowing mental moments arising from the subliminal life-continuum mind (Bhavanga-citta), mental content, and attention.\n\nThe Sautrāntika school held a form of phenomenalism that saw the world as imperceptible. It held that external objects exist only as a support for cognition, which can only apprehend mental representations. This influenced the later Yogācāra school of Mahayana Buddhism. The Yogācāra school is often called the mind-only school because of its internalist stance that consciousness is the ultimate existing reality. The works of Vasubandhu have often been interpreted as arguing for some form of Idealism. Vasubandhu uses the dream argument and a mereological refutation of atomism to attack the reality of external objects as anything other than mental entities. Scholarly interpretations of Vasubandhu's philosophy vary widely, and include phenomenalism, neutral monism and realist phenomenology.\n\nThe Indian Mahayana schools were divided on the issue of the possibility of reflexive awareness (\"svasaṃvedana\"). Dharmakīrti accepted the idea of reflexive awareness as expounded by the Yogācāra school, comparing it to a lamp that illuminates itself while also illuminating other objects. This was strictly rejected by Mādhyamika scholars like Candrakīrti. Since in the philosophy of the Mādhyamika all things and mental events are characterized by emptiness, they argued that consciousness could not be an inherently reflexive ultimate reality since that would mean it was self-validating and therefore not characterized by emptiness. These views were ultimately reconciled by the 8th century thinker Śāntarakṣita. In Śāntarakṣita's synthesis he adopts the idealist Yogācāra views of reflexive awareness as a conventional truth into the structure of the two truths doctrine. Thus he states: \"By relying on the Mind-Only system, know that external entities do not exist. And by relying on this Middle Way system, know that no self exists at all, even in that [mind].\" \n\nThe Yogācāra school also developed the theory of the repository consciousness (\"ālayavijñāna\") to explain continuity of mind in rebirth and accumulation of karma. This repository consciousness acts as a storehouse for karmic seeds (bija) when all other senses are absent during the process of death and rebirth as well as being the causal potentiality of dharmic phenomena. Thus according to B. Alan Wallace: \nNo constituents of the body—in the brain or elsewhere—transform into mental states and processes. Such subjective experiences do not emerge from the body, but neither do they emerge from nothing. Rather, all objective mental appearances arise from the substrate, and all subjective mental states and processes arise from the substrate consciousness.\n\nTibetan Buddhist theories of mind evolved directly from the Indian Mahayana views. Thus the founder of the Gelug school, Je Tsongkhapa discusses the Yogācāra system of the Eight Consciousnesses in his \"Explanation of the Difficult Points\". He would later come to repudiate Śāntarakṣita's pragmatic idealism. \nAccording to the 14th Dalai Lama the mind can be defined \"as an entity that has the nature of mere experience, that is, 'clarity and knowing'. It is the knowing nature, or agency, that is called mind, and this is non-material.\" The simultaneously dual nature of mind is as follows:\nThe 14th Dalai Lama has also explicitly laid out his theory of mind as experiential dualism which is described above under the different types of dualism.\n\nBecause Tibetan philosophy of mind is ultimately soteriological, it focuses on meditative practices such as Dzogchen and Mahamudra that allow a practitioner to experience the true reflexive nature of their mind directly. This unobstructed knowledge of one's primordial, empty and non-dual Buddha nature is called rigpa. The mind's innermost nature is described among various schools as pure luminosity or \"clear light\" ('od gsal) and is often compared to a crystal ball or a mirror. Sogyal Rinpoche speaks of mind thus:\n\"Imagine a sky, empty, spacious, and pure from the beginning; its essence is like this. Imagine a sun, luminous, clear, unobstructed, and spontaneously present; its nature is like this.\"\n\nThe central issue in Chinese Zen philosophy of mind is in the difference between the pure and awakened mind and the defiled mind. Chinese Chan master Huangpo described the mind as without beginning and without form or limit while the defiled mind was that which was obscured by attachment to form and concepts. The pure Buddha-mind is thus able to see things \"as they truly are\", as absolute and non-dual \"thusness\" (Tathatā). This non-conceptual seeing also includes the paradoxical fact that there is no difference between a defiled and a pure mind, as well as no difference between samsara and nirvana.\n\nIn the Shobogenzo, the Japanese philosopher Dogen argued that body and mind are neither ontologically nor phenomenologically distinct but are characterized by a oneness called \"shin jin\" (bodymind). According to Dogen, \"casting off body and mind\" (\"Shinjin datsuraku\") in zazen will allow one to experience things-as-they-are (\"genjokoan\") which is the nature of original enlightenment (\"hongaku\").\n\nThere are countless subjects that are affected by the ideas developed in the philosophy of mind. Clear examples of this are the nature of death and its definitive character, the nature of emotion, of perception and of memory. Questions about what a person is and what his or her identity consists of also have much to do with the philosophy of mind. There are two subjects that, in connection with the philosophy of the mind, have aroused special attention: free will and the self.\n\nIn the context of philosophy of mind, the problem of free will takes on renewed intensity. This is certainly the case, at least, for materialistic determinists. According to this position, natural laws completely determine the course of the material world. Mental states, and therefore the will as well, would be material states, which means human behavior and decisions would be completely determined by natural laws. Some take this reasoning a step further: people cannot determine by themselves what they want and what they do. Consequently, they are not free.\n\nThis argumentation is rejected, on the one hand, by the compatibilists. Those who adopt this position suggest that the question \"Are we free?\" can only be answered once we have determined what the term \"free\" means. The opposite of \"free\" is not \"caused\" but \"compelled\" or \"coerced\". It is not appropriate to identify freedom with indetermination. A free act is one where the agent could have done otherwise if it had chosen otherwise. In this sense a person can be free even though determinism is true. The most important compatibilist in the history of the philosophy was David Hume. More recently, this position is defended, for example, by Daniel Dennett.\n\nOn the other hand, there are also many incompatibilists who reject the argument because they believe that the will is free in a stronger sense called libertarianism. These philosophers affirm the course of the world is either a) not completely determined by natural law where natural law is intercepted by physically independent agency, b) determined by indeterministic natural law only, or c) determined by indeterministic natural law in line with the subjective effort of physically non-reducible agency. Under Libertarianism, the will does not have to be deterministic and, therefore, it is potentially free. Critics of the second proposition (b) accuse the incompatibilists of using an incoherent concept of freedom. They argue as follows: if our will is not determined by anything, then we desire what we desire by pure chance. And if what we desire is purely accidental, we are not free. So if our will is not determined by anything, we are not free.\n\nThe philosophy of mind also has important consequences for the concept of \"self\". If by \"self\" or \"I\" one refers to an essential, immutable nucleus of the \"person\", some modern philosophers of mind, such as Daniel Dennett believe that no such thing exists. According to Dennett and other contemporaries, the self is considered an illusion. The idea of a self as an immutable essential nucleus derives from the idea of an immaterial soul. Such an idea is unacceptable to modern philosophers with physicalist orientations and their general skepticism of the concept of \"self\" as postulated by David Hume, who could never catch himself \"not\" doing, thinking or feeling anything. However, in the light of empirical results from developmental psychology, developmental biology and neuroscience, the idea of an essential inconstant, material nucleus—an integrated representational system distributed over changing patterns of synaptic connections—seems reasonable.\n\n"}
{"id": "39098", "url": "https://en.wikipedia.org/wiki?curid=39098", "title": "Physical law", "text": "Physical law\n\nA physical law or a law of physics is a statement \"inferred from particular facts, applicable to a defined group or class of phenomena, and expressible by the statement that a particular phenomenon always occurs if certain conditions be present.\" Physical laws are typically conclusions based on repeated scientific experiments and observations over many years and which have become accepted universally within the scientific community. The production of a summary description of our environment in the form of such laws is a fundamental aim of science. These terms are not used the same way by all authors.\n\nThe distinction between natural law in the political-legal sense and law of nature or physical law in the scientific sense is a modern one, both concepts being equally derived from \"physis\", the Greek word (translated into Latin as \"natura\") for \"nature\".\n\nSeveral general properties of physical laws have been identified. Physical laws are:\n\n\nSome of the more famous laws of nature are found in Isaac Newton's theories of (now) classical mechanics, presented in his \"Philosophiae Naturalis Principia Mathematica\", and in Albert Einstein's theory of relativity. Other examples of laws of nature include Boyle's law of gases, conservation laws, the four laws of thermodynamics, etc.\n\nMany scientific laws are couched in mathematical terms (e.g. Newton's Second law \"F\" = , or the uncertainty principle, or the principle of least action, or causality). While these scientific laws explain what our senses perceive, they are still empirical, and so are not \"mathematical\" laws. (Mathematical laws can be proved purely by mathematics and not by scientific experiment.)\n\nOther laws reflect mathematical symmetries found in Nature (say, Pauli exclusion principle reflects identity of electrons, conservation laws reflect homogeneity of space, time, Lorentz transformations reflect rotational symmetry of space–time). Laws are constantly being checked experimentally to higher and higher degrees of precision. This is one of the main goals of science. Just because laws have never been observed to be violated does not preclude testing them at increased accuracy or in new kinds of conditions to confirm whether they continue to hold, or whether they break, and what can be discovered in the process. It is always possible for laws to be invalidated or proven to have limitations, by repeatable experimental evidence, should any be observed.\n\nWell-established laws have indeed been invalidated in some special cases, but the new formulations created to explain the discrepancies can be said to generalize upon, rather than overthrow, the originals. That is, the invalidated laws have been found to be only close approximations (see below), to which other terms or factors must be added to cover previously unaccounted-for conditions, e.g. very large or very small scales of time or space, enormous speeds or masses, etc. Thus, rather than unchanging knowledge, physical laws are better viewed as a series of improving and more precise generalizations.\n\nMany fundamental physical laws are mathematical consequences of various symmetries of space, time, or other aspects of nature. Specifically, Noether's theorem connects some conservation laws to certain symmetries. For example, conservation of energy is a consequence of the shift symmetry of time (no moment of time is different from any other), while conservation of momentum is a consequence of the symmetry (homogeneity) of space (no place in space is special, or different than any other). The indistinguishability of all particles of each fundamental type (say, electrons, or photons) results in the Dirac and Bose quantum statistics which in turn result in the Pauli exclusion principle for fermions and in Bose–Einstein condensation for bosons. The rotational symmetry between time and space coordinate axes (when one is taken as imaginary, another as real) results in Lorentz transformations which in turn result in special relativity theory. Symmetry between inertial and gravitational mass results in general relativity.\n\nThe inverse square law of interactions mediated by massless bosons is the mathematical consequence of the 3-dimensionality of space.\n\nOne strategy in the search for the most fundamental laws of nature is to search for the most general mathematical symmetry group that can be applied to the fundamental interactions.\n\nSome laws are only approximations of other more general laws, and are good approximations with a restricted domain of applicability. For example, Newtonian dynamics (which is based on Galilean transformations) is the low-speed limit of special relativity (since the Galilean transformation is the low-speed approximation to the Lorentz transformation). Similarly, the Newtonian gravitation law is a low-mass approximation of general relativity, and Coulomb's law is an approximation to Quantum Electrodynamics at large distances (compared to the range of weak interactions). In such cases it is common to use the simpler, approximate versions of the laws, instead of the more accurate general laws.\n\nCompared to pre-modern accounts of causality, laws of nature fill the role played by divine causality on the one hand, and accounts such as Plato's theory of forms on the other.\n\nThe observation that there are underlying regularities in nature dates from prehistoric times, since the recognition of cause-and-effect relationships is an implicit recognition that there are laws of nature. The recognition of such regularities as independent scientific laws \"per se\", though, was limited by their entanglement in animism, and by the attribution of many effects that do not have readily obvious causes—such as meteorological, astronomical and biological phenomena—to the actions of various gods, spirits, supernatural beings, etc. Observation and speculation about nature were intimately bound up with metaphysics and morality.\n\nIn Europe, systematic theorizing about nature (\"physis\") began with the early Greek philosophers and scientists and continued into the Hellenistic and Roman imperial periods, during which times the intellectual influence of Roman law increasingly became paramount.The formula \"law of nature\" first appears as \"a live metaphor\" favored by Latin poets Lucretius, Virgil, Ovid, Manilius, in time gaining a firm theoretical presence in the prose treatises of Seneca and Pliny. Why this Roman origin? According to [historian and classicist Daryn] Lehoux's persuasive narrative, the idea was made possible by the pivotal role of codified law and forensic argument in Roman life and culture.\n\nFor the Romans . . . the place par excellence where ethics, law, nature, religion and politics overlap is the law court. When we read Seneca's \"Natural Questions\", and watch again and again just how he applies standards of evidence, witness evaluation, argument and proof, we can recognize that we are reading one of the great Roman rhetoricians of the age, thoroughly immersed in forensic method. And not Seneca alone. Legal models of scientific judgment turn up all over the place, and for example prove equally integral to Ptolemy's approach to verification, where the mind is assigned the role of magistrate, the senses that of disclosure of evidence, and dialectical reason that of the law itself.\n\nThe precise formulation of what are now recognized as modern and valid statements of the laws of nature dates from the 17th century in Europe, with the beginning of accurate experimentation and development of advanced forms of mathematics. During this period, natural philosophers such as Isaac Newton were influenced by a religious view which held that God had instituted absolute, universal and immutable physical laws. In chapter 7 of \"The World\", René Descartes described \"nature\" as matter itself, unchanging as created by God, thus changes in parts \"are to be attributed to nature. The rules according to which these changes take place I call the 'laws of nature'.\" The modern scientific method which took shape at this time (with Francis Bacon and Galileo) aimed at total separation of science from theology, with minimal speculation about metaphysics and ethics. Natural law in the political sense, conceived as universal (i.e., divorced from sectarian religion and accidents of place), was also elaborated in this period (by Grotius, Spinoza, and Hobbes, to name a few).\n\nSome mathematical theorems and axioms are referred to as laws because they provide logical foundation to empirical laws.\n\nExamples of other observed phenomena sometimes described as laws include the Titius–Bode law of planetary positions, Zipf's law of linguistics, Moore's law of technological growth. Many of these laws fall within the scope of uncomfortable science. Other laws are pragmatic and observational, such as the law of unintended consequences. By analogy, principles in other fields of study are sometimes loosely referred to as \"laws\". These include Occam's razor as a principle of philosophy and the Pareto principle of economics.\n\n\n\n"}
{"id": "40600057", "url": "https://en.wikipedia.org/wiki?curid=40600057", "title": "Planck's principle", "text": "Planck's principle\n\nIn sociology of scientific knowledge, Planck's principle is the view that scientific change does not occur because individual scientists change their mind, but rather that successive generations of scientists have different views.\n\nThe reason for the name is the statements by Max Planck:\n\nPlanck's quote has been used by Thomas Kuhn, Paul Feyerabend and others to argue that scientific revolutions are arational, rather than spreading through \"mere force of truth and fact\". It has been described as Darwinian rather than Lamarckian conceptual evolution.\n\nWhether age influences the readiness to accept new ideas has been empirically criticised. In the case of acceptance of evolution in the years after Darwin's \"On the Origin of Species\" age was a minor factor. Similarly, it was a weak factor in accepting cliometrics.\n"}
{"id": "30226192", "url": "https://en.wikipedia.org/wiki?curid=30226192", "title": "Polar concept argument", "text": "Polar concept argument\n\nA polar concept argument is a type of argument that posits the understanding of one concept, from the mere understanding of its polar opposite. A well-known instance of a polar concept argument is Gilbert Ryle's argument against scepticism (1960). According to Anthony Grayling's characterisation, Ryle's argument can be stated as follows:\n\nAccording to Ryle's polar concept argument, counterfeit and genuine coins come in pairs, and one cannot conceive of counterfeit coins without also capturing the essence of the genuine coins at the same time. When one grasps the essence of one polar concept, one also grasps immediately the essence of its polar opposite. Ryle's original argument (1960) runs as follows:\n\nA polar concept argument bears on some more or less strong version of dialectical monism, a philosophical doctrine that views reality as a unified whole, due to the complementarity of polar concepts.\n\n"}
{"id": "1005874", "url": "https://en.wikipedia.org/wiki?curid=1005874", "title": "Principle", "text": "Principle\n\nA principle is a concept or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 \"Principles of Operation\".\n\nExamples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.\n\nIn common English, it is a substantive and collective term referring to rule governance, the absence of which, being \"unprincipled\", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only \"in principle\" but not in fact.\n\nA principle represents values that orient and rule the conduct of persons in a particular society. To \"act on principle\" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.\n\nIt represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.\n\nArchimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the \"population principle\", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.\n\nThe principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\nHowever, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that \"the Earth is round\", corresponds to a direct relation between the subject and the predicate.\n\nAccording to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.\n\nThe principle of the excluding third or \"principium tertium exclusum\" is a principle of the traditional logic formulated canonically by Leibniz as: either \"A\" is \"B\" or \"A\" isn't \"B\". It is read the following way: either \"P\" is true, or its denial ¬\"P\" is.\nIt is also known as \"tertium non datur\" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).\n"}
{"id": "31014293", "url": "https://en.wikipedia.org/wiki?curid=31014293", "title": "Principle of transformation groups", "text": "Principle of transformation groups\n\nThe principle of transformation groups is a rule for assigning \"epistemic\" probabilities in a statistical inference problem. It was first suggested by Edwin T. Jaynes and can be seen as a generalisation of the principle of indifference.\n\nThis can be seen as a method to create \"objective ignorance probabilities\" in the sense that two people who apply the principle and are confronted with the same information will assign the same probabilities.\n\nThe method is motivated by the following normative principle, or desideratum:\n\n\"In two problems where we have the same prior information we should assign the same prior probabilities\"\n\nThe method then comes about from \"transforming\" a given problem into an equivalent one. This method has close connections with group theory, and to a large extent is about finding symmetry in a given problem, and then exploiting this symmetry to assign prior probabilities.\n\nIn problems with discrete variables (e.g. dice, cards, categorical data) the principle reduces to the principle of indifference, as the \"symmetry\" in the discrete case is a permutation of the labels, that is the permutation group is the relevant transformation group for this problem.\n\nIn problems with continuous variables, this method generally reduces to solving a differential equation. Given that differential equations do not always lead to unique solutions, this method cannot be guaranteed to produce a unique solution. However, in a large class of the most common types of parameters it does lead to unique solutions (see the examples below)\n\nConsider a problem where all you are told is that there is a coin, and it has a head (H) and a tail (T). Denote this information by \"I\". You are then asked \"what is the probability of Heads?\". Call this \"problem 1\" and denote the probability \"P(H|I)\". Consider another question \"what is the probability of Tails?\". Call this \"problem 2\" and denote this probability by \"P(T|I)\".\n\nNow from the information which was actually in the question, there is no distinction between heads and tails. The whole paragraph above could be re-written with \"Heads\" and \"Tails\" interchanged, and \"H\" and \"T\" interchanged, and the problem statement would not be any different. Using the desideratum then demands that\n\nformula_1\n\nThe probabilities must add to 1, this means that\n\nformula_2.\n\nThus we have a unique solution. This argument easily extents to \"N\" categories, to give the \"flat\" prior probability \"1/N\".\nThis provides a \"consistency\" based argument to the principle of indifference which goes as follows: \"if someone is truly ignorant about a discrete/countable set of outcomes apart from their potential existence, but does not assign them equal prior probabilities, then they are assigning different probabilities when given the same information\".\n\nThis can be alternatively phrased as: \"a person who does not use the principle of indifference to assign prior probabilities to discrete variables, is either not ignorant about them, or reasoning inconsistently\".\n\nThis is the easiest example for continuous variables. It is given by stating one is \"ignorant\" of the location parameter in a given problem. The statement that a parameter is a \"location parameter\" is that the sampling distribution, or likelihood of an observation \"X\" depends on a parameter formula_3 only through the difference\n\nformula_4\n\nfor some normalised, but otherwise arbitrary distribution \"f(.)\". Examples of location parameters include mean parameter of normal distribution with known variance and median parameter of Cauchy distribution with known inter-quartile range.\nThe two \"equivalent problems\" in this case, given ones knowledge of the sampling distribution formula_4, but no other knowledge about formula_3, is simply given by a \"shift\" of equal magnitude in \"X\" and formula_3. This is because of the relation:\n\nformula_8\n\nSo simply \"shifting\" all quantities up by some number \"b\" and solving in the \"shifted space\" and then \"shifting\" back to the original one should give exactly the same answer as if we just worked on the original space. Making the transformation from formula_3 to formula_10 has a Jacobian of simply 1, and so the prior probability formula_11 must satisfy the functional equation:\n\nformula_12\n\nAnd the only function which satisfies this equation is the \"constant prior\":\n\nformula_13\n\nThus the uniform prior is justified for expressing complete ignorance of a location parameter.\n\nAs in the above argument, a statement that formula_14 is a scale parameter means that the sampling distribution has the functional form:\n\nformula_15\n\nWhere, as before \"f(.)\" is a normalised probability density function. The requirement that probabilities be finite and positive forces the condition formula_16. Examples include the standard deviation of a normal distribution with known mean, the gamma distribution. The \"symmetry\" in this problem is found by noting that\n\nformula_17\n\nBut, unlike in the location parameter case, the Jacobian of this transformation in the sample space and the parameter space is \"a\", not 1. so the sampling probability changes to:\n\nformula_18\n\nWhich is invariant (i.e. has the same form before and after the transformation), and the prior probability changes to:\n\nformula_19\n\nWhich has the unique solution (up to a proportionality constant):\n\nformula_20\n\nWhich is the well-known Jeffreys prior for scale parameters, which is \"flat\" on the log scale, although it should be noted that it is derived using a different argument to that here, based on the Fisher information function. The fact that these two methods give the same results in this case does not imply it in general.\n\nEdwin Jaynes used this principle to provide a resolution to Bertrand's Paradox\nby stating his ignorance about the exact position of the circle. The details are available in the reference or in the link.\n\nThis argument depends crucially on \"I\"; changing the information may result in a different probability assignment. It is just as crucial as changing axioms in deductive logic - small changes in the information can lead to large changes in the probability assignments allowed by \"consistent reasoning\".\n\nTo illustrate suppose that the coin flipping example also states as part of the information that the coin has a side (S) (i.e. it is a \"real coin\"). Denote this new information by \"N\". The same argument using \"complete ignorance\", or more precisely, the information actually described, gives:\n\nformula_21\n\nBut this seems absurd to most people - intuition tells us that we should have P(S) very close to zero. This is because most people's intuition do not see \"symmetry\" between a coin landing on its side compared to landing on heads. Our intuition says that the particular \"labels\" actually carry some information about the problem. A simple argument could be used to make this more formal mathematically (e.g. the physics of the problem make it difficult for a flipped coin to land on its side) - we make a distinction between \"thick\" coins and \"thin\" coins [here thickness is measured relative to the coin's diameter]. It could reasonably be assumed that:\n\nformula_22\n\nNote that this new information probably wouldn't break the symmetry between \"heads\" and \"tails\", so \"that\" permutation would still apply in describing \"equivalent problems\", and we would require:\n\nformula_23\n\nThis is a good example of how the principle of transformation groups can be used to \"flesh out\" personal opinions. All of the information used in the derivation is explicitly stated. If a prior probability assignment doesn't \"seem right\" according to what your intuition tells you, then there must be some \"background information\" which has not been put into the problem. It is then the task to try and work out what that information is. In some sense, by combining the method of transformation groups with one's intuition can be used to \"weed out\" the actual assumptions one has. This makes it a very powerful tool for prior elicitation.\n\nIntroducing the thickness of the coin is permissible because it was not specified in the problem, so this is still only using information in the question. Introducing a \"nuisance parameter\" and then making the answer invariant to this parameter is a very useful technique for solving supposedly \"ill-posed\" problems like Bertrand's Paradox. This has been called \"the well-posing strategy\" by some.\n\nThe real power of this principle lies in its application to continuous parameters, where the notion of \"complete ignorance\" is not so well defined as in the discrete case. However, if applied with infinite limits, it often gives improper prior distributions. Note that the discrete case for a countably infinite set, such as (0,1,2...) also produces an improper discrete prior. For most cases where the likelihood is sufficiently \"steep\" this does not present a problem. However, in order to be absolutely sure to avoid incoherent results and paradoxes, the prior distribution should be approached via a well defined and well behaved limiting process. One such process is the use of a sequence of priors with increasing range, such as formula_24 where the limit formula_25 is to be taken \"at the end of the calculation\" i.e. after the normalisation of the posterior distribution. What this effectively is doing, is ensuring that one is taking the limit of the ratio, and not the ratio of two limits. See Limit of a function#Properties for details on limits and why this order of operations is important.\n\nIf the limit of the ratio does not exist or diverges, then this gives an improper posterior (i.e. a posterior which does not integrate to one). This indicates that the data are so uninformative about the parameters that the prior probability of arbitrarily large values still matters in the final answer. In some sense, an improper posterior means that the information contained in the data has not \"ruled out\" arbitrarily large values. Looking at the improper priors this way, it seems to make some sense that \"complete ignorance\" priors should be improper, because the information used to derive them is so meager that it cannot rule out absurd values on its own. From a state of complete ignorance, only the data or some other form of additional information can rule out such absurdities.\n\n"}
{"id": "39812836", "url": "https://en.wikipedia.org/wiki?curid=39812836", "title": "Process reference models", "text": "Process reference models\n\nA process reference model is a model that has generic functionality and can be used more than once in different models. The creator of a process model benefits from existing process reference models by not needing to reinvent the process model but only reusing it as a starting point in creating a process model for a specific purpose.\n\nDuring the identification of processes ideal for reuse, the designer needs to (1) Get approval (2) Provide Organization Scope Context and (3) Identify Process Standardization Opportunities.\n"}
{"id": "33920294", "url": "https://en.wikipedia.org/wiki?curid=33920294", "title": "Psychotherapy and social action model", "text": "Psychotherapy and social action model\n\nThe psychotherapy and social action model is an approach to psychotherapy characterized by concentration on past and present personal, social, and political obstacles to mental health. In particular, the goal of this therapeutic approach is to acknowledge that individual symptoms are not unique, but rather shared by people similarly oppressed and marginalized. Ultimately, the psychotherapy and social action model aims to aid clients in overcoming mental illness through personal psychotherapy, group coping, and collective social action.\n\n The psychotherapy and social action model was initially proposed by Sue Holland, a psychotherapist with a background in community action. Holland developed this framework in 1980 following her experience working with women coping with psychological disorders at a housing estate in West London. At this estate, Holland observed the psychological difficulties experienced by women, noticing that their mental health was fundamentally tied to the social and economic obstacles they encountered as females in their society. In addition, Holland took issue with the way Depression (mood) was being treated at the shelter, believing that individualized treatment, especially with the use of psychotropic medication, was not successfully addressing the root of the dysfunction for these women. Instead, Holland posited a pathway from individual treatment to sociopolitical action that empowered women to deal with their mental dysfunction both privately and socially. As such, the psychotherapy and social action model is rooted in the ideals of both traditional psychotherapy and feminist empowerment.\n\nThe square model derives from the sociological theory of the four paradigms for the analysis of social theory. Outside the frame of the model, the dichotomy of individual versus social approaches to personal well-being is represented. The two bottom cells of the square delineate the changing of individuals to conform to social convention while the two top cells of the square represent the changing of social structures as opposed to the individual.\n\nThe four cells within the frame represent the four paradigms of social theory including functionalist, interpretive, radical humanist, and radical structuralist paradigms. Functionalism here is rooted in regulation and objective thinking, and represents the individual, status-quo approach to mental health. The interpretive paradigm is characterized by an approach to understanding the social world through subjective experience, and represents psychoeducation within the psychotherapy framework. The radical humanist paradigm is defined by a radial approach to change with an emphasis on “transcending limitations of existing social arrangements.” (Burrell & Morgan, 1979, p. 32). With respect to an approach to therapy, this stage is characterized by the adoption of a social self, such that healing occurs at a group or collective level. The radical structuralist paradigm concentrates on radical change through political or economic emancipation. This is the endpoint of therapy, at which time the client is empowered to challenge sociopolitical structures that foster the conditions perpetuating the manifestation of individual mental illness within an oppressed group.\n\nTaken from her 1992 publication entitled, “From Social Abuse to Social Action: a neighborhood psychotherapy and social action project for women,” Holland formulated her four step approach to mental health and social action for women in treatment for depression as follows:\n\nAt this stage, patients endorse the status-quo characterization of the “individualized patient.” As such, they treat their disorder passively with psychotropic medication and accept the label associated with their illness.\n\nThis stage represents the first alternative to the status-quo treatment of psychiatric disorders: talk therapy. At this stage, clients and therapists are able to explore the meaning of their psychopathology and pinpoint the potential causes through individual therapy.\n\nAt this stage, the client is able to move past the personal challenges that are acknowledged and addressed in psychotherapy and discover that the challenges are universal amongst similarly marginalized individuals. Together, clients aim to acknowledge what is best for the collective.\n\nThe final stage, as the name suggests, is the point at which the collective mobilizes to change the social structures enabling their common oppression. Having changed from an individual to a collective, the clients should feel empowered to undertake social change.\n\nIncluded in this framework is the assumption that only some of the clients in this therapy will traverse all three stages. In Holland’s words, “…many will be content enough with the relief from symptoms and the freedom to get on with their personal lives which the individual therapy gives them.” (Holland, 1992, p. 73). Thus, this framework is fluid based on the personal inclinations of the client throughout the therapeutic process.\n\n• Women’s Action for Mental Health (WAMH)\n• Men’s Advice Network (MAN)\n• Travers (1997)\n"}
{"id": "1875925", "url": "https://en.wikipedia.org/wiki?curid=1875925", "title": "Pythagorean Method of Memorization", "text": "Pythagorean Method of Memorization\n\nPythagorean Method of Memorization (PYMOM), also known as Triangular Movement Cycle (TMC), is a game-based, educational methodology or associative-learning technique that primarily uses corresponding information, such as terms and definitions on opposing sides, displayed on cue cards, to exploit psychological retention of information for academic study and language acquisition. PYMOM is named such because of the shape the cue-cards form during the progression of the game, a right-angled or Pythagorean triangle.\n\nIt is a theoretical educational method that is made up of several established and tested educational methods that have been in use for decades.\n\nPYMOM is a composite body of techniques that claims, in its digital form, to incorporate (to a greater or lesser degree): spaced repetition, non-failure redundant subroutine, chromatics, positive reinforcement, the Von Restorff effect, picture association, selective musical tonality, kinesthetics, the serial-position effect and meditation. There are two branches of this methodology:\n\nAs with both branches, there is only one variable in the game or learning method: a correct or incorrect answer. The initial movement cycle also remains largely unchanged.\n\nThe movement cycle which is most crucial to the methodology and reinforces the spaced repetition, begins with either 3, 4 or 5 cards; 3 cards for a 6-card session, 4 cards for a 10-card session and 5 cards for the most advanced 15-card session. Because two-sided associative cue-cards are being used, all cards are presented with a congruent side up, either all \"terms\" or \"definitions,\" not mixed.\n\nOnce cards have been answered correctly, the predominant row has reached its maximum and a card must be graduated out of this row to continue the game. Thus the card to the far right comes into play. Routines are repeated as each row reaches its maximum. A cue-card is finally eliminated from the game session by being answered correctly once more, after it has graduated to the top tier or row.\n\nThe first manifestation, referred to as the \"Triangular Movement Cycle\" or TMC, was a simple paper-based learning technique that was primarily a manual movement cycle using physical cue-cards, which allowed for manual-spaced repetition to elicit psychological retention of information. Its origins, however, are not very clear. Using TMC, teachers would move the cards for the student in a one-on-one setting according to either the correct or incorrect feedback from the student.This presented challenges for the teacher or tutor using this method. The first challenge lies in the fact that, although TMC lent itself well to a two-party learning group (i.e. teacher & student), it could also be done by the student themselves on their own. It was a very easy system to utilize once learned, however, it was found exceptionally difficult to teach the complex movement cycle and principles behind such to students, especially where a linguistic barrier was present. The second challenge lay in that the educator needed to create and remember innumerable cue-cards or create custom master lists in order to know the correct answers — and properly guide the student, thus progressing or digressing the card in play. TMC often failed to keep the attention of many students owing to the fact that cards were not very visually appealing. To make them so required tremendous effort — and was very time consuming.\n\nThe term \"Pythagorean Method of Memorization\" was coined in 2013 and officially copyrighted in October 2014 by a Canadian company named You Learn Educational Solutions & Linguistics Inc. PYMOM takes the movement cycle from TMC and remedied the challenge of teaching the movement cycle itself to students by providing a software-based solution to handling cycles by means of sub-routines prompted by the user’s input.\n\nPYMOM wove established educational theory into the fabric of TMC to create a viable educational platform for academic and linguistic study by several means. Because spaced repetition is intrinsically part of the movement-cycle subroutines, it adds to the content and surrounding experience making it into a platform. The developers of PYMOM describe it as an “organic learning experience.” The tenets that truly allow a learning system to be a PYMOM-based system are enumerated thusly: The Von Restorff effect: for example, where it features a language, this method is employed to further aid in memory retention of the highlighted word in the phrase.\n\n"}
{"id": "25606638", "url": "https://en.wikipedia.org/wiki?curid=25606638", "title": "Search neutrality", "text": "Search neutrality\n\nSearch neutrality is a principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance. This means that when a user queries a search engine, the engine should return the most relevant results found in the provider's domain (those sites which the engine has knowledge of), without manipulating the order of the results (except to rank them by relevance), excluding results, or in any other way manipulating the results to a certain bias. \nSearch neutrality is related to network neutrality in that they both aim to keep any one organization from limiting or altering a user's access to services on the Internet. Search neutrality aims to keep the organic search results (results returned because of their relevance to the search terms, as opposed to results sponsored by advertising) of a search engine free from any manipulation, while network neutrality aims to keep those who provide and govern access to the Internet from limiting the availability of resources to access any given content.\n\nThe term \"search neutrality\" in context of the internet appears as early as March 2009 in an academic paper by Andrew Odlyzko titled, \"Network Neutrality, Search Neutrality, and the Never-ending Conflict between Efficiency and Fairness in Markets\". In this paper, Odlykzo predicts that if net neutrality were to be accepted as a legal or regulatory principle, then the questions surrounding search neutrality would be the next controversies. Indeed, in December 2009 the New York Times published an opinion letter by Foundem co-founder and lead complainant in an anti-trust complaint against Google, Adam Raff, which likely brought the term to the broader public. According to Raff in his opinion letter, search neutrality ought to be \"the principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance\". On October 11, 2009, Adam and his wife Shivaun launched SearchNeutrality.org, an initiative dedicated to promoting investigations against Google's search engine practices . There, the Raffs note that they chose to frame their issue with Google as \"search neutrality\" in order to benefit from the focus and interest on net neutrality.\n\nIn contrast to net neutrality, answers to such questions, as \"what is search neutrality?\" or \"what are appropriate legislative or regulatory principles to protect search neutrality?\", appear to have less consensus. The idea that neutrality means equal treatment, regardless of the content, comes from debates on net neutrality . Neutrality in search is complicated by the fact that search engines, by design and in implementation, are not intended to be neutral or impartial. Rather, search engines and other information retrieval applications are designed to collect and store information (indexing), receive a query from a user, search for and filter relevant information based on that query (searching/filtering), and then present the user with only a subset of those results, which are ranked from most relevant to least relevant (ranking). \"Relevance\" is a form of bias used to favor some results and rank those favored results. Relevance is defined in the search engine so that a user is satisfied with the results and is therefore subject to the user's preferences. And because relevance is so subjective, putting search neutrality into practice has been so contentious.\n\nSearch neutrality became a concern after search engines, most notably Google, were accused of search bias by other companies. Competitors and companies claim search engines systematically favor some sites (and some kind of sites) over others in their lists of results, disrupting the objective results users believe they are getting. \n\nThe call for search neutrality goes beyond traditional search engines. Sites like Amazon.com and Facebook are also accused of skewing results. Amazon’s search results are influenced by companies that pay to rank higher in their search results while Facebook filters their newsfeed lists to conduct social experiments.\n\nIn order to find information on the Web, most users make use of search engines, which crawl the web, index it and show a list of results ordered by relevance. The use of search engines to access information through the web has become a key factor for online businesses companies, which depend on the flow of users visiting their pages. One of these companies is Foundem. Foundem provides a \"vertical search\" service to compare products available on online markets for the U.K. Many people see these \"vertical search\" sites as spam. Beginning in 2006 and for three and a half years following, Foundem’s traffic and business dropped significantly due to what they assert to be a penalty deliberately applied by Google. It is unclear, however, whether their claim of a penalty was self-imposed via their use of iframe HTML tags to embed the content from other websites. At the time at which Foundem claims the penalties were imposed, it was unclear whether web crawlers crawled beyond the main page of a website using iframe tags without some extra modifications. The former SEO director OMD UK, Jaamit Durrani, among others, offered this alternative explanation, stating that “Two of the major issues that Foundem had in summer was content in iFrames and content requiring javascript to load – both of which I looked at in August, and they were definitely in place. Both are huge barriers to search visibility in my book. They have been fixed somewhere between then and the lifting of the supposed ‘penalty’. I don’t think that’s a coincidence.” \n\nMost of Foundem’s accusations claim that Google deliberately applies penalties to other vertical search engines because they represent competition. Foundem is backed by a Microsoft proxy group, the 'Initiative for Competitive Online Marketplace'.\n\nThe following table details Foundem's chronology of events as found on their website:\n\nGoogle's large market share (85%) has made them a target for search neutrality litigation via antitrust laws. In February 2010, Google released an article on the Google Public Policy blog expressing their concern for fair competition, when other companies at the UK joined Foundem's cause (eJustice.fr, and Microsoft's Ciao! from Bing) also claiming being unfairly penalized by Google.\n\nAfter two years of looking into claims that Google “manipulated its search algorithms to harm vertical websites and unfairly promote its own competing vertical properties,” the Federal Trade Commission (FTC) voted unanimously to end the antitrust portion of its investigation without filing a formal complaint against Google. The FTC concluded that Google’s “practice of favoring its own content in the presentation of search results” did not violate U.S. antitrust laws. The FTC further determined that even though competitors might be negatively impacted by Google's changing algorithms, Google did not change its algorithms to hurt competitors, but as a product improvement to benefit consumers.\n\nThere are a number of arguments for and against search neutrality.\n\n\n\nGoogle’s \"Universal Search\" system has been identified as one of the least neutral search engine practices, and following the implementation of Universal Search websites, such as MapQuest, the company experienced a massive decline in web traffic. This decline has been attributed to Google linking to its own services rather than the services offered at external websites. Despite these claims, Google actually displays Google content on the first page, while rival search engines do not considerably less often than Microsoft's Bing which displays Microsoft content when rivals do not. Bing displays Microsoft content in first place more than twice as often as Google shows Google content in first place. This indicates that as far as there is any 'bias', Google is less biased than its principal competitor.\n"}
{"id": "40592503", "url": "https://en.wikipedia.org/wiki?curid=40592503", "title": "Sure-thing principle", "text": "Sure-thing principle\n\nIn decision theory, the sure-thing principle states that a decision maker who would take a certain action if he knew that event \"E\" has occurred, and also if he knew that the negation of \"E\" has occurred, should also take that same action if he knows nothing about \"E\".\n\nThe principle was coined by L.J. Savage:\nHe formulated the principle as a dominance principle, but it can also be framed probabilistically. Jeffrey and later Pearl showed that Savage's principle is only valid when the probability of the event considered (e.g., the winner of the election) is unaffected by the action (buying the property). Under such conditions, the sure-thing principle is a theorem in the \"do\"-calculus (see Bayes networks). Blyth constructed a counterexample to the sure-thing principle using sequential sampling in the context of Simpson's paradox, but this example violates the required action-independence provision.\n\nThe principle is closely related to independence of irrelevant alternatives, and equivalent under the axiom of truth (everything the agent knows is true). It is similarly targeted by the Ellsberg and Allais paradoxes, in which actual people's choices seem to violate this principle. \n"}
{"id": "37673", "url": "https://en.wikipedia.org/wiki?curid=37673", "title": "Symbol", "text": "Symbol\n\nA symbol is a mark, sign or word that indicates, signifies, or is understood as representing an idea, object, or relationship. Symbols allow people to go beyond what is known or seen by creating linkages between otherwise very different concepts and experiences. All communication (and data processing) is achieved through the use of symbols. Symbols take the form of words, sounds, gestures, ideas or visual images and are used to convey other ideas and beliefs. For example, a red octagon may be a symbol for \"STOP\". On a map, a blue line might represent a river. Numerals are symbols for numbers. Alphabetic letters may be symbols for sounds. Personal names are symbols representing individuals. A red rose may symbolize love and compassion. The variable 'x', in a mathematical equation, may symbolize the position of a particle in space.\n\nIn cartography, an organized collection of symbols forms a legend for a map.\n\nThe word \"symbol\" derives from the Greek σύμβολον \"symbolon\", meaning \"token, watchword\" from σύν \"syn\" \"together\" and βάλλω \"bállō\" \" \"I throw, put.\" The sense evolution in Greek is from \"throwing things together\" to \"contrasting\" to \"comparing\" to \"token used in comparisons to determine if something is genuine.\" Hence, \"outward sign\" of something. The meaning \"something which stands for something else\" was first recorded in 1590, in Edmund Spenser's \"Faerie Queene\".\n\nIn considering the effect of a symbol on the psyche, in his seminal essay \"The Symbol without Meaning\" Joseph Campbell proposes the following definition:\n\"A symbol is an energy evoking, and directing, agent\".\n\nLater, expanding on what he means by this definition Campbell says:\n\nJared Elisha defined symbolism that is something that stands for another, it can be place, object, or a person\n\nHeinrich Zimmer gives a concise overview of the nature, and perennial relevance, of symbols.\n\nIn the book \"Signs and Symbols, \"it is stated that \"A symbol ... is a visual image or sign representing an idea -- a deeper indicator of a universal truth.\"\n\nSymbols are a means of complex communication that often can have multiple levels of meaning. This separates symbols from signs, as signs have only one meaning.\n\nHuman cultures use symbols to express specific ideologies and social structures and to represent aspects of their specific culture. Thus, symbols carry meanings that depend upon one’s cultural background; in other words, the meaning of a symbol is not inherent in the symbol itself but is culturally learned.\n\nSymbols are the basis of all human understanding and serve as vehicles of conception for all human knowledge. Symbols facilitate understanding of the world in which we live, thus serving as the grounds upon which we make judgments. In this way, people use symbols not only to make sense of the world around them, but also to identify and cooperate in society through constitutive rhetoric.\n\nSemiotics is the study of signs, symbols, and signification as communicative behavior. Semiotics studies focus on the relationship of the signifier and the signified, also taking into account interpretation of visual cues, body language, sound, and other contextual clues. Semiotics is linked with both linguistics and psychology. Semioticians thus not only study what a symbol implies, but also how it got its meaning and how it functions to make meaning in society. Symbols allow the human brain continuously to create meaning using sensory input and decode symbols through both denotation and connotation.\n\nSwiss psychoanalyst Carl Jung, who studied archetypes, proposed an alternative definition of symbol, distinguishing it from the term \"sign\". In Jung's view, a sign stands for something known, as a word stands for its referent. He contrasted this with \"symbol\", which he used to stand for something that is unknown and that cannot be made clear or precise. An example of a symbol in this sense is Christ as a symbol of the archetype called \"self\". For example, written languages are composed of a variety of different symbols that create words, p. . Through these written words humans communicate with each other. Kenneth Burke described \"Homo sapiens\" as a \"symbol-using, symbol making, and symbol misusing animal\" to suggest that a person creates symbols as well as misuses them. One example he uses to indicate what he means by the misuse of symbol is the story of a man who, when told that a particular food item was whale blubber, could barely keep from throwing it up. Later, his friend discovered it was actually just a dumpling. But the man's reaction was a direct consequence of the symbol of \"blubber\" representing something inedible in his mind. In addition, the symbol of \"blubber\" was created by the man through various kinds of learning.\n\nBurke goes on to describe symbols as also being derived from Sigmund Freud's work on condensation and displacement, further stating that symbols are not just relevant to the theory of dreams but also to \"normal symbol systems\". He says they are related through \"substitution\", where one word, phrase, or symbol is substituted for another in order to change the meaning. In other words, if one person does not understand a certain word or phrase, another person may substitute a synonym or symbol in order to get the meaning across. However, upon learning the new way of interpreting a specific symbol, the person may change his or her already-formed ideas to incorporate the new information.\n\nJean Dalby Clift says that people not only add their own interpretations to symbols, they also create personal symbols that represent their own understanding of their lives: what she calls \"core images\" of the person. She argues that symbolic work with these personal symbols or core images can be as useful as working with dream symbols in psychoanalysis or counseling.\n\nWilliam Indick suggests that the symbols that are commonly found in myth, legend, and fantasy fulfil psychological functions and hence are why archetypes such as \"the hero,\" \"the princess\" and \"the witch\" have remained popular for centuries.\n\nPaul Tillich argued that, while signs are invented and forgotten, symbols are born and die. There are, therefore, dead and living symbols. A living symbol can reveal to an individual hidden levels of meaning and transcendent or religious realities. For Tillich a symbol always \"points beyond itself\" to something that is unquantifiable and mysterious; symbols open up the \"depth dimension of reality itself\". Symbols are complex, and their meanings can evolve as the individual or culture evolves. When a symbol loses its meaning and power for an individual or culture, it becomes a dead symbol.\nWhen a symbol becomes identified with the deeper reality to which it refers, it becomes idolatrous as the \"symbol is taken for reality.\" The symbol itself is substituted for the deeper meaning it intends to convey. The unique nature of a symbol is that it gives access to deeper layers of reality which are otherwise inaccessible.\n\nA symbol's meaning may be modified by various factors including popular usage, history, and contextual intent.\n\nThe history of a symbol is one of many factors in determining a particular symbol's apparent meaning. Consequently, symbols with emotive power carry problems analogous to false etymologies.\n\nThe context of a symbol may change its meaning. Similar five-pointed stars might signify a law enforcement officer or a member of the armed services, depending upon the uniform.\n\nSymbols are used in cartography to communicate geographical information (generally as point, line, or area features). As with other symbols, visual variables such as size, shape, orientation, texture, and pattern provide meaning to the symbol.\n\nThe form, or shape, of a cartographic symbol is classified into one of three main groups:\n\nPictorial/Representational - a shape or image that clearly resembles the geographic feature being symbolized and can be interpreted without a legend.\n\nAssociative - a mixture of pictorial and geometric elements that produce an easily recognizable shape.\n\nAbstract/Geometric - completely arbitrary shapes chosen to represent a certain feature.\n\nA symbolic action is an action that has no, or little, practical effect but symbolizes, or signals, what the actor wants or believes. The action conveys meaning to the viewers.\n\nSymbolic action may overlap with symbolic speech, such as the use of flag burning to express hostility or saluting the flag to express patriotism.\n\nIn response to intense public criticism, businesses, organizations, and governments may take symbolic actions rather than, or in addition to, directly addressing the identified problems.\n\nSymbolic actions are sometimes derided as slacktivism.\n\n\n"}
