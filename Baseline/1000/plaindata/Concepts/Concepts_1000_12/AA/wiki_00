{"id": "66975", "url": "https://en.wikipedia.org/wiki?curid=66975", "title": "Apophatic theology", "text": "Apophatic theology\n\nApophatic theology, also known as negative theology, is a form of theological thinking and religious practice which attempts to approach God, the Divine, by negation, to speak only in terms of what may not be said about the perfect goodness that is God. It forms a pair together with cataphatic theology, which approaches God or the Divine by affirmations or positive statements about what God \"is\".\n\nThe apophatic tradition is often, though not always, allied with the approach of mysticism, which aims at the vision of God, the perception of the divine reality beyond the realm of ordinary perception.\n\n\"Apophatic\", (adjective); from ἀπόφημι \"apophēmi\", meaning \"to deny\". From \"Online Etymology Dictionary\": \n\"Via negativa\" or \"via negationis\" (Latin), \"negative way\" or \"by way of denial\". The negative way forms a pair together with the \"kataphatic\" or positive way. According to Deirdre Carabine,\nAccording to Fagenblat, \"negative theology is as old as philosophy itself;\" elements of it can be found in Plato's \"unwritten doctrines,\" while it is also present in Neo-Platonic, Gnostic and early Christian writers. A tendency to apophatic thought can also be found in Philo of Alexandria.\n\nAccording to Carabine, \"apophasis proper\" in Greek thought starts with Neo-Platonism, with its speculations about the nature of the One, culminating in the works of Proclus. According to Carabine, there are two major points in the development of apophatic theology, namely the fusion of the Jewish tradition with Platonic philosophy in the writings of Philo, and the works of Dionysius the Pseudo-Areopagite, who infused Christian thought with Neo-Platonic ideas.\n\nThe Early Church Fathers were influenced by Philo, and Meredith even states that Philo \"is the real founder of the apophatic tradition.\" Yet, it was with Pseudo-Dionysius the Areopagite and Maximus the Confessor, whose writings shaped both Hesychasm, the contemplative tradition of the Eastern Orthodox Churches, and the mystical traditions of western Europe, that apophatic theology became a central element of Christian theology and contemplative practice.\n\nFor the ancient Greeks, knowledge of the gods was essential for proper worship. Poets had an important responsibility in this regard, and a central question was how knowledge of the Divine forms can be attained. Epiphany played an essential role in attaining this knowledge. Xenophanes (c. 570 – c. 475 BC) noted that the knowledge of the Divine forms is restrained by the human imagination, and Greek philosophers realized that this knowledge can only be mediated through myth and visual representations, which are culture-dependent.\n\nAccording to Herodotus (484–425 BCE), Homer and Hesiod (between 750 and 650 BC) taught the Greek the knowledge of the Divine bodies of the Gods. The ancient Greek poet Hesiod (between 750 and 650 BC) describes in his \"Theogony\" the birth of the gods and creation of the world, which became an \"ur-text for programmatic, first-person epiphanic narratives in Greek literature,\" but also \"explores the necessary limitations placed on human access to the divine.\" According to Platt, the statement of the Muses who grant Hesiod knowledge of the Gods \"actually accords better with the logic of apophatic religious thought.\"\n\nParmenides (fl. late sixth or early fifth century BC), in his poem \"On Nature\", gives an account of a revelation on two ways of inquiry. \"The way of conviction\" explores Being, true reality (\"what-is\"), which is \"What is ungenerated and deathless,/whole and uniform, and still and perfect.\" \"The way of opinion\" is the world of appearances, in which one's sensory faculties lead to conceptions which are false and deceitful. His distinction between unchanging Truth and shifting opinion is reflected in Plato's allegory of the Cave. Together with the Biblical story of Moses's ascent of Mount Sinai, it is used by Gregory of Nyssa and Pseudo-Dionysius the Areopagite to give a Christian account of the ascent of the soul toward God. Cook notes that Parmenides poem is a religious account of a mystical journey, akin to the mystery cults, giving a philosophical form to a religious outlook. Cook further notes that the philosopher's task is to \"attempt through 'negative' thinking to tear themselves loose from all that frustrates their pursuit of wisdom.\"\n\nPlato (428/427 or 424/423 – 348/347 BCE), \"deciding for Parmenides against Heraclitus\" and his theory of eternal change, had a strong influence on the development of apophatic thought.\n\nPlato further explored Parmenides's idea of timeless truth in his dialogue \"Parmenides\", which is a treatment of the eternal forms, \"Truth, Beauty and Goodness\", which are the real aims for knowledge. The Theory of Forms is Plato's answer to the problem \"how one unchanging reality or essential being can admit of many changing phenomena (and not just by dismissing them as being mere illusion).\"\n\nIn \"The Republic\", Plato argues that the \"real objects of knowledge are not the changing objects of the senses, but the immutable Forms,\" stating that the \"Form of the Good\" is the highest object of knowledge. His argument culminates in the Allegory of the Cave, in which he argues that humans are like prisoners in a cave, who can only see shadows of the Real, the \"Form of the Good\". Humans are to be educated to search for knowledge, by turning away from their bodily desires toward higher contemplation, culminating in an intellectual understanding or apprehension of the Forms, c.q. the \"first principles of all knowledge.\"\n\nAccording to Cook, the \"Theory of Forms\" has a theological flavour, and had a strong influence on the ideas of his Neo-Platonist interpreters Proclus and Plotinus. The pursuit of \"Truth, Beauty and Goodness\" became a central element in the apophatic tradition, but nevertheless, according to Carabine \"Plato himself cannot be regarded as the founder of the negative way.\" Carabine warns not to read later Neo-Platonic and Christian understandings into Plato, and notes that Plato did not identify his Forms with \"one transcendent source,\" an identification which his later interpreters made.\n\nMiddle Platonism (1st century BCE - 3rd century CE) further investigated Plato's \"Unwritten Doctrines,\" which drew on Pythagoras' first principles of the Monad and the Dyad (matter). Middle Platonism proposed a hierarchy of being, with God as its first principle at its top, identifying it with Plato's \"Form of the Good\". An influential proponent of Middle Platonism was Philo (c.25 BCE–c. 50 CE), who employed Middle Platonic philosophy in his interpretation of the Hebrew scriptures, and asserted a strong influence on early Christianity. According to Craig D. Allert, \"Philo made a monumental contribution to the creation of a vocabulary for use in negative statements about God.\" For Philo, God is undescribable, and he uses terms which emphasize God's transcendence.\n\nNeo-Platonism was a mystical or contemplative form of Platonism, which \"developed outside the mainstream of Academic Platonism.\" It started with the writings of Plotinus (204/5–270), and ended with the closing of the Platonic Academy by Emperor Justinian in 529 CE, when the pagan traditions were ousted. It is a product of Hellenistic syncretism, which developed due to the crossover between Greek thought and the Jewish scriptures, and also gave birth to Gnosticism. Proclus was the last head of the Platonic Academy; his student Pseudo-Dinosysius had a far-stretching Neo-Platonic influence on Christianity and Christian mysticism.\n\nPlotinus (204/5–270) was the founder of Neo-Platonism. In the Neo-Platonic philosophy of Plotinus and Proclus, the first principle became even more elevated as a radical unity, which was presented as an unknowable Absolute. For Plotinus, \"the One\" is the first principle, from which everything else emanates. He took it from Plato's writings, identifying the Good of the \"Republic\", as the cause of the other Forms, with \"the One\" of the first hypothesis of the second part of the \"Parmenides\". For Plotinus, \"the One\" precedes the Forms, and \"is beyond Mind and indeed beyond Being.\" From \"the One\" comes the Intellect, which contains all the Forms. \"The One\" is the principle of Being, while the Forms are the principle of the essence of beings, and the intelligibility which can recognize them as such. Plotinus's third principle is Soul, the desire for objects external to the person. The highest satisfaction of desire is the contemplation of \"the One\", which unites all existents \"as a single, all-pervasive reality.\"\n\n\"The One\" is radically simple, and does not even have self-knowledge, since self-knowledge would imply multiplicity. Nevertheless, Plotinus does urge for a search for the Absolute, turning inward and becoming aware of the \"presence of the intellect in the human soul,\" initiating an ascent of the soul by abstraction or \"taking away,\" culminating in a sudden appearance of \"the One\". In the \"Enneads\" Plotinus writes: \nCarabine notes that Plotinus' apophasis is not just a mental exercise, an acknowledgement of the unknowability of \"the One\", but a means to \"extasis\" and an ascent to \"the unapproachable light that is God.\" Pao-Shen Ho, investigating what are Plotinus' methods for reaching \"henosis\", concludes that \"Plotinus' mystical teaching is made up of two practices only, namely philosophy and negative theology.\" According to Moore, Plotinus appeals to the \"non-discursive, intuitive faculty of the soul,\" by \"calling for a sort of prayer, an invocation of the deity, that will permit the soul to lift itself up to the unmediated, direct, and intimate contemplation of that which exceeds it (V.1.6).\" Pao-Shen Ho further notes that \"for Plotinus, mystical experience is irreducible to philosophical arguments.\" The argumentation about \"henosis\" is preceded by the actual experience of it, and can only be understood when \"henosis\" has been attained. Ho further notes that Plotinus's writings have a didactic flavour, aiming to \"bring his own soul and \"the souls of others\" by way of Intellect to union with the One.\" As such, the \"Enneads\" as a spiritual or ascetic teaching device, akin to \"The Cloud of Unknowing\", demonstrating the methods of philosophical and apophatic inquiry. Ultimately, this leads to silence and the abandonment of all intellectual inquiry, leaving contemplation and unity.\n\nProclus (412-485) introduced the terminology which is being used in apophatic and cataphatic theology. He did this in the second book of his \"Platonic Theology\", arguing that Plato states that \"the One\" can be revealed \"through analogy,\" and that \"through negations [\"dia ton apophaseon\"] its transcendence over everything can be shown.\" For Proclus, apophatic and cataphonic theology form a contemplatory pair, with the apophatic approach corresponding to the manifestation of the world from \"the One\", and cataphonic theology corresponding to the return to \"the One\". The analogies are affirmations which direct us toward \"the One\", while the negations underlie the confirmations, being closer to \"the One\". According to Luz, Proclus also attracted students from other faiths, including the Samaritan Marinus. Luz notes that \"Marinus' Samaritan origins with its Abrahamic notion of a single ineffable Name of God () should also have been in many ways compatible with the school's ineffable and apophatic divine principle.\"\nThe Book of Revelation 8:1 mentions \"the silence of the perpetual choir in heaven.\" According to Dan Merkur,\nThe Early Church Fathers were influenced by Philo (c. 25 BCE – c. 50 CE), who saw Moses as \"the model of human virtue and Sinai as the archetype of man's ascent into the \"luminous darkness\" of God.\" His interpretation of Moses was followed by Clement of Alexandria, Origen, the Cappadocian Fathers, Pseudo-Dionysius, and Maximus the Confessor.\n\nGod's appearance to Moses in the burning bush was often elaborated on by the Early Church Fathers, especially Gregory of Nyssa (c. 335 – c. 395), realizing the fundamental unknowability of God; an exegesis which continued in the medieval mystical tradition. Their response is that, although God is unknowable, Jesus as person can be followed, since \"following Christ is the human way of seeing God.\"\n\nClement of Alexandria (c. 150 – c. 215) was an early proponent of apophatic theology. According to R.A. Baker, in Clement's writings the term \"theoria\" develops further from a mere intellectual \"seeing\" toward a spirutal form of contemplation. Clement's apophatic theology or philosophy is closely related to this kind of \"theoria\" and the \"mystic vision of the soul.\" For Clement, God is transcendent and immanent. According to Baker, Clement's apophaticism is mainly driven by Biblical texts, but by the Platonic tradition. His conception of an ineffable God is a synthesis of Plato and Philo, as seen from a Biblical perspective. According to Osborne, it is a synthesis in a Biblical framework; according to Baker, while the Platonic tradition accounts for the negative approach, the Biblical tradition accounts for the positive approach. \"Theoria\" and abstraction is the means to conceive of this ineffable God; it is preceded by dispassion.\n\nAccording to Tertullian (c. 155 – c. 240),\nSaint Cyril of Jerusalem (313-386), in his \"Catechetical Homilies\", states: \nAugustine of Hippo (354-430) defined God \"aliud, aliud valde\", meaning \"other, completely other\", in \"Confessions\" 7.10.16.\n\nApophatic theology found its most influential expression in the works of Pseudo-Dionysius the Areopagite (late 5th to early 6th century), a student of Proclus (412-485), combining a Christian worldview with Neo-Platonic ideas. He is a constant factor in the contemplative tradition of the eastern Orthodox Churches, and from the 9th century onwards his writings also had a strong impact on western mysticism.\n\nDionysius the Areopagite was a pseudonym, taken from Acts of the Apostles chapter 17, in which Paul gives a missionary speech to the court of the Areopagus in Athens. In Paul makes a reference to an altar-inscription, dedicated to the Unknown God, \"a safety measure honoring foreign gods still unknown to the Hellenistic world.\" For Paul, Jesus Christ is this unknown God, and as a result of Paul's speech Dionysius the Areopagite converts to Christianity. Yet, according to Stang, for Pseudo-Dionysius the Areopagite Athens is also the place of Neo-Platonic wisdom, and the term \"unknown God\" is a reversal of Paul's preaching toward an integration of Christianity with Neo-Platonism, and the union with the \"unknown God.\"\n\nAccording to Corrigan and Harrington, \"Dionysius' central concern is how a triune God, ... who is utterly unknowable, unrestricted being, beyond individual substances, beyond even goodness, can become manifest to, in, and through the whole of creation in order to bring back all things to the hidden darkness of their source.\" Drawing on Neo-Platonism, Pseudo-Dionysius described humans ascend to divinity as a process of purgation, illumination and union. Another Neo-Platonic influence was his description of the cosmos as a series of hierarchies, which overcome the distance between God and humans.\n\nIn Orthodox Christianity apophatic theology is taught as superior to cataphatic theology. The fourth-century Cappadocian Fathers stated a belief in the existence of God, but an existence unlike that of everything else: everything else that exists was created, but the Creator transcends this existence, is uncreated. The essence of God is completely unknowable; mankind can know God only through His energies. Gregory of Nyssa (c.335-c.395), John Chrysostom (c. 349 – 407), and Basil the Great (329-379) emphasized the importance of negative theology to an orthodox understanding of God. John of Damascus (c.675/676–749) employed negative theology when he wrote that positive statements about God reveal \"not the nature, but the things around the nature.\"\n\nMaximus the Confessor (580-622) took over Pseudo-Dionysius' ideas, and had a strong influence on the theology and contemplative practices of the Eastern Orthodox Churches. Gregory Palamas (1296–1359) formulated the definite theology of Hesychasm, the Orthodox practices of contemplative prayer and theosis, \"deification.\"\n\nInfluential modern Eastern Orthodox theologians are Vladimir Lossky, John Meyendorff, John S. Romanides and Georges Florovsky. Lossky argues, based on his reading of Dionysius and Maximus Confessor, that positive theology is always inferior to negative theology which is a step along the way to the superior knowledge attained by negation. This is expressed in the idea that mysticism is the expression of dogmatic theology \"par excellence\".\n\nAccording to Lossky, outside of directly revealed knowledge through Scripture and Sacred Tradition, such as the Trinitarian nature of God, God in His essence is beyond the limits of what human beings (or even angels) can understand. He is transcendent in essence (\"ousia\"). Further knowledge must be sought in a direct experience of God or His indestructible energies through \"theoria\" (vision of God). According to Aristotle Papanikolaou, in Eastern Christianity, God is immanent in his hypostasis or existences.\n\nNegative theology has a place in the Western Christian tradition as well. The 9th-century theologian John Scotus Erigena wrote: \n\nWhen he says \"\"He is not anything\" and \"God is not\"\", Scotus does not mean that there is no God, but that God cannot be said to exist in the way that creation exists, i.e. that God is uncreated. He is using apophatic language to emphasise that God is \"other\".\n\nTheologians like Meister Eckhart and Saint John of the Cross (San Juan de la Cruz) exemplify some aspects of or tendencies towards the apophatic tradition in the West. The medieval work, \"The Cloud of Unknowing\" and Saint John's \"Dark Night of the Soul\" are particularly well known. In 1215 apophatism became the official position of the Catholic Church, which, on the basis of Scripture and church tradition, during the Fourth Lateran Council formulated the following dogma:\nThomas Aquinas was born ten years later (1225-1274) and, although in his \"Summa Theologica\" he quotes Pseudo-Dionysius 1,760 times, his reading in a neo-Aristotelian key of the conciliar declaration overthrew its meaning inaugurating the \"analogical way\" as \"tertium\" between \"via negativa\" and \"via positiva\": the \"via eminentiae\" (see also \"analogia entis\"). According to Adrian Langdon,\nAccording to \"Catholic Encyclopedia\", the \"Doctor Angelicus\" and the scholastici declare [that] \nSince then Thomism has played a decisive role in resizing the negative or apophatic tradition of the magisterium.\n\nApophatic statements are still crucial to many modern theologians, restarting in 1800s by Søren Kierkegaard (see his concept of the infinite qualitative distinction) up to Rudolf Otto and Karl Barth (see their idea of \"Wholly Other\", i.e. \"ganz Andere\" or \"totaliter aliter\").\n\nC. S. Lewis, in his book \"Miracles\" (1947), advocates the use of negative theology when first thinking about God, in order to cleanse our minds of misconceptions. He goes on to say we must then refill our minds with the truth about God, untainted by mythology, bad analogies or false mind-pictures.\n\nThe mid-20th century Dutch philosopher Herman Dooyeweerd, who is often associated with a neo-Calvinistic tradition, provides a philosophical foundation for understanding why we can never absolutely know God, and yet, paradoxically, truly know something of God. Dooyeweerd made a sharp distinction between theoretical and pre-theoretical attitudes of thought. Most of the discussion of knowledge of God presupposes theoretical knowledge, in which we reflect and try to define and discuss. Theoretical knowing, by its very nature, is never absolute, always depends on religious presuppositions, and cannot grasp either God or the law side. Pre-theoretical knowing, on the other hand, is intimate engagement, and exhibits a diverse range of aspects. Pre-theoretical intuition, on the other hand, can grasp at least the law side. Knowledge of God, as God wishes to reveal it, is pre-theoretical, immediate and intuitive, never theoretical in nature. The philosopher Leo Strauss considered that the Bible, for example, should be treated as pre-theoretical (everyday) rather than theoretical in what it contains.\n\nIvan Illich (1926-2002), the historian and social critic, can be read as an apophatic theologian, according to a longtime collaborator, Lee Hoinacki, in a paper presented in memory of Illich, called \"Why Philia?\"\n\nAccording to Deirdre Carabine, negative theology has become a hot topic since the 1990s, resulting from a broad effort in the 19 and 20th century to portray Plato as a mysticist, which revived the interest in Neoplatonism and negative theology.\n\nKaren Armstrong, in her book \"The Case for God\" (2009), notices a recovery of apophatic theology in postmodern theology.\n\nThe Arabic term for \"negative theology\" is \"lahoot salbi\", which is a \"system of theology\" or \"nizaam al lahoot\" in Arabic. Different traditions/doctrine schools in Islam called Kalam schools (see Islamic schools and branches) use different theological approaches or \"nizaam al lahoot\" in approaching God in Islam (\"Allah\", Arabic الله) or the ultimate reality. The \"lahoot salbi\" or \"negative theology\" involves the use of \"ta'til\", which means \"negation,\" and the followers of the Mu'tazili school of Kalam, founded by Imam Wasil ibn Ata, are often called the \"Mu'attili\", because they are frequent users of the \"ta'tili\" methodology.\n\nRajab ʿAlī Tabrīzī, an Iranian and Shiat philosopher and mystic of the 17th century. instilled a radical apophatic theology in a generation of philosophers and theologians whose influence extended into the Qajar period. Mulla Rajab affirmed the completely unknowable,\nunqualifiable, and attributeless nature of God and upheld a general view concerning God’s attributes which can only be negatively ‘affirmed’, by means of the via\nnegativa.\n\nShia Islam adopted \"negative theology\". In the words of the Persian Ismaili missionary, Abu Yaqub al-Sijistani: \"There does not exist a tanzíh [\"transcendence\"] more brilliant and more splendid than that by which we establish the absolute transcendence of our Originator through the use of these phrases in which a negative and a negative of a negative apply to the thing denied.\" Early Sunni scholars who held to a literal reading of the Quran and hadith rejected this view, adhering to its opposite, believing that the Attributes of God such as \"Hand\", \"Foot\" etc... should be taken literally and that, therefore, God is like a human being. Today, most Sunnis, like the Ash'ari and Maturidi, adhere to a middle path between negation and anthropomorphism.\n\nMaimonides (1135/1138-1204) was \"the most influential medieval Jewish exponent of the \"via negativa\".\" Maimonides, but also Samuel ibn Tibbon, draw on Bahya ibn Paquda, who shows that our inability to describe God is related to the fact of His absolute unity. God, as the entity which is \"truly One\" (האחד האמת), must be free of properties and is thus unlike anything else and indescribable. According to Rabbi Yosef Wineberg, Maimonides stated that \"[God] is knowledge,\" and saw His Essence, Being and knowledge as completely one, \"a perfect unity and not a composite at all.\" Wineberg quotes Maimonides as stating\nIn \"The Guide for the Perplexed\" Maimonides stated:\nAccording to Fagenblat, it is only in the modern period that negative theology really gains importance in Jewish thought. Yeshayahu Leibowitz (1903-1994) was a prominent modern exponent of Jewish negative theology. According to Leibowitz, a person's faith is his commitment to obey God, meaning God's commandments, and this has nothing to do with a person’s image of God. This must be so because Leibowitz thought that God cannot be described, that God's understanding is not man's understanding, and thus all the questions asked of God are out of place.\n\nThere are interesting parallels in Indian thought, which developed largely separate from Western thought. Early Indian philosophical works which have apophatic themes include the Principal Upanishads (800 BCE to the start of common era) and the Brahma Sutras (from 450 BCE and 200 CE). An expression of negative theology is found in the Brihadaranyaka Upanishad, where Brahman is described as \"neti neti\" or \"neither this, nor that\". Further use of apophatic theology is found in the Brahma Sutras, which state:\n\nBuddhist philosophy has also strongly advocated the way of negation, beginning with the Buddha's own theory of anatta (not-atman, not-self) which denies any truly existent and unchanging essence of a person. Madhyamaka is a Buddhist philosophical school founded by Nagarjuna (2nd-3rd century CE), which is based on a fourfold negation of all assertions and concepts and promotes the theory of emptiness (shunyata). Apophatic assertions are also an important feature of Mahayana sutras, especially the prajñaparamita genre. These currents of negative theology are visible in all forms of Buddhism.\n\nApophatic movements in medieval Hindu philosophy are visible in the works of Shankara (8th century), a philosopher of Advaita Vedanta (non-dualism), and Bhartṛhari (5th century), a grammarian. While Shankara holds that the transcendent noumenon, Brahman, is realized by the means of negation of every phenomenon including language, Bhartṛhari theorizes that language has both phenomenal and noumenal dimensions, the latter of which manifests Brahman.\n\nIn Advaita, Brahman is defined as being Nirguna or without qualities. Anything imaginable or conceivable is not deemed to be the ultimate reality. The Taittiriya hymn speaks of Brahman as \"one where the mind does not reach\". Yet the Hindu scriptures often speak of Brahman's positive aspect. For instance, Brahman is often equated with bliss. These contradictory descriptions of Brahman are used to show that the attributes of Brahman are similar to ones experienced by mortals, but not the same. \n\nNegative theology also figures in the Buddhist and Hindu polemics. The arguments go something like this – Is Brahman an object of experience? If so, how do you convey this experience to others who have not had a similar experience? The only way possible is to relate this unique experience to common experiences while explicitly negating their sameness.\n\nEven though the \"via negativa\" essentially rejects theological understanding in and of itself as a path to God, some have sought to make it into an intellectual exercise, by describing God only in terms of what God is not. One problem noted with this approach is that there seems to be no fixed basis on deciding what God is not, unless the Divine is understood as an abstract experience of full aliveness unique to each individual consciousness, and universally, the perfect goodness applicable to the whole field of reality. Apophatic theology is often accused of being a version of atheism or agnosticism, since it cannot say truly that God exists. \"The comparison is crude, however, for conventional atheism treats the existence of God as a predicate that can be denied (“God is nonexistent”), whereas negative theology denies that God has predicates\". \"God or the Divine is\" without being able to attribute qualities about \"what He is\" would be the prerequisite of positive theology in negative theology that distinguishes theism from atheism. \"Negative theology is a complement to, not the enemy of, positive theology\". Since religious experience—or consciousness of the holy or sacred, is not reducible to other kinds of human experience, an abstract understanding of religious experience cannot be used as evidence or proof that religious discourse or praxis can have no meaning or value. In apophatic theology, the negation of theisms in the \"via negativa\" also requires the negation of their correlative atheisms if the dialectical method it employs is to maintain integrity.\n\n\n\n\n\n\n\n\n\n \n\n\n"}
{"id": "188401", "url": "https://en.wikipedia.org/wiki?curid=188401", "title": "Axiomatic system", "text": "Axiomatic system\n\nIn mathematics, an axiomatic system is any set of axioms from which some or all axioms can be used in conjunction to logically derive theorems. A theory consists of an axiomatic system and all its derived theorems. An axiomatic system that is completely described is a special kind of formal system. A formal theory typically means an axiomatic system, for example formulated within model theory. A formal proof is a complete rendition of a mathematical proof within a formal system.\n\nAn axiomatic system is said to be \"consistent\" if it lacks contradiction, i.e. the ability to derive both a statement and its denial from the system's axioms.\n\nIn an axiomatic system, an axiom is called \"independent\" if it is not a theorem that can be derived from other axioms in the system. A system will be called independent if each of its underlying axioms is independent. Although independence is not a necessary requirement for a system, consistency usually is, but see neutrosophic logic.\n\nAn axiomatic system will be called \"complete\" if for every statement, either itself or its negation is derivable.\n\nBeyond consistency, relative consistency is also the mark of a worthwhile axiom system. This is when the undefined terms of a first axiom system are provided definitions from a second, such that the axioms of the first are theorems of the second.\n\nA good example is the relative consistency of neutral geometry or absolute geometry with respect to the theory of the real number system. Lines and points are undefined terms in absolute geometry, but assigned meanings in the theory of real numbers in a way that is consistent with both axiom systems.\n\nA model for an axiomatic system is a well-defined set, which assigns meaning for the undefined terms presented in the system, in a manner that is correct with the relations defined in the system. The existence of a concrete model proves the consistency of a system. A model is called concrete if the meanings assigned are objects and relations from the real world}, as opposed to an abstract model which is based on other axiomatic systems.\n\nModels can also be used to show the independence of an axiom in the system. By constructing a valid model for a subsystem without a specific axiom, we show that the omitted axiom is independent if its correctness does not necessarily follow from the subsystem.\n\nTwo models are said to be isomorphic if a one-to-one correspondence can be found between their elements, in a manner that preserves their relationship. An axiomatic system for which every model is isomorphic to another is called categorial (sometimes categorical), and the property of categoriality (categoricity) ensures the completeness of a system.\n\nStating definitions and propositions in a way such that each new term can be formally eliminated by the priorly introduced terms requires primitive notions (axioms) to avoid infinite regress. This way of doing mathematics is called the axiomatic method.\n\nA common attitude towards the axiomatic method is logicism. In their book \"Principia Mathematica\", Alfred North Whitehead and Bertrand Russell attempted to show that all mathematical theory could be reduced to some collection of axioms. More generally, the reduction of a body of propositions to a particular collection of axioms underlies the mathematician's research program. This was very prominent in the mathematics of the twentieth century, in particular in subjects based around homological algebra.\n\nThe explication of the particular axioms used in a theory can help to clarify a suitable level of abstraction that the mathematician would like to work with. For example, mathematicians opted that rings need not be commutative, which differed from Emmy Noether's original formulation. Mathematicians decided to consider topological spaces more generally without the separation axiom which Felix Hausdorff originally formulated.\n\nThe Zermelo-Fraenkel axioms, the result of the axiomatic method applied to set theory, allowed the \"proper\" formulation of set-theory problems and helped to avoid the paradoxes of naïve set theory. One such problem was the Continuum hypothesis. Zermelo–Fraenkel set theory with the historically controversial axiom of choice included is commonly abbreviated ZFC, where C stands for choice. Many authors use ZF to refer to the axioms of Zermelo–Fraenkel set theory with the axiom of choice excluded. Today ZFC is the standard form of axiomatic set theory and as such is the most common foundation of mathematics.\n\nMathematical methods developed to some degree of sophistication in ancient Egypt, Babylon, India, and China, apparently without employing the axiomatic method.\n\nEuclid of Alexandria authored the earliest extant axiomatic presentation of Euclidean geometry and number theory. Many axiomatic systems were developed in the nineteenth century, including non-Euclidean geometry, the foundations of real analysis, Cantor's set theory, Frege's work on foundations, and Hilbert's 'new' use of axiomatic method as a research tool. For example, group theory was first put on an axiomatic basis towards the end of that century. Once the axioms were clarified (that inverse elements should be required, for example), the subject could proceed autonomously, without reference to the transformation group origins of those studies.\n\nNot every consistent body of propositions can be captured by a describable collection of axioms. Call a collection of axioms recursive if a computer program can recognize whether a given proposition in the language is an axiom. Gödel's First Incompleteness Theorem then tells us that there are certain consistent bodies of propositions with no recursive axiomatization. Typically, the computer can recognize the axioms and logical rules for deriving theorems, and the computer can recognize whether a proof is valid, but to determine whether a proof exists for a statement is only soluble by \"waiting\" for the proof or disproof to be generated. The result is that one will not know which propositions are theorems and the axiomatic method breaks down. An example of such a body of propositions is the theory of the natural numbers. The Peano Axioms (described below) thus only partially axiomatize this theory.\n\nIn practice, not every proof is traced back to the axioms. At times, it is not clear which collection of axioms a proof appeals to. For example, a number-theoretic statement might be expressible in the language of arithmetic (i.e. the language of the Peano Axioms) and a proof might be given that appeals to topology or complex analysis. It might not be immediately clear whether another proof can be found that derives itself solely from the Peano Axioms.\n\nAny more-or-less arbitrarily chosen system of axioms is the basis of some mathematical theory, but such an arbitrary axiomatic system will not necessarily be free of contradictions, and even if it is, it is not likely to shed light on anything. Philosophers of mathematics sometimes assert that mathematicians choose axioms \"arbitrarily\", but it is possible that although they may appear arbitrary when viewed only from the point of view of the canons of deductive logic, that appearance is due to a limitation on the purposes that deductive logic serves.\n\nThe mathematical system of natural numbers 0, 1, 2, 3, 4, ... is based on an axiomatic system first written down by the mathematician Peano in 1889. He chose the axioms, in the language of a single unary function symbol \"S\" (short for \"successor\"), for the set of natural numbers to be:\n\n\nIn mathematics, axiomatization is the formulation of a system of statements (i.e. axioms) that relate a number of primitive terms in order that a consistent body of propositions may be derived deductively from these statements. Thereafter, the proof of any proposition should be, in principle, traceable back to these axioms.\n\n\n"}
{"id": "173937", "url": "https://en.wikipedia.org/wiki?curid=173937", "title": "Cosmological principle", "text": "Cosmological principle\n\nIn modern physical cosmology, the cosmological principle is the notion that the spatial distribution of matter in the universe is homogeneous and isotropic when viewed on a large enough scale, since the forces are expected to act uniformly throughout the universe, and should, therefore, produce no observable irregularities in the large-scale structuring over the course of evolution of the matter field that was initially laid down by the Big Bang.\n\nAstronomer William Keel explains:\n\nThe cosmological principle is usually stated formally as 'Viewed on a sufficiently large scale, the properties of the universe are the same for all observers.' This amounts to the strongly philosophical statement that the part of the universe which we can see is a fair sample, and that the same physical laws apply throughout. In essence, this in a sense says that the universe is knowable and is playing fair with scientists.\n\nThe cosmological principle depends on a definition of \"observer,\" and contains an implicit qualification and two testable consequences.\n\n\"Observers\" means any observer at any location in the universe, not simply any human observer at any location on Earth: as Andrew Liddle puts it, \"the cosmological principle [means that] the universe looks the same whoever and wherever you are.\"\n\nThe qualification is that variation in physical structures can be overlooked, provided this does not imperil the uniformity of conclusions drawn from observation: the Sun is different from the Earth, our galaxy is different from a black hole, some galaxies advance toward rather than recede from us, and the universe has a \"foamy\" texture of galaxy clusters and voids, but none of these different structures appears to violate the basic laws of physics.\n\nThe two testable structural consequences of the cosmological principle are homogeneity and isotropy. Homogeneity means that the same observational evidence is available to observers at different locations in the universe (\"the part of the universe which we can see is a fair sample\"). Isotropy means that the same observational evidence is available by looking in any direction in the universe (\"the same physical laws apply throughout\" ). The principles are distinct but closely related, because a universe that appears isotropic from any two (for a spherical geometry, three) locations must also be homogeneous.\n\nThe cosmological principle is first clearly asserted in the \"Philosophiæ Naturalis Principia Mathematica\" (1687) of Isaac Newton. In contrast to earlier classical or medieval cosmologies, in which Earth rested at the center of universe, Newton conceptualized the Earth as a sphere in orbital motion around the Sun within an empty space that extended uniformly in all directions to immeasurably large distances. He then showed, through a series of mathematical proofs on detailed observational data of the motions of planets and comets, that their motions could be explained by a single principle of \"universal gravitation\" that applied as well to the orbits of the Galilean moons around Jupiter, the Moon around the Earth, the Earth around the Sun, and to falling bodies on Earth. That is, he asserted the equivalent material nature of all bodies within the Solar System, the identical nature of the Sun and distant stars and thus the uniform extension of the physical laws of motion to a great distance beyond the observational location of Earth itself.\n\nObservations show that more distant galaxies are closer together and have lower content of chemical elements heavier than lithium. Applying the cosmological principle, this suggests that heavier elements were not created in the Big Bang but were produced by nucleosynthesis in giant stars and expelled across a series of supernovae explosions and new star formation from the supernovae remnants, which means heavier elements would accumulate over time. Another observation is that the furthest galaxies (earlier time) are often more fragmentary, interacting and unusually shaped than local galaxies (recent time), suggesting evolution in galaxy structure as well.\n\nA related implication of the cosmological principle is that the largest discrete structures in the universe are in mechanical equilibrium. Homogeneity and isotropy of matter at the largest scales would suggest that the largest discrete structures are parts of a single indiscrete form, like the crumbs which make up the interior of a cake. At extreme cosmological distances, the property of mechanical equilibrium in surfaces lateral to the line of sight can be empirically tested; however, under the assumption of the cosmological principle, it cannot be detected parallel to the line of sight (see timeline of the universe).\n\nCosmologists agree that in accordance with observations of distant galaxies, a universe must be non-static if it follows the cosmological principle. In 1923, Alexander Friedmann set out a variant of Einstein's equations of general relativity that describe the dynamics of a homogeneous isotropic universe. Independently, Georges Lemaître derived in 1927 the equations of an expanding universe from the General Relativity equations. Thus, a non-static universe is also implied, independent of observations of distant galaxies, as the result of applying the cosmological principle to general relativity.\n\nKarl Popper criticized the cosmological principle on the grounds that it makes \"our \"lack\" of knowledge a principle of \"knowing something\"\". He summarized his position as:\n\nAlthough the universe is inhomogeneous at smaller scales, it \"is\" statistically homogeneous on scales larger than 250 million light years. The cosmic microwave background is isotropic, that is to say that its intensity is about the same whichever direction we look at.\n\nHowever, recent findings have called this view into question. Data from the Planck Mission shows hemispheric bias in 2 respects: one with respect to average temperature (i.e. temperature fluctuations), the second with respect to larger variations in the degree of perturbations (i.e. densities). Therefore, the European Space Agency (the governing body of the Planck Mission) has concluded that these anisotropies are, in fact, statistically significant and can no longer be ignored.\n\nThe \"cosmological principle\" implies that at a sufficiently large scale, the universe is homogeneous. This means that different places will appear similar to one another, so sufficiently large structures cannot exist. Yadav and his colleagues have suggested a maximum scale of 260/h Mpc for structures within the universe according to this heuristic. Other authors have suggested values as low as 60/h Mpc. Yadav's calculation suggests that the maximum size of a structure can be about 370 Mpc.\n\nA number of observations conflict with predictions of maximal structure sizes:\n\n\nIn September 2016, however, studies of the expansion of the Universe that have used data taken by the \"Planck\" mission show it to be highly isotropical, reinforcing the cosmological principle\n\nThe perfect cosmological principle is an extension of the cosmological principle, and states that the universe is homogeneous and isotropic in space \"and\" time. In this view the universe looks the same everywhere (on the large scale), the same as it always has and always will. The perfect cosmological principle underpins Steady State theory and emerges from chaotic inflation theory.\n\n"}
{"id": "17910574", "url": "https://en.wikipedia.org/wiki?curid=17910574", "title": "Digital ecosystem", "text": "Digital ecosystem\n\nA digital ecosystem is a distributed, adaptive, open socio-technical system with properties of self-organisation, scalability and sustainability inspired from natural ecosystems. Digital ecosystem models are informed by knowledge of natural ecosystems, especially for aspects related to competition and collaboration among diverse entities. The term is used in the computer industry, the entertainment industry, and the World Economic Forum.\n\nThe concept of Digital Business Ecosystem was put forward in 2002 by a group of European researchers and practitioners, including Francesco Nachira, Paolo Dini and Andrea Nicolai, who applied the general notion of digital ecosystems to model the process of adoption and development of ICT-based products and services in competitive, highly fragmented markets like the European one\n. Elizabeth Chang, Ernesto Damiani and Tharam Dillon started in 2007 the IEEE Digital EcoSystems and Technologies Conference (IEEE DEST). Richard Chbeir, Youakim Badr, Dominique Laurent, and Hiroshi Ishikawa started in 2009 the ACM Conference on Management of Digital EcoSystems (MEDES)\n\nThe digital ecosystem metaphor and models have been applied to a number of business areas related to the production and distribution of knowledge-intensive products and services, including higher education. The perspective of this research is providing methods and tools to achieve a set of objectives of the ecosystem (e.g. sustainability, fairness, bounded information asymmetry, risk control and gracious failure). These objectives are seen as desirable properties whose emergence should be fostered by the digital ecosystem self-organization, rather than as explicit design goals like in conventional IT.\n\n\n"}
{"id": "638834", "url": "https://en.wikipedia.org/wiki?curid=638834", "title": "Economic model", "text": "Economic model\n\nIn economics, a model is a theoretical construct representing economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified, often mathematical, framework designed to illustrate complex processes. Frequently, economic models posit structural parameters. A model may have various exogenous variables, and those variables may change to create various responses by economic variables. Methodological uses of models include investigation, theorizing, and fitting theories to the world.\n\nIn general terms, economic models have two functions: first as a simplification of and abstraction from observed data, and second as a means of selection of data based on a paradigm of econometric study.\n\n\"Simplification\" is particularly important for economics given the enormous complexity of economic processes. This complexity can be attributed to the diversity of factors that determine economic activity; these factors include: individual and cooperative decision processes, resource limitations, environmental and geographical constraints, institutional and legal requirements and purely random fluctuations. Economists therefore must make a reasoned choice of which variables and which relationships between these variables are relevant and which ways of analyzing and presenting this information are useful.\n\n\"Selection\" is important because the nature of an economic model will often determine what facts will be looked at, and how they will be compiled. For example, inflation is a general economic concept, but to measure inflation requires a model of behavior, so that an economist can differentiate between changes in relative prices and changes in price that are to be attributed to inflation.\n\nIn addition to their professional academic interest, the use of models include:\n\nA model establishes an \"argumentative framework\" for applying logic and mathematics that can be independently discussed and tested and that can be applied in various instances. Policies and arguments that rely on economic models have a clear basis for soundness, namely the validity of the supporting model.\n\nEconomic models in current use do not pretend to be \"theories of everything economic\"; any such pretensions would immediately be thwarted by computational infeasibility and the incompleteness or lack of theories for various types of economic behavior. Therefore, conclusions drawn from models will be approximate representations of economic facts. However, properly constructed models can remove extraneous information and isolate useful approximations of key relationships. In this way more can be understood about the relationships in question than by trying to understand the entire economic process.\n\nThe details of model construction vary with type of model and its application, but a generic process can be identified. Generally any modelling process has two steps: generating a model, then checking the model for accuracy (sometimes called diagnostics). The diagnostic step is important because a model is only useful to the extent that it accurately mirrors the relationships that it purports to describe. Creating and diagnosing a model is frequently an iterative process in which the model is modified (and hopefully improved) with each iteration of diagnosis and respecification. Once a satisfactory model is found, it should be double checked by applying it to a different data set.\n\nAccording to whether all the model variables are deterministic, economic models can be classified as stochastic or non-stochastic models; according to whether all the variables are quantitative, economic models are classified as discrete or continuous choice model; according to the model's intended purpose/function, it can be classified as\nquantitative or qualitative; according to the model's ambit, it can be classified as a general equilibrium model, a partial equilibrium model, or even a non-equilibrium model; according to the economic agent's characteristics, models can be classified as rational agent models, representative agent models etc.\n\n\nAt a more practical level, quantitative modelling is applied to many areas of economics and several methodologies have evolved more or less independently of each other. As a result, no overall model taxonomy is naturally available. We can nonetheless provide a few examples that illustrate some particularly relevant points of model construction.\n\n\n\nMost economic models rest on a number of assumptions that are not entirely realistic. For example, agents are often assumed to have perfect information, and markets are often assumed to clear without friction. Or, the model may omit issues that are important to the question being considered, such as externalities. Any analysis of the results of an economic model must therefore consider the extent to which these results may be compromised by inaccuracies in these assumptions, and a large literature has grown up discussing problems with economic models, or at least asserting that their results are unreliable.\n\nOne of the major problems addressed by economic models has been understanding economic growth. An early attempt to provide a technique to approach this came from the French physiocratic school in the Eighteenth century. Among these economists, François Quesnay should be noted, particularly for his development and use of tables he called \"Tableaux économiques\". These tables have in fact been interpreted in more modern terminology as a Leontiev model, see the Phillips reference below.\n\nAll through the 18th century (that is, well before the founding of modern political economy, conventionally marked by Adam Smith's 1776 Wealth of Nations) simple probabilistic models were used to understand the economics of insurance. This was a natural extrapolation of the theory of gambling, and played an important role both in the development of probability theory itself and in the development of actuarial science. Many of the giants of 18th century mathematics contributed to this field. Around 1730, De Moivre addressed some of these problems in the 3rd edition of \"The Doctrine of Chances\". Even earlier (1709), Nicolas Bernoulli studies problems related to savings and interest in the Ars Conjectandi. In 1730, Daniel Bernoulli studied \"moral probability\" in his book Mensura Sortis, where he introduced what would today be called \"logarithmic utility of money\" and applied it to gambling and insurance problems, including a solution of the paradoxical Saint Petersburg problem. All of these developments were summarized by Laplace in his Analytical Theory of Probabilities (1812). Clearly, by the time David Ricardo came along he had a lot of well-established math to draw from.\n\nIn the late 1980s the Brookings Institution compared 12 leading macroeconomic models available at the time. They compared the models' predictions for how the economy would respond to specific economic shocks (allowing the models to control for all the variability in the real world; this was a test of model vs. model, not a test against the actual outcome). Although the models simplified the world and started from a stable, known common parameters the various models gave significantly different answers. For instance, in calculating the impact of a monetary loosening on output some models estimated a 3% change in GDP after one year, and one gave almost no change, with the rest spread between.\n\nPartly as a result of such experiments, modern central bankers no longer have as much confidence that it is possible to 'fine-tune' the economy as they had in the 1960s and early 1970s. Modern policy makers tend to use a less activist approach, explicitly because they lack confidence that their models will actually predict where the economy is going, or the effect of any shock upon it. The new, more humble, approach sees danger in dramatic policy changes based on model predictions, because of several practical and theoretical limitations in current macroeconomic models; in addition to the theoretical pitfalls, (listed above) some problems specific to aggregate modelling are:\n\nComplex systems specialist and mathematician David Orrell wrote on this issue in his book Apollo's Arrow and explained that the weather, human health and economics use similar methods of prediction (mathematical models). Their systems—the atmosphere, the human body and the economy—also have similar levels of complexity. He found that forecasts fail because the models suffer from two problems : (i) they cannot capture the full detail of the underlying system, so rely on approximate equations; (ii) they are sensitive to small changes in the exact form of these equations. This is because complex systems like the economy or the climate consist of a delicate balance of opposing forces, so a slight imbalance in their representation has big effects. Thus, predictions of things like economic recessions are still highly inaccurate, despite the use of enormous models running on fast computers.\n\nEconomic and meteorological simulations may share a fundamental limit to their predictive powers: chaos. Although the modern mathematical work on chaotic systems began in the 1970s the danger of chaos had been identified and defined in \"Econometrica\" as early as 1958:\n\nIt is straightforward to design economic models susceptible to butterfly effects of initial-condition sensitivity.\n\nHowever, the econometric research program to identify which variables are chaotic (if any) has largely concluded that aggregate macroeconomic variables probably do not behave chaotically. This would mean that refinements to the models could ultimately produce reliable long-term forecasts. However the validity of this conclusion has generated two challenges:\n\nMore recently, chaos (or the butterfly effect) has been identified as less significant than previously thought to explain prediction errors. Rather, the predictive power of economics and meteorology would mostly be limited by the models themselves and the nature of their underlying systems (see Comparison with models in other sciences above).\n\nA key strand of free market economic thinking is that the market's invisible hand guides an economy to prosperity more efficiently than central planning using an economic model. One reason, emphasized by Friedrich Hayek, is the claim that many of the true forces shaping the economy can never be captured in a single plan. This is an argument that cannot be made through a conventional (mathematical) economic model, because it says that there are critical systemic-elements that will always be omitted from any top-down analysis of the economy.\n\n\n\n\n"}
{"id": "39228396", "url": "https://en.wikipedia.org/wiki?curid=39228396", "title": "Equivalence principle (geometric)", "text": "Equivalence principle (geometric)\n\nThe equivalence principle is one of the corner-stones of gravitation theory. Different formulations of the equivalence principle are labeled \"weakest\", \"weak\", \"middle-strong\" and \"strong.\" All of these formulations are based on the empirical equality of inertial mass, gravitational active and passive charges.\n\nThe \"weakest\" equivalence principle is restricted to the motion law of a probe point mass in a uniform gravitational field. Its localization is the \"weak\" equivalence principle that states the existence of a desired local inertial frame at a given world point. This is the case of equations depending on a gravitational field and its first order derivatives, e. g., the equations of mechanics of probe point masses, and the equations of electromagnetic and Dirac fermion fields. The \"middle-strong\" equivalence principle is concerned with any matter, except a gravitational field, while the \"strong\" one is applied to all physical laws.\n\nThe above-mentioned variants of the equivalence principle aim to guarantee the transition of General Relativity to Special Relativity in a certain reference frame. However, only the particular \"weakest\" and \"weak\" equivalence principles are true. \nTo overcome this difficulty, the equivalence principle can be formulated in geometric terms as follows.\n\nIn the spirit of Felix Klein's Erlanger program, Special Relativity can be characterized as the Klein geometry of Lorentz group invariants. Then the geometric equivalence principle is formulated to require the existence of Lorentz invariants on a world manifold formula_1. This requirement holds if the tangent bundle formula_2 of formula_1 admits an atlas with Lorentz transition functions, i.e., a structure group of the associated frame bundle formula_4 of linear tangent frames in formula_2 is reduced to the Lorentz group formula_6. By virtue of the well known theorem on structure group reduction, this reduction takes place if and only if the quotient bundle formula_7 possesses a global section, which is a pseudo-Riemannian metric on formula_1.\n\nThus the geometric equivalence principle provides the necessary and sufficient conditions of the existence of a pseudo-Riemannian metric, i.e., a gravitational field on a world manifold.\n\nBased on the geometric equivalence principle, gravitation theory is formulated as gauge theory where a gravitational field is described as a classical Higgs field responsible for spontaneous breakdown of space-time symmetries.\n\n\n"}
{"id": "14162696", "url": "https://en.wikipedia.org/wiki?curid=14162696", "title": "Fluid Concepts and Creative Analogies", "text": "Fluid Concepts and Creative Analogies\n\nFluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought is a 1995 book by Douglas Hofstadter and other members of the Fluid Analogies Research Group exploring the mechanisms of intelligence through computer modeling. It contends that the notions of analogy and fluidity are fundamental to explain how the human mind solves problems and to create computer programs that show intelligent behavior. It analyzes several computer programs that members of the group have created over the years to solve problems that require intelligence.\n\nIt was the first book ever sold by Amazon.com.\n\nThe book is a collection of revised articles that appeared in precedence, each preceded by an introduction by Hofstadter.\nThey describe the scientific work by him and his collaborators in the 1980s and 1990s.\nThe project started in the late 1970s at Indiana University.\nIn 1983 he took a sabbatical year at MIT, working in Marvin Minsky's Artificial Intelligence Lab.\nThere he met and collaborated with Melanie Mitchell, who then became his doctoral student.\nSubsequently, Hofstadter moved to the University of Michigan, where the FARG (Fluid Analogies Research Group) was founded.\nEventually he returned to Indiana University in 1988, continuing the FARG research there.\nThe book was written during a sabbatical year at the Istituto per la Ricerca Scientifica e Tecnologica in Trento, Italy.\n\nUpon publication, Jon Udell, a BYTE senior technical editor-at-large said:\nFifteen years ago, \"Gödel, Escher, Bach: An Eternal Golden Braid\" exploded on the literary scene, earning its author a Pulitzer prize and a monthly column in \"Scientific American\". Douglas Hofstadter's exuberant synthesis of math, music, and art, and his inspired thought experiments with \"tangled hierarchy,\" recursion, pattern recognition, figure/ground reversal, and self-reference, delighted armchair philosophers and AI theorists. But in the end, many people believed that these intellectual games yielded no useful model of cognition on which to base future AI research. Now \"Fluid Concepts and Creative Analogies\" presents that model, along with the computer programs Hofstadter and his associates have designed to test it. These programs work in stripped-down yet surprisingly rich microdomains.\n\nOn April 3, 1995, \"Fluid Concepts and Creative Analogies\" became the first book ordered online by an Amazon.com customer.\n\n\nThe first AI project by Hofstadter stemmed from his teenage fascination with number sequences.\nWhen he was 17, he studied the way that triangular and square numbers interleave, and eventually found a recursive relation describing it.\nIn his first course on AI, he set to the students and to himself the task of writing a program that could extrapolate the rule by which a numeric sequence is generated.\nHe discusses breadth-first and depth-first techniques, but eventually concludes that the results represent expert systems that incarnate a lot of technical knowledge but don't shine much light on the mental processes that humans use to solve such puzzles.\n\nInstead he devised a simplified version of the problem, called SeekWhence, where sequences are based on very simple basic rules not requiring advanced mathematical knowledge.\nHe argues that pattern recognition, analogy, and fluid working hypotheses are fundamental to understand how humans tackle such problems.\n\nJumbo is a program to solve jumbles, word puzzles consisting in five or six scrambled letters that need to be anagrammed to form an English word.\nThe resulting word does not need to be a real one but just to a plausible, that is, to consists of a sequence of letters that is normal in English.\n\nThe constituent elements of Jumbo are the following:\nA \"temperature\" is associated to the present state of the cytoplasm; it determines how probable it is that a destructive codelet is executed.\nThere is a \"freezing\" temperature at which no destruction can occur anymore: a solution has been found.\n\nNumbo is a program by Daniel Defays that tries to solve numerical problems similar to those used in the French game \"Le compte est bon\". The game consists in combining some numbers called \"bricks\", using the operations of multiplication, addition, and subtraction, to obtain a given result.\n\nThe program is modeled on Jumbo and Copycat and uses a permanent network of known mathematical facts, a working memory in the form of a cytoplasm, and a coderack containing codelets to produce free associations of bricks in order to arrive at the result.\n\nThe chapter subtitle \"A Critique of Artificial-intelligence Methodology\" indicates that this is a polemical article, in which David Chalmers, Robert French, and Hofstadter criticize most of the research going on at that time (the early '80s) as exaggerating results and missing the central features of human intelligence.\n\nSome of these AI projects, like the structure mapping engine (SME), claimed to model high faculties of the human mind and to be able to understand literary analogies and to rediscover important scientific breakthroughs.\nIn the introduction, Hofstadter warns about the Eliza effect that leads people to attribute understanding to a computer program that only uses a few stock phrases.\nThe authors claim that the input data for such impressive results are already heavily structured in the direction of the intended discovery and only a simple matching task is left to the computer.\n\nTheir main claim is that it is impossible to model high-level cognition without at the same time modeling low-level perception.\nWhile cognition is necessarily based on perception, they argue that it in turn influences perception itself.\nTherefore, a sound AI project should try to model the two together.\nIn a slogan repeated several times throughout the book: \"cognition is recognition\".\n\nSince human perception is too complex to be modelled by available technology, they favor the restriction of AI projects to limited domains like the one used for the Copycat project.\n\nThis chapter presents, as stated in the full title, \"A Model of Mental Fluidity and Analogy-making\".\nIt is a description of the architecture of the Copycat program, developed by Hofstadter and Melanie Mitchell.\nThe field of application of the program is a domain of short alphabetic sequences.\nA typical puzzle is: \"If abc were changed to abd, how would you change ijk in the same way?\".\nThe program tries to find an answer using a strategy supposedly similar to the way the human mind tackles the question.\n\nCopycat has three major components:\nThe resulting software displays emergent properties.\nIt works according to a \"parallel terraced scan\" that runs several possible processes at the same time.\nIt shows mental fluidity in that concepts may \"slip\" into similar ones.\nIt emulates human behavior in tending to find the most obvious solutions most of the time but being more satisfied (as witnessed by low temperature) by more clever and deep answers that it finds more rarely.\n\nThis chapter compares Copycat with other recent (at the time) work in artificial intelligence.\nSpecifically, it matches it with the claimed results from the structure mapping engine SME and the Analogical Constraint Mapping Engine (ACME).\nThe authors' judgment is that those programs suffer from two defects: Their input is pre-structured by the developers to highlight the analogies that the software is supposed to find; and the general architecture of the programs is serial and deterministic rather than parallel and stochastic like Copycat's, which they consider psychologically more plausible.\n\nSevere criticism is put on the claim that these tools can solve \"real-life\" problems.\nIn fact, only the terms used in the example suggest that the input to the programs comes from a concrete situation.\nThe logical structures don't actually imply any meaning for the term.\n\nFinally a more positive assessment is given to two other projects: Indurkhya' PAN model and Kokinov's AMBR system.\n\nThis chapter looks at those aspects of human creativity that are not yet modeled by Copycat and lays down a research plan for a future extension of the software.\nThe main missing element is the mind's ability to observe itself and reflect on its own thinking process.\nAlso important is the ability to learn and to remember the results of the mental activity.\n\nThe creativity displayed in finding analogies should be applicable at ever higher levels: making analogies between analogies (expression inspired by the title of a book by Stanislaw Ulam), analogies between these second-order analogies, and so on.\n\nAnother of Hofstadter's students, Robert French, was assigned the task of applying the architecture of Copycat to a different domain, consisting in analogies between objects lying on a table in a coffeehouse.\nThe resulting program was named Tabletop.\n\nThe authors present a different and vaster domain to justify the relevance of attacking such a trivial-seeming project.\nThe alternative domain is called Ob-Platte and consists in discovering analogies between geographical locations in different regions or countries.\n\nOnce again arguments are offered against a brute-force approach, which would work on the small Tabletop domain but would become unfeasible on the larger Ob-Platte domain.\nInstead a parallel non-deterministic architecture is used, similar to the one adopted by the Copycat project.\n\nIn the premise to the chapter, title \"The Knotty Problem of Evaluating Research\", Hofstadter considers the question of how research in AI should be assessed.\nHe argues against a strict adherence to a match between the results of an AI program with the average answer of human test subjects.\nHe gives two reasons for his rejection: the AI program is supposed to emulate creativity, while an average of human responses will delete any original insight by any of the single subjects; and the architecture of the program should be more important that its mere functional description.\n\nIn the main article, the architecture of Tabletop is described: it is strongly inspired by that of Copycat and consists of a Slipnet, a Workspace, and a Corerack.\n\nThis last chapter is about a more ambitious project that Hofstadter started with student Gary McGraw.\nThe microdomain used is that of grid fonts: typographic alphabets constructed using a rigid system of small rigid components.\nThe goal is to construct a program that, given only a few or just one letter from the grid font, can generate the whole alphabet \"in the same style\".\nThe difficulty lies in the ambiguity and undefinability of \"style\".\nThe projected program would have a structure very similar to that of Jumble, Numble, Copycat, and Tabletop.\n\nIn the concluding part of the book, Hofstadter analyses some AI projects with a critical eye.\nHe finds that today's AI is missing the gist of human creativity and is making exaggerated claims.\nThe project under scrutiny are the following.\n\nAARON, a computer artist that can draw images of people in outdoor settings in a distinctive style reminiscent of that of a human artist; criticism: the program doesn't have any understanding of the objects it draws, it just uses some graphical algorithms with some randomness thrown in to generate different scenes at every run and to give the style a more natural feel.\n\nRacter, a computer author that wrote a book entitled \"The Policeman's Beard Is Half Constructed\".\nAlthough some of the prose generated by the program is quite impressive, due in part to the Eliza effect, the computer does not have any notion of plot or of the meaning of the words it uses. Furthermore, the book is made up of selected texts from thousands produced by the computer over several years.\n\nAM, a computer mathematician that generates new mathematical concepts. It managed to produce by itself the notion of prime number and the Goldbach conjecture. As with Racter, the question is how much the programmer filtered the output of the program, keeping only the occasional interesting output.\nAlso, mathematics being a very specialized domain, it is doubtful whether the techniques used can be abstracted to general cognition.\n\nAnother mathematical program, called Geometry, was celebrated for making an insightful discovery of an original proof that an isosceles triangle has equal base angles. The proof is based on seeing the triangle in two different ways. However, the program generates all possible ways of seeing the triangle, not even knowing that it is the same triangle.\n\nHofstadter concludes with some methodological remarks on the Turing Test.\nIn his opinion it is still a good definition and he argues that by interacting with a program, a human may be able to have insight not just on its behaviour but also on its structure.\nHowever, he criticises the use that is made of it at present: it encourages the development of fancy natural-language interfaces instead of the investigation of deep cognitive faculties.\n"}
{"id": "1973470", "url": "https://en.wikipedia.org/wiki?curid=1973470", "title": "Fuzzy concept", "text": "Fuzzy concept\n\nA fuzzy concept is a concept of which the boundaries of application can vary considerably according to context or conditions, instead of being fixed once and for all. This means the concept is vague in some way, lacking a fixed, precise meaning, without however being unclear or meaningless altogether. It has a definite meaning, which can be made more precise only through further elaboration and specification - including a closer definition of the context in which the concept is used. The study of the characteristics of fuzzy concepts and fuzzy language is called \"fuzzy semantics\". The inverse of a \"fuzzy concept\" is a \"crisp concept\" (i.e. a precise concept). \n\nA fuzzy concept is understood by scientists as a concept which is \"to an extent applicable\" in a situation. That means the concept has \"gradations\" of significance or \"unsharp\" (variable) boundaries of application. A fuzzy statement is a statement which is true \"to some extent\", and that extent can often be represented by a scaled value. The best known example of a fuzzy concept around the world is an amber traffic light, and indeed fuzzy concepts are widely used in traffic control systems. The term is also used these days in a more general, popular sense - in contrast to its technical meaning - to refer to a concept which is \"rather vague\" for any kind of reason.\n\nIn the past, the very idea of reasoning with fuzzy concepts faced considerable resistance from academic elites. They did not want to endorse the use of imprecise concepts in research or argumentation. Yet although people might not be aware of it, the use of fuzzy concepts has risen gigantically in all walks of life from the 1970s onward. That is mainly due to advances in electronic engineering, fuzzy mathematics and digital computer programming. The new technology allows very complex inferences about \"variations on a theme\" to be anticipated and fixed in a program. \n\nThe new neuro-fuzzy computational methods make it possible, to identify, to measure and respond to fine gradations of significance, with great precision. It means that practically useful concepts can be coded and applied to all kinds of tasks, even if, ordinarily, these concepts are never precisely defined. Nowadays engineers, statisticians and programmers often represent fuzzy concepts mathematically, using fuzzy logic, fuzzy values, fuzzy variables and fuzzy sets.\n\nProblems of vagueness and fuzziness have probably always existed in human experience. The boundary between different things can appear blurry. Sometimes people have to think, when they are not in the best frame of mind to do it, or, they have to talk about something out there, which just isn't sharply defined. Across time, however, philosophers and scientists began to reflect about those kinds of problems, in much more systematic ways.\n\nThe ancient Sorites paradox first raised the logical problem of how we could exactly define the threshold at which a change in quantitative gradation turns into a qualitative or categorical difference. With some physical processes this threshold is relatively easy to identify. For example, water turns into steam at 100 °C or 212 °F (the boiling point depends partly on atmospheric pressure, which decreases at higher altitudes). \n\nWith many other processes and gradations, however, the point of change is much more difficult to locate, and remains somewhat vague. Thus, the boundaries between qualitatively different things may be \"unsharp\": we know that there are boundaries, but we cannot define them exactly. \n\nAccording to the modern idea of the continuum fallacy, the fact that a statement is to an extent vague, does not automatically mean that it is invalid. The problem then becomes one of how we could ascertain the kind of validity that the statement does have.\n\nThe Nordic myth of Loki's wager suggested that concepts that lack precise meanings or precise boundaries of application cannot be usefully discussed at all. However, the 20th century idea of \"fuzzy concepts\" proposes that \"somewhat vague terms\" can be operated with, since we can explicate and define the variability of their application, by assigning numbers to gradations of applicability. This idea sounds simple enough, but it had large implications.\n\nThe intellectual origins of the species of fuzzy concepts as a logical category have been traced back to a diversity of famous and less well-known thinkers, including (among many others) Eubulides, Plato, Cicero, Georg Wilhelm Friedrich Hegel, Karl Marx and Friedrich Engels, Friedrich Nietzsche, Hugh MacColl, Charles S. Peirce, Max Black, Jan Łukasiewicz, Emil Leon Post, Alfred Tarski, Georg Cantor, Nicolai A. Vasiliev, Kurt Gödel, Stanisław Jaśkowski and Donald Knuth. \n\nAcross at least two and a half millennia, all of them had something to say about graded concepts with unsharp boundaries. This suggests at least that the awareness of the existence of concepts with \"fuzzy\" characteristics, in one form or another, has a very long history in human thought. Quite a few logicians and philosophers have also tried to \"analyze\" the characteristics of fuzzy concepts as a recognized species, sometimes with the aid of some kind of many-valued logic or substructural logic.\n\nAn early attempt in the post-WW2 era to create a theory of sets where set membership is a matter of degree was made by Abraham Kaplan and Hermann Schott in 1951. They intended to apply the idea to empirical research. Kaplan and Schott measured the degree of membership of empirical classes using real numbers between 0 and 1, and they defined corresponding notions of intersection, union, complementation and subset. However, at the time, their idea \"fell on stony ground\". J. Barkley Rosser Sr. published a treatise on many-valued logics in 1952, anticipating \"many-valued sets\". Another treatise was published in 1963 by Aleksandr A. Zinov'ev and others\n\nIn 1964, the American philosopher William Alston introduced the term \"degree vagueness\" to describe vagueness in an idea that results from the absence of a definite cut-off point along an implied scale (in contrast to \"combinatory vagueness\" caused by a term that has a number of logically independent conditions of application). \n\nThe German mathematician Dieter Klaua published a German-language paper on fuzzy sets in 1965, but he used a different terminology (he referred to \"many-valued sets\", not \"fuzzy sets\").\n\nTwo popular introductions to many-valued logic in the late 1960s were by Robert J. Ackermann and Nicholas Rescher respectively. Rescher’s book includes a bibliography on fuzzy theory up to 1965, which was extended by Robert Wolf for 1966-1974. Haack provides references to significant works after 1974. Bergmann provides a more recent (2008) introduction to fuzzy reasoning.\n\nUsually the Iranian-born American computer scientist Lotfi A. Zadeh (1921-2017) is credited with inventing the specific idea of a \"fuzzy concept\" in his seminal 1965 paper on fuzzy sets, because he gave a formal mathematical presentation of the phenomenon that was widely accepted by scholars.<ref>Lotfi A. Zadeh, \"Fuzzy sets\". In: \"Information and Control\", Vol. 8, June 1965, pp. 338–353.\n\n"}
{"id": "18562346", "url": "https://en.wikipedia.org/wiki?curid=18562346", "title": "Gossen's laws", "text": "Gossen's laws\n\nGossen's laws, named for Hermann Heinrich Gossen (1810 – 1858), are three laws of economics:\n\nThe citation referenced is the translation by Nicholas Georgescu-Roegen in which the traslator names only two laws: 1) ”If an enjoyment is experienced uninterruptedly, the corresponding intensity of pleasure decreases continuously until satiety is ultimately reached, at which point the intensity becomes nil.\" and, 2) \"A similar decrease of the intensity of pleasure takes place if a previous enjoyment of the same kind of pleasure is repeated. Not only does the initial intensity of pleasure become smaller but also the duration of the enjoyment becomes shorter, so that satiety is reached sooner. Moreover, the sooner the repetition, the smaller becomes the initial intensity as well as the duration of the enjoyment.\" (p.lxxx)\n\n\n"}
{"id": "7949372", "url": "https://en.wikipedia.org/wiki?curid=7949372", "title": "Humanitarian principles", "text": "Humanitarian principles\n\nThere are a number of meanings for the term humanitarian. Here humanitarian pertains to the practice of saving lives and alleviating suffering. It is usually related to emergency response (also called humanitarian response) whether in the case of a natural disaster or a man-made disaster such as war or other armed conflict. Humanitarian principles govern the way humanitarian response is carried out.\n\nThe principle of humanity means that all humankind shall be treated humanely and equally in all circumstances by saving lives and alleviating suffering, while ensuring respect for the individual. It is the fundamental principle of humanitarian response.\n\nThe Code of Conduct for the International Red Cross and Red Crescent Movement and NGOs in Disaster Relief (RC/NGO Code) introduces the concept of the humanitarian imperative which expands the principle of humanity to include the right to receive and to give humanitarian assistance. It states the obligation of the international community \"to provide humanitarian assistance wherever it is needed.\"\n\nProvision of humanitarian assistance must be impartial and not based on nationality, race, religion, or political point of view. It must be based on need alone.\n\nFor most non-governmental humanitarian agencies (NGHAs), the principle of impartiality is unambiguous even if it is sometimes difficult to apply, especially in rapidly changing situations. However, it is no longer clear which organizations can claim to be humanitarian. For example, companies like PADCO, a USAID subcontractor, is sometimes seen as a humanitarian NGO. However, for the UN agencies, particularly where the UN is involved in peace keeping activities as the result of a Security Council resolution, it is not clear if the UN is in position to act in an impartial manner if one of the parties is in violation of terms of the UN Charter.\n\nHumanitarian agencies must formulate and implement their own policies independently of government policies or actions.\n\nProblems may arise because most NGHAs rely in varying degrees on government donors. Thus for some organizations it is difficult to maintain independence from their donors and not be confused in the field with governments who may be involved in the hostilities. The ICRC, has set the example for maintaining its independence (and neutrality) by raising its funds from governments through the use of separate annual appeals for headquarters costs and field operations.\n\nThe core principles are defining characteristics, the necessary conditions for humanitarian response. Organizations such as military forces and for-profit companies may deliver assistance to communities affected by disaster in order to save lives and alleviate suffering, but they are not considered by the humanitarian sector as humanitarian agencies as their response is not based on the core principles.\n\nIn addition to the core principles, there are other principles that govern humanitarian response for specific types of humanitarian agencies such as UN agencies, the Red Cross and Red Crescent Movement, and NGOs.\n\nThe International Red Cross and Red Crescent Movement follows, in addition to the above core principles, the principle of neutrality. For the Red Cross, neutrality means not to take sides in hostilities or engage at any time in controversies of a political, racial, religious or ideological nature.\n\nThe principle of neutrality was specifically addressed to the Red Cross Movement to prevent it from not only taking sides in a conflict, but not to \"engage at any time in controversies of a political, racial, religious or ideological nature.\" The principle of neutrality was left out of the Red Cross/NGO code because some of the NGHAs, while committed to giving impartial assistance, were not ready to forgo their lobbying on justice issues related to political and ideological questions.\n\nUnited Nations General Assembly Resolution 46/182 lists the principle of neutrality, alongside the principles of humanity and impartiality in its annex as a guide to the provision of humanitarian assistance. The resolution is designed to strengthen human response of the UN system, and it clearly applies to the UN agencies.\n\nNeutrality can also apply to humanitarian actions of a state. \"Neutrality remains closely linked with the definition which introduced the concept into international law to designate the status of a State which decided to stand apart from an armed conflict. Consequently, its applications under positive law still depend on the criteria of abstention and impartiality which have characterized neutrality from the outset.\"\n\nThe application of the word neutrality to humanitarian aid delivered by UN agencies or even governments can be confusing. GA Resolution 46/182 proclaims the principle of neutrality, yet as an inter-governmental political organization, the UN is often engaged in controversies of a political nature. According to this interpretation, the UN agency or a government can provide neutral humanitarian aid as long as it does it impartially, based upon need alone.\n\nToday, the word neutrality is widely used within the humanitarian community, usually to mean the provision of humanitarian aid in an impartial and independent manner, based on need alone. Few international NGOs have curtailed work on justice or human rights issues because of their commitment to neutrality.\n\nThe provision of aid must not exploit the vulnerability of victims and be used to further political or religious creeds. All of the major non-governmental humanitarian agencies (NGHAs) by signing up to the RC/NGO Code of Conduct have committed themselves not to use humanitarian response to further political or religious creeds.\n\nAll of the above principles are important requirements for effective field operations. They are based on widespread field experience of agencies engaged in humanitarian response. In conflict situations, their breach may drastically affect the ability of agencies to respond to the needs of the victims.\n\nIf a warring party believes, for example, that an agency is favoring the other side, or that it is an agent of the enemy, access to the victims may be blocked and the lives of humanitarian workers may be put in danger. If one of the parties perceives that an agency is trying to spread another religious faith, there may be a hostile reaction to their activities.\n\nThe core principles, found in the Red Cross/NGO Code of Conduct and in GA Resolution 46/182 are derived from the Fundamental Principles of the Red Cross, particularly principles I (humanity), II (impartiality), III (neutrality—in the case of the UN), and IV (independence).\n\nAccountability has been defined as: \"the processes through which an organisation makes a \ncommitment to respond to and balance the needs of stakeholders in its decision making processes and activities, and delivers against this commitment.\" Humanitarian Accountability Partnership International adds: \"Accountability is about using power responsibly.\"\n\nArticle 9 of the Code of Conduct for the International Red Cross and Red Crescent Movement and NGOs in Disaster Relief states:\n\"We hold ourselves accountable to both those we seek to assist and those from whom we accept resources;\" and thus identifies the two major stake holders: donors and beneficiaries. However, traditionally humanitarian agencies have tended to practice mainly \"upward accountability\", i.e. to their donors.\n\nThe experience of many humanitarian agencies during the Rwandan Genocide, led to a number of initiatives designed to improve humanitarian assistance and accountability, particularly with respect to the beneficiaries. Examples include the Sphere Project, ALNAP, Compas, the People In Aid Code of Good Practice, and the Humanitarian Accountability Partnership International, which runs a \"global quality insurance scheme for humanitarian agencies.\"\n\nThe RC/NGO Code also lists a number of more aspirational principles which are derived from experience with development assistance.\n\nThe Sphere Project Humanitarian Charter uses the language of human rights to remind that the right to life which is proclaimed in both the Universal Declaration of Human Rights and the International Convention on Civil and Political Rights is related to human dignity.\n\nHumanitarian principles are mainly focused on the behavior of organizations. However a humane response implies that humanitarian workers are not to take advantage of the vulnerabilities of those affected by war and violence. Agencies have the responsibility for developing rules of staff conduct which prevent abuse of the beneficiaries.\n\nOne of the most problematic areas is related to the issue of sexual exploitation and abuse of beneficiaries by humanitarian workers. In an emergency where victims have lost everything, women and girls are particularly vulnerable to sexual abuse.\n\nA number of reports which identified the sexual exploitation of refugees in west Africa prodded the humanitarian community to work together in examining the problem and to take measures to prevent abuses. In July 2002, the UN's Interagency Standing Committee (IASC) adopted a plan of action which stated: Sexual exploitation and abuse by humanitarian workers constitute acts of gross misconduct and are therefore grounds for termination of employment. The plan explicitly prohibited the \"Exchange of money, employment, goods, or services for sex, including sexual favours or other forms of humiliating, degrading or exploitative behaviour.\" The major NGHAs as well the UN agencies engaged in humanitarian response committed themselves to setting up internal structures to prevent sexual exploitation and abuse of beneficiaries.\n\nSubstantial efforts have been made in the humanitarian sector to monitor compliance with humanitarian principles. Such efforts include The People In Aid Code of Good Practice, an internationally recognised management tool that helps humanitarian and development organisations enhance the quality of their human resources management. The NGO, Humanitarian Accountability Partnership International, is also working to make humanitarian organizations more accountable, especially to the beneficiaries.\n\nStructures internal to the Red Cross Movement monitor compliance to the Fundamental Principles of the Red Cross.\n\nThe RC/NGO Code is self-enforcing. The SCHR carries out peer reviews among its members which look in part at the issue of compliance with principles set out in the RC/NGO Code\n\n"}
{"id": "41627505", "url": "https://en.wikipedia.org/wiki?curid=41627505", "title": "Imago Universi", "text": "Imago Universi\n\nAndreas Cellarius, German mathematician and cartographer (1596–1665), conceived an Atlas of the Universe, published in 1660, under the title of \"Harmonia Macrocosmica\". Numerous illustrations of the solar system appear in this atlas by different authors known at that time. Referring to Ptolemy, Cellarius called the representation of this Ptolemaic conception of heaven as \"Imago universi secundum Ptolaeum\"\n\n\"Imago\" is a word in Latin which means\" 'image\"' or even \"representation\". Therefore, the title expresses the \"Picture of the Universe according to Ptolemy.\" The Latin expression was used in the Middle Ages to express the representation and size of the known world at that time.\n\n\"Imago Universi\" is also the title, in Latin, of a cosmographic treatise, written in 2013 by the Spanish scientist Gabriel Barceló.\n\nAfter analyzing the history of cosmology, the treatise delves into the prevailing scientific lack of explanation of the rotation of the heavenly bodies in the laws of dynamic behaviour of the sidereal system. The author proposes the application of the Theory of Dynamic Interactions (TID) to astrophysics, in particular, the dynamics of stellar systems and galaxies. This theory allows new comprehension of the dynamics of nature and understands the dynamic equilibrium of the universe, always subjected to rotational accelerations, but repetitive and persistent. The author also highlights that the orbiting always coincides with the intrinsic rotation of celestial bodies. Paradox incorporating the book, noting that this had not been found to date.\n\n\n1. Einstein, Albert: The Origins of the General Theory of Relativity, lecture given at the George A. Foundation Gibson, University of Glasgow, 20 June 1933. Published by Jackson, Wylie and co, Glasgow, 1933.\n\n"}
{"id": "30679665", "url": "https://en.wikipedia.org/wiki?curid=30679665", "title": "Inherent bad faith model", "text": "Inherent bad faith model\n\nThe inherent bad faith model of information processing is a theory in political psychology that was first put forth by Ole Holsti to explain the relationship between John Foster Dulles' beliefs and his model of information processing. \n\nIt is the most widely studied model of one's opponent. A state is presumed to be implacably hostile, and contra-indicators of this are ignored. They are dismissed as propaganda ploys or signs of weakness. An example is John Foster Dulles' position regarding the Soviet Union.\n\n"}
{"id": "13793087", "url": "https://en.wikipedia.org/wiki?curid=13793087", "title": "KK thesis", "text": "KK thesis\n\nThe KK thesis or KK principle is a principle of epistemic logic which states that \"If you know that p is the case then you know that you know that p is the case.\" In formal notation the principle can be stated as: \"Kp→KKp\" (literally: \"Knowing p implies the knowing of knowing p\").\n\n\n"}
{"id": "939578", "url": "https://en.wikipedia.org/wiki?curid=939578", "title": "List of eponymous laws", "text": "List of eponymous laws\n\nThis list of eponymous laws provides links to articles on laws, principles, adages, and other succinct observations or predictions named after a person. In some cases the person named has coined the law – such as Parkinson's law. In others, the work or publications of the individual have led to the law being so named – as is the case with Moore's law. There are also laws ascribed to individuals by others, such as Murphy's law; or given eponymous names despite the absence of the named person.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "182727", "url": "https://en.wikipedia.org/wiki?curid=182727", "title": "Mach's principle", "text": "Mach's principle\n\nIn theoretical physics, particularly in discussions of , Mach's principle (or Mach's conjecture) is the name given by Einstein to an imprecise hypothesis often credited to the physicist and philosopher Ernst Mach. The idea is that the existence of absolute rotation (the distinction of local inertial frames vs. rotating reference frames) is determined by the large-scale distribution of matter, as exemplified by this anecdote:\n\nYou are standing in a field looking at the stars. Your arms are resting freely at your side, and you see that the distant stars are not moving. Now start spinning. The stars are whirling around you and your arms are pulled away from your body. Why should your arms be pulled away when the stars are whirling? Why should they be dangling freely when the stars don't move?\nMach's principle says that this is not a coincidence—that there is a physical law that relates the motion of the distant stars to the local inertial frame. If you see all the stars whirling around you, Mach suggests that there is some physical law which would make it so you would feel a centrifugal force. There are a number of rival formulations of the principle. It is often stated in vague ways, like \"mass out there influences inertia here\". A very general statement of Mach's principle is \"local physical laws are determined by the large-scale structure of the universe\".\n\nThis concept was a guiding factor in Einstein's development of the general theory of relativity. Einstein realized that the overall distribution of matter would determine the metric tensor, which tells you which frame is rotationally stationary. Frame-dragging and conservation of gravitational angular momentum makes this into a true statement in the general theory in certain solutions. But because the principle is so vague, many distinct statements can be (and have been) made that would qualify as a Mach principle, and some of these are false. The Gödel rotating universe is a solution of the field equations that is designed to disobey Mach's principle in the worst possible way. In this example, the distant stars seem to be revolving faster and faster as one moves further away. This example doesn't completely settle the question, because it has closed timelike curves.\n\nThe basic idea also appears before Mach's time, in the writings of George Berkeley. The book \"Absolute or Relative Motion?\" (1896) by Benedict Friedländer and his brother Immanuel contained ideas similar to Mach's principle.\n\nThere is a fundamental issue in relativity theory. If all motion is relative, how can we measure the inertia of a body? We must measure the inertia with respect to something else. But what if we imagine a particle completely on its own in the universe? We might hope to still have some notion of its state of motion. Mach's principle is sometimes interpreted as the statement that such a particle's state of motion has no meaning in that case.\n\nIn Mach's words, the principle is embodied as follows:\n\nAlbert Einstein seemed to view Mach's principle as something along the lines of:\n\nIn this sense, at least some of Mach's principles are related to philosophical holism. Mach's suggestion can be taken as the injunction that gravitation theories should be relational theories. Einstein brought the principle into mainstream physics while working on general relativity. Indeed, it was Einstein who first coined the phrase \"Mach's principle\". There is much debate as to whether Mach really intended to suggest a new physical law since he never states it explicitly.\n\nThe writing in which Einstein found inspiration from Mach was \"The Science of Mechanics\", where the philosopher criticized Newton's idea of absolute space, in particular the argument that Newton gave sustaining the existence of an advantaged reference system: what is commonly called \"Newton's bucket argument\".\n\nIn his \"Philosophiae Naturalis Principia Mathematica\", Newton tried to demonstrate that:\n\nMach, in his book, says that:\n\nThis same thought had been expressed by the philosopher George Berkeley in his \"De Motu\". It is then not clear, in the passages from Mach just mentioned, if the philosopher intended to formulate a new kind of physical action between heavy bodies. This physical mechanism should determine the inertia of bodies, in a way that the heavy and distant bodies of our universe should contribute the most to the inertial forces. More likely, Mach only suggested a mere \"redescription of motion in space as experiences that do not invoke the term \"space\"\". What is certain is that Einstein interpreted Mach's passage in the former way, originating a long-lasting debate.\n\nMost physicists believe Mach's principle was never developed into a quantitative physical theory that would explain a mechanism by which the stars can have such an effect. It was never made clear by Mach himself exactly what his principle was. Although Einstein was intrigued and inspired by Mach's principle, Einstein's formulation of the principle is not a fundamental assumption of general relativity.\n\nBecause intuitive notions of distance and time no longer apply, what exactly is meant by \"Mach's principle\" in general relativity is even less clear than in Newtonian physics and at least 21 formulations of Mach's principle are possible, some being considered more strongly Machian than others. A relatively weak formulation is the assertion that the motion of matter in one place should affect which frames are inertial in another.\n\nEinstein, before completing his development of the general theory of relativity, found an effect which he interpreted as being evidence of Mach's principle. We assume a fixed background for conceptual simplicity, construct a large spherical shell of mass, and set it spinning in that background. The reference frame in the interior of this shell will precess with respect to the fixed background. This effect is known as the Lense–Thirring effect. Einstein was so satisfied with this manifestation of Mach's principle that he wrote a letter to Mach expressing this:\nThe Lense–Thirring effect certainly satisfies the very basic and broad notion that \"matter there influences inertia here\". The plane of the pendulum would not be dragged around if the shell of matter were not present, or if it were not spinning. As for the statement that \"inertia originates in a kind of interaction between bodies\", this too could be interpreted as true in the context of the effect.\n\nMore fundamental to the problem, however, is the very existence of a fixed background, which Einstein describes as \"the fixed stars\". Modern relativists see the imprints of Mach's principle in the initial-value problem. Essentially, we humans seem to wish to separate spacetime into slices of constant time. When we do this, Einstein's equations can be decomposed into one set of equations, which must be satisfied on each slice, and another set, which describe how to move between slices. The equations for an individual slice are elliptic partial differential equations. In general, this means that only part of the geometry of the slice can be given by the scientist, while the geometry everywhere else will then be dictated by Einstein's equations on the slice.\n\nIn the context of an asymptotically flat spacetime, the boundary conditions are given at infinity. Heuristically, the boundary conditions for an asymptotically flat universe define a frame with respect to which inertia has meaning. By performing a Lorentz transformation on the distant universe, of course, this inertia can also be transformed.\n\nA stronger form of Mach's principle applies in Wheeler–Mach–Einstein spacetimes, which require spacetime to be spatially compact and globally hyperbolic. In such universes Mach's principle can be stated as \"the distribution of matter and field energy-momentum (and possibly other information) at a particular moment in the universe determines the inertial frame at each point in the universe\" (where \"a particular moment in the universe\" refers to a chosen Cauchy surface).\n\nThere have been other attempts to formulate a theory that is more fully Machian, such as the Brans–Dicke theory and the Hoyle–Narlikar theory of gravity, but most physicists argue that none have been fully successful. At an exit poll of experts, held in Tübingen in 1993, when asked the question \"Is general relativity perfectly Machian?\", 3 respondents replied \"yes\", and 22 replied \"no\". To the question \"Is general relativity with appropriate boundary conditions of closure of some kind very Machian?\" the result was 14 \"yes\" and 7 \"no\".\n\nHowever, Einstein was convinced that a valid theory of gravity would necessarily have to include the relativity of inertia:\nThe broad notion that \"mass there influences inertia here\" has been expressed in several forms.\nHermann Bondi and Joseph Samuel have listed eleven distinct statements that can be called Mach principles, labelled by \"Mach0\" through \"Mach10\". Though their list is not necessarily exhaustive, it does give a flavor for the variety possible.\n\n\n\n"}
{"id": "26127533", "url": "https://en.wikipedia.org/wiki?curid=26127533", "title": "Marginal abatement cost", "text": "Marginal abatement cost\n\nAbatement cost is the cost of reducing environmental negatives such as pollution. Marginal cost is an economic concept that measures the cost of an additional unit. The marginal abatement cost (MAC), in general, measures the cost of reducing one more unit of pollution.\n\nAlthough marginal abatement costs can be negative, such as when the low carbon option is cheaper than the business-as-usual option, MACs often rise steeply as more pollution is reduced.\n\nMarginal abatement costs are typically used on a marginal abatement cost curve (MACC) or MAC curve, which shows the marginal cost of additional reductions in pollution.\n\nCarbon traders use MAC curves to derive the supply function for modelling carbon price fundamentals. Power companies may employ MAC curves to guide their decisions about long-term capital investment strategies to select among a variety of efficiency and generation options. Economists have used MAC curves to explain the economics of interregional carbon trading. Policy-makers use MAC curves as merit order curves, to analyze how much abatement can be done in an economy at what cost, and where policy should be directed to achieve the emission reductions.\n\nHowever, MAC curves should not be used as abatement supply curves (or merit order curves) to decide which measures to implement in order to achieve a given emission-reduction target. Indeed, the options they list would take decades to implement, and it may be optimal to implement expensive but high-potential measures before introducing cheaper measures.\n\nThe way that MAC curves are usually built has been criticized for lack of transparency and the poor treatment it makes of uncertainty, inter-temporal dynamics, interactions between sectors and ancillary benefits. There is also concern regarding the biased ranking that occurs if some included options have negative costs. \n\nVarious economists, research organizations, and consultancies have produced MAC curves. Bloomberg New Energy Finance and McKinsey & Company have produced economy wide analyses on greenhouse gas emissions reductions for the United States. ICF International produced a California specific curve following AB-32 legislation as have Sweeney and Weyant.\n\nThe Wuppertal Institute for Climate, Environment and Energy produced several marginal abatement cost curves for Germany (also called Cost Potential Curves), depending on the perspective (end-user, utilities, society).\n\nThe US Environmental Protection Agency has done work on a MAC curve for non carbon dioxide emissions such as methane, NO, and HFCs. Enerdata and LEPII-CNRS (France) produce MAC curves with the Prospective Outlook on Long-term Energy Systems (POLES) model for the 6 Kyoto Protocol gases. These curves have been used for various public and private actors either to assess carbon policies or through the use of a carbon market analysis tool.\n\nThe World Bank 2013 low-carbon energy development plan for Nigeria, prepared jointly with the World Bank, ulitizes MAC curves created in Analytica.\n\n"}
{"id": "325499", "url": "https://en.wikipedia.org/wiki?curid=325499", "title": "Marginal cost", "text": "Marginal cost\n\nIn economics, marginal cost is the change in the opportunity cost that arises when the quantity produced is incremented by one unit, that is, it is the cost of producing one more unit of a good. Intuitively, marginal cost at each level of production includes the cost of any additional inputs required to produce the next unit. At each level of production and time period being considered, marginal costs include all costs that vary with the level of production, whereas other costs that do not vary with production are considered fixed. For example, the marginal cost of producing an automobile will generally include the costs of labor and parts needed for the additional automobile and not the fixed costs of the factory that have already been incurred. In practice, marginal analysis is segregated into short and long-run cases, so that, over the long run, all costs (including fixed costs) become marginal.\n\nIf the cost function formula_1 is differentiable, the marginal cost formula_2 is the first derivative of the cost function with respect to the quantity formula_3.\nThe marginal cost can be a function of quantity if the cost function is non-linear. If the cost function is not differentiable, the marginal cost can be expressed as follows.\nwhere formula_6 denotes an incremental change of one unit.\n\nIn the simplest case, the total cost function and its derivative are expressed as follows, where Q represents the production quantity, VC represents variable costs, FC represents fixed costs and TC represents total costs.\n\nSince (by definition) fixed costs do not vary with production quantity, it drops out of the equation when it is differentiated. The important conclusion is that marginal cost \"is not related to\" fixed costs. This can be compared with average total cost or ATC, which is the total cost divided by the number of units produced and \"does\" include fixed costs.\n\nFor discrete calculation without calculus, marginal cost equals the change in total (or variable) cost that comes with each additional unit produced. In contrast, incremental cost is the composition of total cost from the surrogate of contributions, where any increment is determined by the contribution of the cost factors, not necessarily by single units.\n\nFor instance, suppose the total cost of making 1 shoe is $30 and the total cost of making 2 shoes is $40. The marginal cost of producing the second shoe is $40 – $30 = $10.\n\nMarginal cost is not the cost of producing the \"next\" or \"last\" unit. As Silberberg and Suen note, the cost of the last unit is the same as the cost of the first unit and every other unit. In the short run, increasing production requires using more of the variable input — conventionally assumed to be labor. Adding more labor to a fixed capital stock reduces the marginal product of labor because of the diminishing marginal returns. This reduction in productivity is not limited to the additional labor needed to produce the marginal unit – the productivity of every unit of labor is reduced. Thus the costs of producing the marginal unit of output has two components:\nthe cost associated with producing the marginal unit\nand the increase in average costs for all units produced due to the \"damage\" to the entire productive process (∂AC/∂q)q. \nThe first component is the per unit or average cost. The second unit is the small increase in costs due to the law of diminishing marginal returns which increases the costs of all units of sold.\n\nMarginal costs can also be expressed as the cost per unit of labor divided by the marginal product of labor.\n\nBecause formula_11 is the change in quantity of labor that affects a one unit change in output, this implies that this equals formula_12.\n\nwhere MPL is the ratio of increase in the quantity produced per unit increase in labour i.e. ΔQ/ΔL.\nTherefore, formula_13 Since the wage rate is assumed constant, marginal cost and marginal product of labor have an inverse relationship—if marginal cost is increasing (decreasing) the marginal product of labor is decreasing (increasing).\n\nEconomies of scale applies to the long run, a span of time in which all inputs can be varied by the firm so that there are no fixed inputs or fixed costs. Production may be subject to economies of scale (or diseconomies of scale). Economies of scale are said to exist if an additional unit of output can be produced for less than the average of all previous units— that is, if long-run marginal cost is below long-run average cost, so the latter is falling. Conversely, there may be levels of production where marginal cost is higher than average cost, and the average cost is an increasing function of output. For this generic case, minimum average cost occurs at the point where average cost and marginal cost are equal (when plotted, the marginal cost curve intersects the average cost curve from below); this point will \"not\" be at the minimum for marginal cost if fixed costs are greater than 0.\n\nThe portion of the marginal cost curve above its intersection with the average variable cost curve is the supply curve for a firm operating in a perfectly competitive market. (the portion of the MC curve below its intersection with the AVC curve is not part of the supply curve because a firm would not operate at price below the shutdown point) This is not true for firms operating in other market structures. For example, while a monopoly \"has\" an MC curve it does not have a supply curve. In a perfectly competitive market, a supply curve shows the quantity a seller's willing and able to supply at each price – for each price, there is a unique quantity that would be supplied. The one-to-one relationship simply is absent in the case of a monopoly. With a monopoly, there could be an infinite number of prices associated with a given quantity. It all depends on the shape and position of the demand curve and its accompanying marginal revenue curve.\n\nIn perfectly competitive markets, firms decide the quantity to be produced based on marginal costs and sale price. If the sale price is higher than the marginal cost, then they supply the unit and sell it. If the marginal cost is higher than the price, it would not be profitable to produce it. So the production will be carried out until the marginal cost is equal to the sale price. In other words, firms refuse to sell if the marginal cost is greater than the market price.\n\nMarginal costs are not affected by changes in fixed cost. Marginal costs can be expressed as ∆C(q)∕∆Q. Since fixed costs do not vary with (depend on) changes in quantity, MC is ∆VC∕∆Q. Thus if fixed cost were to double, the cost of MC would not be affected, and consequently, the profit-maximizing quantity and price would not change. This can be illustrated by graphing the short run total cost curve and the short-run variable cost curve. The shapes of the curves are identical. Each curve initially increases at a decreasing rate, reaches an inflection point, then increases at an increasing rate. The only difference between the curves is that the SRVC curve begins from the origin while the SRTC curve originates on the y-axis. The distance of the origin of the SRTC above the origin represents the fixed cost – the vertical distance between the curves. This distance remains constant as the quantity produced, Q, increases. MC is the slope of the SRVC curve. A change in fixed cost would be reflected by a change in the vertical distance between the SRTC and SRVC curve. Any such change would have no effect on the shape of the SRVC curve and therefore its slope at any point – MC.\n\nOf great importance in the theory of marginal cost is the distinction between the marginal \"private\" and \"social\" costs. The marginal private cost shows the cost associated to the firm in question. It is the marginal private cost that is used by business decision makers in their profit maximization goals. Marginal social cost is similar to private cost in that it includes the cost of private enterprise but \"also\" any other cost (or offsetting benefit) to society to parties having no direct association with purchase or sale of the product. It incorporates all negative and positive externalities, of both production and consumption. Examples might include a social cost from air pollution affecting third parties or a social benefit from flu shots protecting others from infection.\n\nExternalities are costs (or benefits) that are not borne by the parties to the economic transaction. A producer may, for example, pollute the environment, and others may bear those costs. A consumer may consume a good which produces benefits for society, such as education; because the individual does not receive all of the benefits, he may consume less than efficiency would suggest. Alternatively, an individual may be a smoker or alcoholic and impose costs on others. In these cases, production or consumption of the good in question may differ from the optimum level.\n\nMuch of the time, private and social costs do not diverge from one another, but at times social costs may be either greater or less than private costs. When marginal social costs of production are greater than that of the private cost function, we see the occurrence of a negative externality of production. Productive processes that result in pollution are a textbook example of production that creates negative externalities.\n\nSuch externalities are a result of firms externalizing their costs onto a third party in order to reduce their own total cost. As a result of externalizing such costs, we see that members of society will be negatively affected by such behavior of the firm. In this case, we see that an increased cost of production in society creates a social cost curve that depicts a greater cost than the private cost curve.\n\nIn an equilibrium state, we see that markets creating negative externalities of production will overproduce that good. As a result, the socially optimal production level would be lower than that observed.\n\nWhen marginal social costs of production are less than that of the private cost function, we see the occurrence of a positive externality of production. Production of public goods are a textbook example of production that create positive externalities. An example of such a public good, which creates a divergence in social and private costs, includes the production of education. It is often seen that education is a positive for any whole society, as well as a positive for those directly involved in the market.\n\nExamining the relevant diagram we see that such production creates a social cost curve that is less than that of the private curve. In an equilibrium state, we see that markets creating positive externalities of production will underproduce that good. As a result, the socially optimal production level would be greater than that observed.\n"}
{"id": "762043", "url": "https://en.wikipedia.org/wiki?curid=762043", "title": "Marginal product", "text": "Marginal product\n\nIn economics and in particular neoclassical economics, the marginal product or marginal physical productivity of an input (factor of production) is the change in output resulting from employing one more unit of a particular input (for instance, the change in output when a firm's labor is increased from five to six units), assuming that the quantities of other inputs are kept constant.\n\nThe marginal product of a given input can be expressed \nas:\n\nwhere formula_2 is the change in the firm's use of the input (conventionally a one-unit change) and formula_3 is the change in quantity of output produced (resulting from the change in the input). Note that the quantity formula_4 of the \"product\" is typically defined ignoring external costs and benefits.\n\nIf the output and the input are infinitely divisible, so the marginal \"units\" are infinitesimal, the marginal product is the mathematical derivative of the production function with respect to that input. Suppose a firm's output \"Y\" is given by the production function:\n\nwhere \"K\" and \"L\" are inputs to production (say, capital and labor). Then the marginal product of capital (\"MPK\") and marginal product of labor (\"MPL\") are given by:\n\nIn the \"law\" of diminishing marginal returns, the marginal product initially increases when more of an input (say labor) is employed, keeping the other input (say capital) constant. Here, labor is the variable input and capital is the fixed input (in a hypothetical two-inputs model). As more and more of variable input (labor) is employed, marginal product starts to fall. Finally, after a certain point, the marginal product becomes negative, implying that the additional unit of labor has \"decreased\" the output, rather than increasing it. The reason behind this is the diminishing marginal productivity of labor.\n\nThe marginal product of labor is the slope of the total product curve, which is the production function plotted against labor usage for a fixed level of usage of the capital input.\n\nIn the neoclassical theory of competitive markets, the marginal product of labor equals the real wage. In aggregate models of perfect competition, in which a single good is produced and that good is used both in consumption and as a capital good, the marginal product of capital equals its rate of return. As was shown in the Cambridge capital controversy, this proposition about the marginal product of capital cannot generally be sustained in multi-commodity models in which capital and consumption goods are distinguished.\n\nRelationship of marginal product (MPP) with the total product (TPP)\n\nThe relationship can be explained in three phases-\n(1) Initially, as the quantity of variable input is increased, TPP rises at an increasing rate. In this phase, MPP also rises.\n(2) As more and more quantities of the variable inputs are employed, TPP increases at a diminishing rate. In this phase, MPP starts to fall.\n(3) When the TPP reaches its maximum, MPP is zero. Beyond this point, TPP starts to fall and MPP becomes negative.\n\n"}
{"id": "13671081", "url": "https://en.wikipedia.org/wiki?curid=13671081", "title": "Marginal product of capital", "text": "Marginal product of capital\n\nThe marginal product of capital (MP) is the additional output resulting, ceteris paribus , from the use of an additional unit of physical capital. It equals the reciprocal of the incremental capital-output ratio. Mathematically, it is the partial derivative of the production function with respect to capital. If production output formula_1, then\n\nOne of the key assumptions in economics is diminishing returns, that is the marginal product of capital is positive but decreasing in the level of capital stock, or mathematically\n\nIn a perfectly competitive market, the marginal product of capital is equal to the rental rate of capital. \n\n\n"}
{"id": "20590", "url": "https://en.wikipedia.org/wiki?curid=20590", "title": "Mathematical model", "text": "Mathematical model\n\nA mathematical model is a description of a system using mathematical concepts and language. The process of developing a mathematical model is termed mathematical modeling. Mathematical models are used in the natural sciences (such as physics, biology, earth science, chemistry) and engineering disciplines (such as computer science, electrical engineering), as well as in the social sciences (such as economics, psychology, sociology, political science). \n\nA model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.\n\nMathematical models can take many forms, including dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures. In general, mathematical models may include logical models. In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments. Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.\n\nIn the physical sciences, a traditional mathematical model contains most of the following elements:\n\nMathematical models are usually composed of relationships and \"variables\". Relationships can be described by \"operators\", such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system parameters of interest, that can be quantified. Several classification criteria can be used for mathematical models according to their structure:\n\nMathematical models are of great importance in the natural sciences, particularly in physics. Physical theories are almost invariably expressed using mathematical models.\n\nThroughout history, more and more accurate mathematical models have been developed. Newton's laws accurately describe many everyday phenomena, but at certain limits relativity theory and quantum mechanics must be used; even these do not apply to all situations and need further refinement. It is possible to obtain the less accurate models in appropriate limits, for example relativistic mechanics reduces to Newtonian mechanics at speeds much less than the speed of light. Quantum mechanics reduces to classical physics when the quantum numbers are high. For example, the de Broglie wavelength of a tennis ball is insignificantly small, so classical physics is a good approximation to use in this case.\n\nIt is common to use idealized models in physics to simplify things. Massless ropes, point particles, ideal gases and the particle in a box are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton's laws, Maxwell's equations and the Schrödinger equation. These laws are such as a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by molecular orbital models that are approximate solutions to the Schrödinger equation. In engineering, physics models are often made by mathematical methods such as finite element analysis.\n\nDifferent mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. Euclidean geometry is much used in classical physics, while special relativity and general relativity are examples of theories that use geometries which are not Euclidean.\n\nSince prehistorical times simple models such as maps and diagrams have been used.\n\nOften when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in simulations.\n\nA mathematical model usually describes a system by a set of variables and a set of equations that establish relationships between the variables. Variables may be of many types; real or integer numbers, boolean values or strings, for example. The variables represent some properties of the system, for example, measured system outputs often in the form of signals, timing data, counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.\n\nIn business and engineering, mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: decision variables, state variables, exogenous variables, and random variables.\n\nDecision variables are sometimes known as independent variables. Exogenous variables are sometimes known as parameters or constants.\nThe variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables. Furthermore, the output variables are dependent on the state of the system (represented by the state variables).\n\nObjectives and constraints of the system and its users can be represented as functions of the output variables or state variables. The objective functions will depend on the perspective of the model's user. Depending on the context, an objective function is also known as an \"index of performance\", as it is some measure of interest to the user. Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases.\n\nFor example, economists often apply linear algebra when using input-output models. Complicated mathematical models that have many variables may be consolidated by use of vectors where one symbol represents several variables.\n\nMathematical modeling problems are often classified into black box or white box models, according to how much a priori information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.\n\nUsually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an exponentially decaying function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.\n\nIn black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.\n\nSometimes it is useful to incorporate subjective information into a mathematical model. This can be done based on intuition, experience, or expert opinion, or based on convenience of mathematical form. Bayesian statistics provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a prior probability distribution (which can be subjective), and then update this distribution based on empirical data.\n\nAn example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads. After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use. Incorporation of such subjective information might be important to get an accurate estimate of the probability.\n\nIn general, model complexity involves a trade-off between simplicity and accuracy of the model. Occam's razor is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable. While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including numerical instability. Thomas Kuhn argues that as science progresses, explanations tend to become more complex before a paradigm shift offers radical simplification .\n\nFor example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, Newton's classical mechanics is an approximated model of the real world. Still, Newton's model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the speed of light, and we study macro-particles only.\n\nAny model which is not pure white-box contains some parameters that can be used to fit the model to the system it is intended to describe. If the modeling is done by an artificial neural network or other machine learning, the optimization of parameters is called \"training\", while the optimization of model hyperparameters is called \"tuning\" and often uses cross-validation. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by \"curve fitting\".\n\nA crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately. This question can be difficult to answer as it involves several different types of evaluation.\n\nUsually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data. In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters. An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as cross-validation in statistics.\n\nDefining a metric to measure distances between observed and predicted data is a useful tool of assessing model fit. In statistics, decision theory, and some economic models, a loss function plays a similar role.\n\nWhile it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model. In general, more mathematical tools have been developed to test the fit of statistical models than models involving differential equations. Tools from non-parametric statistics can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model's mathematical form.\n\nAssessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward. If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a \"typical\" set of data.\n\nThe question of whether the model describes well the properties of the system between data points is called interpolation, and the same question for events or data points outside the observed data is called extrapolation.\n\nAs an example of the typical limitations of the scope of a model, in evaluating Newtonian classical mechanics, we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light. Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.\n\nMany types of modeling implicitly involve claims about causality. This is usually (but not always) true of models involving differential equations. As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.\n\nAn example of such criticism is the argument that the mathematical models of optimal foraging theory do not offer insight that goes beyond the common-sense conclusions of evolution and other basic principles of ecology.\n\n\"M\" = (\"Q\", Σ, δ, \"q\", \"F\") where\n\nThe state \"S\" represents that there has been an even number of 0s in the input so far, while \"S\" signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, \"M\" will finish in state \"S\", an accepting state, so the input string will be accepted.\n\nThe language recognized by \"M\" is the regular language given by the regular expression 1*( 0 (1*) 0 (1*) )*, where \"*\" is the Kleene star, e.g., 1* denotes any non-negative number (possibly zero) of symbols \"1\".\n\n\nthat can be written also as:\n\n\n\n\n\n\n\n"}
{"id": "4602393", "url": "https://en.wikipedia.org/wiki?curid=4602393", "title": "Models of scientific inquiry", "text": "Models of scientific inquiry\n\nIn the philosophy of science, models of scientific inquiry have two functions: first, to provide a descriptive account of \"how\" scientific inquiry is carried out in practice, and second, to provide an explanatory account of \"why\" scientific inquiry succeeds as well as it appears to do in arriving at genuine knowledge.\n\nThe search for scientific knowledge ends far back into antiquity. At some point in the past, at least by the time of Aristotle, philosophers recognized that a fundamental distinction should be drawn between two kinds of scientific knowledge—roughly, knowledge \"that\" and knowledge \"why\". It is one thing to know \"that\" each planet periodically reverses the direction of its motion with respect to the background of fixed stars; it is quite a different matter to know \"why\". Knowledge of the former type is descriptive; knowledge of the latter type is explanatory. It is explanatory knowledge that provides scientific understanding of the world. (Salmon, 2006, pg. 3)\n\n\"Scientific inquiry refers to the diverse ways in which scientists study the natural world and propose explanations based on the evidence derived from their work.\"\n\nThe classical model of scientific inquiry derives from Aristotle, who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.\n\nWesley Salmon (1989) began his historical survey of scientific explanation with what he called the \"received view\", as it was received from Hempel and Oppenheim in the years beginning with their \"Studies in the Logic of Explanation\" (1948) and culminating in Hempel's \"Aspects of Scientific Explanation\" (1965). Salmon summed up his analysis of these developments by means of the following Table.\n\nIn this classification, a deductive-nomological (D-N) explanation of an occurrence is a valid deduction whose conclusion states that the outcome to be explained did in fact occur. The deductive argument is called an \"explanation\", its premisses are called the \"explanans\" (L: \"explaining\") and the conclusion is called the \"explanandum\" (L: \"to be explained\"). Depending on a number of additional qualifications, an explanation may be ranked on a scale from \"potential\" to \"true\".\n\nNot all explanations in science are of the D-N type, however. An \"inductive-statistical\" (I-S) explanation accounts for an occurrence by subsuming it under statistical laws, rather than categorical or universal laws, and the mode of subsumption is itself inductive instead of deductive. The D-N type can be seen as a limiting case of the more general I-S type, the measure of certainty involved being complete, or probability 1, in the former case, whereas it is less than complete, probability < 1, in the latter case.\n\nIn this view, the D-N mode of reasoning, in addition to being used to explain particular occurrences, can also be used to explain general regularities, simply by deducing them from still more general laws.\n\nFinally, the \"deductive-statistical\" (D-S) type of explanation, properly regarded as a subclass of the D-N type, explains statistical regularities by deduction from more comprehensive statistical laws. (Salmon 1989, pp. 8–9).\n\nSuch was the \"received view\" of scientific explanation from the point of view of logical empiricism, that Salmon says \"held sway\" during the third quarter of the last century (Salmon, p. 10).\n\nDuring the course of history, one theory has succeeded another, and some have suggested further work while others have seemed content just to explain the phenomena. The reasons why one theory has replaced another are not always obvious or simple. The philosophy of science includes the question: \"What criteria are satisfied by a 'good' theory\". This question has a long history, and many scientists, as well as philosophers, have considered it. The objective is to be able to choose one theory as preferable to another without introducing cognitive bias. Several often proposed criteria were summarized by Colyvan. A good theory:\n\nStephen Hawking supports items 1–4, but does not mention fruitfulness. On the other hand, Kuhn emphasizes the importance of seminality.\n\nThe goal here is to make the choice between theories less arbitrary. Nonetheless, these criteria contain subjective elements, and are heuristics rather than part of scientific method. Also, criteria such as these do not necessarily decide between alternative theories. Quoting Bird:\nIt also is debatable whether existing scientific theories satisfy all these criteria, which may represent goals not yet achieved. For example, explanatory power over all existing observations (criterion 3) is satisfied by no one theory at the moment.\nThe desiderata of a \"good\" theory have been debated for centuries, going back perhaps even earlier than Occam's razor, which often is taken as an attribute of a good theory. Occam's razor might fall under the heading of \"elegance\", the first item on the list, but too zealous an application was cautioned by Albert Einstein: \"Everything should be made as simple as possible, but no simpler.\" It is arguable that \"parsimony\" and \"elegance\" \"typically pull in different directions\". The falsifiability item on the list is related to the criterion proposed by Popper as demarcating a scientific theory from a theory like astrology: both \"explain\" observations, but the scientific theory takes the risk of making predictions that decide whether it is right or wrong:\n\nThomas Kuhn argued that changes in scientists' views of reality not only contain subjective elements, but result from group dynamics, \"revolutions\" in scientific practice which result in paradigm shifts. As an example, Kuhn suggested that the heliocentric \"Copernican Revolution\" replaced the geocentric views of Ptolemy not because of empirical failures, but because of a new \"paradigm\" that exerted control over what scientists felt to be the more fruitful way to pursue their goals.\n\nDeductive logic and inductive logic are quite different in their approaches.\n\nDeductive logic is the reasoning of proof, or logical implication. It is the logic used in mathematics and other axiomatic systems such as formal logic. In a deductive system, there will be axioms (postulates) which are not proven. Indeed, they cannot be proven without circularity. There will also be primitive terms which are not defined, as they cannot be defined without circularity. For example, one can define a line as a set of points, but to then define a point as the intersection of two lines would be circular. Because of these interesting characteristics of formal systems, Bertrand Russell humorously referred to mathematics as \"the field where we don't know what we are talking about, nor whether or not what we say is true\". All theorems and corollaries are proven by exploring the implications of the axiomata and other theorems that have previously been developed. New terms are defined using the primitive terms and other derived definitions based on those primitive terms.\n\nIn a deductive system, one can correctly use the term \"proof\", as applying to a theorem. To say that a theorem is proven means that it is impossible for the axioms to be true and the theorem to be false. For example, we could do a simple syllogism such as the following:\n\n\nNotice that it is not possible (assuming all of the trivial qualifying criteria are supplied) to be in Arches and not be in Utah. However, one can be in Utah while not in Arches National Park. The implication only works in one direction. Statements (1) and (2) taken together imply statement (3). Statement (3) does not imply anything about statements (1) or (2). Notice that we have not proven statement (3), but we have shown that statements (1) and (2) together imply statement (3). In mathematics, what is proven is not the truth of a particular theorem, but that the axioms of the system imply the theorem. In other words, it is impossible for the axioms to be true and the theorem to be false. The strength of deductive systems is that they are sure of their results. The weakness is that they are abstract constructs which are, unfortunately, one step removed from the physical world. They are very useful, however, as mathematics has provided great insights into natural science by providing useful models of natural phenomena. One result is the development of products and processes that benefit mankind.\n\nLearning about the physical world requires the use of inductive logic. This is the logic of theory building. It is useful in such widely divergent enterprises as science and crime scene detective work. One makes a set of observations, and seeks to explain what one sees. The observer forms a hypothesis in an attempt to explain what he/she has observed. The hypothesis will have implications, which will point to certain other observations that would naturally result from either a repeat of the experiment or making more observations from a slightly different set of circumstances. If the predicted observations hold true, one feels excitement that they may be on the right track. However, the hypothesis has not been proven. The hypothesis implies that certain observations should follow, but positive observations do not imply the hypothesis. They only make it more believable. It is quite possible that some other hypothesis could also account for the known observations, and may do better with future experiments. The implication flows in only one direction, as in the syllogism used in the discussion on deduction. Therefore, it is never correct to say that a scientific principle or hypothesis/theory has been proven. (At least, not in the rigorous sense of proof used in deductive systems.)\n\nA classic example of this is the study of gravitation. Newton formed a law for gravitation stating that the force of gravitation is directly proportional to the product of the two masses and inversely proportional to the square of the distance between them. For over 170 years, all observations seemed to validate his equation. However, telescopes eventually became powerful enough to see a slight discrepancy in the orbit of Mercury. Scientists tried everything imaginable to explain the discrepancy, but they could not do so using the objects that would bear on the orbit of Mercury. Eventually, Einstein developed his theory of general relativity and it explained the orbit of Mercury and all other known observations dealing with gravitation. During the long period of time when scientists were making observations that seemed to validate Newton's theory, they did not, in fact, prove his theory to be true. However, it must have seemed at the time that they did. It only took one counterexample (Mercury's orbit) to prove that there was something wrong with his theory.\n\nThis is typical of inductive logic. All of the observations that seem to validate the theory, do not prove its truth. But one counter-example can prove it false. That means that deductive logic is used in the evaluation of a theory. In other words, if A implies B, then not B implies not A. Einstein's theory of General Relativity has been supported by many observations using the best scientific instruments and experiments. However, his theory now has the same status as Newton's theory of gravitation prior to seeing the problems in the orbit of Mercury. It is highly credible and validated with all we know, but it is not proven. It is only the best we have at this point in time.\n\nAnother example of correct scientific reasoning is shown in the current search for the Higgs boson. Scientists on the Compact Muon Solenoid experiment at the Large Hadron Collider have conducted experiments yielding data suggesting the existence of the Higgs boson. However, realizing that the results could possibly be explained as a background fluctuation and not the Higgs boson, they are cautious and waiting for further data from future experiments. Said Guido Tonelli:\n\nA brief overview of the scientific method would then contain these steps as a minimum:\n\n\nWhen a hypothesis has survived a sufficient number of tests, it may be promoted to a scientific theory. A theory is a hypothesis that has survived many tests and seems to be consistent with other established scientific theories. Since a theory is a promoted hypothesis, it is of the same 'logical' species and shares the same logical limitations. Just as a hypothesis cannot be proven but can be disproved, that same is true for a theory. It is a difference of degree, not kind.\n\nArguments from analogy are another type of inductive reasoning. In arguing from analogy, one infers that since two things are alike in several respects, they are likely to be alike in another respect. This is, of course, an assumption. It is natural to attempt to find similarities between two phenomena and wonder what one can learn from those similarities. However, to notice that two things share attributes in several respects does not imply any similarities in other respects. It is possible that the observer has already noticed all of the attributes that are shared and any other attributes will be distinct. Argument from analogy is an unreliable method of reasoning that can lead to erroneous conclusions, and thus cannot be used to establish scientific facts.\n\n\n\nFor interesting explanations regarding the orbit of Mercury and General Relativity, the following links are useful:\n"}
{"id": "161019", "url": "https://en.wikipedia.org/wiki?curid=161019", "title": "Negation", "text": "Negation\n\nIn logic, negation, also called the logical complement, is an operation that takes a proposition formula_1 to another proposition \"not formula_1\", written formula_3 (¬P), which is interpreted intuitively as being true when formula_1 is false, and false when formula_1 is true. Negation is thus a unary (single-argument) logical connective. It may be applied as an operation on notions, propositions, truth values, or semantic values more generally. In classical logic, negation is normally identified with the truth function that takes \"truth\" to \"falsity\" and vice versa. In intuitionistic logic, according to the Brouwer–Heyting–Kolmogorov interpretation, the negation of a proposition formula_1 is the proposition whose proofs are the refutations of formula_1.\n\nNo agreement exists as to the possibility of defining negation, as to its logical status, function, and meaning, as to its field of applicability..., and as to the interpretation of the negative judgment, (F.H. Heinemann 1944).\n\n\"Classical negation\" is an operation on one logical value, typically the value of a proposition, that produces a value of \"true\" when its operand is false and a value of \"false\" when its operand is true. So, if statement formula_1 is true, then formula_3 (pronounced \"not P\") would therefore be false; and conversely, if formula_3 is false, then formula_1 would be true.\n\nThe truth table of formula_3 is as follows:\n\nNegation can be defined in terms of other logical operations. For example, formula_3 can be defined as formula_14 (where formula_15 is logical consequence and formula_16 is absolute falsehood). Conversely, one can define formula_16 as formula_18 for any proposition formula_19 (where formula_20 is logical conjunction). The idea here is that any contradiction is false. While these ideas work in both classical and intuitionistic logic, they do not work in paraconsistent logic, where contradictions are not necessarily false. In classical logic, we also get a further identity, formula_21 can be defined as formula_22, where formula_23 is logical disjunction.\n\nAlgebraically, classical negation corresponds to complementation in a Boolean algebra, and intuitionistic negation to pseudocomplementation in a Heyting algebra. These algebras provide a semantics for classical and intuitionistic logic respectively.\n\nThe negation of a proposition formula_1 is notated in different ways in various contexts of discussion and fields of application. Among these variants are the following:\n\nThe notation N\"p\" is Łukasiewicz notation.\n\nIn set theory formula_25 is also used to indicate 'not member of': formula_26 is the set of all members of formula_27 that are not members of formula_28.\n\nNo matter how it is notated or symbolized, the negation formula_3 can be read as \"it is not the case that formula_1\", \"not that formula_1\", or usually more simply as \"not formula_1\".\n\nWithin a system of classical logic, double negation, that is, the negation of the negation of a proposition formula_1, is logically equivalent to formula_1. Expressed in symbolic terms, formula_35. In intuitionistic logic, a proposition implies its double negation but not conversely. This marks one important difference between classical and intuitionistic negation. Algebraically, classical negation is called an involution of period two.\n\nHowever, in intuitionistic logic we do have the equivalence of formula_36. Moreover, in the propositional case, a sentence is classically provable if its double negation is intuitionistically provable. This result is known as Glivenko's theorem.\n\nDe Morgan's laws provide a way of distributing negation over disjunction and conjunction :\n\nLet formula_39 denote the logical xor operation. In Boolean algebra, a linear function is one such that:\n\nIf there exists formula_40,\nformula_41,\nfor all formula_42.\n\nAnother way to express this is that each variable always makes a difference in the truth-value of the operation or it never makes a difference. Negation is a linear logical operator.\n\nIn Boolean algebra a self dual function is one such that:\n\nformula_43 for all\nformula_44.\nNegation is a self dual logical operator.\n\nThere are a number of equivalent ways to formulate rules for negation. One usual way to formulate classical negation in a natural deduction setting is to take as primitive rules of inference \"negation introduction\" (from a derivation of formula_1 to both formula_19 and formula_47, infer formula_3; this rule also being called \"reductio ad absurdum\"), \"negation elimination\" (from formula_1 and formula_3 infer formula_19; this rule also being called \"ex falso quodlibet\"), and \"double negation elimination\" (from formula_52 infer formula_1). One obtains the rules for intuitionistic negation the same way but by excluding double negation elimination.\n\nNegation introduction states that if an absurdity can be drawn as conclusion from formula_1 then formula_1 must not be the case (i.e. formula_1 is false (classically) or refutable (intuitionistically) or etc.). Negation elimination states that anything follows from an absurdity. Sometimes negation elimination is formulated using a primitive absurdity sign formula_16. In this case the rule says that from formula_1 and formula_3 follows an absurdity. Together with double negation elimination one may infer our originally formulated rule, namely that anything follows from an absurdity.\n\nTypically the intuitionistic negation formula_3 of formula_1 is defined as formula_14. Then negation introduction and elimination are just special cases of implication introduction (conditional proof) and elimination (modus ponens). In this case one must also add as a primitive rule \"ex falso quodlibet\".\n\nAs in mathematics, negation is used in computer science to construct logical statements.\n\nThe \"codice_1\" signifies logical NOT in B, C, and languages with a C-inspired syntax such as C++, Java, JavaScript, Perl, and PHP. \"codice_2\" is the operator used in ALGOL 60, BASIC, and languages with an ALGOL- or BASIC-inspired syntax such as Pascal, Ada, Eiffel and Seed7. Some languages (C++, Perl, etc.) provide more than one operator for negation. A few languages like PL/I and Ratfor use codice_3 for negation. Some modern computers and operating systems will display codice_3 as codice_1 on files encoded in ASCII. Most modern languages allow the above statement to be shortened from codice_6 to codice_7, which allows sometimes, when the compiler/interpreter is not able to optimize it, faster programs.\n\nIn computer science there is also \"bitwise negation\". This takes the value given and switches all the binary 1s to 0s and 0s to 1s. See bitwise operation. This is often used to create ones' complement or \"codice_8\" in C or C++ and two's complement (just simplified to \"codice_9\" or the negative sign since this is equivalent to taking the arithmetic negative value of the number) as it basically creates the opposite (negative value equivalent) or mathematical complement of the value (where both values are added together they create a whole).\n\nTo get the absolute (positive equivalent) value of a given integer the following would work as the \"codice_9\" changes it from negative to positive (it is negative because \"codice_11\" yields true)\n\nTo demonstrate logical negation:\n\nInverting the condition and reversing the outcomes produces code that is logically equivalent to the original code, i.e. will have identical results for any input (note that depending on the compiler used, the actual instructions performed by the computer may differ).\n\nThis convention occasionally surfaces in written speech, as computer-related slang for \"not\". The phrase codice_12, for example, means \"not voting\".\n\nIn Kripke semantics where the semantic values of formulae are sets of possible worlds, negation can be taken to mean set-theoretic complementation. (See also possible world semantics.)\n\n\n"}
{"id": "48243754", "url": "https://en.wikipedia.org/wiki?curid=48243754", "title": "Negative consequentialism", "text": "Negative consequentialism\n\nNegative consequentialism is a version of the ethical theory consequentialism, which is \"one of the major theories of normative ethics.\" Like other versions of consequentialism, negative consequentialism holds that moral right and wrong depend only on the value of outcomes. That is, for negative and other versions of consequentialism, questions such as \"what should I do?\" and \"what kind of person should I be?\" are answered only based on consequences. Negative consequentialism differs from other versions of consequentialism by giving greater weight in moral deliberations to what is bad (e.g. suffering or injustice) than what is good (e.g. happiness or justice).\n\nA specific type of consequentialism is utilitarianism, which says that the consequences that matter are those that affect well-being. Consequentialism is broader than utilitarianism in that consequentialism can say that the value of outcomes depend on other things than well-being; for example, justice, fairness, and equality. Negative utilitarianism is thus a form of negative consequentialism. Much more has been written explicitly about negative utilitarianism than directly about negative consequentialism, although since negative utilitarianism is a form of negative consequentialism, everything that has been written about negative utilitarianism is by definition about a specific (utilitarian) version of negative consequentialism. Similarly to how there are many variations of consequentialism and negative utilitarianism, there are many versions of negative consequentialism, for example negative prioritarianism and negative consequentialist egalitarianism.\n\nG. E. Moore's ethics can be said to be a negative consequentialism (more precisely, a consequentialism with a negative utilitarian component), because he has been labeled a consequentialist, and he said that \"consciousness of intense pain is, by itself, a great evil\" whereas \"the mere consciousness of pleasure, however intense, does not, \"by itself\", appear to be a \"great\" good, even if it has some slight intrinsic value. In short, pain (if we understand by this expression, the consciousness of pain) appears to be a far worse evil than pleasure is a good.\" Moore wrote in the first half of the 20th century before any of the terms 'consequentialism,' 'negative utilitarianism' or 'negative consequentialism' were coined, and he did not use the term 'negative consequentialism' himself. Similarly to Moore, Ingemar Hedenius defended a consequentialism that could be called negative (or could be said to have a negative utilitarian component) because he assigned more importance to suffering than to happiness. Hedenius saw the worst in life, such as infernalistic suffering, as so evil that calculations of happiness versus suffering becomes unnecessary; he did not see that such evil could be counterbalanced by any good, such as happiness.\n\nPhilosophy professor Clark Wolf defends \"negative consequentialism as a component of a larger theory of justice.\" Walter Sinnott-Armstrong interprets Bernard Gert's moral system as a \"sophisticated form of negative objective universal public rule consequentialism.\" Jamie Mayerfeld argues for a strong duty to relieve suffering, which is consequentialist in form. He says that \"suffering is more bad than happiness is good,\" and that \"the lifelong bliss of many people, no matter how many, cannot justify our allowing the lifelong torture of one.\"\n\n"}
{"id": "2795942", "url": "https://en.wikipedia.org/wiki?curid=2795942", "title": "Negative cutting", "text": "Negative cutting\n\nNegative cutting (also known as negative matching and negative conforming) is the process of cutting motion picture negative to match precisely the final edit as specified by the film editor. Original camera negative (OCN) is cut with scissors and joined using a film splicer and film cement. Negative cutting is part of the post-production process and occurs after editing and prior to striking internegatives and release prints. The process of negative cutting has changed little since the beginning of cinema in the early 20th century. In the early 1980s computer software was first used to aid the cutting process. Kodak introduced barcode on motion picture negative in the mid-1990s. This enabled negative cutters to more easily track shots and identify film sections based on keykode.\n\nToward the late 1990s and early 2000s negative cutting changed due to the advent of digital cinema technologies such as digital intermediate (DI), digital projection and high-definition television. In some countries, due to the high cost of online suites, negative cutting is still used for commercials by reducing footage. Increasingly feature films are bypassing the negative cutting process altogether and are being scanned directly from the uncut rushes.\n\nThe existence of digital intermediates (DI) has created a new demand for negative cutters to extract selected takes which are cut from the rushes and re-spliced into new rolls (in edit order) to reduce the volume of footage for scanning.\n\nAfter a film shoot, the original camera negative (OCN) is sent to a film laboratory for processing. Two or three camera rolls are spliced together to create a lab roll approximately long. After developing the lab roll, it is put through a telecine to create a rushes transfer tape. This rushes transfer tape is of lower quality than film and is used for editing purposes only.\n\nThe rushes tape is sent to the Editor who loads it into an offline edit suite. The lab rolls are sent to the negative cutter for logging and storage.\n\nAfter the Editor finishes the Edit it is exported to an offline EDL list and the EDL list is sent to the negative cutter. The negative cutter will translate the Timecode in the EDL list to edge numbers (keykode) using specially designed negative cutting software to find which shot is needed from the rushes negative.\n\nTraditionally a negative cutter would then fine cut the negative to match the Editor's final edit frame accurately. Negative would be spliced together to create rolls less than which would then be sent to the film laboratory to print release prints.\n\nToday most feature films are extracted full takes (as selected takes) and scanned digitally as a digital intermediate. Television series and commercials shot on film follow the same extraction process but are sent for telecine. Each required shot is extracted from the lab roll as a full take and respliced together to create a new selected roll of negative. This reduces the negative required by up to 1/10 of the footage shot, saving considerable time during scanning or telecine. The negative cutter will create a new Online EDL list replacing the rushes roll timecode with the new selected roll timecode.\n\nIn the case of feature films the selected roll and Online EDL are sent to a post production facility for scanning as a digital intermediate. For television commercials or series the selected takes and EDL are sent to a post production facility for re-telecine and compiled in an Online Suite for final grading.\n\nThere have been a number of dedicated software systems that have been developed for and by negative cutters to manage the process of cutting motion picture negative. A number of individual proprietary software systems have been developed starting in the early 1980s. Stan Sztaba developed a system for World Cinevision Services Inc (New York) in 1983 using Apple II DOS and then ProDOS, this system is still used today. Elliott Gamson of Immaculate Matching (New York) developed a system using MS-DOS. Computamatch was one of the first MS-DOS-based systems developed and is still in use today in several countries.\n\nThe first commercially available software product was OSC/R (pronounced \"Oscar\"), a DOS-based application developed in Toronto, Canada by The Adelaide Works. OSC/R was very widely used and at the time was the only negative cutting software on the market until Adelaide Works ceased operation in 1993. OSC/R is still used today in some negative cutting facilities but has been mostly replaced by newer and more advanced systems. Excalibur was a later Windows 98 based product developed by FilmLab Engineering in Britain. Film Fusion is one of the most recent developments and is a Windows XP and Vista based system developed in Sydney, Australia by Popsoft IT.\n\nNegative cutters use various hardware tools such as film synchronizers, re-winders, film splicers, scissors, film cement and film keykode readers. DigiSync, a purpose built keykode reader is used by most negative cutters in conjunction with software for logging the keykode from film. DigiSync was developed by Research In Motion and in 1998 it won a Technical Achievement Academy Award for the design and development of the DigiSync Film Keykode Reader. Research In Motion later moved on to bigger things and invented the BlackBerry Wireless Email Phone and is now a publicly listed company. Other brands of barcode scanners are also in use.\n\n"}
{"id": "213328", "url": "https://en.wikipedia.org/wiki?curid=213328", "title": "Negative feedback", "text": "Negative feedback\n\nNegative feedback (or balancing feedback) occurs when some function of the output of a system, process, or mechanism is fed back in a manner that tends to reduce the fluctuations in the output, whether caused by changes in the input or by other disturbances.\n\nWhereas positive feedback tends to lead to instability via exponential growth, oscillation or chaotic behavior, negative feedback generally promotes stability. Negative feedback tends to promote a settling to equilibrium, and reduces the effects of perturbations. Negative feedback loops in which just the right amount of correction is applied with optimum timing can be very stable, accurate, and responsive.\n\nNegative feedback is widely used in mechanical and electronic engineering, and also within living organisms, and can be seen in many other fields from chemistry and economics to physical systems such as the climate. General negative feedback systems are studied in control systems engineering.\n\n\nNegative feedback as a control technique may be seen in the refinements of the water clock introduced by Ktesibios of Alexandria in the 3rd century BCE. Self-regulating mechanisms have existed since antiquity, and were used to maintain a constant level in the reservoirs of water clocks as early as 200 BCE.\nNegative feedback was implemented in the 17th Century. Cornelius Drebbel had built thermostatically-controlled incubators and ovens in the early 1600s,\nand centrifugal governors were used to regulate the distance and pressure between millstones in windmills. James Watt patented a form of governor in 1788 to control the speed of his steam engine, and James Clerk Maxwell in 1868 described \"component motions\" associated with these governors that lead to a decrease in a disturbance or the amplitude of an oscillation.\n\nThe term \"feedback\" was well established by the 1920s, in reference to a means of boosting the gain of an electronic amplifier.\nFriis and Jensen described this action as \"positive feedback\" and made passing mention of a contrasting \"negative feed-back action\" in 1924.\nHarold Stephen Black came up with the idea of using negative feedback in electronic amplifiers in 1927, submitted a patent application in 1928, and detailed its use in his paper of 1934, where he defined negative feedback as a type of coupling that \"reduced\" the gain of the amplifier, in the process greatly increasing its stability and bandwidth.\n\nKarl Küpfmüller published papers on a negative-feedback-based automatic gain control system and a feedback system stability criterion in 1928.\n\nNyquist and Bode built on Black’s work to develop a theory of amplifier stability.\n\nEarly researchers in the area of cybernetics subsequently generalized the idea of negative feedback to cover any goal-seeking or purposeful behavior.\n\nCybernetics pioneer Norbert Wiener helped to formalize the concepts of feedback control, defining feedback in general as \"the chain of the transmission and return of information\", and negative feedback as the case when:\n\nWhile the view of feedback as any \"circularity of action\" helped to keep the theory simple and consistent, Ashby pointed out that, while it may clash with definitions that require a \"materially evident\" connection, \"the exact definition of feedback is nowhere important\". Ashby pointed out the limitations of the concept of \"feedback\":\nTo reduce confusion, later authors have suggested alternative terms such as \"degenerative\", \"self-correcting\", \"balancing\", or \"discrepancy-reducing\" in place of \"negative\".\n\nIn many physical and biological systems, qualitatively different influences can oppose each other. For example, in biochemistry, one set of chemicals drives the system in a given direction, whereas another set of chemicals drives it in an opposing direction. If one or both of these opposing influences are non-linear, equilibrium point(s) result.\n\nIn biology, this process (in general, biochemical) is often referred to as homeostasis; whereas in mechanics, the more common term is equilibrium.\n\nIn engineering, mathematics and the physical, and biological sciences, common terms for the points around which the system gravitates include: attractors, stable states, eigenstates/eigenfunctions, equilibrium points, and setpoints.\n\nIn control theory, \"negative\" refers to the sign of the multiplier in mathematical models for feedback. In delta notation, −Δoutput is added to or mixed into the input. In multivariate systems, vectors help to illustrate how several influences can both partially complement and partially oppose each other.\n\nSome authors, in particular with respect to modelling business systems, use \"negative\" to refer to the reduction in difference between the desired and actual behavior of a system. In a psychology context, on the other hand, \"negative\" refers to the valence of the feedback – attractive versus aversive, or praise versus criticism.\n\nIn contrast, positive feedback is feedback in which the system responds so as to increase the magnitude of any particular perturbation, resulting in amplification of the original signal instead of stabilization. Any system in which there is positive feedback together with a gain greater than one will result in a runaway situation. Both positive and negative feedback require a feedback loop to operate.\n\nHowever, negative feedback systems can still be subject to oscillations. This is caused by the slight delays around any loop. Due to these delays the feedback signal of some frequencies can arrive one half cycle later which will have a similar effect to positive feedback and these frequencies can reinforce themselves and grow over time. This problem is often dealt with by attenuating or changing the phase of the problematic frequencies. Unless the system naturally has sufficient damping, many negative feedback systems have low pass filters or dampers fitted.\n\nThere are a large number of different examples of negative feedback and some are discussed below.\n\nOne use of feedback is to make a system (say \"T\") self-regulating to minimize the effect of a disturbance (say \"D\"). Using a negative feedback loop, a measurement of some variable (for example, a process variable, say \"E\") is subtracted from a required value (the 'set point') to estimate an operational error in system status, which is then used by a regulator (say \"R\") to reduce the gap between the measurement and the required value. The regulator modifies the input to the system \"T\" according to its interpretation of the error in the status of the system. This error may be introduced by a variety of possible disturbances or 'upsets', some slow and some rapid. The regulation in such systems can range from a simple 'on-off' control to a more complex processing of the error signal.\n\nIt may be noted that the physical form of the signals in the system may change from point to point. So, for example, a change in weather may cause a disturbance to the \"heat\" input to a house (as an example of the system \"T\") that is monitored by a thermometer as a change in \"temperature\" (as an example of an 'essential variable' \"E\"), converted by the thermostat (a 'comparator') into an \"electrical\" error in status compared to the 'set point' \"S\", and subsequently used by the regulator (containing a 'controller' that commands \"gas\" control valves and an ignitor) ultimately to change the \"heat\" provided by a furnace (an 'effector') to counter the initial weather-related disturbance in heat input to the house.\n\nError controlled regulation is typically carried out using a Proportional-Integral-Derivative Controller (PID controller). The regulator signal is derived from a weighted sum of the error signal, integral of the error signal, and derivative of the error signal. The weights of the respective components depend on the application.\n\nMathematically, the regulator signal is given by:\nwhere\n\nThe negative feedback amplifier was invented by Harold Stephen Black at Bell Laboratories in 1927, and granted a patent in 1937 (US Patent 2,102,671 \"a continuation of application Serial No. 298,155, filed August 8, 1928 ...\").\n\nThere are many advantages to feedback in amplifiers. In design, the type of feedback and amount of feedback are carefully selected to weigh and optimize these various benefits.\n\nThough negative feedback has many advantages, amplifiers with feedback can oscillate. See the article on step response. They may even exhibit instability. Harry Nyquist of Bell Laboratories proposed the Nyquist stability criterion and the Nyquist plot that identify stable feedback systems, including amplifiers and control systems.\nThe figure shows a simplified block diagram of a negative feedback amplifier.\n\nThe feedback sets the overall (closed-loop) amplifier gain at a value:\n\nwhere the approximate value assumes β\"A \" » 1. This expression shows that a gain greater than one requires β < 1. Because the approximate gain 1/β is independent of the open-loop gain \"A\", the feedback is said to 'desensitize' the closed-loop gain to variations in \"A \" (for example, due to manufacturing variations between units, or temperature effects upon components), provided only that the gain \"A\" is sufficiently large. In this context, the factor (1+β\"A\") is often called the 'desensitivity factor', and in the broader context of feedback effects that include other matters like electrical impedance and bandwidth, the 'improvement factor'.\n\nIf the disturbance \"D\" is included, the amplifier output becomes:\n\nwhich shows that the feedback reduces the effect of the disturbance by the 'improvement factor' (1+β \"A\"). The disturbance \"D\" might arise from fluctuations in the amplifier output due to noise and nonlinearity (distortion) within this amplifier, or from other noise sources such as power supplies.\n\nThe difference signal \"I\"–β\"O\" at the amplifier input is sometimes called the \"error signal\". According to the diagram, the error signal is:\n\nFrom this expression, it can be seen that a large 'improvement factor' (or a large loop gain β\"A\") tends to keep this error signal small.\n\nAlthough the diagram illustrates the principles of the negative feedback amplifier, modeling a real amplifier as a unilateral forward amplification block and a unilateral feedback block has significant limitations. For methods of analysis that do not make these idealizations, see the article Negative feedback amplifier.\n\nThe operational amplifier was originally developed as a building block for the construction of analog computers, but is now used almost universally in all kinds of applications including audio equipment and control systems.\n\nOperational amplifier circuits typically employ negative feedback to get a predictable transfer function. Since the open-loop gain of an op-amp is extremely large, a small differential input signal would drive the output of the amplifier to one rail or the other in the absence of negative feedback. A simple example of the use of feedback is the op-amp voltage amplifier shown in the figure.\n\nThe idealized model of an operational amplifier assumes that the gain is infinite, the input impedance is infinite, output resistance is zero, and input offset currents and voltages are zero. Such an ideal amplifier draws no current from the resistor divider. \nIgnoring dynamics (transient effects and propagation delay), the infinite gain of the ideal op-amp means this feedback circuit drives the voltage difference between the two op-amp inputs to zero. Consequently, the voltage gain of the circuit in the diagram, assuming an ideal op amp, is the reciprocal of feedback voltage division ratio β:\n\nA real op-amp has a high but finite gain \"A\" at low frequencies, decreasing gradually at higher frequencies. In addition, it exhibits a finite input impedance and a non-zero output impedance. Although practical op-amps are not ideal, the model of an ideal op-amp often suffices to understand circuit operation at low enough frequencies. \nAs discussed in the previous section, the feedback circuit stabilizes the closed-loop gain and desensitizes the output to fluctuations generated inside the amplifier itself.\n\nAn example of the use of negative feedback control is the ballcock control of water level (see diagram). In modern engineering, negative feedback loops are found in fuel injection systems and carburettors. Similar control mechanisms are used in heating and cooling systems, such as those involving air conditioners, refrigerators, or freezers.\n\nSome biological systems exhibit negative feedback such as the baroreflex in blood pressure regulation and erythropoiesis. Many biological process (e.g., in the human anatomy) use negative feedback. Examples of this are numerous, from the regulating of body temperature, to the regulating of blood glucose levels. The disruption of feedback loops can lead to undesirable results: in the case of blood glucose levels, if negative feedback fails, the glucose levels in the blood may begin to rise dramatically, thus resulting in diabetes.\n\nFor hormone secretion regulated by the negative feedback loop: when gland X releases hormone X, this stimulates target cells to release hormone Y. When there is an excess of hormone Y, gland X \"senses\" this and inhibits its release of hormone X. As shown in the figure, most endocrine hormones are controlled by a physiologic negative feedback inhibition loop, such as the glucocorticoids secreted by the adrenal cortex. The hypothalamus secretes corticotropin-releasing hormone (CRH), which directs the anterior pituitary gland to secrete adrenocorticotropic hormone (ACTH). In turn, ACTH directs the adrenal cortex to secrete glucocorticoids, such as cortisol. Glucocorticoids not only perform their respective functions throughout the body but also negatively affect the release of further stimulating secretions of both the hypothalamus and the pituitary gland, effectively reducing the output of glucocorticoids once a sufficient amount has been released.\n\nSelf-organization is the capability of certain systems \"of organizing their own behavior or structure\". There are many possible factors contributing to this capacity, and most often positive feedback is identified as a possible contributor. However, negative feedback also can play a role.\n\nIn economics, automatic stabilisers are government programs that are intended to work as negative feedback to dampen fluctuations in real GDP.\n\nMainstream economics asserts that the market pricing mechanism operates to match supply and demand, because mismatches between them feed back into the decision-making of suppliers and demanders of goods, altering prices and thereby reducing any discrepancy. However Norbert Wiener wrote in 1948:\n\nThe notion of economic equilibrium being maintained in this fashion by market forces has also been questioned by numerous heterodox economists such as financier George Soros and leading ecological economist and steady-state theorist Herman Daly, who was with the World Bank in 1988-1994.\n"}
{"id": "6476061", "url": "https://en.wikipedia.org/wiki?curid=6476061", "title": "Negative pledge", "text": "Negative pledge\n\nNegative pledge is a provision in a contract which prohibits a party to the contract from creating any security interests over certain property specified in the provision.\n\nNegative pledges often appear in security documents, where they operate to prohibit the person who is granting the security interest from creating any other security interests over the same property, which might compete with (or rank \"pari passu\" with) the security of the first secured creditor under the security document in which the negative pledge appears.\n\nIn Australia, negative pledge lending took off after a substantial deal by Pioneer Concrete in 1978. It was a new way of lending, which allowed the banks to lend to corporations, something previously the domain of life insurers.\n\nNegative pledge clauses are almost universal in modern unsecured commercial loan documents. The purpose is to ensure that a borrower, having taken out an unsecured loan, cannot subsequently take out another loan with a different lender, securing the subsequent loan on the specified assets. If the borrower could do this, the original lender would be disadvantaged because the subsequent lender would have first call on the assets in an event of default.\n\n"}
{"id": "6475900", "url": "https://en.wikipedia.org/wiki?curid=6475900", "title": "Negative pregnant", "text": "Negative pregnant\n\nA negative pregnant (sometimes called a pregnant denial) refers to a denial which implies its affirmative opposite by seeming to deny only a qualification of the allegation and not the allegation itself. For example, \"I deny that I owe the plaintiff five hundred dollars\" might imply that the person making the statement owes some other sum of money, and was only denying that they owe that particular amount. \n\nA negative pregnant which appears in pleadings will often elicit a request for further and better particulars, or an interrogatory. In order to avoid a negative pregnant in the above example, one might instead say, \"I deny that I owe the plaintiff five hundred dollars, or any other sum of money.\"\n\nThe issue can also arise in the context of statutory interpretation. For instance, Justice Thurgood Marshall argues in his dissent to \"EEOC v. Aramco\" that the presumption against extraterritoriality is rebutted by a negative inference from the alien-exemption provision of Title VII of the Civil Rights Act of 1964, which states that Title VII \"shall not apply to an employer with respect to the employment of aliens outside any State.\" Marshall concludes that \"Absent an intention that Title VII \"apply\" 'outside any State,' Congress would have had no reason to craft this extraterritorial exemption. And because only discrimination against aliens is exempted, employers remain accountable for discrimination against United States citizens abroad.\" \n\n"}
{"id": "361356", "url": "https://en.wikipedia.org/wiki?curid=361356", "title": "Negentropy", "text": "Negentropy\n\nIn information theory and statistics, negentropy is used as a measure of distance to normality. The concept and phrase \"negative entropy\" was introduced by Erwin Schrödinger in his 1944 popular-science book \"What is Life?\" Later, Léon Brillouin shortened the phrase to \"negentropy\", to express it in a more \"positive\" way: a living system imports negentropy and stores it. In 1974, Albert Szent-Györgyi proposed replacing the term \"negentropy\" with \"syntropy\". That term may have originated in the 1940s with the Italian mathematician Luigi Fantappiè, who tried to construct a unified theory of biology and physics. Buckminster Fuller tried to popularize this usage, but \"negentropy\" remains common.\n\nIn a note to \"What is Life?\" Schrödinger explained his use of this phrase.\nIn 2009, Mahulikar & Herwig redefined negentropy of a dynamically ordered sub-system as the specific entropy deficit of the ordered sub-system relative to its surrounding chaos. Thus, negentropy has SI units of (J kg K) when defined based on specific entropy per unit mass, and (K) when defined based on specific entropy per unit energy. This definition enabled: \"i\") scale-invariant thermodynamic representation of dynamic order existence, \"ii\") formulation of physical principles exclusively for dynamic order existence and evolution, and \"iii\") mathematical interpretation of Schrödinger's negentropy debt.\n\nIn information theory and statistics, negentropy is used as a measure of distance to normality. Out of all distributions with a given mean and variance, the normal or Gaussian distribution is the one with the highest entropy. Negentropy measures the difference in entropy between a given distribution and the Gaussian distribution with the same mean and variance. Thus, negentropy is always nonnegative, is invariant by any linear invertible change of coordinates, and vanishes if and only if the signal is Gaussian.\n\nNegentropy is defined as\n\nwhere formula_2 is the differential entropy of the Gaussian density with the same mean and variance as formula_3 and formula_4 is the differential entropy of formula_3:\n\nNegentropy is used in statistics and signal processing. It is related to network entropy, which is used in independent component analysis.\n\nThere is a physical quantity closely linked to free energy (free enthalpy), with a unit of entropy and isomorphic to negentropy known in statistics and information theory. In 1873, Willard Gibbs created a diagram illustrating the concept of free energy corresponding to free enthalpy. On the diagram one can see the quantity called capacity for entropy. This quantity is the amount of entropy that may be increased without changing an internal energy or increasing its volume. In other words, it is a difference between maximum possible, under assumed conditions, entropy and its actual entropy. It corresponds exactly to the definition of negentropy adopted in statistics and information theory. A similar physical quantity was introduced in 1869 by Massieu for the isothermal process (both quantities differs just with a figure sign) and then Planck for the isothermal-isobaric process. More recently, the Massieu–Planck thermodynamic potential, known also as \"free entropy\", has been shown to play a great role in the so-called entropic formulation of statistical mechanics, applied among the others in molecular biology and thermodynamic non-equilibrium processes.\n\nIn 1953, Léon Brillouin derived a general equation stating that the changing of an information bit value requires at least kT ln(2) energy. This is the same energy as the work Leó Szilárd's engine produces in the idealistic case. In his book, he further explored this problem concluding that any cause of this bit value change (measurement, decision about a yes/no question, erasure, display, etc.) will require the same amount of energy.\n\n"}
{"id": "780566", "url": "https://en.wikipedia.org/wiki?curid=780566", "title": "Possible world", "text": "Possible world\n\nIn philosophy and logic, the concept of a possible world is used to express modal claims. The concept of possible worlds is common in contemporary philosophical discourse but has been disputed.\n\nThose theorists who use the concept of possible worlds consider the \"actual\" world to be one of the many possible worlds. For each distinct way the world could have been, there is said to be a distinct possible world; the actual world is the one we in fact live in. Among such theorists there is disagreement about the nature of possible worlds; their precise ontological status is disputed, and especially the difference, if any, in ontological status between the actual world and all the other possible worlds. One position on these matters is set forth in David Lewis's modal realism (see below). There is a close relation between propositions and possible worlds. We note that every proposition is either true or false at any given possible world; then the \"modal status\" of a proposition is understood in terms of the \"worlds in which it is true\" and \"worlds in which it is false\". The following are among the assertions we may now usefully make:\n\n\nThe idea of possible worlds is most commonly attributed to Gottfried Leibniz, who spoke of possible worlds as ideas in the mind of God and used the notion to argue that our actually created world must be \"the best of all possible worlds\". Arthur Schopenhauer argued that on the contrary our world must be the worst of all possible worlds, because if it were only a little worse it could not continue to exist.\n\nScholars have found implicit earlier traces of the idea of possible worlds in the works of René Descartes, a major influence on Leibniz, Al-Ghazali (\"The Incoherence of the Philosophers\"), Averroes (\"The Incoherence of the Incoherence\"), Fakhr al-Din al-Razi (\"Matalib al-'Aliya\") and John Duns Scotus. The modern philosophical use of the notion was pioneered by David Lewis and Saul Kripke.\n\nA semantics for modal logic was first introduced in the late-1950s work of Saul Kripke and his colleagues. A statement in modal logic that is \"possible\" is said to be true in at least one possible world; a statement that is \"necessary\" is said to be true in all possible worlds.\n\nFrom this groundwork, the theory of possible worlds became a central part of many philosophical developments, from the 1960s onwards – including, most famously, the analysis of counterfactual conditionals in terms of \"nearby possible worlds\" developed by David Lewis and Robert Stalnaker. On this analysis, when we discuss what \"would\" happen \"if\" some set of conditions \"were\" the case, the truth of our claims is determined by what is true at the nearest possible world (or the \"set\" of nearest possible worlds) where the conditions obtain. (A possible world W is said to be near to another possible world W in respect of R to the degree that the same things happen in W and W in respect of R; the more different something happens in two possible worlds in a certain respect, the \"further\" they are from one another in that respect.) Consider this conditional sentence: \"If George W. Bush hadn't become president of the U.S. in 2001, Al Gore would have.\" The sentence would be taken to express a claim that could be reformulated as follows: \"In all nearest worlds to our actual world (nearest in relevant respects) where George W. Bush didn't become president of the U.S. in 2001, Al Gore became president of the U.S. then instead.\" And on this interpretation of the sentence, if there is or are some nearest worlds to the actual world (nearest in relevant respects) where George W. Bush didn't become president but Al Gore didn't either, then the claim expressed by this counterfactual would be false.\n\nToday, possible worlds play a central role in many debates in philosophy, including especially debates over the Zombie Argument, and physicalism and supervenience in the philosophy of mind. Many debates in the philosophy of religion have been reawakened by the use of possible worlds. Intense debate has also emerged over the ontological status of possible worlds, provoked especially by David Lewis's defense of modal realism, the doctrine that talk about \"possible worlds\" is best explained in terms of innumerable, \"really existing\" worlds beyond the one we live in. The fundamental question here is: \"given\" that modal logic works, and that some possible-worlds semantics for modal logic is correct, \"what has to be true\" of the world, and just what \"are\" these possible worlds that we range over in our interpretation of modal statements? Lewis argued that what we range over are real, concrete \"worlds\" that exist just as unequivocally as our actual world exists, but that are distinguished from the actual world simply by standing in no spatial, temporal, or causal relations with the actual world. (On Lewis's account, the only \"special\" property that the \"actual\" world has is a relational one: that \"we\" are in it. This doctrine is called \"the indexicality of actuality\": \"actual\" is a merely indexical term, like \"now\" and \"here\".) Others, such as Robert Adams and William Lycan, reject Lewis's picture as metaphysically extravagant, and suggest in its place an interpretation of possible worlds as consistent, maximally complete sets of descriptions of or propositions about the world, so that a \"possible world\" is conceived of as a complete \"description\" of \"a way the world could be\" – rather than a \"world that is that way\". (Lewis describes their position, and similar positions such as those advocated by Alvin Plantinga and Peter Forrest, as \"\"ersatz\" modal realism\", arguing that such theories try to get the benefits of possible worlds semantics for modal logic \"on the cheap\", but that they ultimately fail to provide an adequate explanation.) Saul Kripke, in \"Naming and Necessity\", took explicit issue with Lewis's use of possible worlds semantics, and defended a \"stipulative\" account of possible worlds as purely \"formal\" (logical) entities rather than either really existent worlds or as some set of propositions or descriptions.\n\nPossible worlds theory in literary studies uses concepts from possible-world logic and applies them to worlds that are created by fictional texts, fictional universe. In particular, possible-world theory provides a useful vocabulary and conceptual framework with which to describe such worlds. However, a literary world is a specific type of possible world, quite distinct from the possible worlds in logic. This is because a literary text houses its own system of modality, consisting of actual worlds (actual events) and possible worlds (possible events). In fiction, the principle of simultaneity, it extends to cover the dimensional aspect, when it is contemplated that two or more physical objects, realities, perceptions and objects non-physical, can coexist in the same space-time. Thus, a literary universe is granted autonomy in much the same way as the actual universe.\n\nLiterary critics, such as Marie-Laure Ryan, Lubomír Doležel, and Thomas Pavel, have used possible-worlds theory to address notions of literary truth, the nature of fictionality, and the relationship between fictional worlds and reality. Taxonomies of fictional possibilities have also been proposed where the likelihood of a fictional world is assessed. Possible-world theory is also used within narratology to divide a specific text into its constituent worlds, possible and actual. In this approach, the modal structure of the fictional text is analysed in relation to its narrative and thematic concerns. Rein Raud has extended this approach onto \"cultural\" worlds, comparing possible worlds to the particular constructions of reality of different cultures. However, the metaphor of the \"cultural possible worlds\" relates to the framework of cultural relativism and, depending on the ontological status ascribed to possible worlds, warrants different, often controversial claims ranging from ethnocentrism to cultural imperialism.\n\n\n\n"}
{"id": "952842", "url": "https://en.wikipedia.org/wiki?curid=952842", "title": "Principle of compositionality", "text": "Principle of compositionality\n\nIn mathematics, semantics, and philosophy of language, the principle of compositionality is the principle that the meaning of a complex expression is determined by the meanings of its constituent expressions and the rules used to combine them. This principle is also called Frege's principle, because Gottlob Frege is widely credited for the first modern formulation of it. However, the idea appears already among Indian philosophers of grammar such as Yāska, and also in Plato's work such as in \"Theaetetus\".\nBesides, the principle was never explicitly stated by Frege, and it was arguably already assumed by George Boole decades before Frege’s work.\n\nThe principle of compositionality states that in a meaningful sentence, if the lexical parts are taken out of the sentence, what remains will be the rules of composition. Take, for example, the sentence \"Socrates was a man\". Once the meaningful lexical items are taken away—\"Socrates\" and \"man\"—what is left is the pseudo-sentence, \"S was a M\". The task becomes a matter of describing what the connection is between S and M.\n\nIt is frequently taken to mean that every operation of the syntax should be associated with an operation of the semantics that acts on the meanings of the constituents combined by the syntactic operation. As a guideline for constructing semantic theories, this is generally taken, as in the influential work on the philosophy of language by Donald Davidson, to mean that every construct of the syntax should be associated by a clause of the T-schema with an operator in the semantics that specifies how the meaning of the whole expression is built from constituents combined by the syntactic rule. In some general mathematical theories (especially those in the tradition of Montague grammar), this guideline is taken to mean that the interpretation of a language is essentially given by a homomorphism between an algebra of syntactic representations and an algebra of semantic objects.\n\nThe principle of compositionality also exists in a similar form in the compositionality of programming languages.\n\nThe principle of compositionality has been the subject of intense debate. Indeed, there is no general agreement as to how the principle is to be interpreted, although there have been several attempts to provide formal definitions of it. (Szabó, 2012)\n\nScholars are also divided as to whether the principle should be regarded as a factual claim, open to empirical testing; an analytic truth, obvious from the nature of language and meaning; or a methodological principle to guide the development of theories of syntax and semantics. The Principle of Compositionality has been attacked in all three spheres, although so far none of the criticisms brought against it have been generally regarded as compelling. Most proponents of the principle, however, make certain exceptions for idiomatic expressions in natural language. (Szabó, 2012)\n\nFurther, in the context of the philosophy of language, the principle of compositionality does not explain all of meaning. For example, you cannot infer sarcasm purely on the basis of words and their composition, yet a phrase used sarcastically means something completely different from the same phrase uttered straightforwardly. Thus, some theorists argue that the principle has to be revised to take into account linguistic and extralinguistic context, which includes the tone of voice used, common ground between the speakers, the intentions of the speaker, and so on. (Szabó, 2012)\n\n\n"}
{"id": "1166245", "url": "https://en.wikipedia.org/wiki?curid=1166245", "title": "Principle of distributivity", "text": "Principle of distributivity\n\nThe principle of distributivity states that the algebraic distributive law is valid for classical logic, where both logical conjunction and logical disjunction are distributive over each other so that for any propositions \"A\", \"B\" and \"C\" the equivalences\nand\nhold.\n\nThe principle of distributivity is valid in classical logic, but invalid in quantum logic.\n\nThe article \"Is Logic Empirical?\" discusses the case that quantum logic is the correct, empirical logic, on the grounds that the principle of distributivity is inconsistent with a reasonable interpretation of quantum phenomena.\n"}
{"id": "8924176", "url": "https://en.wikipedia.org/wiki?curid=8924176", "title": "Principle of nonvacuous contrast", "text": "Principle of nonvacuous contrast\n\nThe principle of nonvacuous contrast is a logical or methodological principle which requires that a genuine predicate never refer to everything, or to nothing, within its universe of discourse.\n"}
{"id": "1788944", "url": "https://en.wikipedia.org/wiki?curid=1788944", "title": "Principle of plenitude", "text": "Principle of plenitude\n\nThe principle of plenitude asserts that the universe contains all possible forms of existence. The historian of ideas Arthur Lovejoy was the first to trace the history of this philosophically important principle explicitly. Lovejoy distinguishes two versions of the principle: a static version, in which the universe displays a constant fullness and diversity, and a temporalized version, in which fullness and diversity gradually increase over time.\n\nLovejoy traces the principle of plenitude to the writings of Plato, finding in the \"Timaeus\" an insistence on \"the necessarily complete translation of all the ideal possibilities into actuality\". By contrast, he takes Aristotle to reject the principle in his \"Metaphysics\", when he writes that \"it is not necessary that everything that is possible should exist in actuality\".\n\nSince Plato, the principle of plenitude has had the following adherents:\n\n\n\n"}
{"id": "1137736", "url": "https://en.wikipedia.org/wiki?curid=1137736", "title": "Principle of sufficient reason", "text": "Principle of sufficient reason\n\nThe principle of sufficient reason states that everything must have a reason or a cause. The modern formulation of the principle is usually attributed to Gottfried Leibniz, although the idea was conceived of and utilized by various philosophers who preceded him, including Anaximander, Parmenides, Archimedes, Plato and Aristotle, Cicero, Avicenna, Thomas Aquinas, and Spinoza. Some philosophers have associated the principle of sufficient reason with \"ex nihilo nihil fit\". Hamilton identified the laws of inference modus ponens with the \"law of Sufficient Reason, or of Reason and Consequent\" and modus tollens with its contrapositive expression.\n\nThe principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\n\nA sufficient explanation may be understood either in terms of \"reasons\" or \"causes,\" for like many philosophers of the period, Leibniz did not carefully distinguish between the two. The resulting principle is very different, however, depending on which interpretation is given.\n\nIt is an open question whether the principle of sufficient reason can be applied to axioms within a logic construction like a mathematical or a physical theory, because axioms are propositions accepted as having no justification possible within the system\nThe principle declares that all propositions considered to be true within a system should be deducible from the set axioms at the base of the construction (with some theoretical exceptions: see Gödel's theorem).\n\nLeibniz identified two kinds of truth, necessary and contingent truths. He believed necessary mathematical truths to be derived from the law of identity (and the principle of contradiction): \"Necessary truths are those that can be demonstrated through an analysis of terms, so that in the end they become identities, just as in Algebra an equation expressing an identity ultimately results from the substitution of values [for variables]. That is, necessary truths depend upon the principle of contradiction.\" Leibniz states that the sufficient reason for necessary truths is that their negation is a contradiction.\n\nLeibniz admitted contingent truths on the basis of infinitary reasons, to which God had access but humans did not:\nIn contingent truths, even though the predicate is in the subject, this can never be demonstrated, nor can a proposition ever be reduced to an equality or to an identity, but the resolution proceeds to infinity, God alone seeing, not the end of the resolution, of course, which does not exist, but the connection of the terms or the containment of the predicate in the subject, since he sees whatever is in the series.Without this qualification, the principle can be seen as a description of a certain notion of closed system, in which there is no 'outside' to provide unexplained events with causes. It is also in tension with the paradox of Buridan's ass. Leibniz denied that the paradox of Buridan's ass could ever occur, saying:\n\nLeibniz also used the principle of sufficient reason to refute the idea of absolute space:\n\nI say then, that if space is an absolute being, there would be something for which it would be impossible there should be a sufficient reason. Which is against my axiom. And I prove it thus. Space is something absolutely uniform; and without the things placed in it, one point in space does not absolutely differ in any respect whatsoever from another point of space. Now from hence it follows, (supposing space to be something in itself, beside the order of bodies among themselves,) that 'tis impossible that there should be a reason why God, preserving the same situation of bodies among themselves, should have placed them in space after one particular manner, and not otherwise; why everything was not placed the quite contrary way, for instance, by changing East into West.\n\nThe principle was one of the four recognised laws of thought, that held a place in European pedagogy of logic and reasoning (and, to some extent, philosophy in general) in the 18th and 19th centuries. It was influential in the thinking of Leo Tolstoy, amongst others, in the elevated form that history could not be accepted as random.\n\nA sufficient reason is sometimes described as the coincidence of every single thing that is needed for the occurrence of an effect (i.e. of the so-called \"necessary conditions\"). Such view could perhaps be also applied to indeterministic systems, as long as randomness is in a way incorporated in the preconditions.\n\nHere is how Hamilton, circa 1837–1838, expressed his \"fourth law\" in his LECT. V. LOGIC. 60–61:\n\nAccording to Schopenhauer's \"On the Fourfold Root of the Principle of Sufficient Reason\", there are four distinct forms of the principle.\n\nFirst Form: The Principle of Sufficient Reason of Becoming (principium rationis sufficientis fiendi); appears as the law of causality in the understanding.\n\nSecond Form: The Principle of Sufficient Reason of Knowing (principium rationis sufficientis cognoscendi); asserts that if a judgment is to express a piece of knowledge, it must have a sufficient ground or reason, in which case it receives the predicate true.\n\nThird Form: The Principle of Sufficient Reason of Being (principium rationis sufficientis essendi); the law whereby the parts of space and time determine one another as regards those relations. Example in arithmetic: Each number presupposes the preceding numbers as grounds or reasons of its being; \"I can reach ten only by going through all the preceding numbers; and only by virtue of this insight into the ground of being, do I know that where there are ten, so are there eight, six, four.\"\n\n\"Now just as the subjective correlative to the first class of representations is the understanding, that to the second the faculty of reason, and that to the third pure sensibility, so is the subjective correlative to this fourth class found to be the inner sense, or generally self-consciousness.\" \n\nFourth Form: The Principle of Sufficient Reason of Acting (principium rationis sufficientis agendi); briefly known as the law of motivation. \"Any judgment that does not follow its previously existing ground or reason\" or any state that cannot be explained away as falling under the three previous headings \"must be produced by an act of will which has a motive.\" As his proposition in 43 states, \"Motivation is causality seen from within.\"\n\nSeveral proofs have been prepared in order to demonstrate that the universe is at bottom causal, i.e. works in accord with the principle in question; perhaps not in every single case (randomness might still play a part here and there), but that causality must be the way it works at least \"in general\", in most of what we see; and that our minds are aware of the principle even before any experience. The two famous arguments or proofs were proposed by Immanuel Kant (from the form of Time, temporal ordering of events and \"directionality\" of time) and by Arthur Schopenhauer (by demonstrating how all perception depends on causality and the intellect).\n\nOnce it is agreed (e.g. from a kind of an \"arrow of time\") that causal interconnections, as a form of principle of sufficient reason, indeed must in general exist everywhere in the universe (at least in the large scale), \"backwards\" causality in general might then be precluded using a form of the paradox of free will (i.e. an event that has a future source might cause us to remove that source quick enough and thus causality would not work).\n\n\n"}
{"id": "23556881", "url": "https://en.wikipedia.org/wiki?curid=23556881", "title": "Psychological continuum model", "text": "Psychological continuum model\n\nThe psychological continuum model (PCM) is a framework to organise prior literature from various academic disciplines to explain sport and event consumer behaviour. \nThe framework suggests four stages – awareness, attraction, attachment and allegiance to describe how sport and event involvement progressively develops with corresponding behaviours (e.g., playing, watching, buying). The PCM uses a vertical framework to characterise various psychological connections that individuals form with objects to explain the role of attitude formation and change that directs behaviours across a variety of consumption activities. Explaining the \"how\" and \"why\" of sport and event consumer behaviour, it discusses how personal, psychological and environmental factors influence a wide range of sport consumption activities.\n\nThe figure shows the four stages of the PCM - awareness, attraction, attachment and allegiance. On each stage, there is a horizontal decision making process. Inputs (green arrows) influence the internal processing (blue boxes) that creates outputs (yellow arrows). The outcomes are shown in the four different stages of the PCM (grey boxes). The unique decision making process is based upon the level of involvement of the consumer towards a sport/team/event. The following sequence is shown in each stage:\n\nInputs --> Internal Processing <--> Output\n\nThe PCM framework states that, through the processing of internal and external inputs, individuals progress upward along the four psychological connection stages. The overall evaluation of an object at a specific stage is the product of the processing of personal, psychological and environmental factors.\n\nAwareness stands for the notion when an individual first learns that a certain sport, event or team exists. In this stage the individual has not formed a preference or favourite. The PCM suggests that awareness of sport, teams and events stems from formal and informal channels, for examples parents, friends, school and media. In most cases awareness begins during childhood, but can also derive from other socializing agents. The value placed on the specific sport and event from a societal perspective is important in the awareness stage. The examples of \"I know about football\" and \"I know about Liverpool FC\" illustrate the awareness stage box.\n\nIn the attraction stage, the individual has a favourite sport, event, team or leisure hobby. Attraction is based upon a number of extrinsic and intrinsic motives. In other words, the sport, event, or leisure hobby provides the opportunity to satisfy needs and receive benefits. The motives stem from a combination of personal, psychological and environmental factors. The Attraction processing creates outcomes of positive affect and intentions, as well as engaging in consumption behaviour related to the sport and event. The examples of \"I like football\" and \"I like Liverpool FC\" illustrate the attraction stage box.\n\nIn the attachment stage the benefits and the sport object are internalised taking on a collective emotional, functional, and symbolic meaning. The psychological connection towards a sport, event, team or leisure hobby strengthens. Internal processes become more important and the influence of socializing agents decreases. Examples for the attachment stage are \"I am a football player\" or \"I am a Liverpool Fan\".\n\nAs the attachment processing continues, the internal collective meaning becomes more durable in terms of persistence and resistance and has greater impact on activities and behaviour. This is noted by the examples of \"I live for football\" and \"I live for Liverpool FC\" within the allegiance stage.\n"}
{"id": "33112023", "url": "https://en.wikipedia.org/wiki?curid=33112023", "title": "Rationes seminales", "text": "Rationes seminales\n\nRationes seminales (Latin, from the Greek \"λόγοι σπερματικοὶ\" or \"logoi spermatikoi\"), translated variously as germinal or causal principles, primordial reasons, original factors, seminal reasons or virtues, or seedlike principles, is a theological theory on the origin of species. It is the doctrine that God created the world in seed form, with certain potentialities, which then developed or unfolded accordingly over time; what appears to be change is simply the realization of the preexisting potentialities. The theory is a metaphor of the growth of a plant: much like a planted seed eventually develops into a tree, so when God created the world he planted \"rationes seminales\", from which all life sprung. It is intended to reconcile the belief that God created all things, with the evident fact that new things are constantly developing.\n\nThe roots of this idea can be found within the Greek philosophy of the Stoics and Neoplatonism\nThe idea was incorporated into Christian thought through the writings of authors such as Athenagoras of Athens, Tertullian, Gregory of Nyssa, Augustine of Hippo, Bonaventure, Albertus Magnus, and Roger Bacon, until mostly rejected in the modern period. Evolution, though now it is seen to be compatible with evolution theories (cf \"Man incarnate spirit\" by Ramon Lucas Lucas). The idea of \"rationes seminales\" was also used as an explanation for spontaneous generation.\n\n"}
{"id": "48000439", "url": "https://en.wikipedia.org/wiki?curid=48000439", "title": "Records Continuum Model", "text": "Records Continuum Model\n\nThe Records Continuum Model (RCM) was created in the 1990s by Monash University academic Frank Upward with input from colleagues Sue McKemmish and Livia Iacovino as a response to evolving discussions about the challenges of managing digital records and archives in the discipline of Archival Science. The RCM was first published in Upward’s 1996 paper \"Structuring the Records Continuum – Part One: Postcustodial principles and properties\". Upward describes the RCM within the broad context of a continuum where activities and interactions transform documents into records, evidence and memory that are used for multiple purposes over time. Upward places the RCM within a post-custodial, postmodern and structuration conceptual framework. Australian academics and practitioners continue to explore, develop and extend the RCM and records continuum theory, along with international collaborators, via the Records Continuum Research Group (RCRG) at Monash University.\n\nThe RCM is an abstract conceptual model that helps to understand and explore recordkeeping activities (as interaction) in relation to multiple contexts over space and time (spacetime). Recordkeeping activities take place from before the records are created by identifying recordkeeping requirements in policies, systems, organizations, processes, laws, social mandates that impact on what is created and how it is managed over spacetime. In a continuum, recordkeeping processes, such as adding metadata, fix documents so that they can be managed as evidence. Those records deemed as having continuing value are retained and managed as an archive. The implication of an RCM approach to records and archives is that systems and processes can be designed and put in place before records are even created. A continuum approach therefore highlights that records are both current and archival at the point of creation.\n\nThe RCM is represented as a series of concentric rings (dimensions of \"Create\", \"Capture\", \"Organize\" and \"Pluralize\") and crossed axes (transactionality, evidentiality, recordkeeping and identity) with each axis labelled with a description of the activity or interaction that occurs at that intersection. \"Create\", \"Capture\", \"Organize\" and \"Pluralize\" represent recordkeeping activities that occur within spacetime. Activities that occur in these dimensions across the axes are explained in the table below:\n\nThe value of the RCM is that it can help to map where on a continuum recordkeeping activities are or can be placed. The RCM can then be used to explore the conceptual and practical assumptions that underpin the practice, in particular the dualisms inherent in the usage and practice of the terms \"records\" and \"archives\". This definition lends itself to a linear reading of the RCM – starting at \"Create\" as the initiating phase and working outwards towards \"Pluralization\" of recorded information. Another linear reading is to consider design first – the role that systems of \"Pluralization\" and \"Organization\" play in designing, planning and implementing recordkeeping and then considering the implications for \"Create\" and \"Capture\". However, these are just two of many ways to interpret the model as the dimensions and axes represent multiple realities that occur within spacetime, any of which can occur simultaneously, concurrently and sequentially in electronic or digital environments, and/or physical spaces.\n\nBy representing multiple realities, the RCM articulates the numerous and diverse perspectives that contribute to records and archives including individual, group, community, organizational, institutional and societal. These contexts reveal the need to take into account various stakeholders and co-contributors in relation to use, access and appraisal of records and archives. Over the lifespan of a record multiple decisions are made by various stakeholders of the records that include, but are not limited to records managers and archivists. Other stakeholders can be identified at various dimensions of interaction, including those involved in providing information (not only the person or organization who produced or captured it), as well as their family and community. Records are therefore not simply physical or digital representations of physical objects held and managed in an archive or repository, but are evidence of multiple perspectives, narratives and contexts that contributed to their formation.\n\nThe RCM is often described as being in contrast or at odds with the lifecycle records model. While the RCM is inclusive of multiple ways of conceptualizing and performing recordkeeping, including a lifecycle approach, there are some significant differences. Firstly, where the lifecycle approach shows clearly demarcated phases in the management of records, a continuum approach conceptualizes elements as continuous with no discernable parts. Secondly, the lifecycle approach identifies clear conceptual and procedural boundaries between active or current records and inactive or historical records, but a continuum approach sees records processes as more integrated across spacetime. In the continuum it is recordkeeping processes that carry records forward through spacetime to enable their use for multiple purposes. What this means is that records are always \"in a state of always becoming...\", and able to contribute new contexts via the recordkeeping processes that occur with them. Archival records are therefore not just historical, but are able to be re-interpreted, re-created, and re-contextualized according to their place and use in spacetime. In this way, archival institutions are nodes in the network of recorded information and its contexts, rather than the end point in a lifecycle stage for records that are managed as \"relics\".\n\nThe RCM is a representation of what is commonly referred to as records continuum theory, as well as Australian continuum thinking and/or approaches. These ideas were evolved as part of an Australian approach to archival management espoused by Ian Maclean, Chief Archivist of the Commonwealth Archives Office in Australia in the 1950s and 1960s. Maclean, whose ideas and practices were the subject of the first RCRG publication in 1994, referred in a 1959 \"American Archivist\" article to a \"continuum of (public) records administration\" from administrative efficiency through recordkeeping to the safe keeping of a \"cultural end-product\". Maclean’s vision challenged the divide between current recordkeeping and archival practice. Fellow contemporary at the Commonwealth Archives Office Peter Scott is also included as a core influence on Australian records continuum theory with his development of the Australian Series System, a registry system that helped identify and document the complex and multiple \"social, functional, provenancial, and documentary relationships\" involved in managing records and recordkeeping processes over spacetime.\n\nFurther influences on the RCRG group include archival professionals and researchers like David Bearman and his work on transactionality and systems thinking, and Terry Cook's ideas about postcustodialism and macroappraisal. Wider influencing ideas include those from philosophers and social theorists Jacques Lacan, Michel Foucault, Jacques Derrida, and Jean-François Lyotard, as well as sociologist Anthony Giddens, with structuration theory being a core component of understanding social interaction over spacetime. Canadian archivist Jay Atherton's critique of the division between records managers and archivists in the 1980s and use of the term \"records continuum\" re-commenced the conversation MacLean began during his career and helped to bring his ideas and this term to Australian records continuum thinking. Atherton's use of the term records continuum has several significant differences in conception, application and heritage when compared to Australian records continuum thinking.\n\nPost-custodiality as an archival concept plays a major role in how the RCM was conceived. This term was born from an identified and urgent need to address the complexities of computer technologies on records creation and management over time and space. Post-custodiality is discussed by Frank Upward and Sue McKemmish in 1994 as part of an exploration of changes in archival discourse commencing in the 1980s by Gerald Ham and expanded on by Terry Cook as part of a \"post-custodial paradigm shift\". Post-custodiality in relation to the RCM is explored by Upward and McKemmish as an entry point into a wider conversation about records and recordkeeping being part of a process in which archival institutions have a part to play beyond that of the archival authority handling, appraisal, describing and arranging physical objects in their custody.\n\nDrawing from the above theoretical foundations, the RCM as a framework acknowledges the central role that recordkeeping activities have on the creation, capture, organization and ongoing management of records over time and throughout spaces such as organizations and institutional archives. Recordkeeping is a practice and a concept clearly defined in the archival and records literature by continuum writers as \"a broad and inclusive concept of integrated recordkeeping and archiving processes for current, regulatory, and historical recordkeeping purposes\". Recordkeeping refers to the activities performed on records that add new contexts such as capturing a record into a system, adding metadata, or selecting it for an archive. In the RCM records are therefore not defined according to their status as objects. Rather, records are understood as being part of a continuum of activity related to known (as well as potentially unknown) contexts. A record (as well as records, collections and archives) are therefore part of larger social, cultural, political, legal and archival processes. It is these contexts that are vital to understanding the role, value and evidential qualities of records in and across spacetime (past, present and potential future).\n\nThe RCM is the most well-known of all the continuum models created, but does not exist in isolation. Several other complementary models have been created by RCM creator Frank Upward, and there are others created by continuum researchers that offer enhanced or alternative ways of understanding the continuum.\n\nThe series of continuum models created by Frank Upward include:\n\nModels created in collaboration:\n\nOther models:\n\n"}
{"id": "6394087", "url": "https://en.wikipedia.org/wiki?curid=6394087", "title": "Revelation principle", "text": "Revelation principle\n\nThe revelation principle is a fundamental principle in mechanism design. It states that if a social choice function can be implemented by an arbitrary mechanism (i.e. if that mechanism has an equilibrium outcome that corresponds to the outcome of the social choice function), then the same function can be implemented by an incentive-compatible-direct-mechanism (i.e. in which players truthfully report type) with the same equilibrium outcome (payoffs).\n\nIn mechanism design, the revelation principle is of utmost importance in finding solutions. The researcher need only look at the set of equilibrium characterized by incentive compatibility. That is, if the mechanism designer wants to implement some outcome or property, he can restrict his search to mechanisms in which agents are willing to reveal their private information to the mechanism designer that has that outcome or property. If no such direct and truthful mechanism exists, no mechanism can implement this outcome/property. By narrowing the area needed to be searched, the problem of finding a mechanism becomes much easier.\n\nThe principle comes in two variants corresponding to the two flavors of incentive-compatibility:\n\nConsider the following example. There is a certain item that Alice values as formula_1 and Bob values as formula_2. The government needs to decide who will receive that item and in what terms. \n\nSuppose we have an arbitrary mechanism Mech that implements Soc.\n\nWe construct a direct mechanism Mech' that is truthful and implements Soc.\n\nMech' simply simulates the equilibrium strategies of the players in Game(Mech). I.e:\n\nReporting the true valuations in Mech' is like playing the equilibrium strategies in Mech. Hence, reporting the true valuations is a Nash equilibrium in Mech', as desired. Moreover, the equilibrium payoffs are the same, as desired.\n\nThe revelation principle says that for every arbitrary \"coordinating device\" a.k.a. correlating there exists another direct device for which the state space equals the action space of each player. Then the coordination is done by directly informing each player of his action.\n\n"}
{"id": "25606638", "url": "https://en.wikipedia.org/wiki?curid=25606638", "title": "Search neutrality", "text": "Search neutrality\n\nSearch neutrality is a principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance. This means that when a user queries a search engine, the engine should return the most relevant results found in the provider's domain (those sites which the engine has knowledge of), without manipulating the order of the results (except to rank them by relevance), excluding results, or in any other way manipulating the results to a certain bias. \nSearch neutrality is related to network neutrality in that they both aim to keep any one organization from limiting or altering a user's access to services on the Internet. Search neutrality aims to keep the organic search results (results returned because of their relevance to the search terms, as opposed to results sponsored by advertising) of a search engine free from any manipulation, while network neutrality aims to keep those who provide and govern access to the Internet from limiting the availability of resources to access any given content.\n\nThe term \"search neutrality\" in context of the internet appears as early as March 2009 in an academic paper by Andrew Odlyzko titled, \"Network Neutrality, Search Neutrality, and the Never-ending Conflict between Efficiency and Fairness in Markets\". In this paper, Odlykzo predicts that if net neutrality were to be accepted as a legal or regulatory principle, then the questions surrounding search neutrality would be the next controversies. Indeed, in December 2009 the New York Times published an opinion letter by Foundem co-founder and lead complainant in an anti-trust complaint against Google, Adam Raff, which likely brought the term to the broader public. According to Raff in his opinion letter, search neutrality ought to be \"the principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance\". On October 11, 2009, Adam and his wife Shivaun launched SearchNeutrality.org, an initiative dedicated to promoting investigations against Google's search engine practices . There, the Raffs note that they chose to frame their issue with Google as \"search neutrality\" in order to benefit from the focus and interest on net neutrality.\n\nIn contrast to net neutrality, answers to such questions, as \"what is search neutrality?\" or \"what are appropriate legislative or regulatory principles to protect search neutrality?\", appear to have less consensus. The idea that neutrality means equal treatment, regardless of the content, comes from debates on net neutrality . Neutrality in search is complicated by the fact that search engines, by design and in implementation, are not intended to be neutral or impartial. Rather, search engines and other information retrieval applications are designed to collect and store information (indexing), receive a query from a user, search for and filter relevant information based on that query (searching/filtering), and then present the user with only a subset of those results, which are ranked from most relevant to least relevant (ranking). \"Relevance\" is a form of bias used to favor some results and rank those favored results. Relevance is defined in the search engine so that a user is satisfied with the results and is therefore subject to the user's preferences. And because relevance is so subjective, putting search neutrality into practice has been so contentious.\n\nSearch neutrality became a concern after search engines, most notably Google, were accused of search bias by other companies. Competitors and companies claim search engines systematically favor some sites (and some kind of sites) over others in their lists of results, disrupting the objective results users believe they are getting. \n\nThe call for search neutrality goes beyond traditional search engines. Sites like Amazon.com and Facebook are also accused of skewing results. Amazon’s search results are influenced by companies that pay to rank higher in their search results while Facebook filters their newsfeed lists to conduct social experiments.\n\nIn order to find information on the Web, most users make use of search engines, which crawl the web, index it and show a list of results ordered by relevance. The use of search engines to access information through the web has become a key factor for online businesses companies, which depend on the flow of users visiting their pages. One of these companies is Foundem. Foundem provides a \"vertical search\" service to compare products available on online markets for the U.K. Many people see these \"vertical search\" sites as spam. Beginning in 2006 and for three and a half years following, Foundem’s traffic and business dropped significantly due to what they assert to be a penalty deliberately applied by Google. It is unclear, however, whether their claim of a penalty was self-imposed via their use of iframe HTML tags to embed the content from other websites. At the time at which Foundem claims the penalties were imposed, it was unclear whether web crawlers crawled beyond the main page of a website using iframe tags without some extra modifications. The former SEO director OMD UK, Jaamit Durrani, among others, offered this alternative explanation, stating that “Two of the major issues that Foundem had in summer was content in iFrames and content requiring javascript to load – both of which I looked at in August, and they were definitely in place. Both are huge barriers to search visibility in my book. They have been fixed somewhere between then and the lifting of the supposed ‘penalty’. I don’t think that’s a coincidence.” \n\nMost of Foundem’s accusations claim that Google deliberately applies penalties to other vertical search engines because they represent competition. Foundem is backed by a Microsoft proxy group, the 'Initiative for Competitive Online Marketplace'.\n\nThe following table details Foundem's chronology of events as found on their website:\n\nGoogle's large market share (85%) has made them a target for search neutrality litigation via antitrust laws. In February 2010, Google released an article on the Google Public Policy blog expressing their concern for fair competition, when other companies at the UK joined Foundem's cause (eJustice.fr, and Microsoft's Ciao! from Bing) also claiming being unfairly penalized by Google.\n\nAfter two years of looking into claims that Google “manipulated its search algorithms to harm vertical websites and unfairly promote its own competing vertical properties,” the Federal Trade Commission (FTC) voted unanimously to end the antitrust portion of its investigation without filing a formal complaint against Google. The FTC concluded that Google’s “practice of favoring its own content in the presentation of search results” did not violate U.S. antitrust laws. The FTC further determined that even though competitors might be negatively impacted by Google's changing algorithms, Google did not change its algorithms to hurt competitors, but as a product improvement to benefit consumers.\n\nThere are a number of arguments for and against search neutrality.\n\n\n\nGoogle’s \"Universal Search\" system has been identified as one of the least neutral search engine practices, and following the implementation of Universal Search websites, such as MapQuest, the company experienced a massive decline in web traffic. This decline has been attributed to Google linking to its own services rather than the services offered at external websites. Despite these claims, Google actually displays Google content on the first page, while rival search engines do not considerably less often than Microsoft's Bing which displays Microsoft content when rivals do not. Bing displays Microsoft content in first place more than twice as often as Google shows Google content in first place. This indicates that as far as there is any 'bias', Google is less biased than its principal competitor.\n"}
{"id": "37055344", "url": "https://en.wikipedia.org/wiki?curid=37055344", "title": "Triangle of opposition", "text": "Triangle of opposition\n\nIn the system of Aristotelian logic, the triangle of opposition is a diagram representing the different ways in which each of the three propositions of the system is logically related ('opposed') to each of the others. The system is also useful in the analysis of syllogistic logic, serving to identify the allowed logical conversions from one type to another.\n\n"}
{"id": "25512250", "url": "https://en.wikipedia.org/wiki?curid=25512250", "title": "Truth table", "text": "Truth table\n\nA truth table is a mathematical table used in logic—specifically in connection with Boolean algebra, boolean functions, and propositional calculus—which sets out the functional values of logical expressions on each of their functional arguments, that is, for each combination of values taken by their logical variables (Enderton, 2001). In particular, truth tables can be used to show whether a propositional expression is true for all legitimate input values, that is, logically valid.\n\nA truth table has one column for each input variable (for example, P and Q), and one final column showing all of the possible results of the logical operation that the table represents (for example, P XOR Q). Each row of the truth table contains one possible configuration of the input variables (for instance, P=true Q=false), and the result of the operation for those values. See the examples below for further clarification. Ludwig Wittgenstein is often credited with inventing the truth table in his \"Tractatus Logico-Philosophicus\", though it appeared at least a year earlier in a paper on propositional logic by Emil Leon Post.\n\nThere are 4 unary operations:\n\nThe output value is always true, regardless of the input value of p\n\nThe output value is never true: that is, always false, regardless of the input value of p\nLogical identity is an operation on one logical value p, for which the output value remains p.\n\nThe truth table for the logical identity operator is as follows:\n\nLogical negation is an operation on one logical value, typically the value of a proposition, that produces a value of \"true\" if its operand is false and a value of \"false\" if its operand is true.\n\nThe truth table for NOT p (also written as ¬p, Np, Fpq, or ~p) is as follows:\n\nThere are 16 possible truth functions of two binary variables:\n\nHere is an extended truth table giving definitions of all possible truth functions of two Boolean variables P and Q:\n\nwhere\n\nThe four combinations of input values for p, q, are read by row from the table above.\nThe output function for each p, q combination, can be read, by row, from the table.\n\nKey:\n\nThe following table is oriented by column, rather than by row. There are four columns rather than four rows, to display the four combinations of p, q, as input. \n\np: T T F F<br>\nq: T F T F\n\nThere are 16 rows in this key, one row for each binary function of the two binary variables, p, q. For example, in row 2 of this Key, the value of Converse nonimplication ('formula_1') is solely T, for the column denoted by the unique combination p=F, q=T; while in row 2, the value of that 'formula_1' operation is F for the three remaining columns of p, q. The output row for formula_1 is thus\n\n2: F F T F\n\nand the 16-row key is\n\nLogical operators can also be visualized using Venn diagrams.\n\nLogical conjunction is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if both of its operands are true.\n\nThe truth table for p AND q (also written as p ∧ q, Kpq, p & q, or p formula_4 q) is as follows:\n\nIn ordinary language terms, if both \"p\" and \"q\" are true, then the conjunction \"p\" ∧ \"q\" is true. For all other assignments of logical values to \"p\" and to \"q\" the conjunction \"p\" ∧ \"q\" is false.\n\nIt can also be said that if \"p\", then \"p\" ∧ \"q\" is \"q\", otherwise \"p\" ∧ \"q\" is \"p\".\n\nLogical disjunction is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if at least one of its operands is true.\n\nThe truth table for p OR q (also written as p ∨ q, Apq, p || q, or p + q) is as follows:\n\nStated in English, if \"p\", then \"p\" ∨ \"q\" is \"p\", otherwise \"p\" ∨ \"q\" is \"q\".\n\nLogical implication and the material conditional are both associated with an operation on two logical values, typically the values of two propositions, which produces a value of \"false\" if the first operand is true and the second operand is false, and a value of \"true\" otherwise.\n\nThe truth table associated with the logical implication p implies q (symbolized as p ⇒ q, or more rarely Cpq) is as follows:\n\nThe truth table associated with the material conditional if p then q (symbolized as p → q) is as follows:\n\nIt may also be useful to note that p ⇒ q and p → q are equivalent to ¬p ∨ q.\n\nLogical equality (also known as biconditional) is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if both operands are false or both operands are true.\n\nThe truth table for p XNOR q (also written as p ↔ q, Epq, p = q, or p ≡ q) is as follows:\n\nSo p EQ q is true if p and q have the same truth value (both true or both false), and false if they have different truth values.\n\nExclusive disjunction is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if one but not both of its operands is true.\n\nThe truth table for p XOR q (also written as p ⊕ q, Jpq, p ≠ q, or p ↮ q) is as follows:\n\nFor two propositions, XOR can also be written as (p ∧ ¬q) ∨ (¬p ∧ q).\n\nThe logical NAND is an operation on two logical values, typically the values of two propositions, that produces a value of \"false\" if both of its operands are true. In other words, it produces a value of \"true\" if at least one of its operands is false.\n\nThe truth table for p NAND q (also written as p ↑ q, Dpq, or p | q) is as follows:\n\nIt is frequently useful to express a logical operation as a compound operation, that is, as an operation that is built up or composed from other operations. Many such compositions are possible, depending on the operations that are taken as basic or \"primitive\" and the operations that are taken as composite or \"derivative\".\n\nIn the case of logical NAND, it is clearly expressible as a compound of NOT and AND.\n\nThe negation of a conjunction: ¬(\"p\" ∧ \"q\"), and the disjunction of negations: (¬\"p\") ∨ (¬\"q\") can be tabulated as follows:\n\nThe logical NOR is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if both of its operands are false. In other words, it produces a value of \"false\" if at least one of its operands is true. ↓ is also known as the Peirce arrow after its inventor, Charles Sanders Peirce, and is a Sole sufficient operator.\n\nThe truth table for p NOR q (also written as p ↓ q, or Xpq) is as follows:\n\nThe negation of a disjunction ¬(\"p\" ∨ \"q\"), and the conjunction of negations (¬\"p\") ∧ (¬\"q\") can be tabulated as follows:\n\nInspection of the tabular derivations for NAND and NOR, under each assignment of logical values to the functional arguments \"p\" and \"q\", produces the identical patterns of functional values for ¬(\"p\" ∧ \"q\") as for (¬\"p\") ∨ (¬\"q\"), and for ¬(\"p\" ∨ \"q\") as for (¬\"p\") ∧ (¬\"q\"). Thus the first and second expressions in each pair are logically equivalent, and may be substituted for each other in all contexts that pertain solely to their logical values.\n\nThis equivalence is one of De Morgan's laws.\n\nTruth tables can be used to prove many other logical equivalences. For example, consider the following truth table:\n\nThis demonstrates the fact that formula_5 is logically equivalent to formula_6.\n\nHere is a truth table that gives definitions of the 6 most commonly used out of the 16 possible truth functions of two Boolean variables P and Q:\n\nwhere\n\nFor binary operators, a condensed form of truth table is also used, where the row headings and the column headings specify the operands and the table cells specify the result. For example, Boolean logic uses this condensed truth table notation:\n\nThis notation is useful especially if the operations are commutative, although one can additionally specify that the rows are the first operand and the columns are the second operand. This condensed notation is particularly useful in discussing multi-valued extensions of logic, as it significantly cuts down on combinatoric explosion of the number of rows otherwise needed. It also provides for quickly recognizable characteristic \"shape\" of the distribution of the values in the table which can assist the reader in grasping the rules more quickly.\n\nTruth tables are also used to specify the function of hardware look-up tables (LUTs) in digital logic circuitry. For an n-input LUT, the truth table will have 2^\"n\" values (or rows in the above tabular format), completely specifying a boolean function for the LUT. By representing each boolean value as a bit in a binary number, truth table values can be efficiently encoded as integer values in electronic design automation (EDA) software. For example, a 32-bit integer can encode the truth table for a LUT with up to 5 inputs.\n\nWhen using an integer representation of a truth table, the output value of the LUT can be obtained by calculating a bit index \"k\" based on the input values of the LUT, in which case the LUT's output value is the \"k\"th bit of the integer. For example, to evaluate the output value of a LUT given an array of \"n\" boolean input values, the bit index of the truth table's output value can be computed as follows: if the \"i\"th input is true, let formula_14, else let formula_15. Then the \"k\"th bit of the binary representation of the truth table is the LUT's output value, where formula_16.\n\nTruth tables are a simple and straightforward way to encode boolean functions, however given the exponential growth in size as the number of inputs increase, they are not suitable for functions with a large number of inputs. Other representations which are more memory efficient are text equations and binary decision diagrams.\n\nIn digital electronics and computer science (fields of applied logic engineering and mathematics), truth tables can be used to reduce basic boolean operations to simple correlations of inputs to outputs, without the use of logic gates or code. For example, a binary addition can be represented with the truth table:\n\nThis truth table is read left to right:\n\nNote that this table does not describe the logic operations necessary to implement this operation, rather it simply specifies the function of inputs to output values.\n\nWith respect to the result, this example may be arithmetically viewed as modulo 2 binary addition, and as logically equivalent to the exclusive-or (exclusive disjunction) binary logic operation.\n\nIn this case it can be used for only very simple inputs and outputs, such as 1s and 0s. However, if the number of types of values one can have on the inputs increases, the size of the truth table will increase.\n\nFor instance, in an addition operation, one needs two operands, A and B. Each can have one of two values, zero or one. The number of combinations of these two values is 2×2, or four. So the result is four possible outputs of C and R. If one were to use base 3, the size would increase to 3×3, or nine possible outputs.\n\nThe first \"addition\" example above is called a half-adder. A full-adder is when the carry from the previous operation is provided as input to the next adder. Thus, a truth table of eight rows would be needed to describe a full adder's logic:\n\nIrving Anellis has done the research to show that C.S. Peirce appears to be the earliest logician (in 1893) to devise a truth table matrix. From the summary of his paper:\n\n\n"}
{"id": "172990", "url": "https://en.wikipedia.org/wiki?curid=172990", "title": "Use–mention distinction", "text": "Use–mention distinction\n\nThe use–mention distinction is a foundational concept of analytic philosophy, according to which it is necessary to make a distinction between a word (or phrase) and it, and many philosophical works have been \"vitiated by a failure to distinguish use and mention\". The distinction is disputed by non-analytic philosophers.\n\nThe distinction between use and mention can be illustrated for the word \"cheese\":\n\nThe first sentence is a statement about the substance called \"cheese\"; it uses the word 'cheese' to refer to that substance. The second is a statement about the word 'cheese' as a signifier; it mentions the word without using it to refer to anything other than itself.\n\nIn written language, mentioned words or phrases often appear between quotation marks (as in Chicago' contains three vowels\") or in italics (as in \"When I say \"honey\", I mean the sweet stuff that bees make\"), and style authorities such as \"Strunk and White\" insist that mentioned words or phrases must always be made visually distinct in this manner. Used words or phrases (much more common than mentioned ones) do not bear any typographic distinction. In spoken language, or in absence of the use of stylistic cues such as quotation marks or italics in written language, the audience must identify mentioned words or phrases through semantic and pragmatic cues.\n\nIf quotation marks are used, it is sometimes the practice to distinguish between the quotation marks used for speech and those used for mentioned words, with double quotes in one place and single in the other:\n\nA few authorities recommend against such a distinction, and prefer one style of quotation mark to be used for both purposes.\n\nThe general phenomenon of a term's having different references in different contexts was called \"suppositio\" (substitution) by medieval logicians. It describes how one has to substitute a term in a sentence based on its meaning—that is, based on the term's referent. In general, a term can be used in several ways. For nouns, they are:\n\nThe last sentence contains a mention example.\n\nThe use–mention distinction is especially important in analytic philosophy. Failure to properly distinguish use from mention can produce false, misleading, or meaningless statements or category errors. For example, the following correctly distinguish between use and mention:\n\nThe first sentence, a mention example, is a statement about the word \"copper\" and not the chemical element. Notably, the word is composed of six letters, but not any kind of metal or other tangible thing. The second sentence, a use example, is a statement about the chemical element copper and not the word itself. Notably, the element is composed of 29 electrons and protons and a number of neutrons, but not any letters.\n\nStanisław Leśniewski was perhaps the first to make widespread use of this distinction and the fallacy that arises from overlooking it, seeing it all around in analytic philosophy of the time, for example in Russell and Whitehead's \"Principia Mathematica\". At the logical level, a use–mention mistake occurs when two heterogeneous levels of meaning or context are confused inadvertently.\nDonald Davidson told that in his student years, \"quotation was usually introduced as a somewhat shady device, and the introduction was accompanied by a stern sermon on the sin of confusing the use and mention of expressions\". He presented a class of sentences like\n\nwhich both use the meaning of the quoted words to complete the sentence, and mention them as they are attributed to W. V. Quine, to argue against his teachers' hard distinction. His claim was that quotations could not be analyzed as simple expressions that mention their content by means of naming it or describing its parts, as sentences like the above would lose their exact, twofold meaning.\n\nSelf-referential statements mention themselves or their components, often producing logical paradoxes, such as Quine's paradox. A mathematical analogy of self-referential statements lies at the core of Gödel's incompleteness theorem (diagonal lemma). There are many examples of self-reference and use–mention distinction in the works of Douglas Hofstadter, who makes the distinction thus:\n\nAlthough the standard notation for mentioning a term in philosophy and logic is to put the term in quotation marks, issues arise when the mention is itself of a mention. Notating using italics might require a potentially infinite number of typefaces, while putting quotation marks within quotation marks may lead to ambiguity.\n\nSome analytic philosophers have said the distinction \"may seem rather pedantic\".\n\nIn a 1977 response to analytic philosopher John Searle, Jacques Derrida mentioned the distinction as \"rather laborious and problematical\".\n\n\n\n\n"}
