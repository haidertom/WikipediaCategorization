{"id": "6679056", "url": "https://en.wikipedia.org/wiki?curid=6679056", "title": "A priori and a posteriori", "text": "A priori and a posteriori\n\nThe Latin phrases a priori ( \"from the earlier\") and a posteriori ( \"from the later\") are philosophical terms of art popularized by Immanuel Kant's \"Critique of Pure Reason\" (first published in 1781, second edition in 1787), one of the most influential works in the history of philosophy. However, in their Latin forms they appear in Latin translations of Euclid's \"Elements\", of about 300 , a work widely considered during the early European modern period as the model for precise thinking.\n\nThese terms are used with respect to reasoning (epistemology) to distinguish \"necessary conclusions from first premises\" (i.e., what must come before sense observation) from \"conclusions based on sense observation\" which must follow it. Thus, the two kinds of knowledge, justification, or argument, may be glossed:\n\n\nThere are many points of view on these two types of knowledge, and their relationship gives rise to one of the oldest problems in modern philosophy.\n\nThe terms \"a priori\" and \"a posteriori\" are primarily used as adjectives to modify the noun \"knowledge\" (for example, \"\"a priori\" knowledge\"). However, \"a priori\" is sometimes used to modify other nouns, such as \"truth\". Philosophers also may use \"apriority\" and \"aprioricity\" as nouns to refer (approximately) to the quality of being \"a priori\".\n\nAlthough definitions and use of the terms have varied in the history of philosophy, they have consistently labeled two separate epistemological notions. See also the related distinctions: deductive/inductive, analytic/synthetic, necessary/contingent.\n\nThe intuitive distinction between \"a priori\" and \"a posteriori\" knowledge (or justification) is best seen via examples, as below:\n\n\nSeveral philosophers reacting to Kant sought to explain \"a priori\" knowledge without appealing to, as Paul Boghossian (MD) explains, \"a special faculty ... that has never been described in satisfactory terms.\" One theory, popular among the logical positivists of the early 20th century, is what Boghossian calls the \"analytic explanation of the a priori.\" The distinction between analytic and synthetic propositions was first introduced by Kant. While Kant's original distinction was primarily drawn in terms of conceptual containment, the contemporary version of the distinction primarily involves, as the American philosopher W. V. O. Quine put it, the notions of \"true by virtue of meanings and independently of fact.\" \"Analytic\" propositions are thought to be true in virtue of their meaning alone, while \"a posteriori analytic\" propositions are thought to be true in virtue of their meaning \"and\" certain facts about the world. According to the analytic explanation of the \"a priori\", all \"a priori\" knowledge is analytic; so \"a priori\" knowledge need not require a special faculty of pure intuition, since it can be accounted for simply by one's ability to understand the meaning of the proposition in question. In short, proponents of this explanation claimed to have reduced a dubious metaphysical faculty of pure reason to a legitimate linguistic notion of analyticity.\n\nHowever, the analytic explanation of \"a priori\" knowledge has undergone several criticisms. Most notably, Quine argued that the analytic–synthetic distinction is illegitimate. Quine states: \"But for all its a priori reasonableness, a boundary between analytic and synthetic statements simply has not been drawn. That there is such a distinction to be drawn at all is an unempirical dogma of empiricists, a metaphysical article of faith.\" While the soundness of Quine's critique is highly disputed, it had a powerful effect on the project of explaining the \"a priori\" in terms of the analytic.\n\nThe metaphysical distinction between necessary and contingent truths has also been related to \"a priori\" and \"a posteriori\" knowledge. A proposition that is \"necessarily true\" is one whose negation is self-contradictory (thus, it is said to be true in every possible world). Consider the proposition that all bachelors are unmarried. Its negation, the proposition that some bachelors are married, is incoherent, because the concept of being unmarried (or the meaning of the word \"unmarried\") is part of the concept of being a bachelor (or part of the definition of the word \"bachelor\"). To the extent that contradictions are impossible, self-contradictory propositions are necessarily false, because it is impossible for them to be true. Thus, the negation of a self-contradictory proposition is supposed to be necessarily true. By contrast, a proposition that is \"contingently true\" is one whose negation is not self-contradictory (thus, it is said that it is \"not\" true in every possible world). As Jason Baehr states, it seems plausible that all necessary propositions are known \"a priori\", because \"[s]ense experience can tell us only about the actual world and hence about what is the case; it can say nothing about what must or must not be the case.\"\n\nFollowing Kant, some philosophers have considered the relationship between aprioricity, analyticity, and necessity to be extremely close. According to Jerry Fodor, \"Positivism, in particular, took it for granted that \"a priori\" truths must be necessary...\" However, since Kant, the distinction between analytic and synthetic propositions had slightly changed. Analytic propositions were largely taken to be \"true by virtue of meanings and independently of fact\", while synthetic propositions were not—one must conduct some sort of empirical investigation, looking to the world, to determine the truth-value of synthetic propositions.\n\nAprioricity, analyticity, and necessity have since been more clearly separated from each other. The American philosopher Saul Kripke (1972), for example, provided strong arguments against this position. Kripke argued that there are necessary \"a posteriori\" truths, such as the proposition that water is HO (if it is true). According to Kripke, this statement is necessarily true (since water and HO are the same thing, they are identical in every possible world, and truths of identity are logically necessary) and \"a posteriori\" (since it is known only through empirical investigation). Following such considerations of Kripke and others (such as Hilary Putnam), philosophers tend to distinguish more clearly the notion of aprioricity from that of necessity and analyticity.\n\nKripke's definitions of these terms, however, diverge in subtle ways from those of Kant. Taking these differences into account, Kripke's controversial analysis of naming as contingent and \"a priori\" would, according to Stephen Palmquist, best fit into Kant's epistemological framework by calling it \"analytic a posteriori\". Aaron Sloman presented a brief defence of Kant's three distinctions (analytic/synthetic, apriori/empirical and necessary/contingent) in . It did not assume \"possible world semantics\" for the third distinction, merely that some part of \"this\" world might have been different.\n\nThus, the relationship between aprioricity, necessity, and analyticity is not easy to discern. However, most philosophers at least seem to agree that while the various distinctions may overlap, the notions are clearly not identical: the \"a priori\"/\"a posteriori\" distinction is epistemological, the analytic/synthetic distinction is linguistic, and the necessary/contingent distinction is metaphysical.\n\nThe phrases \"\"a priori\" and \"a posteriori\"\" are Latin for \"from what comes before\" and \"from what comes later\" (or, less literally, \"from first principles, before experience\" and \"after experience\"). They appear in Latin translations of Euclid's \"Elements\", of about 300 , a work widely considered during the early European modern period as the model for precise thinking.\n\nAn early philosophical use of what might be considered a notion of \"a priori\" knowledge (though not called by that name) is Plato's theory of recollection, related in the dialogue \"Meno\" (380 ), according to which something like \"a priori\" knowledge is knowledge inherent, intrinsic in the human mind.\n\nAlbert of Saxony, a 14th-century logician, wrote on both \"a priori\" and \"a posteriori\".\n\nG. W. Leibniz introduced a distinction between \"a priori\" and \"a posteriori\" criteria for the possibility of a notion in his (1684) short treatise \"Meditations on Knowledge, Truth, and Ideas\". \"A priori\" and \"a posteriori\" arguments for the existence of God appear in his \"Monadology\" (1714).\n\nGeorge Berkeley outlined the distinction in his 1710 work \"A Treatise Concerning the Principles of Human Knowledge\" (para. XXI).\n\nThe 18th-century German philosopher Immanuel Kant (1781) advocated a blend of rationalist and empiricist theories. Kant says, \"Although all our cognition begins with experience, it does not follow that it arises [is caused by] from experience\" According to Kant, \"a priori\" cognition is transcendental, or based on the \"form\" of all possible experience, while \"a posteriori\" cognition is empirical, based on the \"content\" of experience. Kant states, \"[…] it is quite possible that our empirical knowledge is a compound of that which we receive through impressions, and that which the faculty of cognition supplies from itself sensuous impressions [sense data] giving merely the \"occasion\" [opportunity for a cause to produce its effect].\" Contrary to contemporary usages of the term, Kant thinks that \"a priori\" knowledge is not entirely independent of the content of experience. And unlike the rationalists, Kant thinks that \"a priori\" cognition, in its pure form, that is without the admixture of any empirical content, is limited to the deduction of the conditions of possible experience. These \"a priori\", or transcendental conditions, are seated in one's cognitive faculties, and are not provided by experience in general or any experience in particular (although an argument exists that \"a priori\" intuitions can be \"triggered\" by experience). \n\nKant nominated and explored the possibility of a transcendental logic with which to consider the deduction of the \"a priori\" in its pure form. Space, time and causality are considered pure \"a priori\" intuitions. Kant reasoned that the pure \"a priori\" intuitions are established via his transcendental aesthetic and transcendental logic. He claimed that the human subject would not have the kind of experience that it has were these \"a priori\" forms not in some way constitutive of him as a human subject. For instance, a person would not experience the world as an orderly, rule-governed place unless time, space and causality were determinant functions in the form of perceptual faculties, i. e., there can be no experience in general without space, time or causality as particular determinants thereon. The claim is more formally known as Kant's transcendental deduction and it is the central argument of his major work, the \"Critique of Pure Reason\". The transcendental deduction argues that time, space and causality are ideal as much as real. In consideration of a possible logic of the \"a priori\", this most famous of Kant's deductions has made the successful attempt in the case for the fact of subjectivity, what constitutes subjectivity and what relation it holds with objectivity and the empirical.\n\nAfter Kant's death, a number of philosophers saw themselves as correcting and expanding his philosophy, leading to the various forms of German Idealism. One of these philosophers was Johann Fichte. His student (and critic), Arthur Schopenhauer, accused him of rejecting the distinction between \"a priori\" and \"a posteriori\" knowledge:\n\n\n\n\n"}
{"id": "55063170", "url": "https://en.wikipedia.org/wiki?curid=55063170", "title": "Afro-pessimism", "text": "Afro-pessimism\n\nAfro-pessimism is a framework and critical idiom that describes the ongoing effects of racism, colonialism, and historical processes of enslavement including the Trans-Atlantic slave trade, and their impact on structural conditions as well as personal, subjective, and lived experience, and embodied reality. \n\nThe term was first coined in 1990 in an article in \"Jeune Afrique Economie\" by Francophone Cameroonian author Sony Lab'ou Tansi. Writer and intellectual Frank B. Wilderson III developed the term in his political memoir about his time spent teaching and participating in the African National Congress in South Africa during apartheid. \n\nAccording to Tansi, \"Afro-pessimism [is] a terrible word used to conceal the greatest mess of all time,\" which is the \"tragedy\" that Africa's position \"dooms us to construct and build garbage economies in the depths of the most cruel, unbearable, and inhuman form of indignity that humans can swallow\" (as translated by John Conteh-Morgan). According to Wilderson, afro-pessimism theorizes blackness as a position of \"accumulation and fungibility\" (Saidiya Hartman); that is, as condition—or relation—of ontological death, as opposed to a cultural identity or human subjectivity.\n\nWilderson, along with Hortense Spillers, Saidiya Hartman, Achille Mbembe, Jared Sexton, and others who have contributed to afro-pessimist thought, cite the Martinician psychiatrist, philosopher, and writer Frantz Fanon as a foundational figure in the tradition of Afro-pessimism. \n\nAfro-pessimism has been constructed in many ways and with different aims. But Afro-pessimism is chiefly approached a transcendent position, not as a negative or disaffected political attitude in the sense that pessimism might seemingly connote. The Black radical tradition has drawn upon the term as a way to acknowledge the power, depth, and vitality of the resilience and radical imagination of people of African descent. Within this same critique, some have used Afro-pessimism to articulate the subject-position of renunciation, refusal, distancing, dread, doubt and abjection in response to the multitude and ongoing effects and historical traumas of colonialism. This includes the view that dismantling white supremacy would mean dismantling much of the social and political institutions of the modern world.\n\nDiscussions of Afro-pessimism have manifest in an online context, and have continued in Afro-pessimist approaches to art, poetics, and computing.\n\nAfro-pessimist ideas have been part of ongoing conversations about pan-African identity, as an inclusionary concept of blackness among all people of African descent. Pan-African thought has drawn attention to the shared racial identity and also the particulars of the expression of African identity among the African Diaspora and peoples on the African continent. Pan-African thought has analyzed the ongoing struggles of African peoples, and the power of Afrocentricity as a move away from the colonialism and violence of Eurocentricity. The writings of Frantz Fanon, a Martinican psychiatrist, intellectual, and revolutionary, reflect pan-African and Afro-pessimistic approaches to decolonization and black liberation. \n\nThe Pan-African movement négritude represents pessimism as a kind of realist recognition of the historical traumas of colonialism, from an existentialist position. A key figure in the movement, Aimé Césaire, uses pessimism to consider transcendence and a recognition of the breadth of the cultural imagination and perseverance of people of African descent.\n\nAfro-pessimism has also been employed as a term describing a narrative in Western media and International relations theory that portrays post-colonial Africa as unlikely to achieve economic growth and democratic governance. This use of Afro-pessimism has nothing to with Wilderson's definition. This form of Afro-pessimism has been criticized as a Western construct regarding the ongoing portrayal of Africa and African people in Western media, overwhelmingly in terms of tragedy, doom, victimization, and victim-hood. Scholar Toussaint Nothias has characterized these discussions by the components, \"essentialism, racialization, selectivity, ranking framework, and prediction.\" From this Afro-pessimistic perspective, news media that portray Africa and African people by the trope of victimhood, mirror the Eurocentric and ethnocentric of the Western media, language, images, and rhetoric. In this ways the media tends to victimize and exoticize Africa for its going struggles with poverty, health-crisis, famine, and lack of modern development. The victimization is then visible in the humanitarian and development projects, which sometimes use the language of \"saving\" African people from such \"humanitarian disasters\".\n\n\n"}
{"id": "2792", "url": "https://en.wikipedia.org/wiki?curid=2792", "title": "Anthropic principle", "text": "Anthropic principle\n\nThe anthropic principle is a philosophical consideration that observations of the universe must be compatible with the conscious and sapient life that observes it. Some proponents of the anthropic principle reason that it explains why this universe has the age and the fundamental physical constants necessary to accommodate conscious life. As a result, they believe it is unremarkable that this universe has fundamental constants that happen to fall within the narrow range thought to be compatible with life.\nThe strong anthropic principle (SAP) as explained by John D. Barrow and Frank Tipler states that this is all the case because the universe is in some sense compelled to eventually have conscious and sapient life emerge within it. Some critics of the SAP argue in favor of a weak anthropic principle (WAP) similar to the one defined by Brandon Carter, which states that the universe's ostensible fine tuning is the result of selection bias (specifically survivor bias): i.e., only in a universe capable of eventually supporting life will there be living beings capable of observing and reflecting on the matter. Most often such arguments draw upon some notion of the multiverse for there to be a statistical population of universes to select from and from which selection bias (our observance of \"only\" this universe, compatible with \"our\" life) could occur.\n\nThe principle was formulated as a response to a series of observations that the laws of nature and parameters of the universe take on values that are consistent with conditions for life as we know it rather than a set of values that would not be consistent with life on Earth. The anthropic principle states that this is a necessity, because if life were impossible, no living entity would be there to observe it, and thus would not be known. That is, it must be possible to observe \"some\" universe, and hence, the laws and constants of any such universe must accommodate that possibility.\n\nThe term \"anthropic\" in \"anthropic principle\" has been argued to be a misnomer. While singling out our kind of carbon-based life, none of the finely tuned phenomena require human life or some kind of carbon chauvinism. Any form of life or any form of heavy atom, stone, star or galaxy would do; nothing specifically human or anthropic is involved.\n\nThe anthropic principle has given rise to some confusion and controversy, partly because the phrase has been applied to several distinct ideas. All versions of the principle have been accused of discouraging the search for a deeper physical understanding of the universe. The anthropic principle is often criticized for lacking falsifiability and therefore critics of the anthropic principle may point out that the anthropic principle is a non-scientific concept, even though the weak anthropic principle, \"conditions that are observed in the universe must allow the observer to exist\", is \"easy\" to support in mathematics and philosophy, i.e. it is a tautology or truism. However, building a substantive argument based on a tautological foundation is problematic. Stronger variants of the anthropic principle are not tautologies and thus make claims considered controversial by some and that are contingent upon empirical verification.\n\nIn 1961, Robert Dicke noted that the age of the universe, as seen by living observers, cannot be random. Instead, biological factors constrain the universe to be more or less in a \"golden age\", neither too young nor too old. If the universe were one tenth as old as its present age, there would not have been sufficient time to build up appreciable levels of\nmetallicity (levels of elements besides hydrogen and helium) especially carbon, by nucleosynthesis. Small rocky planets did not yet exist. If the universe were 10 times older than it actually is, most stars would be too old to remain on the main sequence and would have turned into white dwarfs, aside from the dimmest red dwarfs, and stable planetary systems would have already come to an end. Thus, Dicke explained the coincidence between large dimensionless numbers constructed from the constants of physics and the age of the universe, a coincidence which had inspired Dirac's varying-G theory.\n\nDicke later reasoned that the density of matter in the universe must be almost exactly the critical density needed to prevent the Big Crunch (the \"Dicke coincidences\" argument). The most recent measurements may suggest that the observed density of baryonic matter, and some theoretical predictions of the amount of dark matter account for about 30% of this critical density, with the rest contributed by a cosmological constant. Steven Weinberg gave an anthropic explanation for this fact: he noted that the cosmological constant has a remarkably low value, some 120 orders of magnitude smaller than the value particle physics predicts (this has been described as the \"worst prediction in physics\"). However, if the cosmological constant were only several orders of magnitude larger than its observed value, the universe would suffer catastrophic inflation, which would preclude the formation of stars, and hence life.\n\nThe observed values of the dimensionless physical constants (such as the fine-structure constant) governing the four fundamental interactions are balanced as if fine-tuned to permit the formation of commonly found matter and subsequently the emergence of life. A slight increase in the strong interaction would bind the dineutron and the diproton, and nuclear fusion would have converted all hydrogen in the early universe to helium. Water, as well as sufficiently long-lived stable stars, both essential for the emergence of life as we know it, would not exist. More generally, small changes in the relative strengths of the four fundamental interactions can greatly affect the universe's age, structure, and capacity for life.\n\nArthur Schopenhauer was among the first atheist proponents of arguments along similar lines to the anthropic principle.\n\nThe phrase \"anthropic principle\" first appeared in Brandon Carter's contribution to a 1973 Kraków symposium honouring Copernicus's 500th birthday. Carter, a theoretical astrophysicist, articulated the Anthropic Principle in reaction to the Copernican Principle, which states that humans do not occupy a privileged position in the Universe. As Carter said: \"Although our situation is not necessarily \"central\", it is inevitably privileged to some extent.\" Specifically, Carter disagreed with using the Copernican principle to justify the Perfect Cosmological Principle, which states that all large regions \"and times\" in the universe must be statistically identical. The latter principle underlay the steady-state theory, which had recently been falsified by the 1965 discovery of the cosmic microwave background radiation. This discovery was unequivocal evidence that the universe has changed radically over time (for example, via the Big Bang).\n\nCarter defined two forms of the anthropic principle, a \"weak\" one which referred only to anthropic selection of privileged spacetime locations in the universe, and a more controversial \"strong\" form which addressed the values of the fundamental constants of physics.\n\nRoger Penrose explained the weak form as follows:\n\nOne reason this is plausible is that there are many other places and times in which we can imagine finding ourselves. But when applying the strong principle, we only have one universe, with one set of fundamental parameters, so what exactly is the point being made? Carter offers two possibilities: First, we can use our own existence to make \"predictions\" about the parameters. But second, \"as a last resort\", we can convert these predictions into \"explanations\" by assuming that there \"is\" more than one universe, in fact a large and possibly infinite collection of universes, something that is now called the multiverse (\"world ensemble\" was Carter's term), in which the parameters (and perhaps the laws of physics) vary across universes. The strong principle then becomes an example of a selection effect, exactly analogous to the weak principle. Postulating a multiverse is certainly a radical step, but taking it could provide at least a partial answer to a question which had seemed to be out of the reach of normal science: \"why do the fundamental laws of physics take the particular form we observe and not another?\"\n\nSince Carter's 1973 paper, the term \"anthropic principle\" has been extended to cover a number of ideas which differ in important ways from those he espoused. Particular confusion was caused in 1986 by the book \"The Anthropic Cosmological Principle\" by John D. Barrow and Frank Tipler, published that year which distinguished between \"weak\" and \"strong\" anthropic principle in a way very different from Carter's, as discussed in the next section.\n\nCarter was not the first to invoke some form of the anthropic principle. In fact, the evolutionary biologist Alfred Russel Wallace anticipated the anthropic principle as long ago as 1904: \"Such a vast and complex universe as that which we know exists around us, may have been absolutely required [...] in order to produce a world that should be precisely adapted in every detail for the orderly development of life culminating in man.\" In 1957, Robert Dicke wrote: \"The age of the Universe 'now' is not random but conditioned by biological factors [...] [changes in the values of the fundamental constants of physics] would preclude the existence of man to consider the problem.\"\n\nWeak anthropic principle (WAP) (Carter): \"[W]e must be prepared to take account of the fact that our location in the universe is \"necessarily\" privileged to the extent of being compatible with our existence as observers.\" Note that for Carter, \"location\" refers to our location in time as well as space.\n\nStrong anthropic principle (SAP) (Carter): \"[T]he universe (and hence the fundamental parameters on which it depends) must be such as to admit the creation of observers within it at some stage. To paraphrase Descartes, \"cogito ergo mundus talis est\".\"<br>The Latin tag (\"I think, therefore the world is such [as it is]\") makes it clear that \"must\" indicates a deduction from the fact of our existence; the statement is thus a truism.\n\nIn their 1986 book, \"The Anthropic Cosmological Principle\", John Barrow and Frank Tipler depart from Carter and define the WAP and SAP as follows:\n\nWeak anthropic principle (WAP) (Barrow and Tipler): \"The observed values of all physical and cosmological quantities are not equally probable but they take on values restricted by the requirement that there exist sites where carbon-based life can evolve and by the requirements that the universe be old enough for it to have already done so.\"<br>Unlike Carter they restrict the principle to carbon-based life, rather than just \"observers\". A more important difference is that they apply the WAP to the fundamental physical constants, such as the fine structure constant, the number of spacetime dimensions, and the cosmological constant—topics that fall under Carter's SAP.\n\nStrong anthropic principle (SAP) (Barrow and Tipler): \"The Universe must have those properties which allow life to develop within it at some stage in its history.\"<br>This looks very similar to Carter's SAP, but unlike the case with Carter's SAP, the \"must\" is an imperative, as shown by the following three possible elaborations of the SAP, each proposed by Barrow and Tipler:\n\nThe philosophers John Leslie and Nick Bostrom reject the Barrow and Tipler SAP as a fundamental misreading of Carter. For Bostrom, Carter's anthropic principle just warns us to make allowance for \"anthropic bias\"—that is, the bias created by anthropic selection effects (which Bostrom calls \"observation\" selection effects)—the necessity for observers to exist in order to get a result. He writes:\n\nStrong self-sampling assumption (SSSA) (Bostrom): \"Each observer-moment should reason as if it were randomly selected from the class of all observer-moments in its reference class.\"<br> Analysing an observer's experience into a sequence of \"observer-moments\" helps avoid certain paradoxes; but the main ambiguity is the selection of the appropriate \"reference class\": for Carter's WAP this might correspond to all real or potential observer-moments in our universe; for the SAP, to all in the multiverse. Bostrom's mathematical development shows that choosing either too broad or too narrow a reference class leads to counter-intuitive results, but he is not able to prescribe an ideal choice.\n\nAccording to Jürgen Schmidhuber, the anthropic principle essentially just says that the conditional probability of finding yourself in a universe compatible with your existence is always 1. It does not allow for any additional nontrivial predictions such as \"gravity won't change tomorrow\". To gain more predictive power, additional assumptions on the prior distribution of alternative universes are necessary.\n\nPlaywright and novelist Michael Frayn describes a form of the Strong Anthropic Principle in his 2006 book \"The Human Touch\", which explores what he characterises as \"the central oddity of the Universe\":\n\nCarter chose to focus on a tautological aspect of his ideas, which has resulted in much confusion. In fact, anthropic reasoning interests scientists because of something that is only implicit in the above formal definitions, namely that we should give serious consideration to there being other universes with different values of the \"fundamental parameters\"—that is, the dimensionless physical constants and initial conditions for the Big Bang. Carter and others have argued that life as we know it would not be possible in most such universes. In other words, the universe we are in is fine tuned to permit life. Collins & Hawking (1973) characterized Carter's then-unpublished big idea as the postulate that \"there is not one universe but a whole infinite ensemble of universes with all possible initial conditions\". If this is granted, the anthropic principle provides a plausible explanation for the fine tuning of our universe: the \"typical\" universe is not fine-tuned, but given enough universes, a small fraction thereof will be capable of supporting intelligent life. Ours must be one of these, and so the observed fine tuning should be no cause for wonder.\n\nAlthough philosophers have discussed related concepts for centuries, in the early 1970s the only genuine physical theory yielding a multiverse of sorts was the many-worlds interpretation of quantum mechanics. This would allow variation in initial conditions, but not in the truly fundamental constants. Since that time a number of mechanisms for producing a multiverse have been suggested: see the review by Max Tegmark. An important development in the 1980s was the combination of inflation theory with the hypothesis that some parameters are determined by symmetry breaking in the early universe, which allows parameters previously thought of as \"fundamental constants\" to vary over very large distances, thus eroding the distinction between Carter's weak and strong principles. At the beginning of the 21st century, the string landscape emerged as a mechanism for varying essentially all the constants, including the number of spatial dimensions.\n\nThe anthropic idea that fundamental parameters are selected from a multitude of different possibilities (each actual in some universe or other) contrasts with the traditional hope of physicists for a theory of everything having no free parameters. As Einstein said: \"What really interests me is whether God had any choice in the creation of the world.\" In 2002, proponents of the leading candidate for a \"theory of everything\", string theory, proclaimed \"the end of the anthropic principle\" since there would be no free parameters to select. String theory now seems to offer no hope of predicting fundamental parameters, and now some who advocate it invoke the anthropic principle as well (see below).\n\nThe modern form of a design argument is put forth by intelligent design. Proponents of intelligent design often cite the fine-tuning observations that (in part) preceded the formulation of the anthropic principle by Carter as a proof of an intelligent designer. Opponents of intelligent design are not limited to those who hypothesize that other universes exist; they may also argue, anti-anthropically, that the universe is less fine-tuned than often claimed, or that accepting fine tuning as a brute fact is less astonishing than the idea of an intelligent creator. Furthermore, even accepting fine tuning, Sober (2005) and Ikeda and Jefferys, argue that the Anthropic Principle as conventionally stated actually undermines intelligent design; see fine-tuned universe.\n\nPaul Davies's book \"The Goldilocks Enigma\" (2006) reviews the current state of the fine tuning debate in detail, and concludes by enumerating the following responses to that debate:\n\nOmitted here is Lee Smolin's model of cosmological natural selection, also known as \"fecund universes\", which proposes that universes have \"offspring\" which are more plentiful if they resemble our universe. Also see Gardner (2005).\n\nClearly each of these hypotheses resolve some aspects of the puzzle, while leaving others unanswered. Followers of Carter would admit only option 3 as an anthropic explanation, whereas 3 through 6 are covered by different versions of Barrow and Tipler's SAP (which would also include 7 if it is considered a variant of 4, as in Tipler 1994).\n\nThe anthropic principle, at least as Carter conceived it, can be applied on scales much smaller than the whole universe. For example, Carter (1983) inverted the usual line of reasoning and pointed out that when interpreting the evolutionary record, one must take into account cosmological and astrophysical considerations. With this in mind, Carter concluded that given the best estimates of the age of the universe, the evolutionary chain culminating in \"Homo sapiens\" probably admits only one or two low probability links.\n\nNo possible observational evidence bears on Carter's WAP, as it is merely advice to the scientist and asserts nothing debatable. The obvious test of Barrow's SAP, which says that the universe is \"required\" to support life, is to find evidence of life in universes other than ours. Any other universe is, by most definitions, unobservable (otherwise it would be included in \"our\" portion of \"this\" universe). Thus, in principle Barrow's SAP cannot be falsified by observing a universe in which an observer cannot exist.\n\nPhilosopher John Leslie states that the Carter SAP (with multiverse) predicts the following:\n\nHogan has emphasised that it would be very strange if all fundamental constants were strictly determined, since this would leave us with no ready explanation for apparent fine tuning. In fact we might have to resort to something akin to Barrow and Tipler's SAP: there would be no option for such a universe \"not\" to support life.\n\nProbabilistic predictions of parameter values can be made given:\nThe probability of observing value \"X\" is then proportional to \"N\"(\"X\") \"P\"(\"X\"). A generic feature of an analysis of this nature is that the expected values of the fundamental physical constants should not be \"over-tuned\", i.e. if there is some perfectly tuned predicted value (e.g. zero), the observed value need be no closer to that predicted value than what is required to make life possible. The small but finite value of the cosmological constant can be regarded as a successful prediction in this sense.\n\nOne thing that would \"not\" count as evidence for the Anthropic Principle is evidence that the Earth or the solar system occupied a privileged position in the universe, in violation of the Copernican principle (for possible counterevidence to this principle, see Copernican principle), unless there was some reason to think that that position was a necessary condition for our existence as observers.\n\nFred Hoyle may have invoked anthropic reasoning to predict an astrophysical phenomenon. He is said to have reasoned from the prevalence on Earth of life forms whose chemistry was based on carbon-12 atoms, that there must be an undiscovered resonance in the carbon-12 nucleus facilitating its synthesis in stellar interiors via the triple-alpha process. He then calculated the energy of this undiscovered resonance to be 7.6 million electronvolts. Willie Fowler's research group soon found this resonance, and its measured energy was close to Hoyle's prediction.\n\nHowever, a recently released paper argues that Hoyle did not use anthropic reasoning to make this prediction.\n\nDon Page criticized the entire theory of cosmic inflation as follows. He emphasized that initial conditions which made possible a thermodynamic arrow of time in a universe with a Big Bang origin, must include the assumption that at the initial singularity, the entropy of the universe was low and therefore extremely improbable. Paul Davies rebutted this criticism by invoking an inflationary version of the anthropic principle. While Davies accepted the premise that the initial state of the visible universe (which filled a microscopic amount of space before inflating) had to possess a very low entropy value—due to random quantum fluctuations—to account for the observed thermodynamic arrow of time, he deemed this fact an advantage for the theory. That the tiny patch of space from which our observable universe grew had to be extremely orderly, to allow the post-inflation universe to have an arrow of time, makes it unnecessary to adopt any \"ad hoc\" hypotheses about the initial entropy state, hypotheses other Big Bang theories require.\n\nString theory predicts a large number of possible universes, called the \"backgrounds\" or \"vacua\". The set of these vacua is often called the \"multiverse\" or \"anthropic landscape\" or \"string landscape\". Leonard Susskind has argued that the existence of a large number of vacua puts anthropic reasoning on firm ground: only universes whose properties are such as to allow observers to exist are observed, while a possibly much larger set of universes lacking such properties go unnoticed.\n\nSteven Weinberg believes the Anthropic Principle may be appropriated by cosmologists committed to nontheism, and refers to that Principle as a \"turning point\" in modern science because applying it to the string landscape \" [...] may explain how the constants of nature that we observe can take values suitable for life without being fine-tuned by a benevolent creator\". Others—most notably David Gross but also Lubos Motl, Peter Woit, and Lee Smolin—argue that this is not predictive. Max Tegmark, Mario Livio, and Martin Rees argue that only some aspects of a physical theory need be observable and/or testable for the theory to be accepted, and that many well-accepted theories are far from completely testable at present.\n\nJürgen Schmidhuber (2000–2002) points out that Ray Solomonoff's theory of universal inductive inference and its extensions already provide a framework for maximizing our confidence in any theory, given a limited sequence of physical observations, and some prior distribution on the set of possible explanations of the universe.\n\nThere are two kinds of dimensions, spatial (bidirectional) and temporal (unidirectional). Let the number of spatial dimensions be \"N\" and the number of temporal dimensions be \"T\". That \"N\" = 3 and \"T\" = 1, setting aside the compactified dimensions invoked by string theory and undetectable to date, can be explained by appealing to the physical consequences of letting \"N\" differ from 3 and \"T\" differ from 1. The argument is often of an anthropic character and possibly the first of its kind, albeit before the complete concept came into vogue. \n\nThe implicit notion that the dimensionality of the universe is special is first attributed to Gottfried Wilhelm Leibniz, who in the Discourse on Metaphysics suggested that the world is Immanuel Kant argued that 3-dimensional space was a consequence of the inverse square law of universal gravitation. While Kant's argument is historically important, John D. Barrow says that it \"[...] gets the punch-line back to front: it is the three-dimensionality of space that explains why we see inverse-square force laws in Nature, not vice-versa\" (Barrow 2002: 204).\n\nIn 1920, Paul Ehrenfest showed that if there is only one time dimension and greater than three spatial dimensions, the orbit of a planet about its Sun cannot remain stable. The same is true of a star's orbit around the center of its galaxy. Ehrenfest also showed that if there are an even number of spatial dimensions, then the different parts of a wave impulse will travel at different speeds. If there are formula_1 spatial dimensions, where \"k\" is a whole number, then wave impulses become distorted. In 1922, Hermann Weyl showed that Maxwell's theory of electromagnetism works only with three dimensions of space and one of time. Finally, Tangherlini showed in 1963 that when there are more than three spatial dimensions, electron orbitals around nuclei cannot be stable; electrons would either fall into the nucleus or disperse.\n\nMax Tegmark expands on the preceding argument in the following anthropic manner. If \"T\" differs from 1, the behavior of physical systems could not be predicted reliably from knowledge of the relevant partial differential equations. In such a universe, intelligent life capable of manipulating technology could not emerge. Moreover, if \"T\" > 1, Tegmark maintains that protons and electrons would be unstable and could decay into particles having greater mass than themselves. (This is not a problem if the particles have a sufficiently low temperature.) If \"N\" < 3, gravitation of any kind becomes problematic, and the universe is probably too simple to contain observers. For example, when \"N\" < 3, nerves cannot cross without intersecting.\n\nIn general, it is not clear how physical law could function if \"T\" differed from 1. If \"T\" > 1, subatomic particles which decay after a fixed period would not behave predictably, because time-like geodesics would not be necessarily maximal. \"N\" = 1 and \"T\" = 3 has the peculiar property that the speed of light in a vacuum is a \"lower bound\" on the velocity of matter; all matter consists of tachyons. \n\nHence anthropic and other arguments rule out all cases except \"N\" = 3 and \"T\" = 1, which happens to describe the world around us.\n\nSome of the metaphysical disputes and speculations include, for example, attempts to back Teilhard de Chardin's earlier interpretation of the universe as being Christ centered (compare Omega Point), expressing a \"creatio evolutiva\" instead the elder notion of \"creatio continua\". From a strictly secular, humanist perspective, it allows as well to put human beings back in the center, an anthropogenic shift in cosmology. Karl W. Giberson has been sort of laconic in stating that\n\nA thorough extant study of the anthropic principle is the book \"The Anthropic Cosmological Principle\" by John D. Barrow, a cosmologist, and Frank J. Tipler, a cosmologist and mathematical physicist. This book sets out in detail the many known anthropic coincidences and constraints, including many found by its authors. While the book is primarily a work of theoretical astrophysics, it also touches on quantum physics, chemistry, and earth science. An entire chapter argues that \"Homo sapiens\" is, with high probability, the only intelligent species in the Milky Way.\n\nThe book begins with an extensive review of many topics in the history of ideas the authors deem relevant to the anthropic principle, because the authors believe that principle has important antecedents in the notions of teleology and intelligent design. They discuss the writings of Fichte, Hegel, Bergson, and Alfred North Whitehead, and the Omega Point cosmology of Teilhard de Chardin. Barrow and Tipler carefully distinguish teleological reasoning from \"eutaxiological\" reasoning; the former asserts that order must have a consequent purpose; the latter asserts more modestly that order must have a planned cause. They attribute this important but nearly always overlooked distinction to an obscure 1883 book by L. E. Hicks.\n\nSeeing little sense in a principle requiring intelligent life to emerge while remaining indifferent to the possibility of its eventual extinction, Barrow and Tipler propose the final anthropic principle (FAP): Intelligent information-processing must come into existence in the universe, and, once it comes into existence, it will never die out.\n\nBarrow and Tipler submit that the FAP is both a valid physical statement and \"closely connected with moral values\". FAP places strong constraints on the structure of the universe, constraints developed further in Tipler's \"The Physics of Immortality\". One such constraint is that the universe must end in a big crunch, which seems unlikely in view of the tentative conclusions drawn since 1998 about dark energy, based on observations of very distant supernovas.\n\nIn his review of Barrow and Tipler, Martin Gardner ridiculed the FAP by quoting the last two sentences of their book as defining a Completely Ridiculous Anthropic Principle (CRAP):\n\nCarter has frequently regretted his own choice of the word \"anthropic\", because it conveys the misleading impression that the principle involves humans specifically, rather than intelligent observers in general. Others have criticised the word \"principle\" as being too grandiose to describe straightforward applications of selection effects.\n\nA common criticism of Carter's SAP is that it is an easy deus ex machina which discourages searches for physical explanations. To quote Penrose again: \"[I]t tends to be invoked by theorists whenever they do not have a good enough theory to explain the observed facts.\"\n\nCarter's SAP and Barrow and Tipler's WAP have been dismissed as truisms or trivial tautologies—that is, statements true solely by virtue of their logical form (the conclusion is identical to the premise) and not because a substantive claim is made and supported by observation of reality. As such, they are criticized as an elaborate way of saying \"if things were different, they would be different\", which is a valid statement, but does not make a claim of some factual alternative over another.\n\nCritics of the Barrow and Tipler SAP claim that it is neither testable nor falsifiable, and thus is not a scientific statement but rather a philosophical one. The same criticism has been leveled against the hypothesis of a multiverse, although some argue that it does make falsifiable predictions. A modified version of this criticism is that we understand so little about the emergence of life, especially intelligent life, that it is effectively impossible to calculate the number of observers in each universe. Also, the prior distribution of universes as a function of the fundamental constants is easily modified to get any desired result.\n\nMany criticisms focus on versions of the strong anthropic principle, such as Barrow and Tipler's \"anthropic cosmological principle\", which are teleological notions that tend to describe the existence of life as a \"necessary prerequisite\" for the observable constants of physics. Similarly, Stephen Jay Gould, Michael Shermer, and others claim that the stronger versions of the anthropic principle seem to reverse known causes and effects. Gould compared the claim that the universe is fine-tuned for the benefit of our kind of life to saying that sausages were made long and narrow so that they could fit into modern hotdog buns, or saying that ships had been invented to house barnacles. These critics cite the vast physical, fossil, genetic, and other biological evidence consistent with life having been fine-tuned through natural selection to adapt to the physical and geophysical environment in which life exists. Life appears to have adapted to the universe, and not vice versa.\n\nSome applications of the anthropic principle have been criticized as an argument by lack of imagination, for tacitly assuming that carbon compounds and water are the only possible chemistry of life (sometimes called \"carbon chauvinism\", see also alternative biochemistry). The range of fundamental physical constants consistent with the evolution of carbon-based life may also be wider than those who advocate a fine tuned universe have argued. For instance, Harnik et al. propose a weakless universe in which the weak nuclear force is eliminated. They show that this has no significant effect on the other fundamental interactions, provided some adjustments are made in how those interactions work. However, if some of the fine-tuned details of our universe were violated, that would rule out complex structures of any kind—stars, planets, galaxies, etc.\n\nLee Smolin has offered a theory designed to improve on the lack of imagination that anthropic principles have been accused of. He puts forth his fecund universes theory, which assumes universes have \"offspring\" through the creation of black holes whose offspring universes have values of physical constants that depend on those of the mother universe.\n\nThe philosophers of cosmology John Earman, Ernan McMullin, and Jesús Mosterín contend that \"in its weak version, the anthropic principle is a mere tautology, which does not allow us to explain anything or to predict anything that we did not already know. In its strong version, it is a gratuitous speculation\". A further criticism by Mosterín concerns the flawed \"anthropic\" inference from the assumption of an infinity of worlds to the existence of one like ours:\n\n\n"}
{"id": "5653", "url": "https://en.wikipedia.org/wiki?curid=5653", "title": "Clarke's three laws", "text": "Clarke's three laws\n\nBritish science fiction writer Arthur C. Clarke formulated three adages that are known as Clarke's three laws, of which the third law is the best known and most widely cited. They were part of his ideas in his extensive writings about the future. These so-called laws include:\n\n\nOne account claimed that Clarke's \"laws\" were developed after the editor of his works in French started numbering the author's assertions. All three laws appear in Clarke's essay \"Hazards of Prophecy: The Failure of Imagination\", first published in \"Profiles of the Future\" (1962). However, they were not published at the same time. Clarke's first law was proposed in the 1962 edition of the essay, as \"Clarke's Law\" in \"Profiles of the Future\".\n\nThe second law is offered as a simple observation in the same essay but its status as Clarke's second law was conferred by others. It was initially a derivative of the first law and formally became Clarke's second law where the author proposed the third law in the 1973 revision of \"Profiles of the Future,\" which included an acknowledgement\".\" It was also here that Clarke wrote about the third law in these words: \"As three laws were good enough for Newton, I have modestly decided to stop there\".\n\nThe third law, despite being latest stated by a decade, is the best known and most widely cited. It appears only in the 1973 revision of the \"Hazards of Prophecy\" essay. It echoes a statement in a 1942 story by Leigh Brackett: \"Witchcraft to the ignorant, … simple science to the learned\". Earlier examples of this sentiment may be found in \"Wild Talents\" (1932) by Charles Fort: \"...a performance that may some day be considered understandable, but that, in these primitive times, so transcends what is said to be the known that it is what I mean by magic,\" and in the short story \"The Hound of Death\" (1933) by Agatha Christie: \"The supernatural is only the natural of which the laws are not yet understood.\"\n\nClarke gave an example of the third law when he said that while he \"would have believed anyone who told him back in 1962 that there would one day exist a book-sized object capable of holding the content of an entire library, he would never have accepted that the same device could find a page or word in a second and then convert it into any typeface and size from Albertus Extra Bold to Zurich Calligraphic\", referring to his memory of \"seeing and hearing Linotype machines which slowly converted ‘molten lead into front pages that required two men to lift them’\".\n\nA fourth law has been proposed for the canon, despite Clarke's declared intention of stopping at three laws. Geoff Holder quotes: \"For every expert, there is an equal and opposite expert,\" which is part of American economist Thomas Sowell's \"For every expert, there is an equal and opposite expert, but for every fact there is not necessarily an equal and opposite fact\", from his 1995 book \"The Vision of the Anointed\".\n\nThe third law has inspired many snowclones and other variations:\n\n\nA of the third law is\n\n\nThe third law has been:\n\n\n\n"}
{"id": "29358535", "url": "https://en.wikipedia.org/wiki?curid=29358535", "title": "Comply or explain", "text": "Comply or explain\n\nComply or explain is a regulatory approach used in the United Kingdom, Germany, the Netherlands and other countries in the field of corporate governance and financial supervision. Rather than setting out binding laws, government regulators (in the UK, the Financial Reporting Council, in Germany, under the Aktiengesetz) set out a code, which listed companies may either comply with, or if they do not comply, explain publicly why they do not. The UK Corporate Governance Code, the German Corporate Governance Code (or Deutscher Corporate Governance Kodex) and the Dutch Corporate Governance Code 'Code Tabaksblat' () use this approach in setting minimum standards for companies in their audit committees, remuneration committees and recommendations for how good companies should divide authority on their boards.\n\nThe purpose of \"comply or explain\" is to \"let the market decide\" whether a set of standards is appropriate for individual companies. Since a company may deviate from the standard, this approach rejects the view that \"one size fits all\", but because of the requirement of disclosure of explanations to market investors, anticipates that if investors do not accept a company's explanations, then investors will sell their shares, hence creating a \"market sanction\", rather than a legal one. The concept was first introduced after the recommendations of the Cadbury Report of 1992.\n\n"}
{"id": "31799202", "url": "https://en.wikipedia.org/wiki?curid=31799202", "title": "Comstock–Needham system", "text": "Comstock–Needham system\n\nThe Comstock–Needham system is a naming system for insect wing veins, devised by John Comstock and George Needham in 1898. It was an important step in showing the homology of all insect wings. This system was based on Needham's \"pretracheation theory\" that was later discredited by Frederic Charles Fraser in 1938.\n\nThe Comstock and Needham system attributes different names to the veins on an insect's wing. From the anterior (leading) edge of the wing towards the posterior (rear), the major longitudinal veins are named:\n\nApart from the costal and the anal veins, each vein can be branched, in which case the branches are numbered from anterior to posterior. For example, the two branches of the subcostal vein will be called Sc and Sc.\n\nThe radius typically branches once near the base, producing anteriorly the R and posteriorly the \"radial sector\" Rs. The radial sector may fork twice.\n\nThe media may also fork twice, therefore having four branches reaching the wing margin.\n\nAccording to the Comstock–Needham system, the cubitus forks once, producing the cubital veins Cu and Cu. \nAccording to some other authorities, Cu may fork again, producing the Cu and Cu.\n\nAs there are several anal veins, they are called A1, A2, and so on. They are usually unforked.\n\nCrossveins link the longitudinal veins, and are named accordingly (for example, the medio-cubital crossvein is termed m-cu). Some crossveins have their own name, like the humeral crossvein h and the sectoral crossvein s.\n\nThe cells are named after the vein on the anterior side; for instance, the cell between Sc and R is called Sc.\n\nIn the case where two cells are separated by a crossvein but have the same anterior longitudinal vein, they should have the same name. To avoid this, they are attributed a number. For example, the R cell is divided in two by the radial cross vein: the basal cell is termed \"first R\", and the distal cell \"second R\".\n\nIf a cell is bordered anteriorly by a forking vein, such as R and R, the cell is named after the distal vein, in this case R.\n\n"}
{"id": "633037", "url": "https://en.wikipedia.org/wiki?curid=633037", "title": "Conceptualism", "text": "Conceptualism\n\nIn metaphysics, conceptualism is a theory that explains universality of particulars as conceptualized frameworks situated within the thinking mind. Intermediate between nominalism and realism, the conceptualist view approaches the metaphysical concept of universals from a perspective that denies their presence in particulars outside the mind's perception of them. Conceptualism is anti-realist about abstract objects, just like immanent realism is (their difference being that immanent realism does not deny the mind-independence of universals, like conceptualism does).\n\nThe evolution of late scholastic terminology has led to the emergence of conceptualism, which stemmed from doctrines that were previously considered to be nominalistic. The terminological distinction was made in order to stress the difference between the claim that universal mental acts correspond with universal intentional objects and the perspective that dismissed the existence of universals outside the mind. The former perspective of rejection of objective universality was distinctly defined as conceptualism.\n\nPeter Abélard was a medieval thinker whose work is currently classified as having the most potential in representing the roots of conceptualism. Abélard’s view denied the existence of determinate universals within things. William of Ockham was another famous late medieval thinker who had a strictly conceptualist solution to the metaphysical problem of universals. He argued that abstract concepts have no \"fundamentum\" outside the mind.\n\nIn the 17th century conceptualism gained favour for some decades especially among the Jesuits: Hurtado de Mendoza, Rodrigo de Arriaga and Francisco Oviedo are the main figures. Although the order soon returned to the more realist philosophy of Francisco Suárez, the ideas of these Jesuits had a great impact on the early modern philosophy.\n\nConceptualism was either explicitly or implicitly embraced by most of the early modern thinkers, including René Descartes, John Locke, Baruch Spinoza, Gottfried Wilhelm Leibniz, George Berkeley, and David Hume – often in a quite simplified form if compared with the elaborate scholastic theories.\n\nSometimes the term is applied even to the radically different philosophy of Immanuel Kant, who holds that universals have no connection with external things because they are exclusively produced by our \"a priori\" mental structures and functions.\n\nIn late modern philosophy, conceptualist views were held by G. W. F. Hegel.\n\nEdmund Husserl's philosophy of mathematics has been construed as a form of conceptualism.\n\nConceptualist realism (a view put forward by David Wiggins in 1980) states that our conceptual framework maps reality.\n\nThough separate from the historical debate regarding the status of universals, there has been significant debate regarding the conceptual character of experience since the release of \"Mind and World\" by John McDowell in 1994. McDowell's touchstone is the famous refutation that Wilfrid Sellars provided for what he called the \"Myth of the Given\"—the notion that all empirical knowledge is based on certain assumed or 'given' items, such as sense data. Thus, in rejecting the Myth of the Given, McDowell argues for perceptual conceptualism, according to which perceptual content is conceptual \"from the ground up\", that is, all perceptual experience is a form of conceptual experience. McDowell's philosophy of justification is considered a form of foundationalism: it is a form of foundationalism because it allows that certain judgements are warranted by experience and it is a coherent form of this view because it maintains that experience can warrant certain judgements because experience is irreducibly conceptual.\n\nA clear motivation of contemporary conceptualism is that the kind of perception that rational creatures like humans enjoy is unique in the fact that it has conceptual character. McDowell explains his position:\n\nI have urged that our perceptual relation to the world is conceptual all the way out to the world’s impacts on our receptive capacities. The idea of the conceptual that I mean to be invoking is to be understood in close connection with the idea of rationality, in the sense that is in play in the traditional separation of mature human beings, as rational animals, from the rest of the animal kingdom. Conceptual capacities are capacities that belong to their subject’s rationality. So another way of putting my claim is to say that our perceptual experience is permeated with rationality. I have also suggested, in passing, that something parallel should be said about our agency.\n\nMcDowell's conceptualism, though rather distinct (philosophically and historically) from conceptualism's genesis, shares the view that universals are not \"given\" in perception from outside the sphere of reason. Particular objects are perceived, as it were, already infused with conceptuality stemming from the spontaneity of the rational subject herself.\n\nThe application of the term \"perceptual conceptualism\" to Kant's philosophy of perception is debatable. Other scholars have argued for a rival interpretation of Kant's work termed perceptual non-conceptualism.\n\n"}
{"id": "47792266", "url": "https://en.wikipedia.org/wiki?curid=47792266", "title": "Construction of Concept Map", "text": "Construction of Concept Map\n\nConcept is usually perceived as a regularity in events or objects or in their records. While constructing a concept map, it is essential to keep in mind that the concept be built with reference to a focus question. Hence, initially, the focus question to which we seek to answer is carefully chosen because learners usually tend to deviate from this question relating only to domains and thus, fails to answer the question.\n\nWith the selected domain and the focus question, the next step is to identify the key concepts which apply to this domain.About 15 to 25 concepts is sufficient which is usually ordered in a rank ordered list. Such list should be established from the most general and inclusive concept for the particular chosen problem. This list will assist in at least in the beginning of the construction of the concept map. The list is referred to as Parking Lot since the list is constantly shuffled to build up the required network of the concept. Two or more concepts are connected to each other using linking words or phrases which can only then complete a meaningful sentence.Another important characteristics of a concept map is the cross-links. This cross link acts as the relationship between two different domains used in the concept map.This helps in a clear representation of the knowledge contained in the concept and also gives a clear background with specific examples. \n"}
{"id": "173937", "url": "https://en.wikipedia.org/wiki?curid=173937", "title": "Cosmological principle", "text": "Cosmological principle\n\nIn modern physical cosmology, the cosmological principle is the notion that the spatial distribution of matter in the universe is homogeneous and isotropic when viewed on a large enough scale, since the forces are expected to act uniformly throughout the universe, and should, therefore, produce no observable irregularities in the large-scale structuring over the course of evolution of the matter field that was initially laid down by the Big Bang.\n\nAstronomer William Keel explains:\n\nThe cosmological principle is usually stated formally as 'Viewed on a sufficiently large scale, the properties of the universe are the same for all observers.' This amounts to the strongly philosophical statement that the part of the universe which we can see is a fair sample, and that the same physical laws apply throughout. In essence, this in a sense says that the universe is knowable and is playing fair with scientists.\n\nThe cosmological principle depends on a definition of \"observer,\" and contains an implicit qualification and two testable consequences.\n\n\"Observers\" means any observer at any location in the universe, not simply any human observer at any location on Earth: as Andrew Liddle puts it, \"the cosmological principle [means that] the universe looks the same whoever and wherever you are.\"\n\nThe qualification is that variation in physical structures can be overlooked, provided this does not imperil the uniformity of conclusions drawn from observation: the Sun is different from the Earth, our galaxy is different from a black hole, some galaxies advance toward rather than recede from us, and the universe has a \"foamy\" texture of galaxy clusters and voids, but none of these different structures appears to violate the basic laws of physics.\n\nThe two testable structural consequences of the cosmological principle are homogeneity and isotropy. Homogeneity means that the same observational evidence is available to observers at different locations in the universe (\"the part of the universe which we can see is a fair sample\"). Isotropy means that the same observational evidence is available by looking in any direction in the universe (\"the same physical laws apply throughout\" ). The principles are distinct but closely related, because a universe that appears isotropic from any two (for a spherical geometry, three) locations must also be homogeneous.\n\nThe cosmological principle is first clearly asserted in the \"Philosophiæ Naturalis Principia Mathematica\" (1687) of Isaac Newton. In contrast to earlier classical or medieval cosmologies, in which Earth rested at the center of universe, Newton conceptualized the Earth as a sphere in orbital motion around the Sun within an empty space that extended uniformly in all directions to immeasurably large distances. He then showed, through a series of mathematical proofs on detailed observational data of the motions of planets and comets, that their motions could be explained by a single principle of \"universal gravitation\" that applied as well to the orbits of the Galilean moons around Jupiter, the Moon around the Earth, the Earth around the Sun, and to falling bodies on Earth. That is, he asserted the equivalent material nature of all bodies within the Solar System, the identical nature of the Sun and distant stars and thus the uniform extension of the physical laws of motion to a great distance beyond the observational location of Earth itself.\n\nObservations show that more distant galaxies are closer together and have lower content of chemical elements heavier than lithium. Applying the cosmological principle, this suggests that heavier elements were not created in the Big Bang but were produced by nucleosynthesis in giant stars and expelled across a series of supernovae explosions and new star formation from the supernovae remnants, which means heavier elements would accumulate over time. Another observation is that the furthest galaxies (earlier time) are often more fragmentary, interacting and unusually shaped than local galaxies (recent time), suggesting evolution in galaxy structure as well.\n\nA related implication of the cosmological principle is that the largest discrete structures in the universe are in mechanical equilibrium. Homogeneity and isotropy of matter at the largest scales would suggest that the largest discrete structures are parts of a single indiscrete form, like the crumbs which make up the interior of a cake. At extreme cosmological distances, the property of mechanical equilibrium in surfaces lateral to the line of sight can be empirically tested; however, under the assumption of the cosmological principle, it cannot be detected parallel to the line of sight (see timeline of the universe).\n\nCosmologists agree that in accordance with observations of distant galaxies, a universe must be non-static if it follows the cosmological principle. In 1923, Alexander Friedmann set out a variant of Einstein's equations of general relativity that describe the dynamics of a homogeneous isotropic universe. Independently, Georges Lemaître derived in 1927 the equations of an expanding universe from the General Relativity equations. Thus, a non-static universe is also implied, independent of observations of distant galaxies, as the result of applying the cosmological principle to general relativity.\n\nKarl Popper criticized the cosmological principle on the grounds that it makes \"our \"lack\" of knowledge a principle of \"knowing something\"\". He summarized his position as:\n\nAlthough the universe is inhomogeneous at smaller scales, it \"is\" statistically homogeneous on scales larger than 250 million light years. The cosmic microwave background is isotropic, that is to say that its intensity is about the same whichever direction we look at.\n\nHowever, recent findings have called this view into question. Data from the Planck Mission shows hemispheric bias in 2 respects: one with respect to average temperature (i.e. temperature fluctuations), the second with respect to larger variations in the degree of perturbations (i.e. densities). Therefore, the European Space Agency (the governing body of the Planck Mission) has concluded that these anisotropies are, in fact, statistically significant and can no longer be ignored.\n\nThe \"cosmological principle\" implies that at a sufficiently large scale, the universe is homogeneous. This means that different places will appear similar to one another, so sufficiently large structures cannot exist. Yadav and his colleagues have suggested a maximum scale of 260/h Mpc for structures within the universe according to this heuristic. Other authors have suggested values as low as 60/h Mpc. Yadav's calculation suggests that the maximum size of a structure can be about 370 Mpc.\n\nA number of observations conflict with predictions of maximal structure sizes:\n\n\nIn September 2016, however, studies of the expansion of the Universe that have used data taken by the \"Planck\" mission show it to be highly isotropical, reinforcing the cosmological principle\n\nThe perfect cosmological principle is an extension of the cosmological principle, and states that the universe is homogeneous and isotropic in space \"and\" time. In this view the universe looks the same everywhere (on the large scale), the same as it always has and always will. The perfect cosmological principle underpins Steady State theory and emerges from chaotic inflation theory.\n\n"}
{"id": "499429", "url": "https://en.wikipedia.org/wiki?curid=499429", "title": "D'Alembert's principle", "text": "D'Alembert's principle\n\nD'Alembert's principle, also known as the Lagrange–d'Alembert principle, is a statement of the fundamental classical laws of motion. It is named after its discoverer, the French physicist and mathematician Jean le Rond d'Alembert. It is the dynamic analogue to the \"principle of virtual work for applied forces\" in a static system and in fact is more general than Hamilton's principle, avoiding restriction to holonomic systems. A holonomic constraint depends only on the coordinates and time. It does not depend on the velocities. If the negative terms in accelerations are recognized as \"inertial forces\", the statement of d'Alembert's principle becomes \"The total virtual work of the impressed forces plus the inertial forces vanishes for reversible displacements\". The principle does not apply for irreversible displacements, such as sliding friction, and more general specification of the irreversibility is required.\n\nThe principle states that the sum of the differences between the forces acting on a system of mass particles and the time derivatives of the momenta of the system itself projected onto any virtual displacement consistent with the constraints of the system is zero. Thus, in symbols d'Alembert's principle is written as following,\n\nwhere :\n\nThis above equation is often called d'Alembert's principle, but it was first written in this variational form by Joseph Louis Lagrange. D'Alembert's contribution was to demonstrate that in the totality of a dynamic system the forces of constraint vanish. That is to say that the generalized forces formula_2 need not include constraint forces. It is equivalent to the somewhat more cumbersome Gauss's principle of least constraint.\n\nThe general statement of d'Alembert's principle mentions \"the time derivatives of the momenta of the system\". The momentum of the \"i\"-th mass is the product of its mass and velocity:\n\nand its time derivative is\n\nIn many applications, the masses are constant and this equation reduces to\n\nwhich appears in the formula given above. However, some applications involve changing masses (for example, chains being rolled up or being unrolled) and in those cases both terms formula_6 and formula_7 have to remain present, giving\n\nTo date, nobody has shown that D'Alembert's principle is equivalent to Newton's Second Law. D'Alembert's principle is a more general case . And it is true only for some very special cases e.g. rigid body constraints. However, an approximate solution to this problem does exist.\n\nConsider Newton's law for a system of particles, i. The total force on each particle is\n\nwhere\n\nMoving the inertial forces to the left gives an expression that can be considered to represent quasi-static equilibrium, but which is really just a small algebraic manipulation of Newton's law:\n\nConsidering the virtual work, formula_11, done by the total and inertial forces together through an arbitrary virtual displacement, formula_12, of the system leads to a zero identity, since the forces involved sum to zero for each particle.\n\nThe original vector equation could be recovered by recognizing that the work expression must hold for arbitrary displacements. Separating the total forces into applied forces, formula_14, and constraint forces, formula_15, yields\n\nIf arbitrary virtual displacements are assumed to be in directions that are orthogonal to the constraint forces (which is not usually the case, so this derivation works only for special cases), the constraint forces do no work. Such displacements are said to be \"consistent\" with the constraints. This leads to the formulation of \"d'Alembert's principle\", which states that the difference of applied forces and inertial forces for a dynamic system does no virtual work:.\n\nThere is also a corresponding principle for static systems called the principle of virtual work for applied forces.\n\nD'Alembert showed that one can transform an accelerating rigid body into an equivalent static system by adding the so-called \"inertial force\" and \"inertial torque\" or moment. The inertial force must act through the center of mass and the inertial torque can act anywhere. The system can then be analyzed exactly as a static system subjected to this \"inertial force and moment\" and the external forces. The advantage is that, in the equivalent static system one can take moments about any point (not just the center of mass). This often leads to simpler calculations because any force (in turn) can be eliminated from the moment equations by choosing the appropriate point about which to apply the moment equation (sum of moments = zero). Even in the course of Fundamentals of Dynamics and Kinematics of machines, this principle helps in analyzing the forces that act on a link of a mechanism when it is in motion. In textbooks of engineering dynamics this is sometimes referred to as \"d'Alembert's principle\".\n\nTo illustrate the concept of \"d'Alembert's principle\", let's use a simple model with a weight formula_18, suspended from a wire. The weight is subjected to a gravitational force, formula_19, and a tension force formula_20 in the wire. The mass accelerates upward with an acceleration formula_21. Newton's Second Law becomes formula_22 or formula_23. As an observer with feet planted firmly on the ground, we see that the force formula_20 accelerates the weight, formula_18, but, if we are moving with the wire we don’t see the acceleration, we feel it. The tension in the wire seems to counteract an acceleration “force” formula_26 or formula_27.\nFor a planar rigid body, moving in the plane of the body (the \"x\"–\"y\" plane), and subjected to forces and torques causing rotation only in this plane, the inertial force is\n\nwhere formula_29 is the position vector of the centre of mass of the body, and formula_30 is the mass of the body. The inertial torque (or moment) is\n\nwhere formula_32 is the moment of inertia of the body. If, in addition to the external forces and torques acting on the body, the inertia force acting through the center of mass is added and the inertial torque is added (acting around the centre of mass is as good as anywhere) the system is equivalent to one in static equilibrium. Thus the equations of static equilibrium\n\nhold. The important thing is that formula_34 is the sum of torques (or moments, including the inertial moment and the moment of the inertial force) taken about \"any\" point. The direct application of Newton's laws requires that the angular acceleration equation be applied \"only\" about the center of mass.\n\nD'Alembert's form of the principle of virtual work states that a system of rigid bodies is in dynamic equilibrium when the virtual work of the sum of the applied forces and the inertial forces is zero for any virtual displacement of the system. Thus, dynamic equilibrium of a system of n rigid bodies with m generalized coordinates requires that is to be\nfor any set of virtual displacements δq. This condition yields m equations,\nwhich can also be written as\nThe result is a set of m equations of motion that define the dynamics of the rigid body system.\n"}
{"id": "638834", "url": "https://en.wikipedia.org/wiki?curid=638834", "title": "Economic model", "text": "Economic model\n\nIn economics, a model is a theoretical construct representing economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified, often mathematical, framework designed to illustrate complex processes. Frequently, economic models posit structural parameters. A model may have various exogenous variables, and those variables may change to create various responses by economic variables. Methodological uses of models include investigation, theorizing, and fitting theories to the world.\n\nIn general terms, economic models have two functions: first as a simplification of and abstraction from observed data, and second as a means of selection of data based on a paradigm of econometric study.\n\n\"Simplification\" is particularly important for economics given the enormous complexity of economic processes. This complexity can be attributed to the diversity of factors that determine economic activity; these factors include: individual and cooperative decision processes, resource limitations, environmental and geographical constraints, institutional and legal requirements and purely random fluctuations. Economists therefore must make a reasoned choice of which variables and which relationships between these variables are relevant and which ways of analyzing and presenting this information are useful.\n\n\"Selection\" is important because the nature of an economic model will often determine what facts will be looked at, and how they will be compiled. For example, inflation is a general economic concept, but to measure inflation requires a model of behavior, so that an economist can differentiate between changes in relative prices and changes in price that are to be attributed to inflation.\n\nIn addition to their professional academic interest, the use of models include:\n\nA model establishes an \"argumentative framework\" for applying logic and mathematics that can be independently discussed and tested and that can be applied in various instances. Policies and arguments that rely on economic models have a clear basis for soundness, namely the validity of the supporting model.\n\nEconomic models in current use do not pretend to be \"theories of everything economic\"; any such pretensions would immediately be thwarted by computational infeasibility and the incompleteness or lack of theories for various types of economic behavior. Therefore, conclusions drawn from models will be approximate representations of economic facts. However, properly constructed models can remove extraneous information and isolate useful approximations of key relationships. In this way more can be understood about the relationships in question than by trying to understand the entire economic process.\n\nThe details of model construction vary with type of model and its application, but a generic process can be identified. Generally any modelling process has two steps: generating a model, then checking the model for accuracy (sometimes called diagnostics). The diagnostic step is important because a model is only useful to the extent that it accurately mirrors the relationships that it purports to describe. Creating and diagnosing a model is frequently an iterative process in which the model is modified (and hopefully improved) with each iteration of diagnosis and respecification. Once a satisfactory model is found, it should be double checked by applying it to a different data set.\n\nAccording to whether all the model variables are deterministic, economic models can be classified as stochastic or non-stochastic models; according to whether all the variables are quantitative, economic models are classified as discrete or continuous choice model; according to the model's intended purpose/function, it can be classified as\nquantitative or qualitative; according to the model's ambit, it can be classified as a general equilibrium model, a partial equilibrium model, or even a non-equilibrium model; according to the economic agent's characteristics, models can be classified as rational agent models, representative agent models etc.\n\n\nAt a more practical level, quantitative modelling is applied to many areas of economics and several methodologies have evolved more or less independently of each other. As a result, no overall model taxonomy is naturally available. We can nonetheless provide a few examples that illustrate some particularly relevant points of model construction.\n\n\n\nMost economic models rest on a number of assumptions that are not entirely realistic. For example, agents are often assumed to have perfect information, and markets are often assumed to clear without friction. Or, the model may omit issues that are important to the question being considered, such as externalities. Any analysis of the results of an economic model must therefore consider the extent to which these results may be compromised by inaccuracies in these assumptions, and a large literature has grown up discussing problems with economic models, or at least asserting that their results are unreliable.\n\nOne of the major problems addressed by economic models has been understanding economic growth. An early attempt to provide a technique to approach this came from the French physiocratic school in the Eighteenth century. Among these economists, François Quesnay should be noted, particularly for his development and use of tables he called \"Tableaux économiques\". These tables have in fact been interpreted in more modern terminology as a Leontiev model, see the Phillips reference below.\n\nAll through the 18th century (that is, well before the founding of modern political economy, conventionally marked by Adam Smith's 1776 Wealth of Nations) simple probabilistic models were used to understand the economics of insurance. This was a natural extrapolation of the theory of gambling, and played an important role both in the development of probability theory itself and in the development of actuarial science. Many of the giants of 18th century mathematics contributed to this field. Around 1730, De Moivre addressed some of these problems in the 3rd edition of \"The Doctrine of Chances\". Even earlier (1709), Nicolas Bernoulli studies problems related to savings and interest in the Ars Conjectandi. In 1730, Daniel Bernoulli studied \"moral probability\" in his book Mensura Sortis, where he introduced what would today be called \"logarithmic utility of money\" and applied it to gambling and insurance problems, including a solution of the paradoxical Saint Petersburg problem. All of these developments were summarized by Laplace in his Analytical Theory of Probabilities (1812). Clearly, by the time David Ricardo came along he had a lot of well-established math to draw from.\n\nIn the late 1980s the Brookings Institution compared 12 leading macroeconomic models available at the time. They compared the models' predictions for how the economy would respond to specific economic shocks (allowing the models to control for all the variability in the real world; this was a test of model vs. model, not a test against the actual outcome). Although the models simplified the world and started from a stable, known common parameters the various models gave significantly different answers. For instance, in calculating the impact of a monetary loosening on output some models estimated a 3% change in GDP after one year, and one gave almost no change, with the rest spread between.\n\nPartly as a result of such experiments, modern central bankers no longer have as much confidence that it is possible to 'fine-tune' the economy as they had in the 1960s and early 1970s. Modern policy makers tend to use a less activist approach, explicitly because they lack confidence that their models will actually predict where the economy is going, or the effect of any shock upon it. The new, more humble, approach sees danger in dramatic policy changes based on model predictions, because of several practical and theoretical limitations in current macroeconomic models; in addition to the theoretical pitfalls, (listed above) some problems specific to aggregate modelling are:\n\nComplex systems specialist and mathematician David Orrell wrote on this issue in his book Apollo's Arrow and explained that the weather, human health and economics use similar methods of prediction (mathematical models). Their systems—the atmosphere, the human body and the economy—also have similar levels of complexity. He found that forecasts fail because the models suffer from two problems : (i) they cannot capture the full detail of the underlying system, so rely on approximate equations; (ii) they are sensitive to small changes in the exact form of these equations. This is because complex systems like the economy or the climate consist of a delicate balance of opposing forces, so a slight imbalance in their representation has big effects. Thus, predictions of things like economic recessions are still highly inaccurate, despite the use of enormous models running on fast computers.\n\nEconomic and meteorological simulations may share a fundamental limit to their predictive powers: chaos. Although the modern mathematical work on chaotic systems began in the 1970s the danger of chaos had been identified and defined in \"Econometrica\" as early as 1958:\n\nIt is straightforward to design economic models susceptible to butterfly effects of initial-condition sensitivity.\n\nHowever, the econometric research program to identify which variables are chaotic (if any) has largely concluded that aggregate macroeconomic variables probably do not behave chaotically. This would mean that refinements to the models could ultimately produce reliable long-term forecasts. However the validity of this conclusion has generated two challenges:\n\nMore recently, chaos (or the butterfly effect) has been identified as less significant than previously thought to explain prediction errors. Rather, the predictive power of economics and meteorology would mostly be limited by the models themselves and the nature of their underlying systems (see Comparison with models in other sciences above).\n\nA key strand of free market economic thinking is that the market's invisible hand guides an economy to prosperity more efficiently than central planning using an economic model. One reason, emphasized by Friedrich Hayek, is the claim that many of the true forces shaping the economy can never be captured in a single plan. This is an argument that cannot be made through a conventional (mathematical) economic model, because it says that there are critical systemic-elements that will always be omitted from any top-down analysis of the economy.\n\n\n\n\n"}
{"id": "39228396", "url": "https://en.wikipedia.org/wiki?curid=39228396", "title": "Equivalence principle (geometric)", "text": "Equivalence principle (geometric)\n\nThe equivalence principle is one of the corner-stones of gravitation theory. Different formulations of the equivalence principle are labeled \"weakest\", \"weak\", \"middle-strong\" and \"strong.\" All of these formulations are based on the empirical equality of inertial mass, gravitational active and passive charges.\n\nThe \"weakest\" equivalence principle is restricted to the motion law of a probe point mass in a uniform gravitational field. Its localization is the \"weak\" equivalence principle that states the existence of a desired local inertial frame at a given world point. This is the case of equations depending on a gravitational field and its first order derivatives, e. g., the equations of mechanics of probe point masses, and the equations of electromagnetic and Dirac fermion fields. The \"middle-strong\" equivalence principle is concerned with any matter, except a gravitational field, while the \"strong\" one is applied to all physical laws.\n\nThe above-mentioned variants of the equivalence principle aim to guarantee the transition of General Relativity to Special Relativity in a certain reference frame. However, only the particular \"weakest\" and \"weak\" equivalence principles are true. \nTo overcome this difficulty, the equivalence principle can be formulated in geometric terms as follows.\n\nIn the spirit of Felix Klein's Erlanger program, Special Relativity can be characterized as the Klein geometry of Lorentz group invariants. Then the geometric equivalence principle is formulated to require the existence of Lorentz invariants on a world manifold formula_1. This requirement holds if the tangent bundle formula_2 of formula_1 admits an atlas with Lorentz transition functions, i.e., a structure group of the associated frame bundle formula_4 of linear tangent frames in formula_2 is reduced to the Lorentz group formula_6. By virtue of the well known theorem on structure group reduction, this reduction takes place if and only if the quotient bundle formula_7 possesses a global section, which is a pseudo-Riemannian metric on formula_1.\n\nThus the geometric equivalence principle provides the necessary and sufficient conditions of the existence of a pseudo-Riemannian metric, i.e., a gravitational field on a world manifold.\n\nBased on the geometric equivalence principle, gravitation theory is formulated as gauge theory where a gravitational field is described as a classical Higgs field responsible for spontaneous breakdown of space-time symmetries.\n\n\n"}
{"id": "15285866", "url": "https://en.wikipedia.org/wiki?curid=15285866", "title": "Experimental system", "text": "Experimental system\n\nIn scientific research, an experimental system is the physical, technical and procedural basis for an experiment or series of experiments. Historian of science Hans-Jörg Rheinberger defines an experimental system as: \"A basic unit of experimental activity combining local, technical, instrumental, institutional, social, and epistemic aspects.\" Scientists (particularly laboratory biologists) and historians and philosophers of biology have pointed to the development and spread of successful experimental systems, such as those based on popular model organism or scientific apparatus, as key elements in the history of science, particularly since the early 20th century. The choice of an appropriate experimental system is often seen as critical for a scientist's long-term success, as experimental systems can be very productive for some kinds of questions and less productive for others, acquiring a sort of momentum that takes research in unpredicted directions.\n\nA successful experimental system must be stable and reproducible enough for scientists to make sense of the system's behavior, but variable and unpredictable enough that it can produce useful results. In many cases, a well-understood experimental system can be \"black-boxed\" as a standard technique, which can then be a component of other experimental systems. Rheinberger divides experimental systems into two parts: the part under investigation (\"epistemic things\") and the well-understood part that provides a stable context for experimentation (\"technical objects\").\n\nThe development of experimental systems in biology often requires the \"domestication\" of a particular organism for the laboratory environment, including the creation of relatively homogeneous lines or strains and the tailoring of conditions to highlight the variable aspects that scientists are interested in. Scientific technologies, similarly, often require the development of a full experimental system to go from a viable concept to a technique that works in practice on a usefully consistent basis. For example, the invention of the polymerase chain reaction (PCR) is generally attributed to Kary Mullis, who came up with the concept in 1983, but the process of development of PCR into the revolutionary technology it became by the early 1990s took years of work by others at Cetus Corporation—and the basic components of the system had been known since the 1960s DNA synthesis work of Har Gobind Khorana—making \"who invented PCR?\" a complicated question.\n\n"}
{"id": "37232", "url": "https://en.wikipedia.org/wiki?curid=37232", "title": "Fermat's principle", "text": "Fermat's principle\n\nIn optics, Fermat's principle or the principle of least time, named after French mathematician Pierre de Fermat, is the principle that the path taken between two points by a ray of light is the path that can be traversed in the least time. This principle is sometimes taken as the definition of a ray of light. However, this version of the principle is not general; a more modern statement of the principle is that rays of light traverse the path of stationary optical length with respect to variations of the path. In other words, a ray of light prefers the path such that there are other paths, arbitrarily nearby on either side, along which the ray would take almost exactly the same time to traverse.\n\nFermat's principle can be used to describe the properties of light rays reflected off mirrors, refracted through different media, or undergoing total internal reflection. It follows mathematically from Huygens' principle (at the limit of small wavelength). Fermat's text \"Analyse des réfractions\" exploits the technique of adequality to derive Snell's law of refraction and the law of reflection.\n\nFermat's principle has the same form as Hamilton's principle and it is the basis of Hamiltonian optics.\n\nThe time T a point of the electromagnetic wave needs to cover a path between the points A and B is given by:\n\n\"c\" is the speed of light in vacuum, \"ds\" an infinitesimal displacement along the ray, \"v\" = \"ds\"/\"dt\" the speed of light in a medium and \"n\" = \"c\"/\"v\" the refractive index of that medium, formula_2 is the starting time (the wave front is in A), formula_3 is the arrival time at B. The optical path length of a ray from a point A to a point B is defined by:\n\nand it is related to the travel time by \"S\" = \"cT\". The optical path length is a purely geometrical quantity since time is not considered in its calculation. An extremum in the light travel time between two points A and B is equivalent to an extremum of the optical path length between those two points. The historical form proposed by Fermat is incomplete. A complete modern statement of the variational Fermat principle is that In the context of calculus of variations this can be written as\n\nIn general, the refractive index is a scalar field of position in space, that is, formula_6 in 3D euclidean space. Assuming now that light has a component that travels along the \"x\" axis, the path of a light ray may be parametrized as formula_7 and\n\nwhere formula_9. The principle of Fermat can now be written as\n\nwhich has the same form as Hamilton's principle but in which \"x\" takes the role of time in classical mechanics. Function formula_12 is the optical Lagrangian from which the Lagrangian and Hamiltonian (as in Hamiltonian mechanics) formulations of geometrical optics may be derived.\n\nClassically, Fermat's principle can be considered as a mathematical consequence of Huygens' principle. Indeed, of all secondary waves (along all possible paths) the waves with the extremal (stationary) paths contribute most due to constructive interference. Suppose that light waves propagate from A to B by all possible routes AB, unrestricted initially by rules of geometrical or physical optics. The various optical paths AB will vary by amounts greatly in excess of one wavelength, and so the waves arriving at B will have a large range of phases and will tend to interfere destructively. But if there is a shortest route AB, and the optical path varies smoothly through it, then a considerable number of neighboring routes close to AB will have optical paths differing from AB by second-order amounts only and will therefore interfere constructively. Waves along and close to this shortest route will thus dominate and AB will be the route along which the light is seen to travel.\n\nFermat's principle is the main principle of quantum electrodynamics which states that any particle (e.g. a photon or an electron) propagates over all available, unobstructed paths and that the interference, or superposition, of its wavefunction over all those paths at the point of observation gives the probability of detecting the particle at this point. Thus, because the extremal paths (shortest, longest, or stationary) cannot be completely canceled out, they contribute most to this interference. In humans, for example, Fermat's principle can be demonstrated in a situation when a lifeguard has to find the fastest way to traverse both beach and water in order to reach a drowning swimmer. The principle has been tested in studies with ants, in which the ants' nest is on one end of a container and food is on the opposite end, but the ants choose to follow the path of least time, rather than the most direct path.\n\nIn the classic mechanics of waves, Fermat's principle follows from the extremum principle of mechanics (see variational principle).\n\nEuclid, c. 320 BCE in his Catoptrics (on mirrors, including spherical mirrors) and Optics, laid the foundations for reflection, which was repeated by Ptolemy, and then in his more detailed books that have surfaced, Hero of Alexandria (Heron) (c. 60) described the principle of reflection, which stated that a ray of light that goes from point A to point B, suffering any number of reflections on flat mirrors in the same medium, has a smaller path length than any nearby path.\n\nIbn al-Haytham (Alhacen), in his \"Book of Optics\" (1021), expanded the principle to both reflection and refraction, and expressed an early version of the principle of least time. His experiments were based on earlier works on refraction carried out by the Greek scientist Ptolemy.\n\nThe generalized principle of least time in its modern form was stated by Fermat in a letter dated January 1, 1662, to Cureau de la Chambre. It was met with objections by Claude Clerselier in May 1662, an expert in optics and leading spokesman for the Cartesians at the time. Amongst his objections, Clerselier states:\n\"... The principle which you take as the basis for your proof, namely that Nature always acts by using the simplest and shortest paths, is merely a moral, and not a physical one. It is not, and cannot be, the cause of any effect in Nature.\n\nThe original French, from Mahoney, is as follows:\n\"Le principe que vous prenez pour fondement de votre démonstration, à savoir que la nature agit toujours par les voies les plus courtes et les plus simples, n’est qu’un principe moral et non point physique, qui n’est point et qui ne peut être la cause d’aucun effet de la nature.\"\nAlthough Fermat's principle does not hold standing alone, we now know it can be derived from earlier principles such as Huygens' principle.\n\nHistorically, Fermat's principle has served as a guiding principle in the formulation of physical laws with the use of variational calculus (see Principle of least action).\n\n"}
{"id": "34802284", "url": "https://en.wikipedia.org/wiki?curid=34802284", "title": "Gauss's Pythagorean right triangle proposal", "text": "Gauss's Pythagorean right triangle proposal\n\nGauss's Pythagorean right triangle proposal is an idea attributed to Carl Friedrich Gauss for a method to signal extraterrestrial beings by constructing an immense right triangle and three squares on the surface of the Earth. The shapes would be a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars.\n\nAlthough credited in numerous sources as originating with Gauss, with exact details of the proposal set out, the specificity of detail, and even whether Gauss made the proposal, have been called into question. Many of the earliest sources do not actually name Gauss as the originator, instead crediting a \"German Astronomer\" or using other nonspecific descriptors, and in some cases naming a different author entirely. The details of the proposal also change significantly upon different retellings. Nevertheless, Gauss's writings reveal a belief and interest in finding a method to contact extraterrestrial life, and that he did, at the least, propose using amplified light using a heliotrope, his own 1818 invention, to signal supposed inhabitants of the Moon.\n\nCarl Friedrich Gauss is credited with an 1820 proposal for a method to signal extraterrestrial beings in the form of drawing an immense right triangle and three squares on the surface of the Earth, intended as a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars. Details vary between sources, but typically the \"drawing\" was to be constructed on the Siberian tundra, and made up of vast strips of pine forest forming the right triangle's borders, with the interior of the drawing and exterior squares composed of fields of wheat. Gauss is said to have been convinced that Mars harbored intelligent life and that this geometric figure, invoking the Pythagorean theorem through the squares on the outside borders (sometimes called a \"windmill diagram\", as originated by Euclid), would demonstrate to such alien observers the reciprocal existence of intelligent life on Earth and its grounding in mathematics. Wheat was said to be chosen by Gauss for contrast with the pine tree borders \"because of its uniform color\".\n\nThe specificity of the proposal's details as it appears in most later sources—even its attribution to Gauss—is called into question in University of Notre Dame Professor Michael J. Crowe's 1986 book, \"The Extraterrestrial Life Debate, 1750–1900\", in which he surveys the origins of the Gauss proposal and observes that:The history of this proposal ... can be traced through two dozen or more pluralist writings reaching back to the first half of the nineteenth century. When this is done, however, it turns out that the story exists in almost as many forms as its retellings. Furthermore, these versions share one characteristic: Never is reference supplied to where in the writings of Gauss ... the [proposal] appear[s]!Some early sources explored by Crowe for the attribution and form of Gauss's proposal include Austrian astronomer, Joseph Johann Littrow's statement in \"Wunder des Himmels\" that \"one of our most distinguished geometers\" proposed that a geometric figure \"for example the well known so-called square of the hypotenuse, be laid out on a large scale, say on a particular broad plain of the earth\". and Patrick Scott's \"Love in the Moon\", in which a \"learned man\" is described as proposing a signal formed by a \"great plantation of tree\" in the form of \"47th Proposition of Euclid\" in \"the great African dessert \". In \"Chambers's Edinburgh Journal\" it was written that a Russian savant had proposed to \"communicate with the moon by cutting a large figure of the forty-seventh proposition of Euclid on the plains of Siberia, which, he said, any fool would understand\".\n\nIn the writings of astronomers Asaph Hall and of Norman Lockyer, each refer separately to a \"German Astronomer\" who proposed the method of contact be by \"fire signals\" from Siberia. Writing in 1902, Simon Newcomb placed the origin of a Siberian triangle \"several hundred miles in extent\" not with Gauss, but at the feet of German astronomer Franz Xaver von Zach. In lectures presented by François Arago at the Paris Observatory, he named Siberia as the location of an extraterrestrial signaling project advanced by an unnamed \"German geometer\", but that the signaling method was to be through the use of mirrors, rather than any large symbol drawn upon the Earth. Despite this version's departure from a geometric figure, the appearance of mirrors as a signaling device has a connection with Gauss's background. Gauss invented the Heliotrope in 1818, an instrument that uses a mirror to reflect sunlight in a manner allowing a square mirror to be seen away even in sunny weather.\n\nGauss wrote of the heliotrope's potential as a celestial signaling device in a March 25, 1822 letter to Heinrich Olbers, by which he reveals a belief and interest in finding a method to contact extraterrestrial life: \"With 100 separate mirrors, each of 16 square feet, used conjointly, one would be able to send good heliotrope-light to the moon ... This would be a discovery even greater than that of America, if we could get in touch with our neighbors on the moon.\" Finally, in the October 1826 issue of the \"Edinburgh New Philosophical Journal\" an unnamed author wrote that in a conversation with Franz von Gruithuisen, Gauss stated words to the effect that \"the plan of erecting a geometrical figure on the plains of Siberia corresponded with his opinion, because, according to his view a correspondence with the inhabitants of the moon could only be begun by means of such mathematical contemplations and ideas, which we and they have in common.\" Crowe concluded in sum that his review of earliest sources failed to confirm the detail of the proposal and Gauss as its author, but that his origination of the idea was not unlikely given the existing evidence.\n"}
{"id": "18562346", "url": "https://en.wikipedia.org/wiki?curid=18562346", "title": "Gossen's laws", "text": "Gossen's laws\n\nGossen's laws, named for Hermann Heinrich Gossen (1810 – 1858), are three laws of economics:\n\nThe citation referenced is the translation by Nicholas Georgescu-Roegen in which the traslator names only two laws: 1) ”If an enjoyment is experienced uninterruptedly, the corresponding intensity of pleasure decreases continuously until satiety is ultimately reached, at which point the intensity becomes nil.\" and, 2) \"A similar decrease of the intensity of pleasure takes place if a previous enjoyment of the same kind of pleasure is repeated. Not only does the initial intensity of pleasure become smaller but also the duration of the enjoyment becomes shorter, so that satiety is reached sooner. Moreover, the sooner the repetition, the smaller becomes the initial intensity as well as the duration of the enjoyment.\" (p.lxxx)\n\n\n"}
{"id": "161999", "url": "https://en.wikipedia.org/wiki?curid=161999", "title": "Idea", "text": "Idea\n\nIn philosophy, ideas are usually taken as mental representational images of some object. Ideas can also be abstract concepts that do not present as mental images. Many philosophers have considered ideas to be a fundamental ontological category of being. The capacity to create and understand the meaning of ideas is considered to be an essential and defining feature of human beings. In a popular sense, an idea arises in a reflexive, spontaneous manner, even without thinking or serious reflection, for example, when we talk about the \"idea\" of a person or a place. A new or original idea can often lead to innovation.\n\nThe word \"idea\" comes from Greek ἰδέα \"idea\" \"form, pattern,\" from the root of ἰδεῖν \"idein\", \"to see.\" \n\nOne view on the nature of ideas is that there exist some ideas (called \"innate ideas\") which are so general and abstract that they could not have arisen as a representation of an object of our perception but rather were in some sense always present. These are distinguished from \"adventitious ideas\" which are images or concepts which are accompanied by the judgment that they are caused or occasioned by an external object.\n\nAnother view holds that we only discover ideas in the same way that we discover the real world, from personal experiences. The view that humans acquire all or almost all their behavioral traits from nurture (life experiences) is known as \"tabula rasa\" (\"blank slate\"). Most of the confusions in the way ideas arise is at least in part due to the use of the term \"idea\" to cover both the representation perceptics and the object of conceptual thought. This can be always illustrated in terms of the scientific doctrines of innate ideas, \"concrete ideas versus abstract ideas\", as well as \"simple ideas versus complex ideas\".\n\nPlato in Ancient Greece was one of the earliest philosophers to provide a detailed discussion of ideas and of the thinking process (it must be noted that in Plato's Greek the word \"idea\" carries a rather different sense from our modern English term). Plato argued in dialogues such as the \"Phaedo\", \"Symposium\", \"Republic\", and \"Timaeus\" that there is a realm of ideas or forms (\"eidei\"), which exist independently of anyone who may have thoughts on these ideas, and it is the ideas which distinguish mere opinion from knowledge, for unlike material things which are transient and liable to contrary properties, ideas are unchanging and nothing but just what they are. Consequently, Plato seems to assert forcefully that material things can only be the objects of opinion; real knowledge can only be had of unchanging ideas. Furthermore, ideas for Plato appear to serve as universals; consider the following passage from the \"Republic\":\nDescartes often wrote of the meaning of \"idea\" as an image or representation, often but not necessarily \"in the mind\", which was well known in the vernacular. Despite that Descartes is usually credited with the invention of the non-Platonic use of the term, he at first followed this vernacular use. In his \"Meditations on First Philosophy\" he says, \"Some of my thoughts are like images of things, and it is to these alone that the name 'idea' properly belongs.\" He sometimes maintained that ideas were innate and uses of the term \"idea\" diverge from the original primary scholastic use. He provides multiple non-equivalent definitions of the term, uses it to refer to as many as six distinct kinds of entities, and divides \"ideas\" inconsistently into various genetic categories. For him knowledge took the form of ideas and philosophical investigation is the deep consideration of these entities.\n\nIn striking contrast to Plato's use of idea is that of John Locke. In his Introduction to An Essay Concerning Human Understanding, Locke defines \"idea\" as \"that term which, I think, serves best to stand for whatsoever is the object of the understanding when a man thinks, I have used it to express whatever is meant by phantasm, notion, species, or whatever it is which the mind can be employed about in thinking; and I could not avoid frequently using it.\" He said he regarded the book necessary to examine our own abilities and see what objects our understandings were, or were not, fitted to deal with. In his philosophy other outstanding figures followed in his footsteps — Hume and Kant in the 18th century, Arthur Schopenhauer in the 19th century, and Bertrand Russell, Ludwig Wittgenstein, and Karl Popper in the 20th century. Locke always believed in \"good sense\" — not pushing things to extremes and on taking fully into account the plain facts of the matter. He considered his common-sense ideas \"good-tempered, moderate, and down-to-earth.\"\n\nAs John Locke studied humans in his work “An Essay Concerning Human Understanding” he continually referenced Descartes for ideas as he asked this fundamental question: “When we are concerned with something about which we have no certain knowledge, what rules or standards should guide how confident we allow ourselves to be that our opinions are right?” A simpler way of putting it is how do humans know ideas, and what are the different types of ideas. An idea to Locke “can simply mean some sort of brute experience.” He shows that there are “No innate principles in the mind.”. Thus, he concludes that “our ideas are all experiential in nature.” An experience can either be a sensation or a reflection: “consider whether there are any innate ideas in the mind before any are brought in by the impression from sensation or reflection.” Therefore, an idea was an experience in which the human mind apprehended something.\n\nIn a Lockean view, there are really two types of ideas: complex and simple. Simple ideas are the building blocks for much more complex ideas, and “While the mind is wholly passive in the reception of simple ideas, it is very active in the building of complex ideas…” Complex ideas, therefore, can either be modes, substances, or relations. Modes are when ideas are combined in order to convey new information. For instance, David Banach gives the example of beauty as a mode. He says that it is the combination of color and form. Substances, however, is different. Substances are certain objects, that can either be dogs, cats, or tables. And relations represent the relationship between two or more ideas. In this way, Locke did, in fact, answer his own questions about ideas and humans.\n\nHume differs from Locke by limiting \"idea\" to the more or less vague mental reconstructions of perceptions, the perceptual process being described as an \"impression.\" Hume shared with Locke the basic empiricist premise that it is only from life experiences (whether their own or others') that humans' knowledge of the existence of anything outside of themselves can be ultimately derived, that they shall carry on doing what they are prompted to do by their emotional drives of varying kinds. In choosing the means to those ends, they shall follow their accustomed associations of ideas. Hume has contended and defended the notion that \"reason alone is merely the 'slave of the passions'.\" \n\nImmanuel Kant defines an \"idea\" as opposed to a \"concept\". \"Regulative ideas\" are ideals that one must tend towards, but by definition may not be completely realized. Liberty, according to Kant, is an idea. The autonomy of the rational and universal subject is opposed to the determinism of the empirical subject. Kant felt that it is precisely in knowing its limits that philosophy exists. The business of philosophy he thought was not to give rules, but to analyze the private judgements of good common sense.\n\nWhereas Kant declares limits to knowledge (\"we can never know the thing in itself\"), in his epistemological work, Rudolf Steiner sees \"ideas\" as \"objects of experience\" which the mind apprehends, much as the eye apprehends light. In \"Goethean Science\" (1883), he declares, \"Thinking ... is no more and no less an organ of perception than the eye or ear. Just as the eye perceives colors and the ear sounds, so thinking perceives ideas.\" He holds this to be the premise upon which Goethe made his natural-scientific observations.\n\nWundt widens the term from Kant's usage to include \"conscious representation of some object or process of the external world\". In so doing, he includes not only ideas of memory and imagination, but also perceptual processes, whereas other psychologists confine the term to the first two groups. One of Wundt's main concerns was to investigate conscious processes in their own context by experiment and introspection. He regarded both of these as \"exact methods\", interrelated in that experimentation created optimal conditions for introspection. Where the experimental method failed, he turned to other \"objectively valuable aids\", specifically to \"those products of cultural communal life which lead one to infer particular mental motives. Outstanding among these are speech, myth, and social custom.\" Wundt designed the basic mental activity apperception — a unifying function which should be understood as an activity of the will. Many aspects of his empirical physiological psychology are used today. One is his principles of mutually enhanced contrasts and of assimilation and dissimilation (i.e. in color and form perception and his advocacy of \"objective\" methods of expression and of recording results, especially in language. Another is the principle of heterogony of ends — that multiply motivated acts lead to unintended side effects which in turn become motives for new actions.\n\nC. S. Peirce published the first full statement of pragmatism in his important works \"\" (1878) and \"\" (1877). In \"How to Make Our Ideas Clear\" he proposed that a \"clear idea\" (in his study he uses concept and \"idea\" as synonymic) is defined as one, when it is apprehended such as it will be recognized wherever it is met, and no other will be mistaken for it. If it fails of this clearness, it is said to be obscure. He argued that to understand an idea clearly we should ask ourselves what difference its application would make to our evaluation of a proposed solution to the problem at hand. Pragmatism (a term he appropriated for use in this context), he defended, was a method for ascertaining the meaning of terms (as a theory of meaning). The originality of his ideas is in their rejection of what was accepted as a view and understanding of knowledge by scientists for some 250 years, i.e. that, he pointed, knowledge was an impersonal fact. Peirce contended that we acquire knowledge as \"participants\", not as \"spectators\". He felt \"the real\", sooner or later, is information acquired through ideas and knowledge with the application of logical reasoning would finally result in. He also published many papers on logic in relation to \"ideas\".\n\nG. F. Stout and J. M. Baldwin, in the \"Dictionary of Philosophy and Psychology\", define \"idea\" as \"the reproduction with a more or less adequate image, of an object not actually present to the senses.\" They point out that an idea and a perception are by various authorities contrasted in various ways. \"Difference in degree of intensity\", \"comparative absence of bodily movement on the part of the subject\", \"comparative dependence on mental activity\", are suggested by psychologists as characteristic of an idea as compared with a perception.\n\nIt should be observed that an idea, in the narrower and generally accepted sense of a mental reproduction, is frequently composite. That is, as in the example given above of the idea of a chair, a great many objects, differing materially in detail, all call a single idea. When a man, for example, has obtained an idea of chairs in general by comparison with which he can say \"This is a chair, that is a stool\", he has what is known as an \"abstract idea\" distinct from the reproduction in his mind of any particular chair (see abstraction). Furthermore, a complex idea may not have any corresponding physical object, though its particular constituent elements may severally be the reproductions of actual perceptions. Thus the idea of a centaur is a complex mental picture composed of the ideas of man and horse, that of a mermaid of a woman and a fish.\n\nDiffusion studies explore the spread of ideas from culture to culture. Some anthropological theories hold that all cultures imitate ideas from one or a few original cultures, the Adam of the Bible, or several cultural circles that overlap. Evolutionary diffusion theory holds that cultures are influenced by one another but that similar ideas can be developed in isolation.\n\nIn the mid-20th century, social scientists began to study how and why ideas spread from one person or culture to another. Everett Rogers pioneered diffusion of innovations studies, using research to prove factors in adoption and profiles of adopters of ideas. In 1976, in his book \"The Selfish Gene\", Richard Dawkins suggested applying biological evolutionary theories to the spread of ideas. He coined the term \"meme\" to describe an abstract unit of selection, equivalent to the gene in evolutionary biology.\n\nJames Boswell recorded Samuel Johnson's opinion about ideas. Johnson claimed that they are mental images or internal visual pictures. As such, they have no relation to words or the concepts which are designated by verbal names.\n\nTo protect the cause of invention and innovation, the legal constructions of Copyrights and Patents were established. Patent law regulates various aspects related to the functional manifestation of inventions based on new ideas or incremental improvements to existing ones. Thus, patents have a direct relationship to ideas.\n\nIn some cases, authors can be granted limited legal monopolies on the manner in which certain works are expressed. This is known colloquially as copyright, although the term intellectual property is used mistakenly in place of \"copyright\". Copyright law regulating the aforementioned monopolies generally does not cover the actual ideas. The law does not bestow the legal status of property upon ideas per se. Instead, laws purport to regulate events related to the usage, copying, production, sale and other forms of exploitation of the fundamental expression of a work, that may or may not carry ideas. Copyright law is fundamentally different from patent law in this respect: patents do grant monopolies on ideas (more on this below).\n\nA copyright is meant to regulate some aspects of the usage of expressions of a work, \"not\" an idea. Thus, copyrights have a negative relationship to ideas.\n\nWork means a tangible medium of expression. It may be an original or derivative work of art, be it literary, dramatic, musical recitation, artistic, related to sound recording, etc. In (at least) countries adhering to the Berne Convention, copyright automatically starts covering the work upon the original creation and fixation thereof, without any extra steps. While creation usually involves an idea, the idea in itself does not suffice for the purposes of claiming copyright. \nConfidentiality and nondisclosure agreements are legal instruments that assist corporations and individuals in keeping ideas from escaping to the general public. Generally, these instruments are covered by contract law.\n\n\n"}
{"id": "41627505", "url": "https://en.wikipedia.org/wiki?curid=41627505", "title": "Imago Universi", "text": "Imago Universi\n\nAndreas Cellarius, German mathematician and cartographer (1596–1665), conceived an Atlas of the Universe, published in 1660, under the title of \"Harmonia Macrocosmica\". Numerous illustrations of the solar system appear in this atlas by different authors known at that time. Referring to Ptolemy, Cellarius called the representation of this Ptolemaic conception of heaven as \"Imago universi secundum Ptolaeum\"\n\n\"Imago\" is a word in Latin which means\" 'image\"' or even \"representation\". Therefore, the title expresses the \"Picture of the Universe according to Ptolemy.\" The Latin expression was used in the Middle Ages to express the representation and size of the known world at that time.\n\n\"Imago Universi\" is also the title, in Latin, of a cosmographic treatise, written in 2013 by the Spanish scientist Gabriel Barceló.\n\nAfter analyzing the history of cosmology, the treatise delves into the prevailing scientific lack of explanation of the rotation of the heavenly bodies in the laws of dynamic behaviour of the sidereal system. The author proposes the application of the Theory of Dynamic Interactions (TID) to astrophysics, in particular, the dynamics of stellar systems and galaxies. This theory allows new comprehension of the dynamics of nature and understands the dynamic equilibrium of the universe, always subjected to rotational accelerations, but repetitive and persistent. The author also highlights that the orbiting always coincides with the intrinsic rotation of celestial bodies. Paradox incorporating the book, noting that this had not been found to date.\n\n\n1. Einstein, Albert: The Origins of the General Theory of Relativity, lecture given at the George A. Foundation Gibson, University of Glasgow, 20 June 1933. Published by Jackson, Wylie and co, Glasgow, 1933.\n\n"}
{"id": "2892645", "url": "https://en.wikipedia.org/wiki?curid=2892645", "title": "Instantiation principle", "text": "Instantiation principle\n\nThe instantiation principle or principle of instantiation or principle of exemplification is the concept in metaphysics and logic (first put forward by David Malet Armstrong) that there can be no uninstantiated or unexemplified properties (or universals). In other words, it is impossible for a property to exist which is not had by some object.\n\nConsider a chair. Presumably chairs did not exist 150,000 years ago. Thus, according to the principle of instantiation, the property of being a chair did not exist 150,000 years ago either. Similarly, if all red objects were to suddenly go out of existence, then the property of being red would likewise go out of existence.\n\nTo make the principle more plausible in the light of these examples, the existence of properties or universals is not tied to their actual existence now, but to their existence in space-time considered as a whole. Thus, any property which \"is\", \"has been\", or \"will be\" instantiated exists. The property of being red would exist even if all red things were to be destroyed, because it has been instantiated. This broadens the range of properties which exist if the principle is true. \n\nThose who endorse the principle of instantiation are known as \"in re\" (in thing or in reality) realists or 'immanent realists'.\n\n"}
{"id": "26279594", "url": "https://en.wikipedia.org/wiki?curid=26279594", "title": "Interpretation (philosophy)", "text": "Interpretation (philosophy)\n\nA philosophical interpretation is the assignment of meanings to various concepts, symbols, or objects under consideration. Two broad types of interpretation can be distinguished: interpretations of physical objects, and interpretations of concepts (Conceptual model).\n\nInterpretation is related to perceiving the things. An aesthetic interpretation is an explanation of the meaning of some work of art. An aesthetic interpretation expresses an understanding of a work of art, a poem, performance, or piece of literature. There may be different interpretations to same work by art by different people owing to their different perceptions or aims. All such interpretations are termed as 'aesthetic interpretations'. Some people, instead of interpreting work of art, believe in interpreting artist himself. It pretty much means \"how or what do I believe about (subject)\"\n\nA judicial interpretation is a conceptual interpretation that explains how the judiciary should interpret the law, particularly constitutional documents and legislation (see statutory interpretation).\n\nIn logic, an interpretation is an assignment of meaning to the symbols of a language. The formal languages used in mathematics, logic, and theoretical computer science are defined in solely syntactic terms, and as such do not have any meaning until they are given some interpretation. The general study of interpretations of formal languages is called \"formal semantics\".\n\nReligious interpretation and similarly religious self-interpretation define a section of religion-related studies (theology, comparative religion, reason) where attention is given to aspects of perception—where religious symbolism and the self-image of all those who hold religious views have important bearing on how others perceive their particular belief system and its adherents.\n\nAn interpretation is a \"descriptive interpretation\" (also called a \"factual interpretation\") if at least one of the undefined symbols of its formal system becomes, in the interpretation, the name of a physical object, or observable property. A descriptive interpretation is a type of interpretation used in science and logic to talk about empirical entities.\n\nWhen scientists attempt to formalize the principles of the empirical sciences, they use an interpretation to model reality, in the same way logicians axiomatize the principles of logic. The aim of these attempts is to construct a formal system that will serve as a conceptual model of reality. Predictions or other statements drawn from such a formal system mirror or map the real world only insofar as these scientific models are true.\n\n"}
{"id": "25200737", "url": "https://en.wikipedia.org/wiki?curid=25200737", "title": "Knowledge arena", "text": "Knowledge arena\n\nA knowledge arena is a virtual space where individuals can manipulate concepts and relationships to form a concept map. Individuals using a computer with appropriate software can represent concepts and the relationships between concepts in a node-relationship-node formalism. The process of thinking about the concepts and making associations between them has been called \"off-loading\" by McAleese.\n\nThe concept map is a form of a semantic network or semantic graph. It is formally based on graph theory. In the concept map, concepts are represented by nodes. The relationship between nodes are represented by \"typed links\" or \"edges\". (See Graph theory.) In creating a map or graphic representation of what is known an individual intentionally interacts with the graphical interface or map and through a reflective process adds nodes (concepts) and /or adds relationships (edges or typed links) or modifies existing node-relationship-node instances. It is likely that the process of engaging with concepts and relationships between concepts brings about the creation of understandings as well as making the understandings explicit.\n\nMany different claims have been made for the utility of the concept map. The interactive and reflective nature of map creation is highlighted by the use of the description Knowledge Arena. Although maps may represent what an individual knows at a point in time; it is likely that by interacting with the concepts and relationships in the knowledge arena individual continues to create and modify what that individual \"knows\".\n\nSee also \n"}
{"id": "31938666", "url": "https://en.wikipedia.org/wiki?curid=31938666", "title": "Knowledge space (philosophy)", "text": "Knowledge space (philosophy)\n\nIn philosophy and media studies, a knowledge space is described as an emerging anthropological space in which the knowledge of individuals becomes the primary focus for social structure, values, and beliefs. The concept is put forward and explored by philosopher and media critic Pierre Lévy in his 1997 book \"Collective Intelligence\".\n\nLevy's notion of the \"knowledge space\" relies on his conception of anthropological spaces, which he defines as \"a system of proximity (space) unique to the world of humanity (anthropological), and thus dependent on human technologies, significations, language, culture, conventions, representations, and emotions\" (5). Building on the language of the philosophers Gilles Deleuze and Félix Guattari, he states that \"anthropological spaces in themselves are neither infrastructures nor superstructures but planes of existence, frequencies, velocities, determined within the social spectrum\" (147). Each space contains \"worlds of signification\" (149) by which humans come to understand and make sense of the world. Furthermore, although one space may dominate, many spaces can and do exist simultaneously.\n\nLevy describes three existing anthropological spaces. They are:\n\nThe knowledge space is an emerging anthropological space which, while it has always existed (139), is only now coming into fruition as a guiding space of humanity. In this space, singularities (individuals) are recognized as singularities and knowledge becomes the guiding value for humanity. Since all human experience represents unique knowledge, within the knowledge space all individuals are valued for their unique knowledge regardless of race (earth space), nationality (territorial space), or economic status (commodity space). Within this space static identity gives way to the \"quantum identities\" as individuals become participates and the distinction between of \"us\" and \"them\" disappears (159). Instead, humanity forms \"collective intelligences\" in which knowledge is valued and freely traded. What is \"real\" becomes \"that which implies the practical activity, intellectual and imaginary, of living subjects\" (168). Life, experiences, and knowledge become the underlying and ever changing guiding path for human societies.\n\nLevy's theories rely heavily on the technological developments of the 1990s, particularly the rise of biotechnology, nanotechnology, the Internet, new media and information technologies. In chapter 3, he describes how technologies have made a shift from the molar to the molecular (a move which makes literal a distinction by Delueze and Guattari) in that technologies now handle units as individuals (his term is \"singularities\") rather than in mass. He suggests that this mirrors our rising recognition of the individuals as singularities rather than massive conglomerated groups.\n"}
{"id": "4021739", "url": "https://en.wikipedia.org/wiki?curid=4021739", "title": "LaSalle's invariance principle", "text": "LaSalle's invariance principle\n\nLaSalle's invariance principle (also known as the invariance principle, Barbashin-Krasovskii-LaSalle principle, or Krasovskii-LaSalle principle ) is a criterion for the asymptotic stability of an autonomous (possibly nonlinear) dynamical system.\n\nSuppose a system is represented as\n\nwhere formula_2 is the vector of variables, with\n\nIf a formula_4 function formula_5 can be found such that\n\nthen the set of accumulation points of any trajectory is contained in formula_8 where formula_8 is the union of complete trajectories contained entirely in the set formula_10. \n\nIf we additionally have that the function formula_11 is positive definite, i.e.\n\nand if formula_8 contains no trajectory of the system except the trivial trajectory formula_16 for formula_17, then the origin is asymptotically stable.\n\nFurthermore, if formula_11 is radially unbounded, i.e.\n\nthen the origin is globally asymptotically stable.\n\nIf \n\nhold only for formula_24 in some neighborhood formula_25 of the origin, and the set\n\ndoes not contain any trajectories of the system besides the trajectory formula_27, then the local version of the invariance principle states that the origin is locally asymptotically stable.\n\nIf formula_28 is negative definite, the global asymptotic stability of the origin is a consequence of Lyapunov's second theorem. The invariance principle gives a criterion for asymptotic stability in the case when formula_29 is only negative semidefinite.\n\nThis section will apply the invariance principle to establish the local asymptotic stability of a simple system, the pendulum with friction. This system can be modeled with the differential equation \n\nwhere formula_31 is the angle the pendulum makes with the vertical normal, formula_32 is the mass of the pendulum, formula_33 is the length of the pendulum, formula_34 is the friction coefficient, and \"g\" is acceleration due to gravity.\n\nThis, in turn, can be written as the system of equations\n\nUsing the invariance principle, it can be shown that all trajectories which begin in a ball of certain size around the origin formula_37 asymptotically converge to the origin. We define formula_38 as\n\nThis formula_38 is simply the scaled energy of the system Clearly, formula_41 is positive definite in an open ball of radius formula_42 around the origin. Computing the derivative,\n\nObserve that formula_44. If it were true that formula_45, we could conclude that every trajectory approaches the origin by Lyapunov's second theorem. Unfortunately, formula_46 and formula_47 is only negative semidefinite. However, the set\n\nwhich is simply the set\n\ndoes not contain any trajectory of the system, except the trivial trajectory x = 0. Indeed, if at some time formula_50, formula_51, then because \nformula_52 must be less than formula_42 away from the origin, formula_54 and formula_55. As a result, the trajectory will not stay in the set formula_56.\n\nAll the conditions of the local version of the invariance principle are satisfied, and we can conclude that every trajectory that begins in some neighborhood of the origin will converge to the origin as formula_57 .\n\nThe general result was independently discovered by J.P. LaSalle (then at RIAS) and N.N. Krasovskii, who published in 1960 and 1959 respectively. While LaSalle was the first author in the West to publish the general theorem in 1960, a special case of the theorem was in communicated in 1952 by Barbashin and Krasovskii, followed by a publication of the general result in 1959 by Krasovskii .\n\n\n\n\n\n"}
{"id": "3108937", "url": "https://en.wikipedia.org/wiki?curid=3108937", "title": "Landauer's principle", "text": "Landauer's principle\n\nLandauer's principle is a physical principle pertaining to the lower theoretical limit of energy consumption of computation. It holds that \"any logically irreversible manipulation of information, such as the erasure of a bit or the merging of two computation paths, must be accompanied by a corresponding entropy increase in non-information-bearing degrees of freedom of the information-processing apparatus or its environment\".\n\nAnother way of phrasing Landauer's principle is that if an observer loses information about a physical system, the observer loses the ability to extract work from that system.\n\nA so-called logically-reversible computation, in which no information is erased, may in principle be carried out without releasing any heat. This has led to considerable interest in the study of reversible computing. Indeed, without reversible computing, increases in the number of computations-per-joule-of-energy-dissipated must come to a halt by about 2050: because the limit implied by Landauer's principle will be reached by then, according to Koomey's law. \n\nAt 20 °C (room temperature, or 293.15 K), the Landauer limit represents an energy of approximately 0.0172 eV, or 2.75 zJ. Theoretically, roomtemperature computer memory operating at the Landauer limit could be changed at a rate of one billion bits per second with energy being converted to heat in the memory media at the rate of only 2.85 trillionths of a watt (that is, at a rate of only 2.85 pJ/s). Modern computers use millions of times as much energy per second.\n\nRolf Landauer first proposed the principle in 1961 while working at IBM. He rigorously justified and stated important limits to an earlier conjecture by John von Neumann. For this reason, it is sometimes referred to as being simply the Landauer bound or Landauer limit.\n\nIn 2011, the principle was generalized to show that while information erasure requires an increase in entropy, that increase could theoretically occur at no energy cost. Instead, the cost can be taken in another conserved quantity, such as angular momentum.\n\nIn a 2012 article published in \"Nature\", a team of physicists from the École normale supérieure de Lyon, University of Augsburg and the University of Kaiserslautern described that for the first time they have measured the tiny amount of heat released when an individual bit of data is erased.\n\nIn 2014, physical experiments tested Landauer's principle and confirmed its predictions.\n\nIn 2016, researchers used a laser probe to measure the amount of energy dissipation that resulted when a nanomagnetic bit flipped from off to on. Flipping the bit required 15 millielectron volts (3 zeptojoules).\n\nA 2018 article published in \"Nature Physics\" features a Landauer erasure performed at cryogenic temperatures (T = 1K) on an array of high-spin (S = 10) quantum molecular magnets. The array is made to act as a spin register where each nanomagnet encodes a single bit of information. The experiment has laid the foundations for the extension of the validity of the Landauer principle to the quantum realm. Owing to the fast dynamics and low \"inertia\" of the single spins used in the experiment, the researchers also showed how an erasure operation can be carried out at the lowest possible thermodynamic cost — that imposed by the Landauer principle — and at a high speed. \n\nLandauer's principle can be understood to be a simple logical consequence of the second law of thermodynamics—which states that the entropy of an isolated system cannot decrease—together with the definition of thermodynamic temperature. For, if the number of possible logical states of a computation were to decrease as the computation proceeded forward (logical irreversibility), this would constitute a forbidden decrease of entropy, unless the number of possible physical states corresponding to each logical state were to simultaneously increase by at least a compensating amount, so that the total number of possible physical states was no smaller than it was originally (i.e. total entropy has not decreased).\n\nYet, an increase in the number of physical states corresponding to each logical state means that, for an observer who is keeping track of the logical state of the system but not the physical state (for example an \"observer\" consisting of the computer itself), the number of possible physical states has increased; in other words, entropy has increased from the point of view of this observer.\n\nThe maximum entropy of a bounded physical system is finite. (If the holographic principle is correct, then physical systems with finite surface area have a finite maximum entropy; but regardless of the truth of the holographic principle, quantum field theory dictates that the entropy of systems with finite radius and energy is finite due to the Bekenstein bound.) To avoid reaching this maximum over the course of an extended computation, entropy must eventually be expelled to an outside environment.\n\nLandauer's principle asserts that there is a minimum possible amount of energy required to erase one bit of information, known as the \"Landauer limit\":\nwhere \"k\" is the Boltzmann constant (approximately 1.38×10 J/K), \"T\" is the temperature of the heat sink in kelvins, and ln 2 is the natural logarithm of 2 (approximately 0.69315).\n\nFor an environment at temperature \"T\", energy \"E\" = \"ST\" must be emitted into that environment if the amount of added entropy is \"S\". For a computational operation in which 1 bit of logical information is lost, the amount of entropy generated is at least \"k\" ln 2, and so, the energy that must eventually be emitted to the environment is \"E\" ≥ \"kT\" ln 2.\n\nThe principle is widely accepted as physical law, but in recent years it has been challenged for using circular reasoning and faulty assumptions, notably in Earman and Norton (1998), and subsequently in Shenker (2000) and Norton (2004, 2011), and defended by Bennett (2003) and Ladyman et al. (2007).\n\n\n"}
{"id": "563405", "url": "https://en.wikipedia.org/wiki?curid=563405", "title": "League system", "text": "League system\n\nA league system is a hierarchy of leagues in a sport. They are often called pyramids, due to their tendency to split into an increasing number of regional divisions further down the system. League systems of some sort are used in many sports in many countries.\n\nIn association football, rugby union and rugby league, league systems are usually connected by the process of promotion and relegation, in which teams from a lower division who finish at the top of the standings in their league are promoted (advanced to the next level of the system) while teams who finish lowest in their division are relegated (move down to a lower division). This process can be automatic each year, or can require playoffs.\n\nIn North America, league systems in the most popular sports do not use promotion or relegation. Most professional sports are divided into major and minor leagues. Baseball and association football (known as soccer in North America) have well-defined pyramid shapes to their minor league hierarchies, each managed by a governing body (Minor League Baseball, an organization under the authority of the Commissioner of Baseball, governs baseball leagues; the United States Soccer Federation designates the American soccer pyramid.) Ice hockey's professional minor league system is linear, with one league at most of the four levels of the game; the ice hockey league system in North America is governed by collective bargaining agreements and affiliation deals between the NHL, AHL and ECHL.\n\nGridiron football does not operate on a league system. Different professional leagues play by very different sets of rules in different seasons (the NFL plays 11-a-side on a 100-yard field in autumn and early winter, the CFL uses 12-a-side on a 110-yard field in summer and early fall, while arena football and the minor indoor leagues each play 8-a-side on a 50-yard field in the spring and early summer). There have been attempts at forming true minor leagues for the professional game (most recently with 2017's The Spring League); none so far have been able to balance the major leagues' requests with the ability to maintain financial solvency.\n\n"}
{"id": "34074829", "url": "https://en.wikipedia.org/wiki?curid=34074829", "title": "Lexical entrainment", "text": "Lexical entrainment\n\nLexical entrainment is the phenomenon in conversational linguistics of the process of the subject adopting the reference terms of their interlocutor. In practice, it acts as a mechanism of the cooperative principle in which both parties to the conversation employ lexical entrainment as a progressive system to develop \"conceptual pacts\" (a working temporary conversational terminology) to ensure maximum clarity of reference in the communication between the parties; this process is necessary to overcome the ambiguity inherent in the multitude of synonyms that exist in language. \n\nLexical entrainment arises by two cooperative mechanisms: \n\nOnce lexical entrainment has come to determine the phrasing for a referent, both parties will use that terminology for the referent for the duration, even if it proceeds to violate the Gricean maxim of quantity. For example, if one wants to refer to a brown loafer out of a set of shoes that consist of: the loafer, a sneaker, and a high-heeled shoe, they will not use \"the shoe\" to describe the object as this phrasing does not unambiguously describe one item in the set under consideration. They will also not call the object \"the brown loafer\" which would violate Grice's maxim of quantity. The speaker will settle on using the term \"the loafer\" as it is just informative enough without giving too much information. \n\nAnother important factor is lexical availability; the ease of conceptualizing a referent in a certain way and then retrieving and producing a label for it. For many objects the most available labels are basic nouns; for example, the word \"dog\". Instead of saying \"animal\" or \"husky\" for the referent, most subjects will default to \"dog\".\nIf in a set of objects one is to refer to either a husky, a table, and a poster, people are still most likely to use the word \"dog.\" This is technically a violation of Grice's maxim of quantity, as using the term \"animal\" is ideal.\n\nLexical entrainment has applications in natural language processing in computers as well as human–human interaction. Currently, the adaptability of computers to modify their referencing to the terms of their human interlocutor is limited, so the entrainment adaptation falls to the human operator; this phenomenon is readily demonstrated in Brennan's 1996 experiment.\n"}
{"id": "274078", "url": "https://en.wikipedia.org/wiki?curid=274078", "title": "Marginal demand", "text": "Marginal demand\n\nMarginal demand in economics is the change in demand for a product or service in response to a specific change in its price.\n\nNormally, as prices for goods or service rise, demand falls, and conversely, as prices for goods or services fall, demand rises.\n\nA product or service where price changes cause a relatively big change in demand is said to have \"elastic\" demand. A product or service where price changes cause a relatively small change in demand is said to have \"inelastic\" demand. See Elasticity of demand.\n\n"}
{"id": "762043", "url": "https://en.wikipedia.org/wiki?curid=762043", "title": "Marginal product", "text": "Marginal product\n\nIn economics and in particular neoclassical economics, the marginal product or marginal physical productivity of an input (factor of production) is the change in output resulting from employing one more unit of a particular input (for instance, the change in output when a firm's labor is increased from five to six units), assuming that the quantities of other inputs are kept constant.\n\nThe marginal product of a given input can be expressed \nas:\n\nwhere formula_2 is the change in the firm's use of the input (conventionally a one-unit change) and formula_3 is the change in quantity of output produced (resulting from the change in the input). Note that the quantity formula_4 of the \"product\" is typically defined ignoring external costs and benefits.\n\nIf the output and the input are infinitely divisible, so the marginal \"units\" are infinitesimal, the marginal product is the mathematical derivative of the production function with respect to that input. Suppose a firm's output \"Y\" is given by the production function:\n\nwhere \"K\" and \"L\" are inputs to production (say, capital and labor). Then the marginal product of capital (\"MPK\") and marginal product of labor (\"MPL\") are given by:\n\nIn the \"law\" of diminishing marginal returns, the marginal product initially increases when more of an input (say labor) is employed, keeping the other input (say capital) constant. Here, labor is the variable input and capital is the fixed input (in a hypothetical two-inputs model). As more and more of variable input (labor) is employed, marginal product starts to fall. Finally, after a certain point, the marginal product becomes negative, implying that the additional unit of labor has \"decreased\" the output, rather than increasing it. The reason behind this is the diminishing marginal productivity of labor.\n\nThe marginal product of labor is the slope of the total product curve, which is the production function plotted against labor usage for a fixed level of usage of the capital input.\n\nIn the neoclassical theory of competitive markets, the marginal product of labor equals the real wage. In aggregate models of perfect competition, in which a single good is produced and that good is used both in consumption and as a capital good, the marginal product of capital equals its rate of return. As was shown in the Cambridge capital controversy, this proposition about the marginal product of capital cannot generally be sustained in multi-commodity models in which capital and consumption goods are distinguished.\n\nRelationship of marginal product (MPP) with the total product (TPP)\n\nThe relationship can be explained in three phases-\n(1) Initially, as the quantity of variable input is increased, TPP rises at an increasing rate. In this phase, MPP also rises.\n(2) As more and more quantities of the variable inputs are employed, TPP increases at a diminishing rate. In this phase, MPP starts to fall.\n(3) When the TPP reaches its maximum, MPP is zero. Beyond this point, TPP starts to fall and MPP becomes negative.\n\n"}
{"id": "15628625", "url": "https://en.wikipedia.org/wiki?curid=15628625", "title": "Marginal use", "text": "Marginal use\n\nAs defined by the Austrian School of economics the marginal use of a good or service is the specific use to which an agent would put a given increase, or the specific use of the good or service that would be abandoned in response to a given decrease. The useful\"ness\" of the marginal use thus corresponds to the marginal utility of the good or service.\n\nOn the assumption that an agent is economically rational, each increase would be put to the specific, feasible, previously unrealized use of greatest priority, and each decrease would result in abandonment of the use of lowest priority amongst the uses to which the good or service had been put. And, in the absence of a complementarity across uses, the “law” of diminishing marginal utility will obtain.\n\nThe Austrian School of economics explicitly arrives at its conception of marginal utility as the utility of the marginal use, and “Grenznutzen” (the Austrian School term from which “marginal utility” was originally derived in translation) literally means \"border-use\"; other schools usually do not make an explicit connection.\n\n"}
{"id": "50154587", "url": "https://en.wikipedia.org/wiki?curid=50154587", "title": "Menthor Editor", "text": "Menthor Editor\n\nMenthor Editor is a free ontology engineering tool for dealing with OntoUML models. It also includes OntoUML syntax validation, Alloy simulation, Anti-Pattern verification, and MDA transformations from OntoUML to OWL, SBVR and Natural Language (Brazilian Portuguese).\n\nMenthor Editor emerged from OLED. OLED was developed at the Ontology & Conceptual Modeling Research Group (NEMO) located at the Federal University of Espírito Santo (UFES) in Vitória city, state of Espírito Santo, Brazil\n\nMenthor Editor is being developed by Menthor using Java. Menthor Editor is available in English and it is a multiplaform software, i.e., it is compatible with Windows, Linux and OS X.\n\n"}
{"id": "44017873", "url": "https://en.wikipedia.org/wiki?curid=44017873", "title": "Negative energy", "text": "Negative energy\n\nNegative energy is a concept used in physics to explain the nature of certain fields, including the gravitational field and various quantum field effects.\n\nIn more speculative theories, negative energy is involved in wormholes which may allow for time travel and warp drives for faster-than-light space travel.\n\nThe strength of the gravitational attraction between two objects represents the amount of gravitational energy in the field which attracts them towards each other. When they are infinitely far apart, the gravitational attraction and hence energy approach zero. As two such massive objects move towards each other, the motion accelerates under gravity causing an increase in the positive kinetic energy of the system. At the same time, the gravitational attraction - and hence energy - also increase in magnitude, but the law of energy conservation requires that the net energy of the system not change. This issue can only be resolved if the change in gravitational energy is negative, thus cancelling out the positive change in kinetic energy. Since the gravitational energy is getting stronger, this decrease can only mean that it is negative.\n\nA universe in which positive energy dominates will eventually collapse in a \"big crunch\", while an \"open\" universe in which negative energy dominates will either expand indefinitely or eventually disintegrate in a \"big rip\". In the zero-energy universe model (\"flat\" or \"Euclidean\"), the total amount of energy in the universe is exactly zero: its amount of positive energy in the form of matter is exactly cancelled out by its negative energy in the form of gravity.\n\nNegative energies and negative energy density are consistent with quantum field theory.\n\nIn quantum theory, the uncertainty principle allows the vacuum of space to be filled with virtual particle-antiparticle pairs which appear spontaneously and exist for only a short time before, typically, annihilating themselves again. Some of these virtual particles can have negative energy. Their behaviour plays a role in several important phenomena, as described below.\n\nIn the Casimir effect, two flat plates placed very close together restrict the wavelengths of quanta which can exist between them. This in turn restricts the types and hence number and density of virtual particle pairs which can form in the intervening vacuum and can result in a negative energy density. This causes an attractive force between the plates, which has been measured.\n\nVirtual particles with negative energy can exist for a short period. This phenomenon is a part of the mechanism involved in Hawking radiation by which black holes evaporate.\n\nIt is possible to arrange multiple beams of laser light such that destructive quantum interference suppresses the vacuum fluctuations. Such a squeezed vacuum state involves negative energy. The repetitive waveform of light leads to alternating regions of positive and negative energy.\n\nAccording to the theory of the Dirac sea, developed by Paul Dirac in 1930, the vacuum of space is full of negative energy. This theory was developed to explain the anomaly of negative-energy quantum states predicted by the Dirac equation.\n\nThe Dirac sea theory correctly predicted the existence of antimatter two years prior to the discovery of the positron in 1932 by Carl Anderson. However, the Dirac sea theory treats antimatter as a hole where there is the absence of a particle rather than as a real particle. Quantum field theory (QFT), developed in the 1930s, deals with antimatter in a way that treats antimatter as made of real particles rather than the absence of particles, and treats a vacuum as being empty of particles rather than full of negative-energy particles like in the Dirac sea theory.\n\nQuantum field theory has displaced the Dirac sea theory as a more popular explanation of these aspects of physics. Both the Dirac sea theory and quantum field theory are equivalent by means of a Bogoliubov transformation, so the Dirac sea can be viewed as an alternative formulation of quantum field theory, and is thus consistent with it.\n\nNegative energy appears in the speculative theory of wormholes, where it is needed to keep the wormhole open. A wormhole directly connects two locations which may be separated arbitrarily far apart in both space and time, and in principle allows near-instantaneous travel between them.\n\nA theoretical principle for a faster-than-light (FTL) warp drive for spaceships has been suggested, involving negative energy. The Alcubierre drive comprises a solution to Einstein's equations of general relativity, in which a bubble of spacetime is moved rapidly by expanding space behind it and shrinking space in front of it.\n\n\n"}
{"id": "1762868", "url": "https://en.wikipedia.org/wiki?curid=1762868", "title": "Negative luminescence", "text": "Negative luminescence\n\nNegative luminescence is a physical phenomenon by which an electronic device emits less thermal radiation when an electric current is passed through it than it does in thermal equilibrium (current off). When viewed by a thermal camera, an operating negative luminescent device looks colder than its environment.\n\nNegative luminescence is most readily observed in semiconductors. Incoming infrared radiation is absorbed in the material by the creation of an electron–hole pair. An electric field is used to remove the electrons and holes from the region before they have a chance to recombine and re-emit thermal radiation. This effect occurs most efficiently in regions of low charge carrier density.\n\nNegative luminescence has also been observed in semiconductors in orthogonal electric and magnetic fields. In this case, the junction of a diode is not necessary and the effect can be observed in bulk material. A term that has been applied to this type of negative luminescence is \"galvanomagnetic luminescence\".\n\nNegative luminescence might appear to be a violation of Kirchhoff's law of thermal radiation. This is not true, as the law only applies in thermal equilibrium.\n\nAnother term that has been used to describe negative luminescent devices is \"Emissivity switch\", as an electric current changes the effective emissivity.\n\nThis effect was first seen by Russian physicists in the 1960s in A.F.Ioffe Physicotechnical Institute, Leningrad, Russia. Subsequently, it was studied in semiconductors such as indium antimonide (InSb), germanium (Ge) and indium arsenide (InAs) by workers in West Germany, Ukraine (Institute of Semiconductor Physics, Kiev), Japan (Chiba University) and the United States. It was first observed in the mid-infrared (3-5 µm wavelength) in the more convenient diode structures in InSb heterostructure diodes by workers at the Defence Research Agency, Great Malvern, UK (now QinetiQ). These British workers later demonstrated LWIR band (8-12 µm) negative luminescence using mercury cadmium telluride diodes.\n\nLater the Naval Research Laboratory, Washington DC, started work on negative luminescence in mercury cadmium telluride (HgCdTe). The phenomenon has since been observed by several university groups around the world.\n\n\n\n"}
{"id": "6476061", "url": "https://en.wikipedia.org/wiki?curid=6476061", "title": "Negative pledge", "text": "Negative pledge\n\nNegative pledge is a provision in a contract which prohibits a party to the contract from creating any security interests over certain property specified in the provision.\n\nNegative pledges often appear in security documents, where they operate to prohibit the person who is granting the security interest from creating any other security interests over the same property, which might compete with (or rank \"pari passu\" with) the security of the first secured creditor under the security document in which the negative pledge appears.\n\nIn Australia, negative pledge lending took off after a substantial deal by Pioneer Concrete in 1978. It was a new way of lending, which allowed the banks to lend to corporations, something previously the domain of life insurers.\n\nNegative pledge clauses are almost universal in modern unsecured commercial loan documents. The purpose is to ensure that a borrower, having taken out an unsecured loan, cannot subsequently take out another loan with a different lender, securing the subsequent loan on the specified assets. If the borrower could do this, the original lender would be disadvantaged because the subsequent lender would have first call on the assets in an event of default.\n\n"}
{"id": "577248", "url": "https://en.wikipedia.org/wiki?curid=577248", "title": "New riddle of induction", "text": "New riddle of induction\n\nGrue and bleen are examples of logical predicates coined by Nelson Goodman in \"Fact, Fiction, and Forecast\" to illustrate the \"new riddle of induction\". These predicates are unusual because their application is time-dependent; many have tried to solve the new riddle on those terms, but Hilary Putnam and others have argued such time-dependency depends on the language adopted, and in some languages it is equally true for natural-sounding predicates such as \"green.\" For Goodman they illustrate the problem of projectible predicates and ultimately, which empirical generalizations are law-like and which are not.\nGoodman's construction and use of \"grue\" and \"bleen\" illustrates how philosophers use simple examples in conceptual analysis.\n\nGoodman defined grue relative to an arbitrary but fixed time \"t\" as follows: An object is grue if and only if it is observed before \"t\" and is green, or else is not so observed and is blue. An object is bleen if and only if it is observed before \"t\" and is blue, or else is not so observed and is green.\n\nTo understand the problem Goodman posed, it is helpful to imagine some arbitrary future time \"t\", say January 1, 10. For all green things we observe up to time \"t\", such as emeralds and well-watered grass, both the predicates \"green\" and \"grue\" apply. Likewise for all blue things we observe up to time \"t\", such as bluebirds or blue flowers, both the predicates \"blue\" and \"bleen\" apply. On January 2, 10, however, emeralds and well-watered grass are \"bleen\" and bluebirds or blue flowers are \"grue\". Clearly, the predicates \"grue\" and \"bleen\" are not the kinds of predicates we use in everyday life or in science, but the problem is that they apply in just the same way as the predicates \"green\" and \"blue\" up until some future time \"t\". From our current perspective (i.e., before time \"t\"), how can we say which predicates are more projectible into the future: \"green\" and \"blue\" or \"grue\" and \"bleen\"?\n\nIn this section, Goodman's new riddle of induction is outlined in order to set the context for his introduction of the predicates \"grue\" and \"bleen\" and thereby illustrate their philosophical importance.\n\nGoodman poses Hume's problem of induction as a problem of the validity of the predictions we make. Since predictions are about what has yet to be observed and because there is no necessary connection between what has been observed and what will be observed, what is the justification for the predictions we make? We cannot use deductive logic to infer predictions about future observations based on past observations because there are no valid rules of deductive logic for such inferences. Hume's answer was that our observations of one kind of event following another kind of event result in our minds forming habits of regularity (i.e., associating one kind of event with another kind). The predictions we make are then based on these regularities or habits of mind we have formed.\n\nGoodman takes Hume's answer to be a serious one. He rejects other philosophers' objection that Hume is merely explaining the origin of our predictions and not their justification. His view is that Hume has identified something deeper. To illustrate this, Goodman turns to the problem of justifying a system of rules of deduction. For Goodman, the validity of a deductive system is justified by its conformity to good deductive practice. The justification of rules of a deductive system depends on our judgements about whether to reject or accept specific deductive inferences. Thus, for Goodman, the problem of induction dissolves into the same problem as justifying a deductive system and while, according to Goodman, Hume was on the right track with habits of mind, the problem is more complex than Hume realized.\n\nIn the context of justifying rules of induction, this becomes the problem of confirmation of generalizations for Goodman. However, the confirmation is not a problem of justification but instead it is a problem of precisely defining how evidence confirms generalizations. It is with this turn that \"grue\" and \"bleen\" have their philosophical role in Goodman's view of induction.\n\nThe new riddle of induction, for Goodman, rests on our ability to distinguish \"lawlike\" from \"non-lawlike\" generalizations. \"Lawlike\" generalizations are capable of confirmation while \"non-lawlike\" generalizations are not. \"Lawlike\" generalizations are required for making predictions. Using examples from Goodman, the generalization that all copper conducts electricity is capable of confirmation by a particular piece of copper whereas the generalization that all men in a given room are third sons is not \"lawlike\" but accidental. The generalization that all copper conducts electricity is a basis for predicting that this piece of copper will conduct electricity. The generalization that all men in a given room are third sons, however, is not a basis for predicting that a given man in that room is a third son.\n\nWhat then makes some generalizations \"lawlike\" and others accidental? This, for Goodman, becomes a problem of determining which predicates are projectible (i.e., can be used in \"lawlike\" generalizations that serve as predictions) and which are not. Goodman argues that this is where the fundamental problem lies. This problem, known as \"Goodman's paradox\", is as follows. Consider the evidence that all emeralds examined thus far have been green. This leads us to conclude (by induction) that all future emeralds will be green. However, whether this prediction is \"lawlike\" or not depends on the predicates used in this prediction. Goodman observed that (assuming \"t\" has yet to pass) it is equally true that every emerald that has been observed is \"grue\". Thus, by the same evidence we can conclude that all future emeralds will be \"grue\". The new problem of induction becomes one of distinguishing projectible predicates such as \"green\" and \"blue\" from non-projectible predicates such as \"grue\" and \"bleen\".\n\nHume, Goodman argues, missed this problem. We do not, by habit, form generalizations from all associations of events we have observed but only some of them. All past observed emeralds were green, and we formed a habit of thinking the next emerald will be green, but they were equally grue, and we do not form habits concerning grueness. \"Lawlike\" predictions (or projections) ultimately are distinguishable by the predicates we use. Goodman's solution is to argue that \"lawlike\" predictions are based on projectible predicates such as \"green\" and \"blue\" and not on non-projectible predicates such as \"grue\" and \"bleen\" and what makes predicates projectible is their \"entrenchment\", which depends on their successful past projections. Thus, \"grue\" and \"bleen\" function in Goodman's arguments to both illustrate the new riddle of induction and to illustrate the distinction between projectible and non-projectible predicates via their relative entrenchment.\n\nThe most obvious response is to point to the artificially disjunctive definition of grue. The notion of predicate \"entrenchment\" is not required. Goodman, however, noted that this move will not work. If we take \"grue\" and \"bleen\" as primitive predicates, we can define green as \"\"grue\" if first observed before \"t\" and \"bleen\" otherwise\", and likewise for blue. To deny the acceptability of this disjunctive definition of green would be to beg the question.\n\nAnother proposed resolution of the paradox (which Goodman addresses and rejects) that does not require predicate \"entrenchment\" is that \"\"x\" is grue\" is not solely a predicate of \"x\", but of \"x\" and a time \"t\"—we can know that an object is green without knowing the time \"t\", but we cannot know that it is grue. If this is the case, we should not expect \"\"x\" is grue\" to remain true when the time changes. However, one might ask why \"\"x\" is green\" is \"not\" considered a predicate of a particular time \"t\"—the more common definition of \"green\" does not require any mention of a time \"t\", but the definition \"grue\" does. As we have just seen, this response also begs the question because \"blue\" can be defined in terms of \"grue\" and \"bleen\", which explicitly refer to time.\n\nRichard Swinburne gets past the objection that green may be redefined in terms of \"grue\" and \"bleen\" by making a distinction based on how we test for the applicability of a predicate in a particular case. He distinguishes between qualitative and locational predicates. Qualitative predicates, like green, \"can\" be assessed without knowing the spatial or temporal relation of \"x\" to a particular time, place or event. Locational predicates, like \"grue\", \"cannot\" be assessed without knowing the spatial or temporal relation of \"x\" to a particular time, place or event, in this case whether \"x\" is being observed before or after time \"t\". Although green can be given a definition in terms of the locational predicates \"grue\" and \"bleen\", this is irrelevant to the fact that green meets the criterion for being a qualitative predicate whereas \"grue\" is merely locational. He concludes that if some \"x\"'s under examination—like emeralds—satisfy both a qualitative and a locational predicate, but projecting these two predicates yields conflicting predictions, namely, whether emeralds examined after time \"t\" shall appear blue or green, we should project the qualitative predicate, in this case green.\n\nRudolf Carnap responded to Goodman's 1946 article. Carnap's approach to inductive logic is based on the notion of \"degree of confirmation\" \"c\"(\"h\",\"e\") of a given hypothesis \"h\" by a given evidence \"e\". Both \"h\" and \"e\" are logical formulas expressed in a simple language \"L\" which allows for\nThe universe of discourse consists of denumerably many individuals, each of which is designated by its own constant symbol; such individuals are meant to be regarded as positions (\"like space-time points in our actual world\") rather than extended physical bodies. A state description is a (usually infinite) conjunction containing every possible ground atomic sentence, either negated or unnegated; such a conjunction describes a possible state of the whole universe. Carnap requires the following semantic properties:\nCarnap distinguishes three kinds of properties:\nTo illuminate this taxonomy, let \"x\" be a variable and \"a\" a constant symbol; then an example of 1. could be \"\"x\" is blue or \"x\" is non-warm\", an example of 2. \"\"x\" = \"a\", and an example of 3. \"x\" is red and not \"x\" = \"a\"\".\n\nBased on his theory of inductive logic sketched above, Carnap formalizes Goodman's notion of projectibility of a property \"W\" as follows: the higher the relative frequency of \"W\" in an observed sample, the higher is the probability that a non-observed individual has the property \"W\". Carnap suggests \"as a tentative answer\" to Goodman, that all purely qualitative properties are projectible, all purely positional properties are non-projectible, and mixed properties require further investigation.\n\nWillard Van Orman Quine discusses an approach to consider only \"natural kinds\" as projectible predicates.\nHe first relates Goodman's grue paradox to Hempel's raven paradox by defining two predicates \"F\" and \"G\" to be (simultaneously) projectible if all their shared instances count toward confirmation of the claim \"each \"F\" is a \"G\"\". Then Hempel's paradox just shows that the complements of projectible predicates (such as \"is a raven\", and \"is black\") need not be projectible, while Goodman's paradox shows that \"is green\" is projectible, but \"is grue\" is not.\n\nNext, Quine reduces projectibility to the subjective notion of \"similarity\". Two green emeralds are usually considered more similar than two grue ones if only one of them is green. Observing a green emerald makes us expect a similar observation (i.e., a green emerald) next time. Green emeralds are a \"natural kind\", but grue emeralds are not. Quine investigates \"the dubious scientific standing of a general notion of similarity, or of kind\". Both are basic to thought and language, like the logical notions of e.g. identity, negation, disjunction. However, it remains unclear how to relate the logical notions to \"similarity\" or \"kind\"; Quine therefore tries to relate at least the latter two notions to each other.\n\nRelation between similarity and kind\n\nAssuming finitely many \"kinds\" only, the notion of \"similarity\" can be defined by that of \"kind\": an object \"A\" is more similar to \"B\" than to \"C\" if \"A\" and \"B\" belong jointly to more kinds than \"A\" and \"C\" do.\n\nVice versa, it remains again unclear how to define \"kind\" by \"similarity\". Defining e.g. the kind of red things as the set of all things that are more similar to a fixed \"paradigmatical\" red object than this is to another fixed \"foil\" non-red object (cf. left picture) isn't satisfactory, since the degree of overall similarity, including e.g. shape, weight, will afford little evidence of degree of redness. (In the picture, the yellow paprika might be considered more similar to the red one than the orange.)\n\nAn alternative approach inspired by Carnap defines a natural kind to be a set whose members are more similar to each other than each non-member is to at least one member. \nHowever, Goodman argued, that this definition would make the set of all red round things, red wooden things, and round wooden things (cf. right picture) meet the proposed definition of a natural kind, while \"surely it is not what anyone means by a kind\".\n\nWhile neither of the notions of similarity and kind can be defined by the other, they at least vary together: if \"A\" is reassessed to be more similar to \"C\" than to \"B\" rather than the other way around, the assignment of \"A\", \"B\", \"C\" to kinds will be permuted correspondingly; and conversely.\n\nBasic importance of similarity and kind\n\nIn language, every general term owes its generality to some resemblance of the things referred to. Learning to use a word depends on a double resemblance, viz. between the present and past circumstances in which the word was used, and between the present and past phonetic utterances of the word.\n\nEvery reasonable expectation depends on resemblance of circumstances, together with our tendency to expect similar causes to have similar effects. This includes any scientific experiment, since it can be reproduced only under similar, but not under completely identical, circumstances. Already Heraclitus' famous saying \"No man ever steps in the same river twice\" highlighted the distinction between similar and identical circumstances.\n\nGenesis of similarity and kind\n\nIn a behavioral sense, humans and other animals have an innate standard of similarity. It is part of our animal birthright, and characteristically animal in its lack of intellectual status, e.g. its alieness to mathematics and logic, cf. bird example.\n\nInduction itself is essentially animal expectation or habit formation.\nOstensive learning\nis a case of induction, and a curiously comfortable one, since each man's spacing of qualities and kind is enough like his neighbor's.\nIn contrast, the \"brute irrationality of our sense of similarity\" offers little reason to expect it being somehow in tune with the unanimated nature, which we never made.\nWhy inductively obtained theories about it should be trusted is the perennial philosophical problem of induction. Quine, following Watanabe,\nsuggests Darwin's theory as an explanation: if people's innate spacing of qualities is a gene-linked trait, then the spacing that has made for the most successful inductions will have tended to predominate through natural selection.\nHowever, this cannot account for the human ability to dynamically refine one's spacing of qualities in the course of getting acquainted with a new area.\n\nIn his book \"Wittgenstein on Rules and Private Language\", Saul Kripke proposed a related argument that leads to skepticism about meaning rather than skepticism about induction, as part of his personal interpretation (nicknamed \"Kripkenstein\" by some) of the private language argument. He proposed a new form of addition, which he called \"quus\", which is identical with \"+\" in all cases except those in which either of the numbers added are equal to or greater than 57; in which case the answer would be 5, i.e.:\n\nHe then asks how, given certain obvious circumstances, anyone could know that previously when I thought I had meant \"+\", I had not actually meant \"quus\". Kripke then argues for an interpretation of Wittgenstein as holding that the meanings of words are not individually contained mental entities.\n\n\n"}
{"id": "39098", "url": "https://en.wikipedia.org/wiki?curid=39098", "title": "Physical law", "text": "Physical law\n\nA physical law or a law of physics is a statement \"inferred from particular facts, applicable to a defined group or class of phenomena, and expressible by the statement that a particular phenomenon always occurs if certain conditions be present.\" Physical laws are typically conclusions based on repeated scientific experiments and observations over many years and which have become accepted universally within the scientific community. The production of a summary description of our environment in the form of such laws is a fundamental aim of science. These terms are not used the same way by all authors.\n\nThe distinction between natural law in the political-legal sense and law of nature or physical law in the scientific sense is a modern one, both concepts being equally derived from \"physis\", the Greek word (translated into Latin as \"natura\") for \"nature\".\n\nSeveral general properties of physical laws have been identified. Physical laws are:\n\n\nSome of the more famous laws of nature are found in Isaac Newton's theories of (now) classical mechanics, presented in his \"Philosophiae Naturalis Principia Mathematica\", and in Albert Einstein's theory of relativity. Other examples of laws of nature include Boyle's law of gases, conservation laws, the four laws of thermodynamics, etc.\n\nMany scientific laws are couched in mathematical terms (e.g. Newton's Second law \"F\" = , or the uncertainty principle, or the principle of least action, or causality). While these scientific laws explain what our senses perceive, they are still empirical, and so are not \"mathematical\" laws. (Mathematical laws can be proved purely by mathematics and not by scientific experiment.)\n\nOther laws reflect mathematical symmetries found in Nature (say, Pauli exclusion principle reflects identity of electrons, conservation laws reflect homogeneity of space, time, Lorentz transformations reflect rotational symmetry of space–time). Laws are constantly being checked experimentally to higher and higher degrees of precision. This is one of the main goals of science. Just because laws have never been observed to be violated does not preclude testing them at increased accuracy or in new kinds of conditions to confirm whether they continue to hold, or whether they break, and what can be discovered in the process. It is always possible for laws to be invalidated or proven to have limitations, by repeatable experimental evidence, should any be observed.\n\nWell-established laws have indeed been invalidated in some special cases, but the new formulations created to explain the discrepancies can be said to generalize upon, rather than overthrow, the originals. That is, the invalidated laws have been found to be only close approximations (see below), to which other terms or factors must be added to cover previously unaccounted-for conditions, e.g. very large or very small scales of time or space, enormous speeds or masses, etc. Thus, rather than unchanging knowledge, physical laws are better viewed as a series of improving and more precise generalizations.\n\nMany fundamental physical laws are mathematical consequences of various symmetries of space, time, or other aspects of nature. Specifically, Noether's theorem connects some conservation laws to certain symmetries. For example, conservation of energy is a consequence of the shift symmetry of time (no moment of time is different from any other), while conservation of momentum is a consequence of the symmetry (homogeneity) of space (no place in space is special, or different than any other). The indistinguishability of all particles of each fundamental type (say, electrons, or photons) results in the Dirac and Bose quantum statistics which in turn result in the Pauli exclusion principle for fermions and in Bose–Einstein condensation for bosons. The rotational symmetry between time and space coordinate axes (when one is taken as imaginary, another as real) results in Lorentz transformations which in turn result in special relativity theory. Symmetry between inertial and gravitational mass results in general relativity.\n\nThe inverse square law of interactions mediated by massless bosons is the mathematical consequence of the 3-dimensionality of space.\n\nOne strategy in the search for the most fundamental laws of nature is to search for the most general mathematical symmetry group that can be applied to the fundamental interactions.\n\nSome laws are only approximations of other more general laws, and are good approximations with a restricted domain of applicability. For example, Newtonian dynamics (which is based on Galilean transformations) is the low-speed limit of special relativity (since the Galilean transformation is the low-speed approximation to the Lorentz transformation). Similarly, the Newtonian gravitation law is a low-mass approximation of general relativity, and Coulomb's law is an approximation to Quantum Electrodynamics at large distances (compared to the range of weak interactions). In such cases it is common to use the simpler, approximate versions of the laws, instead of the more accurate general laws.\n\nCompared to pre-modern accounts of causality, laws of nature fill the role played by divine causality on the one hand, and accounts such as Plato's theory of forms on the other.\n\nThe observation that there are underlying regularities in nature dates from prehistoric times, since the recognition of cause-and-effect relationships is an implicit recognition that there are laws of nature. The recognition of such regularities as independent scientific laws \"per se\", though, was limited by their entanglement in animism, and by the attribution of many effects that do not have readily obvious causes—such as meteorological, astronomical and biological phenomena—to the actions of various gods, spirits, supernatural beings, etc. Observation and speculation about nature were intimately bound up with metaphysics and morality.\n\nIn Europe, systematic theorizing about nature (\"physis\") began with the early Greek philosophers and scientists and continued into the Hellenistic and Roman imperial periods, during which times the intellectual influence of Roman law increasingly became paramount.The formula \"law of nature\" first appears as \"a live metaphor\" favored by Latin poets Lucretius, Virgil, Ovid, Manilius, in time gaining a firm theoretical presence in the prose treatises of Seneca and Pliny. Why this Roman origin? According to [historian and classicist Daryn] Lehoux's persuasive narrative, the idea was made possible by the pivotal role of codified law and forensic argument in Roman life and culture.\n\nFor the Romans . . . the place par excellence where ethics, law, nature, religion and politics overlap is the law court. When we read Seneca's \"Natural Questions\", and watch again and again just how he applies standards of evidence, witness evaluation, argument and proof, we can recognize that we are reading one of the great Roman rhetoricians of the age, thoroughly immersed in forensic method. And not Seneca alone. Legal models of scientific judgment turn up all over the place, and for example prove equally integral to Ptolemy's approach to verification, where the mind is assigned the role of magistrate, the senses that of disclosure of evidence, and dialectical reason that of the law itself.\n\nThe precise formulation of what are now recognized as modern and valid statements of the laws of nature dates from the 17th century in Europe, with the beginning of accurate experimentation and development of advanced forms of mathematics. During this period, natural philosophers such as Isaac Newton were influenced by a religious view which held that God had instituted absolute, universal and immutable physical laws. In chapter 7 of \"The World\", René Descartes described \"nature\" as matter itself, unchanging as created by God, thus changes in parts \"are to be attributed to nature. The rules according to which these changes take place I call the 'laws of nature'.\" The modern scientific method which took shape at this time (with Francis Bacon and Galileo) aimed at total separation of science from theology, with minimal speculation about metaphysics and ethics. Natural law in the political sense, conceived as universal (i.e., divorced from sectarian religion and accidents of place), was also elaborated in this period (by Grotius, Spinoza, and Hobbes, to name a few).\n\nSome mathematical theorems and axioms are referred to as laws because they provide logical foundation to empirical laws.\n\nExamples of other observed phenomena sometimes described as laws include the Titius–Bode law of planetary positions, Zipf's law of linguistics, Moore's law of technological growth. Many of these laws fall within the scope of uncomfortable science. Other laws are pragmatic and observational, such as the law of unintended consequences. By analogy, principles in other fields of study are sometimes loosely referred to as \"laws\". These include Occam's razor as a principle of philosophy and the Pareto principle of economics.\n\n\n\n"}
{"id": "11585926", "url": "https://en.wikipedia.org/wiki?curid=11585926", "title": "Principle of humanity", "text": "Principle of humanity\n\nIn philosophy and rhetoric, the principle of humanity states that when interpreting another speaker we must assume that his or her beliefs and desires are connected to each other and to reality in some way, and attribute to him or her \"the propositional attitudes one supposes one would have oneself in those circumstances\". The principle of humanity was named by Richard Grandy (then an assistant professor of philosophy at Princeton University) who first expressed it in 1973.\n\n"}
{"id": "8924176", "url": "https://en.wikipedia.org/wiki?curid=8924176", "title": "Principle of nonvacuous contrast", "text": "Principle of nonvacuous contrast\n\nThe principle of nonvacuous contrast is a logical or methodological principle which requires that a genuine predicate never refer to everything, or to nothing, within its universe of discourse.\n"}
{"id": "21647661", "url": "https://en.wikipedia.org/wiki?curid=21647661", "title": "Self model", "text": "Self model\n\nThe self-model is the central concept in the theory of consciousness called the self-model theory of subjectivity (SMT). This concept comprises experiences of ownership, of first person perspective, and of a long-term unity of beliefs and attitudes. These features are instantiated in the prefrontal cortex. This theory is an interdisciplinary approach to understanding and explaining the phenomenology of consciousness and the self. This theory has two core contents, the phenomenal self-model (PSM) and the phenomenal model of the intentionality relation (PMIR). Thomas Metzinger advanced the theory in his 1993 book \"Subjekt und Selbstmodell\" (Subject and self-model).\n\nThe PSM is an entity that “actually exists, not only as a distinct theoretical entity but something that will be empirically discovered in the future- for instance, as a specific stage of the global neural dynamics in the human brain”. Involved in the PSM are three phenomenal properties that must occur in order to explain the concept of the self. The first is mineness, “a higher order property of particular forms of phenomenal content,” or the idea of ownership. The second is perspectivalness, which is “a global, structural property of phenomenal space as a whole”. More simply, it is what is commonly referred to as the ecological self, the immovable center of perception. The third phenomenal property is selfhood, which is “the phenomenal target property” or the idea of the self over time. It is the property of phenomenal selfhood that plays the most important role in creating the fictional self and the first person perspective. Metzinger defines the first person perspective as the “existence of single coherent and temporally stable model of reality which is representationally centered around or on a single coherent and temporally stable phenomenal subject”. The first-person perspective can be non-conceptual and is autonomously active due to the constant reception of perceptual information by the brain. The brain, specifically the brainstem and hypothalamus, processes this information into representational content, namely linguistic reflections. The PSM then uses this representational content to attribute phenomenal states to our perceived objects and ourselves. We are thus what Metzinger calls naïve realists, who believe we are perceiving reality directly when in actuality we are only perceiving representations of reality. The data structures and transport mechanisms of the data are “transparent” so that we can introspect on our representations of perceptions, but cannot introspect on the data or mechanisms themselves. These systemic representational experiences are then connected by subjective experience to generate the phenomenal property of selfhood. Subjective experience is the result of the Phenomenal Model of Intentionality Relationship (PMIR). The PMIR is a “conscious mental model, and its content is an ongoing, episodic subject-object relation”. The model is a result of the combination of our unique set of sensory receptors that acquire input, our unique set of experiences that shape connections within the brain, and our unique positions in space that give our perception perspectivalness.\n\nThe prefrontal cortex is implicated in all the functions of the human self model. The following functions all require communication with the prefrontal cortex; agency and association areas of the cortex; spatial perspectivity and the parietal lobes, unity and the temporal lobes.\n\nDisorders of the self model are implicated in several disorders including schizophrenia, autism, and depersonalization. According to this theory, long-term unity is impaired in autism, similar to theory of mind deficits and weak central coherence theory. Individuals with autism are thought to be impaired in assigning mental states to other people, an ability that probably codevelops with long-term unity of self. Weak central coherence, that is, the inability to assemble information into a cohesive whole, reflects the same problems with creating a unified sense of self and benific sense extreme in narcissism.\n\n"}
{"id": "40592503", "url": "https://en.wikipedia.org/wiki?curid=40592503", "title": "Sure-thing principle", "text": "Sure-thing principle\n\nIn decision theory, the sure-thing principle states that a decision maker who would take a certain action if he knew that event \"E\" has occurred, and also if he knew that the negation of \"E\" has occurred, should also take that same action if he knows nothing about \"E\".\n\nThe principle was coined by L.J. Savage:\nHe formulated the principle as a dominance principle, but it can also be framed probabilistically. Jeffrey and later Pearl showed that Savage's principle is only valid when the probability of the event considered (e.g., the winner of the election) is unaffected by the action (buying the property). Under such conditions, the sure-thing principle is a theorem in the \"do\"-calculus (see Bayes networks). Blyth constructed a counterexample to the sure-thing principle using sequential sampling in the context of Simpson's paradox, but this example violates the required action-independence provision.\n\nThe principle is closely related to independence of irrelevant alternatives, and equivalent under the axiom of truth (everything the agent knows is true). It is similarly targeted by the Ellsberg and Allais paradoxes, in which actual people's choices seem to violate this principle. \n"}
{"id": "213446", "url": "https://en.wikipedia.org/wiki?curid=213446", "title": "World view", "text": "World view\n\nA world view or worldview is the fundamental cognitive orientation of an individual or society encompassing the whole of the individual's or society's knowledge and point of view. A world view can include natural philosophy; fundamental, existential, and normative postulates; or themes, values, emotions, and ethics. The term is a calque of the German word Weltanschauung , composed of \"Welt\" ('world') and \"Anschauung\" ('view' or 'outlook'). The German word is also used in English.\n\nIt is a concept fundamental to German philosophy and epistemology and refers to a \"wide world perception\". Additionally, it refers to the framework of ideas and beliefs forming a global description through which an individual, group or culture watches and interprets the world and interacts with it.\n\nWorldview remains a confused and confusing concept in English, used very differently by linguists and sociologists. It is for this reason that James W. Underhill suggests five subcategories: world-perceiving, world-conceiving, cultural mindset, personal world, and perspective.\n\nWorldviews are often taken to operate at a conscious level, directly accessible to articulation and discussion, as opposed to existing at a deeper, pre-conscious level, such as the idea of \"ground\" in Gestalt psychology and media analysis. However, core worldview beliefs are often deeply rooted, and so are only rarely reflected on by individuals, and are brought to the surface only in moments of crises of faith.\n\nThe Prussian philologist Wilhelm von Humboldt (1767–1835) originated the idea that language and worldview are inextricable. Humboldt saw language as part of the creative adventure of mankind. Culture, language and linguistic communities developed simultaneously and could not do so without one another. In stark contrast to linguistic determinism, which invites us to consider language as a constraint, a framework or a prison house, Humboldt maintained that speech is inherently and implicitly creative. Human beings take their place in speech and continue to modify language and thought by their creative exchanges. \n\nEdward Sapir (1884–1939) also gives an account of the relationship between thinking and speaking in English.\n\nThe linguistic relativity hypothesis of Benjamin Lee Whorf (1897–1941) describes how the syntactic-semantic structure of a language becomes an underlying structure for the world view or \"Weltanschauung\" of a people through the organization of the causal perception of the world and the linguistic categorization of entities. As linguistic categorization emerges as a representation of worldview and causality, it further modifies social perception and thereby leads to a continual interaction between language and perception.\n\nWhorf's hypothesis became influential in the late 1940s, but declined in prominence after a decade. In the 1990s, new research gave further support for the linguistic relativity theory in the works of Stephen Levinson (1947–) and his team at the Max Planck institute for psycholinguistics at Nijmegen, Netherlands.\nThe theory has also gained attention through the work of Lera Boroditsky at Stanford University.\n\nOne of the most important concepts in cognitive philosophy and cognitive sciences is the German concept of \"Weltanschauung\". This expression has often been used to refer to the \"wide worldview\" or \"wide world perception\" of a people, family, or person. The \"Weltanschauung\" of a people originates from the unique world experience of a people, which they experience over several millennia.The language of a people reflects the \"Weltanschauung\" of that people in the form of its syntactic structures and untranslatable connotations and its denotations.\n\nThe term \"Weltanschauung\" is often wrongly attributed to Wilhelm von Humboldt, the founder of German ethnolinguistics. However, as Jürgen Trabant points out, and as James W. Underhill reminds us, Humboldt's key concept was \"Weltansicht\". \"Weltansicht\" was used by Humboldt to refer to the overarching conceptual and sensorial apprehension of reality shared by a linguistic community (Nation). On the other hand, \"Weltanschauung\", first used by Kant and later popularized by Hegel, was always used in German and later in English to refer more to philosophies, ideologies and cultural or religious perspectives, than to linguistic communities and their mode of apprehending reality.\n\nA worldview can be expressed as the \"fundamental cognitive, affective, and evaluative presuppositions a group of people make about the nature of things, and which they use to order their lives.\"\n\nIf it were possible to draw a map of the world on the basis of \"Weltanschauung\", it would probably be seen to cross political borders—\"Weltanschauung\" is the product of political borders and common experiences of a people from a geographical region, environmental-climatic conditions, the economic resources available, socio-cultural systems, and the language family. (The work of the population geneticist Luigi Luca Cavalli-Sforza aims to show the gene-linguistic co-evolution of people).\n\nIf the Sapir–Whorf hypothesis is correct, the worldview map of the world would be similar to the linguistic map of the world. However, it would also almost coincide with a map of the world drawn on the basis of music across people.\n\nAs natural language becomes manifestations of world perception, the literature of a people with common \"Weltanschauung\" emerges as holistic representations of the wide world perception of the people. Thus the extent and commonality between world folk-epics becomes a manifestation of the commonality and extent of a worldview.\n\nEpic poems are shared often by people across political borders and across generations. Examples of such epics include the Nibelungenlied of the Germanic people, the Iliad for the Ancient Greeks and Hellenized societies, the Silappadhikaram of the Tamil people, the Ramayana and Mahabharata of the Hindus, the Gilgamesh of the Mesopotamian-Sumerian civilization and the people of the Fertile Crescent at large, The Book of One Thousand and One Nights (Arabian nights) of the Arab world and the Sundiata epic of the Mandé people.\n\nA worldview, according to terror management theory (TMT), serves as a buffer against death anxiety. It is theorised that living up to the ideals of one's worldview provides a sense of self-esteem which provides a sense of transcending the limits of human life (e.g. literally, as in religious belief in immortality, symbolically, as in art works or children to live on after one's death, or in contributions to one's culture). Evidence in support of terror management theory includes a series of experiments by Jeff Schimel and colleagues in which a group of Canadians found to score highly on a measure of patriotism were asked to read an essay attacking the dominant Canadian worldview.\n\nUsing a test of death-thought accessibility (DTA), involving an ambiguous word completion test (e.g. \"COFF__\" could either be completed as either \"COFFEE\" or \"COFFIN\" or \"COFFER\"), participants who had read the essay attacking their worldview were found to have a significantly higher level of DTA than the control group, who read a similar essay attacking Australian cultural values. Mood was also measured following the worldview threat, to test whether the increase in death thoughts following worldview threat were due to other causes, for example, anger at the attack on one's cultural worldview. No significant changes on mood scales were found immediately following the worldview threat.\n\nTo test the generalisability of these findings to groups and worldviews other than those of nationalistic Canadians, Schimel \"et al\" conducted a similar experiment on a group of religious individuals whose worldview included that of creationism. Participants were asked to read an essay which argued in support of the theory of evolution, following which the same measure of DTA was taken as for the Canadian group. Religious participants with a creationist worldview were found to have a significantly higher level of death-thought accessibility than those of the control group.\n\nGoldenberg \"et al\" found that highlighting the similarities between humans and other animals increases death-thought accessibility, as does attention to the physical rather than meaningful qualities of sex.\n\nThe term World View denotes a comprehensive set of opinions, seen as an organic unity, about the world as the medium and exercise of human existence. World View serves as a framework for generating various dimensions of human perception and experience like knowledge, politics, economics, religion, culture, science and ethics. For example, worldview of causality as \"uni-directional\", \"cyclic\", or \"spiral\" generates a framework of the world that reflects these systems of causality.\n\nAn unidirectional view of causality is present in some monotheistic views of the world with a beginning and an end and a single great force with a single end (e.g., Christianity and Islam), while a cyclic worldview of causality is present in religious traditions which are cyclic and seasonal and wherein events and experiences recur in systematic patterns (e.g., Zoroastrianism, Mithraism and Hinduism). These worldviews of causality not only underlie religious traditions but also other aspects of thought like the purpose of history, political and economic theories, and systems like democracy, authoritarianism, anarchism, capitalism, socialism and communism.\n\nThe worldview of a linear and non-linear causality generates various related/conflicting disciplines and approaches in scientific thinking. The \"Weltanschauung\" of the temporal contiguity of act and event leads to underlying diversifications like \"determinism\" vs. \"free will\". A worldview of free will leads to disciplines that are governed by simple laws that remain constant and are static and empirical in scientific method, while a worldview of determinism generates disciplines that are governed with generative systems and rationalistic in scientific method.\n\nSome forms of philosophical naturalism and materialism reject the validity of entities inaccessible to natural science. They view the scientific method as the most reliable model for building an understanding of the world.\n\nNishida Kitaro wrote extensively on \"the Religious Worldview\" in exploring the philosophical significance of Eastern religions.\n\nAccording to Neo-Calvinist David Naugle's \"World view: The History of a Concept\", \"Conceiving of Christianity as a worldview has been one of the most significant developments in the recent history of the church.\"\n\nThe Christian thinker James W. Sire defines a worldview as \"a commitment, a fundamental orientation of the heart, that can be expressed as a story or in a set of presuppositions (assumptions which may be true, partially true, or entirely false) which we hold (consciously or subconsciously, consistently or inconsistently) about the basic construction of reality, and that provides the foundation on which we live and move and have our being.\" He suggests that \"we should all think in terms of worldviews, that is, with a consciousness not only of our own way of thought but also that of other people, so that we can first understand and then genuinely communicate with others in our pluralistic society.\"\n\nThe commitment mentioned by James W. Sire can be extended further. The worldview increases the commitment to serve the world. With the change of a person's view towards the world, he/she can be motivated to serve the world. This serving attitude has been illustrated by Tareq M Zayed as the 'Emancipatory Worldview' in his writing \"History of emancipatory worldview of Muslim learners\".\n\nThe question mentioned above - on whether the super-smart machines, that is, any \"superintelligences\", as expected by some, could have worldviews - is interesting in this context and this would influence human worldviews. \n\nThe philosophical importance of worldviews became increasingly clear during the 20th century for a number of reasons, such as increasing contact between cultures, and the failure of some aspects of the Enlightenment project, such as the rationalist project of attaining all truth by reason alone. Mathematical logic showed that fundamental choices of axioms were essential in deductive reasoning and that, even having chosen axioms not everything that was true in a given logical system could be proven. Some philosophers believe the problems extend to \"the inconsistencies and failures which plagued the Enlightenment attempt to identify universal moral and rational principles\"; although Enlightenment principles such as universal suffrage and the universal declaration of human rights are accepted, if not taken for granted, by many.\n\nThe theory of relativity offers a Weltanschauung that is revolting to absolute space and time, yet provides a context for modern theories of electromagnetism and gravity. In a book review for a new undergraduate textbook on relativity by Wolfgang Rindler, Kenneth Jacobs noted that \"during the post-Sputnik era, special relativity began to take its rightful place in the undergraduate curriculum\". On the adoption of the Weltanschauung, he notes, \"The historical impact of any world picture is ... partly attributable to the zeal of the promulgators and to the efficacy of their teachings.\"\n\nPhilosophers also distinguish the manifest image from the scientific image. These phrases are due to the American 20th century philosopher Wilfrid Sellars. This is one angle on the ancient philosophical distinction between appearance and reality which is particularly pertinent to everyday contemporary living. Indeed, many believe that the scientific image, with its reductionist methodology, will undermine our sense of individual freedom and responsibility. So, many worry that as science advances, particularly cognitive neuroscience, we will be dehumanized. This certainly has powerful Nietzschean undertones. When our immediately given, manifest (sc. obvious) self-conception is shaken, what is lost for the individual and society? And does it have to be that way? Some questions well worth working on, then, are those concerning the refinement of the manifest view of such centrally important concepts such as free will, the self and individuality, and the possibility of real or lived meaning.\n\nWhile Leo Apostel and his followers clearly hold that individuals can construct worldviews, other writers regard worldviews as operating at a community level, or in an unconscious way. For instance, if one's worldview is fixed by one's language, as according to a strong version of the Sapir–Whorf hypothesis, one would have to learn or invent a new language in order to construct a new worldview.\n\nAccording to Apostel, a worldview is an ontology, or a descriptive model of the world. It should comprise these six elements:\n\nFrom across the world across all of the cultures, Roland Muller has suggested that cultural world views can be broken down into three separate world views. It is not simple enough to say that each person is one of these three cultures. Instead, each individual is a mix of the three. For example, a person may be raised in a Power–Fear society, in an Honor–Shame family, and go to school under a Guilt–Innocence system.\n\nIn a Guilt–Innocence focused culture, schools focus on deductive reasoning, cause and effect, good questions, and process. Issues are often seen as black and white. Written contracts are paramount. Communication is direct, and can be blunt.\n\nSocieties with a predominantly Honor–Shame worldview teach children to make honorable choices according to the situations they find themselves in. Communication, interpersonal interaction, and business dealings are very relationship-driven, with every interaction having an effect on the Honor–Shame status of the participants. In an Honor–Shame society the crucial objective is to avoid shame and to be viewed honorably by other people. The Honor–Shame paradigm is especially strong in most regions of Asia.\n\nSome cultures can be seen very clearly in operating under a Power–Fear worldview. In these cultures it is very important to assess the people around you and know where they fall in line according to their level of power. This can be used for good or for bad. A benevolent king rules with power and his citizens fully support him wielding that power. On the converse, a ruthless dictator can use his power to create a culture of fear where his citizens are oppressed.\n\nAccording to Michael Lind, \"a worldview is a more or less coherent understanding of the nature of reality, which permits its holders to interpret new information in light of their preconceptions. Clashes among worldviews cannot be ended by a simple appeal to facts. Even if rival sides agree on the facts, people may disagree on conclusions because of their different premises.\" This is why politicians often seem to talk past one another, or ascribe different meanings to the same events. Tribal or national wars are often the result of incompatible worldviews. Lind has organized American political worldviews into five categories:\nLind argues that even though not all people will fit neatly into only one category or the other, their core worldview shape how they frame their arguments.\n\nOne can think of a worldview as comprising a number of basic beliefs which are philosophically equivalent to the axioms of the worldview considered as a logical theory. These basic beliefs cannot, by definition, be proven (in the logical sense) within the worldview precisely because they are axioms, and are typically argued \"from\" rather than argued \"for\". However their coherence can be explored philosophically and logically.\n\nIf two different worldviews have sufficient common beliefs it may be possible to have a constructive dialogue between them.\n\nOn the other hand, if different worldviews are held to be basically incommensurate and irreconcilable, then the situation is one of cultural relativism and would therefore incur the standard criticisms from philosophical realists.\nAdditionally, religious believers might not wish to see their beliefs relativized into something that is only \"true for them\".\nSubjective logic is a belief-reasoning formalism where beliefs explicitly are subjectively held by individuals but where a consensus between different worldviews can be achieved.\n\nA third alternative sees the worldview approach as only a methodological relativism, as a suspension judgment about the truth of various belief systems but not a declaration that there is no global truth. For instance, the religious philosopher Ninian Smart begins his \"Worldviews: Cross-cultural Explorations of Human Beliefs\" with \"Exploring Religions and Analysing Worldviews\" and argues for \"the neutral, dispassionate study of different religious and secular systems—a process I call worldview analysis.\"\n\nThe comparison of religious, philosophical or scientific worldviews is a delicate endeavor, because such worldviews start from different presuppositions and cognitive values. Clément Vidal has proposed metaphilosophical criteria for the comparison of worldviews, classifying them in three broad categories:\n\n\nDavid Bell has raised interesting questions on worldviews for the designers of superintelligences – machines much smarter than humans.  'Would they need worldviews, where would they get their worldviews and what would they be like?'. The answers would have to relate to, for example, Christian worldviews. Some of the people who consider features of superintelligences say they will have characteristics that are often associated with divinity, raising big open questions for Christian believers. For example, very advanced machines could, perhaps,   ultimately engender in people a terrified reverence and mystical awe in the light of, say, an artificial agent's impressive understanding of the human condition. And perhaps some humans might even be induced to 'worship and serve the creature rather than the Creator'? On the other hand what would the agent's relationship to God be? Anyone attempting to accommodate concepts such as an omnipotent, personal creator's sacrificial, emotional, spiritual and attitudinal demands being made of any man-made entity, superintelligent or not, could be said to have strayed into \"terra prohibita\" theologically, of course. And how would the worldviews of any superintelligences handle the relationships with what it might regard as its \"human\" 'creator'? \n\n"}
