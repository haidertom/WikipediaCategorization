{"id": "6679056", "url": "https://en.wikipedia.org/wiki?curid=6679056", "title": "A priori and a posteriori", "text": "A priori and a posteriori\n\nThe Latin phrases a priori ( \"from the earlier\") and a posteriori ( \"from the later\") are philosophical terms of art popularized by Immanuel Kant's \"Critique of Pure Reason\" (first published in 1781, second edition in 1787), one of the most influential works in the history of philosophy. However, in their Latin forms they appear in Latin translations of Euclid's \"Elements\", of about 300 , a work widely considered during the early European modern period as the model for precise thinking.\n\nThese terms are used with respect to reasoning (epistemology) to distinguish \"necessary conclusions from first premises\" (i.e., what must come before sense observation) from \"conclusions based on sense observation\" which must follow it. Thus, the two kinds of knowledge, justification, or argument, may be glossed:\n\n\nThere are many points of view on these two types of knowledge, and their relationship gives rise to one of the oldest problems in modern philosophy.\n\nThe terms \"a priori\" and \"a posteriori\" are primarily used as adjectives to modify the noun \"knowledge\" (for example, \"\"a priori\" knowledge\"). However, \"a priori\" is sometimes used to modify other nouns, such as \"truth\". Philosophers also may use \"apriority\" and \"aprioricity\" as nouns to refer (approximately) to the quality of being \"a priori\".\n\nAlthough definitions and use of the terms have varied in the history of philosophy, they have consistently labeled two separate epistemological notions. See also the related distinctions: deductive/inductive, analytic/synthetic, necessary/contingent.\n\nThe intuitive distinction between \"a priori\" and \"a posteriori\" knowledge (or justification) is best seen via examples, as below:\n\n\nSeveral philosophers reacting to Kant sought to explain \"a priori\" knowledge without appealing to, as Paul Boghossian (MD) explains, \"a special faculty ... that has never been described in satisfactory terms.\" One theory, popular among the logical positivists of the early 20th century, is what Boghossian calls the \"analytic explanation of the a priori.\" The distinction between analytic and synthetic propositions was first introduced by Kant. While Kant's original distinction was primarily drawn in terms of conceptual containment, the contemporary version of the distinction primarily involves, as the American philosopher W. V. O. Quine put it, the notions of \"true by virtue of meanings and independently of fact.\" \"Analytic\" propositions are thought to be true in virtue of their meaning alone, while \"a posteriori analytic\" propositions are thought to be true in virtue of their meaning \"and\" certain facts about the world. According to the analytic explanation of the \"a priori\", all \"a priori\" knowledge is analytic; so \"a priori\" knowledge need not require a special faculty of pure intuition, since it can be accounted for simply by one's ability to understand the meaning of the proposition in question. In short, proponents of this explanation claimed to have reduced a dubious metaphysical faculty of pure reason to a legitimate linguistic notion of analyticity.\n\nHowever, the analytic explanation of \"a priori\" knowledge has undergone several criticisms. Most notably, Quine argued that the analytic–synthetic distinction is illegitimate. Quine states: \"But for all its a priori reasonableness, a boundary between analytic and synthetic statements simply has not been drawn. That there is such a distinction to be drawn at all is an unempirical dogma of empiricists, a metaphysical article of faith.\" While the soundness of Quine's critique is highly disputed, it had a powerful effect on the project of explaining the \"a priori\" in terms of the analytic.\n\nThe metaphysical distinction between necessary and contingent truths has also been related to \"a priori\" and \"a posteriori\" knowledge. A proposition that is \"necessarily true\" is one whose negation is self-contradictory (thus, it is said to be true in every possible world). Consider the proposition that all bachelors are unmarried. Its negation, the proposition that some bachelors are married, is incoherent, because the concept of being unmarried (or the meaning of the word \"unmarried\") is part of the concept of being a bachelor (or part of the definition of the word \"bachelor\"). To the extent that contradictions are impossible, self-contradictory propositions are necessarily false, because it is impossible for them to be true. Thus, the negation of a self-contradictory proposition is supposed to be necessarily true. By contrast, a proposition that is \"contingently true\" is one whose negation is not self-contradictory (thus, it is said that it is \"not\" true in every possible world). As Jason Baehr states, it seems plausible that all necessary propositions are known \"a priori\", because \"[s]ense experience can tell us only about the actual world and hence about what is the case; it can say nothing about what must or must not be the case.\"\n\nFollowing Kant, some philosophers have considered the relationship between aprioricity, analyticity, and necessity to be extremely close. According to Jerry Fodor, \"Positivism, in particular, took it for granted that \"a priori\" truths must be necessary...\" However, since Kant, the distinction between analytic and synthetic propositions had slightly changed. Analytic propositions were largely taken to be \"true by virtue of meanings and independently of fact\", while synthetic propositions were not—one must conduct some sort of empirical investigation, looking to the world, to determine the truth-value of synthetic propositions.\n\nAprioricity, analyticity, and necessity have since been more clearly separated from each other. The American philosopher Saul Kripke (1972), for example, provided strong arguments against this position. Kripke argued that there are necessary \"a posteriori\" truths, such as the proposition that water is HO (if it is true). According to Kripke, this statement is necessarily true (since water and HO are the same thing, they are identical in every possible world, and truths of identity are logically necessary) and \"a posteriori\" (since it is known only through empirical investigation). Following such considerations of Kripke and others (such as Hilary Putnam), philosophers tend to distinguish more clearly the notion of aprioricity from that of necessity and analyticity.\n\nKripke's definitions of these terms, however, diverge in subtle ways from those of Kant. Taking these differences into account, Kripke's controversial analysis of naming as contingent and \"a priori\" would, according to Stephen Palmquist, best fit into Kant's epistemological framework by calling it \"analytic a posteriori\". Aaron Sloman presented a brief defence of Kant's three distinctions (analytic/synthetic, apriori/empirical and necessary/contingent) in . It did not assume \"possible world semantics\" for the third distinction, merely that some part of \"this\" world might have been different.\n\nThus, the relationship between aprioricity, necessity, and analyticity is not easy to discern. However, most philosophers at least seem to agree that while the various distinctions may overlap, the notions are clearly not identical: the \"a priori\"/\"a posteriori\" distinction is epistemological, the analytic/synthetic distinction is linguistic, and the necessary/contingent distinction is metaphysical.\n\nThe phrases \"\"a priori\" and \"a posteriori\"\" are Latin for \"from what comes before\" and \"from what comes later\" (or, less literally, \"from first principles, before experience\" and \"after experience\"). They appear in Latin translations of Euclid's \"Elements\", of about 300 , a work widely considered during the early European modern period as the model for precise thinking.\n\nAn early philosophical use of what might be considered a notion of \"a priori\" knowledge (though not called by that name) is Plato's theory of recollection, related in the dialogue \"Meno\" (380 ), according to which something like \"a priori\" knowledge is knowledge inherent, intrinsic in the human mind.\n\nAlbert of Saxony, a 14th-century logician, wrote on both \"a priori\" and \"a posteriori\".\n\nG. W. Leibniz introduced a distinction between \"a priori\" and \"a posteriori\" criteria for the possibility of a notion in his (1684) short treatise \"Meditations on Knowledge, Truth, and Ideas\". \"A priori\" and \"a posteriori\" arguments for the existence of God appear in his \"Monadology\" (1714).\n\nGeorge Berkeley outlined the distinction in his 1710 work \"A Treatise Concerning the Principles of Human Knowledge\" (para. XXI).\n\nThe 18th-century German philosopher Immanuel Kant (1781) advocated a blend of rationalist and empiricist theories. Kant says, \"Although all our cognition begins with experience, it does not follow that it arises [is caused by] from experience\" According to Kant, \"a priori\" cognition is transcendental, or based on the \"form\" of all possible experience, while \"a posteriori\" cognition is empirical, based on the \"content\" of experience. Kant states, \"[…] it is quite possible that our empirical knowledge is a compound of that which we receive through impressions, and that which the faculty of cognition supplies from itself sensuous impressions [sense data] giving merely the \"occasion\" [opportunity for a cause to produce its effect].\" Contrary to contemporary usages of the term, Kant thinks that \"a priori\" knowledge is not entirely independent of the content of experience. And unlike the rationalists, Kant thinks that \"a priori\" cognition, in its pure form, that is without the admixture of any empirical content, is limited to the deduction of the conditions of possible experience. These \"a priori\", or transcendental conditions, are seated in one's cognitive faculties, and are not provided by experience in general or any experience in particular (although an argument exists that \"a priori\" intuitions can be \"triggered\" by experience). \n\nKant nominated and explored the possibility of a transcendental logic with which to consider the deduction of the \"a priori\" in its pure form. Space, time and causality are considered pure \"a priori\" intuitions. Kant reasoned that the pure \"a priori\" intuitions are established via his transcendental aesthetic and transcendental logic. He claimed that the human subject would not have the kind of experience that it has were these \"a priori\" forms not in some way constitutive of him as a human subject. For instance, a person would not experience the world as an orderly, rule-governed place unless time, space and causality were determinant functions in the form of perceptual faculties, i. e., there can be no experience in general without space, time or causality as particular determinants thereon. The claim is more formally known as Kant's transcendental deduction and it is the central argument of his major work, the \"Critique of Pure Reason\". The transcendental deduction argues that time, space and causality are ideal as much as real. In consideration of a possible logic of the \"a priori\", this most famous of Kant's deductions has made the successful attempt in the case for the fact of subjectivity, what constitutes subjectivity and what relation it holds with objectivity and the empirical.\n\nAfter Kant's death, a number of philosophers saw themselves as correcting and expanding his philosophy, leading to the various forms of German Idealism. One of these philosophers was Johann Fichte. His student (and critic), Arthur Schopenhauer, accused him of rejecting the distinction between \"a priori\" and \"a posteriori\" knowledge:\n\n\n\n\n"}
{"id": "48624755", "url": "https://en.wikipedia.org/wiki?curid=48624755", "title": "Acceptability", "text": "Acceptability\n\nAcceptability is the characteristic of a thing being subject to acceptance for some purpose. A thing is acceptable if it is sufficient to serve the purpose for which it is provided, even if it is far less usable for this purpose than the ideal example. A thing is unacceptable (or has the characteristic of unacceptability) if it deviates so far from the ideal that it is no longer sufficient to serve the desired purpose, or if it goes against that purpose. From a logical perspective, a thing can be said to be acceptable if it has no characteristics that make it unacceptable:\n\nHungarian mathematician Imre Lakatos developed a concept of acceptability \"taken as \"a measure of the approximation to the truth\"\". This concept was criticized in its applicability to philosophy as requiring that better theories first be eliminated. Acceptability is also a key premise of negotiation, wherein opposing sides each begin from a point of seeking their ideal solution, and compromise until they reach a solution that both sides find acceptable:\n\nWhere an unacceptable proposal has been made, \"a counterproposal is generated if there are any acceptable ones that have had already been explored\". Since the acceptability of proposition to a participant in a negotiation is only known to that participant, the participant may act as though a proposal that is actually acceptable to them is not, in order to obtain a more favorable proposal. \n\nOne concept of acceptability that has been widely studied is acceptable risk in situations affecting human health. The idea of not increasing lifetime risk by more than one in a million has become commonplace in public health discourse and policy. It is a heuristic measure. It provides a numerical basis for establishing a negligible increase in risk.\n\nEnvironmental decision making allows some discretion for deeming individual risks potentially \"acceptable\" if less than one in ten thousand chance of increased lifetime risk. Low risk criteria such as these provide some protection for a case where individuals may be exposed to multiple chemicals e.g. pollutants, food additives or other chemicals. In practice, a true zero-risk is possible only with the suppression of the risk-causing activity.\n\nStringent requirements of 1 in a million may not be technologically feasible or may be so prohibitively expensive as to render the risk-causing activity unsustainable, resulting in the optimal degree of intervention being a balance between risks vs. benefit. For example, emissions from hospital incinerators result in a certain number of deaths per year. However, this risk must be balanced against the alternatives. There are public health risks, as well as economic costs, associated with all options. The risk associated with no incineration is potential spread of infectious diseases, or even no hospitals. Further investigation identifies options such as separating noninfectious from infectious wastes, or air pollution controls on a medical incinerator.\n\nAcceptable variance is the range of variance in any direction from the ideal value that remains acceptable. In project management, variance can be defined as \"the difference between what is planned and what is actually achieved\". Degrees of variance \"can be classified into negative variance, zero variance, acceptable variance, and unacceptable variance\". In software testing, for example, \"[g]enerally 0-5% is considered as acceptable variance\" from an ideal value.\n\nAcceptance testing is a practice used in chemical and engineering fields, intended to check ahead of time whether or not a thing will be acceptable.\n"}
{"id": "28231080", "url": "https://en.wikipedia.org/wiki?curid=28231080", "title": "Archiv für Begriffsgeschichte", "text": "Archiv für Begriffsgeschichte\n\nArchiv für Begriffsgeschichte ('Archive for Conceptual History') is a German peer-reviewed academic journal. It was founded by Erich Rothacker, and is published by Christian Bermes, Ulrich Dierse and Michael Erler. The editor is Annika Hand.\n\nThe journal publishes works on concepts of the history of philosophy and sciences, both from the European and from non-European traditions, on mythological and religious concepts, and on concepts of common parlance which have a characteristic significance for a special era or culture. The journal also embraces articles on revealing metaphors, on problems at translating concepts, as well as on theory and criticism of the method of conceptual history.\n\n\n"}
{"id": "41099486", "url": "https://en.wikipedia.org/wiki?curid=41099486", "title": "Argument-deduction-proof distinctions", "text": "Argument-deduction-proof distinctions\n\nArgument-deduction-proof distinctions originated with logic itself. Naturally, the terminology evolved. \nAn argument, more fully a premise-conclusion argument, is a two-part system composed of premises and conclusion. An argument is \"valid\" if and only if its conclusion is a consequence of its premises. Every premise set has infinitely many consequences each giving rise to a valid argument. Some consequences are obviously so but most are not: most are hidden consequences. Most valid arguments are not yet known to be valid. To determine validity in non-obvious cases deductive reasoning is required. There is no deductive reasoning in an argument \"per se\"; such must come from the outside. \n\nEvery argument's conclusion is a premise of other arguments. The word \"constituent\" may be used for either a premise or conclusion.In the context of this article and in most classical contexts, all candidates for consideration as argument constituents fall under the category of truth-bearer: propositions, statements, sentences, judgments, etc.\n\nA deduction is a three-part system composed of premises, a conclusion, and chain of intermediates — steps of reasoning showing that its conclusion is a consequence of its premises. The reasoning in a deduction is by definition cogent. Such reasoning itself, or the chain of intermediates representing it, has also been called an argument, more fully a deductive argument. In many cases, an argument can be known to be valid by means of a deduction of its conclusion from its premises but non-deductive methods such as Venn diagrams and other graphic procedures have been proposed.\n\nA proof is a deduction whose premises are known truths. A proof of the Pythagorean theorem is a deduction that might use several premises — axioms, postulates, and definitions — and contain dozens of intermediate steps. As Alfred Tarski famously emphasized in accord with Aristotle, truths can be known by proof but proofs presuppose truths not known by proof.\nPremise-conclusion arguments do not require or produce either knowledge of validity or knowledge of truth. Premise sets may be chosen arbitrarily and conclusions may be chosen arbitrarily. \nDeductions require knowing how to reason but they do not require knowledge of truth of their premises. Deductions produce knowledge of the validity of arguments but ordinarily they do not produce knowledge of the truth of their conclusions.\nProofs require knowledge of the truth of their premises, they require knowledge of deductive reasoning, and they produce knowledge of their conclusions.\nModern logicians disagree concerning the nature of argument constituents.Quine devotes the first chapter of \"Philosophy of Logic\" to this issue. Historians have not even been able to agree on what Aristotle took as constituents.\nArgument-deduction-proof distinctions are inseparable from what have been called the \"consequence-deducibility\" distinction and the \"truth-and-consequence conception of proof\". Variations among argument-deduction-proof distinctions are not all terminological.\n\nLogician Alonzo Church never used the word \"argument\" in the above sense and had no synonym. Moreover, Church never explained that deduction is the process of producing knowledge of consequence and it never used the common noun \"deduction\" for an application of the deduction process. His primary focus in discussing proof was “conviction” produced by generation of chains of logical truths—not the much more widely applicable and more familiar general process of demonstration as found in pre-Aristotelian geometry and discussed by Aristotle. He did discuss deductions in the above sense but not by that name: he called them awkwardly “proofs from premises” — an expression he coined for the purpose.\n\nThe absence of argument-deduction-proof distinctions is entirely consonant with Church's avowed Platonistic logicism. Following Dummett's insightful remarks about Frege, which — \"mutatis mutandis\" — apply even more to Church, it might be possible to explain the today-surprising absence.\n"}
{"id": "26276222", "url": "https://en.wikipedia.org/wiki?curid=26276222", "title": "Bateman's principle", "text": "Bateman's principle\n\nBateman's principle, in evolutionary biology, is that in most species, variability in reproductive success (or reproductive variance) is greater in males than in females. It was first proposed by Angus John Bateman (1919–1996), an English geneticist. Bateman suggested that, since males are capable of producing millions of sperm cells with little effort, while females invest much higher levels of energy in order to nurture a relatively small number of eggs, the female plays a significantly larger role in their offspring's reproductive success. Bateman’s paradigm thus views females as the limiting factor of parental investment, over which males will compete in order to copulate successfully.\n\nAlthough Bateman's principle served as a cornerstone for the study of sexual selection for many decades, it has recently been subject to criticism. Attempts to reproduce Bateman's experiments in 2012 and 2013 were unable to support his conclusions. Some scientists have criticized Bateman's experimental and statistical methods, or pointed out conflicting evidence, while others have defended the veracity of the principle and cited evidence in support of it.\n\nTypically it is the females who have a relatively larger investment in producing each offspring. Bateman attributed the origin of the unequal investment to the differences in the production of gametes: sperm are cheaper than eggs. A single male can easily fertilize all females' eggs; she will not produce more offspring by mating with more than one male. A male is capable of fathering more offspring if he mates with several females. By and large, a male's potential reproductive success is limited by the number of females he mates with, whereas a female's potential reproductive success is limited by how many eggs she can produce. According to Bateman's principle, this results in sexual selection, in which males compete with each other, and females become choosy in which males to mate with. Thus, as a result of being anisogamous, males are fundamentally promiscuous, and females are fundamentally selective.\n\nBateman initially published his review in 1948. He was a botanist, contributing to the literature of sexual selection only once in his lifetime. Bateman initially saw his study on Drosophila to be a test of Charles Darwin’s doctrine of sexual selection. He saw Darwin’s theory of natural selection not as flawed, but as incomplete. He felt that if he were to provide a concrete demonstration of how sexual selection played a role in the reproductive success of certain species, he could explain the gap between Darwin’s ideas and sexual dimorphism.\n\nAlthough it is common to confuse Bateman's ideas with those of later scientists, his principle can be expressed in three simple statements. The first is that male reproductive success increases with the number of mates they attempt to copulate with, while female reproductive success does not. The second is that male reproductive success will show greater variance than female. The third is that sexual selection will have a greater effect on the sex with greater variance in reproductive success.\n\nThroughout his research, Bateman conducted experiments using fruit flies in order to observe their copulation and sexual behavior. A total of six series of experiments were conducted with the fruit fly \"Drosophila melanogaster\", using three to five individuals of each sex. Each trial ran for three or four days. Some ran to completion without the transfer of the Drosophila from one environment (bottle) to another. In the others, Bateman transferred the flies and their eggs to a new bottle every day. Bateman also varied the age of the flies depending on the experiment, with an age gap between one and six days total. He never watched the flies' copulations. The flies used were from several inbred strains, which meant they could be identified by their specific inbred strain. Therefore, he inferred the number of involved mates based on the number of offspring that were later found to have mutations from both a male and a female. The difficulty that arose was that if a female Drosophila had copulated with five males and only one larva survived, Bateman would not be able to account for the other four copulations.\n\nAnalysis of the data collected in sets one through four showed that the males' reproductive success, estimated as the number of sired offspring, increased at a steady rate until a total of three mates were reached. It is important to note that Bateman kept the sex ratio of males to females completely even throughout his trials. But after surpassing three mates, male reproductive success began to fall. Female reproductive success also increased with number of mates, but much more gradually than that of the males. The second series of data collected in sets five and six illustrated a dramatically different outcome. Male reproductive success increased at a steady and steep rate, never dropping. Female reproductive success, on the other hand, plateaued after a single mate. Bateman focused mainly on the second series of data when discussing his results. His main conclusion was that the reproductive success of females does not increase with an influx of mates, as one fit mate was enough to successfully complete fertilization. This is often referred to as Bateman’s Gradient.\n\nThroughout 2012 and 2013, Gowaty, Kim, and Anderson took it upon themselves to repeat Bateman's experiment in its entirety. While reproducing his tests, the same fly strains and mutations were used in order to maintain the same methodology. However, one of the 11 strains that Bateman used had gone extinct, and was thus replaced. (Tang-Martinez 2012)\n\nGowaty, Kim, and Anderson found that upon combining certain strains with one another, the offspring were unable to survive to adulthood. (Gowaty, Kim, & Anderson 2012) Thus, Bateman’s numbers regarding the number of individuals not having mated was higher than the actual number. Likewise, his estimate of those that mated with one or more mates was too low. This was valid for both the males and females of this species.\n\nGowaty desired to further explore the reasoning behind the premature death of the Drosophila. She began doing so by running monogamy trials between different strains of flies and found that 25% of the offspring died due to becoming double mutants. (Gowaty 2013) Bateman thought his work fit within the lines of Mendel’s laws of genetics, while Gowaty proved otherwise. The 1948 experiments inferred reproductive success based on the number of adults living by the end of the trial. In reality, many factors were left out of the equation when calculating reproductive success as a function of the number of mates, which had the ability to completely dislodge the accuracy behind Bateman's results. Gowaty was not able to confirm Bateman's conclusions and found no evidence for sexual selection in the experiment. (Gowaty 2013; Tang-Martinez 2012)\n\nNevertheless, some modern experiments between the relationship of number of mates and the reproductive success of males and females support Bateman's principle. Julie Collet conducted an experiment with a population of red jungle fowl. A total of thirteen replicate groups of three males and four females were monitored for ten days. In this experiment, the sex ratio was biased toward females. A male's reproductive success was calculated using the proportion of embryos fathered to the total number of embryos produced by all the females he mated with. The total sexual selection opportunity was calculated using the following formula.\n\nThe \"σ\" represents the variance in RS, while the is the square mean of reproductive success of members of one sex in a group.\n\nIn 2013, Fritzsche and Arnqvist tested Bateman's principle by estimating sexual selection between males and females in four seed beetles. They used a unique experimental design that showed sexual selection to be greater in males than in females. In contrast, sexual selection was also shown to be stronger for females in role-reversed species. They suggested that the Bateman gradient is typically the most accurate and informative measure of sexual selection between different sexes and species (Fritzsche, K. & Arnqvist, G).\n\nMore than 60 years later, Bateman's principle has received considerable attention. Sutherland argued that males' higher variance in reproductive success may result from random mating and coincidence. Hubbell and Johnson suggested that variance in reproductive success can be greatly influenced by the time and allocations of mating. In 2005, Gowaty and Hubbell suggested that mating tendencies are subject to change depending on certain strategies. They argued that there are cases in which males can be more selective than females, whereas Bateman suggested that his paradigm would be “almost universal” among sexually reproducing species. Critics proposed that females might be more subject to sexual selection than males, but not in all circumstances.\n\nExperimental and statistical criticisms followed. Until approximately a decade ago, critics of Bateman’s model focused on his experimental design. In recent years, they have shifted attention to the actual experimental and statistical calculations Bateman published throughout his trials. Birkhead wrote a 2000 review arguing that since Bateman’s experiments lasted only three to four days, the female fruit fly, \"Drosophila melanogaster\", may not have needed to mate repeatedly, as it can store sperm for up to four days; if Bateman had used a species in which females had to copulate more often to fertilize their eggs, the results might have been different. Snyder and Gowaty conducted the first in-depth analysis of the data in Bateman’s 1948 paper. They found sampling biases, mathematical errors, and selective presentation of data.\n\nA 2012 review by Zuleyma Tang-Martínez concluded that various empirical and theoretical studies, especially Gowaty's reproduction of Bateman's original experiment, pose a major challenge to Bateman's conclusions, and that Bateman's principle should be considered an unproven hypothesis in need of further reexamination. According to Tang-Martínez, \"modern data simply don't support most of Bateman's and Trivers's predictions and assumptions.\"\n\nA 2016 review confirmed Darwinian sex roles across the animal kingdom, concluding that \"sexual selection, as captured by standard Bateman metrics, is indeed stronger in males than in females and that it is evolutionarily tied to sex biases in parental care and sexual dimorphism.\"\n\nOne error source that have been shown to give an illusion of greater differential in reproductive success in males than in females genetically is that chromosome effects cause a greater percentage of mutations to be lethal before even reaching sexual maturity in males than in females.\n\nThe assumption that any differential in reproductive success between males and females among the individuals that do reach sexual maturity must be due to sexual selection in the current population is also subject to criticism, such as the possibility of remnants of sexually selected traits in a previous species from which a new species have evolved can be negatively selected due to costs in nutrients and weakened immune systems and that such negative selection would cause a higher difference in reproductive success in males than in females even without any still ongoing sexual selection. Since lower degrees of selection during times of stable environment allows genetic variation to build up by random mutations and allow some individuals in a population to survive environmental change while strong constant selection offsets the effect and increases the risk of the entire population dying out during catastrophic environmental change due to less genetic variation, constant loss of genetic variation caused by sexual selection have been suggested as a factor contributing to higher extinction rates in more sexually dimorphic species besides the nutrient, immunity and other costs of the ornaments themselves. While the ornament cost risk would only be removed when the ornaments have been eliminated by selection, the genetic variation model predicts that the species ability to survive would improve significantly even at an early stage of reduction of sexual dimorphism due to other adaptive mutations arising and surviving due to minimal selection during times of stable environment while the genes causing sexually dimorphic anatomy have only in small part been affected by the mutations. Applied to human evolution, this model can explain why early Homo sapiens display a significantly increased adaptability to environmental change already at its early divergence from Homo erectus that had a high muscular sexual dimorphism, as well as why human anatomy through the history of Homo sapiens show a diversification during times of stable climate and a selective loss of the more robust male forms during environmental change that does not recover during later stability, continuing through the loss of many robust characteristics in regional bottlenecks as recent as the end of the Ice Age and the time around the agricultural revolution. It also explains genetic evidence of human genetic diversity increasing during stable environmental periods and being reduced during bottlenecks related to changes in the environment.\n\nRecently, DNA testing has permitted more detailed investigation of mating behavior in numerous species. The results, in many cases, have been cited as evidence against Bateman's principle.\n\nUntil recently, most bird species were believed to be sexually monogamous. DNA paternity testing, however, has shown that in nearly 90% of bird species, females copulate with multiple males during each breeding season. The superb fairy wren is socially monogamous, but 95% of its clutches contain young fathered by extra-pair males. Up to 87% of tree swallow clutches, 75% of coal tit clutches, and 70% of reed bunting clutches contain young fathered by extra-pair males. Even female waved albatrosses, which typically mate for life, are sexually promiscuous, with 17% of young fathered by extra-pair males.\n\nIn many primate species, females solicit sex from males and may mate with more than one male in quick succession. Female lions may mate 100 times per day with different males while they are in estrus. Females of the pseudoscorpion species, \"Cordylochernes scorpioides\", have been shown to have higher reproductive success when mated with more than one male.\n\nThere are a number of claims about sex differences in humans said by evolutionary psychologists to be well supported that are not logically predicted by Bateman's principle. One example that have been pointed out is language ability that is said to be more developed in women than in men. It is argued that language, being a derived human trait, is not predicted by a model that assumes males to be more selected than females to be present to a higher degree in women than in men but the opposite. The objection about a trade-off between spatial ability and language is found lacking due to the existence of animals that outperform humans on spatial tasks such as birds of prey and bats while having much smaller and more rapidly maturing brains than humans, while great ape language is limited and ungrammatical at best despite great apes being much closer to humans in brain capacity than bats or birds of prey are, showing that the system requirements for spatial ability are much lower than those for language and therefore not a serious trade-off for them. The generalization of Bateman's principle to societies lacking modern birth control is also argued to be incompatible with such generalization of the \"nuns more successful at celibacy than monks\" clause of the female erotic plasticity model, as absence of modern contraceptives would mean that absence of reproduction for many men required that many men had no sexual relationships with women while male sexual exclusivity to a particular gender, if present, would have precluded same-sex relationships as a form of non-celibacy for men who could not find a female mate as well. It is also mentioned that the evolutionary psychologists themselves claim that women are sexually picky due to an evolutionary history without modern contraception, while it is pointed out that the notion of celibacy being impossible for men in general with only a small number of individual exceptions is incompatible with the notion of evolution making celibacy inevitable for a significant percentage of men.\n\nThe most well-known exceptions to Bateman's principle are the existence of sex-role reversed species such as pipefish (seahorses), phalaropes and jacanas in which the males perform the majority of the parental care, and are cryptic while the females are highly ornamented and territorially aggressive (; ; ). \n\nIn these species, however, the typical fundamental sex differences are reversed: females have a faster reproductive rate than males (and thus greater reproductive variance), and males have greater assurance of genetic parentage than do females . Consequently, reversals in sex roles and reproductive variance are consistent with Bateman's principle, and with Robert Trivers's parental investment theory.\n\n\n\n"}
{"id": "3280462", "url": "https://en.wikipedia.org/wiki?curid=3280462", "title": "Belief–desire–intention model", "text": "Belief–desire–intention model\n\nThe belief–desire–intention (BDI) model of human practical reasoning was developed by Michael Bratman as a way of explaining future-directed intention.\n\nBDI is fundamentally reliant on folk psychology (the 'theory theory'), which is the notion that our mental models of the world are theories. It was used as a basis for developing the belief–desire–intention software model.\n\nBDI was part of the inspiration behind the BDI software architecture, which Bratman was also involved in developing. Here, the notion of intention was seen as a way of limiting time spent on deliberating about what to do, by eliminating choices inconsistent with current intentions.\n\nBDI has also aroused some interest in psychology. BDI formed the basis for a computational model of childlike reasoning CRIBB. It has been proposed that autistic children do not recognise other people as folk-psychological agents (i.e., agents with their own beliefs, etc.). BDI has been used to develop a rehabilitation strategy to teach autistic children to reason about other people.\n\n"}
{"id": "6672748", "url": "https://en.wikipedia.org/wiki?curid=6672748", "title": "Causal model", "text": "Causal model\n\nA causal model is a conceptual model that describes the causal mechanisms of a system. Causal models can improve study designs by providing clear rules for deciding which independent variables need to be included/controlled for. \n\nThey can allow some questions to be answered from existing observational data without the need for an interventional study such as a randomized controlled trial. Some interventional studies are inappropriate for ethical or practical reasons, meaning that without a causal model, some questions cannot be answered. \n\nCasual models can help with the question of external validity(whether results from one study apply to unstudied populations). Causal models can allow data from multiple studies to be merged (in certain circumstances) to answer questions that cannot be answered by any individual data set.\n\nCausal models are falsifiable, in that if they do not match data, they must be rejected as invalid.\n\nCausal models have found applications in signal processing, epidemiology and machine learning.\n\n Pearl defines a causal model as an ordered triple formula_1, where U is a set of exogenous variables whose values are determined by factors outside the model; V is a set of endogenous variables whose values are determined by factors within the model; and E is a set of structural equations that express the value of each endogenous variable as a function of the values of the other variables in U and V.\n\nAristotle defined a taxonomy of causality, including \"material\", \"formal\", \"efficient\" and \"final\" causes. Hume rejected Aristotle's taxonomy in favor of counterfactuals. At one point, he denied that objects have \"powers\" that make one a cause and another an effect. Later he adopted \"if the first object had not been, the second had never existed\" (\"but-for\" causation).\n\nIn the late 19th century, the discipline of statistics began to form. After a years-long effort to identify causal rules for domains such as biological inheritance, Galton introduced the concept of mean regression (epitomized by the sophomore slump in sports) which later led him to the non-causal concept of correlation. \n\nAs a positivist, Pearson expunged the notion of causality from much of science as an unprovable special case of association and introduced the correlation coefficient as the metric of association. He wrote, \"Force as a cause of motion is exactly the same as a tree god as a cause of growth\" and that causation was only a \"fetish among the inscrutable arcana of modern science\". Pearson founded \"Biometrika\" and the Biometrics Lab at University College London, which became the world leader in statistics.\n\nIn 1908 Hardy and Weinberg solved the problem of trait stability that had led Galton to abandon causality, by invoking Mendelian inheritance.\n\nIn 1921 Wright's path analysis became the theoretical ancestor of causal modeling and causal graphs. He developed this approach while attempting to untangle the relative impacts of heredity, development and environment on guinea pig coat patterns. He backed up his heretical claims by showing how such analyses could explain the relationship between guinea pig birth weight, in utero time and litter size. Opposition to these ideas by prominent statisticians led them to be ignored for the following 40 years (except among animal breeders). Instead scientists relied on correlations, partly at the behest of Wright's critic (and leading statistician), Fisher. One exception was Burks, a student who in 1926 was the first to apply path diagrams to represent a mediator and to assert that holding a mediator constant induces errors. She may have invented path diagrams independently. \n\nIn 1923, Neyman introduced the concept of a potential outcome, but his paper was not translated from Polish to English until 1990. \n\nIn 1958 Cox wrote warned that controlling for a variable Z is valid only if it is highly unlikely to be affected independent variables. \n\nIn the 1960s, Duncan, Blalock, Goldberger and others rediscovered path analysis. While reading Blalock's work on path diagrams, Duncan remembered a lecture by Ogburn twenty years earlier that mentioned a paper by Wright that mentioned Burk. \n\nSociologists called causal models structural equation modeling, but once it became a rote method, it lost its utility, leading some practitioners to reject any relationship to causality. Economists adopted the algebraic part of path analysis, calling it simultaneous equation modeling. However, economists still avoided attributing causal meaning to their equations.\n\nSixty years after his first paper, Wright published a piece that recapitulated it, following Karlin et al.'s critique, which objected that it handled only linear relationships and that robust, model-free presentations of data are more revealing.\n\nIn 1973 Lewis advocated replacing correlation with but-for causality (counterfactuals). He referred to humans' ability to envision alternative worlds in which a cause did or not occur and in which effect an appeared only following its cause. In 1974 Rubin introduced the notion of \"potential outcomes\" as a language for asking causal questions. \n\nIn 1983 Cartwright proposed that any factor that is \"causally relevant\" to an effect be conditioned on, moving beyond simple probability as the only guide. \n\nIn 1986 Baron and Kenny introduced principles for detecting and evaluating mediation in a system of linear equations. As of 2014 their paper was the 33rd most-cited of all time. That year Greenland and Robins introduced the \"exchangeability\" approach to handling confounding by considering a counterfactual. They proposed assessing what would have happened to the treatment group if they had not received the treatment and comparing that outcome to that of the control group. If they matched, confounding was said to be absent. \n\nPearl's causal metamodel involves a three-level abstraction he calls the ladder of causation. The lowest level, Association (seeing/observing), entails the sensing of regularities or patterns in the input data, expressed as correlations. The middle level, Intervention (doing), predicts the effects of deliberate actions, expressed as causal relationships. The highest level, Counterfactuals (imagining), involves constructing a theory of (part of) the world that explains why specific actions have specific effects and what happens in he absence of such actions.\n\nOne object is associated with another if observing one changes the probability of observing the other. Example: shoppers who buy toothpaste are more likely to also buy dental floss. Mathematically: \n\nor the probability of (purchasing) floss given (the purchase of) toothpaste. Associations can also be measured via computing the correlation of the two events. Associations have no causal implications. One event could cause the other, the reverse could be true, or both events could be caused by some third event (unhappy hygenist shames shopper into treating their mouth better ).\n\nThis level asserts specific causal relationships between events. Causality is assessed by experimentally performing some action that affects one of the events. Example: doubling the price of toothpaste (then what happens). Causality cannot be established by examining history (of price changes) because the price change may have been for some other reason that could itself affect the second event (a tariff that increases the price of both goods). Mathematically: \n\nwhere \"do\" is an operator that signals the experimental intervention (doubling the price).\n\nThe highest, counterfactual, level involves consideration of an alternate version of a past event. Example: What is the probability that If a store had doubled the price of floss, the toothpaste-purchasing shopper would still have bought it? Answering yes asserts the existence of a causal relationship. Models that can answer counterfactuals allow precise interventions whose consequences can be predicted. At the extreme, such models are accepted as physical laws (as in the laws of physics, e.g., inertia, which says that if force is not applied to a stationary object, it will not move).\n\nStatistics revolves around the analysis of relationships among multiple variables. Traditionally, these relationships are described as correlations, associations without any implied causal relationships. Causal models attempt to extend this framework by adding the notion of causal relationships, in which changes in one variable cause changes in others.\n\nTwentieth century definitions of causality relied purely on probabilities/associations. One event (X) was said to cause another if it raises the probability of the other (Y). Mathematically this is expressed as: \n\nA later definition attempted to address this ambiguity by conditioning on background factors. Mathematically: \n\nOther attempts to define causality include Granger causality, a statistical hypothesis test that causality (in economics) can be assessed by measuring the ability to predict the future values of one time series using prior values of another time series.\n\nA cause can be necessary, sufficient, contributory or some combination.\n\nFor \"x\" to be a necessary cause of \"y\", the presence of \"y\" must imply the prior occurrence of \"x\". The presence of \"x\", however, does not imply that \"y\" will occur. Necessary causes are also known as \"but-for\" causes, as in \"y\" would not have occurred but for the occurrence of \"x\". \n\nFor \"x\" to be a sufficient cause of \"y\", the presence of \"x\" must imply the subsequent occurrence of \"y\". However, another cause \"z\" may independently cause \"y\". Thus the presence of \"y\" does not require the prior occurrence of \"x\".\n\nFor \"x\" to be a contributory cause of \"y\", the presence of \"x\" must increase the likelihood of \"y\". If the likelihood is 100%, then \"x\" is instead called sufficient. A contributory cause may also be necessary.\n\nA causal diagram is a directed graph that displays causal relationships between variables in a causal model. A causal diagram includes a set of variables (or nodes). Each node is connected by an arrow to one or more other nodes upon which it has a causal influence. An arrowhead delineates the direction of causality, e.g., an arrow connecting variables A and B with the arrowhead at B indicates that a change in A causes a change in B (with an associated probability).\n\nCausal diagrams include causal loop diagrams, directed acyclic graphs, and Ishikawa diagrams.\n\nCasual diagrams are independent of the quantitative probabilities that inform them. Changes to those probabilities (e.g., due to technological improvements) do not require changes to the model.\n\nCausal models have formal structures with elements with specific properties.\n\nThe three types of connections of three nodes are linear chains, branching forks and merging colliders.\n\nChains are straight line connections with arrows pointing from cause to effect. In this model, B is a mediator in that it mediates the change that A would otherwise have on C. \n\nIn forks, one cause has multiple effects. The two effects have a common cause. Conditioning on B (for a specific value of B) reveals a positive correlation between A and C that is not causal. \n\nAn elaboration of a fork is the confounder:\n\nIn such models, B is a common cause of A and C (which also causes A), making B the confounder. \n\nIn colliders, multiple causes affect one outcome. Conditioning on B (for a specific value of B) often reveals a non-causal negative correlation between A and C. This negative correlation has been called collider bias and the \"explain-away\" effect as in, B explains away the correlation between A and C. The correlation can be positive in the case where contributions from both A and C are necessary to affect B. \n\nA mediator node modifies the effect of other causes on an outcome (as opposed to simply affecting the outcome). \n\nA confounder node affects multiple outcomes, creating a positive correlation among them.\n\nAn instrumental variable is one that: \n\n\nRegression coefficients can serve as estimates of the causal effect of an instrumental variable on an outcome as long as that effect is not confounded. In this way, instrumental variables allow causal factors to be quantified without data on confounders. \n\nFor example, given the model:\n\nZ is an independent variable, because it has a path to the outcome Y and is unconfounded, e.g., by U.\n\nDefinition: In the above example, if Z and X take binary values, then the assumption that Z = 0, X = 1 does not occur is called monotonicity. \n\nRefinements to the technique include creating an instrument by conditioning on other variable to block the paths between the instrument and the confounder and combining multiple variables to form a single instrument. \n\nDefinition: Mendelian randomization uses measured variation in genes of known function to examine the causal effect of a modifiable exposure on disease in observational studies.\n\nBecause genes vary randomly across populations, presence of a gene typically qualifies as an instrumental variable, implying that in many cases, causality can be quantified using regression on an observational study.\n\nIndependence conditions are rules for deciding whether two variables are independent of each other. Variables are independent if the values of one do not directly affect the values of the other. Multiple causal models can share independence conditions. For example, the models\n\nand \n\nhave the same independence conditions, because conditioning on B leaves A and C independent. However, the two models do not have the same meaning and can be falsified based on data. (If observations show an association between A and C after conditioning on B, then both models are incorrect). Conversely, data cannot show which of these two models are correct, because they have the same independence conditions. Conditioning on a variable is a mechanism for conducting hypothetical experiments. Conditioning on a variable involves analyzing the values of other variables for a given value of the conditioned variable. In the first example, conditioning on B implies that observations for a given value of B should show no correlation between A and C. If such a correlation exists, then the model is incorrect. Non-causal models cannot make such distinctions, because they do not make causal assertions. \n\nAn essential element of correlational study design is to identify potentially confounding influences on the variable under study, such as demographics. These variables are controlled for to eliminate those influences. However, the correct list of confounding variables cannot be determined \"a priori\". It is thus possible that a study may control for irrelevant variables or even (indirectly) the variable under study. \n\nCausal models offer a robust technique for identifying appropriate confounding variables. Formally, Z is a confounder if \"Y is associated with Z via paths not going through X\". These can often be determined using data collected for other studies. Mathematically, if \n\nthen X is a confounder for Y. \n\nEarlier, allegedly incorrect definitions include: \n\n\nThe latter is flawed in that given that in the model: \n\nZ matches the definition, but is a mediator, not a confounder, and is an example of controlling for the outcome.\n\nIn the model \n\nTraditionally, B was considered to be a confounder, because it is associated with X and with Y but is not on a causal path nor is it a descendant of anything on a causal path. Controlling for B causes it to become a confounder. This is known as M-bias. \n\nIn a causal model, the method for identifying all appropriate counfounders (deconfounding) is to block every noncausal path between X and Y without disrupting any causal paths. \n\nDefinition: a backdoor path between two variables X and Y is any path from X to Y that starts with an arrow pointing to X. \n\nX and Y are deconfounded if every backdoor path is blocked and no controlled-for variable Z is descended from X. It is not necessary to control for any variables other than the deconfounders. \n\nDefinition: the backdoor criterion is satisfied when all backdoor paths in a model are blocked.\n\nWhen the causal model is a plausible representation of reality and the backdoor criterion is satisfied, then partial regression coefficients can be used as (causal) path coefficients (for linear relationships). \n\nDefinition: a frontdoor path is a direct causal path for which data is available for all variables. \n\nThe following converts a do expression into a do-free expression by conditioning on the variables along the front-door path. \n\nPresuming data for these observable probabilities is available, the ultimate probability can be computed without an experiment, regardless of the existence of other confounding paths and without backdoor adjustment. \n\nQueries are questions asked based on a specific model. They are generally answered via performing experiments (interventions). Interventions take the form of fixing the value of one variable in a model and observing the result. Mathematically, such queries take the form (from the example): \n\nwhere the \"do\" operator indicates that the experiment explicitly modified the price of toothpaste. Graphically, this blocks any causal factors that would otherwise affect that variable. Diagramatically, this erases all causal arrows pointing at the experimental variable. \n\nMore complex queries are possible, in which the do operator is applied (the value is fixed) to multiple variables.\n\nThe do calculus is the set of manipulations that are available to transform one expression into another, with the general goal of transforming expressions that contain the do operator into expressions that do not. Expressions that do not include the do operator can be estimated from observational data alone, without the need for an experimental intervention, which might be expensive, lengthy or even unethical (e.g., asking subjects to take up smoking). The set of rules is complete (it can be used to derive every true statement in this system). An algorithm can determine whether, for a given model, a solution is computable in polynomial time. \n\nThe calculus includes three rules for the transformation of conditional probability expressions involving the do operator. \n\nRule 1 permits the addition or deletion of observations.:\n\nin the case that the variable set Z blocks all paths from W to Y and all arrows leading into X have been deleted. \n\nRule 2 permits the replacement of an intervention with an observation or vice versa.:\n\nin the case that Z satisfies the back-door criterion. \n\nRule 3 permits the deletion or addition of interventions.:\n\nin the case where no causal paths connect X and Y. \n\nThe rules do not imply that any query can have its do operators removed. In those cases, it may be possible to substitute a variable that is subject to manipulation (e.g., diet) in place of one that is not (e.g., blood cholesterol), which can then be transformed to remove the do. Example: \n\nCounterfactuals consider possibilities that are not found in data, such as whether a nonsmoker would have developed cancer had they instead been a heavy smoker. They are the highest step on Pearl's causality ladder. \n\nDefinition: A potential outcome for a variable Y is \"the value Y would have taken for individual \"u\", had X been assigned the value x\". Mathematically: \n\nThe potential outcome is defined at the level of the individual \"u.\" \n\nThe conventional approach to potential outcomes is data-, not model-driven, limiting its ability to untangle causal relationships. It treats causal questions as problems of missing data and gives incorrect answers to even standard scenarios. \n\nIn the context of causal models, potential outcomes are interpreted causally, rather than statistically.\n\nThe first law of causal inference states that the potential outcome \n\ncan be computed by modifying causal model M (by deleting arrows into X) and computing the outcome for some \"x\". Formally: \n\nExamining a counterfactual using a causal model involves three steps. The approach is valid regardless of the form of the model relationships (linear or otherwise) When the model relationships are fully specified, point values can be computed. In other cases, (e.g., when only probabilities are available) a probability-interval statement (non-smoker x would have a 10-20% chance of cancer) can be computed. \n\nGiven the model:\n\nthe equations for calculating the values of A and C derived from regression analysis or another technique can be applied, substituting known values from an observation and fixing the value of other variables (the counterfactual). \n\nApply abductive reasoning (logical inference that uses observation to find the simplest/most likely explanation) to estimate \"u\", the proxy for the unobserved variables on the specific observation that supports the counterfactual. \n\nFor a specific observation, use the do operator to establish the counterfactual (e.g., \"m\"=0), modifying the equations accordingly. \n\nCalculate the values of the output (\"y\") using the modified equations. \n\nDirect and indirect (mediated) causes can only be distinguished via conducting counterfactuals. Understanding mediation requires holding the mediator constant while intervening on the direct cause. In the model\n\nformula_28\n\nM mediates X's influence on Y, while X also has an unmediated effect on Y. Thus M is held constant, while do(X) is computed.\n\nThe Mediation Fallacy instead involves conditioning on the mediator if the mediator and the outcome are confounded, as they are in the above model.\n\nFor linear models, the indirect effect can be computed by taking the product of all the path coefficients along a mediated pathway. The total indirect effect is computed by the sum of the individual indirect effects. For linear models mediation is indicated when the coefficients of an equation fitted without including the mediator vary significantly from an equation that includes it. \n\nIn experiments on such a model, the controlled direct effect (CDE) is computed by forcing the value of the mediator M (do(M = 0)) and randomly assigning some subjects to each of the values of X (do(X=0), do(X=1), ...) and observing the resulting values of Y. \n\nEach value of the mediator has a corresponding CDE.\n\nHowever, a better experiment is to compute the natural direct effect. (NDE) This is the effect determined by leaving the relationship between X and M untouched while intervening on the relationship between X and Y. \n\nFor example, consider the direct effect of increasing dental hygenist visits (X) from every other year to every year, which encourages flossing (M). Gums (Y) get healthier, either because of the hygenist (direct) or the flossing (mediator/indirect). The experiment is to continue flossing while skipping the hygenist visit.\n\nThe indirect effect of X on Y is the \"increase we would see in Y while holding X constant and increasing M to whatever value M would attain under a unit increase in X\". \n\nIndirect effects cannot be \"controlled\" because the direct path cannot be disabled by holding another variable constant. The natural indirect effect (NIE) is the effect on gum health (Y) from flossing (M). The NIE is calculated as the sum of (floss and no-floss cases) of the difference between the probability of flossing given the hygenist and without the hygenist, or: \n\nThe above NDE calculation includes counterfactual subscripts (formula_32). For nonlinear models, the seemingly obvious equivalence \n\ndoes not apply because of anomalies such as threshold effects and binary values. However, \n\nworks for all model relationships (linear and nonlinear). It allows NDE to then be calculated directly from observational data, without interventions or use of counterfactual subscripts. \n\nCausal models provide a vehicle for integrating data across datasets, known as transport, even though the causal models (and the associated data) differ. E.g., survey data can be merged with randomized, controlled trial data. Transport offers a solution to the question of external validity, whether a study can be applied in a different context.\n\nWhere two models match on all relevant variables and data from one model is known to be unbiased, data from one population can be used to draw conclusions about the other. In other cases, where data is known to be biased, reweighting can allow the dataset to be transported. In a third case, conclusions can be drawn from an incomplete dataset. In some cases, data from studies of multiple populations can be combined (via transportation) to allow conclusions about an unmeasured population. In some cases, combining estimates (e.g., P(W|X)) from multiple studies can increase the precision of a conclusion. \n\nDo-calculus provides a general criterion for transport: A target variable can be transformed into another expression via a series of do-operations that does not involve any \"difference-producing\" variables (those that distinguish the two populations). An analogous rule applies to studies that have relevantly different participants. \n\nAny causal model can be implemented as a Bayesian network. Bayesian networks can be used to provide the inverse probability of an event (given an outcome, what are the probabilities of a specific cause). This requires preparation of a conditional probability table, showing all possible inputs and outcomes with their associated probabilities. \n\nFor example, given a two variable model of Disease and Test (for the disease) the conditional probability table takes the form: \nAccording to this table, when a patient does not have the disease, the probability of a positive test is 12%.\n\nWhile this is tractable for small problems, as the number of variables and their associated states increase, the probability table (and associated computation time) increases exponentially. \n\nBayesian networks are used commercially in applications such as wireless data error correction and DNA analysis. \n\n\n"}
{"id": "5653", "url": "https://en.wikipedia.org/wiki?curid=5653", "title": "Clarke's three laws", "text": "Clarke's three laws\n\nBritish science fiction writer Arthur C. Clarke formulated three adages that are known as Clarke's three laws, of which the third law is the best known and most widely cited. They were part of his ideas in his extensive writings about the future. These so-called laws include:\n\n\nOne account claimed that Clarke's \"laws\" were developed after the editor of his works in French started numbering the author's assertions. All three laws appear in Clarke's essay \"Hazards of Prophecy: The Failure of Imagination\", first published in \"Profiles of the Future\" (1962). However, they were not published at the same time. Clarke's first law was proposed in the 1962 edition of the essay, as \"Clarke's Law\" in \"Profiles of the Future\".\n\nThe second law is offered as a simple observation in the same essay but its status as Clarke's second law was conferred by others. It was initially a derivative of the first law and formally became Clarke's second law where the author proposed the third law in the 1973 revision of \"Profiles of the Future,\" which included an acknowledgement\".\" It was also here that Clarke wrote about the third law in these words: \"As three laws were good enough for Newton, I have modestly decided to stop there\".\n\nThe third law, despite being latest stated by a decade, is the best known and most widely cited. It appears only in the 1973 revision of the \"Hazards of Prophecy\" essay. It echoes a statement in a 1942 story by Leigh Brackett: \"Witchcraft to the ignorant, … simple science to the learned\". Earlier examples of this sentiment may be found in \"Wild Talents\" (1932) by Charles Fort: \"...a performance that may some day be considered understandable, but that, in these primitive times, so transcends what is said to be the known that it is what I mean by magic,\" and in the short story \"The Hound of Death\" (1933) by Agatha Christie: \"The supernatural is only the natural of which the laws are not yet understood.\"\n\nClarke gave an example of the third law when he said that while he \"would have believed anyone who told him back in 1962 that there would one day exist a book-sized object capable of holding the content of an entire library, he would never have accepted that the same device could find a page or word in a second and then convert it into any typeface and size from Albertus Extra Bold to Zurich Calligraphic\", referring to his memory of \"seeing and hearing Linotype machines which slowly converted ‘molten lead into front pages that required two men to lift them’\".\n\nA fourth law has been proposed for the canon, despite Clarke's declared intention of stopping at three laws. Geoff Holder quotes: \"For every expert, there is an equal and opposite expert,\" which is part of American economist Thomas Sowell's \"For every expert, there is an equal and opposite expert, but for every fact there is not necessarily an equal and opposite fact\", from his 1995 book \"The Vision of the Anointed\".\n\nThe third law has inspired many snowclones and other variations:\n\n\nA of the third law is\n\n\nThe third law has been:\n\n\n\n"}
{"id": "698226", "url": "https://en.wikipedia.org/wiki?curid=698226", "title": "Concept map", "text": "Concept map\n\nA concept map or conceptual diagram is a diagram that depicts suggested relationships between concepts. It is a graphical tool that instructional designers, engineers, technical writers, and others use to organize and structure knowledge.\n\nA concept map typically represents ideas and information as boxes or circles, which it connects with labeled arrows in a downward-branching hierarchical structure. The relationship between concepts can be articulated in linking phrases such as \"causes\", \"requires\", or \"contributes to\".\n\nThe technique for visualizing these relationships among different concepts is called \"concept mapping\". Concept maps have been used to define the ontology of computer systems, for example with the object-role modeling or Unified Modeling Language formalism.\n\nA concept map is a way of representing relationships between ideas, images, or words in the same way that a sentence diagram represents the grammar of a sentence, a road map represents the locations of highways and towns, and a circuit diagram represents the workings of an electrical appliance. In a concept map, each word or phrase connects to another, and links back to the original idea, word, or phrase. Concept maps are a way to develop logical thinking and study skills by revealing connections and helping students see how individual ideas form a larger whole. An example of the use of concept maps is provided in the context of learning about types of fuel.\n\nConcept maps were developed to enhance meaningful learning in the sciences. A well-made concept map grows within a \"context frame\" defined by an explicit \"focus question\", while a mind map often has only branches radiating out from a central picture. Some research evidence suggests that the brain stores knowledge as productions (situation-response conditionals) that act on declarative memory content, which is also referred to as chunks or propositions. Because concept maps are constructed to reflect organization of the declarative memory system, they facilitate sense-making and meaningful learning on the part of individuals who make concept maps and those who use them.\n\n\nConcept mapping was developed by Joseph D. Novak and his research team at Cornell University in the 1970s as a means of representing the emerging science knowledge of students. It has subsequently been used as a tool to increase meaningful learning in the sciences and other subjects as well as to represent the expert knowledge of individuals and teams in education, government and business. Concept maps have their origin in the learning movement called constructivism. In particular, constructivists hold that learners actively construct knowledge.\n\nNovak's work is based on the cognitive theories of David Ausubel, who stressed the importance of prior knowledge in being able to learn (or \"assimilate\") new concepts: \"The most important single factor influencing learning is what the learner already knows. Ascertain this and teach accordingly.\" Novak taught students as young as six years old to make concept maps to represent their response to focus questions such as \"What is water?\" \"What causes the seasons?\" In his book \"Learning How to Learn\", Novak states that a \"meaningful learning involves the assimilation of new concepts and propositions into existing cognitive structures.\"\n\nVarious attempts have been made to conceptualize the process of creating concept maps. Ray McAleese, in a series of articles, has suggested that mapping is a process of \"off-loading\". In this 1998 paper, McAleese draws on the work of Sowa and a paper by Sweller & Chandler. In essence, McAleese suggests that the process of making knowledge explicit, using \"nodes\" and \"relationships\", allows the individual to become aware of what they know and as a result to be able to modify what they know. Maria Birbili applies that same idea to helping young children learn to think about what they know. The concept of the \"knowledge arena\" is suggestive of a virtual space where learners may explore what they know and what they do not know.\n\nConcept maps are used to stimulate the generation of ideas, and are believed to aid creativity. Concept mapping is also sometimes used for brain-storming. Although they are often personalized and idiosyncratic, concept maps can be used to communicate complex ideas.\n\nFormalized concept maps are used in software design, where a common usage is Unified Modeling Language diagramming amongst similar conventions and development methodologies.\n\nConcept mapping can also be seen as a first step in ontology-building, and can also be used flexibly to represent formal argument — similar to argument maps.\n\nConcept maps are widely used in education and business. Uses include:\n\n"}
{"id": "3699685", "url": "https://en.wikipedia.org/wiki?curid=3699685", "title": "Conceptual architecture", "text": "Conceptual architecture\n\nConceptual architecture is a form of architecture that utilizes conceptualism, characterized by an introduction of ideas or concepts from outside of architecture often as a means of expanding the discipline of architecture. This produces an essentially different kind of building than one produced by the widely held 'architect as a master-builder' model, in which craft and construction are the guiding principles. The finished building as product is less important in conceptual architecture, than the ideas guiding them, ideas represented primarily by texts, diagrams, or art installations. Architects that work in this vein are Diller + Scofidio, Bernard Tschumi, Peter Eisenman and Rem Koolhaas.\n\nConceptual architecture was studied in the essay, \"Notes on Conceptual Architecture: Towards a Definition\" by Peter Eisenman in 1970, and again by the Harvard Design Magazine in Fall of 2003 and Winter 2004, by a series of articles under the heading \"Architecture as Conceptual Art\". But the understanding of design as a construction of a concept was understood by many modernist architects as well. To quote Louis Kahn on Frank Lloyd Wright:\n\n\n"}
{"id": "33346439", "url": "https://en.wikipedia.org/wiki?curid=33346439", "title": "Conceptual design", "text": "Conceptual design\n\nConceptual Design is an early phase of the design process, in which the broad outlines of function and form of something are articulated. It includes the design of interactions, experiences, processes and strategies. It involves an understanding of people's needs - and how to meet them with products, services, & processes. Common artifacts of conceptual design are concept sketches and models.\n\n"}
{"id": "47792266", "url": "https://en.wikipedia.org/wiki?curid=47792266", "title": "Construction of Concept Map", "text": "Construction of Concept Map\n\nConcept is usually perceived as a regularity in events or objects or in their records. While constructing a concept map, it is essential to keep in mind that the concept be built with reference to a focus question. Hence, initially, the focus question to which we seek to answer is carefully chosen because learners usually tend to deviate from this question relating only to domains and thus, fails to answer the question.\n\nWith the selected domain and the focus question, the next step is to identify the key concepts which apply to this domain.About 15 to 25 concepts is sufficient which is usually ordered in a rank ordered list. Such list should be established from the most general and inclusive concept for the particular chosen problem. This list will assist in at least in the beginning of the construction of the concept map. The list is referred to as Parking Lot since the list is constantly shuffled to build up the required network of the concept. Two or more concepts are connected to each other using linking words or phrases which can only then complete a meaningful sentence.Another important characteristics of a concept map is the cross-links. This cross link acts as the relationship between two different domains used in the concept map.This helps in a clear representation of the knowledge contained in the concept and also gives a clear background with specific examples. \n"}
{"id": "1745389", "url": "https://en.wikipedia.org/wiki?curid=1745389", "title": "Cooperative principle", "text": "Cooperative principle\n\nIn social science generally and linguistics specifically, the cooperative principle describes how effective communication in conversation is achieved in common social situations, that is, how listeners and speakers must act cooperatively and mutually accept one another to be understood in a particular way. As phrased by Paul Grice, who introduced it, \"Make your contribution such as is required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which you are engaged.\" Though phrased as a prescriptive command, the principle is intended as a description of how people normally behave in conversation. Jeffries and McIntyre describe Grice's maxims as \"encapsulating the assumptions that we prototypically hold when we engage in conversation\".\n\nThe cooperative principle can be divided into four maxims, called the Gricean maxims, describing specific rational principles observed by people who obey the cooperative principle; these principles enable effective communication. Grice proposed four conversational maxims that arise from the pragmatics of natural language. Applying the Gricean maxims is a way to explain the link between utterances and what is understood from them.\n\n\n\n\nWith respect to this maxim, Grice writes, \"Though the maxim itself is terse, its formulation conceals a number of problems that exercise me a good deal: questions about what different kinds and focuses of relevance there may be, how these shift in the course of a talk exchange, how to allow for the fact that subjects of conversations are legitimately changed, and so on. I find the treatment of such questions exceedingly difficult, and I hope to revert to them in later work.\"\n\n\n\nThese maxims may also be understood as describing the assumptions listeners normally make about the way speakers will talk, rather than prescriptions for how one ought to talk. Philosopher Kent Bach writes:\n\n...[W]e need first to get clear on the character of Grice's maxims. They are not sociological generalizations about speech, nor they are moral prescriptions or proscriptions on what to say or communicate. Although Grice presented them in the form of guidelines for how to communicate successfully, I think they are better construed as presumptions about utterances, presumptions that we as listeners rely on and as speakers exploit (Bach 2005).\n\nGricean maxims generate implicatures. If the overt, surface meaning of a sentence does not seem to be consistent with the Gricean maxims, and yet the circumstances lead us to think that the speaker is nonetheless obeying the cooperative principle, we tend to look for other meanings that could be implied by the sentence.\n\nGrice did not, however, assume that all people should constantly follow these maxims. Instead, he found it interesting when these were not respected, namely either \"flouted\" (with the listener being expected to be able to understand the message) or \"violated\" (with the listener being expected to not note this). Flouting would imply some other, hidden meaning. The importance was in what was \"not\" said. For example, answering \"It's raining\" to someone who has suggested playing a game of tennis only disrespects the maxim of relation on the surface; the reasoning behind this \"fragment\" sentence is normally clear to the interlocutor (the maxim is just \"flouted\").\n\nIt is possible to flout a maxim and thereby convey a different meaning than what is literally spoken. Many times in conversation, this flouting is manipulated by a speaker to produce a negative pragmatic effect, as with sarcasm or irony. One can flout the maxim of quality to tell a clumsy friend who has just taken a bad fall that her gracefulness is impressive and obviously intend to mean the complete opposite. Likewise, flouting the maxim of quantity may result in ironic understatement, the maxim of relevance in blame by irrelevant praise, and the maxim of manner in ironic ambiguity. The Gricean maxims are therefore often purposefully flouted by comedians and writers, who may hide the complete truth and manipulate their words for the effect of the story and the sake of the reader's experience.\n\nGrice's theory is often disputed by arguing that cooperative conversation, like most social behaviour, is culturally determined, and therefore the Gricean maxims and the cooperative principle cannot be universally applied due to intercultural differences. Keenan claims that the Malagasy, for example, follow a completely opposite cooperative principle in order to achieve conversational cooperation. In their culture, speakers are reluctant to share information and flout the maxim of quantity by evading direct questions and replying on incomplete answers because of the risk of losing face by committing oneself to the truth of the information, as well as the fact that having information is a form of prestige. However, Harnish points out that Grice only claims his maxims hold in conversations where his cooperative principle is in effect. The Malagasy speakers choose not to be cooperative, valuing the prestige of information ownership more highly. \n\nAnother criticism is that the Gricean maxims can easily be misinterpreted to be a guideline for etiquette, instructing speakers on how to be moral, polite conversationalists. However, the Gricean maxims, despite their wording, are only meant to describe the commonly accepted traits of successful cooperative communication. Geoffrey Leech introduced the politeness maxims: tact, generosity, approbation, modesty, agreement, and sympathy.\n\n\n\n"}
{"id": "1845675", "url": "https://en.wikipedia.org/wiki?curid=1845675", "title": "Deductive-nomological model", "text": "Deductive-nomological model\n\nThe deductive-nomological model (DN model), also known as Hempel's model, the Hempel–Oppenheim model, the Popper–Hempel model, or the covering law model, is a formal view of scientifically answering questions asking, \"Why...?\". The DN model poses scientific explanation as a deductive structure—that is, one where truth of its premises entails truth of its conclusion—hinged on accurate prediction or postdiction of the phenomenon to be explained.\n\nBecause of problems concerning humans' ability to define, discover, and know causality, it was omitted in initial formulations of the DN model. Causality was thought to be incidentally approximated by realistic selection of premises that \"derive\" the phenomenon of interest from observed starting conditions plus general laws. Still, DN model formally permitted causally irrelevant factors. Also, derivability from observations and laws sometimes yielded absurd answers.\n\nWhen logical empiricism fell out of favor in the 1960s, the DN model was widely seen as a flawed or greatly incomplete model of scientific explanation. Nonetheless, it remained an idealized version of scientific explanation, and one that was rather accurate when applied to modern physics. In the early 1980s, revision to DN model emphasized \"maximal specificity\" for relevance of the conditions and axioms stated. Together with Hempel's inductive-statistical model, the DN model forms scientific explanation's covering law model, which is also termed, from critical angle, subsumption theory.\n\nThe term \"deductive\" distinguishes the DN model's intended determinism from the probabilism of inductive inferences. The term \"nomological\" is derived from the Greek word \"νόμος\" or \"nomos\", meaning \"law\". The DN model holds to a view of scientific explanation whose \"conditions of adequacy\" (CA)—semiformal but stated classically—are \"derivability\" (CA1), \"lawlikeness\" (CA2), \"empirical content\" (CA3), and \"truth\" (CA4).\n\nIn the DN model, a law axiomatizes an unrestricted generalization from antecedent \"A\" to consequent \"B\" by conditional proposition—\"If A, then B\"—and has empirical content testable. A law differs from mere true regularity—for instance, \"George always carries only $1 bills in his wallet\"—by supporting counterfactual claims and thus suggesting what \"must\" be true, while following from a scientific theory's axiomatic structure.\n\nThe phenomenon to be explained is the explanandum—an event, law, or theory—whereas the premises to explain it are explanans, true or highly confirmed, containing at least one universal law, and entailing the explanandum. Thus, given the explanans as initial, specific conditions \"C, C . . . C\" plus general laws \"L, L . . . L\", the phenomenon \"E\" as explanandum is a deductive consequence, thereby scientifically explained.\n\nAristotle's scientific explanation in \"Physics\" resembles the DN model, an idealized form of scientific explanation. The framework of Aristotelian physics—Aristotelian metaphysics—reflected the perspective of this principally biologist, who, amid living entities' undeniable purposiveness, formalized vitalism and teleology, an intrinsic morality in nature. With emergence of Copernicanism, however, Descartes introduced mechanical philosophy, then Newton rigorously posed lawlike explanation, both Descartes and especially Newton shunning teleology within natural philosophy. At 1740, David Hume staked Hume's fork, highlighted the problem of induction, and found humans ignorant of either necessary or sufficient causality. Hume also highlighted the fact/value gap, as what \"is\" does not itself reveal what \"ought\".\n\nNear 1780, countering Hume's ostensibly radical empiricism, Immanuel Kant highlighted extreme rationalism—as by Descartes or Spinoza—and sought middle ground. Inferring the mind to arrange experience of the world into \"substance\", \"space\", and \"time\", Kant placed the mind as part of the causal constellation of experience and thereby found Newton's theory of motion universally true, yet knowledge of things in themselves impossible. Safeguarding science, then, Kant paradoxically stripped it of scientific realism. Aborting Francis Bacon's inductivist mission to dissolve the veil of appearance to uncover the \"noumena\"—metaphysical view of nature's ultimate truths—Kant's transcendental idealism tasked science with simply modeling patterns of \"phenomena\". Safeguarding metaphysics, too, it found the mind's constants holding also universal moral truths, and launched German idealism, increasingly speculative.\n\nAuguste Comte found the problem of induction rather irrelevant since enumerative induction is grounded on the empiricism available, while science's point is not metaphysical truth. Comte found human knowledge had evolved from theological to metaphysical to scientific—the ultimate stage—rejecting both theology and metaphysics as asking questions unanswerable and posing answers unverifiable. Comte in the 1830s expounded positivism—the first modern philosophy of science and simultaneously a political philosophy—rejecting conjectures about unobservables, thus rejecting search for \"causes\". Positivism predicts observations, confirms the predictions, and states a \"law\", thereupon applied to benefit human society. From late 19th century into the early 20th century, the influence of positivism spanned the globe. Meanwhile, evolutionary theory's natural selection brought the Copernican Revolution into biology and eventuated in the first conceptual alternative to vitalism and teleology.\n\nWhereas Comtean positivism posed science as \"description\", logical positivism emerged in the late 1920s and posed science as \"explanation\", perhaps to better unify empirical sciences by covering not only fundamental science—that is, fundamental physics—but special sciences, too, such as biology, psychology, economics, and anthropology. After defeat of National Socialism with World War II's close in 1945, logical positivism shifted to a milder variant, \"logical empiricism\". All variants of the movement, which lasted until 1965, are neopositivism, sharing the quest of verificationism.\n\nNeopositivists led emergence of the philosophy subdiscipline philosophy of science, researching such questions and aspects of scientific theory and knowledge. Scientific realism takes scientific theory's statements at face value, thus accorded either falsity or truth—probable or approximate or actual. Neopositivists held scientific antirealism as instrumentalism, holding scientific theory as simply a device to predict observations and their course, while statements on nature's unobservable aspects are elliptical at or metaphorical of its observable aspects, rather.\n\nDN model received its most detailed, influential statement by Carl G Hempel, first in his 1942 article \"The function of general laws in history\", and more explicitly with Paul Oppenheim in their 1948 article \"Studies in the logic of explanation\". Leading logical empiricist, Hempel embraced the Humean empiricist view that humans observe sequence of sensory events, not cause and effect, as causal relations and casual mechanisms are unobservables. DN model bypasses causality beyond mere constant conjunction: first an event like \"A\", then always an event like \"B\".\n\nHempel held natural laws—empirically confirmed regularities—as satisfactory, and if included realistically to approximate causality. In later articles, Hempel defended DN model and proposed probabilistic explanation by \"inductive-statistical model\" (IS model). DN model and IS model—whereby the probability must be high, such as at least 50%—together form \"covering law model\", as named by a critic, William Dray. Derivation of statistical laws from other statistical laws goes to the \"deductive-statistical model\" (DS model). Georg Henrik von Wright, another critic, named the totality \"subsumption theory\".\n\nAmid failure of neopositivism's fundamental tenets, Hempel in 1965 abandoned verificationism, signaling neopositivism's demise. From 1930 onward, Karl Popper had refuted any positivism by asserting falsificationism, which Popper claimed had killed positivism, although, paradoxically, Popper was commonly mistaken for a positivist. Even Popper's 1934 book embraces DN model, widely accepted as the model of scientific explanation for as long as physics remained the model of science examined by philosophers of science.\n\nIn the 1940s, filling the vast observational gap between cytology and biochemistry, cell biology arose and established existence of cell organelles besides the nucleus. Launched in the late 1930s, the molecular biology research program cracked a genetic code in the early 1960s and then converged with cell biology as \"cell and molecular biology\", its breakthroughs and discoveries defying DN model by arriving in quest not of lawlike explanation but of causal mechanisms. Biology became a new model of science, while special sciences were no longer thought defective by lacking universal laws, as borne by physics.\n\nIn 1948, when explicating DN model and stating scientific explanation's semiformal \"conditions of adequacy\", Hempel and Oppenheim acknowledged redundancy of the third, \"empirical content\", implied by the other three—\"derivability\", \"lawlikeness\", and \"truth\". In the early 1980s, upon widespread view that causality ensures the explanans' relevance, Wesley Salmon called for returning \"cause\" to \"because\", and along with James Fetzer helped replace CA3 \"empirical content\" with CA3' \"strict maximal specificity\".\n\nSalmon introduced \"causal mechanical\" explanation, never clarifying how it proceeds, yet reviving philosophers' interest in such. Via shortcomings of Hempel's inductive-statistical model (IS model), Salmon introduced \"statistical-relevance model\" (SR model). Although DN model remained an idealized form of scientific explanation, especially in applied sciences, most philosophers of science consider DN model flawed by excluding many types of explanations generally accepted as scientific.\n\nAs theory of knowledge, epistemology differs from ontology, which is a subbranch of metaphysics, theory of reality. Ontology poses which categories of being—what sorts of things exist—and so, although a scientific theory's ontological commitment can be modified in light of experience, an ontological commitment inevitably precedes empirical inquiry.\n\nNatural laws, so called, are statements of humans' observations, thus are epistemological—concerning human knowledge—the \"epistemic\". Causal mechanisms and structures existing putatively independently of minds exist, or would exist, in the natural world's structure itself, and thus are ontological, the \"ontic\". Blurring epistemic with ontic—as by incautiously presuming a natural law to refer to a causal mechanism, or to trace structures realistically during unobserved transitions, or to be true regularities always unvarying—tends to generate a \"category mistake\".\n\nDiscarding ontic commitments, including causality \"per se\", DN model permits a theory's laws to be reduced to—that is, subsumed by—a more fundamental theory's laws. The higher theory's laws are explained in DN model by the lower theory's laws. Thus, the epistemic success of Newtonian theory's law of universal gravitation is reduced to—thus explained by—Einstein's general theory of relativity, although Einstein's discards Newton's ontic claim that universal gravitation's epistemic success predicting Kepler's laws of planetary motion is through a causal mechanism of a straightly attractive force instantly traversing absolute space despite absolute time.\n\nCovering law model reflects neopositivism's vision of empirical science, a vision interpreting or presuming unity of science, whereby all empirical sciences are either fundamental science—that is, fundamental physics—or are special sciences, whether astrophysics, chemistry, biology, geology, psychology, economics, and so on. All special sciences would network via covering law model. And by stating \"boundary conditions\" while supplying \"bridge laws\", any special law would reduce to a lower special law, ultimately reducing—theoretically although generally not practically—to fundamental science. (\"Boundary conditions\" are specified conditions whereby the phenomena of interest occur. \"Bridge laws\" translate terms in one science to terms in another science.)\n\nBy DN model, if one asks, \"Why is that shadow 20 feet long?\", another can answer, \"Because that flagpole is 15 feet tall, the Sun is at \"x\" angle, and laws of electromagnetism\". Yet by problem of symmetry, if one instead asked, \"Why is that flagpole 15 feet tall?\", another could answer, \"Because that shadow is 20 feet long, the Sun is at \"x\" angle, and laws of electromagnetism\", likewise a deduction from observed conditions and scientific laws, but an answer clearly incorrect. By the problem of irrelevance, if one asks, \"Why did that man not get pregnant?\", one could in part answer, among the explanans, \"Because he took birth control pills\"—if he factually took them, and the law of their preventing pregnancy—as covering law model poses no restriction to bar that observation from the explanans.\n\nMany philosophers have concluded that causality is integral to scientific explanation. DN model offers a necessary condition of a causal explanation—successful prediction—but not sufficient conditions of causal explanation, as a universal regularity can include spurious relations or simple correlations, for instance \"Z\" always following \"Y\", but not \"Z\" because of \"Y\", instead \"Y\" and then \"Z\" as an effect of \"X\". By relating temperature, pressure, and volume of gas within a container, Boyle's law permits prediction of an unknown variable—volume, pressure, or temperature—but does not explain \"why\" to expect that unless one adds, perhaps, the kinetic theory of gases.\n\nScientific explanations increasingly pose not determinism's universal laws, but probabilism's chance, \"ceteris paribus\" laws. Smoking's contribution to lung cancer fails even the inductive-statistical model (IS model), requiring probability over 0.5 (50%). (Probability standardly ranges from 0 (0%) to 1 (100%).) An applied science that applies statistics seeking associations between events, epidemiology cannot show causality, but consistently found higher incidence of lung cancer in smokers versus otherwise similar nonsmokers, although the proportion of smokers who develop lung cancer is modest. Versus nonsmokers, however, smokers as a group showed over 20 times the risk of lung cancer, and in conjunction with basic research, consensus followed that smoking had been scientifically explained as \"a\" cause of lung cancer, responsible for some cases that without smoking would not have occurred, a probabilistic counterfactual causality.\n\nThrough lawlike explanation, fundamental physics—often perceived as fundamental science—has proceeded through intertheory relation and theory reduction, thereby resolving experimental paradoxes to great historical success, resembling covering law model. In early 20th century, Ernst Mach as well as Wilhelm Ostwald had resisted Ludwig Boltzmann's reduction of thermodynamics—and thereby Boyle's law—to statistical mechanics partly \"because\" it rested on kinetic theory of gas, hinging on atomic/molecular theory of matter. Mach as well as Ostwald viewed matter as a variant of energy, and molecules as mathematical illusions, as even Boltzmann thought possible.\n\nIn 1905, via statistical mechanics, Albert Einstein predicted the phenomenon Brownian motion—unexplained since reported in 1827 by botanist Robert Brown. Soon, most physicists accepted that atoms and molecules were unobservable yet real. Also in 1905, Einstein explained the electromagnetic field's energy as distributed in \"particles\", doubted until this helped resolve atomic theory in the 1910s and 1920s. Meanwhile, all known physical phenomena were gravitational or electromagnetic, whose two theories misaligned. Yet belief in aether as the source of all physical phenomena was virtually unanimous. At experimental paradoxes, physicists modified the aether's hypothetical properties.\n\nFinding the luminiferous aether a useless hypothesis, Einstein in 1905 \"a priori\" unified all inertial reference frames to state special \"principle\" of relativity, which, by omitting aether, converted space and time into \"relative\" phenomena whose relativity aligned electrodynamics with the Newtonian principle Galilean relativity or invariance. Originally epistemic or instrumental, this was interpreted as ontic or realist—that is, a causal mechanical explanation—and the \"principle\" became a \"theory\", refuting Newtonian gravitation. By predictive success in 1919, general relativity apparently overthrew Newton's theory, a revolution in science resisted by many yet fulfilled around 1930.\n\nIn 1925, Werner Heisenberg as well as Erwin Schrödinger independently formalized quantum mechanics (QM). Despite clashing explanations, the two theories made identical predictions. Paul Dirac's 1928 model of the electron was set to special relativity, launching QM into the first quantum field theory (QFT), quantum electrodynamics (QED). From it, Dirac interpreted and predicted the electron's antiparticle, soon discovered and termed \"positron\", but the QED failed electrodynamics at high energies. Elsewhere and otherwise, strong nuclear force and weak nuclear force were discovered.\n\nIn 1941, Richard Feynman introduced QM's path integral formalism, which if taken toward \"interpretation\" as a causal mechanical model clashes with Heisenberg's matrix formalism and with Schrödinger's wave formalism, although all three are empirically identical, sharing predictions. Next, working on QED, Feynman sought to model particles without fields and find the vacuum truly empty. As each known fundamental force is apparently an effect of a field, Feynman failed. Louis de Broglie's waveparticle duality had rendered atomism—indivisible particles in a void—untenable, and highlighted the very notion of discontinuous particles as selfcontradictory.\n\nMeeting in 1947, Freeman Dyson, Richard Feynman, Julian Schwinger, and Sin-Itiro Tomonaga soon introduced \"renormalization\", a procedure converting QED to physics' most predictively precise theory, subsuming chemistry, optics, and statistical mechanics. QED thus won physicists' general acceptance. Paul Dirac criticized its need for renormalization as showing its unnaturalness, and called for an aether. In 1947, Willis Lamb had found unexpected motion of electron orbitals, shifted since the vacuum is not truly empty. Yet \"emptiness\" was catchy, abolishing aether conceptually, and physics proceeded ostensibly without it, even suppressing it. Meanwhile, \"sickened by untidy math, most philosophers of physics tend to neglect QED\".\n\nPhysicists have feared even mentioning \"aether\", renamed \"vacuum\", which—as such—is nonexistent. General philosophers of science commonly believe that aether, rather, is fictitious, \"relegated to the dustbin of scientific history ever since\" 1905 brought special relativity. Einstein was noncommittal to aether's nonexistence, simply said it superfluous. Abolishing Newtonian motion for electrodynamic primacy, however, Einstein inadvertently reinforced aether, and to explain motion was led back to aether in general relativity. Yet resistance to relativity theory became associated with earlier theories of aether, whose word and concept became taboo. Einstein explained special relativity's compatibility with an aether, but Einstein aether, too, was opposed. Objects became conceived as pinned directly on space and time by abstract geometric relations lacking ghostly or fluid medium.\n\nBy 1970, QED along with weak nuclear field was reduced to electroweak theory (EWT), and the strong nuclear field was modeled as quantum chromodynamics (QCD). Comprised by EWT, QCD, and Higgs field, this Standard Model of particle physics is an \"effective theory\", not truly fundamental. As QCD's particles are considered nonexistent in the everyday world, QCD especially suggests an aether, routinely found by physics experiments to exist and to exhibit relativistic symmetry. Confirmation of the Higgs particle, modeled as a condensation within the Higgs field, corroborates aether, although physics need not state or even include aether. Organizing regularities of \"observations\"—as in the covering law model—physicists find superfluous the quest to discover \"aether\".\n\nIn 1905, from special relativity, Einstein deduced mass–energy equivalence, particles being variant forms of distributed energy, how particles colliding at vast speed experience that energy's transformation into mass, producing heavier particles, although physicists' talk promotes confusion. As \"the contemporary locus of metaphysical research\", QFTs pose particles not as existing individually, yet as \"excitation modes\" of fields, the particles and their masses being states of aether, apparently unifying all physical phenomena as the more fundamental causal reality, as long ago foreseen. Yet a \"quantum\" field is an intricate abstraction—a \"mathematical\" field—virtually inconceivable as a \"classical\" field's physical properties. Nature's deeper aspects, still unknown, might elude any possible field theory.\n\nThough discovery of causality is popularly thought science's aim, search for it was shunned by the Newtonian research program, even more Newtonian than was Isaac Newton. By now, most theoretical physicists infer that the four, known fundamental interactions would reduce to superstring theory, whereby atoms and molecules, after all, are energy vibrations holding mathematical, geometric forms. Given uncertainties of scientific realism, some conclude that the concept \"causality\" raises comprehensibility of scientific explanation and thus is key folk science, but compromises precision of scientific explanation and is dropped as a science matures. Even epidemiology is maturing to heed the severe difficulties with presumptions about causality. Covering law model is among Carl G Hempel's admired contributions to philosophy of science.\n\nTypes of inference\n\nRelated subjects\n\n\n"}
{"id": "26167139", "url": "https://en.wikipedia.org/wiki?curid=26167139", "title": "Definitionism", "text": "Definitionism\n\nDefinitionism (also called the classical theory of concepts) is the school of thought in which it is believed that a proper explanation of a theory consists of all the concepts used by that theory being well-defined. This approach has been criticized for its dismissal of the importance of ostensive definitions.\n"}
{"id": "152902", "url": "https://en.wikipedia.org/wiki?curid=152902", "title": "Dormant Commerce Clause", "text": "Dormant Commerce Clause\n\nThe Dormant Commerce Clause, or Negative Commerce Clause, in American constitutional law, is a legal doctrine that courts in the United States have inferred from the Commerce Clause in Article I of the US Constitution. The Dormant Commerce Clause is used to prohibit state legislation that discriminates against interstate or international commerce.\n\nFor example, it is lawful for Michigan to require food labels that specifically identify certain animal parts, if they are present in the product, because the state law applies to food produced in Michigan as well as food imported from other states and foreign countries; the state law would violate the Commerce Clause if it applied only to imported food or if it was otherwise found to favor domestic over imported products. Likewise, California law requires milk sold to contain a certain percentage of milk solids that federal law does not require, which is allowed under the Dormant Commerce Clause doctrine because California's stricter requirements apply equally to California-produced milk and imported milk and so does not discriminate against or inappropriately burden interstate commerce.\n\nThe idea that regulation of interstate commerce may to some extent be an exclusive Federal power was discussed even before adoption of the Constitution, but the framers did not use the word \"dormant\". On September 15, 1787, the Framers of the Constitution debated in Philadelphia whether to guarantee states the ability to lay duties of tonnage without Congressional interference so that the states could finance the clearing of harbors and the building of lighthouses. James Madison believed that the mere existence of the Commerce Clause would bar states from imposing any duty of tonnage: \"He was more and more convinced that the regulation of Commerce was in its nature indivisible and ought to be wholly under one authority.\"\n\nRoger Sherman disagreed: \"The power of the United States to regulate trade being supreme can control interferences of the State regulations when such interferences happen; so that there is no danger to be apprehended from a concurrent jurisdiction.\" Sherman saw the commerce power as similar to the tax power, the latter being one of the concurrent powers shared by the federal and state governments. Ultimately, the Philadelphia Convention decided upon the present language about duties of tonnage in , which says: \"No state shall, without the consent of Congress, lay any duty of tonnage ...\"\n\nThe word \"dormant,\" in connection with the Commerce Clause, originated in dicta of Chief Justice John Marshall. For example, in the case of \"Gibbons v. Ogden\", , he wrote that the power to regulate interstate commerce \"can never be exercised by the people themselves, but must be placed in the hands of agents, or lie dormant.\" Concurring Justice William Johnson was even more emphatic that the Constitution is \"altogether in favor of the exclusive grants to Congress of power over commerce.\"\n\nLater, in the case of \"Willson v. Black-Bird Creek Marsh Co.\", , Marshall wrote: \"We do not think that the [state] act empowering the Black Bird Creek Marsh Company to place a dam across the creek, can, under all the circumstances of the case, be considered as repugnant to the power to regulate commerce in its dormant state, or as being in conflict with any law passed on the subject.\"\n\nIf Marshall was suggesting that the power over interstate commerce is an exclusive federal power, the Dormant Commerce Clause doctrine eventually developed very differently: it treats regulation that does not discriminate against or unduly burden interstate commerce as a concurrent power, rather than an exclusive federal power, and it treats regulation that does so as an exclusive federal power. Thus, the modern doctrine says that congressional power over interstate commerce is somewhat exclusive but \"not absolutely exclusive\". The approach began in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court: \"Either absolutely to affirm, or deny that the nature of this [commerce] power requires exclusive legislation by Congress, is to lose sight of the nature of the subjects of this power, and to assert concerning all of them, what is really applicable but to a part.\" The first clear holding of the Supreme Court striking down a state law under the Dormant Commerce Clause came in 1873.\n\nJustice Anthony Kennedy has written that: \"The central rationale for the rule against discrimination is to prohibit state or municipal laws whose object is local economic protectionism, laws that would excite those jealousies and retaliatory measures the Constitution was designed to prevent.\" In order to determine whether a law violates a so-called \"dormant\" aspect of the Commerce Clause, the court first asks whether it discriminates on its face against interstate commerce. In this context, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter.\n\nThus, in a dormant Commerce Clause case, a court is initially concerned with whether the law facially discriminates against out-of-state actors or has the effect of favoring in-state economic interests over out-of-state interests. Discriminatory laws motivated by \"simple economic protectionism\" are subject to a \"virtually per se rule of invalidity\", \"City of Philadelphia v. New Jersey\" 437 U.S. 617 (1978), \"Dean Milk Co. v. City of Madison, Wisconsin\", 340 U.S. 349 (1951), \"Hunt v. Washington State Apple Advertising Comm.\", 432 U.S. 333 (1977) which can only be overcome by a showing that the State has no other means to advance a legitimate local purpose, \"Maine v. Taylor\", 477 U.S. 131(1986). See also \"Brown-Forman Distillers v. New York State Liquor Authority\", .\n\nOn the other hand, when a law is \"directed to legitimate local concerns, with effects upon interstate commerce that are only incidental\" (United Haulers Association, Inc.), that is, where other legislative objectives are credibly advanced and there is no patent discrimination against interstate trade, the Court has adopted a much more flexible approach, the general contours of which were outlined in \"Pike v. Bruce Church, Inc.\", 397 U.S. 137, 142 (1970) and \"City of Philadelphia v. New Jersey\", 437 U.S. at 624. If the law is not outright or intentionally discriminatory or protectionist, but still has some impact on interstate commerce, the court will evaluate the law using a balancing test. The Court determines whether the interstate burden imposed by a law outweighs the local benefits. If such is the case, the law is usually deemed unconstitutional. See \"Pike v. Bruce Church, Inc.\", . In the Pike case, the Court explained that a state regulation having only \"incidental\" effects on interstate commerce \"will be upheld unless the burden imposed on such commerce is clearly excessive in relation to the putative local benefits\". 397 U.S. at 142, 90 S.Ct. at 847. When weighing burdens against benefits, a court should consider both \"the nature of the local interest involved, and ... whether it could be promoted as well with a lesser impact on interstate activities\". Id. Thus regulation designed to implement public health and safety, or serve other legitimate state interests, but impact interstate commerce as an incident to that purpose, are subject to a test akin to the rational basis test, a minimum level of scrutiny. See \"Bibb v. Navajo Freight Lines, Inc.\" In USA Recycling, Inc. v. Town of Babylon, 66 F.3d 1272, 1281 (C.A.2 (N.Y.), 1995), the court explained:\n\nIf the state activity constitutes \"regulation\" of interstate commerce, then the court must proceed to a second inquiry: whether the activity regulates evenhandedly with only \"incidental\" effects on interstate commerce, or discriminates against interstate commerce. As we use the term here, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter. The party challenging the validity of a state statute or municipal ordinance bears the burden of showing that it discriminates against, or places some burden on, interstate commerce. \"Hughes v. Oklahoma\", 441 U.S. 322, 336, 99 S.Ct. 1727, 1736, 60 L.Ed.2d 250 (1979). If discrimination is established, the burden shifts to the state or local government to show that the local benefits of the statute outweigh its discriminatory effects, and that the state or municipality lacked a nondiscriminatory alternative that could have adequately protected the relevant local interests. If the challenging party cannot show that the statute is discriminatory, then it must demonstrate that the statute places a burden on interstate commerce that \"is clearly excessive in relation to the putative local benefits.\" \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456, 471(1981) (quoting Pike, 397 U.S. at 142, 90 S.Ct. at 847).\n\nOver the years, the Supreme Court has consistently held that the language of the Commerce Clause contains a further, negative command prohibiting certain state taxation even when Congress has failed to legislate on the subject. Examples of such cases are \"Quill Corp. v. North Dakota\", 504 U.S. 298 (1992); \"Northwestern States Portland Cement Co. v. Minnesota\", 358 U.S. 450, 458 (1959) and \"H.P. Hood & Sons, Inc. v. Du Mond\", 336 U.S. 525 (1949).\n\nMore recently, in the 2015 case of \"Comptroller of Treasury of MD. v. Wynne\", the Court addressed Maryland's unusual practice of taxing personal income earned in Maryland, and taxing personal income of its citizens earned outside Maryland, \"without\" any tax credit for income tax paid to other states. The Court held this sort of double-taxation to be a violation of the dormant Commerce Clause. The Court faulted Justice Antonin Scalia's criticism of the dormant Commerce Clause doctrine by saying that he failed to \"explain why, under his interpretation of the Constitution, the Import-Export Clause \nwould not lead to the same result that we reach under the dormant Commerce Clause\".\n\nApplication of the dormant commerce clause to state taxation is another manifestation of the Court's holdings that the Commerce Clause prevents a State from retreating into economic isolation or jeopardizing the welfare of the Nation as a whole, as it would do if it were free to place burdens on the flow of commerce across its borders that commerce wholly within those borders would not bear. The Court's taxation decisions thus \"reflected a central concern of the Framers that was an immediate reason for calling the Constitutional Convention: the conviction that in order to succeed, the new Union would have to avoid the tendencies toward economic Balkanization that had plagued relations among the Colonies and later among the States under the Articles of Confederation.\" \"Wardair Canada, Inc. v. Florida Dept. of Revenue\", 477 U.S. 1 (1986); \"Hughes v. Oklahoma\", 441 U.S. 322 (1979); \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995).\n\nAs with the Court's application of the dormant commerce clause to discriminatory regulation, the pre-New Deal Court attempted to apply a formalistic approach to state taxation alleged to interfere with interstate commerce. The history is described in \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995):\n\nThe command has been stated more easily than its object has been attained, however, and the Court's understanding of the dormant Commerce Clause has taken some turns. In its early stages, the Court held the view that interstate commerce was wholly immune from state taxation \"in any form\", \"even though the same amount of tax should be laid on (intrastate) commerce\". This position gave way in time to a less uncompromising but formal approach, according to which, for example, the Court would invalidate a state tax levied on gross receipts from interstate commerce, or upon the \"freight carried\" in interstate commerce, but would allow a tax merely measured by gross receipts from interstate commerce as long as the tax was formally imposed upon franchises, or \"'in lieu of all taxes upon (the taxpayer's) property,'\" Dissenting from this formal approach in 1927, Justice Stone remarked that it was \"too mechanical, too uncertain in its application, and too remote from actualities, to be of value.\" \n\nAccompanying the revolution in approach in the Court's Congressional powers jurisprudence, the New Deal Court began to change its approach to state taxation as well. The Jefferson Lines decision continues:\n\nIn 1938, the old formalism began to give way with Justice Stone's opinion in \"Western Live Stock v. Bureau of Revenue\", 303 U.S. 250, which examined New Mexico's franchise tax, measured by gross receipts, as applied to receipts from out-of-state advertisers in a journal produced by taxpayers in New Mexico but circulated both inside and outside the State. Although the assessment could have been sustained solely on prior precedent, Justice Stone added a dash of the pragmatism that, with a brief interlude, has since become our aspiration in this quarter of the law. ... The Court explained that \"[i]t was not the purpose of the commerce clause to relieve those engaged in interstate commerce from their just share of state tax burden even though it increases the cost of doing the business.\" \n\nDuring the transition period, some taxes were upheld based on a careful review of the actual economic impact of the tax, and other taxes were reviewed based on the kind of tax involved, whether the tax had a nefarious impact on commerce or not. Under this formalistic approach, a tax might be struck down, and then re-passed with exactly the same economic incidence, but under another name, and then withstand review.\n\nThe absurdity of this approach was made manifest in the two Railway Express cases. In the first, a tax imposed by the state of Virginia on American business concerns operating within the state was struck down because it was a business privilege tax imposed on the privilege of doing business in interstate commerce. But then, in the second, Virginia revised the wording of its statute to impose a \"franchise tax\" on \"intangible property\" in the form of \"going concern\" value as measured by gross receipts.\n\nThe Court upheld the reworded statute as not violative of the prohibition on privilege taxes, even though the impact of the old tax and new were essentially identical. There was no real economic difference between the statutes in Railway Express I and Railway Express II. The Court long since had recognized that interstate commerce may be made to pay its way. Yet under the Spector rule, the economic realities in Railway Express I became irrelevant. The Spector rule (against privilege taxes) had come to operate only as a rule of draftsmanship, and served only to distract the courts and parties from their inquiry into whether the challenged tax produced results forbidden by the Commerce Clause.\n\nThe death knell of formalism occurred in \"Complete Auto Transit, Inc v. Brady\", 430 U.S. 274 (1977), which approved a Mississippi privilege tax upon a Michigan company engaged in the business of shipping automobiles to Mississippi dealers. The Court there explained:\n\nAppellant's attack is based solely on decisions of this Court holding that a tax on the \"privilege\" of engaging in an activity in the State may not be applied to an activity that is part of interstate commerce. See, e. g., \"Spector Motor Service v. O'Connor\", 340 U.S. 602 (1951); \"Freeman v. Hewit\", 329 U.S. 249 (1946). This rule looks only to the fact that the incidence of the tax is the \"privilege of doing business\"; it deems irrelevant any consideration of the practical effect of the tax. The rule reflects an underlying philosophy that interstate commerce should enjoy a sort of \"free trade\" immunity from state taxation. \n\nComplete Auto Transit is the last in a line of cases that gradually rejected a per se approach to state taxation challenges under the commerce clause. In overruling prior decisions which struck down privilege taxes per se, the Court noted the following, in what has become a central component of commerce clause state taxation jurisprudence:\n\nWe note again that no claim is made that the activity is not sufficiently connected to the State to justify a tax, or that the tax is not fairly related to benefits provided the taxpayer, or that the tax discriminates against interstate commerce, or that the tax is not fairly apportioned.\n\nThese four factors, nexus, relationship to benefits, discrimination, and apportionment, have come to be regarded as the four Complete Auto Transit factors applied repeatedly in subsequent cases. Complete Auto Transit must be recognized as the culmination of the Court's emerging commerce clause approach, not just in taxation, but in all of its aspects. Application of Complete Auto Transit to State taxation remains a highly technical and specialized venture, requiring the application of commerce clause principles to an understanding of specialized tax law.\n\nIn addition to satisfying the four-prong test in \"Complete Auto Transit\", the Supreme Court has held state taxes which burden international commerce cannot create a substantial risk of multiple taxations and must not prevent the federal government from \"speaking with one voice when regulating commercial relations with foreign governments\". \"Japan Lines, Ltd. v. County of Los Angeles\", 441 U.S. 434 (1979).\n\nIn \"Kraft Gen. Foods, Inc. v. Iowa Dept. of Revenue and Finance\", 505 U.S. 71 (1992), the Supreme Court considered a case in which Iowa taxed dividends from foreign subsidiaries, without allowing a credit for taxes paid to foreign governments, but not dividends from domestic subsidiaries operating outside Iowa. This differential treatment arose from Iowa's adoption of the definition of \"net income\" used by the Internal Revenue Service. For federal income tax purposes, dividends from domestic subsidiaries are allowed to be exempted from the parent corporations income to avoid double taxation. The Iowa Supreme Court rejected a Commerce Clause claim because Kraft failed to show \"that Iowa businesses receive a commercial advantage over foreign commerce due to Iowa's taxing scheme.\" Considering an Equal Protection Clause challenge, the Iowa Supreme Court held that the use of the federal government's definitions of income were convenient for the state and was \"rationally related to the goal of administrative efficiency\". The Supreme Court rejected the notion that administrative convenience was a sufficient defense for subjecting foreign commerce to a higher tax burden than interstate commerce. The Supreme Court held that \"a State's preference for domestic commerce over foreign commerce is inconsistent with the Commerce Clause even if the State's own economy is not a direct beneficiary of the discrimination.\"\n\nDiscrimination in the flow of interstate commerce has arisen in a variety of contexts. A line of important cases has dealt with local processing requirements. Under the local processing requirement, a municipality seeks to force the local processing of raw materials before they are shipped in interstate commerce.\n\nThe basic idea of the local processing ordinance was to provide favored access to local processors of locally produced raw materials. Examples of Supreme Court decisions in this vein are set out in its Carbone decision. They include \"Minnesota v. Barber\", 136 U.S. 313, (1890) (striking down a Minnesota statute that required any meat sold within the State, whether originating within or without the State, to be examined by an inspector within the State); \"Foster-Fountain Packing Co. v. Haydel\", 278 U.S. 1 (1928) (striking down a Louisiana statute that forbade shrimp to be exported unless the heads and hulls had first been removed within the State); \"Johnson v. Haydel\", 278 U.S. 16 (1928) (striking down analogous Louisiana statute for oysters); \"Toomer v. Witsell\", 334 U.S. 385 (1948) (striking down South Carolina statute that required shrimp fishermen to unload, pack, and stamp their catch before shipping it to another State); \"Pike v. Bruce Church, Inc.\", supra (striking down Arizona statute that required all Arizona-grown cantaloupes to be packaged within the State prior to export); \"South-Central Timber Development, Inc. v. Wunnicke\", 467 U.S. 82 (1984) (striking down an Alaska regulation that required all Alaska timber to be processed within the State prior to export). The Court has defined \"protectionist\" state legislation as \"regulatory measures designed to benefit in-state economic interests by burdening out-of-state competitors\". \"New Energy Co. of Indiana v. Limbach\", 486 U.S. 269, 273–74 (1988).\n\nIn the 1980s, spurred by RCRA's emphasis on comprehensive local planning, many states and municipalities sought to promote investment in more costly disposal technologies, such as waste-to-energy incinerators, state-of-the-art landfills, composting and recycling. Some states and localities sought to promote private investment in these costly technologies by guaranteeing a longterm supply of customers. See Phillip Weinberg, Congress, the Courts, and Solid Waste Transport: Good Fences Don't Always Make Good Neighbors, 25 Envtl. L. 57 (1995); Atlantic Coast Demolition & Recycling, Inc., 112 F.3d 652, 657 (3d Cir. 1997). For about a decade, the use of regulation to channel private commerce to designated private disposal sites was greatly restricted as the result of the Carbone decision discussed below.\n\nFlow control laws typically came in various designs. One common theme was the decision to fund local infrastructure by guaranteeing a minimum volume of business for privately constructed landfills, incinerators, composters or other costly disposal sites. In some locales, choice of the flow control device was driven by state bonding laws, or municipal finance concerns. If a county or other municipality issued general obligation bonds for construction of a costly incinerator, for example, state laws might require a special approval process. If approval could be obtained, the bonds themselves would be counted against governmental credit limitations, or might impact the governmental body's credit rating: in either instance the ability to bond for other purposes might be impaired. But by guaranteeing customers for a privately constructed and financed facility, a private entity could issue its own bonds, privately, on the strength of the public's waste assurance.\n\nThe private character of flow control regimens can thus be explained in part by the desire to utilize particular kinds of public financing devices. It can also be explained by significant encouragement at the national level, in national legislation as well as in federal executive policy to achieve environmental objectives utilizing private resources. Ironically, these public-private efforts often took the form of local processing requirements which ultimately ran afoul of the commerce clause.\n\nThe Town of Clarkstown had decided that it wanted to promote waste assurance through a local private transfer station. The transfer station would process waste and then forward the waste to the disposal site designated by the Town. The ordinance had the following features:\n\nWaste hauling in the Town of Clarkstown was accomplished by private haulers, subject to local regulation. The scheme had the following aspects: (A) The Town promoted the financing of a privately owned transfer station through a waste assurance agreement with the private company. Thus the designated facility was a private company. (B) The Town of Clarkstown forced private haulers to bring their solid waste for local processing at the designated transfer station, even if the ultimate destination of solid waste was an out-of-state disposal site. (C) The primary rationale for forcing in-state waste into the designated private transfer station was financial; it was seen as a device to raise revenue to finance the transfer station.\n\nThe Town of Clarkstown's ordinance was designed and written right in the teeth of the long line of Supreme Court cases which had historically struck down local processing requirements. In short, it was as if the authors of the ordinance had gone to a treatise on the commerce clause and intentionally chosen a device which had been traditionally prohibited. A long line of Supreme Court case law had struck down local processing requirements when applied to goods or services in interstate commerce. As the Court in Carbone wrote:\n\nWe consider a so-called flow control ordinance, which requires all solid waste to be processed at a designated transfer station before leaving the municipality. The avowed purpose of the ordinance is to retain the processing fees charged at the transfer station to amortize the cost of the facility. Because it attains this goal by depriving competitors, including out-of-state firms, of access to a local market, we hold that the flow control ordinance violates the Commerce Clause.\n\nThe Court plainly regarded the decision as a relatively unremarkable decision, not a bold stroke. As the Court wrote: \"The case decided today, while perhaps a small new chapter in that course of decisions, rests nevertheless upon well-settled principles of our Commerce Clause jurisprudence.\" And, the Court made it plain, that the problem with Clarkstown's ordinance was that it created a local processing requirement protective of a local private processing company:\n\nIn this light, the flow control ordinance is just one more instance of local processing requirements that we long have held invalid ... The essential vice in laws of this sort is that they bar the import of the processing service. Out-of-state meat inspectors, or shrimp hullers, or milk pasteurizers, are deprived of access to local demand for their services. Put another way, the offending local laws hoard a local resource—be it meat, shrimp, or milk—for the benefit of local businesses that treat it. 511 U.S. at 392–393.\n\nThe Court's 2007 decision in \"United Haulers Association v. Oneida-Herkimer Solid Waste Management Authority\" starkly illustrates the difference in result when the Court finds that local regulation is not discriminatory. The Court dealt with a flow control regimen quite similar to that considered in Carbone. The \"only salient difference is that the laws at issue here require haulers to bring waste to facilities owned and operated by a state-created public benefit corporation.\" The Court decided that the balancing test should apply, because the regulatory scheme favored the government owned facility, but treated all private facilities equally.\n\nCompelling reasons justify treating these laws differently from laws favoring particular private businesses over their competitors. \"Conceptually, of course, any notion of discrimination assumes a comparison of substantially similar entities.\" \"General Motors Corp. v. Tracy\", 519 U.S. 278 (1997). But States and municipalities are not private businesses—far from it. Unlike private enterprise, government is vested with the responsibility of protecting the health, safety, and welfare of its citizens. See \"Metropolitan Life Ins. Co. v. Massachusetts\", 471 U.S. 724 (1985) ... These important responsibilities set state and local government apart from a typical private business.\n\nThe Court's United Haulers decision demonstrates an understanding of the regulatory justifications for flow control starkly missing in the Carbone decision:\n\nBy the 1980s, the Counties confronted what they could credibly call a solid waste \" 'crisis.' \"... Many local landfills were operating without permits and in violation of state regulations. Sixteen were ordered to close and remediate the surrounding environment, costing the public tens of millions of dollars. These environmental problems culminated in a federal clean-up action against a landfill in Oneida County; the defendants in that case named over local businesses and several municipalities and school districts as third-party defendants The \"crisis\" extended beyond health and safety concerns. The Counties had an uneasy relationship with local waste management companies, enduring price fixing, pervasive overcharging, and the influence of organized crime. Dramatic price hikes were not uncommon: In 1986, for example, a county contractor doubled its waste disposal rate on six weeks' notice\n\nThe Court would not interfere with local government's efforts to solve an important public and safety problem.\n\nThe contrary approach of treating public and private entities the same under the dormant Commerce Clause would lead to unprecedented and unbounded interference by the courts with state and local government. The dormant Commerce Clause is not a roving license for federal courts to decide what activities are appropriate for state and local government to undertake, and what activities must be the province of private market competition. In this case, the citizens of Oneida and Herkimer Counties have chosen the government to provide waste management services, with a limited role for the private sector in arranging for transport of waste from the curb to the public facilities. The citizens could have left the entire matter for the private sector, in which case any regulation they undertook could not discriminate against interstate commerce. But it was also open to them to vest responsibility for the matter with their government, and to adopt flow control ordinances to support the government effort. It is not the office of the Commerce Clause to control the decision of the voters on whether government or the private sector should provide waste management services. \"The Commerce Clause significantly limits the ability of States and localities to regulate or otherwise burden the flow of interstate commerce, but it does not elevate free trade above all other values.\"\n\nThe history of commerce clause jurisprudence evidences a distinct difference in approach where the state is seeking to exercise its public health and safety powers, on the one hand, as opposed to attempting to regulate the flow of commerce. The exact dividing line between the two interests, the right of states to exercise regulatory control over their public health and safety, and the interest of the national government in unfettered interstate commerce is not always easy to discern. One Court has written as follows:\n\nNot surprisingly, the Court's effort to preserve a national market has, on numerous occasions, come into conflict with the states' traditional power to \"legislat[e] on all subjects relating to the health, life, and safety of their citizens.\" \"Huron Portland Cement Co. v. City of Detroit\", 362 U.S. 440, 443 (1960). On these occasions, the Supreme Court has \"struggled (to put it nicely) to develop a set of rules by which we may preserve a national market without needlessly intruding upon the States' police powers, each exercise of which no doubt has some effect on the commerce of the Nation.\" \"Camps Newfound/Owatonna v. Town of Harrison\", 520 U.S. 564, 596 (1997) (Scalia, J., dissenting) (citing \"Okla. Tax Comm'n v. Jefferson Lines\", 514 U.S. 175, 180–83 (1995)); see generally Boris I. Bittker, Regulation of Interstate and Foreign Commerce § 6.01[A], at 6–5 (\"[T]he boundaries of the [State's] off-limits area are, and always have been, enveloped in a haze.\"). Those rules are \"simply stated, if not simply applied.\" Camps Newfound/Owatonna, 520 U.S. at 596 (Scalia, J., dissenting).\n\nA frequently cited example of the deference afforded to the powers of state and local government may be found in \"Exxon Corp. v. Maryland\", 437 U.S. 117 (1978), where the State of Maryland barred producers of petroleum products from operating retail service stations in the state. It is difficult to imagine a regimen which might have greater impact on the way in which markets are organized. Yet, the Court found the legislation constitutionally permitted: \"The fact that the burden of a state regulation falls on some interstate companies does not, by itself establish a claim of discrimination against interstate commerce,\" the Court wrote. The \"Clause protects interstate market, not particular interstate firms, from prohibitive or burdensome regulations.\"\n\nSimilarly, in \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456 (1981) the Court upheld a state law that banned nonreturnable milk containers made of plastic but permitted other nonreturnable milk containers. The Court found that the existence of a burden on out-of-state plastic industry was not 'clearly excessive' in comparison to the state's interest in promoting conservation. And the court continued:\n\nIn Exxon, the Court stressed that the Commerce Clause protects the interstate market, not particular interstate firms, from prohibitive or burdensome regulations. A nondiscriminatory regulation serving substantial state purpose is not invalid simply because it causes some business to shift from a predominantly out-of-state industry to a predominantly in-state industry. Only if the burden on interstate commerce clearly outweighs the State's legitimate purpose does such a regulation violate the commerce clause. When a state statute regarding safety matters applies equally to interstate and intrastate commerce, the courts are generally reluctant to invalidate it even if it may have some impact on interstate commerce. In \"Bibb v. Navajo Freight Lines\" 359 U.S. 520, 524 (1959), the United States Supreme Court stated: 'These safety measures carry a strong presumption of validity when challenged in court. If there are alternative ways of solving a problem, we do not sit to determine which of them is best suited to achieve a valid state objective. Policy decisions are for the state legislature, absent federal entry into the field. Unless we can conclude on the whole record that \"the total effect of the law as a safety measure in reducing accidents and casualties is so slight or problematical as not to outweigh the national interest in keeping interstate commerce free from interferences which seriously impede it\" we must uphold the statute.\n\nThere are two notable exceptions to the dormant Commerce Clause doctrine that can permit state laws or actions that otherwise violate the Dormant Commerce Clause to survive court challenges.\n\nThe first exception occurs when Congress has legislated on the matter. See \"Western & Southern Life Ins. v. State Board of California\", . In this case the Dormant Commerce Clause is no longer \"dormant\" and the issue is a Commerce Clause issue, requiring a determination of whether Congress has approved, preempted, or left untouched the state law at issue.\n\nThe second exception is \"market participation exception\". This occurs when the state is acting \"in the market\", like a business or customer, rather than as a \"market regulator\". For example, when a state is contracting for the construction of a building or selling maps to state parks, rather than passing laws governing construction or dictating the price of state park maps, it is acting \"in the market\". Like any other business in such cases, a state may favor or shun certain customers or suppliers.\n\nThe Supreme Court introduced the market participant doctrine in \"Hughes v. Alexandria Scrap Corp.\", 426 U.S. 794 (1976), which upheld a Maryland program that offered bounties to scrap processors to destroy abandoned automobile hulks. See also \"Wisconsin Dep't of Indus., Labor & Human Relations v. Gould Inc.\", 475 U.S. 282, 289 (1986); \"Reeves, Inc. v. Stake\", 447 U.S. 429, 437 (1980). Because Maryland required out-of-state processors, but not in-state processors, to submit burdensome documentation to claim their bounties, the state effectively favored in-state processors over out-of-state processors. The Court held that because the state was merely attaching conditions to its expenditure of state funds, the Maryland program affected the market no differently than if Maryland were a private company bidding up the price of auto hulks. Because the state was not \"regulating\" the market, its economic activity was not subject to the anti-discrimination principles underlying the dormant Commerce Clause—and the state could impose different paperwork burdens on out-of-state processors. \"Nothing in the purposes animating the Commerce Clause prohibits a State, in the absence of congressional action, from participating in the market and exercising the right to favor its own citizens over others.\"\n\nAnother important case is \"White v. Massachusetts Council of Constr. Employers, Inc.\", in which the Supreme Court held that the City of Boston could require its building contractors to hire at least fifty percent of their workforce from among Boston residents. 460 U.S. at 214–15. Because all of the employees covered by that mandate were \"in a substantial if informal sense, 'working for the city,' \" Boston was considered to be simply favoring its own residents through the expenditures of municipal funds. The Supreme Court stated, \"when a state or local government enters the market as a participant it is not subject to the restraints of the Commerce Clause.\" Id. at 208. Nothing in the Constitution precludes a local government from hiring a local company precisely because it is local.\n\nOther important cases enunciating the market participation exception principle are \"Reeves, Inc. v. Stake\", and \"South-Central Timber Development, Inc. v. Wunnicke\", . The \"Reeves\" case outlines the market participation exception test. In this case state-run cement co-ops were allowed to make restrictive rules (e.g. rules not to sell out-of-state). Here, this government-sponsored business was acting restrictively like an individually owned business and this action was held to be constitutional. \"South-Central Timber\" is important because it limits the market exception. \"South-Central Timber\" holds that the market-participant doctrine is limited in allowing a State to impose burdens on commerce within the market in which it is a participant, but allows it to go no further. The State may not impose conditions that have a substantial regulatory effect outside of that particular market.\n\nThe \"market participation exception\" to the dormant Commerce Clause does not give states unlimited authority to favor local interests, because limits from other laws and Constitutional limits still apply. In \"United Building & Construction Trades Council v. Camden\", , the city of Camden, New Jersey had passed an ordinance requiring that at least forty percent of the employees of contractors and subcontractors on city projects be Camden residents. The Supreme Court found that while the law was not infirm because of the Dormant Commerce Clause, it violated the Privileges and Immunities Clause of Article IV of the Constitution. Justice Rehnquist's opinion distinguishes the market-participant doctrine from the privileges and immunities doctrine. Similarly, Congress has the power itself under the Commerce Clause to regulate and sanction states acting as \"market participants\", but it lacks power to legislate in ways that violate Article IV.\n\nIn the 21st century, the dormant Commerce Clause has been a frequent legal issue in cases arising under state laws regulating some aspects of Internet activity. Because of the interstate, and often international, nature of Internet communications, state laws addressing internet-related subjects such as spam, online sales or online pornography can often trigger Dormant Commerce Clause issues.\n\nA \"negative\" or \"dormant\" component to the Commerce Clause has been the subject of scholarly discussion for many decades. Both Supreme Court Justices Antonin Scalia and Clarence Thomas have rejected the notion of a Dormant Commerce Clause. They believe that such a doctrine is inconsistent with an originalist interpretation of the Constitution—so much so that they believe the doctrine is a \"judicial fraud\".\n\nA number of earlier Supreme Court justices also expressed dissatisfaction with the dormant Commerce Clause doctrine. For example, Chief Justice Taney said this in 1847:\n\nIf it was intended to forbid the States from making any regulations of commerce, it is difficult to account for the omission to prohibit it, when that prohibition has been so carefully and distinctly inserted in relation to other powers ... [T]he legislation of Congress and the States has conformed to this construction from the foundation of the government ... The decisions of this court will also, in my opinion, when carefully examined, be found to sanction the construction I am maintaining.\n\nHowever, that statement by Taney in 1847 was before the doctrine morphed in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court that the Commerce Clause does not always require \"exclusive legislation by Congress\".\n\nIn \"Trailer Marine Transport Corp. v. Rivera Vázquez\", 977 F.2d 1, 7-8 (1st Cir. 1992), the First Circuit held that the dormant Commerce Clause applies to Puerto Rico.\n\n\n"}
{"id": "24856902", "url": "https://en.wikipedia.org/wiki?curid=24856902", "title": "Evidential existentiality", "text": "Evidential existentiality\n\nThe principle of evidential existentiality in philosophy is a principle that explains and gives value to the existence of entities. The principle states that the reality of an entity's existence gives greater value to prove its existence than would be given through any outward studies. The principle has become a backbone of the God argument, stating that because God is a self-evident entity, His existence can only be shared by humans, thus proof of God is unnecessary and moot.\nIt appears that the existence is primarily evident to the self only. The God or Supreme self is perceivable to the self. So evidentially self perception is followed by God perception and so on.\n\n"}
{"id": "15285866", "url": "https://en.wikipedia.org/wiki?curid=15285866", "title": "Experimental system", "text": "Experimental system\n\nIn scientific research, an experimental system is the physical, technical and procedural basis for an experiment or series of experiments. Historian of science Hans-Jörg Rheinberger defines an experimental system as: \"A basic unit of experimental activity combining local, technical, instrumental, institutional, social, and epistemic aspects.\" Scientists (particularly laboratory biologists) and historians and philosophers of biology have pointed to the development and spread of successful experimental systems, such as those based on popular model organism or scientific apparatus, as key elements in the history of science, particularly since the early 20th century. The choice of an appropriate experimental system is often seen as critical for a scientist's long-term success, as experimental systems can be very productive for some kinds of questions and less productive for others, acquiring a sort of momentum that takes research in unpredicted directions.\n\nA successful experimental system must be stable and reproducible enough for scientists to make sense of the system's behavior, but variable and unpredictable enough that it can produce useful results. In many cases, a well-understood experimental system can be \"black-boxed\" as a standard technique, which can then be a component of other experimental systems. Rheinberger divides experimental systems into two parts: the part under investigation (\"epistemic things\") and the well-understood part that provides a stable context for experimentation (\"technical objects\").\n\nThe development of experimental systems in biology often requires the \"domestication\" of a particular organism for the laboratory environment, including the creation of relatively homogeneous lines or strains and the tailoring of conditions to highlight the variable aspects that scientists are interested in. Scientific technologies, similarly, often require the development of a full experimental system to go from a viable concept to a technique that works in practice on a usefully consistent basis. For example, the invention of the polymerase chain reaction (PCR) is generally attributed to Kary Mullis, who came up with the concept in 1983, but the process of development of PCR into the revolutionary technology it became by the early 1990s took years of work by others at Cetus Corporation—and the basic components of the system had been known since the 1960s DNA synthesis work of Har Gobind Khorana—making \"who invented PCR?\" a complicated question.\n\n"}
{"id": "7051566", "url": "https://en.wikipedia.org/wiki?curid=7051566", "title": "Idios kosmos", "text": "Idios kosmos\n\nIdios kosmos comes from Greek and means private world. It exists with, and is opposite to, koinos kosmos (shared world). \"Idios kosmos\" is the view of the world that is developed from personal experience and knowledge and is therefore unique; however, it can be difficult to tell the difference between it and \"koinos kosmos\".\n\nThe two phrases come from the Diels-Kranz fragment B89 of Heraclitus: ὁ Ἡράκλειτός φησι τοῖς ἐγρηγορόσιν ἕνα καὶ κοινὸν κόσμον εἶναι τῶν δὲ κοιμωμένων ἕκαστον εἰς ἴδιον ἀποστρέφεσθαι (\"Heraclitus said that the waking have one common world, but the sleeping turn aside each into a world of his own.\")\n\nThe idea of \"idios kosmos\" is an important part of Philip K. Dick's views on schizophrenia, as expressed in his 1964 essay \"Schizophrenia & 'The Book of Changes'\", drawing on personal experience with the I Ching.\n\n\n"}
{"id": "3108937", "url": "https://en.wikipedia.org/wiki?curid=3108937", "title": "Landauer's principle", "text": "Landauer's principle\n\nLandauer's principle is a physical principle pertaining to the lower theoretical limit of energy consumption of computation. It holds that \"any logically irreversible manipulation of information, such as the erasure of a bit or the merging of two computation paths, must be accompanied by a corresponding entropy increase in non-information-bearing degrees of freedom of the information-processing apparatus or its environment\".\n\nAnother way of phrasing Landauer's principle is that if an observer loses information about a physical system, the observer loses the ability to extract work from that system.\n\nA so-called logically-reversible computation, in which no information is erased, may in principle be carried out without releasing any heat. This has led to considerable interest in the study of reversible computing. Indeed, without reversible computing, increases in the number of computations-per-joule-of-energy-dissipated must come to a halt by about 2050: because the limit implied by Landauer's principle will be reached by then, according to Koomey's law. \n\nAt 20 °C (room temperature, or 293.15 K), the Landauer limit represents an energy of approximately 0.0172 eV, or 2.75 zJ. Theoretically, roomtemperature computer memory operating at the Landauer limit could be changed at a rate of one billion bits per second with energy being converted to heat in the memory media at the rate of only 2.85 trillionths of a watt (that is, at a rate of only 2.85 pJ/s). Modern computers use millions of times as much energy per second.\n\nRolf Landauer first proposed the principle in 1961 while working at IBM. He rigorously justified and stated important limits to an earlier conjecture by John von Neumann. For this reason, it is sometimes referred to as being simply the Landauer bound or Landauer limit.\n\nIn 2011, the principle was generalized to show that while information erasure requires an increase in entropy, that increase could theoretically occur at no energy cost. Instead, the cost can be taken in another conserved quantity, such as angular momentum.\n\nIn a 2012 article published in \"Nature\", a team of physicists from the École normale supérieure de Lyon, University of Augsburg and the University of Kaiserslautern described that for the first time they have measured the tiny amount of heat released when an individual bit of data is erased.\n\nIn 2014, physical experiments tested Landauer's principle and confirmed its predictions.\n\nIn 2016, researchers used a laser probe to measure the amount of energy dissipation that resulted when a nanomagnetic bit flipped from off to on. Flipping the bit required 15 millielectron volts (3 zeptojoules).\n\nA 2018 article published in \"Nature Physics\" features a Landauer erasure performed at cryogenic temperatures (T = 1K) on an array of high-spin (S = 10) quantum molecular magnets. The array is made to act as a spin register where each nanomagnet encodes a single bit of information. The experiment has laid the foundations for the extension of the validity of the Landauer principle to the quantum realm. Owing to the fast dynamics and low \"inertia\" of the single spins used in the experiment, the researchers also showed how an erasure operation can be carried out at the lowest possible thermodynamic cost — that imposed by the Landauer principle — and at a high speed. \n\nLandauer's principle can be understood to be a simple logical consequence of the second law of thermodynamics—which states that the entropy of an isolated system cannot decrease—together with the definition of thermodynamic temperature. For, if the number of possible logical states of a computation were to decrease as the computation proceeded forward (logical irreversibility), this would constitute a forbidden decrease of entropy, unless the number of possible physical states corresponding to each logical state were to simultaneously increase by at least a compensating amount, so that the total number of possible physical states was no smaller than it was originally (i.e. total entropy has not decreased).\n\nYet, an increase in the number of physical states corresponding to each logical state means that, for an observer who is keeping track of the logical state of the system but not the physical state (for example an \"observer\" consisting of the computer itself), the number of possible physical states has increased; in other words, entropy has increased from the point of view of this observer.\n\nThe maximum entropy of a bounded physical system is finite. (If the holographic principle is correct, then physical systems with finite surface area have a finite maximum entropy; but regardless of the truth of the holographic principle, quantum field theory dictates that the entropy of systems with finite radius and energy is finite due to the Bekenstein bound.) To avoid reaching this maximum over the course of an extended computation, entropy must eventually be expelled to an outside environment.\n\nLandauer's principle asserts that there is a minimum possible amount of energy required to erase one bit of information, known as the \"Landauer limit\":\nwhere \"k\" is the Boltzmann constant (approximately 1.38×10 J/K), \"T\" is the temperature of the heat sink in kelvins, and ln 2 is the natural logarithm of 2 (approximately 0.69315).\n\nFor an environment at temperature \"T\", energy \"E\" = \"ST\" must be emitted into that environment if the amount of added entropy is \"S\". For a computational operation in which 1 bit of logical information is lost, the amount of entropy generated is at least \"k\" ln 2, and so, the energy that must eventually be emitted to the environment is \"E\" ≥ \"kT\" ln 2.\n\nThe principle is widely accepted as physical law, but in recent years it has been challenged for using circular reasoning and faulty assumptions, notably in Earman and Norton (1998), and subsequently in Shenker (2000) and Norton (2004, 2011), and defended by Bennett (2003) and Ladyman et al. (2007).\n\n\n"}
{"id": "563405", "url": "https://en.wikipedia.org/wiki?curid=563405", "title": "League system", "text": "League system\n\nA league system is a hierarchy of leagues in a sport. They are often called pyramids, due to their tendency to split into an increasing number of regional divisions further down the system. League systems of some sort are used in many sports in many countries.\n\nIn association football, rugby union and rugby league, league systems are usually connected by the process of promotion and relegation, in which teams from a lower division who finish at the top of the standings in their league are promoted (advanced to the next level of the system) while teams who finish lowest in their division are relegated (move down to a lower division). This process can be automatic each year, or can require playoffs.\n\nIn North America, league systems in the most popular sports do not use promotion or relegation. Most professional sports are divided into major and minor leagues. Baseball and association football (known as soccer in North America) have well-defined pyramid shapes to their minor league hierarchies, each managed by a governing body (Minor League Baseball, an organization under the authority of the Commissioner of Baseball, governs baseball leagues; the United States Soccer Federation designates the American soccer pyramid.) Ice hockey's professional minor league system is linear, with one league at most of the four levels of the game; the ice hockey league system in North America is governed by collective bargaining agreements and affiliation deals between the NHL, AHL and ECHL.\n\nGridiron football does not operate on a league system. Different professional leagues play by very different sets of rules in different seasons (the NFL plays 11-a-side on a 100-yard field in autumn and early winter, the CFL uses 12-a-side on a 110-yard field in summer and early fall, while arena football and the minor indoor leagues each play 8-a-side on a 50-yard field in the spring and early summer). There have been attempts at forming true minor leagues for the professional game (most recently with 2017's The Spring League); none so far have been able to balance the major leagues' requests with the ability to maintain financial solvency.\n\n"}
{"id": "29594530", "url": "https://en.wikipedia.org/wiki?curid=29594530", "title": "Logical hexagon", "text": "Logical hexagon\n\nThe logical hexagon (also called the hexagon of opposition) is a conceptual model of the relationships between the truth values of six statements. It is an extension of Aristotle's square of opposition. It was discovered independently by both Augustin Sesmat and Robert Blanché.\n\nThis extension consists in introducing two statements U and Y. Whereas U is the disjunction of A and E, Y is the conjunction of the two traditional particulars I and O.\n\nThe traditional square of opposition demonstrates two sets of contradictories A and O, and E and I (i.e. they cannot both be true and cannot both be false), two contraries A and E (i.e. they can both be false, but cannot both be true), and two subcontraries I and O (i.e. they can both be true, but cannot both be false) according to Aristotle’s definitions. However, the logical hexagon provides that U and Y are also contradictory.\n\nThe logical hexagon may be interpreted in various ways, including as a model of traditional logic, quantifications, modal logic, order theory, or paraconsistent logic.\n\nFor instance, the statement A may be interpreted as \"Whatever x may be, if x is a man, then x is white.\"\n\nThe statement E may be interpreted as \"Whatever x may be, if x is a man, then x is non-white.\"\n\nThe statement I may be interpreted as \"There exists at least one x that is both a man and white.\"\n\nThe statement O may be interpreted as \"There exists at least one x that is both a man and non-white\"\n\nThe statement Y may be interpreted as \"There exists at least one x that is both a man and white and there exists at least one x that is both a man and non-white\"\n\nThe statement U may be interpreted as \"One of two things, either whatever x may be, if x is a man, then x is white or whatever x may be, if x is a man, then x is non-white.\"\n\nThe logical hexagon may be interpreted in various ways, including as a model of traditional logic, quantifications, modal logic, order theory, or paraconsistent logic.\n\nThe logical hexagon may be interpreted as a model of modal logic such that\n\n\nIt has been proven that both the square and the hexagon, followed by a “logical cube”, belong to a regular series of n-dimensional objects called “logical bi-simplexes of dimension n.” The pattern also goes even beyond this.\n\n\n"}
{"id": "26127533", "url": "https://en.wikipedia.org/wiki?curid=26127533", "title": "Marginal abatement cost", "text": "Marginal abatement cost\n\nAbatement cost is the cost of reducing environmental negatives such as pollution. Marginal cost is an economic concept that measures the cost of an additional unit. The marginal abatement cost (MAC), in general, measures the cost of reducing one more unit of pollution.\n\nAlthough marginal abatement costs can be negative, such as when the low carbon option is cheaper than the business-as-usual option, MACs often rise steeply as more pollution is reduced.\n\nMarginal abatement costs are typically used on a marginal abatement cost curve (MACC) or MAC curve, which shows the marginal cost of additional reductions in pollution.\n\nCarbon traders use MAC curves to derive the supply function for modelling carbon price fundamentals. Power companies may employ MAC curves to guide their decisions about long-term capital investment strategies to select among a variety of efficiency and generation options. Economists have used MAC curves to explain the economics of interregional carbon trading. Policy-makers use MAC curves as merit order curves, to analyze how much abatement can be done in an economy at what cost, and where policy should be directed to achieve the emission reductions.\n\nHowever, MAC curves should not be used as abatement supply curves (or merit order curves) to decide which measures to implement in order to achieve a given emission-reduction target. Indeed, the options they list would take decades to implement, and it may be optimal to implement expensive but high-potential measures before introducing cheaper measures.\n\nThe way that MAC curves are usually built has been criticized for lack of transparency and the poor treatment it makes of uncertainty, inter-temporal dynamics, interactions between sectors and ancillary benefits. There is also concern regarding the biased ranking that occurs if some included options have negative costs. \n\nVarious economists, research organizations, and consultancies have produced MAC curves. Bloomberg New Energy Finance and McKinsey & Company have produced economy wide analyses on greenhouse gas emissions reductions for the United States. ICF International produced a California specific curve following AB-32 legislation as have Sweeney and Weyant.\n\nThe Wuppertal Institute for Climate, Environment and Energy produced several marginal abatement cost curves for Germany (also called Cost Potential Curves), depending on the perspective (end-user, utilities, society).\n\nThe US Environmental Protection Agency has done work on a MAC curve for non carbon dioxide emissions such as methane, NO, and HFCs. Enerdata and LEPII-CNRS (France) produce MAC curves with the Prospective Outlook on Long-term Energy Systems (POLES) model for the 6 Kyoto Protocol gases. These curves have been used for various public and private actors either to assess carbon policies or through the use of a carbon market analysis tool.\n\nThe World Bank 2013 low-carbon energy development plan for Nigeria, prepared jointly with the World Bank, ulitizes MAC curves created in Analytica.\n\n"}
{"id": "274078", "url": "https://en.wikipedia.org/wiki?curid=274078", "title": "Marginal demand", "text": "Marginal demand\n\nMarginal demand in economics is the change in demand for a product or service in response to a specific change in its price.\n\nNormally, as prices for goods or service rise, demand falls, and conversely, as prices for goods or services fall, demand rises.\n\nA product or service where price changes cause a relatively big change in demand is said to have \"elastic\" demand. A product or service where price changes cause a relatively small change in demand is said to have \"inelastic\" demand. See Elasticity of demand.\n\n"}
{"id": "571001", "url": "https://en.wikipedia.org/wiki?curid=571001", "title": "Marginal propensity to consume", "text": "Marginal propensity to consume\n\nIn economics, the marginal propensity to consume (MPC) is a metric that quantifies induced consumption, the concept that the increase in personal consumer spending (consumption) occurs with an increase in disposable income (income after taxes and transfers). The proportion of disposable income which individuals spend on consumption is known as propensity to consume. MPC is the proportion of additional income that an individual consumes. For example, if a household earns one extra dollar of disposable income, and the marginal propensity to consume is 0.65, then of that dollar, the household will spend 65 cents and save 35 cents. Obviously, the household cannot spend \"more\" than the extra dollar (without borrowing).\n\nAccording to John Maynard Keynes, marginal propensity to consume is less than one.\n\nMathematically, the formula_1 function is expressed as the derivative of the consumption function formula_2 with respect to disposable income formula_3, i.e., the instantaneous slope of the formula_2-formula_3 curve.\n\nor, approximately,\n\nMarginal propensity to consume can be found by dividing change in consumption by a change in income, or formula_10. The MPC can be explained with the simple example:\nHere formula_11; formula_12\nTherefore, formula_13 or 83%.\nFor example, suppose you receive a bonus with your paycheck, and it's $500 on top of your normal annual earnings. You suddenly have $500 more in income than you did before. If you decide to spend $400 of this marginal increase in income on a new business suit, your marginal propensity to consume will be 0.8 (formula_14).\n\nThe marginal propensity to consume is measured as the ratio of the change in consumption to the change in income, thus giving us a figure between 0 and 1. The MPC can be more than one if the subject borrowed money or dissaved to finance expenditures higher than their income. The MPC can also be less than zero if an increase in income leads to a reduction in consumption (which might occur if, for example, the increase in income makes it worthwhile to save up for a particular purchase). One minus the MPC equals the marginal propensity to save (in a two sector closed economy), which is crucial to Keynesian economics and a key variable in determining the value of the multiplier.\n\nIn a standard Keynesian model, the MPC is less than the average propensity to consume (APC) because in the short-run some (autonomous) consumption does not change with income. Falls (increases) in income do not lead to reductions (increases) in consumption because people reduce (add to) savings to stabilize consumption. Over the long-run, as wealth and income rise, consumption also rises; the marginal propensity to consume out of long-run income is closer to the average propensity to consume.\n\nThe MPC is not strongly influenced by interest rates; consumption tends to be stable relative to income. In theory one might think that higher interest rates would induce more saving (the substitution effect) but higher interest rates also mean than people do not have to save as much for the future.\n\nEconomists often distinguish between the marginal propensity to consume out of permanent income, and the marginal propensity to consume out of temporary income, because if consumers expect a change in income to be permanent, then they have a greater incentive to increase their consumption. This implies that the Keynesian multiplier should be \"larger\" in response to permanent changes in income than it is in response to temporary changes in income (though the earliest Keynesian analyses ignored these subtleties). However, the distinction between permanent and temporary changes in income is often subtle in practice, and it is often quite difficult to designate a particular change in income as being permanent or temporary. What is more, the marginal propensity to consume should also be affected by factors such as the prevailing interest rate and the general level of consumer surplus that can be derived from purchasing.\n\nMPC's importance depends on the multiplier theory. MPC determines the value of the multiplier. The higher the MPC, the higher the multiplier and vice versa. The relationship between the multiplier and the propensity to consume is as follows:\n\nSince formula_18 is the MPC, the multiplier formula_25 is, by definition, equal to formula_29. The multiplier can also be derived from MPS (marginal propensity to save) and it is the reciprocal of MPS, formula_30\n\nThe above table shows that the size of the multiplier varies directly with the MPC and inversely with the MPS. Since the MPC is always greater than zero and less than one (i.e. formula_31), the multiplier is always between one and infinity (formula_32). If the multiplier is one, it means that the whole increment of income is saved and nothing is spent because the MPC is zero. On the other hand, an infinite multiplier implies that MPC is equal one and the entire increment of income is spent on consumption. It will soon lead to full employment in the economy and then create a limitless inflationary spiral. But these are rare phenomenon. Therefore, the multiplier coefficient varies between one and infinity.\n\nWhen income increases, the MPC falls but more than the APC. Conversely, when income falls, the MPC rises and the APC also rises but at a slower rate than the former. Such changes are only possible during cyclical fluctuations whereas in the short-run there is no change in the MPC and formula_33.\nKeynes is concerned primarily with the MPC, for his analysis pertains to the short-run while the APC is useful in the long-run analysis. The post-Keynesian economists have come to the conclusion that over the long-run APC and MPC are equal and approximate 0.9. In the Keynesian analysis the MPC is given more prominence. Its value is assumed to be positive and less than unity which means that when income increases the whole of it is not spent on consumption. On the contrary, when income falls, consumption expenditure does not decline in the same proportion and never becomes zero. The Keynesian hypothesis is that the marginal propensity to consume is positive but less than unity (formula_34) is of great analytical and practical significance. Besides telling us that consumption is an increasing function of income and it increases by less than the increment of income, this hypothesis helps in explaining\n1) The theoretical possibility of general overproduction or \"underemployment equilibrium\" and also\n2) The relative stability of a highly developed industrial economy. For it implies that the gap between income and consumption at all high levels of income is too wide to be easily filled by investment with the possible consequences that the economy may fluctuate around underemployment equilibrium.\nThus the economic significance of the MPC lies in filling the gap between income and consumption through planned investment to maintain the desired level of income.\n\nThe MPC is higher in the case of poorer people than in rich. When a person earns a higher income, the cost of their basic human needs amount to a smaller fraction of this income, and correspondingly their average propensity to save is higher than that of a person with a lower income. The marginal propensity to save of the richer classes is greater than that of the poorer classes. If, at any time, it is desired to increase aggregate consumption, then the purchasing power should be transferred from the richer classes (with low propensity to consume) to the poorer classes (with a higher propensity to consume). Likewise, if it is desired to reduce community consumption, the purchasing power must be taken away from the poorer classes by taxing consumption. The marginal propensity to consume is higher in a poor country and lower in the case of rich country. The reason is same as stated above. In the case of rich country, most common of the basic needs of the people have already been satisfied, and all the additional increments of income are saved, resulting in a higher marginal propensity to save but in a lower marginal propensity to consume. In a poor country, on the other hand, most of the basic needs of the people remain unsatisfied so that additional increments of income go to increase consumption, resulting in a higher marginal propensity to consume and a lower marginal propensity to save. This is the reason MPC is higher in the underdeveloped countries of Asia and Africa, and lower in developed countries such as the United States, the United Kingdom, Singapore and Germany.\n\nMuch of the current discussion seems to rely on the MPC being unique to a country, and homogeneous across such an economic entity; and the theory and the mathematical formulae apply to this use of the term. However, individuals have an MPC, and furthermore MPC is not homogeneous across society. Even if it was, the nature of the consumption is not homogeneous. Some consumption may be seen as more benevolent (to the economy) than others. Therefore, spending could be targeted where it would do most benefit, and thus generate the highest (closest to 1) MPC. This has traditionally been regarded as construction or other major projects (which also bring a direct benefit in the form of the finished product).\nClearly, some sectors of society are likely to have a much higher MPC than others. Someone with above average wealth or income or both may have a very low (short-term, at least) MPC of nearly zero—saving most of any extra income. But a pensioner, for example, will have an MPC of 1 or even greater than 1. This is because a pensioner is quite likely to spend every penny of any extra income. Further, if the extra income is seen as regular extra income, and guaranteed into the future, the pensioner may actually spend MORE than the extra £1. This would occur where the extra income stream gives confidence that the individual does not need to put aside as much in the form of savings; or perhaps can even dip into existing savings.\nMore importantly, this consumption is much more likely to occur in local small business—local shops, pubs and other leisure activities for example. These types of businesses are themselves likely to have a high MPC, and again the nature of their consumption is likely to be in the same, or next tier of businesses, and also of a benevolent nature.\nOther individuals with a high, and benevolent, MPC would include almost anyone on a low income—students, parents with young children, and the unemployed.\n\n"}
{"id": "3785733", "url": "https://en.wikipedia.org/wiki?curid=3785733", "title": "Marginal rate of technical substitution", "text": "Marginal rate of technical substitution\n\nIn microeconomic theory, the Marginal Rate of Technical Substitution (MRTS)—or Technical Rate of Substitution (TRS)—is the amount by which the quantity of one input has to be reduced (formula_1) when one extra unit of another input is used (formula_2), so that output remains constant (formula_3).\n\nformula_4\n\nwhere formula_5 and formula_6 are the marginal products of input 1 and input 2, respectively.\n\nAlong an isoquant, the MRTS shows the rate at which one input (e.g. capital or labor) may be substituted for another, while maintaining the same level of output. Thus the MRTS is the absolute value of the slope of an isoquant at the point in question.\n\nWhen relative input usages are optimal, the marginal rate of technical substitution is equal to the relative unit costs of the inputs, and the slope of the isoquant at the chosen point equals the slope of the isocost curve (see Conditional factor demands). It is the rate at which one input is substituted for another to maintain the same level of output.\n\n"}
{"id": "1402030", "url": "https://en.wikipedia.org/wiki?curid=1402030", "title": "Marginal revenue productivity theory of wages", "text": "Marginal revenue productivity theory of wages\n\nThe marginal revenue productivity theory of wages is a theory in neoclassical economics stating that wages are paid at a level equal to the marginal revenue product of labor, MRP (the value of the marginal product of labor), which is the increment to revenues caused by the increment to output produced by the last laborer employed. In a model, this is justified by an assumption that the firm is profit-maximizing and thus would employ labor only up to the point that marginal labor costs equal the marginal revenue generated for the firm. \n\nThe marginal revenue product (MRP) of a worker is equal to the product of the marginal product of labour (MP) (the increment to output from an increment to labor used) and the marginal revenue (MR) (the increment to sales revenue from an increment to output): MRP = MP × MR. The theory states that workers will be hired up to the point when the marginal revenue product is equal to the wage rate. If the marginal revenue brought by the worker is less than the wage rate, then employing that laborer would cause a decrease in profit.\n\nThe idea that payments to factors of production equal their marginal productivity had been laid out by John Bates Clark and Knut Wicksell, in simpler models. Much of the MRP theory stems from Wicksell's model.\n\nThe marginal revenue product of labour MRP is the increase in revenue per unit increase in the variable input = ∆TR/∆L \n\nThe change in output is not limited to that directly attributable to the additional worker. Assuming that the firm is operating with diminishing marginal returns then the addition of an extra worker reduces the average productivity of every other worker (and every other worker affects the marginal productivity of the additional worker).\n\nAs above noted the firm will continue to add units of labor until the MRP equals the wage rate \"w\"—mathematically until\n\nUnder perfect competition, marginal revenue product is equal to marginal physical product (extra unit produced as a result of a new employment) multiplied by price.\n\nThis is because the firm in perfect competition is a price taker. It does not have to lower the price in order to sell additional units of the good.\n\nFirms operating as monopolies or in imperfect competition face downward-sloping demand curves. To sell extra units of output, they would have to lower their output's price. Under such market conditions, marginal revenue product will not equal MPP×Price. This is because the firm is not able to sell output at a fixed price per unit. Thus the MRP curve of a firm in monopoly or in imperfect competition will slope downwards, when plotted against labor usage, at a faster rate than in perfect specific competition.\n"}
{"id": "46476968", "url": "https://en.wikipedia.org/wiki?curid=46476968", "title": "Model worker", "text": "Model worker\n\nModel worker (, abbreviated as 劳模 or láomó) is a Communist Chinese political term referring to an exemplary worker who exhibits some or all of the traits appropriate to the ideal of the socialist worker. The idea is similar to the Soviet Stakhanovite icon. Model workers are selected in China by central and provincial-level departments. Some cities and large companies also have processes for selecting and praising model workers.\n\nThe basic criteria for model workers are patriotism, \"worship of science,\" activities in environmental protection, and the pursuit of excellence.\n\nModel workers are often afforded privileges not available to other citizens or Communist Party members. \"The possibility to become a model worker offered peasants and workers one of the few opportunities for upward mobility other than joining the army,\" writes scholar Yu Miin-lin. Model workers have an easier time joining the Communist Party, and also to become a higher-level cadre, manager, or other leader.\n\nOne of the earliest model workers was the teenage textile worker Hao Jianxiu (awarded 1951), who invented the \"Hao Jianxiu Work Method\". She was sent to study at East China Textile Engineering Institute and was elevated to the upper echelon of Chinese politics, serving as Minister of Textile Industry, secretary of the CPC Central Secretariat, and vice chair of the State Planning Commission.\nAnother prominent model worker was Ni Zhifu (awarded 1959), a fitter who invented the \"Ni Zhifu drill\". He was elevated to leadership positions in the municipal governments of Beijing, Shanghai, and Tianjin, and became a member of the Politburo of the Communist Party of China. He also served as Chairman of the All-China Federation of Trade Unions.\n"}
{"id": "4602393", "url": "https://en.wikipedia.org/wiki?curid=4602393", "title": "Models of scientific inquiry", "text": "Models of scientific inquiry\n\nIn the philosophy of science, models of scientific inquiry have two functions: first, to provide a descriptive account of \"how\" scientific inquiry is carried out in practice, and second, to provide an explanatory account of \"why\" scientific inquiry succeeds as well as it appears to do in arriving at genuine knowledge.\n\nThe search for scientific knowledge ends far back into antiquity. At some point in the past, at least by the time of Aristotle, philosophers recognized that a fundamental distinction should be drawn between two kinds of scientific knowledge—roughly, knowledge \"that\" and knowledge \"why\". It is one thing to know \"that\" each planet periodically reverses the direction of its motion with respect to the background of fixed stars; it is quite a different matter to know \"why\". Knowledge of the former type is descriptive; knowledge of the latter type is explanatory. It is explanatory knowledge that provides scientific understanding of the world. (Salmon, 2006, pg. 3)\n\n\"Scientific inquiry refers to the diverse ways in which scientists study the natural world and propose explanations based on the evidence derived from their work.\"\n\nThe classical model of scientific inquiry derives from Aristotle, who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.\n\nWesley Salmon (1989) began his historical survey of scientific explanation with what he called the \"received view\", as it was received from Hempel and Oppenheim in the years beginning with their \"Studies in the Logic of Explanation\" (1948) and culminating in Hempel's \"Aspects of Scientific Explanation\" (1965). Salmon summed up his analysis of these developments by means of the following Table.\n\nIn this classification, a deductive-nomological (D-N) explanation of an occurrence is a valid deduction whose conclusion states that the outcome to be explained did in fact occur. The deductive argument is called an \"explanation\", its premisses are called the \"explanans\" (L: \"explaining\") and the conclusion is called the \"explanandum\" (L: \"to be explained\"). Depending on a number of additional qualifications, an explanation may be ranked on a scale from \"potential\" to \"true\".\n\nNot all explanations in science are of the D-N type, however. An \"inductive-statistical\" (I-S) explanation accounts for an occurrence by subsuming it under statistical laws, rather than categorical or universal laws, and the mode of subsumption is itself inductive instead of deductive. The D-N type can be seen as a limiting case of the more general I-S type, the measure of certainty involved being complete, or probability 1, in the former case, whereas it is less than complete, probability < 1, in the latter case.\n\nIn this view, the D-N mode of reasoning, in addition to being used to explain particular occurrences, can also be used to explain general regularities, simply by deducing them from still more general laws.\n\nFinally, the \"deductive-statistical\" (D-S) type of explanation, properly regarded as a subclass of the D-N type, explains statistical regularities by deduction from more comprehensive statistical laws. (Salmon 1989, pp. 8–9).\n\nSuch was the \"received view\" of scientific explanation from the point of view of logical empiricism, that Salmon says \"held sway\" during the third quarter of the last century (Salmon, p. 10).\n\nDuring the course of history, one theory has succeeded another, and some have suggested further work while others have seemed content just to explain the phenomena. The reasons why one theory has replaced another are not always obvious or simple. The philosophy of science includes the question: \"What criteria are satisfied by a 'good' theory\". This question has a long history, and many scientists, as well as philosophers, have considered it. The objective is to be able to choose one theory as preferable to another without introducing cognitive bias. Several often proposed criteria were summarized by Colyvan. A good theory:\n\nStephen Hawking supports items 1–4, but does not mention fruitfulness. On the other hand, Kuhn emphasizes the importance of seminality.\n\nThe goal here is to make the choice between theories less arbitrary. Nonetheless, these criteria contain subjective elements, and are heuristics rather than part of scientific method. Also, criteria such as these do not necessarily decide between alternative theories. Quoting Bird:\nIt also is debatable whether existing scientific theories satisfy all these criteria, which may represent goals not yet achieved. For example, explanatory power over all existing observations (criterion 3) is satisfied by no one theory at the moment.\nThe desiderata of a \"good\" theory have been debated for centuries, going back perhaps even earlier than Occam's razor, which often is taken as an attribute of a good theory. Occam's razor might fall under the heading of \"elegance\", the first item on the list, but too zealous an application was cautioned by Albert Einstein: \"Everything should be made as simple as possible, but no simpler.\" It is arguable that \"parsimony\" and \"elegance\" \"typically pull in different directions\". The falsifiability item on the list is related to the criterion proposed by Popper as demarcating a scientific theory from a theory like astrology: both \"explain\" observations, but the scientific theory takes the risk of making predictions that decide whether it is right or wrong:\n\nThomas Kuhn argued that changes in scientists' views of reality not only contain subjective elements, but result from group dynamics, \"revolutions\" in scientific practice which result in paradigm shifts. As an example, Kuhn suggested that the heliocentric \"Copernican Revolution\" replaced the geocentric views of Ptolemy not because of empirical failures, but because of a new \"paradigm\" that exerted control over what scientists felt to be the more fruitful way to pursue their goals.\n\nDeductive logic and inductive logic are quite different in their approaches.\n\nDeductive logic is the reasoning of proof, or logical implication. It is the logic used in mathematics and other axiomatic systems such as formal logic. In a deductive system, there will be axioms (postulates) which are not proven. Indeed, they cannot be proven without circularity. There will also be primitive terms which are not defined, as they cannot be defined without circularity. For example, one can define a line as a set of points, but to then define a point as the intersection of two lines would be circular. Because of these interesting characteristics of formal systems, Bertrand Russell humorously referred to mathematics as \"the field where we don't know what we are talking about, nor whether or not what we say is true\". All theorems and corollaries are proven by exploring the implications of the axiomata and other theorems that have previously been developed. New terms are defined using the primitive terms and other derived definitions based on those primitive terms.\n\nIn a deductive system, one can correctly use the term \"proof\", as applying to a theorem. To say that a theorem is proven means that it is impossible for the axioms to be true and the theorem to be false. For example, we could do a simple syllogism such as the following:\n\n\nNotice that it is not possible (assuming all of the trivial qualifying criteria are supplied) to be in Arches and not be in Utah. However, one can be in Utah while not in Arches National Park. The implication only works in one direction. Statements (1) and (2) taken together imply statement (3). Statement (3) does not imply anything about statements (1) or (2). Notice that we have not proven statement (3), but we have shown that statements (1) and (2) together imply statement (3). In mathematics, what is proven is not the truth of a particular theorem, but that the axioms of the system imply the theorem. In other words, it is impossible for the axioms to be true and the theorem to be false. The strength of deductive systems is that they are sure of their results. The weakness is that they are abstract constructs which are, unfortunately, one step removed from the physical world. They are very useful, however, as mathematics has provided great insights into natural science by providing useful models of natural phenomena. One result is the development of products and processes that benefit mankind.\n\nLearning about the physical world requires the use of inductive logic. This is the logic of theory building. It is useful in such widely divergent enterprises as science and crime scene detective work. One makes a set of observations, and seeks to explain what one sees. The observer forms a hypothesis in an attempt to explain what he/she has observed. The hypothesis will have implications, which will point to certain other observations that would naturally result from either a repeat of the experiment or making more observations from a slightly different set of circumstances. If the predicted observations hold true, one feels excitement that they may be on the right track. However, the hypothesis has not been proven. The hypothesis implies that certain observations should follow, but positive observations do not imply the hypothesis. They only make it more believable. It is quite possible that some other hypothesis could also account for the known observations, and may do better with future experiments. The implication flows in only one direction, as in the syllogism used in the discussion on deduction. Therefore, it is never correct to say that a scientific principle or hypothesis/theory has been proven. (At least, not in the rigorous sense of proof used in deductive systems.)\n\nA classic example of this is the study of gravitation. Newton formed a law for gravitation stating that the force of gravitation is directly proportional to the product of the two masses and inversely proportional to the square of the distance between them. For over 170 years, all observations seemed to validate his equation. However, telescopes eventually became powerful enough to see a slight discrepancy in the orbit of Mercury. Scientists tried everything imaginable to explain the discrepancy, but they could not do so using the objects that would bear on the orbit of Mercury. Eventually, Einstein developed his theory of general relativity and it explained the orbit of Mercury and all other known observations dealing with gravitation. During the long period of time when scientists were making observations that seemed to validate Newton's theory, they did not, in fact, prove his theory to be true. However, it must have seemed at the time that they did. It only took one counterexample (Mercury's orbit) to prove that there was something wrong with his theory.\n\nThis is typical of inductive logic. All of the observations that seem to validate the theory, do not prove its truth. But one counter-example can prove it false. That means that deductive logic is used in the evaluation of a theory. In other words, if A implies B, then not B implies not A. Einstein's theory of General Relativity has been supported by many observations using the best scientific instruments and experiments. However, his theory now has the same status as Newton's theory of gravitation prior to seeing the problems in the orbit of Mercury. It is highly credible and validated with all we know, but it is not proven. It is only the best we have at this point in time.\n\nAnother example of correct scientific reasoning is shown in the current search for the Higgs boson. Scientists on the Compact Muon Solenoid experiment at the Large Hadron Collider have conducted experiments yielding data suggesting the existence of the Higgs boson. However, realizing that the results could possibly be explained as a background fluctuation and not the Higgs boson, they are cautious and waiting for further data from future experiments. Said Guido Tonelli:\n\nA brief overview of the scientific method would then contain these steps as a minimum:\n\n\nWhen a hypothesis has survived a sufficient number of tests, it may be promoted to a scientific theory. A theory is a hypothesis that has survived many tests and seems to be consistent with other established scientific theories. Since a theory is a promoted hypothesis, it is of the same 'logical' species and shares the same logical limitations. Just as a hypothesis cannot be proven but can be disproved, that same is true for a theory. It is a difference of degree, not kind.\n\nArguments from analogy are another type of inductive reasoning. In arguing from analogy, one infers that since two things are alike in several respects, they are likely to be alike in another respect. This is, of course, an assumption. It is natural to attempt to find similarities between two phenomena and wonder what one can learn from those similarities. However, to notice that two things share attributes in several respects does not imply any similarities in other respects. It is possible that the observer has already noticed all of the attributes that are shared and any other attributes will be distinct. Argument from analogy is an unreliable method of reasoning that can lead to erroneous conclusions, and thus cannot be used to establish scientific facts.\n\n\n\nFor interesting explanations regarding the orbit of Mercury and General Relativity, the following links are useful:\n"}
{"id": "47077309", "url": "https://en.wikipedia.org/wiki?curid=47077309", "title": "Negative-dimensional space", "text": "Negative-dimensional space\n\nIn topology, a discipline within mathematics, a negative-dimensional space is an extension of the usual notion of space, allowing for negative dimensions.\n\nSuppose that is a compact space of Hausdorff dimension , which is an element of a scale of compact spaces embedded in each other and parametrized by (). Such scales are considered \"equivalent\" with respect to if the compact spaces constituting them coincide for . It is said that the compact space is the \"hole\" in this equivalent set of scales, and is the negative dimension of the corresponding equivalence class.\n\nBy the 1940s, the science of topology had developed and studied a thorough basic theory of topological spaces of positive dimension. Motivated by computations, and to some extent aesthetics, topologists searched\nfor mathematical frameworks that extended our notion of space to allow for negative dimensions. Such dimensions, as well as the fourth and higher dimensions, are hard to imagine since we are not able to directly observe them. It wasn’t until the 1960s that a special topological framework was constructed—the category of spectra. A spectrum is a generalization of space that allows for negative dimensions. The concept of negative-dimensional spaces is applied, for example, to analyze linguistic statistics.\n\n\n"}
{"id": "3558732", "url": "https://en.wikipedia.org/wiki?curid=3558732", "title": "Negative and positive rights", "text": "Negative and positive rights\n\nNegative and positive rights are rights that oblige either action (\"positive rights\") or inaction (\"negative rights\"). These obligations may be of either a legal or moral character. The notion of positive and negative rights may also be applied to liberty rights.\n\nTo take an example involving two parties in a court of law: Adrian has a \"negative right to x\" against Clay if and only if Clay is \"prohibited\" from acting upon Adrian in some way regarding \"x\". In contrast, Adrian has a \"positive right to x\" against Clay if and only if Clay is obliged to act upon Adrian in some way regarding \"x\". A case in point, if Adrian has a \"negative right to life\" against Clay, then Clay is required to refrain from killing Adrian; while if Adrian has a \"positive right to life\" against Clay, then Clay is required to act as necessary to preserve the life of Adrian.\n\nRights considered \"negative rights\" may include civil and political rights such as freedom of speech, life, private property, freedom from violent crime, freedom of religion, \"habeas corpus\", a fair trial, and freedom from slavery.\n\nRights considered \"positive rights\", as initially proposed in 1979 by the Czech jurist Karel Vasak, may include other civil and political rights such as police protection of person and property and the right to counsel, as well as economic, social and cultural rights such as food, housing, public education, employment, national security, military, health care, social security, internet access, and a minimum standard of living. In the \"three generations\" account of human rights, negative rights are often associated with the first generation of rights, while positive rights are associated with the second and third generations.\n\nSome philosophers (see criticisms) disagree that the negative-positive rights distinction is useful or valid.\n\nUnder the theory of positive and negative rights, a negative right is a right \"not to be\" subjected to an action of another person or group—a government, for example—usually in the form of abuse or coercion. As such, negative rights exist unless someone acts to \"negate\" them. A positive right is a right \"to be\" subjected to an action of another person or group. In other words, for a positive right to be exercised, someone else's actions must be \"added\" to the equation. In theory, a negative right forbids others from acting against the right holder, while a positive right obligates others to act with respect to the right holder. In the framework of the Kantian categorical imperative, negative rights can be associated with perfect duties while positive rights can be connected to imperfect duties.\n\nBelief in a distinction between positive and negative rights is usually maintained, or emphasized, by libertarians, who believe that positive rights do not exist until they are created by contract. The United Nations Universal Declaration of Human Rights lists both positive and negative rights (but does not identify them as such). The constitutions of most liberal democracies guarantee negative rights, but not all include positive rights. Nevertheless, positive rights are often guaranteed by other laws, and the majority of liberal democracies provide their citizens with publicly funded education, health care, social security and unemployment benefits.\n\nRights are often spoken of as inalienable and sometimes even absolute. However, in practice this is often taken as graded absolutism; rights are ranked by degree of importance, and violations of lesser ones are accepted in the course of preventing violations of greater ones. Thus, even if the right not to be killed is inalienable, the corresponding obligation on others to refrain from killing is generally understood to have at least one exception: self-defense. Certain widely accepted negative obligations (such as the obligations to refrain from theft, murder, etc.) are often considered prima facie, meaning that the legitimacy of the obligation is accepted \"on its face\"; but even if not questioned, such obligations may still be ranked for ethical analysis.\n\nThus a thief may have a negative obligation not to steal, and a police officer may have a negative obligation not to tackle people—but a police officer tackling the thief easily meets the burden of proof that he acted justifiably, since his was a breach of a lesser obligation and negated the breach of a greater obligation. Likewise a shopkeeper or other passerby may also meet this burden of proof when tackling the thief. But if any of those individuals pulled a gun and shot the (unarmed) thief for stealing, most modern societies would not accept that the burden of proof had been met. The obligation not to kill—being universally regarded as one of the highest, if not the highest obligation—is so much greater than the obligation not to steal that a breach of the latter does not justify a breach of the former. Most modern societies insist that other, very serious ethical questions need come into play before stealing could justify killing.\n\nPositive obligations confer duty. But as we see with the police officer, exercising a duty may violate negative obligations (e.g. not to overreact and kill). For this reason, in ethics positive obligations are almost never considered \"prima facie\". The greatest negative obligation may have just one exception—one higher obligation of self-defense—but even the greatest positive obligations generally require more complex ethical analysis. For example, one could easily justify failing to help, not just one, but a great many injured children quite ethically in the case of triage after a disaster. This consideration has led ethicists to agree in a general way that positive obligations are usually junior to negative obligations because they are not reliably \"prima facie\". Some critics of positive rights implicitly suggest that because positive obligations are not reliably \"prima facie\" they must always be agreed to through contract.\n\nNineteenth-century philosopher Frédéric Bastiat summarized the conflict between these negative and positive rights by saying:\nAccording to Jan Narveson, the view of some that there is no distinction between negative and positive rights on the ground that negative rights require police and courts for their enforcement is \"mistaken\". He says that the question between what one has a right to do and who if anybody enforces it are separate issues. If rights are only negative then it simply means no one has a duty to enforce them, although individuals have a right to use any non-forcible means to gain the cooperation of others in protecting those rights. Therefore, he says \"the distinction between negative and positive is quite robust.\" Libertarians hold that positive rights, which would include a right to be protected, do not exist until they are created by contract. However, those who hold this view do not mean that police, for example, are not obligated to protect the rights of citizens. Since they contract with their employers to defend citizens from violence, then they have created that obligation to their employer. A negative right to life allows an individual to defend his life from others trying to kill him, or obtain voluntary assistance from others to defend his life—but he may not force others to defend him, because he has no natural right to be provided with defense. To force a person to defend one's own negative rights, or the negative rights of a third party, would be to violate that person's negative rights.\n\nOther advocates of the view that there is a distinction between negative and positive rights argue that the presence of a police force or army is not due to any positive right to these services that citizens claim, but rather because they are natural monopolies or public goods—features of any human society that arise naturally, even while adhering to the concept of negative rights only. Robert Nozick discusses this idea at length in his book \"Anarchy, State, and Utopia\".\n\nIn the field of medicine, positive rights of patients often conflict with negative rights of physicians. In controversial areas such as abortion and assisted suicide, medical professionals may not wish to offer certain services for moral or philosophical reasons. If enough practitioners opt out as a result of conscience, a right granted by conscience clause statutes in many jurisdictions, patients may not have any means of having their own positive rights fulfilled. Such was the case of Janet Murdock, a Montana woman who could not find any physician to assist her suicide in 2009. This controversy over positive and negative rights in medicine has become a focal point in the ongoing public debate between conservative ethicist Wesley J. Smith and bioethicist Jacob M. Appel. In discussing \"Baxter v. Montana\", Appel has written:\nSmith replies that this is \"taking the duty to die and transforming it into a duty to kill\", which he argues \"reflects a profound misunderstanding of the government’s role\".\n\nPresumably, if a person has positive rights it implies that other people have positive duties (to take certain actions); whereas negative rights imply that others have negative duties (to avoid certain other actions). Philosopher Henry Shue is skeptical; he believes that all rights (regardless of whether they seem more \"negative\" or \"positive\") requires both kinds of duties at once. In other words, Shue says that honouring a right will require avoidance (a \"negative\" duty) but also protective or reparative actions (\"positive\" duties). The negative positive distinction may be a matter of emphasis; it is therefore unhelpful to describe \"any right\" as though it requires only one of the two types of duties.\n\nTo Shue, rights can always be understood as confronting \"standard threats\" against humanity. Dealing with standard threats requires all kinds of duties, which may be divided across time (e.g. \"if avoiding the harmful behaviour fails, begin to repair the damages\"), but also divided across people. The point is that every right provokes all 3 types of behaviour (avoidance, protection, repair) to some degree. Dealing with a threat like murder, for instance, will require one individual to practice avoidance (e.g. the potential murderer must stay calm), others to protect (e.g. the police officer, who must stop the attack, or the bystander, who may be obligated to call the police), and others to repair (e.g. the doctor who must resuscitate a person who has been attacked). Thus, even the negative right not to be killed can only be guaranteed with the help of some positive duties. Shue goes further, and maintains that the negative and positive rights distinction can be harmful, because it may result in the neglect of necessary duties.\n\nJames P. Sterba makes similar criticisms. He holds that any right can be made to appear either positive or negative depending on the language used to define it. He writes:\n\nSterba has rephrased the traditional \"positive right\" to provisions, and put it in the form of a sort of \"negative right\" \"not to be prevented\" from taking the resources on their own.. Thus, all rights may not only require both \"positive\" and \"negative\" duties, but it seems that rights that do not involve forced labor can be phrased positively or negatively at will. The distinction between positive and negative may not be very useful, or justified, as rights requiring the provision of labor can be rephrased from \"right to education\" or \"right to health care\" to \"right to take surplus money to pay teachers\" or \"right to take surplus money to pay doctors\".\n\n\n\n"}
{"id": "2795942", "url": "https://en.wikipedia.org/wiki?curid=2795942", "title": "Negative cutting", "text": "Negative cutting\n\nNegative cutting (also known as negative matching and negative conforming) is the process of cutting motion picture negative to match precisely the final edit as specified by the film editor. Original camera negative (OCN) is cut with scissors and joined using a film splicer and film cement. Negative cutting is part of the post-production process and occurs after editing and prior to striking internegatives and release prints. The process of negative cutting has changed little since the beginning of cinema in the early 20th century. In the early 1980s computer software was first used to aid the cutting process. Kodak introduced barcode on motion picture negative in the mid-1990s. This enabled negative cutters to more easily track shots and identify film sections based on keykode.\n\nToward the late 1990s and early 2000s negative cutting changed due to the advent of digital cinema technologies such as digital intermediate (DI), digital projection and high-definition television. In some countries, due to the high cost of online suites, negative cutting is still used for commercials by reducing footage. Increasingly feature films are bypassing the negative cutting process altogether and are being scanned directly from the uncut rushes.\n\nThe existence of digital intermediates (DI) has created a new demand for negative cutters to extract selected takes which are cut from the rushes and re-spliced into new rolls (in edit order) to reduce the volume of footage for scanning.\n\nAfter a film shoot, the original camera negative (OCN) is sent to a film laboratory for processing. Two or three camera rolls are spliced together to create a lab roll approximately long. After developing the lab roll, it is put through a telecine to create a rushes transfer tape. This rushes transfer tape is of lower quality than film and is used for editing purposes only.\n\nThe rushes tape is sent to the Editor who loads it into an offline edit suite. The lab rolls are sent to the negative cutter for logging and storage.\n\nAfter the Editor finishes the Edit it is exported to an offline EDL list and the EDL list is sent to the negative cutter. The negative cutter will translate the Timecode in the EDL list to edge numbers (keykode) using specially designed negative cutting software to find which shot is needed from the rushes negative.\n\nTraditionally a negative cutter would then fine cut the negative to match the Editor's final edit frame accurately. Negative would be spliced together to create rolls less than which would then be sent to the film laboratory to print release prints.\n\nToday most feature films are extracted full takes (as selected takes) and scanned digitally as a digital intermediate. Television series and commercials shot on film follow the same extraction process but are sent for telecine. Each required shot is extracted from the lab roll as a full take and respliced together to create a new selected roll of negative. This reduces the negative required by up to 1/10 of the footage shot, saving considerable time during scanning or telecine. The negative cutter will create a new Online EDL list replacing the rushes roll timecode with the new selected roll timecode.\n\nIn the case of feature films the selected roll and Online EDL are sent to a post production facility for scanning as a digital intermediate. For television commercials or series the selected takes and EDL are sent to a post production facility for re-telecine and compiled in an Online Suite for final grading.\n\nThere have been a number of dedicated software systems that have been developed for and by negative cutters to manage the process of cutting motion picture negative. A number of individual proprietary software systems have been developed starting in the early 1980s. Stan Sztaba developed a system for World Cinevision Services Inc (New York) in 1983 using Apple II DOS and then ProDOS, this system is still used today. Elliott Gamson of Immaculate Matching (New York) developed a system using MS-DOS. Computamatch was one of the first MS-DOS-based systems developed and is still in use today in several countries.\n\nThe first commercially available software product was OSC/R (pronounced \"Oscar\"), a DOS-based application developed in Toronto, Canada by The Adelaide Works. OSC/R was very widely used and at the time was the only negative cutting software on the market until Adelaide Works ceased operation in 1993. OSC/R is still used today in some negative cutting facilities but has been mostly replaced by newer and more advanced systems. Excalibur was a later Windows 98 based product developed by FilmLab Engineering in Britain. Film Fusion is one of the most recent developments and is a Windows XP and Vista based system developed in Sydney, Australia by Popsoft IT.\n\nNegative cutters use various hardware tools such as film synchronizers, re-winders, film splicers, scissors, film cement and film keykode readers. DigiSync, a purpose built keykode reader is used by most negative cutters in conjunction with software for logging the keykode from film. DigiSync was developed by Research In Motion and in 1998 it won a Technical Achievement Academy Award for the design and development of the DigiSync Film Keykode Reader. Research In Motion later moved on to bigger things and invented the BlackBerry Wireless Email Phone and is now a publicly listed company. Other brands of barcode scanners are also in use.\n\n"}
{"id": "548558", "url": "https://en.wikipedia.org/wiki?curid=548558", "title": "Negative pressure", "text": "Negative pressure\n\nNegative pressure may refer to:\n\n"}
{"id": "2288927", "url": "https://en.wikipedia.org/wiki?curid=2288927", "title": "Negative thermal expansion", "text": "Negative thermal expansion\n\nNegative thermal expansion (NTE) is an unusual physicochemical process in which some materials contract upon heating, rather than expand as most other materials do. Materials which undergo NTE have a range of potential engineering, photonic, electronic, and structural applications. For example, if one were to mix a negative thermal expansion material with a \"normal\" material which expands on heating, it could be possible to make a zero expansion composite material.\n\nThere are a number of physical processes which may cause contraction with increasing temperature, including transverse vibrational modes, Rigid Unit Modes and phase transitions.\n\nRecently, Liu et al. showed that the NTE phenomenon originates from the existence of high pressure, small volume configurations with higher entropy, with their configurations present in the stable phase matrix through thermal fluctuations. They were able to predict both the colossal positive thermal expansion (In cerium) and zero and infinite negative thermal expansion (in ) \n\nNegative thermal expansion is usually observed in non-close-packed systems with directional interactions (e.g. ice, graphene, etc.) and complex compounds (e.g. , , beta-quartz, some zeolites, etc.). However, in a paper, it was shown that negative thermal expansion (NTE) is also realized in single-component close-packed lattices with pair central force interactions. The following sufficient condition for potential giving rise to NTE behavior is proposed:\n\nformula_1\n\nwhere formula_2 is pair interatomic potential, formula_3 is the equilibrium distance. This condition is (i) necessary and sufficient in 1D and (ii) sufficient, but not necessary in 2D and 3D. \"An approximate\" necessary and sufficient condition is derived in a paper\n\nformula_4\n\nwhere formula_5 is the space dimensionality. Thus in 2D and 3D negative thermal expansion in close-packed systems with pair interactions is realized even when the third derivative of the potential is zero or even negative. Note that one-dimensional and multidimensional cases are qualitatively different. In 1D thermal expansion is cased by anharmonicity of interatomic potential only. Therefore, the sign of thermal expansion coefficient is determined by the sign of the third derivative of the potential. In multidimensional case the geometrical nonlinearity is also present, i.e. lattice vibrations are nonlinear even in the case of harmonic interatomic potential. This nonlinearity contributes to thermal expansion. Therefore, in multidimensional case both formula_6 and formula_7 are present in the condition for negative thermal expansion.\n\nThere are many potential applications for materials with controlled thermal expansion properties, as thermal expansion causes many problems in engineering, and indeed in everyday life. One simple example of a thermal expansion problem is the tendency of dental fillings to expand by an amount different from the teeth, for example when drinking a hot drink, causing toothache. If dental fillings were made of a composite material containing a mixture of materials with positive and negative thermal expansion then the overall expansion could be precisely tailored to that of tooth enamel.\n\nPerhaps one of the most studied materials to exhibit \"negative thermal expansion\" is zirconium tungstate (). This compound contracts continuously over a temperature range of 0.3 to 1050 K (at higher temperatures the material decomposes). Other materials that exhibit this behaviour include: other members of the family of materials (where A = or , M = or ) and . also is an example of controllable \"negative thermal expansion\".\n\nOrdinary ice shows NTE in its hexagonal and cubic phases at very low temperatures (below –200 °C). In its liquid form, pure water also displays negative thermal expansivity below 3.984 °C.\n\nRubber elasticity shows NTE at normal temperatures, but the reason for the effect is rather different from that in most other materials. Put simply, as the long polymer chains absorb energy, they adopt a more contorted configuration, reducing the volume of the material.\n\nQuartz () and a number of zeolites also show NTE over certain temperature ranges. Fairly pure silicon (Si) has a negative coefficient of thermal expansion for temperatures between about 18 K and 120 K.\nCubic Scandium trifluoride has this property which is explained by the quartic oscillation of the fluoride ions. The energy stored in the bending strain of the fluoride ion is proportional to the fourth power of the displacement angle, unlike most other materials where it is proportional to the square of the displacement. A fluorine atom is bound to two scandium atoms, and as temperature increases the fluorine oscillates more perpendicularly to its bonds. This draws the scandium atoms together throughout the material and it contracts. exhibits this property from 10 to 1100 K above which it shows the normal positive thermal expansion.. Shape memory alloys such as NiTi are a nascent class of materials that exhibits zero and negative thermal expansion [13].\n\n 13. ^ Ahadi, A.; Matsushita, Y.; Sawaguchi, T.; Sun, QP.; Tsuchiya, K. (2017). \"Origin of zero and negative thermal expansion in severely-deformed superelastic NiTi alloy\". Acta Materialia. 124, 79–92. doi:10.1107/S0108768194004933.\nhttps://doi.org/10.1016/j.actamat.2016.10.054\n\n\n"}
{"id": "25262679", "url": "https://en.wikipedia.org/wiki?curid=25262679", "title": "Phonemic imagery", "text": "Phonemic imagery\n\nPhonemic imagery refers to the processing of thoughts as words rather than as symbols or other images. It is sometimes referred to as the equivalent of inner speech or covert speech, and sometimes considered as a third phenomenon, separate from but similar to these other forms of internal speech.\n\nPhonemic imagery is a part of the philosophy of consciousness rather than linguistics as it is considered an internal phenomenon of consciousness observed through reflection rather than amenable to empirical observation.\n"}
{"id": "40600057", "url": "https://en.wikipedia.org/wiki?curid=40600057", "title": "Planck's principle", "text": "Planck's principle\n\nIn sociology of scientific knowledge, Planck's principle is the view that scientific change does not occur because individual scientists change their mind, but rather that successive generations of scientists have different views.\n\nThe reason for the name is the statements by Max Planck:\n\nPlanck's quote has been used by Thomas Kuhn, Paul Feyerabend and others to argue that scientific revolutions are arational, rather than spreading through \"mere force of truth and fact\". It has been described as Darwinian rather than Lamarckian conceptual evolution.\n\nWhether age influences the readiness to accept new ideas has been empirically criticised. In the case of acceptance of evolution in the years after Darwin's \"On the Origin of Species\" age was a minor factor. Similarly, it was a weak factor in accepting cliometrics.\n"}
{"id": "1166245", "url": "https://en.wikipedia.org/wiki?curid=1166245", "title": "Principle of distributivity", "text": "Principle of distributivity\n\nThe principle of distributivity states that the algebraic distributive law is valid for classical logic, where both logical conjunction and logical disjunction are distributive over each other so that for any propositions \"A\", \"B\" and \"C\" the equivalences\nand\nhold.\n\nThe principle of distributivity is valid in classical logic, but invalid in quantum logic.\n\nThe article \"Is Logic Empirical?\" discusses the case that quantum logic is the correct, empirical logic, on the grounds that the principle of distributivity is inconsistent with a reasonable interpretation of quantum phenomena.\n"}
{"id": "39812836", "url": "https://en.wikipedia.org/wiki?curid=39812836", "title": "Process reference models", "text": "Process reference models\n\nA process reference model is a model that has generic functionality and can be used more than once in different models. The creator of a process model benefits from existing process reference models by not needing to reinvent the process model but only reusing it as a starting point in creating a process model for a specific purpose.\n\nDuring the identification of processes ideal for reuse, the designer needs to (1) Get approval (2) Provide Organization Scope Context and (3) Identify Process Standardization Opportunities.\n"}
{"id": "33920294", "url": "https://en.wikipedia.org/wiki?curid=33920294", "title": "Psychotherapy and social action model", "text": "Psychotherapy and social action model\n\nThe psychotherapy and social action model is an approach to psychotherapy characterized by concentration on past and present personal, social, and political obstacles to mental health. In particular, the goal of this therapeutic approach is to acknowledge that individual symptoms are not unique, but rather shared by people similarly oppressed and marginalized. Ultimately, the psychotherapy and social action model aims to aid clients in overcoming mental illness through personal psychotherapy, group coping, and collective social action.\n\n The psychotherapy and social action model was initially proposed by Sue Holland, a psychotherapist with a background in community action. Holland developed this framework in 1980 following her experience working with women coping with psychological disorders at a housing estate in West London. At this estate, Holland observed the psychological difficulties experienced by women, noticing that their mental health was fundamentally tied to the social and economic obstacles they encountered as females in their society. In addition, Holland took issue with the way Depression (mood) was being treated at the shelter, believing that individualized treatment, especially with the use of psychotropic medication, was not successfully addressing the root of the dysfunction for these women. Instead, Holland posited a pathway from individual treatment to sociopolitical action that empowered women to deal with their mental dysfunction both privately and socially. As such, the psychotherapy and social action model is rooted in the ideals of both traditional psychotherapy and feminist empowerment.\n\nThe square model derives from the sociological theory of the four paradigms for the analysis of social theory. Outside the frame of the model, the dichotomy of individual versus social approaches to personal well-being is represented. The two bottom cells of the square delineate the changing of individuals to conform to social convention while the two top cells of the square represent the changing of social structures as opposed to the individual.\n\nThe four cells within the frame represent the four paradigms of social theory including functionalist, interpretive, radical humanist, and radical structuralist paradigms. Functionalism here is rooted in regulation and objective thinking, and represents the individual, status-quo approach to mental health. The interpretive paradigm is characterized by an approach to understanding the social world through subjective experience, and represents psychoeducation within the psychotherapy framework. The radical humanist paradigm is defined by a radial approach to change with an emphasis on “transcending limitations of existing social arrangements.” (Burrell & Morgan, 1979, p. 32). With respect to an approach to therapy, this stage is characterized by the adoption of a social self, such that healing occurs at a group or collective level. The radical structuralist paradigm concentrates on radical change through political or economic emancipation. This is the endpoint of therapy, at which time the client is empowered to challenge sociopolitical structures that foster the conditions perpetuating the manifestation of individual mental illness within an oppressed group.\n\nTaken from her 1992 publication entitled, “From Social Abuse to Social Action: a neighborhood psychotherapy and social action project for women,” Holland formulated her four step approach to mental health and social action for women in treatment for depression as follows:\n\nAt this stage, patients endorse the status-quo characterization of the “individualized patient.” As such, they treat their disorder passively with psychotropic medication and accept the label associated with their illness.\n\nThis stage represents the first alternative to the status-quo treatment of psychiatric disorders: talk therapy. At this stage, clients and therapists are able to explore the meaning of their psychopathology and pinpoint the potential causes through individual therapy.\n\nAt this stage, the client is able to move past the personal challenges that are acknowledged and addressed in psychotherapy and discover that the challenges are universal amongst similarly marginalized individuals. Together, clients aim to acknowledge what is best for the collective.\n\nThe final stage, as the name suggests, is the point at which the collective mobilizes to change the social structures enabling their common oppression. Having changed from an individual to a collective, the clients should feel empowered to undertake social change.\n\nIncluded in this framework is the assumption that only some of the clients in this therapy will traverse all three stages. In Holland’s words, “…many will be content enough with the relief from symptoms and the freedom to get on with their personal lives which the individual therapy gives them.” (Holland, 1992, p. 73). Thus, this framework is fluid based on the personal inclinations of the client throughout the therapeutic process.\n\n• Women’s Action for Mental Health (WAMH)\n• Men’s Advice Network (MAN)\n• Travers (1997)\n"}
{"id": "6394087", "url": "https://en.wikipedia.org/wiki?curid=6394087", "title": "Revelation principle", "text": "Revelation principle\n\nThe revelation principle is a fundamental principle in mechanism design. It states that if a social choice function can be implemented by an arbitrary mechanism (i.e. if that mechanism has an equilibrium outcome that corresponds to the outcome of the social choice function), then the same function can be implemented by an incentive-compatible-direct-mechanism (i.e. in which players truthfully report type) with the same equilibrium outcome (payoffs).\n\nIn mechanism design, the revelation principle is of utmost importance in finding solutions. The researcher need only look at the set of equilibrium characterized by incentive compatibility. That is, if the mechanism designer wants to implement some outcome or property, he can restrict his search to mechanisms in which agents are willing to reveal their private information to the mechanism designer that has that outcome or property. If no such direct and truthful mechanism exists, no mechanism can implement this outcome/property. By narrowing the area needed to be searched, the problem of finding a mechanism becomes much easier.\n\nThe principle comes in two variants corresponding to the two flavors of incentive-compatibility:\n\nConsider the following example. There is a certain item that Alice values as formula_1 and Bob values as formula_2. The government needs to decide who will receive that item and in what terms. \n\nSuppose we have an arbitrary mechanism Mech that implements Soc.\n\nWe construct a direct mechanism Mech' that is truthful and implements Soc.\n\nMech' simply simulates the equilibrium strategies of the players in Game(Mech). I.e:\n\nReporting the true valuations in Mech' is like playing the equilibrium strategies in Mech. Hence, reporting the true valuations is a Nash equilibrium in Mech', as desired. Moreover, the equilibrium payoffs are the same, as desired.\n\nThe revelation principle says that for every arbitrary \"coordinating device\" a.k.a. correlating there exists another direct device for which the state space equals the action space of each player. Then the coordination is done by directly informing each player of his action.\n\n"}
{"id": "25606638", "url": "https://en.wikipedia.org/wiki?curid=25606638", "title": "Search neutrality", "text": "Search neutrality\n\nSearch neutrality is a principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance. This means that when a user queries a search engine, the engine should return the most relevant results found in the provider's domain (those sites which the engine has knowledge of), without manipulating the order of the results (except to rank them by relevance), excluding results, or in any other way manipulating the results to a certain bias. \nSearch neutrality is related to network neutrality in that they both aim to keep any one organization from limiting or altering a user's access to services on the Internet. Search neutrality aims to keep the organic search results (results returned because of their relevance to the search terms, as opposed to results sponsored by advertising) of a search engine free from any manipulation, while network neutrality aims to keep those who provide and govern access to the Internet from limiting the availability of resources to access any given content.\n\nThe term \"search neutrality\" in context of the internet appears as early as March 2009 in an academic paper by Andrew Odlyzko titled, \"Network Neutrality, Search Neutrality, and the Never-ending Conflict between Efficiency and Fairness in Markets\". In this paper, Odlykzo predicts that if net neutrality were to be accepted as a legal or regulatory principle, then the questions surrounding search neutrality would be the next controversies. Indeed, in December 2009 the New York Times published an opinion letter by Foundem co-founder and lead complainant in an anti-trust complaint against Google, Adam Raff, which likely brought the term to the broader public. According to Raff in his opinion letter, search neutrality ought to be \"the principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance\". On October 11, 2009, Adam and his wife Shivaun launched SearchNeutrality.org, an initiative dedicated to promoting investigations against Google's search engine practices . There, the Raffs note that they chose to frame their issue with Google as \"search neutrality\" in order to benefit from the focus and interest on net neutrality.\n\nIn contrast to net neutrality, answers to such questions, as \"what is search neutrality?\" or \"what are appropriate legislative or regulatory principles to protect search neutrality?\", appear to have less consensus. The idea that neutrality means equal treatment, regardless of the content, comes from debates on net neutrality . Neutrality in search is complicated by the fact that search engines, by design and in implementation, are not intended to be neutral or impartial. Rather, search engines and other information retrieval applications are designed to collect and store information (indexing), receive a query from a user, search for and filter relevant information based on that query (searching/filtering), and then present the user with only a subset of those results, which are ranked from most relevant to least relevant (ranking). \"Relevance\" is a form of bias used to favor some results and rank those favored results. Relevance is defined in the search engine so that a user is satisfied with the results and is therefore subject to the user's preferences. And because relevance is so subjective, putting search neutrality into practice has been so contentious.\n\nSearch neutrality became a concern after search engines, most notably Google, were accused of search bias by other companies. Competitors and companies claim search engines systematically favor some sites (and some kind of sites) over others in their lists of results, disrupting the objective results users believe they are getting. \n\nThe call for search neutrality goes beyond traditional search engines. Sites like Amazon.com and Facebook are also accused of skewing results. Amazon’s search results are influenced by companies that pay to rank higher in their search results while Facebook filters their newsfeed lists to conduct social experiments.\n\nIn order to find information on the Web, most users make use of search engines, which crawl the web, index it and show a list of results ordered by relevance. The use of search engines to access information through the web has become a key factor for online businesses companies, which depend on the flow of users visiting their pages. One of these companies is Foundem. Foundem provides a \"vertical search\" service to compare products available on online markets for the U.K. Many people see these \"vertical search\" sites as spam. Beginning in 2006 and for three and a half years following, Foundem’s traffic and business dropped significantly due to what they assert to be a penalty deliberately applied by Google. It is unclear, however, whether their claim of a penalty was self-imposed via their use of iframe HTML tags to embed the content from other websites. At the time at which Foundem claims the penalties were imposed, it was unclear whether web crawlers crawled beyond the main page of a website using iframe tags without some extra modifications. The former SEO director OMD UK, Jaamit Durrani, among others, offered this alternative explanation, stating that “Two of the major issues that Foundem had in summer was content in iFrames and content requiring javascript to load – both of which I looked at in August, and they were definitely in place. Both are huge barriers to search visibility in my book. They have been fixed somewhere between then and the lifting of the supposed ‘penalty’. I don’t think that’s a coincidence.” \n\nMost of Foundem’s accusations claim that Google deliberately applies penalties to other vertical search engines because they represent competition. Foundem is backed by a Microsoft proxy group, the 'Initiative for Competitive Online Marketplace'.\n\nThe following table details Foundem's chronology of events as found on their website:\n\nGoogle's large market share (85%) has made them a target for search neutrality litigation via antitrust laws. In February 2010, Google released an article on the Google Public Policy blog expressing their concern for fair competition, when other companies at the UK joined Foundem's cause (eJustice.fr, and Microsoft's Ciao! from Bing) also claiming being unfairly penalized by Google.\n\nAfter two years of looking into claims that Google “manipulated its search algorithms to harm vertical websites and unfairly promote its own competing vertical properties,” the Federal Trade Commission (FTC) voted unanimously to end the antitrust portion of its investigation without filing a formal complaint against Google. The FTC concluded that Google’s “practice of favoring its own content in the presentation of search results” did not violate U.S. antitrust laws. The FTC further determined that even though competitors might be negatively impacted by Google's changing algorithms, Google did not change its algorithms to hurt competitors, but as a product improvement to benefit consumers.\n\nThere are a number of arguments for and against search neutrality.\n\n\n\nGoogle’s \"Universal Search\" system has been identified as one of the least neutral search engine practices, and following the implementation of Universal Search websites, such as MapQuest, the company experienced a massive decline in web traffic. This decline has been attributed to Google linking to its own services rather than the services offered at external websites. Despite these claims, Google actually displays Google content on the first page, while rival search engines do not considerably less often than Microsoft's Bing which displays Microsoft content when rivals do not. Bing displays Microsoft content in first place more than twice as often as Google shows Google content in first place. This indicates that as far as there is any 'bias', Google is less biased than its principal competitor.\n"}
{"id": "5615980", "url": "https://en.wikipedia.org/wiki?curid=5615980", "title": "Tree of knowledge system", "text": "Tree of knowledge system\n\nThe tree of knowledge (ToK) system is a theoretical approach to the unification of psychology developed by Gregg Henriques, associate professor and director of the Combined-Integrated Doctoral Program in Clinical and School Psychology at James Madison University.\n\nThe outline of the system was published in 2003 in \"Review of General Psychology\". Two special issues of the \"Journal of Clinical Psychology\" in December 2004 and January 2005 were devoted to the elaboration and evaluation of the model. The latest evaluation of this model appeared in a December 2008 special issue of \"Theory & Psychology''.\n\nThe official website on the tree of knowledge system claims that the ToK is\n\nHenriques argues that the most difficult problem in psychology as a discipline is that while there is incredible diversity offered by different approaches to psychology, there is no consensus model of what \"psychology\" actually is.\nAccording to the ToK system, the \"problem of psychology\", (as Henriques puts it), is that a clear definition, an agreed upon subject matter, and a coherent conceptual framework have eluded its students for its entire history. He further argues that the patent tendency of psychology has been toward theoretical and substantial fragmentation and increasing insularity among the \"specialties.\" In other words, the discipline has fragmented into different schools of thought and methodology, with no overall framework to interpret and integrate the research of different areas. At its best, the different approaches are a strength of psychology; different approaches lead to novel ideas, and prevent psychologists from clinging to a paradigm that fails to explain a phenomenon. At its worst, adherents of one particular school cling to their beliefs concerning the relative importance of their research and disregard or are ignorant of different approaches. In most cases, individual psychologists have to determine for themselves which elements of which perspective to apply, and how to integrate them into their overall understanding.\nThe reason for psychology's fragmentation, according to the ToK, is that there has been no meta-theoretical frame that allows scholars to agree on the basic questions that need to be addressed. As such, the different schools of thought in psychology are like the blind men who each grab a part of the elephant and proclaim they have discovered its true nature. With its novel depiction of evolving dimensions of complexity, the ToK allows scholars finally to see the elephant. In his 2003 \"Review of General Psychology\" paper, Henriques used the ToK System with the attempt to clarify and align the views of B.F. Skinner and Sigmund Freud. These luminaries were chosen because when one considers their influence and historical opposition, it can readily be argued that they represent two schools of thought that are the most difficult to integrate. Henriques used the meta-perspective offered by the ToK to argue how one can retain the key insights from each school of thought, identify errors and points of confusion, and integrate the insights into a coherent whole.\n\nCultural and personality psychologist, Michael Katzko, however critiques Henriques' position on \"the problem of psychology\":\n\nIn one way, the tree of knowledge system reflects a fairly common hierarchy of nature and of the sciences that has been represented in one way or another since the time of Auguste Comte, who in the 19th century used a hierarchical conception of nature to argue for the existence of sociology. Despite its surface agreement with a standard conception, the ToK System offers a set of ideas that have added implications for both ontology and epistemology. The ontological claim made by the ToK, (and depicted pictorially above), is that cosmic evolution consists of four separable dimensions of complexity, namely matter, life, mind, and culture. The dimension of complexity argument is arguably one of the most complicated aspects of the system. Many have argued nature is hierarchically leveled; for example, a list of such levels might be subatomic particles, atoms, molecules, cells, organ structures, multi-celled organisms, consciousness, and society is common. The ToK System embraces a view of nature as levels, but adds the notion that there are also \"dimensions of complexity\". The difference can be seen pictorially. A view of nature as solely consisting of levels would have a single \"cone\" of complexity, whereas the ToK depicts four \"cones\". The ToK posits that a separate dimension of complexity emerges when a process of selection operates on a unit of information. Thus, according to the ToK, natural selection operating on genetic combinations gives rise to the dimension of Life; behavioral selection operating on neuronal combinations gives rise to the dimension of Mind; and justification operating on symbolic combinations gives rise to Culture.\n\nThe ToK system also offers a new epistemology that Henriques believes will move toward what E.O. Wilson termed \"consilience\". Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. The ToK offers alternative perspectives on how knowledge is obtained because it depicts science itself as both emerging out of culture and as a unique type of \"justification system\" that is based on the values of accuracy and objectivity. A \"justification system\", according to Henriques, refers to any belief system that emerges that coordinates the behaviors of individual humans to human populations. The four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nThe dimension of matter refers to the set of material objects and their behaviors through time. In accordance with modern cosmology, matter is theorized to have emerged from a pure energy singularity at the Big Bang. Space and time were also born at such a point. Nonliving material objects range in complexity from subatomic particles to large organic molecules. The physical sciences (i.e., physics, chemistry, geology, astronomy) describe the behavior of material objects.\n\nThe dimension of life refers to organisms and their behaviors through time. Living objects are considered a unique subset of material objects. Just as quantum particles form the fundamental units of material complexity, genes are the fundamental units of living information. Although many questions about the emergence of life remain unanswered, in accordance with modern biology, the ToK posits that natural selection operating on genetic combinations through time is the unified theory of biology and forms the foundational understanding for the emergence of organic complexity.\n\nMind/cognition in the ToK system refers to the set of mental behaviors. \"Mental behaviors\" are behaviors of animals mediated by the nervous system that produce a functional effect on the animal-environment relationship. As such, Mind/cognition is essentially synonymous with what behavioral psychologists have meant when they use the term behavior. Thus, a fly avoiding a fly swatter, a rat pushing a bar or a human getting a drink of water are all mental behaviors. Mind is not synonymous with sentience or the capacity for mental experience, although such processes are presumed to emerge in the mental/cognitive dimension. Cognition, in the broad sense of the term is meaning bodily-neuro-social information processing, as in EEEE Cognition: Embodied, Embedded, Enactive, Extended. While cognitive science stands for naturalist study of mind, psychology is an approach grounded in the tradition of humanities, especially philosophy. Thus, by defining mind as mental behavior, Henriques argues that the ToK System provides a way to bridge the epistemological differences between cognitive and behavioral science.\n\nCulture in the ToK system refers to the set of sociolinguistic behaviors, which range from large scale nation states to individual human justifications for particular actions. Just as genetic information processing is associated with the Life dimension and neuronal information processing associated with the Mind dimension, symbolic information processing emerges with the Cultural dimension.\n\nQuantum gravity refers to the imagined merger between the twin pillars of physical science which are quantum mechanics, the study of the microscopic (e.g., electrons), and general relativity, the science of the macroscopic (e.g., galaxies). Currently, these two great domains of science cannot be effectively interwoven into a single, physical Theory of Everything. Yet progress is being made, most notably through string theory, loop quantum gravity, black hole thermodynamics and the study of the early universe. Some of the difficulties combining these two pillars of physical science are philosophical in nature and it is possible that the macro view of knowledge offered by the ToK may eventually aid in the construction of a coherent theory of quantum gravity. The reason the ToK might help is that it locates scientific knowledge in relationship to the physical universe.\n\nThe modern synthesis refers to the merger of genetics with natural selection which occurred in the 1930s and 1940s and offers a reasonably complete framework for understanding the emergence of biological complexity. Although there remain significant gaps in biological knowledge surrounding questions such as the origin of life and the emergence of sexual reproduction, the modern synthesis represents the most complete and well-substantiated joint point.\n\nBehavioral investment theory (BIT) is proposed as a merger of the selection science of behaviorism with the information science of cognitive neuroscience (notice the parallel with the modern synthesis). BIT posits that the nervous system evolved as an increasingly flexible computational control system that coordinates the behavioral expenditure of energy of the animal as a whole. Expenditure of behavioral energy is theorized to be computed on an investment value system built evolutionarily through natural selection operating on genetic combinations and ontogenetically through behavioral selection operating on neural combinations. As such, the current behavioral investments of the animal are conceptualized as the joint product of the two vectors of phylogeny and ontogeny. A unique element of BIT is that it finds a core of agreement and builds bridges between five brain-behavior paradigms: (1) cognitive science; (2) behavioral science; (3) evolutionary theory and genetics; (4) neuroscience; and (5) cybernetics/systems theory.\n\nDavid C. Geary noted the similarities between his \"motive-to-control\" hypothesis and Henriques' Behavioral Investment Theory, which were developed independently of each other. Furthermore, Geary suggested that his model \"seem[ed] to fill in many of the proximate mechanisms and evolutionary pressures that define the life-mind joint point, and provided a framework for further development of the mind-culture joint point.\"\n\nThe justification hypothesis (JH) is a novel proposal that allows for both the understanding of the evolution of culture and for identifying what makes humans distinct animals. A basic initial claim of the JH is that the process of justification is a crucial component of human mental behavior at both the individual and societal level. Unlike all other animals, humans everywhere ask for and give explanations for their actions. Arguments, debates, moral dictates, rationalizations, and excuses all involve the process of explaining why one's claims, thoughts or actions are warranted. In virtually every form of social exchange, from warfare to politics to family struggles to science, humans are constantly justifying their behavioral investments to themselves and others.\n\nThe JH can be stated succinctly as follows: The evolution of language gave rise to the problem of justification, and this evolutionary pressure ultimately resulted in the human self-consciousness system and human culture. The JH carries with it three fundamental postulates. \n\nThe problem of psychology, according to the ToK, is its conceptual incoherence, which Henriques identifies by the following: \n\nWhen the various conceptions of psychology (e.g., behavioral, humanistic, cognitive) are viewed through the lens of the ToK System, psychology spans two different dimensions of complexity: the mental and the cultural. In other words, the discipline has historically spanned two fundamentally separate problems: \nIf, as previously thought, nature simply consisted of levels of complexity, psychology would not be crisply defined in relationship to biology or the social sciences. And, indeed, it is frequently suggested that psychology exists in an amorphous space between biology and the social sciences. However, with its dimension of complexity depiction, the ToK System suggests that psychology can be crisply defined as the science of mind, which is the third dimension of complexity. Furthermore, because human behavior exists in the fourth dimension, psychology must be divided into two broad scientific domains of \n\n\"Psychological formalism\" is defined as the science of mind and corresponds to the behavior of animal objects. \"Human psychology\" is considered to be a unique subset of psychological formalism that deals with human behavior at the level of the individual. Because human behavior is immersed in the larger socio-cultural context (level four in the ToK System), human psychology is considered a hybrid discipline that merges the pure science of psychology with the social sciences. It is important to point out that there are other disciplines the ToK System would classify as “hybrids.” Molecular genetics, for example, is a hybrid between chemistry and biology and neuroscience is a hybrid between biology and psychology. As with Henriques' proposed conception of human psychology, both of these disciplines adopt an object level perspective (molecular and cellular, respectively) on phenomena that simultaneously exist as part of meta-level system processes (life and mind, respectively).\n\nThough David A. F. Haaga \"congratulate[d] Dr. Henriques' ambitious, scholarly, provocative paper\", and \"found the Tree of Knowledge taxonomy, the theoretical joint points, the evolutionary history, and the levels of emergent properties highly illuminating\", he asks the rhetorical questions, \n\nIn a similar vein, Scott O. Lilienfeld, who described Henriques' effort as \"thoughtful\", contended that psychology is \"an inherently fuzzy concept that resists precise definition\" and that \"attempts to define psychology [would be] likely to hamper rather than foster consilience across disciplines\". Lilienfield went on further to suggest that the scientist-practitioner gap in psychology lies not in definitional issues, but in different \"epistemic attitudes\" between these two groups. He stated that scientists have an epistemic attitude of empiricism, (where questions regarding human nature are settled by scientific evidence), and that practitioners have an epistemic attitude of romanticism, (where questions of human nature are settled by intuition). Lilienfeld suggested that the solution to the scientist-practitioner gulf isn't definitional, but in \"train[ing] future clinical scientists to appreciate the proper places of romanticism and empiricism within science\".\n\nA frequent question and point of confusion in the ToK System is the definition and meaning of consciousness. As mentioned above, mind is not synonymous with consciousness. And, to understand consciousness from a ToK vantage point, it is crucial to recognize that the term is often ambiguous in its meaning. Two primary meanings are sentience, which is the capacity for mental experience and self-awareness, which is the capacity to be aware of one's awareness. Sentience is conceptualized as a \"level 3\" phenomena, possessed by many animals other than humans and is defined as a \"perceived\" electro-neuro-chemical representation of animal-environment relations. The ingredient of neurological behavior that allows for the emergence of mental experience is considered the \"hard\" problem of conscious and the ToK System does not address this question explicitly. In contrast, through the Justification Hypothesis (see below), the ToK System involves a very direct analysis of the other issue of consciousness, that of self-awareness. \nAnother frequent question that is raised is \"Where does individual human behavior fall on the ToK?\" To analyze human behavior from the context of the ToK, one uses the ToK like a prism to separate the dimensions of behavior into physiochemical, biogenetic, neuropsychological and sociolinguistic. Thus if we imagine a conversation between a husband and wife as follows:\n\nThe words represent the sociolinguistic dimension and are understood as a function of justification. Justification systems are seen both at the level of individual, micro-social and societal (i.e., the context of justification in which men work and women stay at home). The actions of the husband and wife in terms of facial expression, body movement, etc. are seen as the mental dimension and are understood as a function of behavioral investment. The physiological make up of the organ systems and cells of each body is seen as the biogenetic dimension. Finally, the position, temperature, molecular make up is seen as the physiochemical dimension. Each of the more basic dimensions represent conditions of possibility that allow for the emergence of the higher dimension of process. Thus, insufficient oxygen disrupts organic processes which in turn renders neuropsychological and sociolinguistic processes impossible.\n\nAs stated above, the ToK System proposes a new epistemology with the goal of moving academic knowledge toward what E.O. Wilson termed consilience. Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. Henriques argues that the ToK affords new perspectives on how knowledge is obtained because it depicts how science emerges from culture and that the four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nHenriques further argues that developing such a system for integrating knowledge is not just an academic enterprise. He suggests that in an increasingly complex world, the fragmented state of knowledge can be seen as one of the most pressing social problems of our time. Henriques also believes that history seems to attest that the absence of a collective worldview ostensibly condemns humanity to an endless series of conflicts that inevitably stem from incompatible, partially correct, locally situated justification systems. Thus, from Henriques' perspective, there are good reasons for believing that if there was a shared, general background of explanation, humanity might be able to achieve much greater levels of harmonious relations.\nIn a 2008 article on the ToK, Henriques cites Oliver Reiser's 1958 call for unifying scientific knowledge that Henriques implies is similar in theme to the ToK:\n\nWith its depiction of the dimensions of complexity and interlocking theoretical joint points, Henriques' believes that his ToK System offers new avenues that might allow scholars to meet Reiser’s call for academic synthesis. Henriques, like Reiser, believes that with a shared sense of purpose and a common background of explanation, people might yet be able to integrate bodies of knowledge into a unified interpretation of humanity, with humanity's place in nature and its potentialities for creating the good society.\n\n\n"}
{"id": "37055344", "url": "https://en.wikipedia.org/wiki?curid=37055344", "title": "Triangle of opposition", "text": "Triangle of opposition\n\nIn the system of Aristotelian logic, the triangle of opposition is a diagram representing the different ways in which each of the three propositions of the system is logically related ('opposed') to each of the others. The system is also useful in the analysis of syllogistic logic, serving to identify the allowed logical conversions from one type to another.\n\n"}
{"id": "97517", "url": "https://en.wikipedia.org/wiki?curid=97517", "title": "Unintended consequences", "text": "Unintended consequences\n\nIn the social sciences, unintended consequences (sometimes unanticipated consequences or unforeseen consequences) are outcomes that are not the ones foreseen and intended by a purposeful action. The term was popularised in the twentieth century by American sociologist Robert K. Merton.\n\nUnintended consequences can be grouped into three types:\n\nThe idea of \"unintended consequences\" dates back at least to John Locke who discussed the unintended consequences of interest rate regulation in his letter to Sir John Somers, Member of Parliament.\n\nThe idea was also discussed by Adam Smith, the Scottish Enlightenment, and consequentialism (judging by results).\n\nSociologist Robert K. Merton popularised this concept in the twentieth century.\n\nIn \"The Unanticipated Consequences of Purposive Social Action\" (1936), Merton tried to apply a systematic analysis to the problem of unintended consequences of deliberate acts intended to cause social change. He emphasized that his term \"purposive action\", \"[was exclusively] concerned with 'conduct' as distinct from 'behavior.' That is, with action that involves motives and consequently a choice between various alternatives\". Merton's usage included deviations from what Max Weber defined as rational social action: instrumentally rational and value rational. Merton also stated that \"no blanket statement categorically affirming or denying the practical feasibility of \"all\" social planning is warranted.\"\n\nMore recently, the \"law of unintended consequences\" has come to be used as an adage or idiomatic warning that an intervention in a complex system tends to create unanticipated and often undesirable outcomes.\n\nAkin to Murphy's law, it is commonly used as a wry or humorous warning against the hubristic belief that humans can fully control the world around them.\n\nPossible causes of unintended consequences include the world's inherent complexity (parts of a system responding to changes in the environment), perverse incentives, human stupidity, self-deception, failure to account for human nature, or other cognitive or emotional biases. As a sub-component of complexity (in the scientific sense), the chaotic nature of the universe—and especially its quality of having small, apparently insignificant changes with far-reaching effects (e.g., the butterfly effect)—applies.\n\nRobert K. Merton listed five possible causes of unanticipated consequences in 1936:\n\n\nIn addition to Merton's causes, psychologist Stuart Vyse has noted that groupthink, described by Irving Janis, has been blamed for some decisions that result in unintended consequences.\n\nThe creation of \"no-man's lands\" during the Cold War, in places such as the border between Eastern and Western Europe, and the Korean Demilitarized Zone, has led to large natural habitats.\n\nThe sinking of ships in shallow waters during wartime has created many artificial coral reefs, which can be scientifically valuable and have become an attraction for recreational divers. Retired ships have been purposely sunk in recent years, in an effort to replace coral reefs lost to global warming and other factors.\n\nIn medicine, most drugs have unintended consequences ('side effects') associated with their use. However, some are beneficial. For instance, aspirin, a pain reliever, is also an anticoagulant that can help prevent heart attacks and reduce the severity and damage from thrombotic strokes. The existence of beneficial side effects also leads to off-label use—prescription or use of a drug for an unlicensed purpose. Famously, the drug Viagra was developed to lower blood pressure, with its use for treating erectile dysfunction being discovered as a side effect in clinical trials.\n\nThe implementation of a profanity filter by AOL in 1996 had the unintended consequence of blocking residents of Scunthorpe, North Lincolnshire, England from creating accounts due to a false positive. The accidental censorship of innocent language, known as the Scunthorpe problem, has been repeated and widely documented.\n\nThe objective of microfinance initiatives is to foster micro-entrepreneurs but an unintended consequence can be informal intermediation: That is, some entrepreneurial borrowers become informal intermediaries between microfinance initiatives and poorer micro-entrepreneurs. Those who more easily qualify for microfinance split loans into smaller credit to poorer borrowers. Informal intermediation ranges from casual intermediaries at the good or benign end of the spectrum to 'loan sharks' at the professional and sometimes criminal end of the spectrum.\n\nIn 1990, the Australian state of Victoria made safety helmets mandatory for all bicycle riders. While there was a reduction in the number of head injuries, there was also an unintended reduction in the number of juvenile cyclists—fewer cyclists obviously leads to fewer injuries, assuming all else being equal. The risk of death and serious injury per cyclist seems to have increased, possibly due to risk compensation. Research by Vulcan, \"et al.\" found that the reduction in juvenile cyclists was because the youths considered wearing a bicycle helmet unfashionable. A health-benefit model developed at Macquarie University in Sydney suggests that, while helmet use reduces \"the risk of head or brain injury by approximately two-thirds or more\", the decrease in exercise caused by reduced cycling as a result of helmet laws is counterproductive in terms of net health.\n\nProhibition in the 1920s United States, originally enacted to suppress the alcohol trade, drove many small-time alcohol suppliers out of business and consolidated the hold of large-scale organized crime over the illegal alcohol industry. Since alcohol was still popular, criminal organisations producing alcohol were well-funded and hence also increased their other activities. Similarly, the War on Drugs, intended to suppress the illegal drug trade, instead increased the power and profitability of drug cartels who became the primary source of the products.\n\nIn CIA jargon, \"blowback\" describes the unintended, undesirable consequences of covert operations, such as the funding of the Afghan Mujahideen and the destabilization of Afghanistan contributing to the rise of the Taliban and Al-Qaeda.\n\nThe introduction of exotic animals and plants for food, for decorative purposes, or to control unwanted species often leads to more harm than good done by the introduced species.\n\nThe protection of the steel industry in the United States reduced production of steel in the United States, increased costs to users, and increased unemployment in associated industries.\n\nIn 2003, Barbra Streisand unsuccessfully sued Kenneth Adelman and Pictopia.com for posting a photograph of her home online. Before the lawsuit had been filed, only 6 people had downloaded the file, two of them Streisand's attorneys. The lawsuit drew attention to the image, resulting in 420,000 people visiting the site. The Streisand effect was named after this incident, describing when an attempt to censor or remove a certain piece of information instead draws attention to the material being suppressed, resulting in the material instead becoming widely known, reported on, and distributed.\n\nPassenger-side airbags in motorcars were intended as a safety feature, but led to an increase in child fatalities in the mid-1990s as small children were being hit by deploying airbags during collisions. The supposed solution to this problem, moving the child seat to the back of the vehicle, led to an increase in the number of children forgotten in unattended vehicles, some of whom died under extreme temperature conditions.\n\nRisk compensation, or the Peltzman effect, occurs after implementation of safety measures intended to reduce injury or death (e.g. bike helmets, seatbelts, etc.). People may feel safer than they really are and take additional risks which they would not have taken without the safety measures in place. This may result in no change, or even an increase, in morbidity or mortality, rather than a decrease as intended.\n\nThe British government, concerned about the number of venomous cobra snakes in Delhi, offered a bounty for every dead cobra. This was a successful strategy as large numbers of snakes were killed for the reward. Eventually, enterprising people began breeding cobras for the income. When the government became aware of this, they scrapped the reward program, causing the cobra breeders to set the now-worthless snakes free. As a result, the wild cobra population further increased. The apparent solution for the problem made the situation even worse, becoming known as the Cobra effect.\n\nTheobald Mathew's temperance campaign in 19th-century Ireland resulted in thousands of people vowing never to drink alcohol again. This led to the consumption of diethyl ether, a much more dangerous intoxicant — due to its high flammability — by those seeking to become intoxicated without breaking the letter of their pledge.\n\nIt was thought that adding south-facing conservatories to British houses would reduce energy consumption by providing extra insulation and warmth from the sun. However, people tended to use the conservatories as living areas, installing heating and ultimately increasing overall energy consumption.\n\nA reward for lost nets found along the Normandy coast was offered by the French government between 1980 and 1981. This resulted in people vandalizing nets to collect the reward.\n\nBeginning in the 1940s and continuing into the 1960s, the Canadian federal government gave the Catholic Church in Quebec $2.25 per day per psychiatric patient for their cost of care, but only $0.75 a day per orphan. The perverse result is that the orphan children were diagnosed as mentally ill so the church could receive the larger amount of money. This psychiatric misdiagnosis affected up to 20,000 people, and the children are known as the Duplessis Orphans.\n\nThere have been attempts to curb the consumption of sugary beverages by imposing a tax on them. However, a study found that the reduced consumption was only temporary. Also, there was an increase in the consumption of beer among households.\n\nThe New Jersey Childproof Handgun Law, which was intended to protect children from accidental discharge of firearms by forcing all future firearms sold in New Jersey to contain \"smart\" safety features, has delayed, if not stopped entirely, the introduction of such firearms to New Jersey markets. The wording of the law caused significant public backlash, fuelled by gun rights lobbyists, and several shop owners offering such guns received death threats and stopped stocking them In 2014, 12 years after the law was passed, it was suggested the law be repealed if gun rights lobbyists agree not to resist the introduction of \"smart\" firearms.\n\nDrug prohibition can lead drug traffickers to prefer stronger, more dangerous substances, that can be more easily smuggled and distributed than other, less concentrated substances.\n\nTelevised drug prevention advertisements may lead to increased drug use.\n\nAbstinence-only sex education has been shown to increase teenage pregnancy rates, rather than reduce them, when compared to either comprehensive sex education or no sex education at all.\n\nIncreasing usage of search engines, also including recent image search features, has contributed in the ease of which media is consumed. Some abnormalities in usage may have shifted preferences for pornographic film actors, as the producers began using common search queries or tags to label the actors in new roles.\n\nThe passage of the Stop Enabling Sex Traffickers Act has led to a reported increase in risky behaviors by sex workers as a result of quashing their ability to seek and screen clients online, forcing them back onto the streets or into the dark web. The ads posted were previously an avenue for advocates to reach out to those wanting to escape the trade.\n\nMost modern technologies have negative consequences that are both unavoidable and unpredictable. For example, almost all environmental problems, from chemical pollution to global warming, are the unexpected consequences of the application of modern technologies. Traffic congestion, deaths and injuries from car accidents, air pollution, and global warming are unintended consequences of the invention and large scale adoption of the automobile. Hospital infections are the unexpected side-effect of antibiotic resistance, and even human overpopulation is the side effect of various technological (i.e., agricultural and industrial) revolutions.\n\nBecause of the complexity of ecosystems, deliberate changes to an ecosystem or other environmental interventions will often have (usually negative) unintended consequences. Sometimes, these effects cause permanent irreversible changes. Examples include:\n\n"}
