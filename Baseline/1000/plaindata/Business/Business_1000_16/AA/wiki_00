{"id": "21474948", "url": "https://en.wikipedia.org/wiki?curid=21474948", "title": "A Guide to the Business Analysis Body of Knowledge", "text": "A Guide to the Business Analysis Body of Knowledge\n\nA Guide to the Business Analysis Body of Knowledge (BABOK Guide) is the globally recognized standard for the practice of business analysis. \"BABOK Guide\" reflects the collective knowledge of the business analysis community and presents the most widely accepted business analysis practices.\n\n\"BABOK Guide\" recognizes and reflects the fact that business analysis is continually evolving and is practised in a wide variety of forms and contexts. It defines the skills and knowledge that people who work with and employ business analysts should expect a skilled practitioner to demonstrate.\n\n\"BABOK Guide\" is a framework that describes the business analysis tasks that must be performed to deliver a solution that will provide value to the sponsoring organization. Each business analysis task contributes to this overall goal directly or indirectly. Many elements of a task may vary, including the form those tasks take, the order they are performed in, or the relative importance of the tasks.\n\n\"BABOK Guide\" describes the skills, knowledge, and competencies required to perform business analysis effectively. It does not describe the processes that people will follow to do business analysis.\n\nAccording to Capability Maturity Model Integration, organizations interested in process improvement need to adopt industry standards from \"Business Analysis Body of Knowledge\" (and other associated references) to lift their project delivery from the \"ad hoc\" to the \"managed\" level.\n\n\"BABOK Guide\" was first published by International Institute of Business Analysis (IIBA) as a draft document version 1.4, in October 2005, for consultation with the wider business analysis and project management community, to document and standardize generally accepted business analysis practices. The first formal release was at version 1.6 in June 2006. Version 2.0 was released 31 March 2009. Version 3 was released in April 2015.\n\n\"BABOK Guide\" is defined and updated by the professionals who use it every day and is under active review and consultation all the time. Prior to publication, \"BABOK Guide\" undergoes a full public review where any person can provide feedback and comment on the content.\n\nOnce the body of knowledge was established, IIBA created the Certified Business Analysis Professional (CBAP) designation to recognise senior business analysts who could demonstrate in-depth long-term experience in these knowledge areas (5–10 years in a dedicated business analyst role).\n\nIIBA also offers the Certification of Competency on Business Analysis (CCBA) designation that recognises Business Analysts with 3750 hours business analysis experience, including 900 hours experience in two knowledge areas or 500 hours experience in four knowledge areas, and 21 professional development hours.\n\nFor both certifications above the applicant must have a minimum high school education (or equivalent), two references from a career manager, client or Certified Business Analyst Professional and sign the IIBA Code of Conduct.\n\nIIBA also offers the Entry Certificate in Business Analysis (ECBA) that does not require a reference.\n\n\"BABOK Guide\" includes chapters on:\n\n\"BABOK Guide\" organizes business analysis tasks within 6 knowledge areas. Each task describes the typical knowledge, skills, deliverables, and techniques that the business analyst requires to be able to perform those tasks competently. The knowledge areas logically organize tasks but do not specify a sequence, process, or methodology.\n\nThe knowledge areas of \"BABOK Guide\" are:\n\n\n"}
{"id": "20605998", "url": "https://en.wikipedia.org/wiki?curid=20605998", "title": "ActiveReports", "text": "ActiveReports\n\nActiveReports is a .NET reporting tool used by developers of WinForms, ASP.NET, and HTML5 applications. It was originally developed by Data Dynamics, which was then acquired by GrapeCity. ActiveReports is a set of components and tools that facilitates the production of reports to display data in documents and web-based formats. It is written in managed C# code and allows Visual Studio programmers to leverage their knowledge of C# or Visual Basic.NET when programming with ActiveReports.\n\nAmong the components included with ActiveReports are exports to file formats such as PDF, Excel, RTF, and TIFF. The main components are a Visual Studio integrated report designer, and an API that developers use to create customized reports from a variety of data sources. ActiveReports Standard Edition also includes a Visual Query Designer, a customizable Windows Viewer control, an HTML5 Viewer control, and a WPF Viewer control.\n\nThe integrated report designer handles three types of reports:\n\n\nThe Professional Edition of ActiveReports includes the Standard Edition tools plus an End-User Report Designer control that developers use to host the report designer in their own Microsoft Windows applications to let end users create and modify reports. It also includes a server-side ASP.NET web viewer with Flash, PDF, and HTML viewer types; ASP.NET HTTP Handlers that export reports to HTML or PDF format without custom code; a Silverlight viewer; and advanced PDF encryption and rendering features.\n\nActiveReports Server is a scalable server platform with built in load balancing. It provides a secure and extensible report server to publish reports designed in ActiveReports. A comprehensive RESTful API enables integration of the responsive HTML5 Report Portal for end users to view, schedule and export reports.\n\n\n\n\n\n\n\nLatest Service Releases\n\nStandard Edition\nalso support for various things\n\nActiveReports Designer\n\nWindows Forms Viewer\n\nReporting Engine\n\nIncludes all of the Standard Edition features, and adds the following:.\n\nEnd-User Report Designer\n\nASP.NET\n\n\n\nMay 15, 2015 - SD Times 100 GrapeCity wins Best In Show in the User Experience category\n\nFebruary 27, 2015 - Ranked #6 in the ComponentSource Bestselling Product Awards for 2014-2015\n\nMay 15, 2014 - SD Times 100 GrapeCity (dba ComponentOne) wins Best In Show in the User Experience category\n\nFebruary 28, 2014 - Ranked #5 in the ComponentSource Bestselling Product Awards for 2013-2014\n\nOctober 15, 2013 - Other Hot Products recognition in Best Microsoft Windows Development Printing/Reporting Tool of 2013\n\nMay 15, 2013 - SD Times 100 GrapeCity wins Best In Show in the User Experience category\n\nMay 15, 2013 - SD Times 100 GrapeCity wins Best In Show in the APIs, Libraries & Frameworks category\n\nFebruary 27, 2013 - Ranked #4 in the ComponentSource Bestselling Product Awards for 2012-2013\n\nMay 15, 2012 - SD Times 100 GrapeCity wins Best In Show in the Libraries & Frameworks category\n\nApril 30, 2012 - Ranked #11 in the ComponentSource Bestselling Product Awards for 2011-2012\n\nMay 15, 2011 - SD Times 100 GrapeCity wins Best In Show in the Components category\n\nFebruary 28, 2011 - Ranked #6 in the ComponentSource Bestselling Product Awards for 2010-2011\n\nNovember 29, 2010 - Finalist in the 2010 Best of Connections Awards: Visual Studio Developer Products\n\nMay 15, 2010 - SD Times 100 GrapeCity wins Best In Show in the Components & Libraries category\n\nMarch 26, 2010 - Ranked #7 in the ComponentSource Bestselling Product Awards for 2009-2010\n\nMay 7, 2009 - Finalist in the 2009 asp.netPRO Readers' Choice Awards\n\nMarch 13, 2009 - Ranked #4 and #5 in the 2008 ComponentSource Bestselling Publishers List\n\nMarch 13, 2009 - Ranked #2 in the 2008 ComponentSource Bestselling Products List\n\nFebruary 9, 2009 - ActiveReports Customers Give .NET Reporting and BI Tools High Ratings\nJune 2, 2008 - SD Times 100 Finalist in the Components category\n\nMay 15, 2007 - SD Times 100 Finalist in the Components category\n\nFebruary 9, 2007 - 2006 ComponentSource Bestselling Publisher\n\nFebruary 9, 2007 - 2006 ComponentSource Bestselling Product\n\nMay 15, 2006 - SD Times 100 Winner in the Components category\n\nIn the past, ActiveReports was known to be unable to handle large reports. This issue was ongoing across years and versions. Since that time, development efforts have focused on improving large report handling in every release.\n\nActiveReports can be used in many ways, so each project can have a number of reasons for consuming memory. In newer versions, CacheToDisk and CacheToDiskLocation properties were added for PDF exports. Some other considerations that may cause too much memory use in section reports include:\n\n\n"}
{"id": "38231221", "url": "https://en.wikipedia.org/wiki?curid=38231221", "title": "Agile Business Intelligence", "text": "Agile Business Intelligence\n\nAgile Business Intelligence (BI) refers to the use of Agile software development for BI projects to reduce the time it takes for traditional BI to show value to the organization, and to help in quickly adapting to changing business needs. Agile BI enables the BI team and managers to make better business decisions, and to start doing this more quickly.\n\nAgile Business Intelligence (BI) refers to the use of the agile software development methodology for BI projects to reduce the time-to-value of traditional BI and helps in quickly adapting to changing business needs. Agile BI enables the BI team and managers to make better business decisions. Agile methodology works on the iterative principle; this provides the new features of software to the end users sooner than the traditional waterfall process which delivers only the final product. With Agile the requirements and design phases overlap with development, thus reducing the development cycles for faster delivery. It promotes adaptive planning, evolutionary development and delivery, a time-boxed iterative approach, and encourages rapid and flexible response to change. Agile BI encourages business users and IT professionals to think about their data differently and it characterized by low Total Cost of Change (TCC). With agile BI, the focus is not on solving every BI problem at once but rather on delivering pieces of BI functionality in manageable chunks via shorter development cycles and documenting each cycle as it happens. Many companies fail to deliver right information to the right business managers at the right time.\n\nAgile BI is a continual process and not a onetime implementation. Managers and leaders need accurate and quick information about the company and business intelligence provides the data they need. Agile BI enables rapid development using the agile methodology. Agile techniques are a great way to promote development of BI applications, such as dashboards, scorecards, reports and analytic applications.\n\n\"Forrester Research defines agile BI as an approach that combines processes, methodologies, tools and technologies, while incorporating organizational structure, in order to help strategic, tactical and operational decision-makers be more flexible and more responsive to ever-changing business and regulatory requirements\". According to the research by the Aberdeen Group, organizations with the most highly agile BI implementations are more likely to have processes in place for ensuring that business needs are being met. Success of Agile BI implementation also heavily depends on the end user participation and \"frequent collaboration between IT and the business\".\n\nAberdeen’s Maturity Class Framework uses three key performance criteria to distinguish the Best-in-class in the industry:\n\n\nAgile SDLC Iterative Process\n\nBruni in her article 5 Steps To Agile BI outlines the five elements that promote an Agile BI enterprise environment.\n\n\nThere are 12 Agile Principles (Manifesto) Agile Manifesto grouped as Process, People, and Other.\n\n1. Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.\n\n2. Welcome changing requirements, even late in development. Agile processes harness change for the customer's competitive advantage.\n\n3. Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.\n\n4. Working software is the primary measure of progress.\n\n5. Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.\n\n6. Business people and developers must work together daily throughout the project.\n\n7. Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.\n\n8. The most efficient and effective method of conveying information is face-to-face conversation.\n\n9. At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior.\n\n10. Continuous attention to technical excellence and good design enhances agility.\n\n11. Simplicity-the art of maximizing the amount of work not done-is essential.\n\n12. The best architectures, requirements, and designs emerge from self-organizing teams.\n\nKernochan, in his two-year study of organization’s BI process came up with the below model and its characteristic goals:\n\nKernochan’s study found these common issues with the current BI processes:\nThe result concluded that the adding of agility to existing business intelligence will minimize problems. \nOrganizations are slowly trying to move the entire organization processes to agile methodology and development. Agile BI will play a big part in company’s success as it \"emphasizes integration with agile development and innovation\".\n\nThere are couple of factors that influence the success of Business Intelligence Agility.\n\n20% of data is inaccurate and about 50% is inconsistent and these numbers increases with new type of data. Processes need to be re-evaluated and corrected to minimize data entry errors.\n\nOften companies have multiple data stores and data is scattered across multiple data stores. \"Agility theory emphasizes auto-discovery of each new data source, and automated upgrade of metadata repositories to automatically accommodate the new information\".\n\nIs a process in which information from many data stores is pulled and displayed in a summary report. Online analytical processing (OLAP) is a simple type of data aggregation tools which is commonly used.\n\nOne of the key principal of Agile BI is to deliver the right data at the right time to the right individual. Historical data should also be maintained for comparing the current performance with the past.\n\nOne of the largest benefits of Agile BI is in improving the decision-making of its users. Real Agile BI should focus on analysis tools that make an operational process or new product development better. The Agile BI approach will save company money, time, and resources that would otherwise be needed to build a traditional data warehouse using the Waterfall methodology.\n\n\nAgile BI drives its users to self-serve BI. It offers organizations flexibility in terms of delivery, user adoption, and ROI.\n\nUsing Agile methodology, the product is delivered in shorter development cycles with multiple iterations. Each iteration is a working software and can be deployed to production.\n\nIn an Agile development environment, IT and business work together (often in the same room) refining the business needs in each iteration. \"This increases user adoption by focusing on the frequently changing needs of the non-technical business user, leading to high end-user engagement, and resulting in higher user adoption rates\".\n\nOrganizations can achieve increased rate-of-return (ROI) due to shorter development cycles. This minimizes the IT resources and time while delivering working, relevant reports to end-users.\n\n"}
{"id": "12715119", "url": "https://en.wikipedia.org/wiki?curid=12715119", "title": "Amadeus CRS", "text": "Amadeus CRS\n\nAmadeus is a computer reservation system (or global distribution system, since it sells tickets for multiple airlines) owned by the Amadeus IT Group with headquarters in Madrid, Spain. The central database is located at Erding, Germany. The major development centers are located in Sophia Antipolis (France), Bangalore (India), London (UK), and Boston (United States). In addition to airlines, the CRS is also used to book train travel, cruises, car rental, ferry reservations, and hotel rooms. Amadeus also provides New Generation departure control systems to airlines. Amadeus IT Group is a transaction processor for the global travel and tourism industry. The company is structured around two key related areas—its global distribution system and its IT Solutions business area.\n\nAmadeus is a member of IATA, OTA and SITA. Its IATA airline designator code is 1A.\n\n\n\n"}
{"id": "22393095", "url": "https://en.wikipedia.org/wiki?curid=22393095", "title": "Arrow diagramming method", "text": "Arrow diagramming method\n\nArrow diagramming method (ADM) is a network diagramming technique in which activities are represented by arrows. ADM is also known as the activity-on-arrow (AOA) method. \n\nADM is used for scheduling activities in a project plan. Precedence relationships between activities are represented by circles connected by one or more arrows. The length of the arrow represents the duration of the relevant activity. ADM only shows finish-to-start relationships, meaning that each activity is completed before the successor activity starts.\n\nSometimes a \"dummy task\" is added, to represent a dependency between tasks, which does not represent any actual activity. The dummy task is added to indicate precedence that can't be expressed using only the actual activities. Such a dummy task often has a completion time of 0.\n\nUse of ADM as a common project management practice has declined with the adoption of computer-based scheduling tools. In addition, the precedence diagram method (PDM), or activity-on-node (AON), is often favored over ADM.\n\nADM network drawing technique the start and end of each node or event is connected to an arrow.\n\nThe start of the arrow comes out of a node while the tip of the arrow goes into a node. Between the two nodes lies an arrow that represents the activity. \n\nThe event represented by the circular node consumes neither time nor resources. \n\n\n"}
{"id": "41347709", "url": "https://en.wikipedia.org/wiki?curid=41347709", "title": "Boats.com", "text": "Boats.com\n\nboats.com is an online advertising website based in Miami, Florida, United States. In addition to Miami, boats.com has operations in Fareham and Padova. The company has websites in 9 markets and 8 languages, listing over 350,000 boats in approximately 150 countries.\n\n\n\n"}
{"id": "13298130", "url": "https://en.wikipedia.org/wiki?curid=13298130", "title": "Brand culture", "text": "Brand culture\n\nBrand culture is a company culture in which employees \"live\" to brand values, to solve problems and make decisions internally, and deliver a branded customer experience externally. It is the desired outcome of an internal branding, internal brand alignment or employee engagement effort that elevates beyond communications and training.\n"}
{"id": "1844974", "url": "https://en.wikipedia.org/wiki?curid=1844974", "title": "Bursar", "text": "Bursar\n\nA bursar (derived from \"bursa\", Latin for \"purse\") is a professional financial administrator in a school or university. In the United States, bursars usually exist only at the level of higher education (4-year colleges and universities) or at private secondary schools. In Australia, Great Britain, and other countries, bursars are common at lower levels of education.\n\nThe Office of the Bursar is responsible for billing of student tuition accounts. This responsibility involves sending bills and making payment plans; the ultimate goal is to bring all student accounts to a \"paid off\" status. Bursars are not necessarily involved in the financial aid process. Bursars' duties vary from one institution to another. At many institutions, bursars deal only with student finances. At other institutions, bursars also deal with some faculty finance issues. Elsewhere, they also oversee accounts receivable, or the payments that the university receives from outside organizations for which it performs services. In some institutions, all money held by the institution is the bursar's responsibility. Some Bursars (in the UK for instance) also have responsibility for payroll, investments, facilities, IT, HR, Health and Safety and oversight of admin functions at an institution.\n\nThe Bursar Statement is also known as a tuition bill or a student account bill.\n\nThe bursar often reports to a comptroller. For example, Barnard College employs an Associate Comptroller–Bursar. In other cases, the bursar has the same level as the comptroller, and both report to the director or vice president of finance. Some universities in the United States still have a Director of Student Financial Services whose role is similar to that of a bursar. The University of Pennsylvania employs an associate vice president who essentially combines the role of a bursar (using the student financial services title) and that of a registrar.\n\n\nTerry Pratchett's \"Discworld\" novels contain a character called \"The Bursar\". This character, like most other faculty members of Pratchett's Unseen University, is almost always referred to by his job title.\n\nIn the \"Hamilton (musical)\" in the song \"Aaron Burr, Sir\", it is mentioned that Alexander Hamilton punched the bursar.\n\n"}
{"id": "38935938", "url": "https://en.wikipedia.org/wiki?curid=38935938", "title": "Business Anti-Corruption Portal", "text": "Business Anti-Corruption Portal\n\nThe Business Anti-Corruption Portal (BACP) is a one-stop shop for business anti-corruption information offering tools on how to mitigate risks and costs of corruption when doing business abroad. All the information on the Portal is produced by GAN Integrity Solutions, a Denmark-based IT & Professional Services firm. The Portal was established in 2006 and is supported by the European Commission and a number of European governments.\n\nThe BACP is referred to by several government and non-government organisations, amongst others, the Organisation for Economic Co-operation and Development, the United Nations, the OECD Business and Industry Advisory Committee (BIAC), and the World Bank. In addition, the BACP is the only externally cited tool for anti-corruption risk assessment in the UK Bribery Act 2010 Quick Start Guide. The BACP is listed in the UN Global Compact Anti-Corruption Tools Inventory as an effective resource to help companies address and implement the 10th principle: \"Businesses should work against corruption in all its forms, including extortion and bribery.\"\n\nThe Business Anti-Corruption Portal is funded by five European governments and the EU Commission. The five governments are:\n\nThe BACP offers relevant business analysis of corruption risks and legislative framework in over 100 countries across 6 regions: Europe & Central Asia, South Asia, East Asia & the Pacific, Sub-Saharan Africa, the Middle-East & North Africa, and the Americas. The 33 European country profiles are funded with financial support from the Prevention of and Fight against Crime Programme by the European Commission. Each country profile contains the following information:\n\n\nAll the information and sources used for creating the country profiles are publicly available, and the team behind GAN Integrity Solutions updates the country profiles at least once a year. Other tools and information on the Portal are also maintained and updated on regular basis.\n\nThe Portal offers a number of tools that help companies to effectively manage corruption risk, such as:\n\n\n"}
{"id": "41270989", "url": "https://en.wikipedia.org/wiki?curid=41270989", "title": "Business necessity", "text": "Business necessity\n\nA business necessity is a legitimate business purpose that justifies an employment decision as effective and needed to optimally achieve the organization’s goals and ensure that operations run safely and efficiently. This is often presented as a defense of an employment decision that is questioned because it was found to cause disparate impact.\n"}
{"id": "27387189", "url": "https://en.wikipedia.org/wiki?curid=27387189", "title": "Capability management in business", "text": "Capability management in business\n\nCapability management is the approach to the management of an organization, typically a business organization or firm, based on the \"theory of the firm\" as a collection of capabilities that may be exercised to earn revenues in the marketplace and compete with other firms in the industry. \"Capability Management\" seeks to manage the stock of capabilities within the firm to ensure its position in the industry and its ongoing profitability and survival.\n\nPrior to the emergence of capability management, the dominant theory explaining the existence and competitive position of firms, based on Ricardian economics, was the resource-based view of the firm (RBVF). The fundamental thesis of this theory is that firms derive their profitability from their control of resources – and are in competition to secure control of resources. Perhaps the best-known exposition of the Resource-based View of the Firm is that of one of its key originators: economist Edith Penrose.\n\n\"Capability management\" may be regarded as both an extension and alternative to the RBVF that asserts that it is not control over physical resources that is the basis for firm profitability but that \"Companies, like individuals, compete on the basis of their ability to create and utilize knowledge;...\". In short, firms compete not on the basis of control of resources but on the basis of superior Know-How. This Know-How is embedded in the capabilities of the firm – its abilities to do things that are considered valuable (in and by the market).\n\nLeonard defines three types of business capability that a firm might possess: Core Capabilities, Enabling Capabilities and Supplemental Capabilities.\n\nCore Capabilities are defined as those \"built up over time\", that \"cannot be easily imitated\" and therefore \"constitute a competitive advantage for a firm\". They are distinct from the other types of capability and sufficiently superior to similar capabilities in competitor organizations to provide a \"sustainable competitive advantage\". It is implied that such core capabilities are the product of sustained, long organizational learning.\n\nSupplemental Capabilities are defined as those that \"add value to core capabilities but that could be imitated\".\n\nEnabling Capabilities are defined as those that \"are necessary but not sufficient in themselves to competitively distinguish a company.\" In other words, enabling capabilities are those which a firm has to do, in support of its normal operations and core capabilities, but which are not themselves core capabilities (because they could be imitated, developed quickly or would not be very different from competitors' capabilities). Enabling capabilities are distinguished from supplemental capabilities in that they are required, but do not necessarily add value to core capabilities.\n\nA business capability is \"what\" a company needs to do to execute its business strategy (e.g., enable ePayments, tailor solutions at point of sale, demonstrate product concepts with customers, combine elastic and non-elastic materials side by side, etc.).\n\nAnother way to think about a capability is that it is an assembly of people, process and technology for a specific purpose.\n\nCapability Management is the active management, over time, of the portfolio of capabilities in a firm – their development and depreciation in conscious response to changes in the business environment.\n\nCapability management is an approach that uses the organization's customer value proposition to establish performance goals for capabilities based on value contribution. It helps drive out inefficiencies in capabilities that contribute low customer impact and focus efficiencies in areas with high financial leverage; while preserving or investing in capabilities for growth.\n\nOxford economist John Kay defines Distinctive Capabilities as those capabilities a firm has which other firms cannot replicate even after they realize what the benefits are that owning the capability confers. These distinctive capabilities are the source or superior performance of successful firms. Kay's Distinctive Capabilities may be identified with Leonard's inimitable and hard-won \"Core Capabilities\". However, Kay goes further in arguing that in order for a capability to be truly distinctive and the basis for competitive advantage it must meet two further criteria: sustainability and appropriability.\n\nSustainability refers to the firm maintaining the distinctiveness and superiority of the capability relative to other firms despite their efforts to imitate or replicate it. One approach to sustainability is for the firm to develop the capability faster than the competitors through learning and innovation.\n\nAppropriability refers to the firm securing the benefits of the capability – or the exercising of its capability – for itself as opposed to those benefits accruing to the firm's customers, its staff – management or employees, or its shareholders, regulators or other stakeholders. Intellectual Property Rights are one means for securing appropriability.\n\nOn the basis of analysis of empirical data regarding the performance of companies Kay argues that there are only a few types of distinctive capability that meet the additional criteria. Three are said to recur in the analysis: Innovation, Architecture and Reputation. These are briefly discussed below.\n\nIn a 1990 edition of the Harvard Business Review, Gary Hamel and C.K.Prahalad published an article entitled \"The Core Competence of the Corporation\" which defined the notion of a \"core competency\". Core Competencies are identified by three criteria: 1) they are difficult for competitors to imitate 2) they make a substantial contribution to a number of the firm's products (or services) they give the firm access to several markets and 3) they make a substantial contribution to the perceived customer-value of the products (or services). The (superior) competitive position of the firm's products (or services) in its markets is thought to be the expression of the firm's competitive advantage. Hamel and Prahald go on to assert that core competencies are the result of \"collective learning across the corporation\".\n\nSince that publication there has been active debate in the academic literature as to whether (the concept of) \"core competencies\" is the same notion as core capabilities. Several authors consider that the concepts are the same, the differences purely terminological, and use the terms interchangeably while others insist there is a substantive distinction. Given the similarities in their definitions it is a reasonable position to think they are the same. However, neither Leonard, nor Hamel and Prahalad (nor indeed Kay) were philosophically precise enough in their definitions and expositions of the concepts for the identity to be definitively established. One reasonable position is that \"core competencies\" are a view of \"core capabilities\" from a customers and products perspective while \"core capabilities\" are a view of \"core competencies\" from the perspective of knowledge and skills and staff and suppliers in firms. This may reflect the philosophical biases of their respective institutions.\n\nOthers, such as Max Boisot, take the view that competence or competency is some measure of the level of performance in a capability or that competence is a much narrower concept than capability. Hence a firm may have a high or low level of competence in a particular, notional, abstract capability. There is some evidence that general managers often fail to appreciate the subtlety of the definition of \"core competencies\" and over-estimate the degree of their firm's competence in common capabilities. Consequently, they over-identify \"things the firm is good at\" as core competencies – which falls foul of the distinctiveness criterion for a core capability (and/or the inimitability criterion of core competencies and core capabilities). Hence some things managers mistakenly identify as \"core competencies\" may be more properly considered as Enabling or Supplemental Capabilities.\n\nWhen applying the concepts of \"core competence\" or \"core capability\" academics and practitioners should be clear and precise as to their intended semantics for these ambiguous terms.\n\nLeonard analyzes the nature of a (business) capability and concludes that core capabilities \"comprise at least four interdependent dimensions\" (pp. 19) as follows:\n\nAround this complex of systems that realize a core capability Leonard situates a loop \"Capability-Creating Activities\" that comprise \"Shared Problem Solving\" and encompass Present and Future and Internal and External Perspectives (see Leonard, 1995, chapter 3). This capability development loop, is considered a system of organizational learning (knowledge-creating and knowledge-diffusing activities) and comprises the following activities:\nThese activities, however, do not form a simple cycle or sequence and may be conducted in any order and several \"in parallel\" around any particular capability. It is these knowledge-creating and knowledge-diffusing (or knowledge-acquiring and knowledge-sharing) activities that make the capability dynamic (change over time) in the Leonard model.\n\nClearly, Leonard takes a System-of-Systems perspective on organizational or business or enterprise capabilities – and this establishes the link to the notion of a Capability in Systems Engineering. Given that the Leonard model of a capability incorporates elements of skills and knowledge, and is adaptive and intelligent in the sense of importing knowledge from the external context, experimenting and problem-solving, while moving from present to future, it may be considered an ICASOS – an Intelligent Complex Adaptive System-of-Systems – model of an enterprise or firm. Note that the knowledge created and diffused in the Leonard model is organizational knowledge, not personal knowledge and is Know-How, not Know-That, that is often Tacit Knowledge.\n\nAccording to Hadaya and Gagnon, in their book \"Business Architecture - The Missing Link is Strategy Formulation, Implementation and Execution\", a business capability is an integrated set of resources designed to work together to achieve a particular result. A capability is always made up of one or more business functions, business processes, organizational units, know-how assets, information assets and technology assets. ″For example, to have the capability to Fabricate Metal Parts, an organization must have the necessary machines (technology assets), the knowledge of how to operate them (know-how assets), the specific sequence of activities needed to fabricate the parts (process), the drawings of the parts to be fabricated (information assets), and the teams of people (organizational units) specializing in the various types of fabrication required to make the parts (business functions)″. According to these authors, some capabilities also include one or more brands or natural resource deposits. Indeed, ″the Coca Cola Company could not sell 1.9 billion servings a day in more than 200 countries were it not for the power of its brands″. In turn, for a farmer to be able to Produce Vegetables, he must have not only the right business functions, business processes, organizational units, know-how assets, information assets and technology assets, but also a piece of land (natural resource deposit) on which to grow his vegetables. A business capability can also be made up of lower-level capabilities. For example, to have the capability to manufacture cars, an organization must have several lower-level capabilities, including the capability to manufacture engines and the capability to fabricate and assemble the bodywork of the cars.\n\nThe consideration of a portfolio of capabilities in an enterprise in the context of the PRESENT and FUTURE contexts, with the Importing of Knowledge from the EXTERNAL context and the Implementing and Integrating of Knowledge in the INTERNAL context, establishes the link to Enterprise Architecture. In Enterprise Architecture this knowledge and planning of present-to-future evolution is diffused through the medium of models shared and used throughout the enterprise.\n\nThe Leonard model of a Capability is a dynamic model at the micro-level; focused on the detailed mechanisms for development and change of individual capabilities. Building on the work of Hamel and Prahalad, and others David Teece and colleagues developed a macro-level theory of Dynamic capabilities and framework for their management. In this theory a (or perhaps \"the\") \"Dynamic Capability\" is defined as \"the firm’s ability to integrate, build, and reconfigure internal and external competences to address rapidly changing environments.\"\n\nBuilding on earlier work from the 1960s, the central thesis of the Dynamic Capabilities theory is that \"... the real key to a company's success or even to its future development lies in its ability to find or create a competence that is truly distinctive\". In this literature the term \"competence\" is used in both the sense of degree of performance in some capability and as a low-level, short-term capability. Competence in the latter sense echoes the concepts from Hamel and Prahalad, but may also be identified with Leonard's Enabling and Supplemental Capabilities as well as the short-term Core Capabilities of a firm. The term \"capabilities\" is also used somewhat interchangeably with \"competencies\".\n\nThe competencies or low-level capabilities are regarded as assets or knowledge-resources of the enterprise, alongside physical assets and other resources that are difficult for competitor firms to acquire. In this sense Teece's Dynamic Capabilities Theory is a move back towards the Resource-Based View of the Firm, but with a broader notion of \"Resources\" that includes intangible knowledge-based and practice-based resources, not just tangible assets. In the Dynamic Capabilities Theory, \"Resources\" are firm-specific assets that are difficult for competitors to acquire or imitate, \"Organizational Routines\" (based on prior work of Nelson and Winter) or \"Organizational Competences\" are the low-level capabilities of the firm and \"Core Competences\" are taken from the Hamel and Prahalad concept. Citing Dorothy Leonard (or Leonard-Barton), Teece et al. define (higher-level) Dynamic Capabilities as the \"ability to integrate, build and reconfigure\" these tangible and intangible assets.\n\"Dynamic Capabilities thus reflect an organizations ability to achieve new and innovative forms of competitive advantage given path dependencies and market positions.\"\n\n\"Path dependencies\" refers to the history of development of organizational knowledge embedded in its capabilities, routines and technological assets – echoing the concept from Leonard, but on a macro-scale and explaining why competitive advantage takes time and persistence to build. The incorporation of \"Routines\" and path-dependent development of firms in the theory establishes the link to evolutionary theories of management and (business) strategy.\n\nThe Dynamic Capabilities Theory also refers to notions of coherence, congruence and complementary assets across the firms portfolio of assets, routines, competencies and capabilities. This has resonances with similar notions of coherence in System-of-Systems Engineering and in Enterprise Architecture.\n\nTeece et al. also discuss Technological Assets, Structural Assets and Reputational Assets – echoing the three recurrent categories of Distinctive Capability identified by John Kay – Innovation, Architecture and Reputation. The discussion of technological assets and technology-based competences establishes the link to Technology Management and Technology Strategy. Teece et al. also discuss organizational structure, market/industry structure, organizational boundaries, notions of co-specialization in assets, cross-firm integration and the trade-offs between hierarchical management control and a nexus of contracts. This presages notions of the \"Virtual Enterprise\" or \"Extended Enterprise\" and establishes the link to Enterprise Architecture.\n\nIt can be seen therefore that Dynamic Capabilities Theory is a highly integrative theory of the firm that links a wide range of fields including Business Strategy, Strategic Management, Knowledge Management, Technology Management, Technology Strategy, Systems Thinking, Enterprise Architecture or Enterprise Engineering and others.\n\nFrom the perspective of Dynamic Capabilities Theory, Capability Management is the approach to management that focuses on the development of the portfolio of capabilities (resources, assets, routines, knowledge etc.) available to the firm, and the meta level capability of reconfiguring them and integrating them for market exploitation and competitive advantage.\n\nCapability management in defence has a history dating back to the early years of the 21st Century.\nThe UK Ministry of Defence adapted the existing DoDAF architecture framework to the procurement practices of what was then the MOD Procurement Executive, later Defence Equipment and Support, – in particular a Capability management approach to Defence procurement called \"Through-Life Capability Management\" (TLCM). The DoDAF was extended with additional 'Views' for capability planning and notions of capability configurations and capability increments. These views were standard ways to represent how the capabilities in some segment of the Defence Enterprise (such as the Army, Navy or Air Force) were expected to or planned to evolve over time, given investments by MOD and the UK Government. This was published in 2004 as the MOD Architecture Framework (MODAF) version 1.0. The current, and probably last, version of MODAF is 1.4. This will be the last version since MOD intends to migrate to the forthcoming new version of the NATO Architecture Framework, which incorporates much from MODAF, including the concepts and views of capability planning. The notion of capability in MODAF, however, is slightly different from the notion of capability in, for example, Dynamic Capabilities Theory.\n\nIn MODAF \"Capability\" refers to a \"military\" capability – an ability to produce military effects – or enabling or supporting capabilities. Nevertheless, the central thesis that politico-military conflicts or business competitions in markets are contests in which the organisation or enterprise with the superior capabilities wins holds. On this basis the same capability-based strategy and planning techniques are thought to be effective in both spheres.\n\nThe aim of \"Through-Life Capability Management\" (TLCM) was to promote in defence procurement a shift of perspective away from a focus on military equipment towards what was really required from UK Defence organizations: the capabilities, or abilities to produce effects in the world that enhance UK national security. The MOD considers that such capabilities are delivered by trans-organisation networks of operational units across the Defence Enterprise – and hence uses the terminology \"Network Enabled Capability\" (NEC). This is (a part of) MOD's interpretation of the concept of Network Centric Warfare which itself was a 1990s interpretation of a System-Of-Systems approach or perspective in the context of Defence.\n\nThe earlier cultural focus on equipment had led to over-specification, incoherent equipment purchases, unnecessary duplication, an over-emphasis on initial purchase costs, insufficient consideration of recurrent support and maintenance costs, project and programme delays and significant cost overruns that the UK Government could no longer tolerate. TLCM promoted a capability lifecycle perspective, that sought a balanced investment profile in time that would minimise whole-life costs, while efficiently producing effective, coherent and maximally cost-effective Defence capabilities over the medium term – and thereby ensuring maximum Value-for-Money for the UK taxpayer from Defence. TLCM was therefore central to the strategic planning of the evolution of the UK Defence Enterprise. While formally superseded, elements of TLCM live on in MOD's whole-systems, non-equipment-centric System-Of-Systems Approach, which was mandated in UK Defence Strategic Direction in 2013.\n\nCapability-based planning had long been entrenched in the Defense realm in the US, UK, Australia, and Canada before it was adopted within Version 9 of The Open Group Architecture Framework (TOGAF). \n\nCapability Management has in recent years become a popular sub-discipline or method of Enterprise Architecture. Enterprise Architecture seeks to build a rigorous model of an enterprise that identifies its component parts and their relationships for the purpose of planning the evolution of the enterprise. A Capability Management perspective – such as Leonard's model or Teece's Dynamic Capabilities Theory – suggests that a firm is best viewed as a collection of capabilities that have a composition from and configuration of the tangible and intangible assets of the firm. In this view the firm and its portfolio of capabilities evolve in response to the (perceived) demands of the business environment. An enterprise may comprise one or more firms, or parts of firms (or other types of organization) and their interrelationships – and so is describable (may be modelled) in the same terms.\n\nIn the IBM Enterprise Architecture Method, the \"Enterprise Capabilities Neighborhood\" – a segment of the overall Enterprise Architecture description or model – \"captures the strategic intent of the enterprise\". It \"provides the bridge from the organization's strategy to the architectural building blocks that enable and realize the strategy.\" According to the IBM EA Method, \"Both the Enterprise Capabilities and Business Architecture are technology independent.\" It is not clear how this view can be rationalised and made coherent with the Leonard model or the Teece theory which places some technology-implemented system at the center of a capability. Collins and De Meo go on to assert that \"To focus the EA (description) on delivering the right plan for the enterprise, it must be based on a detailed understanding of the Enterprise Capabilities the enterprise has decided it needs...\". Collins and De Meo's \"Enterprise Capabilities\" can thus be identified with Leonard's \"Core Capabilities\". The IBM EA Method defines the \"Strategic Capability Network\" which \"depicts the strategic capabilities and associated enablers of a business, their interrelationships and their combined roles and significance in ... the value net of a business.\" The SCN is therefore a modeling technique and network analysis method that expresses both the Leonard model of Core, Enabling and Supplemental capabilities, the Hamel and Prahalad notion of core competencies and, given EA's time dimension of enterprise evolution, the Dynamic Capabilities Theory.\n\nA process is \"how\" the capability is executed. Much of the reengineering revolution or Business process reengineering focused on how to redesign business processes.\n\nAn organization capability refers to the potential of the people in an organization and their cooperation to get things done. The way leaders foster shared mindset, talent, change, accountability, and collaboration across boundaries define the company's culture and leadership edge.\n\nDave Ulrich makes a distinction between capabilities and competencies: individuals have competencies while organizations have capabilities. Both competencies and capabilities have technical and social elements.\n\nAt the individual-technical intersection, employees in the firm bring functional skills and competencies such as programming, cost accounting, electrical engineering, etc. At the individual-social intersection, leaders also have a set of competencies or skills such as setting the strategic agenda and building relationships. Moving to the intersection of organizational and technical, are business capabilities. In short, they are the technical things or what the firm must know how to do to execute strategy. For example, a financial service firm must know how to manage risk and design innovative products.\nFinally, we have organization capabilities such a talent management, collaboration, and accountability. They integrate all the other parts of the firm and bring it together. When leaders have mastered certain competencies, organization capabilities become visible. For example, when a leaders master \"turning vision in to action\" and \"aligning the organization,\" the organization a whole shows more accountability.\n\nCapability management's earlier ancestors include the value chain, also known as value chain analysis, first described and popularized by Michael Porter. Core competencies (also called core capabilities) are what give a company one or more competitive advantages in creating and delivering value to its customers in its chosen field, a cluster of extraordinary abilities or the excellence that a firm acquires from its founders, and which cannot be easily imitated.\n\nLee Perry, Randall Stott and Norm Smallwood added to the capabilities body of work the concepts of strategic options based on customer value proposition and business focus and types of work which characterized work as either:\n\n\nBuilding on earlier themes, the concept of dynamic capabilities was introduced in 2000. The basic assumption of the dynamic capabilities framework is that in fast changing markets, firms need to respond quickly and innovatively.\n\nAround the same time, Richard Lynch, John Diezemann and James Dowling extended the concepts above in \"The Capable Company: Building the capabilities that make strategy work\". Key additions to the body of work were tools to translate strategic shifts to new sets of capabilities required whether these were core competencies or not. Building on the types of work ideas, the authors added performance target setting based on the capability value contribution. When compared to actual performance, the method outlined an approach to identify capability gaps and priorities. They also laid out a framework to continually align capabilities based on strategy shifts and external changes through the project agenda. The first full capability model was built by the authors in 2001 as the framework for the demerger of Intercontinental Hotels Group (then known as Six Continents) from the parent Six Continents PLC (formerly Bass & Co Brewery). The model included thee levels of capabilities, value contribution, performance targets, capability gaps, recommended actions and sourcing decisions.\n\nIn 2004, the UK Ministry of Defence released its enterprise architecture framework, MODAF. This framework extended the existing DoDAF specification by adding views for capability planning. These views were standard ways to represent how the enterprise was expected to perform over time, expressed in terms of capabilities.\n\nOther important contributions include the concept of value maps for detailing the customer proposition and more recently the profit proposition to identify capabilities that will help create Blue Ocean Strategy. Value maps extend the work of real-time strategy and the capable company by depicting a strategy canvas and providing an action framework to capture markets. In the mid-2000s, a team at Microsoft, in concert with Accelare, developed the motion methodology – a capability-based framework. In 2008, Ric Merrifield, Jack Calhoun and Dennis Stevens, in \"The Next Revolution in Productivity\" added the use of SOA and its role in supporting capability delivery at breakthrough cost and speed. Also introduced was the use of heats maps for capability analyses.\n\nA complete picture of the capabilities is the enterprise capability model. It is a blueprint for the business expressed in terms of the capabilities necessary to execute strategy including delivery of services. Capabilities are described in levels of abstraction; usually three levels of details:\n\n\nAt the higher level, are the attributes of ownership, location, and project road maps. The lower level is where the action is and where performance targets are set, performance assessed and gaps noted. It is at this level sourcing decisions are made or projects established to close gaps. The framework includes strategic, core and enabling capabilities.\n\n\nCompanies like Harvard Pilgrim Health Care and Intercontinental Hotels Group have used capabilities to focus on where to take out costs and outsource non strategic capabilities while improving service and adding brands.\n\nMicrosoft is using capability models to enter into conversations with clients to identify capability and process pain points to better align IT solutions to the business.\n\nCapabilities are also being used in new growth platform development. Platforms are a foundation that spawns multiple products and/or services that, by themselves, are eventually the size of a business unit. These innovations result from identifying new domains created at the intersection of enablers or \"unstoppable trends\" and customer dynamics, linked to an essential set of core capabilities called the platform logic: those capabilities that are unique, valuable, and portable.\n\nBuilding of the earlier type of work logic, Accelare added a distinction in assessment of the capabilities necessary to operative the business by examining the financial impact as well as the customer impact.\n\nSome capabilities directly contribute to the customer value proposition and have a high impact on company financials. These \"advantage capabilities\" are shown in the upper right. Value contribution is assured when performance is among the best in peer organizations at acceptable cost. Keep them inside and protect the intellectual property. Moving to the top left quadrant, strategic support capabilities have high contribution in direct support of advantage capabilities. Keep them close. Value contribution is assured when performed above industry parity at competitive cost. Other capabilities shown in the bottom right are essential. They may not be visible to the customer but contribute to company's business focus and have a big impact on the bottom line. Focus on efficiency improvement; especially in high volume work. Value contribution is assured when performed at industry parity performance below competitors' cost. Other capabilities are \"business necessity\". Value contribution is assured when performed at industry parity performance below competitors' costs. They can be candidates for alternate sourcing.\n\nA capability gap assessment can be portrayed in a heat map.\n\n\nCapability value contribution helps stack rank investments, for example advantage capabilities with high heat move to the top of the agenda, followed by business essential capabilities with large inefficiencies.\n\n\n"}
{"id": "51011938", "url": "https://en.wikipedia.org/wiki?curid=51011938", "title": "Caribbean Catastrophe Risk Insurance Facility Segregated Portfolio Company", "text": "Caribbean Catastrophe Risk Insurance Facility Segregated Portfolio Company\n\nCaribbean Catastrophe Risk Insurance Facility Segregated Portfolio Company (CCRIF SPC) is an insurance company headquartered in the Cayman Islands. The sixteen original member-countries of CCRIF included participants in CARICOM, and the membership of the Board of Directors is selected by CARICOM and by the Caribbean Development Bank.\n\nFounded in 2007, CCRIF is the first multi-country risk pool in the world, and was the first insurance instrument to successfully develop parametric policies backed by both traditional and capital markets. These parametric policies release funds based upon factors of a calamity such as rainfall or wind speed, which can speed up the payout of policies rather than after damages are assessed. Unused funds are kept as reserves for the CCRIF. The fund can also draw upon $140 million in funds underwritten by reinsurance.\n\nOther regions have since setup similar government disaster instance including in the African Union and the Pacific Islands Forum. \n\nIn 2008, CCRIF made its first payout to the government of the Turks and Caicos Islands in response to damage from Hurricane Ike.\n\nDuring its formative years, day-to-day operations were handled by Caribbean Risk Managers Limited (CaribRM), who acted as the facility supervisor. In 2013, Isaac Anthony, previously a member of the Board of Directors of CCRIF and the Permanent Secretary for Planning and National Development in the Government of St Lucia, was appointed the first Chief Executive Officer.\n\nIn 2014, CCRIF reorganized into a segregated portfolio company, and expanded its product line to include parametric insurance for excess rainfall as well as to offer its portfolio of insurance products to countries in Central America, including members of the Council of Ministers of Finance of Central America, Panama and the Dominican Republic (COSEFIN). CCRIF also was the first company to participate in the World Bank Capital-at-Risk Notes Program to diversify its sources of risk capital.\n\nAt the G7 leaders summit in 2015, CCRIF SPC was highlighted as an example to be used, \"... to increase by up to 400 million the number of people in the most vulnerable developing countries who have access to direct or indirect insurance coverage against the negative impact of climate change related hazards by 2020...\" At the 2015 United Nations Climate Change Conference in Paris, Ban Ki-moon, Secretary General of the United Nations, mentioned CCRIF SPC as an example as he launched an initiative on climate resilience that included increased access to insurance, and President Obama announced a commitment of $30 million by the United States government to provide additional insurance through platforms, including CCRIF, in his remarks to the Alliance of Small Island States at the Paris Conference.\n\nIn 2016, the World Bank announced a partnership with CCRIF SPC to create a new insurance product to protect food security and promote the resilience of the small-scale fisheries sector in the Caribbean. Also, the World Bank announced the Pandemic Emergency Facility built using CCRIF SPC as a model.\n\n\n"}
{"id": "9517063", "url": "https://en.wikipedia.org/wiki?curid=9517063", "title": "Category development index (marketing)", "text": "Category development index (marketing)\n\nThe category development index (CDI) measures the sales performance of a category of goods or services in a specific group, compared with its average performance among all consumers. By definition, CDI measures the sales strength of a particular product category within a specific market (e.g., soft drinks in 10–50 year old's).\n\nThe purpose of the CDI metric is to quantify the relative performance of a category within specified customer groups. The CDI helps marketers identify strong and weak segments (usually demographic or geographic) for categories of goods and services.\n\nThe CDI is useful in all marketing strategies when used with the Brand Development Index (BDI). The CDI can give vital data for marketers to allocate advertising to specific areas maximizing product category knowledge and profit.\n\nCDI: An index of how well a category performs in a given market segment, relative to its performance in the market as a whole.\n\nFor example, Boston enjoys high per-capita consumption of ice cream. Bavaria and Ireland show higher per-capita consumption of beer than Iran.\n\n\n"}
{"id": "41655576", "url": "https://en.wikipedia.org/wiki?curid=41655576", "title": "Chairman's Choice", "text": "Chairman's Choice\n\nChairman's Choice is a practice where a company head mandates that his/her company sponsors an event/celebrity etc. for personal gain, or for illogical or non-existent commercial reasons. The practice is less prevalent in developed countries but occurs more often in emerging markets.\n\nSponsorship is not to be confused with charity donations which are not designed to create a commercial return for a company\n"}
{"id": "2024427", "url": "https://en.wikipedia.org/wiki?curid=2024427", "title": "Cohort analysis", "text": "Cohort analysis\n\nCohort analysis is a subset of behavioral analytics that takes the data from a given data set (e.g. an e-commerce platform, web application, or online game) and rather than looking at all users as one unit, it breaks them into related groups for analysis. These related groups, or cohorts, usually share common characteristics or experiences within a defined time-span. Cohort analysis allows a company to “see patterns clearly across the life-cycle of a customer (or user), rather than slicing across all customers blindly without accounting for the natural cycle that a customer undergoes.” By seeing these patterns of time, a company can adapt and tailor its service to those specific cohorts. While cohort analysis is sometimes associated with a cohort study, they are different and should not be viewed as one and the same. Cohort analysis is specifically the analysis of cohorts in regards to big data and business analytics, while in cohort study, data is broken down into similar groups.\n\nThe goal of a business analytic tool is to analyze and present actionable information. In order for a company to act on such information it must be relevant to the situation under analysis. A database full of thousands or even millions of entries of all user data makes it tough to gain actionable data, as that data spans many different categories and time periods. Actionable cohort analysis allows for the ability to drill down to the users of each specific cohort to gain a better understanding of their behaviors, such as if users checked out, and how much did they pay. In cohort analysis \"each new group [cohort] provides the opportunity to start with a fresh set of users,\" allowing the company to look at only the data that is relevant to the current query and act on it.\n\nIn eCommerce, a firm may only be interested in customers who signed up in the last two weeks and who made a purchase, which is an example of a specific cohort. A software developer may only care about the data from users who sign up after a certain upgrade, or who use certain features of the platform.\n\nAn example of cohort analysis of gamers on a certain platform: Expert gamers, cohort 1, will care more about advanced features and lag time compared to new sign-ups, cohort 2. With these two cohorts determined, and the analysis run, the gaming company would be presented with a visual representation of the data specific to the two cohorts. It could then see that a slight lag in load times has been translating into a significant loss of revenue from advanced gamers, while new sign-ups have not even noticed the lag. Had the company simply looked at its overall revenue reports for all customers, it would not have been able to see the differences between these two cohorts. Cohort analysis allows a company to pick up on patterns and trends and make the changes necessary to keep both advanced and new gamers happy.\n\n\"An actionable metric is one that ties specific and repeatable actions to observed results [like user registration, or checkout]. The opposite of actionable metrics are vanity metrics (like web hits or number of downloads) which only serve to document the current state of the product but offer no insight into how we got here or what to do next.\" Without actionable analytics the information that is being presented may not have any practical application, as the only data points represent vanity metrics that do not translate into any specific outcome. While it is useful for a company to know how many people are on their site, that metric is useless on its own. For it to be actionable it needs to relate a \"repeatable action to [an] observed result\".\n\nIn order to perform a proper cohort analysis, there are four main stages:\n\n\n\n"}
{"id": "33085387", "url": "https://en.wikipedia.org/wiki?curid=33085387", "title": "Computhink", "text": "Computhink\n\nComputhink is the developer of the document management, enterprise content management, records management, and document workflow software Contentverse and its cloud version: Contentverse Cloud. Computhink's headquarters are in Lombard, Illinois.\n\nComputhink was established in 1994 in Chicago, IL. Computhink originally offered their document management solution under the name \"The Paperless Office\" on a 32-bit SQL database. In the year 2000, Computhink launched ViewWise and rebranded ViewWise in 2013; creating Contentverse 8 and its cloud version: Contentverse Cloud. Contentverse enables users to create an electronic filing system composed of cabinets, drawers, and folders, using index keys to manage and search a multitude of documents.\n\nContentverse fully enables users to reduce paper use and move to an electronic document management solution.\nA paperless office is a more efficient, cost-saving, compliant organizational strategy.\n\nComputhink Incorporated was founded in 1994 near Chicago, Illinois. The company originally sold a high security identification card production and management software program called IDtel. In their first year, Computhink sold one hundred IDtel systems to the United Nations for $1 million and became their sole ID tag provider. After the success with IDtel, Computhink created their first image viewing and management software program, The Paperless Office, in 1995. This was the first 32-bit software package designed for personal, single user, image viewer and image management use. Originally sold as a single user system, The Paperless Office was later developed to be used as a multiple user system.\n\nBy 1996, Computhink had operating offices in Chicago, London, Kuwait, South Africa, and Moscow with 85 total employees and consultants, and annual revenue exceeded $15 million. The single-user product was phased out, and network versions for small businesses were created and offered. As customer needs changed, Computhink moved from image management to document management. The reseller channel was developed in the United States and Internationally.\n\nBetween 1998 and 1999, The Paperless Office and IDtel were sold in 29 countries, across six continents. At this time, Computhink purchased Whetstone Technologies, the primary partner of Novell, to further expand their market presence. The Paperless Office was re-developed in Java and launched as ViewWise, the first ever server-based image-enabling product in the industry. In 2000, ViewWise became Novell’s preferred imaging partner, and Computhink decided to forego any involvement with the Whetstone Technologies products.\n\nThe network for domestic and international resellers was updated and broadened in 2006. Computhink became qualified as a Microsoft Front Runner, allowing them to release solutions alongside Microsoft’s product line. By 2008, Computhink had begun incorporating partnerships and connections with Kodak, Canon, and other third party viewers that complemented document management software. Computhink reached Tier III for the United Nations.\n\nIn 2013, the staff at Computhink realized that since 2006, the market’s needs and interests had shifted from document management to content management. Despite consistent updates and enhancements of ViewWise, the company decided to take the next step. The product was rebranded to Contentverse as the eighth version of the software. In 2014, version 8.1 of Contentverse was released.\n\n"}
{"id": "57859530", "url": "https://en.wikipedia.org/wiki?curid=57859530", "title": "Data plan", "text": "Data plan\n\nData plan refers to data quotas from a telecommunications or data hosting contract. Data plans are offered by internet service providers. These include mobile data plans, offered on cellular networks, from cellular telephony companies, and those from conventional fixed land line links, amongst other forms of offered data communications. Network data hosting servers also offer plans based on data served, such as for websites.\n"}
{"id": "30429756", "url": "https://en.wikipedia.org/wiki?curid=30429756", "title": "DealCenter", "text": "DealCenter\n\nDealCenter is a social media platform and online meeting management system used at trade shows and events for arranging face-to-face meetings. DealCenter, LLC was established January 2008 though the online meeting planner has been designed and in use by Jaymie Scotto & Associates as an event planning product since 2005, pre-dating today’s social networking communities. The DealCenter platform is an easy and user-friendly way to see who else is attending or exhibiting an event, to search the list of attendees by relevant information (such as company name, product offering or area served etc.) and then to create one-on-one meetings that will be held on-site at the event. The DealCenter system is traditionally rolled out months before the show date, so attendees can pre-plan their meetings and attend the event with a scheduled timetable of targeted prospect, customer or partner meetings.\n\nTo utilize the platform, attendees and exhibitors log on to the customized event website and are able to see the profiles of other participants who will be attending the event. The DealCenter manages the meeting schedules, accepts or declines invitations and allows users to send secure messages via the DealCenter to further expand the business opportunity pipeline. All the planning is done through the platform interface, enabling users to arrive with a roster of meetings already scheduled and confirmed. No contact information is shared or released unless the user voluntarily provides it. The confirmed meetings are assigned a table number and held on-site in a designated area, including, if desired, meeting rooms, bilateral tables, exhibit booths and/or a DealCenter Area. \nThe DealCenter Concierge Service is a new development within the DealCenter technology. The Concierge Service automatically schedules meetings on a user's behalf based on the information provided when creating a profile.\n\n\n\n"}
{"id": "3235624", "url": "https://en.wikipedia.org/wiki?curid=3235624", "title": "Decision analyst", "text": "Decision analyst\n\nDecision analysts are people who use formal methods, particularly \"Expected Utility Theory\", to assist others in decision making.\n"}
{"id": "469578", "url": "https://en.wikipedia.org/wiki?curid=469578", "title": "Decision support system", "text": "Decision support system\n\nA decision support system (DSS) is an information system that supports business or organizational decision-making activities. DSSs serve the management, operations and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance—i.e. unstructured and semi-structured decision problems. Decision support systems can be either fully computerized or human-powered, or a combination of both.\n\nWhile academics have perceived DSS as a tool to support decision making process, DSS users see DSS as a tool to facilitate organizational processes. Some authors have extended the definition of DSS to include any system that might support decision making and some DSS include a decision-making software component; Sprague (1980) defines a properly termed DSS as follows:\n\nDSSs include knowledge-based systems. A properly designed DSS is an interactive software-based system intended to help decision makers compile useful information from a combination of raw data, documents, and personal knowledge, or business models to identify and solve problems and make decisions.\n\nTypical information that a decision support application might gather and present includes:\n\nThe concept of decision support has evolved mainly from the theoretical studies of organizational decision making done at the Carnegie Institute of Technology during the late 1950s and early 1960s, and the implementation work done in the 1960s. DSS became an area of research of its own in the middle of the 1970s, before gaining in intensity during the 1980s. In the middle and late 1980s, executive information systems (EIS), group decision support systems (GDSS), and organizational decision support systems (ODSS) evolved from the single user and model-oriented DSS.\n\nAccording to Sol (1987) the definition and scope of DSS has been migrating over the years: in the 1970s DSS was described as \"a computer-based system to aid decision making\"; in the late 1970s the DSS movement started focusing on \"interactive computer-based systems which help decision-makers utilize data bases and models to solve ill-structured problems\"; in the 1980s DSS should provide systems \"using suitable and available technology to improve effectiveness of managerial and professional activities\", and towards the end of 1980s DSS faced a new challenge towards the design of intelligent workstations.\n\nIn 1987, Texas Instruments completed development of the Gate Assignment Display System (GADS) for United Airlines. This decision support system is credited with significantly reducing travel delays by aiding the management of ground operations at various airports, beginning with O'Hare International Airport in Chicago and Stapleton Airport in Denver Colorado. Beginning in about 1990, data warehousing and on-line analytical processing (OLAP) began broadening the realm of DSS. As the turn of the millennium approached, new Web-based analytical applications were introduced.\n\nThe advent of more and better reporting technologies has seen DSS start to emerge as a critical component of management design. Examples of this can be seen in the intense amount of discussion of DSS in the education environment.\n\nDSS also have a weak connection to the user interface paradigm of hypertext. Both the University of Vermont PROMIS system (for medical decision making) and the Carnegie Mellon ZOG/KMS system (for military and business decision making) were decision support systems which also were major breakthroughs in user interface research. Furthermore, although hypertext researchers have generally been concerned with information overload, certain researchers, notably Douglas Engelbart, have been focused on decision makers in particular.\n\nUsing the relationship with the user as the criterion, Haettenschwiler differentiates \"passive\", \"active\", and \"cooperative DSS\". A \"passive DSS\" is a system that aids the process of decision making, but that cannot bring out explicit decision suggestions or solutions. An \"active DSS\" can bring out such decision suggestions or solutions. A \"cooperative DSS\" allows for an iterative process between human and system towards the achievement of a consolidated solution: the decision maker (or its advisor) can modify, complete, or refine the decision suggestions provided by the system, before sending them back to the system for validation, and likewise the system again improves, completes, and refines the suggestions of the decision maker and sends them back to them for validation.\n\nAnother taxonomy for DSS, according to the mode of assistance, has been created by Daniel Power: he differentiates \"communication-driven DSS\", \"data-driven DSS\", \"document-driven DSS\", \"knowledge-driven DSS\", and \"model-driven DSS\". \n\nUsing scope as the criterion, Power differentiates \"enterprise-wide DSS\" and \"desktop DSS\". An \"enterprise-wide DSS\" is linked to large data warehouses and serves many managers in the company. A \"desktop, single-user DSS\" is a small system that runs on an individual manager's PC.\n\nThree fundamental components of a DSS architecture are:\n\nThe users themselves are also important components of the architecture.\n\nSimilarly to other systems, DSS systems require a structured approach. Such a framework includes people, technology, and the development approach.\n\nThe Early Framework of Decision Support System consists of four phases:\n\nDSS technology levels (of hardware and software) may include:\n\nAn iterative developmental approach allows for the DSS to be changed and redesigned at various intervals. Once the system is designed, it will need to be tested and revised where necessary for the desired outcome.\n\nThere are several ways to classify DSS applications. Not every DSS fits neatly into one of the categories, but may be a mix of two or more architectures.\n\nHolsapple and Whinston classify DSS into the following six frameworks: text-oriented DSS, database-oriented DSS, spreadsheet-oriented DSS, solver-oriented DSS, rule-oriented DSS, and compound DSS. A compound DSS is the most popular classification for a DSS; it is a hybrid system that includes two or more of the five basic structures.\n\nThe support given by DSS can be separated into three distinct, interrelated categories: Personal Support, Group Support, and Organizational Support.\n\nDSS components may be classified as:\n\nDSSs which perform selected cognitive decision-making functions and are based on artificial intelligence or intelligent agents technologies are called intelligent decision support systems (IDSS)\n\nThe nascent field of decision engineering treats the decision itself as an engineered object, and applies engineering principles such as design and quality assurance to an explicit representation of the elements that make up a decision.\n\nDSS can theoretically be built in any knowledge domain.\n\nOne example is the clinical decision support system for medical diagnosis. There are four stages in the evolution of clinical decision support system (CDSS): the primitive version is standalone and does not support integration; the second generation supports integration with other medical systems; the third is standard-based, and the fourth is service model-based.\n\nDSS is extensively used in business and management. Executive dashboard and other business performance software allow faster decision making, identification of negative trends, and better allocation of business resources. Due to DSS all the information from any organization is represented in the form of charts, graphs i.e. in a summarized way, which helps the management to take strategic decision. For example, one of the DSS applications is the management and development of complex anti-terrorism systems. Other examples include a bank loan officer verifying the credit of a loan applicant or an engineering firm that has bids on several projects and wants to know if they can be competitive with their costs.\n\nA growing area of DSS application, concepts, principles, and techniques is in agricultural production, marketing for sustainable development. For example, the DSSAT4 package, developed through financial support of USAID during the 80s and 90s, has allowed rapid assessment of several agricultural production systems around the world to facilitate decision-making at the farm and policy levels. Precision agriculture seeks to tailor decisions to particular portions of farm fields. There are, however, many constraints to the successful adoption on DSS in agriculture.\n\nDSS are also prevalent in forest management where the long planning horizon and the spatial dimension of planning problems demands specific requirements. All aspects of Forest management, from log transportation, harvest scheduling to sustainability and ecosystem protection have been addressed by modern DSSs. In this context the consideration of single or multiple management objectives related to the provision of goods and services that traded or non-traded and often subject to resource constraints and decision problems. The Community of Practice of Forest Management Decision Support Systems provides a large repository on knowledge about the construction and use of forest Decision Support Systems.\nA specific example concerns the Canadian National Railway system, which tests its equipment on a regular basis using a decision support system. A problem faced by any railroad is worn-out or defective rails, which can result in hundreds of derailments per year. Under a DSS, the Canadian National Railway system managed to decrease the incidence of derailments at the same time other companies were experiencing an increase.\n\n"}
{"id": "12779229", "url": "https://en.wikipedia.org/wiki?curid=12779229", "title": "Deployment flowchart", "text": "Deployment flowchart\n\nA deployment flowchart (sometimes referred to as a cross functional flowchart) is a business process mapping tool used to articulate the steps and stakeholders of a given process.\n\n\"Deployment flowcharts consist of a sequence of activity steps and also the interactions between individuals or groups.\" Each participant in the process is displayed on the map (which is constructed as a matrix) - tasks/activity are then articulated in sequence under the column corresponding to that stakeholder.\n\nAs deployment flowcharts highlight the relationships between stakeholders in addition to the process flow they are especially useful in highlighting areas of inefficiency, duplication or unnecessary processing. Often utilized within Six sigma activity, completed flowcharts are commonly used to examine the interfaces between “participants” which are typically causes for delays and other associated issues. Deployment flowcharts are useful for determining who within an organization is required to implement a process and are sometimes used as a business planning tool. \n\nWhile deployment flowcharts can be drawn by hand using pen and paper – various software tools include functionality to construct the flowcharts on computer, these include products such as Microsoft Visio.\n\nAs with other process mapping techniques, deployment flowcharts require a certain degree of detail (and accuracy) to provide useful benefit. Care and attention is required to ensure that the correct stakeholder is attributed to the correct parts of the process. Complex flowcharts may be difficult to interpret, especially where a flow is unclear, or many stakeholders are identified. Consistent terminology throughout the map is also important to provide clarity to the reader.\n\n"}
{"id": "2320741", "url": "https://en.wikipedia.org/wiki?curid=2320741", "title": "Diplomatic correspondence", "text": "Diplomatic correspondence\n\nDiplomatic correspondence is correspondence between one state and another, usually – though not exclusively – of a formal character. It follows several widely observed customs and style in composition, substance, presentation, and delivery and can generally be categorized into letters and notes.\n\nLetters are correspondence between heads of state, typically used for the appointment and recall of ambassadors; for the announcement of the death of a sovereign or an accession to the throne; or for expressions of congratulations or condolence. \n\nLetters between two monarchs of equal rank will typically begin with the salutation \"Sir My Brother\" (or \"Madame My Sister\", in the case of a female monarch) and close with the valediction \"Your Good Brother\" (or Sister, in the case of a female monarch). In the case where one monarch is of inferior rank to the other (for instance, if the Grand Duke of Luxembourg were to correspond to the Queen of the United Kingdom), the inferior monarch will use the salutation \"Sire\" (or \"Madame\"), while the superior monarch may refer to the other as \"Cousin\" instead of \"Brother\". If either the sender or the recipient is the head-of-state of a republic, letters may begin with the salutation \"My Great and Good Friend\" and close with the valediction \"Your Good Friend\"; beneath the signature line will be inscribed \"To Our Great and Good Friend [Name and Title of Recipient]\".\n\nA letter of credence (\"lettres de créance\") is the instrument by which a head of state appoints (\"accredits\") ambassadors to foreign countries. Also known as credentials, the letter closes with a phrase \"asking that credit may be given to all that the ambassador may say in the name of his sovereign or Government.\" The credentials are presented personally to the receiving country's head of state or viceroy in a formal ceremony. Letters of credence are worded carefully, as the sending or acceptance of a letter implies diplomatic recognition of the other government. Letters of credence date to the thirteenth century.\n\nA letter of recall is formal correspondence from one head-of-state notifying a second head-of-state that he or she is recalling his state's ambassador.\n\nIn cases where an envoy is entrusted with unusually extensive tasks that would not be covered by an ordinary permanent legation (such as the negotiation of a special treaty or convention, or representation at a diplomatic congress), an envoy may be given full powers (\"pleins pouvoirs\") \"in letters patent signed by the head of the State\" designing \"either limited or unlimited full powers, according to the requirements of the case.\"\n\nAccording to \"Satow's Diplomatic Practice\", the bestowal of full powers traces its history to the Roman \"plena potestas\"; its purpose was to be able to dispense, as far as possible, with the long delays needed in earlier times for referring problems back to higher authority. Their use at the present day is a formal recognition of the necessity of absolute confidence in the authority and standing of the negotiator.\n\nA \"note verbale\" is the most formal form of note and is so-named as it originally represented a formal record of information delivered orally. Notes verbale are written in the third person and printed on official letterhead; they are typically sealed with an embosser or, in some cases, a stamp. All notes verbale begin with a formal salutation, typically:\n\nNotes verbales will also close with a formal valediction, typically:\n\nNotes verbales composed by the British Foreign Office are written on blue paper.\n\nA collective note is a letter delivered from multiple states to a single recipient state. It is always written in the third person. The collective note has been a rarely used form of diplomatic communication due to the difficulty in obtaining agreements among multiple states to the exact wording of a letter.\n\nAn identic note is a letter delivered from a single state to multiple recipient states. Examples include the identic note sent by Thomas Jefferson regarding action against the Barbary Pirates and that from the United States to China and the Soviet Union in 1929. In it, the United States called on the other two powers to resolve their differences over the Eastern China Railway peacefully.\n\nA \"bout de papier\" (speaking note) may be presented by a visiting official when meeting with an official from another state at the conclusion of the meeting. Prepared in advance, it contains a short summary of the main points addressed by the visiting official during the meeting and, firstly, serves as a memory aid for the visiting official when speaking. It, secondly, removes ambiguity about the subject of the meeting occasioned by verbal miscues by the visiting official. Bouts de papier are always presented without credit or attribution so as to preserve the confidentiality of the meeting in case the document is later disclosed.\n\nA \"démarche\" (non-paper) is considered less formal than the already informal bout de papier. Officially described as \"a request or intercession with a foreign official\" it is a written request that is presented without attribution from the composing state and is, therefore, delivered in-person.\n\nSimilar to a démarche, an \"aide-mémoire\" is a proposed agreement or negotiating text circulated informally among multiple states for discussion without committing the originating delegation's country to the contents. It has no identified source, title, or attribution and no standing in the relationship involved.\n\nStandard diplomatic protocol varies from country to country, but generally requires clear yet concise translation between both parties.\n\nThe earliest forms of diplomatic correspondence were, out of necessity, written in Latin, Latin being a common language among states of a linguistically diverse Europe. By the early 1800s French had firmly supplanted Latin as the language of diplomacy; on one occasion, in 1817, the British attempted to correspond with the Austrian Imperial Court in English, prompting Klemens von Metternich to threaten retaliatory correspondence in German. In modern times, French has largely been replaced by English as a diplomatic lingua franca in correspondence between two states that do not share a common tongue.\n\nStates may sometimes reject diplomatic correspondence addressed to them by returning the original copy to the sending state. This is done as a rebuff of the contents of the correspondence and is typically reserved for cases where the receiving state feels the language used by the sending state is rude, or the subject matter represents an inappropriate intercession into the receiving state's internal affairs.\n\n"}
{"id": "24747714", "url": "https://en.wikipedia.org/wiki?curid=24747714", "title": "Dynamic Business Modeling", "text": "Dynamic Business Modeling\n\nDynamic Business Modeling (\"DBM\") describes the ability to automate business models within an open framework. The independent analyst firm Gartner has recently called Dynamic Business Modeling \"critical for BSS solutions to succeed\".\n\nDynamic Business Modeling is based on principles wherein the business logic of an application is managed independently from the application servers that automate the services and processes defined in the business logic. Business modeling and integration (which itself is defined as part of the business model) are defined in a business logic layer, allowing underlying application servers to be business logic agnostic and therefore need no business driven customization. DBM applied correctly should reduce both the cost and risk in the initial implementation and its future evolution of systems.\n\nPrevious generations of IT systems (from 1990 to approximately 2001) were designed to address specific business models and regulatory practices and no value was given to logic–infrastructure segregation. These systems provided value by automating predefined business models (commonly referred to as \"off-the-shelf\"). As a result, they implicitly drove business strategy where DBM states that they should be driven by it. By being \"predefined\" they do not:\n\n\nDynamic Business modeling is suited for open automation of strategy-driven business models. By removing the need for customization of core application servers it is postulated as more cost efficient, rapidly deployed and evolveable. Dynamic Business Modeling was initially described (though applied much earlier in practice) by Doug Zone at MetraTech Corp. in reference to the billing segment of the enterprise software market. \"Service Oriented Applications\" (also known as \"service based applications\") coined by IBM describe potential methodologies to achieve DBM.\n\nDynamic Business Modeling is defined as the automation of Enterprise Business Models based on the principle that the model's underlying business processes and business services need to be dynamically and openly definable and re-definable.\n\nDynamic Business Modeling is defined as the enabler of a strategic advantage achieved by focused differentiation in any aspect of business (from marketing to finance to operations). This differentiation is achieved through how business is conducted: openly and dynamically defining the business model. Capital investment – human, physical and intellectual – must be aimed at allowing the definition of the business model to be dynamic.\n\nDynamic Business Modeling recognises that businesses dynamically evolve, re-inventing their (business) models to achieve strategic advantage. DBM posits that the role of enterprise software (CRM, billing, ERP) is to dynamically automate and advance the business processes and services that lie behind these Business models.\n\nThe term was first used to describe the architecture of MetraNet, a charging, billing, settlement and customer care from MetraTech Corp.\n\n\n\n\nhttp://ralyx.inria.fr/2008/Raweb/triskell/triskell.pdf\n\n"}
{"id": "49680032", "url": "https://en.wikipedia.org/wiki?curid=49680032", "title": "ECOCITIES (software)", "text": "ECOCITIES (software)\n\nAll member states of the European Union are bound to decrease their greenhouse gas emissions. For example, the EU climate and energy package requires member states to improve their energy efficiency by 20%, increase -renewable energy production by 20% and reduce their emissions by 20%. With about 40% of CO2 emissions heating, cooling and hot water production in buildings is one of the largest greenhouse gas producers. At the same time the building sector has the largest potential for energy savings. In their effort to lead a change towards greater energy efficiency and a reduction of greenhouse gas, many companies, cities and municipalities are in the process of developing low carbon action plans. However, the costs for developing optimal action plans and their continuous monitoring and optimization are very high, thus, often hindered by the tense financial situation, especially of cities and municipalities. With significant economic and environmental downsides; the consequences are additional costs due to the non-compliance to the national and EU emission goals and untapped energy saving potential.\n\nECOCITIES is an energy optimization system for building portfolios combining and extending the benefits of Energy Management Software (EMS), Computer-aided Facility Management (CAFM) software and building portfolio management software. It integrates building administration and monitoring, energy accounting and building portfolio optimization. Thereby, it supports the definition of low carbon action plans in terms of environmental impact (e.g., carbon footprint, energy efficiency) and financial impact (e.g., investment costs, running, costs). The system considers the following options for each building and the resulting (inter-) dependencies:\n\nECOCITIES calculates all energy- and cost-efficient development scenarios, visualizes them on the screen and allows decision makers to interactively explore the consequences of their actions (e.g., what are the citywide costs and the corresponding CO2 reductions of introducing a neighborhood-scale combined heat and power plant). ECOCITIES is an enabler for the realization of energy goals and provides synergies with existing endeavors of achieving national and European energy goals. On an operational level, ECOCITIES supports the following processes:\n"}
{"id": "2286665", "url": "https://en.wikipedia.org/wiki?curid=2286665", "title": "Enterprise asset management", "text": "Enterprise asset management\n\nEnterprise asset management (EAM) involves the management of the maintenance of physical assets of an organization throughout each asset's lifecycle. EAM is used to plan, optimize, execute, and track the needed maintenance activities with the associated priorities, skills, materials, tools, and information. This covers the design, construction, commissioning, operations, maintenance and decommissioning or replacement of plant, equipment and facilities.\n\n\"Enterprise\" refers to the scope of the assets in an Enterprise across departments, locations, facilities and, potentially, supporting business functions eg; Finance & GL, Human Resources and Payroll. Various assets are managed by the modern enterprises at present. The assets may be fixed assets like buildings, plants, machineries or moving assets like vehicles, ships, moving equipments etc. The lifecycle management of the high value physical assets require regressive planning and execution of the work. \n\nEAM arose as an extension of the Computerized maintenance management system (CMMS) which is usually defined as a system for the computerisation of the maintenance of physical assets.\n\nEnterprise asset management software is a computer software that handles every aspect of running a public works or asset-intensive organization. Enterprise asset management (EAM) software applications include features such as asset life-cycle management, preventive maintenance scheduling, warranty management, integrated mobile wireless handheld options and portal-based software interface. Rapid development and availability of mobile devices also affected EAM software which now often supports Mobile enterprise asset management.\n\n\n"}
{"id": "15500558", "url": "https://en.wikipedia.org/wiki?curid=15500558", "title": "Financial roadshows", "text": "Financial roadshows\n\nA financial roadshow is a series of meetings across different cities in which top executives from a company have the opportunity to talk with current or potential investors. They can range from two or three days in one country or continent to marathon, three week trips to financial centers around the world. The most common reason for conducting financial roadshows is an initial public offerings IPO, in which a privately held company’s shares go public and investors have an opportunity to buy into it. Another reason for roadshows is privatization of a government-owned corporation, similar to a private company going public, in which a company seeks investors to buy new stock it is issuing to raise money. Finally, a non-deal roadshow is purely so that executives can hold discussions with current and potential investors, and nothing is offered for sale.\n"}
{"id": "199035", "url": "https://en.wikipedia.org/wiki?curid=199035", "title": "Focus group", "text": "Focus group\n\nA focus group is a small, but demographically diverse group of people and whose reactions are studied especially in market research or political analysis in guided or open discussions about a new product or something else to determine the reactions that can be expected from a larger population. It is a form of qualitative research consisting of interviews in which a group of people are asked about their perceptions, opinions, beliefs, and attitudes towards a product, service, concept, advertisement, idea, or packaging. Questions are asked in an interactive group setting where participants are free to talk with other group members. During this process, the researcher either takes notes or records the vital points he or she is getting from the group. Researchers should select members of the focus group carefully for effective and authoritative responses.\n\nFocus groups have a long history and were used during the Second World War (1939-1945) to examine the effectiveness of propaganda.\nAssociate director sociologist Robert K. Merton set up focus groups at the Bureau of Applied Social Research in the USA prior to 1976. Psychologist and marketing expert Ernest Dichter coined the term \"focus group\" itself before his death in 1991.\n\nIn library and information science, when the library intends to work on its collection, the library consults the users who are the reason the library was established. This is an important process in meeting the needs of the users.\n\nIn the social sciences and urban planning, focus groups allow interviewers to study people in a more natural conversation pattern than typically occurs in a one-to-one interview. In combination with participant observation, they can be used for learning about groups and their patterns of interaction. An advantage is their fairly low cost compared to surveys, as one can get results relatively quickly and increase the sample size of a report by talking with several people at once. Another advantage is that they can be used as an occasion for participants to learn from one another as they exchange and build on one another's views, so that the participants can experience the research as an enriching encounter. This counteracts the extractive nature of research which seeks to \"mine\" participants for data (with no benefit for them) as criticized by various authors, and in particular Indigenous-oriented authors (and others sharing similar sentiments), as explained, for example, by Romm (2015): http://www.qualitative-research.net/index.php/fqs/article/view/2087.\n\nIn the world of marketing, focus groups are seen as an important tool for acquiring feedback regarding new products, as well as various other topics. In marketing, focus groups are usually used in the early stages of product or concept development, when organizations are trying to create an overall direction for marketing initiative. In particular, focus groups allow companies wishing to develop, package, name, or test market a new product, to discuss, view, and/or test the new product before it is made available to the public. This can provide valuable information about the potential market acceptance of the product.\n\nA focus group is an interview, conducted by a trained moderator among a small group of respondents. The interview is conducted in an informal and natural way where respondents are free to give views from any aspect. Focus groups are similar to, but should not be confused with in-depth interviews. The moderator uses a discussion guide that has been prepared in advance of the focus group to guide the discussion. Generally the discussion goes from overall impressions of a brand or product category and becomes more specific as the discussion progresses.\n\nParticipants are recruited on the basis of similar demographics, psychographics, buying attitudes, or behaviors.\n\nRepresentatives of the stake holder (often a design team in the case of testing acceptance on a new product) are not involved in the focus group, not to bias the exercise. However they may attend the focus group, either through video cameras, or by watching through a one way mirror.\n\nTraditional focus groups can provide accurate information, and are less expensive than other forms of traditional marketing research. There can be significant costs however: if a product is to be marketed on a nationwide basis, it would be critical to gather respondents from various locales throughout the country since attitudes about a new product may vary due to geographical considerations. This would require a considerable expenditure in travel and lodging expenses. Additionally, the site of a traditional focus group may or may not be in a locale convenient to a specific client, so client representatives may have to incur travel and lodging expenses as well.\n\nToday, using audience response keypads to collect questionnaire answers is the new industry trend.\n\n\nVariants of focus groups include:\n\n\nA fundamental difficulty with focus groups (and other forms of qualitative research) is the issue of \"observer dependency\": the results obtained are influenced by the researcher or his or her own reading of the group's discussion, raising questions of validity (see experimenter's bias).\nFocus groups are \"One shot case studies\" especially if they are measuring a property-disposition relationship within the social sciences, unless they are repeated. Focus groups can create severe issues of external validity, especially the reactive effects of the testing arrangement. Other common (and related) criticism involve groupthink and social desirability bias.\n\nAnother issue is with the setting itself. If the focus groups are held in a laboratory setting with a moderator who is a professor and the recording instrument is obtrusive, the participants may either hold back on their responses and/or try to answer the moderator's questions with answers the participants feel that the moderator wants to hear. Another issue with the focus group setting is the lack of anonymity. With all of the other participants, there can not be any guarantee of confidentiality.\n\nDouglas Rushkoff argues that focus groups are often useless, and frequently cause more trouble than they are intended to solve, with focus groups often aiming to please rather than offering their own opinions or evaluations, and with data often cherry picked to support a foregone conclusion. Rushkoff cites the disastrous introduction of New Coke in the 1980s as a vivid example of focus group analysis gone bad.\n\nJonathan Ive, Apple’s senior vice president of industrial design, also said that Apple had found a good reason not to do focus groups: \"They just ensure that you don’t offend anyone, and produce bland inoffensive products.\"\n\nThe analysis of focus group data presents both challenges and opportunities when compared to other types of qualitative data. Some authors have suggested that data should be analysed in the same manner as interview data, while others have suggested that the unique features of focus group data – particularly the opportunity that it provides to observe interactions between group members - means that distinctive forms of analysis should be used. Data analysis can take place at the level of the individual or the group.\n\nFocus group data provides the opportunity to analyse the strength with which an individual holds an opinion. If they are presented with opposing opinions or directly challenged, the individual may either modify their position or defend it. Bringing together all the comments that an individual makes in order can enable the researcher to determine whether their view changes in the course of discussion and, if so, further examination of the transcript may reveal which contributions by other focus group members brought about the change.\n\nAt the collective level, focus group data can sometimes reveal shared understandings or common views. However, there is a danger that a consensus can be assumed when not every person has spoken: the researcher will need to consider carefully whether the people who have not expressed a view can be assumed to agree with the majority, or whether they may simply be unwilling to voice their disagreement.\n\nThe United States federal government makes extensive use of focus groups to assess public education materials and messages for their many programs. While many of these are appropriate for the purpose, many others are reluctant compromises which federal officials have had to make as a result of studies independent of whether a focus group is the best or even appropriate methodology.\n\nSwedish artist Måns Wrange has used the concept of the focus group in his work \"The Good Rumor Project\". In this instance the focus group situation is used not only as a means to investigate the opinions of the group members, but also to spread an idea (the \"rumor\") across society with the help of the group members.\n\nVarious creative activity-oriented questions can serve as supplements to verbal questions including but not limited to the following:\n\n\n"}
{"id": "31360441", "url": "https://en.wikipedia.org/wiki?curid=31360441", "title": "Forecast by analogy", "text": "Forecast by analogy\n\nForecast by analogy is a forecasting method that assumes that two different kinds of phenomena share the same model of behaviour. For example, one way to predict the sales of a new product is to choose an existing product which \"looks like\" the new product in terms of the expected demand pattern for sales of the product.\n\n\"Used with care, an analogy is a form of scientific model that can be used to analyze and explain the behavior of other phenomena.\"\n\nAccording to some experts, research has shown that the careful application of analogies improves the accuracy of the forecast.\n\n"}
{"id": "45357466", "url": "https://en.wikipedia.org/wiki?curid=45357466", "title": "Gopili", "text": "Gopili\n\nGopili is a travel metasearch engine launched in UK in December 2014. The website compiles and presents data on domestic and international travel.\n\nGopili is the European brand of KelBillet, which is a multimodal travel search engine in France. Gopili’s website is today available in UK, Spain, Germany, Italy and Russia.\n\nThe website has headquarters in Rennes, France.\n\n"}
{"id": "41261061", "url": "https://en.wikipedia.org/wiki?curid=41261061", "title": "HP Service Manager software", "text": "HP Service Manager software\n\nThe HP Service Manager is one of the applications acquired by HP when it purchased Peregrine Systems in 2005. The application was originally known as PNMS (Peregrine Network Management System). After releasing the first version of PNMS, Peregrine Systems eventually added functionality such as Request Management, Call Management, and Change Management and rebranded the application as Peregrine ServiceCenter.\n\nAfter the acquisition by HP, the application was rebranded as HP Service Manager and was included in the HP OpenView product suite. HP offers the application as a service desk solution that enables IT to work as a single organization, governed by a consistent set of processes to handle service delivery and support quickly and efficiently.\n\nAfter the acquisition, the product has been updated with the following significant changes:\n\nFor more information refer to:\n\nService Manager is an ITSM Tool using the ITIL framework providing a web interface for corporate changes, releases and interactions (request fulfillment) supported by a Service catalog and CMDB. For a summary of the functionality, screenshots, data sheets, white papers and more information refer to the sources listed above.\n\nPropel is a separate web frontend providing a cut down view of HP Service Manager functions. This front end can be tailored to business requirements for service request, incident and change ticket management.\n"}
{"id": "230405", "url": "https://en.wikipedia.org/wiki?curid=230405", "title": "Hamburger University", "text": "Hamburger University\n\nHamburger University is a training facility of McDonald's, located in Oak Brook, Illinois, a western suburb of Chicago. This corporate university was designed to instruct personnel employed by McDonald's in the various aspects of restaurant management. More than 80,000 restaurant managers, mid-managers and owner/operators have \"graduated\" from this facility.\n\nSince its establishment, Hamburger University has been situated on an campus. Restaurant employees receive approximately 32 hours of training in their first month with McDonald's and more than 5,000 students attend Hamburger University each year. Founder Ray Kroc oversaw lessons at its beginning.\n\nIn 2016 McDonald's Corporation announced it would move its U.S. and world headquarters including Hamburger University from its long time home in Oak Brook to a newly built complex on property which formerly housed Oprah Winfrey's Harpo Studios in Chicago. Starting in 2017 McDonald's put up for sale or sold most of its Oak Brook campus.\n\nHamburger University was satirized in the 1986 comedy, \"Hamburger... The Motion Picture\". It was also spoofed by McDonald's itself in a commercial with Ronald McDonald in which several animatronic hamburgers were shown graduating from school.\n"}
{"id": "18134326", "url": "https://en.wikipedia.org/wiki?curid=18134326", "title": "Integrated business planning", "text": "Integrated business planning\n\nIntegrated Business Planning is a planning process that integrates across two or more functions in a business or government entity referred to as an enterprise to maximize\nfinancial value.\n\nThe specific functional areas in a company as well as the industry domain associated with the company defines the specific type of IBP process. \nExamples of IBP processes are:\n\n\nThe key requirement for IBP is that two or more functional process areas must be involved and maximizing (optimizing) of financial value should be done.\n\nCorporate executives, business unit heads and planning managers use IBP to evaluate plans and activities based on the economic impact of each consideration.\n\nBridging Corporate Performance Management and S&OP\nThere has been a lot of focus on Integrated Business Planning in the context of Sales and Operations Planning. Gartner (www.gartner.com) refers to a 5-stage\nS&OP Maturity model wherein IBP is referred to as the Phased 4 & 5. Integrated Business Planning however is more broader than S&OP. It is an approach that combines Enterprise Performance Management (EPM) and sales and operations planning (S&OP) to provide incremental capabilities that neither provides individually. In so doing, IBP platforms address long-standing challenges that financial and operational professionals have struggled to overcome. The result: opportunities for step change improvements to how manufacturers plan, manage and govern their business. Here, the focus is on strengthening the financial integration and reconciliation of plans, as well as increasing the responsiveness of the supply chain using ad-hoc reports and what-if scenario analyses.\n\nIntegrated Business Planning requires the following capabilities to be enabled:\n\na) Enterprise Model\nb) Integrated Planning\n\nc) Enterprise Optimization\n\nIBP has been used to model and integrate the planning efforts in a number of applications, including:\nAll of the above can be summarized as Enterprise Optimization use cases.\n\nSome argue that IBP is not any different from S&OP. See article by Patrick Bower where he calls IBP a marketing hoax, a name developed to create confusion and sell consulting and system services. The main proponents of IBP are in fact consulting companies. In return to this criticism,\nit has been pointed out that IBP is not a marketing hoax, but an important\npart of Enterprise Performance Management (EPM) system.\n\nAnother criticism is that IBP is not academically defined and is supply chain biased in its definition. The lack of academic standard leaves room for interpretation to what IBP is, which is confusing practitioners. In a 2015 S&OP survey, 32% of participants answered that there is no difference between S&OP and IBP, 20% \"did not know\", and 71% of participants answered that there is a need for more industry standards around S&OP.\n\nIt has been called out that IBP has a lack of governance and in need of an industry group to create a unified definition. Due this lack of academic and industry standards, there has been an attempt to create an open source definition for IBP. This definition is as follows:\n\nIntegrated Business Planning (IBP): A holistic planning philosophy, where all organizational functions participate in providing executives periodically with valid, reliable information, in order to\ndecide how to align the enterprise around executing the plans to achieve budget, strategic intent and the envisioned future.\n\n"}
{"id": "6731231", "url": "https://en.wikipedia.org/wiki?curid=6731231", "title": "Lex mercatoria", "text": "Lex mercatoria\n\nLex mercatoria (from the Latin for \"merchant law\"), often referred to as \"the Law Merchant\" in English, is the body of commercial law used by merchants throughout Europe during the medieval period. It evolved similar to English common law as a system of custom and best practice, which was enforced through a system of merchant courts along the main trade routes. It functioned as the international law of commerce. It emphasised contractual freedom and alienability of property, while shunning legal technicalities and deciding cases \"ex aequo et bono\". A distinct feature was the reliance by merchants on a legal system developed and administered by them. States or local authorities seldom interfered, and did not interfere a lot in internal domestic trade. Under \"lex mercatoria\" trade flourished and states took in large amounts of taxation.\n\nIn the last years new theories had changed the understanding of this medieval treatise considering it as proposal for legal reform or a document used for instructional purposes. These theories consider that the treatise cannot be described as a body of laws applicable in its time, but the desire of a legal scholar to improve and facilitate the litigation between merchants. The text is composed by 21 sections and an annex. The sections described procedural matters such as the presence of witnesses and the relation between this body of law and common law. It has been considered as a false statement to define this as a system exclusively based in custom, when there are structures and elements from the existent legal system, such as Ordinances and even concepts proper of the Romano-canonical procedure.\n\nThe \"lex mercatoria\" was originally a body of rules and principles laid down by merchants to regulate their dealings. It consisted of rules and customs common to merchants and traders in Europe, with some local variation. It originated from the need for quick and effective jurisdiction, administered by specialised courts. The guiding spirit of the merchant law was that it ought to derive from commercial practice, respond to the needs of the merchants, and be comprehensible and acceptable to the merchants who submitted to it. International commercial law today owes some of its fundamental principles to the \"lex mercatoria\". This includes choice of arbitration institutions, procedures, applicable law and arbitrators, and the goal to reflect customs, usage and good practice among the parties.\n\nGoods and services flowed freely during the medieval merchant law, thus generating more wealth for all involved. It is debated whether the law was uniform in nature, was spontaneous as a method of dispute resolution, or applied equally to everyone who subordinated to it. The \"lex mercatoria\" was also a means for local communities to protect their own markets. Local kings or lords extracted taxes and set trade restrictions. In 1303 Edward I issued the Carta Mercatoria, a charter to foreign merchants in England, which guaranteed them freedom to trade, with certain protections and exemption under the law. Although the charter was revoked by Edward II, due to complaints by English merchants, foreign merchants retained most of their rights in practice, but these would vary widely with the march of time, events and changes to state policy.\n\nThe \"lex mercatoria\" was the product of customs and practices among traders, and could be enforced through the local courts. However, the merchants needed to solve their disputes rapidly, sometimes on the hour, with the least costs and by the most efficient means. Public courts did not provide this. A trial before the courts would delay their business, and that meant losing money. The \"lex mercatoria\" provided quick and effective justice. This was possible through informal proceedings, with liberal procedural rules. The \"lex mercatoria\" rendered proportionate judgements over the merchants’ disputes, in light of \"fair price\", good commerce, and equity.\n\nJudges were chosen according to their commercial background and practical knowledge. Their reputation rested upon their perceived expertise in merchant trade and their fair-mindedness. Gradually, a professional judiciary developed through the merchant judges. Their skills and reputation would however still rely upon practical knowledge of merchant practice. These characteristics serve as important measures in the appointment of international commercial arbitrators today.\n\nThe \"lex mercatoria\" owed its origin to the fact that the civil law was not sufficiently responsive to the growing demands of commerce, as well as to the fact that trade in pre-medieval times was practically in the hands of those who might be termed cosmopolitan merchants, who wanted a prompt and effective jurisdiction. It was administered for the most part in special courts, such as those of the guilds in Italy, or the fair courts of Germany and France, or as in England, in courts of the Staple or Piepowder.\n\nThe \"lex mercatoria\" was composed of such usages and customs as were common to merchants and traders in all parts of Europe, varied slightly in different localities by special peculiarities. Less procedural formality meant speedier dispensation of justice, particularly when it came to documentation and proof. Out of practical need, the medieval \"lex mercatoria\" originated the “writing obligatory”. By this, creditors could freely transfer the debts owed to them. The “writing obligatory” displaced the need for more complex forms of proof, as it was valid as a proof of debt, without further proof of; transfer of the debt; powers of attorney; or a formal bargain for sale. The \"lex mercatoria\" also strengthened the concept of party autonomy: whatever the rules of the \"lex mercatoria\" were, the parties were always free to choose whether to take a case to court, what evidence to submit and which law to apply.\n\nMerchant law declined as a cosmopolitan and international system of merchant justice towards the end of medieval times. This was to a large extent due to the adoption of national commercial law codes. It was also connected with an increasing modification of local customs to protect the interests of local merchants. The result of the replacement of \"lex mercatoria\" codes with national governed codes was the loss of autonomy of merchant tribunals to state courts. The main reason for this development was the protection of state interests.\n\nThe nationalisation of the \"lex mercatoria\" did not neglect the practises of merchants or their trans-border trade. Some institutions continued to function, and state judges also were appointed for their merchant expertise, just as modern commercial arbitrators. The laws of the merchants were not eradicated, but simply codified. National codes built on the principles laid down by trade commercial practise and to a large extent they embodied \"lex mercatoria\" substantial rules. This was for example the case in France. The Code Commercial was issued in 1807, where \"lex mercatoria\" rules were preserved to govern formation, performance and termination of contracts. In effect, the nation states reconstituted the \"lex mercatoria\" in their image.\n\nEnglish courts applied merchant customs only if they were \"certain\" in nature, \"consistent with law\" and \"in existence since time immemorial.\" English judges also required that merchant customs were proven before the court. But even as early as 1608, Chief Justice Edward Coke described \"lex mercatoria\" as \"a part of the common law,\" and William Blackstone would later concur. The tradition continued especially under Lord Mansfield, who is said to be the father of English commercial law. Precepts of the \"lex mercatoria\" were also kept alive through equity and the admiralty courts in maritime affairs. In the US, traditions of the \"lex mercatoria\" prevailed in the general principles and doctrines of commercial jurisprudence.\n\nThe history of the \"lex mercatoria\" in England is divided into three stages: the first prior to the time of Coke, when it was a special kind of law – as distinct from the common law – administered in special courts for a special class of the community (i.e. the mercantile); the second stage was one of transition, the \"lex mercatoria\" being administered in the common law courts, but as a body of customs, to be proved as a fact in each individual case of doubt; the third stage, which has continued to the present day, dates from the presidency over the king's bench of Lord Mansfield (q.v.), under whom it was moulded into the mercantile law of to-day. To the \"lex mercatoria\" modern English law owes the fundamental principles in the law of partnership, negotiable instruments and trade marks.\nSir John Holt (Chief Justice 1689 to 1710) and Lord Mansfield (Chief Justice, 1756 to 1788) were the leading proponents of incorporating the \"lex mercatoria\" into the common law. Holt did not complete the task, possibly out of his own conservatism (see \"Clerke v Martin\") and it was Lord Mansfield that became known as the 'founder of the commercial law of this country\" (Great Britain). Whilst sitting in Guildhall, Lord Mansfield created,\na body of substantive commercial law, logical, just, modern in character and at the same time in harmony with the principles of the common law. It was due to Lord Mansfield's genius that the harmonisation of commercial custom and the common law was carried out with an almost complete understanding of the requirements of the commercial community, and the fundamental principles of the old law and that that marriage of idea proved acceptable to both merchants and lawyers.\n\n\n\"Lex mercatoria\" precepts have been reaffirmed in new international mercantile law. National trade barriers are torn down in order to induce commerce. The new commercial law is grounded on commercial practice directed at market efficiency and privacy. Dispute resolution has also evolved, and functional methods like international commercial arbitration is now available. These developments have also attracted the interest of empirical sociology of law The principles of the medieval \"lex mercatoria\" – efficiency, party autonomy, and choice of arbitrator – are applied, and arbitrators often render judgements based on customs. The new merchant law encompasses a huge body of international commercial law.\n\nIn summary, nation states somewhat fragmented the medieval \"lex mercatoria\" but it is far from destroyed. Local interests triumphed in the medieval ages, just as national interests do today. A modern variant of the \"lex mercatoria\" is the evolving law and dispute resolution in cyberspace. Internet traders are the fastest growing body of merchants in history. Parties can solve domain-name disputes online expeditiously and quickly. In a virtual court documents are filed and examined online, arguments are made online and decisions are published online – seldom challenged before traditional courts of law. ICANN's UDRP (and its proposals for Rapid Suspension) and Nominet's DRS are examples of this. The medieval, the modern and cyberspace merchant laws face comparable issues of enforceability. They solve the problems somewhat differently, but the reaction of the market is the main incentive to comply with a ruling.\n\nFurther, \"lex mercatoria\" is sometimes used in international disputes between commercial entities. Most often those disputes are decided by arbitrators which sometimes are allowed (explicitly of implied) to apply \"lex mercatoria\" principles. Therefore, some legal practitioners assume that there is a whole set of legal principles named \"lex mercatoria\" in international or transnational commercial law. The most recent and constantly updated set of rules are the \"TransLex Principles\" collected and formulated by Klaus Peter Berger (University of Cologne) and his Center for Transnational Law.\n\nWhat remains of \"lex mercatoria\" precepts today is a qualified faith in self-regulation by merchants, and a reluctance to surrender the efficiencies of merchant practice to state confinement.\n\n\n\n"}
{"id": "36139685", "url": "https://en.wikipedia.org/wiki?curid=36139685", "title": "List of fastest-selling products", "text": "List of fastest-selling products\n\nThis is a list of the fastest-selling product superlatives. The definition of \"fastest-selling\" typically refers to sales results in the first week of a product's release, (i.e., which product had the biggest launch). Sometimes, rather than comparing sales figures relative to time, the target to be reached is fixed (e.g. sales at 5 million units), and the comparison is based upon how many weeks it takes to reach it (e.g. 52 weeks vs. 56 weeks).\n\n\n\n\n\n1. 2018 concert (South Korea) - \"EXO Planet #4 - The Elyxion dot\" at Gocheok Sky Dome. \"EXO\" maintained its own record again by selling all three days tickets (66000 tickets) in 0.2 seconds with more than 1 million people trying to buy it.\n\n2. 2017 Concert (South Korea) -- \"EXO Planet #3 - The ElyXion\" at Gocheok Sky Dome sold out (66000 tickets) in 0.2 seconds. Beating \"EXO\" own previous record of 0.4 seconds (67,040 tickets) from their second tour and being the fastest sold-out tour in the world\n\n3. 2015 Concert (South Korea) - \"EXO Planet #2 - The EXO'luxion\" sold out (67,040 tickets) in 0.4 seconds, which was a record-breaking and at a time was the fastest sold-out concert in the world, though \"EXO\" breaks the world record again in 2017 with their new concert tour \n\n4. 2014 Concert (South Korea) – \"EXO Planet #1 - The Lost Planet\" at the \"Olympic Gymnastics Arena\" sold out (42,000 tickets) in 1.47 seconds \n\n5.2012 Concert (Japan) – TVXQ \"\" at Tokyo Dome sold out (165,000 tickets) within 5 seconds.\n\n6.2008 Concert (Japan) – TVXQ \"3rd Live Tour ~T~ Concert\" at Saitama Super Arena sold out (40,000 tickets) in 1 minute.\n\n7.2013 Show (UK) – The return of Monty Python's \"Flying Circus\" at London's O2 Arena, sold out (14,500 tickets) in 43.5 seconds.\n\n8.2012 Concert (UK) – The Stone Roses at Manchester’s Heaton Park, sold out (220,000 tickets) in 68 minutes.\n\n\n\n"}
{"id": "29533404", "url": "https://en.wikipedia.org/wiki?curid=29533404", "title": "Lloyd's List Intelligence", "text": "Lloyd's List Intelligence\n\nLloyd's List Intelligence \"(formerly Lloyd's MIU)\" is a specialist business information service dedicated to the global maritime community. It is a member of the publicly quoted group Informa plc and forms part of the Lloyd's List Group along with sister company Lloyd's List.\n\nWith a history of collecting maritime data dating back nearly 300 years, Lloyd's List Intelligence provides an interactive online service (www.lloydslistintelligence.com) offering detailed vessel movements, real-time AIS positioning, comprehensive information on ships, companies, ports and casualties as well as credit reports, industry data and analysis including short-term market outlook reports. Lloyd's List Intelligence also provides a range of support services such as in-depth consultancy, investigations, due diligence, market trend analysis and credit risk appraisal for entire portfolios.\n\nIn Feb 2017, Lloyd's List Intelligence launch a newer platform known as Next Gen Lloyd's List Intelligence ()\n\nLloyd's List Intelligence has a global presence with principal offices located in the UK, US, Singapore and Australia. In addition, Lloyd's List Intelligence directly employs expert analysts and researchers in Greece, India, Canada and China.\n\nInformation is sought from a multitude of sources and incorporates some of the most extensive resources available including the world's largest AIS (Automatic Identification System) network, the largest online maritime database and the largest databank of marine / energy / commodity company credit reports.\n\nIts unique global network of specialist sources also includes the Lloyd's Agency Network of 700 agents and sub-agents for vessel movements' data, the leading registries and classification societies for vessel characteristics, and major company registries around the globe for corporate data.\n\nFounded on the heritage of the maritime industry bible Lloyd's List, it has a 300-year-old history in providing information to the maritime world. Tracking vessel movements has developed into a global competence since the early days - Lloyd's List got its name back in 1734, when it was a journal listing ship arrival and departures.\n\nToday, Lloyd's List Intelligence brings together the expertise of a global staff of maritime analysts and journalists, with the most extensive system of shore based intelligence gathering to create the complete information support service for the maritime industry.\n\nLloyd's List Intelligence provides a wide range of services via industry-specific channels. Each channel provides a dedicated resource to that sector offering a combination of specialised industry data, tools and analysis as well as access to detailed vessel movements, AIS positioning and information on over 155,000 vessels, 165,000 maritime companies and details of 19,900 ports and terminals. This is all supplemented with the Ask the Analyst service, which offers direct access to a global team of expert analysts to perform business critical studies (including investigations, due diligence, in-depth competitor analysis).\n\nLloyd's List Intelligence channels include:\n\nEach channel is available as a standalone subscription or can be combined with additional channels and services to meet the individual needs of an organisation.\n\nOther Services provided include discreet investigations into vessels, asset tracking, companies and due diligence, Lloyd's List Intelligence also offers the unique Portfolio Review Service (PRS) which can be used to assess the overall risk and credit exposures for entire portfolios. Individual credit reports can also be commissioned or existing reports can be purchased direct from their databank of over 17,000 marine, energy and commodity company credit reports.\n\nLloyd's List Intelligence employs over 200 staff across the globe. These include specialist journalists and 60 market and data analysts who collect and process information from 1,500 sources, 24 hours a day, 7 days a week, 365 days a year.\n\nMore than 1,400 locations are covered globally by land based AIS receivers to provide live coverage of over commercial ports and terminals in 132 countries. Over 100 million positions are reported every day from 40,000 unique vessels.\n\nIn addition, tracking is enhanced with satellite AIS reports as a result of an agreement with ORBCOMM Inc. Satellite AIS positions are recorded from 1Nm to more than 1,500Nm from shore, providing greater reporting where land based AIS is reduced.\n\nVessel movements are also uniquely corroborated by visual intelligence from thousands of exclusive contacts around the globe.\n\nKey data suppliers include:\n\n\n"}
{"id": "43646968", "url": "https://en.wikipedia.org/wiki?curid=43646968", "title": "Machiavellianism in the workplace", "text": "Machiavellianism in the workplace\n\nMachiavellianism in the workplace is the employment of cunning and duplicitous behavior in a business setting. The term \"Machiavellianism\" is from the book \"The Prince\" by Machiavelli which lays out advice to rulers how to govern his or her subjects. Machiavellianism has been studied extensively over the past 40 years as a personality characteristic that shares features with manipulative leadership tactics. It has in recent times been adapted and applied to the context of the workplace and organizations by many writers and academics. The Machiavellian typically manipulates on occasions where it is advantageous to achieve the required objectives.\n\nOliver James identifies Machiavellianism as one of the dark triadic personality traits in the workplace, the others being narcissism and psychopathy.\n\nA new model of Machiavellianism based in organizational settings consists of three factors:\n\nThe presence of Machiavellianism in an organisation has been positively correlated with counterproductive workplace behaviour and workplace deviance.\n\nIndividuals who are high in Machiavellianism may be more willing and more skilled at lying and less likely to give honest answers during interviews. Individuals high in Machiavellianism have stronger intentions to use deception in interviews compared to psychopaths or narcissists and are also more likely to see the use of lying in interviews as fair. Men and women high in Machiavellianism may use different tactics to influence interviewers. In one study, which examined the how much applicants allowed the interviewers to direct the topics covered during the interview, women high in Machiavellianism tended to allow interviewers more freedom to direct the content of the interview. Men high in Machiavellianism gave interviewers the least amount of freedom in directing the content of the interview. Men high in Machiavellianism were also more likely to make up information about themselves or their experiences during job interviews.\n\nAccording to Namie, Machiavellians manipulate and exploit others to advance their perceived personal agendas and to maintain dominance over others.\n\nThe following are the guiding principles of Machiavellianism:\n\nHigh Machiavellians may be expected to do the following:\n\nIn studies there was a positive correlation between Machiavellianism and workplace bullying. Machiavellianism predicted involvement in bullying others. The groups of bullies and bully-victims had a higher Machiavellianism level compared to the groups of victims and persons non-involved in bullying. The results showed that being bullied was negatively related to the perceptions of clan and adhocracy cultures and positively related to the perceptions of hierarchy culture.\n\nIn research, Machiavellianism was positively associated with subordinate perceptions of abusive supervision (an overlapping concept with workplace bullying).\n\n\n\n"}
{"id": "22064549", "url": "https://en.wikipedia.org/wiki?curid=22064549", "title": "Management due diligence", "text": "Management due diligence\n\nManagement due diligence is the process of appraising a company's senior management—evaluating each individual's effectiveness in contributing to the organization's strategic objectives.\n\nAssessing company management is crucial when closing business deals. It can mean the difference between long-term success or sudden failure. It also helps the organisation understand how the teams perform their roles in context with the company's future business plan. This helps clarify the structure of the organisation's work-force.\nThe management due diligence process can be identified as an informative tool for external stakeholders, and can also be referred to as Management Assessment as it addresses the team’s dynamics and highlight the risks.\n\nManagement assessment usually focuses on assessing the leadership skills and characteristics of the organisation's managers—such as the ability to adjust to a changing environment and communicate effectively with other individuals. These characteristics are key points in successful leaders.\n\nA leader must consider all factors concerning a strategic decision, such as possible effects on employees and customers. Engaging employees and customers in the decision process helps build better relationships.\n\nCompanies typically apply the due diligence process when they are about to engage in a major transaction with another company—such as selling or purchasing products or services, or buying (merging with or acquiring) the other company. Some transactions require a due diligence report that includes managements. Transactions that might require managerial assessments include:\n\nOrganizations considering a merger, acquisition or alliance should perform due diligence. This due diligence should investigate the other party's management team. Many mergers and acquisitions fail because of human resources and management-related issues, such as cultural clashes. These incidents occur because of different cultural values or different individual beliefs. To avoid such incidents, and cut costs on the long run, management teams must be assessed thoroughly.\n\nBefore organizations signs a partnership contract, they must investigate the other organization’s matters and affiliations, organizational structures, and behaviors. A management due diligence process achieves this.\n\nWhen forming a relationship with another organizations, management due diligence helps an organization introduce management structure the behavior of individuals.\n\nNot only buyers carry out management due diligence, but also sellers of an organization. Usually, the process of selling an organization or adopting any external growth strategies requires the sharing of warranties. These warranties require private information from the organization—possibly including information and activities that they must shield from the view of third parties. In this case, the seller should carry out due diligence to ensure secure data.\n\nManagement due diligence ensures sustainable profit and growth for organisations, as it identifies the human capital components. It ensures that highly skilled people are assigned the correct jobs and responsibilities. It increases the chance of a good return on investment by reducing risk.\n\nManagement due diligence identifies strengths and weaknesses of individuals in the management team and assesses their contributions to the organisation. It assesses management team members' abilities to reach common goals.\n\nIt identifies undiscovered dangers that eventually affect productivity. These might include unacknowledged motives or personal conflicts between individuals in management. It helps organizations efficiently appraise candidates for a management team position.\n\nManagement due diligence gives an organization a basis for expectations for team and individual performance. Accordingly, the organization can determine whether managers need training.\n\nSince management due diligence lies in the financial analysis of a due diligence report, It shares the same process as creating a due diligence report with few variations. \n\nPreparation is key to an effective management due diligence process. In this phase, organizations gain sufficient knowledge about other organizations. This helps them decide on communication methods between them and other entities, in addition to putting resources in place to promote a successful process. After settling those issues, the organization must:\n\n\"Form a team for the analysis process\" from skilled people with enough experience. After forming the team, the organization assigns responsibilities and settles on a process timeline. The organization may fill gaps in expertise by hiring or contracting external people.\n\n\"Involve managers\" as early as possible as they must get to know the other organization's management team. Early introductions help managers deal with later obstacles.\n\n\"Create checklists\" tailored to particular risks associated with the other organization.\n\n\"Prepare a list of data requests\" for the information an organization needs to complete the process. Such data could include the business plan's management team section, or management organizational structure.\n\n\"After negotiations, have both parties sign a confidentiality report\" to protect sensitive data from third parties. Agree on a method to store all confidential data—for example, an online data repository that both parties can view.\n\nIn this phase the organization begins analyzing gathered data. The team tries to confirm the target's representations and \"soft\" aspects of the target, such as its corporate culture. The team must make sure the other organization fits with its own after assessing their management quality. After gathering all the information, the team advises on whether their organization should continue to work with the other organization.\n\nAfter the team finishes analyzing the management team, they submit a report to the final decision makers. If the team exposes irregularities or unexpected risks, the organization can bid on contract changes. If everything passes the assessment, team members switch to integration planning.\n\nBy performing management due diligence to assess the individuals working in an organization, different aspects must be appraised. The diagram outlines the main aspects that must be evaluated. The four circles in the middle represent the basic qualities that are considered essential for an individual assessment. The bigger circle \"Role\" represents the duties of the individuals in a certain organization. These duties usually vary from one individual to another depending on the job description of that individual. The biggest circle representing the employing organization is located in a market, making it easy to get affected by various external factors. These external factors are capable of hindering the organization from achieving its strategic goals and long-term objectives, making this a challenge that has to be dealt with.\n\nFor the individual assessment to be precise and accurate, it must be done after the organization's requirements have been highlighted and responsibilities of each individual are clear. Having such knowledge helps the business organization overcome future challenges and move closer to strategic objectives.\n\nManagement due diligence needs:\nThe management team is an asset to any organisation,\n\nTo assure reliable data, both investor and individuals under assessment must be involved in the feedback process. This can be costly and time-consuming to both parties. Since due diligence can be a detective game, organizations must find individuals who can detect small issues and opportunities. Organizations sometimes bring in outside experts.\n\nThe expense of the due diligence process, and the time involved, can be softened by dividing it into two stages. \nExecutives may be so interested in a deal that they ignore identified risks and move ahead—and later suffer from management issues. \nInformation gathering can involve interviewing the management team, but the team may see them as expensive and time-consuming. People doing the analysis might not be familiar with the organization's sector, which can lead to wrong conclusions.\n"}
{"id": "6903579", "url": "https://en.wikipedia.org/wiki?curid=6903579", "title": "Performance supervision system", "text": "Performance supervision system\n\nA performance supervision system (PSS) is a software system used to improve the performance of a process plant. Typical process plants include oil refineries, paper mills, and chemical plants.\n\nThe PSS gathers real-time data from the process control system, typically a distributed control system. Using this data, the PSS can calculate performance metrics for process equipment, controls, and operations.\n"}
{"id": "8375628", "url": "https://en.wikipedia.org/wiki?curid=8375628", "title": "Philadelphia Fed Report", "text": "Philadelphia Fed Report\n\nThe Philadelphia Fed Report, formally known as the Business Outlook Survey and sometimes abbreviated as BOS, is a monthly survey produced by the Federal Reserve Bank of Philadelphia (informally known as the Philadelphia Fed) which questions manufacturers on general business conditions. The index covers the Third Federal Reserve District (the jurisdiction of the Philadelphia Fed), namely covers eastern and central Pennsylvania, the nine southern counties of New Jersey, and Delaware. The report is sometimes also called the Philadelphia Fed Index because it includes reporting of some index values.\n\nThe Philadelphia Fed Report has been published monthly since May 1968, and archives for all historical data (revised to account for seasonal adjustments) are available on the website.\n\nThe Philadelphia Index is conducted monthly by the Federal Reserve Bank of Philadelphia and questions voluntary participants on things such as unemployment, new orders, shipments, inventories, and prices paid. The report is released on the third Thursday of every month, making it the earliest such regional report which is released to investors. For instance, the report for April 2014 was released on Thursday, April 17, 2014.\n\nThe survey is a one-page survey asking respondents to include their name, title, company name, and mailing address, and then seeking their opinion on how they think various indicators related to business changed over the past month, as well as how they anticipate it changing over the next 6 months. For each question, they may select between the options of \"decrease\", \"no change\", and \"increase\". The questions include:\n\n\nThe percentage of respondents selecting particular responses can be accessed, and charted over time to compare responses in different months.\n\nHigher survey figures suggest higher production, which contribute to economic growth. Results are calculated as the difference between percentage scores with zero acting as the centerline point. As such, values greater than zero indicate growth, while values less than zero indicate contraction.\n\nSome months have additional special questions included in the survey. For instance, the month of April 2014 asked the following questions:\n\n\nfor the following list of factors:\n\n\nThe Philadelphia Fed Report has been the subject of some academic literature on economic indicators as well as the relationship between business uncertainty and economic activity.\n\nThe indices reported in the Philadelphia Fed Report are widely cited and used in the financial press.\n\nThe monthly reports are also the subject of many articles in the financial press and blogs, such as \"Business Insider\", Yahoo! Finance, RTTNews, and Advisor Perspectives.\n\n"}
{"id": "56676228", "url": "https://en.wikipedia.org/wiki?curid=56676228", "title": "Playbour", "text": "Playbour\n\nPlaybour (sometimes spelled playbor) is a term used to describe a hybrid form of play and labour, specifically in the digital games industry. The term was coined by Julian Kücklich in 2005 in his article \"Precarious playbour: Modders and the digital games industry.\" Kücklich describes playbour as a type of free labour that fits neither traditional definitions of work nor the categories of play or leisure. Nevertheless, forms of playbour are often incorrectly perceived as being just an extension of play. Playbour is also sometimes categorized as a part of the gamification of society in general.\n\nWhile the term playbour itself is a relatively new invention, the phenomena it concerns – those of productive leisure and free labour in the digital games industry – have been around at least since the late 1990s. Kücklich states that since the early 1990s, the relationship between the players of digital games and the digital games industry has changed substantially. He attributes the alteration to the rise of computer game modification, or \"modding\", as a widespread practice among players.\n\nCreating mods for digital games is the perfect example of playbour: a growing number of consumers of digital games are not satisfied with just playing the game, but prefer to enhance their playing experience by creating content for the game themselves. In modding we can see the very essence of playbour, or free labour: while the work of creating a modification can be seen as playful and enjoyable, it is still work. In this respect modding can be fairly similar to another form of collaborative digital production; open-source software development. The first game to gain a modder base of considerable size is widely agreed to be Doom (1993). The emergence of Doom mods is usually attributed to the fact that the game's code was deliberately designed to facilitate player-created content. The most famous modder creation is probably Counter-Strike, originally a team-based mod for Half-Life (1998). It later also became the first commercially released mod.\n\nNot only is modding a big part of gaming culture, but it is also an increasingly important source of value for the digital games industry. Game companies usually retain the intellectual property rights of the modifications. Since players need to have a copy of the original game to run the modifications, mods may add to the shelf-life of the product. In addition to that, mods may help with establishing brands (e.g. Counter-Strike), thus saving companies a significant amount of money. Marketing costs often take up a large percentage of a game's budget, but successfully established brands require less marketing. Mods also increase customer loyalty and are an important source of innovation in the digital games industry. Another way that game companies can benefit from the modding culture is that they can recruit modders as employees who are already trained at no cost to the company.\n\nWhile game companies may benefit economically from playbour, the players doing the work are not entirely uncompensated: they may obtain virtual in-game rewards or social capital such as followers on social media. The experience of play also has value in itself. Playbour is a voluntary activity and players don't usually view modding as labour. Nevertheless, it has been suggested that playboures are exploited by the games industry.\n\n"}
{"id": "253149", "url": "https://en.wikipedia.org/wiki?curid=253149", "title": "Porter's five forces analysis", "text": "Porter's five forces analysis\n\nPorter's Five Forces Framework is a tool for analyzing competition of a business. It draws from industrial organization (IO) economics to derive five forces that determine the competitive intensity and, therefore, the attractiveness (or lack of it) of an industry in terms of its profitability. An \"unattractive\" industry is one in which the effect of these five forces reduces overall profitability. The most unattractive industry would be one approaching \"pure competition\", in which available profits for all firms are driven to normal profit levels. The five-forces perspective is associated with its originator, Michael E. Porter of Harvard University. This framework was first published in \"Harvard Business Review\" in 1979.\n\nPorter refers to these forces as the microenvironment, to contrast it with the more general term macroenvironment. They consist of those forces close to a company that affect its ability to serve its customers and make a profit. A change in any of the forces normally requires a business unit to re-assess the marketplace given the overall change in industry information. The overall industry attractiveness does not imply that every firm in the industry will return the same profitability. Firms are able to apply their core competencies, business model or network to achieve a profit above the industry average. A clear example of this is the airline industry. As an industry, profitability is low because the industry's underlying structure of high fixed costs and low variable costs afford enormous latitude in the price of airline travel. Airlines tend to compete on cost, and that drives down the profitability of individual carriers as well as the industry itself because it simplifies the decision by a customer to buy or not buy a ticket. A few carriers--Richard Branson's Virgin Atlantic is one--have tried, with limited success, to use sources of differentiation in order to increase profitability.\n\nPorter's five forces include three forces from 'horizontal' competition--the threat of substitute products or services, the threat of established rivals, and the threat of new entrants--and two others from 'vertical' competition--the bargaining power of suppliers and the bargaining power of customers.\n\nPorter developed his five forces framework in reaction to the then-popular SWOT analysis, which he found both lacking in rigor and \"ad hoc\". Porter's five-forces framework is based on the structure–conduct–performance paradigm in industrial organizational economics. It has been applied to try to address a diverse range of problems, from helping businesses become more profitable to helping governments stabilize industries. Other Porter strategy tools include the value chain and generic competitive strategies.\n\nProfitable industries that yield high returns will attract new firms. New entrants eventually will decrease profitability for other firms in the industry. Unless the entry of new firms can be made more difficult by incumbents, abnormal profitability will fall towards zero (perfect competition), which is the minimum level of profitability required to keep an industry in business.\n\nThe following factors can have an effect on how much of a threat new entrants may pose:\n\nA substitute product uses a different technology to try to solve the same economic need. Examples of substitutes are meat, poultry, and fish; landlines and cellular telephones; airlines, automobiles, trains, and ships; beer and wine; and so on. For example, tap water is a substitute for Coke, but Pepsi is a product that uses the same technology (albeit different ingredients) to compete head-to-head with Coke, so it is not a substitute. Increased marketing for drinking tap water might \"shrink the pie\" for both Coke and Pepsi, whereas increased Pepsi advertising would likely \"grow the pie\" (increase consumption of all soft drinks), while giving Pepsi a larger market share at Coke's expense. \n\nPotential factors:\n\nThe bargaining power of customers is also described as the market of outputs: the ability of customers to put the firm under pressure, which also affects the customer's sensitivity to price changes. Firms can take measures to reduce buyer power, such as implementing a loyalty program. Buyers' power is high if buyers have many alternatives. It is low if they have few choices. \n\nPotential factors:\n\nThe bargaining power of suppliers is also described as the market of inputs. Suppliers of raw materials, components, labor, and services (such as expertise) to the firm can be a source of power over the firm when there are few substitutes. If you are making biscuits and there is only one person who sells flour, you have no alternative but to buy it from them. Suppliers may refuse to work with the firm or charge excessively high prices for unique resources.\n\nPotential factors are:\n\nFor most industries the intensity of competitive rivalry is the major determinant of the competitiveness of the industry. Having an understanding of industry rivals is vital to successfully market a product. Positioning pertains to how the public perceives a product and distinguishes it from competitors. A business must be aware of its competitors marketing strategy and pricing and also be reactive to any changes made. \n\nPotential factors:\n\nStrategy consultants occasionally use Porter's five forces framework when making a qualitative evaluation of a firm's strategic position. However, for most consultants, the framework is only a starting point. They might use value chain or another type of analysis in conjunction. Like all general frameworks, an analysis that uses it to the exclusion of specifics about a particular situation is considered naive.\n\nAccording to Porter, the five forces framework should be used at the line-of-business industry level; it is not designed to be used at the industry group or industry sector level. An industry is defined at a lower, more basic level: a market in which similar or closely related products and/or services are sold to buyers. (See industry information.) A firm that competes in a single industry should develop, at a minimum, one five forces analysis for its industry. Porter makes clear that for diversified companies, the primary issue in corporate strategy is the selection of industries (lines of business) in which the company will compete. The average \"Fortune Global 1,000\" company competes in 52 industries .\n\nPorter's framework has been challenged by other academics and strategists. For instance, Kevin P. Coyne and Somu Subramaniam claim that three dubious assumptions underlie the five forces:\n\nAn important extension to Porter's work came from Adam Brandenburger and Barry Nalebuff of Yale School of Management in the mid-1990s. Using game theory, they added the concept of complementors (also called \"the 6th force\") to try to explain the reasoning behind strategic alliances. Complementors are known as the impact of related products and services already in the market. The idea that complementors are the sixth force has often been credited to Andrew Grove, former CEO of Intel Corporation. Martyn Richard Jones, while consulting at Groupe Bull, developed an augmented five forces model in Scotland in 1993. It is based on Porter's Framework and includes Government (national and regional) as well as pressure groups as the notional 6th force. This model was the result of work carried out as part of Groupe Bull's Knowledge Asset Management Organisation initiative.\n\nPorter indirectly rebutted the assertions of other forces, by referring to innovation, government, and complementary products and services as \"factors\" that affect the five forces.\n\nIt is also perhaps not feasible to evaluate the attractiveness of an industry independently of the resources that a firm brings to that industry. It is thus argued (Wernerfelt 1984) that this theory be combined with the resource-based view (RBV) in order for the firm to develop a sounder framework.\n\n\n"}
{"id": "10812093", "url": "https://en.wikipedia.org/wiki?curid=10812093", "title": "Public consultation", "text": "Public consultation\n\nPublic consultation, or simply consultation, is a regulatory process by which the public's input on matters affecting them is sought. Its main goals are in improving the efficiency, transparency and public involvement in large-scale projects or laws and policies. It usually involves \"notification\" (to publicise the matter to be consulted on), \"consultation\" (a two-way flow of information and opinion exchange) as well as \"participation\" (involving interest groups in the drafting of policy or legislation). A frequently used tool for understanding different levels of community participation in consultation is known as Arnstein's ladder.\n\nThe process is typical of Commonwealth countries such as the United Kingdom, Canada, New Zealand or Australia, though most democratic countries have similar systems. In the United States, for example, this process is called \"public notice and comment\" (see Rulemaking). Some organisations such as the OECD also use such processes. In Canada, the word \"consultation\" has a special meaning among some First Nations Groups: \"it is the duty of the Crown and third parties to consult with First Nations who have asserted, but not proved, aboriginal rights or title.\"\n\nThere is great variation of public consultations. In some countries there is a list of all consultations, or consultations are mentioned in normal news feed. Depending on the country there can be national or regional public consultations.\n\nIneffective consultations are considered to be cosmetic consultations that were done due to obligation or show and not true participatory decision making.\n\n\nAustralian consultations\n\nCanadian consultations\n\nNew Zealand consultations\nUnited Kingdom consultations\n\nUSA / Federal level\n\nEuropean Commission / European Union level\n"}
{"id": "31850212", "url": "https://en.wikipedia.org/wiki?curid=31850212", "title": "Small and Medium Enterprises Lending in Afghanistan", "text": "Small and Medium Enterprises Lending in Afghanistan\n\nSmall and Medium Enterprises in Afghanistan employ 10 to 500 employees, have sales up to US$1 million and paid-in capital of up to US$1 million. Lenders are banks, financing companies, some MFIs, local money exchange service providers (Hawala dealers), credit unions and societies. Most of these loans are disbursed against collateral of title deeds, land deeds, property and automobile deeds. Some small loans are secured by neighbors, personal guarantees and character.\n\nThe SME loans that are disbursed by these lenders range from US$2,000-250,000.\n\nThe majority of SMEs operate in cities such as Kabul, Mazar-e-Sharif, Herat, Kandahar, Ghazni, Jalalabad, Kunduz, Faizabad, Maimana, JawzJan and Taloqan.\n\nInterest rates are marked from 12% up to 24% depending on mutual deals, loan amounts and credit history. Most formal lenders charge 0.5% to 2.0% of the loan amount as an up-front processing fee.\n"}
{"id": "10916051", "url": "https://en.wikipedia.org/wiki?curid=10916051", "title": "Subordination (finance)", "text": "Subordination (finance)\n\nSubordination in banking and finance refers to the order of priorities in claims for ownership or interest in various assets.\n\nSubordination is the process by which a creditor is placed in a lower priority for the collection of its debt from its debtor's assets than the priority the creditor previously had, In common parlance, the debt is said to be subordinated but in reality, it is the right of the creditor to collect the debt that has been reduced in priority. The priority of right to collect the debt is important when a debtor owes more than one creditor but has assets of insufficient value to pay them all in full at the time of a default. Except in bankruptcy proceedings, the creditor with the first priority for collection will generally have the first claim on the debtor's assets for its debt and the creditors whose rights are subordinate will thus have fewer assets to satisfy their claims. Subordination can take place by operation of law or by agreement among the creditors.\n\nSubordination is also an issue in the priority of security interests in the ownership of property. For example, in real estate, mortgages and other liens on the title to secure the payment or repayment of money usually take their priority from the time they attach to the title. The purpose of this ordering of priority is to determine, in a foreclosure resulting from a default, who gets paid first with the sale proceeds from the foreclosure proceeding. Earlier mortgages or other liens are often subordinated by their holders to later ones in order to accomplish agreed-upon ends. An example is for the holder of a mortgage on undeveloped land to subordinate that mortgage to a later construction loan mortgage. The motivation is either the belief that improvement of the land will benefit the first lender or that the first mortgage requires that it be subordinated to a future construction loan.\n\nThe \"subordination percentage\" of a security is the percentage of the total capital which is subordinate to the security in question. Thus, the security will not suffer any losses until after that percentage of capital has been lost.\n\nIn the automotive financing industry, many dealerships are allowed to designate personal loans which are payable to the ownership as subordinable debt. The finance institution and dealership may come to an agreement which allows this debt to stay within the confines of the financial statement and concurrently improve the dealership financial position from a liquidity perspective.\n\nBankruptcy courts in the United States have the power and authority to subordinate prior claims on the assets of a bankrupt debtor to the claims of junior claimants based on principles of equity. This is a remedy called \"equitable subordination.\" The basis for subordination is usually the inequitable conduct of the prior claimant with respect to junior claimants. Equitable subordination can be used to subordinate both secured and unsecured claims.\n\n"}
{"id": "37284984", "url": "https://en.wikipedia.org/wiki?curid=37284984", "title": "Target culture", "text": "Target culture\n\nTarget culture is a pejorative term used to refer to the perceived negative effects of rigid adherence to performance targets by businesses and organisations. The term is primarily used to refer to this kind of behaviour within the provision of public services in the United Kingdom. Target culture often stems from not being able to accurately measure a broad social good like health, education or crime prevention: instead, specific target like increasing the number of people passing an examination or the number of arrests made by a police force is used.\n\nSchool league tables and other education statistics are often criticised as an example of target culture. The use of number of GCSE examinations passed at grade C as a measure of educational attainment has led schools to focus specifically on getting students on the boundary between grades C and D to improve enough to get a C.\n\nIn 2007, the Police Federation of England and Wales criticised the bureaucratisation of policing and argued that the use of targets increased the number of arrests made for petty offences.\n\nThe failure and investigation of Stafford Hospital has been blamed on target culture by some. The Guardian report in an interview with an anonymous senior NHS doctor that Stafford Hospital \"was a graphic illustration of the growing disconnect we see every day between a target-driven culture and the best interests of our patients\".\n\nIn 2008, the British Conservative Party politician Liam Fox, writing for the website ConservativeHome, lists Bristol Eye Centre as an example of the negative effects of a target culture: by trying to meet government targets to recruit more patients, they failed to make follow-up appointments with existing patients leading to some patients losing some or all of their eyesight.\n"}
{"id": "8481045", "url": "https://en.wikipedia.org/wiki?curid=8481045", "title": "The World Challenge", "text": "The World Challenge\n\nThe World Challenge is a competition that was first hosted in 2005 by BBC and sponsored by Newsweek and Shell. It accepts projects from around the world that deal with social, environmental and community issues and uses business methods to try to improve upon those issues in the world.\n\n\n\n\n\n\n"}
{"id": "46413010", "url": "https://en.wikipedia.org/wiki?curid=46413010", "title": "Toronto Global Forum", "text": "Toronto Global Forum\n\nThe Toronto Global Forum (TGF) is an annual economic event organized by the International Economic Forum of the Americas since 2007.\n\nThe Toronto Global Forum is a not-for-profit organization presenting annual conferences on global economic issues. The TGF mission is to promote open debate and dialogues on national and international issues. Each year, the event welcomes heads of state, Fortune 500 CEOs, international organizations, central bank governors, civil society, etc. to foster free discussion between world leaders. The forum brings together more than 2,800 people from across the globe every year.\n\nThe forum also provide a platform to facilitate meetings between key world leaders in order to encourage global discourse.\n\nThe International Economic Forum of the Americas hosts two other annual events: the Conference of Montreal (founded in 1995), as well as the World Strategic Forum, held in Miami (founded in 2011).\n\n\nCurrent members of the advisory board of the Toronto Global Forum are:\n\n\n\n\n"}
{"id": "372478", "url": "https://en.wikipedia.org/wiki?curid=372478", "title": "Video game industry", "text": "Video game industry\n\nThe video game industry is the economic sector involved in the development, marketing, and monetization of video games. It encompasses dozens of job disciplines and its component parts employ thousands of people worldwide.\n\nThe computer and video game industry has grown from focused markets to mainstream. They took in about US$9.5 billion in the US in 2007, 11.7 billion in 2008, and 25.1 billion in 2010 (ESA annual report).\n\nModern personal computers owe many advancements and innovations to the game industry: sound cards, graphics cards and 3D graphic accelerators, faster CPUs, and dedicated co-processors like PhysX are a few of the more notable improvements.\n\nSound cards were developed for addition of digital-quality sound to games and only later improved for music and audiophiles. Early on, graphics cards were developed for more colors. Later, graphic cards were developed for graphical user interfaces (GUIs) and games; GUIs drove the need for high resolution, and games began using 3D acceleration. They also are one of the only pieces of hardware to allow multiple hookups (such as with SLI or CrossFire graphics cards). CD- and DVD-ROMs were developed for mass distribution of media in general; however the ability to store more information on cheap easily distributable media was instrumental in driving their ever-higher speeds.\n\nBen Sawyer of Digitalmill observes that the game industry value chain is made up of six connected and distinctive layers:\n\nThe game industry employs those experienced in other traditional businesses, but some have experience tailored to the game industry. Some of the disciplines specific to the game industry include: game programmer, game designer, level designer, game producer, game artist and game tester. Most of these professionals are employed by video game developers or video game publishers. However, many hobbyists also produce computer games and sell them commercially. Game developers and publishers sometimes employ those with extensive or long-term experience within the modding communities.\n\nPrior to the 1970s, there was no significant commercial aspect of the video game industry, but many advances in computing would set the stage for the birth of the industry.\n\nMany early publicly-available interactive computer-based game machines used or other mechanisms to mimic a display; while technically not \"video games\", they had elements of interactivity between the player and the machine. Some examples of these included the 1940 \"Nimatron\", an electromagentic relay-based Nim-playing device designed by Edward Condon and built by Westinghouse Electric for the New York World's Fair, \"Bertie the Brain\", an arcade game of tic-tac-toe, built by Josef Kates for the 1950 Canadian National Exhibition, and Nimrod created by engineering firm Ferranti for the 1951 Festival of Britain,\n\nThe development of cathode ray tube—the core technology behind televisions—created several of the first true video games. In 1947 Thomas T. Goldsmith Jr. and Estle Ray Mann filed a patent for a \"cathode ray tube amusement device\". Their game, which uses a cathode ray tube hooked to an oscilloscope display, challenges players to fire a gun at target.\n\nBetween the 1950s and 1960s, with mainframe computers becoming available to campus colleges, students and others started to develop games that could be played at terminals that accessed the mainframe. One of the first known examples is \"Spacewar!\", developed by Harvard and MIT employees Martin Graetz, Steve Russell, and Wayne Wiitanen. The introduction of easy-to-program languages like BASIC for mainframes allowed for more simplistic games to be developed.\n\nIn 1971, the arcade game, \"Computer Space was released.\" The following year, Atari, Inc. released the first commercially successful video game, \"Pong\", the original arcade version of which sold over 19,000 arcade cabinets. That same year saw the introduction of video games to the home market with the release of the early video game console, the Magnavox Odyssey. However, both the arcade and home markets would be dominated by \"Pong\" clones, which flooded the market and led to the video game crash of 1977. The crash eventually came to an end with the success of Taito's \"Space Invaders\", released in 1978, sparking a renaissance for the video game industry and paving the way for the golden age of video arcade games. The game's success inspired arcade machines to become prevalent in mainstream locations such as shopping malls, traditional storefronts, restaurants and convenience stores during the golden age. \"Space Invaders\" would go on to sell over 360,000 arcade cabinets worldwide, and by 1982, generate a revenue of $2 billion in quarters, equivalent to $4.6 billion in 2011.\n\nSoon after, \"Space Invaders\" was licensed for the Atari VCS (later known as Atari 2600), becoming the first \"killer app\" and quadrupling the console's sales. The success of the Atari 2600 in turn revived the home video game market during the second generation of consoles, up until the North American video game crash of 1983. By the end of the 1970s, the personal computer game industry began forming from a hobby culture.\n\nThe early 1980s saw the golden age of video arcade games reach its zenith. The total sales of arcade video game machines in North America increased significantly during this period, from $50 million in 1978 to $900 million by 1981, with the arcade video game industry's revenue in North America tripling to $2.8 billion in 1980. By 1981, the arcade video game industry was generating an annual revenue of $5 billion in North America, equivalent to $12.3 billion in 2011. In 1982, the arcade video game industry reached its peak, generating $8 billion in quarters, equivalent to over $18.5 billion in 2011, surpassing the annual gross revenue of both pop music ($4 billion) and Hollywood films ($3 billion) combined at that time. This was also nearly twice as much revenue as the $3.8 billion generated by the home video game industry that same year; both the arcade and home markets combined add up to a total revenue of $11.8 billion for the video game industry in 1982, equivalent to over $27.3 billion in 2011. The arcade video game industry would continue to generate an annual revenue of $5 billion in quarters through to 1985. The most successful game of this era was Namco's \"Pac-Man\", released in 1980, which would go on to sell over 350,000 cabinets, and within a year, generate a revenue of more than $1 billion in quarters; in total, \"Pac-Man\" is estimated to have grossed over 10 billion quarters ($2.5 billion) during the 20th century, equivalent to over $3.4 billion in 2011.\n\nThe early part of the decade saw the rise of 8-bit home computing, and home-made games, especially in Europe (with the ZX Spectrum and Commodore 64) and Asia (with the NEC PC-88 and MSX). This time also saw the rise of video game journalism, which was later expanded to include covermounted cassettes and CDs.\nIn 1983, the North American industry crashed due to the production of too many badly developed games (quantity over quality), resulting in the fall of the North American industry. The industry would eventually be revitalized by the release of the Nintendo Entertainment System, which resulted in the home console market being dominated by Japanese companies such as Nintendo, while a professional European computer game industry also began taking shape with companies such as Ocean Software and Gremlin Interactive. The latter part of the decade saw the rise of the Game Boy handheld system. In 1987, Nintendo lost a legal challenge against Blockbuster Entertainment, which enabled games rentals in the same way as movies.\n\nThe 1990s saw advancements in game related technology. Among the significant advancements were:\n\nAside from technology, in the early part of the decade, licensed games became more popular, as did video game sequels.\n\nThe video game industry generated worldwide sales of $19.8 billion in 1993 (equivalent to $31 billion in 2011), $20.8 billion in 1994 (equivalent to $32 billion in 2011), and an estimated $30 billion in 1998 (equivalent to $41.5 billion in 2011). In the United States alone, in 1994, arcades were generating $7 billion in quarters (equivalent to $11 billion in 2011) while home console game sales were generating revenues of $6 billion (equivalent to $9 billion in 2011). Combined, this was nearly two and a half times the $5 billion revenue generated by movies in the United States at the time.\n\nIn 2000s, the video game industry is a juggernaut of development; profit still drives technological advancement which is then used by other industry sectors. Technologies such as Smartphones, virtual reality and augmented reality are major drivers for game hardware and gameplay development. Though maturing, the video game industry was still very volatile, with third-party video game developers quickly cropping up, and just as quickly, going out of business. Nevertheless, many casual games and indie games were developed and become popular and successful, such as \"Braid\" and \"Limbo\". Game development for mobile phones (such as iOS and Android devices) and social networking sites emerged. For example, a Facebook game developer, Zynga, has raised in excess of $300 million.\n\nThough not the main driving force, indie games continue to have a significant impact on the industry, with sales of some of these titles such as \"Spelunky\", \"Fez\", \"Don't Starve\", \"Castle Crashers\", and \"Minecraft\", exceeding millions of dollars and over a million users. The 2010s have seen a larger shift to casual and mobile gaming; in 2016, the mobile gaming market is estimated to have taken $38 billion in revenues, compared to $6 billion for the console market and $33 billion for personal computing gaming. Games centered on virtual reality and augmented reality equipment also arose during this decade. As of 2014, newer game companies arose that vertically integrate live operations and publishing such as crowdfunding and other direct-to-consumer efforts, rather than relying on a traditional publishers, and some of these have grown to substantial size. Spurred by some initial events in the late 2000s, eSports centered around professional players in organized competitions and leagues for prize money, grew greatly over this decade, drawing hundreds of millions of viewers and reaching nearly $500 million in revenue by 2016 and expected to break $1 billion by 2019.\n\nEarly on, development costs were minimal, and video games could be quite profitable. Games developed by a single programmer, or by a small team of programmers and artists, could sell hundreds of thousands of copies each. Many of these games only took a few months to create, so developers could release multiple titles per year. Thus, publishers could often be generous with benefits, such as royalties on the games sold. Many early game publishers started from this economic climate, such as Origin Systems, Sierra Entertainment, Capcom, Activision and Electronic Arts.\n\nAs computing and graphics power increased, so too did the size of development teams, as larger staffs were needed to address the ever-increasing technical and design complexities. The larger teams consist of programmers, artists, game designers, and producers. Their salaries can range anywhere from $50,000 to $120,000 generating large labor costs for firms producing videogames which can often take between one and three years to develop. Now budgets typically reach millions of dollars despite the growing popularity of middleware and pre-built game engines. In addition to growing development costs, marketing budgets have grown dramatically, sometimes consisting of two to three times of the cost of development.\n\nThe game development team has to select a profitable and suitable method to sell or earn money from the finished game. Traditionally, the game monetization method is to sell hard copies in retail store. Now some developers are turning to alternative production and distribution methods, such as online distribution, to reduce costs and increase revenue.\n\nToday, the video game industry has a major impact on the economy through the sales of major systems and games such as \"\", which took in over $650 USD million of sales in the game's first five days and which set a five-day global record for a movie, book or videogame. The game's income was more than the opening weekend of \"Spider-Man 3\" and the previous title holder for a video game \"Halo 3\". Many individuals have also benefited from the economic success of video games including the former chairman of Nintendo and Japan's third richest man: Hiroshi Yamauchi. Today the global video game market is valued at over $93 billion.\n\nThe industry wide adoption of high-definition graphics during the seventh generation of consoles greatly increased development teams' sizes and reduced the number of high-budget, high-quality titles under development. In 2013 Richard Hilleman of Electronic Arts estimated that only 25 developers were working on such titles for the eighth console generation, compared to 125 at the same point in the seventh generation-console cycle seven or eight years earlier.\n\nThe games industry's shift from brick and mortar retail to digital downloads led to a severe sales decline at video game retailers such as GameStop, following other media retailers superseded by Internet delivery, such as Blockbuster, Tower Records, and Virgin Megastores. GameStop diversified its services by purchasing chains that repair wireless devices and expanding its trade-in program through which customers trade used games for credit towards new games. The company began to produce its own merchandise and games. In Britain, the games retailer Game revamped its stores so customers would spend time playing games there. It built a gaming arena for events and tournaments. The shift to digital marketplaces, especially for smartphones, led to an influx of inexpensive and disposable titles, as well as lower engagement among gamers who otherwise purchased new games from retail. Customers also shifted away from the tradition of buying games on their first day of release.\n\nPublishers often funded trade-in deals to encourage consumers to purchase new games. Trade-in customers at the Australia retailer Game would purchase twice the games per year as non-trade-in customers. The sale of pre-owned games kept retailers in business, and composed about a third of Game's revenue. Retailers also saved on the UK's value-added tax, which only taxed the retailer's profit on pre-owned games, rather than the full sale on regular games. The former trade-in retail executives behind the trade-in price comparison site Trade In Detectives estimated that the United Kingdom's trade-in industry was about a third of the size of its new games business. They figured that sites such as eBay, which convert used games into cash, compose about a quarter of the UK's trade-in market, but do not keep the credit within the industry. While consumers might appear to receive better offers on these sites, they also take about 15 percent of the selling price in fees. Alternatively, some retailers will match the trade-in values offered by their competitors. Microsoft's original plan for the Xbox One attempted to translate trade-in deals for the digital marketplace, with a database of product licenses that shops would be able to resell with publisher permission, though the plan was poorly received or poorly sold.\n\nVideo game industry practices are similar to those of other entertainment industries (e.g., the music recording industry), but the video game industry in particular has been accused of treating its development talent poorly. This promotes independent development, as developers leave to form new companies and projects. In some notable cases, these new companies grow large and impersonal, having adopted the business practices of their forebears, and ultimately perpetuate the cycle.\n\nHowever, unlike the music industry, where modern technology has allowed a fully professional product to be created extremely inexpensively by an independent musician, modern games require increasing amounts of manpower and equipment. This dynamic makes publishers, who fund the developers, much more important than in the music industry.\n\nIn the video game industry, it is common for developers to leave their current studio and start their own. A particularly famous case is the \"original\" independent developer Activision, founded by former Atari developers. Activision grew to become the world's second largest game publisher. In the mean time, many of the original developers left to work on other projects. For example, founder Alan Miller left Activision to start another video game development company, Accolade (now Atari née Infogrames).\n\nActivision was popular among developers for giving them credit in the packaging and title screens for their games, while Atari disallowed this practice. As the video game industry took off in the mid-1980s, many developers faced the more distressing problem of working with fly-by-night or unscrupulous publishers that would either fold unexpectedly or run off with the game profits.\n\nThe industry claims software piracy to be a big problem, and take measures to counter this. \nDigital rights management have proved to be the most unpopular with gamers, as a measure to counter piracy.\nThe most popular and effective strategy to counter piracy is to change the business model to Freemium, where gamers pay for their in-game needs or service. Strong server-side security is required for this, to properly distinguish authentic transactions from hacked (faked) transactions.\n\nOn various Internet forums, some gamers have expressed disapproval of publishers having creative control since publishers are more apt to follow short-term market trends rather than invest in risky but potentially lucrative ideas. On the other hand, publishers may know better than developers what consumers want. The relationship between video game developers and publishers parallels the relationship between recording artists and record labels in many ways. But unlike the music industry, which has seen flat or declining sales in the early 2000s, the video game industry continues to grow. Also, personal computers have made the independent development of music almost effortless, while the gap between an independent game developer and the product of a fully financed one grows larger.\n\nIn the computer games industry, it is easier to create a startup, resulting in many successful companies. The console games industry is a more closed one, and a game developer must have up to three licenses from the console manufacturer:\n\nIn addition, the developer must usually buy development systems from the console manufacturer in order to even develop a game for consideration, as well as obtain concept approval for the game from the console manufacturer. Therefore, the developer normally has to have a publishing deal in place before starting development on a game project, but in order to secure a publishing deal, the developer must have a track record of console development, something which few startups will have.\n\nAn alternative method for publishing video games is to self-publish using the shareware or open source model over the Internet.\n\nGaming conventions are an important showcase of the industry. The major annual gaming conventions include gamescom in Cologne (Germany), the E3 in Los Angeles (USA), the Penny Arcade Expo, and .\n\nAs with other forms of media, video games have often been released in different world regions at different times. The practice has been used where localization is not done in parallel with the rest of development or where the game must be encoded differently, as in PAL vs. NTSC. It has also been used to provide price discrimination in different markets or to focus limited marketing resources. Developers may also stagger digital releases so as not to overwhelm the servers hosting the game.\n\nInternational video game revenue is estimated to be $81.5B in 2014. This is more than double the revenue of the international film industry in 2013. In 2015, it was estimated at .\n\nThe largest nations by estimated video game revenues in 2016 are China ($24.4B), the United States ($23.5B) and Japan ($12.4B). The largest regions in 2015 were Asia-Pacific ($43.1B), North America ($23.8B), and Western Europe ($15.6B).\n\nGaming conventions are an important showcase of the industry. The annual gamescom in Cologne (Germany) is a major expo for video games. The E3 in Los Angeles (USA) is also of global importance, but is an event for industry insiders only.\n\nOther notable conventions and trade fairs include Tokyo Game Show (Japan), Brasil Game Show (Brazil), EB Games Expo (Australia), KRI (Russia), ChinaJoy (China) and the annual Game Developers Conference. Some publishers, developers and technology producers also have their own regular conventions, with BlizzCon, QuakeCon, Nvision and the X shows being prominent examples.\n\nVideo gaming is still in its infancy throughout the African continent, but due to the continent's young population and increasing technological literacy, the sector is growing rapidly. African countries such as South Africa, Nigeria and Kenya have been making rapid advances in mobile game development, both within their country and internationally, but due to limited funding and a market over-crowded with western games, success has thus far been minimal.\n\nCanada has the third largest video game industry in terms of employment numbers. The video game industry has also been booming in Montreal since 1997, coinciding with the opening of Ubisoft Montreal. Recently, the city has attracted world leading game developers and publishers studios such as Ubisoft, EA, Eidos Interactive, Artificial Mind and Movement, BioWare, Warner Bros. Interactive Entertainment and Strategy First, mainly because video games jobs have been heavily subsidized by the provincial government. Every year, this industry generates billions of dollars and thousands of jobs in the Montreal area. Vancouver has also developed a particularly large cluster of video game developers, the largest of which, Electronic Arts, employs over two thousand people. The \"Assassin's Creed\" series, along with the \"Tom Clancy\" series have all been produced in Canada and have achieved worldwide success. For consumers, the largest video games convention in Canada is the Enthusiast Gaming Live Expo (EGLX).\n\nChina is the largest country by game revenue, and has a gaming public that exceeds the population of the entire United States. It is home to Asia Game Show, the largest game convention in the world by attendance. In 2014, the Xbox One became the first new game console sold since China's ban on consoles in 2000.\n\nGermany has the largest video games market in Europe, with revenues of $4.1 billion forecast for 2017. The annual gamescom in Cologne is Europe's largest gaming expo.\n\nOne of the earliest internationally successful video game companies was Gütersloh-based Rainbow Arts (founded in 1984) who were responsible for publishing the popular \"Turrican\" series of games. The Anno series and The Settlers series are globally popular strategy game franchises since the 1990s. The Gothic series, and Risen are established RPG franchises. The X series by Egosoft is the best-selling space simulation. The FIFA Manager series was also developed in Germany. The German action game (2012) was successful in the markets and received largely positive reviews. One of the most famed titles to come out of Germany is \"Far Cry\" (2004) by Frankfurt-based Crytek, who also produced the topseller \"Crysis\" and its sequels later.\n\nOther well-known current and former developers from Germany include Ascaron, Blue Byte, Deck13, Phenomic, Piranha Bytes, Radon, Related, Spellbound and Yager Development. Publishers include Deep Silver (Koch Media), dtp entertainment, Kalypso and Nintendo Europe. Bigpoint Games, Gameforge, Goodgame Studios and Wooga are among the world's leading browser game and social network game developers/distributors.\n\nThe Japanese video game industry is markedly different from the industry in North America, Europe and Australia.\nJapanese companies have created some of the largest and most lucrative titles ever made, such as the \"Mario\", \"Final Fantasy\", \"Metal Gear\", \"Pokémon\" and \"Resident Evil\" series of games.\nIn recent years, consoles and arcade games have both been overtaken by downloadable free-to-play games on the PC and mobile platforms.\n\nThe UK industry is the third largest in the World in terms of developer success and sales of hardware and software by country alone but fourth behind Canada in terms of people employed. The size of the UK game industry is comparable to its film or music industries. In recent years some of the studios have become defunct or been purchased by larger companies such as \"LittleBigPlanet\" developer, Media Molecule and Codemasters. The country is home to some of the world's most successful video game franchises, such as \"Tomb Raider\", \"Grand Theft Auto\", \"Fable\", \"\" and \"Total War\".\n\nThe country also went without tax relief until March 21, 2012 when the British government changed its mind on tax relief for UK developers, which without, meant most of the talented development within the UK may move overseas for more profit, along with parents of certain video game developers which would pay for having games developed in the UK. The industry trade body TIGA estimates that it will increase the games development sector's contribution to UK GDP by £283 million, generate £172 million in new and protected tax receipts to HM Treasury, and could cost just £96 million over five years. Before the tax relief was introduced there was a fear that the UK games industry could fall behind other leading game industries around the world such as France and Canada, of which Canada overtook the UK in terms of job numbers in the industry in 2010.\n\nThe United States has the largest video games presence in the world in terms of total industry employees. In 2004, the U.S. game industry as a whole was worth US$10.3 billion. U.S. gaming revenue is forecast to reach $23.5 billion in 2017, making it the second largest market behind China.\n\nMagnavox is credited for releasing the first video game console, the Odyssey. Activision was the first developer to create independent/third-party video games. Once the fastest-growing American company, Atari crashed in spectacular fashion, resulting in the North American video game crash of 1983. This resulted in the domination of the Japanese video game industry worldwide throughout the 1980s and 1990s.\n\nThe U.S. is home to major game development companies such as Activision Blizzard (\"Call of Duty\", \"World of Warcraft\"), Electronic Arts (\"FIFA\", \"Battlefield\", \"Mass Effect\"), and Take-Two Interactive (Civilization, \"NBA 2K\" series, \"Grand Theft Auto\"). ZeniMax Media (\"Doom\", \"Fallout\", \"The Elder Scrolls\") is the world's largest privately held video game company. Niantic (\"Ingress\", \"Pokémon Go\") is a notable mobile game developer.\n\nValve Corporation operates Steam, the largest computer gaming platform, with an active user base (~125 million) that rivals the combined user bases of the current console generation (~150 million). While not specifically focused on games, the largest mobile gaming platforms are operated by Google (Google Play), and Apple Inc. (App Store), with the majority of mobile revenue coming from Asia. Microsoft operates Xbox, the only major game console hardware franchise not controlled by a Japanese company. Sony established Sony Interactive Entertainment in Silicon Valley to run PlayStation, the world's largest and longest-running video game console franchise.\n\nIntel and Nvidia are the largest makers of PC graphics chips. Advanced Micro Devices has become the most important console processor vendor, with all three of the eighth generation home consoles using AMD GPUs, and two of them use AMD CPUs. Microsoft, Nintendo, and Sony were not aware that they were all using AMD processors until all their consoles were announced, underscoring the secrecy found within the game industry. Notable game engine developers include Epic Games (Unreal Engine) and Unity Technologies (Unity).\n\nThe West Coast is home to important video game conventions, such as Electronic Entertainment Expo (E3), one of the largest video game industry-only events in the world, and Penny Arcade Expo (PAX West), the largest public video game convention in North America. The West Coast is also home to many of the major American video game industry companies, particularly the regions of Los Angeles, San Francisco Bay Area, and Seattle. Major game development regions outside of the West Coast include the Northeast and Texas.\n\nOver 150 million Americans play video games, with an average age of 35 and a gender breakdown of 59 percent male and 41 percent female. American gamers are more likely to vote than non-gamers, feel that the economy is the most important political issue, and lean conservative.\n\nSeveral important game development personalities were born in or moved to the United States. Notable developers include Ralph H. Baer (Magnavox Odyssey, the \"Father of Video Games\"), Jonathan Blow (\"Braid\"), John D. Carmack (\"Doom\", \"Quake\"), and Alexey Pajitnov (\"Tetris\"). Mario is named after Mario Segale, a former landlord of Nintendo of America. Some right-wing and left-wing activists, including Jack Thompson and Anita Sarkeesian, have met extreme resistance from the gaming public in response to the perceived politicizing of their art form.\n\nThe United States recognizes eSports players as professional athletes. Major League Gaming has eSports arenas and studios across the nation. Robert Morris University has a League of Legends varsity team, whose members are eligible for scholarships.\n\nPlayers become fourth-party developers, allowing for more open source models of game design, development and engineering. Players also create modifications (mods), which in some cases become just as popular as the original game for which they were created. An example of this is the game \"Counter-Strike\", which began as a mod of the video game \"Half-Life\" and eventually became a very successful, published game in its own right.\n\nWhile this \"community of modifiers\" may only add up to approximately 1% of a particular game's user base, the number of those involved will grow as more games offer modifying opportunities (such as, by releasing source code) and the video user base swells. According to Ben Sawyer, as many as 600,000 established online game community developers existed as of 2012. This effectively added a new component to the game industry value chain and if it continues to mature, it will integrate itself into the overall industry.\n\nThe industry has seen a shift towards games with multiplayer facilities. A larger percentage of games on all types of platforms include some type of competitive online multiplayer capability.\n\nIn addition, the industry is experiencing further significant change driven by convergence, with technology and player comfort being the two primary reasons for this wave of industry convergence. Video games and related content can now be accessed and played on a variety of media, including: cable television, dedicated consoles, handheld devices and smartphones, through social networking sites or through an ISP, through a game developer's website, and online through a game console and/or home or office personal computer. In fact, 12% of U.S. households already make regular use of game consoles for accessing video content provided by online services such as Hulu and Netflix. In 2012, for the first time, entertainment usage passed multiplayer game usage on Xbox, meaning that users spent more time with online video and music services and applications than playing multiplayer games. This rapid type of industry convergence has caused the distinction between video game console and personal computers to disappear. A game console with high-speed microprocessors attached to a television set is, for all intents and purposes, a computer and monitor.\n\nAs this distinction has been diminished, players' willingness to play and access content on different platforms has increased. The growing video gamer demographic accounts for this trend, as former president of the Entertainment Software Association Douglas Lowenstein explained at the 10th E3 expo, \"Looking ahead, a child born in 1995, E3's inaugural year, will be 19 years old in 2014. And according to Census Bureau data, by the year 2020, there will be 174 million Americans between the ages of 5 and 44. That's 174 million Americans who will have grown up with PlayStations, Xboxes, and GameCubes from their early childhood and teenage years...What this means is that the average gamer will be both older and, given their lifetime familiarity with playing interactive games, more sophisticated and discriminating about the games they play.\"\n\nEvidence of the increasing player willingness to play video games across a variety of media and different platforms can be seen in the rise of casual gaming on smartphones, tablets, and social networking sites as 92% of all smartphone and tablet owners play games at least once a week, 45% play daily, and industry estimates predict that, by 2016, one-third of all global mobile gaming revenue will come from tablets alone. Apple's App Store alone has more than 90,000 game apps, a growth of 1,400% since it went online. In addition, game revenues for iOS and Android mobile devices now exceed those of both Nintendo and Sony handheld gaming systems combined.\n\n\n"}
{"id": "14252700", "url": "https://en.wikipedia.org/wiki?curid=14252700", "title": "Whitewash waiver", "text": "Whitewash waiver\n\nWhitewash Resolution is a proposed resolution for the waiver of rights of independent shareholders to receive a mandatory takeover from the investor and its concert parties for the ordinary shares of the company not already owned or controlled by them.\n\nA whitewash waiver is a corporate law concept originating in Hong Kong and Singapore. In some cases an investor will apply to the Executive for the Whitewash Waiver, which, if granted, will be subject to the approval of independent shareholders.\n"}
{"id": "2683953", "url": "https://en.wikipedia.org/wiki?curid=2683953", "title": "Work card", "text": "Work card\n\nA work card is like an Identity Card which verifies that a person has been given work, or is eligible to perform work in a given profession or jurisdiction. The work card is not a work visa, although it may be used in conjunction with a work visa, permanent resident card or other documentation.\n\nWork cards are often used in countries with high unemployment to certify that the individual meets certain legal requirements (such as head of household, or with dependent children) making him or her eligible for work.\n\nWork cards are also used in certain industries like construction (where specialized training and safety skills are required) or gambling (where background and credit checks are required to reduce the incidence of crime).\n\nWork cards are used in some employment situations, such as prostitution, so that government officials may track the number of workers in a given industry. Frequent renewal of work cards may also be required to ensure that workers undergo regular health check-ups, or to gather information on working conditions or the incident of crime (such as assault against the prostitute, or a prostitute's criminal background).\n\nWork cards are increasingly used in the European Union (EU) to verify an individual's citizenship in a member-nation, and the kind of work which that individual may engage in. For example, citizens of states with provisional membership in the EU must obtain both an EU work card and a work card from nation in which they wish to work.\n\nIn cases where a union has won the closed shop, a work card may be issued by a trade union. The work card will permit the non-union worker to work in the industry or for the employer with union permission.\n\n\n"}
