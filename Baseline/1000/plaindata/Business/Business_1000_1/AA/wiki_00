{"id": "17434798", "url": "https://en.wikipedia.org/wiki?curid=17434798", "title": "Advisor Group", "text": "Advisor Group\n\nAdvisor Group is an Independent Broker-Dealer as of May 2016 as the result of AIG's sale of Advisor Group to Lightyear Capital LLC, a private equity firm specializing in financial services investing, and PSP Investments. With its headquarters in Phoenix, Arizona, it is one of the largest network of independent broker-dealers in the US, with over 6,000 registered representatives, many of whom are also investment advisors. There are four separate broker-dealers that constitute Advisor Group: SagePoint Financial based in Phoenix, AZ; Royal Alliance in New Jersey; FSC Securities Corporation located in Atlanta, GA; and Woodbury Financial Services located in Oakdale, MN.\n\nThe CEO of Advisor Group is Jamie Price and the chairman is Valerie Brown.\n"}
{"id": "9342783", "url": "https://en.wikipedia.org/wiki?curid=9342783", "title": "Affective labor", "text": "Affective labor\n\nAffective labor is work carried out that is intended to produce or modify emotional experiences in people. Coming out of Autonomist feminist critiques of marginalized and so-called \"invisible\" labor, it has been the focus of critical discussions by, e.g., Antonio Negri, Michael Hardt, Juan Martin Prada, and Michael Betancourt.\n\nAlthough its history is as old as that of labor itself, affective labor has been of increasing importance to modern economies since the emergence of mass culture in the nineteenth century. The most visible institutionalized form of affective labor is perhaps advertising, which typically attempts to make audiences relate to products through particular effects. Yet there are many other areas in which affective labor figures prominently, including service and care industries whose purpose is to make people feel in particular ways. Domestic work, frequently ignored by other analysts of labor, has also been a critical focus of theories of affective labor.\n\nThe phrase affective labor, seen broadly, has its roots in the Autonomist critiques of the 1970s, in particular those that theorize a dynamic form of capitalism that is able to move away from purely industrial labor. In particular, the \"Fragment on Machines,\" from Marx's \"Grundrisse\", and conceptions of immaterial labor decentered the focus of labor theory and sparked debate over what constituted real labor:\n\nNo longer does the worker insert a modified natural thing (Naturgegenstand) as middle link between the object (Objekt) and himself; rather, he inserts the process of nature, transformed into an industrial process, as a means between himself and inorganic nature, mastering it. He steps to the side of the production process instead of being its chief actor. In this transformation, it is neither the direct human labour he himself performs, nor the time during which he works, but rather the appropriation of his own general productive power, his understanding of nature and his mastery over it by virtue of his presence as a social body – it is, in a word, the development of the social individual which appears as the great foundation-stone of production and of wealth.\n\nMeanwhile, movements such as Selma James and Marirosa Dalla Costa's Wages for housework campaign attempted to activate the most exploited and invisible sectors of the economy and challenge the typical, male and industrial focus of labor studies.\n\nAntonio Negri and Michael Hardt have begun to develop this concept in their books \"Empire\" and \"\".\n\nIn their recent work, Hardt and Negri focus on the role affective labor plays in the current mode of production (which can be referred to as \"imperial\", \"late capitalist\", or \"postmodern\"). In this passage from \"Multitude\" they briefly define their key terms:\n\n\"Unlike emotions, which are mental phenomena, affects refer equally to body and mind. In fact, affects, such as joy and sadness, reveal the present state of life in the entire organism, expressing a certain state of the body along with a certain mode of thinking. Affective labor, then, is labor that produces or manipulates affects... One can recognize affective labor, for example, in the work of legal assistants, flight attendants, and fast food workers (service with a smile). One indication of the rising importance of affective labor, at least in the dominant countries, is the tendency for employers to highlight education, attitude, character, and \"prosocial\" behavior as the primary skills employees need. A worker with a good attitude and social skills is another way of saying a worker is adept at affective labor.\"\n\nThe most important point in their scholarship with respect to this issue is that immaterial labor, of which affective labor is a specific form, has achieved dominance in the current mode of production. This does not mean that there are more immaterial laborers than material laborers, or that immaterial labor produces more capital than material labor. Instead, this dominance is signaled by the fact that, in developed countries, labor is more often figured as immaterial than material. To illustrate the significance of this claim, they draw a comparison between the early twenty-first century and that of the mid-nineteenth century, famously engaged by Karl Marx, in which factory labor was dominant even if it was not the form of labor practiced by the most people. One popular, albeit slightly less than perfect example, of this might be that, whereas Fred Flintstone, as an average American, drove a crane in a quarry, Homer Simpson sits at a desk and provides safety.\n\nMichael Betancourt has suggested that affective labor may have a role in the development and maintenance of what he has termed \"agnotologic capitalism\". His point is that affective labor is a symptom of the disassociation between the reality of capitalist economy and the alienation it produces:\n\nThe affective labor created to address this alienation is part of the mechanisms where the agnotological order maintains its grip on the social: managing the emotional states of the consumers, who also serve as the labor reserve, is a necessary precondition for the effective management of the quality and range of information.\n\nHis construction of affective labor is concerned with its role as an enabler for a larger capitalist superstructure, where the reduction of alienation is a precondition for the elimination of dissent. Affective labor is part of a larger activity where the population is distracted by affective pursuits and fantasies of economic advancement.\n\n\n"}
{"id": "28502793", "url": "https://en.wikipedia.org/wiki?curid=28502793", "title": "Alexandria (library software)", "text": "Alexandria (library software)\n\nAlexandria is browser based cross-platform library automation software used by thousands of libraries around the world, both public libraries and school libraries. These include the Houston Independent School District, Philadelphia Public Schools, and the Berkeley Unified School District.\n\nCOMPanion Corporation was founded in 1987 by company president Bill Schjelderup in Salt Lake City, Utah.\n\nCOMPanion Corp maintains different versions simultaneously, with the most recent and up-to-date version being Alexandria v7.15.3 with plans to release Alexandria v7.16.1 in the summer of 2016. Alexandria v7 was officially released on August 5, 2014. Alexandria v6 is still supported although active development has ceased, with the exception of maintenance updates.\n\n\n\n"}
{"id": "376998", "url": "https://en.wikipedia.org/wiki?curid=376998", "title": "American system of manufacturing", "text": "American system of manufacturing\n\nThe American system of manufacturing was a set of manufacturing methods that evolved in the 19th century. The two notable features were the extensive use of interchangeable parts and mechanization for production, which resulted in more efficient use of labor compared to hand methods. The system was also known as armory practice because it was first fully developed in armories, namely, the United States Armories at Springfield in Massachusetts and Harpers Ferry in Virginia (later West Virginia), inside contractors to supply the United States Armed Forces, and various private armories. The name \"American system\" came not from any aspect of the system that is unique to the American national character, but simply from the fact that for a time in the 19th century it was strongly associated with the American companies who first successfully implemented it, and how their methods contrasted (at that time) with those of British and continental European companies. In the 1850s, the \"American system\" was contrasted to the British factory system which had evolved over the previous century. Within a few decades, manufacturing technology had evolved further, and the ideas behind the \"American\" system were in use worldwide. Therefore, in manufacturing today, which is global in the scope of its methods, there is no longer any such distinction.\n\nThe American system involved semi-skilled labor using machine tools and jigs to make standardized, identical, interchangeable parts, manufactured to a tolerance, which could be assembled with a minimum of time and skill, requiring little to no fitting.\n\nSince the parts are interchangeable, it was also possible to separate manufacture from assembly, and repair—an example of the division of labor. This meant that all three functions could be carried out by semi-skilled labor: manufacture in smaller factories up the supply chain, assembly on an assembly line in a main factory, and repair in small specialized shops or in the field. The result is that more things could be made, more cheaply, and with higher quality, and those things also could be distributed further, and lasted longer, because repairs were also easier and cheaper. In the case of each function, the system of interchangeable parts typically involved substituting specialized machinery to replace hand tools.\n\nInterchangeability of parts was finally achieved by combining a number of innovations and improvements in machining operations and machine tools, which were developed primarily for making textile machinery. These innovations included the invention of new machine tools and jigs (in both cases, for guiding the cutting tool), fixtures for holding the work in the proper position, and blocks and gauges to check the accuracy of the finished parts.\n\nEnglish machine tool manufacturer Joseph Whitworth was appointed as a British commissioner for the New York International Exhibition. Accompanied by another British commissioner, he traveled around several states visiting various manufacturers, and as a result published a highly influential report on American manufacturing, from which he is quoted:\n\nThe American system contributed to efficiency gains through division of labor. Division of labor helped manufacturing transition from small artisan's shops to early factories. Key pieces of evidence supporting efficiency gains include increase in firm size, evidence of returns to scale, and an increase in non-specialized labor. The need for firms to train uneducated people to perform only one thing in the productivity chain allowed for the use of non-specialized labor. Women and children were employed more frequently within larger firms, especially those producing furniture and clothing..\n\nIn the late 18th century, French General Jean Baptiste Vaquette de Gribeauval suggested that muskets could be manufactured faster and more economically if they were made from interchangeable parts. This system would also make field repairs easier to carry out under battle conditions. He provided patronage to Honoré Blanc, who attempted to implement the \"Système Gribeauval\", but never succeeded. Until then, under the British factory system, skilled machinists were required to produce parts from a design. But however skilled the machinist, parts were never identical, and each part had to be manufactured separately to fit its counterpart—almost always by one person who produced each completed item from start to finish.\n\nMass production using interchangeable parts was first achieved in 1803 by Marc Isambard Brunel in cooperation with Henry Maudslay, and Simon Goodrich, under the management of (with contributions by) Brigadier-General Sir Samuel Bentham, the Inspector General of Naval Works at Portsmouth Block Mills at Portsmouth Dockyard, for the British Royal Navy during the Napoleonic War. By 1808 annual production had reached 130,000 sailing blocks. This method of working did not catch on in general manufacturing in Britain for many decades, and when it did it was imported from America, becoming known as the \"American System of Manufacturing\", even though it originated in England.\n\nThe Lowell system is also related to the American system during this time. It emphasized procuring, training, and providing housing and other living necessities for the workforce, as well as using semi-automated machines in a centralized factory building or complex.\n\nGribeauval's idea was conveyed to the US by two routes. First, Blanc's friend Thomas Jefferson championed it, sending copies of Blanc's memoirs and papers describing his work to Secretary of War Henry Knox. Second, artillery officer Louis de Tousard (who had served with Lafayette) was an enthusiast of Gribeauval's ideas. Tousard wrote two influential documents after the American Revolution; one was used as the blueprint for West Point, and the other became the officer's training manual.\n\nThe War Department, which included officers trained at West Point on Tousard's manual, established the armories at Springfield and Harper's Ferry and tasked them with solving the problem of interchangeability. The task was finally accomplished in the 1820s. Historian David A. Hounshell believes that this was done by Captain John H. Hall, an inside contractor at Harper's Ferry. In a letter dated 1822 Hall makes the claim he achieved interchangeability in 1822. But historian Diana Muir argues that it is more probable that it was Simeon North, a Connecticut arms contractor manufacturing guns for the US Army. North, not Hall, was the inventor of the crucial milling machine in 1816, and had an advantage over Hall in that he worked closely with the first industry that mass-produced complex machines from mass-produced interchangeable parts, the Connecticut clock-making industry. By 1815 the idea of interchangeability was well established in the US government system of procurement; Congressional contracts stipulated this quality in muskets, rifles and pistols ordered after that date. Interchangeability of firearms parts at the U.S. armories was found to have been in use for a number of years by the time of the 1853 British Parliamentary Commissions Committee on Small Arms inquiry.\n\nA critical factor in making interchangeable metal parts was the invention of several machine tools, such as the slide rest lathe, screw cutting lathe, turret lathe, milling machine and metal planer. One of the most important and versatile of these machine tools was David Wilkinson's lathe, for which he received a $10,000 award from the government of the United States.\n\nEli Whitney is generally credited with the idea and the practical application, but both are incorrect attributions. Based on his reputation as the inventor of the cotton gin, the US government gave him a contract in 1798 for 10,000 muskets to be produced within two years. It actually took eight years to deliver the order, as Whitney perfected and developed new techniques and machines. In a letter to Treasury Secretary Oliver Wolcott apologizing for the delays, Whitney wrote:\nWhitney did use machinery; however, there is no evidence that he produced any new type of metalworking machinery. After completing the initial contract, Whitney went on to produce another 15,000 muskets within the following two years. Whitney never actually expressed any interest in interchangeability until 1800, when Treasury Secretary Wolcott exposed him to the memoirs of Blanc, but he spent far more time and energy promoting the idea than developing it.\n\nIn order to spread knowledge of manufacturing techniques, the War Department made contractors open their shops to other manufacturers and competitors. The armories also openly shared manufacturing techniques with private industry. Additionally, the idea migrated from the armories to industry as machinists trained in the armory system were hired by other manufacturers. Skilled engineers and machinists thus influenced American clockmakers and sewing machine manufacturers Wilcox and Gibbs and Wheeler and Wilson, who used interchangeable parts before 1860. Late to adopt the interchangeable system were Singer Corporation sewing machine (1870s), reaper manufacturer McCormick Harvesting Machine Company (1870s–80s) and several large steam engine manufacturers such as Corliss (mid-1880s) as well as locomotive makers. Large scale of production of bicycles in the 1880s used the interchangeable system.\n\nThe idea would also help lead to the American \"Golden Age\" of manufacturing when Henry Ford mass-produced the automobile. Mastering true interchangeability on the assembly line, the Ford plant produced standard model cars. These efficient production strategies allowed these automobiles to be affordable for the middle class.\n\nThe idea of interchangeable parts and the separate assembly line was not new, though it was little used. The idea was first developed in East Asia during the Warring States period and later the Qin Dynasty over 2200 years ago – bronze crossbow triggers and locking mechanisms were mass-produced and made to be interchangeable. Venice during the late Middle Ages had ships that were produced using pre-manufactured parts, assembly lines, and mass production. The Venetian Arsenal apparently produced nearly one ship every day, in what was effectively the world's first factory.\n\n"}
{"id": "38600379", "url": "https://en.wikipedia.org/wiki?curid=38600379", "title": "Auto transport broker", "text": "Auto transport broker\n\nAn auto transport broker is a type of cargo broker that specializes in the shipping and transportation of vehicles. Most vehicles shipped in the U.S. are cars and trucks, but many brokers handle boats, RVs, motorcycles and other types of vehicles as well. Auto transport is classified as \"specialized freight trucking\" under NAICS code 484230.\n\nAn auto transport broker is part of the personal vehicle freight business industry chain. In the U.S., these broker companies must have proper licensing and authority from the FMCSA to be allowed to broker vehicles for customers. The individual or business that needs to move a car or other vehicle is the shipper; the shipper contacts a broker to have their vehicle shipped or transported. Once a broker is booked, the broker's job is to find a carrier, which is the individual or company that actually employs drivers and operates the car transport equipment.\n\nBrokers are employed because they have access to freight load boards, where they can post the job and find carriers that have transportation resources in the area. They can also get lower shipping prices by getting competing bids by different carriers. However, brokers and carriers are not always separate entities - a number of auto transport companies handle both brokerage and transport.\n\nThe US Department of Transportation keeps statistics on cargo shipments, showing over $651 billion worth of motorized and other vehicles (including parts) moved by truck in 2007. Of that number, $452bn of cargo was moved via for-hire truck. Official statistics about the size of the secondary auto transport market, number of commercial-size car carrier vehicles on the road and number of vehicles shipped aren't kept by the DOT.\n\nWith the advent of the Internet, the auto transport industry has seen an influx of new brokers, attracted by the low cost of starting a brokerage business online. While this encouraged greater competition and lower costs in the industry, government agencies have also seen a \"dramatic increase in complaints against auto transporters and auto transport brokers\" due to Internet fraud.\n\nAuto transport brokers in the USA are subject to government licensing. The candidate must obtain an Operating Authority number from the Federal Motor Carrier Safety Administration by filling out a form on the FMCSA website. There is a small application processing fee. Brokers are also required to obtain a bond. Known as a Freight Broker Bond, it exists to cover losses by the motor carrier, in case the broker commits fraud or other lapses. Prior to 2012, the minimum bond was $10,000, although many brokers chose to obtain higher amounts.\n\nThe Moving Ahead for Progress in the 21st Century Act, signed by President Obama on July 6 of 2012, introduced a number of new regulations for auto transport brokers. The chief among them is raising the minimum broker bond from $10,000 to $75,000. The new provision goes in force on October 1, 2013, and has applied to all existing brokers retroactively.\n\nThe BMC-84 Freight Broker Bond is paid for on an annual basis. The freight broker bond cost is figured as a percentage of the bond amount depending on \nOther rules include:\n\n\nThe Association of Independent Property Brokers & Agents (AIPBA), a property broker trade group that claims 1,400 members, has petitioned and lobbied against higher bond requirements when these have been proposed in the past, and has harshly criticized the new law. The founder & president of AIPBA, James Lamb, has called the law a lobbyist-driven attempt to \"eliminate small brokers from the market\" and establish an oligopoly that charges customers more and pays carriers less.\n\nThe National Association for Minority Truckers (NAfMT) has also come out against the higher bond requirements. NAfMT CEO Kevin Reid called the $75,000 bond an \"unreasonable barrier to entry for would-be entrepreneurs.\" He also spoke out against the new restrictions on owner-operators brokering out excess freight. The NAfMT has joined efforts by AIPBA to repeal the stricter surety requirements.\n\nOther industry associations have been supportive of the law. The Owner-Operator Independent Drivers Association (OOIDA), a group that represents independent trucking owner-operators, has been a key force behind the new regulations. While the final rules in MAP-21 fell short of the OOIDA's wishes, Todd Spencer, executive vice president of the organization, praised them as a \"win-win\" for truckers and legitimate brokers.\n\nThe Transportation Intermediaries Association (TIA), a major third-party logistics trade organization, has also advocated for the new FMCSA regulations through its lobbying arm TIAPAC as a way to protect motor carriers from both incompetent and unscrupulous brokers. TIA board member Ken Lund acknowledged that the new bond may be difficult for smaller brokers to pay, but defended it as \"reasonably priced\" and useful to prevent fraud.\n"}
{"id": "28401929", "url": "https://en.wikipedia.org/wiki?curid=28401929", "title": "Banking BPO services", "text": "Banking BPO services\n\nBanking business process outsourcing or banking BPO is a highly specialized sourcing strategy used by banks and lending institutions to support the business acquisition and account servicing activities associated with the customer lending lifecycle. These specific BPO services are usually offered through multi-year service-level agreements for all or portions of the credit card lending, consumer lending or commercial lending segments of the financial services market. Some larger financial services organizations choose to extend their sourcing strategy to include other outsourced services such as ITO systems and software, human resources outsourcing and benefits services, finance and accounting outsourcing (FAO) services, procurement or training outsourcing.\n\nBanking BPO services are typically defined by industry analysts, advisors and leaders in the sourcing industry, such as the set of discrete processes or transactional activities that support the lending lifecycle as follows:\n\n\n"}
{"id": "11971726", "url": "https://en.wikipedia.org/wiki?curid=11971726", "title": "BatchMaster Software", "text": "BatchMaster Software\n\nBatchMaster Software is a United States-based company that develops Enterprise Resource Planning (ERP) solution for formula/recipe based process manufacturers. BatchMaster Software is a SAP Business One VAR and ISV, as well as a Microsoft Gold Certified Partner.\n\nBatchMaster Software is a global vendor of ERP software for the process manufacturing industry, such as Food & Beverage, Nutraceutical, Chemicals & Coatings, Cosmetics & Personal Care, Pharmaceutical & Life Sciences.\n\nIt has a staff of more than 300 professionals with half of them engaged in Research and Development. The company is headquartered in Laguna Hills, California, USA and has offices in New York, India, New Zealand and Mexico.\n\nBatchMaster was founded by Randy Peck as Pacific Micro Software Engineering and later changed the name to BatchMaster DOS. In 2000, the Company was acquired by eWorkplace Solutions and was reincorporated as BatchMaster Software. The company then started the project to come up with Windows based application software. In 2001, Infocus Solutions Pvt. Ltd. (ISPL) was formed in Indore, India to finish the project. ISPL started its operation with a team of seven people and within four years more than hundred people were working for the organization. The Company formally announced its India operations in 2006 and changed the name to BatchMaster Software Pvt. Ltd.\n\nBatchMaster ERP is the flagship product of the company and offers integration with:\n\n"}
{"id": "1094950", "url": "https://en.wikipedia.org/wiki?curid=1094950", "title": "Burn rate", "text": "Burn rate\n\nBurn rate is the rate at which a company is losing money. It is typically expressed in monthly terms. E.g., \"the company's burn rate is currently $65,000 per month.\" In this sense, the word \"burn\" is a synonymous term for negative cash flow. It is also measure for how fast a company will use up its shareholder capital. If the shareholder capital is exhausted, the company will either have to start making a profit, find additional funding, or close down.\n\nThe term came into common use during the dot-com era when many start-up companies went through several stages of funding before emerging into profitability and positive cash flows and thus becoming self-sustainable (or, as for the majority, failing to find additional funding and sustainable business models and thus going bankrupt). In between funding events, burn rate becomes an important management measure, since together with the available funds, it provides a time measure to when the next funding event needs to take place.\n\nSome entrepreneurs and investors say that part of the reasons behind the dot-com bust was the unsound management and financial investor practices to keep the burn rate up, taking it as a proxy for how fast the start-up company was acquiring a customer base.\n\nThe term burn rate can also refer to how quickly individuals spend their money, particularly their discretionary income. For example, Mackenzie Investments commissioned a test to gauge the spending and saving behavior of Canadians to determine if they are “Overspenders.”\n\nAside from financing, the term burn rate is also used in project management to determine the rate at which hours (allocated to a project) are being used, to identify when work is going out of scope, or when efficiencies are being lost. Simply put, the burn rate of any project is the rate at which the project budget is being burned (spent).\n\nIn earned value management, burn rate is calculated via the formula, 1/CPI, where CPI stands for Cost Performance Index, which is equal to Earned Value / Actual Cost.\n\nThe term burn rate is also used in a number of scientific settings:\n\n"}
{"id": "26551795", "url": "https://en.wikipedia.org/wiki?curid=26551795", "title": "Business and Professional Ethics Journal", "text": "Business and Professional Ethics Journal\n\nBusiness and Professional Ethics Journal is a peer-reviewed academic journal that examines ethical issues in business encountered by professionals working in large organizational structures. It provides an outlet for original work that contributes to the development of alternative theories and practices within business and professional ethics, and that examines why global ethical issues, such as poverty alleviation and sustainability, emerge and persist. The journal is published by the Philosophy Documentation Center in cooperation with the Institute for Business and Professional Ethics at DePaul University. Members of the Society for Ethics Across the Curriculum have online access to this journal as a benefit of membership.\n\n\"Business and Professional Ethics Journal\" is abstracted and indexed by ABI/INFORM, Academic ASAP, Business Periodicals Index, Business Source Premier, MEDLINE, PAIS International, Periodicals Index Online, Philosopher's Index, PhilPapers, ProQuest Research Library, Public Affairs Index, Scopus, Social Sciences Abstracts, TOC Premier, Wilson Business Abstracts, and Wilson OmniFile.\n\nThe journal was established in 1981 by Robert Baum, Norman E. Bowie, and Deborah Johnson. The journal has published special issues in cooperation with professional organizations in several countries, including The Academy of Business in Society (EABIS), Australian Association for Professional and Applied Ethics, Canadian Society for the Study of Practical Ethics, the Markkula Center for Applied Ethics, Middlesex University Business School, and the Society for Business Ethics. In 2004 the \"Business and Professional Ethics Journal\" merged with \"Professional Ethics\". The editors-in-chief are Daryl Koehn (DePaul University) and Bas van der Linden (Radboud University Nijmegen).\n\n"}
{"id": "47827092", "url": "https://en.wikipedia.org/wiki?curid=47827092", "title": "Business guru", "text": "Business guru\n\nA business guru or management guru is a leading authority on business practices and can be defined as 'a person with influential ideas or theories about business'. The earliest use of the term business guru can be tracked back to the 1960s being used in \"Business Week\". There are no existing qualifications that make someone a business guru. The lists of people who have been accepted as business gurus have constantly changed over time. However, there are some people who have been accepted by a great majority as a business guru and also some organizations which have created their own lists of gurus. One English writer has described management gurus as \"overwhelmingly a US phenomenon.\"\n\nThere is no definitive list of business gurus, but some writers have proposed \"personal\" lists. These lists are mostly created by organizations such as business magazines or management writers. There have been many business guru lists created through history.\n\nA list consisting of people who are included in almost all of the lists created, collectively known as the \"Famous Five\", are: Frederick Winslow Taylor, Michael Porter, Alfred Sloan, Peter Drucker, and Douglas McGregor.\n\nIn 2001, \"Harvard Business Review\" asked the gurus to name their favorite gurus. The people named were Peter Drucker, James March and Herbert Simon.\n\nAnother list includes Peter Drucker, Michael Porter, and Tom Peters as the three leading gurus of our time.\n\nThere are also many gurus who have emerged and disappeared through history. For example, the Japanese were known for making improvements to the business world and bringing out gurus in the 1980s, which included Keniche Ohmae and Akio Morita. Then European gurus emerged, which included Yves Doz, Geert Hofstede, Manfred Kets De Vries and Charles Handy.\n\nOne management expert, Gary Hamel, says there have been \"few genuine breakthroughs\" since the work of Taylor and Max Weber. In his book, Hamel says that management is \"stuck in a time warp.\" Similarly, even one of the authors of a book about management gurus warns that management theory is \"not served well by fads,\" citing Enron as a \"management fad for its supposed culture of innovation.\"\n\n"}
{"id": "8912174", "url": "https://en.wikipedia.org/wiki?curid=8912174", "title": "Business history", "text": "Business history\n\nBusiness history deals with the history of business organizations, of business methods, of government regulation of businesses, of labor relations, and of business impacts on society. It also includes biographies of individual companies, executives, and entrepreneurs. It is related to economic history.\n\nEven before academic studies began, Americans were enthralled by the Robber baron debate. As the United States industrialized very rapidly after the Civil War, a few hundred prominent men made large fortunes by building and controlling major industries, such as railroads, shipping, steel, mining and banking. Yet the newer who gathered the most attention was railroader Cornelius Vanderbilt. Historian Stephen Frazier argues that probably most Americans admired Vanderbilt; they agreed with biographer William Augustus Croffut who wrote in 1886:\nHowever, Fraser goes on, there was a minority who vehemently dissented:\nBy the Great Depression of the 1930s, Fraser continues:\n\nHowever a counterattack by academic historians began as the Depression ended. Business historian Allan Nevins challenged this view of big businessmen by advocating the \"Industrial Statesman\" thesis. Nevins, in his \"John D. Rockefeller: The Heroic Age of American Enterprise\" (2 vols., 1940), took on Josephson. He argued that while Rockefeller may have engaged in some unethical and illegal business practices, this should not overshadow his bringing order to industrial chaos of the day. Gilded Age capitalists, according to Nevins, sought to impose order and stability on competitive business, and that their work made the United States the foremost economy by the 20th century. Business journalist Ferdinand Lundberg later criticized Nevins for confusing readers. By contrast, historian Priscilla Roberts argues that Nevins' studies of inventors and businessmen brought about a reassessment of American industrialization and its leaders. She writes: \n\nHistorians and biographers who followed Nevins' lead include Jean Strouse, Ron Chernow, David Nasaw, and T. J. Stiles, chronicling the lives and careers of such dominant figures as J. Pierpont Morgan, John D. Rockefeller, Andrew Carnegie, and Cornelius Vanderbilt. Though these later biographers did not confer heroic status on their subjects, they used historical and biographical investigations to establish a more complex understanding of the American past, and the history of American economic development in particular.\n\nIn 1958 historian Hal Bridges finds that \"The most vehement and persistent controversy in business history has been that waged by the critics and defenders of the 'robber baron' concept of the American businessman.\" In terms of the Robber Baron model, by the end of the 20th century scholars had mostly discarded it although it remained influential in the popular culture. Richard White, historian of the transcontinental railroads, stated in 2011 he has no use for the concept, because it had been killed off by historians Robert Wiebe and Alfred Chandler. He notes that \"Much of the modern history of corporations is a reaction against the Robber Barons and fictions.\"\n\nMeanwhile, business history as an academic discipline was founded by Professor N. S. B. Gras at the Harvard University Graduate School of Business Administration, starting in 1927. He defined the field's subject matter and approach, wrote the first general treatise in the field, and helped Harvard build a tradition of scholarship as well as the leading library in the field. He edited a series of monographs, the Harvard Studies in Business History. He also served as editor of the \"Bulletin of the Business Historical Society\" (1926–1953), a journal which later became the \"Business History Review\" (1954-date). N.S.B. Grass and Henrietta M. Larson, \"Casebook in American business history\" (1939) defined the field for a generation.\n\nBusiness history in the U.S. took off in the 1960s with a high volume of product and innovative methodologies. Scholars worked to develop theoretical explanations of the growth of business enterprise, the study of strategy and structure by Alfred Chandler being a prime example. The relationship between business and the federal government became a focal point of study. On the whole, the 1960s affirmed the conclusions of the earlier decades regarding the close interrelationship between government and business enterprise.\n\nAfter 1960 the most influential scholar was Alfred D. Chandler (1918-2007) at the Harvard Business School. In a career that spanned over sixty years, Chandler produced numerous groundbreaking monographs, articles, and reviews. Intensely focused on only a few areas of the discipline, Chandler nonetheless succeeded in establishing and developing an entirely new realm of business history.\n\nChandler's masterwork was \"The Visible Hand: The Managerial Revolution in American Business\" (1977). His first two chapters looked at traditional owner-operated small business operations in commerce and production, including the largest among them, the slave plantations in the South. Chapters 3-5 summarize the history of railroad management, with stress on innovations not just in technology but also in accounting, finance and statistics. He then turned to the new business operations made possible by the rail system in mass distribution, such as jobbers, department stores and mail order. A quick survey (ch 8) review mass innovation in mass production.The integration of mass distribution and mass production (ch 9-11) led to many mergers and the emergence of giant industrial corporations by 1900. Management for Chandler was much more than the CEO, it was the whole system of techniques and included middle management (ch 11) as well as the corporate structure of the biggest firms, Standard Oil, General Electric, US Steel, and DuPont (ch 13-14). Chandler argued that modern large-scale firms arose to take advantage of the national markets and productive techniques available after the rail network was in place. He found that they prospered because they had higher productivity, lower costs, and higher profits. The firms created the \"managerial class\" in America because they needed to coordinate the increasingly complex and interdependent system. This ability to achieve efficiency through coordination, not some anti-competitive monopolistic greed by robber barons, explained the high levels of concentration in modern American industry.\n\nChandler's work was somewhat ignored in history departments, but proved influential business, economics, and sociology. In sociology, for example, prior to Chandler's research, sociologists assumed there were no differences between governmental, corporate, and nonprofit organizations. Chandler's focus on corporations clearly demonstrated that there were differences, and this thesis has guided organizational sociologists' work since the 1970s. It also motivated sociologists to investigate and critique Chandler's work more closely, turning up instances in which Chandler assumed American corporations acted for reasons of efficiency when they actually operated in a context of politics or conflict. Other historians, such as Gabriel Kolko, challenged the very notion of \"efficiency through coordination\", arguing instead that big business had, for reasons of inefficiency and a dislike of market discipline, openly sought government assistance to keep market forces at bay.\n\nLamoreaux \"et al.\" (2003)offers a new synthesis of American business history during the 19th-20th centuries. Moving beyond the markets-versus-hierarchies framework that underlies the previously dominant interpretation of Chandler, the authors highlight the great variety of coordination mechanisms in use in the economy at any given time. Drawing on late-20th-century theoretical work in economics, they show how the relative advantages and disadvantages of these different mechanisms have shifted in complex and often unpredictable ways as a result of changing economic circumstances. One advantage of this perspective is that it avoids the teleology that has characterized so much writing in the field. As a result, the authors can situate the \"New Economy\" of the late 20th century in broad historical context without succumbing to the temptation to view it as a climactic stage in the process of economic development. They thus provide a particularly persuasive example of the importance of business history to the understanding of national and international history.\n\nUnderstanding the development of business history as a discipline meriting its own aims, theories and methods is often understood as a transition from dominating themes of 'company biography', toward more analytical 'comparative' approaches. This 'comparative' trend enabled practitioners to underline their work with 'generalist' potential. Questions of comparative business performance have become a staple, featuring into the wider economic histories of nations, regions and communities. For many this transition was first achieved by Alfred D. Chandler. Chandler’s successors as Isidor Straus Professor of Business History at Harvard Business School continued to emphasize the importance of comparative research and course development. In 1995 Thomas K. McCraw published \"Creating Modern Capitalism\" (Cambridge, MA 1995) This book compared the business histories of Britain, Germany, Japan and the United States since the Industrial Revolution, and was used as the text of a new year MBA course at Harvard Business School. Geoffrey Jones, who was McCraw’s successor as Isidor Straus Professor of Business History, also pursued a comparative research agenda. He published a comparative study of the history of globalization called \"Multinationals and Global Capitalism\" (Oxford, 2005). In 2010, Jones also published a comparative history of the global beauty industry entitled \"Beauty Imagined: A History of the Global Beauty Industry\" (Oxford, 2010). More recently, Jones and the Business History Initiative at the Harvard Business School has sought to facilitate research and teaching on African, Asian and Latin American business history in a project called Creating Emerging Markets, which includes interviews with long-time leaders of firms and NGOs in those regions.\n\nA trend in recent years has been to compare the business histories of individual countries. Geoffrey Jones (academic) and Andrea Lluch have published a comparative study of the historical impact of globalization on Argentina and Chile. In 2011 Jones and his co-editor Walter A. Friedman published an editorial in \"Business History Review\" which identified comparative research as essential for the future of business history as a discipline.\n\nAmerican historians working in French business history led by Rondo Cameron argued that most of the business enterprises in France were family-owned, small in scale, and managed conservatively. By contrast, French business historians emphasized the success of national economic planning since the end of World War II. They argued that the economic development in this period stemmed from various phenomena of the late 19th century: the corporation system, the joint-stock deposit and investment banks, and the technological innovations in the steel industry. To clarify the contributions of 19th-century entrepreneurs to the economic development in France, French scholars support two journals, \" Enterprises et Histoire\" and \"Revue d'Histoire de la Siderurgie.\"\n\nBarbero (2008) examines the development of the field of Latin American business history, from the 1960s to 2007. Latin American business history developed in the 1960s, but until the 1980s it was dominated by either highly politicized debates over Latin American underdevelopment or biographies of Latin American entrepreneurs. Since the 1980s, Latin American business history has become a much more professionalized and an integrated part of Latin American academia. It is much less politicized and has moved beyond entrepreneurial biography to histories of companies and industries. However, Latin American business historians have still not devoted enough attention to agricultural enterprises or comparative histories between the many countries. Probably most importantly, Latin American business historians have to become much more versed in business history theory and methodology so as to get beyond mere summation of the region's economic past.\n\nBusiness History in Britain emerged in the 1950s following the publication of a series of influential company histories and the establishment of the journal \"Business History\" in 1958 at the University of Liverpool. The most influential of these early company histories was Charles Wilson (historian)’s \"History of Unilever\", the first volume of which was published in 1954. Other examples included Coleman’s work on Courtaulds and artificial fibres, Alford on Wills and the tobacco industry, Barker on Pilkington’s and glass manufacture. These early studies were conducted by primarily by economic historians interested in the role of leading firms in the development of the wider industry, and therefore went beyond mere corporate histories. Although some work examined the successful industries of the industrial revolution and the role of the key entrepreneurs, in the 1970s scholarly debate in British business history became increasingly focused on economic decline. For economic historians, the loss of British competitive advantage after 1870 could at least in part be explained by entrepreneurial failure, prompting further business history research into individual industry and corporate cases. The Lancashire cotton textile industry, which had been the leading take-off sector in the industrial revolution, but which was slow to invest in subsequent technical developments, became an important topic of debate on this subject. William Lazonick for example argued that cotton textile entrepreneurs in Britain failed to develop larger integrated plants on the American model; a conclusion similar to Chandler’s synthesis of a number of comparative case studies.\n\nStudies of British business leaders have emphasized how they fit into the class structure, especially their relationship to the aristocracy, and the desire to use their wealth to purchase landed estates, and hereditary titles. Biography has been of less importance in British business history, but there are compilations.\nBritish business history began to widen its scope in the 1980s, with research work conducted at the LSE's Business History Unit, led first by Leslie Hannah, then by Terry Gourvish. Other research centres followed, notably at Glasgow and Reading, reflecting an increasing involvement in the discipline by Business and Management School academics. More recent editors of \"Business History\", Geoffrey Jones (academic)(Harvard Business School), Charles Harvey (University of Newcastle Business School), John Wilson (Liverpool University Management School) and Steven Toms (Leeds University Business School) have promoted management strategy themes such as networks, family capitalism, corporate governance, human resource management, marketing and brands, and multi-national organisations in their international as well as merely British context. Employing these new themes has allowed business historians to challenge and adapt the earlier conclusions of Chandler and others about the performance of the British economy.\n\n\n\n\n\n"}
{"id": "29731991", "url": "https://en.wikipedia.org/wiki?curid=29731991", "title": "Business intelligence deployment", "text": "Business intelligence deployment\n\nUtilising the DW/BI system is the final step before business users can get access to the information. The first impression the business community gets is when introduced to the BI frontend drives. Because the acceptance from the users is important the deployment has to be thoughtfully planned to ensure that the DW/BI system can perform and is delivering the results it is designed to.\nTo ensure that the implementation can perform and deliver it has to be exposed to extensive end-to-end testing . The process of testing is an ongoing activity along the development process, because defects that should be correct later in the lifecycle are difficult to find and are associated with exponentially increasing costs. A way of securing that the testing is done through the development lifecycle is to follow a methodology.\nKimball prescribe that before adding the DW/BI system, it should have passed a mock test that will cover the following procedures;\nDocumentation and Training\nMaintenance and Support\n"}
{"id": "185092", "url": "https://en.wikipedia.org/wiki?curid=185092", "title": "Business rule", "text": "Business rule\n\nA business rule defines or constrains some aspect of business and always resolves to either true or false. Business rules are intended to assert business structure or to control or influence the behavior of the business. Business rules describe the operations, definitions and constraints that apply to an organization. Business rules can apply to people, processes, corporate behavior and computing systems in an organization, and are put in place to help the organization achieve its goals.\n\nFor example, a business rule might state that \"no credit check is to be performed on return customers\". Other examples of business rules include requiring a rental agent to disallow a rental tenant if their credit rating is too low, or requiring company agents to use a list of preferred suppliers and supply schedules.\n\nWhile a business rule may be informal or even unwritten, documenting the rules clearly and making sure that they don't conflict is a valuable activity. When carefully managed, rules can be used to help the organization to better achieve goals, remove obstacles to market growth, reduce costly mistakes, improve communication, comply with legal requirements, and increase customer loyalty.\n\nBusiness rules tell an organization \"what\" it can do in detail, while strategy tells it \"how\" to focus the business at a macro level to optimize results. Put differently, a strategy provides high-level direction about what an organization should do. Business rules provide detailed guidance about how a strategy can be translated to action.\n\nBusiness rules exist for an organization whether or not they are ever written down, talked about or even part of the organization's consciousness. However it is a fairly common practice for organizations to gather business rules. This may happen in one of two ways.\n\nOrganizations may choose to proactively describe their business practices, producing a database of rules. While this activity may be beneficial, it may be expensive and time-consuming. For example, they might hire a consultant to comb through the organization to document and consolidate the various standards and methods currently in practice.\n\nGathering business rules is also called rules harvesting or business rule mining. The business analyst or consultant can extract the rules from IT documentation (like use cases, specifications or system code). They may also organize workshops and interviews with subject matter experts (commonly abbreviated as SMEs). Software technologies designed to capture business rules through analysis of legacy source code or of actual user behavior can accelerate the rule gathering processing.\n\nMore commonly, business rules are discovered and documented informally during the initial stages of a project. In this case the collecting of the business rules is incidental. In addition, business projects, such as the launching of a new product or the re-engineering of a complex process, might lead to the definition of new business rules. This practice of incidental, or emergent, business rule gathering is vulnerable to the creation of inconsistent or even conflicting business rules within different organizational units, or within the same organizational unit over time. This inconsistency creates problems that can be difficult to find and fix.\n\nAllowing business rules to be documented during the course of business projects is less expensive and easier to accomplish than the first approach, but if the rules are not collected in a consistent manner, they are not valuable. In order to teach business people about the best ways to gather and document business rules, experts in business analysis have created the Business Rules Methodology. This methodology defines a process of capturing business rules in natural language, in a verifiable and understandable way. This process is not difficult to learn, can be performed in real-time, and empowers business stakeholders to manage their own business rules in a consistent manner.\n\nAccording to the white paper by the Business Rules Group, a statement of a business rule falls into one of four categories:\nThe most basic element of a business rule is the language used to express it. The very definition of a term is itself a business rule that describes how people think and talk about things. Thus, defining a term is establishing a category of business rule. Terms have traditionally been documented in a Glossary or as entities in a conceptual model.\n\nThe nature or operating structure of an organization can be described in terms of the facts that relate terms to each other. To say that a customer can place an order is NOT a business rule, but a \"fact\". Facts can be documented as natural language sentences or as relationships, attributes, and generalization structures in a graphical model.\n\nEvery enterprise constrains behavior in some way, and this is closely related to constraints on what data may or may not be updated. To prevent a record from being made is, in many cases, to prevent an action from taking place.\n\nBusiness rules (including laws of nature) define how knowledge in one form may be transformed into other knowledge, possibly in a different form.\n\nBusiness rules are gathered in these situations: \nThis lack of consistent approach is mostly due to the cost and effort required to maintain the list of rules.\n\nWhile newer software tools are able to combine business rule management and execution, it is important to realize that these two ideas are distinct, and each provides value that is different from the other. Software packages automate business rules using business logic. The term \"business rule\" is sometimes used interchangeably with \"business logic\"; however the latter connotes an engineering practice and the former an intrinsic business practice. There is value in outlining an organization's business rules regardless of whether this information is used to automate its operations.\n\nOne of the pitfalls in trying to fill the gap between rules management and execution is trying to give business rules the syntax of logic, and merely describing logical constructs in a natural language. Translation for engines is easier, but business users will no longer be able to write down the rules.\n\nBusiness rules can be expressed using modeling approaches such as Unified Modeling Language, Z notation, Business Process Execution Language, Business Process Modeling Notation, Decision Model and Notation or the Semantics of Business Vocabulary and Business Rules (SBVR).\n\nBusiness rules encoded in computer code in an operational program are known as business logic.\n\nSimilar to how Business Risks can be structured as:\n\nIf <condition(s)> Then <consequence(s)>\n\na Business Rule can be structured as:\n\nWhen <condition(s)> Then <imposition(s)> Otherwise <consequence(s)>\n\n\n\n"}
{"id": "22697384", "url": "https://en.wikipedia.org/wiki?curid=22697384", "title": "Business sector", "text": "Business sector\n\nIn economics, the business sector or corporate sector - sometimes popularly called simply \"business\" - is \"the part of the economy made up by companies\". It is a subset of the domestic economy,\nexcluding the economic activities of general government, of private households, and of non-profit organizations serving individuals. An alternative analysis of economies, the three-sector theory, subdivides them into: \n\n\nIn the United States the business sector accounted for about 78 percent of the value of gross domestic product (GDP) . Kuwait and Tuvalu each had business sectors accounting for less than 40% of GDP .\n\n\n\n"}
{"id": "12482068", "url": "https://en.wikipedia.org/wiki?curid=12482068", "title": "Business travel", "text": "Business travel\n\nBusiness travel is travel undertaken for work or business purposes, as opposed to other types of travel, such as for leisure purposes or regularly commuting between one's home and workplace.\n\nThe reasons to conduct business travel might include:\n\n\n"}
{"id": "3176153", "url": "https://en.wikipedia.org/wiki?curid=3176153", "title": "Chief Happiness Officer", "text": "Chief Happiness Officer\n\nThe Chief Happiness Officer (CHO) in a company is the manager of workers' happiness. Probably originating in North America, CHO posts are being created in European and UK companies to ensure workers' welfare needs are met.\n\n"}
{"id": "44435092", "url": "https://en.wikipedia.org/wiki?curid=44435092", "title": "Chief Roughneck Award", "text": "Chief Roughneck Award\n\nThe Chief Roughneck Award has presented annually since 1955 by U.S. Steel Tubular Products, Inc., a subsidiary of United States Steel Corporation. It honors the highest ideals of the petroleum industry and is awarded to honor the achievements and character of a petroleum executive. Recently it has been presented at the annual meeting of the Independent Petroleum Association of America in San Antonio, Texas.\n\nThe bronze Joe Roughneck bust, created by Torg Thompson and presented to each Chief Roughneck recipient, began life in Lone Star Steel Company print advertising.\n\nJoe Roughneck's sculpture has been dedicated in several oil patch parks, saluted by governors, and featured in newspaper and magazine articles.\n\nThe award is recognized as one of the most meaningful honors in the industry; the award and the character behind it symbolize the spirit, determination, leadership and integrity of individuals who have made a lasting impression on the energy industry.\n\n\n"}
{"id": "56726769", "url": "https://en.wikipedia.org/wiki?curid=56726769", "title": "Chief ethics officer", "text": "Chief ethics officer\n\nThe Chief Ethics Officer (CFO) is a senior ranking individual in an organization. The primary role is to build a strong ethical culture within the organization. In order to perform these responsibilities the CFO must be given support, independence, and opportunity to influence key decision-making board members. The chief ethics officer normally reports to the chief executive officer, and have some access to the board of directors.\n\nChief Ethics Officer role was redefined in 1991 for businesses, when Congress passed the Federal Guidelines for Organization (FSCO). Before 1991 this role existed in the financial services and the health care industry without much influence because of the rash of accounting scandals such as the Enron scandal. The need for chief ethics officers has increased tremendously. The role of chief ethics officers has increased since the passage of the Sarbanes–Oxley Act (SOA), and amendments of the U.S. Federal Sentencing Guidelines (FSCO). The role of Chief Ethics Officer is no longer a figure head position.\n\nIn 2002, after congress passed the Dodd-Frank Wall Street Reform Act and the Consumer Protection Act, chief ethics officer responsibilities have expanded. the role has expanded to be more specific. The chief ethics officer is responsible for Developing and distributing codes of ethics, developing training programs for employees', monitor and comply too government regulations. His role is also to monitor and audit ethical conduct. In addition his responsibilities entails administering punishments for violation of ethical codes of conduct.\n\nThe primary objective of pricing ethics is to prevent the establishment of monopolies, inequitable pricing practices, and reduce or restrict competition. Such laws are some times called pro-competitive legislation, which were enacted to encourage competition, and prevent activities that restrain trade.\n\nThe following are some of the acts passed by Congress, to help prevent unethical practices by businesses to manipulate prices in order to monopolize their industries, such as the Sherman Antitrust Act of 1890, which prohibits organizations from holding monopolies in their industry, the Robinson–Patman Act of 1936, which bans price discrimination between retailers and wholesalers, and the Consumer Goods Pricing Acts of 1975, which prohibits price maintenance agreements among manufactures and resellers in interstate commerce.\n\nOrganizational ethics is the ethics of an organization, and it is how an organization responds to an internal or external issue. Organizational ethics is independent of the organizational culture. Organizational culture tends to display the same ethical views of the surrounding community. To reinforce positive views of the community, certain businesses give incentives to both employees and the community. Business organizations give to community by granting scholarships, funds and resources. Employees also give back to the community by contributing their time, gifts and talents. When the local residents obtain the skills from skilled employees then these individuals can apply and fulfill critical skills shortages needed by the organization. When the incentive value method represent the community in a positive way everyone wins, such as the business organization, and the community.\n\nEthical communication is a must for a businesses to have a long and successful future. A business cannot act unethically when dealing with its customers, employees or other businesses. Unethical communication consist of false or misleading information, which is a violation of the Federal Trade Communication. Other unethical means of marketing material, is by fraud, scams, and e-mail, or text messaging unsolicited marketing material (SPAM).\n"}
{"id": "343044", "url": "https://en.wikipedia.org/wiki?curid=343044", "title": "Chief operating officer", "text": "Chief operating officer\n\nThe chief operating officer (COO), also called the chief operations officer, is one of the highest-ranking executive positions in an organization, comprising part of the \"C-Suite\". The COO is responsible for the daily operation of the company, and routinely reports to the highest-ranking executive, usually the chief executive officer (CEO).\n\nThe COO is usually the second in command at the firm, especially if the highest-ranking executive is the Chairman and CEO.\n\nUnlike other C-Suite positions, which tend to be defined according to commonly designated responsibilities across most companies, the COO job tends to be defined in relation to the specific CEO with whom they work, given the close working relationship of these two individuals. In many ways, the selection of a COO is similar to the selection of a Vice President or Chief of Staff of the United States: power and responsibility structures vary in government and private regimes depending on the style and needs of the President or CEO. Thus, the COO role meets individual expectations and changes as leadership teams adjust.\n\nIn a similar vein to the COO, the title of corporate President as a separate position (as opposed to being combined with a \"C-Suite\" designation, such as \"President and CEO\" or \"President and COO\") is also loosely defined. The President is usually the legally recognized highest rank of corporate officer, ranking above the various Vice Presidents (including Senior Vice President and Executive Vice President), but on its own generally considered subordinate, in practice, to the CEO. \n\nLloyd E. Reuss was President of General Motors from 1990 to 1992, as the right-hand man of Chairman and CEO Robert C. Stempel. Stempel insisted on naming Reuss as company president in charge of North American operations, the board reluctantly agreed but showed their displeasure by not giving Reuss the title of COO.\n\nRichard D. Parsons was number two in the company hierarchy during his tenure as President of Time Warner from 1995 to 2001, but he had no authority over the operating divisions, and instead took on assignments at the behest of Chairman and CEO Gerald Levin.\n\nMichael Capellas was appointed President of Hewlett-Packard in order to ease its acquisition and integration of Compaq, where Capellas was previously Chairman and CEO. Capellas ended up serving just six months as HP president before departing. His former role of president was not filled as the executives who reported to him then reported directly to the CEO.\n\nIn 2007, the investment banking firms of Bear Stearns and Morgan Stanley each had two presidents (Warren Spector and Alan Schwartz at Bear, Robert Scully and Zoe Cruz at Morgan) reporting to one CEO (who was also chairman of the board); each president was essentially a co-COO (despite the lack of title) overseeing half of the firm's business divisions. Schwartz became sole president of Bear after Spector was ousted, and several months later assumed the position of CEO as well when James Cayne was forced to resign (Cayne remained chairman).\n\nTom Anselmi of Maple Leaf Sports & Entertainment was Chief Operating Officer from 2004 until September 6, 2013. Between the departure of Richard Peddie and the hiring of Tim Leiweke for the posts of President and CEO, Anselmi added the title of President from September 4, 2012 to June 30, 2013, however he remained COO and did not receive the title of CEO.\n\nRichard Fuld, the chairman and CEO of Lehman Brothers, had a succession of \"number twos\" under him, usually titled as President and Chief Operating Officer. Chris Pettit was Fuld's second-in-command for two decades until November 26, 1996, when he resigned as President and board member. Pettit lost a power struggle with his deputies (Steve Lessing, Tom Tucker, and Joseph M. Gregory) on March 15 that year that caused him to relinquish its COO title, likely brought about after the three men found about Pettit's extramarital affairs, which violated Fuld's unwritten rules on marriage and social etiquette. Bradley Jack and Joseph M. Gregory were appointed co-COOs in 2002, but Jack was demoted to the Office of the Chairman in May 2004 and departed in June 2005 with a severance package of $80 million, making Gregory the sole COO. While Fuld was considered the \"face\" of Lehman brothers, Gregory was in charge of day-to-day operations and he influenced culture to drive the bottom line. Gregory was demoted on June 12, 2008 and replaced as President and COO by Bart McDade, who had been serving as head of Equities, and McDade would see Lehman through bankruptcy.\n\nThomas W. LaSorda served as President and CEO of Chrysler from January 1, 2006 to August 5, 2007 while Chrysler was owned by Daimler-Benz. When Cerberus Capital bought majority control of Chrysler, Bob Nardelli was appointed Chairman and CEO of Chrysler while LaSorda became Vice Chairman and President. Despite the appointment of a second Vice Chairman and President, Jim Press, LaSorda stayed on. LaSorda's titles as Vice Chairman and President officially stated that he was in charge of manufacturing, procurement and supply, employee relations, global business development and alliances. However, LaSorda's actual role was to find a new partner or buyer for Chrysler, leading to speculation that Cerberus Capital was less interested in rebuilding the auto manufacturer than it was to turning profit though a leveraged buyout.\n\nResearch in Motion's corporate structure had more than one COO, including Jim Rowan as chief operating officer for global operations, and Thorsten Heins as COO of products and sales.\n\nThe Walt Disney Company has used the President and COO titles in varied ways for their number two executive. Ron W. Miller was President from 1978 to 1984, while serving additionally as CEO for 18 months from 1983 to 84. Frank Wells was President from 1984 to 1994, where he reported to the board of directors and not Chairman and CEO Michael Eisner. When Wells died in a helicopter crash, no replacement President was named as his duties were resumed by Eisner. Michael Ovitz was President from 1995 to 1997, being hired by Eisner and then dismissed not long afterwards. Bob Iger was President and COO from 2000 to 2005, when he succeeded Eisner as CEO. Thomas O. Staggs was COO from 2015 to 2016, during that time the senior executive team had a dual reporting structure to both Staggs and Iger; Staggs resigned after the board did not give him assurances that he would succeed as CEO.\n\nManulife has used the President and COO titles for separate roles. From June 5 until September 30, 2017, Rocco \"Roy\" Gori served as President where he oversaw Manulife's global operating businesses, with his subordinates being the general managers of the Canadian, U.S., and Asia Divisions, and the Chief Investment Officer. Gori reported to Chief Executive Officer Donald Guloien before additionally assuming the title of CEO on October 1, 2017 upon Guloien's retirement. Linda Mantia, the Chief Operating Officer, reported to the President on Corporate Strategy while continuing to report to the CEO on all other matters including Corporate Development, Analytics, Technology, Marketing, Innovation, Human Resources, Regulatory and Public Affairs, Global Resourcing and Procurement, and the Global Program Office.\n\nSome modern companies operate without a COO. For example, in 2007 almost 58% of Fortune 500 companies did not have a COO. In these instances the CEO either takes on more roles and responsibilities, or the roles traditionally assigned to the COO are carried out by sub C-Suite executives. Although the number of COOs has been in decline for the past 10 years, there are reasons to anticipate an increased utilization of the position in the future, including:\n\nThe role of the COO differs from industry to industry and from organization to organization. Some organizations function without a COO. Others may have two COOs, each assigned to oversee several business lines or divisions, such as Lehman Brothers from 2002 to 2004 when Bradley Jack and Joseph M. Gregory were the co-COOs. A COO could also be brought in from other organizations as a \"fixer,\" such as Daniel J. O'Neill who in 1999 joined Molson in that capacity.\n\nIn the manufacturing sector, the primary role of the COO is routinely one of operations management, meaning that the COO is responsible for the development, design, operation, and improvement of the systems that create and deliver the firm's products. The COO is responsible for ensuring that business operations are efficient and effective and that the proper management of resources, distribution of goods and services to customers and analysis of queue systems is conducted.\n\nDespite the functional diversity associated with the role of COO, there are some common functions the COOs usually perform:\n\nRoutinely in large organizations the COO will be the heir apparent to the CEO. Individuals may have worked their way (internally) up the company ladder before being named COO, or may have been recruited from an outside company. Either way, the position is used as a training and testing ground for the next CEO.\n\nA 2003 Crist Associates study revealed that only 17% of companies that promote a COO to a CEO replace the COO within the next year.\n\nAn Accenture study found that approximately one in nine COOs moved into the CEO's shoes within a year of their departure and that half of COOs see themselves as the \"heir apparent.\" COOs transitioning into the CEO role often face similar challenges including:\n\nAccording to researchers Miles and Bennett, just knowing these common pitfalls can help a COO \"heir\" better prepare for the transition, thereby avoiding them in totality or ensuring that at least they do not evolve into full derailers once they are in the CEO seat.\n\nBecause the COO is often responsible for serving as an information conduit to the CEO, it is essential that the relationship between COO and CEO be a positive one. Trust is the most important ingredient necessary for a CEO-COO relationship to thrive. The CEO must have full confidence that the COO is not making direct passes for their job, can get the work done, and shares their vision (rather than using their trusted spot and access to information to undermine the CEO's strategy or implement his/her own vision). When a relationship built upon trust is created between the CEO and COO, firm performance is improved and shareholder results are strengthened. Seven strategies that are key to building trust in the CEO-COO relationship include:\n\n\nIn addition to having a strong and trusting relationship with the CEO, the COO should also have an effective relationship with the board. A good relationship between COO and the board allows the board to better understand and independently judge a potential successor. A strong relationship between the board and the COO also offers the board an additional expert opinion on the health of the company, and status of key initiatives. It benefits the CEO to allow such a relationship to form because it reflects confidence and fosters transparency. It also reinforces that the CEO is capable of developing talent, and helps the CEO to retain the COO by further empowering the individual. A strong relationship benefits the COOs in that they are able to expand their experience as well as their professional network. Additionally, if they are looking to be the next CEO, it allows them to develop credibility with the board. Researchers advise the COO to go beyond simply presenting at board meetings, to ensure they are developing strong one-on-one relationships with each board director. Researchers also urge the COO to develop his or her own voice, independent of the CEO.\n\nAny breakdown in trust between the CEO and COO can lead to failure. Additionally, the COO typically has to be a high-level leader who is comfortable being fully in charge. Many executives with the leadership skills necessary to be a top-level COO would prefer to be running their own organization as opposed to taking orders from a CEO. For COOs who are expecting to serve their time and be promoted to the top spot, their timelines for such a move can often be out of sync with the CEOs, causing a breakdown in the relationship. COOs can also find themselves trapped into being labeled an \"operations\" person or a \"Number 2\" as opposed to being seen as a strategic and top-level leader by the Board of Directors, which causes some executives to steer clear of the position. Harry Levinson, Ph.D., effectively summarized the challenges of the COO position: \"The relationship between the chief executive officer and the chief operating officer in any organization is fraught with many psychological complexities. Perhaps it is the most difficult of all organizational working relationships because more than others, it is a balancing act on the threshold of power.\"\n\nNathan Bennett and Stephen A. Miles have published extensively on the subject of the COO. In addition to writing a book dedicated to analyzing this hard-to-understand position, their research has been published in the \"Harvard Business Review\", \"MIT Management Review\" and Chief Executive.net. Their work focuses on identifying different types of COOs, when the role works to deliver additional value, and when the role fails to produce the desired results. Their research includes interviews with more than a hundred CEOs and COOs on the topic.\n"}
{"id": "77444", "url": "https://en.wikipedia.org/wiki?curid=77444", "title": "Commercial law", "text": "Commercial law\n\nCommercial law, also known as trade law, is the body of law that applies to the rights, relations, and conduct of persons and businesses engaged in commerce, merchandising, trade, and sales. It is often considered to be a branch of civil law and deals with issues of both private law and public law.\n\nCommercial law includes within its compass such titles as principal and agent; carriage by land and sea; merchant shipping; guarantee; marine, fire, life, and accident insurance; bills of exchange, negotiable instruments, contracts and partnership. Many of these categories fall within Financial law, an aspected of Commercial law pertaining specifically to financing and the financial markets. It can also be understood to regulate corporate contracts, hiring practices, and the manufacture and sales of consumer goods. Many countries have adopted civil codes that contain comprehensive statements of their commercial law.\n\nIn the United States, commercial law is the province of both the United States Congress, under its power to regulate interstate commerce, and the states, under their police power. Efforts have been made to create a unified body of commercial law in the United States; the most successful of these attempts has resulted in the general adoption of the Uniform Commercial Code, which has been adopted in all 50 states (with some modification by state legislatures), the District of Columbia, and the U.S. territories.\n\nVarious regulatory schemes control how commerce is conducted, particularly \"vis-a-vis\" employees and customers. Privacy laws, safety laws (e.g., the Occupational Safety and Health Act in the United States), and food and drug laws are some examples.\n\n\n"}
{"id": "49597315", "url": "https://en.wikipedia.org/wiki?curid=49597315", "title": "Computer bureau", "text": "Computer bureau\n\nComputer bureaus developed during the early 1960s, following the development of time-sharing operating systems. These allowed the services of a single large and expensive mainframe computer to be divided up and sold as a fungible commodity. Development of telecommunications and the first modems encouraged the growth of computer bureau as they allowed immediate access to the computer facilities from a customer's own premises.\n\nThe computer bureau model shrank during the 1980s, as cheap commodity computers, particularly the PC clone but also the minicomputer allowed services to be hosted on-premises.\n\n"}
{"id": "34780328", "url": "https://en.wikipedia.org/wiki?curid=34780328", "title": "Credit Managers' Index", "text": "Credit Managers' Index\n\nThe Credit Managers' Index (CMI) is a monthly economic indicator of financial activity reflecting credit managers' responses to levels of favorable and unfavorable factors. The measure has been sourced in stories from publications such as the \"Wall Street Journal\", \"CFO\" and \"Bloomberg\".\n\nTracked since February 2002, the CMI is produced by the National Association of Credit Management (NACM) and is currently conducted by Armada Corporate Intelligence's Chris Kuehl, who also serves as NACM's economic advisor. The CMI is compiled through a voluntary poll of credit and finance professionals in the service and manufacturing sectors. The CMI results generally are released on the last business day of each month.\n\nA CMI number of more than 50 indicates an economy in expansion; less than 50 indicates contraction.\n\nUnlike many economic indexes, the CMI resisted the month-to-month swings during the most recent economic downturn. The index accurately signaled that the economic plunge was stabilizing and beginning to recover from the recession.\n\nThe Chartered Institute of Credit Management (CICM) in the UK produces a similar Index on a quarterly basis reflecting its members' responses to questions about the same factors and it uses the same methodology.\n\nThe index is based on survey responses of approximately 1,000 trade credit managers in the second half of each month. There is typically an approximately equal representation between the manufacturing and service sectors. Respondents from throughout the United States are asked to comment on whether they are seeing improvement, deterioration, or no change for various favorable and unfavorable factors.\n\nThe computation of seasonality is based on the formula used by the U.S. Census Bureau and most of the federal government's statistical gathering apparatus. This is designed to streamline comparisons between the CMI diffusion index and comparable indices.\n"}
{"id": "1152274", "url": "https://en.wikipedia.org/wiki?curid=1152274", "title": "Cultural industry", "text": "Cultural industry\n\nAccording to international organizations such as UNESCO and the General Agreement on Tariffs and Trade (GATT), cultural industries (sometimes also known as \"creative industries\") combine the creation, production, and distribution of goods and services that are cultural in nature and usually protected by intellectual property rights.\n\nThe notion of cultural industries generally includes textual, music, television, and film production and publishing, as well as crafts and design. For some countries, architecture, the visual and performing arts, sport, advertising, and cultural tourism may be included as adding value to the content and generating values for individuals and societies. They are knowledge-based and labour-intensive, creating employment and wealth. By nurturing creativity and fostering innovation societies will maintain cultural diversity and enhance economic performance.\n\nCultural industries worldwide have adapted To The New digital technologies and to the arrival of national, regional and international (de)regulatory policies. These factors have radically altered the context in which cultural goods, services, and investments flow between countries and, consequently, these industries have undergone a process of internationalization and progressive concentration, resulting in the formation of a few big conglomerates: a new global oligopoly.\n\n\n1. \"Exploring The Cultural and Creative Industries Debate\". Culture Action Europe. Retrieved 2013-07-07.\n[nonexistent/incorrect reference]\n"}
{"id": "17617913", "url": "https://en.wikipedia.org/wiki?curid=17617913", "title": "Customer Demand Planning", "text": "Customer Demand Planning\n\nCustomer Demand Planning (CDP) is a business-planning process that enables sales teams to develop demand forecasts as input to service-planning processes, production, inventory planning and revenue planning.\n\nCDP is an important aspect of managing value chains. Generally, the first step of CDP is to forecast product demand. A manager can plan resource deployment in accordance with the resulting forecasts. It's a bottom-up approach vs. top down planning. Associated risks with this method are: Low forecast accuracy and numbers of planners required. There are various software systems created by companies such as Dynasys, Avercast, Demand Solutions, RightChain TM, SAS System, Agentrics, Manugistics, Oracle, Petrolsoft Corporation (now Aspen Technology), StatSoft, ToolsGroup and GMDH Shell that are designed to forecast demand and plan operations. To test the added value of implementing bottom-up approach, SAP APO applications are providing simulations functionalities to estimate the resulting Demand Forecast Accuracy (e.g. POS sales ; Sales invoices ; shipments, etc.)\n\nIn the manufacturer to retailer model, customer collaborative partnerships have been become more common since the 1990s. Although there was a lot of industry support behind CPFR (Collaborative Planning, Forecasting and Replenishment), manufacturers and retailers are adopting different versions of collaborative forecasting and replenishment strategies. These include Collaborative-VMI, CPFR, Account Based Forecasting, CMI, Shared Single Forecast and replenishment etc.\n\nThe challenges and complexities faced by the retailer are somewhat different from the challenges faced by suppliers (manufacturers and distributors). Therefore, the demand management technology for the merchant is somewhat different from that of the brand or manufacturer on the supply-side.\nCustomer Demand Planning aims at matching customer supply planning logic and imply CPFR type collaboration.\nAreas components of Demand Management include Customer Experience, Demand Creation, Inventory and Pricing Optimization, Channel Management, Sourcing, Transportation Optimization and Advanced Practices in Technology.\n\n"}
{"id": "5073551", "url": "https://en.wikipedia.org/wiki?curid=5073551", "title": "Customer service representative", "text": "Customer service representative\n\nCustomer service representatives (CSRs), customer service advisors, or customer service associates (CSAs) interact with customers to handle complaints, process orders, and provide information about an organization’s products and services. Qualifications include good communication, problem-solving, and computer skills.\n\nCustomer service representative positions often require at least a high school diploma. Representatives often have some experience with office software.\n\nFirst impressions are very important in this job. The moment one begins to communicate with a customer, one is being judged. The way one communicates with a customer will not only influence how the conversation develops, but the overall impression of the CSR and the organization they represent. There are five key customer service skills that define the best CSRs: \n\nAlthough earnings may vary, the median hourly average for CSRs in 2017 was $15.81, in the US. People in this job typically have less than 20 years' experience. Skills that are associated with high pay for this job are those in customer service metrics, Microsoft Office, customer relationship management, and oral and verbal communication.\n\n"}
{"id": "4166591", "url": "https://en.wikipedia.org/wiki?curid=4166591", "title": "Dashboard (business)", "text": "Dashboard (business)\n\nDashboards often provide at-a-glance views of key performance indicators (KPIs) relevant to a particular objective or business process. In the other, \"dashboard\" has another name for \"progress report\" or \"report.\"\n\nThe \"dashboard\" is often displayed on a web page which is linked to a database that allows the report to be constantly updated. For example, a manufacturing dashboard may show numbers related to productivity such as number of parts manufactured, or number of failed quality inspections per hour. Similarly, a human resources dashboard may show numbers related to staff recruitment, retention and composition, for example number of open positions, or average days or cost per recruitment.\n\nThe term dashboard originates from the automobile dashboard where drivers monitor the major functions at a glance via the instrument cluster.\n\nDigital dashboards allow managers to monitor the contribution of the various departments in their organization. To gauge exactly how well an organization is performing overall, digital dashboards allow you to capture and report specific data points from each department within the organization, thus providing a \"snapshot\" of performance.\n\nBenefits of using digital dashboards include:\n\nDashboards can be broken down according to role and are either strategic, analytical, operational, or informational. Strategic dashboards support managers at any level in an organization, and provide the quick overview that decision makers need to monitor the health and opportunities of the business. Dashboards of this type focus on high level measures of performance, and forecasts. Strategic dashboards benefit from static snapshots of data (daily, weekly, monthly, and quarterly) that are not constantly changing from one moment to the next. Dashboards for analytical purposes often include more context, comparisons, and history, along with subtler performance evaluators. Analytical dashboards typically support interactions with the data, such as drilling down into the underlying details. Dashboards for monitoring operations are often designed differently from those that support strategic decision making or data analysis and often require monitoring of activities and events that are constantly changing and might require attention and response at a moment's notice.\n\nDigital dashboards may be laid out to track the flows inherent in the business processes that they monitor. Graphically, users may see the high-level processes and then drill down into low level data. This level of detail is often buried deep within the corporate enterprise and otherwise unavailable to the senior executives.\n\nThree main types of digital dashboard dominate the market today: stand alone software applications, web-browser based applications, and desktop applications also known as desktop widgets. The last are driven by a widget engine.\n\nSpecialized dashboards may track all corporate functions. Examples include human resources, recruiting, sales, operations, security, information technology, project management, customer relationship management and many more departmental dashboards. For a smaller organization like a startup a compact startup scorecard dashboard tracks important activities across lot of domains ranging from social media to sales.\n\nDigital dashboard projects involve business units as the driver and the information technology department as the enabler. The success of digital dashboard projects often depends on the metrics that were chosen for monitoring. Key performance indicators, balanced scorecards, and sales performance figures are some of the content appropriate on business dashboards.\n\nBalanced Scoreboards and Dashboards have been linked together as if they were interchangeable. However, although both visually display critical information, the difference is in the format: Scoreboards can open the quality of an operation while dashboards provide calculated direction. \nA balanced scoreboard has what they called a \"prescriptive\" format. It should always contain these components (Active Strategy)...\nEach of these sections ensures that a Balanced Scorecard is essentially connected to the businesses critical strategic needs.\n\nThe design of a dashboard is more loosely defined. Dashboards are usually a series of graphics, charts, gauges and other visual indicators that can be monitored and interpreted. Even when there is a strategic link, on a dashboard, it may not be noticed as such since objectives are not normally present on dashboards. However, dashboards can be customized to link their graphs and charts to strategic objectives.\n\nDigital dashboard technology is available \"out-of-the-box\" from many software providers. Some companies however continue to do in-house development and maintenance of dashboard applications. For example, GE Aviation has developed a proprietary software/portal called \"Digital Cockpit\" to monitor the trends in aircraft spare parts business.\n\nGood dashboard design practices take into account and address the following:\n\n\nA good information design will clearly communicate key information to users and makes supporting information easily accessible.\n\nThere are a few key elements to a good dashboard:.\n\nThe idea of digital dashboards followed the study of decision support systems in the 1970s. Early predecessors of the modern business dashboard were first developed in the 1980s in the form of Executive Information Systems (EISs). Due to problems primarily with data refreshing and handling, it was soon realized that the approach wasn’t practical as information was often incomplete, unreliable, and spread across too many disparate sources. Thus, EISs hibernated until the 1990s when the information age quickened pace and data warehousing, and online analytical processing (OLAP) allowed dashboards to function adequately. Despite the availability of enabling technologies, the dashboard use didn't become popular until later in that decade, with the rise of key performance indicators (KPIs), and the introduction of Robert S. Kaplan and David P. Norton's Balanced Scorecard. In the late 1990s, Microsoft promoted a concept known as the Digital Nervous System and \"digital dashboards\" were described as being one leg of that concept. Today, the use of dashboards forms an important part of Business Performance Management (BPM).\n\n"}
{"id": "9310", "url": "https://en.wikipedia.org/wiki?curid=9310", "title": "Enterprise resource planning", "text": "Enterprise resource planning\n\nEnterprise resource planning (ERP) is the integrated management of core business processes, often in real-time and mediated by software and technology.\n\nERP is usually referred to as a category of business-management software — typically a suite of integrated applications—that an organization can use to collect, store, manage, and interpret data from these many business activities.\n\nERP provides an integrated and continuously updated view of core business processes using common databases maintained by a database management system. ERP systems track business resources—cash, raw materials, production capacity—and the status of business commitments: orders, purchase orders, and payroll. The applications that make up the system share data across various departments (manufacturing, purchasing, sales, accounting, etc.) that provide the data. ERP facilitates information flow between all business functions and manages connections to outside stakeholders.\n\nEnterprise system software is a multibillion-dollar industry that produces components supporting a variety of business functions. IT investments have become the largest category of capital expenditure in United States-based businesses over the past decade. Though early ERP systems focused on large enterprises, smaller enterprises increasingly use ERP systems.\n\nThe ERP system integrates varied organizational systems and facilitates error-free transactions and production, thereby enhancing the organization's efficiency. However, developing an ERP system differs from traditional system development.\nERP systems run on a variety of computer hardware and network configurations, typically using a database as an information repository.\n\nThe Gartner Group first used the abbreviation ERP in the 1990s to extend upon the capabilities of material requirements planning (MRP), and the later manufacturing resource planning (MRP II), as well as computer-integrated manufacturing. Without replacing these terms, ERP came to represent a larger whole that reflected the evolution of application integration beyond manufacturing.\n\nNot all ERP packages developed from a manufacturing core; ERP vendors variously began assembling their packages with finance-and-accounting, maintenance, and human-resource components. By the mid-1990s ERP systems addressed all core enterprise functions. Governments and non–profit organizations also began to use ERP systems.\n\nERP systems experienced rapid growth in the 1990s. Because of the year 2000 problem and the introduction of the euro that disrupted legacy systems, many companies took the opportunity to replace their old systems with ERP.\n\nERP systems initially focused on automating back office functions that did not directly affect customers and the public. Front office functions, such as customer relationship management (CRM), dealt directly with customers, or e-business systems such as e-commerce, e-government, e-telecom, and e-finance—or supplier relationship management (SRM) became integrated later, when the internet simplified communicating with external parties.\n\n\"ERP II\" was coined in 2000 in an article by Gartner Publications entitled \"ERP Is Dead—Long Live ERP II\". It describes web–based software that provides real–time access to ERP systems to employees and partners (such as suppliers and customers). The ERP II role expands traditional ERP resource optimization and transaction processing. Rather than just manage buying, selling, etc.—ERP II leverages information in the resources under its management to help the enterprise collaborate with other enterprises.\nERP II is more flexible than the first generation ERP. Rather than confine ERP system capabilities within the organization, it goes beyond the corporate walls to interact with other systems. Enterprise application suite is an alternate name for such systems. ERP II systems are typically used to enable collaborative initiatives such as supply chain management (SCM), customer relationship management (CRM), and business intelligence (BI) among business partner organizations through the use of various e-business technologies.\n\nDevelopers now make more effort to integrate mobile devices with the ERP system. ERP vendors are extending ERP to these devices, along with other business applications. Technical stakes of modern ERP concern integration—hardware, applications, networking, supply chains. ERP now covers more functions and roles—including decision making, stakeholders' relationships, standardization, transparency, globalization, etc.\n\nERP systems typically include the following characteristics:\n\nAn ERP system covers the following common functional areas. In many ERP systems, these are called and grouped together as ERP modules:\n\nGovernment resource planning (GRP) is the equivalent of an ERP for the public sector and an integrated office automation system for government bodies. The software structure, modularization, core algorithms and main interfaces do not differ from other ERPs, and ERP software suppliers manage to adapt their systems to government agencies.\n\nBoth system implementations, in private and public organizations, are adopted to improve productivity and overall business performance in organizations, but comparisons (private vs. public) of implementations shows that the main factors influencing ERP implementation success in the public sector are cultural.\n\n\nMost ERP systems incorporate best practices. This means the software reflects the vendor's interpretation of the most effective way to perform each business process. Systems vary in how conveniently the customer can modify these practices. In addition, best practices reduced risk by 71% compared to other software implementations.\n\nUse of best practices eases compliance with requirements such as IFRS, Sarbanes-Oxley, or Basel II. They can also help comply with de facto industry standards, such as electronic funds transfer. This is because the procedure can be readily codified within the ERP software and replicated with confidence across multiple businesses who share that business requirement.\n\nERP systems connect to real–time data and transaction data in a variety of ways. These systems are typically configured by systems integrators, who bring unique knowledge on process, equipment, and vendor solutions.\n\nDirect integration—ERP systems have connectivity (communications to plant floor equipment) as part of their product offering. This requires that the vendors offer specific support for the plant floor equipment their customers operate. ERP vendors must be experts in their own products and connectivity to other vendor products, including those of their competitors.\n\nDatabase integration—ERP systems connect to plant floor data sources through staging tables in a database. Plant floor systems deposit the necessary information into the database. The ERP system reads the information in the table. The benefit of staging is that ERP vendors do not need to master the complexities of equipment integration. Connectivity becomes the responsibility of the systems integrator.\n\nEnterprise appliance transaction modules (EATM)—These devices communicate directly with plant floor equipment and with the ERP system via methods supported by the ERP system. EATM can employ a staging table, web services, or system–specific program interfaces (APIs). An EATM offers the benefit of being an off–the–shelf solution.\n\nCustom–integration solutions—Many system integrators offer custom solutions. These systems tend to have the highest level of initial integration cost, and can have a higher long term maintenance and reliability costs. Long term costs can be minimized through careful system testing and thorough documentation. Custom–integrated solutions typically run on workstation or server-class computers.\n\nERP's scope usually implies significant changes to staff work processes and practices. Generally, three types of services are available to help implement such changes—consulting, customization, and support. Implementation time depends on business size, number of modules, customization, the scope of process changes, and the readiness of the customer to take ownership for the project. Modular ERP systems can be implemented in stages. The typical project for a large enterprise takes about 14 months and requires around 150 consultants. Small projects can require months; multinational and other large implementations can take years. Customization can substantially increase implementation times.\n\nBesides that, information processing influences various business functions e.g. some large corporations like Wal-Mart use a just in time inventory system. This reduces inventory storage and increases delivery efficiency, and requires up-to-date data. Before 2014, Walmart used a system called Inforem developed by IBM to manage replenishment.\n\nImplementing ERP typically requires changes in existing business processes. Poor understanding of needed process changes prior to starting implementation is a main reason for project failure. The difficulties could be related to the system, business process, infrastructure, training, or lack of motivation.\n\nIt is therefore crucial that organizations thoroughly analyze business processes before they implement ERP software. Analysis can identify opportunities for process modernization. It also enables an assessment of the alignment of current processes with those provided by the ERP system. Research indicates that risk of business process mismatch is decreased by:\n\nERP implementation is considerably more difficult (and politically charged) in decentralized organizations, because they often have different processes, business rules, data semantics, authorization hierarchies, and decision centers. This may require migrating some business units before others, delaying implementation to work through the necessary changes for each unit, possibly reducing integration (e.g., linking via Master data management) or customizing the system to meet specific needs.\n\nA potential disadvantage is that adopting \"standard\" processes can lead to a loss of competitive advantage. While this has happened, losses in one area are often offset by gains in other areas, increasing overall competitive advantage.\n\nConfiguring an ERP system is largely a matter of balancing the way the organization wants the system to work with the way it was designed to work. ERP systems typically include many settings that modify system operations. For example, an organization can select the type of inventory accounting—FIFO or LIFO—to use; whether to recognize revenue by geographical unit, product line, or distribution channel; and whether to pay for shipping costs on customer returns.\n\nTwo-tier ERP software and hardware lets companies run the equivalent of two ERP systems at once: one at the corporate level and one at the division or subsidiary level. For example, a manufacturing company could use an ERP system to manage across the organization using independent global or regional distribution, production or sales centers, and service providers to support the main company’s customers. Each independent center or subsidiary may have its own business models, workflows, and business processes.\n\nGiven the realities of globalization, enterprises continuously evaluate how to optimize their regional, divisional, and product or manufacturing strategies to support strategic goals and reduce time-to-market while increasing profitability and delivering value. With two-tier ERP, the regional distribution, production, or sales centers and service providers continue operating under their own business model—separate from the main company, using their own ERP systems. Since these smaller companies' processes and workflows are not tied to main company's processes and workflows, they can respond to local business requirements in multiple locations.\n\nFactors that affect enterprises' adoption of two-tier ERP systems include:\n\nERP systems are theoretically based on industry best practices, and their makers intend that organizations deploy them \"as is\". ERP vendors do offer customers configuration options that let organizations incorporate their own business rules, but gaps in features often remain even after configuration is complete.\n\nERP customers have several options to reconcile feature gaps, each with their own pros/cons. Technical solutions include rewriting part of the delivered software, writing a homegrown module to work within the ERP system, or interfacing to an external system. These three options constitute varying degrees of system customization—with the first being the most invasive and costly to maintain. Alternatively, there are non-technical options such as changing business practices or organizational policies to better match the delivered ERP feature set. Key differences between customization and configuration include:\n\n\nCustomization advantages include that it:\n\nCustomization disadvantages include that it may:\n\nERP systems can be extended with third–party software, often via vendor-supplied interfaces. Extensions offer features such as:\n\nData migration is the process of moving, copying, and restructuring data from an existing system to the ERP system. Migration is critical to implementation success and requires significant planning. Unfortunately, since migration is one of the final activities before the production phase, it often receives insufficient attention. The following steps can structure migration planning:\n\n\nOften, data migration is incomplete because some of the data in the existing system is either incompatible or not needed in the new system. As such, the existing system may need to be kept as an archived database to refer back to once the new ERP system is in place.\n\nThe most fundamental advantage of ERP is that the integration of myriad business processes saves time and expense. Management can make decisions faster and with fewer errors. Data becomes visible across the organization. Tasks that benefit from this integration include:\n\nERP systems centralize business data, which:\n\n\n\nThe term \"postmodern ERP\" was coined by Gartner in 2013, when it first appeared in the paper series \"Predicts 2014\". According to Gartner's definition of the postmodern ERP strategy, legacy, monolithic and highly customized ERP suites, in which all parts are heavily reliant on each other, should sooner or later be replaced by a mixture of both cloud-based and on-premises applications, which are more loosely coupled and can be easily exchanged if needed.\n\nThe basic idea is that there should still be a core ERP solution that would cover most important business functions, while other functions will be covered by specialist software solutions that merely extend the core ERP. This concept is similar to the so-called best-of-breed approach to software implementation, but it shouldn't be confused with it. While in both cases, applications that make up the whole are relatively loosely connected and quite easily interchangeable, in the case of the latter there is no ERP solution whatsoever. Instead, every business function is covered by a separate software solution.\n\nThere is, however, no golden rule as to what business functions should be part of the core ERP, and what should be covered by supplementary solutions. According to Gartner, every company must define their own postmodern ERP strategy, based on company's internal and external needs, operations and processes. For example, a company may define that the core ERP solution should cover those business processes that must stay behind the firewall, and therefore, choose to leave their core ERP on-premises. At the same time, another company may decide to host the core ERP solution in the cloud and move only a few ERP modules as supplementary solutions to on-premises.\n\nThe main benefits that companies will gain from implementing postmodern ERP strategy are speed and flexibility when reacting to unexpected changes in business processes or on the organizational level. With the majority of applications having a relatively loose connection, it is fairly easy to replace or upgrade them whenever necessary. In addition to that, following the examples above, companies can select and combine cloud-based and on-premises solutions that are most suited for their ERP needs. The downside of postmodern ERP is that it will most likely lead to an increased number of software vendors that companies will have to manage, as well as pose additional integration challenges for the central IT.\n\n"}
{"id": "8285969", "url": "https://en.wikipedia.org/wiki?curid=8285969", "title": "Entrepreneur in Residence", "text": "Entrepreneur in Residence\n\nEntrepreneur in Residence or Executive in Residence is a kind of position in venture capital firms, law firms or business schools that is usually temporary and not formal. This is when an institution brings in an entrepreneur, who is usually in the process of starting or expanding his/her new company.\n\nIn a Venture Capital firm the entrepreneur is funded to work with some of the investment team of the company as well as being exposed to business-building processes, where he/she can assist the firm with any existing portfolio. In addition he/she is expected to help the company evaluate new potential investments, especially if it is an area of the E.I.R's experience. \nOn the other hand, the company usually benefits from this by getting significant access to the new company started by the E.I.R. This is due to the fact that the company gets to, usually, be the first investor in E.I.R's new company, giving them a chance to influence some company decisions.\n\nIn a law firm scenario, the entrepreneur is expected to provide services to the firm’s clients who are also involved in entrepreneur activity. \nFor a firm having an entrepreneur who can interact with other entrepreneurs and advise them is a way of gaining client trust by helping them with business decisions and networks.\nIn addition Law firms can also have the previous E.I.Rs as their clients once they start up their company. This is an easy and accessible clientele with whom the companies had previous relationships. \nThis is a very beneficial relationship for the E.I.R, who will develop relationships with several other entrepreneurs, that could be potential partners in the future. The E.I.R gets a chance to show his/her knowledge and understanding to the law firm's client and will develop further relationships with any partners of the client's partner as well.\nWhile in a Business school, an E.I.R is expected to provide guidance to business students who are intending to go through the same path as those who are helping them. The type of nurturing an E.I.R can provide to a \nbusiness school environment helps students, professors or whoever wishes to start a business within the institution environment. The E.I.R helps them develop ideas, organize them and follow the right path towards making these ideas reality.\nE.I.Rs are usually interested in these types of opportunities for several reasons, including expanding their own CV through teaching and influence as well as developing relationships with future business owners who could be future partners of the E.I.R.\n\n"}
{"id": "14371951", "url": "https://en.wikipedia.org/wiki?curid=14371951", "title": "Executive summary", "text": "Executive summary\n\nAn executive summary, or management summary, is a short document or section of a document, produced for business purposes, that summarizes a longer report or proposal or a group of related reports in such a way that readers can rapidly become acquainted with a large body of material without having to read it all. It usually contains a brief statement of the problem or proposal covered in the major document(s), background information, concise analysis and main conclusions. It is intended as an aid to decision-making by managers and has been described as the most important part of a business plan.\n\nAn executive summary differs from an abstract in that an abstract will usually be shorter and is typically intended as an overview or orientation rather than being a condensed version of the full document. Abstracts are extensively used in academic research where the concept of the executive summary is not in common usage. \"An abstract is a brief summarizing statement... read by parties who are trying to decide whether or not to read the main document\", while \"an executive summary, unlike an abstract, is a document in miniature that may be read in place of the longer document\".\n\nThere is general agreement on the structure of an executive summary - books and training courses emphasise similar points. Typically, an executive summary will:\n\nExecutive summaries are important as a communication tool in both academia and business. For example, members of Texas A&M University Department of Agricultural Economics observes that \"An executive summary is an initial interaction between the writers of the report and their target readers: decision makers, potential customers, and/or peers. A business leader’s decision to continue reading a certain report often depends on the impression the executive summary gives.\"\n\nIt has been said that, by providing an easy digest of an often complex matter, an executive summary can lead policy makers and others to overlook important issues. Prof. Amanda Sinclair of the University of Melbourne has argued that this is often an active rather than a passive process. In one study, centred on globalization, she found that policy makers face \"pressures to adopt a simple reading of complex issues\" and \"to depoliticise and universalize all sorts of differences\". She claims that \"all research was framed under pre-defined and generic headings, such as business case points. The partners' reports were supposed to look the same. The standardization of research occurred via vehicles such as executive summaries: “executives only read the summaries” we were told”. Similarly Colin Leys, writing in The Socialist Register, argues that executive summaries are used to present dumbed down arguments: \"there is remarkably little adverse comment on the steep decline that has occurred since 1980 in the quality of government policy documents, whose level of argumentation and use of evidence is all too often inversely related to the quality of their presentation (in the style of corporate reports, complete with executive summaries and flashy graphics).\"\n\n\n"}
{"id": "49975329", "url": "https://en.wikipedia.org/wiki?curid=49975329", "title": "Expert as a service", "text": "Expert as a service\n\nExpert-as-a-Service (ExaaS) is an online delivery model of human consulting and/or automated knowledge transfer. Similarly to SaaS (Software as a service) the concept of ExaaS comes from the remote nature of processing data and/or providing service. The user of ExaaS accesses the service through a webpage, VoiP, IM, etc. or any related mobile app. Typical ExaaS providers are consultants, lawyers, physicians, teachers etc. \nBiztech Magazine claims that \"...Everything as a service...Clouding Will Disrupt the World...\" and according to Business2Customers magazine there are \"...3 Reasons Why Experts-as-a-Service is the Future of Consulting...\"\n\nExaaS is the general term for newer consulting or expertise based online services like CaaS (Consulting-as-a-Service) or JaaS (Justice-as-a-Service) \n\nCaaS companies or individuals deliver online consulting services (The Consulting-as-a-Service model (CONaaS) by Luke Marson, sales diagnostics by Miklos Kadar, \nIntroducing Executive As A Service by GconnTec )\n\nAccording to BusinessInsider JaaS companies \"...Find You Money You Didn't Know You Were Owed...\" by providing legal or consumer rights services. And according to Henrik Zillmer \"...There’s a new wave of customer empowerment coming...\" \nHe lists honourable mentions of JaaS services like Paribus, Airhelp, Fixed, BillFixers, 71lbs )\n"}
{"id": "51171662", "url": "https://en.wikipedia.org/wiki?curid=51171662", "title": "Flok (company)", "text": "Flok (company)\n\nflok (formerly Loyalblocks) is an American tech startup based in New York City that provides marketing solutions such as chatbots/AI, customer loyalty programs, mobile apps and CRM services to local businesses.\n\nIn January 2017, the company was acquired by Wix.com.\n\nflok was founded in 2011 by Ido Gaver and Eran Kirshenboim and has offices in Tel Aviv, Israel. In May 2013, flok secured a $9 million Series A Round from General Catalyst Partners with participation from Founder Collective and existing investor Gemini Israel Ventures. In total, flok has raised over $18 million in venture capital in three rounds.\n\nIn May 2014, flok announced a self-service loyalty platform for SMBs to build their own programs with beacon integration. At that time, approximately 40,000 businesses were using the service. In 2016, flok released a turnkey chatbot service for local businesses, and was featured in AdWeek for developing the first \"weed bot\" chatbot for a California cannabis business. \n\nflok offers an eponymous customer-facing app that consumers use to receive rewards and deals form partner businesses, and a flok business app for merchants to manage the platform. Both are available for iOS and Android operating systems. flok's main products center around customer loyalty and rewards. The platform offers a digital punch card, proximity marketing, push messaging and CRM services in addition to its chatbots and AI features.\n\nflok provides partner businesses with beacons that can locate customers and verify store visits. As of July 2016, flok had a 4-star rating on Merchant Maverick review site.\n"}
{"id": "32419826", "url": "https://en.wikipedia.org/wiki?curid=32419826", "title": "Forged endorsement", "text": "Forged endorsement\n\nForged endorsement is a legal term used to describe a fraudulent payment. For example, someone may write a cheque with a forged signature. In this case the forged signature makes the endorsement fraudulent. Forging endorsements can be use to prevent the person or legal entity that the payment is made out to from being able to receive its value (such as cashing a cheque).\n\nIf an instrument is endorsed in full, it cannot be endorsed or negotiated except by an endorsement signed by the person to whom or to whose order the instrument is payable. Thus, if such an instrument is negotiated by way of a forged endorsement, the endorsee will not acquire the title even though he be a purchaser for value and in good faith, because the endorsement is nullity. But where the instrument has been endorsed in blank, it can be negotiated by mere delivery and the holder derives his title independent of the forged endorsement and can claim the amount from any of the parties to the instrument.\n\nA bill is endorsed, “payable to X or order”. X endorses it in blank and it comes into the hands of Y, who simply delivers it to A. A would then forge Y's endorsement and transfers it to B. B, as the holder, does not derive his title through the forged endorsement to Y, but through the genuine endorsement of X and can claim payment from any of the parties to the instrument in spite of the intervening forged endorsement.\n"}
{"id": "2881760", "url": "https://en.wikipedia.org/wiki?curid=2881760", "title": "Horeca", "text": "Horeca\n\nHoreca (also HoReCa, HORECA) is an abbreviation for the food service industry. The term is a syllabic abbreviation of the words Hotel/Restaurant/Café.\n\nThe Dutch \"Uniforme Voorwaarden Horeca\" (UVH) is translated into English as Uniform Conditions for the Hotel and Catering Industry. This code covers hotels, bars, restaurants and related businesses in the Netherlands. \"Koninklijk Horeca Nederland\" is the Dutch trade association for the hotel and catering industry.\n\nThis sector is one of the fastest growing in Europe. In 2004, more than 7.8 million people were employed and the sector generated more than $338 billion turnover. Jobs tend to be temporary, with irregular hours, low pay, and few career prospects. There is a high proportion of young people working in the sector. Some distribution companies use this term to define the food & beverage service trade channel or the hospitality trade channel.\n\nSome tobacco companies use this term to define the food & beverage service trade channel or the hospitality trade channel.\n\n"}
{"id": "58962344", "url": "https://en.wikipedia.org/wiki?curid=58962344", "title": "IFSA Network", "text": "IFSA Network\n\nThe IFSA Network (previously known as the International Finance Students Association) is a global non-profit organization entirely run by students. Its main aim is to give students in the fields of finance, economics, management and engineering, a path to enter the professional world of finance by providing them with lectures, workshops, competitions and connecting them through a global network.\n\nThe IFSA Network was founded in 2014 in Rotterdam, at the Rotterdam School of Management. Its first focus was to regroup people studying Finance, and connect them both between themselves and established financial actors. It quickly spread across Europe and Asia, before opening chapters in South Africa and the Americas.\n\nThe IFSA Network is composed of different chapters throughout the world. The concept is to have semi-independent units, each composed of their own board & active members and to create synergy between these in order to provide the best opportunities to their members. The objective of the association is to build a comprehensive network that connects finance students across the globe and enable them to interact and communicate with each other.\n\nEach chapter has a board, composed of a Chairman, Vice-Chairman, Treasure and General Secretary. This board manages day to day operations locally and supervises its regional activities. Chapters also typically have departments (Finance, Marketing, Events), with a department head managing the analysts.\nChairmen also regularly connect to establish international strategies and overview national operations.\n\nAs of 2017, The IFSA Network has the following chapters:\n\nRotterdam School of Management\n\nOxford University\n\nCambridge University\n\nUniversity College London\n\nLondon School of Economics\n\nWarwick University\n\nImperial College, London\n\nEcole Polytechnique\n\nHEC Paris\n\nESSEC\n\nEcole Centrale Paris\n\nSupelec University\n\nENSAE\n\nSciences Po University\n\nEcole des Ponts\n\nEcole des Mines\n\nDauphine University\n\nTelecom University\n\nESCP Europe.\n\nUniversity of Geneva\n\nUniversity of Sankt-Gallen\n\nUniversity of Pavia\n\nUniversità Commerciale Luigi Bocconi\n\nNew Economic School\n\nTechnical University\n\nLudwig Maximilian University\n\nGoethe University\n\nHarvard University\n\nColumbia University\n\nFederal University of Rio de Janeiro\n\nUniversity of Cape Town\n\nKyoto University\n\nChinese University of Hong-Kong\n\nShaheed Sukhdev College of Business Studies\n\nThe IFSA Trader’s Cup is the world’s largest student run trading challenge . All people studying in a recognised university are free to participate. The competition relies on securities trading, and aims to give students an opportunity to distinguish themselves from the competition. The competition is held once per academic year, starting in spring of 2016. For the second edition of the competition, students from more than 35 universities, located in 28 countries on 6 continents competed.\n\nThe Global Case Competition at Harvard is the world’s most prestigious case competition organized by students. Its goal is to regroup the best students in finance & economics throughout the world for the length of a few days, in order to test their knowledge and skills and offer them a fantastic opportunity to meet like-minded students, world leading institutions, and renowned guests. The competition is a unique opportunity for participants, connecting the worlds smartest academics from the U.S.A., Europe, Asia, Latin America and Africa. Taking part in this event is also a great moment in which people can interact with the invited companies , financial institutions, guests and partners. For the first two editions this event was organized with the Harvard Extension Business Society, an official Harvard Club.\nDuring the first edition, a team from HEC Paris, a leading French university, won the challenge.\nThe second edition was won by the London Business School.\n\nThe various IFSA Network chapters try to offer their communities various local events to connect with the professional world. Often, an industry professional will come to the IFSA Network events and meet students. The purpose of those venues is to give students a taste of the professional world, share tips and knowledge, and give an opportunity to network.\n\nIn 2018, the IFSA Network in cooperation with the Boston Consulting Group and InvestSoc, organised the first, three-week long International Cape Town Case Competition. The competition has a unique format where teams consisting of 2 to 4 participants, propose solutions for crises that are presented to them. In their solutions, participants are able to showcase their knowledge, creativity and problem-solving skills on a current macroeconomic issue. \nA jury picks the 15 best international teams and the best local teams, from Cape Town. The selected teams arrive in Cape Town, South Africa, to participate in the second round, and present in front of a select panel of distinguished professors and practitioners. The winning team is awarded a cash prize of 5000 South African Rand.\n\nThe IFSA Network is very selective when it comes to opening chapters and selecting board members. Striving for excellency, it will only open chapters in elite universities, and hires only the most ambitious and brightest students. The purpose of this is to ensure smooth operations and maintain a high-level of execution in all of the organisation’s activities. Ultimately, the goal is to have establishments around the world, regrouping all major academic institutions.\n"}
{"id": "13889958", "url": "https://en.wikipedia.org/wiki?curid=13889958", "title": "Incremental operating margin", "text": "Incremental operating margin\n\nIncremental operating margin is the increase or decrease of income from continuing operations before stock-based compensation, interest expense and income-tax expense between two periods, divided by the increase or decrease in revenue between the same two periods.\n"}
{"id": "252173", "url": "https://en.wikipedia.org/wiki?curid=252173", "title": "Industrial relations", "text": "Industrial relations\n\nIndustrial relations or employment relations is the multidisciplinary academic field that studies the employment relationship; that is, the complex interrelations between employers and employees, labor/trade unions, employer organizations and the state. \n\nThe newer name, \"employment relations\" is increasingly taking precedence because \"industrial relations\" is often seen to have relatively narrow connotations. Nevertheless, industrial relations has frequently been concerned with employment relationships in the broadest sense, including \"non-industrial\" employment relationships. This is sometimes seen as paralleling a trend in the separate but related disciple of human resource management.\n\nWhile some scholars regard or treat industrial/employment relations as synonymous with employee relations and labour relations, this is controversial, because of the narrower focus of employee/labour relations, i.e. on employees or labour, from the perspective of employers, managers and/or officials. In addition, employee relations is often perceived as dealing only with non-unionized workers, whereas labour relations is seen as dealing with , i.e unionized workers. Some academics, universities and other institutions regard human resource management as synonymous with one or more of the above disciplines, although this too is controversial.\n\nIndustrial relations examines various employment situations, not just ones with a unionized workforce. However, according to Bruce E. Kaufman, \"To a large degree, most scholars regard trade unionism, collective bargaining and labour–management relations, and the national labour policy and labour law within which they are embedded, as the core subjects of the field.\"\n\nInitiated in the United States at end of the 19th century, it took off as a field in conjunction with the New Deal. However, it is generally regarded as a separate field of study only in English-speaking countries, having no direct equivalent in continental Europe. In recent times, industrial relations has been in decline as a field, in correlation with the decline in importance of trade unions and also with the increasing preference of business schools for the human resource management paradigm.\n\nIndustrial relations has three faces: science building, problem solving, and ethical. In the science building phase, industrial relations is part of the social sciences, and it seeks to understand the employment relationship and its institutions through high-quality, rigorous research. In this vein, industrial relations scholarship intersects with scholarship in labour economics, industrial sociology, labour and social history, human resource management, political science, law, and other areas.\n\nIndustrial relations scholarship assumes that labour markets are not perfectly competitive and thus, in contrast to mainstream economic theory, employers typically have greater bargaining power than employees. Industrial relations scholarship also assumes that there are at least some inherent conflicts of interest between employers and employees (for example, higher wages versus higher profits) and thus, in contrast to scholarship in human resource management and organizational behaviour, conflict is seen as a natural part of the employment relationship. Industrial relations scholars therefore frequently study the diverse institutional arrangements that characterize and shape the employment relationship—from norms and power structures on the shop floor, to employee voice mechanisms in the workplace, to collective bargaining arrangements at company, regional, or national level, to various levels of public policy and labour law regimes, to varieties of capitalism (such as corporatism, social democracy, and neoliberalism).\n\nWhen labour markets are seen as imperfect, and when the employment relationship includes conflicts of interest, then one cannot rely on markets or managers to always serve workers' interests, and in extreme cases to prevent worker exploitation. Industrial relations scholars and practitioners, therefore, support institutional interventions to improve the workings of the employment relationship and to protect workers' rights. The nature of these institutional interventions, however, differ between two camps within industrial relations. The pluralist camp sees the employment relationship as a mixture of shared interests and conflicts of interests that are largely limited to the employment relationship. In the workplace, pluralists, therefore, champion grievance procedures, employee voice mechanisms such as works councils and trade unions, collective bargaining, and labour–management partnerships. In the policy arena, pluralists advocate for minimum wage laws, occupational health and safety standards, international labour standards, and other employment and labour laws and public policies. These institutional interventions are all seen as methods for balancing the employment relationship to generate not only economic efficiency but also employee equity and voice. In contrast, the Marxist-inspired critical camp sees employer–employee conflicts of interest as sharply antagonistic and deeply embedded in the socio-political-economic system. From this perspective, the pursuit of a balanced employment relationship gives too much weight to employers' interests, and instead deep-seated structural reforms are needed to change the sharply antagonistic employment relationship that is inherent within capitalism. Militant trade unions are thus frequently supported.\n\nIndustrial relations has its roots in the industrial revolution which created the modern employment relationship by spawning free labour markets and large-scale industrial organizations with thousands of wage workers. As society wrestled with these massive economic and social changes, labour problems arose. Low wages, long working hours, monotonous and dangerous work, and abusive supervisory practices led to high employee turnover, violent strikes, and the threat of social instability. Intellectually, industrial relations was formed at the end of the 19th century as a middle ground between classical economics and Marxism, with Sidney Webb and Beatrice Webb's \"Industrial Democracy\" (1897) being a key intellectual work. Industrial relations thus rejected the classical econ.\n\nInstitutionally, industrial relations was founded by John R. Commons when he created the first academic industrial relations program at the University of Wisconsin in 1920. Another scholarly pioneer in industrial relations and labour research was Robert F. Hoxie. Early financial support for the field came from John D. Rockefeller Jr. who supported progressive labour–management relations in the aftermath of the bloody strike at a Rockefeller-owned coal mine in Colorado. In Britain, another progressive industrialist, Montague Burton, endowed chairs in industrial relations at the universities of Leeds, Cardiff, and Cambridge in 1929–1930.\n\nBeginning in the early 1930s there was a rapid increase in membership of trade unions in the United States, and with that came frequent and sometimes violent labour–management conflict. During the Second World War these were suppressed by the arbitration powers of the National War Labor Board.\n\nHowever, as the Second World War drew to a close and in anticipation of a renewal of labour–management conflict after the war, there was a wave of creations of new academic institutes and degree programs that sought to analyse such conflicts and the role of collective bargaining. The most known of these was the Cornell University School of Industrial and Labor Relations, founded in 1945. But counting various forms, there were over seventy-five others. These included the Yale Labor and Management Center, directed by E. Wight Bakke, which began in 1945. An influential industrial relations scholar in the 1940s and 1950s was Neil W. Chamberlain at Yale and Columbia universities. \n\nIn the 1950s, industrial relations was formalized as a distinct academic discipline with the emergence in the UK of the so-called \"Oxford school\", including , Hugh Clegg, and Alan Fox, Lord William McCarthy, Sir George Bain (all of whom taught at Nuffield College, Oxford), as well as Otto Kahn-Freund (Brasenose College, Oxford).\n\nIndustrial relations was formed with a strong problem-solving orientation that rejected both the classical economists' \"laissez-faire\" solutions to labour problems and the Marxist solution of class revolution. It is this approach that underlies the New Deal legislation in the United States, such as the National Labor Relations Act and the Fair Labor Standards Act.\n\nBy the early 21st century, the academic field of industrial relations was often described as being in crisis. In academia, its traditional positions are threatened on one side by the dominance of mainstream economics and organizational behaviour, and on the other by postmodernism. \n\nIn policy-making circles, the industrial relations emphasis on institutional intervention is trumped by a neoliberal emphasis on the \"laissez-faire\" promotion of free markets. In practice, trade unions are declining and fewer companies have industrial relations functions. The number of academic programs in industrial relations is therefore shrinking, while fields such as human resource management and organizational behaviour grow. The importance of this work, however, is stronger than ever, and the lessons of industrial relations remain vital. The challenge for industrial relations is to re-establish these connections with the broader academic, policy, and business worlds.\n\nIndustrial relations scholars such as Alan Fox have described three major theoretical perspectives or frameworks, that contrast in their understanding and analysis of workplace relations. The three views are generally known as unitarism, pluralism, and the radical or critical school. Each offers a particular perception of workplace relations and will, therefore, interpret such events as workplace conflict, the role of unions and job regulation differently. The perspective of the critical school is sometimes referred to as the \"conflict model\", although this is somewhat ambiguous, as pluralism also tends to see conflict as inherent in workplaces. Radical theories are strongly identified with Marxist theories, although they are not limited to these.\n\nIn pluralism, the organization is perceived as being made up of divergent sub-groups, each with its own legitimate interests and loyalties and with their own set of objectives and leaders. In particular, the two predominant sub-groups in the pluralist perspective are the management and trade unions. The pluralist perspective also supports that conflict is inherent in dealing with industrial relations since different sub-groups have different opinions in the day-to-day operations.\n\nConsequently, the role of management would lean less towards enforcing and controlling and more toward persuasion and coordination. Trade unions are deemed as legitimate representatives of employees, conflict is dealt by collective bargaining and is viewed not necessarily as a bad thing and, if managed, could, in fact, be channeled towards evolution and positive change. It is the opposite of the unitary approach, there are different the group within the environment. Hence, the interest of employers and employee are divergent. The employers want to maximize profit and employees want to enjoy social benefits in the form of increased wages, conducive environment. Therefore, conflict is inevitable and the need for the trade union to protect the interest of both parties. Also, there is dual authority/loyalty in this approach. Therefore, employees are loyal to the management as well as their labour leaders.\n\nIn unitarism, the organization is perceived as an integrated and harmonious whole with the idea of \"one happy family\" in which management and other members of the staff all share a common purpose by emphasizing mutual co-operation. Furthermore, unitarism has a paternalistic approach: it demands loyalty of all employees and is managerial in its emphasis and application. Consequently, trade unions are deemed as unnecessary since the loyalty between employees and organizations are considered mutually exclusive, and there cannot be two sides of industry. Conflict is perceived as the result of poor management.\n\nThis view of industrial relations looks at the nature of the capitalist society, where there is a fundamental division of interest between capital and labour, and sees workplace relations against this background. This perspective sees inequalities of power and economic wealth as having their roots in the nature of the capitalist economic system. Conflict is therefore seen as a natural outcome of capitalism, thus it is inevitable and trade unions are a natural response of workers to their exploitation by capital. Whilst there may be periods of acquiescence, the Marxist view would be that institutions of joint regulation would enhance rather than limit management's position as they presume the continuation of capitalism rather than challenge it.\n"}
{"id": "24200863", "url": "https://en.wikipedia.org/wiki?curid=24200863", "title": "Local information systems", "text": "Local information systems\n\nA Local information system (LIS) is a form of information system built with business intelligence tools, designed primarily to support geographic reporting. They overlap with some capabilities of geographic information systems (GIS), although their primary function is the reporting of statistical data rather than the analysis of geospatial data. LIS also tend to offer some common knowledge management functionality for storage and retrieval of unstructured data such as documents. They deliver functionality to load, store, analyse and present statistical data that has a strong geographic reference. In most cases the data is structured as indicators and is linked to discrete geographic areas, for example population figures for US counties or numbers claiming unemployment benefit across wards in England. The ability to present this data using data visualization tools like charts and maps is also a core feature of these systems.\n\nThe term \"LIS\" has emerged since 2004, primarily in the UK public sector. To date it is not widely used elsewhere although other terms like Community Information Systems apply to solutions, primarily in North America, that have a great deal of overlap. Another widely used and largely synonymous term is Data Observatory. Data Observatory is a more widely used term internationally particularly within the area of public health where sites which often include this type of statistical reporting application are often termed a health observatory.\n\nThe primary application for LIS is to provide a place-focused evidence base that is easily accessible to a wide range of users including data experts, managers, policy makers, front-line staff and citizens. They provide a wide range of statistics and reports allowing users to review the current evidence base and build a picture of localities and neighbourhoods for their area of interest. LIS are commonly used by partnerships where they need to come together to provide joined-up services for a common area. The ability to have a common evidence base and a platform to share sensitive and non-sensitive data is critical in this situation. LIS enables partners to publish a wide range of indicators in the form of defined outputs which combine locally and nationally available data into more meaningful intelligence aimed at specific user groups\n\nIn the UK, like many other countries, there has been a rapid growth in the availability of small area statistics. National Neighbourhood Statistics projects across the UK, set up as a result of the PAT 18 report, have opened up access to a wide range of government small area based statistics. This has been accompanied by a gradual shift across the public sector, a shift that remains very much on-going, towards the recognition that policy and decisions should be influenced to a greater degree by evidence. There also continues to be a growing acceptance that some services can be more effectively delivered by targeting resources at specific areas of need − the idea of high demand 'hotspots'. This relies on having reliable and detailed data about the needs of customers, in this case citizens, and where they live, work and take leisure time. \n\nIn England in particular this has led to a rapid increase in the number of Local Information Systems particularly within local Authorities and Local Strategic Partnerships. This development has been actively supported by the Department of Communities and Local Government (CLG) under their ‘Neighbourhood Renewal’ agenda. A national research project was funded to identify examples and disseminate best practice – this reported publicly in 2004 and led to a more formal report being published in 2006. CLG’s role as a catalyst in this area is further re-enforced through its provision of Neighbourhood Renewal Funds (NRF) - this funding was used by a number of authorities to pump-prime their initial LIS developments.\n\nAn initiative is currently on-going through the CLG Information Management Programme to coordinate all LIS activity across local government and partnerships. This has led to regular national LIS meetings and a dedicated LIS forum. To date it is estimated that approximately 50 per cent of top tier authorities in England now have some form of LIS. In some cases these have been built as bespoke solutions, in other cases they are based on off-the-shelf products. Elsewhere within the UK this figure is lower although an initiative has been launched in Scotland in 2009 resulting in a Scottish LIS Toolkit to complement the English version.\n\nThe range of data managed within a LIS can be wide and classified in many different ways. Most common is some form of domain specific classification where indicators are grouped into top level categories like ‘Demography’, ‘Health and Welfare’, ‘Crime and Community Safety’, ‘Education and Children's Services’, ‘Environment’ and ‘Economy’. There may also be cross-cutting themes such as ‘Performance’ and ‘Social Disadvantage’. In the UK key government data sources include ONS Neighbourhood Statistics, CLG, Dept for Work and Pensions, NOMIS, Audit Commission and several areas of NHS information services. However the real value of LIS is their ability to combine national data with local data available from a wide range of internal business systems including those of partners. This local data is often not provided to central government and, even when it is, it tends to be in a form that limits its value.\n\n\n"}
{"id": "9691901", "url": "https://en.wikipedia.org/wiki?curid=9691901", "title": "NxTier", "text": "NxTier\n\nNxTier, is an ASP company that provided software solutions to the logistics and supply chain industry. Using their VSC platform, the industry has visibility and control of their data virtually anywhere in the world at any time. The VSC platform is an ASP application deployed over the Internet.\nThe platform allows logistic and supply chain industry providers to schedule and dispatch transportation management system, accept and transmit EDI, provide Web tracking functionality, manage orders with the OMS , perform business analytics and consolidate and optimize orders and loads.\n\nThe system was originally created specifically for the third-party logistics (3PL) market. Later it was quickly adopted by the specialized delivery, home delivery and the less than truckload (LTL) markets. Today it has furthered it reach to full truckload carrier market, the service industry; dispatch and scheduling, retail build out, appliance and furniture as well as retail replenishment. As the internet became a more reliable platform NxTier positioned its VSC platform to compete in the EDI space to reduce or eliminate value added network (VAN) based fees.\nNxtier VSC (Visibility, Synchronization, Control) platform provides multiple components of an ERP system. It also provides interfaces to other platforms and systems.\n\nThe platform is Internet-based, ERP and can be accessed with any internet connection.\n\nFounded in 1994, in Worcester, Massachusetts, to help the transportation industry increase visibility into the supply chain by giving them greater visibility and Control over their IT functions and data.\n\n"}
{"id": "27380685", "url": "https://en.wikipedia.org/wiki?curid=27380685", "title": "Open Angel Forum", "text": "Open Angel Forum\n\nThe Open Angel Forum is a recurring event designed to aid early stage startup companies seeking funding, via presentations to prospective angel investors.\n\nLaunched in 2009 by tech entrepreneur and media figure Jason Calacanis, the event was the culmination of a series of public comments by Calacanis questioning the ethics of groups who charge fledgling companies to present to wealthy investors. Calacanis, already co-founder of startup competition TechCrunch50 with Michael Arrington (which ran from 2007-2009), had spoken out repeatedly against specific groups he characterized as preying on small, hopeful companies seeking financing.\n\nThe conversation evolved quickly over the course of several days, beginning with a blog post calling for \"Jihad\" against the Keiretsu Forum, a discussion on his online broadcast This Week in Startups, and two impassioned polemics outlining the impetus for creating an alternative model: the first self-described as \"declaring war\" and \"written with boiling blood\" quickly followed by a second containing quotes from other prominent tech industry pundits such as Fred Wilson and Brad Feld as well as anonymous individuals claiming to be formerly affiliated or familiar with the groups in question.\n\nUpon formalizing the organization, several principles were outlined regarding qualifications and structure, the most notable legacy of the initial controversy being a strict rule against fees for both presenters and investors for participation. Startups presenting at the Open Angel Forum are picked by the event's organizers (which include regional \"chapter heads\" as well). Six companies are chosen and each presents for 10 minutes. Angel investors must apply for admission as well, and required to be active, with at least four investments in the past year. The first event was held on January 14, 2009 in Los Angeles and to date events have been held in six cities: Los Angeles, San Francisco, Palo Alto, Boston, Boulder, CO and New York City.\n\nThough news reports and blog postings by participating companies, as well as the organization itself, have touted funding offers following the events, the group does not maintain a comprehensive list of presenters, nor have they announced the success rate or other metrics as to the efficacy of the program as a source for startup capital for participants.\n"}
{"id": "28192984", "url": "https://en.wikipedia.org/wiki?curid=28192984", "title": "Partner relationship management", "text": "Partner relationship management\n\nPartner relationship management (PRM) is a system of methodologies, strategies, software, and web-based capabilities that help a vendor to manage partner relationships. The general purpose of PRM is to enable vendors to better manage their partners through the introduction of reliable systems, processes and procedures for interacting with them. Web-based PRM systems typically include a Content Management System, a partner and customer contact database, and the notion of a partner portal which allows partners to log in and interact with a vendor's sales opportunity database and obtain product, pricing, and training information. There are a number of solution providers who offer PRM software companies who rely heavily on a PRM solution to stay relevant in their respective industries.\n\nVendors who implement a PRM solution are typically motivated by a need to reduce the financial overhead and establish new partnerships to drive channel revenue and scale. Partners may also be integrators or managed service providers. Unlike CRM systems, which are tailored toward getting an end customer to purchase from you, a PRM system is focused on getting a partner to sell on your behalf. As a result, they commonly offer web-based self-service tools, information, and resources to partner resellers. Tools often include:\n\nGartner reports that PRM solutions have mainly been adopted by companies in the hardware technology, software technology, telecommunication, and manufacturing industries.\n\nThe PRM application market has expanded significantly in the last 10 years, with vendors offering improved end-to-end and point solutions for the management of channel sales partners.\n\n"}
{"id": "38358855", "url": "https://en.wikipedia.org/wiki?curid=38358855", "title": "Process capital", "text": "Process capital\n\nProcess capital is the value to an enterprise which is derived from the techniques, procedures, and programs that implement and enhance the delivery of goods and services. Process capital is one of the three components of structural capital, itself a component of intellectual capital. Process capital can be seen as the value of processes to any entity, whether for profit or not-for profit, but is most commonly used in reference to for-profit entities.\n\nA process comprises a \"series or network of value-added activities, performed by their relevant roles or collaborators, to purposefully achieve the common business goal.\n\nProcess capital can be created and enhanced by using business process mapping, business process modeling and business process management \n\nOrganisations invest in process capital in order to build a company’s unique infrastructure for achieving operational and strategic goals. Given the dynamics of industry and technology, the development of process capital evolves and interacts with environmental changes. Organisations have invested in information technology (IT) and organisational change programs to build process capital for achieving business excellence through customer satisfaction. The vast investments include: IT infrastructure implementation, quality-improvement projects, process-redesign projects, and various process integration projects. Although process capital plays an important role in organising resources, processing information, interacting with stakeholders, and delivering organisational values, few studies have discussed its specific content, and it is rare to focus attention on the level of its management. Instead, process capital has usually been hidden in the measurement of IT investment or organisational intellectual capital as an intangible element of organisational assets. Failure to treat process capital as a separate and unique management issue is widespread among both businesses and researchers because most of the systems and processes within the organisations are interdependent. Therefore, a systematic approach to measuring process capital is necessary in order to manage process potential to its full extent.\n\nCapital is something owned which provides ongoing services. In the national accounts, or to firms, capital is made up of durable investment goods, normally summed in units of money. Process capital, in practice, embraces the practical knowledge of operations, techniques, and employee programs in the effort to extend and enhance the efficiency of manufacturing or the delivery of products and services for long-term value.\n\nFailure to treat process capital as a separate and unique management issue is widespread among both businesses and researchers because most of the systems and processes within the organisations are interdependent. Therefore, a systematic approach to measuring process capital is necessary in order to manage process potential to its full extent.\n\nProcess capital is essential for strategy development and implementation. Business processes are large with technology, location and other factors combining to generate limitless possibilities. Throughout the process of developing and appropriating technology-enabled processes, collective brainpower is formalised, captured, and leveraged to produce an asset of higher value and affect organisational performance in all aspects.\n\nOrganisational performance can include the operational, managerial, and strategic impacts of different business efforts on the management of business processes. However, because organisational performance is influenced by numerous factors, the benefits from process capital can be expected to take up to several years to filter through the various levels of business performance. For example, a process integration technology may take months to develop and transform into real processes and to generate increased productivity. In addition, after processes emerge into business operation, greater managerial and strategic performance may appear later. Therefore, it is important to use proper measures to reflect process value for both the short and long term.\n\nThe input method is measured by the resources invested in process capital for business operations. In economic terms, this is expected to predict the future value of the processes. The two measurement indicators for this method are investment in information technology and administrative expanses. Firm size and industry type are the two controlled variables that are used in all three methods.\n\nThe output method is measured by the total effort of managing the technology and operations which achieve business effectiveness. The process indicators measure the actual value of labor productivity, such as profit per employee, to determine the contribution to the firm's overall productivity. This method is based on reviewing past performances to predict future results.\n\nManagement capability is the capability of constructing and coordinating resources to integrate and develop processes to respond to changing business conditions. The method measures the value of the process capital at the time of the measurement as a percentage change of the productivity compared to the previous year.\n\nStudies show that process capital has a positive influence on competitive advantages of firms.\n\nCompanies engaging in environmental management and green innovation actively can not only minimise production waste and increase productivity, but also charge relatively high prices for green products, improve corporate images, and thereby obtain corporate competitive advantages under the trends of popular environmentalism consciousness of consumers and severe international regulations of environmental protection. Therefore, the stocks of organisational capabilities, organisational commitments, knowledge management systems, reward systems, information technology systems, databases, managerial institution, operation processes, managerial philosophies, organisational culture, company images, patents, copy rights, and trademarks, etc. about environmental protection or green innovation within a company can help companies obtain competitive advantages.\n\nThe measurement of green structural capital comprises the following nine items:\n\n\"(1) whether the management system of environmental protection in the firm is superior to that of its major competitors;\"\n\n\"(2) whether innovations about environmental protection\nin the firm are more than those of its major competitors;\"\n\n\"(3) whether the profits earned from environmental protection activities of the firm is Effect of Green Intellectual Capital on Competitive Advantages of Firms 277 more than that of its major competitors;\"\n\n\"(4) whether the ratio of investments in R&D expenditures about environmental protection in the firm to its sales is more than that of its major competitors;\"\n\n\"(5) whether the ratio of employees about environmental management to the total employees in the firm is more than that of its major competitors;\"\n\n\"(6) whether investments in environmental protection facilities in the firm are more than those of its major competitors;\"\n\n\"(7) whether the competence in the development of green products in the firm is better than that of its major competitors;\"\n\n\"(8) whether the overall operation processes about environmental protection in the firm work smoothly;\"\n\n\"(9) whether the knowledge management system about environmental management in the firm is favorable for the accumulation and sharing of the knowledge of environmental management.\" \n\nProcess capital is one of the three components of structural capital. Nursing structural capital is knowledge converted into information structures that nurses can use to assist with their clinical decision-making and care planning. Nursing structural capital in the form of practice guidelines, care maps or protocols is believed to provide relevant information to nurses for improving the quality of care they deliver. Care maps, practice guidelines and protocols have been found to contribute to improved patient outcomes and reduce the rate of adverse events.\n\nThe intellectual capital model distinguishes between three distinct elements: Human Capital, Structural Capital, and Relational Capital. Leadership represents ideas of Human Capital, the intellectual value of the employees in a firm. All intellectual capital first originates as Human Capital. Innovation and process capital represents components of structural capital, elements that show the legal and process value of the company. Innovation capital includes embodied knowledge sets like patents and copyrights. Process capital tends to consist of more intangible elements of a tacit knowledge set which includes process technologies. In other words, structural capital consists of elements with which the firm's members interact to create more knowledge or get the work done. Relational capital is moderated by cultural capital and both of these elements represent the knowledge needed to provide ongoing value-added relations with shareholders.\n\n(Figure 1 source.\n\nThe key principle illustrated in Figure 1 is that no element of intellectual capital by itself creates ongoing value for the firm but is value creating only when interacting with other elements of the intellectual capital model. Thus, the true nature of the intellectual capital machine within the firm is dynamic and with reference to measurement issues. Measuring one elements may affect the measurement of another and once measurements are made of the element its magnitude and direction of flow may have already changed. Total knowledge of the intellectual capital system through measurement is impossible.\n\nA problem that may result from narrowly defining the knowledge stocks to be measured is that of reification and institutionalisation of such stock. Identifying a set of importance may over-emphasise the real strategic value of the set, especially in times of strategical change. When knowledge stocks are identified and attention given to them their importance may be rectified by those within the firm and results in the development of core rigidities, or strategic commitment, which may stagnate potentially revitalising innovation. Leonard-Barton identified knowledge as one of the most difficult to change. When it is past success and attention, it creates core rigidities. The term she uses to demonstrate the basic principle is a \"way of seeing is also a way of not seeing.\" \n\nThe predominance of a structuralization assumption is that all knowledge needs to be structuralized to be valuable to a firm, leading to reification of knowledge as explicit. This assumption treats all knowledge as easily distinguishable from human experience.\n\nA woman who had been working in North America for a German company was fired when the firm downsized and moved all its operations back to Germany. Later, the company learned that the woman had been working on strategically important information on the North American market, thus the knowledge this woman had created had been erroneously \"let go\" in the process of downsizing. The moral of the story espoused by the author was that the company should have structuralized this knowledge so that it could be owned and retained by the firm even after eliminating the woman from its payroll. Why was the woman fired? Surely, the firm should have been more aware of its developing knowledge sets because knowledge sets are the bases from which all intellectual capital emanates. Furthermore, the strategically relevant information is communicable and could have been codified.\n\nHow does the firm manage both its structural capital as product and its human capital as process? If this flip side seems humanistic, it is. The human intellect is the only entity that has the capacity to create new knowledge that has value. Computers may reassemble information into new modes analytically but cannot integrate these into something new. Structuralization of human capital may be only possible and desired when it can be used to further help the development of human capital, as non-contextual data or information, or sold as a product for profit. An important insight is that the firm need not own something outright in order for it to profit from association. An example of such an intangible tacit knowledge set that the firm could hardly structuralize and yet profits by association is seen in the relational capital created between a sales agent and client.\n\nThe move towards structuralization of tacit, experiential knowledge may result in other problems. Attempts to measure knowledge stock may disrupt the very process involved in creating new knowledge stocks. In such attempts, will we have killed the goose in order to get the golden egg? That is, the very process of measurement may interfere with the process by which knowledge creates value. Thus, firms should also look towards leveraging intellectual capital through \"organising\" rather than \"structuralizaing\" their knowledge assets: organising attempts to coordinate the intellectual capital elements such that their interaction increases ongoing value of the firm as a \" going concern\" rather than simply as an asset on a balance sheet.\n\nAnother problem here may be called the \"false recipe\" syndrome where the metaphor is that of the chef who, unconsciously or otherwise, neglects to communicate subtle but important elements of a recipe. Managers often lose the support of labor in initiating productivity improvement exercise because, rightly or wrongly, they are perceived as designed to put workers out of a job.\n\nIn managing intellectual capital there are at least two distinct things that are being managed. Figure 2 illustrates these two streams flowing from the development of human capital. (Figure 2 source.)\n\nThe first is the knowledge stock itself. The most successful examples of managing intellectual capital from the literature are indicative of this type. Because this type represents the structural capital of the \"structuralization\" assumption, this is not surprising. It is, however, important. Usually these consist of the products of the intellectual capital development process.\n\nThe second thing being managed is the knowledge work itself. This is more difficult to manage than the first because of its intangible nature. Inevitably the knowledge worker will be the best equipped to understand the full requirements of any particular job. In this case, managers must act as facilitators, setting up the structures and process that will help the knowledge worker will be best equipped to understand the full requirements of any particular job. In this case, managers must act as facilitators, setting up the structures and processes that will help the knowledge worker's development and allow for the efficient interaction of human capital with the other elements of the intellectual capital framework.\n"}
{"id": "29536120", "url": "https://en.wikipedia.org/wiki?curid=29536120", "title": "Project controller", "text": "Project controller\n\nThe project controller is a key member of the project team and works directly with the project manager to help define the project's goals and objectives; create and maintain a project's budget and schedule, analyse progress reported against the work schedules; and recommend actions to improve progress. In order to ensure accurate documentation and reporting on a consistent basis, many organisations are positioning the project manager and project controller as part of a centralised project support organisation. \n\nProject controllers are often employed by consulting firms that perform management, technology, and/or human capital work for an external client. Although titles vary from firm to firm, most large consulting firms such as Accenture, PWC, Deloitte and Bearing Point have individuals who perform project controller duties. The project controller is responsible for overseeing the financial health of the project by analyzing costs, revenue, risks, and pricing for the consulting firm. The role of the project controller can vary depending on the type and scale of the project, but most all project controllers maintain budget tracking tools, issue invoices and billings, manage fee structures, and forecast potential gains and losses in pursuing future work with a client.\n\nNot all projects have a project controller. Smaller projects that are expected to be shorter in duration typically may have no project controller overseeing the daily financial activity of the project. The responsibility will usually be assumed by the project manager of the project. For larger scale projects or projects requiring on-site interaction with the external client (typically public sector projects), there may be one or several project controllers to ensure that staffing levels and billing rates are appropriate for the budgetary scope of the project. Because most project controllers are serving the internal project management team, they are typically considered non-billable to the client. However, in projects where client staff and the consulting firm work in the same unit, some clients have agreed to be billed for project controllers to facilitate financial transactions and communications between the consulting firm and the client. Some project controllers will also perform expense audits for the project to ensure that all expenses to the project are in compliance with the consulting firm's and the client's expense policies.\n\nAlong with profit/loss analysis and budgeting, project controllers are often involved in the original pricing of a project. The pricing of a project occurs before the work begins and is meant decide the best fee structure and overall cost to the client for the service that will be provided. The project controller helps determine the optimal price to charge the client in order to retain profit at the end of the engagement. \n\nAlthough there are no formal qualification criteria, project controllers usually have graduate level training in project management or finance such as MSF or MBA degrees, or are qualified accountants (i.e. CIMA, CCA, ACCA, CGA, CMA, or CA designation). \"Industry experience\" is often a pre-requisite and so controllers often have undergraduate degrees in related fields. Also, many analysts originally enter this domain through their practice as consultants or accountants and so a very wide range of qualifications is common.\n\nIn some firms, it is (additionally) preferred that analysts earn a professional certification such as the Chartered Institute of Management Accountants (CIMA) Chartered Financial Analyst (CFA) designation, or the Certified International Investment Analyst (CIIA) designation. However, professional designation is rarely a requirement, and many have reached the peak of the profession without ever sitting for the CFA or the CIIA exam. \n\nBasic analytical skills, and strong numerical skills are necessary and a proficient understanding of and experience with Microsoft Excel is usually required.\n\n\n"}
{"id": "38252698", "url": "https://en.wikipedia.org/wiki?curid=38252698", "title": "Rapid Results", "text": "Rapid Results\n\nRapid Results is a structured process that mobilizes teams to achieve tangible results over a rapid time frame and accelerate organizational learning. Schaffer Consulting, a management consulting firm headquartered in Stamford, Connecticut, developed the Rapid Results approach based on their experience working with clients across industries to achieve breakthrough levels of performance. The approach and its practitioners have since been recognized in \"The New York Times\", \"Harvard Business Review\", and \"Foreign Policy\", among other publications. In 2007, Schaffer Consulting founded the Rapid Results Institute (RRI) to refine the methodology to support non-profit work and development projects across Sub-Saharan Africa, South America, the United States, and the Middle East.\n\nThe objective of Rapid Results initiatives is to achieve dramatic results, formed under the pressure of short time frames and ambitious targets. Rapid Results initiatives begin with a call to action for significant performance improvement delivered by a single leader or group of leaders to cross-functional teams of 8-10 people. Team members then set and commit to \"seemingly unreasonable\" short-term goals — often in 100 days or less — tied to the strategic imperatives outlined by their leaders. Teams then experiment with new ways of working, capturing learnings along the way, and persisting until they achieve desired outcomes. Rapid Results aims to stimulate innovation, collaboration, and more effective execution in and across organizations and stakeholder groups. Leaders leverage initial results to create longer-term and wider-scale impact in subsequent waves of Rapid Results projects.\n\nLeaders in many industries leverage Rapid Results to achieve performance goals, including accelerating growth, increasing productivity, and realizing cost savings.\n\nLynn Chambers, group head of Talent at the London Stock Exchange Group, says, “This approach can be taken to accelerate progress on almost any goal.” Dean Scarborough, president and chief executive officer of Avery Dennison describes Rapid Results “driving an incredible amount of creativity” among team members. Martha Marsh, former president and CEO of Stanford Hospital and Clinics, observes that Rapid Results “provide an intense focus on getting results, while helping build the capacity of our organization to drive transformational change.”\n\nFormed in 2007, the Rapid Results Institute works with a broad range of partners — including government agencies, not-for-profit organizations, and international development agencies — across a wide spectrum of development initiatives. Past and ongoing projects include H.I.V. prevention in Eritrea and Ethiopia, public sector reform in Kenya, waste management in Brazil, and housing homeless veterans in the United States.\n\nThe Rapid Results Institute is partnering with the 100,000 Homes Campaign and the US Department of Veterans Affairs to house homeless veterans across the United States.\n\nFollowing a project on HIV/AIDS awareness among youth, Eritrean Minister of Education Osman Saleh called the Rapid Results approach \"a new movement.\" In Kenya, an independent evaluation by the AIDS Research and Treatment journal reported that “significant improvement in PMTCT [prevention of mother-to-child-transmission] services can be achieved through introduction of an RRI [Rapid Results Initiative], which appears to lead to sustained benefits for pregnant HIV-infected women and their infants.”\n\nDue to its success in a variety of contexts, the Rapid Results approach has been adopted by the World Bank, as well as by many Kenyan government ministries.\n\n"}
{"id": "1838287", "url": "https://en.wikipedia.org/wiki?curid=1838287", "title": "Shared services", "text": "Shared services\n\nShared services is the provision of a service by one part of an organization or group, where that service had previously been found, in more than one part of the organization or group. Thus the funding and resourcing of the service is shared and the providing department effectively becomes an internal service provider. The key here is the idea of 'sharing' within an organization or group. This sharing needs to fundamentally include shared accountability of results by the unit from where the work is migrated to the provider. The provider, on the other hand, needs to ensure that the agreed results are delivered based on defined measures (KPIs, cost, quality etc.).\n\nShared services is similar to collaboration that might take place between different organizations such as a Hospital Trust or a Police Force. For example, adjacent Trusts might decide to collaborate by merging their HR or IT functions.\n\nThere are two arguments for sharing services: The ‘less of a common resource' argument and the ‘efficiency through industrialization' argument. The former is ‘obvious': if you have fewer managers, IT systems, buildings etc; if you use less of some resource, it will reduce costs. The second argument is ‘efficiency through industrialization’. This argument assumes that efficiencies follow from specialization and standardization – resulting in the creation of ‘front' and ‘back' offices. The typical method is to simplify, standardize and then centralize, using an IT 'solution' as the means.\n\nShared services is different from the model of outsourcing, which is where an external third party is paid to provide a service that was previously internal to the buying organization, typically leading to redundancies and re-organization. There is an ongoing debate about the advantages of shared services over outsourcing. It is sometimes assumed that a joint venture between a government department and a commercial organization is an example of shared services. The joint venture involves the creation of a separate legal commercial entity (jointly owned), which provides profit to its shareholders.\n\nTraditionally the development of a shared-service organization (SSO) or shared-service centre (SSC) within an organization is an attempt to reduce costs (often attempted through economies of scale), standardized processes (through centralization). A global Service Center Benchmark study carried out by the Shared Services & Outsourcing Network (SSON) and the Hackett Group, which surveyed more than 250 companies, found that only about a third of all participants were able to generate cost savings of 20% or greater from their SSOs. At NASA, the 2006 switch to a shared services model is realizing nearly $20 million of savings annually. Further, by the end of 2015, the NASA Shared Services Center is expected to save the organization a total of over $200 million, according to NASA’s Director of Service Delivery.\n\nA large-scale cultural and process transformation can be a key component of a move to shared services and may include redundancies and changes of work practices. It is claimed that transformation often results in a better quality of work life for employees although there are few case studies to back this up .\n\nShared services are more than just centralization or consolidation of similar activities in one location. Shared services can mean running these service activities like a business and delivering services to internal customers at a cost, quality, and timeliness that is competitive with alternatives.\n\nThe management thinker and inventor of The Vanguard Method, Professor John Seddon claims that shared service projects based on attempts to achieve 'economies of scale' are a mix of the plausibly obvious and a little hard data, brought together to produce two broad assertions, for which there is little hard factual evidence. He argues that shared service projects fail (and often end up costing more than they hoped to save) because they cause a disruption to the service flow by moving the work to a central location, creating waste in handoffs, rework and duplication, lengthening the time it takes to deliver a service and consequently creating failure demand (demand caused by a failure to do something or do something right for a customer).\n\nA shared service can take a variety of different commercial structures. The basic commercial structures include:\n\n\nIt is sometimes argued that there are three basic location variations for a shared service including:\n\n\nThis is not just to take advantage of wage arbitrage but to appreciate the talents of particular economies in delivering specific service offerings.\n\nThe difficulty with this argument is that near-shore and off-shore are normally associated with the outsourcing model and are difficult to reconcile with the notion of an internally shared service as distinct from an externally purchased service. Clearly, the use of off-shore facilities by a government department is not an example of shared services.\n\nIn establishing and running a shared service, benchmarking and measurement is considered by some as a necessity. Benchmarking is the comparison of the service provision usually against best in class. The measurement occurs by using agreed key performance indicators (KPIs). Although the amount of KPIs chosen differs greatly it is generally accepted that fewer than 10 carefully chosen KPIs will deliver the best results.\n\nOrganizations do attempt to define benchmarks for processes and business operations.\n\nBenchmarking can be used to achieve different goals including:<br>\n1. To drive performance improvements using benchmarks as a means for setting performance targets that are met either through incremental performance improvements or transformational change.<br>\n- Strategic: with a focus on a long term horizon; and <br>\n- Tactical: with a focus on the short and medium term\n\n2. To focus an organization on becoming world class with processes that deliver the highest levels of performance that are better than those of its peer group.\n\nThe private sector has been moving towards shared services since the beginning of the 1980s. Large organizations such as the BBC, BP, Bristol Myers Squibb, Ford, GE, HP, Pfizer, Rolls-Royce, ArcelorMittal, and SAP are operating them with great success. According to the English Institute of Chartered Accountants, more than 30% of U.S. Fortune 500 companies have implemented a shared-service center, and are reporting cost savings in their general accounting functions of up to 46%.\n\nThe conventional accounting practice used to generate these figures is disputed however by management thinker Professor John Seddon, who argues that the measurement known as 'unit cost' tells you nothing about overall costs. Overall costs include 'failure demand', which is defined as a failure to do something or do something right for the customer.\n\nThe public sector has taken note of the benefits derived in the private sector and continues to strive for best practice. The United States and Australia among others have had shared services in government since the late 1990s. However, the failures of these projects are increasingly being reported by the press and exposed by opposition politicians.\n\nThe UK government under a central drive to efficiency following from the Gershon Review are working to an overall plan for realizing the benefits of shared services. The Cabinet Office has established a team specifically tasked with the role of accelerating the take up and developing the strategy for all government departments to converge and consolidate. The savings potential of this transformation in the UK Public Sector was initially estimated by the Cabinet Office at £1.4bn per annum (20% of the estimated cost of HR and Finance functions). The National Audit Office (United Kingdom) in its November 2007 report pointed out that this £1.4bn figure lacked a clear baseline of costs and contained several uncertainties, such as the initial expenditure required and the time frame for the savings.\n\nThere are reports of UK government shared service centres failing to realise savings, such as the Department of Transport's project, described as 'stupendous incompetence' by the Parliament's Public Accounts Committee.\n\nThe Northern Ireland Civil Service (NICS), has implemented shared services for a number of departments and functions. For example, IT Assist (the ICT Shared Service Centre) provides common infrastructure and desktop services to NICS staff in the office, at home or when mobile working.\n\nThe government of Canada instituted Shared Services Canada on August 4, 2011, with the objective of consolidating its data centres, networks and email systems. This follows a tendency to centralize IT services that has been followed by the provinces of British Columbia, Québec, and Ontario, as well as the federal government of the United States of America and some states like Texas. PriceWaterhouseCoopers recommended integrating government data centres in a report ordered by Public Works and Government Services Canada, revealed in December 2011.\n\nIn the Republic of Ireland, the health service nationally has been reorganized from a set of regional Health Boards to a unified national structure, the Health Services Executive. Within this structure there will be a National Shared Services Organisation, based on the model developed at the former Eastern Health Shared Services, where procurement, HR, finance and ICT services were provided to health agencies in the Eastern Region of Ireland on a business-to business basis.\n\nOrganizations that have centralized their IT functions have now begun to take a close look at the technology services that their IT departments provide to internal customers, evaluating where it makes sense to provide specific technology components as a shared service. E-mail and scanning operations were obvious early candidates; many organizations with document-intensive operations are deploying scanning centers as a shared service.\n\nMany large organizations, in both the public and private sectors, are now considering deploying enterprise content management (ECM) technology as a shared service.\n\n\n"}
{"id": "1943311", "url": "https://en.wikipedia.org/wiki?curid=1943311", "title": "Textile industry", "text": "Textile industry\n\nThe textile industry is primarily concerned with the design, production and distribution of yarn, cloth and clothing. The raw material may be natural, or synthetic using products of the chemical industry.\n\nCotton is the world's most important natural fibre. In the year 2007, the global yield was 25 million tons from 35 million hectares cultivated in more than 50 countries.\nThere are five stages:\n\nArtificial fibres can be made by extruding a polymer, through a spinneret into a medium where it hardens. Wet spinning (rayon) uses a coagulating medium. In dry spinning (acetate and triacetate), the polymer is contained in a solvent that evaporates in the heated exit chamber. In melt spinning (nylons and polyesters) the extruded polymer is cooled in gas or air and then sets. All these fibres will be of great length, often kilometres long.\n\nArtificial fibres can be processed as long fibres or batched and cut so they can be processed like a natural fibre.\n\nNatural fibres are either from animals (sheep, goat, rabbit, silk-worm) mineral (asbestos) or from plants (cotton, flax, sisal). These vegetable fibres can come from the seed (cotton), the stem (known as bast fibres: flax, hemp, jute) or the leaf (sisal). Without exception, many processes are needed before a clean even staple is obtained- each with a specific name. With the exception of silk, each of these fibres is short, being only centimeters in length, and each has a rough surface that enables it to bond with similar staples.\n\n There are some indications that weaving was already known in the Palaeolithic. An indistinct textile impression has been found at Pavlov, Moravia. Neolithic textiles were found in pile dwellings excavations in Switzerland and at El Fayum, Egypt at a site which dates to about 5000 BC.\n\nIn Roman times, wool, linen and leather clothed the European population, and silk, imported along the Silk Road from China, was an extravagant luxury. The use of flax fiber in the manufacturing of cloth in Northern Europe dates back to Neolithic times.\n\nDuring the late medieval period, cotton began to be imported into Northern Europe. Without any knowledge of what it came from, other than that it was a plant, noting its similarities to wool, people in the region could only imagine that cotton must be produced by plant-borne sheep. John Mandeville, writing in 1350, stated as fact the now-preposterous belief: \"There grew in India a wonderful tree which bore tiny lambs on the edges of its branches. These branches were so pliable that they bent down to allow the lambs to feed when they are hungry.\" This aspect is retained in the name for cotton in many European languages, such as German \"Baumwolle\", which translates as \"tree wool\". By the end of the 16th century, cotton was cultivated throughout the warmer regions of Asia and the Americas.\n\nThe main steps in the production of cloth are producing the fibre, preparing it, converting it to yarn, converting yarn to cloth, and then finishing the cloth. The cloth is then taken to the manufacturer of garments. The preparation of the fibres differs the most, depending on the fibre used. Flax requires retting and dressing, while wool requires carding and washing. The spinning and weaving processes are very similar between fibers, however.\n\nSpinning evolved from twisting the fibers by hand, to using a drop spindle, to using a spinning wheel. Spindles or parts of them have been found in archaeological sites and may represent one of the first pieces of technology available. They were invented in the Indian subcontinent between 500 and 1000 AD.\n\nUp until the 18th century, Mughal Empire was the most important center of manufacturing in international trade. Up until 1750, India produced about 25% of the world's industrial output. The largest manufacturing industry in Mughal Empire (16th to 18th centuries) was textile manufacturing, particularly cotton textile manufacturing, which included the production of piece goods, calicos, and muslins, available unbleached and in a variety of colours. The cotton textile industry was responsible for a large part of the empire's international trade. Bengal had a 25% share of the global textile trade in the early 18th century. Bengal cotton textiles were the most important manufactured goods in world trade in the 18th century, consumed across the world from the Americas to Japan. The most important center of cotton production was the Bengal Subah province, particularly around its capital city of Dhaka.\n\nBengal accounted for more than 50% of textiles and around 80% of silks imported by the Dutch from Asia and marketed it to the world, Bengali silk and cotton textiles were exported in large quantities to Europe, Asia, and Japan, and Bengali muslin textiles from Dhaka were sold in Central Asia, where they were known as \"daka\" textiles. Indian textiles dominated the Indian Ocean trade for centuries, were sold in the Atlantic Ocean trade, and had a 38% share of the West African trade in the early 18th century, while Bengal calicos were major force in Europe, and Bengal textiles accounted for 30% of total English trade with Southern Europe in the early 18th century.\n\nIn early modern Europe, there was significant demand for textiles from The Mughal Empire, including cotton textiles and silk products. European fashion, for example, became increasingly dependent on textiles and silks imported from The Mughal Empire. In the late 17th and early 18th centuries, The Mughal Empire accounted for 95% of British imports from Asia.\n\nThe key British industry at the beginning of the 18th century was the production of textiles made with wool from the large sheep-farming areas in the Midlands and across the country (created as a result of land-clearance and enclosure). This was a labour-intensive activity providing employment throughout Britain, with major centres being the West Country; Norwich and environs; and the West Riding of Yorkshire. The export trade in woolen goods accounted for more than a quarter of British exports during most of the 18th century, doubling between 1701 and 1770.\n\nExports by the cotton industry – centered in Lancashire – had grown tenfold during this time, but still accounted for only a tenth of the value of the woolen trade. Before the 17th century, the manufacture of goods was performed on a limited scale by individual workers, usually on their own premises (such as weavers' cottages). Goods were transported around the country by clothiers who visited the village with their trains of packhorses. Some of the cloth was made into clothes for people living in the same area, and a large amount of cloth was exported. River navigations were constructed, and some contour-following canals. In the early 18th century, artisans were inventing ways to become more productive. Silk, wool, fustian, and linen were being eclipsed by cotton, which was becoming the most important textile. This set the foundations for the changes.\n\nThe woven fabric portion of the textile industry grew out of the industrial revolution in the 18th century as mass production of yarn and cloth became a mainstream industry.\n\nIn 1734 in Bury, Lancashire John Kay invented the flying shuttle — one of the first of a series of inventions associated with the cotton woven fabric industry. The flying shuttle increased the width of cotton cloth and speed of production of a single weaver at a loom. Resistance by workers to the perceived threat to jobs delayed the widespread introduction of this technology, even though the higher rate of production generated an increased demand for spun cotton. \n\nIn 1761, the Duke of Bridgewater's canal connected Manchester to the coal fields of Worsley and in 1762, Matthew Boulton opened the Soho Foundry engineering works in Handsworth, Birmingham. His partnership with Scottish engineer James Watt resulted, in 1775, in the commercial production of the more efficient Watt steam engine which used a separate condenser.\n\nIn 1764, James Hargreaves is credited as inventor of the spinning jenny which multiplied the spun thread production capacity of a single worker — initially eightfold and subsequently much further. Others credit the invention to Thomas Highs. Industrial unrest and a failure to patent the invention until 1770 forced Hargreaves from Blackburn, but his lack of protection of the idea allowed the concept to be exploited by others. As a result, there were over 20,000 spinning jennies in use by the time of his death. Also in 1764, Thorp Mill, the first water-powered cotton mill in the world was constructed at Royton, Lancashire, and was used for carding cotton. With the spinning and weaving process now mechanized, cotton mills cropped up all over the North West of England.\n\nThe stocking frame invented in 1589 for silk became viable when in 1759, Jedediah Strutt introduced an attachment for the frame which produced what became known as the Derby Rib, that produced a knit and purl stitch. This allowed stockings to be manufactured in silk and later in cotton. In 1768, Hammond modified the stocking frame to weave weft-knitted openworks or nets by crossing over the loops, using a mobile tickler bar- this led in 1781 to Thomas Frost's square net. Cotton had been too coarse for lace, but by 1805 Houldsworths of Manchester were producing reliable 300 count cotton thread.\n\nWith the Cartwright Loom, the Spinning Mule and the Boulton & Watt steam engine, the pieces were in place to build a mechanised woven fabric textile industry. From this point there were no new inventions, but a continuous improvement in technology as the mill-owner strove to reduce cost and improve quality. Developments in the transport infrastructure; that is the canals and after 1831 the railways facilitated the import of raw materials and export of finished cloth.\n\nFirstly, the use of water power to drive mills was supplemented by steam driven water pumps, and then superseded completely by the steam engines. For example, Samuel Greg joined his uncle's firm of textile merchants, and, on taking over the company in 1782, he sought out a site to establish a mill.Quarry Bank Mill was built on the River Bollin at Styal in Cheshire. It was initially powered by a water wheel, but installed steam engines in 1810. Quarry Bank Mill in Cheshire still exists as a well-preserved museum, having been in use from its construction in 1784 until 1959. It also illustrates how the mill owners exploited child labour, taking orphans from nearby Manchester to work the cotton. It shows that these children were housed, clothed, fed and provided with some education. In 1830, the average power of a mill engine was 48 hp, but Quarry Bank mill installed a new 100 hp water wheel. William Fairbairn addressed the problem of line-shafting and was responsible for improving the efficiency of the mill. In 1815 he replaced the wooden turning shafts that drove the machines at 50rpm, to wrought iron shafting working at 250 rpm, these were a third of the weight of the previous ones and absorbed less power.\nSecondly, in 1830, using an 1822 patent, Richard Roberts manufactured the first loom with a cast iron frame, the Roberts Loom. In 1842 James Bullough and William Kenworthy, made the Lancashire Loom, a semiautomatic power loom: although it is self-acting, it has to be stopped to recharge empty shuttles. It was the mainstay of the Lancashire cotton industry for a century, until the Northrop Loom (invented in 1894, with an automatic weft replenishment function) gained ascendancy.\nThirdly, also in 1830, Richard Roberts patented the first self-acting mule. Stalybridge mule spinners strike was in 1824; this stimulated research into the problem of applying power to the winding stroke of the mule. The draw while spinning had been assisted by power, but the push of the wind had been done manually by the spinner, the mule could be operated by semiskilled labor. Before 1830, the spinner would operate a partially powered mule with a maximum of 400 spindles; after, self-acting mules with up to 1300 spindles could be built.\n\nThe industrial revolution changed the nature of work and society The three key drivers in these changes were textile manufacturing, iron founding and steam power. The geographical focus of textile manufacture in Britain was Manchester and the small towns of the Pennines and southern Lancashire.\n\nTextile production in England peaked in 1926, and as mills were decommissioned, many of the scrapped mules and looms were bought up and reinstated in India.\n\nMajor changes came to the textile industry during the 20th century, with continuing technological innovations in machinery, synthetic fibre, logistics, and globalization of the business. The business model that had dominated the industry for centuries was to change radically. Cotton and wool producers were not the only source for fibres, as chemical companies created new synthetic fibres that had superior qualities for many uses, such as rayon, invented in 1910, and DuPont's nylon, invented in 1935 as in inexpensive silk substitute, and used for products ranging from women's stockings to tooth brushes and military parachutes.\n\nThe variety of synthetic fibres used in manufacturing fibre grew steadily throughout the 20th century. In the 1920s, the computer was invented; in the 1940s, acetate, modacrylic, metal fibres, and saran were developed; acrylic, polyester, and spandex were introduced in the 1950s. Polyester became hugely popular in the apparel market, and by the late 1970s, more polyester was sold in the United States than cotton.\n\nBy the late 1980s, the apparel segment was no longer the largest market for fibre products, with industrial and home furnishings together representing a larger proportion of the fibre market. Industry integration and global manufacturing led to many small firms closing for good during the 1970s and 1980s in the United States; during those decades, 95 percent of the looms in North Carolina, South Carolina and Georgia shut down, and Alabama and Virginia also saw many factories close.\n\nThe worldwide market for textiles and apparel exports in 2013 according to United Nations Commodity Trade Statistics Database stood at $772 billion.\n\nThe largest exporters of textiles in 2013 were China ($274 billion), India ($40 billion), Italy ($36 billion), Germany ($35 billion), Bangladesh ($28 billion) and Pakistan ($27 Billion).\n\nIn 2016, the largest apparel exporting nations were China ($161 billion), Bangladesh ($28 billion), Vietnam ($25 billion), India ($18 billion), Hong Kong ($16 billion), Turkey ($15 billion) and Indonesia ($7 billion).\n\nThe Multi Fibre Arrangement (MFA) governed the world trade in textiles and garments from 1974 through 2004, imposing quotas on the amount developing countries could export to developed countries. It expired on 1 January 2005.\n\nThe MFA was introduced in 1974 as a short-term measure intended to allow developed countries to adjust to imports from the developing world. Developing countries have a natural advantage in textile production because it is labor-intensive and they have low labor costs. According to a World Bank/International Monetary Fund (IMF) study, the system has cost the developing world 27 million jobs and $40 billion a year in lost exports.\n\nHowever, the Arrangement was not negative for all developing countries. For example, the European Union (EU) imposed no restrictions or duties on imports from the very poor countries, such as Bangladesh, leading to a massive expansion of the industry there.\n\nAt the General Agreement on Tariffs and Trade (GATT) Uruguay Round, it was decided to bring the textile trade under the jurisdiction of the World Trade Organization (WTO). The WTO Agreement on Textiles and Clothing provided for the gradual dismantling of the quotas that existed under the MFA. This process was completed on 1 January 2005. However, large tariffs remain in place on many textile products.\n\nBangladesh was expected to suffer the most from the ending of the MFA, as it was expected to face more competition, particularly from China. However, this was not the case. It turns out that even in the face of other economic giants, Bangladesh’s labor is “cheaper than anywhere else in the world.” While some smaller factories were documented making pay cuts and layoffs, most downsizing was essentially speculative – the orders for goods kept coming even after the MFA expired. In fact, Bangladesh's exports increased in value by about $500 million in 2006.\n\nFor textiles, like for many other products, there are certain national and international standards and regulations that need to be complied with to ensure quality, safety and sustainability.\n\nThe following standards amongst others apply to textiles:\n\n\n\n"}
{"id": "36324477", "url": "https://en.wikipedia.org/wiki?curid=36324477", "title": "The Entertainer (discount publisher)", "text": "The Entertainer (discount publisher)\n\nThe Entertainer is a publisher of buy one get one free (2 for 1) offers for restaurants, leisure attractions, spas, hotel accommodation and more across the Middle East, Asia, Africa and Europe. \n\nThe Entertainer has 46 products (books and apps) that cover 40 destinations across 15 countries. \n\nThe Entertainer is privately owned and has offices in the UAE, Qatar, Singapore, Hong Kong, Malaysia, South Africa, Cyprus and the UK.\n\nIn early 2012, Abraaj Capital announced their purchase of 50% of the Entertainer. In March 2012, the Entertainer announced a significant investment from Abraaj Capital’s small to mid cap investment arm – RED Investments.\n\nIn November 2013, the Entertainer launched a smartphone app. In addition to all the offers in the printed books, the App includes access to even more exclusive offers added each month, which includes brunches, new restaurant openings and special promotions. Features include GPS mapping, advanced search capabilities allowing you to filter against certain attributes, social sharing and integration with Trip Advisor and Careem.\n\nIn 2015, the average Dubai customer saved over AED 6,000 (US$1,600) with the Entertainer App, which has seen over 2 million buy one get one free mobile offers redeemed in the UAE alone.\n\nThe Entertainer signs up restaurants, leisure attractions, spas, gyms, hotels and more to sign up each year. Each merchant provides three buy one get one free offers to be featured (apart from hotels who can feature just one offer). Merchants must provide these offers with few restrictions so that they can be used 7 days a week (excluding public holidays and approved dates). \n"}
{"id": "13306545", "url": "https://en.wikipedia.org/wiki?curid=13306545", "title": "Trade sale", "text": "Trade sale\n\nA trade sale is a common means of exit to a trade buyer. This allows the management to withdraw from the business and may open up the prospect of collaboration on larger projects. The term trade sale is mostly used in the context of venture capital funded businesses and refers to the sale of a company in its early stages. \n\nIt normally entails the disposal of a company's shares or assets and even liabilities, in whole or in part. This may refer to a strategic buyer who intends to grow theirs business or to a financial buyer who wants to generate a financial return on their invested capital at the time of exit. \n\nTrade sales are the most frequently used to exit vehicle both in Europe and the US.\n\nThe term trade sale may also refer to business-to-business sales, rather than sales made directly to the public.\n"}
{"id": "2751096", "url": "https://en.wikipedia.org/wiki?curid=2751096", "title": "Unstructured data", "text": "Unstructured data\n\nUnstructured data (or unstructured information) is information that either does not have a pre-defined data model or is not organized in a pre-defined manner. Unstructured information is typically text-heavy, but may contain data such as dates, numbers, and facts as well. This results in irregularities and ambiguities that make it difficult to understand using traditional programs as compared to data stored in fielded form in databases or annotated (semantically tagged) in documents.\n\nIn 1998, Merrill Lynch cited a rule of thumb that somewhere around 80-90% of all potentially usable business information may originate in unstructured form. This rule of thumb is not based on primary or any quantitative research, but nonetheless is accepted by some. Other sources have reported similar or higher percentages of unstructured data.\n\nIDC and EMC project that data will grow to 40 zettabytes by 2020, resulting in a 50-fold growth from the beginning of 2010. More recently, IDC and Seagate predict that the global datasphere will grow to 163 zettabytes by 2025 and majority of that will be unstructured.The Computer World magazine states that unstructured information might account for more than 70%–80% of all data in organizations.\n\nThe earliest research into business intelligence focused in on unstructured textual data, rather than numerical data. As early as 1958, computer science researchers like H.P. Luhn were particularly concerned with the extraction and classification of unstructured text. However, only since the turn of the century has the technology caught up with the research interest. In 2004, the SAS Institute developed the SAS Text Miner, which uses Singular Value Decomposition (SVD) to reduce a hyper-dimensional textual space into smaller dimensions for significantly more efficient machine-analysis. The mathematical and technological advances sparked by machine textual analysis prompted a number of businesses to research applications, leading to the development of fields like sentiment analysis, voice of the customer mining, and call center optimization. The emergence of Big Data in the late 2000s led to a heightened interest in the applications of unstructured data analytics in contemporary fields such as predictive analytics and root cause analysis.\n\nThe term is imprecise for several reasons:\n\nTechniques such as data mining, natural language processing (NLP), and text analytics provide different methods to find patterns in, or otherwise interpret, this information. Common techniques for structuring text usually involve manual tagging with metadata or part-of-speech tagging for further text mining-based structuring. The Unstructured Information Management Architecture (UIMA) standard provided a common framework for processing this information to extract meaning and create structured data about the information.\n\nSoftware that creates machine-processable structure can utilize the linguistic, auditory, and visual structure that exist in all forms of human communication. Algorithms can infer this inherent structure from text, for instance, by examining word morphology, sentence syntax, and other small- and large-scale patterns. Unstructured information can then be enriched and tagged to address ambiguities and relevancy-based techniques then used to facilitate search and discovery. Examples of \"unstructured data\" may include books, journals, documents, metadata, health records, audio, video, analog data, images, files, and unstructured text such as the body of an e-mail message, Web page, or word-processor document. While the main content being conveyed does not have a defined structure, it generally comes packaged in objects (e.g. in files or documents, …) that themselves have structure and are thus a mix of structured and unstructured data, but collectively this is still referred to as \"unstructured data\". For example, an HTML web page is tagged, but HTML mark-up typically serves solely for rendering. It does not capture the meaning or function of tagged elements in ways that support automated processing of the information content of the page. XHTML tagging does allow machine processing of elements, although it typically does not capture or convey the semantic meaning of tagged terms.\n\nSince unstructured data commonly occurs in electronic documents, the use of a content or document management system which can categorize entire documents is often preferred over data transfer and manipulation from within the documents. Document management thus provides the means to convey structure onto document collections.\n\nSearch engines have become popular tools for indexing and searching through such data, especially text.\n\nSpecific computational workflows have been developed to impose structure upon the unstructured data contained within text documents. These workflows are generally designed to handle sets of thousands or even millions of documents, or far more than manual approaches to annotation may permit. Several of these approaches are based upon the concept of online analytical processing, or OLAP, and may be supported by data models such as text cubes. Once document metadata is available through a data model, generating summaries of subsets of documents (i.e., cells within a text cube) may be performed with phrase-based approaches.\n\nBiomedical research generates one major source of unstructured data as researchers often publish their findings in scholarly journals. Though the language in these documents is challenging to derive structural elements from (e.g., due to the complicated technical vocabulary contained within and the domain knowledge required to fully contextualize observations), the results of these activities may yield links between technical and medical studies and clues regarding new disease therapies. Recent efforts to enforce structure upon biomedical documents include self-organizing map approaches for identifying topics among documents, general-purpose unsupervised algorithms, and an application of the CaseOLAP workflow to determine associations between protein names and cardiovascular disease topics in the literature. CaseOLAP defines phrase-category relationships in an accurate (identifies relationships), consistent (highly reproducible), and efficient manner. This platform offers enhanced accessibility and empowers the biomedical community with phrase-mining tools for widespread biomedical research applications.\n\n\n\n"}
