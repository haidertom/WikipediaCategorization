{"id": "14452587", "url": "https://en.wikipedia.org/wiki?curid=14452587", "title": "Account manager", "text": "Account manager\n\nAn account manager is a person who works for a company and is responsible for the management of sales and relationships with particular customers. An account manager maintains the company's existing relationships with a client or group of clients, so that they will continue using the company for business. The account manager does not manage the daily running of the account itself. They manage the relationship with the client of the account(s) they are assigned to. Generally, a client will remain with one account manager throughout the duration of hiring the company. Account managers serve as the interface between the customer service and the sales team in a company. They are assigned a company's existing client accounts. The purpose of being assigned particular clients is to create long term relationships with the portfolio of assigned clients. The account manager serves to understand the customer's demands, plan how to meet these demands, and generate sales for the company as a result.\n\nKey accounts provide the most business because they contain a small number of clients which contribute a large portion of the company's sales. According to research, sales from a company's key accounts have increased from 23% in 1975 to 60% currently.\n\nThe responsibilities of an account manager can vary depending on the industry they work in, size of the company and nature of the business. Each customer account can vary in demands and an account manager may work with brand managers for one account and a media department for another. Account managers usually report directly to the account director or agency director of the activity and status of accounts and transactions. An account manager may also manage a single account or a variety of accounts depending on the requirement of the company. Although the responsibility can vary between companies and between accounts, there are a shared set of common responsibilities which are as follows:\n\n\nThere are situations in which an account manager, for example in advertising firms, is responsible for more than one account and therefore looks after multiple clients. When account locations do not overlap the account manager can be placed at the divisional, district, or territory level. When a sales team has a senior sales manager, the account manager coordinates sales accounts from other departments or specialties. In this scenario, the sales team will work under the direct supervision of influencers and deciders instead of with a buyer.\n\n\nGlobal account managers and national account managers may work together in a hierarchical or matrix structure. The trend is to move responsibility for the major key accounts to the global level.\n\nKey account manager is assigned to a company headquarters to oversee the account team assigned to a particular account. Key account management includes sales but also includes planning and managing the full relationship between a business and its most important customers. An account manager who works in this role will engage in a variety of tasks including project management, coordination, strategic planning, relationship management, negotiation, leadership and innovative development of opportunities, and keeping record of transaction of sale and purchase goods. The tasks may include working with product design and application, logistics, sales support, and marketing.\n\nThe basic assumption for a key account management model is the correct classification of the key accounts. A basic model often used in the period of 1950-1970 was the classification model of Webster. This model has been adapted by Milman and Wilson into a two-dimensional model and was paramount in the period of 1970-1990. Bensaou has tested this model empirically by his research of carmakers in the United States and Japan and made revisions. De Blick synthesized the adaptations into the 4S-model, a key account classification model. By the late 1990s, key account management spread to most B2B (business-to-business) models.\n\nAccount managers can work for small or large companies either corporate, financial, retail, or government for instance. Any company with a specific clients they conduct business with, can employ an account manager. Typical employers can be: \n\nAccount managers usually work in an office setting and can work more than 40 hours weekly. Travel is usually included in the job description. National or global account managers will very likely experience extra travel.\n\nAlthough personality and an aptitude for sales is key, a degree in business, marketing, or related field is typically required and depending on the nature of the account, a background in marketing or media studies may be preferred. Due to each company having different requirements, it is important to obtain information from each potential company of employment.\n\n\n"}
{"id": "20605998", "url": "https://en.wikipedia.org/wiki?curid=20605998", "title": "ActiveReports", "text": "ActiveReports\n\nActiveReports is a .NET reporting tool used by developers of WinForms, ASP.NET, and HTML5 applications. It was originally developed by Data Dynamics, which was then acquired by GrapeCity. ActiveReports is a set of components and tools that facilitates the production of reports to display data in documents and web-based formats. It is written in managed C# code and allows Visual Studio programmers to leverage their knowledge of C# or Visual Basic.NET when programming with ActiveReports.\n\nAmong the components included with ActiveReports are exports to file formats such as PDF, Excel, RTF, and TIFF. The main components are a Visual Studio integrated report designer, and an API that developers use to create customized reports from a variety of data sources. ActiveReports Standard Edition also includes a Visual Query Designer, a customizable Windows Viewer control, an HTML5 Viewer control, and a WPF Viewer control.\n\nThe integrated report designer handles three types of reports:\n\n\nThe Professional Edition of ActiveReports includes the Standard Edition tools plus an End-User Report Designer control that developers use to host the report designer in their own Microsoft Windows applications to let end users create and modify reports. It also includes a server-side ASP.NET web viewer with Flash, PDF, and HTML viewer types; ASP.NET HTTP Handlers that export reports to HTML or PDF format without custom code; a Silverlight viewer; and advanced PDF encryption and rendering features.\n\nActiveReports Server is a scalable server platform with built in load balancing. It provides a secure and extensible report server to publish reports designed in ActiveReports. A comprehensive RESTful API enables integration of the responsive HTML5 Report Portal for end users to view, schedule and export reports.\n\n\n\n\n\n\n\nLatest Service Releases\n\nStandard Edition\nalso support for various things\n\nActiveReports Designer\n\nWindows Forms Viewer\n\nReporting Engine\n\nIncludes all of the Standard Edition features, and adds the following:.\n\nEnd-User Report Designer\n\nASP.NET\n\n\n\nMay 15, 2015 - SD Times 100 GrapeCity wins Best In Show in the User Experience category\n\nFebruary 27, 2015 - Ranked #6 in the ComponentSource Bestselling Product Awards for 2014-2015\n\nMay 15, 2014 - SD Times 100 GrapeCity (dba ComponentOne) wins Best In Show in the User Experience category\n\nFebruary 28, 2014 - Ranked #5 in the ComponentSource Bestselling Product Awards for 2013-2014\n\nOctober 15, 2013 - Other Hot Products recognition in Best Microsoft Windows Development Printing/Reporting Tool of 2013\n\nMay 15, 2013 - SD Times 100 GrapeCity wins Best In Show in the User Experience category\n\nMay 15, 2013 - SD Times 100 GrapeCity wins Best In Show in the APIs, Libraries & Frameworks category\n\nFebruary 27, 2013 - Ranked #4 in the ComponentSource Bestselling Product Awards for 2012-2013\n\nMay 15, 2012 - SD Times 100 GrapeCity wins Best In Show in the Libraries & Frameworks category\n\nApril 30, 2012 - Ranked #11 in the ComponentSource Bestselling Product Awards for 2011-2012\n\nMay 15, 2011 - SD Times 100 GrapeCity wins Best In Show in the Components category\n\nFebruary 28, 2011 - Ranked #6 in the ComponentSource Bestselling Product Awards for 2010-2011\n\nNovember 29, 2010 - Finalist in the 2010 Best of Connections Awards: Visual Studio Developer Products\n\nMay 15, 2010 - SD Times 100 GrapeCity wins Best In Show in the Components & Libraries category\n\nMarch 26, 2010 - Ranked #7 in the ComponentSource Bestselling Product Awards for 2009-2010\n\nMay 7, 2009 - Finalist in the 2009 asp.netPRO Readers' Choice Awards\n\nMarch 13, 2009 - Ranked #4 and #5 in the 2008 ComponentSource Bestselling Publishers List\n\nMarch 13, 2009 - Ranked #2 in the 2008 ComponentSource Bestselling Products List\n\nFebruary 9, 2009 - ActiveReports Customers Give .NET Reporting and BI Tools High Ratings\nJune 2, 2008 - SD Times 100 Finalist in the Components category\n\nMay 15, 2007 - SD Times 100 Finalist in the Components category\n\nFebruary 9, 2007 - 2006 ComponentSource Bestselling Publisher\n\nFebruary 9, 2007 - 2006 ComponentSource Bestselling Product\n\nMay 15, 2006 - SD Times 100 Winner in the Components category\n\nIn the past, ActiveReports was known to be unable to handle large reports. This issue was ongoing across years and versions. Since that time, development efforts have focused on improving large report handling in every release.\n\nActiveReports can be used in many ways, so each project can have a number of reasons for consuming memory. In newer versions, CacheToDisk and CacheToDiskLocation properties were added for PDF exports. Some other considerations that may cause too much memory use in section reports include:\n\n\n"}
{"id": "13292275", "url": "https://en.wikipedia.org/wiki?curid=13292275", "title": "Analytics in higher education", "text": "Analytics in higher education\n\nAcademic analytics is basically defined as the process of evaluating and analysing organisational data received from university systems for reporting and decision making reasons (Campbell, & Oblinger, 2007). According to Campbell & Oblinger (2007), accrediting agencies, governments, parents and students are all calling for the adoption of new modern and efficient ways of improving and monitoring student success. This has ushered the higher education system into an era characterised by increased scrutiny from the various stakeholders. For instance, the Bradley review acknowledges that benchmarking activities such as student engagement serve as indicators for gauging the institution’s quality (Commonwealth Government of Australia, 2008). \n\nIncreased competition, accreditation, assessment and regulation are the major factors encouraging the adoption of analytics in higher education. Although institutions of higher learning gather a lot of vital data that can significantly aid in solving problems like attrition and retention, the collected data is not being analysed adequately and hence translated into useful data (Goldstein, 2005.) Subsequently, higher education leadership are forced to make critical and vital decisions based on inadequate information that could be achieved by properly utilising and analysing the available data (Norris, Leonard, & strategic Initiatives Inc., 2008). This gives rise to strategic problems. This setback also depicts itself at the tactical level. Learning and teaching at institutions of higher education if often a diverse and complex experience. Each and every teacher, student or course is quite different. However, LMS is tasked with taking care of them all. LMS is at the centre of academic analytics. It records each and every student and staff’s information and results in a click within the system. When this crucial information is added, compared and contrasted with different enterprise information systems provides the institution with a vast array of useful information that can be harvested to gain a competitive edge (Dawson & McWilliam, 2008; Goldstein, 2005; Heathcoate & Dawson, 2005).\n\nIn order to retrieve meaningful information from institution sources i.e. LMS, the information has to be correctly interpreted against a basis of educational efficiency, and this action requires analysis from people with learning and teaching skills. Therefore, a collaborative approach is required from both the people guarding the data and those who will interpret it, otherwise the data will remain to be a total waste (Baepler & Murdoch, 2010). Decision making at its most basic level is based on presumption or intuition (a person can make conclusions and decisions based on experience without having to do data analysis) (Siemens & Long, 2011). However, a lot of decisions made at institutions of higher learning are too vital to be based on anecdote, presumption or intuition since significant decisions need to be backed by data and facts.\n\nAnalytics, which is often termed as business intelligence, has come out as new software and hardware that enables businesses to gather and analyse large amounts of information or data. The analytics process is made up of gathering, analysing, data manipulation and employing the results to answer critical questions such as ‘why’. Analytics was first applied in the admissions department in higher education institutions. The institutions normally used some formulas to choose students from a large pool of applicants. These formulas drew their information from high school transcripts and standardized test scores. In today’s world, analytics is commonly used in administrative units such as fund raising and admissions. The use and application of academic analytics is meant to grow due to the ever-increasing concerns about student success and accountability. Academic analytics primarily marries complex and vast data with predictive modelling and statistical techniques to better decision making. Current academic analytics initiatives are bent to use data to predict students experiencing difficulty (Arnold, & Pistilli, 2012, April). This allows advisors and faculty members to intervene by tailoring procedures which will meet the student’s learning needs (Arnold, 2010). As such, academic analytics possesses the ability to improve learning, student success and teaching. Analytics has become a valuable tool for institutions because of its ability to predict, model and improve decision making.\n\nAnalysis is made up of five basic steps: capture, report, predict, act and refine\nCapture: All analytic efforts are centred on data. Consequently, academic analytics can be rooted in data from various sources such as a CMS, and financial systems (Campbell, Finnegan, & Collins, 2006). Additionally, the data comes in various different formats for example spread sheets. Also, data can be got from the institution’s external environment. To capture data, academic analytics needs to determine the type of available data, methods of harnessing it and the formats it is in. \nReport: After the data has been captured and stored in a central location, analysts will examine the data, perform queries, identify patterns, trends and exceptions depicted by the data. The standard deviation and mean (descriptive statistics) are mostly generated.\n\nPredict: After analysing the warehoused data through the use of statistics, a predictive model is developed. These models vary depending on the question nature and type of data. To develop a probability, these models employ statistical regression concepts and techniques. Predictions are made after the use of statistical algorithms.\n\nAct: The major goal and aim of analytics is to enable the institution to take actions based on the probabilities and predictions made. These actions might vary from invention to information. The interventions to address problems might be in the form of a personal email, phone call or an automated contact from faculty advisors about study resources and skills, such as office hours or help sessions. Undoubtedly, institutions have to come up with appropriate mechanisms for impact measurement; such as did the students actually respond or attend the help sessions when invited.\nRefine: Academic analytics should also be made up of a process aimed at self-improvement. Statistics processes should be continually updated since the measurement of project impacts is not a one-time static effort but rather a continual effort. For instance, admission analytics should be updated or revised yearly. \n\nAnalytics affects executive officers, students, faculty members, IT staff and student affairs staff. Whereas students will be keen to know academic analytics will affect their grades, faculty members will be interested in finding out how the information and data can be appropriated for other purposes (Pistilli, Arnold & Bethune, 2012). Moreover, the institution staff will be focussed on finding how the analysis will enable them to effectively accomplish their jobs while the institution president will be focussed on freshman retention and increase in graduation rates.\n\nAnalytics have been criticised for various reasons such as profiling. Their main use is to profile students into successful and unsuccessful categories. However, some individuals argue that profiling of students tends to bias people’s behaviours and expectations (Ferguson, 2012). Additionally, there is no clear guidelines on which profiling issues should be prohibited or allowed in institutions of higher learning.\n\n\nArnold, K. E. (2010). Signals: Applying Academic Analytics. Educause Quarterly, 33(1), n1. (accountability)\n\nArnold, K. E., & Pistilli, M. D. (2012, April). Course Signals at Purdue: Using learning analytics to increase student success. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge (pp. 267-270). ACM.\n\nBaepler, P., & Murdoch, C. J. (2010). Academic analytics and data mining in higher education. International Journal for the Scholarship of Teaching and Learning, 4(2), 17.\n\nCampbell, J. P., & Oblinger, D. G. (2007). Academic Analytics. Educause Article.\n\nCampbell, J. P., Finnegan, C., & Collins, B. (2006). Academic analytics: Using the CMS as an early warning system. In WebCT impact conference.\nCommonwealth Government of Australia. (2008). Review of Australian Higher Education o. Document Number)\n\nDawson, S., & McWilliam, E. (2008). Investigating the application of IT generated data as an indicator of learning and teaching performance: Queensland University of Technology and the University of British Columbia. (A. L. a. T. Council o. Document Number)\n\nFerguson, R. (2012). Learning analytics: drivers, developments and challenges. International Journal of Technology Enhanced Learning, 4(5), 304-317.\nGoldstein, P. (2005). Academic analytics: The uses of management information and technology in Higher Education o. Document Number)\n\nHeathcoate, L., & Dawson, S. (2005). Data Mining for Evaluation, Benchmarking and Reflective Practice in a LMS. E-Learn 2005: World conference on E-Learning in corporate, government, healthcare and higher education.\n\nNorris, D. M., Leonard, J., & Strategic Initiatives Inc. (2008). What Every Campus Leader Needs to Know About Analytics o. Document Number)\n\nPistilli, M. D., Arnold, K., & Bethune, M. (2012). Signals: Using academic analytics to promote student success. EDUCAUSE Review Online, 1-8.\n\nSiemens, G., & Long, P. (2011). Penetrating the fog: Analytics in learning and education. Educause Review, 46(5), 30-32.\n"}
{"id": "31063787", "url": "https://en.wikipedia.org/wiki?curid=31063787", "title": "Anaplan", "text": "Anaplan\n\nAnaplan is a platform, powered by proprietary Hyperblock technology, purpose-built for Connected Planning, which enables dynamic, collaborative, and intelligent planning. Large global enterprises use the solution to connect people, data, and plans to enable real-time planning and decision-making in rapidly changing business environments to give customers a competitive advantage. Based in San Francisco, Anaplan has over 20 offices globally, 175 partners, and more than 900 customers worldwide. \n\nAnaplan was founded in 2006 by Guy Haddleton and Michael Gould in Yorkshire, UK. After four years of platform development, Anaplan was officially launched to the public in October 2010.\n\nBy the end of 2010, Anaplan had started to gain market traction. In 2012 Anaplan brought on Frederic Laluyaux as CEO, attracted new funding, and expanded. Laluyaux left the company in April 2016.\n\nThe company closed a Series B funding in January 2012 with a $11.4M investment coming from Granite Ventures and Shasta Ventures as the lead of investors of this round.\n\nIn February 2013 Anaplan acquired its reseller \"Vue Analytics\" in U.K. for an undisclosed amount, with the goal to strengthen its market position in Europe, Middle East and Africa. Anaplan closed its Series C funding in March 2013 with a $33M investment from Meritech, Shasta Ventures, Granite Ventures, Salesforce.com, and additional private investors.\n\nAnaplan announced at its Hub 2014 conference in San Francisco that it has completed a Series D round of $100 million in financing, bringing total investment in the company to $150 million. Lead investor of this round was DFJ Growth.\n\nIn 2015, Anaplan scaled up its cloud platform and updated the user interface.. In 2016, the company updated its App Hub, added platform enhancements, and improved tools for app-builders.\n\nAnother round of funding was announced in January 2016, amounting to $90 million, along with confirmation of Anaplan's status as a tech \"unicorn,\" with a valuation of over $1 billion. Lead investor of this Series E investment was PremjiInvest.\n\nIn January 2017 Anaplan appointed Frank A. Calderoni to be CEO.\n\nOn October 12, 2018, Anaplan listed with NYSE for public trading under the ticker symbol PLAN.\n\nAnaplan's product is a cloud computing, multi-tenant data architecture SaaS platform with a patented, in-memory calculation engine (the Hyperblock).\n\nAnaplan's Hyperblock architecture is a hybrid of relational, vertical, and OLAP databases with an in-memory data store multi-threaded calculation engine. The Hyperblock automatically records updates at a granular level by amending only the affected cells. As volumes scale, users can instantaneously update or change models. A patent application for the technology was filed on November 19, 2009, and US Patent 8151056B2 was awarded on April 3, 2012.\n\nThe “Living Blueprint” is the brain behind Anaplan’s in-memory platform. This master repository of business rules is where all the intelligence behind a model is generated and maintained. This allows adjustments to the model to be made in seconds, thus allowing the model to grow and adapt with a constantly changing business. Anaplan is delivered in a Software-as-a-Service (“SaaS”) model and is completely cloud-based. This eliminates the need for dedicated hardware or maintenance teams to make the platform work. It also enables “Zero Deployment” – users can log onto the platform on almost any device, from anywhere.\n\nIn November 2014 Anaplan announced the launch of the Anaplan App Hub, a community for Anaplan users to build, share, and deploy planning apps.\n\nIn 2015 Gartner labeled Anaplan a “Visionary” in their Magic Quadrant reports for both corporate performance management (CPM) and sales performance management (SPM). In March 2018, Workiva partnered with Anaplan to enhance wdesk performance.\n\nAnaplan has an annual global user conference: Connected Planning Xperience (CPX), formerly known as Anaplan Hub. The Anaplan community, including customers, prospects, partners, and Anaplan employees, come together for training, breakout sessions, keynotes, product announcements, and technical education.\n\nIn 2014 Anaplan expanded the Hub conference beyond its flagship event in San Francisco to include a world tour. The CPX tour now includes conferences in London, Paris, Singapore, and Tokyo. \n\nFor 2018, the main global event moved to Las Vegas, with Malcolm Gladwell as keynote speaker.\n"}
{"id": "2173412", "url": "https://en.wikipedia.org/wiki?curid=2173412", "title": "Apparent authority", "text": "Apparent authority\n\nIn the United States, the United Kingdom, Canada and South Africa, apparent authority (also called \"ostensible authority\") relates to the doctrines of the law of agency. It is relevant particularly in corporate law and constitutional law. Apparent authority refers to a situation where a reasonable third party would understand that an agent had authority to act. This means a principal is bound by the agent's actions, even if the agent had no \"actual\" authority, whether express or implied. It raises an estoppel because the third party is given an assurance, which he relies on and would be inequitable for the principal to deny the authority given. Apparent authority can legally be found, even if actual authority has not been given.\n\nThere must be some act or some knowing omission on the part of the principal - if the agent alone acts to give the third party this false impression, then the principal is not bound. However, the principal will be bound if the agent so acts in the \"presence\" of the principal, and the principal stands silently and says nothing to dissuade the third party from believing that the agent has the authority to bind the principal. Apparent authority can also occur where a principal terminates the authority of an agent, but does not inform third parties of this termination. This is called lingering apparent authority. Business owners can avoid being liable by giving public notice of the termination of authority, and by contacting any individual third parties who would have had reason to know of such authority.\n\nIn relation to companies, the apparent authority of directors, officers and agents of the company is normally referred to as \"ostensible authority.\" Apparent authority issues also arise in the Fourth Amendment context, concerning who has authority to consent to a search.\n\nThe doctrine of apparent authority is based on the concept of estoppel, thus, it prevents the principal from denying the existence of agency to a third party, provided that a representation, as to the agent's authority, has been made by him to the third party either through his words or by his actions.\n\nIn law, apparent authority refers to the authority of an agent as it appears to others, and it can operate both to enlarge actual authority and to create authority where no actual authority exists. The law relating to companies and to ostensible authority are in reality only a sub-set of the rules relating to apparent authority and the law of agency generally, but because of the prevalence of the issue in relation to corporate law (companies, being artificial persons, are only ever able to act at all through their human agents), it has developed its own specific body of case law. However, some jurisdictions use the terms interchangeably.\n\nIn \"Freeman and Lockyer v Buckhurst Park Properties (Mangal) Ltd\" [1964] 2 QB 480 the director in question managed the company's property and acted on its behalf and in that role employed the plaintiff architects to draw up plans for the development of land held by the company. The development ultimately collapsed and the plaintiffs sued the company for their fees. The company denied that the director had any authority to employ the architects. The court found that, while he had never been appointed as managing director (and therefore had no actual authority, express or implied) his actions were within his ostensible authority and the board had been aware of his conduct and had acquiesced in it. Diplock LJ identified four factors which must be present before a company can be bound by the acts of an agent who has no authority to do so; it must be shown that:\n\nThe agent must have been held out by someone with actual authority to carry out the transaction and an agent cannot hold himself out as having authority for this purpose. The acts of the company as principal must constitute a representation (express or by conduct) that the agent had a particular authority and must be reasonably understood so by the third party. In determining whether the principal had represented his agent as having such authority, the court has to consider the totality of the company's conduct. The most common form of holding out is permitting the agent to act in the conduct of the company's business, and in many cases this is inferred simply from allowing the agent to use a particular title, such as 'finance director'.\n\nThe apparent authority must not be undermined by any limitations on the company's capacity or powers found in the memorandum or articles of association, although in many countries, the effect of this is reduced by company law reforms abolishing or restricting the application of the \"ultra vires\" doctrine to companies. However, statutory reforms do not affect the general principle that a third party cannot rely upon ostensible authority where it is aware of some limitation which prevents the authority arising, or is put on enquiry as to the extent of an individual's authority. In some circumstances, the very nature of a transaction would be held to put a person on enquiry.\n\nThe rule in \"Turquand's\" case does not enable a third party to hold the company to an unauthorized transaction \"per se\". It allows a third party to assume that a transaction which is within the authority of the directors has been properly authorized, but it requires the third party to establish the fact of authority, actual or apparent, in the first place.\n\nIt is open to the principal to ratify an unauthorised agreement entered into by an agent. Ratification is the explicit or implicit action of the principal in agreeing, after the unauthorised act, to the act of the agent. Ratification by the principal causes such act to become binding on the third party. Note that without ratification by the principal, the third party is not bound to the unauthorized agreement created by an agent with no apparent authority, until the principal ratifies it. Whereas in the situation of an act done by an agent with ostensible (or apparent) authority, the principal and the third party are bound from the moment the agreement is consummated by the agent and third party.\n\n"}
{"id": "22550609", "url": "https://en.wikipedia.org/wiki?curid=22550609", "title": "Associate attorney", "text": "Associate attorney\n\nAn associate attorney is a lawyer and an employee of a law firm who does not hold an ownership interest as a partner.\n\nAn associate may be a junior or senior attorney, but normally does not hold an ownership interest in the firm even if associated with the firm for many years. First-year associates are entry-level junior attorneys and are generally recent law school graduates in their first year of law practice.\n\nGenerally, an associate has the goal of being made a partner in the firm, after a number of years gaining practice experience and being assigned to increasingly important and remunerative tasks. At firms with an \"up or out\" policy, associates who are repeatedly passed over for promotion to partner may be asked to resign. Some firms will also have \"non-partner-track\" associates who, though performing satisfactorily as employees, for whatever reason, will not be promoted to partner.\n\nJunior attorneys were formerly called \"law clerks\"; the term \"associate attorney\" was coined by Emory Buckner, hiring partner of Root, Clark & Bird (which subsequently became Dewey Ballantine) in the 1920s. The term \"law clerk\" now generally refers to an attorney who serves as a research and writing assistant in a judge's chambers, although some law firms use the term to refer to a lawyer or non-lawyer who has specialized knowledge in one of the firm's practice areas but is not classified as a practicing attorney at the law firm.\n\nSummer associates are current law students who have usually completed their second year of school (or in some cases, their first year of law school) and are interning at the firm for the summer. Summer associates have not passed the bar exam and are not attorneys. The summer associate program is often the primary method by which a law firm recruits for full-time associate positions.\n\nAccording to published data from the New York Times, the annual base salary for partner-track first year associate attorneys at top law firms in major U.S. legal markets such as New York, California, Massachusetts, the District of Columbia, and Texas can range from $160,000 to $180,000 per year—with salary varying depending on the size and reputation of the firm.\n"}
{"id": "38722262", "url": "https://en.wikipedia.org/wiki?curid=38722262", "title": "Ayasdi", "text": "Ayasdi\n\nAyasdi is a machine intelligence software company that offers a software platform and applications to organizations looking to analyze and build predictive models using big data or highly dimensional data sets. Organizations and governments have deployed Ayasdi's software across a variety of use cases including the development of clinical pathways for hospitals, anti-money laundering, fraud detection, trading strategies, customer segmentation, oil and gas well development, drug development, disease research, information security, anomaly detection, and national security applications.\n\nAyasdi focuses on hypothesis-free, automated analytics at scale. In effect, the Ayasdi system consumes the target data set, runs many different unsupervised and supervised machine learning algorithms on the data, automatically finds and ranks best fits, and then applies topological data analysis to find similar groups within the resultant data. It presents the end analysis in the form of a network similarity map, which is useful for an analyst to use to further explore the groupings and correlations that the system has uncovered. This reduces the risk of bias since the system surfaces \"what the data says\" in an unbiased fashion, rather than relying on analysts or data scientists manually running algorithms in support of pre-existing hypotheses. Ayasdi then generates mathematical models which are deployed in predictive and operational systems and applications.\n\nOrganizations using Ayasdi have found Ayasdi's automated, platform-based approach to machine intelligence to be two to five orders of magnitude more efficient than existing approaches to big data analytics, as measured in the amount of time and expense required to complete analysis and build models using large and complex data sets. One widely reported example at a top five global systemically important bank was that to build models required for the annual Comprehensive Capital Analysis and Review (CCAR) process took 1,800 person-months with traditional manual big data analytics and machine learning tools, but took 6 person-months with Ayasdi. A project at a second global systemically important bank showed Ayasdi reducing the time to build risk models from 3,000 person-hours to 10 minutes.\n\nAyasdi was founded in 2008 by Gunnar Carlsson, Gurjeet Singh, and Harlan Sexton after 12 years of research and development at Stanford University. While at Stanford, the founders received $1.25 million in DARPA and IARPA grants for \"high-risk, high-payoff research\". In 2012 Ayasdi landed a Series A round of funding led by Floodgate Capital and Khosla Ventures for $10.25 million. On July 16, 2013, Ayasdi closed $30.6 million in Series B funding from Institutional Venture Partners, GE Ventures, and Citi Ventures. On March 25, 2015, Ayasdi announced a new $55 million round of Series C funding, led by Kleiner Perkins Caufield & Byers, and joined by four current investors, Institutional Venture Partners, Khosla Ventures, Floodgate Capital, Citi Ventures, and two new investors, Centerview Capital Technology and Draper Nexus.\n\nAyasdi is a machine intelligence platform. It includes dozens of statistical and both supervised and unsupervised machine learning algorithms and can be extended to include whatever algorithms are required for a particular class of analysis. The platform is extensively automated and is in production at scale at many global 100 companies and at governments in the world. It features Topological Data Analysis as a unifying analytical framework, which automatically calculates groupings and similarity across large and highly dimensional data sets, generating network maps with greatly assist analysts in understanding how data clusters and which variables are relevant. When compared with manual approaches to statistical analysis and machine learning, results with Ayasdi will typically be achieved much faster to achieve and more accurate due to the automation and scalability built into the platform. The Ayasdi platform also develops mathematical models, including predictive models, based on the results of the analysis. This allows Ayasdi to deployed as an operational system, or as a part of operational systems, and not just for analysis.\n\nAyasdi can be deployed on-premises using Intel-based servers, or on either public or private cloud infrastructure. The platform runs on Linux and Hadoop.\n\nAyasdi also develops machine intelligence applications. One example is Ayasdi Care, which is a suite of cloud-based applications for healthcare providers that is focused on managing and improving patient outcomes, revenue and population health. For example, Ayasdi clinical variation, one of the applications in Ayasdi Care, automatically discovers the ideal care paths for medical procedures based on analyzing historical patient data, billing records and insurance claims.\n\nAyasdi customers include many large enterprises, medical research institutions and governments across industries including health care, financial services, oil and gas, security, life sciences, and the public sector.\n\n"}
{"id": "44253151", "url": "https://en.wikipedia.org/wiki?curid=44253151", "title": "Base erosion and profit shifting", "text": "Base erosion and profit shifting\n\nBase erosion and profit shifting or BEPS refers to corporate tax planning strategies used by multinationals to \"shift\" profits from higher–tax jurisdictions to lower–tax jurisdictions, thus \"eroding\" the \"tax–base\" of the higher–tax jurisdictions. The OECD defines BEPS strategies as also: \"exploiting gaps and mismatches in tax rules\"; however, academics proved corporate tax havens (e.g. Ireland, the Caribbean, Luxembourg, the Netherlands, Singapore, Switzerland, and Hong Kong), who are the largest global BEPS hubs, use OECD–whitelisted tax structures and OECD–compliant BEPS tools. Corporate tax havens offer BEPS tools to \"shift\" profits to the haven, and additional BEPS tools to avoid paying taxes within the haven (e.g. Ireland's \"Green Jersey\"). BEPS tools are mostly associated with U.S. technology and life science multinationals. Tax academics showed use of BEPS tools by U.S. multinationals, via tax havens, maximised long–term U.S. exchequer receipts and/or shareholder returns, at the expense of other jurisdictions.\n\nA January 2017 OECD report estimates that BEPS tools are responsible for tax losses of circa $100–240 billion per annum. A June 2018 report by tax academic Gabriel Zucman (et alia), estimated that the figure is closer to $200 billion per annum. The Tax Justice Network estimated that profits of $660 billion were \"shifted\" in 2015 (due to Apple's Q1 2015 leprechaun economics restructuring, the largest individual BEPS transaction in history). The effect of BEPS tools is most felt in developing economies, who are denied the tax revenues needed to build infrastructure.\n\nMost BEPS activity is associated with industries with intellectual property (\"IP\"), namely Technology (e.g. Apple, Google, Microsoft, Oracle), and Life Sciences (e.g. Allergan, Medtronic, Pfizer and Merck & Co) (see here). IP is described as the \"raw materials of tax avoidance\", and IP–based BEPS tools are responsible for the largest global BEPS income flows. Corporate tax havens have some of the most advanced IP tax leglislation in their statutate books.\n\nMost BEPS activity is also most associated with U.S. multinationals, and is attributed to the historical U.S. \"worldwide\" corporate taxation system. Pre the Tax Cuts and Jobs Act of 2017 (\"TCJA\"), the U.S. was one of only eight jurisdictions to operate a \"worldwide\" tax system. Most global jurisdictions operate a \"territorial\" corporate tax system with lower tax rates for foreign sourced income, thus avoiding the need to \"shift\" profits (i.e. IP can be charged directly from the home country at preferential rates and/or terms; the TCJA now enables this in the U.S. with the FDII–regime).\n\nResearch in June 2018, identified Ireland as the world's largest BEPS hub. Ireland is larger than the \"aggregate\" Caribbean tax haven BEPS system. The largest global BEPS hubs, from the Zucman–Tørsløv–Wier table below, are synonymous with the top 10 global tax havens:\n\n(†) Mostly consists of The Cayman Islands and The British Virgin Islands\n\nResearch in September 2018, by the National Bureau of Economic Research, using repatriation tax data from the TCJA, said that: \"In recent years, about half of the foreign profits of U.S. multinationals have been booked in tax haven affiliates, most prominently in Ireland (18%), Switzerland, and Bermuda plus Caribbean tax havens (8%–9% each). One of the authors of this research was also quoted as saying: “Ireland solidifies its position as the #1 tax haven.”; and also that: “U.S. firms book more profits in Ireland than in China, Japan, Germany, France & Mexico combined. Irish tax rate: 5.7%.”\n\nResearch identifies three main BEPS techniques used for \"shifting\" profits to a corporate tax haven via OECD–compliant BEPS tools:\n\nBEPS tools could not function if the corporate tax haven did not have a network of bilateral tax treaties that accept the haven’s BEPS tools, which \"shift\" the profits to the haven. Modern corporate tax havens, who are the main global BEPS hubs, have extensive networks of bilateral tax treaties. The U.K. is the leader with over 122, followed by the Netherlands with over 100. The \"blacklisting\" of a corporate tax haven is a serious event, which is why major BEPS hubs are OECD-compliant. Ireland was the first major corporate tax haven to be \"blacklisted\" by a G20 economy; Brazil, in September 2016.\n\nAn important academic study in July 2017 published in \"Nature\", \"Conduit and Sink OFCs\", showed that the pressure to maintain OECD–compliance had split corporate–focused tax havens into two different classifications: Sink OFCs, which act as the \"terminus\" for BEPS flows, and Conduit OFCs, which act as the \"conduit\" for flows from higher–tax locations to the Sink OFCs. It was noted that the 5 major Conduit OFCs, namely, Ireland, the Netherlands, the United Kingdom, Singapore and Switzerland, all have a top–ten ranking in the 2018 \"Global Innovation Property Centre (GIPC) IP Index\".\n\nOnce profits are \"shifted\" to the corporate tax haven (or Conduit OFC), additional tools are used to avoid paying \"headline\" tax rates in the haven. Some of these tools are OCED–compliant (e.g. patent boxes, Capital Allowances for Intangible Assets (\"CAIA\") or \"Green Jersey\"), others became OECD–proscribed (e.g. Double Irish and Dutch Double–Dipping), while others have not attracted OECD attention (e.g. Single Malt).\n\nBecause of the need for BEPS hubs (or Conduit OFCs) to have extensive bilateral tax treaties (e.g. so that their BEPS tools will be accepted by the higher–tax locations), they go to great lengths to obscure the fact that \"effective tax rates\" paid by multinationals in their jurisdiction are close to zero percent, rather than the \"headline\" corporate tax rate of the haven (see Table 1). Higher–tax jurisdictions do not enter into full bilateral tax treaties with obvious tax havens (e.g. the Cayman Islands, a major Sink OFC). This is achieved with financial secrecy laws, and by the avoidance of \"country–by–country reporting\" (\"CbCr\") or the need to file public accounts, by multinationals in the haven's jurisdiction. BEPS hubs (or Conduit OFCs) strongly deny they are corporate tax havens, and that their use of IP is as a tax avoidance tool. They call themselves \"knowledge economies\".\n\nThe complex accounting tools, and the detailed tax legislation, that corporate tax havens require to become OECD–compliant BEPS hubs, requires both advanced international tax–law professional services firms, and a high degree of coordination with the State, who encode their BEPS tools into the State's statutory legislation. Tax investigators call such jurisdictions \"captured states\", and explain that most leading BEPS hubs started as established financial centres, where the necessary skills and State support for tax avoidance tools, already existed.\n\nThe BEPS tools used by tax havens have been known and discussed for decades in Washington. For example, when Ireland was pressured by the EU-OECD to close its double Irish BEPS tool, the largest in history, to new entrants in January 2015, existing users, which include Google and Facebook, were given a 5-year extension to 2020. Even before 2015, Ireland had already publicly replaced the double Irish with two new BEPS tools: the single malt (as used by Microsoft and Allergan), and capital allowances for intangible assets (\"CAIA\"), also called the \"Green Jersey\", (as used by Apple in Q1 2015). None of these new BEPS tools have been as yet proscribed by the OECD. Tax experts show that disputes between higher-tax jurisdictions and tax havens are very rare.\n\nTax experts describe a more complex picture of an implicit acceptance by Washington that U.S. multinationals could use BEPS tools on non–U.S. earnings to offset the very high U.S. 35% corporate tax rate from the historical U.S. \"worldwide\" corporate tax system (see source of contradictions). Other tax experts, including a founder of academic tax haven research, James R. Hines Jr., note that U.S. multinational use of BEPS tools and corporate tax havens had actually increased the long–term tax receipts of the U.S. exchequer, at the expense of other higher–tax jurisdictions, making the U.S a major beneficiary of BEPS tools and corporate-tax havens. \n\nThe 1994 Hines–Rice paper on U.S. multinational use of tax havens was the first to use the term \"profit shifting\". Hines–Rice concluded that: \"low foreign tax rates [from tax havens] ultimately enhance U.S. tax collections\". For example, the Tax Cuts and Jobs Act of 2017 (\"TCJA\") levied 15.5% on the untaxed offshore cash reserves built up by U.S. multinationals with BEPS tools from 2004–2017. Had these U.S. multinationals not used BEPS tools, and paid their full foreign taxes, their foreign tax credits would have removed most of their residual exposure to any U.S. tax liability, under the U.S. tax code.\n\nThe U.S. was one of the only major developed nations not to sign up to the 2016 to curtail BEPS tools.\n\nThe 2012 G20 Los Cabos summit tasked the OECD to develop a \"BEPS Action Plan\", which 2013 G-20 St. Petersburg summit approved. An OECD BEPS Multilateral Instrument, consisting of \"15 Actions\" designed to be implemented domestically and through bilateral tax treaty provisions, were agreed at the 2015 G20 Antalya summit.\n\nThe OECD BEPS Multilateral Instrument (\"MLI\"), was adopted on 24 November 2016 and has since been signed by over 78 jurisdictions. It came into force in July 2018. Many tax havens opted–out from several of the Actions, including Action 12 (Disclosure of aggressive tax planning), which was considered onerous by corporations who use BEPS tools.\n\nThe acknowledged \"architect\" of the largest ever global corporate BEPS tools (e.g. Google and Facebooks' Double Irish and Apple's Green Jersey), tax partner Feargal O'Rourke from PriceWaterhouseCoopers (\"PwC), predicted in May 2015 that the OECD's MLI would be a success for the leading corporate tax havens, at the expense of the smaller, less developed, traditional tax havens, whose BEPS tools were not sufficiently robust.\n\nIn August 2016, the Tax Justice Network's Alex Cobham described the OECD's MLI as a failure due to the opt–outs and watering–down of individual BEPS Actions. In December 2016, Cobham highlighted that one of the critical anti–BEPS Actions, full public \"country–by–country–reporting\" (\"CbCr\"), had been dropped due to lobbying by the U.S. multinationals. CbCr is the only way to conclusively observe the level of BEPS activitiy and OECD compliance in any country.\n\nIn June 2017, a U.S. Treasury official explained that the reason why U.S. refused to sign up to the OECD's MLI, or any of its Actions, was because: \"the U.S. tax treaty network has a low degree of exposure to base erosion and profit shifting issues\".\n\nThe Tax Cuts and Jobs Act of 2017 (\"TCJA\") moved the U.S. from a \"worldwide\" corporate tax system to a hybrid \"territorial\" tax system. The TCJA includes anti–BEPS tool regimes including the GILTI–tax and BEAT–tax regimes. It also contains its own BEPS tools, namely the FDII–tax regime. The TCJA could represent a major change in Washington's tolerance of U.S. multinational use of BEPS tools. Tax experts in early 2018 forecast the demise of the two major U.S. corporate tax havens, Ireland and Singapore, in the expectation that U.S. multinationals would no longer need foreign BEPS tools.\n\nHowever, by mid–2018, U.S. multinationals had not repatriated any BEPS tools, and the evidence is that they have increased exposure to corporate tax havens. In March–May 2018, Google committed to doubling its office space in Ireland, while in June 2018 it was shown that Microsoft is preparing to execute Apple's Irish BEPS tool, the \"Green Jersey\" (see Irish experience post–TCJA). In July 2018, one of Ireland's leading tax experts forecasted a potential boom in U.S. multinationals on–shoring their BEPS tools from the Caribbean to Ireland, and not to the U.S. as was expected after TCJA.\n\nIn May 2018, it was shown that the TCJA contains technical issues that incentivise these actions. For example, by accepting Irish tangible, and intangible, capital allowances in the GILTI calculation, Irish BEPS tools like the \"Green Jersey\" enable U.S. multinationals to achieve \"U.S. effective tax rates\" of 0–3% via the TCJA's foreign \"participation relief\" system. There is debate as to whether these are drafting mistakes to be corrected, or concessions to enable U.S. multinationals to reduce their effective corporate tax rates to circa 10% (the Trump administration's original target).\n\n\n"}
{"id": "56669014", "url": "https://en.wikipedia.org/wiki?curid=56669014", "title": "Bleisure travel", "text": "Bleisure travel\n\nBleisure travel (UK /ˈbleʒ.əʳ/ US /ˈbliː.ʒɚ/) is a portmanteau of “business” and “leisure”, and, it refers to “the activity of combining business travel with leisure time”.\n\nThe term bleisure was first published in 2009 by the Future Laboratory as part of their biannual Trend Briefing. The term was originally coined by writer and silent revolutionist, Jacob Strand, then a future forecaster working for The Future Laboratory. And co-written by journalist and futurologist Miriam Rayman.\n\nIn corporate business travel, extending a business trip for personal purposes is also known as “bizcation”\n\nThis phenomenon has been studied from 2011, from this year on, a report shows that bleisure travel has been maintaining a constant growth, accounting for 7% of all business trips.\n\nBleisure travellers can be described as “individuals who combine leisure with professional business obligations when abroad”.\n\nThe elements characterising bleisure travellers are some and different, this make difficult to draw a defined profile of these individuals. Bleisure is a widespread practice among US travellers, especially for those working in Technology, Healthcare, Public Administration sectors. A report shows that US traveller add bleisure to nearly half of the cases, precisely 52% for International trips and 42% for domestic ones. The main reasons for travelling would be conferences and conventions, team building, client meetings and presentations.\n\nBleisure travellers could be grouped according to various aspects, in particular gender, age and trip frequency.\n\nA research indicates that female business travellers would be more likely to take bleisure trips than male. In both groups 20% of travellers would take one or more bleisure trips in a year time, however women look to register higher rate: 8.5% against 6.8% of men.\n\nYounger travellers (between 20 and 25) look significantly more likely to add weekends to their business trips, measuring a rate close to 15%. This result is two or three times higher than those for 45-50 age interval. Millennials travellers deserve a special attention because they appear to be the individuals shaping the future of business travel, nearly twice likely to travel for business than Baby Boomers. This could be attributed to their flexible attitude to life, which would lead to the blurring of the line between their private and professional life. Millennials seem to work often from home or during the evening hours, to dedicate leisure on-goings to traditional work hours. A consequence of this freedom could be their willingness to extend business trips including vacation activities. In comparison with other age group, Millennials look to be more likely to take bleisure trips, compared to older travellers. However, it is not clear whether this reflects unique preferences or transitory life events.\n\nFrequent travellers taking 20 trips or more per year seem to be less than 5% likely to take a bleisure trip during the year, they would account for 8% of all bleisure trips. By contrast, one third of all bleisure trips would be taken by employees travelling once a month.\n\nIt seems evident that Millennials workers are increasingly demanding for bleisure from their work trip. Despite the state-of-the-art technology available nowadays, Millennials still prefer face-to-face meetings to get business done, therefore travel is still important for business companies.\n\nBleisure travel looks as a good opportunity to save on travel expenses, mostly for those who do not take a lot of vacation: 66% of US business traveller spend more money on leisure activities because of the money they save on travel. 60% take bleisure trips because do not have a lot of regular vacations. To decide whether to turn a business travel into bleisure or not, employee takes into consideration travelling on exciting destinations, additional costs required to extend the trip, how close the trip is to the weekend, the number of night they must stay for business, how affordable the hotel is, whether they have friends or family in the area, whether they can bring friends or family along. The general trend would indicate the addition of one or two days as the most frequent option, only 23% of bleisure travellers would extend their trips for more than three days. In many cases leisure days equal (37%) or even exceed business days (42%). The length of business trip seems to be a crucial factor when considering extending the trip in 62% of cases. In particular, when business days are more than three, bleisure travellers are not only likely to extend the trip, but also to visit different cities in the area.\n\nFor the employees:\nFor the companies:\n\nThe more likely bleisure destinations are cities offering a combination of different elements, which could allow the travellers to add leisure time to their business trip, such as sightseeing locations, sport venues and cultural events.\n\nThe difference between the origin and the destination city seems to affect bleisure rates: in the case of domestic trips, the amount of bleisure is expected to be low because the two cities should be easily accessible and share similar culture. On the other hand, long intercontinental trips would offer the chance to know realities that are different from ordinary. Researches seem to confirm this trend: bleisure trips would account for 5.2% of the domestic trips and 9.7% of the international ones. The highest bleisure rate (18.4%) appears in international trips when the origin and destination cities are located in different geographical regions.\n\nThe top bleisure cities in US are: Honolulu (above 20%), Miami, Orlando, Las Vegas (12-15%) NYC, San Francisco, Los Angeles, San Diego and Fort Lauderdale (9-12%). In Europe rates looks lower, this could be due to the shorter domestic and continental routes: Lisbon, Barcelona, Nice, Istanbul (8-11%), London, Dublin, Amsterdam, Moscow, Rome, Geneva, Paris, Madrid (6-8%).\n\nGiven that bleisure travel is a rising phenomenon, there is still no official regulation, therefore extending business trips for bleisure would have to be decided by management on a case-by-case basis. Some preventive measures could be followed in advance to avoid misunderstanding.\n"}
{"id": "41672405", "url": "https://en.wikipedia.org/wiki?curid=41672405", "title": "BlueSpice MediaWiki", "text": "BlueSpice MediaWiki\n\nBlueSpice MediaWiki (BlueSpice for short) is free wiki software based on MediaWiki and licensed by GNU General Public License. It is especially developed for businesses as an enterprise wiki distribution for MediaWiki and used in over 150 countries.\n\nBlueSpice is published in two editions:\n\n\nBoth editions are software collections, which are based on MediaWiki. The extensions are developed by Hallo Welt! GmbH, its technology partners or independent MediaWiki developers.\n\nBlueSpice is mostly used as\n\n\nSome central features of BlueSpice are:\n\n\nBlueSpice is written in the PHP programming language and uses MySQL, Apache/IIS, Tomcat (optional). The editions can be installed on top of an existing MediaWiki installation or as a standalone installation that includes MediaWiki.\n\nThe distribution is a collection of extensions, which can be extended with user-specific features or skins. While every single extension can be deactivated, BlueSpice editions integrate and standardize extensions to improve the user experience and maintenance.\n\nAccording to the MediaWiki standard all extensions are published under the GPLv3 license.\n\nThe German company Hallo Welt! has been working on developing the open source wiki software BlueSpice since 2007. The project was originally initiated by IBM (\"bluepedia\"), who wanted to deploy MediaWiki but was unable to live with its downsides.\n\nIn 2011, Hallo Welt! decided to publish their wiki as free Open source software. The stable version of BlueSpice for MediaWiki was released July 4, 2011. From this point on, a free download has been available at SourceForge. The first release of BlueSpice was a couple of extensions and is today a complete stand-alone distribution, which has the latest MediaWiki as a core system but offers in the free version more than 50 distinct extensions and a completely different user interface. Following independent sources the BlueSpice free distribution is one of the most popular wiki software for knowledge management in organisations.\n\nIn autumn 2013, Hallo Welt! released the completely reworked version BlueSpice 2. According to the BlueSpice developers this release aims for opening up BlueSpice for freelance developers in the global MediaWiki community, and lays the foundation for many new language versions.\n\nIn 2014, BlueSpice for MediaWiki became a project of Translatewiki.net.\nIn January 2015 the developers announced that they will change to a subscription model.\n\n\n"}
{"id": "14204007", "url": "https://en.wikipedia.org/wiki?curid=14204007", "title": "Business Intelligence 2.0", "text": "Business Intelligence 2.0\n\nBusiness Intelligence 2.0 (BI 2.0) is a future development of the existing business intelligence model that began in the mid-2000s, where data can be obtained from many sources. The process allows for the querying of real-time corporate data by employees, but approaches the data with a web browser based solution. This is in contrast to previous proprietary querying tools that characterizes previous BI software.\n\nThe growth in service-oriented architectures (SOA) is one of the main factors for this happening. BI 2.0 is intended to be more flexible and adaptive than normal business intelligence. Data exchange processes also differ, with XBRL (Extensible Business Reporting Language), Web Services and various Semantic Web ontologies enable using data external to an organization, such as benchmarking type information.\n\nBusiness Intelligence 2.0 is believed to have been named after Web 2.0, although it takes elements from both Web 2.0 (a focus on user empowerment and community collaboration, technologies like RSS and the concept of mashups), and the Semantic Web, sometimes called \"Web 3.0\" (semantic integration through shared ontologies to enable easier exchange of data).\n\nAccording to analytics expert Neil Raden, BI 2.0 also implies a move away from the standard data warehouse that business intelligence tools have used, which \"will give way to context, contingency and the need to relate information quickly from many sources.\"\n\n\n\n"}
{"id": "53360944", "url": "https://en.wikipedia.org/wiki?curid=53360944", "title": "Business metadata", "text": "Business metadata\n\nBusiness metadata is data that adds business context to other data. It provides information authored by business people and/or used by business people. It is in contrast to technical metadata, which is data used in the storage and structure of the data in a database or system. Technical metadata includes the database table name and column name, data type, indexes referencing the data, ETL jobs involving the data, when the data was last updated, accessed, etc.\n\nAccording to noted author and columnist Lowell Fryman, \"The essence of business metadata is in reducing or eliminating the barriers of communication between human and human, as well as human and computer, so that the data conveyed from reports, information systems, or business intelligence applications can be crystal clear, can facilitate business operations, and can be leveraged for all business decision-making processes.\"\n\nDan Linstedt, creator of the Data Vault methodology, says business metadata \"...provide[s] definition of the functionality, definition of the data, definition of the elements, and definition of how the data is used within business...business metadata includes business requirements, time-lines, business metrics, business process flows, and business terminology.\"\n\nBusiness metadata is important because it can greatly facilitate the usefulness of the data to business people. A simple example of business metadata is a glossary entry. Hover functionality in an application or web form can enable a glossary definition to be shown when cursor is on a field or term.\n\nOther examples of business metadata include annotation ability within applications. For example, a business user may be viewing a business intelligence (BI) report and notice a trend in the data. The user may have background knowledge as to why this trend occurs. Some business intelligence tools enable the user to create an annotation within the report that explains the trend. Such an annotation can enhance other users' understanding of the data. This example is especially powerful because it is created by a business user for the use of other business people.\n\nOther examples of business metadata are:\n"}
{"id": "1780904", "url": "https://en.wikipedia.org/wiki?curid=1780904", "title": "Business park", "text": "Business park\n\nA business park or office park is an area of land in which many office buildings are grouped together. The first office park opened in Mountain Brook, Alabama, in the early 1950s to avoid racial tension in city centers.\n\nThese are popular in many suburban locations, where development is cheaper because of the lower land costs and the lower building costs for building wider, not necessarily higher. Some businesses prefer the larger floorplates as more efficient, reducing time lost moving between floors. They are also often located near motorways or main roads for easy access.\n\nThe impact of these areas on the urban fabric has been criticized:\n\n\n\n"}
{"id": "4722073", "url": "https://en.wikipedia.org/wiki?curid=4722073", "title": "CCU delivery", "text": "CCU delivery\n\nCustomer Configuration Updating (CCU) is a software development method for structuring the process of providing customers with new versions of products and updates production. This method is developed by researchers of the Utrecht University.\n\nThis article is about the delivery phase of the CCU method. Delivery concerns the process which starts at the moment a product is finished until the actual shipping of the product to the customer.\n\nAs described in the general entry of CCU, the delivery phase is the second phase of the CCU method. In figure one the CCU method is depicted. The phases of CCU that are not covered in this article are concealed by a transparent grey rectangle.\n\nAs can be seen in figure one, the delivery phase is in between the release phase and the deployment phase. A software vendor develops and releases a software product and afterwards it has to be transported to the customer. This phase is the delivery process. This process is highly complex because the vendor often has to deal with a product which has multiple versions, variable features, dependency on external products, and different kinds of distribution options. The CCU method helps the software vendor in structuring this process.\n\nIn figure 2, the process-data diagram of the delivery phase within CCU is depicted. This way of modeling was invented by Saeki (2003). On the left side you can see the meta-process model and on the right side the meta-data model. The two models are linked to each other by the relationships visualized as dotted lines. The meta-data model (right side) shows the concepts involved in the process and how the concepts are related to each other. For instance it is visible that a package consists of multiple parts, being the: software package, system description, manual, and license and management information. The numbers between the relations indicate in what quantity the concepts are related. For example the “1..1” between package and software package means that a package has to contain at least 1 software package and at the most 1 software package. So in this case a package just has to contain 1 software package. On the left side of the picture the process-data model is depicted. This consists of all the activities within the delivery process. This article is based on this process-data model. The meta-process model (left side of the process-data diagram) is divided into several parts which are presented along with the corresponding paragraphs throughout the article to make it easier to understand.\n\nThe tables that describe the concepts of the meta-data model and the activities of the process-data model are presented beneath figure 2.\n\nThe table of concepts contains all concepts used in the meta-data model with their explanations along with the source from which the explanations are derived.\n\nThe activity table contains the explanations of the activities along with the source from which the explanations are derived. Because the method is quite innovative a lot of the activity’s are designed especially for this model and therefore the explanations do not have a source.\n\nIn order to deliver the developed product to the customer, the vendor needs to package the different components of its product into a package. By doing this, the customer will receive all the information and software components at once fulfilling all its needs. After combining all elements into one package the software vendor will carefully have to check if the package is complete. The package will have to provide the customer with all the tools and information to use the product. When this is not the case the software vendor will get a lot of questions from its customers which will consume a lot of time. It is therefore very important that the package is checked carefully before it is shipped. The package can be a physical combination of different elements packed into for example a box, but it can also be a digital combination of files which contain all the elements. Within the CCU process it is stated that a package will consist of five elements, being: software package, system description, manual, and license and management information. In the following paragraphs is explained how these elements fit into the CCU delivery phase.\n\nOne of the elements of the package will be the software package. The software package is a package in itself, because it consists of the different software components that together form the product. In contrast with the overall package, the software package is always a technical package in which all the files needed are combined in order to run the software product. Another concept of the software package is the version. This keeps track of the modifications made to the software product. By relating it to the software package the vendor and the customer are able to keep track of the functionality and properties of the product the customer is using.\n\nIt is a general description of what the product and its functionalities. In addition it will also describe of what components, the product consists and how these are related to other product software already in place. In case of a software update it will for example describe how the previous version of the software is modified by this product. Besides this, it will also describe the requirements needed to run the software product properly. For example what other products and configurations need to be in place in order to let this product run properly.\n\nThe manual is the document that will provide the customer with guidance in deploying and using the product.\n\nThe license is in this case a Software license agreement in which is stated how the customer is permitted to use the product. For example it can state how many users are permitted to use the software product. In this situation the license agreement is a contract or a certificate which is the customers prove of its using permits. The software vendor has its own part of the agreement which in most cases is stored in a system. An elaboration of this part can be found at the receive feedback section of this article. The license agreement shipped to the customer can be a digital document as well as a physical document.\n\nThis piece of information should contain the information that is relevant for managing the system at the customer site. In many cases this information is already part of the manual. However in particular situations this information is meant only for the management of the system and not for the users of the system and is therefore supplied as a separate document.\n\nAfter the package is assembled it needs to be distributed to the customers. This section within the delivery process is about the actual delivery of the package to the customers.\n\nThe software distribution of a product can be done offline as well as online. In an offline situation the package is a physical package which contains all the elements. The software is stored on a data carrier such as a CD or a DVD, and the documents might also be stored in a digital form on this data carrier, or they might be in physical form such as a booklet. The package as a whole is a physical product. In an online situation the entire package needs to be in a digital form. The consequences on the distribution process are described in the following paragraphs. CCU is designed to fit both situations but as bandwidth is growing it is making more sense to distribute especially updates and new versions to existing customers online. In this article both ways are discussed. In the process-data model it is assumed that the software vendor conducts both distribution channels. As a practical example: HISComp, a provider of medical information systems distributes its software straightforward via CDs. However they use their website to distribute patches for the software products.\n\nAfter a new package is assembled, the customer needs to be made aware of the new release. In the process-data model this is being depicted as a loop which states advertising the update until the customers are being properly informed. Besides this, the package ready for delivery, needs to be stored in a repository for the online distribution. In addition the vendor needs to create transfer channels. For the online distribution this means that the vendor needs to create online channels to its repository. In most cases this means that a link to the product on the website of the vendor is created. In case of updates it is largely applicable that the current version of the software product at the customer site automatically checks the repository for new updates of the product. In case of offline distribution, the vendor needs to create physical transfer channels. This can be shops or just a contract with a courier company.\n\nThe distribution begins with the request for a product by the customer. This can be done automatically when the current product of the customer searches for an update at the online repository. The customer can also manually do a request for a product via the website of the vendor. A third option is that the customer does the request via telephone or e-mail.\n\nWhen the vendor is aware of the customer request it will determine the customer needs. By checking what the customer current configuration is and what the customer desires. This process can also take place automatically by checking the customer configuration in the configuration management system. More information on this system is provided in the next chapter. When it is clear what product the customer needs and the possible modifications to this product it is necessary to determine if the customer current configuration suits the new product. The current configuration is compared to the constraints of the new product. This can also be done automatically by the configuration management system. When the configuration of the customer appears to be insufficient the customer is informed about this. For example the vendor can make clear to the customer that it will need an external product for this new product to run properly. Besides this the Customer Relationship Management (CRM) system of the vendor is updated. There is more information about this in the chapter about CRM.\n\nWhen the customer configuration is sufficient the vendor will check the current license of the customer. If the customer does not have a proper license for the requested product the license needs to be obtained. The customer will be informed about this and the CRM system will be updated again. If the customer has the proper license or wants to buy the proper license along with the product, the product is delivered to the customer.\n\nThe Software Configuration Management system, is a system at the vendor’s site which keeps track of the configurations at the customer site. By storing this in a system the vendor will be able to give the customer particular service when it needs a new product. In the software configuration management system information about the products used by the customer, the version of these products, as well as which updates are already being done, is stored. In some cases it is possible that the vendor did some modifications to the product particularly for this customer. This will also have to be stored in the system. Also there needs to be configuration data, some generic information about the configuration the customer is using. For example what operating platform the customer uses for its software. What also should be stored in this system is information about the feedback that the vendor gets from the customer. This includes bug reports, product usage data, error reports and usage questions. More information about this feedback can be found in the CCU phase activation and usage.\n\nBy storing all this information the vendor can determine the customer needs very precisely whenever a customer requests a product or an update. As already stated the vendor can also easily inform the customer about some adaptations the customer needs to make to its configuration in order to let the product function properly. Another advantage of storing this information in a system is that it will ease the process of online delivery. The checking of the configuration needs and constraints can all be done automatically when a customer does a request.\n\nThe customer relationship management system contains all kinds of data about the customers of a company. In this article we will discuss the function of this customer data in the CCU delivery process. Information about the license agreement between the customer and the software vendor is stored in the CRM system. In the meta-data model this repository and online distribution is linked to the CRM system this can again be done automatically. The system will check if the license of a customer is sufficient to obtain a certain product or update.\n\nIn order to keep all the described systems up-to-date at the vendor site it is important that the vendor receives a lot of\n\nAn example of a successful application of the CCU method can be found at Exact Software (ES). ES is a manufacturer of accounting and enterprise resource planning software based in the Netherlands. ES has combined Product Data Management (PDM), Customer Relationship Management (CRM) and Software Configuration Management (SCM) in order to maintain the configuration at the customer site in a better and less complex way. ES has a module in its CRM software that contains all contracts of each customer. This is linked to their PDM system. Every contract corresponds to files that can be downloaded for a new version or update of a previous version. In the delivery phase this means that the customers are able to obtain all the products through an online connection. So ES sells contracts (licenses) and stores them into their CRM system, the delivery of the actual products can be done by the customers themselves completely automated requiring little effort. The PDM system is on its turn linked to the SCM system which keeps track of the configurations the customers are using. In the delivery phase this means that ES is able to automatically determine the customer needs whenever a customer does a request.\n\n\n"}
{"id": "23154038", "url": "https://en.wikipedia.org/wiki?curid=23154038", "title": "Channel coordination", "text": "Channel coordination\n\nChannel coordination (or supply chain coordination) aims at improving supply chain performance by aligning the plans and the objectives of individual enterprises. It usually focuses on inventory management and ordering decisions in distributed inter-company settings. Channel coordination models may involve multi-echelon inventory theory, multiple decision makers, asymmetric information, as well as recent paradigms of manufacturing, such as mass customization, short product life-cycles, outsourcing and delayed differentiation. The theoretical foundations of the coordination are based chiefly on the contract theory. The problem of channel coordination was first modeled and analyzed by Anantasubramania Kumar in 1992.\n\nThe decentralized decision making in supply chains leads to a dilemma situation which results in a suboptimal overall performance called double marginalization. Recently, partners in permanent supply chains tend to extend the coordination of their decisions in order to improve the performance for all of the participants. Some practical realizations of this approach are Collaborative Planning, Forecasting, and Replenishment (CPFR), Vendor Managed Inventory (VMI) and Quick Response (QR).\n\nThe theory of channel coordination aims at supporting the performance optimization by developing arrangements for aligning the different objectives of the partners. These are called coordination mechanisms or schemes, which control the flows of information, materials (or service) and financial assets along the chains. In general, a contracting scheme should consist of the following components:\n\n\nThe appropriate planning methods are necessary for optimizing the behavior of the production. The second component should support the information visibility and transparency both within and among the partners and facilitates the realization of real-time enterprises. Finally, the third component should guarantee that the partners act upon to the common goals of the supply chain.\n\nThe general method for studying coordination consists of two steps. At first, one assumes a central decision maker with complete information who solves the problem. The result is a first-best solution which provides bound on the obtainable system-wide performance objective. In the second step one regards the decentralized problem and designs such a contract protocol that approaches or even achieves the performance of the first-best.\n\nA contract is said to coordinate the channel, if thereby the partners' optimal local decisions lead to optimal system-wide performance. Channel coordination is achievable in several simple models, but it is more difficult (or even impossible) in more realistic cases and in the practice. Therefore, the aim is often only the achievement of mutual benefit compared to the uncoordinated situation.\n\nAnother widely studied alternative direction for channel coordination is the application of some negotiation protocols. Such approaches apply iterative solution methods, where the partners exchange proposals and counter-proposals until an agreement is reached. For this reason, this approach is commonly referred to as collaborative planning. The negotiation protocols can be characterized according to the following criteria:\n\n\nAn also commonly used instrument for aligning plans of different decision makers is the application of some auction mechanisms. However, “auctions are most applicable in pure market interactions at the boundaries of a supply chain but not within a supply chain″, therefore they are usually not considered as channel coordination approaches.\n\nThere are several classifications of channel coordination contracts, but they are not complete, and the considered classes are not disjoint. Instead of a complete classification, a set of aspects are enumerated below which generalizes the existing taxonomies by allowing classification along multiple viewpoints.\n\nMost of the related models consider either one-period horizon or two-period horizon with forecast update. In the latter, the production can be based on the preliminary forecast with normal production mode or on the updated forecast with emergency production, which means shorter lead-time, but higher cost. Besides, the horizon can consist of multiple periods and it can be even infinite. The practically most widespread approach is the rolling horizon planning, i.e., updating and extending an existing plan in each period.\n\nAlmost all contract-based models regard only one product. Some models study the special cases of substitute or complementary products. However, considering more products in the general case is necessary if technological or financial constraints—like capacity or budget limits—exist.\n\nOn one hand, the demand can be stochastic (uncertain) or deterministic. On the other hand, it can be considered static (constant over time) or dynamic (e.g., having seasonality).\n\nIn most of the models the players are regarded to be risk neutral. This means that they intend to maximize their expected profit (or minimize their expected costs). However, some studies regard risk averse players who want to find an acceptable trade-off considering both the expected value and the variance of the profit.\n\nThe models differ in their attitude towards stockouts. Most authors consider either backlogs, when the demand must be fulfilled later at the expense of providing lower price or lost sales which also includes some theoretical costs (e.g., loss of goodwill, loss of profit, etc.). Some models include a service level constraint, which limits the occurrence or quantity of expected stockouts. Even the 100% service level can be achieved with additional or emergency production (e.g., overtime, outsourcing) for higher costs.\n\nThis viewpoint shows the largest variations in the different models. The main decision variables are quantity-related (production quantity, order quantity, number of options, etc.), but sometimes prices are also decision variables. The parameters can be either constant or stochastic. The most common parameters are related to costs: fixed (ordering or setup) cost, production cost and inventory holding cost. These are optional; many models disregard fixed or inventory holding costs. There exist numerous other parameters: prices for the different contracts, salvage value, shortage penalty, lead-time, etc.\n\nMost of the one-period models apply the newsvendor model. On two-period horizon, this is extended with the possibility of two production modes. On a multiple period horizon the base-stock, or in case of deterministic demand the EOQ models are the most widespread. In such cases the optimal solution can be determined with simple algebraic operations. These simple models usually completely disregard technological constraints; however, in real industrial cases resource capacity, inventory or budget constraints may be relevant. This necessitates more complex models, such as LP, MIP, stochastic program, and thus more powerful mathematical programming techniques may be required.\n\nAs for the optimization criteria, the most usual objectives are the profit maximization or cost minimization, but other alternatives are also conceivable, e.g., throughput time minimization. Considering multiple criteria is not yet prevalent in the coordination literature.\n\nThe most often studied dilemmas involve the two players and call them customer and supplier (or buyer-seller). There are also extensions of this simple model: the multiple customers with correlated demand and the multiple suppliers with different production parameters. Multi-echelon extensions are also conceivable, however, sparse in the literature. When the coordination is within a supply chain (typically a customer-supplier relation), it is called vertical, otherwise horizontal. An example for the latter is when different suppliers of the same customer coordinate their transportation.\n\nSometimes the roles of the participants are also important. The most frequently considered companies are manufacturers, retailers, distributors or logistic companies.\n\nOne of the most important characteristics of the coordination is the power relations of the players. The power is influenced by several factors, such as possessed process know-how, number of competitors, ratio in the value creation, access to the market and financial resources.\n\nThe players can behave in a cooperative or opportunistic way. In the former case, they share a common goal and act like a team, while in the latter situation each player is interested only in its own goals. These two behaviors are usually present in a mixed form, since the opportunistic claims for profitability and growth are sustainable usually only with a certain cooperative attitude.\n\nThe relation can be temporary or permanent. In the temporary case usually one- or two-period models are applied, or even an auction mechanism. However, the coordination is even more important in permanent relations, where the planning is usually done in a rolling horizon manner. When coordinating a permanent supply relation, one has to consider the learning effect, i.e., players intend to learn each other's private information and behavior.\n\nThe simplest possible coordination is aimed only at aligning the (material) flows within the supply chain in order to gain executable plans and avoid shortages. In a more advanced form of coordination, the partners intend to improve supply chain performance by approaching or even achieving the optimal plan according to some criteria. Generally, a coordinated plan may incur losses for some of the players compared to the uncoordinated situation, which necessitates some kind of side-payment in order to provide a win-win situation. In addition, even some sort of fairness may be required, but it is not only hard to guarantee, but even to define.\n\nMost of the coordination approaches requires that the goal should be achieved in an equilibrium in order to exclude the possibility that an opportunistic player deviates from the coordinated plan.\n\nSome papers study the symmetric information case, when all of the players know exactly the same parameters. This approach is very convenient for cost and profit sharing, since all players know the incurring system cost. The asymmetric case, when there is an information gap between the players is more realistic, but poses new challenges. The asymmetry typically concerns either the cost parameters, the capacities or the quantities like the demand forecast. The demand and the forecast are often considered to be qualitative, limited to only two possible values: high and low. In case of stochastic demand, the uncertainty of the forecasts can also be private information.\n\nThe decision making roles of the players depend on the specified decision variables. However, there is a more-or-less general classification in this aspect: forced and voluntary compliance. Under forced compliance the supplier is responsible for satisfying all orders of the customer, therefore it does not have the opportunity to decide about the production quantity. Under voluntary compliance, the supplier decides about the production quantity and it cannot be forced to fill an order. This latter is more complex analytically, but more realistic as well. Even so, several papers assume that the supplier decides about the price and then the customer decides the order quantity.\n\nFrom the viewpoint of game theory the models can take cooperative or non-cooperative approaches. The cooperative approach studies, how the players form coalitions therefore these models are usually applied on the strategic level of network design. Other typical form of cooperative games involves some bargaining framework—e.g., the Nash bargaining model—for agreeing upon the parameters of the applied contracts.\n\nOn the other hand, on the operational level, the non-cooperative approach is used. Usually the sequential Stackelberg game model is considered, where one of the players, the leader moves first and then the follower reacts. Both cases—the supplier or the customer as the Stackelberg leader—are widely studied in the literature. In case of information asymmetry, a similar sequential model is used and it is called principal–agent setting. The study of the long-term supply relationship can also be modeled as a repeated game.\n\nTo sum up, a collaboration generally consists of a cooperative, followed by a non-cooperative game. However, most researches concentrate only on one of the phases.\n\nSome coordination mechanisms require the existence of an independent, trusted third party. If such a mediator exists, the powerful theory of the market mechanism design can be applied for channel coordination. Although at first glance the involvement of a third party seems to be unrealistic, in the area of planning such mediators already exist as application service providers.\n\nThere are many variants of the contracts, some widespread forms are\nbriefly described below. Besides, there exist several combinations\nand customized approaches, too.\n\nIn this case the customer\npays not only for the purchased goods, but in addition a\nfixed amount called franchise fee per order. This\nis intended to compensate the supplier for his fixed setup\ncost.\n\nThis contract specifies two prices\nand a quantity threshold. If the order size is below\nthe threshold, the customer pays the higher price, and\nif it is above, she pays a lower price for the\nunits above the threshold.\n\nUnder quantity discount contract, the customer pays a wholesale price depending on the order quantity. This resembles to the sales rebate contract, but there is no threshold defined. The mechanism for specifying the contract can be complex. The contract has been applied in many situations, for example, in an international supply chain with fluctuating exchange rates.\n\nWhile advance capacity purchase is popular in the supply chain practice, there are situations where a manufacturer prefers to delay its capacity purchase to have better information about the uncertain demand.\n\nWith these types of contracts the\nsupplier offers that it will buy back the remaining\nobsolete inventory at a discounted price. This\nsupports the sharing of inventory risk between the\npartners. A variation of this contract is the\nbackup agreement, where the customer gives a\npreliminary forecast and then makes an order less or\nequal to the forecasted quantity. If the order is\nless, it must also pay a proportional penalty for the\nremaining obsolete inventory. Buyback agreements are\nwidespread in the newspaper, book, CD and fashion\nindustries.\n\nIn this case the customer\ngives a preliminary forecast and then it can give fixed\norder in an interval around the forecast. Such contracts\nare widespread in several markets, e.g., among the\nsuppliers of the European automotive industry.\n\nWith revenue sharing the customer\npays not only for the purchased goods, but also shares a\ngiven percentage of her revenue with the supplier. This\ncontract is successfully used in video cassette rental\nand movie exhibition fields. It can be proved, that the\noptimal revenue sharing and buyback contracts are\nequivalent, i.e., they generate the same profits for the\npartners.\n\nThe option contracts are originated from\nthe product and stock exchange. With an option contract,\nthe customer can give fixed orders in advance, as well\nas buy rights to purchase more (call option) or return\n(put option) products later. The options can be bought at a\npredefined option price and executed at the\nexecution price. This approach is a\ngeneralization of some previous contract types.\n\nThis contract can be used when the buyer does not order, only communicates the forecasts and consumes from the inventory filled by the supplier. The VMI contract specifies that not only the consumed goods should be paid, but also the forecast imprecision, i.e., the difference between the estimated and realized demand. In this way, the buyer is inspired to increase the forecast quality, and the risk of market uncertainty is shared between the partners.\n\n\n"}
{"id": "39105755", "url": "https://en.wikipedia.org/wiki?curid=39105755", "title": "Chemonics", "text": "Chemonics\n\nChemonics International is a private international development company that works for bilateral and multilateral donors and the private sector to manage projects in developing countries. The organization bids primarily on contracts from the U.S. Agency for International Development (USAID) and manages projects that cover a variety of technical sectors. These sectors include agriculture and food security, corporate partnerships, democracy and governance, economic growth, education and youth, environment and natural resources, gender equality, social inclusion, health, peace and stability, supply chain solutions, and water. In addition to its headquarters in Washington, D.C., the company also has project offices in different countries, covering Asia, Africa, Eurasia, Europe, Latin America, and the Middle East.\n\nChemonics was founded in 1975 by Thurston F. (Tony) Teele.\n\nChemonics has been subject to criticism from the US Agency for International Development (USAID) Office of Inspector General (OIG) for their work on several multimillion-dollar aid contracts.\n\nIn 2012 Chemonics came under scrutiny by the OIG for their work in Haiti after the 2010 earthquake. Chemonics was the largest single recipient of post-earthquake funds from USAID, receiving over $196 million in contracts many of which were \"no-bid.\"\n\nAudits specifically cited Chemonics lack of a comprehensive monitoring and evaluation plan and that \"some of the performance indicators Chemonics developed were not well-defined.\" Chemonics also spent more than 75 percent of program budgets on material and equipment when an expenditure of only 30 percent was planned.\n\nAn Inspector General's report also found that local communities were not sufficiently involved with Chemonics' work and stated \"Chemonics used contractors from Port-au-Prince to implement a number of activities in Cap-Haitien and Saint-Marc; these contractors brought their own people to do the jobs instead of hiring locals.\" When locals were required by USAID, Chemonics' policies \"limited the transparency of the selection process and increase the risk of corruption or favoritism by granting decision-making authority to a few individuals.\"\n\nIn November 2006, USAID Afghanistan awarded a $62 million contract to Chemonics, with an expected end date of March 2010. A 2008 audit of the contract by OIG found that Chemonics' \"results fell considerably short of intended results\" and \"buildings constructed by Chemonics’ subcontractors were not acceptable because of significant construction defects.\"\n\nIn 2016, A Department of Labor investigation into Chemonics’ hiring practices found that the group discriminated against applicants based on race while trying to fill entry-level positions. According to the Guardian, none of the 124 Black Americans who applied for the jobs in Chemonics International’s regional business units were hired. Chemonics agreed to pay $482,243 to job applicants who were subjected to racial discrimination in the company’s hiring process, the Guardian reported.\n"}
{"id": "56864778", "url": "https://en.wikipedia.org/wiki?curid=56864778", "title": "Chen Lip Keong", "text": "Chen Lip Keong\n\nTan Sri Dr. Chen Lip Keong is a Malaysian businessman.\n\n"}
{"id": "6422341", "url": "https://en.wikipedia.org/wiki?curid=6422341", "title": "Chief creative officer", "text": "Chief creative officer\n\nA chief creative officer (CCO) is the highest ranking position of the creative team within a company. Depending on the type of company, this position may be responsible for the overall look and feel of marketing, media, and branding associated with the organization. The CCO may also be charged with managing, developing, and leading the team of creative directors, art directors, designers, and copywriters.\n\nThe CCO directs a company's creative output, developing the artistic design strategy that defines the company's brand. The CCO creates the unique image of the firm and deliver this distinctive design to consumers and to create a clear brand image which is a fundamental and essential work throughout the company. Advertisements present a certain memorable artistic design while also structured to accomplish functional goals. The CCO ensures that the design and functionality combine harmoniously so the firm can present a product that successfully represents its creative brand.\n\nCCOs, in some cases are called creative directors, are demanded in firms which involve creative thinking and artistic design. The position of generic head of marketing is usually replaced by chief creative officer in advertising firms. The power of chief creative officer can even be compared to CEO and at the early stages of a small company, usually at the start of partnerships, the CCO and the CEO manage jointly.\n\n"}
{"id": "10853282", "url": "https://en.wikipedia.org/wiki?curid=10853282", "title": "Chief gaming officer", "text": "Chief gaming officer\n\nA chief gaming officer (abbreviated as CGO) is an executive position whose holder is focused on research and technical issues within a computer game company.\n\nThe first two companies that have employed this executive position are Bigpoint and Golden Worlds Entertainment Media Group. \n\nThe chief gaming officer or chief game officer is in charge of heading both the game development and the online/offline publishing functions of the company.\n\nThe CGO has authority to manage the online game production cycle from start to finish. As head of game development, he or she is solely responsible for the conceptualization and planning of new games, the budget allocations to different gaming projects, the budget allocation within each game, assembling a team of software developers and game designers, and ultimately, the approval of new games.\n\nAs head of online publishing, the CGO also manages the online publishing, web portal, localization, and user interaction for all of the company products. The CGO negotiates and executes the appropriate agreements for web hosting and networking services for game delivery to the community, resolving issues with local internet service providers, and making sure that appropriate online payment agreements and systems are in place in each country. In addition to the online publishing of individual games, he acts as the portal manager, coordinating the development and publishing teams for the company web portal, which is of vital importance for a company delivering its products and collecting its user fees exclusively through the Internet.\n\nThe CGO has the ultimate decision-making authority over the localization function, being the final person responsible for adapting each game to the geographical market where it is delivered. He interfaces directly with the community manager and marketing manager to ensure that the final product is delivered and marketed in the most effective way, consistently integrating customer feedback into new releases.\n\nThe CGO reports directly to the chief operating officer.\n\n"}
{"id": "9481604", "url": "https://en.wikipedia.org/wiki?curid=9481604", "title": "Commitment control", "text": "Commitment control\n\nThis is a term used in the retail industry. Retailers contract with their suppliers to supply a quantity of product in a specified time period to a specific location at an agreed price. The quantity and value of the balance of contract represents a risk to the retailer from the point of view that the retailer will have to discount a product that experiences a slump in demand. The setting of targets and maxima at various levels in the retailer's product hierarchies and time dimensions; and the subordination of purchasing to these limits, is known as \"Commitment control\".\n"}
{"id": "180472", "url": "https://en.wikipedia.org/wiki?curid=180472", "title": "Consumer privacy", "text": "Consumer privacy\n\nConsumer privacy is a form of information privacy concerned with the legal and political issues arising from the interaction of the public's expectation of privacy with the collection and dissemination of data by businesses or merchants. Consumer privacy concerns date back to the first commercial couriers and bankers who enforced strong measures to protect customer privacy. In modern times, the ethical codes of most professions specify measures to protect customer privacy, including medical privacy, client confidentiality, and national security. Since most organizations have a competitive incentive to retain exclusive access to customer data, and since customer trust is usually a priority, many companies adopt security engineering measures to protect customer privacy.\n\nConsumer privacy protection is the use of laws and regulations to protect individuals from privacy loss due to the failures and limitations of corporate customer privacy measures. Corporations may be inclined to share data for commercial advantage and fail to officially recognize it as sensitive to avoid legal liability in the chance that lapses of security may occur. Modern consumer privacy law originated from telecom regulation when it was recognized that a telephone company had access to unprecedented levels of information. Customer privacy measures were seen as deficient to deal with the many hazards of corporate data sharing, corporate mergers, employee turnover, and theft of data storage devices (e.g., hard drives) that could store a large amount of data in a portable location.\n\nConsumer privacy concerns date back to the first commercial couriers and bankers who enforced strong measures to protect customer privacy. Harsh punitive measures were passed as the result of failing to keep a customer's information private. In modern times, the ethical codes of most professions specify privacy measures for the consumer of any service, including medical privacy, client confidentiality, and national security. These codes are particularly important in a carceral state, where no privacy in any form nor limits on state oversight or data use exists.\n\nCorporate customer privacy practices are approaches taken by commercial organizations to ensure that confidential customer data is not stolen or abused. Since most organizations have strong competitive incentives to retain exclusive access to these data, and since customer trust is usually a high priority, most companies take some security engineering measures to protect customer privacy. There is also a concern that companies may sell consumer data if they have to declare bankruptcy, although it often violates their own privacy policies. These vary in effectiveness, and would not typically meet the much higher standards of client confidentiality applied by ethical codes or legal codes in banking or law, nor patient privacy measures in medicine, nor rigorous national security measures in military and intelligence organizations.\n\nSince they operate for-profit, commercial organizations also cannot spend unlimited funds on precautions while remaining competitive; a commercial context tends to limit privacy measures and to motivate organizations to share data when working in partnership. The damage done by privacy loss is not measurable, nor can it be undone, and commercial organizations have little or no interest in taking unprofitable measures to drastically increase the privacy of customers. Corporations may be inclined to share data for commercial advantage and fail to officially recognize it as sensitive to avoid legal liability in the chance that lapses of security may occur. This has led to many moral hazards and customer privacy violation incidents.\n\nSome services—notably telecommunications, including Internet—require collecting a vast array of information about users’ activities in the course of business, and may also require consultation of these data to prepare bills. In the US and Canada, telecom data must be kept for seven years to permit dispute and consultation about phone charges. These sensitivities have led telecom regulation to be a leader in consumer privacy regulation, enforcing a high level of confidentiality on the sensitive customer communication records.\n\nFocusing on telecom has been outmoded to some degree as other industries also gather sensitive data. Such common commercial measures as software-based customer relationship management, rewards programs, and target marketing tend to drastically increase the amount of information gathered (and sometimes shared). These very drastically increase privacy risks and have accelerated the shift to regulation, rather than relying on the corporate desire to preserve goodwill.\n\nConcerns have led to consumer privacy laws in most countries, especially in the European Union, Australia, New Zealand and Canada. Notably, among developed countries, the United States has no such law and relies on corporate customer privacy disclosed in privacy policies to ensure consumer privacy in general. Modern privacy law and regulation may be compared to parts of the Hippocratic Oath, which includes a requirement for doctors to avoid mentioning the ills of patients to others—not only to protect them, but to protect their families— and also recognizes that innocent third parties can be harmed by the loss of control of sensitive personal information.\n\nModern consumer privacy law originated from telecom regulation when it was recognized that a telephone company—especially a monopoly (known in many nations as a PTT)—had access to unprecedented levels of information: the direct customer's communication habits and correspondents and the data of those who shared the household. Telephone operators could frequently hear conversations—inadvertently or deliberately—and their job required them to dial the exact numbers. The data gathering required for the process of billing began to become a privacy risk as well. Accordingly, strong rules on operator behaviour, customer confidentiality, records keeping and destruction were enforced on telephone companies in every country. Typically only police and military authorities had legal powers to wiretap or see records. Even stricter requirements emerged for various banks' electronic records In some countries, financial privacy is a major focus of the economy, with severe criminal penalties for violating it.\n\nThrough the 1970s many other organizations in developed nations began to acquire sensitive data, but there were few or no regulations in place to prevent them from sharing or abusing the data. Customer trust and goodwill were generally thought to be sufficient in first-world countries, notably the United States, to ensure the protection of truly sensitive data; \"caveat emptor\" was applied in these situations. But in the 1980s, smaller organizations also began to get access to computer hardware and software, and these simply did not have the procedures or personnel or expertise, nor less the time, to take rigorous measures to protect their customers. Meanwhile, via target marketing and rewards programs, companies were acquiring ever more data.\n\nGradually, customer privacy measures were seen as deficient to deal with the many hazards of corporate data sharing, corporate mergers, employee turnover, and theft of data storage devices (e.g., hard drives) that could store a large amount of data in a portable location. Explicit regulation of consumer privacy gained further support, especially in the European Union, where each nation had laws that were incompatible (e.g., some restricted the collection, some the compilation, and some the dissemination of data); it was possible to violate privacy within the EU simply doing these things from different places in the European Common Market as it existed before 1992.\n\nThrough the 1990s, the proliferation of mobile telecom, the introduction of customer relationship management, and the use of the Internet in developed nations brought the situation to the forefront, and most countries had to implement strong consumer privacy laws, often over the objections of business. The European Union and New Zealand passed particularly strong laws that were used as a template for more limited laws in Australia and Canada and some states of the United States (where no federal law for consumer privacy exists, although there are requirements specific to banking and telecom privacy). In Austria around the 1990s, the mere mention of a client's name in a semi-public social setting was enough to earn a junior bank executive a stiff jail sentence.\n\nAfter the terrorist attacks against the United States on September 11, 2001, privacy took a back-seat to national security in legislators' minds. Accordingly, concerns of consumer privacy in the United States have tended to go unheard of as questions of citizen privacy versus the state, and the development of a police state or carceral state, have occupied advocates of strong privacy measures. Whereas it may have appeared prior to 2002 that commercial organizations and the consumer data they gathered were of primary concern, it has appeared since then in most developed nations to be much less of a concern than political privacy and medical privacy (e.g., as violated by biometrics). Indeed, people have recently been stopped at airports solely due to their political views, and there appears to be minimal public will to stop practices of this nature.\n\n"}
{"id": "39248723", "url": "https://en.wikipedia.org/wiki?curid=39248723", "title": "Contract failure", "text": "Contract failure\n\nContract failure describes a situation in which the consumer of a good or service is unable to evaluate its quality, thus incentivizing the producer to produce a lower quality good or service. Such behavior creates suboptimal economic conditions. Contract failure is one explanation for the existence of non-profit organizations, although even non-profits can fall victim to contract failure in the right situations. Contract failure is related to, but distinct from, market failure. Generally, non-profit organizations are more trusted because their corporate structures do not provide incentives to cheat.\n\nThe known cause of contract failure is called information asymmetry; when one party (the producer) has more information than the other party (the consumer) about a product or service. There is information inequality between the two parties. According to Young, there are three causes in which situations dealing with asymmetric information arise from, to include the following, 1) the quality of a product or service is too complex to be judged such as medical care or higher education; 2) the end consumer of the product or service cannot evaluate it him or herself such as a child in daycare or an elderly individual in a nursing home; and 3) the product or service is not consumed by the individual who purchased it, therefore the purchaser would never know if the producer delivered what was promised.\n\nWhen contract failure occurs, there is a suboptimal provision of public goods, which results in market failure. Arrow argues that nonprofits will step in and provide the necessary good or service in response to market failure. When markets potentially take advantage of the information asymmetry situation, nonprofits must protect the consumer.\n\nAccording to Hansmann, the “non-distribution constraint - prohibits the distribution of residual earnings to individuals who exercise control over the firm”. It prohibits those who have a vested interest in the organization from receiving the organization’s profit for personal gain. This constraint is a common characteristic of nonprofits, which creates less of an incentive for the organization to take advantage of the consumer’s lack of knowledge. The nonprofit has no reason to cheat the consumer out of quality or service delivery because the organization's individuals cannot benefit directly. Therefore, the consumer is more likely to trust a nonprofit organization providing services than to trust a for-profit organization because of the nonprofit’s non-distribution constraint. According to Easley and O'Hara, state law stipulates that the organizations day-to-day costs should be reasonable.\n"}
{"id": "36808856", "url": "https://en.wikipedia.org/wiki?curid=36808856", "title": "Costimator", "text": "Costimator\n\nCostimator is an American-based series of Cost Estimating software developed by Thomas Charkiewicz in 1982 and is designed to model manufacturing costs. The software is designed, developed and marketed by MTI of West Springfield, MA.\n\nCostimator was designed by Thomas Charkiewicz, a former machinist and manufacturing manager who studied computer-aided manufacturing at the University of Massachusetts. Costimator was released in late 1982, designed to model manufacturing costs.\n\nIn 2002, IBM bought Costimator OEM from MTI Systems, Inc. Many of the products on the market focused on machine shop operations. Costimator had its roots in the machining industry, but later branched out to include several other manufacturing processes.\n\nJohn Kagan, the former manager of PC Cost Management at IBM and Lenovo estimated that between 2003 and 2004, IBM saved more than $10 million using Costimator.\n\nCostimator also includes a function known as IQ builder which customers can use to model their own manufacturing process based on historical data. The data was derived from the company’s customers in addition to various independent industry sources. Labor costs come from the U.S. Bureau of Labor Statistics.\n\nIn 2009, MTI Systems, Inc. reached an agreement with European distributor Premier Manufacturing Solutions, LTD (Stockport, UK) to market its flagship products Costimator OEM and Costimator JS to companies throughout Europe. In February 2011, Cimtronics and MTI Systems, Inc. also agreed to terms on a distribution partnership.\n\n"}
{"id": "43574082", "url": "https://en.wikipedia.org/wiki?curid=43574082", "title": "Daigou", "text": "Daigou\n\nDaigou (Chinese: 代购 dàigòu (); also 海外代购 hǎiwài dàigòu), Overseas personal shopper\nis a channel of commerce in which a person outside of China purchases commodities (mainly luxury goods but also groceries) for a customer in mainland China, \nsince prices for luxury goods can be 30 to 40 percent higher in China than abroad. \nThe phrase means \"buying on behalf of\". \n\"Daigou\" sales across sectors total $15 billion annually. \nIn 2014 the value of the \"daigou\" business just in luxury goods increased from CN¥55 billion to CN¥75 billion yuan (USD $8.8 billion to $12 billion).\n\n\"Daigou\" purchases are often from luxury brand boutiques in major fashion cities like Paris, London, New York City, Hong Kong, Tokyo and Seoul. Some \"daigou\" operators use Weibo and WeChat to communicate with their clients. \nThe large demand for \"daigou\" service is due to concern over unsafe products, especially food safety problems, \nand China's high import tariffs on luxury goods. \nSome \"daigou\" service providers intentionally sell counterfeit made-in-China products that have been altered to appear purchased abroad. \nA 2015 survey of Chinese online luxury shoppers found that 35% have used \"daigou\" to purchase luxury goods online, while only 7% used the website of the brand they are buying, or think they are buying.\nApproximately 80% of Chinese luxury purchases are made abroad.\nAsian-American sales associates at Macy's Herald Square sued Macy's for racial discrimination in September 2017, alleging that store managers instructed sales associates not to sell more than one unit to any single Asian customer, and that they were fired when they spoke up about the alleged discrimination.\n\nThe daigou business is thriving in Australia and the government has also shown its support to recognize the job-creating potential of this grey industry.\n\n\n"}
{"id": "24093035", "url": "https://en.wikipedia.org/wiki?curid=24093035", "title": "Decision-making software", "text": "Decision-making software\n\nDecision-making software (DM software) is a term for computer applications that help individuals and organisations make choices and take decisions, typically by ranking, prioritizing or choosing from a number of options.\n\nAn early example of DM software was described in 1973. Before the advent of the World Wide Web, most DM software was spreadsheet-based, with the first web-based DM software appearing in the mid-1990s. Nowadays, many DM software products (mostly web-based) are available – e.g. see the comparison table below.\n\nMost DM software focuses on ranking, prioritizing or choosing from among alternatives characterized on multiple criteria or attributes. Thus most DM software is based on decision analysis, usually multi-criteria decision-making, and so is often referred to as \"decision analysis\" or \"multi-criteria decision-making\" software – commonly shortened to \"decision-making software\". Some decision support systems include a DM software component.\n\nDM software can assist decision-makers “at various stages of the decision-making process, including problem exploration and formulation, identification of decision alternatives and solution constraints, structuring of preferences, and tradeoff judgements.”\n\nThe purpose of DM software is to support the analysis involved at these various stages of the decision-making process, not to replace it. DM software \"should be used to support the process, not as the driving or dominating force.\" \n\nDM software frees users \"from the technical implementation details [of the decision-making method employed], allowing them to focus on the fundamental value judgements\". Nonetheless, DM software should not be employed blindly. \"Before using a software, it is necessary to have a sound knowledge of the adopted methodology and of the decision problem at hand.\"\n\nAs mentioned earlier, most DM software is based on multi-criteria decision making (MCDM). MCDM involves evaluating and combining alternatives' characteristics on two or more criteria or attributes in order to rank, prioritize or choose from among the alternatives.\n\nDM software employs a variety of MCDM methods; popular examples include:\n\nThere are significant differences between these methods and, accordingly, the DM software implementing them. Such differences include:\n\nIn the process of helping decision-makers to rank, prioritize or choose from among alternatives, DM software products often include a variety of features and tools; common examples include:\n\nDM software includes the following notable examples.\n\n"}
{"id": "22062208", "url": "https://en.wikipedia.org/wiki?curid=22062208", "title": "Discovery-driven planning", "text": "Discovery-driven planning\n\nDiscovery-driven planning is a planning technique first introduced in a \"Harvard Business Review\" article by Rita Gunther McGrath and Ian C. MacMillan in 1995 and subsequently referenced in a number of books and articles. Its main thesis is that when one is operating in arenas with significant amounts of uncertainty, that a different approach applies than is normally used in conventional planning. In conventional planning, the correctness of a plan is generally judged by how close outcomes come to projections. In discovery-driven planning, it is assumed that plan parameters may change as new information is revealed. With conventional planning, it is considered appropriate to fund the entire project, as the expectation is that one can predict a positive outcome. In discovery-driven planning, funds are released based on the accomplishment of key milestones or checkpoints, at which point additional funding can be made available predicated on reasonable expectations for future success. Conventional project management tools, such as stage-gate models or the use of financial tools to assess innovation, have been found to be flawed in that they are not well suited for the uncertainty of innovation-oriented projects \n\nDiscovery-driven planning has been widely used in entrepreneurship curricula and has recently been cited by Steve Blank as a foundational idea in the lean startup methodology \n\nA discovery-driven plan incorporates five disciplines or plan elements:\n\nUsing discovery-driven planning, it is often possible to iterate the ideas in a plan, encouraging experimentation at lowest possible cost. The methodology is consistent with the application of real options reasoning to business planning, in which ventures are considered \"real\" options. A real option is a small investment made today which buys the right, but not the obligation to make further investments.\n\n\n"}
{"id": "239863", "url": "https://en.wikipedia.org/wiki?curid=239863", "title": "Distribution (marketing)", "text": "Distribution (marketing)\n\nDistribution (or place) is one of the four elements of the marketing mix. Distribution is the process of making a product or service available for the consumer or business user that needs it. This can be done directly by the producer or service provider, or using indirect channels with distributors or intermediaries. The other three elements of the marketing mix are product, pricing, and promotion.\n\nDecisions about distribution need to be taken in line with a company's overall strategic vision and mission. Developing a coherent distribution plan is a central component of strategic planning. At the strategic level, there are three broad approaches to distribution, namely mass, selective or exclusive distribution. The number and type of intermediaries selected largely depends on the strategic approach. The overall distribution channel should add value to the consumer.\n\nDistribution is fundamentally concerned with ensuring that products reach target customers in the most direct and cost efficient manner. In the case of services, distribution is principally concerned with access. Although distribution, as a concept, is relatively simple, in practice distribution management may involve a diverse range of activities and disciplines including: detailed logistics, transportation, warehousing, storage, inventory management as well as channel management including selection of channel members and rewarding distributors.\n\nPrior to designing a distribution system, the planner needs to determine what the distribution channel is to achieve in broad terms. The overall approach to distributing products or services depends on a number of factors including the type of product, especially perishability; the market served; the geographic scope of operations and the firm's overall mission and vision. The process of setting out a broad statement of the aims and objectives of a distribution channel is a strategic level decision.\n\nStrategically, there are three approaches to distribution:\n\n\n\nSummary of strategic approaches to distribution\n\nIn consumer markets, another key strategic level decision is whether to use a push or pull strategy. In a \"push strategy\", the marketer uses intensive advertising and incentives aimed at distributors, especially retailers and wholesalers, with the expectation that they will stock the product or brand, and that consumers will purchase it when they see it in stores. In contrast, in a \"pull strategy\", the marketer promotes the product directly to consumers hoping that they will pressure retailers to stock the product or brand, thereby pulling it through the distribution channel. The choice of a push or pull strategy has important implications for advertising and promotion. In a push strategy the promotional mix would consist of trade advertising and sales calls while the advertising media would normally be weighted towards trade magazines, exhibitions and trade shows while a pull strategy would make more extensive use consumer advertising and sales promotions while the media mix would be weighted towards mass-market media such as newspapers, magazines, television and radio.\n\nDistribution of products takes place by means of channels to become available on markets, in stores or in webshops. Channels are sets of interdependent organizers (called intermediaries or distributors) involved in making the product available for consumption to end-user. This is mostly accomplished through merchant retailers or wholesalers, or in international context by importers. In certain specialist markets, agents or brokers may become involved in distribution channel. \n\nTypical intermediaries involved in distribution include:\n\n\nA firm can design any number of channels they require to reach customers efficiently and effectively. Channels can be distinguished by the number of intermediaries between producer and consumer. If there are no intermediaries then this is known as a \"zero-level\" distribution system or direct marketing. A \"level one\" (sometimes called \"one-tier\") channel has a single intermediary. A \"level two\" (alternatively a \"two-tier\") channel has two intermediaries, and so on. This flow is typically represented as being manufacturer to retailer to consumer, but may involve other types of intermediaries. In practice, distribution systems for perishable goods tend to be shorter - direct or single intermediary, because of the need to reduce the time a product spends in transit or in storage. In other cases, distribution systems can become quite complex involving many levels and different types of intermediaries.\n\nIn practice, many organizations use a mix of different channels; a direct sales force may call on larger customers may be complemented with agents to cover smaller customers and prospects. When a single organisation uses a variety of different channels to reach its markets, this is known as a \"multi-channel\" distribution network. In addition, online retailing or e-commerce is leading to disintermediation, the removal of intermediaries from a supply chain. Retailing via smartphone or m-commerce is also a growth area.\n\nThe firm's marketing department needs to design the most suitable channels for the firm's products, then select appropriate channel members or intermediaries. An organisation may need to train staff of intermediaries and motivate the intermediary to sell the firm's products. The firm should monitor the channel's performance over time and modify the channel to enhance performance.\n\nTo motivate intermediaries the firm can use positive actions, such as offering higher margins to the intermediary, special deals, premiums and allowances for advertising or display. On the other hand, negative actions may be necessary, such as threatening to cut back on margin, or hold back delivery of product. Care must be exercised when considering negative actions as these may fall foul of regulations and can contribute to a public backlash and a public relations disaster.\n\nChannel conflict can arise when one intermediary's actions prevent another intermediary from achieving their objectives. Vertical channel conflict occurs between the levels within a channel, and horizontal channel conflict occurs between intermediaries at the same level within a channel. Channel conflict is a perennial problem. There are risks that a powerful channel member may coordinate the interests of the channel for personal gain.\n\nChannel-switching (not to be confused with zapping or channel surfing on TV) is the action of consumers switching from one type of channel intermediary to a different type of intermediary for their purchases. Examples include switching from brick-and-mortar stores to online catalogues and e-commerce providers; switching from grocery stores to convenience stores or switching from top tier department stores to mass market discount outlets. A number of factors have led to an increase in channel switching behaviour; the growth of e-commerce, the globalization of markets, the advent of Category killers (such as Officeworks and Kids 'R Us) as well as changes in the legal or statutory environment. For instance, in Australia and New Zealand, following a relaxation of laws prohibiting supermarkets from selling therapeutic goods, consumers are gradually switching away from pharmacies and towards supermarkets for the purchase of minor analgesics, cough and cold preparations and complementary medicines such as vitamins and herbal remedies.\n\nFor the consumer, channel switching offers a more diverse shopping experience. However, marketers need to be alert to channel switching because of its potential to erode market share. Evidence of channel switching can suggest that disruptive forces are at play, and that consumer behaviour is undergoing fundamental changes. A consumer may be prompted to switch channels when the product or service can be found at cheaper prices, when superior models become available, when a wider range is offered, or simply because it is more convenient to shop through a different channel (e.g. online or one-stop shopping). As a hedge against market share losses due to switching behaviour, some retailers engage in multi-channel retailing.\n\nThe emergence of a service-dominant logic perspective has focussed scholarly attention on how distribution networks serve to create customer value and to consider how value is co-created by all the players within the distribution chain, including the value created by customers themselves. This emphasis on value-creation is contributing to a change in terminology surrounding distribution processes; \"distribution networks\" are often termed \"value-chains\" while \"distribution centres\" are often termed \"customer fulfillment centres.\" For example, the retail giant Amazon, which utilises both direct online distribution alongside bricks and mortar stores, now calls its despatch centres \"customer fulfillment centres\". Although the term, \"customer fulfillment centre\" has been criticised on the grounds that it is a neologism, its use is becoming increasingly mainstream as it slowly makes its way into introductory marketing textbooks.\n\nDisintermediation occurs when manufacturers or service providers eliminate intermediaries from the distribution network and deal directly with purchasers. Disintermediation is found in industries where radically new types of channel intermediaries displace traditional distributors. The widespread public acceptance of online shopping has been a major trigger for disintermediation in some industries. Certain types of traditional intermediaries are dropping by the wayside.\n\n"}
{"id": "22460781", "url": "https://en.wikipedia.org/wiki?curid=22460781", "title": "European Council for Business Education", "text": "European Council for Business Education\n\nThe European Council for Business Education (ECBE) was founded in 1991 as an independent, not-for-profit organisation headquartered in Belgium. The aim of ECBE is to accredit schools of business from across Europe to ensure excellence in business education and teaching. ECBE's accreditation process is based upon self and peer evaluation, aimed to ensure the continual improvement of educational quality. ECBE is overseen by a Board of Directors elected from the accredited members. A separate Board of Commissioners, independent of the Board of Directors are selected to oversee the application of procedures and the awarding of accreditation. ECBE is a registered affiliate of the European Association for Quality Assurance in Higher Education (ENQA). and Council for Higher Education Accreditation (CHEA) International Quality Group (IQG) \n\n\n\n"}
{"id": "42081226", "url": "https://en.wikipedia.org/wiki?curid=42081226", "title": "Fellowship of Companies for Christ International", "text": "Fellowship of Companies for Christ International\n\nFellowship of Companies for Christ International (FCCI) is a membership-based 501(c)(3)non-profit. Founded in 1977 as Fellowship of Companies for Christ (FCC) in Atlanta, Georgia. In 1985, ‘International’ was added to the name to reflect the international vision. FCCI members include Christian business owners, CEOs, managing directors, professionals and other business leaders. Members believe Jesus Christ can be a positive change agent in their personal lives, the culture of their workplaces and the broader marketplace where they conduct business. While FCC originated with a small circle of business and ministry leaders in the 1970s in Atlanta, Georgia, US, FCCI now reports activities in over 100 nations, as of 12/2015.\n\n\"The Fellowship of Companies for Christ International aims higher than your basic business-networking organization. It's a group that supplies spiritual inspiration and guidance for born-again-Christian CEOs\"—Inc. Magazine\n\nFellowship of Companies for Christ International equips and encourages company leaders to operate their businesses and conduct their personal lives according to Biblical principles. For over 35 years, as a cross-denominational group, FCCI has been exploring the question: \"What does it mean to run a company for Christ?\" FCCI is now a global fellowship with a local vision, focused on personal, business and community transformation through a growing relationship with Christ, believing that Christ can change the world through how members do business.\n\nBiblical mandate--FCCI embraces the discipleship of leaders and nations as described in Great Commission. FCCI's Vision is unique as it focuses on leaders' transformation impacting their personal lives, leadership styles, and business cultures that can lead to a transformation of their community, city and their respective nations. --\"Transforming the world through Christ, one business leader at a time.\"\n\nLong before What Would Jesus Do?-WWJD wristbands appeared in the 1990s, in the 1970s FCCI's founding circles wrestled with the question: \"What would Jesus do--if He were running my company?\" This question was penned by Charles Sheldon in 1896 in a series of articles that became the book In His Steps\n\nFCCI is part of a revival of interest on matters of faith, expressly discussed in workplaces, including prayer, study and discussion groups in break rooms, conference rooms and dedicated spaces allowing free expression of faith, in compliance with Title 7 requirements.\n\nFCCI sees business as a bridge to reach the world for Christ by allowing clients, employees, leaders, competitors and stakeholders to experience the blessings that come with alignment to Biblical principles. A company's culture is positively impacted by an outflowing of the Abundant Life experienced by its leader. When a city, region or nation has enough leaders and businesses in alignment with Biblical principles, cultures can change through how business is done. FCCI promotes a purposeful strategy to positively impact the cultural hub cities around the world, expressed in the Influential Cities Outreach.\n\nFCCI supports a broader movement of Fortune 500 Companies to proprietorships that allow free expression of faith within the (US Federal law)Title 7 context. FCCI estimated the number of workplace study and prayer groups was around 10,000.\n\nOperating as a 501c3 non-profit sustainable through membership and donations, FCCI produces an annual international conference drawing Christian business leaders from dozens of countries. Speakers address issues common to all business owners and leaders from a Biblical perspective. Live conference sessions are streamed from the conference around the world and video is captured, segmented and uploaded for ongoing use in Business Leadership Groups in the USA and other nations. The International Conference is a source of watershed moments for attendees, sometimes provoking transformational change in their personal or business lives The FCCI conference has also exhibited a catalytic effect, sometimes inspiring spinoffs when attendees return home.\n\nFCCI Business Leadership Groups [BLGs] convene weekly or monthly for fellowship, encouragement, prayer, counsel and Godly wisdom. Business issues are discussed in the groups from a biblical perspective which considers the impact on both relationships and resources.\n\nConcepts of Biblical stewardship are promoted to FCCI's membership and modeled by the organization, a founding member of the Evangelical Counsel for Financial Accountability ECFA FCCI promotes the Biblical concepts that our purpose in work: 1. is of utmost importance to God and 2. is key to eternal reward (Col 3:17, 23-24). Business as Missions and Business as Ministry is most effective when Eternal Purpose is clearly understood, Eternal Values are clearly communicated, taught and most importantly, modeled by business leaders. The preeminence of eternal purpose is central to all of FCCI's activities. This idea is reinforced by the name given to FCCI's process for developing leaders-Pathway to Purpose.\nFCCI provivdes ongoing prayer support for CEOs, business leaders, their employees and others through a through a web-facilitated network of volunteers.\n\nFCCI's global fellowship is served by a small team of employees, supporting a cadre of distinguished volunteers leading Business Leadership Groups and Area Teams, all overseen by an International Board.\n\nP2P is a template for all of FCCI's conferences, seminars, groups and materials. \"P2P\" promotes four developmental objectives: Spiritual Maturity, Servant Leadership, Business Excellence and Kingdom Impact. The concept of a \"Kingdom Company\" is promoted as one where Biblical purpose and values, servant leadership and excellence lead to business transformation. Jesus Christ is the model; His teaching is the framework—all energized by the work of the Holy Spirit. Kingdom Companies are great places for anyone to work and often present non-Christians with their first exposure to Kingdom principles at work. Proselyting is not the objective. Conducting oneself in a manner that provokes questions on matters of faith is a goal (1Pet 3:15.) FCCI resources are developed with all of these P2P objectives in mind.\n\nIn the 1970s, FCCI's founding circle included: Larry Burkett, Bert Stumberg, Bobby Mitchell, Bill Leonard, Jim Moye, Smith Lanier, Ben Lively, Thomas Harris and Jimmy Pursell. Bruce Wilkinson, Larry Burkett, Walt Wiley, Ron Blue, Stanley Tam, Truett Cathy, Smith Lanier and Cade Willis were involved in the early years, providing teaching, training and leadership direction in various roles.\nA few days after September 11, 2001 at the 2011 FCCI International Conference in Maui, Dr. Henry Blackaby spoke very powerfully in one of FCCI's most memorable moments. \nMore recent content contributors for FCCI's conferences and group materials include: Ken Blanchard, Rick Warren, Dr. Richard Blackaby, Dr. John Townsend (author), Joel Manby [Undercover Boss], Dr. Tony Evans (radio), Pat Gelsinger and Francis Chan.\n\n"}
{"id": "2544706", "url": "https://en.wikipedia.org/wiki?curid=2544706", "title": "Fortune European Businessman of the Year", "text": "Fortune European Businessman of the Year\n\nFortune magazine gives an annual award for the person it considers to be the European Businessman of the Year. Fortune is based in the United States, but it has a European edition which features this award prominently. Currently the award is announced in January for the preceding calendar year. Other organisations also issue similar awards.\n"}
{"id": "2531827", "url": "https://en.wikipedia.org/wiki?curid=2531827", "title": "Inc. (magazine)", "text": "Inc. (magazine)\n\nInc. is an American weekly magazine which publishes about small businesses and startups. The magazine publishes annual lists of the 500 and 5000 fastest-growing privately held small companies in the U.S., called the \"Inc. 500\" and \"Inc. 5000\". It was founded in 1979 and is based in New York City.\n\n\"Inc.\" was founded in Boston by Bernie Goldhirsh, and its first issue appeared in April 1979. Goldhirsh was an MIT-trained engineer who worked at Polaroid and on ballistic missiles before becoming an entrepreneur and founding \"Sail\" magazine, which he sold for $10 million, using the profits to found \"Inc.\" Paul W. Kellam, who had joined Goldhirsh's company as editor of \"Marine Business\", was tapped as the first editor. Goldhirsh kept a low profile, and longtime editor George Gendron was the \"public face\" of the magazine for two decades. Though long considered the younger upstart compared to most business publications, \"Inc.\" suffered following the dot-com era as titles like \"Fast Company\" seemed to grab more attention, but the tech crash and subsequent retrenchment saw the magazine stabilize its circulation and image. In 2000, widowed and battling cancer, Goldhirsh sold the magazine to Gruner + Jahr for a reported price of over $200 million. The magazine was purchased in 2005 by Morningstar founder, Joe Mansueto, and \"Inc.\" and its sister magazine \"Fast Company\" constitute the publishing arm of Mansueto Ventures. \n\nThe magazine is based in New York City, and its editor-in-chief is Eric Schurenberg. In December 2013, Schurenberg was appointed as President of \"Inc.\", replacing the long-tenured Bob LaPointe. In late January 2014, \"Inc.\" announced that Reuters opinion editor, James Ledbetter, would take over as editor of the magazine and web site.\n\nIn October 1981, \"Inc.\" became the first magazine to feature Steve Jobs on its cover, alongside the proclamation, \"This man has changed business forever.\"\n\n\"Inc.\" produces the radio show \"Inc. Radio\" in association with Entertainment Radio Network. \"Inc.\" publishes books under the imprint An Inc. Original in association with Greenleaf Book Group.\n\nThe Inc. 500 is an annual list of the 500 fastest-growing private companies in the United States, introduced in 1982. The Inc. 5000 is an expansion of the Inc. 500, which ranks the country’s top 5000 fastest-growing private companies and also features a special ranking of the top 10% of the list as the Inc. 500.\n\nThe Inc. 5000 is ranked according to percentage revenue growth over a three-year period. To qualify, companies must have been founded and generating revenue by the first week of the starting calendar year, and therefore able to show three full calendar years of sales. Additionally, they had to be U.S.-based, privately held, and independent—not subsidiaries or divisions of other companies—as of December 31 of the last year measured. Revenue in the initial year must have been at least $100,000, and revenue in the most recent year must have been at least $2 million.\n\nIn its first issue in 1979, \"Inc.\" magazine published the Inc. 100, a list of the fastest-growing publicly held small companies; in 1982, the list was expanded to become the Inc. 500, and in 2007, it was expanded again to become the Inc. 5000.\n\nThe Inc. 500 | 5000 Conference and Awards Ceremony is an annual event that promotes the list publication. The event also offers workshops and lectures by key speakers.\n\nThis annual conference brings together the current year's class of Inc. 500 | 5000 honorees, the list's alumni, as well as entrepreneurs from the general public. The event has featured a number of well-known business and political figures and expert speakers including: Bill Clinton, Bob Berry, Marshall Goldsmith, Scott Cook, Bernard Marcus, and Thomas Friedman.\n\nThe Inc. 5000 expanded to a European edition, which was released on February 24, 2015.\n\n\n"}
{"id": "38781483", "url": "https://en.wikipedia.org/wiki?curid=38781483", "title": "Ince Park", "text": "Ince Park\n\nInce Park is a resource recovery facility being developed by Peel Group near Ince, Cheshire.\n\nIt will be the largest such facility in the UK. Ince Park will be utilized by taking waste and transforming it into energy. It is a facility dedicated to waste management and environmental technologies. It is located on the south bank of the Manchester Ship Canal. It is 126 acre site that has road and rail access and it can accommodate ships into the canal. It is being developed by a joint venture partnership by Peel Environmental and Covanta Energy. This park will possibly generate over 110 megawatts of renewable and low cost energy. The main concept of Ince Park is to perceive waste as a greener and more sustainable resource for energy rather than a costly issue that needs to be dealt with. It will also provide careers in the areas of waste, manufacturing, engineering, power generation, warehousing, supply chain, and logistics. Ince Park has received approval with the planning and will be the centre of Eco Park. It will be able to provide about 16 percent of the North West's renewable energy.\n"}
{"id": "3568755", "url": "https://en.wikipedia.org/wiki?curid=3568755", "title": "Incident management", "text": "Incident management\n\nAn incident is an event that could lead to loss of, or disruption to, an organization's operations, services or functions. Incident management (IcM) is a term describing the activities of an organization to identify, analyze, and correct hazards to prevent a future re-occurrence. These incidents within a structured organization are normally dealt with by either an incident response team (IRT), an incident management team (IMT), or Incident Command System (ICS). Without effective incident management, an incident can disrupt business operations, information security, IT systems, employees, customers, or other vital business functions.\n\nAn incident is an event that could lead to loss of, or disruption to, an organization's operations, services or functions. Incident management (IcM) is a term describing the activities of an organization to identify, analyze, and correct hazards to prevent a future re-occurrence. If not managed, an incident can escalate into an emergency, crisis or a disaster. Incident management is therefore the process of limiting the potential disruption caused by such an event, followed by a return to business as usual. Without effective incident management, an incident can disrupt business operations, information security, IT systems, employees, customers, or other vital business functions.\n\nIncident management is considered to be much more than just the analysis of perceived threats and hazards towards and organization in order to work out the risk of that event occurring, and therefore the ability of that organization to conduct business as usual activities during the incident. An important part of risk management process and business resilience planning that Incident management is a real time physical activity.\n\nThe planning that has happened to formulate the response to an incident—be that a disaster, emergency, crisis or accident—has been done so that effective business resilience can take place to ensure minimal loss or damage whether that is to tangible or non tangible assets of that organization. Efficient physical management of the incident—making best use of both time and resources that are available and understanding how to get more resources from outside the organization when needed by clear and timely liaison—ensure the plan is implemented.\n\nNational Fire Protection Association states that incident management can be described as, '[a]n IMS [incident management system] is \"the combination of facilities, equipment, personnel, procedures and communications operating within a common organizational structure, designed to aid in the management of resources during incidents\".\n\nThe physical incident management is the real time response that may last for hours, days, or longer. The United Kingdom Cabinet Office have produced the National Recovery Guidance (NRG), which is aimed at local responders as part of the implementation of the Civil Contingencies Act 2004 (CCA). It describes the response as the following: \"Response encompasses the actions taken to deal with the immediate effects of an emergency. In many scenarios, it is likely to be relatively short and to last for a matter of hours or days – rapid implementation of arrangements for collaboration, co-ordination and communication are, therefore, vital. Response encompasses the effort to deal not only with the direct effects of the emergency itself (eg fighting fires, rescuing individuals) but also the indirect effects (eg disruption, media interest)\".\n\nInternational Organization for Standardization (ISO), which is the world's largest developer of international standards also makes a point in the description of its risk management, principles and guidelines document ISO 31000:2009 that, \"Using ISO 31000 can help organizations increase the likelihood of achieving objectives, improve the identification of opportunities and threats and effectively allocate and use resources for risk treatment\". This again shows the importance of not just good planning but effective allocation of resources to treat the risk.\n\nToday, an important role is played by a Computer Security Incident Response Team (CSIRT), due to the rise of internet crime, and is a common example of incident faced by companies in developed nations all across the world. For example, if an organization discovers that an intruder has gained unauthorized access to a computer system, the CSIRT would analyze the situation, determine the breadth of the compromise, and take corrective action. Computer forensics is one task included in this process. Currently, over half of the world’s hacking attempts on Trans National Corporations (TNCs) take place in North America (57%). 23% of attempts take place in Europe. Having a well-rounded Computer Security Incident Response team is integral to providing a secure environment for any organization, and is becoming a critical part of the overall design of many modern networking teams.\n\nIncidents within a structured organization are normally dealt with by either an incident response team (IRT), or an incident management team (IMT). These are often designated beforehand or during the event, and are placed in control of the organization whilst the incident is dealt with, to restore normal functions.\n\nThe Incident Command System (ICS) is designed to deal with a larger incident involving a respond from multiple agencies. Popular with public safety agencies and jurisdictions in the United States, Canada and other countries, it is growing in practice in the private sector as organizations begin to manage without or co-manage emergencies with public safety agencies. ICS is a command and control mechanism that provides an expandable structure to manage emergency agencies. Although some of the details vary by jurisdiction, ICS normally consists of five primary elements: command, operations, planning, logistics and finance/administration. Several special staff positions, including public affairs, safety, and liaison, report directly to the incident commander (IC) when the emergency warrants establishment of those positions.\n\nAnother response system is the Gold–silver–bronze command structure, which is used by emergency services in the United Kingdom and other countries. The system has three levels of command: a gold commander sets the overall strategy, a silver commander is in direct charge of those at the scene and bronze commanders direct responders on the ground. An individual agency may use the system, or multiple agencies may use the system as they liaise. A common feature of the ICS and Gold-Silver-Bronze systems is that they create a separate command system to the agencies' usual hierarchy.\n\nUsually as part of the wider management process in private organizations, incident management is followed by post-incident analysis where it is determined why the incident happened despite precautions and controls. This analysis is normally overseen by the leaders of the organization, with the view of preventing repetition of the incident through precautionary measures and often changes in policy. This information is then used as feedback to further develop the security policy and/or its practical implementation. In the United States, the National Incident Management System, developed by the Department of Homeland Security, integrates effective practices in emergency management into a comprehensive national framework. This often results in a higher level of contingency planning, exercise and training, as well as an evaluation of the management of the incident.\n\nIncident management software systems are designed for collecting consistent, time sensitive, documented incident report data. Many of these products include features to automate the approval process of an incident report or case investigation. These products may also have the ability to collect real time incident information such as time and date data. Additionally incident report systems will automatically send notifications, assign tasks and escalations to appropriate individuals depending on the incident type, priority, time, status and custom criteria. Modern products provide the ability for administrators to configure the Incident report forms as needed, create analysis reports and set access controls on the data. These incident reports may have the ability for customization that may best suit the organizations using the systems. Some of these products have the ability to collect images, video, audio and other data. Incident management software systems exist that relate directly to specific industries.\n\nDuring the root cause analysis, human factors should be assessed. James Reason conducted a study into the understanding of adverse effects of human factors. The study found that major incident investigations, such as Piper Alpha and Kings Cross Underground Fire, made it clear that the causes of the accidents were distributed widely within and outside the organization. There are two types of events: active failure—an action that has immediate effects and has the likelihood to cause an accident—and latent or delayed action—events can take years to have an effect and are usually combined with triggering events that then cause the accident.\n\nActive failures are unsafe acts (errors and violations) committed by, for instance, the operators of machinery and supervisors of tasks. It is the people at the human-system interface whose actions can have immediate adverse consequences.\n\nLatent failures are created as the result of decisions taken at the higher echelons of an organisation. Their damaging consequences may lie dormant for a long time, only becoming evident when they combine with local triggering factors (e.g., the spring tide, the loading difficulties at Zeebrugge harbour, etc.) to breach the system's defences. Decisions taken in the higher echelons of an organization can trigger the events towards an accident becoming more likely, the planning, scheduling, forecasting, designing, policy making, etc., can have a slow burning effect. The actual unsafe act that triggers an accident can be traced back through the organization and the subsequent failures can be exposed, showing the accumulation of latent failures within the system as a whole that led to the accident becoming more likely and ultimately happening. Better improvement action can be applied, and reduce the likelihood of the event happening again.\n\n\n\n"}
{"id": "19185041", "url": "https://en.wikipedia.org/wiki?curid=19185041", "title": "List of frequent flyer programs", "text": "List of frequent flyer programs\n\nFrequent-flyer programs are customer loyalty programs used by many passenger airlines. This is a list of current airlines with frequent-flyer programs, and the names of those programs.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "33797591", "url": "https://en.wikipedia.org/wiki?curid=33797591", "title": "MIT150", "text": "MIT150\n\nThe MIT150 is a list published by the Boston Globe, in honor of the 150th anniversary of the Massachusetts Institute of Technology (MIT) in 2011, listing 150 of the most significant innovators, inventions or ideas from MIT, its alumni, faculty, and related people and organizations in the 150 year history of the institute.\n\nThe top 30 innovators and inventions on the list are:\n\n\n\n"}
{"id": "20635304", "url": "https://en.wikipedia.org/wiki?curid=20635304", "title": "Managerial hubris", "text": "Managerial hubris\n\nManagerial hubris is the unrealistic belief held by managers in bidding firms that they can manage the assets of a target firm more efficiently than the target firm's current management. Managerial hubris is one reason a manager may choose to invest in a merger that on average generates no profits.\n\n"}
{"id": "19688", "url": "https://en.wikipedia.org/wiki?curid=19688", "title": "Mind map", "text": "Mind map\n\nA mind map is a diagram used to visually organize information. A mind map is hierarchical and shows relationships among pieces of the whole. It is often created around a single concept, drawn as an image in the center of a blank page, to which associated representations of ideas such as images, words and parts of words are added. Major ideas are connected directly to the central concept, and other ideas branch out from those major ideas.\n\nMind maps can also be drawn by hand, either as \"rough notes\" during a lecture, meeting or planning session, for example, or as higher quality pictures when more time is available. Mind maps are considered to be a type of spider diagram. A similar concept in the 1970s was \"idea sun bursting\".\n\nAlthough the term \"mind map\" was first popularized by British popular psychology author and television personality Tony Buzan, the use of diagrams that visually \"map\" information using branching and radial maps traces back centuries. These pictorial methods record knowledge and model systems, and have a long history in learning, brainstorming, memory, visual thinking, and problem solving by educators, engineers, psychologists, and others. Some of the earliest examples of such graphical records were developed by Porphyry of Tyros, a noted thinker of the 3rd century, as he graphically visualized the concept categories of Aristotle. Philosopher Ramon Llull (1235–1315) also used such techniques.\n\nThe semantic network was developed in the late 1950s as a theory to understand human learning and developed further by Allan M. Collins and M. Ross Quillian during the early 1960s. Mind maps are similar in radial structure to concept maps, developed by learning experts in the 1970s, but differ in that the former are simplified by focusing around a single central key concept.\n\nBuzan's specific approach, and the introduction of the term \"mind map\", arose during a 1974 BBC TV series he hosted, called \"Use Your Head\". In this show, and companion book series, Buzan promoted his conception of radial tree, diagramming key words in a colorful, radiant, tree-like structure.\n\nBuzan says the idea was inspired by Alfred Korzybski's general semantics as popularized in science fiction novels, such as those of Robert A. Heinlein and A. E. van Vogt. He argues that while \"traditional\" outlines force readers to scan left to right and top to bottom, readers actually tend to scan the entire page in a non-linear fashion. Buzan's treatment also uses then-popular assumptions about the functions of cerebral hemispheres in order to explain the claimed increased effectiveness of mind mapping over other forms of note making.\n\nBuzan suggests the following guidelines for creating mind maps:\n\n\nAs with other diagramming tools, mind maps can be used to generate, visualize, structure, and classify ideas, and as an aid to studying and organizing information, solving problems, making decisions, and writing.\n\nMind maps have many applications in personal, family, educational, and business situations, including notetaking, brainstorming (wherein ideas are inserted into the map radially around the center node, without the implicit prioritization that comes from hierarchy or sequential arrangements, and wherein grouping and organizing is reserved for later stages), summarizing, as a mnemonic technique, or to sort out a complicated idea. Mind maps are also promoted as a way to collaborate in color pen creativity sessions.\n\nIn addition to these direct use cases, data retrieved from mind maps can be used to enhance several other applications; for instance expert search systems, search engines and search and tag query recommender. To do so, mind maps can be analysed with classic methods of information retrieval to classify a mind map's author or documents that are linked from within the mind map.\n\n\nCunningham (2005) conducted a user study in which 80% of the students thought \"mindmapping helped them understand concepts and ideas in science\". Other studies also report some subjective positive effects on the use of mind maps. Positive opinions on their effectiveness, however, were much more prominent among students of art and design than in students of computer and information technology, with 62.5% vs 34% (respectively) agreeing that they were able to understand concepts better with mind mapping software. Farrand, Hussain, and Hennessy (2002) found that spider diagrams (similar to concept maps) had limited, but significant, impact on memory recall in undergraduate students (a 10% increase over baseline for a 600-word text only) as compared to preferred study methods (a 6% increase over baseline). This improvement was only robust after a week for those in the diagram group and there was a significant decrease in motivation compared to the subjects' preferred methods of note taking. A meta study about concept mapping concluded that concept mapping is more effective than \"reading text passages, attending lectures, and participating in class discussions\". The same study also concluded that concept mapping is slightly more effective \"than other constructive activities such as writing summaries and outlines\". However, results were inconsistent, with the authors noting \"significant heterogeneity was found in most subsets\". In addition, they concluded that low-ability students may benefit more from mind mapping than high-ability students.\n\nJoeran Beel and Stefan Langer conducted a comprehensive analysis of the content of mind maps. They analysed 19,379 mind maps from 11,179 users of the mind mapping applications SciPlore MindMapping (now Docear) and MindMeister. Results include that average users create only a few mind maps (mean=2.7), average mind maps are rather small (31 nodes) with each node containing about three words (median). However, there were exceptions. One user created more than 200 mind maps, the largest mind map consisted of more than 50,000 nodes and the largest node contained ~7,500 words. The study also showed that between different mind mapping applications (Docear vs MindMeister) significant differences exist related to how users create mind maps.\n\nThere have been some attempts to create mind maps automatically. Brucks & Schommer created mind maps automatically from full-text streams. Rothenberger et al. extracted the main story of a text and presented it as mind map. And there is a patent about automatically creating sub-topics in mind maps.\n\nMind-mapping software can be used to organize large amounts of information, combining spatial organization, dynamic hierarchical structuring and node folding. Software packages can extend the concept of mind-mapping by allowing individuals to map more than thoughts and ideas with information on their computers and the Internet, like spreadsheets, documents, Internet sites and images. It has been suggested that mind-mapping can improve learning/study efficiency up to 15% over conventional note-taking.\n"}
{"id": "6406997", "url": "https://en.wikipedia.org/wiki?curid=6406997", "title": "Ordinary course of business", "text": "Ordinary course of business\n\nIn law, the ordinary course of business covers the usual transactions, customs and practices of a certain business and of a certain firm. This term is used particularly to judge the validity of certain transactions. It is used in several different sections of the Uniform Commercial Code of the United States.\n\nSection 1-201 of the Uniform Commercial Code defines a \"Buyer in the ordinary course of business\" by a four-part test:\n\n"}
{"id": "40928146", "url": "https://en.wikipedia.org/wiki?curid=40928146", "title": "Plotly", "text": "Plotly\n\nPlotly, also known by its URL, Plot.ly, is a technical computing company headquartered in Montreal, Quebec, that develops online data analytics and visualization tools. Plotly provides online graphing, analytics, and statistics tools for individuals and collaboration, as well as scientific graphing libraries for Python, R, MATLAB, Perl, Julia, Arduino, and REST.\n\nPlotly was founded by Alex Johnson, Jack Parmer, Chris Parmer, and Matthew Sundquist. The founders' backgrounds are in science, energy, and data analysis and visualization. Early employees include Christophe Viau, a Canadian software engineer and Ben Postlethwaite, a Canadian geophysicist. Plotly was named one of the Top 20 Hottest Innovative Companies in Canada by the Canadian Innovation Exchange. Plotly was featured in \"startup row\" at PyCon 2013, and sponsored the SciPy 2018 conference.\n\nPlotly raised $5.5 million during its Series A funding, led by MHS Capital, Siemens Venture Capital, Rho Ventures, Real Ventures, and Silicon Valley Bank.\n\nGoogle and NASA have used Plotly for data analytics, and The Boston Globe and Washington Post newsrooms have produced data journalism using Plotly.\n\nPlotly's main products include:\n\nPlotly was built using Python and the Django framework, with a front end using JavaScript and the visualization library D3.js, HTML and CSS. Files are hosted on Amazon S3.\n\n"}
{"id": "5588452", "url": "https://en.wikipedia.org/wiki?curid=5588452", "title": "Procurement software", "text": "Procurement software\n\nProcurement software is business software that helps to automate the purchasing function of organizations. Activities including raising and approving purchase orders, selecting and ordering the product or service, receiving and matching the invoice and order, and paying the bill is handled electronically, enabling the procurement department to see everything that is ordered, ensure that nothing can be ordered without correct approvals, and lets them get the best value by combining several orders for the same type of good or even getting suppliers to bid for the business. Buying organisation's choice can be driven by the particular strengths offered by each individual system and the number of vendors available through them. A multinational or otherwise large organization will use a shared procurement system to take advantage of economies of scale to drive down the cost of purchases. While traditionally most e-procurement software systems have been designed for larger organizations, there are now also systems available for the SMB market, such as the Free-Procurement Project and an open source alternative by ProcuMan. Whilst some services are available to purchase through automated systems, the key strength of these systems lies in the procurement of commodities that are much easier to standardise.\n\nThe key benefit for organisations using procurement software include ease of administration and potential long-term cost savings. Having a single interface for procurement related management information cuts down the time and effort required to monitor organisational spending. The use of such software also allows procurement managers to control the vendors used by the wider organisation so all employees take advantage of negotiated rates and other terms of service.\n\nCommon Features of Procurement Software Systems\n\nRequisitions - Creation of a purchase order with line items to be fulfilled by a vendor. Automated Sending via Fax or Email.\n\nVendor Follow Up - Automated, or reminders to follow up with Vendors to Confirm Purchase Orders.\n\nReceiving of Goods or Services - Maintaining a physical inventory of goods.\n\nFinancial Settlement - Creation of financial and/or inventory related transactions as goods are physically received.\n\nDocument Management System - To archive relevant documents. \n\nElectronic Signature - Tool to sign documents online.\n\n"}
{"id": "40942718", "url": "https://en.wikipedia.org/wiki?curid=40942718", "title": "Public relations campaigns of Edward Bernays", "text": "Public relations campaigns of Edward Bernays\n\nThe following is a list of public relations, propaganda, and marketing campaigns orchestrated by Edward Bernays (22 November 1891 – 9 March 1995).\n\nBernays is regarded as the pioneer of public relations. His influence radically changed the persuasion tactics used in campaign advertising and political campaigns.\n\nBernays was the nephew of Sigmund Freud. His early adoption of Freud's psychoanalytic theory was instrumental in defining the goals and strategies of public relations. Freud theorized that people are motivated by unconscious desires. To develop public relations, Bernays synthesized elements of Freud's work with Gustave Le Bon's researches into crowd psychology, and Wilfred Trotter's theories of herd instinct.\n\nIn the 1920s, it was considered taboo for women to smoke in public. George W. Hill, president of the American Tobacco Company, realized that his company was missing a share of the market. He hired Bernays to change the taboo and persuade women to start smoking. Bernays contacted psychoanalyst Abraham Brill to understand the societal perceptions that discouraged women from smoking. Brill told him that for feminists, cigarettes were like \"torches of freedom\" that symbolized their nonconformity and freedom from male oppression. \n\nBernays used this information to build a strategy. He contacted a female friend and asked her to find a group of women to march in the New York City Easter Day parade. He asked her to tell the press that a group of women's rights marchers would light \"Torches of Freedom\". The women lit Lucky Strike cigarettes in front of the eager photographers. The New York Times (1 April 1929) printed: \"Group of Girls Puff at Cigarettes as a Gesture of 'Freedom'\".\n\nIn the 1930s, he attempted to convince women that Lucky Strike cigarettes' forest green pack was the most fashionable color. Letters were written to interior and fashion designers, department stores, and prominent women of society pushing green as the new hot color for the season. Balls, gallery exhibitions, and window displays all featured green after Bernays got through with them. The result was that green did indeed become a very popular color for the 1934 season and Lucky Strike kept their pack color and female clientele intact.\n\nAfter his semi-retirement in the 1960s, he worked with the pro-health anti-smoking lawyer John Banzhaf's group, ASH and supported other anti-smoking campaigns.\n\nBernays helped the Aluminum Company of America (Alcoa) and other special interest groups to convince the American public that water fluoridation was safe and beneficial to human health. This was achieved by using the American Dental Association in a highly successful media campaign.\n\nBernays applied Freud's observations to convince the public, among other things, that bacon and eggs was the true all-American breakfast.\n\nHe called for a focus group to learn why housewives didn't want to buy instant cake mixes. He secured cake mix sales by adding a (symbolic) egg to the list of necessary ingredients.\n\nBernays once engineered a \"pancake breakfast\" with vaudevillians for Calvin Coolidge in what is widely considered one of the first overt publicity stunts for a US president.\n\nBernays's most extreme political propaganda activities were said to be conducted on behalf of the multinational corporation United Fruit Company (renamed Chiquita Brands International in 1984) and the U.S. government to facilitate the successful overthrow (see Operation PBSUCCESS) of the democratically elected president of Guatemala, Colonel Jacobo Arbenz Guzman. Bernays's propaganda (documented in the BBC documentary \"The Century of the Self\"), branding Arbenz as communist, was published in major U.S. media. According to a book review by John Stauber and Sheldon Rampton of Larry Tye's biography of Bernays, \"The Father of Spin: Edward L. Bernays & The Birth of PR\", \"the term 'banana republic' actually originated in reference to United Fruit's domination of corrupt governments in Guatemala and other Central American countries.\"\n\nWhen Venida, a hairnet manufacturer, hired Bernays' services, he started a campaign to convince women to grow their hair longer so they would buy more hairnets. Although the campaign failed to influence many women, it convinced government officials to require hairnets for certain jobs.\n\nBernays worked with Procter & Gamble for Ivory-brand bar soap. The campaign successfully convinced people that Ivory soap was medically superior to other soaps. He also promoted soap through sculpting contests and floating contests because the soap floated better than its competitors'.\n\nIn the 1930s, his Dixie Cup campaign was designed to convince consumers that only disposable cups were sanitary by linking the imagery of the overflowing cup with subliminal images of vaginas and venereal disease.\n\nIn 1920, Bernays successfully hosted the first NAACP convention in Atlanta, Georgia. His campaign was considered successful because there was no violence at the convention. His campaign focused on the important contributions of African-Americans to Whites living in the South. He later received an award from the NAACP for his contribution.\n\nIn October 1929, Bernays was involved in promoting Light's Golden Jubilee. The event, which spanned across several major cities in the U.S., was designed to celebrate the 50th anniversary of Thomas Edison's invention of the light-bulb (though a form of light-bulb had been simultaneously invented by Joseph Swan). The publicity elements of the Jubilee–including the special issuance of a U.S. postage stamp and Edison's \"re-creating\" the invention of the light bulb for a nationwide radio audience – provided evidence of Bernays's love for big ideas and \"ballyhoo\". A follow-up event for the 75th anniversary, produced for television by David O. Selznick, was titled \"Light's Diamond Jubilee\" and broadcast on all four American TV networks on October 24, 1954.\n\nBernays was the publicity director of the 1939 New York World's Fair.\n\nIn 1913, Bernays secured one of his first consulting contracts. American actor Richard Bennett hired him to defend a play that promoted sex education against police interference. Bernays set up a front organization called the Medical Review of Reviews Sociological Fund. Although the organization purported to fight sexually transmitted disease, its sole purpose was to endorse Bennett's play.\n\nIn 1915, to prepare for the Ballets Russes' US tour, Sergei Diaghilev hired Bernays to convince American magazines to publish articles telling readers that ballet is fun to watch.\n\n"}
{"id": "38252698", "url": "https://en.wikipedia.org/wiki?curid=38252698", "title": "Rapid Results", "text": "Rapid Results\n\nRapid Results is a structured process that mobilizes teams to achieve tangible results over a rapid time frame and accelerate organizational learning. Schaffer Consulting, a management consulting firm headquartered in Stamford, Connecticut, developed the Rapid Results approach based on their experience working with clients across industries to achieve breakthrough levels of performance. The approach and its practitioners have since been recognized in \"The New York Times\", \"Harvard Business Review\", and \"Foreign Policy\", among other publications. In 2007, Schaffer Consulting founded the Rapid Results Institute (RRI) to refine the methodology to support non-profit work and development projects across Sub-Saharan Africa, South America, the United States, and the Middle East.\n\nThe objective of Rapid Results initiatives is to achieve dramatic results, formed under the pressure of short time frames and ambitious targets. Rapid Results initiatives begin with a call to action for significant performance improvement delivered by a single leader or group of leaders to cross-functional teams of 8-10 people. Team members then set and commit to \"seemingly unreasonable\" short-term goals — often in 100 days or less — tied to the strategic imperatives outlined by their leaders. Teams then experiment with new ways of working, capturing learnings along the way, and persisting until they achieve desired outcomes. Rapid Results aims to stimulate innovation, collaboration, and more effective execution in and across organizations and stakeholder groups. Leaders leverage initial results to create longer-term and wider-scale impact in subsequent waves of Rapid Results projects.\n\nLeaders in many industries leverage Rapid Results to achieve performance goals, including accelerating growth, increasing productivity, and realizing cost savings.\n\nLynn Chambers, group head of Talent at the London Stock Exchange Group, says, “This approach can be taken to accelerate progress on almost any goal.” Dean Scarborough, president and chief executive officer of Avery Dennison describes Rapid Results “driving an incredible amount of creativity” among team members. Martha Marsh, former president and CEO of Stanford Hospital and Clinics, observes that Rapid Results “provide an intense focus on getting results, while helping build the capacity of our organization to drive transformational change.”\n\nFormed in 2007, the Rapid Results Institute works with a broad range of partners — including government agencies, not-for-profit organizations, and international development agencies — across a wide spectrum of development initiatives. Past and ongoing projects include H.I.V. prevention in Eritrea and Ethiopia, public sector reform in Kenya, waste management in Brazil, and housing homeless veterans in the United States.\n\nThe Rapid Results Institute is partnering with the 100,000 Homes Campaign and the US Department of Veterans Affairs to house homeless veterans across the United States.\n\nFollowing a project on HIV/AIDS awareness among youth, Eritrean Minister of Education Osman Saleh called the Rapid Results approach \"a new movement.\" In Kenya, an independent evaluation by the AIDS Research and Treatment journal reported that “significant improvement in PMTCT [prevention of mother-to-child-transmission] services can be achieved through introduction of an RRI [Rapid Results Initiative], which appears to lead to sustained benefits for pregnant HIV-infected women and their infants.”\n\nDue to its success in a variety of contexts, the Rapid Results approach has been adopted by the World Bank, as well as by many Kenyan government ministries.\n\n"}
{"id": "155698", "url": "https://en.wikipedia.org/wiki?curid=155698", "title": "Sales", "text": "Sales\n\nSales are activities related to selling or the number of goods or services sold in a given time period.\n\nThe \"seller\" or the provider of the goods or services complete a sale in response to an acquisition, appropriation, requisition or a direct interaction with the \"buyer\" at the point of sale. There is a passing of title (property or ownership) of the item, and the settlement of a price, in which agreement is reached on a price for which transfer of ownership of the item will occur. The \"seller\", not the purchaser generally executes the sale and it may be completed prior to the obligation of payment. In the case of indirect interaction, a person who sells goods or service on behalf of the owner is known as a salesman or saleswoman or salesperson, but this often refers to someone selling goods in a store/shop, in which case other terms are also common, including \"salesclerk\", \"shop assistant\", and \"retail clerk\".\n\nIn common law countries, sales are governed generally by the common law and commercial codes. In the United States, the laws governing sales of goods are somewhat uniform to the extent that most jurisdictions have adopted Article 2 of the Uniform Commercial Code, albeit with some non-uniform variations.\n\nA person or organization expressing an interest in acquiring the offered item of value is referred to as a potential buyer, prospective customer or prospect. Buying and selling are understood to be two sides of the same \"coin\" or transaction. Both seller and buyer engage in a process of negotiation to consummate the exchange of values. The exchange, or selling, process has implied rules and identifiable stages. It is implied that the selling process will proceed fairly and ethically so that the parties end up nearly equally rewarded. The stages of selling, and buying, involve getting acquainted, assessing each party's need for the other's item of value, and determining if the values to be exchanged are equivalent or nearly so, or, in buyer's terms, \"worth the price\". Sometimes, sellers have to use their own experiences when selling products with appropriate discounts.\n\nFrom a management viewpoint it is thought of as a part of marketing, although the skills required are different. Sales often form a separate grouping in a corporate structure, employing separate specialist operatives known as \"salespersons\" (singular: \"salesperson\"). Selling is considered by many to be a sort of persuading \"art\". Contrary to popular belief, the methodological approach of selling refers to a \"systematic process of repetitive and measurable milestones, by which a salesman relates his or her offering of a product or service in return enabling the buyer to achieve their goal in an economic way\". According to a 2018 survey of salespeople, selling has become more difficult in recent years due to changes in technology and general access to prospects. While the sales process refers to a systematic process of repetitive and measurable milestones, the definition of the selling is somewhat ambiguous due to the close nature of advertising, promotion, public relations, and direct marketing.\n\nSelling is the profession-wide term, much like marketing defines a profession. Recently, attempts have been made to clearly understand who is in the sales profession, and who is not. There are many articles looking at marketing, advertising, promotions, and even public relations as ways to create a unique transaction.\n\nTwo common terms used to describe a salesperson are \"Farmer\" and \"Hunter\". The reality is that most professional salespeople have a little of both. A hunter is often associated with aggressive personalities who use aggressive sales technique. In terms of sales methodology, a hunter refers to a person whose focus is on bringing in and closing deals. This process is called \"sales capturing\". An example is a commodity sale such as a long distance salesperson, shoe salesperson and to a degree a car salesperson. Their job is to find and convert buyers. A sales farmer is someone who creates sales demand through activities that directly influence and alter the buying process.\n\nMany believe that the focus of selling is on the human agents involved in the exchange between buyer and seller. Effective selling also requires a systems approach, at minimum involving roles that sell, enable selling, and develop sales capabilities. Selling also involves salespeople who possess a specific set of sales skills and the knowledge required to facilitate the exchange of value between buyers and sellers that is unique from marketing, advertising, etc.\n\nWithin these three tenets, the following definition of professional selling is offered by the American Society for Training and Development (ASTD):\nTeam selling is one way to influence sales. Team selling is \"a group of people representing the sales department and other functional areas in the firm, such as finance, production, and research and development\". (Spiro) Team selling came about in the 1990s through total quality management (TQM). TQM occurs when companies work to improve their customer satisfaction by constantly improving all of their operations.\n\nMarketing and sales differ greatly but generally have the same goal. Selling is the final stage in marketing which puts the plan into effect, which also includes pricing, promotion, place, and product (the 4 P's). A marketing department in an organization has the goals of increasing the desirability and value to the customer and increasing the number and engagement of interactions between potential customers and the organization. Achieving this goal may involve the sales team using promotional techniques such as advertising, sales promotion, publicity, and public relations, creating new sales channels, or creating new products (new product development), among other things. It can also include bringing the potential customer to visit the organization's website(s) for more information, or to contact the organization for more information, or to interact with the organization via social media such as Twitter, Facebook and blogs. Social values also play a major role in consumer decision processes. Marketing is the whole of the work on persuasion made for the whole of the target people. Sales are the persuasion and effort that from one person to one person (B2C), one person makes to the corporation (B2B) in the face or in the phone or in the digital environment, to make a living resource enter the company.\n\nThe field of sales process engineering views \"sales\" as the output of a larger system, not just as the output of one department. The larger system includes many functional areas within an organization. From this perspective, \"sales\" and \"marketing\" (among others, such as \"customer service\") label for a number of processes whose inputs and outputs supply one another to varying degrees. In this context, improving an \"output\" (such as sales) involves studying and improving the broader sales process, as in any system, since the component functional areas interact and are interdependent.\n\nMany large corporations structure their marketing departments so they are directly integrated with all lines of business. They create multiple teams with a singular focus and the managers of these teams must coordinate efforts in order to drive profits and business success. For example, an \"inbound\" focused campaign seeks to drive more customers \"through the door\", giving the sales department a better chance of selling their product to the consumer. A good marketing program would address any potential downsides as well.\n\nThe sales department would aim to improve the interaction between the customer and the sales facility or mechanism (example, website) or salesperson. As Sales is the forefront of any organization, this would always need to take place before any other business process may begin. Sales management would break down the selling process and then increase the effectiveness of the discrete processes as well as the interaction between processes. For example, in many out-bound sales environments, the typical process includes outbound calling, the sales pitch, handling objections, opportunity identification, and the close. Each step of the process has sales-related issues, skills, and training needs, as well as marketing solutions to improve each discrete step, as well as the whole process. In many cases becoming a salesperson is a default career as not many people aspire to be a salesman but rather fall into the job due to circumstances. It can be highly rewarding as you receive remuneration in the form of a salary and also commission.\n\nOne further common complication of marketing involves the inability to measure results for a lot of marketing initiatives. In essence, many marketing and advertising executives often lose sight of the objective of sales/revenue/profit, as they focus on establishing a creative/innovative program, without concern for the top or bottom lines – a fundamental pitfall of marketing for marketing's sake.\n\nMany companies find it challenging to get marketing and sales on the same page. The two departments, although different in nature, handle very similar concepts and have to work together for sales to be successful. Building a good relationship between the two that encourages communication can be the key to success – even in a down economy.\n\nThe idea that marketing can potentially eliminate the need for salespeople depends entirely on context. For example, this may be possible in some B2C situations; however, for many B2B transactions (for example, those involving industrial organizations) this is mostly impossible. Another dimension is the value of the goods being sold. Fast-moving consumer-goods (FMCG) require no sales people at the point of sale to get them to jump off the supermarket shelf and into the customer's trolley. However, the purchase of large mining equipment worth millions of dollars will require a salesperson to manage the sales process – particularly in the face of competitors. Small and medium businesses selling such large ticket items to a geographically-disperse client base use manufacturers' representatives to provide this highly personal service while avoiding the large expense of a captive sales force.\n\nAnother area of discussion involves the need for alignment and integration of corporate sales and marketing functions. According to a report from the Chief Marketing Officer (CMO) Council, only 40 percent of companies have formal programs, systems or processes in place to align and integrate the two critical functions.\n\nTraditionally, these two functions, as referenced above, have operated separately, left in siloed areas of tactical responsibility. Glen Petersen's book \"The Profit Maximization Paradox\" sees the changes in the competitive landscape between the 1950s and the time of writing as so dramatic that the complexity of choice, price, and opportunities for the customer forced this seemingly simple and integrated relationship between sales and marketing to change forever. Petersen goes on to highlight that salespeople spend approximately 40 percent of their time preparing customer-facing deliverables while leveraging less than 50 percent of the materials created by marketing, adding to perceptions that marketing is out of touch with the customer and that sales is resistant to messaging and strategy.\n\nThere are two types of sales: direct selling and indirect selling.\n\nA sale can take place through:\n\n\nAgents in the sales process can represent either of two parties in the sales process; for example:\n\n\nIn the United States, the Fair Labor Standards Act defines outside sales representatives as \"employees [who] sell their employer's products, services, or facilities to customers away from their employer's place(s) of business, in general, either at the customer's place of business or by selling door-to-door at the customer's home\" while defining those who work \"from the employer's location\" as inside sales. Inside sales generally involves attempting to close business primarily over the phone via telemarketing, while outside sales (or \"field\" sales) will usually involve initial phone work to book sales calls at the potential buyer's location to attempt to close the deal in person. Some companies have an inside sales department that works with outside representatives and book their appointments for them. Inside sales sometimes refers to upselling to existing customers.\n\n"}
{"id": "29722856", "url": "https://en.wikipedia.org/wiki?curid=29722856", "title": "Social BI", "text": "Social BI\n\nSocial BI, or Social Business Intelligence, refers to the creation, publishing and sharing of custom business analytics reports and dashboards by end users of \"Cloud\" technologies.\n\nSocial or collaborative BI is the use of Enterprise 2.0 tools and practices with business intelligence outputs for the purpose of making collective decisions. First enabled by the rapid growth of social media networks in 2009, Social BI allows for the collaborative development of post user-generated analytics among business analysts and data mining professionals. This has removed previous barriers to self-service BI while still employing traditional analytics applications.\n\nSocial BI can also be interpreted as providing business intelligence based on social networks data. For example, a company selling consumer electronics goods needs to know how people are responding to their latest advertisements or promotions. The reports and visualizations made using social media represent what people are talking about in real time. Pulling data from different social media and preparing understandable reports will help company to decide upon further steps. These dashboards, visualizations, reports based on social media will be of help for companies to get efficient feedback and act accordingly.\n\nOn September 13, 2011, Dachis Group introduced the Social Business Index to provide some insights into how ‘social’ companies are, and how they stack up against similar corporations in their respective industries and their competitors, and provide some ‘social business’ benchmarks by company, subsidiary, geography, department and brand.\n"}
{"id": "53971331", "url": "https://en.wikipedia.org/wiki?curid=53971331", "title": "Stakeholder approach", "text": "Stakeholder approach\n\nIn management, a stakeholder approach suggests that managers should formulate and implement processes which satisfy stakeholders' needs in order to ensure the long-term success of the firm. According to the degree of participation of the different groups, the company can take advantage of market imperfections in order to create valuable opportunities. It emphasizes active management of the business environment, relationships and the promotion of shared interests. This approach is based on the stakeholder theory which arises as a counterpart to the dominant way of understanding business and management that is focused on shareholders satisfaction. The implementation of this approach can reinforce the firm values and create competitive advantage. However, it has been criticized for overvaluing stakeholders and its difficulty to reach consensus. \n\nThis approach is more able to create competitive advantage because it creates a link between the firm and stakeholders. The latter will perceive the coherent application of the organizational values and will relate those values with his own. In that way, the company will have the needed information about stakeholders in order to treat them well and develop important initiatives. By doing that, the firm’s reputation and loyalty will be reinforced among customers and other stakeholders alike, it will create stronger brand recognition and will increase trust in the firm. Even if there are limits in loyalty and reputation can be damaged, those two key elements can make a big difference creating barriers to other companies that may want to have information about stakeholder utility functions. A firm that follows the stakeholder approach will get the information needed to work for satisfying the stakeholders’ needs, making it easier to develop expertise. Those acquired skills can be transmitted, promoted and reinforced across the business operation of the firm creating core competencies. Over time, this approach can become an indispensable issue in the organizational culture.\n\nFirms that manage for stakeholders are more able to attract a higher-quality workforce. Employees’ job satisfaction has an impact on the firm’s ability to foster innovation. Workers who are satisfied with their jobs are more likely to engage in long-term thinking and generate potentially valuable ideas. Those firms can use information about stakeholder to devise new ways of satisfying them. Reciprocity is a key aspect in this approach: when stakeholders stand to benefit, they are more likely to reveal information about their utility function. That is why firms and firm managers can better meet consumers' needs by understanding their own customers and suppliers and using this information strategically and flexibly.\n\nThe implication of all the stakeholders may produce divergent views making it difficult to reach consensus. Each stakeholder may care mostly about its own benefits or self-interests. Trying to satisfy a large number of players will complicate governance. It can result in time consuming in engaging all the parties. Moreover, in this approach, an equality of stakeholders and business is implied in negotiating issues of mutual interest. That assumption has been critiqued in terms of an inequality of resources, negotiating power and time required. The identification of prices and opportunity costs for the different stakeholders difficult and reduce the operability of this approach.\n\nIt has been suggested that obtaining information about stakeholders' utility functions may produce costs that can exceed the benefits. Therefore, in its intention to create value, managing for stakeholders can end up allocating too many resources to stakeholders. Also, having into account that the power among stakeholders is not equal, some powerful actors can get much of the firm’s profitability. And with that distribution of value, shareholders cannot expect a maximization of returns.\n\n"}
{"id": "32724180", "url": "https://en.wikipedia.org/wiki?curid=32724180", "title": "Stock Market Learning", "text": "Stock Market Learning\n\nStock Market Learning is an international competition run by the Savings Banks in Europe with the support of the European Savings Banks Group (ESBG) based in Brussels. It aims to provide pupils a better understanding of the stock market and of the economic and financial world by simulating transactions in stock exchanges. The competition was held for the first time in Germany in 1983, while the Savings Banks in Europe became actively involved in 1999. By 2010, over 1 million teams had taken part in the initiative.\n\nEach team is assigned an account with virtual capital that is to be used to buy and sell securities. The purpose is to increase initial assets by conducting individual investing strategies. While capital is virtual, trading is based on real stock prices at certain stock exchanges.\n\nThe list of tradable securities consists primarily of shares but also includes fixed-interests securities and investment funds. Since 2009, there is both a conventional and sustainability competition. The former takes into consideration the total performance of the account, whereas the latter is based on the sustainability account value, i.e. on the earnings gained from shares listed in a sustainability index.\n\nIn 2010, Stock Market Learning received the official UN Decade Project of Education for Sustainable Development award from the German Commission for UNESCO.\n\n"}
{"id": "1337683", "url": "https://en.wikipedia.org/wiki?curid=1337683", "title": "Theory of the firm", "text": "Theory of the firm\n\nThe theory of the firm consists of a number of economic theories that explain and predict the nature of the firm, company, or corporation, including its existence, behaviour, structure, and relationship to the market.\n\nIn simplified terms, the theory of the firm aims to answer these questions:\n\nFirms exist as an alternative system to the market-price mechanism when it is more efficient to produce in a non-market environment. For example, in a labor market, it might be very difficult or costly for firms or organizations to engage in production when they have to hire and fire their workers depending on demand/supply conditions. It might also be costly for employees to shift companies every day looking for better alternatives. Similarly, it may be costly for companies to find new suppliers daily. Thus, firms engage in a long-term contract with their employees or a long-term contract with suppliers to minimize the cost or maximize the value of property rights.\n\nThe First World War period saw change of emphasis in economic theory away from industry-level analysis which mainly included analyzing markets to analysis at the level of the firm, as it became increasingly clear that perfect competition was no longer an adequate model of how firms behaved. Economic theory until then had focused on trying to understand markets alone and there had been little study on understanding why firms or organisations exist. Markets are guided by prices and quality as illustrated by vegetable markets where a buyer is free to switch sellers in an exchange.\n\nThe need for a revised theory of the firm was emphasized by empirical studies by Adolf Berle and Gardiner Means, who made it clear that ownership of a typical American corporation is spread over a wide number of shareholders, leaving control in the hands of managers who own very little equity themselves. R. L. Hall and Charles J. Hitch found that executives made decisions by rule of thumb rather than in the marginalist way. \n\nAccording to Ronald Coase, people begin to organise their production in firms when the transaction cost of coordinating production through the market exchange, given imperfect information, is greater than within the firm.\n\nRonald Coase set out his transaction cost theory of the firm in 1937, making it one of the first (neo-classical) attempts to define the firm theoretically in relation to the market. One aspect of its 'neoclassicism' lies in presenting an explanation of the firm consistent with constant returns to scale, rather than relying on increasing returns to scale. Another is in defining a firm in a manner which is both realistic and compatible with the idea of substitution at the margin, so instruments of conventional economic analysis apply. He notes that a firm’s interactions with the market may not be under its control (for instance because of sales taxes), but its internal allocation of resources are: “Within a firm, … market transactions are eliminated and in place of the complicated market structure with exchange transactions is substituted the entrepreneur … who directs production.” He asks why alternative methods of production (such as the price mechanism and economic planning), could not either achieve all production, so that either firms use internal prices for all their production, or one big firm runs the entire economy. \n\nCoase begins from the standpoint that markets could in theory carry out all production, and that what needs to be explained is the existence of the firm, with its \"distinguishing mark … [of] the supersession of the price mechanism.\" Coase identifies some reasons why firms might arise, and dismisses each as unimportant:\n\n\nInstead, for Coase the main reason to establish a firm is to avoid some of the transaction costs of using the price mechanism. These include discovering relevant prices (which can be reduced but not eliminated by purchasing this information through specialists), as well as the costs of negotiating and writing enforceable contracts for each transaction (which can be large if there is uncertainty). Moreover, contracts in an uncertain world will necessarily be incomplete and have to be frequently re-negotiated. The costs of haggling about division of surplus, particularly if there is asymmetric information and asset specificity, may be considerable. \n\nIf a firm operated internally under the market system, many contracts would be required (for instance, even for procuring a pen or delivering a presentation). In contrast, a real firm has very few (though much more complex) contracts, such as defining a manager's power of direction over employees, in exchange for which the employee is paid. These kinds of contracts are drawn up in situations of uncertainty, in particular for relationships which last long periods of time. Such a situation runs counter to neo-classical economic theory. The neo-classical market is instantaneous, forbidding the development of extended agent-principal (employee-manager) relationships, of planning, and of trust. Coase concludes that “a firm is likely therefore to emerge in those cases where a very short-term contract would be unsatisfactory”, and that “it seems improbable that a firm would emerge without the existence of uncertainty”.\n\nHe notes that government measures relating to the market (sales taxes, rationing, price controls) tend to increase the size of firms, since firms internally would not be subject to such transaction costs. Thus, Coase defines the firm as \"the system of relationships which comes into existence when the direction of resources is dependent on the entrepreneur.\" We can therefore think of a firm as getting larger or smaller based on whether the entrepreneur organises more or fewer transactions.\n\nThe question then arises of what determines the size of the firm; why does the entrepreneur organise the transactions he does, why no more or less? Since the reason for the firm's being is to have lower costs than the market, the upper limit on the firm's size is set by costs rising to the point where internalising an additional transaction equals the cost of making that transaction in the market. (At the lower limit, the firm’s costs exceed the market’s costs, and it does not come into existence.) In practice, diminishing returns to management contribute most to raising the costs of organising a large firm, particularly in large firms with many different plants and differing internal transactions (such as a conglomerate), or if the relevant prices change frequently.\n\nCoase concludes by saying that the size of the firm is dependent on the costs of using the price mechanism, and on the costs of organisation of other entrepreneurs. These two factors together determine how many products a firm produces and how much of each.\n\nAccording to Louis Putterman, most economists accept distinction between intra-firm and interfirm transaction but also that the two shade into each other; the extent of a firm is not simply defined by its capital stock. George Barclay Richardson for example, notes that a rigid distinction fails because of the existence of intermediate forms between firm and market such as inter-firm co-operation.\n\nKlein (1983) asserts that “Economists now recognise that such a sharp distinction does not exist and that it is useful to consider also transactions occurring within the firm as representing market (contractual) relationships.” The costs involved in such transactions that are within a firm or even between the firms are the transaction costs.\n\nUltimately, whether the firm constitutes a domain of bureaucratic direction that is shielded from market forces or simply “a legal fiction”, “a nexus for a set of contracting relationships among individuals” (as Jensen and Meckling put it) is “a function of the completeness of markets and the ability of market forces to penetrate intra-firm relationships”.\n\nIt was only in the 1960s that the neo-classical theory of the firm was seriously challenged by alternatives such as managerial and behavioral theories. Managerial theories of the firm, as developed by William Baumol (1959 and 1962), Robin Marris (1964) and Oliver E. Williamson (1966), suggest that managers would seek to maximise their own utility and consider the implications of this for firm behavior in contrast to the profit-maximising case. (Baumol suggested that managers’ interests are best served by maximising sales after achieving a minimum level of profit which satisfies shareholders.) More recently this has developed into ‘principal–agent’ analysis (e.g., Spence and Zeckhauser and Ross (1973) on problems of contracting with asymmetric information) which models a widely applicable case where a principal (a shareholder or firm for example) cannot costlessly infer how an agent (a manager or supplier, say) is behaving. This may arise either because the agent has greater expertise or knowledge than the principal, or because the principal cannot directly observe the agent’s actions; it is asymmetric information which leads to a problem of moral hazard. This means that to an extent managers can pursue their own interests. Traditional managerial models typically assume that managers, instead of maximising profit, maximise a simple objective utility function (this may include salary, perks, security, power, prestige) subject to an arbitrarily given profit constraint (profit satisficing).\n\nThe behavioural approach, as developed in particular by Richard Cyert and James G. March of the Carnegie School places emphasis on explaining how decisions are taken within the firm, and goes well beyond neoclassical economics. Much of this depended on Herbert A. Simon’s work in the 1950s concerning behaviour in situations of uncertainty, which argued that “people possess limited cognitive ability and so can exercise only ‘bounded rationality’ when making decisions in complex, uncertain situations”. Thus individuals and groups tend to \"satisfice\"—that is, to attempt to attain realistic goals, rather than maximize a utility or profit function. Cyert and March argued that the firm cannot be regarded as a monolith, because different individuals and groups within it have their own aspirations and conflicting interests, and that firm behaviour is the weighted outcome of these conflicts. Organisational mechanisms (such as \"satisficing\" and sequential decision-taking) exist to maintain conflict at levels that are not unacceptably detrimental. Compared to ideal state of productive efficiency, there is organisational slack (Leibenstein’s X-inefficiency).\n\nArmen Alchian and Harold Demsetz's analysis of team production extends and clarifies earlier work by Coase. Thus according to them the firm emerges because extra output is provided by team production, but that the success of this depends on being able to manage the team so that metering problems (it is costly to measure the marginal outputs of the co-operating inputs for reward purposes) and attendant shirking (the moral hazard problem) can be overcome, by estimating marginal productivity by observing or specifying input behaviour. Such monitoring as is therefore necessary, however, can only be encouraged effectively if the monitor is the recipient of the activity’s residual income (otherwise the monitor herself would have to be monitored, ad infinitum). For Alchian and Demsetz, the firm therefore is an entity which brings together a team which is more productive working together than at arm’s length through the market, because of informational problems associated with monitoring of effort. In effect, therefore, this is a \"principal-agent\" theory, since it is asymmetric information within the firm which Alchian and Demsetz emphasise must be overcome. In Barzel (1982)’s theory of the firm, drawing on Jensen and Meckling (1976), the firm emerges as a means of centralising monitoring and thereby avoiding costly redundancy in that function (since in a firm the responsibility for monitoring can be centralised in a way that it cannot if production is organised as a group of workers each acting as a firm).\n\nThe weakness in Alchian and Demsetz’s argument, according to Williamson, is that their concept of team production has quite a narrow range of application, as it assumes outputs cannot be related to individual inputs. In practice this may have limited applicability (small work group activities, the largest perhaps a symphony orchestra), since most outputs within a firm (such as manufacturing and secretarial work) are separable, so that individual inputs can be rewarded on the basis of outputs. Hence team production cannot offer the explanation of why firms (in particular, large multi-plant and multi-product firms) exist, etc.\n\nFor Oliver E. Williamson, the existence of firms derives from ‘asset specificity’ in production, where assets are specific to each other such that their value is much less in a second-best use. This causes problems if the assets are owned by different firms (such as purchaser and supplier), because it will lead to protracted bargaining concerning the gains from trade, because both agents are likely to become locked into a position where they are no longer competing with a (possibly large) number of agents in the entire market, and the incentives are no longer there to represent their positions honestly: large-numbers bargaining is transformed into small-number bargaining.\n\nIf the transaction is a recurring or lengthy one, re-negotiation may be necessary as a continual power struggle takes place concerning the gains from trade, further increasing the transaction costs. Moreover, there are likely to be situations where a purchaser may require a particular, firm-specific investment of a supplier which would be profitable for both; but after the investment has been made it becomes a sunk cost and the purchaser can attempt to re-negotiate the contract such that the supplier may make a loss on the investment (this is the hold-up problem, which occurs when either party asymmetrically incurs substantial costs or benefits before being paid for or paying for them). In this kind of a situation, the most efficient way to overcome the continual conflict of interest between the two agents (or coalitions of agents) may be the removal of one of them from the equation by takeover or merger. Asset specificity can also apply to some extent to both physical and human capital, so that the hold-up problem can also occur with labour (e.g. labour can threaten a strike, because of the lack of good alternative human capital; but equally the firm can threaten to fire).\n\nProbably the best constraint on such opportunism is reputation (rather than the law, because of the difficulty of negotiating, writing and enforcement of contracts). If a reputation for opportunism significantly damages an agent’s dealings in the future, this alters the incentives to be opportunistic.\n\nWilliamson sees the limit on the size of the firm as being given partly by costs of delegation (as a firm’s size increase its hierarchical bureaucracy does too), and the large firm’s increasing inability to replicate the high-powered incentives of the residual income of an owner-entrepreneur. This is partly because it is in the nature of a large firm that its existence is more secure and less dependent on the actions of any one individual (increasing the incentives to shirk), and because intervention rights from the centre characteristic of a firm tend to be accompanied by some form of income insurance to compensate for the lesser responsibility, thereby diluting incentives. Milgrom and Roberts (1990) explain the increased cost of management as due to the incentives of employees to provide false information beneficial to themselves, resulting in costs to managers of filtering information, and often the making of decisions without full information. This grows worse with firm size and more layers in the hierarchy. Empirical analyses of transaction costs have attempted to measure and operationalize transaction costs. Research that attempts to measure transaction costs is the most critical limit to efforts to potential falsification and validation of transaction cost economics.\n\nThe theory of the firm considers what bounds the size and output variety of firms. This includes how firms may be able to combine labour and capital so as to lower the average cost of output, either from increasing, decreasing, or constant returns to scale for one product line or from economies of scope for more than one product line.\n\nEfficiency wage models like that of Shapiro and Stiglitz (1984) suggest wage rents as an addition to monitoring, since this gives employees an incentive not to shirk, given a certain probability of detection and the consequence of being fired. Williamson, Wachter and Harris (1975) suggest promotion incentives within the firm as an alternative to morale-damaging monitoring, where promotion is based on objectively measurable performance. (The difference between these two approaches may be that the former is applicable to a blue-collar environment, the latter to a white-collar one). Leibenstein (1966) sees a firm’s norms or conventions, dependent on its history of management initiatives, labour relations and other factors, as determining the firm’s \"culture\" of effort, thus affecting the firm’s productivity and hence size.\n\nGeorge Akerlof (1982) develops a gift exchange model of reciprocity, in which employers offer wages unrelated to variations in output and above the market level, and workers have developed a concern for each other’s welfare, such that all put in effort above the minimum required, but the more able workers are not rewarded for their extra productivity; again, size here depends not on rationality or efficiency but on social factors. In sum, the limit to the firm’s size is given where costs rise to the point where the market can undertake some transactions more efficiently than the firm.\n\nRecently, Yochai Benkler further questioned the rigid distinction between firms and markets based on the increasing salience of “commons-based peer production” systems such as open source software (e.g., Linux), Wikipedia, Creative Commons, etc. He put forth this argument in \"The Wealth of Networks: How Social Production Transforms Markets and Freedom\", which was released in 2006 under a Creative Commons share-alike license.\n\nIn modern contract theory, the “theory of the firm” is often identified with the “property rights approach” that was developed by Sanford J. Grossman, Oliver D. Hart, and John H. Moore. The property rights approach to the theory of the firm is also known as the “Grossman-Hart-Moore theory”. In their seminal work, Grossman and Hart (1986), Hart and Moore (1990) and Hart (1995) developed the incomplete contracting paradigm. They argue that if contracts cannot specify what is to be done given every possible contingency, then property rights (and hence firm boundaries) matter. Specifically, consider a seller of an intermediate good and a buyer. Should the seller own the physical assets that are necessary to produce the good (non-integration) or should the buyer be the owner (integration)? After relationship-specific investments have been made, the seller and the buyer bargain. When they are symmetrically informed, they will always agree to collaborate. Yet, the division of the ex post surplus depends on the parties’ disagreement payoffs (the payoffs they would get if no ex post agreement were reached), which in turn depend on the ownership structure. Thus, the ownership structure has an influence on the incentives to invest. A central insight of the theory is that the party with the more important investment decision should be the owner. Another prominent conclusion is that joint asset ownership is suboptimal if investments are in human capital. \n\nThe Grossman-Hart-Moore model has been successfully applied in many contexts, e.g. with regard to privatization. Chiu (1998) and DeMeza and Lockwood (1998) have extended the model by considering different bargaining games that the parties may play ex post (which can explain ownership by the less important investor). Oliver Williamson (2002) has criticized the Grossman-Hart-Moore model because it is focused on ex ante investment incentives, while it neglects ex post inefficiencies. Schmitz (2006) has studied a variant of the Grossman-Hart-Moore model in which a party may have or acquire private information about its disagreement payoff, which can explain ex post inefficiencies and ownership by the less important investor. Several variants of the Grossman-Hart-Moore model such as the one with private information can also explain joint ownership.\n\n\n"}
{"id": "19799715", "url": "https://en.wikipedia.org/wiki?curid=19799715", "title": "Transnationality Index", "text": "Transnationality Index\n\nThe Transnationality Index (TNI) is a means of ranking multinational corporations that is employed by economists and politicians. It is calculated as the arithmetic mean of the following three ratios (where \"foreign\" means outside of the corporation's home country):\n\nThe Transnationality Index was developed by the United Nations Conference on Trade and Development.\n\nMultinational corporations are also ranked by the amount of foreign assets that they own. However, the TNI ranking can differ markedly from this. For example, as of 2000, General Electric was the second largest multinational corporation in terms of foreign assert ownership. However, it ranked only 73rd by the TNI, with an index of only 40%. Although the company had large investments outside of the United States, most of its sales, employment, and assets were within the U.S.. In contrast, Exxon has a TNI of 68% and Vodafone has a TNI of 81%. As of 2001, General Electric ranked 75th, with a TNI of 36.7%. The 14 most transnational corporations originated in small countries (Switzerland, the United Kingdom, The Netherlands, Belgium, and Canada), whereas the largest multinational corporations in terms of foreign asset ownership all had low TNI scores. General Motors, the fourth largest multinational corporation in terms of foreign asset ownership only ranked 83rd (30.7%) on the TNI top 100. IBM ranked 50th (53.7%), Volkswagen ranked 45th (55.7%), and Toyota, the sixth largest multinational corporation in terms of foreign asset ownership, only ranked 82nd (30.9%) on the broader TNI scale.\n\nPeter Dicken, an honorary fellow of the School of Environment and Development at the University of Manchester, argues that TNI data refute the assertions of hyperglobalism. The data, he argues, prove false the claim that multinational corporations are \"inexorably, and inevitably, abandoning their ties to their country of origin\". If that were the case, we would expect the largest multinational corporations to have the majority of their assets, sales, and employmnent outside of their countries of origin, and thus the majority of those corporations to have high TNIs. In fact, in the UNCTAD TNI data for the top 100 multinational corporations for 2001, the mean TNI is 52.6%, 57 of the 100 have a TNI greater than 50%, and only a mere 16 have a TNI greater than 75%. Thus, he concludes, measured TNI data provide little evidence for multinational corporations having the proportions of their assets, sales, and employees outside of their home countries that one would expect for truly global firms.\n\n"}
{"id": "47884334", "url": "https://en.wikipedia.org/wiki?curid=47884334", "title": "Voluntary Principles on Security and Human Rights", "text": "Voluntary Principles on Security and Human Rights\n\nThe Voluntary Principles on Security and Human Rights is a collaborative effort by governments, major multinational extractive companies, and NGOs to provide guidance to companies on tangible steps that they can take to minimize the risk of human rights abuses in communities located near extraction sites. The principles documents provide guidance to companies in developing practices that maintain the safety and security of their operations while respecting the human rights of those who come into contact with security forces related to those operations. The Principles give guidance on risk assessment, public safety and security, human rights abuses, and the interaction between companies and private security. The initiative's Secretariat is administered by the Washington, D.C. office of Foley Hoag, LLP.\n\nThe written principles represent a voluntary agreement between participating companies, governments and NGO's on what steps companies should take to ensure their security practices respect human rights. To distinguish between the principles and the multi-stakeholder initiative, the principles are frequently abbreviated to the VPs and the tripartite organization is abbreviated to the VPI (Voluntary Principles Initiative).\n\nThe introduction to the text of the VPs includes the following statement that captures the various interests that the principles attempt to address:\n“Acknowledging that security is a fundamental need, shared by individuals, communities, businesses, and governments alike, and acknowledging the difficult security issues faced by Companies operating globally, we recognize that security and respect for human rights can and should be consistent”\n\nThe VPs include provisions defining expectations that:\n\nThrough the inclusion of elements of the VPs in services contracts with security providers, the VPs have been cited as a precedent for the inclusion of codes of conduct in legal contracts.\n\nWhile the VPs aren't designed to deal with root causes of conflicts, they do guide companies to have measures in place to prevent conflicts from escalating to violent confrontation. In an April 2015 article in The newsletter of the International Council on Mining and Metals, International Alert noted that while this potential seems to be recognized at senior levels in companies that have committed to the VPs, there is work still to be done to better implement the principles \"on the ground\". This reflects both the challenges of translating the principles into practices appropriate in each operating context, and also challenges companies can have in engaging with, and reaching agreements with, public authorities in operating locations.\n\nThe Principles are significant in two ways:\n\nThe Principles have also been cited as an example of moving the notion of corporate citizenship from principle into practice, though the commitment by participating companies to follow the approach articulated in the Principles.\n\nThe Voluntary Principles Initiative (VPI) was established in 2000, and although developed before the United Nations Guiding Principles on Business and Human Rights, the VPs are consistent with the responsibility of business to respect human rights as outlined in the UN Guiding Principles.\n\nThe Initiative is a multi-stakeholder initiative composed of governments, multinational oil, gas, and mining companies, and non-governmental organizations. The initiative is chaired by a member government, rotating between country members annually. As of 2015 participants in the initiative include 9 national governments, 27 companies, and 10 non-governmental organizations. In 2010 the law firm Foley Hoag LLP was selected to act as the Secretariat of the VPI.\n\nThe VPI holds an annual two-day plenary meeting to discuss progress and issues in the implementation of the VPs and to agree on collective priorities for the Initiative in the following year.\n\nSupporting documentation produced by the VPI for the VPs includes definitions of role and responsibilities of participating companies, governments, and non-governmental organizations.\n\nMember organizations have also collaborated to produce guidance documents to assist companies and civil society organizations in working with the VPs. \n\nMember countries also make efforts to encourage companies operating internationally to adhere to the VPs. The Canadian government includes the VPs in its framework \"Doing Business the Canadian Way: A Strategy to Advance Corporate Social Responsibility in Canada’s Extractive Sector Abroad\". The Swiss government describes its efforts as both raising awareness of the VPs among mining companies based in Switzerland, and also engaging with host governments in resource rich countries to persuade those governments to join the initiative and also to promote dialogue between host governments, companies and civil society - particularly in Peru and the Democratic Republic of the Congo.\n\n"}
