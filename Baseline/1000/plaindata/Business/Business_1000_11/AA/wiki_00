{"id": "24061342", "url": "https://en.wikipedia.org/wiki?curid=24061342", "title": "AMS Device Manager", "text": "AMS Device Manager\n\nAMS Device Manager is plant asset management software from Asset Optimization (a business unit of Emerson Process Management). It provides a single application for predictive diagnostics, documentation, calibration management, and device configuration for managing field instruments and digital valve controllers.\n\nAMS Device Manager is based on open communication standards, and is a core component of the PlantWeb digital plant architecture.\n\nAMS Device Manager is used in chemical, food & beverage, life sciences, LNG, pulp & paper, refining, and water & wastewater companies. It is used to increase quality, throughput, and availability, while reducing costs around operations & maintenance, safety, health & environment, In addition, AMS Device Manager facilitates plants and mills to startup faster and deliver a significant return on investment.\n\nAMS Device Manager supports digital instrument inputs and outputs; FOUNDATION fieldbus, HART, PROFIBUS DP, PROFIBUS PA, and WirelessHART.\n\n"}
{"id": "14452587", "url": "https://en.wikipedia.org/wiki?curid=14452587", "title": "Account manager", "text": "Account manager\n\nAn account manager is a person who works for a company and is responsible for the management of sales and relationships with particular customers. An account manager maintains the company's existing relationships with a client or group of clients, so that they will continue using the company for business. The account manager does not manage the daily running of the account itself. They manage the relationship with the client of the account(s) they are assigned to. Generally, a client will remain with one account manager throughout the duration of hiring the company. Account managers serve as the interface between the customer service and the sales team in a company. They are assigned a company's existing client accounts. The purpose of being assigned particular clients is to create long term relationships with the portfolio of assigned clients. The account manager serves to understand the customer's demands, plan how to meet these demands, and generate sales for the company as a result.\n\nKey accounts provide the most business because they contain a small number of clients which contribute a large portion of the company's sales. According to research, sales from a company's key accounts have increased from 23% in 1975 to 60% currently.\n\nThe responsibilities of an account manager can vary depending on the industry they work in, size of the company and nature of the business. Each customer account can vary in demands and an account manager may work with brand managers for one account and a media department for another. Account managers usually report directly to the account director or agency director of the activity and status of accounts and transactions. An account manager may also manage a single account or a variety of accounts depending on the requirement of the company. Although the responsibility can vary between companies and between accounts, there are a shared set of common responsibilities which are as follows:\n\n\nThere are situations in which an account manager, for example in advertising firms, is responsible for more than one account and therefore looks after multiple clients. When account locations do not overlap the account manager can be placed at the divisional, district, or territory level. When a sales team has a senior sales manager, the account manager coordinates sales accounts from other departments or specialties. In this scenario, the sales team will work under the direct supervision of influencers and deciders instead of with a buyer.\n\n\nGlobal account managers and national account managers may work together in a hierarchical or matrix structure. The trend is to move responsibility for the major key accounts to the global level.\n\nKey account manager is assigned to a company headquarters to oversee the account team assigned to a particular account. Key account management includes sales but also includes planning and managing the full relationship between a business and its most important customers. An account manager who works in this role will engage in a variety of tasks including project management, coordination, strategic planning, relationship management, negotiation, leadership and innovative development of opportunities, and keeping record of transaction of sale and purchase goods. The tasks may include working with product design and application, logistics, sales support, and marketing.\n\nThe basic assumption for a key account management model is the correct classification of the key accounts. A basic model often used in the period of 1950-1970 was the classification model of Webster. This model has been adapted by Milman and Wilson into a two-dimensional model and was paramount in the period of 1970-1990. Bensaou has tested this model empirically by his research of carmakers in the United States and Japan and made revisions. De Blick synthesized the adaptations into the 4S-model, a key account classification model. By the late 1990s, key account management spread to most B2B (business-to-business) models.\n\nAccount managers can work for small or large companies either corporate, financial, retail, or government for instance. Any company with a specific clients they conduct business with, can employ an account manager. Typical employers can be: \n\nAccount managers usually work in an office setting and can work more than 40 hours weekly. Travel is usually included in the job description. National or global account managers will very likely experience extra travel.\n\nAlthough personality and an aptitude for sales is key, a degree in business, marketing, or related field is typically required and depending on the nature of the account, a background in marketing or media studies may be preferred. Due to each company having different requirements, it is important to obtain information from each potential company of employment.\n\n\n"}
{"id": "26236025", "url": "https://en.wikipedia.org/wiki?curid=26236025", "title": "Acuris", "text": "Acuris\n\nAcuris, formerly Mergermarket Group (Mergermarket Ltd.), is a media company that provides specialist news, research, analysis and data on financial markets. It is owned by BC Partners, a private equity group. Singapore sovereign wealth fund GIC Private Limited has a minority stake.\n\nThe company has 1,300 staff, including 600 journalists and analysts, in 67 locations around the world, with headquarters in London, New York and Hong Kong.\n\nAcuris began with a single product, Mergermarket, which was established in December 1999 by founders Caspar Hobbs, Charlie Welsh and Gawn Rowan Hamilton. Its founding idea was that in the M&A market, “There is a lot of information out there but very little intelligence.”\n\nIn August 2006 the company, then known as Mergermarket Ltd, was acquired by The Financial Times Group for £101m, publisher of the Financial Times newspaper and FT.com. FT Group was a division of Pearson PLC, the international media group.\n\nIn 2013, the company was renamed to Mergermarket Group. In November 2013, Pearson agreed to sell Mergermarket Group to Londonprivate equity investor BC Partners in a transaction valuing the business intelligence and news service at £382m including debt. Based on the deal, Mergermarket Group was valued at 15 times its last year operating income.\n\nOn 15 January 2014, Moody's Investors Service assigned a B3 corporate family rating to the Mergermarket Group.\n\nIn July 2017, the Mergermarket Group rebranded as Acuris. The announcement of this move fueled speculation that a sale of the company might come in the next year. Indeed, BC Partners sold a minority stake of around 30% to Singapore sovereign wealth fund GIC in July 2017.\n\nAcuris sells web-based subscription services organised under five divisions:\n\nFixed Income\n\nTransactions & Infrastructure\n\nCompliance\n\nEquities\n\nResearch\n\n2007 – Infinata\n\n2009 – Capital Profile\n\n2010 – Xtract Research\n\n2012 – Inframation Group\n\n2014 – Perfect Information\n\n2014 – Law Report Group\n\n2015 – AVCJ and Unquote\n\n2016 – C6 Group\n\n2016 – Creditflux\n\n2017 – Sale of Infinata\n\n2017 – TIM Group\n\n2018 - Spark Spread\n"}
{"id": "41952259", "url": "https://en.wikipedia.org/wiki?curid=41952259", "title": "Adobe Marketing Cloud", "text": "Adobe Marketing Cloud\n\nAdobe Experience Cloud (AEC), formerly Adobe Marketing Cloud (AMC), is a collection of integrated online marketing and Web analytics products by Adobe Systems. \n\nAdobe Marketing Cloud includes a set of analytics, social, advertising, media optimization, targeting, Web experience management and content management products aimed at the advertising industry and hosted on Amazon Web Services. Like other Adobe Cloud services (e.g., Adobe Creative Cloud), the Adobe Marketing Cloud allows users with valid subscriptions to download the entire collection and use it directly on their computers with open access to online updates.\n\nThe Adobe Marketing Cloud collection was introduced to the public in October 2012 as Adobe began retiring the Omniture name it acquired in October 2009. Products of the defunct company were then integrated step by step into the new Cloud service which includes the following eight applications: Adobe Analytics, Adobe Target, Adobe Social, Adobe Experience Manager, Adobe Media Optimizer, Adobe Campaign, Audience Manager and Primetime. In November 2013, Adobe Systems introduced mobile features to its Marketing Cloud, making smartphones and other mobile devices new targets for analytics.\n\nOn September 15, 2009, Omniture, Inc. and Adobe Systems announced that Adobe would be acquiring Omniture, an online marketing and web analytics business unit in Orem, Utah. The deal of $1.8 billion, was completed on October 23, 2009, and is now joined by other Adobe acquisitions such as Day Software and Efficient Frontier, as the main components of Adobe's Digital Marketing Business Unit. Around 2012, Adobe withdrew the Omniture brand while its products were being integrated into the Adobe Marketing Cloud.\n\nOn May 21, 2018 Adobe announced the acquisition of Magento for 1.68$ billion. The addition of the Magento Commerce will enable commerce features to be integrated into the Adobe Experience Cloud. \n\nIn the same year, on September 20, 2018 Adobe acquired the marketing automation company Marketo.. The acquisition is expected to close in Q4 2018.\n"}
{"id": "45361330", "url": "https://en.wikipedia.org/wiki?curid=45361330", "title": "Arbeitskreis Börse", "text": "Arbeitskreis Börse\n\nThe Arbeitskreis Börse (abbreviated AKB or AK Boerse) is a non-profit organization located in Mannheim (Germany), focused on investment banking, capital markets, consulting and start-up companies within the financial technology sector. It is the oldest financial association lead by students in Germany and with over 1,000 members among the largest student organizations in Germany. The Arbeitskreis Börse is dedicated to connect students interested in financial markets associated with worldwide leading banks and consulting firms. These include Bank of America Merrill Lynch, KPMG and Oliver Wyman, which are premium partners of Arbeitskreis Börse. It is organized first and foremost by students of the University of Mannheim but is receiving increasing support from students of other tertiary establishments in Mannheim.\nThe Arbeitskreis Börse was founded in response to a perceived gap in the services provided by student societies at the University of Mannheim. Students interested in the stock exchange and financial markets, that sought to actively participate in capital markets did not have the opportunity to do so whilst engaging on campus as no such opportunity had yet been offered. \nTherefore, a small group of students independently organized an evening event within the university with the aim of creating a platform for students interested in the financial markets . Supported by the University this group developed into a non-profit association which was registered by the city of Mannheim on November 6th 1984.\n\nCorresponding with positive developments within financial markets, membership within the Arbeitskreis Börse steadily grew to exceed 1,000 students within a few years. Through their stock market analysis and research papers the Arbeitskries Börse rapidly became well throughout the Rhine-Neckar area, which led to leading global investment banks and consulting firms taking notice of the Arbeitskreis Börse. At the University of Mannheim the lecture series \"Semester Program of the Arbeitskreis Börse\" was established and the Arbeitskreis Börse focused on building partnerships with financial institutions within the European financial hubs, London and Frankfurt in the following years. To satisfy external requests the \"Kreis der Freunde und Förderer des AKB\" was founded in October 1988, an association that could be joined by everyone interested in the Arbeitskreis Börse.\n\nToday the Arbeitskreis Börse is the largest finance association run by students in Germany and the largest student society at the University of Mannheim. The Arbeitskreis Börse still provides the same extensive semester program and within the framework of their lectures they attract renowned speakers from both the world's leading investment banks and consulting firms. While doing this the students of the University of Mannheim get the chance to develop business contacts at several networking events. \n\nAs a student initiative, the Arbeitskreis Börse is mainly organized by students of the University of Mannheim in addition to several students from other tertiary establishments in Mannheim. In June, the Arbeitskreis Börse annually appoints its new leading team and executive board. The team is made up of the heads of five resorts: member management, event management, IT, marketing and social events. The executive board consists of five additional students who are responsible for resort management, finances, working groups, semester programme, special events, contacts to the university, developments of the Arbeitskreis Börse as an association, premium partners and other cooperation partners.\n\nThe Arbeitskreis Börse is supported by external partners, including Oliver Wyman, BCG, Bank of America Merrill Lynch, KPMG, PwC as well as the Mannheim Business School and alumni.\n\nThe Arbeitskreis Börse attempts to form a direct link between the theoretical frameworks covered in academic studies and the practical application of these within a company. \nDuring each semester various events are organized on Mannheim's main campus - the Mannheim Palace in Mannheim, Baden-Württemberg, Germany. Firstly topical lectures are held by renowned individuals from within the financial or consulting industry, these have for instance included Armin von Falkenhayn, Head of Global Corporate and Investment Banking Germany at Bank of America Merrill Lynch; Dr. Marcus Schenk, Chief Financial Officer at Deutsche Bank: Christian Zorn, Head of Investment Banking Germany at Morgan Stanley and Frank Mattern, former CEO at McKinsey & Co.. Occasionally representatives of other industries such as Kurt Bock, CEO at BASF SE, are invited to hold such lectures. Secondly, several company workshops as well as student work groups are conducted to support Mannheim students aspiring a career in the financial sector. Within these lectures, the various companies prepare case studies that provide insight into their structure and everyday work environment. Following these lectures, the audience is able to discuss specific topics with top tier personalities and get to know company recruiters as well as employees.\n\nThe companies that have held lectures or offered workshops during the past years in cooperation with the Arbeitskreis Börse are - besides the premium partners Bank of America Merrill Lynch, KPMG and Oliver Wyman - leading banks such as Morgan Stanley, Goldman Sachs, J. P. Morgan, Credit Suisse, Commerzbank, Fidelity, Deutsche Bank, Barclays, Royal Bank of Scotland, Lazard, N M Rothschild & Sons, UBS, RBC Capital Markets, Leonardo & Co. and Jefferies, the central banks European Central Bank and Deutsche Bundesbank in addition to leading consulting firms such as Bain & Company, Boston Consulting Group, McKinsey & Co., Roland Berger Strategy Consultants, PricewaterhouseCoopers, L.E.K. Consulting, strategy&. Further cooperating companies are Ernst & Young, Perella Weinberg Partners, Blackstone, 3I, Fresenius and BASF SE.\n\nIn cooperation with several company partners the Arbeitskreis Börse organizes an extensive lecture series every semester, composed of up to 12 company presentations and technical lectures concerned with current topics from the financial community and industry. To do so, companies from the fields of investment banking, asset management, consulting and auditing are invited to create a thought provoking presentation that explores various points of view. The lectures demonstrate relationships between the latest case studies and theoretical foundations from academia as well as findings from their business applications. At the following get-together with snacks and drinks students have the opportunity to get in personal contact with the speakers, to discuss specialist topics and ask questions regarding internships and career options.\n\nSimilar to this, the Arbeitskreis Börse, in cooperation with company partners organises numerous workshops every semester. Case studies offer a rare opportunity to gain better insight into specific business divisions and to analyse complex business matters. After the presentation of results, students come into contact with employees and are able ask specific questions and acquire information about a potential career within these companies. Getting to know preferences and strengths during the workshops makes it easier for a single business division or a certain enterprise to make a decision.\n\nAnother task of the Arbeitskreis Börse is to transmit knowledge in Excel and Bloomberg and to encourage active discussions with problems from the area of finance industry and management consultancy. Students can meet weekly to acquire and deepen fundamental knowledge in the respective subject areas under instruction from experienced Alumni to be optimally prepared for internships and entry into the workforce. To advanced students, a platform to discuss highly demanding problems is offered. The knowledge acquired in the working groups is indispensable for future entry into financial business.\n\nDuring each semester, numerous social events take place, which promote cohesion among members and help familiarise students with recently joined fellow students as well provide a temporary escape from stressful university life. Thus the students become part of a large community that outlasts their time at university and allows Alumni to return to Mannheim annually.\n\n2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "25290751", "url": "https://en.wikipedia.org/wiki?curid=25290751", "title": "Asset lock", "text": "Asset lock\n\nAn asset lock is a legal clause that prevents the assets of a company being used for private gain rather than the stated purposes of the organisation. Asset locks may be incorporated into the formal structure of a \"bencom\" (a type of industrial and provident society), community interest company, or charitable organisation.\n"}
{"id": "244933", "url": "https://en.wikipedia.org/wiki?curid=244933", "title": "Asset stripping", "text": "Asset stripping\n\nAsset stripping is a term used to refer to the practice of selling off a company's assets in order to improve returns for equity investors. With a lower level of assets, it is maintained, the business is rendered financially less stable or viable. For example, the sale-and-leaseback of a building would lead to an increased rental bill for the company.\n\nThe proceeds of the sale of assets may be used to lower the company's net debt. Alternatively, they may be used to pay a dividend to equityholders, leaving the company with lower net worth – i.e. the same level of debt but fewer assets (and weaker earnings) to support that debt.\n\nIn many cases where the term is used, a financial investor, referred to as a 'corporate raider', takes control of another company and then auctions off the acquired company's assets.\n\nThe term is generally used in a pejorative sense as such activity is not considered helpful to the company. Asset stripping has been considered to be a problem in economies such as Russia or China which are making a transition to being market economies. In these situations, managers of a state-owned company have been known to sell the assets they control, leaving behind nothing but debts to the state.\n\nEarly innovators of asset stripping were Carl Icahn, Victor Posner, and Nelson Peltz; all of whom were investors in the 1970s and 1980s. Carl Icahn performed one of the most notorious and hostile takeovers when he acquired Trans World Airlines in 1985. Here Icahn stripped TWA of its assets, selling them individually to repay the debt assimilated during the takeover. This particular corporate raid formed the idea of selling a company's assets in order to repay debt, and eventually increase the raider's net worth.\n\nOne of the biggest corporate raids that failed to materialize was the takeover of Gulf Oil by T. Boone Pickens. In 1984, Pickens attempted to acquire Gulf Oil and sell its assets individually to gain net worth. However, the purchase would have had been severely detrimental to Chevron; a customer of Gulf Oil. Therefore, Chevron stepped-in and merged with Gulf Oil for $13.2 billion, which at that time was the biggest merger between two companies.\n\nIn 2011, BC Partners acquired Phones 4u for a fee in the region of £700 million. At this point in time Phones 4u had already entered administration and had deep financial struggles. However, this did not prevent BC Partners from taking a £223 million dividend in order to pay off some of its own debts. Under the ownership of BC Partners, Phones 4u had very little financial freedom to expand and claim back the contract of EE. In September 2014 O2, Vodafone and Three decided to withdraw the rights for Phones 4u to sell their products. Due to the already poor financial situation of Phones 4u, the company has now no alternative but to sell its individual assets and close down. The net worth of Phones 4u's assets are estimated to exceed £1.4 billion, which provides BC Partners with the credit to pay off some of its debts and significantly improve its net worth.\n\nAsset stripping is highly controversial topic within the financial world. The benefits of asset stripping generally go to the corporate raiders, who can slash the debts they may have whilst improving their net worth. \nHowever, since asset stripping often results in thousands of employees losing their jobs without much consideration of the consequences to the affected community, the concept can be unpopular in the public sphere. One particular example of where asset stripping cost a significant number of workers their jobs was in the Fontainebleau Las Vegas LLC case. After the takeover, 433 people lost their jobs when assets were sold off and the company was stripped.\n\nThe process of asset stripping is not an illegal practice. If a corporate raider sells the target companies assets individually and pays off its debts the financial regulators have no room for investigation. However, some firms perform the process illegally and if found guilty may incur a substantial fine or even prison.\n\nAsset stripping by private equity firms in Europe is now regulated pursuant to the Alternative Investment Fund Managers Directive.\n\nThis is one of two methods a corporate raider can use to strip assets illegally. For this method to work, the corporate raider and the targeted firm must have the same director. Assets of the targeted firm are transferred to the corporate raider to ensure they remain safe from debt collectors. This process lets the corporate raider improve their net worth while leaving liabilities with the targeted company.\n\nThis method acts on completely fraudulent terms, and results in a higher punishment from the Financial Conduct Authority (FCA). Here, corporate raiders take ownership of a company on hostile terms, transfer the assets to their name and then put the dilapidated firm into liquidation. This ensures that the corporate raider improves their net worth, and has no liability to deal with the firm recently placed into liquidation. In comparison to phoenixing, this method has the highest rate of jail sentences issued for those found guilty.\n\n"}
{"id": "4898130", "url": "https://en.wikipedia.org/wiki?curid=4898130", "title": "Association for Business Communication", "text": "Association for Business Communication\n\nThe Association for Business Communication (ABC) is a learned society for the field of business communication. Its mission statement states: \"The Association for Business Communication (ABC) is an international organization committed to fostering excellence in business communication scholarship, research, education, and practice\". The organization is interdisciplinary, with members belonging to academic fields such as management, marketing, English, foreign languages, speech, communication, linguistics, and information systems. Additionally the organization brings together university academicians, business practitioners, and business consultants.\n\nABC is an international organization, divided into nine regional divisions (Europe, Asia and Pacific Rim, Caribbean and Central America, and five North American regions: Canada plus Eastern, Midwest, Southeast, Southwest, and Western United States), each with its own separate academic conferences. Each year, the association holds an International Conference in October or November. Midyear, regional conferences are held—one in the European region and one or two of the North American regions. The Asia and Pacific Rim region holds a conference every two years or more frequently.\n\nA board of directors and an executive committee lead the ABC. The board of directors is directly elected with a vice president elected from each of 9 regions and 12 directors at large, with staggered terms. The executive committee consists of a permanent position of executive director and four officers of the association. The members of the executive committee serve for four years in rotating capacity, beginning as second vice president in the first year, then first vice president the next, president the next, and past president the last year on the committee. The organization as a whole elects the second vice president position from among candidates on the board of directors.\n\nThe current executive director of the ABC is James M. Dubinsky (Virginia Tech). He took office in 2011.\n\nFor 2018-2019 the executive committee consists of President Lisa Gueldenzoph Snyder (North Carolina A & T University), First Vice President Marcel Robles (Eastern Kentucky University), Second Vice President Geert Jacobs (Ghent University), and immediate Past President Deborah Roebuck (Kennesaw State University).\n\nTwo peer-reviewed academic journals, \"International Journal of Business Communication\" and the \"Business and Professional Communication Quarterly\" are published by SAGE Publications on behalf of ABC.\n\nThe ABC has standing committees, as follow:\n\nThe ABC has ad hoc committees for interests that have not yet warranted a standing committee, but may in the future become so. Many of the standing committees began as ad hoc committees. In 2017, the ABC had these ad hoc committees:\n\nAdditionally, ABC members pursue a number of professional objectives through voluntary interest groups. A Special Interest Group may be convened by the First Vice President or by members’ current interest groups. In 2017, the ABC had the following Special Interest Groups:\n\nABC was founded in 1936, beginning with a modest membership of 72 members, all but one from the United States (the only exception being from Canada). The organization, based at the University of Illinois, was then named the “Association of College Teachers of Business Writers.” The next year, 1937, that name changed to the “American Business Writing Association.”\n\nBy the 1960s, the field had grown considerably and became heavily interested in areas well beyond business writing (such as oral presentations, negotiations, and nonverbal communication among others). In 1967, the Board of Directors voted to change the name of the organization to the “American Business Communication Association” to reflect this change.\n\nBy the late 1970s, as the membership of the organization grew to include more members from outside the Americas and as the focus of research expanded heavily into the fields of intercultural communication and cross-cultural business communication practice, the term “American” became increasingly inaccurate of both the membership and focus of the organization. In 1985, the Board of Directors voted to change the name to its current “Association for Business Communication.”\n\nIn 1990, ABC moved its headquarters from Illinois first to the University of North Texas (1990–1994) and then to Baruch College CUNY in New York City (from 1994–2007) and then to Stephen F. Austin State University in Nacogdoches, Texas (from 2007-2011). Presently, the ABC is headquartered at Stephen F. Austin State University in Nacogdoches, Texas.\n\n"}
{"id": "13712439", "url": "https://en.wikipedia.org/wiki?curid=13712439", "title": "Available-to-promise", "text": "Available-to-promise\n\nAvailable-to-promise (ATP) is a business function that provides a response to customer order enquiries, based on resource availability. It generates available quantities of the requested product, and delivery due dates. Therefore, ATP supports order promising and fulfillment, aiming to manage demand and match it to production plans.\n\nAvailable-to-promise functions are IT-enabled and usually integrated in enterprise management software packages. However, ATP execution may need to be adjusted for the way a certain company operates.\n\nA fundamental distinction between ATP functions is based on the push-pull strategy. Push-based ATP is based on forecasts regarding future demand - based on anticipation of demand, ATP quantities and availability dates are computed. A prominent example is the traditional determination of ATP based on the Master Production Schedule. The push-based approach is fundamentally limited by dependence on forecasts, which may prove inaccurate. Gross ATP represents the total available supply, and net ATP represents the supply remaining to support new demands, after existing commitments to customers have been accounted for.\n\nPull-based models, on the other hand, dynamically allocate resources in response to actual customer orders. This means that pull-based ATP is able to balance forecast-driven resource replenishment with order-triggered resource utilization, but because resources are allocated with each coming order, the process will yield myopic results.\n\nATP functions can be executed in real time, driven by each individual order, or in batch mode – meaning that at a certain time interval, the system checks availability for orders piled up in that period of time. The process is triggered by the need to check resource availability before making a commitment to deliver an order. For example, ATP calculation using SAP software depends on the level of \"stock, planned receipts (production orders, purchase orders, planned orders and so on), and planned requirements (sales orders, deliveries, reservations, etc.)\"\n\n"}
{"id": "11454119", "url": "https://en.wikipedia.org/wiki?curid=11454119", "title": "BIRT Project", "text": "BIRT Project\n\nThe Business Intelligence and Reporting Tools (BIRT) Project is an open source software project that provides reporting and business intelligence capabilities for rich client and web applications, especially those based on Java and Java EE. BIRT is a top-level software project within the Eclipse Foundation, an independent not-for-profit consortium of software industry vendors and an open source community.\n\nThe project's stated goals are to address a wide range of reporting needs within a typical application, ranging from operational or enterprise reporting to multi-dimensional online analytical processing (OLAP). Initially, the project has focused on and delivered capabilities that allow application developers to easily design and integrate reports into applications.\n\nThe project is supported by an active community of users at BIRT Developer Center and developers at the Eclipse.org BIRT Project page.\n\nBIRT has two main components: a visual report designer within the Eclipse IDE for creating BIRT Reports, and a runtime component for generating reports that can be deployed to any Java environment. The BIRT project also includes a charting engine that is both fully integrated into the report designer and can be used standalone to integrate charts into an application.\n\nBIRT Report designs are persisted as XML and can access a number of different data sources including JDO datastores, JFire Scripting Objects, POJOs, SQL databases, Web Services and XML.\n\nThe BIRT project was first proposed and sponsored by Actuate Corporation when Actuate joined the Eclipse Foundation as a Strategic Developer on August 24, 2004. The project was subsequently approved and became a top-level project within the Eclipse community on October 6, 2004 The project contributor community includes IBM, and Innovent Solutions.\n\nIn 2007 IBM's Tivoli Division adopted BIRT as the infrastructure for its Tivoli Common Reporting (TCR) product. TCR produces historical reports on Tivoli-managed IT resources and processes.\n\nThe initial project code base was designed and developed by Actuate beginning in early 2004 and donated to the Eclipse Foundation when the project was approved.\n\n"}
{"id": "40343976", "url": "https://en.wikipedia.org/wiki?curid=40343976", "title": "Banishment room", "text": "Banishment room\n\nA banishment room (also known as a chasing-out-room and a boredom room) is a modern employee exit management strategy whereby employees are transferred to another department where they are assigned meaningless work until they become disheartened enough to quit. Since the resignation is voluntary, the employee would not be eligible for certain benefits. The legality and ethicality of the practice is questionable and may be construed as constructive dismissal in some regions.\n\nThe practice, which is not officially acknowledged, is common in Japan which has strong labor laws and a tradition of permanent employment.\n\n"}
{"id": "56669014", "url": "https://en.wikipedia.org/wiki?curid=56669014", "title": "Bleisure travel", "text": "Bleisure travel\n\nBleisure travel (UK /ˈbleʒ.əʳ/ US /ˈbliː.ʒɚ/) is a portmanteau of “business” and “leisure”, and, it refers to “the activity of combining business travel with leisure time”.\n\nThe term bleisure was first published in 2009 by the Future Laboratory as part of their biannual Trend Briefing. The term was originally coined by writer and silent revolutionist, Jacob Strand, then a future forecaster working for The Future Laboratory. And co-written by journalist and futurologist Miriam Rayman.\n\nIn corporate business travel, extending a business trip for personal purposes is also known as “bizcation”\n\nThis phenomenon has been studied from 2011, from this year on, a report shows that bleisure travel has been maintaining a constant growth, accounting for 7% of all business trips.\n\nBleisure travellers can be described as “individuals who combine leisure with professional business obligations when abroad”.\n\nThe elements characterising bleisure travellers are some and different, this make difficult to draw a defined profile of these individuals. Bleisure is a widespread practice among US travellers, especially for those working in Technology, Healthcare, Public Administration sectors. A report shows that US traveller add bleisure to nearly half of the cases, precisely 52% for International trips and 42% for domestic ones. The main reasons for travelling would be conferences and conventions, team building, client meetings and presentations.\n\nBleisure travellers could be grouped according to various aspects, in particular gender, age and trip frequency.\n\nA research indicates that female business travellers would be more likely to take bleisure trips than male. In both groups 20% of travellers would take one or more bleisure trips in a year time, however women look to register higher rate: 8.5% against 6.8% of men.\n\nYounger travellers (between 20 and 25) look significantly more likely to add weekends to their business trips, measuring a rate close to 15%. This result is two or three times higher than those for 45-50 age interval. Millennials travellers deserve a special attention because they appear to be the individuals shaping the future of business travel, nearly twice likely to travel for business than Baby Boomers. This could be attributed to their flexible attitude to life, which would lead to the blurring of the line between their private and professional life. Millennials seem to work often from home or during the evening hours, to dedicate leisure on-goings to traditional work hours. A consequence of this freedom could be their willingness to extend business trips including vacation activities. In comparison with other age group, Millennials look to be more likely to take bleisure trips, compared to older travellers. However, it is not clear whether this reflects unique preferences or transitory life events.\n\nFrequent travellers taking 20 trips or more per year seem to be less than 5% likely to take a bleisure trip during the year, they would account for 8% of all bleisure trips. By contrast, one third of all bleisure trips would be taken by employees travelling once a month.\n\nIt seems evident that Millennials workers are increasingly demanding for bleisure from their work trip. Despite the state-of-the-art technology available nowadays, Millennials still prefer face-to-face meetings to get business done, therefore travel is still important for business companies.\n\nBleisure travel looks as a good opportunity to save on travel expenses, mostly for those who do not take a lot of vacation: 66% of US business traveller spend more money on leisure activities because of the money they save on travel. 60% take bleisure trips because do not have a lot of regular vacations. To decide whether to turn a business travel into bleisure or not, employee takes into consideration travelling on exciting destinations, additional costs required to extend the trip, how close the trip is to the weekend, the number of night they must stay for business, how affordable the hotel is, whether they have friends or family in the area, whether they can bring friends or family along. The general trend would indicate the addition of one or two days as the most frequent option, only 23% of bleisure travellers would extend their trips for more than three days. In many cases leisure days equal (37%) or even exceed business days (42%). The length of business trip seems to be a crucial factor when considering extending the trip in 62% of cases. In particular, when business days are more than three, bleisure travellers are not only likely to extend the trip, but also to visit different cities in the area.\n\nFor the employees:\nFor the companies:\n\nThe more likely bleisure destinations are cities offering a combination of different elements, which could allow the travellers to add leisure time to their business trip, such as sightseeing locations, sport venues and cultural events.\n\nThe difference between the origin and the destination city seems to affect bleisure rates: in the case of domestic trips, the amount of bleisure is expected to be low because the two cities should be easily accessible and share similar culture. On the other hand, long intercontinental trips would offer the chance to know realities that are different from ordinary. Researches seem to confirm this trend: bleisure trips would account for 5.2% of the domestic trips and 9.7% of the international ones. The highest bleisure rate (18.4%) appears in international trips when the origin and destination cities are located in different geographical regions.\n\nThe top bleisure cities in US are: Honolulu (above 20%), Miami, Orlando, Las Vegas (12-15%) NYC, San Francisco, Los Angeles, San Diego and Fort Lauderdale (9-12%). In Europe rates looks lower, this could be due to the shorter domestic and continental routes: Lisbon, Barcelona, Nice, Istanbul (8-11%), London, Dublin, Amsterdam, Moscow, Rome, Geneva, Paris, Madrid (6-8%).\n\nGiven that bleisure travel is a rising phenomenon, there is still no official regulation, therefore extending business trips for bleisure would have to be decided by management on a case-by-case basis. Some preventive measures could be followed in advance to avoid misunderstanding.\n"}
{"id": "17099795", "url": "https://en.wikipedia.org/wiki?curid=17099795", "title": "Blindspots analysis", "text": "Blindspots analysis\n\nBlindspots analysis (also blind spots analysis) is a method aimed at uncovering obsolete, incomplete, or incorrect assumptions in a decision maker’s mental scheme of the environment. Michael Porter used the term \"blind spots\" to refer to conventional wisdom which no longer holds true, but which still guides business strategy. The concept was further popularized by Barbara Tuchman, in her book \"The March of Folly\" (1984), to describe political decisions and strategies which were clearly wrong in their assumptions, and by other authors since, such as social psychologists Mahzarin Banaji and Anthony Greenwald in their study of prejudice.\n\nBen Gilad fully developed, in his book, \"Business Blindspots\" (1994), the following three-step \"Gilad method\" for uncovering blind spots\n\nUnderlying Blindspots Analysis is an assumption about the inherent biases of decision making at the top of organizations (business, government or otherwise) exceeding those of their subordinates or outsiders. While many top executives in business and government organizations are smart, capable people, they are also vulnerable to several decision biases that come with their powerful positions, including cognitive dissonance, motivated cognitions, overconfidence, and ego-involvement. The impaired ability of leaders to see reality for what it is, and the more objective (less ego-involved) analysis of analysts and mid-level planners means that Step 3 of the Blindspots Analysis can be a powerful tool for pointing to potential blinders\n\n\n"}
{"id": "4822", "url": "https://en.wikipedia.org/wiki?curid=4822", "title": "Board of directors", "text": "Board of directors\n\nA board of directors is a recognized group of people who jointly oversee the activities of an organization, which can be either a for-profit business, nonprofit organization, or a government agency. Such a board's powers, duties, and responsibilities are determined by government regulations (including the jurisdiction's corporations law) and the organization's own constitution and bylaws. These authorities may specify the number of members of the board, how they are to be chosen, and how often they are to meet.\n\nIn an organization with voting members, the board is accountable to, and might be subordinate to, the organization's full membership, which usually vote for the members of the board. In a stock corporation, non-executive directors are voted for by the shareholders and the board is the highest authority in the management of the corporation. The board of directors appoints the chief executive officer of the corporation and sets out the overall strategic direction. In corporations with dispersed ownership, the identification and nomination of directors (that shareholders vote for or against) are often done by the board itself, leading to a high degree of self-perpetuation. In a non-stock corporation with no general voting membership, the board is the supreme governing body of the institution; its members are sometimes chosen by the board itself.\n\nOther names include board of directors and advisors, board of governors, board of managers, board of regents, board of trustees, or board of visitors. It may also be called \"the executive board\" and is often simply referred to as \"the board\".\n\nTypical duties of boards of directors include:\n\nThe legal responsibilities of boards and board members vary with the nature of the organization, and between jurisdictions. For companies with publicly trading stock, these responsibilities are typically much more rigorous and complex than for those of other types.\n\nTypically, the board chooses one of its members to be the \"chairman\" (often now called the \"chair\" or \"chairperson\"), who holds whatever title is specified in the by-laws or articles of association. However, in membership organizations, the members elect the president of the organization and the president becomes the board chair, unless the by-laws say otherwise.\n\nThe directors of an organization are the persons who are members of its board. Several specific terms categorize directors by the presence or absence of their other relationships to the organization.\n\nAn inside director is a director who is also an employee, officer, chief executive, major shareholder, or someone similarly connected to the organization. Inside directors represent the interests of the entity's stakeholders, and often have special knowledge of its inner workings, its financial or market position, and so on.\n\nTypical inside directors are:\n\nAn inside director who is employed as a manager or executive of the organization is sometimes referred to as an executive director (not to be confused with the title executive director sometimes used for the CEO position in some organizations). Executive directors often have a specified area of responsibility in the organization, such as finance, marketing, human resources, or production.\n\nAn outside director is a member of the board who is not otherwise employed by or engaged with the organization, and does not represent any of its stakeholders. A typical example is a director who is president of a firm in a different industry. Outside directors are not employees of the company or affiliated with it in any other way.\n\nOutside directors bring outside experience and perspectives to the board. For example, for a company that only serves a domestic market, the presence of CEOs from global multinational corporations as outside directors can help to provide insights on export and import opportunities and international trade options. One of the arguments for having outside directors is that they can keep a watchful eye on the inside directors and on the way the organization is run. Outside directors are unlikely to tolerate \"insider dealing\" between insider directors, as outside directors do not benefit from the company or organization. Outside directors are often useful in handling disputes between inside directors, or between shareholders and the board. They are thought to be advantageous because they can be objective and present little risk of conflict of interest. On the other hand, they might lack familiarity with the specific issues connected to the organization's governance and they might not know about the industry or sector in which the organization is operating.\n\n\nIndividual directors often serve on more than one board. This practice results in an interlocking directorate, where a relatively small number of individuals have significant influence over a large number of important entities. This situation can have important corporate, social, economic, and legal consequences, and has been the subject of significant research.\n\nThe process for running a board, sometimes called the board process, includes the selection of board members, the setting of clear board objectives, the dissemination of documents or board package to the board members, the collaborative creation of an agenda for the meeting, the creation and follow-up of assigned action items, and the assessment of the board process through standardized assessments of board members, owners, and CEOs. The science of this process has been slow to develop due to the secretive nature of the way most companies run their boards, however some standardization is beginning to develop. Some who are pushing for this standardization in the USA are the National Association of Corporate Directors, McKinsey Consulting and The Board Group.\n\nA board of directors conducts its meetings according to the rules and procedures contained in its governing documents. These procedures may allow the board to conduct its business by conference call or other electronic means. They may also specify how a quorum is to be determined.\n\nMost organizations have adopted \"Robert's Rules of Order\" as its guide to supplement its own rules. In this book, the rules for conducting board meetings may be less formal if there is no more than about a dozen board members present. An example of the informality is that motions are not required if it's clear what is being discussed.\n\nHistorically, nonprofit boards have not uncommonly had large boards with up to twenty-four members, but a modern trend is to have smaller boards as small as six or seven people. Studies suggest that after seven people, each additional person reduces the effectiveness of group decision-making.\n\nThe role and responsibilities of a board of directors vary depending on the nature and type of business entity and the laws applying to the entity (see types of business entity). For example, the nature of the business entity may be one that is traded on a public market (public company), not traded on a public market (a private, limited or closely held company), owned by family members (a family business), or exempt from income taxes (a non-profit, not for profit, or tax-exempt entity). There are numerous types of business entities available throughout the world such as a corporation, limited liability company, cooperative, business trust, partnership, private limited company, and public limited company.\n\nMuch of what has been written about boards of directors relates to boards of directors of business entities actively traded on public markets. More recently, however, material is becoming available for boards of private and closely held businesses including family businesses.\n\nA board-only organization is one whose board is self-appointed, rather than being accountable to a base of members through elections; or in which the powers of the membership are extremely limited.\n\nIn membership organizations, such as a society made up of members of a certain profession or one advocating a certain cause, a board of directors may have the responsibility of running the organization in between meetings of the membership, especially if the membership meets infrequently, such as only at an annual general meeting. The amount of powers and authority delegated to the board depend on the bylaws and rules of the particular organization. Some organizations place matters exclusively in the board's control while in others, the general membership retains full power and the board can only make recommendations.\n\nThe setup of a board of directors vary widely across organizations and may include provisions that are applicable to corporations, in which the \"shareholders\" are the members of the organization. A difference may be that the membership elects the officers of the organization, such as the president and the secretary, and the officers become members of the board in addition to the directors and retain those duties on the board. The directors may also be classified as officers in this situation. There may also be ex-officio members of the board, or persons who are members due to another position that they hold. These ex-officio members have all the same rights as the other board members.\n\nMembers of the board may be removed before their term is complete. Details on how they can be removed are usually provided in the bylaws. If the bylaws do not contain such details, the section on disciplinary procedures in \"Robert's Rules of Order\" may be used.\n\nIn a publicly held company, directors are elected to represent and are legally obligated as fiduciaries to represent owners of the company—the shareholders/stockholders. In this capacity they establish policies and make decisions on issues such as whether there is dividend and how much it is, stock options distributed to employees, and the hiring/firing and compensation of upper management.\n\nTheoretically, the control of a company is divided between two bodies: the board of directors, and the shareholders in general meeting. In practice, the amount of power exercised by the board varies with the type of company. In small private companies, the directors and the shareholders are normally the same people, and thus there is no real division of power. In large public companies, the board tends to exercise more of a supervisory role, and individual responsibility and management tends to be delegated downward to individual professional executives (such as a finance director or a marketing director) who deal with particular areas of the company's affairs.\n\nAnother feature of boards of directors in large public companies is that the board tends to have more \"de facto\" power. Many shareholders grant proxies to the directors to vote their shares at general meetings and accept all recommendations of the board rather than try to get involved in management, since each shareholder's power, as well as interest and information is so small. Larger institutional investors also grant the board proxies. The large number of shareholders also makes it hard for them to organize. However, there have been moves recently to try to increase shareholder activism among both institutional investors and individuals with small shareholdings.\n\nA contrasting view is that in large public companies it is upper management and not boards that wield practical power, because boards delegate nearly all of their power to the top executive employees, adopting their recommendations almost without fail. As a practical matter, executives even choose the directors, with shareholders normally following management recommendations and voting for them.\n\nIn most cases, serving on a board is not a career unto itself. For major corporations, the board members are usually professionals or leaders in their field. In the case of outside directors, they are often senior leaders of other organizations. Nevertheless, board members often receive remunerations amounting to hundreds of thousands of dollars per year since they often sit on the boards of several companies. Inside directors are usually not paid for sitting on a board, but the duty is instead considered part of their larger job description. Outside directors are usually paid for their services. These remunerations vary between corporations, but usually consist of a yearly or monthly salary, additional compensation for each meeting attended, stock options, and various other benefits. such as travel, hotel and meal expenses for the board meetings. Tiffany & Co., for example, pays directors an annual retainer of $46,500, an additional annual retainer of $2,500 if the director is also a chairperson of a committee, a per-meeting-attended fee of $2,000 for meetings attended in person, a $500 fee for each meeting attended via telephone, in addition to stock options and retirement benefits.\n\nIn some European and Asian countries, there are two separate boards, an executive board for day-to-day business and a supervisory board (elected by the shareholders and employees) for supervising the executive board. In these countries, the CEO (chief executive or managing director) presides over the executive board and the chairman presides over the supervisory board, and these two roles will always be held by different people. This ensures a distinction between management by the executive board and governance by the supervisory board and allows for clear lines of authority. The aim is to prevent a conflict of interest and too much power being concentrated in the hands of one person. There is a strong parallel here with the structure of government, which tends to separate the political cabinet from the management civil service. In the United States, the board of directors (elected by the shareholders) is often equivalent to the supervisory board, while the executive board may often be known as the executive committee (operating committee or executive council), composed of the CEO and their direct reports (other C-level officers, division/subsidiary heads).\n\nThe development of a separate board of directors to manage/govern/oversee a company has occurred incrementally and indefinitely over legal history. Until the end of the 19th century, it seems to have been generally assumed that the general meeting (of all shareholders) was the supreme organ of a company, and that the board of directors merely acted as an agent of the company subject to the control of the shareholders in general meeting.\n\nHowever, by 1906, the English Court of Appeal had made it clear in the decision of \"Automatic Self-Cleansing Filter Syndicate Co Ltd v Cuninghame\" [1906] 2 Ch 34 that the division of powers between the board and the shareholders in general meaning depended on the construction of the articles of association and that, where the powers of management were vested in the board, the general meeting could not interfere with their lawful exercise. The articles were held to constitute a contract by which the members had agreed that \"the directors and the directors alone shall manage.\"\n\nThe new approach did not secure immediate approval, but it was endorsed by the House of Lords in \"Quin & Axtens v Salmon\" [1909] AC 442 and has since received general acceptance. Under English law, successive versions of Table A have reinforced the norm that, unless the directors are acting contrary to the law or the provisions of the Articles, the powers of conducting the management and affairs of the company are vested in them.\n\nThe modern doctrine was expressed in \"John Shaw & Sons (Salford) Ltd v Shaw\" [1935] 2 KB 113 by Greer LJ as follows:\n\nA company is an entity distinct alike from its shareholders and its directors. Some of its powers may, according to its articles, be exercised by directors, certain other powers may be reserved for the shareholders in general meeting. If powers of management are vested in the directors, they and they alone can exercise these powers. The only way in which the general body of shareholders can control the exercise of powers by the articles in the directors is by altering the articles, or, if opportunity arises under the articles, by refusing to re-elect the directors of whose actions they disapprove. They cannot themselves usurp the powers which by the articles are vested in the directors any more than the directors can usurp the powers vested by the articles in the general body of shareholders.\nIt has been remarked that this development in the law was somewhat surprising at the time, as the relevant provisions in Table A (as it was then) seemed to contradict this approach rather than to endorse it.\n\nIn most legal systems, the appointment and removal of directors is voted upon by the shareholders in general meeting or through a proxy statement. For publicly traded companies in the U.S., the directors which are available to vote on are largely selected by either the board as a whole or a nominating committee. Although in 2002 the New York Stock Exchange and the NASDAQ required that nominating committees consist of independent directors as a condition of listing, nomination committees have historically received input from management in their selections even when the CEO does not have a position on the board. Shareholder nominations can only occur at the general meeting itself or through the prohibitively expensive process of mailing out ballots separately; in May 2009 the SEC proposed a new rule allowing shareholders meeting certain criteria to add nominees to the proxy statement. In practice for publicly traded companies, the managers (inside directors) who are purportedly accountable to the board of directors have historically played a major role in selecting and nominating the directors who are voted on by the shareholders, in which case more \"gray outsider directors\" (independent directors with conflicts of interest) are nominated and elected.\n\nDirectors may also leave office by resignation or death. In some legal systems, directors may also be removed by a resolution of the remaining directors (in some countries they may only do so \"with cause\"; in others the power is unrestricted).\n\nSome jurisdictions also permit the board of directors to appoint directors, either to fill a vacancy which arises on resignation or death, or as an addition to the existing directors.\n\nIn practice, it can be quite difficult to remove a director by a resolution in general meeting. In many legal systems, the director has a right to receive special notice of any resolution to remove him or her; the company must often supply a copy of the proposal to the director, who is usually entitled to be heard by the meeting. The director may require the company to circulate any representations that he wishes to make. Furthermore, the director's contract of service will usually entitle him to compensation if he is removed, and may often include a generous \"golden parachute\" which also acts as a deterrent to removal.\n\nA recent study examines how corporate shareholders voted in director elections in the United States. It found that directors received fewer votes from shareholders when their companies performed poorly, had excess CEO compensation, or had poor shareholder protection. Also, directors received fewer votes when they did not regularly attend board meetings or received negative recommendations from a proxy advisory firm. The study also shows that companies often improve their corporate governance by removing poison pills or classified boards and by reducing excessive CEO pay after their directors receive low shareholder support.\n\nBoard accountability to shareholders is a recurring issue. In 2010, the \"New York Times\" noted that several directors who had overseen companies which had failed in the financial crisis of 2007–2010 had found new positions as directors. The SEC sometimes imposes a ban (a \"D&O bar\") on serving on a board as part of its fraud cases, and one of these was upheld in 2013.\n\nThe exercise by the board of directors of its powers usually occurs in board meetings. Most legal systems require sufficient notice to be given to all directors of these meetings, and that a quorum must be present before any business may be conducted. Usually, a meeting which is held without notice having been given is still valid if all of the directors attend, but it has been held that a failure to give notice may negate resolutions passed at a meeting, because the persuasive oratory of a minority of directors might have persuaded the majority to change their minds and vote otherwise.\n\nIn most common law countries, the powers of the board are vested in the board as a whole, and not in the individual directors. However, in instances an individual director may still bind the company by his acts by virtue of his ostensible authority (see also: the rule in \"Turquand's Case\").\n\nBecause directors exercise control and management over the organization, but organizations are (in theory) run for the benefit of the shareholders, the law imposes strict duties on directors in relation to the exercise of their duties. The duties imposed on directors are fiduciary duties, similar to those that the law imposes on those in similar positions of trust: agents and trustees.\n\nThe duties apply to each director separately, while the powers apply to the board jointly. Also, the duties are owed to the company itself, and not to any other entity. This does not mean that directors can never stand in a fiduciary relationship to the individual shareholders; they may well have such a duty in certain circumstances.\n\nDirectors must exercise their powers for a proper purpose. While in many instances an improper purpose is readily evident, such as a director looking to feather his or her own nest or divert an investment opportunity to a relative, such breaches usually involve a breach of the director's duty to act in good faith. Greater difficulties arise where the director, while acting in good faith, is serving a purpose that is not regarded by the law as proper.\n\nThe seminal authority in relation to what amounts to a proper purpose is the Supreme Court decision in Eclairs Group Ltd v JKX Oil & Gas plc (2015). The case concerned the powers of directors under the articles of association of the company to disenfranchise voting rights attached to shares for failure to properly comply with notice served on the shareholders. Prior to that case the leading authority was \"Howard Smith Ltd v Ampol Ltd\" [1974] AC 821. The case concerned the power of the directors to issue new shares. It was alleged that the directors had issued a large number of new shares purely to deprive a particular shareholder of his voting majority. An argument that the power to issue shares could only be properly exercised to raise new capital was rejected as too narrow, and it was held that it would be a proper exercise of the director's powers to issue shares to a larger company to ensure the financial stability of the company, or as part of an agreement to exploit mineral rights owned by the company. If so, the mere fact that an incidental result (even if it was a desired consequence) was that a shareholder lost his majority, or a takeover bid was defeated, this would not itself make the share issue improper. But if the sole purpose was to destroy a voting majority, or block a takeover bid, that would be an improper purpose.\n\nNot all jurisdictions recognised the \"proper purpose\" duty as separate from the \"good faith\" duty however.\n\nDirectors cannot, without the consent of the company, fetter their discretion in relation to the exercise of their powers, and cannot bind themselves to vote in a particular way at future board meetings. This is so even if there is no improper motive or purpose, and no personal advantage to the director.\n\nThis does not mean, however, that the board cannot agree to the company entering into a contract which binds the company to a certain course, even if certain actions in that course will require further board approval. The company remains bound, but the directors retain the discretion to vote against taking the future actions (although that may involve a breach by the company of the contract that the board previously approved).\n\nAs fiduciaries, the directors may not put themselves in a position where their interests and duties conflict with the duties that they owe to the company. The law takes the view that good faith must not only be done, but must be manifestly seen to be done, and zealously patrols the conduct of directors in this regard; and will not allow directors to escape liability by asserting that his decision was in fact well founded. Traditionally, the law has divided conflicts of duty and interest into three sub-categories.\n\nBy definition, where a director enters into a transaction with a company, there is a conflict between the director's interest (to do well for himself out of the transaction) and his duty to the company (to ensure that the company gets as much as it can out of the transaction). This rule is so strictly enforced that, even where the conflict of interest or conflict of duty is purely hypothetical, the directors can be forced to disgorge all personal gains arising from it. In \"Aberdeen Ry v Blaikie\" (1854) 1 Macq HL 461 Lord Cranworth stated in his judgment that:\n\nHowever, in many jurisdictions the members of the company are permitted to ratify transactions which would otherwise fall foul of this principle. It is also largely accepted in most jurisdictions that this principle can be overridden in the company's constitution.\n\nIn many countries, there is also a statutory duty to declare interests in relation to any transactions, and the director can be fined for failing to make disclosure.\n\nDirectors must not, without the informed consent of the company, use for their own profit the company's assets, opportunities, or information. This prohibition is much less flexible than the prohibition against the transactions with the company, and attempts to circumvent it using provisions in the articles have met with limited success.\n\nIn \"Regal (Hastings) Ltd v Gulliver\" [1942] All ER 378 the House of Lords, in upholding what was regarded as a wholly unmeritorious claim by the shareholders, held that:\n\nAnd accordingly, the directors were required to disgorge the profits that they made, and the shareholders received their windfall.\n\nThe decision has been followed in several subsequent cases, and is now regarded as settled law.\n\nDirectors cannot compete directly with the company without a conflict of interest arising. Similarly, they should not act as directors of competing companies, as their duties to each company would then conflict with each other.\n\nTraditionally, the level of care and skill which has to be demonstrated by a director has been framed largely with reference to the non-executive director. In \"Re City Equitable Fire Insurance Co\" [1925] Ch 407, it was expressed in purely subjective terms, where the court held that:\n\nHowever, this decision was based firmly in the older notions (see above) that prevailed at the time as to the mode of corporate decision making, and effective control residing in the shareholders; if they elected and put up with an incompetent decision maker, they should not have recourse to complain.\n\nHowever, a more modern approach has since developed, and in \"Dorchester Finance Co Ltd v Stebbing\" [1989] BCLC 498 the court held that the rule in \"Equitable Fire\" related only to skill, and not to diligence. With respect to diligence, what was required was:\n\nThis was a dual subjective and objective test, and one deliberately pitched at a higher level.\n\nMore recently, it has been suggested that both the tests of skill and diligence should be assessed objectively and subjectively; in the United Kingdom, the statutory provisions relating to directors' duties in the new Companies Act 2006 have been codified on this basis.\n\nIn most jurisdictions, the law provides for a variety of remedies in the event of a breach by the directors of their duties:\n\nHistorically, directors' duties have been owed almost exclusively to the company and its members, and the board was expected to exercise its powers for the financial benefit of the company. However, more recently there have been attempts to \"soften\" the position, and provide for more scope for directors to act as good corporate citizens. For example, in the United Kingdom, the Companies Act 2006 requires directors of companies \"to promote the success of the company for the benefit of its members as a whole\" and sets out the following six factors regarding a director's duty to promote success:\n\nThis represents a considerable departure from the traditional notion that directors' duties are owed only to the company. Previously in the United Kingdom, under the Companies Act 1985, protections for non-member stakeholders were considerably more limited (see, for example, s.309 which permitted directors to take into account the interests of employees but which could only be enforced by the shareholders and not by the employees themselves). The changes have therefore been the subject of some criticism.\n\nMost companies have weak mechanisms for bringing the voice of society into the board room. They rely on personalities who weren't appointed for their understanding of societal issues. Often they give limited focus (both through time and financial resource) to issues of corporate responsibility and sustainability. A Social Board has society designed into its structure. It elevates the voice of society through specialist appointments to the board and mechanisms that empower innovation from within the organisation. Social Boards align themselves with themes that are important to society.These may include measuring worker pay ratios, linking personal social and environmental objectives to remuneration, integrated reporting, fair tax and B-Corp Certification. \n\nSocial Boards recognise that they are part of society and that they require more than a licence to operate to succeed.They balance short-term shareholder pressure against long-term value creation, managing the business for a plurality of stakeholders including employees, shareholders, supply chains and civil society.\n\nThe Sarbanes–Oxley Act of 2002 has introduced new standards of accountability on boards of U.S. companies or companies listed on U.S. stock exchanges. Under the Act, directors risk large fines and prison sentences in the case of accounting crimes. Internal control is now the direct responsibility of directors. The vast majority of companies covered by the Act have hired internal auditors to ensure that the company adheres to required standards of internal control. The internal auditors are required by law to report directly to an audit board, consisting of directors more than half of whom are outside directors, one of whom is a \"financial expert.\"\n\nThe law requires companies listed on the major stock exchanges (NYSE, NASDAQ) to have a majority of independent directors—directors who are not otherwise employed by the firm or in a business relationship with it.\n\nAccording to the Corporate Library's study, the average size of publicly traded company's board is 9.2 members, and most boards range from 3 to 31 members. According to Investopedia, some analysts think the ideal size is seven. State law may specify a minimum number of directors, maximum number of directors, and qualifications for directors (e.g. whether board members must be individuals or may be business entities).\n\nWhile a board may have several committees, two—the compensation committee and audit committee—are critical and must be made up of at least three independent directors and no inside directors. Other common committees in boards are nominating and governance.\n\nDirectors of Fortune 500 companies received median pay of $234,000 in 2011. Directorship is a part-time job. A recent National Association of Corporate Directors study found directors averaging just 4.3 hours a week on board work. Surveys indicate that about 20% of nonprofit foundations pay their board members, and 2% of American nonprofit organizations do. 80% of nonprofit organizations require board members to personally contribute to the organization, as BoardSource recommends. This percentage has increased in recent years.\n\nAccording to John Gillespie, a former investment banker and co-author of a book critical of boards, \"Far too much of their time has been for check-the-box and cover-your-behind activities rather than real monitoring of executives and providing strategic advice on behalf of shareholders\". At the same time, scholars have found that individual directors have a large effect on major corporate initiatives such as mergers and acquisitions and cross-border investments.\n\nThe issue of gender representation on corporate boards of directors has been the subject of much criticism in recent years. Governments and corporations have responded with measures such as legislation mandating gender quotas and comply or explain systems to address the disproportionality of gender representation on corporate boards. A study of the French corporate elite has found that certain social classes are also disproportionately represented on boards, with those from the upper and, especially, upper-middle classes tending to dominate.\n\n\n"}
{"id": "30214247", "url": "https://en.wikipedia.org/wiki?curid=30214247", "title": "Business History Conference", "text": "Business History Conference\n\nThe Business History Conference (BHC) is a scholarly organization devoted to encouraging all aspects of research, writing, and teaching about business history and about the environment in which businesses operate. Founded in 1954, the organization is now international in scope, with approximately 30 percent of its membership residing outside North America. The BHC is based at the Center for the History of Business, Technology, and Society at the Hagley Library and relies on Center Director Roger Horowitz and Center Coordinator Carol Lockman to manage the BHC's annual meetings, finances, membership, and other business. Horowitz also serves as the BHC's Secretary-Treasurer and Lockman as Managing Editor of the quarterly journal \"Enterprise & Society\". In addition, the BHC publishes an online collection of abstracts and selected papers from its annual meeting, \"BEH On-Line\". The organization also operates H-Business, one of the earliest H-Net discussion lists, and maintains an on-line full-text archives of its print proceedings journal, \"Business and Economic History\". It also publishes The Exchange, a blog devoted to news of interest to business and economic historians. The BHC holds an annual meeting that provides a forum for discussing current research in business history and related fields and offers an opportunity for people with similar interests to meet and exchange ideas. Participation from overseas scholars is especially encouraged, and joint meetings with the European Business History Association are held regularly. The BHC sponsors a number of awards and prizes, including the Hagley Prize in Business History and the Cambridge Journals Article Prize; it endeavors to support scholars entering the field through its travel-to-meeting grants, its Doctoral Dissertation Colloquium, and its Krooss Dissertation Prize. Sub-groups within the organization promote the interests of women in business history, business historians teaching at business schools, and emerging scholars.\n\nThe BHC is a member of the International Economic History Association and is an affiliated organization of the American Historical Association and of H-Net.\n"}
{"id": "35828623", "url": "https://en.wikipedia.org/wiki?curid=35828623", "title": "Business reporting", "text": "Business reporting\n\nBusiness reporting or enterprise reporting refers to both \"the public reporting of operating and financial data by a business enterprise,\" and \"the regular provision of information to decision-makers within an organization to support them in their work.\" It is a fundamental part of the larger movement towards improved business intelligence and knowledge management. Implementation often involves extract, transform, and load (ETL) procedures in coordination with a data warehouse and then using one or more reporting tools. Reports can be distributed in print form, via email or accessed via a corporate intranet.\n\nWith the expansion of information technology there has been an increase in the production of unified reports which join different views of an organization in one place. This reporting process involves querying data sources with different logical models to produce a human readable report. For example, a decision maker may need to query a human resources databases and a capital improvements databases to show how efficiently space is being used across an entire corporation.\n\nReporting can also be used for verification and cross-checks. Audit teams like FINRA and SEC adhere to reports for all business firms. Standard Business Reporting is a group of international programs instigated by a number of governments with the end of make business the centre when it comes to managing business-to-government reporting obligations.\n\n"}
{"id": "1885799", "url": "https://en.wikipedia.org/wiki?curid=1885799", "title": "Business valuation", "text": "Business valuation\n\nBusiness valuation is a process and a set of procedures used to estimate the economic value of an owner's interest in a business. Valuation is used by financial market participants \nto determine the price they are willing to pay or receive to effect a sale of a business. In addition to estimating the selling price of a business, the same valuation tools are often used by business appraisers to resolve disputes related to estate and gift taxation, divorce litigation, allocate business purchase price among business assets, establish a formula for estimating the value of partners' ownership interest for buy-sell agreements, and many other business and legal purposes such as in shareholders deadlock, divorce litigation and estate contest. In some cases, the court would appoint a forensic accountant as the joint expert doing the business valuation.\n\nBefore the value of a business can be measured, the valuation assignment must specify the reason for and circumstances surrounding the business valuation. These are formally known as the business value standard and premise of value.\n\nThe standard of value is the hypothetical conditions under which the business will be valued. The premise of value relates to the assumptions, such as assuming that the business will continue forever in its current form (going concern), or that the value of the business lies in the proceeds from the sale of all of its assets minus the related debt (sum of the parts or assemblage of business assets).\n\n\n\nPremise of value for fair value Calculation\n\nBusiness valuation results can vary considerably depending upon the choice of both the standard and premise of value. In an actual business sale, it would be expected that the buyer and seller, each with an incentive to achieve an optimal outcome, would determine the fair market value of a business asset that would compete in the market for such an acquisition. If the synergies are specific to the company being valued, they may not be considered. Fair value also does not incorporate discounts for lack of control or marketability.\n\nNote, however, that it is possible to achieve the fair market value for a business asset that is being liquidated in its secondary market. This underscores the difference between the standard and premise of value.\n\nThese assumptions might not, and probably do not, reflect the actual conditions of the market in which the subject business might be sold. However, these conditions are assumed because they yield a uniform standard of value, after applying generally accepted valuation techniques, which allows meaningful comparison between businesses which are similarly situated.\n\nA business valuation report generally begins with a summary of the purpose and scope of business appraisal as well as its date and stated audience. What follows is a description of national, regional and local economic conditions existing as of the valuation date, as well as the conditions of the industry in which the subject business operates.\nA common source of economic information for the first section of the business valuation report is the Federal Reserve Board's Beige Book, published eight times a year by the Federal Reserve Bank. State governments and industry associations also publish useful statistics describing regional and industry conditions.\n\nThe financial statement analysis generally involves common size analysis, ratio analysis (liquidity, turnover, profitability, etc.), trend analysis and industry comparative analysis. This permits the valuation analyst to compare the subject company to other businesses in the same or similar industry, and to discover trends affecting the company and/or the industry over time. By comparing a company's financial statements in different time periods, the valuation expert can view growth or decline in revenues or expenses, changes in capital structure, or other financial trends. How the subject company compares to the industry will help with the risk assessment and ultimately help determine the discount rate and the selection of market multiples.\n\nIt is important to mention that among the financial statements, the primary statement to show the liquidity of the company is cash flow.\nCash flow shows the company's cash in and out flow.\n\nThe key objective of normalization is to identify the ability of the business to generate income for its owners. A measure of the income is the amount of cash flow that the owners can remove from the business without adversely affecting its operations. The most common normalization adjustments fall into the following four categories:\n\nThree different approaches are commonly used in business valuation: the income approach, the asset-based approach, and the market approach. Within each of these approaches, there are various techniques for determining the value of a business using the definition of value appropriate for the appraisal assignment. Generally, the income approaches determine value by calculating the net present value of the benefit stream generated by the business (discounted cash flow); the asset-based approaches determine value by adding the sum of the parts of the business (net asset value); and the market approaches determine value by comparing the subject company to other companies in the same industry, of the same size, and/or within the same region. A number of business valuation models can be constructed that utilize various methods under the three business valuation approaches. Venture Capitalists and Private Equity professionals have long used the First chicago method which essentially combines the income approach with the market approach.\n\nIn certain cases equity may also be valued by applying the techniques and frameworks developed for financial options, via a real options framework, as discussed below.\n\nIn determining which of these approaches to use, the valuation professional must exercise discretion. Each technique has advantages and drawbacks, which must be considered when applying those techniques to a particular subject company. Most treatises and court decisions encourage the valuator to consider more than one technique, which must be reconciled with each other to arrive at a value conclusion. A measure of common sense and a good grasp of mathematics is helpful.\n\nThe income approach relies upon the economic principle of expectation: the value of business is based on the expected economic benefit and level of risk associated with the investment. Income based valuation methods determine fair market value by dividing the benefit stream generated by the subject or target company times a discount or capitalization rate. The discount or capitalization rate converts the stream of benefits into present value. There are several different income methods, including capitalization of earnings or cash flows, discounted future cash flows (\"DCF\"), and the excess earnings method (which is a hybrid of asset and income approaches). The result of a value calculation under the income approach is generally the fair market value of a controlling, marketable interest in the subject company, since the entire benefit stream of the subject company is most often valued, and the capitalization and discount rates are derived from statistics concerning public companies. IRS Revenue Ruling 59-60 states that earnings are preeminent for the valuation of closely held operating companies.\n\nHowever, income valuation methods can also be used to establish the value of a severable business asset as long as an income stream can be attributed to it. An example is licensable intellectual property whose value needs to be established to arrive at a supportable royalty structure.\n\nA discount rate or capitalization rate is used to determine the present value of the expected returns of a business. The discount rate and capitalization rate are closely related to each other, but distinguishable. Generally speaking, the discount rate or capitalization rate may be defined as the yield necessary to attract investors to a particular investment, given the risks associated with that investment.\n\n\nThere are several different methods of determining the appropriate discount rates. The discount rate is composed of two elements: (1) the risk-free rate, which is the return that an investor would expect from a secure, practically risk-free investment, such as a high quality government bond; plus (2) a risk premium that compensates an investor for the relative level of risk associated with a particular investment in excess of the risk-free rate. Most importantly, the selected discount or capitalization rate must be consistent with stream of benefits to which it is to be applied.\n\nCapitalization and discounting valuation calculations become mathematically equivalent under the assumption that the business income grows at a constant rate.\n\nThe capital asset pricing model (CAPM) provides one method of determining a discount rate in business valuation. The CAPM originated from the Nobel Prize-winning studies of Harry Markowitz, James Tobin, and William Sharpe. The method derives the discount rate by adding risk premium to the risk-free rate. The risk premium is derived by multiplying the equity risk premium with \"beta\", a measure of stock price volatility. Beta is compiled by various researchers for particular industries and companies, and measures systematic risks of investment.\n\nOne of the criticisms of the CAPM is that beta is derived from volatility of prices of publicly traded companies, which differ from non-publicly companies in liquidity, marketability, capital structures and control. Other aspects such as access to credit markets, size, and management depth are generally different, too. The rate build-up method also requires an assessment of the subject company's risk, which provides valuation of itself. Where a privately held company can be shown to be sufficiently similar to a public company, the CAPM may be suitable. However, it requires the knowledge of market stock prices for calculation. For private companies that do not sell stock on the public capital markets, this information is not readily available. Therefore, calculation of beta for private firms is problematic. The build-up cost of capital model is the typical choice in such cases.\n\nWith regard to capital market-oriented valuation approaches there are numerous valuation approaches besides the traditional CAPM model. They include, for example, the Arbitrage pricing theory (APT) as well as the Consumption-based Capital Asset Pricing Model (CCAPM). Furthermore, alternative capital market models were developed, having in common that expected return hinge on \"multiple\" risk sources and thus being less restrictive:\n\n\nNevertheless, even these models are not wholly consistent, as they also show market anomalies. However, the method of incomplete replication and risk covering come along without the need of capital market data and thus being more solid. Equally notable is the existence of investment based approaches, considering different investment opportunities and determining an investment program by means of linear optimization. Among them the approximative decomposition valuation approach can be found.\n\nThe Cost of Equity (Ke) is computed by using the Modified Capital Asset Pricing Model (Mod. CAPM)\n\nformula_1\n\nWhere:\n\nformula_2 = Risk free rate of return (Generally taken as 10-year Government Bond Yield)\n\nformula_3 = Beta Value (Sensitivity of the stock returns to market returns)\n\nformula_4 = Cost of Equity\n\nformula_5 = Market Rate of Return\n\nSCRP = Small Company Risk Premium\n\nCSRP= Company specific Risk premium\n\nThe weighted average cost of capital is an approach to determining a discount rate. The WACC method determines the subject company's actual cost of capital by calculating the weighted average of the company's cost of debt and cost of equity. The WACC must be applied to the subject company's net cash flow to total invested capital.\n\nOne of the problems with this method is that the valuator may elect to calculate WACC according to the subject company's existing capital structure, the average industry capital structure, or the optimal capital structure. Such discretion detracts from the objectivity of this approach, in the minds of some critics.\n\nIndeed, since the WACC captures the risk of the subject business itself, the existing or contemplated capital structures, rather than industry averages, are the appropriate choices for business valuation.\n\nOnce the capitalization rate or discount rate is determined, it must be applied to an appropriate economic income stream: pretax cash flow, aftertax cash flow, pretax net income, after tax net income, excess earnings, projected cash flow, etc. The result of this formula is the indicated value before discounts. Before moving on to calculate discounts, however, the valuation professional must consider the indicated value under the asset and market approaches.\n\nCareful matching of the discount rate to the appropriate measure of economic income is critical to the accuracy of the business valuation results. Net cash flow is a frequent choice in professionally conducted business appraisals. The rationale behind this choice is that this earnings basis corresponds to the equity discount rate derived from the Build-Up or CAPM models: the returns obtained from investments in publicly traded companies can easily be represented in terms of net cash flows. At the same time, the discount rates are generally also derived from the public capital markets data.\n\nThe Build-Up Method is a widely recognized method of determining the after-tax net cash flow discount rate, which in turn yields the capitalization rate. The figures used in the Build-Up Method are derived from various sources. This method is called a \"build-up\" method because it is the sum of risks associated with various classes of assets. It is based on the principle that investors would require a greater return on classes of assets that are more risky. The first element of a Build-Up capitalization rate is the risk-free rate, which is the rate of return for long-term government bonds. Investors who buy large-cap equity stocks, which are inherently more risky than long-term government bonds, require a greater return, so the next element of the Build-Up method is the equity risk premium. In determining a company's value, the long-horizon equity risk premium is used because the Company's life is assumed to be infinite. The sum of the risk-free rate and the equity risk premium yields the long-term average market rate of return on large public company stocks.\n\nSimilarly, investors who invest in small cap stocks, which are riskier than blue-chip stocks, require a greater return, called the \"size premium.\" Size premium data is generally available from two sources: Morningstar's (formerly Ibbotson & Associates') Stocks, Bonds, Bills & Inflation and Duff & Phelps' Risk Premium Report.\n\nBy adding the first three elements of a Build-Up discount rate, we can determine the rate of return that investors would require on their investments in small public company stocks. These three elements of the Build-Up discount rate are known collectively as the \"systematic risks.\" This type of investment risk cannot be avoided through portfolio diversification. It arises from external factors and affect every type of investment in the economy. As a result, investors taking systematic risk are rewarded by an additional premium.\n\nIn addition to systematic risks, the discount rate must include \"unsystematic risk\" representing that portion of total investment risk that can be avoided through diversification. Public capital markets do not provide evidence of unsystematic risk since investors that fail to diversify cannot expect additional returns.\n\nUnsystematic risk falls into two categories. One of those categories is the \"industry risk premium\". It is also known as idiosyncratic risk and can be observed by studying the returns of a group of companies operating in the same industry sector. Morningstar's yearbooks contain empirical data to quantify the risks associated with various industries, grouped by SIC industry code.\n\nThe other category of unsystematic risk is referred to as \"company specific risk.\" Historically, no published data has been available to quantify specific company risks. However, as of late 2006, new research has been able to quantify, or isolate, this risk for publicly traded stocks through the use of Total Beta calculations. P. Butler and K. Pinkerton have outlined a procedure which sets the following two equations together:\n\nTotal Cost of Equity (TCOE) = risk-free rate + total beta*equity risk premium\nTCOE = risk-free rate + beta*equity risk premium + size premium + company-specific risk premium\n\nThe only unknown in the two equations is the company specific risk premium.\n\nWhile it is possible to isolate the company-specific risk premium as shown above, many appraisers just key in on the total cost of equity (TCOE) provided by the following equation: TCOE = risk-free rate + Total beta*equity risk premium.\n\nIt is similar to using the market approach in the income approach instead of adding separate (and potentially redundant) measures of risk in the build-up approach. The use of total beta (developed by Aswath Damodaran) is a relatively new concept. It is, however, gaining acceptance in the business valuation community since it is based on modern portfolio theory. Total beta can help appraisers develop a cost of capital who were content to use their intuition alone when previously adding a purely subjective company-specific risk premium in the build-up approach.\n\nIt is important to understand why this capitalization rate for small, privately held companies is significantly higher than the return that an investor might expect to receive from other common types of investments, such as money market accounts, mutual funds, or even real estate. Those investments involve substantially lower levels of risk than an investment in a closely held company. Depository accounts are insured by the federal government (up to certain limits); mutual funds are composed of publicly traded stocks, for which risk can be substantially minimized through portfolio diversification.\n\nClosely held companies, on the other hand, frequently fail for a variety of reasons too numerous to name. Examples of the risk can be witnessed in the storefronts on every Main Street in America. There are no federal guarantees. The risk of investing in a private company cannot be reduced through diversification, and most businesses do not own the type of hard assets that can ensure capital appreciation over time. This is why investors demand a much higher return on their investment in closely held businesses; such investments are inherently much more risky. (This paragraph is biased, presuming that by the mere fact that a company is closely held, it is prone towards failure.)\n\nThe value of asset-based analysis of a business is equal to the sum of its parts. That is the theory underlying the asset-based approaches to business valuation. The asset approach to business valuation reported on the books of the subject company at their acquisition value, net of depreciation where applicable. These values must be adjusted to fair market value wherever possible.\nThe value of a company's intangible assets, such as goodwill, is generally impossible to determine apart from the company's overall enterprise value. For this reason, the asset-based approach is not the most probative method of determining the value of going business concerns. In these cases, the asset-based approach yields a result that is probably lesser than the fair market value of the business.\nIn considering an asset-based approach, the valuation professional must consider whether the shareholder whose interest is being valued would have any authority to access the value of the assets directly. Shareholders own shares in a corporation, but not its assets, which are owned by the corporation. A controlling shareholder may have the authority to direct the corporation to sell all or part of the assets it owns and to distribute the proceeds to the shareholder(s). The non-controlling shareholder, however, lacks this authority and cannot access the value of the assets. As a result, the value of a corporation's assets is not the true indicator of value to a shareholder who cannot avail himself of that value. The asset based approach is the entry barrier value and should preferably to be used in businesses having mature or declining growth cycle and is more suitable for capital intensive industry.\n\nAdjusted net book value may be the most relevant standard of value where liquidation is imminent or ongoing; where a company earnings or cash flow are nominal, negative or worth less than its assets; or where net book value is standard in the industry in which the company operates. The adjusted net book value may also be used as a \"sanity check\" when compared to other methods of valuation, such as the income and market approaches...\n\nThe market approach to business valuation is rooted in the economic principle of competition: that in a free market the supply and demand forces will drive the price of business assets to a certain equilibrium. Buyers would not pay more for the business, and the sellers will not accept less, than the price of a comparable business enterprise. The buyers and sellers are assumed to be equally well informed and acting in their own interests to conclude a transaction. It is similar in many respects to the \"comparable sales\" method that is commonly used in real estate appraisal. The market price of the stocks of publicly traded companies engaged in the same or a similar line of business, whose shares are actively traded in a free and open market, can be a valid indicator of value when the transactions in which stocks are traded are sufficiently similar to permit meaningful comparison.\n\nThe difficulty lies in identifying public companies that are sufficiently comparable to the subject company for this purpose. Also, as for a private company, the equity is less liquid (in other words its stocks are less easy to buy or sell) than for a public company, its value is considered to be slightly lower than such a market-based valuation would give.\n\nWhen there is a lack of comparison with direct competition, a meaningful alternative could be a vertical value-chain approach where the subject company is compared with, for example, a known downstream industry to have a good feel of its value by building useful correlations with its downstream companies. Such comparison often reveals useful insights which help business analysts better understand performance relationship between the subject company and its downstream industry. For example, if a growing subject company is in an industry more concentrated than its downstream industry with a high degree of interdependence, one should logically expect the subject company performs better than the downstream industry in terms of growth, margins and risk.\n\nGuideline Public Company method entails a comparison of the subject company to publicly traded companies. The comparison is generally based on published data regarding the public companies' stock price and earnings, sales, or revenues, which is expressed as a fraction known as a \"multiple.\" If the guideline public companies are sufficiently similar to each other and the subject company to permit a meaningful comparison, then their multiples should be similar. The public companies identified for comparison purposes should be similar to the subject company in terms of industry, product lines, market, growth, margins and risk.\n\nHowever, if the subject company is privately owned, its value must be adjusted for lack of marketability. This is usually represented by a discount, or a percentage reduction in the value of the company when compared to its publicly traded counterparts. This reflects the higher risk associated with holding stock in a private company. The difference in value can be quantified by applying a discount for lack of marketability. This discount is determined by studying prices paid for shares of ownership in private companies that eventually offer their stock in a public offering. Alternatively, the lack of marketability can be assessed by comparing the prices paid for restricted shares to fully marketable shares of stock of public companies.\n\nAs above, in certain cases equity may be valued by applying the techniques and frameworks developed for financial options, via a real options framework. For general discussion as to context see #\"Valuing flexibility\" under corporate finance, and Contingent claim valuation; for detail as to applicability and other considerations see #\"Limitations\" under real options valuation.\n\nIn general, equity may be viewed as a call option on the firm, and this allows for the valuation of troubled firms which may otherwise be difficult to analyse. The classic application of this approach is to the valuation of distressed securities, already discussed in the original Black-Scholes paper. Here, since the principle of limited liability protects equity investors, shareholders would choose not to repay the firm's debt where the value of the firm (as perceived) is less than the value of the outstanding debt; see bond valuation. Of course, where firm value is greater than debt value, the shareholders would choose to repay (i.e. exercise their option) and not to liquidate. Thus analogous to out the money options which nevertheless have value, equity will (may) have value even if the value of the firm falls (well) below the face value of the outstanding debt—and this value can (should) be determined using the appropriate option valuation technique. (A further application of this principle is the analysis of principal–agent problems; see contract design under principal–agent problem.)\n\nCertain business situations, and the parent firms in those cases, are also logically analysed under an options framework; see \"Applications\" under the Real options valuation references. Just as a financial option gives its owner the right, but not the obligation, to buy or sell a security at a given price, companies that make strategic investments have the right, but not the obligation, to exploit opportunities in the future; management will of course only exercise where this makes economic sense. Thus, for companies facing uncertainty of this type, the stock price may (should) be seen as the sum of the value of existing businesses (i.e., the discounted cash flow value) plus any real option value. Equity valuations here, may (should) thus proceed likewise. Compare PVGO.\n\nA common application is to natural resource investments. Here, the underlying asset is the resource itself; the value of the asset is a function of both quantity of resource available and the price of the commodity in question. The value of the resource is then the difference between the value of the asset and the cost associated with developing the resource. Where positive (\"in the money\") management will undertake the development, and will not do so otherwise, and a resource project is thus effectively a call option. A may (should) therefore also be analysed using the options approach. Specifically, the value of the firm comprises the value of already active projects determined via DCF valuation (or other standard techniques) and undeveloped reserves as analysed using the real options framework. See Mineral economics.\n\nProduct patents may also be valued as options, and the value of firms holding these patents — typically firms in the , , and sectors — can (should) similarly be viewed as the sum of the value of products in place and the portfolio of patents yet to be deployed. As regards the option analysis, since the patent provides the firm with the right to develop the product, it will do so only if the present value of the expected cash flows from the product exceeds the cost of development, and the patent rights thus correspond to a call option. See Patent valuation § Option-based method. Similar analysis may be applied to options on films (or other works of intellectual property) and the valuation of film studios.\n\nBesides mathematical approaches for the valuation of companies a rather unknown method includes also the cultural aspect. The so-called Cultural valuation method (Cultural Due Diligence) seeks to combine existing knowledge, motivation and internal culture with the results of a net-asset-value method. Especially during a company takeover uncovering hidden problems is of high importance for a later success of the business venture.\n\nThe valuation approaches yield the fair market value of the Company as a whole. In valuing a minority, non-controlling interest in a business, however, the valuation professional must consider the applicability of discounts that affect such interests.\nDiscussions of discounts and premiums frequently begin with a review of the \"levels of value\". There are three common levels of value: controlling interest, marketable minority, and non-marketable minority.\nThe intermediate level, marketable minority interest, is less than the controlling interest level and higher than the non-marketable minority interest level. The marketable minority interest level represents the perceived value of equity interests that are freely traded without any restrictions. These interests are generally traded on the New York Stock Exchange, AMEX, NASDAQ, and other exchanges where there is a ready market for equity securities. These values represent a minority interest in the subject companies – small blocks of stock that represent less than 50% of the company's equity, and usually much less than 50%.\nControlling interest level is the value that an investor would be willing to pay to acquire more than 50% of a company's stock, thereby gaining the attendant prerogatives of control. Some of the prerogatives of control include: electing directors, hiring and firing the company's management and determining their compensation; declaring dividends and distributions, determining the company's strategy and line of business, and acquiring, selling or liquidating the business. This level of value generally contains a control premium over the intermediate level of value, which typically ranges from 25% to 50%. An additional premium may be paid by strategic investors who are motivated by synergistic motives.\nNon-marketable, minority level is the lowest level on the chart, representing the level at which non-controlling equity interests in private companies are generally valued or traded. This level of value is discounted because no ready market exists in which to purchase or sell interests. Private companies are less \"liquid\" than publicly traded companies, and transactions in private companies take longer and are more uncertain. Between the intermediate and lowest levels of the chart, there are restricted shares of publicly traded companies.\nDespite a growing inclination of the IRS and Tax Courts to challenge valuation discounts, Shannon Pratt suggested in a scholarly presentation recently that valuation discounts are actually increasing as the differences between public and private companies is widening . Publicly traded stocks have grown more liquid in the past decade due to rapid electronic trading, reduced commissions, and governmental deregulation. These developments have not improved the liquidity of interests in private companies, however.\nValuation discounts are multiplicative, so they must be considered in order. Control premiums and their inverse, minority interest discounts, are considered before marketability discounts are applied.\n\nThe first discount that must be considered is the discount for lack of control, which in this instance is also a minority interest discount. Minority interest discounts are the inverse of control premiums, to which the following mathematical relationship exists:\nMID = 1 – [1 / (1 + CP)]\nThe most common source of data regarding control premiums is the Control Premium Study, published annually by Mergerstat since 1972. Mergerstat compiles data regarding publicly announced mergers, acquisitions and divestitures involving 10% or more of the equity interests in public companies, where the purchase price is $1 million or more and at least one of the parties to the transaction is a U.S. entity. Mergerstat defines the \"control premium\" as the percentage difference between the acquisition price and the share price of the freely traded public shares five days prior to the announcement of the M&A transaction.\nWhile it is not without valid criticism, Mergerstat control premium data (and the minority interest discount derived therefrom) is widely accepted within the valuation profession.\n\nA \"discount for lack of marketability\" (DLOM) may be applied to a minority block of stock to alter the valuation of that block. \n\nAnother factor to be considered in valuing closely held companies is the marketability of an interest in such businesses. Marketability is defined as the ability to convert the business interest into cash quickly, with minimum transaction and administrative costs, and with a high degree of certainty as to the amount of net proceeds. There is usually a cost and a time lag associated with locating interested and capable buyers of interests in privately held companies, because there is no established market of readily available buyers and sellers.\n\nAll other factors being equal, an interest in a publicly traded company is worth more because it is readily marketable. Conversely, an interest in a private-held company is worth less because no established market exists. \"The IRS Valuation Guide for Income, Estate and Gift Taxes, Valuation Training for Appeals Officers\" acknowledges the relationship between value and marketability, stating: \"Investors prefer an asset which is easy to sell, that is, liquid.\"\n\nThe discount for lack of control is separate and distinguishable from the discount for lack of marketability. It is the valuation professional's task to quantify the lack of marketability of an interest in a privately held company. Because, in this case, the subject interest is not a controlling interest in the Company, and the owner of that interest cannot compel liquidation to convert the subject interest to cash quickly, and no established market exists on which that interest could be sold, the discount for lack of marketability is appropriate.\n\nSeveral empirical studies have been published that attempt to quantify the discount for lack of marketability. These studies include the restricted stock studies and the pre-IPO studies. The aggregate of these studies indicate average discounts of 35% and 50%, respectively. Some experts believe the Lack of Control and Marketability discounts can aggregate discounts for as much as ninety percent of a Company's fair market value, specifically with family-owned companies.\n\nRestricted stocks are equity securities of public companies that are similar in all respects to the freely traded stocks of those companies except that they carry a restriction that prevents them from being traded on the open market for a certain period of time, which is usually one year (two years prior to 1990). This restriction from active trading, which amounts to a lack of marketability, is the only distinction between the restricted stock and its freely traded counterpart. Restricted stock can be traded in private transactions and usually do so at a discount. The restricted stock studies attempt to verify the difference in price at which the restricted shares trade versus the price at which the same unrestricted securities trade in the open market as of the same date.\nThe underlying data by which these studies arrived at their conclusions has not been made public. Consequently, it is not possible when valuing a particular company to compare the characteristics of that company to the study data. Still, the existence of a marketability discount has been recognized by valuation professionals and the Courts, and the restricted stock studies are frequently cited as empirical evidence. Notably, the lowest average discount reported by these studies was 26% and the highest average discount was 40%.\n\nIn addition to the restricted stock studies, U.S. publicly traded companies are able to sell stock to offshore investors (SEC Regulation S, enacted in 1990) without registering the shares with the Securities and Exchange Commission. The offshore buyers may resell these shares in the United States, still without having to register the shares, after holding them for just 40 days. Typically, these shares are sold for 20% to 30% below the publicly traded share price. Some of these transactions have been reported with discounts of more than 30%, resulting from the lack of marketability. These discounts are similar to the marketability discounts inferred from the restricted and pre-IPO studies, despite the holding period being just 40 days.\nStudies based on the prices paid for options have also confirmed similar discounts. If one holds restricted stock and purchases an option to sell that stock at the market price (a put), the holder has, in effect, purchased marketability for the shares. The price of the put is equal to the marketability discount. The range of marketability discounts derived by this study was 32% to 49%. However, ascribing the entire value of a put option to marketability is misleading, because the primary source of put value comes from the downside price protection. A correct economic analysis would use deeply in-the-money puts or Single-stock futures, demonstrating that marketability of restricted stock is of low value because it is easy to hedge using unrestricted stock or futures trades.\n\nAnother approach to measure the marketability discount is to compare the prices of stock offered in initial public offerings (IPOs) to transactions in the same company's stocks prior to the IPO. Companies that are going public are required to disclose all transactions in their stocks for a period of three years prior to the IPO. The pre-IPO studies are the leading alternative to the restricted stock stocks in quantifying the marketability discount.\n\nThe pre-IPO studies are sometimes criticized because the sample size is relatively small, the pre-IPO transactions may not be arm's length, and the financial structure and product lines of the studied companies may have changed during the three year pre-IPO window.\n\nThe studies confirm what the marketplace knows intuitively: Investors covet liquidity and loathe obstacles that impair liquidity. Prudent investors buy illiquid investments only when there is a sufficient discount in the price to increase the rate of return to a level which brings risk-reward back into balance.\nThe referenced studies establish a reasonable range of valuation discounts from the mid-30%s to the low 50%s. The more recent studies appeared to yield a more conservative range of discounts than older studies, which may have suffered from smaller sample sizes. Another method of quantifying the lack of marketability discount is the Quantifying Marketability Discounts Model (QMDM).\n\nThe evidence on the market value of specific businesses varies widely, largely depending on reported market transactions in the equity of the firm. A fraction of businesses are \"publicly traded,\" meaning that their equity can be purchased and sold by investors in stock markets available to the general public. Publicly traded companies on major stock markets have an easily calculated \"market capitalization\" that is a direct estimate of the market value of the firm's equity. Some publicly traded firms have relatively few recorded trades (including many firms traded \"over the counter\" or in \"pink sheets\"). A far larger number of firms are privately held. Normally, equity interests in these firms (which include corporations, partnerships, limited-liability companies, and some other organizational forms) are traded privately, and often irregularly. As a result, previous transactions provide limited evidence as to the current value of a private company primarily because business value changes over time, and the share price is associated with considerable uncertainty due to limited market exposure and high transaction costs.\n\nA number of stock market indicators in the United States and other countries provide an indication of the market value of publicly traded firms. The Survey of Consumer Finance in the US also includes an estimate of household ownership of stocks, including indirect ownership through mutual funds. The 2004 and 2007 SCF indicate a growing trend in stock ownership, with 51% of households indicating a direct or indirect ownership of stocks, with the majority of those respondents indicating indirect ownership through mutual funds. Few indications are available on the value of privately held firms. Anderson (2009) recently estimated the market value of U.S. privately held and publicly traded firms, using Internal Revenue Service and SCF data. He estimates that privately held firms produced more income for investors, and had more value than publicly held firms, in 2004.\n\n"}
{"id": "46685455", "url": "https://en.wikipedia.org/wiki?curid=46685455", "title": "Century Golden Resources Group", "text": "Century Golden Resources Group\n\nCentury Golden Resources Group is a Chinese privately held real estate development company.\n\nIt is headed by the Chinese billionaire Huang Rulun, and has over 20,000 employees. The company has invested in 20 five-star hotels and 10 shopping malls, with revenues in 2013 of nearly $5 billion.\n"}
{"id": "3919967", "url": "https://en.wikipedia.org/wiki?curid=3919967", "title": "Change request", "text": "Change request\n\nA change request is a document containing a call for an adjustment of a system; it is of great importance in the change management process.\n\nA change request is declarative, i.e. it states what needs to be accomplished, but leaves out how the change should be carried out. Important elements of a change request are an ID, the customer (ID), the deadline (if applicable), an indication whether the change is required or optional, the change type (often chosen from a domain-specific ontology) and a change abstract, which is a piece of narrative (Keller, 2005). An example of a change request can be found in Figure 1 on the right.\n\nChange requests typically originate from one of five sources: \nAdditionally, in Project Management, change requests may also originate from an unclear understanding of the goals and the objectives of the project.\n\nChange requests have many different names, which essentially describe the same concept:\n\n\n"}
{"id": "10853282", "url": "https://en.wikipedia.org/wiki?curid=10853282", "title": "Chief gaming officer", "text": "Chief gaming officer\n\nA chief gaming officer (abbreviated as CGO) is an executive position whose holder is focused on research and technical issues within a computer game company.\n\nThe first two companies that have employed this executive position are Bigpoint and Golden Worlds Entertainment Media Group. \n\nThe chief gaming officer or chief game officer is in charge of heading both the game development and the online/offline publishing functions of the company.\n\nThe CGO has authority to manage the online game production cycle from start to finish. As head of game development, he or she is solely responsible for the conceptualization and planning of new games, the budget allocations to different gaming projects, the budget allocation within each game, assembling a team of software developers and game designers, and ultimately, the approval of new games.\n\nAs head of online publishing, the CGO also manages the online publishing, web portal, localization, and user interaction for all of the company products. The CGO negotiates and executes the appropriate agreements for web hosting and networking services for game delivery to the community, resolving issues with local internet service providers, and making sure that appropriate online payment agreements and systems are in place in each country. In addition to the online publishing of individual games, he acts as the portal manager, coordinating the development and publishing teams for the company web portal, which is of vital importance for a company delivering its products and collecting its user fees exclusively through the Internet.\n\nThe CGO has the ultimate decision-making authority over the localization function, being the final person responsible for adapting each game to the geographical market where it is delivered. He interfaces directly with the community manager and marketing manager to ensure that the final product is delivered and marketed in the most effective way, consistently integrating customer feedback into new releases.\n\nThe CGO reports directly to the chief operating officer.\n\n"}
{"id": "28632367", "url": "https://en.wikipedia.org/wiki?curid=28632367", "title": "Chief innovation officer", "text": "Chief innovation officer\n\nA chief innovation officer (CINO) or chief technology innovation officer (CTIO) is a person in a company who is primarily responsible for managing the process of innovation and change management in an organization, as well as being in some cases the person who \"originates new ideas but also recognizes innovative ideas generated by other people\". The CINO also manages Technological change.\n\nThe term \"chief innovation officer\" was first coined and described in the 1998 book \"Fourth Generation R&D\". Organizations with a CINO/CTIO are practicing part of the fourth generation of innovation theory and practice to emerge since 1900.\nSuccessful chief innovation officers focus on delivering on the key principles behind innovation - leadership, creating networks, harnessing VOC/HOC in idea development, leveraging the right incentives, and building/running an effective, transparent, and efficient innovation process.\n\nThe CINO is responsible for managing the innovation process inside the organization that identifies strategies, business opportunities and new technologies and then develops new capabilities and architectures with partners, new business models and new industry structures to serve those opportunities.\n\nCINO/CTIO doesn't have to report to the CEO or another C-level executive. CINO/CTIO is a functional title, similar to the chief information security officer. The words \"chief\" and \"officer\" are used to communicate that a person in this position is responsible for driving innovation throughout the entire organization. Using a functional \"chief ... officer\" title helps to communicate that this is a cross-organizational position and enables this person to work across organizational silos.\n\nThe CINO/CTIO focuses on radical or breakthrough innovation. The coined term CINO/CTIO is used to differentiate the position from the chief information officer, who is responsible for the information technology and computer systems that support enterprise goals.\n\nA chief technology innovation officer (CTIO) is focused on the organizational innovation through technology. This is an important strategic position especially in an organization that has significant technology component in addition to the traditional information technology. This position is typically held by a person with a broad technical expertise in that organization's industry.\n\nThe title chief technology innovation officer is commonly used in the organizations that have a technology component as a part of its core business. The CTIO is responsible for maintaining organizational technological strategy, defining the requirements for new technology implementations and communicating them to key business stakeholders.\n\nThe CTIO is responsible for organizational leadership on technology issues, management of the technology research and review function, ensuring the alignment of technology vision with business strategy, and for driving technology innovation throughout the entire organization. The position can also be responsible for tracking new technology developments in areas of interest to the organization to ensure that it maintains a technological edge within the industry, analyzing and improving upon technology standards and maintaining organizational awareness of new technologies.\n\n"}
{"id": "14577096", "url": "https://en.wikipedia.org/wiki?curid=14577096", "title": "Contingent payment sales", "text": "Contingent payment sales\n\nIn business dealings, transactions often occur that include variables based on future events that can be difficult to ascertain (for example, a company may sell in an amount stock along with a percentage of that company’s net profits.) As these transactions are contingent on payments that occur in the future, and the total selling price cannot be determined as of the end of the taxable year of the sale, they are known as Contingent Payment Sales. Because of the uncertainty surrounding the final amounts of these transactions, they are difficult to evaluate for the purpose of tax liability.\n\nIf the maximum amount of sales can be determined in the year in which they occur by assuming that all contingencies are met, the price can be calculated in a manner similar to the installment sales method. If the amount is reduced in subsequent years, than the formula is recomputed accordingly.\n\nIf the maximum selling price cannot be determined, but the period over which payments may be received is fixed, then the seller’s basis is recovered ratably over the period during which payment may be received under the contract.\n\nSeeking to build growth momentum, General Mills studied areas of potential growth and value creation in the spring of 1998. This had generated some smaller acquisitions and a general receptivity to acquisition proposals by the firm. In early 2000, the firm’s financial advisers suggested that Diageo might be interested in selling Pillsbury, in an effort to focus Diageo on its beverage business, and that Pillsbury would complement General Mills’ existing businesses.\n\nIn March 2000, Diageo’s chief operating officer contacted General Mills’ chairman and CEO to explore a possible sale of Pillsbury. General Mills submitted its proposed deal terms to Diageo in June 2000—the total proposed payment was $10.0 billion. Diageo submitted an asking price of $10.5 billion. The two sides would budge no further, and it looked as if the negotiations would\nfounder. General Mills did not want to issue more than one-third of its post-transaction shares to Diageo, and believed that its shares were undervalued in the stock market. Diageo believed it was necessary to value General Mills’ shares at the current trading prices. In an effort to bridge the difference in positions, the two firms agreed upon including in the terms of the deal a contingent payment on the first anniversary of the transaction that would depend on General Mills’ share price. James Lawrence, chief financial officer of General Mills, said, “We genuinely believe this is a way in which they could have their cake and we could eat it, too. There’s no question in my mind that, absent this instrument, we wouldn’t have been able to reach this deal.”\n\nDavid Van Benschoten, General Mills’ treasurer, added that the contingent payment was another example of the “development of the use of [options] in the past 20 years as finance has come to first understand, and work with, the constructs of optionality.”\n"}
{"id": "40745971", "url": "https://en.wikipedia.org/wiki?curid=40745971", "title": "Corporate DNA", "text": "Corporate DNA\n\nCorporate DNA is business jargon for organizational culture. It is a metaphor based on the biological term \"DNA\", the molecule that encodes the genetic instructions in living organisms.\n\nIn a 1997 book, Gareth Morgan defined the corporate DNA metaphor as the \"visions, values, and sense of purpose that bind an organization together\" to enable individuals to \"understand and absorb the mission and challenge of the whole enterprise\". Lindgreen and Swaen define it as an \"organization's culture and strategy\". Ken Baskin defines it as \"flexible, universally available database of company procedures and structures\" which develops from the company's history, and that the organization's employees behave to satisfy the resultant corporate identity. Baskin also likens the availability of information throughout an organization to the presence of DNA in all of an organism's cells. Arnold Kransdorff defines corporate DNA as the set of institution-specific experiences that \"characterizes any organization's ability to perform\".\n\nIn a \"Strategy+Business\" article, Gary Neilson, Bruce A. Pasternack, and Decio Mendes state that the four DNA bases for an organization are its structure, decision rights, motivating factors, and information. In the book \"DNA Profiling : The Innovative Company: How to Increase Creative Ability in Business\", Isabelle Denervaud and Olivier Chatin state that the four organizational bases are actors, ideation, emotion, and collaboration. Denervaud and Chatin also extend the metaphor by identifying factors that may mutate organizational DNA (discontinuity, traditional playing fields, new lands, and individuals), much like a genetic mutation may change the nucleotide sequence of the genome of an organism, and Neilson, Pasternack, and Mendes describe a company adapting to structural and environmental changes analogous to biological adaptation. The term DNA is also used to describe an organization's ability to innovate.\n\nIn \"Corporate Culture: The Ultimate Strategic Asset\", Eric Flamholtz and Yvonne Randle state that organization culture is \"transmitted to generations of employees\" via that organization's DNA, and that the DNA of the culture of the company is established \"during its initial stages\" reflecting the \"personal and professional values\" of the founders. They also state that it can be \"transformed through the entrance of new people with new ideas\", and that significant differences in organizational performance can be derived from small changes in organizational DNA. According to Virginia Healy-Tangney, such changes must come throughout the organization and are time-consuming. These changes may result in increased productivity and profitability, and in a reduction of employee turnover.\n\nPreservation of organizational DNA is important to ensure business continuity and persistence. Organizations have implemented various techniques to prevent organizational DNA from being controlled by external influences, such as Mars, Incorporated remaining privately owned to limit control of share capital. Another method is by branding corporate work environments to \"clearly reflect the culture\" of the organization.\nDNA is used to describe the skills, properties, or qualities of an individual that describe that individual's character.\nThe term is also used to describe the set of architectural and spatial characteristics considered by the inhabitants of a city to be constituents of that city's identity. This may include \"materials and colours, a typical arrangement of scale and architectural forms, building lot size, roof lines, scale of public and semi-public spaces\" which should be respected by new buildings and urban spaces in the city. In an article in \"Fast Company\", Kelli Richards claims that the organizational culture at Apple Inc. in the 1990s has become part of the DNA of Silicon Valley.\n\n"}
{"id": "6074982", "url": "https://en.wikipedia.org/wiki?curid=6074982", "title": "Corporate sourcing", "text": "Corporate sourcing\n\nCorporate sourcing refers to a system where divisions of companies coordinate the procurement and distribution of materials, parts, equipment, and supplies for the organization. This is a supply chain, purchasing/procurement, and inventory function. This enables bulk discounting, auditing, and Sarbanes-Oxley compliance.\n\nDuties of a corporate sourcing agent include:\n\nSome of corporate sourcing agents:\nRichman Chemical, Onetouch, Worldwide Brands, SAOS\n\n\n"}
{"id": "17014742", "url": "https://en.wikipedia.org/wiki?curid=17014742", "title": "Desktop outsourcing", "text": "Desktop outsourcing\n\nDesktop outsourcing is the process in which an organization contracts a third party to maintain and manage parts of its IT infrastructure. Contracts vary in depth and can range from Computer hard- and software maintenance to Desktop virtualisation, SaaS-implementations and Helpdesk operation. It is estimated, that 32% of U.S. and Canadian IT organisations make use of desktop outsourcing in 2014. Recent market reports suggest the adoption of BYOD policies to allow the end-user a free choice of devices in their working environment may increase this market share.\n\nJustification for desktop outsourcing could include shifting focus and energy to areas of core competency, reducing staffing costs, and the routine maintenance, upgrades, and repairs associated with managing multitudes of PC systems and servers. (Applegate et al. 2007). Managers may also seek desktop outsourcing as a method of simplifying organisational structures to cut costs associated with them. For smaller companies it might also be more viable financially to outsource their desktop at a set price per machine, rather than creating an entire internal IT department. Recent market growth can also be attributed to the decreasing price of hardware, making replacement more favourable than repairs. Possible risks when desktop outsourcing are ensuring continued support for old a unique systems the company depends on, specifically if any of the systems in question were internally developed. This may cause the contractor to be unable to fulfil their contractual obligations, as in the case of the US Navy outsourcing their IT systems to EDS in 2003.\n\nA 2012 TechNavio market report forecasts that the desktop outsourcing market will grow by 4.65% yearly, between 2012 and 2016. Atos, CSC, HP and IBM are considered the leading desktop outsourcing vendors for that time frame. A Gartner report from 2013, however, considers the desktop outsourcing market to be in decline, with growth only occurring on the Latin American and Asia/Pacific markets.\n\n\n\n"}
{"id": "10141539", "url": "https://en.wikipedia.org/wiki?curid=10141539", "title": "Diamond model", "text": "Diamond model\n\nStrategic analysis typically focuses on two views of organization: the industry-view and the resource-based view (RBV). These views analyse the organisation without taking into consideration relationship between the organizations strategic choice (i.e. Porter generic strategies) and institutional frameworks.\" The National Diamond\"' is a tool for analyzing the organization's task environment. The National Diamond highlights that strategic choices should not only be a function of industry structure and a firm's resources, it should also be a function of the constraints of the institutional framework. Institutional analysis (such as the National Diamond) becomes increasingly important as firms enter new operating environments and operate within new institutional frameworks.\n\nMichael Porter's National Diamond framework resulted from a study of patterns of comparative advantage among industrialized nations. It works to integrate much of Porter's previous work in his competitive five forces theory, his value chain framework as well as his theory of competitive advantage into a consolidated framework that looks at the sources of competitive advantage sourcable from the national context. It can be used both to analyze a firm's ability to function in a national market, as well as analyse a national market's ability to compete in an international market.\n\nIt recognizes four pillars of research (factor conditions, demand conditions, related and supporting industries, firm structure, strategy and rivalry) that one must undertake in analysing the viability of a nation competing in a particular international market, but it also can be used as a comparative analysis tool in recognising which country a particular firm is suited to expanding into.\n\nTwo of the aforementioned pillars focus on the (national) macroeconomics environment to determine if the demand is present along with the factors needed for production (i.e. both extreme ends of the value chain). Another pillar focuses on the specific relationships supporting industries have with the particular firm/nation/industry being studied. The last pillar it looks at the firm's strategic response (microeconomics) i.e. its strategy, taking into account the industry structure and rivalry (see five forces). In this way it tries to highlight areas of competitive advantage as well as competitive weakness, by looking at a companies/nations suitability to the particular conditions of a particular market.\n\nThe six different components of the framework are:\nFactor endowment can be categorized into two forms:\n\nFor example, in analyzing Hollywood's preeminence in film production, Porter has pointed out the local concentration of skilled labor, including the different schools of film (UCLA & USC) in the area. Also, resource constraints may encourage development of substitute capabilities; Japan's relative lack of raw materials has spurred miniaturization and zero-defect manufacturing.\n\nFor many firms, the presence of related and supporting industries is of critical importance to the growth of that particular industry. A critical concept here is that national competitive strengths tend to be associated with \"clusters\" of industries. For example, Silicon Valley in the USA and Silicon Glen in the UK are techno clusters of high-technology industries which includes individual computer software and semi-conductor firms. In Germany, a similar cluster exists around chemicals, synthetic dyes, textiles and textile machinery.\n\nDemand conditions in the domestic market provide the primary driver of growth, innovation and quality improvement. The premise is that a strong domestic market stimulates the firm from being a startup to a slightly expanded and bigger organization. As an illustration, we can take the case of Germany which has some of the world's premier automobile companies like Mercedes, BMW, Porsche. German auto companies have dominated the world when it comes to the high-performance segment of the world automobile industry. However, their position in the market of cheaper, mass-produced autos is much weaker. This can be linked to a domestic market which has traditionally demanded a high level of engineering performance. Also, the transport infrastructure of Germany, with its Autobahns does tend to favor high-performance automobiles.\n\nNational performance in particular sectors is inevitably related to the strategies and the structure of the firms in that sector. Competition plays a big role in driving innovation and the subsequent upgradation of competitive advantage. Since domestic competition is more direct and impacts earlier than steps taken by foreign competitors, the stimulus provided by them is higher in terms of innovation and efficiency. As an example, the Japanese automobile industry with 8 major competitors (Honda, Toyota, Suzuki, Isuzu, Nissan, Mazda, Mitsubishi, and Subaru) provide intense competition in the domestic market, as well as the foreign markets in which they compete.\n\nThe role of government in Porter's Diamond Model is \"acting as a catalyst and challenger; it is to encourage - or even push - companies to raise their aspirations and move to higher levels of competitive performance …\" . They must encourage companies to raise their performance, stimulate early demand for advanced products, focus on specialized factor creation and to stimulate local rivalry by limiting direct cooperation and enforcing anti-trust regulations.\n\nThe role of chance basically denotes the idea that it may occur that many times a product or an enterprise may get an opportunity to maximize its benefits out of sheer luck. Thus chance if received plays a key role in determining the fate of the product as well.\n\nCriticism on Porter's national diamond model revolve around a number of assumptions that underlie it. As described by Davies and Ellis: \nPorter generalized from the North American, European and Japanese experiences; for countries developing in the presence of these now developed regions of the world, the model may need to be re-examined.\n\n\n"}
{"id": "54135637", "url": "https://en.wikipedia.org/wiki?curid=54135637", "title": "EZOfficeinventory", "text": "EZOfficeinventory\n\nEZOfficeinventory is a cloud-based asset tracking software used in tracking physical and technological assets.\n\nLaunched via TechCrunch in 2011, EZOfficeInventory was founded as a Software as a service (SaaS) asset management solution by University of Illinois alumni; Syed Ali.\n\nIn 2016, the University of Pennsylvania adopted the equipment tracking software in organizing the dissemination of probes, sensors and patient simulators within its graduate nursing school.\n\nThe asset tracking software enables companies to track and manage assets. The inventory management system within the software helps track stock levels of inventories in organizations.\n\n"}
{"id": "53681449", "url": "https://en.wikipedia.org/wiki?curid=53681449", "title": "Ecoprenuership", "text": "Ecoprenuership\n\nEcoprenuership, also known as Green Entrepreneurship, not to be mistaken for “Sustainable Business” or “Green Business” is most simply referred to as the development of enterprise through entrepreneurship while incorporating an environmentally responsible perspective into the operations and goals of the entrepreneur. Ecopreneurship is also a term coined to represent the process of principles of entrepreneurship being applied to create businesses that solve environmental problems or operate sustainably.\n\nLiterature began being published on Ecoprenuership in the early 1990s with the introduction of \"Merging Economic and Environmental Concerns Through Ecoprenuership\" by Gwynn Schuyler (Schuyler. 1998). Schuyler offers a simple yet unprecedented definition of Ecoprenuership.\" Since Schuyler's definition in 1998 the definition of Ecopreuership has developed and concepts defining the term have changed. David Kainrath of the Umea School of Business has defined Ecoprenuership as the convergence of three main concepts, including:\nFurthermore, Kainrath states that an Ecoprenuerial Company is \"a company in which the three Ecoprenuership Concepts manifest.\"\n\nSystems Thinking is a core principle to any business concerned with sustainability and the environment. It is an approach to problem solving that studies how something interacts with its environment as a whole, whether that be social, economic or natural. This is in contrast to a linear thinking model, which would isolate a problem and study only its directly related processes to find solutions. It consists of the notion that in order to understand vertical problems (looking deeply at one particular issue), you must understand and evaluate the horizontal environment as a whole (the entire system and its interrelated functions). As it pertains to business is best illustrated in the book \"Entrepreneurship and Sustainability\" by Andrea Larsen,\"Systems thinking applied to new ventures reminds us that companies operate in complex sets of interlocking living and non-living, including markets and supply chains as well as non-living systems... Taking a systems perspective reminds us that we are accustomed to thinking of business in terms of discrete units with clear boundaries between them. We forget that these boundaries exist primarily in our minds or as legal constructs.\"\n\nProduct design incorporating sustainability can happen at any stage of the business, including material extraction, logistics, the manufacturing process, disposal, etc. Sustainable product design can be achieved using innovative technology (or Eco-innovation), cradle to cradle design, bio-mimicry, etc. In a description by the government of Canada's department on Innovation, Science and Economic Development, sustainable product design is further explained:\n\n\"Product design offers the opportunity to incorporate green and socially responsible attributes into a product. Referred to as Design for Sustainability (D4S), it is a process that addresses environmental and social considerations in the earliest stages of the product development process to minimize negative environmental and social impacts throughout the product's life cycle and to comply with the principles of economic, social and ecological sustainability.\n\nSustainable product design can encompass the selection of materials, use of resources, production requirements and planning for the final disposition (recycling, reuse, remanufacturing, or disposal) of a product. It takes into account the socio-economic circumstances of the company and the opportunity for the firm to address social problems associated with poverty, safety, inequity, health and the working environment. It is not a stand-alone methodology but one that must be integrated with a company's existing product design so that environmental and social parameters can be integrated with traditional product attributes such as quality, cost, and functionality.\"\n\nCompanies practicing ecopreneurship aim to solve environmental issues by developing new technology or innovating existing technologies. Within the past five years modern society has witnessed development of precedent setting technologies such as long range electric vehicles as well as the improvement though innovation of existing technologies such as solar panels. Examples of innovation backed by an environmentally responsible attitude has inspired a new class of entrepreneurs who identify as Ecoprenuers.\n\nThe development of innovative technologies has developed an opportunity for Ecoprenuers to apply technology to sustainable or green businesses. Cosmin Nacu and Silvia Avasilcai state that \"this trend has created a full range of opportunities for entrepreneurs: from creating green-technology, using technology to promote environmental sustainability (e.g. energy management) to simply make the existing business more environmentally friendly to take advantage of the benefits\" (Source). The development of new and innovative technologies are complimentary to the development of green enterprises and enable Ecoprenuers to continue developing enterprise that are environmentally sustainable.\n\nCradle-to-cradle design is a popular environmental approach to product design that seeks to eliminate waste by designing products that can be continuously recirculated through our economy. This is contrasted against a \"cradle to grave\" design which typically includes single use products or products made of multiple different materials that cannot be separated in order to be recycled properly.[5] Cradle to Cradle design is mimicked after processes in the natural environment which do not create waste but instead every output is an input for another organism. Cradle to Cradle design is often achieved through using environmentally friendly resources (non-toxic) that can either be recycled into other products or composted. Another important component of cradle to cradle design is the ability to easily take products apart for better reuse as well as designing with durability in mind. This idea was popularized by the 2002 book \"\" written by William McDonough and Michael Braungart.\n\nThe Sustainable Business Industry is one of the biggest opportunities in commerce in the past 20 years according to Thierry Volery, author of Ecopreneurship: Rationale, current issues and futures challenges Although, there continues to be a trend of challenges that Ecoprenuers face when developing their enterprise. \"Ecology and economy have often been seen as conflicting, because the prevailing view is that there is an inherent and fixed trade-off between the two\" and Ecoprenuers are constantly subjected to higher prices of operation that are inherit to operating in an environmentally sustainable way Rationale, current issues and futures challenges|url. The fact that Ecoprenuers aim to sustain \"public goods\" such as air, oceans, topsoil and wild species develops a challenge of measuring the use and value of these goods. Volery, author of Ecopreneurship: Rationale, current issues and futures challenges states \"it will never be profitable to produce public goods privately, because the producer who incurs the cost of production cannot prevent the consumer from using the good freely\" because public goods are non-excludable in consumption meaning that citizens cannot stop consuming certain public goods such as water or air.\n\nOpportunities to develop sustainable business through Ecoprenuership represent a huge opportunity to take advantage of business practices of the future. Companies leading the way in sustainable business practices are taking advantage of sustainable revenue opportunities: according to the Department for Business, Innovation and Skills the UK green economy to grow by 4.9 to 5.5 percent a year by 2015.[44] Since environmental concerns have become pressing many profitable opportunities have arisen. Government backed grants often lay the foundation for Ecoprenuers to take advantage of a quickly growing industry while being able to kickstart their endeavors with government money.\n\nTriple bottom line Accounting is an accounting method that combines traditional accounting methods of measuring profit with those that measure social and environmental benefits as well. The phrase was created by John Elkington in 1994 at his company SustainAbility. Some criticisms have sprung up over what methods are to be used to measure environmental and social impacts.\n\nEcopreneurs may decide to develop their company under traditional business legal forms like a sole proprietorship or an LLC or they might chose some newer forms discussed below. These business forms are popular among the environmentally conscious community for their emphasis on social benefit.\n\n"}
{"id": "904795", "url": "https://en.wikipedia.org/wiki?curid=904795", "title": "Electronic News Production System", "text": "Electronic News Production System\n\nElectronic News Production System (ENPS) is a software application developed by the Associated Press's Broadcast Technology division for producing, editing, timing, organizing and running news broadcasts. The system is scalable and flexible enough to handle anything from the local news at a small-market station to large organizations spanning remote bureaus in multiple countries.\n\nThe basic organization of each news broadcast is called a \"rundown\" (US) or \"running order\" (UK). The run-down is a grid listing scripts, video, audio, character generator data, teleprompter control, director notations, camera operator cues, and timing estimates for each section of the broadcast.\n\nENPS integrates scripts, wire feeds, device control, and production information in a server/client environment. On the server side, ENPS runs an identical backup server (called a \"buddy\") at all times as a fail-safe. If the primary server fails, all users are redirected to the buddy server until such time as the primary comes back on-line. All document changes are queued on the buddy and copied back to the primary automatically when it returns to production. Note that this is not a mirror server as the changed data is copied to the buddy, but there is no direct replication inherent within the intercommunications between the servers, so if the data is corrupted due to hardware failure on one server, this corruption will not be replicated to the \"buddy\".\n\nDevice control can be managed either through a serial interface, or the MOS (Media Object Server) protocol. MOS functionality is included in the base ENPS license, but may be an extra add-on for the device that needs to interface with ENPS. MOS items such as video or audio clips can be added directly to scripts, and then used by third party software and devices during the broadcast. Many broadcast media systems support the MOS protocol to a greater or lesser degree by implementing any of the seven MOS Protocol 'profiles'. An example of a MOS System which fully supports all seven MOS Profiles is SIENNA , used by ENPS customers including the BBC.\n\nThe ENPS client utilizes the .NET framework and other Windows-only technologies. The client, therefore, will only run in the Microsoft Windows operating system.\n\nENPS was originally developed by the Associated Press for use at the BBC in the United Kingdom as a replacement for the text mode system BASYS (which developed into Avid iNEWS), and the Corporation has the largest installation of the system with over 12,000 users in 300 different locations .\n\nOn December 15, 2011 Synaptic Digital announced a partnership with ENPS to deliver their content on thenewsmarket.com directly to subscribers of their feed.\n\n\n"}
{"id": "37574401", "url": "https://en.wikipedia.org/wiki?curid=37574401", "title": "Employee of the month (program)", "text": "Employee of the month (program)\n\nThe Employee of the Month (EOM) is a type of reward program given out by companies (often to encourage the staff to work harder and more productively). It involves recognising employees for achieving excellence in their field, and being the best worker across all fields. Traditionally, the award is given out every month, but that is not necessarily the case. The system is popular in the United States.\n\nOne of the main factors of Employee of the Month systems that make them unique is the criteria used to work out who is the \"best\" worker for a given month. A document called Employee of the Month Selection Criteria suggests the two main criteria are \"quality of work standards\" and \"attitude standards\".\n\nEmployee of the month is usually based on performance, quality, attendance and many other determining factors from previous month(s) based on the associate's role. The \"best\" worker is then rewarded the following month with the title of \"Employee of the Month\". During this month, the associate is usually rewarded with a certificate, trophy, or other honors.\n"}
{"id": "49107824", "url": "https://en.wikipedia.org/wiki?curid=49107824", "title": "Epos Now", "text": "Epos Now\n\nEpos Now is a cloud-based software provider, specialising in the design and manufacture of electronic point of sale (commonly referred to as EPOS), which encompasses features including but not limited to reporting, stock control, and CRM for retail and hospitality businesses. Epos Now software can be operated from any device or platform with a web-browser or by using Epos Now's IOS or Android app.\n\nEpos Now's UK headquarters are located in Norwich, England and their US headquarters are in Orlando, Florida. Founded in June 2011, the company was a pioneer in cloud-technology in the EPOS industry and is notable for being the first EPOS company to introduce an AppStore to their customers, which allows users to customise their system. The Epos Now AppStore was introduced in December 2014. Epos Now's innovation was recognised in 2016, when the company won a Queen's Award for Enterprise, the UK's highest accolade for business success.\n\nEpos Now is the UK's 13th fastest growing private technology company, and the fastest growing EPOS company in the UK.\n\nEpos Now was founded by Jacyn Heavens in 2011 with no external investment. CEO and founder Jacyn Heavens identified a gap in the market for an affordable EPOS system whilst managing a bar, and after searching for an EPOS system that would be suitable for his business, he started building software that would suit business owners like himself. The Epos Now Appstore was officially launched in September 2015, making it the first Appstore to be released within the EPOS industry. There are over 20,000 businesses using Epos Now.\n\nIn 2015, Epos Now was awarded 'Epos Innovation of the Year' by Retail Systems, and was shortlisted for EDP Business Awards and Tech Cities 2015.\n\nIn 2016 Epos Now was shortlisted for the UK Cloud Awards in the \"Most Innovative SMB Product of the Year\" category and the Engagement & Loyalty Awards in the \"Most Innovative Technology\" category. Epos Now was also a finalist in 2016 The Grocer Gold Awards, for the \"Technology Supplier of the Year\" category.\n\nOn 21 April 2016 it was announced the Epos Now had won a Queen's Award for Enterprise in the Innovation category.\n\nEpos Now were named Gold Stevie Award winners for The International Business Awards as \"The Most Innovative company of the Year 2016\".\n\nEpos Now won three Eastern Daily Press Business Awards in 2016 in the categories of; Employer of the Year, Tech Innovator of the year and Business of the Year sponsored by Barclays. \n\nThe company made the Tech Track 100 three years running, being named the 13th and 32nd and 53rd fastest growing tech company in the UK and named 30th fastest growing company in 2016's Deloitte Fast 50 with a growth of 597%. Epos Now were named 179th fastest growing tech company in the Deloitte 2016 Fast 500 EMEA. Epos Now were named Europes 46th fastest growing company in the Financial Times 1000 listing.\n\nEpos Now were recognised for their commitment to the professional development of their employees winning a Princess Royal Training Award in 2017.\n\nEpos Now were noted as 'Best SaaS Product for small business/SMBs' in the 2017 SaaS Awards.\n"}
{"id": "41154446", "url": "https://en.wikipedia.org/wiki?curid=41154446", "title": "European Council for Distance Learning", "text": "European Council for Distance Learning\n\nThe European Council for Distance Learning or EUCDL is a private non-profit organization registered under the EU law to help online and Distance Universities/schools to improve their know-how using the latest e-learning technologies and methods to ensure high quality.\n\n\n\"EUCDL\" has 4 partners:\n\n"}
{"id": "43688015", "url": "https://en.wikipedia.org/wiki?curid=43688015", "title": "FSN analysis", "text": "FSN analysis\n\nIn FSN analysis, items are classified according to their rate of consumption. The items are classified broadly into three groups: F – means Fast moving, S – means Slow moving, N – means Non-moving. The FSN analysis is conducted generally on the following basis:\nFSN analysis helps a company in identification of the following\n"}
{"id": "5622782", "url": "https://en.wikipedia.org/wiki?curid=5622782", "title": "Grandfather principle", "text": "Grandfather principle\n\nThe grandfather principle is related to the control and order within hierarchical organisations.\n\nExamples of the principle's use in decision making:\n\n\nExamples of the principle's use related to responsibility and authorisation:\n\n\nExamples of the principle's use to limit organisational corruption:\n\n"}
{"id": "15715221", "url": "https://en.wikipedia.org/wiki?curid=15715221", "title": "Latigent", "text": "Latigent\n\nLatigent LLC was an international provider of Business Intelligence, Enterprise RSS, and Call Center Reporting and Analytics software.\n\nLatigent, LLC of Chicago, IL, was founded in 2002, as a privately held and funded company, by Chris Crosby and Jason Kolb, two veterans of the contact center industry. Both founders believed that there was a more efficient way of using the information typically gathered in the contact center to better serve the customer and more profitably serve the enterprise. As a result, the Latigent BlueVue performance management product was born and introduced to the market in 2003.\n\nIn December 2005, Latigent released BlueVue II, a Business Information (BI) suite. Besides reporting functionality and complete data warehousing capabilities, BlueVue II also offered data integration capabilities for performance management.\n\nOn September 27, 2007 Cisco Systems announced a definitive agreement to acquire Latigent LLC.\n\n"}
{"id": "45112545", "url": "https://en.wikipedia.org/wiki?curid=45112545", "title": "Looker (company)", "text": "Looker (company)\n\nLooker Data Sciences (doing business as Looker) is an American computer software company headquartered in Santa Cruz, California. Looker markets a data exploration and discovery business intelligence platform.\n\nThe company was founded in Santa Cruz, California in January, 2012 by Lloyd Tabb and Ben Porterfield. The product grew out of Tabb's experience building software at companies like Netscape, LiveOps, and Luminate before founding Looker.\n\nLooker makes use of a simple modeling language called LookML that lets data teams define the relationships in their database so business users can explore, save, and download data without needing to know the SQL query language.\nThe product was the first commercially available business intelligence platform built for and aimed at scalable or massively parallel relational database management systems like Amazon Redshift, Google BigQuery, HP Vertica, Netezza, and Teradata.\n\nOn August 13, 2013, Looker announced a Series A round of funding from Redpoint Ventures, First Round Capital, and PivotNorth Capital, raising more than $18M. Prior to the Series A round, Looker raised $2M in a seed round from First Round Capital and PivotNorth Capital. On March 11, 2015 Looker raised $30M in series B funding.\nIn July, 2015, Jen Grant joined as chief marketing officer, and the company estimated it has 140 employees. On January 14, 2016 Looker raised $48M in series C funding from Kleiner Perkins Claufield & Byers. At that time, the company estimated 450 customers, including Jet.com. On March 30th, 2017 Looker raised $81.5M in series D funding led by Capital G. On June 21st, 2016 Looker celebrated the release of \"Winning With Data,\" a book coauthored by Looker CEO, Frank Bien and Redpoint Partner, Tomasz Tunguz. \n\n"}
{"id": "43842723", "url": "https://en.wikipedia.org/wiki?curid=43842723", "title": "Mixpanel", "text": "Mixpanel\n\nMixpanel is a business analytics service company. It tracks user interactions with web and mobile applications and provides tools for targeted communication with them.\n\nIts tool-set contains in-app A/B tests and user survey forms. Data collected is used to build custom reports and measure user engagement and retention. Mixpanel works with web applications, in particular SaaS, but also supports mobile apps.\n\nMixpanel was founded by Suhail Doshi and Tim Trefren in 2009 and is based in San Francisco, California. It is backed by Y Combinator, and its list of investors includes Andreessen Horowitz, Max Levchin and Keith Rabois.\n\nIn April 2018, founder and CEO Suhail Doshi announced he would step down and become chairman of the board. He was replaced as CEO by Amir Movafaghi.\n\n"}
{"id": "41352219", "url": "https://en.wikipedia.org/wiki?curid=41352219", "title": "Mobile virtual network enabler", "text": "Mobile virtual network enabler\n\nA mobile virtual network enabler (MVNE) is a company that provides network infrastructure and related services, such as business support systems, administration and operations support systems to a Mobile Virtual Network Operator (MVNO). This enables MVNOs to offer services to their own customers with their own brands. The MVNE does not have a relationship with consumers, but rather is a provider of network enablement platforms and services. \n\nMVNEs specialise in planning, implementation and management of mobile services. Typically this includes SIM provisioning and configuration, customer billing, customer relationship management and value-added service platforms. In effect, they enable an MVNO to outsource both the initial integration with the MNO and the ongoing business and technical operations management. A related type of company is mobile virtual network aggregator, or MVNA. MVNE is a telecom solution, whereas MVNA is a business model which includes wholesale of an operator's airtime and routing of traffic over the MVNE's own switches. \n\nThe benefits of using an MVNE include a reduction in the upfront capital expenses of an MVNO, financing arrangements offered by MVNEs to cover start-up costs and reduced wholesale airtime costs achieved through economies of scale of hosting multiple MVNOs on a single MVNE platform. The other benefits could be reduced operational expenses by outsourcing management of business and technical operations, smoother launch processes and benefiting from previous experience of the MVNE as a negotiating channel for smaller MVNOs to reach a wholesale agreement with the MNO.\n\nHowever, not all MVNEs are the same. There is significant variation in the level of experience, technical capability, integration and operational support. Furthermore, using an MVNE may not be appropriate for all MVNOs. The considerations for this decision are manifold, however, some of the key reasons for an MVNO against using an MVNE are:\n\nAdoption of the MVNE model will have a significant impact on the margins of an MVNO business, acting either to improve or shrink these margins relative to direct or self-build models. As such, the decision to use an MVNE should not be taken lightly as the impacts could range from customer experience to business efficiency.\n\n\n"}
{"id": "4638828", "url": "https://en.wikipedia.org/wiki?curid=4638828", "title": "Operating lease", "text": "Operating lease\n\nThe expression \" operating lease\" is somewhat confusing as it has a different meaning based on the context that is under consideration.\nFrom a product characteristic stand point, this type of a lease, as distinguished from a finance lease, is one where the lessor takes residual risk. As such, the lease is non full payout. From an accounting stand point, this type of lease (if it fails to meet varied criteria that define a finance lease) results in off balance sheet financing.\n\nThe determination of whether a lease is a finance (also called capital) lease or an operating lease from an accounting point of view is defined in the United States by Statement of Financial Accounting Standards No. 13 (FAS 13). In countries covered by International Financial Reporting Standards, the tests are defined in IAS 17. In July 2006, the Financial Accounting Standards Board (FASB) and the International Accounting Standards Board (IASB) announced the commencement of a joint project to comprehensively reconsider lease accounting. In July 2008, the boards decided to defer any changes to lessor accounting, while continuing with the project for lessee accounting, with the stated intention to recognise an asset and liability for all lessee leases (in essence, eliminating operating lease accounting). This culminated in the issuance of IFRS 16 and FASB Topic 842. Both are effective January 1,2019.The similarity in the two pronouncements is that leases, which previously qualified as operating leases- and hence resulted in off balance sheet treatment, are now to be capitalized by the lessee.\n\nUnlike a finance lease, at the end of the operating lease the title to the asset does not pass to the lessee, but remains with the lessor. Accordingly, at the end of an operating lease, the lessee has several options:\n\n\nOperating leases, where the lessor takes a residual position, offer a host of benefits to the lessee the type of which finance leases do not\n\n"}
{"id": "14190268", "url": "https://en.wikipedia.org/wiki?curid=14190268", "title": "Operational system", "text": "Operational system\n\nAn operational system is a term used in data warehousing to refer to a system that is used to process the day-to-day transactions of an organization. These systems are designed in a manner that processing of day-to-day transactions is performed efficiently and the integrity of the transactional data is preserved.\nSometimes operational systems are referred to as operational databases, transaction processing systems, or online transaction processing systems (OLTP). However, the use of the last two terms as synonyms may be confusing, because operational systems can be batch processing systems as well.\n\nAny enterprise must necessarily maintain a lot of data about its operation. \n\n"}
{"id": "6406997", "url": "https://en.wikipedia.org/wiki?curid=6406997", "title": "Ordinary course of business", "text": "Ordinary course of business\n\nIn law, the ordinary course of business covers the usual transactions, customs and practices of a certain business and of a certain firm. This term is used particularly to judge the validity of certain transactions. It is used in several different sections of the Uniform Commercial Code of the United States.\n\nSection 1-201 of the Uniform Commercial Code defines a \"Buyer in the ordinary course of business\" by a four-part test:\n\n"}
{"id": "27701207", "url": "https://en.wikipedia.org/wiki?curid=27701207", "title": "Process ontology", "text": "Process ontology\n\nIn philosophy, a process ontology refers to a universal model of the structure of the world as an ordered wholeness. Such ontologies are fundamental ontologies, in contrast to the so-called applied ontologies. Fundamental ontologies do not claim to be accessible to any empirical proof in itself, but to be a structural design pattern, out of which empirical phenomena can be explained and put together consistently. Throughout Western history, the dominating fundamental ontology is the so-called substance theory. However, fundamental process ontologies are becoming more important in recent times, because the progress in the discovery of the foundations of physics spurred the development of a basic concept able to integrate such boundary notions as \"energy,\" \"object\", and those of the physical dimensions of space and time.\n\nIn computer science, a process ontology is a description of the components and their relationships that make up a process. A formal process ontology is an ontology in the knowledge domain of processes. Often such ontologies take advantage of the benefits of an upper ontology. Planning software can be used to perform plan generation based on the formal description of the process and its constraints. Numerous efforts have been made to define a process/planning ontology.\n\nA process may be defined as a set of transformations of input elements into output elements with specific properties, with the transformations characterized by parameters and constraints, such as in manufacturing or biology. A process may also be defined as the workflows and sequence of events inherent in processes such as manufacturing, engineering and business processes.\n\nThe Process Specification Language (PSL) is a process ontology developed for the formal description and modeling of basic manufacturing, engineering and business processes. This ontology provides a vocabulary of classes and relations for concepts at the ground level of event-instances, object-instances, and timepoints. PSL’s top level is built around the following:\n\nIn a process/planning ontology developed for the ontology Cyc, classes and relations above the ground level of PSL allow processes to be described purely at the type-level. The ground level of PSL uses the primitives of event-instance, object-instance, and timepoint description. The types above the ground level of PSL have also been expressed in PSL, showing that the type-level and the ground level are relatively independent. The type-levels for the Cyc process ontology above this ground level use the following concepts:\n\nThe project SUPER (Semantics Utilised for Process management within and between EnteRprises) has a goal of the definition of ontologies for Semantic Business Process Management (SBPM), but these ontologies can be reused in diverse environments. Part of this project is to define an Upper Process Ontology (UPO) that ties together all other SUPER ontologies. The results of the project SUPER include the UPO and a set of ontologies for processes and organizations. Most of the ontologies are written in WSML, and some are also written in OCML.\n\nA candidate model for the UPO was DDPO (DOLCE+DnS Plan Ontology), a planning ontology which specifies plans and distinguishes between abstract and executable plans. DOLCE (Descriptive Ontology for Linguistic and Cognitive Engineering) aims at capturing the ontological categories underlying natural language and human commonsense. DnS (Descriptions and Situations), is a constructivist ontology that allows for context-sensitive redescriptions of the types and relations postulated by other given ontologies (or ground vocabularies). Together in DDPO, DOLCE and DnS are used to build a Plan Ontology that includes physical and non-physical objects (social entities, mental objects and states, conceptualizations, information objects, constraints), events, states, regions, qualities, and constructivist situations. The main target of DDPO is tasks, namely the types of actions, their sequencing, and the controls performed on them.\n\nThe ontology oXPDL is a process interchange ontology based on the standardised XML Process Definition Language (XPDL). The purpose of oXPDL is to model the semantics of XPDL process models in standardized Web ontology languages such as OWL and WSML, while incorporating features of existing standard ontologies such as PSL, RosettaNet, and SUMO.\n\nThe General Formal Ontology (GFO) is an ontology integrating processes and objects. GFO includes elaborations of categories like objects, processes, time and space, properties, relations, roles, functions, facts, and situations. GFO allows for different axiomatizations of its categories, such as the existence of atomic time-intervals vs. dense time. Two of the specialties of GFO are its account of persistence and its time model. Regarding persistence, the distinction between endurants (objects) and perdurants (processes) is made explicit within GFO by the introduction of a special category, a persistant. A persistant is a special category with the intention that its instances \"remain identical\" over time. With respect to time, time intervals are taken as primitive in GFO, and time-points (called \"time boundaries\") are derived. Moreover, time-points may coincide, which is convenient for modelling instantaneous changes.\n\nThe multi metamodel process ontology (m3po) combines workflows and choreography descriptions so that it can be used as a process interchange ontology. For internal business processes, Workflow Management Systems are used for process modelling and allow describing and executing business processes. For external business processes, choreography descriptions are used to describe how business partners can cooperate. A choreography can be considered to be a view of an internal business process with the internal logic not visible, similar to public views on private workflows. The m3po ontology unifies both internal and external business processes, combining reference models and languages from the workflow and choreography domains. The m3po ontology is written in WSML. The related ontology m3pl, written in PSL using the extension FLOWS (First Order Logic for Web Services), enables the extraction of choreography interfaces from workflow models.\n\nThe m3po ontology combines features of the following reference models and languages:\n\nThe m3po ontology is organized using five key aspects of workflow specifications and workflow management. Because different workflow models put a different emphasis on the five aspects, the most elaborate reference model for each aspect was used and combined into m3po.\n\n\n"}
{"id": "1465149", "url": "https://en.wikipedia.org/wiki?curid=1465149", "title": "Professional conference organiser", "text": "Professional conference organiser\n\nA professional conference organiser, professional congress organiser (PCO) or conference company is a company which specialises in the organisation and management of congresses, conferences, seminars and similar events.\n\nPCOs can typically work as consultants for academic and professional associations. They usually provide full service management for conferences including conference design, abstract management software, program development, registration, site and venue selection and booking, audiovisuals, IT support, logistics, leisure management, marketing, printing and web services, sourcing speakers, funding, sponsorship and exhibitor sales, financial management and budget control.\n\nOther companies offer related services including travel agents and public relations companies. They tend to focus on limited areas such as destination management.\n\nRecent surveys of UK conference venues have found that a third of conference bookings were made by PCOs or venue-finding agencies. In 2006 UK-based conferences generated £7.6 billion in direct sales giving PCOs a central role in some £2.5 billion of revenue generation. The UK is ranked second behind the US for global market share of conferences. Thus, although there is no one source of global statistics for the conference market it appears that PCOs play a central role in several billion dollars' worth of revenue generation worldwide.\n\n\n"}
{"id": "990677", "url": "https://en.wikipedia.org/wiki?curid=990677", "title": "SAS (software)", "text": "SAS (software)\n\nSAS (previously \"Statistical Analysis System\") is a software suite developed by SAS Institute for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics.\n\nSAS was developed at North Carolina State University from 1966 until 1976, when SAS Institute was incorporated. SAS was further developed in the 1980s and 1990s with the addition of new statistical procedures, additional components and the introduction of JMP. A point-and-click interface was added in version 9 in 2004. A social media analytics product was added in 2010.\n\nSAS is a software suite that can mine, alter, manage and retrieve data from a variety of sources and perform statistical analysis on it. SAS provides a graphical point-and-click user interface for non-technical users and more advanced options through the SAS language.\n\nSAS programs have DATA steps, which retrieve and manipulate data, and PROC steps, which analyze the data. Each step consists of a series of statements.\n\nThe DATA step has executable statements that result in the software taking an action, and declarative statements that provide instructions to read a data set or alter the data's appearance. The DATA step has two phases: compilation and execution. In the compilation phase, declarative statements are processed and syntax errors are identified. Afterwards, the execution phase processes each executable statement sequentially. Data sets are organized into tables with rows called \"observations\" and columns called \"variables\". Additionally, each piece of data has a descriptor and a value.\n\nThe PROC step consists of PROC statements that call upon named procedures. Procedures perform analysis and reporting on data sets to produce statistics, analyses, and graphics. There are more than 300 procedures and each one contains a substantial body of programming and statistical work. PROC statements can also display results, sort data or perform other operations.\n\nSAS macros are pieces of code or variables that are coded once and referenced to perform repetitive tasks.\n\nSAS data can be published in HTML, PDF, Excel and other formats using the Output Delivery System, which was first introduced in 2007. The SAS Enterprise Guide is SAS's point-and-click interface. It generates code to manipulate data or perform analysis automatically and does not require SAS programming experience to use.\n\nThe SAS software suite has more than 200 components Some of the SAS components include:\n\nThe development of SAS began in 1966 after North Carolina State University re-hired Anthony Barr to program his analysis of variance and regression software so that it would run on IBM System/360 computers. The project was funded by the National Institute of Health and was originally intended to analyze agricultural data to improve crop yields. Barr was joined by student James Goodnight, who developed the software's statistical routines, and the two became project leaders. In 1968, Barr and Goodnight integrated new multiple regression and analysis of variance routines. In 1972, after issuing the first release of SAS, the project lost its funding. According to Goodnight, this was because NIH only wanted to fund projects with medical applications. Goodnight continued teaching at the university for a salary of $1 and access to mainframe computers for use with the project, until it was funded by the University Statisticians of the Southern Experiment Stations the following year. John Sall joined the project in 1973 and contributed to the software's econometrics, time series, and matrix algebra. Another early participant, Caroll G. Perkins, contributed to SAS' early programming. Jolayne W. Service and Jane T. Helwig created SAS' first documentation.\n\nThe first versions of SAS were named after the year in which they were released. In 1971, SAS 71 was published as a limited release. It was used only on IBM mainframes and had the main elements of SAS programming, such as the DATA step and the most common procedures in the PROC step. The following year a full version was released as SAS 72, which introduced the MERGE statement and added features for handling missing data or combining data sets. In 1976, Barr, Goodnight, Sall, and Helwig removed the project from North Carolina State and incorporated it into SAS Institute, Inc.\n\nSAS was re-designed in SAS 76 with an open architecture that allowed for compilers and procedures. The INPUT and INFILE statements were improved so they could read most data formats used by IBM mainframes. Generating reports was also added through the PUT and FILE statements. The ability to analyze general linear models was also added as was the FORMAT procedure, which allowed developers to customize the appearance of data. In 1979, SAS 79 added support for the CMS operating system and introduced the DATASETS procedure. Three years later, SAS 82 introduced an early macro language and the APPEND procedure.\n\nSAS version 4 had limited features, but made SAS more accessible. Version 5 introduced a complete macro language, array subscripts, and a full-screen interactive user interface called Display Manager. In 1985, SAS was rewritten in the C programming language. This allowed for the SAS' Multivendor Architecture that allows the software to run on UNIX, MS-DOS, and Windows. It was previously written in PL/I, Fortran, and assembly language.\n\nIn the 1980s and 1990s, SAS released a number of components to complement Base SAS. SAS/GRAPH, which produces graphics, was released in 1980, as well as the SAS/ETS component, which supports econometric and time series analysis. A component intended for pharmaceutical users, SAS/PH-Clinical, was released in the 1990s. The Food and Drug Administration standardized on SAS/PH-Clinical for new drug applications in 2002. Vertical products like SAS Financial Management and SAS Human Capital Management (then called CFO Vision and HR Vision respectively) were also introduced. JMP was developed by SAS co-founder John Sall and a team of developers to take advantage of the graphical user interface introduced in the 1984 Apple Macintosh and shipped for the first time in 1989. Updated versions of JMP were released continuously after 2002 with the most recent release being from 2016.\n\nSAS version 6 was used throughout the 1990s and was available on a wider range of operating systems, including Macintosh, OS/2, Silicon Graphics, and Primos. SAS introduced new features through dot-releases. From 6.06 to 6.09, a user interface based on the windows paradigm was introduced and support for SQL was added. Version 7 introduced the Output Delivery System (ODS) and an improved text editor. ODS was improved upon in successive releases. For example, more output options were added in version 8. The number of operating systems that were supported was reduced to UNIX, Windows and z/OS, and Linux was added. SAS version 8 and SAS Enterprise Miner were released in 1999.\n\nIn 2002, the Text Miner software was introduced. Text Miner analyzes text data like emails for patterns in Business Intelligence applications. In 2004, SAS Version 9.0 was released, which was dubbed \"Project Mercury\" and was designed to make SAS accessible to a broader range of business users. Version 9.0 added custom user interfaces based on the user's role and established the point-and-click user interface of SAS Enterprise Guide as the software's primary graphical user interface (GUI). The Customer Relationship Management (CRM) features were improved in 2004 with SAS Interaction Management. In 2008 SAS announced Project Unity, designed to integrate data quality, data integration and master data management.\n\nSAS sued World Programming, the developers of a competing implementation, World Programming System, alleging that they had infringed SAS's copyright in part by implementing the same functionality. This case was referred from the United Kingdom's High Court of Justice to the European Court of Justice on 11 August 2010. In May 2012, the European Court of Justice ruled in favor of World Programming, finding that \"the functionality of a computer program and the programming language cannot be protected by copyright.\"\n\nA free version was introduced for students in 2010. SAS Social Media Analytics, a tool for social media monitoring, engagement and sentiment analysis, was also released that year. SAS Rapid Predictive Modeler (RPM), which creates basic analytical models using Microsoft Excel, was introduced that same year. JMP 9 in 2010 added a new interface for using the R programming language from JMP and an add-in for Excel. The following year, a High Performance Computing appliance was made available in a partnership with Teradata and EMC Greenplum. In 2011, the company released Enterprise Miner 7.1. The company introduced 27 data management products from October 2013 to October 2014 and updates to 160 others. At the 2015 SAS Global Forum, it announced several new products that were specialized for different industries, as well as new training software.\n\nSAS had many releases since 1972. Since release 9.3, SAS/STAT has its own release numbering.\nAs of 2011 SAS's largest set of products is its line for customer intelligence. Numerous SAS modules for web, social media and marketing analytics may be used to profile customers and prospects, predict their behaviors and manage and optimize communications. SAS also provides the SAS Fraud Framework. The framework's primary functionality is to monitor transactions across different applications, networks and partners and use analytics to identify anomalies that are indicative of fraud. SAS Enterprise GRC (Governance, Risk and Compliance) provides risk modeling, scenario analysis and other functions in order to manage and visualize risk, compliance and corporate policies. There is also a SAS Enterprise Risk Management product-set designed primarily for banks and financial services organizations.\n\nSAS' products for monitoring and managing the operations of IT systems are collectively referred to as SAS IT Management Solutions. SAS collects data from various IT assets on performance and utilization, then creates reports and analyses. SAS' Performance Management products consolidate and provide graphical displays for key performance indicators (KPIs) at the employee, department and organizational level. The SAS Supply Chain Intelligence product suite is offered for supply chain needs, such as forecasting product demand, managing distribution and inventory and optimizing pricing. There is also a \"SAS for Sustainability Management\" set of software to forecast environmental, social and economic effects and identify causal relationships between operations and an impact on the environment or ecosystem.\n\nSAS has product sets for specific industries, such as government, retail, telecommunications and aerospace and for marketing optimization or high-performance computing.\n\nIn a 2005 article for the \"Journal of Marriage and Family\" comparing statistical packages from SAS and its competitors Stata and SPSS, Alan C. Acock wrote that SAS programs provide \"extraordinary range of data analysis and data management tasks,\" but were difficult to use and learn. SPSS and Stata, meanwhile, were both easier to learn (with better documentation) but had less capable analytic abilities, though these could be expanded with paid (in SPSS) or free (in Stata) add-ons. Acock concluded that SAS was best for power users, while occasional users would benefit most from SPSS and Stata. A comparison by the University of California, Los Angeles, gave similar results.\n\nCompetitors such as Revolution Analytics and Alpine Data Labs advertise their products as considerably cheaper than SAS'. In a 2011 comparison, Doug Henschen of \"InformationWeek\" found that start-up fees for the three are similar, though he admitted that the starting fees were not necessarily the best basis for comparison. SAS' business model is not weighted as heavily on initial fees for its programs, instead focusing on revenue from annual subscription fees.\n\nAccording to IDC, SAS is the largest market-share holder in \"advanced analytics\" with 35.4 percent of the market as of 2013. It is the fifth largest market-share holder for business intelligence (BI) software with a 6.9% share and the largest independent vendor. It competes in the BI market against conglomerates, such as SAP BusinessObjects, IBM Cognos, SPSS Modeler, Oracle Hyperion, and Microsoft BI. SAS has been named in the Gartner Leader's Quadrant for Data Integration Tool and for Business Intelligence and Analytical Platforms.\nA study published in 2011 in \"BMC Health Services Research\" found that SAS was used in 42.6 percent of data analyses in health service research, based on a sample of 1,139 articles drawn from three journals.\n\n\n"}
{"id": "389401", "url": "https://en.wikipedia.org/wiki?curid=389401", "title": "Scientific management", "text": "Scientific management\n\nScientific management is a theory of management that analyzes and synthesizes workflows. Its main objective is improving economic efficiency, especially labour productivity. It was one of the earliest attempts to apply science to the engineering of processes and to management. Scientific management is sometimes known as Taylorism after its founder, Frederick Winslow Taylor.\n\nTaylor began the theory's development in the United States during the 1880s and '90s within manufacturing industries, especially steel. Its peak of influence came in the 1910s; Taylor died in 1915 and by the 1920s, scientific management was still influential but had entered into competition and syncretism with opposing or complementary ideas.\n\nAlthough scientific management as a distinct theory or school of thought was obsolete by the 1930s, most of its themes are still important parts of industrial engineering and management today. These include: analysis; synthesis; logic; rationality; empiricism; work ethic; efficiency and elimination of waste; standardization of best practices; disdain for tradition preserved merely for its own sake or to protect the social status of particular workers with particular skill sets; the transformation of craft production into mass production; and knowledge transfer between workers and from workers into tools, processes, and documentation.\n\nTaylor's own names for his approach initially included \"shop management\" and \"process management\". However, \"scientific management\" came to national attention in 1910 when crusading attorney Louis Brandeis (then not yet Supreme Court justice) popularized the term. Brandeis had sought a consensus term for the approach with the help of practitioners like Henry L. Gantt and Frank B. Gilbreth. Brandeis then used the consensus of \"scientific management\" when he argued before the Interstate Commerce Commission (ICC) that a proposed increase in railroad rates was unnecessary despite an increase in labor costs; he alleged scientific management would overcome railroad inefficiencies (The ICC ruled against the rate increase, but also dismissed as insufficiently substantiated that concept the railroads were necessarily inefficient.) Taylor recognized the nationally-known term \"scientific management\" as another good name for the concept, and adopted it in the title of his influential 1911 monograph.\n\nThe Midvale Steel Company, \"one of America's great armor plate making plants,\" was the birthplace of scientific management. In 1877, at age 22, Frederick W. Taylor started as a clerk in Midvale, but advanced to foreman in 1880. As foreman, Taylor was \"constantly impressed by the failure of his [team members] to produce more than about one-third of [what he deemed] a good day's work.\" Taylor determined to discover, by scientific methods, how long it should take men to perform each given piece of work; and it was in the fall of 1882 that he started to put the first features of scientific management into operation. \n\nHorace Bookwalter Drury, in his 1918 work, \"Scientific management: A History and Criticism\", identified seven other leaders in the movement, most of whom learned of and extended scientific management from Taylor's efforts:\n\nEmerson's testimony in late 1910 to the Interstate Commerce Commission brought the movement to national attention and instigated serious opposition. Emerson contended the railroads might save $1,000,000 a day by paying greater attention to efficiency of operation. By January 1911, a leading railroad journal began a series of articles denying they were inefficiently managed.\nWhen steps were taken to introduce scientific management at the government-owned Rock Island Arsenal in early 1911, it was opposed by Samuel Gompers, founder and President of the American Federation of Labor (an alliance of craft unions). When a subsequent attempt was made to introduce the bonus system into the government's Watertown Arsenal foundry during the summer of 1911, the entire force walked out for a few days. Congressional investigations followed, resulting in a ban on the use of time studies and pay premiums in Government service.\n\nTaylor's death in 1915 at age 59 left the movement without its original leader. In management literature today, the term \"scientific management\" mostly refers to the work of Taylor and his disciples (\"classical\", implying \"no longer current, but still respected for its seminal value\") in contrast to newer, improved iterations of efficiency-seeking methods. Today, task-oriented optimization of work tasks is nearly ubiquitous in industry.\n\nFlourishing in the late 19th and early 20th century, scientific management built on earlier pursuits of economic efficiency. While it was prefigured in the folk wisdom of thrift, it favored empirical methods to determine efficient procedures rather than perpetuating established traditions. Thus it was followed by a profusion of successors in applied science, including time and motion study, the Efficiency Movement (which was a broader cultural echo of scientific management's impact on business managers specifically), Fordism, operations management, operations research, industrial engineering, management science, manufacturing engineering, logistics, business process management, business process reengineering, lean manufacturing, and Six Sigma. There is a fluid continuum linking scientific management with the later fields, and the different approaches often display a high degree of compatibility.\n\nTaylor rejected the notion, which was universal in his day and still held today, that the trades, including manufacturing, were resistant to analysis and could only be performed by craft production methods. In the course of his empirical studies, Taylor examined various kinds of manual labor. For example, most bulk materials handling was manual at the time; material handling equipment as we know it today was mostly not developed yet. He looked at shoveling in the unloading of railroad cars full of ore; lifting and carrying in the moving of iron pigs at steel mills; the manual inspection of bearing balls; and others. He discovered many concepts that were not widely accepted at the time. For example, by observing workers, he decided that labor should include rest breaks so that the worker has time to recover from fatigue, either physical (as in shoveling or lifting) or mental (as in the ball inspection case). Workers were allowed to take more rests during work, and productivity increased as a result.\n\nSubsequent forms of scientific management were articulated by Taylor's disciples, such as Henry Gantt; other engineers and managers, such as Benjamin S. Graham; and other theorists, such as Max Weber. Taylor's work also contrasts with other efforts, including those of Henri Fayol and those of Frank Gilbreth, Sr. and Lillian Moller Gilbreth (whose views originally shared much with Taylor's but later diverged in response to Taylorism's inadequate handling of human relations).\n\nScientific management requires a high level of managerial control over employee work practices and entails a higher ratio of managerial workers to laborers than previous management methods. Such detail-oriented management may cause friction between workers and managers.\n\nTaylor observed that some workers were more talented than others, and that even smart ones were often unmotivated. He observed that most workers who are forced to perform repetitive tasks tend to work at the slowest rate that goes unpunished. This slow rate of work has been observed in many industries and many countries and has been called by various terms. Taylor used the term \"soldiering\", a term that reflects the way conscripts may approach following orders, and observed that, when paid the same amount, workers will tend to do the amount of work that the slowest among them does. Taylor describes soldiering as \"the greatest evil with which the working-people ... are now afflicted.\"\n\nThis reflects the idea that workers have a vested interest in their own well-being, and do not benefit from working above the defined rate of work when it will not increase their remuneration. He therefore proposed that the work practice that had been developed in most work environments was crafted, intentionally or unintentionally, to be very inefficient in its execution. He posited that time and motion studies combined with rational analysis and synthesis could uncover one best method for performing any particular task, and that prevailing methods were seldom equal to these best methods. Crucially, Taylor himself prominently acknowledged that if each employee's compensation was linked to their output, their productivity would go up. Thus his compensation plans usually included piece rates. In contrast, some later adopters of time and motion studies ignored this aspect and tried to get large productivity gains while passing little or no compensation gains to the workforce, which contributed to resentment against the system.\n\nScientific management evolved in an era when mechanization and automation were still in their infancy. The ideas and methods of scientific management extended the American system of manufacturing in the transformation from craft work (with humans as the only possible agents) to mechanization and automation, although proponents of scientific management did not predict the extensive removal of humans from the production process. Concerns over labor-displacing technologies rose with increasing mechanization and automation.\n\nBy factoring processes into discrete, unambiguous units, scientific management laid the groundwork for automation and offshoring, prefiguring industrial process control and numerical control in the absence of any machines that could carry it out. Taylor and his followers did not foresee this at the time; in their world, it was \"humans\" that would execute the optimized processes. (For example, although in their era the instruction \"open valve A whenever pressure gauge B reads over value X\" would be carried out by a human, the fact that it had been reduced to an algorithmic component paved the way for a machine to be the agent.) However, one of the common threads between their world and ours is that the agents of execution need not be \"smart\" to execute their tasks. In the case of computers, they are \"not able\" (yet) to be \"smart\" (in that sense of the word); in the case of human workers under scientific management, they were often \"able\" but were \"not allowed\". Once the time-and-motion men had completed their studies of a particular task, the workers had very little opportunity for further thinking, experimenting, or suggestion-making. They were forced to \"play dumb\" most of the time, which occasionally led to revolts.\n\nThe middle ground between the craft production of skilled workers and full automation is occupied by systems of extensive mechanization and partial automation operated by semiskilled and unskilled workers. Such systems depend on algorithmic workflows and knowledge transfer, which require substantial engineering to succeed. Although Taylor's intention for scientific management was simply to optimize work methods, the process engineering that he pioneered also tends to build the skill into the equipment and processes, removing most need for skill in the workers. Such engineering has governed most industrial engineering since then. It is also the essence of successful offshoring. The common theme in all these cases is that businesses engineer their way out of their need for large concentrations of skilled workers, and the high-wage environments that sustain them. This creates competitive advantage on the local level of individual firms, although the pressure it exerts systemically on employment and employability is an externality.\n\nTaylor often expressed views of workers that may be considered insulting. He recognized differences between workers, stressed the need to select the right person for the right job, and championed the workers by advocating frequent breaks and good pay for good work. He often failed to conceal his condescending attitude towards less intelligent workers, describing them as \"stupid\" and comparing them to draft animals in that they have to have their tasks managed for them in order to work efficiently.\n\nOther thinkers soon offered more ideas on the roles that workers play in mature industrial systems. These included ideas on improvement of the individual worker with attention to the worker's needs, not just the needs of the whole. James Hartness published \"The Human Factor in Works Management\" in 1912, while Frank Gilbreth and Lillian Moller Gilbreth offered their own alternatives to Taylorism. The human relations school of management evolved in the 1930s to complement rather than replace scientific management, with Taylorism determining the organisation of the work process, and human relations helping to adapt the workers to the new procedures. Today's efficiency-seeking methods, such as lean manufacturing, include respect for workers and fulfillment of their needs as integral parts of the theory. (Workers slogging their way through workdays in the business world do encounter flawed implementations of these methods that make jobs unpleasant; but these implementations generally lack managerial competence in matching theory to execution.) Clearly, a syncretism has occurred since Taylor's day, although its implementation has been uneven, as lean management in capable hands has produced good results for both managers and workers, but in incompetent hands has damaged enterprises.\n\nWith the division of labor that became commonplace as Taylorism was implemented in manufacturing, workers lost their sense of connection to the production of goods. Workers began to feel disenfranchised with the monotonous and unfulfilling work they were doing in factories. Before scientific management, workers felt a sense of pride when completing their good, which went away when workers only completed one part of production. \"The further 'progress' of industrial development... increased the anomic or forced division of labor,\" the opposite of what Taylor thought would be the effect. Partial adoption of Taylor's principles by management seeking to boost efficiency, while ignoring principles such as fair pay and direct engagement by managers, led to further tensions and the rise of unions to represent workers needs.\n\nTaylor had a largely negative view of unions, and believed they only led to decreased productivity. Although he opposed them, his work with scientific management led disenfranchised workers to look to unions for support.\n\nUnder scientific management, the demands of work intensified. Workers became dissatisfied with the work environment and became angry. During one of Taylor's own implementations at the Watertown Arsenal in Massachusetts, a strike led to an investigation of Taylor's methods by a U.S. House of Representatives committee. The committee reported in 1912, concluding that scientific management did provide some useful techniques and offered valuable organizational suggestions, but that it also gave production managers a dangerously high level of uncontrolled power. After an attitude survey of the workers revealed a high level of resentment and hostility towards scientific management, the Senate banned Taylor's methods at the arsenal.\n\nScientific management lowered worker morale and exacerbated existing conflicts between labor and management. As a consequence, the method inadvertently strengthened labor unions and their bargaining power in labor disputes, thereby neutralizing most or all of the benefit of any productivity gains it had achieved. Thus its net benefit to owners and management ended up as small or negative. It took new efforts, borrowing some ideas from scientific management but mixing them with others, to produce more productive formula.\n\nScientific management may have exacerbated grievances among workers about oppressive or greedy management. It certainly strengthened developments that put workers at a disadvantage: the erosion of employment in developed economies via both offshoring and automation. Both were made possible by the deskilling of jobs, which was made possible by the knowledge transfer that scientific management achieved. Knowledge was transferred both to cheaper workers and from workers into tools. Jobs that once would have required craft work first transformed to semiskilled work, then unskilled. At this point the labor had been commoditized, and thus the competition between workers (and worker populations) moved closer to pure than it had been, depressing wages and job security. Jobs could be offshored (giving one human's tasks to others—which could be good for the new worker population but was bad for the old) or they could be rendered nonexistent through automation (giving a human's tasks to machines). Either way, the net result from the perspective of developed-economy workers was that jobs started to pay less, then disappear. The power of labor unions in the mid-twentieth century only led to a push on the part of management to accelerate the process of automation, hastening the onset of the later stages just described.\n\nIn a central assumption of scientific management, \"the worker was taken for granted as a cog in the machinery.\" While scientific management had made jobs unpleasant, its successors made them less remunerative, less secure, and finally nonexistent as a consequence of structural unemployment.\n\nIt is often assumed that Fordism derives from Taylor's work. Taylor apparently made this assumption himself when visiting the Ford Motor Company's Michigan plants not too long before he died, but it is likely that the methods at Ford were evolved independently, and that any influence from Taylor's work was indirect at best. Charles E. Sorensen, a principal of the company during its first four decades, disclaimed any connection at all. There was a belief at Ford, which remained dominant until Henry Ford II took over the company in 1945, that the world's experts were worthless, because if Ford had listened to them, it would have failed to attain its great successes. Henry Ford felt that he had succeeded \"in spite of\", not \"because of\", experts, who had tried to stop him in various ways (disagreeing about price points, production methods, car features, business financing, and other issues). Sorensen thus was dismissive of Taylor and lumped him into the category of useless experts. Sorensen held the New England machine tool vendor Walter Flanders in high esteem and credits him for the efficient floorplan layout at Ford, claiming that Flanders knew nothing about Taylor. Flanders may have been exposed to the spirit of Taylorism elsewhere, and may have been influenced by it, but he did not cite it when developing his production technique. Regardless, the Ford team apparently did independently invent modern mass production techniques in the period of 1905-1915, and they themselves were not aware of any borrowing from Taylorism. Perhaps it is only possible with hindsight to see the zeitgeist that (indirectly) connected the budding Fordism to the rest of the efficiency movement during the decade of 1905-1915.\n\nScientific management appealed to managers of planned economies because central economic planning relies on the idea that the expenses that go into economic production can be precisely predicted and can be optimized by design. The opposite theoretical pole would be laissez-faire thinking in which the invisible hand of free markets is the only possible \"designer\". In reality most economies today are somewhere in between. Another alternative for economic planning is workers' self-management.\n\nIn the Soviet Union, Taylorism was advocated by Aleksei Gastev and \"nauchnaia organizatsia truda\" (\"the movement for the scientific organisation of labor\"). It found support in both Vladimir Lenin and Leon Trotsky. Gastev continued to promote this system of labor management until his arrest and execution in 1939. In the 1920s and 1930s, the Soviet Union enthusiastically embraced Fordism and Taylorism, importing American experts in both fields as well as American engineering firms to build parts of its new industrial infrastructure. The concepts of the Five Year Plan and the centrally planned economy can be traced directly to the influence of Taylorism on Soviet thinking. As scientific management was believed to epitomize American efficiency, Joseph Stalin even claimed that \"the combination of the Russian revolutionary sweep with American efficiency is the essence of Leninism.\"\n\nSorensen was one of the consultants who brought American know-how to the USSR during this era, before the Cold War made such exchanges unthinkable. As the Soviet Union developed and grew in power, both sides, the Soviets and the Americans, chose to ignore or deny the contribution that American ideas and expertise had made: the Soviets because they wished to portray themselves as creators of their own destiny and not indebted to a rival, and the Americans because they did not wish to acknowledge their part in creating a powerful communist rival. Anti-communism had always enjoyed widespread popularity in America, and anti-capitalism in Russia, but after World War II, they precluded any admission by either side that technologies or ideas might be either freely shared or clandestinely stolen.\n\nBy the 1950s, scientific management had grown dated, but its goals and practices remained attractive and were also being adopted by the German Democratic Republic as it sought to increase efficiency in its industrial sectors. In the accompanying photograph from the German Federal Archives, workers discuss standards specifying how each task should be done and how long it should take. The workers are engaged in a state-planned instance of process improvement, but they are pursuing the same goals that were contemporaneously pursued in capitalist societies, as in the Toyota Production System.\n\nIn 1911, organized labor erupted with strong opposition to scientific management, spreading from Samuel Gompers, founder and president of the American Federal of Labor (AFL), in the US to far around the globe. By 1913 Vladimir Lenin wrote that the \"most widely discussed topic today in Europe, and to some extent in Russia, is the 'system' of the American engineer, Frederick Taylor\"; Lenin decried it as merely a \"'scientific' system of sweating\" more work from laborers. Again in 1914, Lenin derided Taylorism as \"man’s enslavement by the machine.\" However, after the Russian Revolutions brought him to power, Lenin wrote in 1918 that the \"Russian is a bad worker [who must] learn to work. The Taylor system... is a combination of the refined brutality of bourgeois exploitation and a number of the greatest scientific achievements in the field of analysing mechanical motions during work, the elimination of superfluous and awkward motions, the elaboration of correct methods of work, the introduction of the best system of accounting and control, etc. The Soviet Republic must at all costs adopt all that is valuable in the achievements of science and technology in this field.\" \n\nThe Watertown Arsenal in Massachusetts provides an example of the application and repeal of the Taylor system in the workplace, due to worker opposition. In the early 2001, neglect in the Watertown shops included overcrowding, dim-lighting, lack of tools and equipment, and questionable management strategies in the eyes of the workers. Frederick W. Taylor and Carl G. Barth visited Watertown in April 1909 and reported on their observations at the shops. Their conclusion was to apply the Taylor system of management to the shops to produce better results. Efforts to install the Taylor system began in June 1909. Over the years of time study and trying to improve the efficiency of workers, criticisms began to evolve. Workers complained of having to compete with one another, feeling strained and resentful, and feeling excessively tired after work. There is, however, no evidence that the times enforced were unreasonable. In June 1913, employees of the Watertown Arsenal petitioned to abolish the practice of scientific management there. A number of magazine writers inquiring into the effects of scientific management found that the \"conditions in shops investigated contrasted favorably with those in other plants\".\n\nCriticism of Taylor's principles of effective workmanship and the productivity of the workers continues today. Often, his theories are described as man-contemptuous and portrayed as now overhauled. In practice, however, the principles of Taylor are still being pursued by Kaizen and Six Sigma and similar methodologies, which are based on the development of working methods and courses based on systematic analysis rather than relying on tradition and rule of thumb.\n\nTaylorism is, according to Stephen P. Waring, considered very controversial, despite its popularity. It is often criticized for turning the worker into an \"automaton\" or \"machine\". Due to techniques employed with scientific management, employees claim to have become overworked and were hostile to the process. Criticisms commonly came from workers who were subjected to an accelerated work pace, lower standards of workmanship, lower product-quality, and lagging wages. Workers defied being reduced to such machines, and objected to the practices of Taylorism. Many workers formed unions, demanded higher pay, and went on strike to be free of control issues. This ignited class conflict, which Taylorism was initially meant to prevent. Efforts to resolve the conflicts included methods of scientific collectivism, making agreements with unions, and the personnel management movement.\n\nIn the middle of 1960 some counter-movements to Taylorism arose. Representatives of the so-called Human Relations movement urged humanization and democratization of the working world. The criticism of Taylorism supports the unilateral approach of labor. Strictly speaking, Taylorism is not a scientific theory. All theories of F. W. Taylor are based on experiments. On the basis of samples, conclusions were made, which were then generalized. There is no representativeness of the selected sample.\n\nAnother reason for criticizing Taylor's methods stemmed from Taylor's belief that the scientific method included the calculations of exactly how much time it takes a man to do a particular task, or his rate of work. However, the opposition to this argument is that such a calculation relies on certain arbitrary, non-scientific decisions such as what constituted the job, which men were timed, and under which conditions. Any of these factors are subject to change, and therefore can produce inconsistencies.\n\nSome dismiss so-called \"scientific management\"/Taylorism as pseudoscience.\n\nScientific management was one of the first attempts to systematically treat management and process improvement as a scientific problem. It may have been the first to do so in a \"bottom-up\" way and found a lineage of successors that have many elements in common. With the advancement of statistical methods, quality assurance and quality control began in the 1920s and 1930s. During the 1940s and 1950s, the body of knowledge for doing scientific management evolved into operations management, operations research, and management cybernetics. In the 1980s total quality management became widely popular, and in the 1990s \"re-engineering\" went from a simple word to a mystique. Today's Six Sigma and lean manufacturing could be seen as new kinds of scientific management, although their evolutionary distance from the original is so great that the comparison might be misleading. In particular, Shigeo Shingo, one of the originators of the Toyota Production System, believed that this system and Japanese management culture in general should be seen as a kind of scientific management.\n\nPeter Drucker saw Frederick Taylor as the creator of knowledge management, because the aim of scientific management was to produce knowledge about how to improve work processes. Although the typical application of scientific management was manufacturing, Taylor himself advocated scientific management for all sorts of work, including the management of universities and government. For example, Taylor believed scientific management could be extended to \"the work of our salesmen\". Shortly after his death, his acolyte Harlow S. Person began to lecture corporate audiences on the possibility of using Taylorism for \"sales engineering\" (Person was talking about what is now called sales process engineering—engineering the processes that salespeople use—not about what we call sales engineering today.) This was a watershed insight in the history of corporate marketing.\n\nGoogle's methods of increasing productivity and output can be seen to be influenced by Taylorism as well. The Silicon Valley company is a forerunner in applying behavioral science to increase knowledge worker productivity. In classic scientific management as well as approaches like lean management or business process reengineering leaders and experts develop and define standard. Leading high-tech companies use the concept of nudge management to increase productivity of employees. More and more business leaders start to make use of this new scientific management.\n\nToday's militaries employ all of the major goals and tactics of scientific management, if not under that name. Of the key points, all but wage incentives for increased output are used by modern military organizations. Wage incentives rather appear in the form of skill bonuses for enlistments.\n\nScientific management has had an important influence in sports, where stop watches and motion studies rule the day. (Taylor himself enjoyed sports, especially tennis and golf. He and a partner won a national championship in doubles tennis. He invented improved tennis racquets and improved golf clubs, although other players liked to tease him for his unorthodox designs, and they did not catch on as replacements for the mainstream implements).\n\nModern human resources can be seen to have begun in the scientific management era, most notably in the writings of Katherine M. H. Blackford, who was also a proponent of eugenics.\n\nPractices descended from scientific management are currently used in offices and in medicine (e.g. managed care) as well.\n\nIn the 21st century the tendency to overcome Taylorism is very great. The trend is moving away from assembly line work, since people are increasingly being replaced by machines in production plants and sub-processes are automated, so that human labor is not necessary in these cases. The desire for automated workflow in companies is intended to reduce costs and support the company at the operational level.\n\nFurthermore, it can be observed that many companies try to make the workplace as comfortable as possible for the employees. This is achieved by light flooded rooms, Feng Shui methods in the workplace or even by creative jobs. The efficiency and creativity of the employees is to be promoted by a pleasant atmosphere at the workplace. Approaches of the Scientific Management, in which attempts are also made to make the work environment pleasant, are partly recognizable here.\n\nIn the works of Gouldner and Crozier, the recognition of the plurality of industrial forms is being discussed. In the 21st century, we have a modern corporate management, where managers are given the available positions in companies and are given the right to take legal action.\n\nThe working world of the 21st century is mainly based on Total Quality Management. This is derived from quality control. In contrast to Taylorism, by which products are produced in the shortest possible time without any form of quality control and delivered to the end customer, the focus in the 21st century is on quality control at TQM. In order to avoid error rates, it is necessary to hire specialists to check all the products which have been manufactured before they are delivered to the end customer. The quality controls have improved over time, and incorrect partial processes can be detected in time and removed from the production process.\n\nTaylorism approaches are largely prevalent in companies where machines can not perform certain activities. Certain subprocesses are still to be carried out by humans, such as the sorting out of damaged fruit in the final process before the goods are packed by machines. It turns out that the quality control is ultimately to be verified by the individual man. Certain activities remain similar to the approach of Taylorism. There are no \"zero error programs\", employees have to be trained and thus reduce error rates.\n\nThrough the invention of the management one managed positions, which are equipped with disposition rights. The positions are occupied by paid employees and form the basis for the current, modern corporate management. In order to be able to perceive these positions, it was no longer necessary to bring in resources such as capital, but instead qualifications were necessary. Written rights are also passed on to employees, which means that the leaders of an organization tend to fall into the background and merely have a passive position.\n\nThe structure and size of a company must be distinguished. Depending on which dispositions are predominant, the size of the company, the sector, and the number of employees in an organization, one can examine whether approaches of Taylorism are prevalent. It is believed to be predominant in the automotive industry. In spite of the fact that a lot of activities have been replaced by machines during the production, it is ultimately the person who can check the quality of a product.\n\nTaylorism led to a performance increase in companies. All superfluous working steps are avoided. The company benefits from the productivity of the workers and this in turn from higher wages. Unused productivity resources were effectively exploited by Taylorism.\n\nToday's work environment in the 21st century benefits from the humanity of working conditions. Corporate strategies are increasingly focused on the flexibility of work. Flexible adaptation to demand should be possible. The qualifications of the employees, the work content as well as the work processes are determined by the competition situation on the market. The aim is to promote self-discipline and the motivation of employees in order to achieve their own tasks and at the same time to prevent monotonous work. Technical progress has led to more humane working conditions since inhumane work steps are done by the machines.\n\nTaylorism's approach is called inhuman. The increased wage alone is not a permanent incentive for the workers to carry out the same monotonous work. Worker-friendly work structures are required. People no longer want to be perceived merely as executive organ. The complete separation from manual and headwork leads to a lack of pleasure in the execution of the work steps.\n\nIn the 21st century the rising level of education leads to better trained workers, but the competitive pressure also rises. The interplay of economic as well as the pressure to innovate also lead to uncertainty among employees. The national diseases in the 21st century have become burn-out phenomena and depressions, often in conjunction with the stress and the increased performance pressure in the work.\n\n\n\n"}
{"id": "1838287", "url": "https://en.wikipedia.org/wiki?curid=1838287", "title": "Shared services", "text": "Shared services\n\nShared services is the provision of a service by one part of an organization or group, where that service had previously been found, in more than one part of the organization or group. Thus the funding and resourcing of the service is shared and the providing department effectively becomes an internal service provider. The key here is the idea of 'sharing' within an organization or group. This sharing needs to fundamentally include shared accountability of results by the unit from where the work is migrated to the provider. The provider, on the other hand, needs to ensure that the agreed results are delivered based on defined measures (KPIs, cost, quality etc.).\n\nShared services is similar to collaboration that might take place between different organizations such as a Hospital Trust or a Police Force. For example, adjacent Trusts might decide to collaborate by merging their HR or IT functions.\n\nThere are two arguments for sharing services: The ‘less of a common resource' argument and the ‘efficiency through industrialization' argument. The former is ‘obvious': if you have fewer managers, IT systems, buildings etc; if you use less of some resource, it will reduce costs. The second argument is ‘efficiency through industrialization’. This argument assumes that efficiencies follow from specialization and standardization – resulting in the creation of ‘front' and ‘back' offices. The typical method is to simplify, standardize and then centralize, using an IT 'solution' as the means.\n\nShared services is different from the model of outsourcing, which is where an external third party is paid to provide a service that was previously internal to the buying organization, typically leading to redundancies and re-organization. There is an ongoing debate about the advantages of shared services over outsourcing. It is sometimes assumed that a joint venture between a government department and a commercial organization is an example of shared services. The joint venture involves the creation of a separate legal commercial entity (jointly owned), which provides profit to its shareholders.\n\nTraditionally the development of a shared-service organization (SSO) or shared-service centre (SSC) within an organization is an attempt to reduce costs (often attempted through economies of scale), standardized processes (through centralization). A global Service Center Benchmark study carried out by the Shared Services & Outsourcing Network (SSON) and the Hackett Group, which surveyed more than 250 companies, found that only about a third of all participants were able to generate cost savings of 20% or greater from their SSOs. At NASA, the 2006 switch to a shared services model is realizing nearly $20 million of savings annually. Further, by the end of 2015, the NASA Shared Services Center is expected to save the organization a total of over $200 million, according to NASA’s Director of Service Delivery.\n\nA large-scale cultural and process transformation can be a key component of a move to shared services and may include redundancies and changes of work practices. It is claimed that transformation often results in a better quality of work life for employees although there are few case studies to back this up .\n\nShared services are more than just centralization or consolidation of similar activities in one location. Shared services can mean running these service activities like a business and delivering services to internal customers at a cost, quality, and timeliness that is competitive with alternatives.\n\nThe management thinker and inventor of The Vanguard Method, Professor John Seddon claims that shared service projects based on attempts to achieve 'economies of scale' are a mix of the plausibly obvious and a little hard data, brought together to produce two broad assertions, for which there is little hard factual evidence. He argues that shared service projects fail (and often end up costing more than they hoped to save) because they cause a disruption to the service flow by moving the work to a central location, creating waste in handoffs, rework and duplication, lengthening the time it takes to deliver a service and consequently creating failure demand (demand caused by a failure to do something or do something right for a customer).\n\nA shared service can take a variety of different commercial structures. The basic commercial structures include:\n\n\nIt is sometimes argued that there are three basic location variations for a shared service including:\n\n\nThis is not just to take advantage of wage arbitrage but to appreciate the talents of particular economies in delivering specific service offerings.\n\nThe difficulty with this argument is that near-shore and off-shore are normally associated with the outsourcing model and are difficult to reconcile with the notion of an internally shared service as distinct from an externally purchased service. Clearly, the use of off-shore facilities by a government department is not an example of shared services.\n\nIn establishing and running a shared service, benchmarking and measurement is considered by some as a necessity. Benchmarking is the comparison of the service provision usually against best in class. The measurement occurs by using agreed key performance indicators (KPIs). Although the amount of KPIs chosen differs greatly it is generally accepted that fewer than 10 carefully chosen KPIs will deliver the best results.\n\nOrganizations do attempt to define benchmarks for processes and business operations.\n\nBenchmarking can be used to achieve different goals including:<br>\n1. To drive performance improvements using benchmarks as a means for setting performance targets that are met either through incremental performance improvements or transformational change.<br>\n- Strategic: with a focus on a long term horizon; and <br>\n- Tactical: with a focus on the short and medium term\n\n2. To focus an organization on becoming world class with processes that deliver the highest levels of performance that are better than those of its peer group.\n\nThe private sector has been moving towards shared services since the beginning of the 1980s. Large organizations such as the BBC, BP, Bristol Myers Squibb, Ford, GE, HP, Pfizer, Rolls-Royce, ArcelorMittal, and SAP are operating them with great success. According to the English Institute of Chartered Accountants, more than 30% of U.S. Fortune 500 companies have implemented a shared-service center, and are reporting cost savings in their general accounting functions of up to 46%.\n\nThe conventional accounting practice used to generate these figures is disputed however by management thinker Professor John Seddon, who argues that the measurement known as 'unit cost' tells you nothing about overall costs. Overall costs include 'failure demand', which is defined as a failure to do something or do something right for the customer.\n\nThe public sector has taken note of the benefits derived in the private sector and continues to strive for best practice. The United States and Australia among others have had shared services in government since the late 1990s. However, the failures of these projects are increasingly being reported by the press and exposed by opposition politicians.\n\nThe UK government under a central drive to efficiency following from the Gershon Review are working to an overall plan for realizing the benefits of shared services. The Cabinet Office has established a team specifically tasked with the role of accelerating the take up and developing the strategy for all government departments to converge and consolidate. The savings potential of this transformation in the UK Public Sector was initially estimated by the Cabinet Office at £1.4bn per annum (20% of the estimated cost of HR and Finance functions). The National Audit Office (United Kingdom) in its November 2007 report pointed out that this £1.4bn figure lacked a clear baseline of costs and contained several uncertainties, such as the initial expenditure required and the time frame for the savings.\n\nThere are reports of UK government shared service centres failing to realise savings, such as the Department of Transport's project, described as 'stupendous incompetence' by the Parliament's Public Accounts Committee.\n\nThe Northern Ireland Civil Service (NICS), has implemented shared services for a number of departments and functions. For example, IT Assist (the ICT Shared Service Centre) provides common infrastructure and desktop services to NICS staff in the office, at home or when mobile working.\n\nThe government of Canada instituted Shared Services Canada on August 4, 2011, with the objective of consolidating its data centres, networks and email systems. This follows a tendency to centralize IT services that has been followed by the provinces of British Columbia, Québec, and Ontario, as well as the federal government of the United States of America and some states like Texas. PriceWaterhouseCoopers recommended integrating government data centres in a report ordered by Public Works and Government Services Canada, revealed in December 2011.\n\nIn the Republic of Ireland, the health service nationally has been reorganized from a set of regional Health Boards to a unified national structure, the Health Services Executive. Within this structure there will be a National Shared Services Organisation, based on the model developed at the former Eastern Health Shared Services, where procurement, HR, finance and ICT services were provided to health agencies in the Eastern Region of Ireland on a business-to business basis.\n\nOrganizations that have centralized their IT functions have now begun to take a close look at the technology services that their IT departments provide to internal customers, evaluating where it makes sense to provide specific technology components as a shared service. E-mail and scanning operations were obvious early candidates; many organizations with document-intensive operations are deploying scanning centers as a shared service.\n\nMany large organizations, in both the public and private sectors, are now considering deploying enterprise content management (ECM) technology as a shared service.\n\n\n"}
{"id": "21538073", "url": "https://en.wikipedia.org/wiki?curid=21538073", "title": "Sourcing advisory", "text": "Sourcing advisory\n\nSourcing advisory is the use of third-party (external entity) advice during the sourcing process. As such, it may refer to advice sought during outsourcing, offshoring or global sourcing.\n\nGiven the strategic nature of the advisory and the prevalence of sourcing amongst global organizations, most management consulting firms today offer sourcing advisory services. In addition, there exist a host of specialized sourcing adivsory firms that focus on providing sourcing advisory and research inputs to the industry. Sourcing advisors can also be independent freelance consultants.\n\nIn recent years software tools to automate sourcing advisory emerged. Building on Software as a Service (SaaS), the trade mark \"Sourcing Advisory as a Service\" (SAaaS) is used in the market.\n\nThe range of services provided by sourcing advisory firms include:\n\n"}
{"id": "35268722", "url": "https://en.wikipedia.org/wiki?curid=35268722", "title": "The Partnership for Excellence", "text": "The Partnership for Excellence\n\nThe Partnership for Excellence (formerly the \"Ohio Partnership for Excellence\") is a nonprofit organization that administers a state-level award program for performance excellence in Ohio, Indiana, and West Virginia. The award is based on the Baldrige Criteria for Performance Excellence, which are developed and maintained by the Baldrige Performance Excellence Program, a program of the National Institute of Standards and Technology (NIST).\nThe Ohio Partnership for Excellence was incorporated as a 501c(3) nonprofit organization in 1998 to provide state level Baldrige assessment services and resources for interested organizations. In 2011, it was renamed The Partnership for Excellence (TPE) to reflect regional expansion that includes the states of Indiana and West Virginia.\n\nThe first award cycle was completed in 1999-2000.\n\nIn January 2008, Dr. Elaine D. Edgar was appointed to Executive Director. After retirement from the role in March 2011, she was replaced by Colonel Alfred C. Faber, Jr. who was appointed to the position of President/CEO.\n\n\nTPE is a membership organization composed of interested individuals and organizations throughout Ohio, Indiana, and West Virginia. TPE collects annual membership dues for the January to December membership year.\n\nTPE's Board of Examiners is composed of volunteer individuals who successfully complete annual training on the Baldrige Criteria for Performance Excellence. Advisors and examiners used during the awards process are drawn from the Board of Examiners.\n\nTPE’s main product is a feedback report to the partnering organizations that successfully complete the award process. The report contains strengths as well as opportunities for improvement (OFI) derived from both the submitted application and a site visit. This is a distinct difference from the national Baldrige Award process where only organizations with highly graded applications are granted a site visit.\n\nTPE also provides Baldrige-related educational services such as conferences, seminars, and direct partnering with individuals and organizations.\n\nTPE provides three options for organizations wanting to improve their organizational performance:\n\nOne or two advisors from TPE Board of Examiners meet with the leadership team of the partnering organization to review the Criteria for Performance Excellence and assist in developing a 5-page Organizational Profile. After review of the organizational profile, TPE provides the organization with a list of strengths and opportunities for improvement. Upon successful completion of this process, the organization receives a \"Spirit\" recognition award at TPE's annual \"Quest for Success\" conference.\n\nOne or two advisors from TPE Board of Examiners meet with the leadership team of the partnering organization to develop a 25-page application addressing the Baldrige Criteria for Performance Excellence at the \"overall\" level. After review of this application, TPE provides the organization with a list of strengths and opportunities for improvement. Upon successful completion of this process, the organization receives a \"Pioneer\" recognition award at TPE's annual \"Quest for Success\" conference.\n\nThis is the traditional 50-page application assessment mirroring the Malcolm Baldrige National Quality Award's assessment cycle. Organizations participating in this option submit an \"Intent to Apply\" in October before the application due date in December each year. TPE assigns a team of 7 members from the Board of Examiners to review (independently and then by consensus) the application before a 3-day site visit is conducted. TPE then provides the organization with a list of strength and opportunities for improvement. Based on the recommendations of the examiners, TPE's Board of Trustees recognizes each organization with a Bronze, Silver, Gold, or Platinum Award at the annual \"Quest for Success\" conference.\n\nNote:\n\n"}
{"id": "26937282", "url": "https://en.wikipedia.org/wiki?curid=26937282", "title": "US India Business Summit", "text": "US India Business Summit\n\nUIBS (short for US India Business Summit) is an annual Business Expo between the U.S. and India that is best known for its multidisciplinary speeches and panels, but is held only every other year. It is devoted to what it calls \"Friendship and Business between the United States and India\".\n\nThe goal of UIBS is to promote business and investment between United States and India in key areas of technology, agriculture, healthcare, infrastructure, supply chain, logistics, real estate and investment etc. Our special emphasis is to connect entrepreneurs and small-to- medium size businesses. UIBS 2010 is organized by the Georgia Tech Center for International Business Education & Research (CIBER), USA India Initiatives, Inc. (USIII) and US India Business and Research Center (USIBRC) with support of Georgia Department of Economic Development, the Metro Atlanta Chamber of Commerce, US Commercial Services, and the US Department of Commerce.\n\nUIBS 2010 is organized by Georgia Tech Center for International Business Education & Research (CIBER), USA India Initiatives, Inc. (USIII) and US India Business and Research Center (USIBRC) with support of Georgia Department of Economic Development, the Metro Atlanta Chamber of Commerce, U.S. Commercial Service of United States of America, Department of Commerce.\n\nThe UIBS mission statement is:\nThe Goal of UIBS is to promote business and investment between USA and India in key areas of technology, agriculture, healthcare, infrastructure, supply chain, logistics, real estate and investment etc. Our special emphasis is to connect entrepreneurs and small-to- medium size businesses.\n\n\n"}
