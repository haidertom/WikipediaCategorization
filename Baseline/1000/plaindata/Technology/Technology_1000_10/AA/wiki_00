{"id": "17504593", "url": "https://en.wikipedia.org/wiki?curid=17504593", "title": "1:1 pixel mapping", "text": "1:1 pixel mapping\n\n1:1 pixel mapping is a video display technique applicable to devices with native fixed pixels, such as LCD monitors and plasma displays. A monitor that has been set to 1:1 pixel mapping will display an input source without scaling it, such that each pixel received is mapped to a single native pixel on the monitor. This technique avoids loss of sharpness due to scaling artifacts and normally avoids incorrect aspect ratio due to stretching. If the input resolution is less than the monitor's native resolution, this will result in black borders around the image (e.g. letterboxing or windowboxing).\n\n"}
{"id": "22758286", "url": "https://en.wikipedia.org/wiki?curid=22758286", "title": "2degrees", "text": "2degrees\n\n2degrees is a telecommunications provider that operates in New Zealand. Its mobile network launched on 4 August 2009 after nine years of planning. 2degrees offers prepaid and pay-monthly monthly mobile services as well as fixed-line phone and broadband services.\n\nIt has spent over NZ$550 million building its mobile network, which covers Ashburton, Auckland, Christchurch, Dunedin, Hamilton, Hastings, Invercargill, Levin, Napier, Nelson, New Plymouth, Oamaru, Palmerston North, Queenstown, Rotorua, Taupo, Tauranga, Timaru, Wanganui, Wellington and Whangarei. The network works with GSM-900, UMTS-900 and UMTS-2100 mobiles, similar to the competing Vodafone network. In areas without 2degrees coverage, handsets roam on Vodafone NZ's GSM and UMTS network. 2degrees refers to areas where it has its own 3G coverage as \"mobile broadband zones\".\n\n2degrees is majority owned by US-based Trilogy International Partners.\n\nIn March 2015 2degrees announced it had acquired Snap,\na broadband-based ISP, and from 28 July began offering broadband and home-phone services in addition to existing mobile services.\n\nThe name of the company refers to a local variation of the six degrees of separation concept.\n\n2degrees was formerly known as NZ Communications and previously as Econet Wireless. Planning began in 2000 but details were not revealed until 11 May 2009 and pricing was announced a day before launch. 2degrees accepted its first customers 4 August 2009 for 2G calling/txting only. A year later on 3 August 2010 3G was turned on and new data plans announced for use in areas where 2degrees has its cell towers. 2degrees launched its 4G network in 2014.\n\n2degrees initially did not have nationwide mobile coverage, but its own network has been extended to many towns, cities and rural areas. Users can seamlessly roam onto Vodafone's network in places where 2degrees has no cell towers however 2degrees now has coverage of 97.5% of places New Zealanders live and work.\n\nRural Broadband Initiative (RBI): 2degrees mobile phones can roam onto RBI cell sites. These rural cell sites are open access for all internet providers in New Zealand to buy wholesale packages and retail them to rural customers for household and business use. 2Degrees mobile phones automatically roam to these cell sites were available due to the roaming agreement with Vodafone. RBI has Vodafone installing 154 new rural cell towers and upgrading 265 towers to provide 3G and later 4G services, between 2011 and 2017.\n\n2degrees towers have been deployed in these locations with 2G and 3G coverage (additionally 4G where noted):\n2degrees also operated a Wi-fi network in Wellington city. The network was on a trial with some selected members of the public (about 20,000 people).\n\nAs of 2014, 4G LTE services are on (band 3) 1800 MHz. In addition (band 28) 700 MHz is on trial in central Auckland; 700 MHz ought to be able to penetrate large buildings.\n\nOn 15 September 2017, 2degrees announced they would be shutting down the 2G network on 15 March 2018.\n\nIn some places where 2degrees has no telecommunications towers, users can roam on the Vodafone New Zealand 3G (UMTS) network. While roaming on the Vodafone network, users cannot use broadband \"zone data packs\" or \"data clock\"; but minutes, text messages and included \"NZ carryover plan data\", and \"NZ data packs\" can be used.\n\nThe company provides mobile services on its own cellular network. With support for 3G (UMTS 900MHz and 2100MHz) and 4G (LTE 700MHz, 900 MHz and 1800 MHz). WiFi Calling is also supported (handset dependent).\n\n2degrees have a number of new features not found on other New Zealand mobile phone networks, including:\n\nThe mobile network code is 530-24. NZ-24 or NZ Comms may be displayed on the mobile phones network list. On modern phones, with recent firmware 2degrees will be displayed.\n\nThe native STD prefix for the network is 022. New Zealand has mobile number portability, so customers switching from other networks may keep their existing mobile number.\n\n2degrees (still called NZ Communications on the Three website and Telstra roaming site) is open to customers with handsets from some foreign networks, including Three, Telstra and Orange UK. These foreign customers can place calls using 2degrees cell sites in cities, towns and localities New Zealand described as broadband zones by 2degrees.\n\nIn February 2011 2degrees announced that they had obtained financing for a further $100 million network expansion.\n\n2degrees have an ongoing network expansion in place, having recently secured financing to further expand its network and roll out a 4G LTE network.\n\nIn 1999, the New Zealand Government auctioned off 3G spectrum radio spectrum licence. Rangiaho Everton claimed that the auction breached the Treaty of Waitangi because she believed radio spectrum is taonga and the government has no right to sell it. Everton lodged a claim with the Waitangi Tribunal, which was upheld. It was not until Labour won the 1999 election that Māori were allocated one of the four 2 GHz 3G spectrum licences at a discounted price plus $5 million to develop it. In February 2001, Simon \"Tex\" Edwards, a former banker, established NZ Communications Limited. Later in 2001, NZ Communications received further financial backing from Strive Masiyiwa's Econet Wireless, which Edwards also owns shares, and then a 30% stake from the Hautaki Trust, which is the trading arm of the pan-Maori trust Te Hauarahi Tika. In 2007, NZ Communications Ltd began building towers for New Zealand's third mobile network.\n\nIn June 2008, Trilogy International Partners, which was established in 2005 by Strive Masiyiwa, John Stanton, Bradley Horwitz and others, purchased the 26% stake from Econet Wireless in NZ Communications Ltd.\n\nIn 2009, NZ Communications changed its name to 2degrees and began a roaming deal with Vodafone New Zealand. The deal allowed NZ Communications' customers to automatically roam onto Vodafone's 2G network. At the time the deal was announced, it was suggested the deal might also be expanded to include roaming on Vodafone's 3G network too at NZ Communications' request. Also in 2009, Trilogy increased its stake from 26% to 52% while the Hautaki Trust stake was reduced from 20% to 13%, and Eric Hertz replaced Mike Reynolds as CEO in July. \n\nIn mid 2009, 2degrees was owned by Trilogy International Partners, a US venture capital firm specializing in mobile networks (58.66%), Communication Venture Partners, a London-based company that invests in telecommunications and related software businesses (27.13%), Te Huarahi Tika Trust (10.17%) and KLR Hong Kong (0.50%). In July 2009, General Enterprise Management Services, a Hong Kong-based private equity fund, sold its 25.76 percent shares to Trilogy.\n\nIn 2012 when Tex Edwards stepped down as strategist, Trilogy owned a 58% stake in 2degrees, the Netherlands' Tesbrit BV owned a 32%, and the Hautaki Trust owned a 10% stake.\n\nOn 30 March 2013, 2degrees CEO Eric Hertz and his wife Kathy were killed when their twin-engine Beechcraft Baron, which was flying from Auckland to Timaru, ditched in the sea near Raglan at about 12:30pm after reporting engine failure. The plane was found at the bottom of the sea off the coast of Kawhia, 56 metres underwater, on 2 April. In a statement, Hertz' family thanked New Zealanders for their support. Hertz was succeeded as CEO of 2degrees by chairman Stewart Sherriff and Bradley Horwitz became chairman.\n\nIn 2016, Tex Edwards sold his remaining stake in 2degrees.\n\nIn early 2017, 2degrees has 23% of the wireless market in New Zealand and Trilogy International Partners owned a 73.2% stake in 2dregrees. Then, Canada's Trilogy International Partners sold its 63% stake to a new entity in which Trilogy International owns a 51% stake. Later, in mid 2017, Tesbrit BV was allowed to purchase up to a 49.9% stake in 2degrees.\n\nAugust 2018, CEO Stewart Sherriff announced his retirement from 2degrees.\n\nThe company is part of New Zealand Telecommunications Forum.\n\n2degrees has 56 retail stores, including fifteen throughout Auckland, one in Wellington City, one in Paraparaumu, four in Hamilton, two in Tauranga, two in Christchurch and one in Dunedin. The company also runs several smaller kiosk stores, which tend to be located in shopping centers. They also offer their products at 1,523,741 supermarkets, petrol stations and convenience stores.\n\n2degrees halved the prevalent pricing for prepay mobile in the New Zealand market, with voice calls costing 44 cents. SMS messages are charged at 9 cents. Customers will receive 300 to 500 free SMS messages per $30–$50 prepay top-up. Also, customers will receive a special rate of 22 cents for on-network and landline calls, as well as 2 cents per on-network SMS, provided they have topped up within the last 30 days.\n\nMobile Zone Data became available after 3G coverage was turned on. In regards to SIM swapping, it is worth noting that the customer must have a blank SIM card which may only be purchased from the following retailers: 2degrees Mobile (walk-in & online purchases), Harvey Norman, Noel Leeming, Warehouse Stationary and JB Hifi. 2degrees SIM cards purchased from stores such as supermarkets are not blank. 2degrees now also provides an online SIM swap option in which the 18-digit SIM number is required which can be found inscribed on the SIM itself.\n\n2degrees auctioned 85 special numbers on New Zealand auction website TradeMe for charity, raising over $65,000. The highest selling number was 022 888 8888, likely due to the number eight being considered lucky in some Asian cultures. New customers can choose their own number, on the 2degrees website.\n\n2degrees has run commercials featuring Rhys Darby, a comedian known for making jokes and sketches about New Zealand life. They were filmed on location by Film Construction Ltd, a television commercial and digital content production house in Auckland.\n\n\n"}
{"id": "37853", "url": "https://en.wikipedia.org/wiki?curid=37853", "title": "Bussard ramjet", "text": "Bussard ramjet\n\nThe Bussard ramjet is a theoretical method of spacecraft propulsion proposed in 1960 by the physicist Robert W. Bussard, popularized by Poul Anderson's novel \"Tau Zero\", Larry Niven in his \"Known Space\" series of books, Vernor Vinge in his \"Zones of Thought\" series, and referred to by Carl Sagan in the television series and book \"\".\n\nBussard proposed a ramjet variant of a fusion rocket capable of reasonable interstellar travel, using enormous electromagnetic fields (ranging from kilometers to many thousands of kilometers in diameter) as a ram scoop to collect and compress hydrogen from the interstellar medium. High speeds force the reactive mass into a progressively constricted magnetic field, compressing it until thermonuclear fusion occurs. The magnetic field then directs the energy as rocket exhaust opposite to the intended direction of travel, thereby accelerating the vessel.\n\nSince the time of Bussard's original proposal, it has been discovered that the region surrounding the Solar System has a much lower density of hydrogen than was believed at that time (see Local Interstellar Cloud). John Ford Fishback made an important contribution to the details for the Bussard ramjet in 1969, T. A. Heppenheimer analyzed Bussard's original suggestion of fusing protons, but found the Bremsstrahlung losses from compressing protons to fusion densities was greater than the power that could be produced by a factor of about 1 billion, thus indicating that the proposed version of the Bussard ramjet was infeasible. However Daniel P. Whitmire's 1975 analysis indicates that a ramjet may achieve net power via the CNO cycle, which produces fusion at a much higher rate (~10 times higher) than the proton-proton chain.\n\nRobert Zubrin and Dana Andrews analyzed one hypothetical version of the Bussard ramscoop and ramjet design in 1985. They determined that their version of the ramjet would be unable to accelerate into the solar wind. However, in their calculations they assumed that:\n\n\nIn the Zubrin/Andrews interplanetary ramjet design, they calculated that the drag force d/dt(\"mv\") equals the mass of the scooped ions collected per second multiplied by the velocity of the scooped ions within the solar system relative to the ramscoop. The velocity of the (scooped) collected ions from the solar wind was assumed to be 500,000 m/s.\n\nThe exhaust velocity of the ions when expelled by the ramjet was assumed not to exceed 100,000 m/s. The thrust of the ramjet d/dt(\"mv\") was equal to the mass of ions expelled per second multiplied by 100,000 meters per second. In the Zubrin/Andrews design of 1985, this resulted in the condition that d/dt(\"mv\") > d/dt(\"mv\"). This condition resulted in the drag force exceeding the thrust of the hypothetical ramjet in the Zubrin/Andrews version of the design.\n\nThe problem of using the interstellar medium as the sole fuel source led to study of the Ram Augmented Interstellar Rocket (RAIR). The RAIR carries its nuclear fuel supply and exhausts the reaction products to produce some of its thrust. However it greatly enhances its performance by scooping the interstellar medium and using this as extra reaction mass to augment the rocket. The propulsion system of the RAIR consists of three subsystems: a fusion reactor, a scoop field, and a plasma accelerator. The scoop field funnels interstellar gas into an \"accelerator\" (this could for example be a heat exchange system transferring thermal energy from the reactor directly to the interstellar gas) which is supplied power from a reactor. One of the best ways to understand this concept is to consider that the hydrogen nuclear fuel carried on board acts as a fuel (energy source) whereas the interstellar gas collected by the scoop and then exhausted at great speed from the back acts as a propellant (the reaction mass), the vehicle therefore has a limited fuel supply but an unlimited propellant supply. A normal Bussard ramjet would have an infinite supply of both, however theory suggests that where a Bussard ramjet would suffer drag from the fact that the interstellar gas ahead of it would have to be accelerated to its speed before entering the fusion reactor, whereas a RAIR system would be able to transfer energy via the \"accelerator\" mechanism from the reactor to the interstellar gas without having to accelerate the gas up to the ship's speed before putting this gas through the \"accelerator\", and so would suffer far less drag.\n\nBeamed energy coupled with a vehicle scooping hydrogen from the interstellar medium is another variant. A laser array in the solar system beams to a collector on a vehicle which uses something like a linear accelerator to produce thrust. This solves the fusion reactor problem for the ramjet. There are limitations because of the attenuation of beamed energy with distance.\n\nThe calculations (by Robert Zubrin and an associate) inspired the idea of a magnetic parachute or sail. This could be important for interstellar travel because it means that deceleration at the destination can be performed with a magnetic parachute rather than a rocket.\n\nSeveral of the obvious technical difficulties with the Bussard ramjet can be overcome by prelaunching fuel along the spacecraft's trajectory using something like a magnetic rail-gun.\n\nThe advantages of this system include\n\nThe major disadvantages of this system include\n\n\n"}
{"id": "4277314", "url": "https://en.wikipedia.org/wiki?curid=4277314", "title": "Cellular repeater", "text": "Cellular repeater\n\nA cellular repeater (also known as cell phone signal booster or amplifier) is a type of bi-directional amplifier used to improve cell phone reception. A cellular repeater system commonly consists of a donor antenna that receives and transmits signal from nearby cell towers, coaxial cables, a signal amplifier, and an indoor rebroadcast antenna.\n\nA \"donor antenna\" is typically installed by a window or on the roof a building and used to communicate back to a nearby cell tower. A donor antenna can be any of several types, but is usually directional or omnidirectional. An omnidirectional antenna (which broadcast in all directions) is typically used for a repeater system that amplify coverage for all cellular carriers. A directional antenna is used when a particular tower or carrier needs to be isolated for improvement. The use of a highly directional antenna can help improve the donor's signal-to-noise ratio, thus improving the quality of signal redistributed inside a building.\n\nSome cellular repeater systems can also include an omnidirectional antenna for rebroadcasting the signal indoors. Depending on attenuation from obstacles, the advantage of using an omnidirectional antenna is that the signal will be equally distributed in all directions.\n\nCelullar repeater systems include a signal amplifier. Standard GSM channel selective repeaters (operated by telecommunication operators for coverage of large areas and big buildings) have output power around 2 W, high power repeaters have output power around 10 W. The power gain is calculated by the following equation:\nA repeater needs to secure sufficient isolation between the donor and the service antenna. When the isolation is lower than actual gain plus a margin (of typically 5–15 dB), the repeater may go into in loop oscillation. This oscillation can cause interference to the cellular network.\n\nThe isolation may be improved by antenna type selection in a macro environment, which involves adjusting the angle between the donor and service antennas (ideally 180°), space separation (typically the vertical distance in the case of the tower installation between donor and service antenna is several meters), insertion into an attenuating environment (e.g. installing a metal mesh between donor and service antennas), and/or reduction of reflections (no near obstacles in front of the donor antenna such as trees or buildings).\n\nIsolation can be also improved by integrated feature called ICE (interference cancellation equipment) offered in some products (e.g., NodeG, RFWindow). Activation of this feature has a negative impact on internal delay (higher delay => approximately +5 μs up to standard rep. delay) and consequently a shorter radius from donor site. Amplification and filtering introduce a delay (typically between 5 and 15 μs), depending on the type of repeater and features used. Additional distance also adds propagation delay.\n\nIn many rural areas the housing density is too low to make construction of a new base station commercially viable. Installing a home cellular repeater may remedy this. In flat rural areas the signal is unlikely to suffer from multipath interference. \n\nCertain construction materials can attenuate cell phone signal strength. Older buildings, such as churches, often block celullar signals. Any building that has a significant thickness of concrete, or a large amount of metal used in its construction, will attenuate the signal. Concrete floors are often poured onto a metal pan, which completely blocks most radio signals. Some solid foam insulation and some fiberglass insulation used in roofs or exterior walls have foil backing, which can reduce transmittance. Energy efficient windows and metal window screens are also very effective at blocking radio signals. Some materials have peaks in their absorption spectra, which decrease signal strength.\n\nLarge buildings, such as warehouses, hospitals, and factories, often lack cellular reception. Low signal strength also tends to occur in underground areas (such as basements, and in shops and restaurants located towards the centre of shopping malls). In these cases, an external antenna is usually used.\n\nEven in urban areas (which usually have strong cellular signals throughout), there may be dead zones caused by destructive interference of waves. These usually have an area of a few blocks and will usually only affect one of the two frequency ranges used by cell phones. This happens because different wavelengths of the different frequencies interfere destructively at different points. Directional antennas can be helpful at overcoming this issue since they may be used to select a single path from several (see Multipath interference for more details).\n\nThe longer wavelengths have the advantage of diffracting more, and so line of sight is not as necessary to obtain a good signal. Because the frequencies that cell phones use are too high to reflect off the ionosphere as shortwave radio waves do, cell phone waves cannot travel via the ionosphere. (See Diffraction and Attenuation for more details).\n\nRepeaters are available for all of the GSM frequency bands. Some repeaters will handle different types of networks (such as multi-mode GSM and UMTS). Repeater systems are available for certain Satellite phone systems, allowing these to be used indoors without a clear line of sight to the satellite.\n\nIt used to be legal to use the low power devices available for home and small scale use in commercial areas (offices, shops, bars, etc.).\n\nOn February 20, 2013, the FCC released a Report & Order, thus establishing two Safe Harbors and defining the use of \"network safe\" consumer boosters on licensed spectrum. The Safe Harbors represent a compromise solution between Technology Manufacturers and Wireless Operators. Only a few companies have a product compatible with the new FCC regulations.\n\nThe FCC has defined two types of repeaters:\n\n\nThese new rules by the FCC were implemented on March 1, 2014. Here are the rules.\n\nIn May 2011, Ofcom stated the following:\n\nInstallation or use of repeater devices (as with any radio equipment) is a criminal offence unless two conditions are satisfied:\n\nUnder WT Act 2006 section 1.15, the wireless act also allows an exemption if the device does not \"involve undue interference with wireless telegraphy\". This is expected to follow the US-style regulations where a mobile repeater must have protection built in against interference.\n\nOfcom stated that \"Repeater devices transmit or re-transmit in the cellular frequency bands. Only the mobile network operators are licensed to use equipment that transmits in these bands. Installation or use of repeater devices by anyone without a licence is a criminal offence under Section 8 of the WT Act 2006.\" Repeaters operating in rural and less densely populated areas do not pose a quantifiable problem.\n"}
{"id": "418017", "url": "https://en.wikipedia.org/wiki?curid=418017", "title": "Changelog", "text": "Changelog\n\nA changelog is a log or record of all notable changes made to a project. The project is often a website or software project, and the changelog usually includes records of changes such as bug fixes, new features, etc. Some open source projects include a changelog as one of the top level files in their distribution.\n\nA changelog has historically included all changes made to a project. The \"Keep a CHANGELOG\" site instead advocates that a changelog \"not\" include \"all\" changes, but that it should instead contain \"a curated, chronologically ordered list of notable changes for each version of a project\" and should not be a \"dump\" of a git log \"because this helps nobody\".\n\nAlthough the canonical naming convention for the file is \"ChangeLog\", it is sometimes alternatively named as \"CHANGES\" or \"HISTORY\" (\"NEWS\" is usually a different file reflecting changes between releases, not between the commits). Another convention is to call it a CHANGELOG. Some project maintainers will append a \".txt\" suffix to the file name if the changelog is plain text, or a \".md\" suffix if it is in Markdown.\n\nSome revision control systems are able to generate the relevant information that is suited as a changelog if the goal is to include all changes.\n\nChangelog files are organized by paragraphs, which define a unique change within a function or file.\nThe GNU Coding standards recommend the following format:\nNote that between the date and the name, and again between the name and the email address, there are two spaces each. It is common to enclose the email address in < and >. The Emacs Editor creates such entries when creating additional changelog entries.\n\nMost Wiki software includes \"changelogs\" as a fundamental feature (often called \"history\" in this context). For example, the \"View History\" link at the top of a Wikipedia entry links to that page's changelog. This feature is vital for complying with the attribution requirements of some copyright licenses.\n\nA product changelog allows keeping customers in the loop about what's new. It helps to announce new features, latest releases, and relevant news directly in-app.\n\n\n"}
{"id": "5044343", "url": "https://en.wikipedia.org/wiki?curid=5044343", "title": "Citizen Cyborg", "text": "Citizen Cyborg\n\nCitizen Cyborg: Why Democratic Societies Must Respond to the Redesigned Human of the Future is a 2004 non-fiction book by bioethicist and sociologist James Hughes, which articulates democratic transhumanism as a socio-political ideology and program.\n\nThe editors of the popular science magazine \"Scientific American\" recommended \"Citizen Cyborg\" in their April 2005 issue.\n\n\n\nReviews\n"}
{"id": "498635", "url": "https://en.wikipedia.org/wiki?curid=498635", "title": "Citizen journalism", "text": "Citizen journalism\n\nThe concept of citizen journalism (also known as \"public\", \"participatory\", \"democratic\", \"guerrilla\" or \"street\" journalism) is based upon public citizens \"playing an active role in the process of collecting, reporting, analyzing, and disseminating news and information.\" Similarly, Courtney C. Radsch defines citizen journalism \"as an alternative and activist form of news gathering and reporting that functions outside mainstream media institutions, often as a response to shortcomings in the professional journalistic field, that uses similar journalistic practices but is driven by different objectives and ideals and relies on alternative sources of legitimacy than traditional or mainstream journalism\". Jay Rosen proposes a simpler definition: \"When the people formerly known as the audience employ the press tools they have in their possession to inform one another.\"\n\nCitizen journalism should not be confused with Community journalism or Civic journalism, both of which are practiced by professional journalists; Collaborative journalism which is the practice of professional and non-professional journalists working together; and Social journalism that denotes a digital publication with a hybrid of professional and non-professional journalism.\n\nCitizen journalism is a specific form of both citizen media and user-generated content. By juxtaposing the term \"citizen\", with its attendant qualities of civic-mindedness and social responsibility, with that of \"journalism\", which refers to a particular profession, Courtney C. Radsch argues that this term best describes this particular form of \"online\" and \"digital\" journalism conducted by amateurs, because it underscores the link between the practice of journalism and its relation to the political and public sphere.\n\nNew media technology, such as social networking and media-sharing websites, in addition to the increasing prevalence of cellular telephones, have made citizen journalism more accessible to people worldwide. Recent advances in new media have started to have a profound political impact. Due to the availability of technology, citizens often can report breaking news more quickly than traditional media reporters. Notable examples of citizen journalism reporting from major world events are, the 2010 Haiti earthquake, the Arab Spring, the Occupy Wall Street movement, the 2013 protests in Turkey, the Euromaidan events in Ukraine, and Syrian Civil War and the 2014 Ferguson unrest.\n\nCritics of the phenomenon, including professional journalists and news organizations, claim that citizen journalism is unregulated, too subjective, amateur, and haphazard in quality and coverage.\n\nCitizen journalism, as a form of alternative media, presents a \"radical challenge to the professionalized and institutionalized practices of the mainstream media\".\n\nAccording to Terry Flew, there have been three elements critical to the rise of citizen journalism: open publishing, collaborative editing, and distributed content. Mark Glaser, a freelance journalist who frequently writes on new media issues, said in 2006:\n\nThe idea behind citizen journalism is that people without professional journalism training can use the tools of modern technology and the global distribution of the Internet to create, augment or fact-check media on their own or in collaboration with others. For example, you might write about a city council meeting on your blog or in an online forum. Or you could fact-check a newspaper article from the mainstream media and point out factual errors or bias on your blog. Or you might snap a digital photo of a newsworthy event happening in your town and post it online. Or you might videotape a similar event and post it on a site such as YouTube. The accessibility of online media has also enhanced the interest for journalism among youth and many websites, like 'Far and Wide' a publication focusing on travel and international culture, as well as WorldWeekly a news blog covering a range of topics from world politics to science, are founded and run by students.\n\nIn \"What is Participatory Journalism?\", J. D. Lasica classifies media for citizen journalism into the following types:\n\n\nThe literature of citizen, alternative, and participatory journalism is most often situated in a democratic context and theorized as a response to corporate news media dominated by an economic logic. Some scholars have sought to extend the study of citizen journalism beyond the Western, developed world, including Sylvia Moretzsohn, Courtney C. Radsch, and Clemencia Rodríguez. Radsch, for example, wrote that \"Throughout the Arab world, citizen journalists have emerged as the vanguard of new social movements dedicated to promoting human rights and democratic values.\"\n\nOne criticism of the term \"citizen journalism\" to describe this concept is that the word \"citizen\" has a conterminous relation to the nation-state. The fact that many millions of people are considered stateless and often, are without citizenship (such as refugees or immigrants without papers) limits the concept to those recognised only by governments. Additionally, the global nature of many participatory media initiatives, such as the Independent Media Center, makes talking of journalism in relation to a particular nation-state largely redundant as its production and dissemination do not recognise national boundaries.\nSome additional names given to the concept based on this analysis are, \"grassroots media,\" \"people's media,\" or \"participatory media.\"\n\nAccording to Vincent Campbell, theories of citizenship can be categorized into two core groups: those that consider journalism \"for\" citizenship and those that consider journalism \"as\" citizenship. The classical model of citizenship is the base of the two theories of citizenship. The classical model is rooted in the ideology of informed citizens and places emphasis on the role of journalists rather than on citizens. The classical model has 4 main characteristics: journalists' role of informing citizens; citizens are assumed to be informed if they regularly attend to the news they are supplied with; more informed citizens are more likely to participate; and the more informed citizens participate, the more democratic a state is more likely to be.\n\nThe first characteristic suggesting that the role of journalism is to inform citizens upholds the theory that journalism is \"for\" citizens. One of the main issues with this first theory is that there is a normative judgement surrounding the amount and nature of information that citizens should have as well as what the relationship between the two should be. One branch of journalism \"for\" citizens is the \"monitorial citizen\" coined by Michael Schudson. The \"monitorial citizen\" suggests that citizens appropriately and strategically select what news and information they consume. The \"monitorial citizen\" along with other forms of this ideology conceive individuals as those who do things with information to enact change and citizenship. However, this production of information does not equal to an act of citizenship, but instead an act of journalism. Therefore, citizens and journalists are portrayed as distinctive roles whereas journalism is used by citizens \"for\" citizenship and conversely, journalists serve citizens.\n\nThe second theory considers journalism \"as\" citizenship. This theory focuses on the different aspects of citizen identity and activity and understands citizen journalism as directly constituting citizenship. The term \"liquid citizenship\" coined by Zizi Papacharissi depicts how the lifestyles that individuals engage in allow them to interact with other individuals and organizations, which thus remaps the conceptual periphery of civic, political, and social. This \"liquid citizenship\" allows the interactions and experiences that individuals face to become citizen journalism where they create their own forms of journalism. An alternative approach of journalism as citizenship rests between the distinction between \"dutiful\" citizens and \"actualizing\" citizens. \"Dutiful\" citizens engage in traditional citizenship practices, while \"actualizing\" citizens engage in non-traditional citizenship practices. This alternative approach suggests that \"actualizing\" citizens are less likely to use traditional media and more likely to use online and social media as sources of information, discussion, and participation. Thus, journalism in the form of online and social media practices become a form of citizenship for actualizing citizens.\n\nCriticisms have been made against citizen journalism, especially from among professionals in the field. Citizen journalists are often portrayed as unreliable, biased and untrained – as opposed to professionals who have \"recognition, paid work, unionized labour and behaviour that is often politically neutral and unaffiliated, at least in the claim if not in the actuality\".\nCitizen journalists gather material by being on the streets. Their tools can be narrowed down to a camera, social media and an instinct to start recording whenever something seems newsworthy or out of order. Much of their knowledge regarding the issues that are raised are obtained through their experience as a part of the community.\n\nHowever, some major news reporting agencies, threatened by the speed with which news is reported and delivered by citizen journalism, have launched campaigns to bring in readers and financial support. For example, Bill Johnson, president of Embarcadero Media, which publishes several northern California newspapers, issued an online statement asking readers to subscribe to local newspapers in order to keep them financially solvent. Johnson put special emphasis on the critical role played by local newspapers, which, he argues, \"reflect the values of the residents and businesses, challenge assumptions, and shine a light on our imperfections and aspirations.\"\n\nThe idea that every citizen can engage in acts of journalism has a long history in the United States. The contemporary citizen journalist movement emerged after journalists began to question the predictability of their coverage of events such as the 1988 U.S. presidential election. Those journalists became part of the public, or civic, journalism movement, which sought to counter the erosion of trust in the news media and the widespread disillusionment with politics and civic affairs.\n\nInitially, discussions of public journalism focused on promoting journalism that was \"for the people\" by changing the way professional reporters did their work. According to Leonard Witt, however, early public journalism efforts were \"often part of 'special projects' that were expensive, time-consuming, and episodic. Too often these projects dealt with an issue and moved on. Professional journalists were driving the discussion. They would have the goal of doing a story on welfare-to-work (or the environment, or traffic problems, or the economy), and then they would recruit a cross-section of citizens and chronicle their points of view. Since not all reporters and editors bought into this form of public journalism, and some outright opposed it, reaching out to the people from the newsroom was never an easy task.\" By 2003, in fact, the movement seemed to be petering out, with the Pew Center for Civic Journalism closing its doors.\n\nTraditionally, the term \"citizen journalism\" has had a history of struggle with deliberating on a concise and mutually agreed upon definition. Even today, the term lacks a clear form of conceptualization. Although the term lacks conceptualization, alternative names of the term are unable to comprehensively capture the phenomenon. For example, one of the interchangeable names with \"citizen journalism\" is \"user-generated content\" (UGC). However, the issue with this alternative term is that it eliminates the potential civic virtues of citizen journalism and considers it to be stunted and proprietorial.\n\nWith today's technology the citizen journalist movement has found new life as the average person can capture news and distribute it globally. As Yochai Benkler has noted, \"the capacity to make meaning – to encode and decode humanly meaningful statements – and the capacity to communicate one's meaning around the world, are held by, or readily available to, at least many hundreds of millions of users around the globe.\" Professor Mary-Rose Papandrea, a constitutional law professor at Boston College, notes in her article, \"Citizen Journalism and the Reporter's Privilege,\" that:\n\nIn 1999, activists in Seattle created a response to the WTO meeting being held there. These activists understood the only way they could get into the corporate media was by blocking the streets. Then they realized that a scant 60 seconds of coverage would show them being carted off by the police, but without any context to explain why they were protesting. They knew they had to create an alternative media model.\n\nSince then, the Indymedia movement has experienced exponential growth, and IMCs have been created in more than 200 cities all over the world.\nSimultaneously, journalism \"by the people\" began to flourish, enabled by emerging internet and networking technologies, such as weblogs, chat rooms, message boards, wikis, and mobile computing. A relatively new development is the use of convergent polls, allowing editorials and opinions to be submitted and voted on. Over time, the poll converges on the most broadly accepted editorials and opinions. In South Korea, OhmyNews became popular and commercially successful with the motto, \"Every Citizen is a Reporter.\" Founded by Oh Yeon-ho on February 22, 2000, it has a staff of 40 or more traditional reporters and editors who write about 20% of its content, with the rest coming from other freelance contributors who mostly are ordinary citizens. OhmyNews now has an estimated 50,000 contributors, and has been credited with transforming South Korea's conservative political environment.\n\nIn 2000, The Raven launched a Web television station aimed at participatory journalism, reporting on events in the Daytona Beach area. In 2001, themeparkinsider.com became the first online publication to win a major journalism award for a feature that was reported and written entirely by readers, earning an Online Journalism Award from the Online News Association and Columbia Graduate School of Journalism for its \"Accident Watch\" section, where readers tracked injury accidents at theme parks and shared accident prevention tips.\n\nDuring the 2004 U.S. presidential election, both the Democratic and Republican parties issued press credentials to citizen bloggers covering the convention, marking a new level of influence and credibility for nontraditional journalists. Some bloggers also began \"watchdogging\" the work of conventional journalists, monitoring their work for biases and inaccuracy.\n\nA recent trend in citizen journalism has been the emergence of what blogger Jeff Jarvis terms hyperlocal journalism, as online news sites invite contributions from local residents of their subscription areas, who often report on topics that conventional newspapers tend to ignore. \"We are the traditional journalism model turned upside down,\" explains Mary Lou Fulton, the publisher of the Northwest Voice in Bakersfield, California. \"Instead of being the gatekeeper, telling people that what's important to them 'isn't news', we're just opening up the gates and letting people come on in. We are a better community newspaper for having thousands of readers who serve as the eyes and ears for the Voice, rather than having everything filtered through the views of a small group of reporters and editors.\"\n\nIn June 2009, hundreds of thousands of Iranians took to the streets of Tehran, to protest the election outcome. Through citizen journalism and technological developments, the internet and social media were considered to be large sparks of this movements, although there was a lack of real outcomes politically.\n\nCitizen journalism played a role in the uprisings of the Arab Spring A study of women cyber-activists in several Arab countries found that \"a significant proportion of cyberactivism revolves around influencing the mainstream media agenda, as an increasingly symbiotic relationship between citizen and professional journalism has developed throughout the Arab Spring.\"\n\nOccupy protests were influenced by live interactive media coverage through citizen journalists such as Tim Pool, Jerry Nelson and The Citizen Journals on Facebook.\n\nAccording to Jay Rosen, citizen journalists are \"the people formerly known as the audience,\" who \"\"were\" on the receiving end of a media system that ran one way, in a broadcasting pattern, with high entry fees and a few firms competing to speak very loudly while the rest of the population listened in isolation from one another— and who \"today\" are not in a situation like that \"at all\". ... The people formerly known as the audience are simply the public made realer, less fictional, more able, less predictable.\"\n\nAbraham Zapruder, who filmed the assassination of President John Fitzgerald Kennedy with a home-movie camera, is sometimes presented as an ancestor to citizen journalists. Egyptian citizen Wael Abbas was awarded several international reporting prizes for his blog Misr Digital (Digital Egypt) and a video he publicized of two policemen beating a bus driver helped lead to their conviction.\n\nPublic Journalism is now being explored via new media, such as the use of mobile telephones. Mobile telephones have the potential to transform reporting and places the power of reporting in the hands of the public. Mobile telephony provides low-cost options for people to set up news operations.\n\nDuring 9/11 many eyewitness accounts of the terrorist attacks on the World Trade Center came from citizen journalists. Images and stories from citizen journalists close to the World Trade Center offered content that played a major role in the story.\n\nIn 2004, when the 9.1-magnitude underwater earthquake caused a huge tsunami in Banda Aceh Indonesia and across the Indian Ocean, a weblog-based virtual network of previously unrelated bloggers emerged that covered the news in real-time, and became a vital source for the traditional media for the first week after the tsunami.\nA large amount of news footage from many people who experienced the tsunami was widely broadcast,\nas well as a good deal of \"on the scene\" citizen reporting and blogger analysis that was subsequently picked up by the major media outlets worldwide.\nSubsequent to the citizen journalism coverage of the disaster and aftermath, researchers have suggested that citizen journalists may, in fact, play a critical role in the disaster warning system itself, potentially with higher reliability than the networks of tsunami warning equipment based on technology alone which then require interpretation by disinterested third parties.\n\nThe microblog Twitter played an important role during the 2009 Iranian election protests, after foreign journalists had effectively been \"barred from reporting\". Twitter delayed scheduled maintenance during the protests that would have shut down coverage in Iran due to the role it played in public communication.\n\nSometimes citizen journalists are, at the same time, bloggers and after some time they often become professional journalists, just as Paweł Rogaliński, a prized Polish blogger and journalist did.\n\nToday, individually produced citizen journalism exists in the form of social media platforms such as blogs, YouTube, and Twitter. These social media platforms encourage and facilitate engagement with other citizens who participate in creating content through commenting, liking, linking, and sharing. This practice is considered to be the 21st century version of individualized citizen journalism. The first wave of this type of citizen journalism came about in the form of amateur news bloggers. These bloggers often created content and narrative that challenged and critiqued the mainstream news outlets. The majority of the content produced by these amateur news bloggers was not actually original content, but curated information that was primarily monitored and edited by these various bloggers. However, recently there has been a decline in the amateur news blogger due to social media platforms that are much easier to run and maintain. These social media platforms allow individuals to easily share and create and content.\n\nWikimedia Foundation hosts a participatory journalism web site, Wikinews. The website allows contributors to write news which undergo a peer review prior to publications in some language editions (English, German, Russian) but not in others (Norwegian).\n\nCitizen journalists also may be activists within the communities they write about. This has drawn some criticism from traditional media institutions such as \"The New York Times\", which have accused proponents of public journalism of abandoning the traditional goal of objectivity. Many traditional journalists view citizen journalism with some skepticism, believing that only trained journalists can understand the exactitude and ethics involved in reporting news. See, e.g., Nicholas Lemann, Vincent Maher, and Tom Grubisich.\n\nAn academic paper by Vincent Maher, the head of the New Media Lab at Rhodes University, outlined several weaknesses in the claims made by citizen journalists, in terms of the \"three deadly E's\", referring to ethics, economics, and epistemology.\n\nAn analysis by language and linguistics professor, Patricia Bou-Franch, found that some citizen journalists resorted to abuse-sustaining discourses naturalizing violence against women. She found that these discourses were then challenged by others who questioned the gendered ideologies of male violence against women.\n\nAn article in 2005 by Tom Grubisich reviewed ten new citizen journalism sites and found many of them lacking in quality and content. Grubisich followed up a year later with, \"Potemkin Village Redux.\" He found that the best sites had improved editorially and were even nearing profitability, but only by not expensing editorial costs. Also according to the article, the sites with the weakest editorial content were able to expand aggressively because they had stronger financial resources.\n\nAnother article published on Pressthink examined Backfence, a citizen journalism site with three initial locations in the D.C. area, which reveals that the site has only attracted limited citizen contributions. The author concludes that, \"in fact, clicking through Backfence's pages feels like frontier land -– remote, often lonely, zoned for people but not home to any. The site recently launched for Arlington, Virginia. However, without more settlers, Backfence may wind up creating more ghost towns.\"\n\nDavid Simon, a former Baltimore Sun reporter and writer-producer of the popular television series, \"The Wire,\" criticized the concept of citizen journalism—claiming that unpaid bloggers who write as a hobby cannot replace trained, professional, seasoned journalists.\nAn editorial published by \"The Digital Journalist\" web magazine expressed a similar position, advocating to abolish the term \"citizen journalist\", and replacing it with \"citizen news gatherer\".\nWhile the fact that citizen journalists can report in real time and are not subject to oversight opens them to criticism about the accuracy of their reporting, news stories presented by mainstream media also misreport facts occasionally that are reported correctly by citizen journalists. As low as 32% of the American population have a fair amount of trust in the media.\n\nJournalism has been affected significantly due to citizen journalism. This is because citizen journalism allows people to post as much content as they want, whenever they want. In order to stay competitive, traditional news sources are forcing their journalist to compete. This means that journalist now have to write, edit and add pictures into their content and they must do so at a rapid pace, as it is perceived by news companies that it's essential for journalist to produce content at the same rate that citizens can post content on the internet. This is hard though, as many news companies are facing budget cuts and cannot afford to pay journalists the proper amount for the amount of work they do. Despite the uncertainties of a job in journalism and rising tuition costs there has been a 35% increase in journalism majors throughout the past few years according to Astra Taylor in her book The People's Platform.\n\nEdward Greenberg, a New York City litigator, notes higher vulnerability of unprofessional journalists in court compared to the professional ones:\nThe view stated above does not mean that professional journalists are fully protected by shield laws. In the 1972 Branzburg v. Hayes case the Supreme Court of the United States invalidated the use of the First Amendment as a defense for reporters summoned to testify before a grand jury. In 2005, the reporter's privilege of Judith Miller and Matthew Cooper was rejected by the appellate court.\n\nCitizen journalism has largely increased during the last decade of the twentieth century and throughout the twenty-first century. This rise of participation can be associated with the creation of the internet which introduced new ways in communicating and engaging news. Due to this shift in technology, individuals were able to access more news than previously and at a much faster rate. This larger quantity also made it so there was a larger variety of sources which people were able to consume media and news.\n\nNatalie Fenton discusses the role of citizen journalism within the digital age and has three characteristics associated with the topic: speed and space, multiplicity and poly-centrality and interactivity and participation. These characteristics were due to the invention of the internet, which, made way for amateur and citizen journalist to make a name for themselves within the industry. This was happening throughout the 1990s, however, once the mid 2000s began, the introduction of technologies such as the smartphone increased the ability to access the internet and made it so that individuals were able to use it globally and on the go. With these technological advancements, individuals were able to participate in journalism, like never before. Pictures or videos could be uploaded online in a matter of minutes and this paved the way for social media to grow as a strong producer in the industry.\n\nIn 2017, there are a number of different social media platforms through which people can access for their news. Many large corporations have even started to shift their focus onto internet sites, such as Facebook or YouTube and this has also made it easier for the existence of Alternative Media groups to exist. This transition into a digital realm of media has created many new possibilities for people to participate in journalism and it is due to the technological advancements such as the internet and smartphones.\n\nAs society continues to move forward in a digital age new possibilities in the realm of technology emerge and can be associated with the journalism industry. New devices such as Virtual Reality, open new avenues, which media companies and people will be able to participate with journalism. As society continues to move towards embracing technology as part of their lives, citizen journalism should increase in accessibility and participation.\n\nDan Gillmor, the former technology columnist for the \"San Jose Mercury News\", is one of the foremost proponents of citizen journalism and founded a nonprofit, the Center for Citizen Media, to help promote it.\n\nThe Canadian Broadcasting Corporation's French-language television network also has organized a weekly public affairs program called, \"5 sur 5\", which has been organizing and promoting citizen-based journalism since 2001. On the program, viewers submit questions on a wide variety of topics, and they, accompanied by staff journalists, get to interview experts to obtain answers to their questions.\n\nJay Rosen, a journalism professor at New York University, was one of public journalism's earliest proponents. From 1993 to 1997, he directed the \"Project on Public Life and the Press\", funded by the Knight Foundation and housed at NYU. He also currently runs the PressThink weblog.\n\nProfessor Charles Nesson, William F. Weld Professor of Law at Harvard Law School and the founder of the Berkman Center for Internet & Society, chairs the Advisory Board for Jamaican citizen journalism startup On the Ground News Reports.\n\nOne of the leading proponents for citizen journalism in Australia is Margo Kingston, author and former political journalist for the \"Sydney Morning Herald\". Kingston launched one of the world's first mainstream citizen journalism platforms, Webdiary, in 2000, well before \"The New York Times\", \"The Washington Post\" and \"The Guardian\". Kingston resigned from Webdiary in 2005 but the site continues and has been preserved in Pandora, Australia's National Web Archive. After a period of retirement, Kingston returned to citizen journalism in 2013 by co-publishing a new site No Fibs. It was on this site that Kingston published an exclusive story that the Australian Prime Minister, Tony Abbott, had inappropriately claimed expenses for promoting his book.\n\nIn March 2014, blogger and novelist James Wesley Rawles launched a web site that provides free press credentials for citizen journalists called the Constitution First Amendment Press Association (CFAPA). According to David Sheets of the Society for Professional Journalists, Rawles keeps no records on who gets these credentials.\n\nMaurice Ali, a citizen journalist from Canada, founded one of the first international citizen journalist associations called the International Association of Independent Journalists Inc. (IAIJ) in 2003. The association through its President (Maurice Ali) have published studies and articles on citizen journalism, attended and spoken at UNESCO and United Nations events as advocates of citizen journalism worldwide.\n\n"}
{"id": "10551079", "url": "https://en.wikipedia.org/wiki?curid=10551079", "title": "Comparison of recording mediums", "text": "Comparison of recording mediums\n\nThis article details a comparison of audio recording mediums.\n\nThe typical duration of a vinyl album is about 15 to 25 minutes per side. Classical music and spoken word recordings can extend to over 30 minutes on a side. If a side exceeds the average time, the maximum groove amplitude is reduced to make room for the additional program material. This can cause hiss in the sound from lower quality amplifiers when the volume is turned up to compensate for the lower recorded level. An extreme example, Todd Rundgren's \"Initiation\" LP, with 36 minutes of music on one side, has a \"technical note\" at the bottom of the inner sleeve: \"if the sound does not seem loud enough on your system, try re-recording the music onto tape.\" The total of around 40–45 minutes often influences the arrangement of tracks, with the preferred positions being the opening and closing tracks of each side.\n\nAlthough the term EP is commonly used to describe a 7\" single with more than two tracks, technically they are not different from a normal 7\" single. The EP uses reduced dynamic range and a smaller run-off groove area to extend the playing time. However, there are examples of singles, such as The Beatles' \"Hey Jude\" or Queen's \"Bohemian Rhapsody\", which are six minutes long or more. (in 1989, RCA released 'Dreamtime' by the band Love and Rockets, which clocks at 8:40). These longer recordings would require the same technical approach as an EP. The term EP has also been used for 10\" 45 rpm records, typically containing a reduced number of tracks.\n\nVinyl albums have a large 12\" (30 cm) album cover, which also allows cover designers scope for imaginative designs, often including fold-outs and leaflets.\n\n\n\n"}
{"id": "2471637", "url": "https://en.wikipedia.org/wiki?curid=2471637", "title": "Comparison of the AK-47 and M16", "text": "Comparison of the AK-47 and M16\n\nThe two most common assault rifles in the world are the Soviet AK-47 and the American M16. These Cold War-era rifles have been used in conflicts both large and small since the 1960s. They are used by military, police, security forces, revolutionaries, terrorists, criminals and civilians alike and will most likely continue to be used for decades to come. As a result, they have been the subject of countless comparisons and endless debate.\n\nThe AK-47 was finalized, adopted and entered widespread service in the Soviet Army in the early 1950s. Its firepower, ease of use, low production costs, and reliability were perfectly suited for the Soviet Army's new mobile warfare doctrines. More AK-type weapons have been produced than all other assault rifles combined. In 1974, the Soviets began replacing their AK-47 and AKM rifles with a newer design, the AK-74, which uses 5.45×39mm ammunition.\n\nThe M16 entered U.S. service in the mid-1960s. Despite its early failures, the M16 proved to be a revolutionary design and stands as the longest continuously serving rifle in American military history. It is a benchmark against which other assault rifles are judged. The U.S. Military has largely replaced the M16 in combat units with a shorter and lighter version called the M4 carbine.\n\nThe Germans were the first to pioneer the assault rifle concept, during World War II, based upon research that showed that most firefights happen within 400 meters and that contemporary rifles were over-powered for most small arms combat. They would soon develop a select-fire intermediate powered rifle combining the firepower of a submachine gun with the range and accuracy of a rifle.\n\nThe result was the Sturmgewehr 44, which the Germans produced in large numbers; approximately half a million were made. It fired a new and revolutionary intermediate powered cartridge, the 7.92×33mm Kurz. This new cartridge was developed by shortening the standard 7.92×57mm Mauser round and giving it a lighter 125-grain bullet, that limited range but allowed for more controllable automatic fire. A smaller lighter cartridge also allowed soldiers to carry more ammunition \"to support the higher consumption rate of automatic fire.\"\n\nThe Sturmgewehr 44 features an inexpensive, easy-to-make, stamped steel design and a 30-round detachable box magazine. \"This weapon was the prototype of all successful automatic rifles. Characteristically (and unlike previous rifles and the M-14) it had a straight stock with the barrel under the gas cylinder to reduce the turning moment of recoil of the rifle in the shoulder and thus help reduce the tendency of shots to climb in automatic fire. The barrel and overall length were shorter than a traditional rifle and it had a pistol grip to hold the weapon more securely in automatic fire. The principle of this weapon — the reduction of muzzle impulse to get usable automatic fire within the actual ranges of combat — was probably the most important advance in small arms since the invention of smokeless powder.\"\n\nLike the Germans, the Soviets were influenced by experience showing most combat happens within 400 meters and that their soldiers were consistently outgunned by heavily armed German troops, especially those armed with the Sturmgewehr 44 assault rifles. On July 15, 1943, a Sturmgewehr was demonstrated before the People's Commissariat of Arms of the USSR. The Soviets were so impressed with the Sturmgewehr, that they immediately set about developing an intermediate caliber automatic rifle of their own, to replace the badly outdated Mosin–Nagant bolt-action rifles and PPSh-41 submachine guns that armed most of the Soviet Army.\n\nThe Soviets soon developed the 7.62×39mm M43 cartridge, the semi-automatic SKS carbine and the RPD light machine gun. Shortly after World War II, the Soviets developed the AK-47 assault rifle, which would quickly replace the SKS in Soviet service. The AK-47 was finalized, adopted and entered widespread service in the Soviet army in the early 1950s. Its firepower, ease of use, low production costs, and reliability were perfectly suited for the Red Army's new mobile warfare doctrines. In the 1960s, the Soviets introduced the RPK light machine gun, itself an AK-47 type weapon with a bi-pod, a stronger receiver, and a longer, heavier barrel that would eventually replace the RPD light machine gun.\n\nThe AK-47 was widely supplied or sold to nations allied with the USSR, and the blueprints were shared with several friendly nations (the People's Republic of China standing out among these with the Type 56). As a result, more AK-type weapons have been produced than all other assault rifles combined. \"Of the estimated 500 million firearms worldwide, approximately 100 million belong to the Kalashnikov family, three-quarters of which are AK-47s.\"\n\nOn the other hand, the U.S. Army was influenced by combat experience with semi-automatic weapons such as the M1 Garand and M1 Carbine, which enjoyed a significant advantage over enemies armed primarily with bolt-action rifles. Although U.S. Army studies of World War II combat accounts had very similar results to that of the Germans and Soviets, the U.S. Army failed to recognize the importance of the assault rifle concept, and instead maintained its traditional views and preference for high-powered semi-automatic rifles. At the time, the U.S. Army believed that the Sturmgewehr 44 was \"intended in a general way to serve the same purpose as the U.S. carbine\" and was in many ways inferior to the M1 carbine, and was of \"little importance\".\n\nAfter World War II, the United States military started looking for a single automatic rifle to replace the M1 Garand, M1/M2 Carbines, M1918 Browning Automatic Rifle, M3 \"Grease Gun\" and Thompson submachine gun. However, early experiments with select-fire versions of the M1 Garand proved disappointing. During the Korean War, the select-fire M2 Carbine largely replaced the submachine gun in US service and became the most widely used Carbine variant. However, combat experience suggested that the .30 Carbine round was under-powered. American weapons designers reached the same conclusion as the Germans and Soviets: an intermediate round was necessary, and recommended a small-caliber, high-velocity cartridge.\n\nHowever, senior American commanders having faced fanatical enemies and experienced major logistical problems during WWII and the Korean War, insisted that a single powerful .30 caliber cartridge be developed, that could not only be used by the new automatic rifle, but by the new general-purpose machine gun (GPMG) in concurrent development. This culminated in the development of the 7.62×51mm NATO cartridge.\n\nThe United States Army then began testing several rifles to replace the obsolete M1 Garand. Springfield Armory's T44E4 and heavier T44E5 were essentially updated versions of the Garand chambered for the new 7.62 mm round, while Fabrique Nationale submitted their FN FAL as the T48. ArmaLite entered the competition late, hurriedly submitting several AR-10 prototype rifles in the fall of 1956 to the United States Army's Springfield Armory for testing.\n\nThe AR-10 featured an innovative straight-line barrel/stock design, forged aluminum alloy receivers and with phenolic composite stocks. It had rugged elevated sights, an oversized aluminum flash suppressor and recoil compensator, and an adjustable gas system. The final prototype, featured an upper and lower receiver with the now-familiar hinge and takedown pins, and the charging handle was on top of the receiver placed inside of the carry handle. For a 7.62mm NATO rifle, the AR-10 was incredibly lightweight at only 6.85 lbs. empty. Initial comments by Springfield Armory test staff were favorable, and some testers commented that the AR-10 was the best lightweight automatic rifle ever tested by the Armory.\n\nIn the end the United States Army chose the T44 now called the M14 rifle which was an improved M1 Garand with a 20-round magazine and automatic fire capability. The U.S. also adopted the M60 general purpose machine gun (GPMG). Its NATO partners adopted the FN FAL and HK G3 rifles, as well as the FN MAG and Rheinmetall MG3 GPMGs.\n\nThe first confrontations between the AK-47 and the M14 came in the early part of the Vietnam War. Battlefield reports indicated that the M14 was uncontrollable in full-auto and that soldiers could not carry enough ammo to maintain fire superiority over the AK-47. And, while the M2 Carbine offered a high rate of fire, it was under-powered and ultimately outclassed by the AK-47. A replacement was needed: A medium between the traditional preference for high-powered rifles such as the M14, and the lightweight firepower of the M2 Carbine.\n\nAs a result, the Army was forced to reconsider a 1957 request by General Willard G. Wyman, commander of the U.S. Continental Army Command (CONARC) to develop a .223 caliber (5.56 mm) select-fire rifle weighing 6 lbs (2.7 kg) when loaded with a 20-round magazine. The 5.56mm round had to penetrate a standard U.S. helmet at 500 yards (460 meters) and retain a velocity in excess of the speed of sound, while matching or exceeding the wounding ability of the .30 Carbine cartridge.\n\nThis request ultimately resulted in the development of a scaled-down version of the Armalite AR-10, called ArmaLite AR-15 rifle. However, despite overwhelming evidence that the AR-15 could bring more firepower to bear than the M14, the Army opposed the adoption of the new rifle. In January 1963, Secretary of Defense Robert McNamara concluded that the AR-15 was the superior weapon system and ordered a halt to M14 production. At the time, the AR-15 was the only rifle available that could fulfill the requirement of a universal infantry weapon for issue to all services.\n\nAfter modifications (most notably, the charging handle was re-located from under the carrying handle like AR-10 to the rear of the receiver), the new redesigned rifle was subsequently adopted as the M16 Rifle. \"(The M16) was much lighter compared to the M14 it replaced, ultimately allowing Soldiers to carry more ammunition. The air-cooled, gas-operated, magazine-fed assault rifle was made of steel, aluminum alloy and composite plastics, truly cutting-edge for the time. Designed with full and semi-automatic capabilities, the weapon initially did not respond well to wet and dirty conditions, sometimes even jamming in combat. After a few minor modifications, the weapon gained in popularity among troops on the battlefield.\"\n\nDespite its early failures the M16 proved to be a revolutionary design and stands as the longest continuously serving rifle in American military history. It has been adopted by many U.S. allies and the 5.56×45mm NATO cartridge has become not only the NATO standard, but \"the standard assault-rifle cartridge in much of the world.\" It also led to the development of small-caliber high-velocity service rifles by every major army in the world, including the USSR and People's Republic of China. It is a benchmark against which other assault rifles are judged.\n\nThe M16 is a select-fire, 5.56×45mm, air-cooled, direct impingement gas-operated, magazine-fed rifle, with a rotating bolt and straight-line recoil design. It was designed above all else to be a lightweight assault rifle, and to fire a new lightweight, high velocity small caliber cartridge to allow the soldier to carry more ammunition. It was designed to be manufactured with the extensive use of aluminium and synthetic materials by state of the art Computer Numerical Control (CNC) automated machinery. The M16 is a Modular Weapon System, easily configured as an assault rifle, a carbine, a submachine gun and an open-bolt squad automatic weapon. It is easy to assemble, modify and repair using a few simple hand tools, and a flat surface to work on.\n\nAt peak production, Colt's manufacturing capacity was approximately 333,000 units per year The M16 continues to benefit from every advance in the CNC field. which allows more and more small manufacturers to mass-produce M16s and semi-automatic AR-15 type rifles. The M16's aluminum lower receivers may be forged or cast. Their receivers may also be made from titanium and a variety of other metallic alloys, composites or polymers. If necessary, the M16 can be machined from a billet of steel and fitted with wooden furniture. The M16's internal components such as the bolt carrier group and charging handle may also be made of titanium. The M16's aluminum receiver and other parts may even be 3D printed, allowing \"people with no gunsmith training to assemble a working assault rifle at home\". This makes the M16 ideal for market economy production, spread among many small manufacturers around the country, using a variety of materials and manufacturing methods; this ensures it would be nearly impossible to disrupt U.S. M16 rifle production in the case of a major conflict.\n\nAs of 2015, the United States military buys M4 Carbines for $647 (USD) per unit. Approximately 8 million M16 type rifles have been made worldwide.\n\nThe AK-47 is a select-fire, 7.62×39mm, air-cooled, long-stroke-piston gas-operated, magazine-fed rifle, with a rotating bolt. It was designed to be a simple, reliable automatic rifle that could be manufactured quickly and cheaply, using mass production methods that were state of the art in the Soviet Union during the late 1940s. The AK-47's barrel and bolt were milled out of a steel billet and hard chromed. Its receiver was originally designed to be stamped from sheet metal with a milled trunnion insert. However, there were many difficulties during the initial phase of production causing high rejection rates due to faulty receivers. Instead of halting production, a heavy forged steel machined receiver was substituted for the sheet metal receiver. This was a more costly and time consuming process, but advanced the program's development and accelerated production. The AK's furniture was simply made out of wood, which was a non-strategic material, and perfectly fits the Soviet manufacturing philosophy, where large manufacturing plants produce basic weapons in very large quantities.\n\nIn 1959, the sheet metal stamping process was perfected, simplifying production and reducing the weight of the rifle from to without magazine. Most of the AK type rifles in use today are of this lighter stamped-steel AKM variety. Over time, AK production has been simplified through the use of rivets, spot welding and by further reducing the number of machined parts.\n\nCurrent model AK's are made using modern manufacturing processes and have many component parts produced by investment casting. This method gives a detailed and accurate product with excellent metallurgical properties. They come in 7.62×39mm (AK-103), 5.45×39mm (AK-74M) and 5.56×45mm (AK-101), with cold hammer forged barrels. They are also made with the use of synthetic/plastic furniture, such as folding stocks, handguards and pistol-grips.\n\nAt peak production, Kalashnikov Concern (formerly Izhmash) can produce around 95 units per hour (about 832,000 units per year). Because of its stamped-steel design it is not possible to manufacture the AK-47 series efficiently in small plants, due to the large amount of metal stamping equipment needed for mass production. However, the milled-steel AK-47 has spawned a cottage industry of sorts and has been copied and manufactured (one gun at a time) in small shops around the world.\n\nAs of 2014, Kalashnikov Concern sells the AK-103 at a government price of $150 to $160 (USD) per unit. There are places around the world where an AK-47 type rifle can be purchased on the Black Market \"for as little as $6, or traded for a chicken or a sack of grain.\" Approximately 100 million AK-47 type rifles have been made worldwide.\n\n\"The AR-15/M16-series rifles are considered the finest human-engineered assault rifles in the world.\" The M16 is ergonomically superior to the AK-47 in most respects. It is much easier and faster to change magazines and get the M16 back into action than with the AK-47. This is due to several factors, such as perfectly located magazine release and bolt release buttons, a flared magazine well for fast magazine insertions, and the ability to simply insert the magazine into the M16 in a conventional manner, rather than the \"rock and lock\" method required with the AK-47. In addition, it is easier for an M16 user to keep the strong hand on the pistol-grip and sights on target while performing magazine changes than with the AK-47.\n\nThe M16 has a well designed safety lever located on the left side of the weapon that is easily manipulated by the user's thumb while maintaining a strong hold on the pistol-grip. With the AK-47 the safety is a large lever on the right side of the weapon that is not at all easy to manipulate. \"It is slow, uncomfortable and sometimes stiff to operate.\" For most users, the hand must come off the pistol-grip to either apply or disengage the safety. It also makes a \"loud and distinctive click\" when used. While the fire selector \"is considered by many as the main drawback of the whole AK design\", its most frequently criticized feature is its trigger mechanism. \"The Kalashnikov trigger system, conceptually derived from that of the U.S. .30 M1 Garand rifle, is all too often plagued with an objectionable, and sometimes quite painful, \"trigger slap\" and a creepy and unpredictable trigger pull.\"\n\nWith the proper mind-set, training and practice, soldiers armed with both the AK-47 and M16 are quite deadly. However, the M16's direct impingement gas operation system, straight-line recoil design and smaller caliber give it less recoil than the AK-47 and makes it easier to control in full-auto.\n\n\"The (M16's) Stoner system provides a very symmetric design that allows straight line movement of the operating components. This allows recoil forces to drive straight to the rear. Instead of connecting or other mechanical parts driving the system, high pressure gas performs this function, reducing the weight of moving parts and the rifle as a whole.\" The M16's straight-line recoil design, where the recoil spring is located in the stock directly behind the action, and serves the dual function of operating spring and recoil buffer. The stock being in line with the bore also reduces muzzle rise, especially during automatic fire. Because recoil does not significantly shift the point of aim, faster follow-up shots are possible and user fatigue is reduced. Also, current model M16 flash-suppressors also act as compensators to reduce recoil further.\n\nWith the AK-47's long-stroke piston gas system, the piston is mechanically fixed to the bolt group and moves through the entire operating cycle. The primary disadvantage to this system is the disruption of the point of aim due to the center of mass changing during the action cycle and energetic and abrupt stops at the beginning and end of bolt carrier travel. However, the AK-47's heavier weight and slower rate-of-fire do a good job of mitigating any disadvantage. In addition, newer AK-47 type rifles use a muzzle brake or compensator to reduce recoil. Some AK type rifles also have vertical foregrips to improve handling characteristics and to counter the effects of recoil.\n\n\"A longer rifle barrel has the advantages of a longer sight radius, theoretically allowing a shooter to obtain a higher degree of accuracy from the improved precision of the sights alone. A longer barrel also provides a longer path for the projectile to stabilize prior to exiting the barrel, while allotting a longer period of time for the propellant charge to act on the projectile, often resulting in higher muzzle velocities and more consistent trajectories. A long barrel inherently provides more mass available for heat transfer, increasing the heat transfer rate incurred between shots, in turn allotting less warpage in the barrel, helping to improve consistency (and ultimately accuracy).\"\n\nThe M16 has a 50.8 cm (20.0 in) barrel and a 500mm (19.75 inches) sight radius. The M16 uses an L-type flip, aperture rear sight and it is adjustable with two setting, 0 to 300 meters and 300 to 400 meters. The front sight is a post, adjustable for elevation in the field. The rear sight can be adjusted in the field for windage. The sights can be adjusted with a bullet tip and soldiers are trained to zero their own rifles. The sight picture is the same as the M14, M1 Garand, M1 Carbine and the M1917 Enfield. The M16 also has a \"Low Light Level Sight System\", which includes a front sight post with a small glass vial of (glow-in-the-dark) radioactive Tritium H3 and a larger aperture rear sight. The M16 can mount a scope on the carrying handle. With the advent of the M16A2, a new fully adjustable rear sight was added, allowing the rear sight to be dialed in for specific range settings between 300 and 800 meters and to allow windage adjustments without the need of a tool or cartridge. Current issue M16A4s and M4s have detachable carrying handles and use Picatinny rails which allow for the use of various scopes and sighting devices. The current United States Army and Air Force issue M4 Carbine comes with the M68 Close Combat Optic and Back-up Iron Sight. The United States Marine Corps uses the ACOG Rifle Combat Optic and the United States Navy uses EOTech Holographic Weapon Sight.\n\nThe AK-47 has a 41.5 cm (16.3 in) barrel and a 378mm (14.88 inches) sight radius. The AK-47 uses a notched rear tangent iron sight, it is adjustable and is calibrated in hundreds from 100 to 800 meters (100 to 1000 meters for AKM models). The front sight is a post adjustable for elevation in the field. Windage adjustment is done by the armory before issue. The \"fixed\" battle setting can be used for all ranges up to 300 meters. This \"point-blank range\" setting marked \"П\", allows the shooter to fire at close range targets without adjusting the sights. Longer range settings are intended for area suppression. These settings mirror the Mosin–Nagant and SKS rifles which the AK-47 replaced. Some AK type rifles have a front sight with a flip-up luminous dot that is calibrated at 50 meters, for improved night fighting. All current AK-47s (100 series), have a side rail for mounting a variety of scopes and sighting devices, such as the PSO-1 Optical Sniper Sight. However, their side folding stocks cannot be folded with the optics mounted.\nA brief comparison between cartridges reveals that the M16's lighter, higher-velocity 5.56×45mm cartridge has much better range and accuracy than the AK-47's heavier 7.62×39mm cartridge.\n\nThe M16 rifle is \"accurate beyond description\". Its light recoil, high-velocity and flat trajectory allow shooters to take head shots out to 300 meters. Newer M16s use the newer M855 cartridge increasing their effective range to 600 meters. They are also more accurate than their predecessors and are capable of shooting 1–3 inch groups at 100 yards. \"In Fallujah, Marines with ACOG-equipped M16A4s created a stir by taking so many head shots that until the wounds were closely examined, some observers thought the insurgents had been executed.\" The newest M855A1 EPR cartridge is even more accurate and during testing \"...has shown that, on average, 95 percent of the rounds will hit within an 8 × 8-inch target at 600 meters.\"\n\nThe AK-47's accuracy has always been considered to be \"good enough\" to hit an adult male torso out to about 300 meters. \"At 300 meters, expert shooters (firing AK-47s) at prone or at bench rest positions had difficulty putting ten consecutive rounds on target.\" Despite the Soviet engineers' best efforts and \"no matter the changes, the AK-47's accuracy could not be significantly improved; when it came to precise shooting, it was a stubbornly mediocre arm.\" Curiously, the newer stamped steel receiver AKM models are actually less accurate than their predecessors. \"There are advantages and disadvantages in both forged/milled receivers and stamped receivers. Milled/Forged Receivers are much more rigid, flexing less as the rifle is fired thus not hindering accuracy as much as stamped receivers. Stamped receivers on the other hand are a bit more rugged since it has some give in it and have less chances of having metal fatigue under heavy usage.\" As a result, the milled AK-47's are capable of shooting 3–5 inch groups at 100 yards, whereas the stamped AKM's are capable of shooting 4–6 inch groups at 100 yards. The best shooters are able to hit a man-sized target at 800 metres within five shots (firing from prone or supported position) or ten shots (standing).\n\nA brief comparison between cartridges reveals that the AK-47's heavier 7.62×39mm cartridge has much better penetration than the M16's lighter, higher-velocity 5.56×45mm cartridge. However, it also reveals that the M16's lighter, higher-velocity 5.56mm bullet has a tendency to fragment on impact causing larger wounds than the AK-47's heavier 7.62mm bullet, which does not fragment on impact.\n\nThe AK-47's heavier 7.62×39mm round has superior penetration when compared to the M16's lighter 5.56×45mm round and is better in circumstances where a soldier has to shoot through heavy foliage, walls or a common vehicle's metal body and into an opponent attempting to use these things as cover. The 7.62×39mm M43 projectile does not generally fragment in soft tissue and has an unusual tendency to remain intact even after making contact with bone. The 7.62×39mm round produces significant wounding in cases where the bullet tumbles in tissue, but produces relatively minor wounds in cases where the bullet exits before beginning to yaw. In the absence of yaw, the M43 round can pencil through tissue with relatively little injury and its wounding potential is limited to the small permanent wound channel the bullet itself makes.\n\nThe original ammunition for the M16 was the 5.56×45mm M193 round. When fired from a 20\" barrel at ranges of up to 100 meters, the thin-jacketed lead-cored round traveled fast enough (above 2900 ft/s) that the force of striking a human body would cause the round to yaw (or tumble) and fragment into about a dozen pieces of various sizes thus created wounds that were out of proportion to its caliber. These wounds were much larger than those produced by AK-47 and they were so devastating that many considered the M16 to be an inhumane weapon. As the 5.56mm round's velocity decreases, so does the number of fragments that it produces. The 5.56mm round does not normally fragment at distances beyond 200 meters or at velocities below 2500 ft/s, and its lethality becomes largely dependent on shot placement.\n\nIn March 1970, the U.S. recommended that all NATO forces adopt the 5.56×45mm cartridge. This shift represented a change in the philosophy of the military's long-held position about caliber size. By the mid 1970s, other armies were looking at M16-style weapons. A NATO standardization effort soon started and tests of various rounds were carried out starting in 1977. The U.S. offered the 5.56×45mm M193 round, but there were concerns about its penetration in the face of the wider introduction of body armor. In the end the Belgian 5.56×45mm SS109 round was chosen (STANAG 4172) in October 1980. The SS109 round was based on the U.S. cartridge but included a new stronger, heavier, 62 grain bullet design, with better long range performance and improved penetration (specifically, to consistently penetrate the side of a steel helmet at 600 meters). Due to its design and lower muzzle velocity (about 3110 ft/s) the Belgian SS109 round is considered more humane because it is less likely to fragment than the U.S. M193 round. The NATO 5.56×45mm standard ammunition produced for U.S. forces is designated M855.\n\nMost, if not all, of the 7.62×39mm ammunition found today is of the upgraded M67 variety. This variety deleted the steel insert, shifting the center of gravity rearward and allowing the projectile to destabilize (or yaw) at about , nearly earlier in tissue than the M43 round. This change also reduces penetration in ballistic gelatin to ≈ for the newer M67 round verses ≈ for the older M43 round. However, like the M43, the wounding potential of M67 is mostly limited to the small permanent wound channel the bullet itself makes, especially when the bullet yaws (tumbles).\n\nThere is now relative parity between the wounding capacity of the M67 and the current M855 5.56×45mm round. However, there have been repeated and consistent reports of the M855's inability to wound effectively (i.e. fragment) when fired from the short barreled M4 carbine (even at close ranges). The M4's 14.5\" barrel length reduces muzzle velocity to about 2900 ft/s. This reduced wounding ability is one reason that, despite the Army's transition to short-barrel M4's, the Marine Corps has decided to continue using the M16A4 with its 20″ barrel as the 5.56×45mm M855 is largely dependent upon high velocity in order to wound effectively.\n\nIn 2003, the U.S. Army contended that the lack of lethality of the 5.56×45mm was more a matter of perception than fact. With good shot placement to the head and chest, the target was usually defeated without issue. The majority of failures were the result of hitting the target in non-vital areas such as extremities. However, a minority of failures occurred in spite of multiple hits to the chest. In 2006, a study found that 20% of soldiers using the M4 Carbine wanted more lethality or stopping power. In June 2010, the United States Army announced it began shipping its new 5.56mm, lead-free, M855A1 Enhanced Performance Round to active combat zones. This upgrade is designed to maximize performance of the 5.56×45mm round, to extend range, improve accuracy, increase penetration and to consistently fragment in soft-tissue when fired from not only standard length M16s, but also the short-barreled M4 carbines. The U.S. Army was so impressed with the M855A1 EPR round that they also developed the 7.62×51mm M80A1 EPR version.\n\nDuring the 1990s, the Russians developed the AK-101 in 5.56×45mm NATO for the world export market. In addition, Bulgaria, Hungary, Poland and Yugoslavia (i.e. Serbia) have also rechambered their locally produced AK variants to 5.56mm NATO. And, Finland, Israel, South Africa and Sweden have made AK type rifles in 5.56×45mm since the 1970s. \n\nRates of fire\n\nBoth the AK-47 and the M16 are select-fire weapons capable of firing in semi-automatic and full-auto, or semi-auto and 3-round-burst for the later model M16s. However, the semi-auto and 3-round-burst capability of the M16A2 and M4 models have less combat capability than their predecessors or AK-47 type rifles. This is due to the elimination of full-auto mode of fire In addition, the burst mechanism does not recycle; if one or two rounds are fired because the trigger is not held long enough, the next pull of the trigger will not result in a three-round burst, but will result in one or two shots being fired. The M4A1 and HK416 have abandoned the 3-round-burst capability and returned to the more traditional semi-automatic and full-auto modes of fire, while the M16A4 retains the 3-round burst mode.\n\nAvailable firepower\n\nThe standard magazine capacity for both the AK-47 and M16 type rifles is 30 rounds, although lower and higher capacity magazines are available for both systems. However, the single most limiting factor in terms of firepower is the amount of ammunition that a soldier can carry. A soldier armed with an M16 can carry far more ammo than a soldier armed with an AK-47. Assuming a maximum 10 kilogram ammo-load...\n\nAdditional firepower\n\nNeither the AK-47 nor the M16 were designed to mount accessories, except of course for their respective bayonets and a simple clamp type bipod for the M16. However, with the advent of the Picatinny rail and by sheer happenstance, the M16 has proven itself to be a remarkably adaptable weapon system, capable of mounting a wide range of accessories, including grenade launchers, fore-grips, removable carry handle/rear sight assemblies, bipods, laser systems, electronic sights, night vision, tactical lights, etc. The AK-47 can also use Picatinny rail mounted accessories, although its design and smaller fore-stock make it less adaptable.\n\nIn addition, the M16 is \"the Swiss Army knife of rifles\" a modular weapon system whose components can be arranged in a variety of different configuration. For example, an M16A2 with its standard iron sights and a standard fore-stock can be easily converted, in a matter of seconds and without the use of tools to an M16A4 with Picatinny rails, optical sights and a variety of accessories. This is accomplished by simply pushing in two pins, removing the A2 upper receiver/barrel and replacing it with an A4 upper receiver/barrel. Or, an M16A4 Rifle can be converted to an M4 Carbine in a few minutes by replacing the upper receiver/barrel and using simple hand-tools to replace the fixed buttstock with a telescoping buttstock. As such, the M16 can be easily converted into different calibers and different types of weapons. The AK-47 has no such capability.\n\nToday, bayonets are rarely used in combat. However, both the AK-47 and M16 retain bayonet lugs and bayonets are still issued. Also, bayonets are still used for controlling prisoners and as a weapon of \"last resort\". In addition, some authorities have concluded that bayonets serve as useful training aids in building morale and increasing desired aggression in troops.\n\nThe M16 is 44.25 inches (1124mm) long with an M7 bayonet attached. The M7 bayonet is based on earlier designs such as the M4, M5, & M6 bayonets, all of which are direct descendants of the M3 Fighting Knife and have spear-point blade with a half sharpened secondary edge. The newer M9 bayonet has a clip-point blade with saw teeth along the spine, and can be used as a multi-purpose knife and wire-cutter when combined with its scabbard. The current USMC OKC-3S bayonet bears a resemblance to the Marines' iconic Ka-Bar fighting knife with serrations near the handle.\n\nThe AK is 40.15 inches (1020mm) long with an AKM type bayonet attached. The AK-47 has an adequate but unremarkable bayonet. However, the AKM Type I bayonet (introduced in 1959) was a revolutionary design. It has a Bowie style (clip-point) blade with sawteeth along the spine, and can be used as a multi-purpose knife and wire-cutter when combined with its steel scabbard. This design was copied by other nations and formed the basis of the US M9 bayonet. The AK-74 bayonet (introduced in 1983) represents a further refinement of the AKM bayonet. \"It introduced a radical blade cross-section, that has a flat milled on one side near the edge and a corresponding flat milled on the opposite side near the false edge. The blade has a new spear point and an improved one-piece molded plastic grip making it a more effective fighting knife. It also has saw-teeth on the false edge and the usual hole for use as a wire-cutter. Some Chinese AK type rifles such as the Type 56 include an integral folding spike bayonet, similar to the SKS rifle.\n\nThe AK-47 has always enjoyed a reputation for rugged reliability and has a malfunction rate of one per 1000 rounds fired. It uses a long-stroke gas system, where the gas is sent from the barrel to push a piston attached to the bolt carrier, thus operating the action. The gas tube is fairly large and is visible above the barrel with ports or vents to allow the excess \"dirty\" gas to escape without affecting the action. The AK-47 is often built with generous clearances, allowing it to function easily in a dirty environment with little or no maintenance. This makes it reliable but less accurate.\n\nThe M16 has always had a reputation for poor reliability and has a malfunction rate of two per 1000 rounds fired. The M16 uses a unique gas powered operating system. \"This gas operating system works by passing high pressure propellant gasses tapped from the barrel down a tube and into the carrier group within the upper receiver, and is commonly but incorrectly referred to as a \"direct impingement gas system\". The gas expands within a donut shaped gas cylinder within the carrier. Because the bolt is prevented from moving forward by the barrel, the carrier is driven to the rear by the expanding gasses and thus converts the energy of the gas to movement of the rifle’s parts. The bolt bears a piston head and the cavity in the bolt carrier is the piston sleeve. It is more correct to call it an \"internal piston\" system.\" This design is much lighter and more compact than a gas-piston design. However, this design requires that combustion byproducts from the discharged cartridge be blown into the receiver as well. This accumulating carbon and vaporized metal build-up within the receiver and bolt-carrier negatively affects reliability and necessitates more intensive maintenance on the part of the individual soldier. The DI operation increases the amount of heat that is deposited in the receiver while firing the M16 and causes essential lubricant to be \"burned off\". This requires frequent and generous applications of appropriate lubricant. Lack of proper lubrication is the most common source of weapon stoppages or jams.\n\nThe original M16 fared poorly in the jungles of Vietnam and was infamous for reliability problems in the harsh environment. As a result, it became the target of a Congressional investigation. The investigation found that:\n\nWhen these issues were addressed and corrected by the M16A1, the reliability problems decreased greatly. According to a 1968 Department of Army report, the M16A1 rifle achieved widespread acceptance by U.S. troops in Vietnam. \"Most men armed with the M16 in Vietnam rated this rifle's performance high, however, many men entertained some misgivings about the M16's reliability. When asked what weapon they preferred to carry in combat, 85 percent indicated that they wanted either the M16 or its submachine gun version, the XM177E2. (The M14 was preferred by 15 percent, while less than one percent wished to carry either the Stoner rifle, the AK-47, the carbine or a pistol.)\" In March 1970, the \"President’s Blue Ribbon Defense Panel\" concluded that the issuance of the M16 saved the lives of 20,000 U.S. servicemen during the Vietnam War, who would have otherwise died had the M14 remained in service. However the M16 rifle's reputation continues to suffer.\n\nAfter the introduction of the M4 Carbine, it was found that the shorter barrel length of 14.5 inches also has a negative effect on reliability, as the gas port is located closer to the chamber than the gas port of the standard length M16 rifle: 7.5 inches instead of the 13 inches. This affects the M4’s timing and increases the amount of stress and heat on the critical components, thereby reducing reliability.<ref name=\"armalite.com\"/ In a 2002 assessment the USMC found that the M4 malfunctioned three times more often than the M16A4 (the M4 failed 186 times for 69,000 rounds fired, while the M16A4 failed 61 times). Thereafter, the Army and Colt worked to make modifications to the M4s and M16A4s in order to address the problems found.\n\nIn tests conducted in 2005 and 2006 the Army found that on average, the new M4s and M16s fired approximately 5,000 rounds between stoppages. In 2010, U.S. Marines operating in Afghanistan reported no reliability problems with their M16 rifles and M4 carbines. \"This is more so given the account of Chief Warrant Officer Joshua S. Smith, the Marine responsible for weapons training and performance in the Third Battalion, Sixth Marines, which is engaged in daily fighting in Marja. 'We've had nil in the way of problems; we've had no issues,' he said of the M-4s and M-16s. The battalion has about 350 M-16s and 700 M-4s, he said.\"\n\nThe newest version of the M16 in U.S. service is the HK416 (a.k.a. the M27 Infantry Automatic Rifle) which uses a proprietary gas system derived from the HK G36, replacing the direct impingement gas system used by the standard M16/M4. The HK system uses a short-stroke gas piston driving an operating rod to force the bolt carrier to the rear. This design prevents combustion gases from entering the weapon's interior, a shortcoming with direct impingement systems. The reduction in heat and fouling of the bolt carrier group increases the reliability of the weapon and extends the interval between stoppages. The short-stroke gas piston require less maintenance and cleaning. It reduces operator cleaning time and stress on critical components. \"Improving the service interval requirements provides a major benefit to soldiers that may not have the ability or opportunity to thoroughly clean their rifle. Also, the design of the external gas piston system is less susceptible to build up of other contaminants in extreme environments.\" During factory tests the HK416 fired 10,000 rounds in full-auto without malfunctioning.\n\n\"Magazines are one of the most important elements of any firearm design. They are responsible for the feeding portion of the cycle of operation. Even in the most proven arm that reliably extracts and ejects, the magazine has to be 100-percent reliable for it to consistently feed properly.\"\n\nThe AK-47’s 30-round magazines have a pronounced curve that allows them to smoothly feed ammunition into the chamber. Their heavy steel construction combined with \"feed-lips\" (the surfaces at the top of the magazine that control the angle at which the cartridge enters the chamber) machined from a single steel billet makes them highly resistant to damage. These magazines are so strong that \"Soldiers have been known to use their mags as hammers, and even bottle openers.\" This makes the AK-47 magazine more reliable, although heavier than U.S. and NATO magazines. The early slab-sided steel AK-47 magazines weigh empty. The later steel AKM magazines had lighter sheet-metal bodies with prominent reinforcing ribs weighing empty. The current issue steel-reinforced plastic magazines are even lighter, weighing empty. Early steel AK-47 magazines are 9.75 inches long, and the later ribbed steel AKM and newer plastic magazines are about an inch shorter.\n\nThe M16's magazine was meant to be a lightweight, disposable item. As such, it is made of pressed/stamped aluminum and was not designed to be durable. Therefore, it is easier to damage than an AK-47 magazine and the feed lips are proportionally weaker when compared to the AK-47. The M16 originally used a 20-round magazine which was later replaced by a bent 30-round design. As a result, the magazine follower tends to rock or tilt, causing malfunctions. Many non-U.S. and commercial magazines have been developed to effectively mitigate these shortcomings (e.g., H&K's all-stainless-steel magazine, Magpul's polymer P-MAG, etc.). Standard USGI aluminum 30-round M16 magazines weigh empty and are 7.1 inches long. The newer plastic magazines are about a half inch longer.\n\nIn 2009, the U.S. Military began fielding an \"improved magazine\" identified by a tan-colored follower. \"The new follower incorporates an extended rear leg and modified bullet protrusion for improved round stacking and orientation. The self-leveling/anti-tilt follower minimizes jamming while a wider spring coil profile creates even force distribution. The performance gains have not added weight or cost to the magazines.\"\n\nIn July 2016, the U.S. Army introduced the new Enhanced Performance Magazine. These new magazines have tan bodies and blue followers. \"Unlike previous magazines, the EPM uses a new, modified magazine body (and therefore not interchangeable, hence the different color) which presents the rounds at a more favorable angle to the rifle’s feedway, improving reliability and, importantly, preventing the hardened steel tips of new 5.56mm M855A1 Enhanced Performance Rounds from contacting the aluminum feed ramp section on M4 type rifles.\" This allows for a dramatic 300% increase in the number of rounds fired between stoppages.\n\nAK-47 type rifles are made in dozens of countries, with \"quality ranging from finely engineered weapons to pieces of questionable workmanship.\" As a result, the AK-47 has a service/system life of approximately 6,000, to 10,000, to 15,000 rounds. The AK-47 was designed to be a cheap, simple, easy to manufacture assault rifle, perfectly matching Soviet military doctrine that treats equipment and weapons as disposable items. As units are often deployed without adequate logistical support and dependent on \"battlefield cannibalization\" for resupply, it is actually more cost-effective to replace rather than repair weapons.\n\nBoth the AK-47 and the M16 have small parts and springs that need to be replaced every few thousand rounds. However...\"Every time (an AK) is disassembled beyond the field stripping stage, it will take some time for some parts to regain their fit, some parts may tend to shake loose and fall out when firing the weapon. Some parts of the AK-47 line are riveted together. Repairing these can be quite a hassle, since the end of the rivet has to be ground off and a new one set after the part is replaced.\"\n\nM16 type rifles are made by dozens of manufactures around the world, to the highest standards \"the goal of which is to ensure that products designed for military use meet the necessary requirements with regard to quality, durability, ruggedness, commonality, interchangeability, total cost of ownership, logistics and other military and defense-related objectives.\" The M16's barrel life is approximately 15,000 rounds for standard issue M16A4s and M4s. Cold hammer forged steel barrels such as those used on the HK416 have service life of 20,000 to 50,000 rounds depending on the intensity of use. A badly worn M16 barrel will cause the bullets to tumble in flight. However, the M16’s upper receiver/barrel may be swapped out in a matter of seconds, without the use of tools, simply by pushing out two pins. The M16 was designed to be a serviceable assault rifle, perfectly matching American military doctrine where units are resupplied on a continuous basis, and are expected to perform most of their own maintenance and repairs in the field. As such, American units are well supplied and are quickly provided with whatever spare-parts they need by their logistical support systems.\n\nAn M16 rifle that has been declared non-serviceable may be sent to a Small Arms Repair Facility, where it is overhauled, upgraded and returned to service. The M16 rifle may be recycled almost infinitely, as any individual part can be easily replaced until none of the original parts remain.\n\n\"The AK-74 assault rifle was a Soviet answer to the US M16.\" The Russians realized that the M16 had better range and accuracy over the AKM, and that its lighter cartridge allows soldiers to carry more ammunition. Therefore, in 1967, the USSR issued an official requirement to replace the AKM and the 7.62×39mm cartridge. They soon began to develop the AK-74 and the 5.45×39mm cartridge. AK-74 production began in 1974, and it was unveiled in 1977, when it was carried by Soviet parachute troops during the annual Red Square parade. It would soon replace the AKM and become the standard Soviet infantry rifle. In 1979, the AK-74 saw combat for the first time in Afghanistan.\n\nThe AK-74 is best described as a modified version of the 7.62×39mm AKM rifle. These modifications were primarily the result of converting the rifle to the 5.45×39mm cartridge, some early models are reported to have been rebarreled AKMs. The AK-74 and AKM share 9 assemblies and 52 parts (36% & 53% parts commonality, respectively).\n\nThe 5.45×39mm cartridge is much lighter than the 7.62×39mm round that it replaced, allowing soldiers to carry 1.5 times more ammunition. The AK-74 also offers improved range and accuracy over the AKM. However, the AK-74s range and accuracy is still \"inferior to most Western weapons,\" including current issue M16 type rifles.\n\nThe 5.45 mm bullet tumbles in soft tissue producing temporary cavities at a depth of 10 cm (3.9 in) and 35 cm (13.8 in). This effect is similar to, but more rapid than with 7.62×39mm cartridge. The 5.45mm round offers better penetration over the U.S. round. However, unlike its counterpart, the 5.45mm round \"does not deform or fragment when striking soft tissues.\" Nevertheless, during the Afghan war the Mujahedeen called the 5.45×39mm round the \"Poison Bullet\" due to the severe wounds it produced to extremities and the resulting need to amputate.\n\nThe following Summary has been taken directly from the\n\"Rifle Evaluation Study\", United States Army, Combat Development Command, ADA046961, 20 Dec 1962. Additional information can be found in \"Rifle Evaluation Study\", United States Army, Infantry Combat Developments Agency, ADA050268, 10 Dec 1962\".\n\nNote: This is the first time that the United States Army compared the AR-15/M16 and the AK-47.\n\nNight firing\n\nThe AR-15 was not equipped with any flash suppressor during the conduct of this test. Also, there was only a small amount of ammunition available for use in the AK-47. As a result, the night firing capability of both the AR-15 and AK-47 were not properly tested. In a subsequent test at Fort Benning an AR-15 equipped with a flash suppressor was tested against both the M14 and the AK-47. The AK-47 was not equipped with a flash suppressor.\n\n\n"}
{"id": "25862206", "url": "https://en.wikipedia.org/wiki?curid=25862206", "title": "Crisis camp", "text": "Crisis camp\n\nA crisis camp is a BarCamp gathering of IT professionals, software developers, and computer programmers to aid in the relief efforts of a major crisis such as those caused by earthquakes, floods, or hurricanes. Projects that crisis camps often work on include setting up social networks for people to locate missing friends and relatives, creating maps of affected areas, and creating inventories of needed items such as food and clothing.\n\nFollowing the 2010 Haiti earthquake, many crisis camps were set up around the world, often under the name \"Crisis Camp Haiti\", to help with the relief effort. \nDue to the 2011 Tōhoku earthquake and tsunami, the Crisis Commons volunteer community was mobilized and part of the effort is being coordinated by Japanese students at U.S. universities.\n\n"}
{"id": "34549363", "url": "https://en.wikipedia.org/wiki?curid=34549363", "title": "Critical code studies", "text": "Critical code studies\n\nCritical code studies (CCS) is an emerging academic subfield, related to software studies, digital humanities, cultural studies, computer science, human-computer interface, and the do-it-yourself maker culture. Its primary focus is on the cultural significance of computer code, without excluding or focusing solely upon the code's functional purpose.\n\nAs introduced by Mark C. Marino, critical code studies was initially a method by which scholars \"can read and explicate code the way we might explicate a work of literature,\" but the concept also draws upon Espen Aarseth's conception of a cybertext as a \"mechanical device for the production and consumption of verbal signs\" (Cybertext, 21), arguing that in order to understand a digital artifact we must also understand the constraints and capabilities of the authoring tools used by the creator of the artifact, as well as the memory storage and interface required for the user to experience the digital artifact.\n\nEvidence that critical code studies has gained momentum since 2006 include an article by Matthew Kirschenbaum in the Chronicle of Higher Education, CCS sessions at the Modern Language Association in 2011 that were \"packed\" with attendees, several academic conferences devoted wholly to critical code studies, and a book devoted to the explication of a single line of computer code, titled \"10 PRINT CHR$(205.5+RND(1)); : GOTO 10\".\n\n\n"}
{"id": "3568143", "url": "https://en.wikipedia.org/wiki?curid=3568143", "title": "DARPA XG", "text": "DARPA XG\n\nThe neXt Generation program or XG is a technology development project sponsored by DARPA's Strategic Technology Office, with the goals to \"develop both the enabling technologies and system concepts to dynamically redistribute allocated spectrum along with novel waveforms in order to provide dramatic improvements in assured military communications in support of a full range of worldwide deployments.\"\n\nIn the Wireless World Research Forum of 27 October 2003, Preston Marshall, program manager of DARPA XG Program, said \"The primary product of the XG program is not a new radio, but a set of advanced technologies for dynamic spectrum\naccess.\"\n\n\n"}
{"id": "13068104", "url": "https://en.wikipedia.org/wiki?curid=13068104", "title": "DITA Open Toolkit", "text": "DITA Open Toolkit\n\nDITA Open Toolkit (DITA-OT) is an open-source publishing engine for XML content authored in the Darwin Information Typing Architecture (DITA).\n\nThe toolkit's extensible plug-in mechanism allows users to add their own transformations and customize the default output, which includes: \n\nOriginally developed by IBM and released to open source in 2005, the distribution packages contain Ant, Apache FOP, Java, Saxon, and Xerces.\n\nMost DITA authoring tools and DITA CMSs integrate the DITA-OT, or parts of it, into their publishing workflows.\n\nStandalone tools have also been developed to run the DITA-OT via a graphical user interface instead of the command line.\n\n"}
{"id": "32244195", "url": "https://en.wikipedia.org/wiki?curid=32244195", "title": "DataSplice", "text": "DataSplice\n\nDataSplice, LLC is a mobile software company headquartered in Fort Collins, Colorado and offers mobile applications which extend enterprise systems, including packaged software for Enterprise Asset Management (EAM) and computerized maintenance management systems (CMMS). The software provides an interface from these systems to handhelds, smartphones, tablet computers and mobile computers. It may also be used on a desktop system as a unified/simplified interface for multiple systems.\nThe software offered is a mobile middleware, with an emphasis on IBM's Maximo EAM system. Its primary client base is focused on utilities, gas/oil, defense, aerospace and other markets which utilizes field service management systems which require tracking, asset management and regulatory accountability.\n\nDataSplice does not use a proprietary software platform, but rather utilizes the Common Language Infrastructure (CLI) of Microsoft.NET framework’s ADO.NET, which allows for connectivity to different database systems such as MySQL and Oracle. The extensible system consists of three components, including a remote client (for handheld and/or desktop use), a server which communicates with the primary EAM, and an administration client for configuring the system.\n\nOriginally known as Optimization Resources, which was founded in 1991, DataSplice was spun off as both a company and product in 2001. The original management and development staff continue to be engaged in daily operations. DataSplice is a privately held company.\n\nThe product's main emphasis is providing a simplified mobile interface into IBM Maximo. \nThe product consists of three components: Remote Client, Administration Client and the Server. The primary modules are Inventory, Work Orders, Inspections, Condition Monitoring and Asset Management.\n\nThe product's remote client, is HTML5 compliant, and as such is platform agnostic. Supported systems include iOS (iPad), Android (Droid), and Windows Mobile and 8 (Surface, Phone and Desktop). The remote client is also able to utilize bar code scanners, mobile printers, and RFID readers.\n\nIn 2010, DataSplice introduced InspecTMI, a field inspections and operator rounds mobile collection system geared toward highly regulated inspection scenarios, such as substations, power generation, and safety inspections.\n"}
{"id": "23108196", "url": "https://en.wikipedia.org/wiki?curid=23108196", "title": "Department for Business, Innovation and Skills", "text": "Department for Business, Innovation and Skills\n\nThe Department for Business, Innovation and Skills (BIS) was a ministerial department of the United Kingdom Government created on 5 June 2009 by the merger of the Department for Innovation, Universities and Skills (DIUS) and the Department for Business, Enterprise and Regulatory Reform (BERR). It was disbanded on the creation of the Department for Business, Energy and Industrial Strategy on 14 July 2016.\n\nFollowing the department's dissolution, it no longer has ministers responsible.\n\nThe Permanent Secretary was Sir Martin Donnelly.\n\nSome policies apply to England alone due to devolution, while others are not devolved and therefore apply to other nations of the United Kingdom. The department was responsible for UK Government policy in the following areas:\n\nEconomic policy is mostly devolved but several important policy areas are reserved to Westminster. Further and higher education policy is mostly devolved. Reserved and excepted matters are outlined below.\n\nScotland\n\nReserved matters:\n\n\nThe Scottish Government Economy and Education Directorates handle devolved economic and further and higher education policy respectively.\n\nNorthern Ireland\n\nReserved matters:\n\nExcepted matter:\n\nThe department's main counterparts are:\n\nWales\n\nUnder the Welsh devolution settlement, specific policy areas are transferred to the Welsh Government rather than reserved to Westminster.\n\nPrecursor departments:\n"}
{"id": "195113", "url": "https://en.wikipedia.org/wiki?curid=195113", "title": "Digital divide", "text": "Digital divide\n\nA digital divide is an economic and social inequality with regard to access to, use of, or impact of information and communication technologies (ICT). The divide within countries (such as the digital divide in the United States) may refer to inequalities between individuals, households, businesses, or geographic areas, usually at different socioeconomic levels or other demographic categories. The divide between differing countries or regions of the world is referred to as the global digital divide, examining this technological gap between developing and developed countries on an international scale.\n\nThe term \"digital divide\" describes a gap in terms of access to and usage of information and communication technology. It was traditionally considered to be a question of having or not having access, but with a global mobile phone penetration of over 95%, it is becoming a relative inequality between those who have more and less bandwidth and more or fewer skills. Conceptualizations of the digital divide have been described as \"who, with which characteristics, connects how to what\":\nDifferent authors focus on different aspects, which leads to a large variety of definitions of the digital divide. \"For example, counting with only 3 different choices of subjects (individuals, organizations, or countries), each with 4 characteristics (age, wealth, geography, sector), distinguishing between 3 levels of digital adoption (access, actual usage and effective adoption), and 6 types of technologies (fixed phone, mobile... Internet...), already results in 3x4x3x6 = 216 different ways to define the digital divide. Each one of them seems equally reasonable and depends on the objective pursued by the analyst\".\nThe \"digital divide\" is also referred to by a variety of other terms which have similar meanings, though may have a slightly different emphasis: digital inclusion, digital participation, basic digital skills, media literacy and digital accessibility.\n\nThe National Digital Inclusion Alliance, a US-based nonprofit organization, has found the term \"digital divide\" to be problematic, since there are a multiplicity of divides. Instead, they chosen to use the term \"digital inclusion\", providing a definition:\nDigital Inclusion refers to the activities necessary to ensure that all individuals and communities, including the most disadvantaged, have access to and use of Information and Communication Technologies (ICTs). This includes 5 elements: 1) affordable, robust broadband internet service; 2) internet-enabled devices that meet the needs of the user; 3) access to digital literacy training; 4) quality technical support; and 5) applications and online content designed to enable and encourage self-sufficiency, participation and collaboration.\n\nThe infrastructure by which individuals, households, businesses, and communities connect to the Internet address the physical mediums that people use to connect to the Internet such as desktop computers, laptops, basic mobile phones or smartphones, iPods or other MP3 players, gaming consoles such as Xbox or PlayStation, electronic book readers, and tablets such as iPads.\n\nTraditionally the nature of the divide has been measured in terms of the existing numbers of subscriptions and digital devices. Given the increasing number of such devices, some have concluded that the digital divide among individuals has increasingly been closing as the result of a natural and almost automatic process. Others point to persistent lower levels of connectivity among women, racial and ethnic minorities, people with lower incomes, rural residents, and less educated people as evidence that addressing inequalities in access to and use of the medium will require much more than the passing of time. Recent studies have measured the digital divide not in terms of technological devices, but in terms of the existing bandwidth per individual (in kbit/s per capita). \n\nAs shown in the Figure on the side, the digital divide in kbit/s is not monotonically decreasing, but re-opens up with each new innovation. For example, \"the massive diffusion of narrow-band Internet and mobile phones during the late 1990s\" increased digital inequality, as well as \"the initial introduction of broadband DSL and cable modems during 2003–2004 increased levels of inequality\". This is because a new kind of connectivity is never introduced instantaneously and uniformly to society as a whole at once, but diffuses slowly through social networks. As shown by the Figure, during the mid-2000s, communication capacity was more unequally distributed than during the late 1980s, when only fixed-line phones existed. The most recent increase in digital equality stems from the massive diffusion of the latest digital innovations (i.e. fixed and mobile broadband infrastructures, e.g. 3G and fiber optics FTTH).\nMeasurement methodologies of the digital divide, and more specifically an Integrated Iterative Approach General Framework (Integrated Contextual Iterative Approach – ICI) and the digital divide modeling theory under measurement model DDG (Digital Divide Gap) are used to analyze the gap existing between developed and developing countries, and the gap among the 27 members-states of the European Union.\n\nInstead of tracking various kinds of digital divides among fixed and mobile phones, narrow- and broadband Internet, digital TV, etc., it has recently been suggested to simply measure the amount of kbit/s per actor. This approach has shown that the digital divide in kbit/s per capita is actually widening in relative terms: \"While the average inhabitant of the developed world counted with some 40 kbit/s more than the average member of the information society in developing countries in 2001, this gap grew to over 3 Mbit/s per capita in 2010.\" \n\nThe upper graph of the Figure on the side shows that the divide between developed and developing countries has been diminishing when measured in terms of subscriptions per capita. In 2001, fixed-line telecommunication penetration reached 70% of society in developed OECD countries and 10% of the developing world. This resulted in a ratio of 7 to 1 (divide in relative terms) or a difference of 60% (divide in measured in absolute terms). During the next decade, fixed-line penetration stayed almost constant in OECD countries (at 70%), while the rest of the world started a catch-up, closing the divide to a ratio of 3.5 to 1. The lower graph shows the divide not in terms of ICT devices, but in terms of kbit/s per inhabitant. While the average member of developed countries counted with 29 kbit/s more than a person in developing countries in 2001, this difference got multiplied by a factor of one thousand (to a difference of 2900 kbit/s). In relative terms, the fixed-line capacity divide was even worse during the introduction of broadband Internet at the middle of the first decade of the 2000s, when the OECD counted with 20 times more capacity per capita than the rest of the world. This shows the importance of measuring the divide in terms of kbit/s, and not merely to count devices. The International Telecommunications Union concludes that \"the bit becomes a unifying variable enabling comparisons and aggregations across different kinds of communication technologies\".\n\nHowever, research shows that the digital divide is more than just an access issue and cannot be alleviated merely by providing the necessary equipment. There are at least three factors at play: information accessibility, information utilization and information receptiveness. More than just accessibility, individuals need to know how to make use of the information and communication tools once they exist within a community. Information professionals have the ability to help bridge the gap by providing reference and information services to help individuals learn and utilize the technologies to which they do have access, regardless of the economic status of the individual seeking help.\n\nInternet connectivity can be utilized at a variety of locations such as homes, offices, schools, libraries, public spaces, Internet cafe and others. There are also varying levels of connectivity in rural, suburban, and urban areas.\n\nCommon Sense Media, a nonprofit group based in San Francisco, surveyed almost 1,400 parents and reported in 2011 that 47 percent of families with incomes more than $75,000 had downloaded apps for their children, while only 14 percent of families earning less than $30,000 had done so.\n\nThe gap in a digital divide may exist for a number of reasons. Obtaining access to ICTs and using them actively has been linked to a number of demographic and socio-economic characteristics: among them income, education, race, gender, geographic location (urban-rural), age, skills, awareness, political, cultural and psychological attitudes. Multiple regression analysis across countries has shown that income levels and educational attainment are identified as providing the most powerful explanatory variables for ICT access and usage. Evidence was found that Caucasians are much more likely than non-Caucasians to own a computer as well as have access to the Internet in their homes. As for geographic location, people living in urban centers have more access and show more usage of computer services than those in rural areas. Gender was previously thought to provide an explanation for the digital divide, many thinking ICT were male gendered, but controlled statistical analysis has shown that income, education and employment act as confounding variables and that women with the same level of income, education and employment actually embrace ICT more than men (see Women and ICT4D). However, each nation has its own set of causes or the digital divide. For example, the digital divide in Germany is unique because it is not largely due to difference in quality of infrastructure.\n\nOne telling fact is that \"as income rises so does Internet use ...\", strongly suggesting that the digital divide persists at least in part due to income disparities. Most commonly, a digital divide stems from poverty and the economic barriers that limit resources and prevent people from obtaining or otherwise using newer technologies.\n\nIn research, while each explanation is examined, others must be controlled in order to eliminate interaction effects or mediating variables, but these explanations are meant to stand as general trends, not direct causes. Each component can be looked at from different angles, which leads to a myriad of ways to look at (or define) the digital divide. For example, measurements for the intensity of usage, such as incidence and frequency, vary by study. Some report usage as access to Internet and ICTs while others report usage as having previously connected to the Internet. Some studies focus on specific technologies, others on a combination (such as Infostate, proposed by Orbicom-UNESCO, the Digital Opportunity Index, or ITU's ICT Development Index). Based on different answers to the questions of who, with which kinds of characteristics, connects how and why, to what there are hundreds of alternatives ways to define the digital divide. \"The new consensus recognizes that the key question is not how to connect people to a specific network through a specific device, but how to extend the expected gains from new ICTs\". In short, the desired impact and \"the end justifies the definition\" of the digital divide.\n\nDuring the mid-1990s the US Department of Commerce, National Telecommunications & Information Administration (NTIA) began publishing reports about the Internet and access to and usage of the resource. The first of three reports is entitled \"Falling Through the Net: A Survey of the ‘Have Nots’ in Rural and Urban America\" (1995), the second is \"Falling Through the Net II: New Data on the Digital Divide\" (1998), and the final report \"Falling Through the Net: Defining the Digital Divide\" (1999). The NTIA’s final report attempted to clearly define the term digital divide; \"the digital divide—the divide between those with access to new technologies and those without—is now one of America's leading economic and civil rights issues. This report will help clarify which Americans are falling further behind, so that we can take concrete steps to redress this gap.\" Since the introduction of the NTIA reports, much of the early, relevant literature began to reference the NTIA’s digital divide definition. The digital divide is commonly defined as being between the \"haves\" and \"have-nots.\"\n\nThe Facebook Divide, a concept derived from the \"digital divide\", is the phenomenon with regard to access to, use of, or impact of Facebook on individual society and among societies. It is suggested at the International Conference on Management Practices for the New Economy (ICMAPRANE-17) on February 10–11, 2017. Additional concepts of Facebook Native and Facebook Immigrants are suggested at the conference. The Facebook Divide, Facebook native, Facebook immigrants, and Facebook left-behind are concepts for social and business management research. Facebook Immigrants are utilizing Facebook for their accumulation of both bonding and bridging social capital. These Facebook Native, Facebook Immigrants, and Facebook left-behind induced the situation of Facebook inequality. In February 2018, the Facebook Divide Index was introduced at the ICMAPRANE conference in Noida, India, to illustrate the Facebook Divide phenomenon.\n\nOvercoming the divide \n\nAn individual must be able to connect in order to achieve enhancement of social and cultural capital as well as achieve mass economic gains in productivity. Therefore, access is a necessary (but not sufficient) condition for overcoming the digital divide. Access to ICT meets significant challenges that stem from income restrictions. The borderline between ICT as a necessity good and ICT as a luxury good is roughly around the \"magical number\" of US$10 per person per month, or US$120 per year, which means that people consider ICT expenditure of US$120 per year as a basic necessity. Since more than 40% of the world population lives on less than US$2 per day, and around 20% live on less than US$1 per day (or less than US$365 per year), these income segments would have to spend one third of their income on ICT (120/365 = 33%). The global average of ICT spending is at a mere 3% of income. Potential solutions include driving down the costs of ICT, which includes low cost technologies and shared access through Telecentres.\n\nFurthermore, even though individuals might be capable of accessing the Internet, many are thwarted by barriers to entry such as a lack of means to infrastructure or the inability to comprehend the information that the Internet provides. Lack of adequate infrastructure and lack of knowledge are two major obstacles that impede mass connectivity. These barriers limit individuals' capabilities in what they can do and what they can achieve in accessing technology. Some individuals have the ability to connect, but they do not have the knowledge to use what information ICTs and Internet technologies provide them. This leads to a focus on capabilities and skills, as well as awareness to move from mere access to effective usage of ICT.\n\nThe United Nations is aiming to raise awareness of the divide by way of the World Information Society Day which has taken place yearly since May 17, 2006. It also set up the Information and Communications Technology (ICT) Task Force in November 2001. Later UN initiatives in this area are the World Summit on the Information Society, which was set up in 2003, and the Internet Governance Forum, set up in 2006.\n\nIn the year 2000, the United Nations Volunteers (UNV) programme launched its Online Volunteering service, which uses ICT as a vehicle for and in support of volunteering. It constitutes an example of a volunteering initiative that effectively contributes to bridge the digital divide. ICT-enabled volunteering has a clear added value for development. If more people collaborate online with more development institutions and initiatives, this will imply an increase in person-hours dedicated to development cooperation at essentially no additional cost. This is the most visible effect of online volunteering for human development.\n\nSocial media websites serve as both manifestations of and means by which to combat the digital divide. The former describes phenomena such as the divided users demographics that make up sites such as Facebook and Myspace or Word Press and Tumblr. Each of these sites host thriving communities that engage with otherwise marginalized populations. An example of this is the large online community devoted to Afrofuturism, a discourse that critiques dominant structures of power by merging themes of science fiction and blackness. Social media brings together minds that may not otherwise meet, allowing for the free exchange of ideas and empowerment of marginalized discourses.\n\nAttempts to bridge the digital divide include a program developed in Durban, South Africa, where very low access to technology and a lack of documented cultural heritage has motivated the creation of an \"online indigenous digital library as part of public library services.\" This project has the potential to narrow the digital divide by not only giving the people of the Durban area access to this digital resource, but also by incorporating the community members into the process of creating it.\n\nTo address the divide The Gates Foundation began the Gates Library Initiative. The Gates Foundation focused on providing more than just access, they placed computers and provided training in libraries. In this manner if users began to struggle while using a computer, the user was in a setting where assistance and guidance was available. Further, the Gates Library Initiative was \"modeled on the old-fashioned life preserver: The support needs to be around you to keep you afloat.\"\n\nIn nations where poverty compounds effects of the digital divide, programs are emerging to counter those trends. Prior conditions in Kenya—lack of funding, language and technology illiteracy contributed to an overall lack of computer skills and educational advancement for those citizens. This slowly began to change when foreign investment began. In the early 2000s, The Carnegie Foundation funded a revitalization project through the Kenya National Library Service (KNLS). Those resources enabled public libraries to provide information and communication technologies (ICT) to their patrons. In 2012, public libraries in the Busia and Kiberia communities introduced technology resources to supplement curriculum for primary schools. By 2013, the program expanded into ten schools.\n\nCommunity Informatics (CI) provides a somewhat different approach to addressing the digital divide by focusing on issues of \"use\" rather than simply \"access\". CI is concerned with ensuring the opportunity not only for ICT access at the community level but also, according to Michael Gurstein, that the means for the \"effective use\" of ICTs for community betterment and empowerment are available. Gurstein has also extended the discussion of the digital divide to include issues around access to and the use of \"open data\" and coined the term \"data divide\" to refer to this issue area.\n\nOnce an individual is connected, Internet connectivity and ICTs can enhance his or her future social and cultural capital. Social capital is acquired through repeated interactions with other individuals or groups of individuals. Connecting to the Internet creates another set of means by which to achieve repeated interactions. ICTs and Internet connectivity enable repeated interactions through access to social networks, chat rooms, and gaming sites. Once an individual has access to connectivity, obtains infrastructure by which to connect, and can understand and use the information that ICTs and connectivity provide, that individual is capable of becoming a \"digital citizen\".\n\nIn the United States, research provided by Sungard Availability Services notes a direct correlation between a company's access to technological advancements and its overall success in bolstering the economy. The study, which includes over 2,000 IT executives and staff officers, indicates that 69 percent of employees feel they do not have access to sufficient technology in order to make their jobs easier, while 63 percent of them believe the lack of technological mechanisms hinders their ability to develop new work skills. Additional analysis provides more evidence to show how the digital divide also affects the economy in places all over the world. A BCG Report suggests that in countries like Sweden, Switzerland, and the U.K., the digital connection among communities is made easier, allowing for their populations to obtain a much larger share of the economies via digital business. In fact, in these places, populations hold shares approximately 2.5 percentage points higher. During a meeting with the United Nations a Bangladesh representative expressed his concern that poor and undeveloped countries would be left behind due to a lack of funds to bridge the digital gap.\n\nThe digital divide also impacts children's ability to learn and grow in low-income school districts. Without Internet access, students are unable to cultivate necessary tech skills in order to understand today's dynamic economy. Federal Communication Commission's Broadband Task Force created a report showing that about 70% of teachers give students homework that demand access to broadband. Even more, approximately 65% of young scholars use the Internet at home to complete assignments as well as connect with teachers and other students via discussion boards and shared files. A recent study indicates that practically 50% of students say that they are unable to finish their homework due to an inability to either connect to the Internet, or in some cases, find a computer. This has led to a new revelation: 42% of students say they received a lower grade because of this disadvantage. Finally, according to research conducted by the Center for American Progress, \"if the United States were able to close the educational achievement gaps between native-born white children and black and Hispanic children, the U.S. economy would be 5.8 percent—or nearly $2.3 trillion—larger in 2050\".\n\nFurthermore, according to the 2012 Pew Report \"Digital Differences\", a mere 62% of households who make less than $30,000 a year use the Internet, while 90% of those making between $50,000 and $75,000 had access. Studies also show that only 51% of Hispanics and 49% of African Americans have high-speed Internet at home. This is compared to the 66% of Caucasians that too have high-speed Internet in their households. Overall, 10% of all Americans don't have access to high-speed Internet, an equivalent of almost 34 million people. Supplemented reports from the Guardian demonstrate the global effects of limiting technological developments in poorer nations, rather than simply the effects in the United States. Their study shows that the rapid digital expansion excludes those who find themselves in the lower class. 60% of the world's population, almost 4 billion people, have no access to the Internet and are thus left worse off.\n\nSince gender, age, racial, income, and educational gaps in the digital divide have lessened compared to past levels, some researchers suggest that the digital divide is shifting from a gap in access and connectivity to ICTs to a knowledge divide. A knowledge divide concerning technology presents the possibility that the gap has moved beyond access and having the resources to connect to ICTs to interpreting and understanding information presented once connected.\n\nThe second-level digital divide, also referred to as the production gap, describes the gap that separates the consumers of content on the Internet from the producers of content. As the technological digital divide is decreasing between those with access to the Internet and those without, the meaning of the term digital divide is evolving. Previously, digital divide research has focused on accessibility to the Internet and Internet consumption. However, with more and more of the population with access to the Internet, researchers are examining how people use the Internet to create content and what impact socioeconomics are having on user behavior.\nNew applications have made it possible for anyone with a computer and an Internet connection to be a creator of content, yet the majority of user generated content available widely on the Internet, like public blogs, is created by a small portion of the Internet using population. Web 2.0 technologies like Facebook, YouTube, Twitter, and Blogs enable users to participate online and create content without having to understand how the technology actually works, leading to an ever-increasing digital divide between those who have the skills and understanding to interact more fully with the technology and those who are passive consumers of it. Many are only nominal content creators through the use of Web 2.0, posting photos and status updates on Facebook, but not truly interacting with the technology.\n\nSome of the reasons for this production gap include material factors like the type of Internet connection one has and the frequency of access to the Internet. The more frequently a person has access to the Internet and the faster the connection, the more opportunities they have to gain the technology skills and the more time they have to be creative.\n\nOther reasons include cultural factors often associated with class and socioeconomic status. Users of lower socioeconomic status are less likely to participate in content creation due to disadvantages in education and lack of the necessary free time for the work involved in blog or web site creation and maintenance. Additionally, there is evidence to support the existence of the second-level digital divide at the K-12 level based on how educators' use technology for instruction. Schools' economic factors have been found to explain variation in how teachers use technology to promote higher-order thinking skills.\n\nThe global digital divide describes global disparities, primarily between developed and developing countries, in regards to access to computing and information resources such as the Internet and the opportunities derived from such access. As with a smaller unit of analysis, this gap describes an inequality that exists, referencing a global scale.\n\nThe Internet is expanding very quickly, and not all countries—especially developing countries—are able to keep up with the constant changes. The term \"digital divide\" doesn't necessarily mean that someone doesn’t have technology; it could mean that there is simply a difference in technology. These differences can refer to, for example, high-quality computers, fast Internet, technical assistance, or telephone services. The difference between all of these is also considered a gap.\n\nIn fact, there is a large inequality worldwide in terms of the distribution of installed telecommunication bandwidth. In 2014 only 3 countries (China, US, Japan) host 50% of the globally installed bandwidth potential (see pie-chart Figure on the right). This concentration is not new, as historically only 10 countries have hosted 70–75% of the global telecommunication capacity (see Figure). The U.S. lost its global leadership in terms of installed bandwidth in 2011, being replaced by China, which hosts more than twice as much national bandwidth potential in 2014 (29% versus 13% of the global total).\n\nThe global digital divide is a special case of the digital divide, the focus is set on the fact that \"Internet has developed unevenly throughout the world\" causing some countries to fall behind in technology, education, labor, democracy, and tourism. The concept of the digital divide was originally popularized in regard to the disparity in Internet access between rural and urban areas of the United States of America; the \"global\" digital divide mirrors this disparity on an international scale.\n\nThe global digital divide also contributes to the inequality of access to goods and services available through technology. Computers and the Internet provide users with improved education, which can lead to higher wages; the people living in nations with limited access are therefore disadvantaged. This global divide is often characterized as falling along what is sometimes called the north-south divide of \"northern\" wealthier nations and \"southern\" poorer ones.\n\nSome people argue that basic necessities need to be considered before achieving digital inclusion, such as an ample food supply and quality health care. Minimizing the global digital divide requires considering and addressing the following types of access:\nInvolves \"the distribution of ICT devices per capita…and land lines per thousands\". Individuals need to obtain access to computers, landlines, and networks in order to access the Internet. This access barrier is also addressed in Article 21 of the Convention on the Rights of Persons with Disabilities by the United Nations. \nThe cost of ICT devices, traffic, applications, technician and educator training, software, maintenance and infrastructures require ongoing financial means.\nFinancial access and \"the levels of household income play a significant role in widening the gap\" \nEmpirical tests have identified that several socio-demographic characteristics foster or limit ICT access and usage. Among different countries, educational levels and income are the most powerful explanatory variables, with age being a third one. \n\nWhile a Global Gender Gap in access and usage of ICT's exist, empirical evidence show that this due to unfavorable conditions with respect to employment, education and income and not to technophobia or lower ability. On the contrary, in the contexts under study, women with the prerequsites for access and usage turn out to be more active users of digital tools than men.\nIn order to use computer technology, a certain level of information literacy is needed. Further challenges include information overload and the ability to find and use reliable information. \nComputers need to be accessible to individuals with different learning and physical abilities including complying with Section 508 of the Rehabilitation Act as amended by the Workforce Investment Act of 1998 in the United States. \nIn illustrating institutional access, Wilson states \"the numbers of users are greatly affected by whether access is offered only through individual homes or whether it is offered through schools, community centers, religious institutions, cybercafés, or post offices, especially in poor countries where computer access at work or home is highly limited\". \nGuillen & Suarez argue that \"democratic political regimes enable a faster growth of the Internet than authoritarian or totalitarian regimes\". The Internet is considered a form of e-democracy and attempting to control what citizens can or cannot view is in contradiction to this. Recently situations in Iran and China have denied people the ability to access certain websites and disseminate information. Iran has prohibited the use of high-speed Internet in the country and has removed many satellite dishes in order to prevent the influence of Western culture, such as music and television.\nMany experts claim that bridging the digital divide is not sufficient and that the images and language needed to be conveyed in a language and images that can be read across different cultural lines. A 2013 study conducted by Pew Research Center noted how participants taking the survey in Spanish were nearly twice as likely to not use the internet.\n\nIn the early 21st century, residents of developed countries enjoy many Internet services which are not yet widely available in developing countries, including:\n\n\nThere are four specific arguments why it is important to \"bridge the gap\":\n\n\nWhile these four arguments are meant to lead to a solution to the digital divide, there are a couple other components that need to be considered. The first one is rural living versus suburban living. Rural areas used to have very minimal access to the Internet, for example. However, nowadays, power lines and satellites are used to increase the availability in these areas. Another component to keep in mind is disabilities. Some people may have the highest quality technologies, but a disability they have may keep them from using these technologies to their fullest extent.\n\nUsing previous studies (Gamos, 2003; Nsengiyuma & Stork, 2005; Harwit, 2004 as cited in James), James asserts that in developing countries, \"internet use has taken place overwhelmingly among the upper-income, educated, and urban segments\" largely due to the high literacy rates of this sector of the population. As such, James suggests that part of the solution requires that developing countries first build up the literacy/language skills, computer literacy, and technical competence that low-income and rural populations need in order to make use of ICT.\n\nIt has also been suggested that there is a correlation between democrat regimes and the growth of the Internet. One hypothesis by Gullen is, \"The more democratic the polity, the greater the Internet use...Government can try to control the Internet by monopolizing control\" and Norris \"et al.\" also contends, \"If there is less government control of it, the Internet flourishes, and it is associated with greater democracy and civil liberties.\n\nFrom an economic perspective, Pick and Azari state that \"in developing nations…foreign direct investment (FDI), primary education, educational investment, access to education, and government prioritization of ICT as all important\". Specific solutions proposed by the study include: \"invest in stimulating, attracting, and growing creative technical and scientific workforce; increase the access to education and digital literacy; reduce the gender divide and empower women to participate in the ICT workforce; emphasize investing in intensive Research and Development for selected metropolitan areas and regions within nations\".\n\nThere are projects worldwide that have implemented, to various degrees, the solutions outlined above. Many such projects have taken the form of Information Communications Technology Centers (ICT centers). Rahnman explains that \"the main role of ICT intermediaries is defined as an organization providing effective support to local communities in the use and adaptation of technology. Most commonly an ICT intermediary will be a specialized organization from outside the community, such as a non-governmental organization, local government, or international donor. On the other hand, a social intermediary is defined as a local institution from within the community, such as a community-based organization.\n\nOther proposed solutions that the Internet promises for developing countries are the provision of efficient communications within and among developing countries, so that citizens worldwide can effectively help each other to solve their own problems. Grameen Banks and Kiva loans are two microcredit systems designed to help citizens worldwide to contribute online towards entrepreneurship in developing communities. Economic opportunities range from entrepreneurs who can afford the hardware and broadband access required to maintain Internet cafés to agribusinesses having control over the seeds they plant.\n\nAt the Massachusetts Institute of Technology, the IMARA organization (from Swahili word for \"power\") sponsors a variety of outreach programs which bridge the Global Digital Divide. Its aim is to find and implement long-term, sustainable solutions which will increase the availability of educational technology and resources to domestic and international communities. These projects are run under the aegis of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and staffed by MIT volunteers who give training, install and donate computer setups in greater Boston, Massachusetts, Kenya, Indian reservations the American Southwest such as the Navajo Nation, the Middle East, and Fiji Islands. The CommuniTech project strives to empower underserved communities through sustainable technology and education. According to Dominik Hartmann of the MIT's Media Lab, interdisciplinary approaches are needed to bridge the global digital divide.\n\nBuilding on the premise that any effective solution must be decentralized, allowing the local communities in developing nations to generate their own content, one scholar has posited that social media—like Facebook, YouTube, and Twitter—may be useful tools in closing the divide. As Amir Hatem Ali suggests, \"the popularity and generative nature of social media empower individuals to combat some of the main obstacles to bridging the digital divide\". Facebook’s statistics reinforce this claim. According to Facebook, more than seventy-five percent of its users reside outside of the US. Moreover, more than seventy languages are presented on its website. The reasons for the high number of international users are due to many the qualities of Facebook and other social media. Amongst them, are its ability to offer a means of interacting with others, user-friendly features, and the fact that most sites are available at no cost. The problem with social media, however, is that it can be accessible, provided that there is physical access. Nevertheless, with its ability to encourage digital inclusion, social media can be used as a tool to bridge the global digital divide.\n\nSome cities in the world have started programs to bridge the digital divide for their residents, school children, students, parents and the elderly. One such program, founded in 1996, was sponsored by the city of Boston and called the Boston Digital Bridge Foundation. It especially concentrates on school children and their parents, helping to make both equally and similarly knowledgeable about computers, using application programs, and navigating the Internet.\n\nFree Basics is a partnership between social networking services company Facebook and six companies (Samsung, Ericsson, MediaTek, Opera Software, Nokia and Qualcomm) that plans to bring affordable access to selected Internet services to less developed countries by increasing efficiency, and facilitating the development of new business models around the provision of Internet access. In the whitepaper realised by Facebook's founder and CEO Mark Zuckerberg, connectivity is asserted as a \"human right\", and Internet.org is created to improve Internet access for people around the world.\n\n\"Free Basics provides people with access to useful services on their mobile phones in markets where internet access may be less affordable. The websites are available for free without data charges, and include content about news, employment, health, education and local information etc. By introducing people to the benefits of the internet through these websites, we hope to bring more people online and help improve their lives.\"\n\nHowever, Free Basics is also accused of violating net neutrality for limiting access to handpicked services. Despite a wide deployment in numerous countries, it has been met with heavy resistance notably in India where the Telecom Regulatory Authority of India eventually banned it in 2016.\n\nSeveral projects to bring internet to the entire world with a satellite constellation have been devised in the last decade, one of these being Starlink by Elon Musk's company SpaceX. Unlike Free Basics, it would provide people with a full internet access and would not be limited to a few selected services. In the same week Starlink was announced, serial-entrepreneur Richard Branson announced his own project OneWeb, a similar constellation with approximately 700 satellites that has already procured communication frequency licenses for their broadcast spectrum and could possibly be operational as early as in 2019.\n\nThe biggest hurdle of these projects is the astronomical financial and logistical costs of launching so many satellites. After the failure of previous satellite-to-consumer space ventures, satellite industry consultant Roger Rusch said \"It's highly unlikely that you can make a successful business out of this.\" Musk has publicly acknowledged this business reality, and indicated in mid-2015 that while endeavoring to develop this technically-complicated space-based communication system he wants to avoid overextending the company and stated that they are being measured in the pace of development.\n\nOne Laptop Per Child (OLPC) is another attempt to narrow the digital divide. This organization, founded in 2005, provides inexpensively produced \"XO\" laptops (dubbed the \"$100 laptop\", though actual production costs vary) to children residing in poor and isolated regions within developing countries. Each laptop belongs to an individual child and provides a gateway to digital learning and Internet access. The XO laptops are designed to withstand more abuse than higher-end machines, and they contain features in context to the unique conditions that remote villages present. Each laptop is constructed to use as little power as possible, have a sunlight-readable screen, and is capable of automatically networking with other XO laptops in order to access the Internet—as many as 500 machines can share a single point of access.\n\nSeveral of the 67 principles adopted at the World Summit on the Information Society convened by the United Nations in Geneva in 2003 directly address the digital divide:\n\n\n\n\n"}
{"id": "37822922", "url": "https://en.wikipedia.org/wiki?curid=37822922", "title": "Dip reader", "text": "Dip reader\n\nA DIP reader (Document Insertion Processor) is an electronic device for reading an electronically encoded card that is inserted and then removed from the device.\n\nA typical dip reader is used for reading credit cards where the data are either encoded on a magnetic stripe or an internal computer chip. The magnetic stripe on a card is typically read as the card is extracted. If the card is a smart card, then the data transfer typically takes place when the card is fully inserted. In this case, the card is held while data transfer is taking place.\n"}
{"id": "22728417", "url": "https://en.wikipedia.org/wiki?curid=22728417", "title": "Distinguo", "text": "Distinguo\n\nDistinguo is a proprietary software application for Semantic search based on description logic that enables users to search for meaning instead of just keywords. This API permits developers to integrate into their applications a tool to parse natural language (generating an XML summary), and then measure the semantic \"distance\" between a query and a target text. \"Guha et al.\" distinguish two major forms of search: Navigational and Research. In navigational search, the user is using the search engine as a navigation tool to navigate to a particular intended document. Semantic Search is not directly applicable to navigational searches. In Research Search, the user enters a phrase which is intended to denote the object of the research. The user will not know in advance which particular documents will contain the information; the task is to locate any number of documents which together will produce the sought information.\n\nThe program is not available as a stand-alone or consumer application; it is a powerful software component for inclusion in other text analysis solutions such as any project requiring the comparison of meanings or the measurement of the difference between words and longer texts. Users can compare a simple query to a large database of texts, locating texts containing similar meanings and ranking them according to their similarity.\n\nIn addition to measuring the similarity between words, sentences, or texts written in natural language, semantic searching can also distill the meanings of a set of texts and then provide comparative information about those meanings. Users enter a word or phrase which represents a subject for which similar related information is required. Possible typical applications would be for comparing reports, minutes of meetings, insurance claims, medical histories, research papers, and legal decisions from many court cases.\n\nDistinguo is a C++ Application Programming Interface (API), in two different applications:\n\nDistinguo Index: a tool for expanding search keywords to include inflected forms, synonyms, hypernyms, hyponyms, and other words related by meaning.\n\nDistinguo Context: a tool for analyzing the meaning of full sentences or even of full texts; it can then match this text with other texts containing the same or similar ideas.\n\nThey are delivered for integration into other solutions. Distinguo tools are supplied as C++ libraries, and can be integrated into software solutions for its own features, or to supplement or refine statistical search methods.\n\nThe result of the syntactic analysis, as well as the format of the ontologies, is represented in XML. The calculation of semantic similarities may be in the form of a numerical coefficient, or an ontology showing the information present in the first ontology and missing in the second. The format of the texts and of the XML is a string of characters in the programming language 'C'.\n\nDistinguo Index and Distinguo Context are based on algorithms for the parsing of language and the matching and ranking semantic results developed by Semantica Software of Luxembourg in association with Ultralingua and is in constant further research and development. Other uses could be integrated into electronic dictionary or phrasebook applications.\n\n\n\n"}
{"id": "3415737", "url": "https://en.wikipedia.org/wiki?curid=3415737", "title": "Doctest", "text": "Doctest\n\ndoctest is a module included in the Python programming language's standard library that allows the easy generation of tests based on output from the standard Python interpreter shell, cut and pasted into docstrings.\n\nDoctest makes innovative use of the following Python capabilities:\n\nWhen using the Python shell, the primary prompt: »> , is followed by new commands. The secondary prompt: ... , is used when continuing commands on multiple lines; and the result of executing the command is expected on following lines.\nA blank line, or another line starting with the primary prompt is seen as the end of the output from the command.\n\nThe doctest module looks for such sequences of prompts in a docstring, re-executes the extracted command and checks the output against the output of the command given in the docstrings test example.\n\nThe default action when running doctests is for no output to be shown when tests pass. This can be modified by options to the doctest runner. In addition, doctest has been integrated with the Python unit test module allowing doctests to be run as standard unittest testcases. Unittest testcase runners allow more options when running tests such as the reporting of test statistics such as tests passed, and failed.\n\nAlthough doctest does not allow a Python program to be embedded in narrative text, it does allow for verifiable examples to be embedded in docstrings, where the docstrings can contain other text. Docstrings can in turn be extracted from program files to generate documentation in other formats such as HTML or PDF. \nA program file can be made to contain the documentation, tests, as well as the code and the tests easily verified against the code. This allows code, tests, and documentation to evolve together.\n\nDoctests are well suited to provide an introduction to a library by demonstrating how the API is used.\n\nOn the basis of the output of Python's interactive interpreter, text can be mixed with tests that exercise the library, showing expected results.\n\nExample one shows how narrative text can be interspersed with testable examples in a docstring. \nIn the second example, more features of doctest are shown, together with their explanation. \nExample three is set up to run all doctests in a file when the file is run, but when imported as a module, the tests will not be run.\n\nThis example also simulates input to the function from a file by using the Python StringIO module\n\nBoth the EpyText format of Epydoc and Docutils' reStructuredText format support the markup of doctest sections within docstrings.\n\nIn C++, the doctest framework is the closest possible implementation of the concept - tests can be written directly in the production code with minimal overhead and the option to strip them from the binary.<ref name=\"C++ github.com/onqtam/doctest\">\nC++ github.com/onqtam/doctest</ref>\n\nThe ExUnit.DocTest Elixir library implements functionality similar to Doctest.\n\nAn implementation of Doctest for Haskell.<ref name=\"Haskell github.com/sol/doctest\">Haskell github.com/sol/doctest</ref>\n\nWriting documentation tests in Elm.<ref name=\"Elm github.com/tshm/elm-doctest\">Elm github.com/tshm/elm-doctest</ref>\n\nWriting documentation tests in Rust.\n\nbyexample is based on doctest and supports documentation tests in several languages like Python, Shell and Ruby.\n\n"}
{"id": "8103238", "url": "https://en.wikipedia.org/wiki?curid=8103238", "title": "Drive testing", "text": "Drive testing\n\nDrive testing is a method of measuring and assessing the coverage, capacity and Quality of Service (QoS) of a mobile radio network.\n\nThe technique consists of using a motor vehicle containing mobile radio network air interface measurement equipment that can detect and record a wide variety of the physical and virtual parameters of mobile cellular service in a given geographical area.\n\nBy measuring what a wireless network subscriber would experience in any specific area, wireless carriers can make directed changes to their networks that provide better coverage and service to their customers.\n\nDrive testing requires a mobile vehicle outfitted with drive testing measurement equipment. The equipment are usually highly specialized electronic devices that interface to OEM mobile handsets. This ensures measurements are realistic and comparable to actual user experiences.\n\nDrive test equipment typically collects data relating to the network itself, services running on the network such as voice or data services, radio frequency scanner information and GPS information to provide location logging.\n\nThe data set collected during drive testing field measurements can include information such as:\n\n\nDrive testing can broadly be categorized into three distinct topics:\n\n\nThe result produced by drive testing for each of these purposes is different.\n\nFor benchmarking, sophisticated multi-channel tools such as Focus Infocom's DMTS and XGMA, DingLi Communications' Pilot Fleet, Ascom's Symphony, Rohde & Schwarz-SwissQual's Diversity Benchmarker or Keysight Nemo Invex II are used to measure several network technologies and service types simultaneously to very high accuracy, to provide directly comparable information regarding competitive strengths and weaknesses. Results from benchmarking activities,such a comparative coverage analysis or comparative data network speed analysis, are frequently used in marketing campaigns. Drive testing to gather network bench-marking data is the only way mobile network operators can collect accurate competitive data on the true level of their own and their competitors technical performance and quality levels.\n\nOptimization and troubleshooting information is more typically used to aid in finding specific problems during the rollout phases of new networks or to observe specific problems reported by consumers during the operational phase of the network lifecycle. In this mode drive testing data is used to diagnose the root cause of specific, typically localized, network issues such as dropped calls or missing neighbour cell assignments.\n\nService quality monitoring typically involves making test calls across the network to a fixed test unit to assess the relative quality of various services using Mean opinion score (MOS). Quality monitoring focuses on the end user experience of the service, and allows mobile network operators to react to what effectively subjective quality degradations by investigating the technical cause of the problem in time-correlated data collected during the drive test. Service quality monitoring is typically carried out in an automated fashion, using devices that run largely without human intervention carried in vehicles that regularly ply typical drive testing routes such as garbage collection vehicles, taxis or buses.\n\nDrive testing can be conducted at any time on a live network and very rarely will there be any network intrusion.\n\n1 - Drive testing LTE, available from: <http://www.viavisolutions.com/sites/default/files/technical-library-files/drivetesting_lte_wp_nsd_tm_ae_0.pdf>\n"}
{"id": "7726042", "url": "https://en.wikipedia.org/wiki?curid=7726042", "title": "Electronic performance support systems", "text": "Electronic performance support systems\n\nAn electronic performance support system (EPSS) is any computer software program or component that improves user performance.\n\nEPSSs can help an organization to reduce the cost of training staff while increasing productivity and performance. They can empower employees to perform tasks with a minimum amount of external intervention or training. By using this type of system an employee, especially a new employee, will often not only be able to complete his or her work more quickly and accurately, but, as a secondary benefit, will also learn more about the job and the employer's business.\n\nAn EPSS is best considered when\nThese situations often occur when new systems (e.g. customer relationship management, enterprise resource planning) are introduced, upgraded or consolidated, and in certain call centres when agents must perform using complex systems, processes or products.\n\nThere are different views about the components and characteristics of EPSS. For example, from Barker and Banerji’s (1995) point of view, an EPSS has four functional levels, which should be brought together:\n\n\nIn \"Electronic Performance Support Systems\", Gloria Gery defined EPSS as:\nan integrated electronic environment that is available to and easily \naccessible by each employee and is structured to provide immediate, individualized \non-line access to the full range of information, software, guidance, advice and \nassistance, data, images, tools, and assessment and monitoring systems to permit \njob performance with minimal support and intervention by others.\nAlso, in 1991, Barry Raybould gave a shorter definition:\na computer-based system that improves worker productivity by providing on-the-job access to integrated information, advice, and learning experiences.\n\nFrom a business perspective, a former Nortel Networks executive, William Bezanson (2002) provided a definition linked to application usability and organizational results: A performance support system provides just-in-time, just enough training, information, tools, and help for users of a product or work environment, to enable optimum performance by those users when and where needed, thereby also enhancing the performance of the overall business.\n\nAn EPSS must be distinguished from a traditional online help system. Online help usually supports a single software application and is not necessarily focused on the entire range of job tasks (which may involve multiple applications), but just that specific software. With online help, cross-referencing is often not available and the information provided is limited and rarely combined with procedures or complex tasks. Perhaps most critically, on-line help cannot be customized to the user or the job task; in fact, the same software screen may require different inputs depending on the user and job task. Online help is also not contextual to the user's current situation and requires users to search through for the solution to their problem. \n\nEPSS must also be differentiated from e-learning simulations that replay a series of steps on-demand within a software application. Simulations are more closely associated with on-demand training, not just-in-time support, because of the longer time considerations, complexity, and media restrictions for playing a simulation. An EPSS can be considered a part of the e-learning category, as it is on-demand learning, and notes that the EPSS modality fits more within the informal learning definition.\n\nIn his book, Bezanson points out that \"knowledge management\" is the noun corresponding to the verb of \"performance support\". The knowledge documented in the system plays a critical role in any EPSS system. This concept was originated by Raybould (1997) who separates out the repository, delivery and infrastructure aspects of the EPSS from the knowledge base it contains. In fact, Bezanson emphasizes the advantage that an LMS (learning management system) will interface with the EPSS to supply the knowledge base, content-courseware, or other tracking capabilities that an EPSS may require if the LMS precedes the implementation of the EPSS.\n\nEPSS's role in the future of training and work have been noted by followers of the trend towards more informal learning systems driven by knowledge management systems.\nTony O'Driscoll (1999) summarizes: \n\nTo this day, analysts such as Forrester's Claire Schooley and Bersin & Associates' Chris Howard write similarly themed articles based on their research in informal learning, technology and training.\n\n\n"}
{"id": "4980221", "url": "https://en.wikipedia.org/wiki?curid=4980221", "title": "Electronic serial number", "text": "Electronic serial number\n\n\"Electronic serial numbers\" (\"ESNs\") were created by the U.S. Federal Communications Commission (FCC) to uniquely identify mobile devices, from the days of AMPS in the United States starting in the early 1980s. The administrative role was taken over by the Telecommunications Industry Association in 1997 and is still maintained by them. ESNs are currently mainly used with CDMA phones (and were previously used by AMPS and TDMA phones), compared to International Mobile Equipment Identity (IMEI) numbers used by all GSM phones.\n\nThe first 8 bits of the ESN were originally the manufacturer code, leaving 24 bits for the manufacturer to assign up to 16,777,215 codes to mobiles. To allow more than 256 manufacturers to be identified, the manufacturer code was extended to 14 bits, leaving 18 bits for the manufacturer to assign up to 262,144 codes. Manufacturer code 0x80 is reserved from assignment and is used instead as an 8-bit prefix for pseudo-ESNs (pESN). The remaining 24 bits are the least significant bits of the SHA-1 hash of a mobile equipment identifier (MEID). Pseudo-ESNs are not guaranteed to be unique (the MEID is the unique identifier if the phone has a pseudo-ESN).\n\nESNs are often represented as either 11-digit decimal numbers or 8 digit hexadecimal numbers. For the decimal format the first three digits are the decimal representation of the first 8 bits (between 00 and 255 inclusive) and the next 8 digits are derived from the remaining 24 bits and will be between 0000000 and 16777215 inclusive. The decimal format of pseudo ESNs will therefore begin with 128. The decimal format separately displays 8 bit manufacturer codes in the first 3 digits, but 14 bit codes are not displayed as separate digits. The hexadecimal format displays an ESN as 8 digits and also does not separately display 14 bit manufacturer codes which occupy 3.5 hexadecimal digits.\n\nAs ESNs have essentially run out, a new serial number format, MEID, was created by 3GPP2 and was first implemented by Verizon in 2006. MEIDs are 56 bits long, the same length as the IMEI and, in fact, MEID was created to be a superset of IMEI. The main difference between MEID and IMEI is that the MEID allows hexadecimal digits while IMEI allows only decimal digits – \"IMEI shall consist of decimal digits (0 through 9) only\".\n\nThe last of the previously unused ESN codes were allocated in November 2008. Applications for assignments were accepted until June 30, 2010 using reclaimed ESN codes, those previously assigned to AMPS or TDMA phones and therefore not present on CDMA2000 systems. Reclaimed codes have also been used for UIMID assignments. Codes are assigned according to industry guidelines.\n\nAlthough ESN assignments may still occur in the future based on applications received before June 30, 2010, there have not been any assignments made since December 31, 2010.\n\n"}
{"id": "32741783", "url": "https://en.wikipedia.org/wiki?curid=32741783", "title": "Emtech", "text": "Emtech\n\nThe Emtech (short for \"Emerging Technologies\") conference, produced by the Massachusetts Institute of Technology's Technology Review magazine, is an annual conference highlighting invention and new developments in engineering and technology. Started in 1999, the 2011 conference is planned for October 18-19 at MIT.\n\nIn addition to two days of presentations, the conference highlights the winners of the annual TR35 award, recognizing the world's top 35 innovators under the age of 35. Some of the most famous winners of the award include Larry Page and Sergey Brin (creators of Google), Mark Zuckerberg (creator of Facebook), Jack Dorsey (creator of Twitter), and Konstantin Novoselov, who later won the Nobel Prize in Physics.\n"}
{"id": "55234564", "url": "https://en.wikipedia.org/wiki?curid=55234564", "title": "Female education in STEM", "text": "Female education in STEM\n\nFemale education in STEM includes child and adult female represented in the fields of science, technology, engineering, and mathematics (STEM ). In 2017, 33% of students in STEM fields were women.\n\nThe organization UNESCO has stated that this gender disparity is due to discrimination, biases, social norms and expectations that influence the quality of education women receive and the subjects they study. UNESCO also believes that having more women in STEM fields is desirable because it would help bring about sustainable development. \n\nGender differences in STEM education participation are already visible in early childhood care and education in science- and math-related play, and become more pronounced at higher levels of education. Girls appear to lose interest in STEM subjects with age, particularly between early and late adolescence. This decreased interest affects participation in advanced studies at the secondary level and in higher education. Female students represent 35% of all students enrolled in STEM-related fields of study at this level globally . Differences are also observed by disciplines, with female enrollment lowest in engineering, manufacturing and construction, natural science, mathematics and statistics and ICT fields. Significant regional and country differences in female representation in STEM studies can be observed, though, suggesting the presence of contextual factors affecting girls’ and women’s engagement in these fields. Women leave STEM disciplines in disproportionate numbers during their higher education studies, in their transition to the world of work and even in their career cycle.\n\nData on gender differences in learning achievement present a complex picture, depending on what is measured (subject, knowledge acquisition against knowledge application), the level of education/age of students, and geographic location. Overall, women's participation has been increasing, but significant regional variations exist. For example, where data are available in Africa, Latin America and the Caribbean, the gender gap is largely in favour of boys in mathematics achievement in secondary education. In contrast, in the Arab States, girls perform better than boys in both subjects in primary and secondary education. As with the data on participation, national and regional variations in data on learning achievement suggest the presence of contextual factors affecting girls’ and women’s engagement in these fields. Girls’ achievement seems to be stronger in science than mathematics and where girls do better than boys, the score differential is up to three times higher than where boys do better. girls tend to outperform boys in certain sub-topics such as biology and chemistry but do less well in physics and earth science.\n\nThe gender gap has fallen significantly in science in secondary education among TIMSS trend countries: 14 out of 17 participating countries had no gender gap in science in 2015, compared to only one in 1995. However, the data is less well known outside of these 17 countries. The gender gap in boys' favour is slightly bigger in mathematics but improvements over time in girls’ favour are also observed in certain countries, despite the important regional variations. Gender differences are observed within mathematic sub-topics with girls outperforming boys in topics such as algebra and geometry but doing less well in \"number\". Girls’ performance is stronger in assessments that measure knowledge acquisition than those measuring knowledge application. Country coverage in terms of data availability is quite limited while data is collected at different frequency and against different variables in the existing studies. There are large gaps in our knowledge of the situation in low- and middle-income countries in sub-Saharan Africa, Central Asia, and South and West Asia, particularly at secondary level.\n\nAccording to UNESCO, there are multiple and overlapping factors which influence girls' and women's participation, achievement and progression in STEM studies and careers, all of which interact in complex ways, including: \n\nThe question of whether there are differences in cognitive ability between men and women has long been a topic of debate among researchers and scholars. Some studies have found no differences in the neural mechanism of learning based on sex.\n\nLoss of interest has been the major reason cited for girls opting out of STEM. However, some have stated that this choice is influenced heavily by the socialization process and stereotyped ideas about gender roles, including stereotypes about gender and STEM. Gender stereotypes that communicate the idea that STEM studies and careers are male domains can negatively affect girls’ interest, engagement and achievement in STEM and discourage them from pursuing STEM careers. Girls who assimilate such stereotypes have lower levels of self-efficacy and confidence in their ability than boys. Self-efficacy affects both STEM education outcomes and aspirations for STEM careers to a considerable extent. Some studies have shown that girls appear to lose interest in STEM subjects with age.\n\nParents, including their beliefs and expectations, play an important role in shaping girls’ attitudes towards, and interest in, STEM studies. Parents with traditional beliefs about gender roles and who treat girls and boys unequally can reinforce stereotypes about gender and ability in STEM. Parents can also have a strong influence on girls' STEM participation and learning achievement through the family values, environment, experiences and encouragement that they provide. Some research finds that parents’ expectations, particularly the mother’s expectations, have more influence on the higher education and career choices of girls than those of boys. Higher socio-economic status and parental educational qualifications are associated with higher scores in mathematics and science for both girls and boys. Girls’ science performance appears to be more strongly associated with mothers’ higher educational qualifications, and boys’ with their fathers’. Family members with STEM careers can also influence girls’ STEM engagement. The broader socio-cultural context of the family can also play a role. Factors such as ethnicity, the language used at home, immigrant status and family structure may also have an influence on girls’ participation and performance in STEM. Peers can also impact on girls’ motivation and feeling of belonging in STEM education. Influence of female peers is a significant predictor of girls’ interest and confidence in mathematics and science.\n\nQualified teachers with specialization in STEM can positively influence girls’ performance and engagement with STEM education and their interest in pursuing STEM careers. Female STEM teachers often have stronger benefits for girls, possibly by acting as role models and by helping to dispel stereotypes about sex-based STEM ability. Teachers’ beliefs, attitudes, behaviours and interactions with students, as well as Curricula and learning materials, can all play a role as well. Opportunities for real-life experiences with STEM, including hands-on practice, apprenticeships, career counselling and mentoring can expand girls’ understanding of STEM studies and professions and maintain interest. Assessment processes and tools that are gender-biased or include gender stereotypes may negatively affect girls’ performance in STEM. Girls’ learning outcomes in STEM can also be compromised by psychological factors such as mathematics or test anxiety.\n\nA 2018 study found that while girls perform better or equal to boys in two out of three countries, in nearly all countries more girls were qualified to college-level study than had enrolled. Researchers found that female enrolment to STEM subjects was relatively lower in countries with a high degree of gender equality – a gender-equality paradox.\n\nCultural and social norms influence girls’ perceptions about their abilities, role in society and career and life aspirations.The degree of gender equality in wider society influences girls’ participation and performance in STEM. In countries with greater gender equality, girls tend to have more positive attitudes and confidence about mathematics and the gender gap in achievement in the subject is smaller. Targeted measures to promote gender equality, such as gender mainstreaming legislation or policies such as quotas, financial incentives or other, can increase girls’ and women’s participation in STEM education and careers. Gender stereotypes portrayed in the media are internalized by children and adults and affect the way they see themselves and others. Media can perpetuate or challenge gender stereotypes about STEM abilities and careers.\n"}
{"id": "32906538", "url": "https://en.wikipedia.org/wiki?curid=32906538", "title": "Hoa Lac Hi-tech Park", "text": "Hoa Lac Hi-tech Park\n\nHoa Lac Hi-tech Park is the first and largest hi-tech park in Vietnam with total area of 1586 ha (app. 4000 acres). Located in the area of Hanoi Capital, convenient for transportation to Noi Bai International Airport and Hai Phong deep Seaport, Hoa Lac Hi-tech Park will be developed as model of a science city with over 200,000 people working and living and consists of the following main functional zones:\nCurrently, there are two software companies located here, including FPT Software and Viettel Software. They are two biggest software companies of Vietnam.\n\n\n"}
{"id": "175502", "url": "https://en.wikipedia.org/wiki?curid=175502", "title": "How-to", "text": "How-to\n\nA how-to is an informal, often short, description of how to accomplish a specific task. A how-to is usually meant to help non-experts, may leave out details that are only important to experts, and may also be greatly simplified from an overall discussion of the topic.\n\nOne of the earliest how-to books was published in 1569 and entitled, \"A booke of the arte and maner, how to plant set Up intro an divers other new practise, by one of the Abbey of Saint Vincent in Fraunce\" by Leonard Mascall.\" \n\nPerhaps the best known full-length book in the genre is \"How to Win Friends and Influence People\", written by Dale Carnegie in 1936.\n\nA similar concept can be seen in many of the \"[topic] For Dummies\" series of tutorials and also in many other introductory surveys entitled with the suffix \"101\" (based on academic numberings of entry-level courses).\n\nIt is common practice to write the phrase as \"HOWTO\" in the open-source community. This is consistent with the traditional naming scheme for technical documentation, such as FAQ and README. Another possible reason is that this makes it easier to find a how-to in search engines like Google as searching with the words \"to\" and \"how\" does not necessarily deliver relevant search results.\n\nHow-tos have a long history as a way of sharing knowledge on the Internet, but are less successful than FAQs, manuals, recipes and guides.\n\nIn the beginning, most how-tos on the Internet were the result of a complex process in which an author wrote a how-to for potential readers. After 2001, user added content played a more and more important role on the internet in a trend that is widely referred to as Web 2.0. This had a profound impact on the way in which how-tos are generated on the internet, as the readers and users were able to add to and improve the on-line content.\n\n"}
{"id": "27205653", "url": "https://en.wikipedia.org/wiki?curid=27205653", "title": "ISO-TimeML", "text": "ISO-TimeML\n\nISO 24617-1:2009, ISO-TimeML is the International Organization for Standardization ISO/TC37 standard for time and event markup and annotation. The scope is standardization of principles and methods relating to the annotation of temporal events in the contexts of electronic documentation and language.\n\nThe goals of ISO-TimeML are to provide a common model for the creation and use of temporal and event annotation, as a means of managing time-related data within documents, and to enable later categorization and data extraction with use of this meta-data.\n\nISO-TimeML was presented to the ISO for consideration as a standard in August 2007. In this presentation, the preliminaries of ISO-TimeML were outlined, and potential applications were examined. In the following year, revisions were made to ISO-TimeML as the standard transitioned from a New Project (NP) to a Working Project (WP). The ISO-TimeML voting period began in October 2008 and was approved as an international standard by March 2009.\n\nThe ISO/TC37 standards are currently elaborated as high level specifications and deal with word segmentation (ISO 24614), annotations (ISO 24611 a.k.a. MAF, ISO 24612 a.k.a. LAF, ISO 24615 a.k.a. SynAF, and ISO 24617-1 a.k.a. SemAF/Time), feature structures (ISO 24610), multimedia containers (ISO 24616 a.k.a. MLIF), and lexicons (ISO 24613 a.k.a. LMF). These standards are based on low level specifications dedicated to constants, namely data categories (revision of ISO 12620), language codes (ISO 639), scripts codes (ISO 15924), country codes (ISO 3166) and Unicode (ISO 10646).\n\nThe two level organization forms a coherent family of standards with the following common and simple rules:\n\nJoint work between ISO/TC 37/SC 4/WG 2 (TDG 3) and the TimeML\nWorking Group that was agreed on at the TDG 3 and LIRICS Working Group\nMeeting, USC/ISI, Marina del Rey, CA, U.S.A., 2006-04-20/21/22.\n\nProposed Project Leaders and Editors:\n\n\n"}
{"id": "41257045", "url": "https://en.wikipedia.org/wiki?curid=41257045", "title": "Interaction Design Foundation", "text": "Interaction Design Foundation\n\nThe Interaction Design Foundation is a non-profit educational organization which produces open content and Open Access educational materials online with the stated goal of \"democratizing education by making world-class educational materials free for anyone, anywhere.\" \nMaterials are learning materials at graduate level targeted at both industry and academia in the fields of interaction design, computer science, user experience, information architecture, and design.\nThe centerpiece of the Interaction-Design.org is the peer reviewed \"Encyclopedia of Human-Computer Interaction\", which currently number 40+ textbooks written by 100+ leading designers and professors as well as commentaries and HD video interviews shot around the world. \nThe website features professional and academic textbooks, video lectures, a conference calendar, and a comprehensive bibliography of the most authoritative publications within the design of interactive technology.\n\nIn June 2013, the Interaction Design Foundation launched a 4 year 35,000 mile bike tour, named \"Share the Knowledge Tour\", to raise awareness of the rising cost of education - with weekly events on university campuses.\n\nFinancial sponsors include the German software company SAP. Authors include Harvard professor Clayton Christensen and New York Times bestselling author, Robert Spence who invented the \"magnifying glass\" visualization that is familiar to anyone with an iPhone or iMac, and Stu Card who performed the research that led to the computer mouse's commercial introduction by Xerox.\n\nThe Executive Board currently include Don Norman, Ken Friedman, Bill Buxton, Irene Au, Michael Arent, Daniel Rosenberg, Jonas Lowgren and Olof Schybergson.\n\n\n"}
{"id": "27912738", "url": "https://en.wikipedia.org/wiki?curid=27912738", "title": "John Holdren", "text": "John Holdren\n\nJohn Paul Holdren (Sewickley, Pennsylvania, March 1, 1944) is an American scientist who served as the senior advisor to President Barack Obama on science and technology issues through his roles as Assistant to the President for Science and Technology, Director of the White House Office of Science and Technology Policy, and Co-Chair of the President’s Council of Advisors on Science and Technology (PCAST).\n\nHoldren was previously the Teresa and John Heinz Professor of Environmental Policy at the Kennedy School of Government at Harvard University, director of the Science, Technology, and Public Policy Program at the School's Belfer Center for Science and International Affairs, and Director of the Woods Hole Research Center.\n\nHoldren was born in Sewickley, Pennsylvania, and grew up in San Mateo, California. He trained in aeronautics, astronautics and plasma physics and earned a bachelor's degree from the Massachusetts Institute of Technology in 1965 and a Ph.D. from Stanford University in 1970 supervised by Oscar Buneman.\n\nHoldren taught at Harvard for 13 years and at the University of California, Berkeley for more than two decades. His work has focused on the causes and consequences of global environmental change, population control, energy technologies and policies, ways to reduce the dangers from nuclear weapons and materials, and science and technology policy. He has also taken measures to contextualize the United State's current energy challenge, noting the role that nuclear energy could play.\nHoldren was involved in the famous Simon–Ehrlich wager in 1980. He, along with two other scientists helped Paul R. Ehrlich establish the bet with Julian Simon, in which they bet that the price of five key metals would be higher in 1990. The bet was centered around a disagreement concerning the future scarcity of resources in an increasingly polluted and heavily populated world. Ehrlich and Holdren lost the bet, when the price of metals had decreased by 1990.\n\nHoldren was chair of the Executive Committee of the Pugwash Conferences on Science and World Affairs from 1987 until 1997 and delivered the Nobel Peace Prize acceptance lecture on behalf of Pugwash Conferences in December 1995. From 1993 until 2003, he was chair of the Committee on International Security and Arms Control of the National Academy of Sciences, and Co-Chairman of the bipartisan National Committee on Energy Policy from 2002 until 2007. Holdren was elected President of the American Association for the Advancement of Science (AAAS) (2006–2007), and served as board Chairman (2007–2008). He was the founding chair of the advisory board for \"Innovations\", a quarterly journal about entrepreneurial solutions to global challenges published by MIT Press, and has written and lectured extensively on the topic of global warming.\n\nHoldren served as one of President Bill Clinton's science advisors (PCAST) from 1994 to 2001. Eight years later, President Barack Obama nominated Holdren for the position of science advisor and Director of the Office of Science and Technology Policy in December 2008, and he was confirmed on March 19, 2009, by a unanimous vote in the Senate. He testified to the nomination committee that he does not believe that government should have a role in determining optimal population size and that he never endorsed forced sterilization.\n\nOverpopulation was an early concern and interest. In a 1969 article, Holdren and co-author Paul R. Ehrlich argued, \"if the population control measures are not initiated immediately, and effectively, all the technology man can bring to bear will not fend off the misery to come.\" In 1973, Holdren encouraged a decline in fertility to well below replacement in the United States, because \"210 million now is too many and 280 million in 2040 is likely to be much too many.\" In 1977, Paul R. Ehrlich, Anne H. Ehrlich, and Holdren co-authored the textbook \"\".\nOther early publications include \"Energy\" (1971), \"Human Ecology\" (1973), \"Energy in Transition\" (1980), \"Earth and the Human Future\" (1986), \"Strategic Defenses and the Future of the Arms Race\" (1987), \"Building Global Security Through Cooperation (1990)\", and \"Conversion of Military R&D\" (1998).\n\nHoldren also authored over 200 articles and papers and has co-authored and co-edited some 20 books and book-length reports including:\n\nHoldren lives in Falmouth, Massachusetts, with his wife, biologist Cheryl E. Holdren (formerly Cheryl Lea Edgar), with whom he has two children and five grandchildren.\n\n\n"}
{"id": "165331", "url": "https://en.wikipedia.org/wiki?curid=165331", "title": "Kinescope", "text": "Kinescope\n\nKinescope , shortened to kine , also known as telerecording in Britain, is a recording of a television program on motion picture film, directly through a lens focused on the screen of a video monitor. The process was pioneered during the 1940s for the preservation, re-broadcasting and sale of television programmes before the use of commercial broadcast-quality videotape became prevalent for these purposes.\n\nTypically, the term can refer to the process itself, the equipment used for the procedure (a 16 mm or 35 mm movie camera mounted in front of a video monitor, and synchronized to the monitor's scanning rate), or a film made using the process. Kinescopes were the only practical way to preserve live television broadcasts prior to the introduction of videotape in 1956. A small number of theatrically released feature films have also been produced as kinescopes.\n\nThe term \"kinescope\" originally referred to the cathode ray tube used in television receivers, as named by inventor Vladimir K. Zworykin in 1929. Hence, the recordings were known in full as kinescope films or kinescope recordings.  RCA was granted a trademark for the term (for its cathode ray tube) in 1932; it voluntarily released the term to the public domain in 1950.\n\nThe General Electric laboratories in Schenectady, New York experimented with making still and motion picture records of television images in 1931.\n\nThere is some evidence to suggest that the BBC experimented with filming the output of the television monitor before its television service was suspended in 1939 due to the outbreak of World War II. A BBC executive, Cecil Madden, later recalled filming a production of \"The Scarlet Pimpernel\" in this way, only for film director Alexander Korda to order the burning of the negative as he owned the film rights to the book, which he felt had been infringed. However, the evidence for this is purely anecdotal, and indeed there is no written record of any BBC Television production of \"The Scarlet Pimpernel\" during the 1936–1939 period. The incident is, however, dramatised in Jack Rosenthal's 1986 television play \"The Fools on the Hill\".\n\nSome of the surviving live transmissions of the Nazi German television station Fernsehsender Paul Nipkow, dating as far back as the 1930s, were recorded by pointing a 35mm camera to a receiver's screen, although most surviving Nazi live television programs such as the 1936 Summer Olympics (not to confuse with the cinematic footage made during the same event by Leni Riefenstahl for her film \"Olympia\"), a number of Nuremberg Rallies, or official state visits (such as Benito Mussolini's) were shot directly on 35mm instead and transmitted over the air as a television signal, with only a two minutes' delay from the original event, by means of the so-called \"Zwischenfilmverfahren\" (see intermediate film system) from an early outside broadcast van on the site.\n\nAccording to a 1949 film produced by RCA, silent films had been made of early experimental telecasts during the 1930s. The films were produced by aiming a camera at television monitors - at a speed of eight frames per second, resulting in somewhat jerky reproductions of the images. By the mid-1940s, RCA and NBC were refining the filming process and including sound; the images were less jerky but still somewhat fuzzy.\n\nBy early 1946, television cameras were being attached to American guided missiles to aid in their remote steering. Films were made of the television images they transmitted for further evaluation of the target and the missile's performance.\n\nThe first known surviving example of the telerecording process in Britain is from October 1947, showing the singer Adelaide Hall performing at the RadiOlympia event. Hall sings \"Chi-Baba, Chi-Baba (My Bambino Go to Sleep)\" and \"I Can't Give You Anything But Love\", as well as accompanying herself on ukulele and dancing. When the show was originally broadcast on BBC TV it was 60 minutes in length and also included performances from Winifred Atwell, Evelyn Dove, Cyril Blake and his Calypso Band, Edric Connor and Mable Lee, and was produced by Eric Fawcett. The six-minute footage of Miss Hall is all that survives of the show.\n\nFrom the following month, the wedding of Princess Elizabeth to Prince Philip also survives, as do various early 1950s productions such as \"It is Midnight, Dr Schweitzer\", \"The Lady from the Sea\" and the opening two episodes of \"The Quatermass Experiment\", although in varying degrees of quality. A complete 7-hour set of telerecordings of Queen Elizabeth II's 1953 coronation also exists. \n\nIn the era before satellite communications, kinescopes were used to distribute live events such as a royal wedding as quickly as possible to other countries of the Commonwealth that had started a television service. A Royal Air Force aircraft would fly the telerecording from the UK to Canada, where it would be broadcast over the whole North American network.\n\nEven after the introduction of videotape, the BBC and the ITV companies made black and white kinescopes of selected programs for international sales, and continued to do so until the early 1970s by which time programs were being videotaped in color. Most, if not all, videotapes from the 405-line era have long since been wiped as have many from the introduction of 625-line video to the early days of color. Consequently, the majority of British shows that still exist before the introduction of color, and a number thereafter, do so in the form of these telerecordings. A handful of shows, including some episodes of \"Doctor Who\" and most of the first series of \"Adam Adamant Lives!\", were deliberately telerecorded for ease of editing rather than being videotaped.\n\nIn September 1947, Eastman Kodak introduced the Eastman Television Recording Camera, in cooperation with DuMont Laboratories, Inc. and NBC, for recording images from a television screen under the trademark \"Kinephoto\". Prior to the introduction of videotape in 1956, kinescopes were the only way to record television broadcasts, or to distribute network television programs that were broadcast live from New York or other originating cities, to stations not connected to the network, or to stations that wished to show a program at a time different than the network broadcast. Although the quality was less than desirable, television programs of all types from prestigious dramas to regular news shows were handled in this manner.\n\nNBC, CBS, and DuMont set up their main kinescope recording facilities in New York City, while ABC chose Chicago. By 1951, NBC and CBS were each shipping out some 1,000 16mm kinescope prints each week to their affiliates across the United States, and by 1955 that number had increased to 2,500 per week for CBS. By 1954 the television industry’s film consumption surpassed that of all of the Hollywood studios combined.\n\nAfter the network of coaxial cable and microwave relays carrying programs to the West Coast was completed in September 1951, CBS and NBC instituted a \"hot kinescope\" process in 1952, where shows being performed in New York were transmitted west, filmed on two kinescope machines in 35 mm negative and 16 mm reversal film (the latter for backup protection) in Los Angeles, rushed to film processing, and then transmitted from Los Angeles three hours later for broadcast in the Pacific Time Zone.\n\nIn September 1956, NBC began making color \"hot kines\" of some of its color programs using a lenticular film process which, unlike color negative film, could be processed rapidly using standard black-and-white methods.\n\nEven after the introduction of Quadruplex videotape machines in 1956 removed the need for \"hot kines\", the television networks continued to use kinescopes in the \"double system\" method of videotape editing. It was impossible to slow or freeze frame a videotape at that time, so the unedited tape would be copied to a kinescope, and edited conventionally. The edited kinescope print was then used to conform the videotape master. More than 300 videotaped network series and specials used this method over a 12-year period, including the fast-paced \"Rowan & Martin's Laugh-In\".\n\nWith the variable quality of Kinescopes, networks looked towards alternative methods to replace them with a higher degree of quality.\n\nPrograms originally shot with film cameras (as opposed to kinescopes) were also used in television’s early years, although they were generally considered inferior to the big-production \"live\" programs because of their lower budgets and loss of immediacy.\n\nIn 1951, the stars and producers of the Hollywood-based television series \"I Love Lucy\", Desi Arnaz and Lucille Ball, decided to film the show directly onto 35 mm film using the three-camera system, instead of broadcasting it live. Normally, a live program originating from Los Angeles would be performed live in the late afternoon for the Eastern Time Zone, and seen on a kinescope three hours later in the Pacific Time Zone. But as an article in \"American Cinematographer\" explained,\n\nThe \"I Love Lucy\" decision introduced reruns to most of the American television audience, and set a pattern for the syndication of TV shows after their network runs (and later, for first-run airings via syndication).\n\nThe program director of the DuMont Television Network, James L. Caddigan, devised an alternative — the Electronicam. In this, all the studio TV cameras had built-in 35 mm film cameras which shared the same optical path. An Electronicam technician threw switches to mark the film footage electronically, identifying the camera \"takes\" called by the director. The corresponding film segments from the various cameras then were combined by a film editor to duplicate the live program. The \"Classic 39\" syndicated episodes of \"The Honeymooners\" were filmed using Electronicam (as well as the daily five-minute syndicated series \" Les Paul & Mary Ford At Home\" in 1954–55), but with the introduction of a practical videotape recorder only one year away, the Electronicam system never saw widespread use. The DuMont network did not survive into the era of videotape, and in order to gain clearances for its programs, was heavily dependent on kinescopes, which it called Teletranscriptions.\n\nAttempts were made for many years to take television images, convert them to film via kinescope, then project them in theaters for paying audiences. In the mid-1960s, Producer/entrepreneur H. William \"Bill\" Sargent, Jr. used conventional analog Image Orthicon video camera tube units, shooting in the B&W 819-line interlaced 25fps French video standard, using modified high-band quadruplex VTRs to record the signal. The promotors of Electronovision (not to be confused with Electronicam) gave the impression that this was a new system created from scratch, using a high-tech name (and avoiding the word kinescope) to distinguish the process from conventional film photography. Nonetheless, the advances in picture quality were, at the time, a major step ahead. By capturing more than 800 lines of resolution at 25 frame/s, raw tape could be converted to film via kinescope recording with sufficient enhanced resolution to allow big-screen enlargement. The 1960s productions used Marconi image orthicon video cameras, which have a characteristic white \"glow\" around black objects (and a corresponding black glow around white objects), which was a defect of the pickup. Later vidicon and plumbicon video camera tubes produced much cleaner, more accurate pictures.\n\nIn 1951, singer Bing Crosby’s company Bing Crosby Enterprises made the first experimental magnetic video recordings; however, the poor picture quality and very high tape speed meant it would be impractical to use. In 1956, Ampex introduced the first commercial Quadruplex videotape recorder, followed in 1958 by a color model. Offering high quality and instant playback at a much lower cost, Quadruplex tape quickly replaced kinescope as the primary means of recording television broadcasts.\n\nU.S. television networks continued to make kinescopes of their daytime dramas available (many of which still aired live into the late 1960s) as late as 1969 for their smaller network affiliates that did not yet have videotape capability but wished to time-shift the network programming. Some of these programs aired up to two weeks after their original dates, particularly in Alaska and Hawaii. Many episodes of programs from the 1960s survive only through kinescoped copies. The last 16 mm kinescopes of television programs ended in the late 1970s, as video tape recorders became more affordable.\n\nIn Australia, kinescopes were still being made of some evening news programs as late as 1977, if they were recorded at all. A recording of a 1975 episode of Australian series \"This Day Tonight\" is listed on the National Archives of Australia website as a kinescope, while surviving episodes of the 1978 drama series \"The Truckies\" also exist as kinescopes, indicating that the technology was still being used by ABC at that point.\n\nIn later years, film and television producers were often reluctant to include kinescope footage in anthologies because of its inherent inferior quality. While it is true that kinescopes did look inferior to live transmissions in the 1950s, it was due to the industry's technical limitations at that time. Even the best live transmission could look hazy by the time it reached the home viewer. Advances in broadcast technology soon allowed for a wider gray scale in black-and-white, and a fuller spectrum of colors, making kinescopes a perfectly viable commodity. This was demonstrated in the feature film \"Ten from Your Show of Shows\", a compilation of Sid Caesar kinescopes released to theaters. Reviewers were astonished at how good the kinescoped image looked on a large screen. Kinescopes have since lost their stigma of inferiority, and are commonly consulted today for archival purposes.\n\nUp until the early 1960s, much of the BBC's output, and British television in general, was broadcast live, and telerecordings would be used to preserve a programme for repeat showings, which had previously required the entire production being performed live for a second time. In the UK, telerecordings continued to be made after the advent of commercial broadcast videotape from 1958 as they possessed several distinct advantages, particularly for overseas program sales. Firstly, they were cheaper, easier to transport and more durable than video. Secondly, they could be used in any country regardless of the television broadcasting standard, which was not true of videotape. Thirdly, the system could be used to make black and white copies of color programs for sale to television stations who were not yet broadcasting in color.\n\nThe telerecording system could be of a very high quality, easily reproducing the full detail of the television picture. One side effect of the system was that it removed the 'fluid' look of interlaced video and 'filmized' the picture.\n\nThe system was largely used for black and white reproduction. Although some color telerecordings were made, they were generally in the minority as by the time color programs were widely needed for sale, video standards conversion was easier and higher quality and the price of videotape had become much reduced. Before videotape became the exclusive transmission format during the early to mid-1980s, any (color) video recordings used in documentaries or filmed program inserts were usually transferred onto film.\n\nIn the 1950s a home telerecording kit was introduced in Britain, allowing enthusiasts to make 16 mm film recordings of television programs . The major drawback, apart from the short duration of a 16 mm film magazine, was that a large opaque frame had to be placed in front of the TV set in order to block out any stray reflections, making it impossible to watch the set normally while filming. It is not known if any recordings made using this equipment still exist.\n\nBritish broadcasters used telerecordings for domestic purposes well into the 1960s, with 35 mm being the film gauge usually used as it produced a higher quality result. For overseas sales, 16 mm film would be used, as it was cheaper. Although domestic use of telerecording in the UK for repeat broadcasts dropped off sharply after the move to color in the late 1960s, 16 mm black and white film telerecordings were still being offered for sale by British broadcasters well into the 1970s.\n\nTelerecording was still being used internally at the BBC in the 1980s too, to preserve copies for posterity of programs which were not necessarily of the highest importance, but which nonetheless their producers wanted to be preserved. If there were no videotape machines available on a given day, then a telerecording would be made. There is evidence to suggest that the children's magazine program \"Blue Peter\" was occasionally being telerecorded as late as 1985. After this point, however, cheap domestic videotape formats such as VHS could more easily be used to keep a back-up reference copy of a program.\n\nAnother occasional use of telerecording into the late 1980s was by documentary makers working in 16 mm film who wished to include a videotape-sourced excerpt in their work, although such use was again rare.\n\nIn other territories, film telerecordings were stopped being produced after the introduction of videotape. In Czechoslovakia, the first videotape recorders (Machtronics MVR-15) were introduced in 1966, but soon were replaced by the Ampex 2\" Quadruplex in 1967. Most of the programs, like TV dramas, were recorded on video, but only a few programmes continued to be telerecorded onto 16mm film. The last telerecording was produced in 1969 and soon after all programmes were recorded on video only.\n\nKinescopes were intended to be used for immediate rebroadcast, or for an occasional repeat of a prerecorded program; thus, only a small fraction of kinescope recordings remain today. Many television shows are represented by only a handful of episodes, such as with the early television work of comedian Ernie Kovacs, and the original version of \"Jeopardy!\" hosted by Art Fleming.\n\nAnother purpose of Kinescopes involved satisfying show sponsors. Kinescopes sometimes would be sent to the advertising agency for the sponsor of a show so that the ad agency could determine whether or not the sponsor's ads appeared properly. Due to this practice, some kinescopes have actually been discovered in the storage areas of some of these older advertising agencies or in the storage areas of the program sponsors themselves.\n\nKinescopes were also used for some live television programs, like \"Captain Kangaroo\", when back-to-back episodes were made in a day for different time zones.\n\nTelerecordings form an important part of British television heritage, preserving what would otherwise have been lost. Nearly every pre-1960s British television programme in the archives is in the form of a telerecording, along with the vast majority of existing 1960s output. Videotape was expensive and could be wiped and re-used; film was cheaper, smaller, and in practice more durable. Only a very small proportion of British television from the black and white era survives at all; perhaps 5% from the 1953-58 period and 8-10% from the 1960s.\n\nMany recovered programmes, particularly those made by the BBC, have been returned as telerecordings by foreign broadcasters or private film collectors from the 1980s onwards, as the BBC has taken stock of the large gaps in its archive and sought to recover as much of the missing material as possible. Many of these surviving telerecorded programmes, such as episodes of \"Doctor Who\", \"Steptoe and Son\" and \"Till Death Us Do Part\" continue to be transmitted on satellite television stations such as UKTV Gold, and many such programmes have been released on VHS and DVD.\n\nIn late 2008 the BBC transmitted an episode of \"Dad's Army\" after the original colour had been restored to the only surviving monochrome film recording of \"Room at the Bottom\".\n\nNTSC television images are scanned at roughly 60 Hz, with two interlaced fields per frame, displayed at 30 frames per second.\n\nA kinescope must be able to:\n\nIn kinescoping an NTSC signal, 525 lines are broadcast in one frame. A 35 mm or 16 mm camera exposes one frame of film for every one frame of television (525 lines), and moving a new frame of film into place during the time equivalent of one field of television (131.25 lines). In the British 405-line television system, the French 819-line television system and the European 625-line television system, television ran at 25 frames—or more correctly, 50 fields—per second, so the film camera would also be run at 25 frames per second rather than the cinematic film standard of 24 frames.\n\nTherefore, in order to maintain successful kinescope photography, a camera must expose one frame of film for \"exactly\" 1/30th or 1/25th of a second, the time in which one frame of video is transmitted, and move to another frame of film within the small interval of 1/120 of a second. In some instances, this was accomplished through means of an electronic shutter which cuts off the TV image at the end of every set of visible lines.\n\nMost U.S. kinescope situations, however, utilized a mechanical shutter, revolving at 24 revolutions per second. This shutter had a closed angle of 72° and an open angle of 288°, yielding the necessary closed time of 1/120 of a second and open time 1/30 of a second. Using this shutter, in 1 second of video (60 fields equaling 30 frames), 48 television fields (totaling to 24 frames of video) would be captured on 24 frames of film, and 12 additional fields would be omitted as the shutter closed and the film advanced.\n\nBecause television is a field- rather than frame-based system, however, not all the information in the picture can be retained on film in the same way as it can on videotape. The time taken physically to move the film on by one frame and stop it so that the gate can be opened to expose a new frame of film to the two fields of television picture is much longer than the vertical blanking interval between these fields—so the film is still moving when the start of the next field is being displayed on the television screen. It is not possible to accelerate the film fast enough to get it there in time without destroying the perforations in the film stock—and the larger the film gauge used, the worse the problem becomes.\n\nThe problem of adapting the way the image is either displayed or captured on film, to get around the above, was solved in various different ways as time went on—improving the quality of the image.\n\nThe 72°/288° shutter and the systematic loss of 12 fields per second were not without its side effects. In going from 30 frame/s to 24 frame/s, the camera photographed \"part\" of some fields. The juncture on the film frame where these part-fields met was called a \"splice\".\n\nIf the timing was accurate, the splice was invisible. However, if the camera and television were out of phase, a phenomenon known as \"shutter bar\" or \"banding\" took place. If the shutter was slow in closing, overexposure resulted where the part-fields joined and the \"shutter bar\" took the form of a white line. If the shutter closed too soon, underexposure took place and the line was black. The term \"banding\" referred to the phenomenon occurring on the screen as two bars.\n\nA simpler system less prone to breakdown was to suppress one of the two fields in displaying the television picture. This left the time in which the second field was displayed for the film camera to advance the film by one frame, which proved enough. This method was also called 'Skip field' recording.\n\nThis method had several disadvantages. In missing out every second field of video, half the information of the picture was lost on such recordings. The resulting film consisted of fewer than 200 lines of picture information and as a result the line structure was very apparent; the missing field information also made movement look very 'jerky'.\n\nA development on the suppressed field system was to display the image from one of the fields at a much higher intensity on the television screen during the time when the film gate was closed, and then capture the image as the second field was being displayed. By adjusting the intensity of the first field, it was possible to arrange it so that the luminosity of the phosphor had decayed to exactly match that of the second field, so that the two appeared to be at the same level and the film camera captured both.This method came to be preferred.\n\nAnother technique developed by the BBC known as 'spot wobble' involved the addition of an extremely high frequency but low voltage sine wave to the vertical deflection plate of the television screen, which changed the moving 'spot' through which the television picture was displayed into an elongated oval. While this made the image slightly blurred, it removed the visible line structure and resulted in a better image. It also prevented moiré patterns appearing when the resulting film was re-broadcast on television and the lines of the recording did not match the scan lines.\n\nThe first successful procedure was to use the Mechau film projector mechanism in reverse. The Mechau system used a synchronised rotating mirror to display each frame of a film in sequence without the need for a gate. When reversed, a high-quality television monitor was set up in place of the projection screen, and unexposed film stock is run through at the point where the lamp was illuminating the film.\n\nThis procedure had the advantage of capturing both fields of the frame on a film, but it was difficult to keep the mirrors running at the right speed and all the equipment adjusted correctly, which often resulted in poor quality output. An additional problem was that the whole procedure took place in an open room and it was known for insects to settle on the screen which were then permanently present on the film recording. The Mechau film magazine only held enough for nine minutes so two recorders were needed to run in sequence in order to record anything longer.\n\nLenses did not need a great depth of field, but had to be capable both of producing a very sharp image with high resolution of a flat surface and of doing so at high speed. In order to keep from light fall-off on the perimeter of the lens, a coated lens was preferable. 40 mm or 50 mm lenses were usually used with 16mm in calibrated mounts. Focus was checked by examining a print yielded under a microscope.\n\nIn order to record half-hour programs without interruption, magazines were designed which accommodated a load of 1,200 feet for 16 mm film. Stations recording on 35 mm utilized 6,000 foot magazines for one hour of continuous recording.\n\nThe camera could be equipped with sound recording to place the soundtrack and picture on the same film for single system sound recording. More commonly, the alternative double system, whereby the soundtrack was recorded on an optical recorder or magnetic dubber in sync with the camera, yielded a better quality sound track and greatly facilitated editing.\n\nKinescope tubes intended for photographic use were coated with phosphors rich in blue and ultra-violet radiations. This permitted the use of positive type emulsions for photographing in spite of their slow film speeds. The brightness range of kinescope tubes were about 1 to 30.\n\nKinescope images were capable of great flexibility. The operator could make the image brighter or darker, adjust contrast, width and height, turn left, right or upside down, and positive or negative.\n\nSince kinescopes were able to produce a negative picture, direct positive recordings could be made by simply photographing a negative image on the kinescope tube. When making a negative film, in order for final prints to be in the correct emulsion position, the direction of the image was reversed on the television. This applied only when double system sound was used.\n\nFor kinescopes, 16 mm film was the common choice by most studios because of the lower cost of stock and film processing, but in the larger network markets, it was not uncommon to see 35 mm kinescopes, particularly for national rebroadcast. By law, all film supplied to TV stations, both 16 mm and 35 mm had to be on a non-flammable, safety film base.\n\nFor U.S. video recording, fine grain positive stock was the most common used because of its low cost and high resolution yield. Of the fine grain stocks, the following were recommended by film manufacturers:\n\nVideotape engineer Frederick M. Remley wrote of kinescope recordings,\n\nBecause each field is sequential in time to the next, a kinescope film frame that captured two interlaced fields at once often showed a ghostly fringe around the edges of moving objects, an artifact not as visible when watching television directly at 50 or 60 fields per second.\n\nSome kinescopes filmed the television pictures at the same frame rate of 30 full frames per second, resulting in more faithful picture quality than those that recorded at 24 frames per second. The standard was later changed for color TV to 59.94 fields/s. or 29.97 frame/s. when color TV was invented. \n\nIn the era of early color TV, the chroma information included in the video signal filmed could cause visible artifacts. It was possible to filter the chroma out, but this was not always done. Consequently, the color information was included (but not in color) in the black & white film image. Using modern computing techniques, the color may now be recovered, a process known as color recovery.\n\nIn recent years, the BBC has introduced a video process called \"VidFIRE\", which can restore kinescope recordings to their original frame rate by interpolating video fields between the film frames.\n\nCertain performers or production companies would require that a kinescope be made of every television program. Such is the case with performers Jackie Gleason and Milton Berle, for whom nearly complete program archives exist. As Jackie Gleason’s program was broadcast live in New York, the show was kinescoped for later rebroadcast for the West Coast. Per his contract, he would receive one copy of each broadcast, which he kept in his vault, and only released them to the public (on home video) shortly before his death in 1987.\n\nMilton Berle sued NBC late in his life, believing the kinescopes of a major portion of his programs were lost. However, the programs were later found in a warehouse in Los Angeles.\n\nMark Goodson-Bill Todman Productions, the producers of such TV game shows as \"What's My Line?\", had a significant portion of their output recorded on both videotape and kinescopes. These programs are rebroadcast on the American cable TV’s Game Show Network.\n\nAll of the NBC Symphony Orchestra telecasts with Arturo Toscanini, from 1948 to 1952, were preserved on kinescopes and later released on VHS and LaserDisc by RCA and on DVD by Testament. The original audio from the kinescopes, however, was replaced with high fidelity sound that had been recorded simultaneously either on transcription discs or magnetic tape.\n\nIn the mid-90s, Edie Adams, wife of Ernie Kovacs, claimed that so little value was given to the kinescope recordings of the DuMont Television Network that after the network folded in 1956 its entire archive was dumped into upper New York bay. Today however, efforts are made to preserve the few surviving DuMont kinescopes, with the UCLA Film and Television Archive having collected over 300 for preservation.\n\nIn September 2010, a kinescope of game 7 of the 1960 World Series was found in the wine cellar of Bing Crosby. The game was thought lost forever, but was preserved due to Crosby's superstition about watching the game live. The film was transferred to DVD and was broadcast on the MLB Network shortly afterwards.\n\nBecause videotape records at fifty interlaced fields per second and telerecordings at twenty-five progressive frames per second, videotaped programmes that exist now only as telerecordings have lost their characteristic \"live video\" look and the motion now looks filmic. One solution to this problem is VidFIRE, an electronic process to restore video-type motion.\n\nEarly Australian television drama series were recorded as kinescopes, such as \"Autumn Affair\" and \"Emergency\", along with variety series like \"The Lorrae Desmond Show\". Kinescopes continued to be made after video-tape was introduced to Australia; most existing episodes of the 1965-1967 children's series \"Magic Circle Club\" are kinescopes (per listings for episodes on National Film and Sound Archive website)\n\n\n"}
{"id": "489256", "url": "https://en.wikipedia.org/wiki?curid=489256", "title": "List of Canon products", "text": "List of Canon products\n\nThe following provides a partial list of products manufactured under the Canon brand.\n\nOther products manufactured and/or service-rendered under the Canon brand may not appear here. Such products may include office or industrial application devices, wireless LAN products, and semiconductor and precision products.\n\n\n\n\n\nSeiki Kogaku (now Canon) began to develop and subsequently to produce rangefinder cameras with the Kwanon prototype in 1933, based on the Leica II 35mm camera, with separate rangefinder and view finder systems (3 windows). Production began with the Hansa Canon on the Leica III format through WWII. Post war Canon resumed production of pre-war designs in early 1946 with the JII viewfinder and the S1 rangefinder. But in late 1946 they introduced the SII which departed from the Leica design by offering a combined viewfinder/rangefinder system, reducing the windows on the front of the camera to two. However, in most other respects these cameras remained visually similar to the Leica III.\n\n\nIn 1956, Canon departed from the Leica II Style and developed a more contemporary look, along with a Contax style self-timer level to the left of the lens mount. This was the first Canon camera with a swing-open camera back for film loading. Upper end models had a new three-mode viewfinders and winding triggers.\n\n\nCanon partnered with US manufacturer Bell & Howell between 1961–1976 and a few Canon products were sold in the US under the Bell & Howell brand e.g. Canon 7 Rangefinder, Canon EX-EE, and the Canon TX.\n\n\"(See also:)\"\n\nCanon developed and produced the Canon R lens mount for film SLR cameras in 1959. The FL lens mount replaced R-mounts in 1964.\n\nDetails \n\nCanon developed and produced the Canon FL lens-mount standard for film SLR cameras from 1964 to replace the Canon R lens-mount standard. The FD lens mount standard replaced FL-mounts in 1971.\n\nIn 1969 Canon introduced an economy camera/lens system where the rear three elements (in two groups) were built-on-to the camera, and several front element options could be interchanged. This had been used by Zeiss-Ikon in their mid-level cameras of their Contaflex series, and by Kodak in early interchangeable lenses for the top-end Retina series (later going to full lenses). Canon offered four lens options: 35mm f/3.5, 50mm f/1.8, 95mm f/3.5, and 1255mm f/3.5.\n\nThrough the lens metering was center weighted and automatic exposure was shutter speed priority. Only two cameras were offered and the line was not successful.\n\n\nCanon developed and produced the Canon FD lens mount standard for film SLR cameras from 1971 to replace the FL lens mount standard.\nThe FD mount had two variants – original lenses used a breechlock collar to mount whilst later versions used a standard bayonet twist lock with a short twist action.\nThe EF lens mount standard superseded FD-mounts in 1987. Canon ceased to produce FD-mount cameras in 1994.\n\n\n\n\nIn 1987, Canon introduced the EOS Single-lens reflex camera system along with the EF lens-mount standard to replace the 16-year-old FD lens-mount standard; EOS became the sole SLR camera-system used by Canon . Canon also used EOS for its digital SLR cameras. All current film and digital SLR cameras produced by Canon use the EOS autofocus system. Canon introduced this system in 1987 along with the EF lens mount standard. The last non-EOS based SLR camera produced by Canon, the Canon T90 of 1986, is widely regarded as the template for the EOS line of camera bodies, although the T90 employed the older FD lens-mount standard.\n\nFor a detailed list of EOS Film and digital SLR cameras, see \"Canon EOS\".\n\n\"See Canon EOS\"\n\n\n\"See Canon EOS\"\n\n\n\n\n\n\n\n\n\"US names listed\"\n\n\n\n\n\n\n\n\nCanon 7d\n\n\n\n\n\n\n\n\nSpeedlite 300EZ,\nSpeedlite 420EZ,\nSpeedlite 430EZ,\nSpeedlite 540EZ\n\nThe 300T is a layover from the FD system, it was introduced with the FD mount Canon T90, but is compatible in TTL mode with most non-digital EF cameras.\n\nMacro Twin Lite MT-24EX\n, Macro Ring Lite MR-14EX\n, Macro Ring Lite ML-3\n\n\nThe \"iR\" series uses Ultra Fast Rendering (UFR) printing system, and some models use UFR II, a page description language.\n\n\n\n\nStarWriter Jet 300 — a word processor and \"Personal Publishing System\".\n\nBeginning in Spring 1993, Canon produced a series of notebooks with integrated inkjet printers called NoteJet. The initial price for the first-model NoteJet was U.S. $2,499. The NoteJet lineup was eventually discontinued, and computers belonging to the series are valued by collectors.\n\n\nCanon printers are supplied with Canon Advanced Printing Technology (CAPT), a printer driver software-stack developed by Canon. The company claims that its use of data compression reduces their printer's memory requirement, good quality compared to conventional laser printers, and also claim that it increases the data transfer rate when printing high-resolution graphics.\n\n\n\nCanon refers to inkjet printers as \"bubblejets\", hence the frequent BJC-prefix.\n\n\nIn Japan, the models are denoted with a trailing “i”, whereas in the rest of the world they are denoted with a leading “i”. While the 50i corresponds to the i70, for all other corresponding models the numerical model numbers are identical.\nThe “X” denotes models sold under special dispensation by retail outles in Europe.\n\n\n\nSince about 2005 Canon introduced a numbering scheme for some whereby the least significant (non-zero) digit signifies the geographic region (“3” signifying Japan) the device is sold in. This leads to a large number of models, all belonging to the same family, but possibly incompatible to some degree, and also makes it difficult to ascertain whether a device is unique or part of an existing family. The software driver filename will often use the family designation.\n\nSome MP devices have fax capability (MP740).\nR=remote\n\nThe DS700 and DS810 are inkjet printers, all the other models are thermal dye-sublimation printers using ALPS technology.\n\n\n\"See Canon EF lenses for the product line-up\".\n\"See Canon EF-S lenses for the product line-up\".\n\nEF-S lenses are built for APS-C 1.6x crop sensors, so it will only work with models that use this sensor size, such as: Canon EOS Digital Rebel series (300D through 750D and 760D, 100D, and 1000D through 1200D), and newer cameras in the prosumer Canon EOS Digital series (20D through 80D, 20Da, 60Da, 7D, and 7D MkII). When EF-S lenses are used on a 35mm (full frame) camera, the back element will hit the mirror assembly or cause massive amounts of vignetting since the sensor is bigger than the image produced by the lens.\n\n\"See Canon FD lenses for the product line-up\".\n\n\"See Canon FL lenses for the product line-up\".\n\n\n\n\"Note: Even though the tilt-shift and dedicated macro lenses are designated TS-E and MP-E respectively, these lenses are still compatible with the EF mount.\"\n\n\n\nApplications bundled with Canon Digital Cameras and printers include:\n\nCanon TrueType Font Pack is a floppy disk collection of supplementar truetype fonts for some Canon printers of years '90 and useful for Windows 3.1 and 95.\n\nThe fonts contained in the collection was:\n\nCanon TrueType Font Pack is a floppy disk collection of supplementar truetype fonts for some Canon printers of years '90 and useful for Windows 3.1 and 95.\n\nThe fonts contained in the collection was:\n\n\n"}
{"id": "662340", "url": "https://en.wikipedia.org/wiki?curid=662340", "title": "List of duplicating processes", "text": "List of duplicating processes\n\nThis is a partial list of text and image duplicating processes used in business and government from the Industrial Revolution forward. Some are mechanical and some are chemical. There is naturally some overlap with printing processes and photographic processes, but the challenge of precisely duplicating business letters, forms, contracts, and other paperwork prompted some unique solutions as well. There were many short-lived inventions along the way.\n\n\"Within each type, the methods are arranged in very rough chronological order.\"\n\n\n\n"}
{"id": "9840203", "url": "https://en.wikipedia.org/wiki?curid=9840203", "title": "List of longest runways", "text": "List of longest runways\n\n\n\n"}
{"id": "1577252", "url": "https://en.wikipedia.org/wiki?curid=1577252", "title": "List of radars", "text": "List of radars\n\nThis is a list of radars. A radar is an electronic system used to detect, range (determine the distance of), and map various types of targets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEarly S-band RADAR Designations\nFrom February 1943 the US used a universal system to identify radar variants, consisting of three letters and a number, respectively designating platform, type of equipment, function, and version. This system was continued after WWII with multiservice designations being prefixed by 'AN/' for Army-Navy.\n\nBuShips 1943 classifications\nMulti-service classifications\n\nMulti-service classification codes according to the Joint Electronics Type Designation System.\n\nSpecific radar systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "21016331", "url": "https://en.wikipedia.org/wiki?curid=21016331", "title": "Location awareness", "text": "Location awareness\n\nLocation awareness refers to devices that can passively or actively determine their location. Navigational instruments provide location coordinates for vessels and vehicles. Surveying equipment identifies location with respect to a well-known locationa wireless communications device. \"Network location awareness\" (NLA) describes the location of a node in a network.\n\nThe term applies to navigating, real-time locating and positioning support with global, regional or local scope. The term has been applied to traffic, logistics, business administration and leisure applications. Location awareness is supported by navigation systems, positioning systems and/or locating services.\n\nLocation awareness without the active participation of the device is known as non-cooperative locating or detection.\n\nThe term originated for configurations settings of network systems and addressed network entities. \"Network location awareness\" (NLA) services collect network configuration and location information, and notify applications when this information changes. With the advent of global positioning systems and radio-equipped mobile devices, the term was redefined to include consumer-focused applications.\n\nWhile location awareness began as a matter of static user location, the notion was extended to reflect movement. Context models have been proposed to support context-aware applications which use location to tailor interfaces, refine application-relevant data, increase the precision of information retrieval, discover services, make user interaction implicit and build smart environments. For example, a location-aware mobile phone may confirm that it is currently in a building.\n\nDescription in logical terms uses a structured textual form. International standardisation offers a common method using ISO/TS 16952 as originated with German standards DIN EN 61346 and DIN EN 81346.\n\nLocation in mathematical terms offers coordinates that refer to a nominated point of reference.\n\nLocation in network terms relates to locating network nodes. These include:\n\n\n\"Crisp\" locating offers precise coordinates, using wireless signals or optical sighting, possibly with phase angle measurements. Coordinates are relative to either a standardized system of coordinates, e.g. WGS84, or a fixed object such as a building plan. Real-time locating adds timely delivery of results, especially for moving targets. Real time locating is defined with ISO/IEC 19762-5 and ISO/IEC 24730-1. Fuzzy locating offers less precision, e.g., presence \"near\" a point of reference. Measuring wireless power levels can supply this degree of precision. Less sophisticated systems can use wireless distance measurements to estimate a point of reference in polar coordinates (distance and direction) from another site. Index locating indicates presence at a known location, as with fixed RFID readers and RFID tags.\n\nLocation-aware systems address the acquisition of coordinates in a grid (for example using distance metrics and lateration algorithms) or at least distances to reference points (for example discriminating presence at a certain choke point on a corridor or in a room of a building).\n\nNavigation and reckoning are key concerns for seafarers, aviators and professional drivers. The task is to dynamically determine the current location and the time, distance and direction to destination. radar served for regional demand and NAVSTAR satellite systems for global demand. GPS and similar systems have become ubituitous in long-haul transport operation and are becoming a standard automobile feature.\n\nSurveying is the static complement to navigating. It is essential for delineating land ownership and for architects and civil engineers designing construction projects. Optical surveying technology preceded LASER triangulating aids.\n\nCurrently location awareness is applied to design innovative process controls, and is integral to ubiquitous and wearable computing. On mobile devices, location aware search can prioritize results that are close to the device. Conversely, the device location can be disclosed to others, at some cost to the bearer's privacy.\n\nRFID provides a time/location reference for an object, but does not indicate that the object remains at that location, which is sufficient for applications that limit access, such as tracking objects entering and leaving a warehouse, or for objects moving on a fixed route, such as charging tolls for crossing a bridge.\n\nLocation awareness enables new applications for ubiquitous computing systems and mobile phones. Such applications include the automatic reconfiguration of a computing device to suit the location in which it is currently being used (examples include ControlPlane and Locamatic), or publishing a user's location to appropriate members of a social network, and allowing retailers to publish special offers to potential customers who are near to the retailers. Allegedly, individuals gain self confidence with confirmation of current whereabouts.\n\nWhile governments have created global systems for computing locations, independent localized systems exist at scales ranging from one building to sub-national regions.\n\nSuch solutions may apply concepts of RTLS and WPAN, wireless LAN or DECT, with results in proprietary terms of floor plans or room numbers. Local systems degrade as distance from the locality increases. Applications include the automatic reconfiguration of a computing device to suit the location in which it is currently being used.\n\nThis approach uses for example mobile phone systems, such as 3GPP, GSM or LTE, typically returning information in standardized coordinates as with WGS84 in standardized formats such as NMEA for outdoor usage or in symbolic coordinates referring to street addresses.\n\nThis approach relies on GPS technology, currently supplied by NAVSTAR and may in future employ the pending Galileo (satellite navigation) system, generally adopting WGS84 and NMEA. Applications include avalanche rescue or emergency and mountain rescue as well as with search and rescue, (SAR) and combat search and rescue, (CSAR).\n\n\n"}
{"id": "17864", "url": "https://en.wikipedia.org/wiki?curid=17864", "title": "Luddite", "text": "Luddite\n\nThe Luddites were a secret oath-based organization of English textile workers in the 19th century, where a radical faction destroyed textile machinery as a form of protest. The group was protesting the use of machinery in a \"fraudulent and deceitful manner\" to get around standard labour practices. Luddites feared correctly that the time spent learning the skills of their craft would go to waste as machines would replace their role in the industry. It is a misconception that the Luddites protested against the machinery itself in an attempt to halt the progress of technology. Over time, however, the term has come to mean one opposed to industrialisation, automation, computerisation, or new technologies in general. The Luddite movement began in Nottingham and culminated in a region-wide rebellion that lasted from 1811 to 1816. Mill and factory owners took to shooting protesters and eventually the movement was suppressed with military force.\n\nAlthough the origin of the name Luddite () is uncertain, the movement was said to be named after Ned Ludd, an apprentice who allegedly smashed two stocking frames in 1779 and whose name had become emblematic of machine destroyers. Ned Ludd, however, was completely fictional and used as a way to shock and provoke the government. The name evolved into the imaginary General Ludd or King Ludd, who, like Robin Hood, was reputed to live in Sherwood Forest.\n\nThe lower classes of the 18th century, generally speaking, were not openly disloyal to the king or government. Overall, violent action was rare because punishments were harsh. The majority of individuals were primarily concerned with meeting their own daily needs. This movement towards aggression in the 19th century can be seen as part of the rise in English working-class discontent due to the Industrial Revolution. Working conditions in the mills were harsh but efficient enough to threaten the livelihoods of skilled artisans. The new inventions allowed for faster and cheaper labour because machines were operated by less-skilled, low-wage labourers. The Luddites were not afraid of technology and did not attempt to eliminate technology out of fear. Their goal was instead to gain a better bargaining position with their employers. Luddism was in fact a prototypical, insurrectionary labour movement involving a loose coalition of dissenters.\n\nResearch by Kevin Binfield and others asserts that since organized action by stockingers had occurred at various times since 1675, the movements of the early 19th century must be viewed in the context of the hardships suffered by the working class during the Napoleonic Wars, rather than as an absolute aversion to machinery. Irregular rises in food prices provoked the Keelmen working in the port of Tyne to riot in 1710 and tin miners to steal from granaries at Falmouth in 1727. There was a rebellion in Northumberland and Durham in 1740, and the assault of Quaker corn dealers in 1756. Skilled artisans in the cloth, building, shipbuilding, printing and cutlery trades organized friendly societies to peacefully insure themselves against unemployment, sickness, and in some cases against intrusion of \"foreign\" labour into their trades, as was common among guilds.\n\nMalcolm L. Thomis argued in his 1970 history, \"The Luddites\", that without the structure of a union, machine-breaking was one of very few mechanisms workers could use to increase pressure on employers, to undermine lower-paid competing workers and to create solidarity among workers, \"These attacks on machines did not imply any necessary hostility to machinery as such; machinery was just a conveniently exposed target against which an attack could be made.\"\n\nAn agricultural variant of Luddism, centering on the breaking of threshing machines, occurred during the widespread Swing Riots of 1830 in southern and eastern England.\n\nThe Luddite movement emerged during the harsh economic climate of the Napoleonic Wars, which saw a rise of difficult working conditions in the new textile factories. Luddites objected primarily to the rising popularity of automated textile equipment, threatening the jobs and livelihoods of skilled workers as this technology allowed them to be replaced by cheaper and less skilled workers. The movement began in Arnold, Nottingham on 11 March 1811 and spread rapidly throughout England over the following two years. Handloom weavers burned mills and pieces of factory machinery. Textile workers destroyed industrial equipment during the late 18th century, prompting acts such as the Protection of Stocking Frames, etc. Act 1788.\n\nThe Luddites met at night on the moors surrounding industrial towns to practice drills and maneuvers. Their main areas of operation began in Nottinghamshire in November 1811, followed by the West Riding of Yorkshire in early 1812 then Lancashire by March 1813. They smashed stocking frames and cropping frames among others. There does not seem to have been any political motivation behind the Luddite riots and there was no national organization. The men were merely attacking what they saw as the reason for the decline in their livelihoods. Luddites battled the British Army at Burton's Mill in Middleton and at Westhoughton Mill, both in Lancashire. The Luddites and their supporters anonymously sent death threats to, and possibly attacked, magistrates and food merchants. Activists smashed Heathcote's lacemaking machine in Loughborough in 1816. He and other industrialists had secret chambers constructed in their buildings that could be used as hiding places during an attack.\n\nIn 1817, an unemployed Nottingham stockinger and probably ex-Luddite, named Jeremiah Brandreth led the Pentrich Rising. While this was a general uprising unrelated to machinery, it can be viewed as the last major Luddite act.\n\nThe British Army clashed with the Luddites on several occasions. At one time there were more British soldiers fighting the Luddites than there were fighting Napoleon on the Iberian Peninsula. Three Luddites, led by George Mellor, ambushed and assassinated mill owner William Horsfall of Ottiwells Mill in Marsden, West Yorkshire at Crosland Moor in Huddersfield. Horsfall had remarked that he would \"Ride up to his saddle in Luddite blood.\" Mellor fired the fatal shot to Horsfall's groin, and all three men were arrested.\n\nLord Byron denounced what he considered to be the plight of the working class, the government’s inane policies and ruthless repression in the House of Lords on 27 February 1812: \"I have been in some of the most oppressed provinces of Turkey; but never, under the most despotic of infidel governments, did I behold such squalid wretchedness as I have seen since my return, in the very heart of a Christian country.\"\n\nThe British government sought to suppress the Luddite movement with a mass trial at York in January 1813, following the attack on Cartwrights mill at Rawfolds near Cleckheaton. The government charged over 60 men, including Mellor and his companions, with various crimes in connection with Luddite activities. While some of those charged were actual Luddites, many had no connection to the movement. Although the proceedings were legitimate jury trials, many were abandoned due to lack of evidence and 30 men were acquitted. These trials were certainly intended to act as show trials to deter other Luddites from continuing their activities. The harsh sentences of those found guilty, which included execution and penal transportation, quickly ended the movement.\n\nParliament made \"machine breaking\" (i.e. industrial sabotage) a capital crime with the Frame Breaking Act of 1812 and the Malicious Damage Act 1861. Lord Byron opposed this legislation, becoming one of the few prominent defenders of the Luddites after the treatment of the defendants at the York trials. Coincidently, Lord Byron's only legitimate daughter Ada Lovelace would become the first computer programmer by combining the technology of the Analytical Engine with the Jacquard loom.\n\nIn 1867 Karl Marx wrote that it would be some time before workers were able to distinguish between the machines and \"the form of society which utilizes these instruments\" and their ideas. \"The instrument of labour, when it takes the form of a machine, immediately becomes a competitor of the workman himself.\"\n\nIn the 19th century, occupations that arose from the growth of trade and shipping in ports, also in 'domestic' manufacturers, were notorious for precarious employment prospects. Underemployment was chronic during this period, and it was common practice to retain a larger workforce than was typically necessary for insurance against labour shortages in boom times.\n\nMoreover, the organization of manufacture by merchant-capitalists in the textile industry was inherently unstable. While the financiers' capital was still largely invested in raw material, it was easy to increase commitment where trade was good and almost as easy to cut back when times were bad. Merchant-capitalists lacked the incentive of later factory owners, whose capital was invested in building and plants, to maintain a steady rate of production and return on fixed capital. The combination of seasonal variations in wage rates and violent short-term fluctuations springing from harvests and war, periodic outbreaks of violence are more easily understood.\n\nIn 1956, a speech said that \"organized workers were by no means wedded to a Luddite Philosophy.\" More recently, the term Neo-Luddism has emerged to describe opposition to many forms of technology. According to a manifesto drawn up by the Second Luddite Congress (April 1996; Barnesville, Ohio), Neo-Luddism is \"a leaderless movement of passive resistance to consumerism and the increasingly bizarre and frightening technologies of the Computer Age.\"\n\nThe term “Luddite fallacy” is used by economists in reference to the fear that technological unemployment inevitably generates structural unemployment and is consequently macroeconomically injurious. If a technological innovation results in a reduction of necessary labour inputs in a given sector, then the industry-wide cost of production falls, which lowers the competitive price and increases the equilibrium supply point which, theoretically, will require an increase in aggregate labour inputs.\n\n\n\n"}
{"id": "45821282", "url": "https://en.wikipedia.org/wiki?curid=45821282", "title": "MessagEase", "text": "MessagEase\n\nMessagEase is an input method and virtual keyboard for touchscreen devices. It relies on a new entry system designed by Saied B. Nesbat, formatted as a 3x3 matrix keypad where users may press or swipe up, down, left, right, or diagonally to access all keys and symbols. It is a keyboard that was designed for devices like cell phones, mimicking the early cell phones' limited number of 12 keys.\n\nThe most common letters (the large letters in the illustration below) are accessed by a tap. Less common letters are accessed by a slide. Example: Tapping the center square generates an 'o'. Sliding to the left from the same square generates a 'c'. A green trail shows the path of the finger. The keyboard supports multiple user dictionaries, used for word prediction and correction.\n\nThe software is developed and patented by ExIdeas, based in Belmont, California. It was first released in 2002 for the Palm, along with a paper in 2003.\n\nThe keyboard layout has a 3x3 matrix that allows for full-text entry. The letter placement is optimized for minimal movement distance between letters, allowing for faster typing.\n\nThe layout is 67% faster than a standard QWERTY software keyboard, and 31% faster than a multi-tap keyboard, when typing is modeled with Fitt's law.\n\nThe 9 most frequent letters in English texts: ETAONRISH, are placed on the keyboard so they can be accessed on a single click.\n\nThe next 17 less frequent letters: DLFCMUGYPWBVKJXQZ, are placed as to be triggered by a single move of the finger from or to the central key (O) (except for Z which is centered around the 'E' key together with some punctuation characters). For example, the letter V is typed by dragging the finger from A to O, and the letter D by moving from O to E.\n\nThe moves producing special characters, which includes 38 characters including accents and punctuation marks, are displayed on a complete keyboard showing up when the user drags the space bar upwards.\n\nThis is not an alternate keyboard in the sense that the key pair moves are valid on both keyboard. It is rather a mnemonic help, which is normally hidden to avoid overwhelming the user with spurious information.\n\nA small vertical bar on the right gives direct access to the cut/copy/paste operations, the numeric keypad, the uppercase/lowercase control, as well the usual F1-F12 control keys. This is also commanded by moving the finger from one cell to an other.\n\nA set of small movements makes the life of the typist easier, like drawing a small circle or a back and forth movement to write a letter uppercase, or prolonging the movement to put accent on letters.\n\nThe keyboard can be resized to fit the need of the user, and is also provided in a double sized version with the numeric keypad on the side of the alphabetic keypad.\n\nThe keyboard is currently available for Android devices, iOS devices and the Apple Watch.\n\n\"Currently supported languages:\"\nMessagEase was released in 2002 for the Palm. It was also originally a competitor to the T9 predictive input method, on a 12-button phone, with 9 number buttons. In this first iteration, each of the 9 primary characters needed to be pressed twice in a row, and secondary characters were entered by first pressing the main button, and then pressing one of the remaining 8 buttons. In this first iteration, because many letters required two presses, it was not significantly faster than the Multi-tap input method.\n\nMessagEase is now exclusively for touch screens, and no longer has physical 12-button support. All characters are now entered by tapping or swiping.\n\n"}
{"id": "53680386", "url": "https://en.wikipedia.org/wiki?curid=53680386", "title": "MicroVision, Inc.", "text": "MicroVision, Inc.\n\nMicroVision, Inc. is a US company that develops laser scanning technology for projection, 3D sensing, and image capture. MicroVision's display technology uses a micro-electrical mechanical systems (MEMS) scanning mirror with lasers, optics and electronics to project and/or capture images. The company licenses its products primarily to original equipment manufacturers (OEMs) such as Sony Corporation and STMicroelectronics\n\nThe MEMS scanning micro-mirror is the basis of MicroVision’s technology platform. The MEMS design consists of a silicon device with a millimeter-scale mirror at the center. The mirror is connected to flexures that allow it to swing vertically and horizontally to display (or capture) an image. In projection-mode the MEMS laser beam scanning display method can be compared to raster scanning in a cathode ray tube (CRT) display.\nProduct applications include mobile projection, virtual retinal display, head-mounted display, automotive head-up display, and 3D depth sensing (LiDAR). Head-mounted displays are an emerging area of development. \n\nIn May 2018, Microvision entered into a license agreement with a global technology company to use company's display technology to manufacture and sell display-only engines.\n\n"}
{"id": "48144", "url": "https://en.wikipedia.org/wiki?curid=48144", "title": "Microcomputer", "text": "Microcomputer\n\nA microcomputer is a small, relatively inexpensive computer with a microprocessor as its central processing unit (CPU). It includes a microprocessor, memory, and minimal input/output (I/O) circuitry mounted on a single printed circuit board. Microcomputers became popular in the 1970s and 1980s with the advent of increasingly powerful microprocessors. The predecessors to these computers, mainframes and minicomputers, were comparatively much larger and more expensive (though indeed present-day mainframes such as the IBM System z machines use one or more custom microprocessors as their CPUs). Many microcomputers (when equipped with a keyboard and screen for input and output) are also personal computers (in the generic sense).\n\nThe abbreviation \"micro\" was common during the 1970s and 1980s, but has now fallen out of common usage.\n\nThe term \"microcomputer\" came into popular use after the introduction of the minicomputer, although Isaac Asimov used the term in his short story \"The Dying Night\" as early as 1956 (published in \"The Magazine of Fantasy and Science Fiction\" in July that year). Most notably, the microcomputer replaced the many separate components that made up the minicomputer's CPU with one integrated microprocessor chip.\n\nThe French developers of the Micral N (1973) filed their patents with the term \"Micro-ordinateur\", a literal equivalent of \"Microcomputer\", to designate a solid state machine designed with a microprocessor.\nIn the USA, the earliest models such as the Altair 8800 were often sold as kits to be assembled by the user, and came with as little as 256 bytes of RAM, and no input/output devices other than indicator lights and switches, useful as a proof of concept to demonstrate what such a simple device could do. \nHowever, as microprocessors and semiconductor memory became less expensive, microcomputers in turn grew cheaper and easier to use:\nAll these improvements in cost and usability resulted in an explosion in their popularity during the late 1970s and early 1980s.\nA large number of computer makers packaged microcomputers for use in small business applications. By 1979, many companies such as Cromemco, Processor Technology, IMSAI, North Star Computers, Southwest Technical Products Corporation, Ohio Scientific, Altos Computer Systems, Morrow Designs and others produced systems designed either for a resourceful end user or consulting firm to deliver business systems such as accounting, database management, and word processing to small businesses. This allowed businesses unable to afford leasing of a minicomputer or time-sharing service the opportunity to automate business functions, without (usually) hiring a full-time staff to operate the computers. A representative system of this era would have used an S100 bus, an 8-bit processor such as an Intel 8080 or Zilog Z80, and either CP/M or MP/M operating system.\nThe increasing availability and power of desktop computers for personal use attracted the attention of more software developers. In time, and as the industry matured, the market for personal computers standardized around IBM PC compatibles running DOS, and later Windows.\nModern desktop computers, video game consoles, laptops, tablet PCs, and many types of handheld devices, including mobile phones, pocket calculators, and industrial embedded systems, may all be considered examples of microcomputers according to the definition given above.\n\nEveryday use of the expression \"microcomputer\" (and in particular the \"micro\" abbreviation) has declined significantly from the mid-1980s and has declined in commonplace usage since 2000. The term is most commonly associated with the first wave of all-in-one 8-bit home computers and small business microcomputers (such as the Apple II, Commodore 64, BBC Micro, and TRS 80). Although, or perhaps because, an increasingly diverse range of modern microprocessor-based devices fit the definition of \"microcomputer\", they are no longer referred to as such in everyday speech.\n\nIn common usage, \"microcomputer\" has been largely supplanted by the term \"personal computer\" or \"PC\", which specifies a computer that has been designed to be used by one individual at a time, a term first coined in 1959. IBM first promoted the term \"personal computer\" to differentiate themselves from other microcomputers, often called \"home computers\", and also IBM's own mainframes and minicomputers. However, following its release, the IBM PC itself was widely imitated, as well as the term. The component parts were commonly available to producers and the BIOS was reverse engineered through cleanroom design techniques. IBM PC compatible \"clones\" became commonplace, and the terms \"personal computer\", and especially \"PC\", stuck with the general public, often specifically for a DOS or (nowadays) Windows-compatible computer.\n\nSince the advent of microcontrollers (monolithic integrated circuits containing RAM, ROM and CPU all onboard), the term \"micro\" is more commonly used to refer to that meaning.\n\nMonitors, keyboards and other devices for input and output may be integrated or separate. Computer memory in the form of RAM, and at least one other less volatile, memory storage device are usually combined with the CPU on a system bus in one unit. Other devices that make up a complete microcomputer system include batteries, a power supply unit, a keyboard and various input/output devices used to convey information to and from a human operator (printers, monitors, human interface devices). Microcomputers are designed to serve only one user at a time, although they can often be modified with software or hardware to concurrently serve more than one user. Microcomputers fit well on or under desks or tables, so that they are within easy access of users. Bigger computers like minicomputers, mainframes, and supercomputers take up large cabinets or even dedicated rooms.\n\nA microcomputer comes equipped with at least one type of data storage, usually RAM. Although some microcomputers (particularly early 8-bit home micros) perform tasks using RAM alone, some form of secondary storage is normally desirable. In the early days of home micros, this was often a data cassette deck (in many cases as an external unit). Later, secondary storage (particularly in the form of floppy disk and hard disk drives) were built into the microcomputer case.\n\nAlthough they did not contain any microprocessors, but were built around transistor-transistor logic (TTL), Hewlett-Packard calculators as far back as 1968 had various levels of programmability comparable to microcomputers. The HP 9100B (1968) had rudimentary conditional (if) statements, statement line numbers, jump statements (go to), registers that could be used as variables, and primitive subroutines. The programming language resembled assembly language in many ways. Later models incrementally added more features, including the BASIC programming language (HP 9830A in 1971). Some models had tape storage and small printers. However, displays were limited to one line at a time. The HP 9100A was referred to as a personal computer in an advertisement in a 1968 Science magazine, but that advertisement was quickly dropped. HP was reluctant to sell them as \"computers\" because the perception at that time was that a computer had to be big in size to be powerful, and thus decided to market them as calculators. Additionally, at that time, people were more likely to buy calculators than computers, and, purchasing agents also preferred the term \"calculator\" because purchasing a \"computer\" required additional layers of purchasing authority approvals. HP virtual museum\n\nThe Datapoint 2200, made by CTC in 1970, was also comparable to microcomputers. While it contains no microprocessor, the instruction set of its custom TTL processor was the basis of the instruction set for the Intel 8008, and for practical purposes the system behaves approximately as if it contains an 8008. This is because Intel was the contractor in charge of developing the Datapoint's CPU, but ultimately CTC rejected the 8008 design because it needed 20 support chips.\n\nAnother early system, the Kenbak-1, was released in 1971. Like the Datapoint 2200, it used discrete transistor–transistor logic instead of a microprocessor, but it functioned like a microcomputer in some ways. It was marketed as an educational and hobbyist tool, but it was not a commercial success; production ceased shortly after introduction.\n\nIn late 1972, a French team headed by François Gernelle within a small company, Réalisations & Etudes Electroniqes (R2E), developed and patented a computer based on a microprocessor – the Intel 8008 8-bit microprocessor. This Micral-N was marketed in early 1973 as a \"Micro-ordinateur\" or \"microcomputer\", mainly for scientific and process-control applications. About a hundred Micral-N were installed in the next two years, followed by a new version based on the Intel 8080. Meanwhile, another French team developed the Alvan, a small computer for office automation which found clients in banks and other sectors. The first version was based on LSI chips with an Intel 8008 as peripheral controller (keyboard, monitor and printer), before adopting the Zilog Z80 as main processor.\n\nIn late 1972, a Sacramento State University team led by Bill Pentz built the Sac State 8008 computer, able to handle thousands of patients' medical records. The Sac State 8008 was designed with the Intel 8008. It had a full set of hardware and software components: a disk operating system included in a series of programmable read-only memory chips (PROMs); 8 Kilobytes of RAM; IBM's Basic Assembly Language (BAL); a hard drive; a color display; a printer output; a 150 bit/s serial interface for connecting to a mainframe; and even the world's first microcomputer front panel.\n\nIn early 1973, Sord Computer Corporation (now Toshiba Personal Computer System Corporation) completed the SMP80/08, which used the Intel 8008 microprocessor. The SMP80/08, however, did not have a commercial release. After the first general-purpose microprocessor, the Intel 8080, was announced in April 1974, Sord announced the SMP80/x, the first microcomputer to use the 8080, in May 1974.\n\nVirtually all early microcomputers were essentially boxes with lights and switches; one had to read and understand binary numbers and machine language to program and use them (the Datapoint 2200 was a striking exception, bearing a modern design based on a monitor, keyboard, and tape and disk drives). Of the early \"box of switches\"-type microcomputers, the MITS Altair 8800 (1975) was arguably the most famous. Most of these simple, early microcomputers were sold as electronic kits—bags full of loose components which the buyer had to solder together before the system could be used.\n\nThe period from about 1971 to 1976 is sometimes called the of microcomputers. Many companies such as DEC, National Semiconductor, Texas Instruments offered their microcomputers for use in terminal control, peripheral device interface control and industrial machine control. There were also machines for engineering development and hobbyist personal use. In 1975, the Processor Technology SOL-20 was designed, which consisted of one board which included all the parts of the computer system. The SOL-20 had built-in EPROM software which eliminated the need for rows of switches and lights. The MITS Altair just mentioned played an instrumental role in sparking significant hobbyist interest, which itself eventually led to the founding and success of many well-known personal computer hardware and software companies, such as Microsoft and Apple Computer. Although the Altair itself was only a mild commercial success, it helped spark a huge industry.\n\nBy 1977, the introduction of the second generation, known as home computers, made microcomputers considerably easier to use than their predecessors because their predecessors' operation often demanded thorough familiarity with practical electronics. The ability to connect to a monitor (screen) or TV set allowed visual manipulation of text and numbers. The BASIC language, which was easier to learn and use than raw machine language, became a standard feature. These features were already common in minicomputers, with which many hobbyists and early produces were familiar.\n\nIn 1979, the launch of the VisiCalc spreadsheet (initially for the Apple II) first turned the microcomputer from a hobby for computer enthusiasts into a business tool. After the 1981 release by IBM of its IBM PC, the term personal computer became generally used for microcomputers compatible with the IBM PC architecture (PC compatible).\n\n"}
{"id": "51993925", "url": "https://en.wikipedia.org/wiki?curid=51993925", "title": "Ministry of Science, Technology and Innovation (Uganda)", "text": "Ministry of Science, Technology and Innovation (Uganda)\n\nThe Ministry of Science, Technology and Innovation (MSTI), is a cabinet-level government ministry of Uganda. It is responsible for the planning, coordinating and implement government efforts to encourage scientific and technological innovation in educational institutions, industry, agriculture, commerce and daily life, on the country's path to middle-income status.\n\nThe ministry is headed by Minister of Science, Technology and Innovation, Elioda Tumwesigye.\n\nThe headquarters of the ministry are located at Rumee Building, on Lumumba Avenue, in the Central Division of Kampala, Uganda's capital and largest city.\n\n\n\n"}
{"id": "41352219", "url": "https://en.wikipedia.org/wiki?curid=41352219", "title": "Mobile virtual network enabler", "text": "Mobile virtual network enabler\n\nA mobile virtual network enabler (MVNE) is a company that provides network infrastructure and related services, such as business support systems, administration and operations support systems to a Mobile Virtual Network Operator (MVNO). This enables MVNOs to offer services to their own customers with their own brands. The MVNE does not have a relationship with consumers, but rather is a provider of network enablement platforms and services. \n\nMVNEs specialise in planning, implementation and management of mobile services. Typically this includes SIM provisioning and configuration, customer billing, customer relationship management and value-added service platforms. In effect, they enable an MVNO to outsource both the initial integration with the MNO and the ongoing business and technical operations management. A related type of company is mobile virtual network aggregator, or MVNA. MVNE is a telecom solution, whereas MVNA is a business model which includes wholesale of an operator's airtime and routing of traffic over the MVNE's own switches. \n\nThe benefits of using an MVNE include a reduction in the upfront capital expenses of an MVNO, financing arrangements offered by MVNEs to cover start-up costs and reduced wholesale airtime costs achieved through economies of scale of hosting multiple MVNOs on a single MVNE platform. The other benefits could be reduced operational expenses by outsourcing management of business and technical operations, smoother launch processes and benefiting from previous experience of the MVNE as a negotiating channel for smaller MVNOs to reach a wholesale agreement with the MNO.\n\nHowever, not all MVNEs are the same. There is significant variation in the level of experience, technical capability, integration and operational support. Furthermore, using an MVNE may not be appropriate for all MVNOs. The considerations for this decision are manifold, however, some of the key reasons for an MVNO against using an MVNE are:\n\nAdoption of the MVNE model will have a significant impact on the margins of an MVNO business, acting either to improve or shrink these margins relative to direct or self-build models. As such, the decision to use an MVNE should not be taken lightly as the impacts could range from customer experience to business efficiency.\n\n\n"}
{"id": "13591771", "url": "https://en.wikipedia.org/wiki?curid=13591771", "title": "Monochrome monitor", "text": "Monochrome monitor\n\nA monochrome monitor is a type of CRT computer monitor which was very common in the early days of computing, from the 1960s through the 1980s, before color monitors became popular. They are still widely used in applications such as computerized cash register systems, owing to the age of many registers. Green screen was the common name for a monochrome monitor using a green \"P1\" phosphor screen.\n\nAbundant in the early-to-mid-1980s, they succeeded Teletype terminals and preceded color CRTs and later LCDs as the predominant visual output device for computers.\n\nUnlike color monitors, which display text and graphics in multiple colors through the use of alternating-intensity red, green, and blue phosphors, monochrome monitors have only one color of phosphor (\"mono\" means \"one\", and \"chrome\" means \"color\"). All text and graphics are displayed in that color. Some monitors have the ability to vary the brightness of individual pixels, thereby creating the illusion of depth and color, exactly like a black-and-white television.\n\nTypically, only a limited set of brightness levels was provided to save display memory which was very expensive in the '70s and '80s. Either normal/bright or normal/dim (1 bit) per character as in the VT100 or black/white per pixel in the Macintosh 128K or black, dark gray, light gray, white (2bit) per pixel like the NeXT MegaPixel Display.\n\nMonochrome monitors are commonly available in three colors: if the P1 phosphor is used, the screen is green monochrome. If the P3 phosphor is used, the screen is amber monochrome. If the P4 phosphor is used, the screen is white monochrome (known as \"paper white\"); this is the same phosphor as used in early television sets.\nAn amber screen was claimed to give improved ergonomics, specifically by reducing eye strain; this claim appears to have little scientific basis. However, the color amber is a softer light, and would be less disruptive to a user's circadian rhythm. \n\nWell-known examples of early monochrome monitors are the VT100 from Digital Equipment Corporation, released in 1978, the Apple Monitor III in 1980, and the IBM 5151, which accompanied the IBM PC model 5150 upon its 1981 release.\n\nThe 5151 was designed to work with the PC's Monochrome Display Adapter (MDA) text-only graphics card, but the third-party Hercules Graphics Card became a popular companion to the 5151 screen because of the Hercules' comparatively high-resolution bitmapped 720×348 pixel monochrome graphics capability, much used for business presentation graphics generated from spreadsheets like Lotus 1-2-3. This was much higher resolution than the alternative IBM Color Graphics Adapter 320×200 pixel, or 640×200 pixel graphic standard. It could also run most programs written for the CGA card's standard graphics modes. Monochrome monitors continued to be used, even after the introduction of higher resolution color IBM Enhanced Graphics Adapter and Video Graphics Array standards in the late 1980s, for dual-monitor applications.\n\nPixel for pixel, the monochrome monitors produce sharper text and images than color CRT monitors. This is because a monochrome monitor is made up of a continuous coating of phosphor and the sharpness can be controlled by focusing the electron beam; whereas on a color monitor, each pixel is made up of three phosphor dots (one red, one blue, one green) separated by a mask. Monochrome monitors were used in almost all dumb terminals and are still widely used in text-based applications such as computerized cash registers and point of sale systems because of their superior sharpness and enhanced readability.\n\nSome green screen displays were furnished with a particularly full/intense phosphor coating, making the characters very clear and sharply defined (thus easy to read) but generating an afterglow-effect (sometimes called a \"ghost image\") when the text scrolled down the screen or when a screenful of information was quickly replaced with another as in word processing page up/down operations. Other green screens avoided the heavy afterglow-effects, but at the cost of much more pixelated character images. The 5151, amongst others, had brightness and contrast controls to allow the user to set their own compromise.\n\nThe ghosting effects of the now-obsolete green screens have become an eye-catching visual shorthand for computer-generated text, frequently in \"futuristic\" settings. The opening titles of the first Ghost in the Shell film and the Matrix source code of the Matrix trilogy science fiction films prominently feature computer displays with ghosting green text. Green text is also featured in 's computer in \"Lost\" series.\n\nMonochrome monitors are particularly susceptible to screen burn (hence the advent, and name, of the screensaver), because the phosphors used are very high intensity. Another effect of the high-intensity phosphors is an effect known as \"ghosting\", wherein a dim afterglow of the screen's contents is briefly visible after the screen has been blanked. This has a certain place in pop culture, as evidenced in movies such as \"The Matrix\".\n\nThis ghosting effect is deliberate on some monitors, known as \"long persistence\" monitors. These use the relatively long decay period of the phosphor glow to reduce flickering and eye strain.\n"}
{"id": "47911144", "url": "https://en.wikipedia.org/wiki?curid=47911144", "title": "Narrowband IoT", "text": "Narrowband IoT\n\nNarrowband IoT (NB-IoT) is a Low Power Wide Area Network (LPWAN) radio technology standard developed by 3GPP to enable a wide range of cellular devices and services. The specification was frozen in 3GPP Release 13 (LTE Advanced Pro), in June 2016. Other 3GPP IoT technologies include eMTC (enhanced Machine-Type Communication) and EC-GSM-IoT. \n\nNB-IoT focuses specifically on indoor coverage, low cost, long battery life, and high connection density. NB-IoT uses a subset of the LTE standard, but limits the bandwidth to a single narrow-band of 200kHz. It uses OFDM modulation for downlink communication and SC-FDMA for uplink communications. \n\n\n"}
{"id": "3898021", "url": "https://en.wikipedia.org/wiki?curid=3898021", "title": "Nyctography", "text": "Nyctography\n\nNyctography is a form of substitution cipher writing created by Lewis Carroll (Charles Lutwidge Dodgson) in 1891.\n\nNyctography is written with a nyctograph (also invented by Carroll) and uses a system of dots and strokes all based on a dot placed in the upper left corner. Using the Nyctograph, one could quickly jot down ideas or notes without the aid of light.\n\nCarroll invented the Nyctograph and Nyctography because he was often awakened during the night with thoughts that needed to be written down immediately, and didn't want to go through the lengthy process of lighting a lamp just to have to extinguish it shortly thereafter.\n\nThe device consisted of a gridded card with sixteen square holes, each a quarter inch wide, and system of symbols representing an alphabet of Carroll's design, which could then be transcribed the following day. \n\nHe first named it \"typhlograph\", but at the suggestion of one of his brother-students, this was subsequently changed into \"Nyctograph\".\n\nInitially, Carroll used an oblong of card with an oblong cut out of the centre to guide his writing in the dark. This did not appear to be satisfactory as the results were illegible. The new and final version of the nyctograph is recorded in his journal of September 24, 1891, and is the subject of a letter to \"The Lady\" magazine of October 29, 1891: \nFrom the description it appears that Carroll’s nyctograph was a single row of 16 boxes cut from a piece of card. Carroll would enter one of his symbols in each box, then move the card down to the next line (which, in the darkness, probably, he would have to estimate) and then repeat the process.\n\nEach character had a large dot or circle in the upper-left corner. Beside the 26 letters of the alphabet, there were five additional characters for 'and', 'the', the corners of the letter 'f' to indicate that the following characters were digits ('figures'), the corners of the letter 'l' to indicate that they were letters, and the corners of the letter 'd' to indicate that the following six characters were a date in DDMMYY format. There was no capitalization, punctuation or digits \"per se\", though modern font designers have created them (e.g. capitals may be double-scored, punctuation marks may have the large dot at the bottom right corner, digits at the bottom left). \n\nAs in braille, letters were assigned to represent digits. The values were taken from his Memoria Technica, which assigned two consonants to each digit, with vowels unassigned, so that any number could be read off as a word. For nyctography, one of the consonants was used for each digit. Most are the initials of the numerals, as follows. (In brackets are the other values of the Memoria Technica, which apart from leftover \"j\" for 3 have their own motivations.)\n\n\n\n"}
{"id": "3535793", "url": "https://en.wikipedia.org/wiki?curid=3535793", "title": "OpenDocument adoption", "text": "OpenDocument adoption\n\nThe following article details governmental and other organizations from around the world who are in the process of evaluating the suitability of using (adopting) OpenDocument, an open document file format for saving and exchanging office documents that may be edited.\n\nThe OpenDocument format (ODF) was accepted as a standard by OASIS in May 2005, and by ISO in November 2006, as standard ISO/IEC 26300:2006.\n\nMicrosoft submitted another format, Office Open XML (aka OOXML), to Ecma International where it was accepted as a standard in December 2006. The Office Open XML specification was published as standard ISO/IEC 29500:2008 in November 2008.\n\nOpenDocument has been officially approved by national standards bodies of Brazil, Croatia, Denmark, Ecuador, Hungary, Italy, Malaysia, Russia, South Korea, South Africa and Sweden.\n\nNATO with its 28 members (Albania, Belgium, Bulgaria, Canada, Croatia, the Czech Republic, Denmark, Estonia, France, Germany, Greece, Hungary, Iceland, Italy, Latvia, Lithuania, Luxembourg, the Netherlands, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Turkey, the UK, and the USA) uses ODF as a mandatory standard for all members.\n\nOn October 23, 2007, the Department of Public Service and Administration of the South African government released a report on interoperability standards in government information systems. It specifies ODF as the standard for \"working office document formats\" (with UTF-8/ASCII text and comma-separated values data as the only alternatives).\n\nSince April 2008 ODF is a national standard too, not only the standard to be used by government departments. South African code for the ODF standard is \"SANS 26300:2008/ISO/IEC 26300:2006\". By September 2008 all departments will be able to read and write in the Open Document Format. In 2009, ODF will become the default document format for South African government departments.\n\nThe Hong Kong government releases an Interoperability Framework\nevery year recommending file formats for various tasks. In their latest version they recommend the use of Microsoft Office '97 or OpenOffice.org v2.0 (based on OpenDocument 1.0) file formats for collaborative editing of text documents, spreadsheets and slideshow presentations.\n\nChandershekhar, India's secretary of Ministry of Information and Technology, said, \"We are glad to note that with formation of a National ODF alliance, India too would be playing a pivotal role in spearheading the ODF revolution. Further, considering the huge potential of eGovernance in the nation as well as the need to adopt open standards to make our data systems more inter-operable and independent of any limiting proprietary tools, we feel that ODF is a great technological leap and a big boon to further propel IT right to India's grass root levels. I congratulate this initiative of leading private & public organisations and wish them all the best in this endeavor.\"\n\nThe Allahabad High Court of India has decided, as policy, to use OpenDocument format for its documents.\n\nGovernment agencies are required to:\n\nIn 2007 Kerala released an Information Technology Policy\ndesigned to turn Kerala into a knowledge society.\n\nThey decided that open standards such as ODF would be followed in e-governance projects to avoid proprietary lock in.\n\nOn June 29, 2007, the government of Japan published a new interoperability framework which gives preference to the procurement of products that follow open standards including the ODF standards. On July 2 the government declared that they wouldn't stop from adopting alternative document formats, because they hold the view that formats like Office Open XML which other organizations such as Ecma International and ISO had also approved was, according to them, an open standard, too. Also, they said that it was one of the preferences, whether the format is open, to choose which software the government shall deploy.\n\nGovernment ministries and agencies are required to:\n\nOpen formats:\n\nJapan's Diet passed an open standards software incentive as part of its omnibus Special Taxation Measures law:\n\nIn August 2007, The Malaysian government announced plans to adopt open standards and the Open Document Format (ODF) within the country's public sector. The Malaysian Administration Modernization and Management Planning Unit (MAMPU) issued a tender for a nine-month study to evaluate the usage of open standards.\n\nFrom April 2008 on the use of ODF is mandatory within the public sector.\n\nSouth Korean government adopted OpenDocument as a part of Korean Industrial Standards KS X ISO/IEC 26300 in 2007, but public documents are still made and distributed in .hwp format. There is no regulation of legislation about OpenDocument since 2007.\n\nAlready in 2009 ODF was chosen as official standard. But only from 2014 large scale migrations to ODF (and because of freedom of choice that brought) to LibreOffice started.\n\nThe European Commission has, since at least 2003, been investigating various options for storing documents in an XML-based format, commissioning technical studies such as the \"Valoris Report\". In March 2004, the Telematics between Administrations Committee (TAC) asked an OpenOffice team and a Microsoft team to present on the relative merits of their XML-based office document formats.\n\nIn May 2004, TAC issued a set of recommendations, in particular noting that, \"Because of its specific role in society, the public sector must avoid <nowiki>[a situation where]</nowiki> a specific product is forced on anyone interacting with it electronically. Conversely, any document format that does not discriminate against market actors and that can be implemented across platforms should be encouraged. Likewise, the public sector should avoid any format that does not safeguard equal opportunities to market actors to implement format-processing applications, especially where this might impose product selection on the side of citizens or businesses. In this respect standardisation initiatives will ensure not only a fair and competitive market but will also help safeguard the interoperability of implementing solutions whilst preserving competition and innovation.\" It then issued recommendations, including:\n\nAn official recommendation for a certain format was not issued however.\n\nA memorandum on the use of open standards for creating and exchanging office documents was approved by Belgium's federal Council of Ministers on June 23, 2006. OpenDocument was proposed as the standard for exchanging office documents such as texts, spreadsheets, presentations within the federal civil service.\n\nSince September 2007, every federal government department has to be able to accept and read OpenDocument documents.\n\nGovernment agencies are required to:\n\nThe Danish Parliament has decided that ODF must be used by state authorities after April 1, 2011\n\nUntil then Government agencies are required to:\n\nGovernment entities are permitted to:\n\nFinland's Ministry of Justice has chosen OpenOffice.org and thus the OpenDocument format as their main document format from the beginning of 2007. The decision was made after deep research of ODF possibilities. Other ministries may follow.\n\nAccording to French government's RGI (general interoperability framework), ODF is the \"recommended format for office documents within French administrations\". OOXML is tolerated for \"information exchange needs through tables\".\n\nGovernment agencies are required to:\n\nGovernment agencies are encouraged to:\n\nGovernment agencies are prohibited from:\n\nA large number of Bundesländer, state and goverenmental offices and cities widely use products that support ODF (e.g. StarOffice, OpenOffice.org).\n\nIn December 2008 the governmental IT-Board of the Bundesregierung decided to make use of the ODF-Format in the Federal administration in order to improve IT-security and interoperability.\n\nThe Federal Foreign Office has migrated totally to the use of ODF formats also in the 250 foreign offices abroad (it has reduced its IT costs to a third in comparison to other Ministries ). In a message to the participants of the first international ODF-workshop in October 2007 the Federal Foreign Minister stated: \"The Open Document Format, as a completely open and ISO standardized format, is an excellent vehicle for the free exchange of knowledge and information in the globalized age.\"\n\nThe Federal office for security in IT (Bundesamt für Sicherheit in der Informationstechnik ) uses with StarOffice on all computers the ODF format in a cast deployment.\n\nSince September 2007 all communications with the Federal Court of Justice (Bundesgerichtshof) and the Federal Patent Court (Bundespatentgericht) may be transmitted in the ODF format. The same has already applied for a while to other high courts, i.e. the Bundesarbeitsgericht, the Bundessozialgericht and many other courts in the Bundesland of Nordrhein-Westfalen and of the Free State of Saxony (Sachsen).\n\nFederal agencies will be able to receive, read, send and edit ODF documents beginning no later than 2010.\nUnder Germany's Standards and Architectures for eGovernment Applications 4.0 (SAGA 4.0), ODF recommended for editable text documents, a multi-stakeholder initiative that recommends and mandates standards to be used by the German federal government.\n\nThe City of Freiburg in Baden-Württemberg uses OpenOffice.org and OpenDocument.\nThe City of Munich in Bavaria has already migrated 14000 desktops to OpenOffice.org and OpenDocument while migrating as a whole to Linux.\n\nThe Italian standardization organization UNI adopted ISO/IEC 26300 (ODF 1.0) on January 26, 2007 (UNI CEI ISO/IEC 26300:2007).\n\nThe Italian region of Umbria announced its migration to LibreOffice and OpenDocument format\n\nIn 2015, the Italian Ministry of Defence announced that it would standardise on ODF and install LibreOffice on 150,000 PCs.\n\nFrom the beginning of 2009 onwards, open source software and the ODF format will be the standard for reading, publishing and the exchange of information for all governmental organisations. Whenever the software used is not open source special reasons have to be given.\n\nNorway's Ministry of Government Administration and Reform decided in December 2007 that ODF (ISO/IEC 26300) MUST be used from 2009 when publishing documents that are meant to be changed after downloading, i.e. forms that are to be filled in by the user. So all website forums will use this format.\n\nFrom 2010 it is mandatory for all government agencies to use document formats (PDF or) ODF when exchanging documents as attachments to e-mail between government and users.\n\nIn 2008, the Portuguese Parliament discussed a bill proposed by the PCP) determining that the adoption of open standard formats – namely ODF – shall be mandatory within all public administration agencies.\n\nOn 21 June 2011, the government published a law on Open Standards. On 8 November 2012, the list of mandatory standards was published: ODF version 1.1 becomes obligatory in July 2014.\n\nIn March 2012, the Câmara Municipal de Vieira do Minho (county of Vieira do Minho) in Portugal announced its migration to LibreOffice.\n\nDespite the heated discussions about adopting Free Software and Open Standards in Romanian public administration - especially at eLiberatica 2009 - the Romanian Ministry for Communications and Information Society passed no official bill to enforce the use of Open Standards, particularly OpenDocument. In June 2009, the ministry Gabriel Sandu declared, in an interview to Ziarul Financiar, that „we cannot give up Microsoft licenses overnight”, despite the harsh critics from the large national Free Software community and a few important local IT businesses.\n\nHowever, the OpenDocument format and OpenDocument-capable software are widely used, not only by enthusiasts, but by businesses as well, including large parts of the Romanian government agencies, mayoralties, courts, notaries, insurance firms, accountants, engineers, etc. A (grossly incomplete) list of implementations, including Romania, is maintained here \n\nThe Russian standardization organization Federal Agency on Technical Regulating and Metrology adopted ISO/IEC 26300 as national standard GOST R ISO/IEC 26300-2010 (ГОСТ Р ИСО/МЭК 26300-2010) on December 21, 2010.\nGovernment administration is required to:\n\nIf any other specialized file format is used for technical reasons, all parties in the document exchange must agree that there is a technical interoperability for using that file format.\n\nIn Slovakia, all public authorities should be able to read ODF format since August 2006 and can use this format for electronic communication and for publication of documents.\nSince October 2008, public authorities must be able to read text documents in ODF 1.0 format.\nSince July 2010, public authorities must be able to read text documents in ODF format up to version 1.2.\n\nSince March 2009, documents in ODF 1.0 format are allowed for use with the electronic signature and qualified electronic signature.\n\nGovernment agencies are required to:\n\nGovernment agencies are required to:\n\nGovernment agencies are required to:\n\nSweden has published ODF 1.0 as a national in August 2008. This has not been announced officially. The standards institute only added the prefix \"SS\" before the ISO number SS-ISO/IEC 26300:2008.\n\nA memorandum on the use of open standards for creating and exchanging office documents is being strongly suggested by Turkish Ministry of Development since 2008. According to \"Interoperability Report\" Turkish Ministry of Development, government agencies are required to:\n\nUnder the “Open Source, Open Standards and Re–Use: Government Action Plan”\n\nBECTA (British Education Communication Technology Agency) is the UK agency in charge of defining information technology (IT) policy for all schools in the United Kingdom, including standards for all the schools' infrastructure. In 2005 they published a comprehensive document describing the policy for infrastructure in schools.\n\nThis document highly recommends the use of OpenDocument and a few other formats for office document data. BECTA explains this as follows: \"Any office application used by institutions must be able to be saved to (and so viewed by others) using a commonly agreed format that ensures an institution is not locked into using specific software. The main aim is for all office based applications to provide functionality to meet the specifications described here (whether licensed software, open source or unlicensed freeware) and thus many application providers could supply the educational institution ICT market.\".\nIn July 2014, the UK Government formally adopted the use of OpenDocument for document exchange.\nBristol City Council has adopted the StarOffice suite and with it the OpenDocument format across 5500 desktop computers.\n\nIn September 2007 the Argentinian Province of Misiones decided via decrete that the use of ODF shall be mandatory within the government. Around a million people live in this province, which is one of the 23 provinces of Argentina.\n\nWith the publication of \"e-Ping Interoperability Framework\", Brazil became the first South American country to officially recommend the adoption of OpenDocuments within the government.\n\nAs stated in version v3.0 of 2007: \"Preferred adoption of Open Formats: e-PING defines that, whenever possible, open standards will be used in technical specifications. \"Proprietary standards\" will be accepted, in this transition period, with the perspective of replacement as soon as there are conditions for a complete migration. With no loss to these goals, are to be respected those situations when there is the need to consider security requisites and information integrity. When available, Free Software solutions are to be considered preferential, accordingly with the policy defined by the Comitê Executivo de Governo Eletrônico (CEGE)\"\n\nSince April 2008 ODF is a national standard in Brazil, coded as NBRISO/IEC26300.\n\nAs of the 2010 version, the transition period has officially ended, and proprietary document formats may no longer be used in the federal public administration, although in practice this is not the case in many federal agencies and departments.\n\nAdditionally, a number of parties, including local governments, companies and non-governmental organizations have signed into the Brasília Protocol, which formalize intentions and set goals for adopting the use of the Open Document standard.\n\nGovernment agencies and state-owned companies are required to:\n\nSince June 2008 the \"Agency for the Development of Government Electronic Management and Information and Knowledge Society of Uruguay\" recommends that public documents use either ODF or PDF. ODF should be used for documents in the process of being edited and the latter for documents in final form.\nAll organizations of the Federal Government of Venezuela must:\n\nCanadian governments do not have mandatory file formats. Microsoft office products are widely used by all levels of governments in Canada.\n\nThe United States state of Massachusetts has been examining its options for implementing XML-based document processing. In early 2005, Eric Kriss, Secretary of Administration and Finance in Massachusetts, was the first government official in the United States to publicly connect open formats to a public policy purpose: \"It is an overriding imperative of the American democratic system that we cannot have our public documents locked up in some kind of proprietary format, perhaps unreadable in the future, or subject to a proprietary system license that restricts access.\"\n\nAt a September 16, 2005 meeting with the Mass Technology Leadership Council Kriss stated that he believes this is fundamentally an issue of sovereignty. While supporting the principle of private intellectual property rights, he said sovereignty trumped any private company's attempt to control the state's public records through claims of intellectual property.\n\nSubsequently, in September 2005, Massachusetts became the first state to formally endorse OpenDocument formats for its public records and, at the same time, reject Microsoft's new XML format, now standardized as ISO/IEC 29500:2008 — Office Open XML. This decision was made after a two-year examination of file formats, including many discussions with Microsoft, other vendors, and various experts, plus some limited trial programs in individual communities like Saugus and Billerica. Microsoft Office, which has a nearly 100% market share among the state's employees, did not support OpenDocument formats until Service Pack 2 of Office 2007. Microsoft had indicated that OpenDocument formats will not be supported in new versions of Office, even though they support many other formats (including ASCII, RTF, and WordPerfect), and analysts believe it would be easy for Microsoft to implement the standard. If Microsoft chooses not to implement OpenDocument, Microsoft will disqualify themselves from future consideration. Several analysts (such as Ovum) believe that Microsoft will eventually support OpenDocument. On 6 July 2006 Microsoft announced that they would support the OpenDocument format and create a plugin to allow Office to save to ODF.\n\nAfter this announcement by Massachusetts supporting OpenDocument, a large number of people and organizations spoke up about the policy, both pro and con (see the references section). Adobe, Corel, IBM, and Sun all sent letters to Massachusetts supporting the measure. In contrast, Microsoft sent in a letter highly critical of the measure. A group named \"Citizens Against Government Waste\" (CAGW) also opposed the decision. The group claimed that Massachusetts' policy established \"an arbitrary preference for open source,\" though both open source software and proprietary software can implement the specification, and both kinds of developers were involved in creating the standard (CAGW, 2005). However, InternetNews and Linux Weekly News noted that CAGW has received funding from Microsoft, and that in 2001 CAGW was caught running an astroturfing campaign on behalf of Microsoft when two letters they submitted supporting Microsoft in Microsoft's anti-trust case were found to have the signatures of deceased persons (Linux Weekly News). James Prendergast, executive director of a coalition named \"Americans for Technology Leadership\" (ATL), also criticized the state's decision in a Fox News article. In the article, Prendergast failed to disclose that Microsoft is a founding member of ATL. Fox News later published a follow-up article disclosing that fact.\n\nState Senator Marc R. Pacheco and State Secretary William F. Galvin have expressed reservations about this plan. Pacheco held a hearing on October 31, 2005, on the topic of OpenDocument. Pacheco did not want OpenDocument to be declared as the executive branch standard, primarily on procedural grounds. Pacheco believed that the executive branch had to receive permission to set an executive standard from the multi-branch IT Advisory Board. In contrast, The Massachusetts Information Technology Division (ITD), and its general council, believe the Advisory board's role is to advise ITD, and ITD did discuss the issue with the IT Advisory Board, but ITD's Peter Quinn and Linda Hamel (ITD's General Counsel) asserted that there is no requirement that \"ITD approach the Advisory Board for permission to adopt policies that will impact only the Executive Department.\" Hamel later filed a legal briefing justifying ITD's position (Hamel, 2005). Massachusetts' Supreme Court has ruled that the various branches of government are prohibited from mandating IT standards on each other; this ruling appears to support ITD's claim. Pacheco also did not like the process used to select OpenDocument. However, Pacheco appears to have had many fundamental\nmisunderstandings of the issues. Andy Updegrove said that at the time, \"Senator Pacheco doesn't understand the difference between open source and open standards (and certainly doesn't understand the difference between OpenDocument and OpenOffice). More than once, he indicated that he thought that the policy would require the Executive Agencies to use OpenOffice.org, not realizing that there are other compliant alternatives. He also thought that this would act to the detriment of Massachusetts software vendors, who (he thinks) would be excluded from doing business with the Commonwealth.\" Pacheco also thought that OpenOffice.org was under the GPL, but in fact it is released under the LGPL (Jones, October 31, 2005) (Jones, November 14, 2005). He attempted to halt implementation of OpenDocument in the executive branch via an amendment (to S. 2256), but the amended bill was never sent to the governor.\n\nSince then in 2007 Massachusetts has amended its approved technical standards list to include Office Open XML.\n\nOfficial Information Documents from the Commonwealth of Massachusetts:\n\nIn November 2005, James Gallt, associate director for the National Association of State Chief Information Officers, said that a number of other state agencies are also exploring the use of OpenDocument (LaMonica, November 10, 2005).\n\nIn April 2006, a bill was introduced in the Minnesota state legislature to require all state agencies to use open data formats. It is expected that the OpenDocument Format will be advanced as a way of meeting the proposed requirement. (Gardner, April 7, 2006).\n\nIn late 2007 and early 2008, New York State issued a Request for Public Comment concerning electronic records policy.\n\n\nIt was announced on 31 March 2006, that the National Archives of Australia had settled on OpenDocument as their choice for a cross-platform/application document format.\n\nAccording to OASIS' OpenDocument datasheet, \"Singapore's Ministry of Defence, France's Ministry of Finance and its Ministry of Economy, Finance, and Industry, Brazil's Ministry of Health, the City of Munich, Germany, UK's Bristol City Council, and the City of Vienna in Austria are all adopting applications that support OpenDocument.\" (OASIS, 2005b).\n\n"}
{"id": "811550", "url": "https://en.wikipedia.org/wiki?curid=811550", "title": "Over-the-air programming", "text": "Over-the-air programming\n\nOver-the-Air programming (OTA) refers to various methods of distributing new software, configuration settings, and even updating encryption keys to devices like cellphones, set-top boxes or secure voice communication equipment (encrypted 2-way radios). One important feature of OTA is that one central location can send an update to all the users, who are unable to refuse, defeat, or alter that update, and that the update applies immediately to everyone on the channel. A user could \"refuse\" OTA but the \"channel manager\" could also \"kick them off\" the channel automatically.\n\nIn the context of the mobile content world these include over-the-air service provisioning (OTASP), over-the-air provisioning (OTAP) or over-the-air parameter administration (OTAPA), or provisioning handsets with the necessary settings with which to access services such as WAP or MMS.\n\nAs mobile phones accumulate new applications and become more advanced, OTA configuration has become increasingly important as new updates and services come on stream. OTA via SMS optimizes the configuration data updates in SIM cards and handsets and enables the distribution of new software updates to mobile phones or provisioning handsets with the necessary settings with which to access services such as WAP or MMS. OTA messaging provides remote control of mobile phones for service and subscription activation, personalization and programming of a new service for mobile operators and telco third parties.\n\nVarious standardization bodies were established to help develop, oversee, and manage OTA. One of them is the Open Mobile Alliance (OMA).\n\nMore recently, with the new concepts of Wireless Sensor Networks and the Internet of Things, where the networks consist of hundreds or thousands of nodes, OTA is taken to a new direction: for the first time OTA is applied using unlicensed frequency bands (868 MHz, 900 MHz, 2400 MHz) and with low consumption and low data rate transmission using protocols such as 802.15.4 and ZigBee.\n\nMotes are often located in places that are either remote or difficult to access. As an example, Libelium has implemented a smart and easy-to-use OTA programming system for ZigBee WSN devices. This system enables firmware upgrades without the need of physical access, saving time and money if the nodes must be re-programmed.\n\nOn modern mobile devices such as smartphones, an over-the-air update may refer simply to a software update that is distributed over Wi-Fi or mobile broadband using a function built into the operating system, with the \"over-the-air\" aspect referring to its use of wireless internet instead of requiring the user to connect the device to a computer via USB to perform the update.\n\nFirmware updates are available for download from the OTA service.\n\nThe OTA mechanism requires the existing software and hardware of the target device to support the feature, namely the receipt and installation of new software received via the wireless network from the provider.\n\nNew software is transferred to the phone, installed, and put into use. It is often necessary to turn the phone off and back on for the new programming to take effect, though many phones will automatically perform this action.\n\nDepending on implementation, OTA software delivery can be initiated upon action, such as a call to the provider's customer support system or other dialable service, or can be performed automatically. Typically it is done via the former method to avoid service disruption at an inconvenient time, but this requires subscribers to manually call the provider. Often, a carrier will send a broadcast SMS text message to all subscribers (or those using a particular model of phone) asking them to dial a service number to receive a software update.\n\nVerizon Wireless in the U.S. provides a number of OTA functions to its subscribers via the *228 service code. Option 1 updates phone configuration, option 2 updates the PRL. Similarly Voitel Wireless and StraightTalk, which both use Verizon network, use *22890 service code to program Verizon based wireless phones. Interop Technologies provides a number of nationwide wireless operators in the US with an SS7 Based Over-the-Air device management solution. This solution allows operators to manage wireless device functionality including renumbering handsets, updating phone settings, applications and subscriber data and adjusting PRL to manage cost structures.\n\nTo provision parameters in a mobile device OTA, the device needs to have a provisioning client capable of receiving, processing and setting the parameters. For example, a Device Management client in a device may be capable of receiving and provisioning applications, or connectivity parameters.\n\nIn general, the term OTA implies the use of wireless mechanisms to send provisioning data or update packages for firmware or software updates to a mobile device — this is so that the user does not have to go to a store or a service center to have applications provisioned, parameters changed or firmware or software updated. Non-OTA options for a user are a) to go to a store and seek help b) use a PC and a cable to connect to the device and change settings on a device, add software to device, etc.\n\nThere are a number of standards that describe OTA functions. One of the first was the GSM 03.48 series.\nThe ZigBee suite of standards includes the ZigBee Over-the-Air Upgrading Cluster which is part of the ZigBee Smart Energy Profile and provides an interoperable (vendor-independent) way of updating device firmware.\n\nOTA is similar to firmware distribution methods used by other mass-produced consumer electronics, such as cable modems, which use TFTP as a way to remotely receive new programming, thus reducing the amount of time spent by both the owner and the user of the device on maintenance.\n\nOver-the-air provisioning (OTAP) is also available in wireless environments (though it is disabled by default for security reasons). It allows an access point (AP) to discover the IP address of its controller. When enabled, the controller tells the other APs to include additional information in the Radio Resource Management Packets (RRM) that would assist a new access point in learning of the controller. It is sent in plain text however, which would make it vulnerable to sniffing. That's why it is disabled by default.\n\n"}
{"id": "38138695", "url": "https://en.wikipedia.org/wiki?curid=38138695", "title": "Proto.io", "text": "Proto.io\n\nProto.io is an application prototyping platform launched in 2011 and developed by the Labs Division of SNQ Digital. Originally designed to prototype on mobile devices, Proto.io has expanded to allow users to prototype apps for anything with a screen interface, including Smart TV’s, digital camera interfaces, cars, airplanes, and gaming consoles. Proto.io utilizes a drag and drop user interface (UI) and does not require coding.\n\nSince its launch in 2011, there have been three versions of Proto.io released.\n\nIn 2011, SNQ made the 100% web-based Proto.io tool available online. The web-based environment allowed users to create a project for either the iPad or iPhone. After a user created a few screens for a developing app, Proto.io could then link those pages together with interactive actions that are custom to hand held devices, such as clicks, taps, tap and holds, and swipes. With the platform, users could also create reusable templates into which prepackaged and editable elements could be dragged. Once the user had completed the prototype, Proto.io could then publish and preview the finished product not only on the web browser but also on the actual mobile device.\n\nProto.io V2 was released in early 2012 and expanded the supported mobile devices to accommodate for the Android platform, to include the Android Smartphone and Tablet. The platform also came with a newer user interface. Proto.io V2 also added collaboration features like comments and annotations as well as export to HTML functionality.\n\nOn September 28, 2012, with the release of version 3 of the platform, Proto.io became the first prototyping tool to allow users to prototype on almost any device with a screen interface, and the first mobile prototyping tool to support full feature animations of user interface items within a prototype screen. The included icon gallery contains thousands of SVG icons for use as buttons, lists and tab bars. Proto.io V3 also supports web fonts, which allows the user to access all available online fonts.\n"}
{"id": "2074318", "url": "https://en.wikipedia.org/wiki?curid=2074318", "title": "RailTel Corporation of India", "text": "RailTel Corporation of India\n\nRailTel Corporation of India Ltd. is a \"Miniratna\" (public sector) enterprise of Government of India focusing on providing broadband and VPN services. RailTel was formed in September 2000 with the objective of creating nationwide broadband, telecom and multimedia network, to modernise train control operation and safety system of Indian Railways. RailTel's network passes through around 5,000 stations across the country, covering all major commercial centres.\n\nThe Indian Railways (IR) was initially solely dependent on the Department of Telecom (now BSNL) for their control and administrative communication circuits. To increase circuit efficiency, the Railways began building up its own communication systems from early 1970s based on overhead telephone lines, quad cables and microwave signalling. In 1983, the Railway Reforms Committee decided to introduce optical fibre cable (OFC) based communications in IR to provide safety, reliability, availability and serviceability through use of a dedicated network. The decision was also taken to create a network independent of the DoT and replace the existing microwave telecom systems (60% of which had reached end of life) with OFC.\n\nIndian Railways commissioned the first OFC on the Churchgate–Virar line in Mumbai in 1988 for train operation and control purpose, which consisted of 60 km of network across 28 stations. The network was expanded in Central India with the commissioning of 900 km of OFC network in 1991–92 across Durg–Nagpur, Nagpur–Itarsi and Itarsi–Bhusaval sections of the Howrah–Nagpur–Mumbai line, and in Eastern India with the commissioning of 60 km of OFC network in Tatanagar–Chakradhrapur section of the same line.\n\nThe second National Telecom Policy in 1999 opened the National Long-Distance segment under favourable licensing conditions with revenue sharing to assist mobile network operators to spread their networks across India. In 2000, the Government announced the formation of a telecom corporation to build a nationwide broadband multimedia telecommunication network. RailTel was established in September 2000 as a Public Sector Undertaking (PSU), wholly owned by the Indian Railways.\n\nRailTel, in collaboration with Google, provides free WiFi access at selected railways stations across India. Google chose railway stations as the location to provide free WiFi because stations have access to reliable power supply and fibre provided by RailTel, and because the passengers at a station come from all demographics of India.\n\nThe free WiFi service was launched at Mumbai Central railway station in January 2016. In April 2016, the service was expanded to 9 more railway stations. In June 2016, Google announced that free Wi-Fi was available across 19 stations in India and was being used by over 1.5 million people. Google and RailTel plan to provide free WiFi at 100 railway stations across the country by the end of 2016.\n\nIn September 2016, Google announced a public WiFi initiative called Google Station. Google plans to expand free WiFi coverage under the initiative to locations such as cafes and malls across India, and later expand worldwide.\n\nIn June 2018, Google announced that it's Free Wi-Fi project is now powering 400 Indian railway stations. As a result, there are now more than 8 million people accessing the internet each month via the project.\n\nBased on its nationwide fibre network, RailTel offers various bandwidth intensive application to its customers. One such initiative is RailWire, a joint venture with MSOs to provide Voice, Video and Multimedia access on a single wire at a customer's home or office.\n\nRailTel has received the 12th National Awards for Excellence in Cost Management 2014.\n"}
{"id": "55533945", "url": "https://en.wikipedia.org/wiki?curid=55533945", "title": "Woz U", "text": "Woz U\n\nWoz U is a tech education platform launched by Apple co-founder Steve Wozniak that focuses on both students and companies. Woz U is Arizona-based with plans to launch physical locations for learning in more than 30 cities across the globe. Although the company is registered in Arizona, it works through its Texas-based partner, Southern Careers Institute. Less than a week after the launch announcement, the company came under scrutiny by state regulators about the lack of state accreditation.\n\nIn fall of 2018, a CBS News investigation of Woz U cast some doubts on the professionalism of the expensive curriculum. CBS interviewed two dozen current and former students and employees, who shared their dissatisfaction with the content quality, such as documentation typos leading to confusing program errors, while some promised live lectures were actually recorded and out-of-date. One student described the 33-week online program as \"a $13,000 e-book\". A former \"enrollment counselor\" described a high-pressure sales environment, which the company denied. In a prepared statement, Woz U president Chris Coleman admitted the documentation errors and said quality control efforts were being implemented, and said curriculum was reviewed by Wozniak. The founder declined interview requests, then dodged a reporter's unannounced appearance at a conference.\n\n"}
{"id": "43356578", "url": "https://en.wikipedia.org/wiki?curid=43356578", "title": "Yabacon Valley", "text": "Yabacon Valley\n\nYabacon Valley (YV) is a nickname for an area within Yaba. Yaba is a suburb of Lagos, Nigeria and located at the mainland of the Lagos. This area is already growing as Nigeria’s technology hub and cluster of hundreds of banking institutions, educational institutions, technology and startup companies which steadily attracts angel investors, venture capitalists, enthusiasts and media people from all over the world. This cluster is the major reason many technology firms are considering opening up shops in Yaba. Close to the region is the Lagos Lagoon which lies on its south-western side, it empties into the Atlantic via Lagos Harbor, a main channel through the heart of the city, 0.5 km to 1 km wide and 10 km long. The term originally was born from an unintentional act of an absent mind, manipulating the Silicon Valley name to create a nick version for this cluster while writing a story title. More so, due to the lagoon near this region, the term Silicon lagoon has also been used to refer to this cluster although this is yet to stick and its origin or creator is unknown.\n\nRegardless of the tech clusters in Africa, Yabacon Valley (YV) or Silicon lagoon — whichever name you chose to call it — continues to be a leading hub for high-tech innovation and development, buoyed by the country’s budding, technology-savvy middle-class and massive online population of 45 million internet users. Nigeria’s fledgling technology start-up scene is witnessing a flurry of activities in the frame of new investment drive, acquisitions, strategic partnerships as well as plans to establish more incubation centers.\n\nGeographically, Yabacon Valley encompasses all Yaba Local Council Development area which was carved out of the old Lagos Mainland Local Government created in 1977 as a separate local government following the national reform of Local government in September 1976. The Lagos Mainland carved out of Lagos city council which administered the Lagos Metropolitan city consists of Lagos Island and Lagos Mainland. So with the creation of three more Local government on 27 August 1991, the former Lagos mainland was re-constituted with Surulere carved out of it. The present Yaba Local Council Area has the look of an urban setting; some areas however mirror rural features and these areas are simply referred to as blighted areas like Makoko and Iwaya. Nonetheless, Yabacon Valley is a development, a commercial nerve center for all regardless of their political and cultural affiliations.\n\nIt is not known if the term \"Yabacon Valley\" has been mentioned or used in reference of this tech cluster in the past, but its first published use is credited to The Business Aim, an online publishing platform with focus on business, strategy, innovation, startup culture and everything new. According to Google search results, the name was first used by \"Blaise Aboh\", then an award-winning editor, now Data Analysis/Design Lead and Founding Partner at Orodata Science Nigeria a Civic technology organization as part of a title for an article on reasons for the arrival of startup accelerators, and also to announce the emergence of a new player named Passion Incubator in Yaba technology ecosystem. The article is dated 13 March 2014. The term is still not widely known however it is been used in conversations among the geeks, players and enthusiasts in the tech ecosystem especially on technology focused blogs and social media. There's still controversy surrounding the name, arguments and comments made on article a popular Nigerian technology blog Techcabal confirms this as few are disgruntled and think a better nickname can be created. They feel the name ought to be something else without the ‘con’ and the ‘valley’ arguing why Nigeria must mimic the west in almost everything. But there are many who actually like the name. Weekly in conversations, this name pops up and there are those who are of the opinion that if the name is disliked so much, it should not be brought up regularly.\n\nLate 2015, a tech blog's community post declared Yabacon Valley is dead. Long live Yaba Right'. The conversation was about how the name Yabacon Valley was not a good branding of the tech cluster, and why other suggestions like 'Yaba Right' or 'Yaba District' or even 'Silicon Yaba' were more suitable names. The Executive Director of Paradigm Initiative Nigeria (PIN) a social enterprise, Gbenga Sesan in a comment said that 'If something wasn't broken, why fix it'? That the whole thing was but a fruitless argument. He was of the opinion that when the time was right, the right name would come, that 'instead of looking for a new name by all means, let all cities focus on real work without assumptions'. It must be noted that one person alone does not have the power to name a country, let alone a district. It comes from careful or careless deliberations, negotiation, discuss, or in the case of YV; incessant community debates and arguments over its origin, originality and suitability. \nYabacon is a portmanteau of ‘Yaba’ the Lagos suburb and ‘Silicon’, a chemical element used to create most semiconductors commercially for electronic computers. Although there are technology companies in this area, there are no companies involved in the making of semiconductors since the cluster is still at its green stage and Nigeria is yet to advance to the technology level of manufacturing electronics. Thus, the name is just a sheer but unconscious imitation of America's Silicon valley. The name Yabacon Valley (YV) is alive although proclaimed dead because the Yaba technology community made it a continuous topic of discussion on blogs, social media and community posts. The name was an ordinary creation of a creative writer while penning the title of an article. The tech community made it the monster it is today.\n\nThe population of Yabacon Valley is between 200 and 300 thousand approximately. It is assumed that females outnumber males in the majority of the localities in Yabacon Valley due to concentration of tertiary institutions in the area.\n\nNorth of this area is Shiro Street down to other side of Morocco road towards the roundabout. This also includes Abule- Ijesha South - Muritala Muhammed way from Jibowu to Wright street junction. To the East is the Lagos lagoon, Onike, Onitiri, Makoko, Iwaya, University of Lagos communities also overlooking lagoon. To the West; a descent of 3rd mainland bridge to Wright Street to Murtala Muhammed way to include Total services station at that junction.\n\nYaba is a part of Lagos Nigeria with many small towns and communities such as Onyigbo, Ebute metta, Makoko, Sabo, Akoka, Abule Ijesha, Onike, Jibowu, and Iwaya among others.\n\nYaba Local council development area as it is today has its secretariat at 198, Herbert Macaulay Street, carved out of the old Lagos Mainland local government which was created in 1977 as a separate Local government following the national reform of Local government in September 1976. Lagos Mainland carved out of Lagos city council which administered the Lagos Metropolitan city; this consists of Lagos Island and Lagos Mainland. So with the creation of three more Local government on 27 August 1991, the former lagos mainland was re-constituted with Surulere carved out of it.\nYaba Local council Development area as one of the Thirty Seven (37) newly created Local council Development areas was created out of Lagos Mainland by the administration of Senator Bola Ahmed Tinubu after the state assembly passed a law creating new local council Development areas.\nThe present Yaba local council area wears an urban setting; some part however mirrored rural features and these areas are simply referred to as blighted areas like Makoko and Iwaya. Nonetheless, Yaba Local council development area is the commercial nerve centre for all regardless of their political and cultural affiliations.\n\n\"Perhaps the strongest push responsible for this industrial powerhouse and cluster is the increase in demand of products leveraging technology for growth\".\n\nYabacon valley has two of Nigeria’s leading educational institutions in Yaba College of Technology (Yabatech) and University of Lagos. Both higher institutions are known to be of higher learning standards and have some of the country’s best brains as lecturers. There of course are the less seasoned institutions which are prestigious in their own right – the federal technical college Akoka (a teacher training institute) and Queens’ college one of Africa’s best post primary educational institutions.\nHistorical significance, landmarks and memorials. Yaba also has notable landmarks like the statue of Herbert Macaulay the father of nationalism in Nigeria. Many other landmarks such as Tejuosho Market, E-center, National library of Nigeria, Yabatech among other notable landmarks there are also historically significant landmarks, such as the tallest building on the mainland Corner stone house, the first filling station in Nigeria - (Total at Sabo) and many more.\n\nYaba has one of the fastest expanding middle class groups in Nigeria as a whole. This is why developers fall over themselves to renovate old buildings or expand them out rightly to create apartments for working class folks. Real estate boom is in no small way an issue to consider when middle-class families demand for accommodation.\n\nCrime rate in most parts of yaba is low. Perhaps the heavy presence of security details around the place deters criminals or could be that the ever busy nature of the place seems to inconvenience criminals. Also there is the possibility that many so called criminals may consider productive work readily available for idle hands to be more rewarding whatever the case crime rate is relatively low.\n\nLagos island is the nerve center of commerce and the service based industry in Nigeria as a whole. Living in Yaba keeps you close to the Victoria island, Ikoyi and Lekki and thereby reduces your transportation expenses, stress due to heavy traffic jams which has found common place in these aforementioned areas.\n\nThe local economy in this part of Lagos is rapidly expanding. Eateries, banks, hotels, retail stores, insurance companies, night clubs are booming implying a lot of opportunities both for investors and employment seekers. The expansion of this area is rapid and many companies are setting up so as to not miss out of its financial benefits.\n\nIn 2011, Wennovation Hub in partnership with African Leadership Forum started incubating startups in the tech ecosystem but not much noise was made till 2012 when Bosun Tijani a social innovator & entrepreneur and a group of individuals set out to help animate a community of change agents who believe in building a strong base for Nigeria through technology. They were certain they could help accelerate a movement of people who are driven by the need to disrupt the status quo in Nigeria through smart application of technology. The name was later changed to Cc-HUB, and it became Nigeria’s first startup incubator. With investment and support in cash and kind from organisations such as the Indigo Trust, Omidyar Network, MainOne Cable Company and the Lagos State government, it soon gained momentum and proceeded to install a fibre-optic-powered information superhighway. In 2011, former banker Seun Onigbinde co-founded BudgIT, a fiscal transparency project, on the third floor of CC Hub’s six-storey building in Yaba. As one of the first early-stage startups to benefit from CC Hub’s incubation drive in 2011, it received $5,000 of its $90,000 seed funding from billionaire businessman Tony Elumelu. Big names like Konga, eCommerce company valued at approximately $200 million as after raising $20 million in Series C rounds, arrived in 2013, while Africa Internet Group which has $469 million in 4 Rounds from six investors transferred six of its companies to Yaba in 2014. In same 2014 BudgIT received $400,000 grant from Omidyar. Mid 2016, Andela – a Nigerian-founded talent accelerator for programmers that has campuses in Lagos, Nairobi and New York – received $24 million in investment from the Chan Zuckerberg Initiative. In 2015 Hotels.ng, which claims to be the largest hotel booking site in Nigeria secured $1.2 million in funding from Omidyar Network to expand its listings across Africa.\n\nA number of startups including Iroko – the biggest digital retailer of Nollywood worldwide with total funding of $40 million – and Paystack, an alternative e-payments company which raised $1.3 million investment in December 2016 from international investors Tencent, Comcast Ventures and more, have offices outside the cluster but still intermingle with those from Yabacon Valley.\n\nA day after Mark Zuckerberg, the founder of Facebook, a social media company with $350 billion market valuation walked on the streets of Yaba in Lagos Nigeria, local and international media went agog as to why Yaba is 'Nigeria's Silicon Valley' after all. In Zuckerberg’s first visit to Africa, his first stop was at Co-Creation Hub (CcHub) in Yaba, Lagos, ground zero for start-ups in order to listen, learn and take ideas back to California on how Facebook can better support tech development and entrepreneurship across Africa. In a Facebook post he said; \"the energy here is amazing and I’m excited to learn as much as I can, I’m looking forward to meeting more people in Nigeria\" after he met and interacted with kids at a summer coding camp in CCHub, developers and entrepreneurs in the startup ecosystem in Nigeria. Then he went to Andela where his wife foundation Chan Zuckerberg foundation had made an investment months back. The visit was part of a series of global town hall meetings.\n\n"}
