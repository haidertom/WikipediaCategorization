{"id": "48575463", "url": "https://en.wikipedia.org/wiki?curid=48575463", "title": "APM Automation Solutions", "text": "APM Automation Solutions\n\nAPM Automation Solutions is an Israeli-based developer of solids volume and level measurement instrumentation established in Tel Aviv, Israel. The APM technology is used in all the bulk solids industries such as:food and beverage, metals and mining, power, cement, coal, chemical, pulp and paper, and other industries. The outfit recently became part of the Emerson Process Management business under Rosemount Brand.\n\nAPM Automation Solutions was established in 2005 in Tel Aviv, Israel by Ofir Perl and Yossi Zlotnick. After the acquisition, Ofir Perl became the GM in Rosemount Analytical under Emerson Process Management.\nYossi Zlotnick was also a co-founder of the firm. He is the CTO in the APM site under the Rosemount Level team. The co-founders have been involved in managing and leading large-scale development projects in telecommunications and signal processing fields for many years.\n\nAPM Automation Solutions has set a network of global distributors/agents that supply its products in over 30 countries across the US, Europe and Asia-Pacific. In December 18, 2013, APM Automation Solutions was acquired by Emerson Process Management. By this acquisition, Emerson expands its capabilities in solids measurement applications. Emerson operates in 150 countries and has a market cap of $48.1 billion on the New York Stock Exchange with annual sales of $24.4 billion.\n\nAPM Automation Solutions is a manufacturer of volume and level measurement instrumentation. The company offers acoustic imaging and 3D mapping technologies e.g. the 3DLevelScanner. These technologies are \"meant to take the guesswork out of measuring the level, volume and mass of bulk solids and powders\" stored inside a silo or open bin. These have found applications in food & beverage, metals and mining, power, chemical production, cement, coal, pulp and paper, and other industries.\n\n"}
{"id": "30891062", "url": "https://en.wikipedia.org/wiki?curid=30891062", "title": "Adaptxt", "text": "Adaptxt\n\nAdaptxt is a predictive text application for mobile phones, developed by KeyPoint Technologies, a UK-based software company. The application is designed to improve text entry on mobile devices by making it faster and error-free. It achieves this by predicting the next word as well as the word being typed, continuously adapting to the user's writing-style and vocabulary.\n\nLaunched in 2006, Adaptxt supported Windows Mobile smartphones only. Adaptxt provides features such as context-based next-word suggestions, word completion, personal dictionary, dynamic language detection, conversion from SMS language to standard English and vice versa, multilingual text entry and a feature that learns new words and context while typing.\n\nThe application stores the new words in the user’s personal dictionary, where they can be edited. KeyPoint also claims that Adaptxt users can type in any of the installed languages without changing their keyboard language or dictionary, thanks to an engine capable of recognizing the language in use. This feature is supposed to be an added value for users who are bi-lingual or occasionally type in a foreign language.\n\nA version for Symbian S60 smartphones was released in 2008, with improved support for third-party applications.\n\nIn April 2009, KeyPoint introduced a feature to scan personal data, including calendar entries, phonebook contacts, SMS and email inbox and sent items. The “Scan Facebook” feature learns words from the user’s Facebook profile. The new words learnt are added to the personal dictionary and offered as suggestions during text entry.\n\nLater that year, KeyPoint also launched a new version of Adaptxt with error correction. This feature provides alternative suggestions for incorrect spellings. Automated correction of spelling mistakes were also made available, with an option to revert to the exact word entered by the user in case of unwanted corrections. All these features can now be found in the Symbian version of the product and most of them can also be found in the Windows Mobile version.\n\nKeyPoint was also the first software company to introduce professional add-on dictionaries that provide industry-specific word and phrase suggestions related to a particular profession, such as medicine, law, IT and telecommunications, business and finance. Users can download additional language and professional dictionaries from the product website directly to their devices.\n\nIn May 2011, KeyPoint made Adaptxt open source through the project \"OpenAdaptxt\".\n\nAn Android-compatible version of the application has been launched in November 2011.\n\nAdaptxt supports both touch-screen and hard-keyboard devices with 12-key, 20-key or QWERTY layouts. On 12-key phones, Adaptxt offers both Multi-tap and Predictive entry modes.\n\nMore than 50 languages and respective keyboard layouts are supported by Adaptxt, including: English US, English UK, European French, Canadian French, German, Italian, European Spanish, Latin-American Spanish, European Portuguese, Brazilian Portuguese, Danish, Swedish, Finnish, Norwegian, Icelandic, Estonian, Latvian, Lithuanian, Polish, Czech, Slovak, Hungarian, Slovenian, Serbian, Croatian, Greek, Turkish, Bulgarian, Romanian, Russian, Belarusian, Ukrainian, Galician, Basque, Catalan, Filipino, Indonesian, Malay, Vietnamese, Hausa, Hinglish, Arabic, Persian, Urdu, Hebrew, Hindi, Marathi, Arabic, Tamil, Telugu, Malayalam, Scottish Gaelic, Manx Gaelic and Irish.\n"}
{"id": "49449678", "url": "https://en.wikipedia.org/wiki?curid=49449678", "title": "Ami Dar", "text": "Ami Dar\n\nAmi Dar (born January 7, 1961) is the Founder and Executive Director of Idealist.org. Idealist serves more than 120,000 organizations around the world and has more than 1.4 million visitors every month between the English site (idealist.org), and the Spanish site (idealistas.org). \n\nDar was born January 7, 1961, in Jerusalem, Israel, the eldest of three children, to a school teacher mother and diplomat father. He grew up in Peru and Mexico, and it was in Mexico City where he first became aware of the contrast of wealth and poverty around him, which started him on a path of dedication to social justice. \n\nIn 1976, Dar and his family returned to Israel and from 1979 - 1982, he completed his mandatory service as a paratrooper. During this time Dar had an insight that fundamentally changed his thinking about borders and humanity and how labels define and separate people, which is often referred to as the \"sock sharers story.\" \n\nIn 1988 Dar joined Aladdin Knowledge Systems, a software company based in Tel Aviv. From 1988 to 1992 he served as the International Marketing Manager. In 1992 he was named President and he relocated to New York City to establish their North American branch. \n\nBy 1995 he had founded an early iteration of Idealist, The Contact Center Network sponsored meeting spaces in several communities where people could connect with neighbors who might share interests and ideas for local action. In 1996 Dar began calling the online network Idealist.org. \n\nIn January of 2018 the Idealist team launched Idealists of the World which has over 23,300 members world wide. The Idealists of the World participate in global Idealist Days which occur when the day and the month are the same number. The first global Idealist Day occurred on March 3rd, 2018 (3/3) with \"75 events mobilizing nearly 500 people around the world, and 3000 people in the Idealists of the World Facebook group.\" The philosophy behind Idealist Day is rooted in Ami Dar's vision of a world progressing towards positive social change. This is accomplished by people making connections online and in person, bridging divides to collaborate together for a better tomorrow. Regarding this vision Dar has stated,I can't help noticing that all over the world, behind every label and stereotype, there are people who share some basic values. And I can’t help thinking that if these people could somehow work together, the world would be a very different place. What are these values? Treating others the way we’d like to be treated is a good start. But we can go beyond that. For example, I believe that in every country and every culture there are many people who would agree with this sentence:\n\n\"“Working with others, in a spirit of generosity and mutual respect, I want to help build a world where all people can lead free and dignified lives.”\"\n\nThe Stern Family Fund awarded Dar a $100,000 Public Interest Pioneer grant (2000). \n\nDar was named an Ashoka Fellow (2004). Fellows are leading social entrepreneurs recognized to have innovative solutions to social problems and the potential to change patterns across society. They demonstrate unrivaled commitment to bold new ideas and prove that compassion, creativity, and collaboration are tremendous forces for change. Ashoka Fellows work in over 60 countries around the globe in every area of human need.\n\nHe is a Board Member Emeritus of the Nonprofit Technology Network (NTEN), the largest community of nonprofit professionals transforming technology into social change.\n\nDar was named \"Time Magazine's\" Philanthropy Innovator (2005).\n\nHe was recognized in the \"Nonprofit Times: Power and Influence\", 50 Most Influential People in 2000, 2002, 2003, 2004, and 2005.\n\nDar received Duke University's Fuqua School of Business, CASE Leadership in Social Entrepreneurship Award (2006).\n\nDar wrote the Forward to \"Nonprofit Management 101: A Complete and Practical Guide for Leaders and Professionals.\" Wiley.com. May 03, 2011.\n\nHe gave the Commencement Address for City University of New York (CUNY) School of Professional Studies in June 23, 2011.\n\nIn 2012, Dar was profiled in Forbes magazine. \n\nDar's career was profiled by Bloomberg in 2014.\n"}
{"id": "9344216", "url": "https://en.wikipedia.org/wiki?curid=9344216", "title": "Autocloning", "text": "Autocloning\n\nAutocloning is a proprietary method of fabricating 2D and 3D photonic crystals for free space applications.\n\n"}
{"id": "52615946", "url": "https://en.wikipedia.org/wiki?curid=52615946", "title": "Civic Technology Companies", "text": "Civic Technology Companies\n\nCivic technology companies are platforms, products, and services that facilitate civic engagement. Civic technology encompasses any type of technology that enables greater participation in government affairs, or \"assists government in delivering citizen services and strengthening ties with the public\". The phrase can essentially be used to describe any company that is concerned with improving the quality, access, and efficiency of government services within the political system through technological means.\n\n\n\n"}
{"id": "30819581", "url": "https://en.wikipedia.org/wiki?curid=30819581", "title": "Comparison of Nikon DSLR cameras", "text": "Comparison of Nikon DSLR cameras\n\nThe following table compares general and technical features of Nikon DSLR cameras.\n\nKey:\n\n"}
{"id": "14192567", "url": "https://en.wikipedia.org/wiki?curid=14192567", "title": "Comparison of orbital launch systems", "text": "Comparison of orbital launch systems\n\nThis is a comparison of orbital launch systems. The following exposes the full list of conventional orbital launch systems. For the short simple list of conventional launcher families, see: Comparison of orbital launchers families. For the list of predominantly solid-fuelled orbital launch systems, see: Comparison of solid-fuelled orbital launch systems.\n\nSpacecraft propulsion is any method used to accelerate spacecraft and artificial satellites. A conventional solid rocket or a conventional solid-fuel rocket is a rocket with a motor that uses solid propellants (fuel/oxidizer). Orbital launch systems are rockets and other systems capable of placing payloads into or beyond Earth orbit. All current spacecraft use conventional chemical rockets (bipropellant or solid-fuel) for launch, though some have used air-breathing engines on their first stage.\n\nThe following chart shows the number of launch systems developed in each country, and broken down by operational status. Rocket variants are not distinguished; i.e., the Atlas V series is only counted once for all its configurations 401–431, 501–551, 552, and N22.\n"}
{"id": "1850719", "url": "https://en.wikipedia.org/wiki?curid=1850719", "title": "Controlled vocabulary", "text": "Controlled vocabulary\n\nControlled vocabularies provide a way to organize knowledge for subsequent retrieval. They are used in subject indexing schemes, subject headings, thesauri, taxonomies and other forms of knowledge organization systems. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designers of the schemes, in contrast to natural language vocabularies, which have no such restriction.\n\nIn library and information science controlled vocabulary is a carefully selected list of words and phrases, which are used to tag units of information (document or work) so that they may be more easily retrieved by a search. Controlled vocabularies solve the problems of homographs, synonyms and polysemes by a bijection between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency.\n\nFor example, in the Library of Congress Subject Headings (a subject heading system that uses a controlled vocabulary), authorized terms—subject headings in this case—have to be chosen to handle choices between variant spellings of the same word (American versus British), choice among scientific and popular terms (\"cockroach\" versus \"Periplaneta americana\"), and choices between synonyms (\"automobile\" versus \"car\"), among other difficult issues.\n\nChoices of authorized terms are based on the principles of \"user warrant\" (what terms users are likely to use), \"literary warrant\" (what terms are generally used in the literature and documents), and \"structural warrant\" (terms chosen by considering the structure, scope of the controlled vocabulary).\n\nControlled vocabularies also typically handle the problem of homographs, with qualifiers. For example, the term \"pool\" has to be qualified to refer to either \"swimming pool\" or the game \"pool\" to ensure that each authorized term or heading refers to only one concept.\n\nThere are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences.\n\nHistorically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not.\n\nFor example, the Library of Congress Subject Heading itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term \"Broader term\" and \"Narrow term\".\n\nThe terms are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document's text. Well known subject heading systems include the Library of Congress system, MeSH, and Sears. Well known thesauri include the Art and Architecture Thesaurus and the ERIC Thesaurus.\n\nChoosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue.\n\nControlled vocabulary elements (terms/phrases) employed as tags, to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as metadata.\n\nThere are three main types of indexing languages.\n\n\nWhen indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example, using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each document.\n\nIn recent years free text search as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is \"indexed\"). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors.\n\nControlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce irrelevant items in the retrieval list. These irrelevant items (false positives) are often caused by the inherent ambiguity of natural language. Take the English word \"football\" for example. \"Football\" is the name given to a number of different team sports. Worldwide the most popular of these team sports is association football, which also happens to be called \"soccer\" in several countries. The word \"football\" is also applied to rugby football (rugby union and rugby league), American football, Australian rules football, Gaelic football, and Canadian football. A search for \"football\" therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by tagging the documents in such a way that the ambiguities are eliminated.\n\nCompared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually relevant to the search topic).\n\nIn some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, there is no need to search for other terms that might be synonyms of that term.\n\nHowever, a controlled vocabulary search may also lead to unsatisfactory recall, in that it will fail to retrieve some documents that are actually relevant to the search question.\n\nThis is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with that of the indexer.\n\nAnother possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example, an article might mention football as a secondary focus, and the indexer might decide not to tag it with \"football\" because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless.\n\nOn the other hand, free text searches have high exhaustivity (every word is searched) so although it has much lower precision, it has potential for high recall as long as the searcher overcome the problem of synonyms by entering every combination.\n\nControlled vocabularies may become outdated rapidly in fast developing fields of knowledge, unless the authorized terms are updated regularly. Even in an ideal scenario, a controlled vocabulary is often less specific than the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while this precise problem is not a factor in a free text, as it uses the author's own words.\n\nThe use of controlled vocabularies can be costly compared to free text searches because human experts or expensive automated systems are necessary to index each entry. Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision.\n\nNumerous methodologies have been developed to assist in the creation of controlled vocabularies, including faceted classification, which enables a given data record or document to be described in multiple ways.\n\nControlled vocabularies, such as the Library of Congress Subject Headings, are an essential component of bibliography, the study and classification of books. They were initially developed in library and information science. In the 1950s, government agencies began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the Medical Subject Headings (MeSH) developed by the U.S. National Library of Medicine. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup X.25 networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first full text databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library.\n\nIn large organizations, controlled vocabularies may be introduced to improve technical communication. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing. This consistency of terms is one of the most important concepts in technical writing and knowledge management, where effort is expended to use the same word throughout a document or organization instead of slightly different ones to refer to the same thing.\n\nWeb searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a Semantic Web, in which the content of Web pages is described using a machine-readable metadata scheme. One of the first proposals for such a scheme is the Dublin Core Initiative. An example of a controlled vocabulary which is usable for indexing web pages is PSH.\n\nIt is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web. To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page's contents. The eXchangeable Faceted Metadata Language (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on faceted classification principles.\n\nControlled vocabularies of the Semantic Web define the concepts and relationships (terms) used to describe a field of interest or area of concern. For instance, to declare a person in a machine-readable format, a vocabulary is needed that has the formal definition of \"Person\", such as the Friend of a Friend (FOAF) vocabulary, which has a Person class that defines typical properties of a person including, but not limited to, name, honorific prefix, affiliation, email address, and homepage, or the Person vocabulary of Schema.org. Similarly, a book can be described using the Book vocabulary of Schema.org and general publication terms from the Dublin Core vocabulary, an event with the Event vocabulary of Schema.org, and so on.\n\nTo use machine-readable terms from any controlled vocabulary, web designers can choose from a variety of annotation formats, including RDFa, HTML5 Microdata, or JSON-LD in the markup, or RDF serializations (RDF/XML, Turtle, N3, TriG, TriX) in external files.\n\n"}
{"id": "33232179", "url": "https://en.wikipedia.org/wiki?curid=33232179", "title": "Corporate Technology Directory", "text": "Corporate Technology Directory\n\nThe Corporate Technology Directory also known as the CorpTech directory of technology companies was a directory of technology companies published from 1986-2004 by CorpTech. It listed thousands of technology companies including software, services, and hardware as well as developers.\n\nThe directory was later made available in digital form as a cd and subsequently database subscription.\n\n"}
{"id": "16089380", "url": "https://en.wikipedia.org/wiki?curid=16089380", "title": "Cut-off factor", "text": "Cut-off factor\n\nCut-off factor (AKA \"cut-off length\") is a factor used to calculate the length of a hose cut to achieve the desired overall length of hose plus fittings. It is commonly seen in hydraulic hose and fitting specifications. The cut-off factor is specific to a particular hose fitting.\n\nThe formula used in calculating the optimum overall length is:\n\nformula_1.\n\nIn this formula, C1 represents the cut-off factor of the first hose end and C2 represents the cut-off factor of the second hose end.\n"}
{"id": "58794547", "url": "https://en.wikipedia.org/wiki?curid=58794547", "title": "Direct Autonomous Authentication", "text": "Direct Autonomous Authentication\n\nDirect Autonomous Authentication (DAA) is a cybersecurity platform developed by San Francisco-based technology company Averon.\n\nThe DAA platform enables secure authentication of a mobile user whilst simultaneously preserving privacy of the user.\n\nThe technology was developed in stealth from late 2015, first publicly introduced by Averon in 2018, and featured at the 2018 Consumer Electronics Show as a new technology that combats the increasing threats of cybercrime and consumer account hacking.\nIn contrast to legacy methods of cybersecurity, the DAA platform bypasses end user actions, and rather than focusing on the authentication of a user's device, DAA instead provides autonomous authentication of a user's mobile phone number, since the mobile phone number continues to be associated with the user even when they lose, destroy or upgrade their mobile phone. \n\nThe DAA method uses a proprietary mix of technology developed by Averon that works inside the secure mobile network data pipelines together with encrypted technology already within every smartphone. The combination of these autonomous authentication methods has been described by research analysts as a faster, more secure, and stronger method of cybersecurity than traditional methods.\n\nBlockchain technology incorporated into the DAA platform ensures the privacy of end users. No identifiable personal data is maintained on the platform, therefore public disclosure of one's authentic identity (such as for the purpose of verified social media interactions) is voluntary. DAA technology affords the end user full control over identity disclosure in any given online interaction, which can be controlled by the end user in varying degrees from fully anonymous to fully identified publicly. In cases involving the need for anonymity with regard to an end user's safety, such as in cases of whistleblowers or political activists, the DAA platform's blockchain technology provides a method for both complete anonymity with the option of voluntary verification of limited but often needed data (such as verifying an anonymous user's general location). Thus DAA technology alleviates the heretofore insurmountable challenge of protecting user privacy with the need for authentication.\n\nThe DAA technology platform was designed to be seamlessly adopted for utilization in a wide variety of industries and use cases in which mobile authentication of users is required.\n\nSince its introduction to the market in 2018, the DAA platform has been recognized by a number of industry groups for its innovation, including winning the Gold prize at the 2018 Edison Awards, a Cybersecurity Excellence Award, and a BIG Innovation 2018 Award.\n"}
{"id": "18025626", "url": "https://en.wikipedia.org/wiki?curid=18025626", "title": "Document automation", "text": "Document automation\n\nDocument automation (also known as document assembly) is the design of systems and workflows that assist in the creation of electronic documents. These include logic-based systems that use segments of pre-existing text and/or data to assemble a new document. This process is increasingly used within certain industries to assemble legal documents, contracts and letters. Document automation systems can also be used to automate all conditional text, variable text, and data contained within a set of documents.\n\nAutomation systems allow companies to minimize data entry, reduce the time spent proof-reading, and reduce the risks associated with human error. Additional benefits include: time and financial savings due to decreased paper handling, document loading, storage, distribution, postage/shipping, faxes, telephone, labor and waste.\n\nThe basic functions are to replace the cumbersome manual filling in of repetitive documents with template-based systems where the user answers software-driven interview questions or data entry screen. The information collected then populates the document to form a good first draft'. Today's more advanced document automation systems allow users to create their own data and rules (logic) without the need for programming.\n\nWhile document automation software is used primarily in the legal, financial services, and risk management industries, it can be used in any industry that creates transaction-based documents. A good example of how document automation software can be used is with commercial mortgage documents. A typical commercial mortgage transaction can include several documents including:\nSome of these documents can contain as many as 80 to 100 pages, with hundreds of optional paragraphs and data elements. Document automation software has the ability to automatically fill in the correct document variables based on the transaction data. In addition, some document automation software has the ability to create a document suite where all related documents are encapsulated into one file, making updates and collaboration easy and fast. Established companies in this field includes the likes of Contract Express from Thomson Reuters\n\nSimpler software applications that are easier to learn can also be used to automate the preparation of documents, without undue complexity. For example, Pathagoras holds itself out as a 'plain text, no fields allowed' document assembly system. Clipboard managers allow the user to save frequently-used text fragments, organize them into logical groups, and then quickly access them to paste into final documents.\n\nThere are many documents used in logistics. They are called: invoices, packing lists/slips/sheets (manifests), content lists, pick tickets, arrival acknowledgement forms/reports of many types (e.g. MSDS, damaged goods, returned goods, detailed/summary, etc.), import/export, delivery, bill of lading (BOL), etc. These documents are usually the contracts between the consignee and the consignor, so they are very important for both parties and any intermediary, like a third party logistics company (3PL) and governments. Document handling within logistics, supply chain management and distribution centers is usually performed manual labor or semi-automatically using bar code scanners, software and tabletop laser printers. There are some manufacturers of high speed document automation systems that will automatically compare the laser printed document to the order and either insert or automatically apply an enclosed wallet/pouch to the shipping container (usually a flexible polybag or corrugated fiberboard/rigid container). See below for external website video links showing these document automation systems. Protection of Privacy and Identity Theft are major concerns, especially with the increase of e-Commerce, Internet/Online shopping and Shopping channel (other, past references are catalogue and mail order shopping) making it more important than ever to guarantee the correct document is married or associated to the correct order or shipment every time. Software that produce documents are: ERP, WMS, TMS, legacy middleware and most accounting packages.\n\nA number of research projects have looked into wider standardization and automation of documents in the freight industry.\n\nThe role of automation technology in the production of legal documents has been widely recognised. For example, Richard Susskind’s book ‘The End of Lawyers’ looks at the use of document automation software that enables clients to generate employment contracts and Wills with the use of an online interview or decision tree. Susskind regards Document Assembly as one of 10 'disruptive technologies' that are altering the face of the legal profession. In large law firms document assembly systems are increasingly being used to systemise work, such as complex term sheets and the first drafts of credit agreements.\n\nWith the liberalisation of the UK legal services market spearheaded by the Legal Services Act 2007 large institutions have broadened their services to include legal assistance for their customers. Most of these companies use some element of document automation technology to provide legal document services over the Web. This has been seen as heralding a trend towards commoditisation whereby technologies like document automation result in high volume, low margin legal services being ‘packaged’ and provided to a mass-market audience.\n\nIn United States, companies like LegalZoom and RocketLawyer offer automated document production for individuals and small businesses..\n\nCompanies like Contract Express by Thomson Reuters, HotDocs (acquired by AbacusNext) and HelpSelf Legal also provide document automation services that allow lawyers to customize their own interview workflows to merge into legal documents. These and other sites are incorporating artificial intelligence into their document automation software.\n\nInsurance policies and certificates, depending on the type, policy documents can also be hundreds of pages long and include specific information on the insured. Typically, in the past, these insurance document packets were created by a) typing out free-form letters, b) adding pre-printed brochures c) editing templates and d) customizing graphics with the required information, then manually sorting and inserting all the documents into one packet and mailing them to the insured. The various documents included in one packet could include the following kinds of documents:\n\nA lot of work can go into putting one packet together. In most policy admin systems, the system will generate some kind of policy statement as a starting point but might need to be customized and enhanced with other required materials.\n\n"}
{"id": "2933543", "url": "https://en.wikipedia.org/wiki?curid=2933543", "title": "Electronic funds transfer", "text": "Electronic funds transfer\n\nElectronic funds transfer (EFT) are electronic transfer of money from one bank account to another, either within a single financial institution or across multiple institutions, via computer-based systems, without the direct intervention of bank staff.\n\nAccording to the United States Electronic Fund Transfer Act of 1978 it is \"a funds transfer initiated through an electronic terminal, telephone, computer (including on-line banking) or magnetic tape for the purpose of ordering, instructing, or authorizing a financial institution to debit or credit a consumer’s account. \"\n\nEFT transactions are known by a number of names across countries and different payment systems. For example, in the United States, they may be referred to as \"electronic checks\" or \"e-checks\". In the United Kingdom, the term \"bank transfer\" and \"bank payment\" are used, while in several other European countries \"giro transfer\" is the common term.\n\nEFTs include, but are not limited to: \n\n\n"}
{"id": "1021118", "url": "https://en.wikipedia.org/wiki?curid=1021118", "title": "Fast fracture", "text": "Fast fracture\n\nIn structural engineering and material science, fast fracture is a term given to a phenomenon in which a flaw (such as a crack) in a material expands quickly, and leads to catastrophic failure of the material. Stress acting on a material when fast fracture occurs is less than the material's yield stress. A very representative example of this is what happens when poking a blown up balloon with a needle, that is, fast fracture of the balloon's material.\n\n"}
{"id": "41896815", "url": "https://en.wikipedia.org/wiki?curid=41896815", "title": "Fast interrupt request", "text": "Fast interrupt request\n\nFast Interrupt Requests (FIQs) are a specialized type of Interrupt Request, a standard technique used in computer CPUs to deal with events which need to be processed as they occur such as receiving data from a network card, or keyboard or mouse actions. FIQs are specific to the ARM CPU architecture, which supports two types of interrupts; FIQs for fast, low latency interrupt handling and Interrupt Requests (IRQs), for more general interrupts.\n\nAn FIQ takes priority over an IRQ in an ARM system. Also, only one FIQ source at a time is supported. This helps reduce interrupt latency as the interrupt service routine can be executed directly without determining the source of the interrupt. A context save is not required for servicing an FIQ since it has its own set of banked registers. This reduces the overhead of context switching.\n"}
{"id": "52972673", "url": "https://en.wikipedia.org/wiki?curid=52972673", "title": "Feel Train", "text": "Feel Train\n\nFeel Train is a technology collaborative co-founded by Courtney Stanton and Darius Kazemi and based in Portland, Oregon.\n\nFeel Train is a worker-owned cooperative. Stanton and Kazemi are its first two worker-owners, and the organization is chartered to allow a maximum of eight employees, each with equal salary, equal share in the company and equal firing power over others, including the founders. \n\nFeel Train projects have included the Stay Woke Bot, a Twitter bot developed in collaboration with activists DeRay Mckesson and Samuel Sinyangwe, and Shortcut, an app developed with radio program \"This American Life\" to facilitate sharing audio clips across social media, similar to the way gifs allow video clips to be shared. Feel Train is also developing a Twitter bot based on the Obama Social Media Archive called Relive 44, which beginning in May 2017 will repost, eight years later, every tweet from President Barack Obama (whose first tweet came in May 2009.)\n\nFeel Train website\n"}
{"id": "4575623", "url": "https://en.wikipedia.org/wiki?curid=4575623", "title": "First-out alarm", "text": "First-out alarm\n\nA first-out alarm is an alarm that indicates in some manner that it was the first of a series. This is necessary in circumstances such as an automatic trip or shutdown of equipment, where many alarms will announce as a result of a shutdown. The first-out alarm will clearly identify the root cause of the trip or shutdown.\n\nISA Alarm Standard 18.1 Annunciator Sequences and Specifications\n"}
{"id": "3694654", "url": "https://en.wikipedia.org/wiki?curid=3694654", "title": "Gestell", "text": "Gestell\n\nGestell (or sometimes Ge-stell) is a German word used by twentieth-century German philosopher Martin Heidegger to describe what lies behind or beneath modern technology. Heidegger introduced the term in 1954 in \"The Question Concerning Technology\", a text based on the lecture \"The Framework\" (\"Das Gestell\") first presented on December 1, 1949, in Bremen. It was derived from the root word \"stellen\", which means \"to put\" or \"to place\" and combined with the German prefix \"Ge-\", which denotes a form of \"gathering\" or \"collection\". The term gathers together all kinds of entities and orders them in a certain way. \n\nHeidegger applied the concept of \"Gestell\" to his exposition of the essence of technology. He concluded that technology is fundamentally Enframing (\"Gestell\").\nAs such, the essence of technology is \"Gestell\". Indeed, \"\"Gestell\", literally 'framing', is an all-encompassing view of technology, not as a means to an end, but rather a mode of human existence\". Such enframing pertains to the manner reality appears or unveils itself in the period of modern technology and people born into this \"mode of ordering\" are always embedded into the Gestell (enframing). \n\nIn defining the essence of technology as \"Gestell\", Heidegger indicated that all that has come to presence in the world has been enframed. Thus what is revealed in the world, what has shown itself as itself (the truth of itself) required first an Enframing, literally a way to exist in the world, to be able to be seen and understood. Concerning the essence of technology and how we see things in our technological age, the world has been framed as the \"standing-reserve.\" Heidegger writes,\n\nEnframing means the gathering together of that setting-upon which sets upon man, i.e., challenges him forth, to reveal the real, in the mode of ordering, as standing-reserve. Enframing means that way of revealing which holds sway in the essence of modern technology and which is itself nothing technological.\n\nFurthermore, Heidegger uses the word in a way that is uncommon by giving \"Gestell\" an active role. In ordinary usage the word would signify simply a display apparatus of some sort, like a book rack, or picture frame; but for Heidegger, \"Gestell\" is literally a challenging forth, or \"perform\"ative \"gathering together\", for the purpose of revealing or presentation. If applied to science and modern technology, \"standing reserve\" is active in the case of a river once it generates electricity or the earth if revealed as a coal-mining district or the soil as a mineral deposit. \n\nFor some scholars, Gestell effectively explains the violence of technology. This is attributed to Heidegger's explanation that, when Gestell holds sway, \"it drives out every other possibility of revealing\" and that it \"conceals that revealing which, in the sense of \"poiesis\", lets what presences come forth into appearance.\"\n\n"}
{"id": "20895829", "url": "https://en.wikipedia.org/wiki?curid=20895829", "title": "History of rockets", "text": "History of rockets\n\nThe first rockets were used as propulsion systems for arrows, and may have appeared as early as the 10th century Song dynasty China. However more solid documentary evidence does not appear until the 13th century. The technology probably spread across Eurasia in the wake of the Mongol invasions of the mid-13th century. Usage of rockets as weapons before modern rocketry is attested in China, Korea, Indian subcontinent, and Europe. One of the first recorded rocket launchers is the \"wasp nest\" fire arrow launcher produced by the Ming dynasty in 1380. In Europe rockets were also used in the same year at the Battle of Chioggia. The Joseon kingdom of Korea used a type of mobile multiple rocket launcher known as the \"Munjong Hwacha\" by 1451. Iron-cased rockets, known as Mysorean rockets, were developed in Kingdom of Mysore by the mid 18th century in India, and were later copied by the British. The later models and improvements were known as the Congreve rocket and used in the Napoleonic Wars.\n\nUse of liquid propellants instead of gunpowder greatly improved the effectiveness of rocket artillery in World War I, and opened up the possibility of manned spaceflight after 1918.\n\nThe dating of the invention of the first rocket, otherwise known as the gunpowder propelled fire arrow, is disputed. The History of Song attributes the invention to two different people at different times, Feng Zhisheng in 969 and Tang Fu in 1000. However Joseph Needham argues that rockets could not have existed befoire the 12th century, since the gunpowder formulas listed in the Wujing Zongyao are not suitable as rocket propellant.\n\nRockets may have been used as early as 1232, when reports appeared describing fire arrows and 'iron pots' that could be heard for 5 leagues (25 km, or 15 miles) when they exploded upon impact, causing devastation for a radius of , apparently due to shrapnel. Rockets are recorded to have been used by the Song navy in a military exercise dated to 1245. Internal-combustion rocket propulsion is mentioned in a reference to 1264, recording that the 'ground-rat,' a type of firework, had frightened the Empress-Mother Gongsheng at a feast held in her honor by her son the Emperor Lizong.\n\nSubsequently, rockets are included in the military treatise \"Huolongjing\", also known as the Fire Drake Manual, written by the Chinese artillery officer Jiao Yu in the mid-14th century. This text mentions the first known multistage rocket, the 'fire-dragon issuing from the water' (huo long chu shui), thought to have been used by the Chinese navy.\n\nRocket launchers known as \"wasp nests\" were ordered by the Ming army in 1380.\n\nThe American historian Frank H. Winter proposed in \"The Proceedings of the Twentieth and Twenty-First History Symposia of the International Academy of Astronautics\" that southern China and the Laotian community rocket festivals might have been key in the subsequent spread of rocketry in the Orient.\n\nThe Chinese fire arrow was adopted by the Mongols in northern China, who employed Chinese rocketry experts as mercenaries in the Mongol army. Rockets are thought to have spread via the Mongol invasions to other areas of Eurasia in the mid 13th century.\n\nRocket-like weapons are reported to have been used at the Battle of Mohi in the year 1241.\n\nBetween 1270 and 1280, Hasan al-Rammah wrote his \"al-furusiyyah wa al-manasib al-harbiyya\" (\"The Book of Military Horsemanship and Ingenious War Devices\"), which included 107 gunpowder recipes, 22 of which are for rockets. According to Ahmad Y Hassan, al-Rammah's recipes were more explosive than rockets used in China at the time. The terminology used by al-Rammah indicates a Chinese origin for the gunpowder weapons he wrote about, such as rockets and fire lances. Ibn al-Baitar, an Arab from Spain who had immigrated to Egypt, described saltpeter as \"snow of China\" ( ). Al-Baytar died in 1248. The earlier Arab historians called saltpeter \"Chinese snow\" and \" Chinese salt.\"\nThe Arabs used the name \"Chinese arrows\" to refer to rockets. The Arabs called fireworks \"Chinese flowers\". While saltpeter was called \"Chinese Snow\" by Arabs, it was called \"Chinese salt\" ( \"namak-i čīnī\") by the Iranians, or \"salt from the Chinese marshes\" ( ).\n\nIn 1300 Mongol mercenaries in India are recorded to have used hand held rockets. By the mid-14th century Indians were also using rockets in warfare.\n\nThe Korean kingdom of Joseon started producing gunpowder in 1374 and was producing cannons and rockets by 1377. However the multiple rocket launching carts known as the \"Munjong hwacha\" did not appear until 1451.\n\nIn Europe, Roger Bacon mentions gunpowder in his \"Opus Majus\" of 1267.\n\nHowever rockets do not feature in European warfare until the 1380 Battle of Chioggia.\n\nKonrad Kyeser described rockets in his famous military treatise Bellifortis around 1405.\n\nJean Froissart (c. 1337 – c. 1405) had the idea of launching rockets through tubes, so that they could make more accurate flights. Froissart's idea is a forerunner of the modern bazooka.\n\nAccording to the 18th-century historian Ludovico Antonio Muratori, rockets were used in the war between the Republics of Genoa and Venice at Chioggia in 1380. It is uncertain whether Muratori was correct in his interpretation, as the reference might also have been to bombard, but\nMuratori is the source for the widespread claim that the earliest recorded European use of rocket artillery dates to 1380.\nKonrad Kyeser described rockets in his famous military treatise Bellifortis around 1405.\nKyeser describes three types of rockets, swimming, free flying and captive.\n\nJoanes de Fontana in \"Bellicorum instrumentorum liber\" (c. 1420) described flying rockets in the shape of doves, running rockets in the shape of hares, and a large car driven by three rockets, as well as a large rocket torpedo with the head of a sea monster.\n\nIn the mid-16th century, Conrad Haas wrote a book that described rocket technology that combined fireworks and weapons technologies. This manuscript was discovered in 1961, in the Sibiu public records (Sibiu public records \"Varia II 374\"). His work dealt with the theory of motion of multi-stage rockets, different fuel mixtures using liquid fuel, and introduced delta-shape fins and bell-shaped nozzles.\n\nThe name \"Rocket\" comes from the Italian \"rocchetta\", meaning \"bobbin\" or \"little spindle\", given due to the similarity in shape to the bobbin or spool used to hold the thread to be fed to a spinning wheel. The Italian term was adopted into German in the mid 16th century, by Leonhard Fronsperger in a book on rocket artillery published in 1557, using the spelling \"rogete\", and by Conrad Haas as \"rackette\"; adoption into English dates to ca. 1610. Johann Schmidlap, a German fireworks maker, is believed to have experimented with staging in 1590.\n\n\"Lagari Hasan Çelebi\" was a legendary Ottoman aviator who, according to an account written by Evliya Çelebi, made a successful manned rocket flight. Evliya Çelebi purported that in 1633 Lagari Hasan Çelebi launched in a 7-winged rocket using 50 okka (63.5 kg, or 140 lbs) of gunpowder from Sarayburnu, the point below Topkapı Palace in Istanbul.\n\n\"Artis Magnae Artilleriae pars prima\" (\"Great Art of Artillery, the First Part\", also known as \"The Complete Art of Artillery\"), first printed in Amsterdam in 1650, was translated to French in 1651, German in 1676, English and Dutch in 1729 and Polish in 1963. For over two centuries, this work of Polish-Lithuanian Commonwealth nobleman Kazimierz Siemienowicz was used in Europe as a basic artillery manual. The book provided the standard designs for creating rockets, fireballs, and other pyrotechnic devices. It contained a large chapter on caliber, construction, production and properties of rockets (for both military and civil purposes), including multi-stage rockets, batteries of rockets, and rockets with delta wing stabilizers (instead of the common guiding rods).\n\nIn 1792, the first iron-cased rockets were successfully developed and used by Mysorean rulers of the Kingdom of Mysore in India against the larger British East India Company forces during the Anglo-Mysore Wars. The British then took an active interest in the technology and developed it further during the 19th century. The Mysore rockets of this period were much more advanced than the British had previously seen, chiefly because of the use of iron tubes for holding the propellant; this enabled higher thrust and longer range for the missile (up to 2 km range). After Tipu's eventual defeat in the Fourth Anglo-Mysore War and the capture of the Mysore iron rockets, they were influential in British rocket development, inspiring the Congreve rocket, which was soon put into use in the Napoleonic Wars.\n\nWilliam Congreve, son of the Comptroller of the Royal Arsenal, Woolwich, London, became a major figure in the field. From 1801, Congreve researched on the original design of Mysore rockets and set on a vigorous development program at the Arsenal's laboratory. Congreve prepared a new propellant mixture, and developed a rocket motor with a strong iron tube with conical nose. This early Congreve rocket weighed about 32 pounds (14.5 kilograms). The Royal Arsenal's first demonstration of solid fuel rockets was in 1805. The rockets were effectively used during the Napoleonic Wars and the War of 1812. Congreve published three books on rocketry.\n\nFrom there, the use of military rockets spread throughout the western world. At the Battle of Baltimore in 1814, the rockets fired on Fort McHenry by the rocket vessel HMS \"Erebus\" were the source of the \"rockets' red glare\" described by Francis Scott Key in The Star-Spangled Banner. Rockets were also used in the Battle of Waterloo.\n\nEarly rockets were very inaccurate. Without the use of spinning or any controlling feedback loop, rockets had a strong tendency to veer sharply off of their intended course. The early Mysorean rockets and their successor British Congreve rockets reduced this somewhat by attaching a long stick to the end of a rocket (similar to modern bottle rockets) to make it harder for the rocket to change course. The largest of the Congreve rockets was the 32-pound (14.5 kg) Carcass, which had a 15-foot (4.6 m) stick. Originally, sticks were mounted on the side, but this was later changed to mounting in the center of the rocket, reducing drag and enabling the rocket to be more accurately fired from a segment of pipe.\n\nIn 1815, Alexander Dmitrievich Zasyadko began his work on creating military gunpowder rockets. He constructed rocket-launching platforms, which allowed rockets to be fired in salvos (6 rockets at a time), and gun-laying devices. Zasyadko elaborated a tactic for military use of rocket weaponry. In 1820, Zasyadko was appointed head of the Petersburg Armory, Okhtensky Powder Factory, pyrotechnic laboratory and the first Highest Artillery School in Russia. He organized rocket production in a special rocket workshop and created the first rocket sub-unit in the Russian army.\n\nArtillery captain Józef Bem of the Kingdom of Poland started experiments with what was then called in Polish \"raca kongrewska\". These culminated in his 1819 report \"Notes sur les fusees incendiares\" (German edition: \"Erfahrungen über die Congrevischen Brand-Raketen bis zum Jahre 1819 in der Königlischen Polnischen Artillerie gesammelt\", Weimar 1820). The research took place in the Warsaw Arsenal, where captain Józef Kosiński also developed the multiple-rocket launchers adapted from horse artillery gun carriage. The 1st Rocketeer Corps was formed in 1822, it first saw combat during the Polish–Russian War 1830–31.\n\nThe accuracy problem was greatly improved in 1844 when William Hale modified the rocket design so that thrust was slightly vectored, causing the rocket to spin along its axis of travel like a bullet. The Hale rocket removed the need for a rocket stick, travelled further due to reduced air resistance, and was far more accurate.\n\nIn 1865 the British Colonel Edward Mounier Boxer built an improved version of the Congreve rocket placing two rockets in one tube, one behind the other.\n\nAt the beginning of the 20th century, there was a burst of scientific investigation into interplanetary travel, largely driven by the inspiration of fiction by writers such as Jules Verne and H. G. Wells as well as philosophical movements like Russian cosmism. Scientists seized on the rocket as a technology that was able to achieve this in real life, a possibility first recognized in 1861 by William Leitch.\n\nIn 1903, high school mathematics teacher Konstantin Tsiolkovsky (1857–1935), inspired by Verne and Cosmism, published \"Исследование мировых пространств реактивными приборами\" (\"The Exploration of Cosmic Space by Means of Reaction Devices\"), the first serious scientific work on space travel. The Tsiolkovsky rocket equation—the principle that governs rocket propulsion—is named in his honor (although it had been discovered previously). He also advocated the use of liquid hydrogen and oxygen for propellant, calculating their maximum exhaust velocity. His work was essentially unknown outside the Soviet Union, but inside the country it inspired further research, experimentation and the formation of the Society for Studies of Interplanetary Travel in 1924.\nIn 1912, Robert Esnault-Pelterie published a lecture on rocket theory and interplanetary travel. He independently derived Tsiolkovsky's rocket equation, did basic calculations about the energy required to make round trips to the Moon and planets, and he proposed the use of atomic power (i.e. radium) to power a jet drive.\nIn 1912 Robert Goddard, inspired from an early age by H.G. Wells, began a serious analysis of rockets, concluding that conventional solid-fuel rockets needed to be improved in three ways. First, fuel should be burned in a small combustion chamber, instead of building the entire propellant container to withstand the high pressures. Second, rockets could be arranged in stages. Finally, the exhaust speed (and thus the efficiency) could be greatly increased to beyond the speed of sound by using a De Laval nozzle. He patented these concepts in 1914. He also independently developed the mathematics of rocket flight.\n\nDuring World War I Yves Le Prieur, a French naval officer and inventor, later to create a pioneering scuba diving apparatus, developed air-to-air solid-fuel rockets. The aim was to destroy observation captive balloons (called saucisses or Drachens) used by German artillery. These rather crude black powder, steel-tipped incendiary rockets (made by Ruggieri) were first tested from a Voisin aircraft, wing-bolted on a fast Picard Pictet sports car and then used in battle on real aircraft. A typical layout was eight electrically fired Le Prieur rockets fitted on the interpane struts of a Nieuport aircraft. If fired at sufficiently short distance, a spread of Le Prieur rockets proved to be quite deadly. Belgian ace Willy Coppens claimed dozens of Drachen kills during World War I.\n\nIn 1920, Goddard published these ideas and experimental results in \"A Method of Reaching Extreme Altitudes\". The work included remarks about sending a solid-fuel rocket to the Moon, which attracted worldwide attention and was both praised and ridiculed. A \"New York Times\" editorial suggested:\n\nIn 1923, Hermann Oberth (1894–1989) published \"Die Rakete zu den Planetenräumen\" (\"The Rocket into Planetary Space\"), a version of his doctoral thesis, after the University of Munich had rejected it.\n\nIn 1924, Tsiolkovsky also wrote about multi-stage rockets, in 'Cosmic Rocket Trains'.\n\nModern rockets originated when Goddard attached a supersonic (de Laval) nozzle to the combustion chamber of a liquid-fueled rocket engine. These nozzles turn the hot gas from the combustion chamber into a cooler, hypersonic, highly directed jet of gas, more than doubling the thrust and raising the engine efficiency from 2% to 64%. On 16 March 1926 Robert Goddard launched the world's first liquid-fueled rocket in Auburn, Massachusetts.\n\nDuring the 1920s, a number of rocket research organizations appeared worldwide. In 1927 the German car manufacturer Opel began to research rocket vehicles together with Mark Valier and the solid-fuel rocket builder Friedrich Wilhelm Sander. In 1928, Fritz von Opel drove a rocket car, the Opel-RAK.1 on the Opel raceway in Rüsselsheim, Germany. In 1928 the Lippisch Ente flew: rocket power launched the manned glider, although it was destroyed on its second flight. In 1929 von Opel started at the Frankfurt-Rebstock airport with the Opel-Sander RAK 1-airplane, which was damaged beyond repair during a hard landing after its first flight.\n\nIn the mid-1920s, German scientists had begun experimenting with rockets that used liquid propellants capable of reaching relatively high altitudes and distances. In 1927 and also in Germany, a team of amateur rocket engineers had formed the \"Verein für Raumschiffahrt\" (Society for Space Travel, or VfR), and in 1931 launched a liquid propellant rocket (using oxygen and gasoline).\n\nRocketry in the Soviet Union also began with amateur societies, foremost was the Group for the Study of Reactive Propulsion (GIRD) headed by Friedrich Zander and Sergei Korolev. From 1931 to 1937 in the Soviet Union, extensive scientific work on rocket engine design occurred at the Gas Dynamics Laboratory (GDL) in Leningrad, which was merged with GIRD in 1933 bringing rocketry fully under government control. The well-funded and -staffed laboratory built over 100 experimental engines under the direction of Valentin Glushko. The work included regenerative cooling, hypergolic propellant ignition, and fuel injector designs that included swirling and bi-propellant mixing injectors. However, Glushko's arrest during Stalinist purges in 1938 curtailed the development.\n\nSimilar work was also done from 1932 onwards by the Austrian professor Eugen Sänger, who migrated from Austria to Germany in 1936. He worked there on rocket-powered spaceplanes such as Silbervogel (sometimes called the \"antipodal\" bomber).\n\nOn November 12, 1932 at a farm in Stockton NJ, the American Interplanetary Society's attempt to static-fire their first rocket (based on German Rocket Society designs) failed in a fire.\n\nIn 1936, a British research programme based at Fort Halstead under the direction of Dr Alwyn Crow started work on a series of unguided solid-fuel rockets that could be used as anti-aircraft weapons. In 1939, a number of test firings were carried out in the British colony of Jamaica, on a purpose built range.\n\nIn the 1930s, the German \"Reichswehr\" (which in 1935 became the \"Wehrmacht\") began to take an interest in rocketry. Artillery restrictions imposed by the 1919 Treaty of Versailles limited Germany's access to long-distance weaponry. Seeing the possibility of using rockets as long-range artillery fire, the Wehrmacht initially funded the VfR team, but because their focus was strictly scientific, created its own research team. At the behest of military leaders, Wernher von Braun, at the time a young aspiring rocket scientist, joined the military (followed by two former VfR members) and developed long-range weapons for use in World War II by Nazi Germany.\n\nAt the start of the war, the British had equipped their warships with unrotated projectile unguided anti-aircraft rockets, and by 1940, the Germans had developed a surface-to-surface multiple rocket launcher, the \"Nebelwerfer\" and the Soviets already had introduced the RS-132 air-to-ground rocket. All of these rockets were developed for a variety of roles, notably the Katyusha rocket.\n\nIn 1943, production of the V-2 rocket began in Germany. It had an operational range of and carried a warhead, with an amatol explosive charge. It normally achieved an operational maximum altitude of around , but could achieve if launched vertically. The vehicle was similar to most modern rockets, with turbopumps, inertial guidance and many other features. Thousands were fired at various Allied nations, mainly Belgium, as well as England and France. While they could not be intercepted, their guidance system design and single conventional warhead meant that they were insufficiently accurate against military targets. A total of 2,754 people in England were killed, and 6,523 were wounded before the launch campaign was ended. There were also 20,000 deaths of slave labour during the construction of V-2s. While it did not significantly affect the course of the war, the V-2 provided a lethal demonstration of the potential for guided rockets as weapons.\n\nIn parallel with the guided missile programme in Nazi Germany, rockets were also used on aircraft, either for assisting horizontal take-off (RATO), vertical take-off (Bachem Ba 349 \"Natter\") or for powering them (Me 163, etc.). During the war Germany also developed several guided and unguided air-to-air, ground-to-air and ground-to-ground missiles (see list of World War II guided missiles of Germany).\n\nAt the end of World War II, competing Russian, British, and US military and scientific crews raced to capture technology and trained personnel from the German rocket program at Peenemünde. Russia and Britain had some success, but the United States benefited the most. The US captured a large number of German rocket scientists, including von Braun, and brought them to the United States as part of Operation Paperclip. In America, the same rockets that were designed to rain down on Britain were used instead by scientists as research vehicles for developing the new technology further. The V-2 evolved into the American Redstone rocket, used in the early space program.\n\nAfter the war, rockets were used to study high-altitude conditions, by radio telemetry of temperature and pressure of the atmosphere, detection of cosmic rays, and further research; notably the Bell X-1, the first manned vehicle to break the sound barrier. This continued in the US under von Braun and the others, who were destined to become part of the US scientific community.\n\nIndependently, in the Soviet Union's space program research continued under the leadership of the chief designer Sergei Korolev. With the help of German technicians, the V-2 was duplicated and improved as the R-1, R-2, and R-5 missiles. German designs were abandoned in the late 1940s, and the foreign workers were sent home. A new series of engines built by Glushko and based on inventions of Aleksei Mihailovich Isaev formed the basis of the first ICBM, the R-7. The R-7 launched the first satellite, Sputnik 1, and later Yuri Gagarin, the first man into space, and the first lunar and planetary probes. This rocket is still in use today. These prestigious events attracted the attention of top politicians, along with additional funds for further research.\n\nOne problem that had not been solved was atmospheric reentry. It had been shown that an orbital vehicle easily had enough kinetic energy to vaporize itself, and yet it was known that meteorites can make it down to the ground. The mystery was solved in the US in 1951 when H. Julian Allen and A. J. Eggers, Jr. of the National Advisory Committee for Aeronautics (NACA) made the counterintuitive discovery that a blunt shape (high drag) permitted the most effective heat shield. With this type of shape, around 99% of the energy goes into the air rather than the vehicle, and this permitted safe recovery of orbital vehicles.\n\nThe Allen and Eggers discovery, initially treated as a military secret, was eventually published in 1958. Blunt body theory made possible the heat shield designs that were embodied in the Mercury, Gemini, Apollo, and Soyuz space capsules, enabling astronauts and cosmonauts to survive the fiery re-entry into Earth's atmosphere. Some spaceplanes such as the Space Shuttle made use of the same theory. At the time the STS was being conceived, Maxime Faget, the Director of Engineering and Development at the Manned Spacecraft Center, was not satisfied with the purely \"lifting re-entry\" method (as proposed for the cancelled X-20 \"Dyna-Soar\"). He designed a space shuttle which operated as a blunt body by entering the atmosphere at an extremely high angle of attack of 40° with the underside facing the direction of flight, creating a large shock wave that would deflect most of the heat around the vehicle instead of into it. The Space Shuttle essentially uses a combination of a \"ballistic entry\" (Blunt body theory) and then at an altitude of about , the re-entry interface takes place. Here the atmosphere is dense enough for the Space Shuttle to begin its \"lifting re-entry\" by reducing the angle-of-attack, pointing the nose down and using the lift its wings generate to \"start flying\" (gliding) towards the landing site.\n\nRockets became extremely important militarily as modern intercontinental ballistic missiles (ICBMs) when it was realized that nuclear weapons carried on a rocket vehicle were essentially impossible for existing defense systems to stop once launched, and launch vehicles such as the R-7, Atlas, and Titan became delivery platforms for these weapons.\nFueled partly by the Cold War, the 1960s became the decade of rapid development of rocket technology particularly in the Soviet Union (Vostok, Soyuz, Proton) and in the United States (e.g. the X-15 and X-20 Dyna-Soar aircraft). There was also significant research in other countries, such as France, Britain, Japan, Australia, etc., and a growing use of rockets for Space exploration, with pictures returned from the far side of the Moon and unmanned flights for Mars exploration.\n\nIn America, the manned spaceflight programs, Project Mercury, Project Gemini, and later the Apollo program, culminated in 1969 with the first manned landing on the moon using the Saturn V, causing the New York Times to retract its earlier editorial implying that spaceflight couldn't work:\n\nIn the 1970s, the United States made five more lunar landings before cancelling the Apollo program in 1975. The replacement vehicle, the partially reusable Space Shuttle, was intended to be cheaper, but no large reduction in costs was achieved. Meanwhile, in 1973, the expendable Ariane programme was begun, a launcher that by the year 2000 would capture much of the geosat market.\n\n\n"}
{"id": "39395954", "url": "https://en.wikipedia.org/wiki?curid=39395954", "title": "Hyperscale", "text": "Hyperscale\n\nIn computing, hyperscale is the ability of an architecture to scale appropriately as increased demand is added to the system. This typically involves the ability to seamlessly provision and add compute, memory, networking, and storage resources to a given node or set of nodes that make up a larger computing, distributed computing, or grid computing environment. Hyperscale computing is necessary in order to build a robust and scalable cloud, big data, map reduce, or distributed storage system and is often associated with the infrastructure required to run large distributed sites such as Facebook, Google, Microsoft, Amazon, or Oracle. Companies like Ericsson and Intel provide hyperscale infrastructure kits for IT service providers.\n\n"}
{"id": "15275", "url": "https://en.wikipedia.org/wiki?curid=15275", "title": "ISO 216", "text": "ISO 216\n\nISO 216 specifies international standard (ISO) paper sizes used in most countries in the world today, although not in Canada, the United States, Mexico, Colombia, or the Dominican Republic. The standard defines the \"A\" and \"B\" series of paper sizes, including A4, the most commonly available paper size worldwide. Two supplementary standards, ISO 217 and ISO 269, define related paper sizes; the ISO 269 \"C\" series is commonly listed alongside the A and B sizes.\n\nAll ISO 216, ISO 217 and ISO 269 paper sizes (except some envelopes) have the same aspect ratio, :1, within rounding to millimetres. This ratio has the unique property that when cut or folded in half widthways, the halves also have the same aspect ratio. Each ISO paper size is one half of the area of the next larger size in the same series.\n\nIn 1786, the German scientist Georg Christoph Lichtenberg described the advantages of basing a paper size on an aspect ratio of in a letter to Johann Beckmann. The formats that became ISO paper sizes A2, A3, B3, B4, and B5 were developed in France. They were listed in a 1798 law on taxation of publications that was based in part on page sizes.\nThe main advantage of this system is its scaling. Rectangular paper with an aspect ratio of has the unique property that, when cut or folded in half midway between its shorter sides, each half has the same aspect ratio and half the area of the whole sheet before it was divided. Equivalently, if one lays two same-sized sheets paper with an aspect ratio of side-by-side along their longer side, they form a larger rectangle with the aspect ratio of and double the area of each individual sheet.\n\nThe ISO system of paper sizes exploit these properties of the aspect ratio. In each series of sizes (for example, series A), the largest size is numbered 0 (for example, A0), and each successive size (for example, A1, A2, etc.) has half the area of the preceding sheet and can be cut by halving the length of the preceding size sheet. The new measurement is rounded down to the nearest millimetre. A folded brochure can be made by using a sheet of the next larger size (for example, an A4 sheet is folded in half to make a brochure with size A5 pages. An office photocopier or printer can be designed to reduce a page from A4 to A5 or to enlarge a page from A4 to A3. Similarly, two sheets of A4 can be scaled down to fit one A4 sheet without excess empty paper.\n\nThis system also simplifies calculating the weight of paper. Under ISO 536, paper's grammage is defined as a sheet's weight in grams (g) per area in square metres (abbreviated g/m or gsm). Since an A0 sheet has an area of 1 m, its weight in grams is the same as its grammage. One can derive the grammage of other sizes by arithmetic division in g/m. A standard A4 sheet made from 80 g/m paper weighs 5 g, as it is (four halvings, ignoring rounding) of an A0 page. Thus the weight, and the associated postage rate, can be easily approximated by counting the number of sheets used.\n\nISO 216 and its related standards were first published between 1975 and 1995:\n\nPaper in the A series format has an aspect ratio of (≈ 1.414, when ignoring rounding). A0 is defined so that it has an area of 1 square metre before rounding to the nearest millimeter. Successive paper sizes in the series (A1, A2, A3, etc.) are defined by halving the length of the preceding paper size and rounding down, so that the long side of A(\"n\"+1) is the same length as the short side of A\"n\". Hence, each next size is roughly half of the prior size. So, an A1 page can fit 2 A2 pages inside the same area.\n\nThe most used of this series is the size A4, which is and thus almost exactly in area. For comparison, the letter paper size commonly used in North America () is about 6 mm (0.24 in) wider and 18 mm (0.71 in) shorter than A4. Then, the size of A5 paper is half of A4, as 148 x 210 mm (5.8 x 8.3 in). \n\nThe geometric rationale behind the square root of 2 is to maintain the aspect ratio of each subsequent rectangle after cutting or folding an A-series sheet in half, perpendicular to the larger side. Given a rectangle with a longer side, , and a shorter side, , ensuring that its aspect ratio, , will be the same as that of a rectangle half its size, , which means that , which reduces to ; in other words, an aspect ratio of 1:.\n\nThe formula that gives the larger border of the paper size A\"n\" in metres and without rounding off is the geometric sequence:\nThe paper size A\"n\" thus has the dimension\nand area (before rounding)\n\nThe measurement in millimetres of the long side of A\"n\" can be calculated as\n(brackets represent the floor function).\n\nThe B series is defined in the standard as follows: \"A subsidiary series of sizes is obtained by placing the geometrical means between adjacent sizes of the A series in sequence.\" The use of the geometric mean makes each step in size: B0, A0, B1, A1, B2 … smaller than the previous one by the same factor. As with the A series, the lengths of the B series have the ratio , and folding one in half (and rounding down to the nearest millimeter) gives the next in the series. The shorter side of B0 is exactly 1 metre.\n\nThe measurement in millimetres of the long side of B\"n\" can be calculated as\n\nThere is also an incompatible Japanese B series which the JIS defines to have 1.5 times the area of the corresponding JIS A series (which is identical to the ISO A series). Thus, the lengths of JIS B series paper are ≈ 1.22 times those of A-series paper. By comparison, the lengths of ISO B series paper are ≈ 1.19 times those of A-series paper.\n\nThe C series formats are geometric means between the B series and A series formats with the same number (e.g., C2 is the geometric mean between B2 and A2). The width to height ratio is as in the A and B series. The C series formats are used mainly for envelopes. An A4 page will fit into a C4 envelope. C series envelopes follow the same ratio principle as the A series pages. For example, if an A4 page is folded in half so that it is A5 in size, it will fit into a C5 envelope (which will be the same size as a C4 envelope folded in half). The lengths of ISO C series paper are therefore ≈ 1.09 times those of A-series paper.\n\nA, B, and C paper fit together as part of a geometric progression, with ratio of successive side lengths of , though there is no size half-way between B\"n\" and A(\"n\" − 1): A4, C4, B4, \"D4\", A3, …; there is such a D-series in the Swedish extensions to the system.\n\nThe measurement in millimetres of the long side of C\"n\" can be calculated as\n\nThe tolerances specified in the standard are:\n\nThese are related to comparison between series A, B and C.\n\nThe ISO 216 formats are organized around the ratio 1:; two sheets next to each other together have the same ratio, sideways. In scaled photocopying, for example, two A4 sheets reduced to A5 size fit exactly onto one A4 sheet, and an A4 sheet in magnified size onto an A3 sheet; in each case, there is neither waste nor want.\n\nThe principal countries not generally using the ISO paper sizes are the United States and Canada, which use the Letter, Legal and Executive system. Although they have also officially adopted the ISO 216 paper format, Mexico, Panama, Venezuela, Colombia, the Philippines, and Chile also use mostly U.S. paper sizes.\n\nRectangular sheets of paper with the ratio 1: are popular in paper folding, such as origami, where they are sometimes called \"A4 rectangles\" or \"silver rectangles\". In other contexts, the term \"silver rectangle\" can also refer to a rectangle in the proportion 1:(1 + ), known as the silver ratio.\n\nAn important adjunct to the ISO paper sizes, particularly the A series, are the technical drawing line widths specified in ISO 128, and the matching technical pen widths of 0.13, 0.18, 0.25, 0.35, 0.5, 0.7, 1.0, 1.40, and 2.0 mm, as specified in . Color codes are assigned to each size to facilitate easy recognition by the drafter. These sizes increase by a factor of , so that particular pens can be used on particular sizes of paper, and then the next smaller or larger size can be used to continue the drawing after it has been reduced or enlarged, respectively. For example, a continuous thick line on A0 size paper shall be drawn with a 0.7 mm pen, the same line on A1 paper shall be drawn with a 0.5 mm pen, and finally on A2, A3, or A4 paper it shall be drawn with a 0.35 mm pen.\n\nThe earlier DIN 6775 standard upon which ISO 9175-1 is based also specified a term and symbol for easy identification of pens and drawing templates compatible with the standard, called , which may still be found on some technical drafting equipment.\n\n\n"}
{"id": "12675001", "url": "https://en.wikipedia.org/wiki?curid=12675001", "title": "List of LNG terminals", "text": "List of LNG terminals\n\nLiquefied natural gas (LNG) is the liquefied form of natural gas, which has a much smaller volume than natural gas in its gaseous form. This liquefied condition is used to facilitate the carriage of natural gas over long distances, often by sea, in specialized tanks. \n\nLNG port terminals are purpose-built port terminals designed to accommodate large LNG carrier ships designed to load, carry and unload LNG. These LNG terminals are located adjacent to a gas liquefaction and storage plant (export), or to a gas regasification and storage plant (import), which are themselves connected to gas pipelines connected to on-shore or off-shore gas fields (export) or to storage and distribution plants (import).\n\n\n\n\n\n\n\n\nLahari\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following LNG off-loading and regasification terminals are located in the United States and Gulf of Mexico:\n\n\n\nNeed to add Annova LNG and Texas LNG which along with Rio Grande LNG are seeking FERC approval to build and operate at the Port of Brownsville, TX.\n\nAlso, NextDecade, parent company of Rio Grande LNG, is seeking DOE permission for a proposed Galveston Bay LNG project. \n\n\n\n\n\nThe United States has had a massive shift in LNG terminal planning and construction starting in 2010-2011 due to rapid increase in domestic supply with the widespread adoption of hydraulic fracking. Many brand-new LNG import terminals are planning or have begun addition of liquefaction facilities to operate as export terminals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe country also has liquefaction terminals in more remote areas for export, and imports from the Middle East in areas with dense population.\n\n\n\n\n\n\n\n\n\nKaliningrad LNG Terminal\n\n\n\nProposed LNG Terminals:\n\n\n\n11.http://www.lngglobal.com/latest/sabine-pass-progress-report-train-2-first-cargo-mid-august-2016.html\n"}
{"id": "37630506", "url": "https://en.wikipedia.org/wiki?curid=37630506", "title": "List of smart TV platforms and middleware software", "text": "List of smart TV platforms and middleware software\n\nThe following list encompasses notable smart TV platforms and application software that are used as software framework and middleware platforms used by more than just one manufacturer.\n\nFor TV sets and companion boxes vendors, available under OEM license.\n\nIncludes first and third-party solutions.\n\n"}
{"id": "33023741", "url": "https://en.wikipedia.org/wiki?curid=33023741", "title": "List of water shortages", "text": "List of water shortages\n\nTo be included, the outage must conform to these criteria:\n\nIn other words:\n\n\n"}
{"id": "2886768", "url": "https://en.wikipedia.org/wiki?curid=2886768", "title": "Local eGovernment", "text": "Local eGovernment\n\nLocal eGovernment is eGovernment as it relates to local government, police, fire, national park and transport authorities.\n\nThe UK government’s objective was that all of public services in England should be capable of being delivered electronically by 31 December 2005. The \"Modernising Government\" White Paper of March 1999 proposed \"all dealings with government being deliverable electronically by 2008.\" Prime Minister Tony Blair then announced on 30 March 2000 that the target date had been brought forward from 2008 to 2005.\n\nElectronic delivery is defined as delivery through internet protocols and other ICT methods and includes delivery by telephone if the officer receiving the call can access electronic information and/or update records online there and then.\n\nThe average local authority achieved a reported 97% e-enablement by 31 December 2005, though it is suspected the actual figure is much lower. The target also applied to the Civil Service, which achieved a reported 96%.\n\nFrom December 2001 to April 2006 local authorities were required to publish an \"IEG Statement\" on their degree of e-enablement (electronic enablement). The measures include BVPI 157 and the Priority Outcomes.\n\nBVPI 157 measures the percentage of 'interactions' provided by a local authority that are e-enabled. The figure must be published annually as part of the Best Value performance management framework. The figure is calculated by considering the following ten types of interaction as they apply to the services provided by the local authority:\nLocal authorities may use a proprietary list of services, but are required to validate their list against the \"Local Government Services List\" (LGSL).\nBVPI 157 forms part of the annual IEG statement.\n\nTo guide local authorities in implementing eGovernment, the Department for Transport, Local Government and the Regions (and its successor the Office of the Deputy Prime Minister (ODPM)) issued a list of key targets, most commonly referred to as the Priority Outcomes or PSOs (Priority Service Outcomes). The list was issued in the snappily entitled document \"Defining E-government Outcomes for 2005 to Support the Delivery of Priority Services & National Strategy Transformation Agenda for Local Authorities in England – Version 1.0\".\nIn September 2004, detailed guidance on the Priority Outcomes was issued in the document \"Priority Outcomes - Explanatory Notes for Practitioners\", published by the IDeA (Improvement and Development Agency) and endorsed by the ODPM.\n\nPriority Outcomes are rated with three levels of obligation:\n\nThe following technologies are recommended in Priority Outcomes: Explanatory Notes for Practitioners as those likely to be essential in the achievement of the Priority Outcomes:\n\nLocal authority websites are also subject to assessments by Socitm (Society of Information Technology Management) Insight in their annual \"Better Connected\" reports.\n\nModernising Government White Paper, March 1999 on the Official-Documents.gov.uk website\n\nPriority Outcomes - Explanatory Notes for Practitioners on the IDeA website\n\nDefining E-government Outcomes for 2005 to Support the Delivery of Priority Services & National Strategy Transformation Agenda for Local Authorities in England – Version 1.0 (Microsoft Word document) on the ODPM website\n\nLocal Government Services List and related information on the Electronic Service Delivery website.\n\n\n"}
{"id": "17864", "url": "https://en.wikipedia.org/wiki?curid=17864", "title": "Luddite", "text": "Luddite\n\nThe Luddites were a secret oath-based organization of English textile workers in the 19th century, where a radical faction destroyed textile machinery as a form of protest. The group was protesting the use of machinery in a \"fraudulent and deceitful manner\" to get around standard labour practices. Luddites feared correctly that the time spent learning the skills of their craft would go to waste as machines would replace their role in the industry. It is a misconception that the Luddites protested against the machinery itself in an attempt to halt the progress of technology. Over time, however, the term has come to mean one opposed to industrialisation, automation, computerisation, or new technologies in general. The Luddite movement began in Nottingham and culminated in a region-wide rebellion that lasted from 1811 to 1816. Mill and factory owners took to shooting protesters and eventually the movement was suppressed with military force.\n\nAlthough the origin of the name Luddite () is uncertain, the movement was said to be named after Ned Ludd, an apprentice who allegedly smashed two stocking frames in 1779 and whose name had become emblematic of machine destroyers. Ned Ludd, however, was completely fictional and used as a way to shock and provoke the government. The name evolved into the imaginary General Ludd or King Ludd, who, like Robin Hood, was reputed to live in Sherwood Forest.\n\nThe lower classes of the 18th century, generally speaking, were not openly disloyal to the king or government. Overall, violent action was rare because punishments were harsh. The majority of individuals were primarily concerned with meeting their own daily needs. This movement towards aggression in the 19th century can be seen as part of the rise in English working-class discontent due to the Industrial Revolution. Working conditions in the mills were harsh but efficient enough to threaten the livelihoods of skilled artisans. The new inventions allowed for faster and cheaper labour because machines were operated by less-skilled, low-wage labourers. The Luddites were not afraid of technology and did not attempt to eliminate technology out of fear. Their goal was instead to gain a better bargaining position with their employers. Luddism was in fact a prototypical, insurrectionary labour movement involving a loose coalition of dissenters.\n\nResearch by Kevin Binfield and others asserts that since organized action by stockingers had occurred at various times since 1675, the movements of the early 19th century must be viewed in the context of the hardships suffered by the working class during the Napoleonic Wars, rather than as an absolute aversion to machinery. Irregular rises in food prices provoked the Keelmen working in the port of Tyne to riot in 1710 and tin miners to steal from granaries at Falmouth in 1727. There was a rebellion in Northumberland and Durham in 1740, and the assault of Quaker corn dealers in 1756. Skilled artisans in the cloth, building, shipbuilding, printing and cutlery trades organized friendly societies to peacefully insure themselves against unemployment, sickness, and in some cases against intrusion of \"foreign\" labour into their trades, as was common among guilds.\n\nMalcolm L. Thomis argued in his 1970 history, \"The Luddites\", that without the structure of a union, machine-breaking was one of very few mechanisms workers could use to increase pressure on employers, to undermine lower-paid competing workers and to create solidarity among workers, \"These attacks on machines did not imply any necessary hostility to machinery as such; machinery was just a conveniently exposed target against which an attack could be made.\"\n\nAn agricultural variant of Luddism, centering on the breaking of threshing machines, occurred during the widespread Swing Riots of 1830 in southern and eastern England.\n\nThe Luddite movement emerged during the harsh economic climate of the Napoleonic Wars, which saw a rise of difficult working conditions in the new textile factories. Luddites objected primarily to the rising popularity of automated textile equipment, threatening the jobs and livelihoods of skilled workers as this technology allowed them to be replaced by cheaper and less skilled workers. The movement began in Arnold, Nottingham on 11 March 1811 and spread rapidly throughout England over the following two years. Handloom weavers burned mills and pieces of factory machinery. Textile workers destroyed industrial equipment during the late 18th century, prompting acts such as the Protection of Stocking Frames, etc. Act 1788.\n\nThe Luddites met at night on the moors surrounding industrial towns to practice drills and maneuvers. Their main areas of operation began in Nottinghamshire in November 1811, followed by the West Riding of Yorkshire in early 1812 then Lancashire by March 1813. They smashed stocking frames and cropping frames among others. There does not seem to have been any political motivation behind the Luddite riots and there was no national organization. The men were merely attacking what they saw as the reason for the decline in their livelihoods. Luddites battled the British Army at Burton's Mill in Middleton and at Westhoughton Mill, both in Lancashire. The Luddites and their supporters anonymously sent death threats to, and possibly attacked, magistrates and food merchants. Activists smashed Heathcote's lacemaking machine in Loughborough in 1816. He and other industrialists had secret chambers constructed in their buildings that could be used as hiding places during an attack.\n\nIn 1817, an unemployed Nottingham stockinger and probably ex-Luddite, named Jeremiah Brandreth led the Pentrich Rising. While this was a general uprising unrelated to machinery, it can be viewed as the last major Luddite act.\n\nThe British Army clashed with the Luddites on several occasions. At one time there were more British soldiers fighting the Luddites than there were fighting Napoleon on the Iberian Peninsula. Three Luddites, led by George Mellor, ambushed and assassinated mill owner William Horsfall of Ottiwells Mill in Marsden, West Yorkshire at Crosland Moor in Huddersfield. Horsfall had remarked that he would \"Ride up to his saddle in Luddite blood.\" Mellor fired the fatal shot to Horsfall's groin, and all three men were arrested.\n\nLord Byron denounced what he considered to be the plight of the working class, the government’s inane policies and ruthless repression in the House of Lords on 27 February 1812: \"I have been in some of the most oppressed provinces of Turkey; but never, under the most despotic of infidel governments, did I behold such squalid wretchedness as I have seen since my return, in the very heart of a Christian country.\"\n\nThe British government sought to suppress the Luddite movement with a mass trial at York in January 1813, following the attack on Cartwrights mill at Rawfolds near Cleckheaton. The government charged over 60 men, including Mellor and his companions, with various crimes in connection with Luddite activities. While some of those charged were actual Luddites, many had no connection to the movement. Although the proceedings were legitimate jury trials, many were abandoned due to lack of evidence and 30 men were acquitted. These trials were certainly intended to act as show trials to deter other Luddites from continuing their activities. The harsh sentences of those found guilty, which included execution and penal transportation, quickly ended the movement.\n\nParliament made \"machine breaking\" (i.e. industrial sabotage) a capital crime with the Frame Breaking Act of 1812 and the Malicious Damage Act 1861. Lord Byron opposed this legislation, becoming one of the few prominent defenders of the Luddites after the treatment of the defendants at the York trials. Coincidently, Lord Byron's only legitimate daughter Ada Lovelace would become the first computer programmer by combining the technology of the Analytical Engine with the Jacquard loom.\n\nIn 1867 Karl Marx wrote that it would be some time before workers were able to distinguish between the machines and \"the form of society which utilizes these instruments\" and their ideas. \"The instrument of labour, when it takes the form of a machine, immediately becomes a competitor of the workman himself.\"\n\nIn the 19th century, occupations that arose from the growth of trade and shipping in ports, also in 'domestic' manufacturers, were notorious for precarious employment prospects. Underemployment was chronic during this period, and it was common practice to retain a larger workforce than was typically necessary for insurance against labour shortages in boom times.\n\nMoreover, the organization of manufacture by merchant-capitalists in the textile industry was inherently unstable. While the financiers' capital was still largely invested in raw material, it was easy to increase commitment where trade was good and almost as easy to cut back when times were bad. Merchant-capitalists lacked the incentive of later factory owners, whose capital was invested in building and plants, to maintain a steady rate of production and return on fixed capital. The combination of seasonal variations in wage rates and violent short-term fluctuations springing from harvests and war, periodic outbreaks of violence are more easily understood.\n\nIn 1956, a speech said that \"organized workers were by no means wedded to a Luddite Philosophy.\" More recently, the term Neo-Luddism has emerged to describe opposition to many forms of technology. According to a manifesto drawn up by the Second Luddite Congress (April 1996; Barnesville, Ohio), Neo-Luddism is \"a leaderless movement of passive resistance to consumerism and the increasingly bizarre and frightening technologies of the Computer Age.\"\n\nThe term “Luddite fallacy” is used by economists in reference to the fear that technological unemployment inevitably generates structural unemployment and is consequently macroeconomically injurious. If a technological innovation results in a reduction of necessary labour inputs in a given sector, then the industry-wide cost of production falls, which lowers the competitive price and increases the equilibrium supply point which, theoretically, will require an increase in aggregate labour inputs.\n\n\n\n"}
{"id": "19490705", "url": "https://en.wikipedia.org/wiki?curid=19490705", "title": "MIT Center for Collective Intelligence", "text": "MIT Center for Collective Intelligence\n\nThe MIT Center for Collective Intelligence (CCI) is a research center at the Massachusetts Institute of Technology, headed by Professor Thomas W. Malone, that focuses on the study of collective intelligence. \n\nThe Center for Collective Intelligence brings together faculty from across MIT to conduct research on how new communications technologies are changing the way people work together. It involves people from many diverse organizations across MIT including; the MIT Media Lab, the MIT Computer Science and Artificial Intelligence Laboratory, the Department of Brain and Cognitive Sciences, the MIT Sloan School of Management, and the Dalai Lama Center for 21st Century Ethics and Transformative Values. \n\nCCI was founded in 2006 by professor Thomas W. Malone. To a great extent, this is a continuation of the research Malone and his colleagues have conducted at the Center for Coordination Science, as well as within initiatives such as \"Inventing the Organizations of the 21st Century\".\n\nThe center's mission is to find novel answers to one basic research motif: \"How can people and computers be connected so that—collectively—they act more intelligently than any individuals, groups, or computers have ever done before?'\"\n\nIn order to answer this question, the researchers conduct three types of research: \n\nFinally, they also work on developing theories to explain the phenomena of collective-intelligence.\n\nThe center is sponsored by several corporates and non-profit organizations.\n\nProfessor Thomas W. Malone is the founder and director of CCI. In addition to Malone, the faculty steering committee includes: \nProfessor Randall Davis (Research Director, Computer Science and Artificial Intelligence Laboratory, Department of Electrical Engineering and Computer Science), Alex (Sandy) Pentland (Toshiba Professor of Media Arts and Science and Director, Human Dynamics Group at the Media Laboratory), and Josh Tenenbaum - Paul E. Newton Associate Professor and Head, Computational Cognitive Science Group (Department of Brain and Cognitive Sciences).\n\nThe advisory board includes Tim Berners-Lee, Jimmy Wales and Alpheus Bingham (former CEO of Innocentive, Inc.).\nThe center also has an active community of research scientists, students, scholars and other affiliates.\n"}
{"id": "23612695", "url": "https://en.wikipedia.org/wiki?curid=23612695", "title": "Mobile Access Protocol", "text": "Mobile Access Protocol\n\nMAP27\nMobile Access Protocol for MPT 1327 equipment\n\nThis standard specifies an interface between a mobile radio and a data terminal equipment. This\ninterface gives access to and defines network layer procedures for call set-up and data transfer as specified in MPT-8702509549 MPT1327 and MPT1343 or derivations thereof. A conformance test definition is outside the scope of this standard.\n"}
{"id": "21815281", "url": "https://en.wikipedia.org/wiki?curid=21815281", "title": "Mobile community", "text": "Mobile community\n\nA mobile community is a group of people generally united by shared interests or goals who interact: \n\n\n"}
{"id": "49478366", "url": "https://en.wikipedia.org/wiki?curid=49478366", "title": "Mobile phone use in schools", "text": "Mobile phone use in schools\n\nThe use of mobile phones in school settings or environments is a topic of debate. \nSupporting parents believe that cell phones address their safety concerns by enabling them to communicate with their children. Teachers and administrators opposed to mobile phone usage in schools believe that they cause disruption, and may be used for malicious purposes such as cheating on tests or taking inappropriate photographs. Students become addicted to playing games and messaging others on their mobile devices, causing them to pay less attention in class and miss important lessons.\n\nA study of a group of undergraduate students, published in the journal \"Computers in Human Behavior\" in 2015, found that among undergraduate students, total mobile-phone use (measured in number of minutes per day, not limited to school time) was \"a significant and negative predictor of college students' academic performance, objectively measured as cumulative GPA.\" The study controlled for various other factors. Also, the use of mobile technology largely explains the inadequate behaviors in the use of ICT in the personal and school environments among young people, which justifies the need to promote actions that contribute to a more responsible use of this type of technology in all areas of his personal, school and social life.\n\nA 2015 study by the London School of Economics, conducted in four cities of England, found that test scores increased by more than 6% in schools which banned cell phones.\n\nIn 2016, researchers Julia Irwin and Natasha Gupta of Macquarie University performed an experiment in 2016 testing the effect of Facebook distractions in the classroom. The study found that students interested in the subject material and the way it was presented were less likely to be distracted by Facebook, however, those same students with access to phones still performed lower than students that were not allowed access to cell phones during the lecture.\n\nA 2017 collective study published by \"Applied Cognitive Psychology\", found that college students retained less knowledge when allowed to use or have a cell phone on them during lectures. During the experiment, students that were not allowed access to a cell phone tested better than students that had access to cell phones.\n\nDespite the numerous drawbacks that come with the use of cellphones in classrooms, there are also exist certain benefits. A 2017 study conducted by Dr. James Derounian at the University of Gloucestershire involving 100 participants revealed that 45% of students believe that the use of phones in classrooms supports their education. One of the most common strengths listed was the use of phones for accessing digital textbook and thus engaging deeper with the material presented. However, Derounian mentioned that there could be \"an element of social desirability conveyed in the student views given\".\n\nAn article by Emma Henderson for the \"Independent\" in the UK goes over \"phantom vibrations\" caused by \"learned bodily behavior\". The part of the body the phone is close to becomes very sensitive and the slightest vibration can cause a person to believe the phone has vibrated, hence the name \"phantom vibration\". 9 out of 10 people have claimed to have felt their mobile device vibrate while in the pocket when it in fact did not. It can be beneficial for students not only out of respect for the professor to have their mobile phones out of their pockets but also to themselves to help break this cycle.\n\nIn the United Kingdom, no schools banned mobile phones in 2001. However, by 2007, 50% of the schools had banned these devices, and by 2012, this number had increased to 98%. According to \"CNN Money\", students' academic skills improved when policies were implemented to ban cell phone use in schools. Schools banning students from carrying phones helped students score higher on exams and reduced the students' temptations to use cell phones for non-scholarly purposes.\n\nIn the past, some U.S. schools have installed mobile phone jammers to prevent mobile phones from working on campuses. The sale and use of jammers is illegal in the U.S. under the federal Communications Act of 1934, because jammers cut off 9-1-1 calls and can disrupt air navigation near airports, and in 2012 the Federal Communication Commission (FCC) stepped up enforcement of the law. Mt. Spokane High School in Washington state at one point installed a jammer in school to prevent students from calling and text-messaging, but removed the device after determining that it was \"probably not legal\" under federal law. In 2015, one Florida science teacher received a five-day unpaid suspension for installing a jammer in his classroom.\n\nIn 2005, the New York City Department of Education imposed a citywide ban on mobile phones in public schools. According to the \"New York Times\", the ban was \"inconsistently enforced, with some schools allowing students to carry phones as long as staff members do not hear or see them, and other schools—particularly those with metal detectors at the doors—maintaining a strict ban. Outside those schools, small businesses have sprung up that allow students to park their phones inside a truck for a dollar a day.\" The ban was unpopular among parents because it impeded communication with children. In March 2015, the citywide ban was lifted, fulfilling a campaign promise made by Mayor Bill de Blasio. Under the new policy, school principals (in consultation with teachers and parents) may set rules on use and storage of mobile phones during instructional time and lunch breaks. The default rule would be that phones would be required to remain hidden, but principals could also choose to \"require students to store phones in backpacks or in a designated place; allow use of phones during lunch or in designated areas; or allow phones to be used for instructional purposes\". De Blasio said that the policy shift would allow to parents stay in better touch with their children, particularly in emergencies, especially in case of an emergency, and Schools Chancellor Carmen Fariña noted that the change means that students in schools with metal detector would no longer have to pay outside vendors to store phones for them during the school day.\n\nStudents tend to support the side that grants them the opportunity to bring mobile phones in school campus, arguing that phones allow them to reach their parents if any problem occurs. Parents also argue that there isn't a replacement for mobile phones, so phones are an essential device to carry around. If their child is in danger or doesn't feel safe, he or she has to be able to reach out to them. They also believe that having a phone shows responsibility. \n\nTheft of mobile phones is a concern in some schools. For example, in the Wichita School District (USD 259) public schools, 80 cases of theft of cell phones were reported in 2014. In 2012, following an undercover investigation, thirteen students in Bucks County, Pennsylvania, all juveniles, were arrested and charged with running a cell phone theft ring that resulted in the theft of several thousand dollars' worth of mobile phones, tablets, and other electronics.\n\nAn increasing number of schools are now allowing use of cell phones as learning tools. However, the use of cell phones in schools is challenging. Some schools reported allowing all students to use cell phones at the same time slows down school bandwidth speeds.\n\nAccording to the Governors Highway Safety Association, while no state bans all mobile-phone use for all drivers, 20 states and the District of Columbia prohibit school bus drivers from using mobile phones. School bus drivers have been fired or suspended for using the phone or text-messaging while driving.\n\nMobile phones can be very beneficial in schools. There are many apps used that help students excel.\n\nQuizlet is an online study tool that allows you to make digital notecards and allows you to see other peoples notecard sets. The Quizlet app is very easy and accessible to use. Quizlet has multiple ways to test yourself using different variations of games. Quizlet has over 30 million active users.\n\nTophat is used in many different classes to record attendance and take quizzes. It allows the professors to know whether you were in class or not and whether you were participating or not.\n\nTurningPoint is similar to the previous mentioned app, Tophat. It is used as an interactive polling inside the classroom. Professors can also track attendance and provide quizzes for the students with the use of this app.\n\nRider is an app currently used at multiple colleges to locate the buses throughout the bus system. It allows students to track the buses and give them an estimated time of arrival.\n\nThese apps have helped students around the world on a day to day basis. These apps encourage the use of cell phones and/or computers during school classes. Students and professors are moving more and more to only using electronics. \n\nWith the rise of use of mobile cell phones in school, applications for these cellphones have been created to support this. As of February 2018, 80,000 applications were available for teacher use.  A variety of messaging apps provide communication for student to student relationships as well as teacher to student. Some popular apps for both student, teacher, and parent use are ClassDojo, and Remind. This accounts for the 72% of iTunes’ top selling education apps are for preschoolers and elementary school students. Apps like Remind and ClassDojo offer many different abilities such as language translation, scheduled reminders, and parent messages.\n\nClassDojo is one of the Apps that is used widely throughout schools. According to crunchbase.com, it is a “means to encourage learning, skill development and character building among students”. The app offers a platform for teachers to share pictures, videos, and reports with parents and administrators. The Remind App is another way for teachers to communicate with parents and administration. The App allows teachers to send out scheduled text messages to parents, and also provides a class blog for the teacher to update with upcoming due dates, tests/quizzes, and other class information. WhatsApp is different from the other apps because it provides communication for students to other students. The app offers group chats, video messaging, video class, and photo messaging. Another app that allows students to communicate is GroupMe. GroupMe allows students to communicate in a group chat, while also only using WiFi instead of cellular data. Some college-aged students use this app for sharing course information.\n\nTechnology in schools is becoming a common practice throughout many grades and age levels. The creation of messaging applications helps support this boom of usage in schools. This new technology comes with both pros and cons. A pro of messaging apps is their easy usage and accessibility to student, teachers, and parents. \nAccessibility of the messaging apps makes it easier for parents with disabilities, demanding full time jobs, and language barriers to communicate more efficiently. A con is that not all students and parents have this technology available to them. This can cause a gap in students who have cellphones and computers and those who do not. Increased access and transparency can make students shift their responsibilities to their parents, minimizing student ownership. Another con is the ease in sharing of information can lead to academic dishonesty. This is a policy colleges are cracking down on.\n\nAs the kinks of messaging applications become smoothed out over time, the future of them lies ahead. By minimizing both student and parent entitlement, setting communication boundaries, stating expectations early, and reinforcing student responsibility some of the cons from messaging apps can be eliminated. \nThere is hope that they become more advanced, specifically when using them for help via a bot. Advancement in this field will allow for higher frequency of use, more emotional connection, and higher convenience for users. \n\n\n"}
{"id": "227018", "url": "https://en.wikipedia.org/wiki?curid=227018", "title": "Open standard", "text": "Open standard\n\nAn open standard is a standard that is publicly available and has various rights to use associated with it, and may also have various properties of how it was designed (e.g. open process). There is no single definition and interpretations vary with usage.\n\nThe terms \"open\" and \"standard\" have a wide range of meanings associated with their usage. There are a number of definitions of open standards which emphasize different aspects of openness, including the openness of the resulting specification, the openness of the drafting process, and the ownership of rights in the standard. The term \"standard\" is sometimes restricted to technologies approved by formalized committees that are open to participation by all interested parties and operate on a consensus basis.\n\nThe definitions of the term \"open standard\" used by academics, the European Union and some of its member governments or parliaments such as Denmark, France, and Spain preclude open standards requiring fees for use, as do the New Zealand, South African and the Venezuelan governments. On the standard organisation side, the World Wide Web Consortium (W3C) ensures that its specifications can be implemented on a royalty-free basis.\n\nMany definitions of the term \"standard\" permit patent holders to impose \"reasonable and non-discriminatory licensing\" royalty fees and other licensing terms on implementers or users of the standard. For example, the rules for standards published by the major internationally recognized standards bodies such as the Internet Engineering Task Force (IETF), International Organization for Standardization (ISO), International Electrotechnical Commission (IEC), and ITU-T permit their standards to contain specifications whose implementation will require payment of patent licensing fees. Among these organizations, only the IETF and ITU-T explicitly refer to their standards as \"open standards\", while the others refer only to producing \"standards\". The IETF and ITU-T use definitions of \"open standard\" that allow \"reasonable and non-discriminatory\" patent licensing fee requirements.\n\nThere are those in the open-source software community who hold that an \"open standard\" is only open if it can be freely adopted, implemented and extended. While open standards or architectures are considered non-proprietary in the sense that the standard is either unowned or owned by a collective body, it can still be publicly shared and not tightly guarded. The typical example of “open source” that has become a standard is the personal computer originated by IBM and now referred to as Wintel, the combination of the Microsoft operating system and Intel microprocessor. There are three others that are most widely accepted as “open” which include the GSM phones (adopted as a government standard), Open Group which promotes UNIX and the like, and the Internet Engineering Task Force (IETF) which created the first standards of SMTP and TCP/IP. Buyers tend to prefer open standards which they believe offer them cheaper products and more choice for access due to network effects and increased competition between vendors.\n\nOpen standards which specify formats are sometimes referred to as open formats.\n\nMany specifications that are sometimes referred to as standards are proprietary and only available under restrictive contract terms (if they can be obtained at all) from the organization that owns the copyright on the specification. As such these specifications are not considered to be fully \"open\". Joel West has argued that \"open\" standards are not black and white but have many different levels of \"openness\". Ultimately a standard needs to be open enough that it will become adopted and accepted in the market, but still closed enough that firms can get a return on their investment in developing the technology around the standard. A more open standard tends to occur when the knowledge of the technology becomes dispersed enough that competition is increased and others are able to start copying the technology as they implement it. This occurred with the Wintel architecture as others were able to start imitating the software. Less open standards exist when a particular firm has much power (not ownership) over the standard, which can occur when a firm’s platform “wins” in standard setting or the market makes one platform most popular.\n\nOn August 12, 2012, the Institute for Electrical and Electronics Engineers (IEEE), Internet Society (ISOC), World Wide Web Consortium (W3C), Internet Engineering Task Force (IETF) and Internet Architecture Board (IAB), jointly affirmed a set of principles which have contributed to the exponential growth of the Internet and related technologies. The “OpenStand Principles” define open standards and establish the building blocks for innovation. Standards developed using the OpenStand principles are developed through an open, participatory process, support interoperability, foster global competition, are voluntarily adopted on a global level and serve as building blocks for products and services targeted to meet the needs of markets and consumers. This drives innovation which, in turn, contributes to the creation of new markets and the growth and expansion of existing markets.\n\nThere are five, key OpenStand Principles, as outlined below:\n\n1. Cooperation\nRespectful cooperation between standards organizations, whereby each respects the autonomy, integrity, processes, and intellectual property rules of the others.\n\n2. Adherence to Principles - Adherence to the five fundamental principles of standards development, namely\n\n\n3. Collective Empowerment\nCommitment by affirming standards organizations and their participants to collective empowerment by striving for standards that:\n\n\n4. Availability\nStandards specifications are made accessible to all for implementation and deployment. Affirming standards organizations have defined procedures to develop specifications that can be implemented under fair terms. Given market diversity, fair terms may vary from royalty-free to fair, reasonable, and non-discriminatory terms (FRAND).\n\n5. Voluntary Adoption\nStandards are voluntarily adopted and success is determined by the market.\n\nThe ITU-T is a standards development organization (SDO) that is one of the three sectors of the International Telecommunications Union (a specialized agency of the United Nations). The ITU-T has a Telecommunication Standardization Bureau director's \"Ad Hoc\" group on IPR that produced the following definition in March 2005, which the ITU-T as a whole has endorsed for its purposes since November 2005:\n\nThe ITU-T, ITU-R, ISO, and IEC have harmonized on a common patent policy under the banner of the WSC. However, the ITU-T definition should not necessarily be considered also applicable in ITU-R, ISO and IEC contexts, since the Common Patent Policy \ndoes not make any reference to \"open standards\" but rather only to \"standards.\"\n\nIn section 7 of its RFC 2026, the IETF classifies specifications that have been developed in a manner similar to that of the IETF itself as being \"open standards,\" and lists the standards produced by ANSI, ISO, IEEE, and ITU-T as examples. As the IETF standardization processes and IPR policies have the characteristics listed above by ITU-T, the IETF standards fulfill the ITU-T definition of \"open standards.\"\n\nHowever, the IETF has not adopted a specific definition of \"open standard\"; both RFC 2026 and the IETF's mission statement (RFC 3935) talks about \"open process,\" but RFC 2026 does not define \"open standard\" except for the purpose of defining what documents IETF standards can link to.\n\nRFC 2026 belongs to a set of RFCs collectively known as BCP 9 (Best Common Practice, an IETF policy).\nRFC 2026 was later updated by BCP 78 and 79 (among others). As of 2011 BCP 78 is RFC 5378 (Rights Contributors Provide to the IETF Trust), and BCP 79 consists of RFC 3979 (Intellectual Property Rights in IETF Technology) and a clarification in RFC 4879. The changes are intended to be compatible with the \"Simplified BSD License\" as stated in the IETF Trust Legal Provisions and Copyright FAQ based on RFC 5377.\n\nIn August 2012, the IETF combined with the W3C and IEEE to launch OpenStand and to publish The Modern Paradigm for Standards. This captures \"the effective and efficient standardization processes that have made the Internet and Web the premiere platforms for innovation and borderless commerce\". The declaration is then published in the form of RFC 6852 in January 2013.\n\nThe European Union defined the term for use within its European Interoperability Framework for Pan-European eGovernment Services, Version 1.0 although it does not claim to be a universal definition for all European Union use and documentation.\nTo reach interoperability in the context of pan-European eGovernment services, guidance needs to focus on open standards.\nThe word \"open\" is here meant in the sense of fulfilling the following requirements:\n\nThe Network Centric Operations Industry Consortium (NCOIC) defines open standard as the following:\nSpecifications for hardware and/or software that are publicly available implying that multiple vendors can compete directly based on the features and performance of their products. It also implies that the existing open system can be removed and replaced with that of another vendor with minimal effort and without major interruption.\nThe Danish government has attempted to make a definition of open standards, \nwhich also is used in pan-European software development projects. It states:\nThe French Parliament approved a definition of \"open standard\" in its \"Law for Confidence in the Digital Economy.\" The definition is:\n\nA clear Royalty Free stance and far reaching requirements case is the one for India's Government \n4.1 Mandatory Characteristics\nAn Identified Standard will qualify as an “Open Standard”, if it meets the following criteria:\n\nItaly has a general rule for the entire public sector dealing with Open Standards, although concentrating on data formats, in Art. 68 of the Code of the Digital Administration (\"Codice dell'Amministrazione Digitale\")\n\n[applications must] allow representation of data under different formats, at least one being an open data format.\n\n[it is defined] an open data format, a data format which is made public, is thoroughly documented and neutral with regard to the technological tools needed to peruse the same data.\nA Law passed by the Spanish Parliament requires that all electronic services provided by the Spanish public administration must be based on open standards. It defines an open standard as royalty free, according to the following definition:\nAn open standard fulfills the following conditions:\nThe Venezuelan Government approved a \"free software and open standards law.\" \nThe decree includes the requirement that the Venezuelan public sector must use free software based on open standards, and includes a definition of open standard:\nArticle 2: for the purposes of this Decree, it shall be understood as \nk) Open standards: technical specifications, published and controlled by an organization in charge of their development, that have been accepted by the industry, available to everybody for their implementation in free software or other [type of software], promoting competitivity, interoperability and flexibility.\nThe South African Government approved a definition in the \"Minimum Interoperability Operating Standards Handbook\" (MIOS).\n\nFor the purposes of the MIOS, a standard shall be considered open if it meets all of these criteria. There are standards which we are obliged to adopt for pragmatic reasons which do not necessarily fully conform to being open in all respects. In such cases, where an open standard does not yet exist, the degree of openness will be taken into account when selecting an appropriate standard:\n\n\nThe E-Government Interoperability Framework (e-GIF) defines open standard as royalty free according to the following text:\nWhile a universally agreed definition of \"open standards\" is unlikely to be\nresolved in the near future, the e-GIF accepts that a definition of “open standards”\nneeds to recognise a continuum that ranges from closed to open, and encompasses\nvarying degrees of \"openness.\" To guide readers in this respect, the e-GIF\nendorses \"open standards\" that exhibit the following properties:\n\n\nThe e-GIF performs the same function in e-government as the Road Code does on\nthe highways. Driving would be excessively costly, inefficient, and ineffective if\nroad rules had to be agreed each time one vehicle encountered another.\nOne of the most popular definitions of the term \"open standard,\" as measured by Google ranking, is the one developed by Bruce Perens.\nHis definition lists a set of principles that he believes must be met by an open standard:\n\n\nVijay Kapoor, national technology officer, Microsoft, defines what open standards are as follows:\nLet's look at what an open standard means: 'open' refers to it being royalty-free, while 'standard' means a technology approved by formalized committees that are open to participation by all interested parties and operate on a consensus basis. An open standard is publicly available, and developed, approved and maintained via a collaborative and consensus driven process.\nOverall, Microsoft's relationship to open standards was, at best, mixed. While Microsoft participated in the most significant standard-setting organizations that establish open standards, it was often seen as oppositional to their adoption.\n\nThe Open Source Initiative defines the requirements and criteria for open standards as follows:\n\nThe Requirement\n\nAn \"open standard\" must not prohibit conforming implementations in open source software.\n\nThe Criteria\n\nTo comply with the Open Standards Requirement, an \"open standard\" must satisfy the following criteria. If an \"open standard\" does not meet these criteria, it will be discriminating against open source developers.\n\n\nKen Krechmer identifies ten \"rights\":\n\n\nAs an important provider of Web technology ICT Standards, notably XML, http, HTML, CSS and WAI, the World Wide Web Consortium (W3C) follows a process that promotes the development of high-quality standards.\n\nLooking at the end result, the spec alone, up for adoption, is not enough. The participative/inclusive process leading to a particular design, and the supporting resources available with it should be accounted when we talk about Open Standards:\nIn August 2012, the W3C combined with the IETF and IEEE to launch OpenStand and to publish The Modern Paradigm for Standards. This captures \"the effective and efficient standardization processes that have made the Internet and Web the premiere platforms for innovation and borderless commerce\".\n\nThe Digital Standards Organization (DIGISTAN) states that \"an open standard must be aimed at creating unrestricted competition between vendors and unrestricted choice for users.\" Its brief definition of \"open standard\" (or \"free and open standard\") is \"a published specification that is immune to vendor capture at all stages in its life-cycle.\" Its more complete definition as follows:\n\nA key defining property is that an open standard is immune to vendor capture at all stages in its life-cycle. Immunity from vendor capture makes it possible to improve upon, trust, and extend an open standard over time.\"\nThis definition is based on the EU's EIF v1 definition of \"open standard,\" but with changes to address what it terms as \"vendor capture.\" They believe that \"Many groups and individuals have provided definitions for 'open standard' that reflect their economic interests in the standards process. We see that the fundamental conflict is between vendors who seek to capture markets and raise costs, and the market at large, which seeks freedom and lower costs... Vendors work hard to turn open standards into franchise standards. They work to change the statutory language so they can cloak franchise standards in the sheep's clothing of 'open standard.' A robust definition of \"free and open standard\" must thus take into account the direct economic conflict between vendors and the market at large.\"\n\nThe Free Software Foundation Europe (FSFE) uses a definition which is based on the European Interoperability Framework v.1, and was extended after consultation with industry and community stakeholders. FSFE's standard has been adopted by groups such as the SELF EU Project, the 2008 Geneva Declaration on Standards and the Future of the Internet, and international Document Freedom Day teams.\n\nAccording to this definition an Open Standard is a format or protocol that is:\n\n\nThe Foundation for a Free Information Infrastructure's definition is said to coincide with the definition issued in the European Interoperability Framework released in 2004.\nA specification that is public, the standard is inclusive and it has been developed and is maintained in an open standardization process, everybody can implement it without any restriction, neither payment, to license the IPR (granted to everybody for free and without any condition). This is the minimum license terms asked by standardization bodies as W3C. Of course, all the other bodies accept open standards. But specification itself could cost a fair amount of money (ie. 100-400Eur per copy as in ISO because copyright and publication of the document itself). \n\nThe UK government's definition of open standards applies to software interoperability, data and document formats. The criteria for open standards are published in the “Open Standards Principles” policy paper and are as follows.\n\nNote that because the various definitions of \"open standard\" differ in their requirements, the standards listed below may not be open by every definition.\n\n\nDiSEqC is an open standard, no license is required or royalty is to be paid to the rightholder EUTELSAT.\nDiSEqC is a trademark of EUTELSAT.\nConditions for use of the trademark and the DiSEqC can be obtained from EUTELSAT.\n\n\n\n\n\n\n\nIn 2002 and 2003 the controversy about using reasonable and non-discriminatory (RAND) licensing for the use of patented technology in web standards increased. Bruce Perens, important associations as FSF or FFII and others have argued that the use of patents restricts who can implement a standard to those able or willing to pay for the use of the patented technology. The requirement to pay some small amount per user, is often an insurmountable problem for free/open source software implementations which can be redistributed by anyone. Royalty free (RF) licensing is generally the only possible license for free/open source software implementations. Version 3 of the GNU General Public License includes a section that enjoins anyone who distributes a program released under the GPL from enforcing patents on subsequent users of the software or derivative works.\n\nOne result of this controversy was that many governments (including the Danish, French and Spanish governments singly and the EU collectively) specifically affirmed that \"open standards\" required royalty-free licenses. Some standards organizations, such as the W3C, modified their processes to essentially only permit royalty-free licensing.\n\nPatents for software, formulas and algorithms are currently enforceable in the US but not in the EU. The European Patent Convention Article 52 paragraph (2)(c) expressly prohibits algorithms, business methods and software from being covered by patents. The US has only allowed them since 1989 and there has been growing controversy in recent years as to either the benefit or feasibility.\n\nA standards body and its associated processes cannot \"force\" a patent holder to give up its right to charge license fees, especially if the company concerned is not a member of the standards body and unconstrained by any rules that were set during the standards development process. In fact, this element discourages some standards bodies from adopting an \"open\" approach, fearing that they will lose out if their members are more constrained than non-members. Few bodies will carry out (or require their members to carry out) a full patent search. Ultimately, the only sanctions a standards body can apply on a non-member when patent licensing is demanded is to cancel the standard, try to rework around it, or work to invalidate the patent. Standards bodies such as W3C and OASIS require that the use of required patents be granted under a royalty-free license as a condition for joining the body or a particular working group, and this is generally considered enforceable.\n\nExamples of patent claims brought against standards previously thought to be open include JPEG and the Rambus case over DDR SDRAM. The H.264 video codec is an example of a standards organization producing a standard that has known, non-royalty-free required patents.\n\nOften the scope of the standard itself determines how likely it is that a firm will be able to use a standard as patent-like protection. Richard Langlois argues that standards with a wide scope may offer a firm some level of protection from competitors but it is likely that Schumpeterian creative destruction will ultimately leave the firm open to being \"invented around\" regardless of the standard a firm may benefit from.\n\n\n\n\n"}
{"id": "43923886", "url": "https://en.wikipedia.org/wiki?curid=43923886", "title": "PACT (interaction design)", "text": "PACT (interaction design)\n\nIn interaction design, PACT (an acronym for People, Activities, Contexts, Technologies) is a structure used to analyse with whom, what and where a user interact with a user interface \n"}
{"id": "57169337", "url": "https://en.wikipedia.org/wiki?curid=57169337", "title": "Phandeeyar", "text": "Phandeeyar\n\nPhandeeyar (standing for \"creation place\") is a technology seed accelerator based in Yangon, Myanmar. Formally beginning in 2015, Phandeeyar provides funding and training for emerging startups. Phandeeyar also conducts trainings, hosts workshops, and holds competitions for startups within the digital sector.\n\nIn 2014, Phandeeyar emerged from a hackathon named \"Code for Change Myanmar\" where its founder and current CEO, David Madden, wanted to promote the growth of Myanmar's digital sector. Through this hackathon, politics and elections were addressed. \n\nMadden states the need for better financial support for emerging start-ups in Myanmar which also led to the creation of the accelerator.\n\nPhandeeyar's sponsors and investors include the Omidyar Network which has donated 2 million US dollars to the company, the United States Institute of Peace, the Open Society Foundation, and the Schmidt Family Foundation.\n\nDavid Madden states that Phandeeyar's objective is to connect those in the tech field with others actively working in the social sector through different events. \n\nIn 2015, Phandeeyar hosted a two week event event called \"MaePaySoe\" (translating to \"let's vote\") where more than one hundred web developers gathered to create technology which informs the general public about potential political candidates in Myanmar and the country's voting process.\nIn 2016, around fifteen to twenty startups were projected to be accepted into the accelerator in a three year span. Phandeeyar also plans to invest around $2 million into startups in the near future. From the startups accepted, Phandeeyar would claim a 12 percent ownership of the companies. \nThose selected for Phandeeyar's 6 month training program would receive a funding of $25,000 to launch their ventures in addition to business partnerships to help with the early stages of their startups. These partnerships include leaders from regional companies such as the Myanmar Information Technology, CarsDB, Muru-D. and Golden Gate Ventures. Towards the end of the program, companies have the opportunity to pitch their startup ideas to venture capitalists and angel investors. \n\nChate Sat is a digital platform to connect freelance workers with employers to find jobs. After its launch from Phandeeyar, Chate Sat recruited around 800 businesses looking to contract work and 5,000 freelancers. Chate Sat also received funding from some investors such as Vulpes Investment Management Ltd. \n\nGoP is an online, tourism-based website that compiles different traveling information into one platform. Nyunt Win Aung, one of the company's co-founders, states that its increased website traffic will increase their bookings, directly as a result of more funding. \n\nWhite Merak is a phone application used to read comics made by local artists and the company's team. Additionally, this platform shows animated comics, as well as a bilingual setting where users can switch comic text from Burmese to English and vice versa. \n\nEZ Stay is an online hotel booking platform co-founded by Aung Phyo Lwi focused on informing those traveling to Myanmar about different hotel and motel availabilities. According to Aung Phyo Lwi, this website can better market local Myanmar businesses.\n\nIn April 2018, Myanmar civil rights groups and Myanmar-based technology companies accused Facebook of failing to effectively detect hate speech online in Myanmar, which many have claimed to have contributed to an anti-Rohingya sentiment in the country. Phandeeyar, along with other companies, released an open letter to the social media platform, posing questions about Facebook's transparency in accordance with these issues and the possible implications of social media with current events in Myanmar.\n"}
{"id": "46649163", "url": "https://en.wikipedia.org/wiki?curid=46649163", "title": "Privileged positions of business and science", "text": "Privileged positions of business and science\n\nThe privileged positions of business and science refer to the unique authority that persons in these areas hold in economic, political, and technosocial affairs. Businesses have strong decision-making abilities in the function of society, essentially choosing what technological innovations to develop. Scientists and technologists have valuable knowledge and the ability to pursue the technological innovations they want. They proceed largely without public scrutiny and as if they had the consent of those potentially affected by their discoveries and creations.\n\nBusinesses have considerable power in decision-making processes in business-oriented societies. High level executives make discrete decisions about technological innovation without regard to consequences. Political scientist Edward Woodhouse states that businesses make key economic decisions that include creating jobs, choosing industrial plant and equipment, and deciding which new products to develop and market. Additionally, businesses hold a privileged position related to politics in the sense that they are capable of considerable influence over key public choices. The leadership role that business has in the economy gives executives of large corporations an unusual kind of degree of influence over governmental policy making.\n\nGovernment officials have incentives to carry out businesses' demands. They know that failure of businesses to maintain high employment will upset voters more quickly than anything else. Our economy functions as a chain effect. Businesses that get what they want provide plentiful jobs and stimulate cash flow in the economy. As a result, citizens are happy, and this happiness translates to more trust and faith in the government party. Government officials thus have a higher chance of reelection.\n\nIt is clear that the businesses have the most influence in this chain effect. If a business fails, jobs are lost, citizens are unhappy, and the government party loses trust and their chance of reelection. One example of this was President Bush’s plummeting popularity and resulting defeat by Bill Clinton. Therefore, it would make sense for government officials to reach out to these executives and provide valuable resources to benefit both parties. Power is shifted from government officials to business executives, because these executives have the ability to influence government officials to meet their demands. As a result, a large category of decisions is turned over to businessmen, and taken off the agenda of government. In a way, business managers become the public officials they represent.\n\nThis privilege of power is unique to corporations only. If workers want reform on unfair working conditions or pay, they must form a worker’s union. However, a worker’s union has considerably less influence than a corporation. As opposed to workers’ unions, businesses exercise more control on governmental policy making because workers are expendable – workers need money more urgently than society needs their services, so a business can operate for a lot longer than workers can afford to stay off the job.\n\nWithout the knowledge of scientists and technologists, there would be no possibility of producing new technology and progress in a positive manner. Those who have these types of knowledge and abilities are essentially the \"seed\" of technological innovation. They have the following three privileges:\n\n\nBased on previous scientific knowledge, one or several are able to use it and/or build on it to innovate. Kleinman and Vallas, point out that scientists control the modification, mediation and contest of emerging knowledge. Those with this knowledge can also teach future generations the same to keep this process moving forward. Woodhouse that states that engineers are among the top professionals that promote the new innovations without consequences. This is the foundation of the privileged position of science, that only a select have scientific and/or technological knowledge and capable of substantial feats, good or bad. Engineers and scientists have the powerful position of being able to choose the path of innovation for emerging technology. Technologists and scientists hold a privileged position additionally in that their innovations often have political implications. Woodhouse makes the comparison of innovation to legislative acts, stating that they both establish a framework that will follow for many generations.\n\nBusiness and science are part of an important societal circle specifically relating to the production and implementation of technology. In simple terms, scientists and technologists design and create, while businesses provide the means to create, produce and implement. As Woodhouse claims, it would be a mistake to overlook business executives' and elected officials' dependence on the technologists because innovation is so highly regarded.\n\nBusinesses that rely on scientists and technologists cannot function without them, but have authority over them. Woodhouse claims that the one reason green technology has not emerged faster and stronger is because the brown chemistry formula fulfilled many of the needs of scientists and businessmen. In other words, since brown chemistry was innovated over less toxic and recyclable green technologies, it is shown that business has more authority over that of technologists and scientists. Woodhouse points out that there has been considerable delay on a promising area of green chemistry: supercritical fluids. The opinions range from targeting the culprit as the complexity of the equipment, to maintenance difficulties, to the equipment costs. This is an example of the interconnection of business and science, and how technology can be slowed by technology and/or business (and vice versa). Woodhouse also states that twentieth-century chemists, chemical engineers, and chemical industry executives made central decisions which considerably modeled society's experiences with chemicals. The chemical industry, which includes scientists, technologists, and businessmen, uniquely controls what happens or what can happen to large number of people who are exposed to chemicals.\n\nBusiness and science are considered of utmost importance in current developed countries. However, one working without the other has the potential to cause major issues. Woodhouse points out what the vice president of Shaw Carpets explained at a congressional hearing: that the V.P. is the person whose job it is to tell the chemists and chemical engineers that it is possible to produce carpets using \"green\" processes, because the students had not learned this from their respective university curricula. There can be many positive outcomes of the unison operation of business and science, and it is vital that this does occur in order to prevent major problems and unintended consequences.\n\n\n"}
{"id": "15676540", "url": "https://en.wikipedia.org/wiki?curid=15676540", "title": "Product design specification", "text": "Product design specification\n\nA product design specification (PDS) is a statement of how a design is made (specify the design), what it is intended to do, and how far it complies with the requirements. Requirements may be gathered in the Product Requirement Specification (PRS). Its aim is to ensure that the subsequent design and development of a product meets the needs (or requirements) of the user. Product design specification is one of the elements of product lifecycle management.\n\n"}
{"id": "11241476", "url": "https://en.wikipedia.org/wiki?curid=11241476", "title": "Project on Emerging Nanotechnologies", "text": "Project on Emerging Nanotechnologies\n\nThe Project on Emerging Nanotechnologies was established in 2005 as a partnership between the Woodrow Wilson International Center for Scholars and the Pew Charitable Trusts. The Project was intended to address the social, political, and public safety aspects of nanotechnology. It intended in particular to look for research and policy gaps and opportunities in knowledge and regulatory processes, and to develop strategies for closing them. The project worked with multiple U.S. and foreign governments and organizations.\n\nThe project's stated goal was \"to inform the debate and to create an active public and policy dialogue. It was not an advocate either for, or against, particular nanotechnologies. Rather, the Project sought to ensure that as these technologies are developed, potential human health and environmental risks were anticipated, properly understood, and effectively managed.\"\n\nThey have produced many publications on the various aspects of nanotechnology policy. One of the notable reports is on \"Managing the Effects of Nanotechnology\", written by J. Clarence (Terry) Davies in 2006. They also maintain several online databases including the widely cited consumer products inventory, the \"Nanotechnology Health and Environmental Implications: An inventory of current research\" as well as a series of PEN Reports. Their work has also been published in academic journals such as Nature Nanotechnology.\n\nA major activity of the Project was testimony on public forums.\n\nThe Advisory Board included Linda J. Fisher, Vice President and Chief sustainability officer at DuPont, Margaret A. Hamburg M.D., Vice President for Biological Programs, Nuclear Threat Initiative, Donald Kennedy, editor-in-chief of Science magazine and president emeritus and Bing Professor of Environmental Science, Emeritus, at Stanford University, John Ryan is Director of the Bionanotechnology IRC at Oxford University, and Stan Williams, Senior Fellow and Director of Quantum Science Research at Hewlett-Packard.\n\n"}
{"id": "7612776", "url": "https://en.wikipedia.org/wiki?curid=7612776", "title": "SDEP", "text": "SDEP\n\nThe SDEP (Street events Data Exchange Protocol) comprises an XML data schema and web service WSDL for exchanging information about streetworks, roadworks, and street events between systems.\n\nElgin was funded by the UK NeSDS Government e-Standards Programme to conduct a consultation and convene meetings to define the requirements of a common data exchange protocol for streetworks registers and other systems handling street events data. SDEP was developed to allow the open exchange of such data between back office systems used by local authorities to manage their highway networks in order to enable e-Government and streetworks co-ordination.\n\nThe SDEP consultation group comprised ELGIN (Chair), Mayrise Ltd., Symology Ltd., Pitney Bowes Inc., Exor Corporation (Bentley Systems), Office of the Deputy Prime Minister and Transport for London, with the National Traffic Control Centre in an observing capacity.\n\n"}
{"id": "44402859", "url": "https://en.wikipedia.org/wiki?curid=44402859", "title": "SIP extensions for the IP Multimedia Subsystem", "text": "SIP extensions for the IP Multimedia Subsystem\n\nThe Session Initiation Protocol (SIP) is the signaling protocol selected by the 3rd Generation Partnership Project (3GPP) to create and control multimedia sessions with two or more participants in the IP Multimedia Subsystem (IMS), and therefore is a key element in the IMS framework.\n\nSIP was developed by the Internet Engineering Task Force (IETF) as a standard for controlling multimedia communication sessions in Internet Protocol (IP) networks, working in the application layer of the Internet Protocol Suite. Several SIP extensions have been added to the basic protocol specification in order to extend its functionality. These extensions are based on Request for Comments (RFC) protocol recommendations by the IETF.\n\nThe 3GPP, which is a collaboration between groups of telecommunications associations aimed at developing and maintaining the IMS, stated a series of requirements for SIP to be successfully used in the IMS. Some of them could be addressed by using existing capabilities and extensions in SIP while, in other cases, the 3GPP had to collaborate with the IETF to standardize new SIP extensions to meet the new requirements. In any case, the IETF evolves SIP in a generic basis, so that the use of its extensions is not restricted to the IMS framework.\n\nThe 3GPP has stated several general requirements stated for operation of the IMS. These include an efficient use of the radio interface by minimizing the exchange of signaling messages between the mobile terminal and the network, a minimum session setup time by performing tasks prior to session establishment instead of during session establishment, a minimum support required in the terminal, the support for roaming and non-roaming scenarios with terminal mobility management (supported by the access network, not SIP), and support for IPv6 addressing.\n\nOther requirements involve protocol extensions, such as SIP header fields to exchange user or server information, and SIP methods to support new network functionality: requirement for registration, re-registration, de-registration, event notifications, instant messaging or call control primitives with additional capabilities such as call transference.\n\nOther specific requirements are:\n\nFinally, it is also necessary that other protocols and network services such as DHCP or DNS are adapted to work with SIP, for instance for outbound proxy (P-CSCF) location and SIP Uniform Resource Identifier (URI) to IP address resolution, respectively.\n\nThere is a mechanism in SIP for extension negotiation between user agents (UA) or servers, consisting of three header fields: \"supported\", \"require\" and \"unsupported\", which UAs or servers (i.e. user terminals or call session control function (CSCF) in IMS) may use to specify the extensions they understand. When a client initiates a SIP dialog with a server, it states the extensions it \"requires\" to be used and also other extensions that are understood (\"supported\"), and the server will then send a response with a list of extensions that it \"requires\". If these extensions are not listed in the client's message, the response from the server will be an error response. Likewise, if the server does not support any of the client's required extensions, it will send an error response with a list of its \"unsupported\" extensions. This kind of extensions are called \"option tags\", but SIP can also be extended with new \"methods\". In that case, user agents or servers use the \"Allow\" header to state which methods they support. To \"require\" the use of a particular method in a particular dialog, they must use an \"option tag\" associated to that method.\n\nThese two extensions allow users to specify their preferences about the service the IMS provides.\n\nWith the caller preferences extension, the calling party is able to indicate the kind of user agent they want to reach (e.g. whether it is fixed or mobile, a voicemail or a human, personal or for business, which services it is capable to provide, or which methods it supports) and how to search for it, with three header fields: \"Accept-Contact\" to describe the desired destination user agents, \"Reject-Contact\" to state the user agents to avoid, and \"Request-Disposition\" to specify how the request should be handled by servers in the network (i.e. whether or not to redirect and how to search for the user: sequentially or in parallel).\n\nBy using the user agent capabilities extension, user agents (terminals) can describe themselves when they register so that others can search for them according to their caller preferences extension headers. For this purpose, they list their capabilities in the \"Contact\" header field of the REGISTER message.\n\nThe aim of event notification is to obtain the status of a given resource (e.g. a user, one's voicemail service) and to receive updates of that status when it changes.\n\nEvent notification is necessary in the IMS framework to inform about the \"presence\" of a user (i.e. \"online\" or \"offline\") to others that may be waiting to contact them, or to notify a user and its P-CSCF of its own \"registration\" state, so that they know if they are reachable and what public identities they have registered. Moreover, event notification can be used to provide additional services such as voicemail (i.e. to notify that they have new voice messages in their inbox).\n\nTo this end, the specific event notification extension defines a framework for event notification in SIP, with two new methods: SUBSCRIBE and NOTIFY, new header fields and response codes and two roles: the \"subscriber\" and the \"notifier\". The entity interested in the state information of a resource (the \"subscriber\") sends a SUBSCRIBE message with the Uniform Resource Identifier (URI) of the resource in the request initial line, and the type of event in the \"Event header\". Then the entity in charge of keeping track of the state of the resource (the \"notifier\"), receives the SUBSCRIBE request and sends back a NOTIFY message with a \"subscription-state header\" as well as the information about the status of the resource in the message body. Whenever the resource state changes, the \"notifier\" sends a new NOTIFY message to the \"subscriber\". Each kind of event a subscriber can subscribe to is defined in a new \"event package\". An \"event package\" describes a new value for the SUBSCRIBE \"Event header\", as well as a MIME type to carry the event state information in the NOTIFY message.\n\nThere is also an \"allow-events\" header to indicate event notification capabilities, and the \"202 accepted\" and \"489 bad event\" response codes to indicate if a subscription request has been preliminary accepted or has been turned down because the \"notifier\" does not understand the kind of event requested.\n\nIn order to make an efficient use of the signaling messages, it is also possible to establish a limited notification rate (not real-time notifications) through a mechanism called \"event throttling\". Moreover, there is also a mechanism for \"conditional event notification\" that allows the \"notifier\" to decide whether or not to send the complete NOTIFY message depending on if there is something new to notify since last subscription or there is not.\n\nThe event notification framework defines how a user agent can subscribe to events about the state of a resource, but it does not specify how that state can be published. The SIP extension for event state publication was defined to allow user agents to publish the state of an event to the entity (\"notifier\") that is responsible for composing the event state and distributing it to the \"subscribers\".\n\nThe state publication framework defines a new method: PUBLISH, which is used to request the publication of the state of the resource specified in the request-URI, with reference to the event stated in the \"Event header\", and with the information carried in the message body.\n\nThe functionality of sending instant messages to provide a service similar to text messaging is defined in the instant messaging extension. These messages are unrelated to each other (i.e. they do not originate a SIP dialog) and sent through the SIP signaling network, sharing resources with control messages.\n\nThis functionality is supported by the new MESSAGE method, that can be used to send an instant message to the resource stated in the request-URI, with the content carried in the message body. This content is defined as a MIME type, being \"text/plain\" the most common one.\n\nIn order to have an instant messaging session with related messages, the Message Session Relay Protocol (MSRP) should be used.\n\nThe REFER method extension defines a mechanism to request a user agent to contact a resource which is identified by a URI in the \"Refer-To\" header field of the request message. A typical use of this mechanism is call transfer: during a call, the participant who sends the REFER message tells the recipient to contact to the user agent identified by the URI in the corresponding header field. The REFER message also implies an event subscription to the result of the operation, so that the sender will know whether or not the recipient could contact the third person.\n\nHowever, this mechanism is not restricted to call transfer, since the \"Refer-To\" header field can be any kind of URI, for instance, an HTTP URI, to require the recipient to visit a web page.\n\nIn the basic SIP specification, only requests and final responses (i.e. 2XX response codes) are transmitted reliably, this is, they are retransmitted by the sender until the acknowledge message arrives (i.e. the corresponding response code to a request, or the ACK request corresponding to a 2XX response code). This mechanism is necessary since SIP can run not only over reliable transport protocols (TCP) that assure that the message is delivered, but also over unreliable ones (UDP) that offer no delivery guarantees, and it is even possible that both kinds of protocols are present in different parts of the transport network.\n\nHowever, in such an scenario as the IMS framework, it is necessary to extend this reliability to provisional responses to INVITE requests (for session establishment, this is, to start a call). The reliability of provisional responses extension provides a mechanism to confirm that provisional responses such as the \"180 Ringing\" response code, that lets the caller know that the callee is being alerted, are successfully received. To do so, this extension defines a new method: PRACK, which is the request message used to tell the sender of a provisional response that his or her message has been received. This message includes a \"RACK\" header field which is a sequence number that matches the \"RSeq\" header field of the provisional response that is being acknowledged, and also contains the \"CSeq\" number that identifies the corresponding INVITE request. To indicate that the user agent requests or supports reliable provisional responses, the \"100rel\" option tag will be used.\n\nThe aim of the UPDATE method extension is to allow user agents to provide updated session description information within a dialog, before the final response to the initial INVITE request is generated. This can be used to negotiate and allocate the call resources before the called party is alerted.\n\nIn the IMS framework, it is required that once the callee is alerted, the chances of a session failure are minimum. An important source of failure is the inability to reserve network resources to support the session, so these resources should be allocated before the phone rings. However, in the IMS, to reserve resources the network needs to know the callee's IP address, port and session parameters and therefore it is necessary that the initial offer/answer exchange to establish a session has started (INVITE request). In basic SIP, this exchange eventually causes the callee to be alerted. To solve this problem, the concept of preconditions was introduced. In this concept the caller states a set of constraints about the session (i.e. codecs and QoS requirements) in the offer, and the callee responds to the offer without establishing the session or alerting the user. This establishment will occur if and only if both the caller and the callee agree that the preconditions are met.\n\nThe preconditions SIP extension affects both SIP, with a new option tag (\"precondition\") and defined offer/answer exchanges, and Session Description Protocol (SDP), which is a format used to describe streaming media initialization parameters, carried in the body of SIP messages. The new SDP attributes are meant to describe the \"current status\" of the resource reservation, the \"desired status\" of the reservation to proceed with session establishment, and the \"confirmation status\", to indicate when the reservation status should be confirmed.\n\nIn the IMS, the initial session parameter negotiation can be done by using the provisional responses and session description updating extensions, along with SDP in the body of the messages.\nThe first offer, described by means of SDP, can be carried by the INVITE request and will deal with the caller's supported codecs. This request will be answered by the provisional reliable response code 183 Session Progress, that will carry the SDP list of supported codecs by both the caller and the callee. The corresponding PRACK to this provisional answer will be used to select a codec and initiate the QoS negotiation.\n\nThe QoS negotiation is supported by the PRACK request, that starts resource reservation in the calling party network, and it is answered by a 2XX response code. Once this response has been sent, the called party has selected the codec too, and starts resource reservation on its side. Subsequent UPDATE requests are sent to inform about the reservation progress, and they are answered by 2XX response codes. In a typical offer/answer exchange, one UPDATE will be sent by the calling party when its reservation is completed, then the called party will respond and eventually finish allocating the resources. It is then, when all the resources for the call are in place, when the caller is alerted.\n\nIn the IMS framework it is fundamental to handle user identities for authentication, authorization and accounting purposes. The IMS is meant to provide multimedia services over IP networks, but also needs a mechanism to charge users for it. All this functionality is supported by new special header fields.\n\nThe Private Header Extensions to SIP, also known as P-Headers, are special header fields whose applicability is limited to private networks with a certain topology and characteristics of lower layers' protocols. They were designed specifically to meet the 3GPP requirements because a more general solution was not available.\n\nThese header fields are used for a variety of purposes including charging and information about the networks a call traverses:\n\n\nMore private headers have been defined for user database accessing:\n\n\nThe private extensions for asserted identity within trusted networks are designed to enable a network of trusted SIP servers to assert the identity of authenticated users, only within an administrative domain with previously agreed policies for generation, transport and usage of this identification information. These extensions also allow users to request privacy so that their identities are not spread outside the \"trust domain\". To indicate so, they must insert the privacy token \"id\" into the Privacy header field.\n\nThe main functionality is supported by the \"P-Asserted-Identity\" extension header. When a proxy server receives a request from an untrusted entity and authenticates the user (i.e. verifies that the user is who he or she says that he or she is), it then inserts this header with the identity that has been authenticated, and then forwards the request as usual. This way, other proxy servers that receive this SIP request within the \"Trust Domain\" (i.e. the network of trusted entities with previously agreed security policies) can safely rely on the identity information carried in the P-Asserted-Identity header without the necessity of re-authenticating the user.\n\nThe \"P-Preferred-Identity\" extension header is also defined, so that a user with several public identities is able to tell the proxy which public identity should be included in the P-Asserted-Identity header when the user is authenticated.\n\nFinally, when privacy is requested, proxies must withhold asserted identity information outside the trusted domain by removing P-Asserted-Identity headers before forwarding user requests to untrusted identities (outside the \"Trust Domain\").\n\nThere exist analogous extension headers for handling the identification of services of users, instead of the users themselves. In this case, Uniform Resource Names are used to identify a service (e.g. a voice call, an instant messaging session, an IPTV streaming)\n\nAccess security in the IMS consists of first authenticating and authorizing the user, which is done by the S-CSCF, and then establishing secure connections between the P-CSCF and the user. There are several mechanisms to achieve this, such as:\n\n\nThe security mechanisms agreement extension for SIP was then introduced to provide a secure mechanism for negotiating the security algorithms and parameters to be used by the P-CSCF and the terminal. This extension uses three new header fields to support the negotiation process:\n\n\nThe necessity in the IMS of reserving resources to provide quality of service (QoS) leads to another security issue: admission control and protection against denial-of-service attacks. To obtain transmission resources, the user agent must present an authorization token to the network (i.e. the policy enforcement point, or PEP) . This token will be obtained from its P-CSCF, which may be in charge of QoS policy control or have an interface with the policy control entity in the network (i.e. the policy decision function, or PDF) which originally provides the authorization token.\n\nThe private extensions for media authorization link session signaling to the QoS mechanisms applied to media in the network, by defining the mechanisms for obtaining authorization tokens and the \"P-Media-Authorization\" header field to carry these tokens from the P-CSCF to the user agent. This extension is only applicable within administrative domains with trust relationships. It was particularly designed for specialized SIP networks like the IMS, and not for the general Internet.\n\nSource routing is the mechanism that allows the sender of a message to specify partially or completely the route the message traverses. In SIP, the \"route\" header field, filled by the sender, supports this functionality by listing a set of proxies the message will visit. In the IMS context, there are certain network entities (i.e. certain CSCFs) that must be traversed by requests from or to a user, so they are to be listed in the \"Route\" header field. To allow the sender to discover such entities and populate the \"route\" header field, there are mainly two extension header fields: \"path\" and \"service-route\".\n\nThe extension header field for registering non-adjacent contacts provides a \"Path\" header field which accumulates and transmits the SIP URIs of the proxies that are situated between a user agent and its registrar as the REGISTER message traverses then. This way, the registrar is able to discover and record the sequence of proxies that must be transited to get back to the user agent.\n\nIn the IMS every user agent is served by its P-CSCF, which is discovered by using the Dynamic Host Configuration Protocol or an equivalent mechanism when the user enters the IMS network, and all requests and responses from or to the user agent must traverse this proxy. When the user registers to the home registrar (S-CSCF), the P-CSCF adds its own SIP URI in a \"Path\" header field in the REGISTER message, so that the S-CSCF receives and stores this information associated with the contact information of the user. This way, the S-CSCF will forward every request addressed to that user through the corresponding P-CSCF by listing its URI in the \"route\" header field.\n\nThe extension for service route discovery during registration consists of a \"Service-Route\" header field that is used by the registrar in a 2XX response to a REGISTER request to inform the registering user of the entity that must forward every request originated by him or her.\n\nIn the IMS, the registrar is the home network's S-CSCF and it is also required that all requests are handled by this entity, so it will include its own SIP URI in the \"service-route\" header field. The user will then include this SIP URI in the \"Route\" header field of all his or her requests, so that they are forwarded through the home S-CSCF.\n\nIn the IMS it is possible for a user to have multiple terminals (e.g. a mobile phone, a computer) or application instances (e.g. video telephony, instant messaging, voice mail) that are identified with the same public identity (i.e. SIP URI). Therefore, a mechanism is needed in order to route requests to the desired device or application. That is what a \"Globally Routable User Agent URI (GRU)\" is: a URI that identifies a specific user agent instance (i.e. terminal or application instance) and it does it globally (i.e. it is valid to route messages to that user agent from any other user agent on the Internet).\n\nThese URIs are constructed by adding the \"gr\" parameter to a SIP URI, either to the public SIP URI with a value that identifies the user agent instance, or to a specially created URI that does not reveal the relationship between the GRUU and the user's identity, for privacy purposes. They are commonly obtained during the registration process: the registering user agent sends a Uniform Resource Name (URN) that uniquely identifies that SIP instance, and the registrar (i.e. S-CSCF) builds the GRUU, associates it to the registered identity and SIP instance and sends it back to the user agent in the response. When the S-CSCF receives a request for that GRUU, it will be able to route the request to the registered SIP instance.\n\nThe efficient use of network resources, which may include a radio interface or other low-bandwidth access, is essential in the IMS in order to provide the user with an acceptable experience in terms of latency. To achieve this goal, SIP messages can be compressed using the mechanism known as \"SigComp\" (signaling compression).\n\nCompression algorithms perform this operation by substituting repeated words in the message by its position in a dictionary where all these words only appear once. In a first approach, this dictionary may be built for each message by the compressor and sent to the decompressor along with the message itself. However, as many words are repeated in different messages, the extended operations for \"SigComp\" define a way to use a shared dictionary among subsequent messages. Moreover, in order to speed up the process of building a dictionary along subsequent messages and provide high compression ratios since the first INVITE message, SIP provides a static SIP/SDP dictionary which is already built with common SIP and SDP terms.\n\nThere is a mechanism to indicate that a SIP message is desired to be compressed. This mechanism defines the \"comp=sigcomp\" parameter for SIP URIs, which signals that the SIP entity identified by the URI supports \"SigComp\" and is willing to receive compressed messages. When used in request-URIs, it indicates that the request is to be compressed, while in Via header fields it signals that the subsequent response is to be compressed.\n\nIn order to obtain even shorter SIP messages and make a very efficient use of the resources, the content indirection extension makes it possible to replace a MIME body part of the message with an external reference, typically an HTTP URI. This way the recipient of the message can decide whether or not to follow the reference to fetch the resource, depending on the bandwidth available.\n\nNetwork address translation (NAT) makes it impossible for a terminal to be reached from outside its private network, since it uses a private address that is mapped to a public one when packets originated by the terminal cross the NAT. Therefore, NAT traversal mechanisms are needed for both the signaling plane and the media plane.\n\nInternet Engineering Task Force's RFC 6314 summarizes and unifies different methods to achieve this, such as symmetric response routing and client-initiated connections for SIP signaling, and the use of STUN, TURN and ICE, which combines both previous ones, for media streams\n\nInternet Engineering Task Force's RFC 6157 describes the necessary mechanisms to guarantee that SIP works successfully between both Internet Protocol versions during the transition to IPv6. While SIP signaling messages can be transmitted through heterogeneous IPv4/IPv6 networks as long as proxy servers and DNS entries are properly configured to relay messages across both networks according to these recommendations, user agents will need to implement extensions so that they can directly exchange media streams. These extensions are related to the Session Description Protocol offer/answer initial exchange, that will be used to gather the IPv4 and IPv6 addresses of both ends so that they can establish a direct communication.\n\nApart from all the explained extensions to SIP that make it possible for the IMS to work successfully, it is also necessary that the IMS framework interworks and exchanges services with existing network infrastructures, mainly the Public switched telephone network (PSTN).\n\nThere are several standards that address this requirements, such as the following two for services interworking between the PSTN and the Internet (i.e. the IMS network):\n\nAnd also for PSTN-SIP gateways to support calls with one end in each network:\n\nMoreover, the SIP INFO method extension is designed to carry user information between terminals without affecting the signaling dialog and can be used to transport the dual-tone multi-frequency signaling to provide telephone keypad function for users.\n\n\n\n"}
{"id": "10856211", "url": "https://en.wikipedia.org/wiki?curid=10856211", "title": "Silicon Wadi", "text": "Silicon Wadi\n\nSilicon Wadi (, lit: \"Silicon Valley\") is an area with a high concentration of high-technology companies on the coastal plain of Israel, similar to Silicon Valley in the U.S. state of California, and is the reason Israel is nicknamed the Start-Up Nation. The area covers much of the country, although especially high concentrations of high-tech industry can be found in the area around Tel Aviv, including small clusters around the cities of Ra'anana, Petah Tikva, Herzliya, Netanya, the academic city of Rehovot and its neighbour Rishon Le Zion. In addition, high-tech clusters can be found in Haifa and Caesarea. More recent high-tech establishments have been raised in Jerusalem, and in towns such as Yokneam Illit and Israel's first \"private city,\" Airport City, near Tel Aviv.\n\nSilicon Wadi is a pun based on the Californian region of Silicon Valley. \"Wadi\" is the Arabic word for a valley or dry river bed, also commonly used in colloquial Hebrew.\n\nIsraeli high-tech firms originally began to form in the 1960s. In 1961 ECI Telecom was founded, followed in 1962 by Tadiran and Elron Electronic Industries regarded by many to be the \"Fairchild of Israel.\" The number of internationally successful firms grew slowly, with only one or two new successful firms each year until the early 1990s. Motorola was the first US corporation to set up an R&D unit in Israel, in 1964. The center initially developed wireless products including remote irrigation systems and later developed leading chips such as the 68030. Following the 1967 French arms embargo, Israel was forced to develop a domestic military industry, focusing on developing a technological edge over its neighbors. Some of these military firms started to seek and develop civilian applications of military technology. In the 1970s more commercial innovations began, many of which were based on military R&D, including: Scitex digital printing systems, which were based on fast rotation drums from fast-rotation electronic warfare systems, and Elscint, which developed innovative medical imaging and became a leading force in its market.\n\nHigh-tech firms continued to struggle throughout this period with marketing and many products, such as a mini-computer developed in the 1970s by Elbit, who were unable to successfully commercialise the product.\n\nSlowly, the international computing industry shifted the emphasis from hardware (in which Israel had no comparative advantage) to software products (in which human capital plays a larger role). The country became one of the first nations to compete in global software markets. By the 1980s a diverse set of software firms had developed. Each found niches which were not dominated by US firms and between 1984 and 1991 \"pure\" software exports increased from $5 million to $110 million. Many of the important ideas here were developed by graduates of Mamram, the Israeli computer corps, established by the IDF in the 1960s.\n\nDuring the 1980s and early 1990s several successful software companies emerged from Israel, including: Amdocs (established in 1982 as Aurec Information), Cimatron (established in 1982), Magic Software Enterprises (established in 1983), Comverse (established in 1983 as Efrat Future Technologies), Aladdin Knowledge Systems (established in 1985), NICE Systems (established in 1986), Mercury Interactive (established in 1989) and Check Point Software Technologies (established in 1993).\n\nThe 1990s saw the real takeoff of high-tech industries in Israel, with international media attention increasing awareness of innovation in the country. Growth increased, whilst new immigrants from the Soviet Union increased the available high-tech workforce. Peace agreements including the 1993 Oslo Peace Accord increased the investment environment and Silicon Wadi began to develop into a noticeable high-tech cluster.\n\nIn 1998, Mirabilis, an Israeli company that developed the ICQ instant messaging program, which revolutionized communication over the Internet, was purchased by America Online (AOL) for $407 million in cash, 18 months after it was founded and having no revenues. The free service attracted a user base of 15 million in that period and by 2001, ICQ had over 100 million users worldwide.\n\nThe success of Mirabilis triggered the dot-com boom in Israel; thousands of start-up companies were established between 1998 and 2001, while venture capital raised by Israeli companies reached $1,851 million in 1999, peaking at $3,701 million in 2000. Over fifty Israeli companies had initial public offerings on NASDAQ and other international stock markets during that period.\n\nFor more than 50 years, local demand fueled Israeli industrial expansion, as the country's population grew rapidly and the standard of living rose. More recently, world demand for Israeli advanced technologies, software, electronics, and other sophisticated equipment has stimulated industrial growth. Israel's high status in new technologies is the result of its emphasis on higher education and research and development. Cultural factors contributing to the expansion includes chutzpah and openness to immigration. The government also assists industrial growth by providing low-rate loans from its development budget. The main limitations experienced by industry are the scarcity of domestic raw materials and sources of energy and the restricted size of the local market. One certain advantage is that many Israeli university graduates are likely to become IT entrepreneurs or join startups, about twice as much as US university graduates, who are also attracted to traditional corporate executive positions, according to Charles A. Holloway, co-director of the Center for Entrepreneurial Studies and a professor at the Stanford Graduate School of Business of Stanford University. ICQ, for instance, is one of the world's most famous Israeli software products, developed by 4 young entrepreneurs. IBM has its IBM Content Discovery Engineering Team in Jerusalem, which is part of a number of IBM R&D Labs in Israel.\n\nAccording to research conducted by Prof. Shmuel Ellis, Chair of the Management Department at Tel Aviv University's Faculty of Management, together with Prof. Israel Drori of the School of Business Administration at the College of Management and Prof. Zur Shapira, Chair of the Management and Organizations Department at New York University, the RAD Group, founded in 1981 by brothers Yehuda and Zohar Zisapel, has been \"the most fertile ground\" for creating Israeli entrepreneurs, having produced 56 \"serial entrepreneurs\" who established more than one start-up each. RAD Group \"graduates\" were responsible for the establishment of a total of 111 significant high-tech initiatives.\n\nDue to the small size of Israel, the concentration of high-tech firms across much of the country is enough for it to be recognised as one large cluster. Most activity is located in the densely populated areas of metropolitan Tel Aviv, Haifa (Matam), and Jerusalem (Technology Park, Malha, Har Hotzvim and JVP Media Quarter in Talpiot), and the Startup Village Ecosystem in the Yokneam area, although some secondary with additional activity include the corridor to Beer Sheba, including Kiryat Gat, and the Western Galilee. In all, this is an area no larger than 6000 square kilometers, half of the extended Silicon Valley's geographical coverage.\n\nMany international technology companies have research and development facilities in this region, including companies such as Intel, IBM, Google, Facebook, Hewlett-Packard, Philips, Cisco Systems, Oracle Corporation, SAP, BMC Software, Microsoft, Motorola and CA. Many Israeli high-tech companies are based in the region, including Zoran Corporation, CEVA, Inc., Aladdin Knowledge Systems, Mellanox, NICE Systems, Horizon Semiconductors, RAD Data Communications, RADWIN, Radware, Tadiran Telecom, Radvision, Check Point Software Technologies, Amdocs, Babylon Ltd., Elbit, Israel Aerospace Industries and the solar thermal equipment designer and manufacturer Solel, with most of them being listed on the NASDAQ, which even has an Israel Index. Intel developed its dual-core Core Duo processor at its Israel Development Center located at the \"Merkaz Ta'asiya ve'Meida\" (Matam - Scientific Industries Center) in the city of Haifa. In 2006, more than 3,000 start-ups were created in Israel, a number that is only second to the US. Newsweek Magazine has also named Tel Aviv as one of the world's top ten \"Hot High-Tech Cities\". In 1998, Tel Aviv was named by \"Newsweek\" as one of the ten technologically most influential cities in the world. In 2012, the city was also named one of the best places for high-tech startup companies, placed only second behind its California counterpart.\n\nThe importance of Silicon Wadi was first recognised internationally by Wired magazine, who in 2000, ranked locations by the strength of cluster effects, giving the Israeli high-tech cluster the same rank as Boston, Helsinki, London, and Kista in Sweden, second only to Silicon Valley.\n\n\nThe origins of the now thriving venture capital industry in Israel can be traced to a government initiative in 1993 named the Yozma program (\"Initiative\" in Hebrew); which offered attractive tax incentives to any foreign venture-capital investments in Israel and offered to double any investment with funds from the government. As a result, Between 1991 and 2000, Israel's annual venture-capital outlays, nearly all private, rose nearly 60-fold, from $58 million to $3.3 billion; companies launched by Israeli venture funds rose from 100 to 800; and Israel's information-technology revenues rose from $1.6 billion to $12.5 billion. By 1999, Israel ranked second only to the United States in invested private-equity capital as a share of GDP. And it led the world in the share of its growth attributable to high-tech ventures: 70 percent.\n\nIsrael's thriving venture capital industry has played an important role in financing and funding Silicon Wadi. The financial crisis of 2007-2010 affected the availability of venture capital locally. In 2009, there were 63 mergers and acquisitions in the Israeli market worth a total of $2.54 billion; 7% below 2008 levels ($2.74 billion), when 82 Israeli companies were merged or acquired, and 33% lower than 2007 proceeds ($3.79 billion) when 87 Israeli companies were merged or acquired. Numerous high tech Israeli companies have been acquired by global corporations for its provision of reliable and quality corporate personnel.\n\nIsrael's venture capital industry has about 70 active venture capital funds, of which 14 international VCs with Israeli offices. Additionally, there are some 220 international funds, including Polaris Venture Partners, Accel Partners and Greylock Partners, that do not have branches in Israel, but actively invest in Israel through an in-house specialist.\n\nIn 2009, the life sciences sector led the market with $272 million or 24% of total capital raised, followed by the software sector with $258 million or 23%, the communications sector with $219 million or 20%, and the Internet sector with 13% of capital raised in 2009.\n\nWith such an impressive record for creating profit driven technology, Israel has become the top choice for many business leaders and high technology industry giants. As of 2010, more than 35,000 personnel are employed in multinationals research and development centers across Israel, making 'Silicon Wadi' a source for worldwide strategic technology development. In recent years, East Asian multinationals and investors, especially from Mainland China, have actively invested and opened up offices in Israel, including Chinese technology giants such as Alibaba, Baidu, Tencent and Kuang-Chi. Around 60 foreign R&D centers are engaged in a diverse range of activities including biotechnology, chemicals, industrial machinery, communication equipment, scientific instruments, medical devices, flash memory storage equipment, computer hardware components, software, semiconductors and internet. \n\n"}
{"id": "1227094", "url": "https://en.wikipedia.org/wiki?curid=1227094", "title": "Single-source publishing", "text": "Single-source publishing\n\nSingle-source publishing, also known as single-sourcing publishing, is a content management method which allows the same source content to be used across different forms of media and more than one time. The labor-intensive and expensive work of editing need only be carried out once, on only one document; that source document can then be stored in one place and reused. This reduces the potential for error, as corrections are only made one time in the source document.\n\nThe benefits of single-source publishing primarily relate to the editor rather than the user. The user benefits from the consistency that single-sourcing brings to terminology and information. This assumes the content manager has applied an organized conceptualization to the underlying content (A poor conceptualization can make single-source publishing less useful). Single-source publishing is sometimes used synonymously with multi-channel publishing though whether or not the two terms are synonymous is a matter of discussion.\n\nWhile there is a general definition of single-source publishing, there is no single official delineation between single-source publishing and multi-channel publishing, nor are there any official governing bodies to provide such a delineation. Single-source publishing is most often understood as the creation of one source document in an authoring tool and converting that document into different file formats or human languages (or both) multiple times with minimal effort. Multi-channel publishing can either be seen as synonymous with single-source publishing, or similar in that there is one source document but the process itself results in more than a mere reproduction of that source.\n\nThe origins of single-source publishing lie, indirectly, with the release of Windows 3.0 in 1990. With the eclipsing of MS-DOS by graphical user interfaces, help files went from being unreadable text along the bottom of the screen to hypertext systems such as WinHelp. On-screen help interfaces allowed software companies to cease the printing of large, expensive help manuals with their products, reducing costs for both producer and consumer. This system raised opportunities as well, and many developers fundamentally changed the way they thought about publishing. Writers of software documentation did not simply move from being writers of traditional bound books to writers of electronic publishing, but rather they became authors of central documents which could be reused multiple times across multiple formats.\n\nThe first single-source publishing project was started in 1993 by Cornelia Hofmann at Schneider Electric in Seligenstadt, using software based on Interleaf to automatically create paper documentation in multiple languages based on a single original source file.\n\nXML, developed during the mid- to late-1990s, was also significant to the development of single-source publishing as a method. XML, a markup language, allows developers to separate their documentation into two layers: a shell-like layer based on presentation and a core-like layer based on the actual written content. This method allows developers to write the content only one time while switching it in and out of multiple different formats and delivery methods.\n\nIn the mid-1990s, several firms began creating and using single-source content for technical documentation (Boeing Helicopter, Sikorsky Aviation and Pratt & Whitney Canada) and user manuals (Ford owners manuals) based on tagged SGML and XML content generated using the Arbortext Epic editor with add-on functions developed by a contractor. The concept behind this usage was that complex, hierarchical content that did not lend itself to discrete componentization could be used across a variety of requirements by tagging the differences within a single document using the capabilities built into SGML and XML.\nFord, for example, was able to tag its single owner's manual files so that 12 model years could be generated via a resolution script running on the single completed file. Pratt & Whitney, likewise, was able to tag up to 20 subsets of its jet engine manuals in single-source files, calling out the desired version at publication time. World Book Encyclopedia also used the concept to tag its articles for American and British versions of English.\n\nStarting from the early 2000s, single-source publishing was used with an increasing frequency in the field of technical translation. It is still regarded as the most efficient method of publishing the same material in different languages. Once a printed manual was translated, for example, the online help for the software program which the manual accompanies could be automatically generated using the method. Metadata could be created for an entire manual and individual pages or files could then be translated from that metadata with only one step, removing the need to recreate information or even database structures.\n\nAlthough single-source publishing is now decades old, its importance has increased urgently as of the 2010s. As consumption of information products rises and the number of target audiences expands, so does the work of developers and content creators. Within the industry of software and its documentation, there is a perception that the choice is to embrace single-source publishing or render one's operations obsolete.\n\nEditors using single-source publishing have been criticized for below-standard work quality, leading some critics to describe single-source publishing as the \"conveyor belt assembly\" of content creation. \n\nWhile heavily used in technical translation, there are risks of error in regard to indexing. While two words might be synonyms in English, they may not be synonyms in another language. In a document produced via single-sourcing, the index will be translated automatically and the two words will be rendered as synonyms. This is because they are synonyms in the source language, while in the target language they are not.\n\n\n\n\n"}
{"id": "22021031", "url": "https://en.wikipedia.org/wiki?curid=22021031", "title": "Social translucence", "text": "Social translucence\n\nSocial translucence (also referred as \"social awareness\") is a term that was proposed by Thomas Erickson and Wendy Kellogg to refer to \"design digital systems that support coherent behavior by making participants and their activities visible to one another\".\n\nSocial translucence represents a tool for transparency in socio-technical systems, which function is to\n\nSocial translucence is, in particular, a core element in online social networking such as Facebook or LinkedIn, in which they intervene in the possibility for people to expose their online identity, but also in the creation of awareness of other people activities, that are for instance present in the activity feeds that these systems make available.\n\nSocial translucence mechanisms have been made available in many web 2.0 systems such as:\n\nParticipation of people in online communities, in general, differ from their participatory behavior in real-world collective contexts. Humans in daily life are used to making use of \"social cues\" for guiding their decisions and actions e.g. if a group of people is looking for a good restaurant to have lunch, it is very likely that they will choose to enter to a local that have some customers inside instead of one that it is empty (the more crowded restaurant could reflect its popularity and in consequence, its quality of service). However, in online social environments, it is not straightforward how to access to these sources of information which are normally being logged in the systems, but this is not disclosed to the users.\n\nThere are some theories that explain how this social translucence can affect the behavior of people in real-life scenarios. The American philosopher George Herbert Mead states that humans are social creatures, in the sense that people's actions cannot be isolated from the behavior of the whole collective they are part of because every individuals' acts are influenced by larger social practices that act as a general behavior's framework. In his performance framework, the Canadian sociologist Erving Goffman postulates that in everyday social interactions individuals perform their actions by collecting information from others first, in order to know in advance what they may expect from them and in this way being able to plan how to behave more effectively.\n\nAccording to Erickson et al., social translucent systems should respect the principles of visibility (making significant social information available to users), awareness (bringing our social rules to guide our actions based on external social cues) and accountability (being able to identify who did what and when) in order to allow people to effectively facilitate users communication and collaboration in virtual environments. Zolyomi et al. proposed the principle of identity as a fourth dimension for social translucence by arguing that the design of socio-technical systems should have a rich description of who is visible, in order to give people control over disclosure and mechanisms to advocate for their needs. McDonald et al. proposed a system architecture for structuring the development of social translucent systems, which comprises two dimensions: types of user actions in the system, and a second describing the processing and interpretation done by the system. This framework can guide designers to determine what activities are important to social translucence and need to be reflected, and how interpretive levels of those actions might provide contextual salience to the users \n\nIn the same way that in the real-world, providing social cues in virtual communities can help people to understand better the situations they face in these environments, to alleviate their decision-making processes by enabling their access to more informed choices, to persuade them to participate in the activities that take place there, and to structure their own schedule of individual and group activities more efficiently.\n\nIn this frame of reference, an approach called \"social context displays\" has been proposed for showing social information -either from real or virtual environments- in digital scenarios. It is based on the use of graphical representations to visualize the presence and activity traces of a group of people, thus providing users with a third-party view of what is happening within the community i.e. who are actively participating, who are not contributing to the group efforts, etc. This social-context-revealing approach has been studied in different scenarios (e.g. IBM video-conference software, large community displaying social activity traces in a shared space called NOMATIC*VIZ), and it has been demonstrated that its application can provide users with several benefits, like providing them with more information to make better decisions and motivating them to take an active attitude towards the management of their self and group representations within the display through their actions in the real-life.\n\nThe feeling of personal accountability in front of others that social translucence can report to users can be used for the design of systems for supporting behavior change (e.g. weight loss, smoking cessation), if combined with the appropriate type of feedback.\n\nBy making the traces of activity of users publicly available for others to access it is natural that it can raise users concerns related to which are their rights over the data they generate, who are the final users that can have access to their information and how they can know and control their privacy policies. There are several perspectives that try to contextualize this privacy issue. One perspective is to see privacy as a tradeoff between the degree of invasion to the personal space and the number of benefits that the user could perceive from the social system by disclosing their online activity traces. Another perspective is examining the concession between the visibility of people within the social system and their level of privacy, which can be managed at an individual or at a group level by establishing specific permissions for allowing others to have access to their information. Other authors state that instead of enforcing users to set and control privacy settings, social systems might focus on raising their awareness about who their audiences are so they can manage their online behavior according to the reactions they expect from those different user groups.\n\n\n"}
{"id": "29192", "url": "https://en.wikipedia.org/wiki?curid=29192", "title": "Space elevator", "text": "Space elevator\n\nA space elevator is a proposed type of planet-to-space transportation system. The main component would be a cable (also called a tether) anchored to the surface and extending into space. The design would permit vehicles to travel along the cable from a planetary surface, such as the Earth's, directly into space or orbit, without the use of large rockets. An Earth-based space elevator would consist of a cable with one end attached to the surface near the equator and the other end in space beyond geostationary orbit (35,786 km altitude). The competing forces of gravity, which is stronger at the lower end, and the outward/upward centrifugal force, which is stronger at the upper end, would result in the cable being held up, under tension, and stationary over a single position on Earth. With the tether deployed, climbers could repeatedly climb the tether to space by mechanical means, releasing their cargo to orbit. Climbers could also descend the tether to return cargo to the surface from orbit.\n\nThe concept of a tower reaching geosynchronous orbit was first published in 1895 by Konstantin Tsiolkovsky. His proposal was for a free-standing tower reaching from the surface of Earth to the height of geostationary orbit. Like all buildings, Tsiolkovsky's structure would be under compression, supporting its weight from below. Since 1959, most ideas for space elevators have focused on purely tensile structures, with the weight of the system held up from above by centrifugal forces. In the tensile concepts, a space tether reaches from a large mass (the counterweight) beyond geostationary orbit to the ground. This structure is held in tension between Earth and the counterweight like an upside-down plumb bob.\n\nTo construct a space elevator on Earth, the cable material would need to be both stronger and lighter (have greater specific strength) than any known material. Development of new materials that meet the demanding specific strength requirement must happen before designs can progress beyond discussion stage. Carbon nanotubes (CNTs) have been identified as possibly being able to meet the specific strength requirements for an Earth space elevator. Other materials considered have been boron nitride nanotubes, and diamond nanothreads, which were first constructed in 2014.\n\nA prototype was launched in 2018 to tether to future stations as well as the International Space Station. It is a miniature version to be further examined before making the decision to build up a large structure in the coming years. \n\nThe concept is applicable to other planets and celestial bodies. For locations in the solar system with weaker gravity than Earth's (such as the Moon or Mars), the strength-to-density requirements for tether materials are not as problematic. Currently available materials (such as Kevlar) are strong and light enough that they could be used as the tether material for elevators there.\n\nThe key concept of the space elevator appeared in 1895 when Russian scientist Konstantin Tsiolkovsky was inspired by the Eiffel Tower in Paris. He considered a similar tower that reached all the way into space and was built from the ground up to the altitude of 35,786 kilometers, the height of geostationary orbit. He noted that the top of such a tower would be circling Earth as in a geostationary orbit. Objects would attain horizontal velocity as they rode up the tower, and an object released at the tower's top would have enough horizontal velocity to remain there in geostationary orbit. Tsiolkovsky's conceptual tower was a compression structure, while modern concepts call for a tensile structure (or \"tether\").\n\nBuilding a compression structure from the ground up proved an unrealistic task as there was no material in existence with enough compressive strength to support its own weight under such conditions. In 1959 another Russian scientist, Yuri N. Artsutanov, suggested a more feasible proposal. Artsutanov suggested using a geostationary satellite as the base from which to deploy the structure downward. By using a counterweight, a cable would be lowered from geostationary orbit to the surface of Earth, while the counterweight was extended from the satellite away from Earth, keeping the cable constantly over the same spot on the surface of the Earth. Artsutanov's idea was introduced to the Russian-speaking public in an interview published in the Sunday supplement of \"Komsomolskaya Pravda\" in 1960, but was not available in English until much later. He also proposed tapering the cable thickness so that the stress in the cable was constant. This gave a thinner cable at ground level that became thickest at the level of geostationary orbit.\n\nBoth the tower and cable ideas were proposed in the quasi-humorous \"Ariadne\" column in \"New Scientist\", December 24, 1964.\n\nIn 1966, Isaacs, Vine, Bradner and Bachus, four American engineers, reinvented the concept, naming it a \"Sky-Hook\", and published their analysis in the journal \"Science\". They decided to determine what type of material would be required to build a space elevator, assuming it would be a straight cable with no variations in its cross section, and found that the strength required would be twice that of any then-existing material including graphite, quartz, and diamond.\n\nIn 1975 an American scientist, Jerome Pearson, reinvented the concept yet again, publishing his analysis in the journal \"Acta Astronautica\". He designed a tapered cross section that would be better suited to building the elevator. The completed cable would be thickest at the geostationary orbit, where the tension was greatest, and would be narrowest at the tips to reduce the amount of weight per unit area of cross section that any point on the cable would have to bear. He suggested using a counterweight that would be slowly extended out to , almost half the distance to the Moon as the lower section of the elevator was built. Without a large counterweight, the upper portion of the cable would have to be longer than the lower due to the way gravitational and centrifugal forces change with distance from Earth. His analysis included disturbances such as the gravitation of the Moon, wind and moving payloads up and down the cable. The weight of the material needed to build the elevator would have required thousands of Space Shuttle trips, although part of the material could be transported up the elevator when a minimum strength strand reached the ground or be manufactured in space from asteroidal or lunar ore.\n\nAfter the development of carbon nanotubes in the 1990s, engineer David Smitherman of NASA/Marshall's Advanced Projects Office realized that the high strength of these materials might make the concept of a space elevator feasible, and put together a workshop at the Marshall Space Flight Center, inviting many scientists and engineers to discuss concepts and compile plans for an elevator to turn the concept into a reality.\n\nIn 2000, another American scientist, Bradley C. Edwards, suggested creating a long paper-thin ribbon using a carbon nanotube composite material. He chose the wide-thin ribbon-like cross-section shape rather than earlier circular cross-section concepts because that shape would stand a greater chance of surviving impacts by meteoroids. The ribbon cross-section shape also provided large surface area for climbers to climb with simple rollers. Supported by the NASA Institute for Advanced Concepts, Edwards' work was expanded to cover the deployment scenario, climber design, power delivery system, orbital debris avoidance, anchor system, surviving atomic oxygen, avoiding lightning and hurricanes by locating the anchor in the western equatorial Pacific, construction costs, construction schedule, and environmental hazards.\n\nTo speed space elevator development, proponents have organized several competitions, similar to the Ansari X Prize, for relevant technologies. Among them are , which organized annual competitions for climbers, ribbons and power-beaming systems from 2005 to 2009, the Robogames Space Elevator Ribbon Climbing competition, as well as NASA's Centennial Challenges program, which, in March 2005, announced a partnership with the Spaceward Foundation (the operator of Elevator:2010), raising the total value of prizes to US$400,000.\nThe first European Space Elevator Challenge (EuSEC) to establish a climber structure took place in August 2011.\n\nIn 2005, \"the LiftPort Group of space elevator companies announced that it will be building a carbon nanotube manufacturing plant in Millville, New Jersey, to supply various glass, plastic and metal companies with these strong materials. Although LiftPort hopes to eventually use carbon nanotubes in the construction of a space elevator, this move will allow it to make money in the short term and conduct research and development into new production methods.\" Their announced goal was a space elevator launch in 2010. On February 13, 2006 the LiftPort Group announced that, earlier the same month, they had tested a mile of \"space-elevator tether\" made of carbon-fiber composite strings and fiberglass tape measuring wide and 1 mm (approx. 13 sheets of paper) thick, lifted with balloons.\n\nIn 2007, held the 2007 Space Elevator games, which featured US$500,000 awards for each of the two competitions, ($1,000,000 total) as well as an additional $4,000,000 to be awarded over the next five years for space elevator related technologies. No teams won the competition, but a team from MIT entered the first 2-gram (0.07 oz), 100-percent carbon nanotube entry into the competition. Japan held an international conference in November 2008 to draw up a timetable for building the elevator.\n\nIn 2008 the book \"Leaving the Planet by Space Elevator\" by Dr. Brad Edwards and Philip Ragan was published in Japanese and entered the Japanese best-seller list. This led to Shuichi Ono, chairman of the Japan Space Elevator Association, unveiling a space-elevator plan, putting forth what observers considered an extremely low cost estimate of a trillion yen (£5 billion / $8 billion) to build one.\n\nIn 2012, the Obayashi Corporation announced that in 38 years it could build a space elevator using carbon nanotube technology. At 200 kilometers per hour, the design's 30-passenger climber would be able to reach the GEO level after a 7.5 day trip. No cost estimates, finance plans, or other specifics were made. This, along with timing and other factors, hinted that the announcement was made largely to provide publicity for the opening of one of the company's other projects in Tokyo.\n\nIn 2013, the International Academy of Astronautics published a technological feasibility assessment which concluded that the critical capability improvement needed was the tether material, which was projected to achieve the necessary strength-to-weight ratio within 20 years. The four-year long study looked into many facets of space elevator development including missions, development schedules, financial investments, revenue flow, and benefits. It was reported that it would be possible to operationally survive smaller impacts and avoid larger impacts, with meteors and space debris, and that the estimated cost of lifting a kilogram of payload to GEO and beyond would be $500.\n\nIn 2014, Google X's Rapid Evaluation R&D team began the design of a Space Elevator, eventually finding that no one had yet manufactured a perfectly formed carbon nanotube strand longer than a meter. They thus decided to put the project in \"deep freeze\" and also keep tabs on any advances in the carbon nanotube field.\n\nIn 2018, researchers at Japan's Shizuoka University launched a mini-elevator consisting of two cube stats and a tether. The prototype was launched as a test bed for a larger structure.\n\nIn 1979, space elevators were introduced to a broader audience with the simultaneous publication of Arthur C. Clarke's novel, \"The Fountains of Paradise\", in which engineers construct a space elevator on top of a mountain peak in the fictional island country of \"Taprobane\" (loosely based on Sri Lanka, albeit moved south to the Equator), and Charles Sheffield's first novel, \"The Web Between the Worlds\", also featuring the building of a space elevator. Three years later, in Robert A. Heinlein's 1982 novel \"Friday\" the principal character makes use of the \"Nairobi Beanstalk\" in the course of her travels. In Kim Stanley Robinson's 1993 novel \"Red Mars\", colonists build a space elevator on Mars that allows both for more colonists to arrive and also for natural resources mined there to be able to leave for Earth. In David Gerrold's 2000 novel, \"Jumping Off The Planet\", a family excursion up the Ecuador \"beanstalk\" is actually a child-custody kidnapping. Gerrold's book also examines some of the industrial applications of a mature elevator technology. In a biological version, Joan Slonczewski's 2011 novel \"The Highest Frontier\" depicts a college student ascending a space elevator constructed of self-healing cables of anthrax bacilli. The engineered bacteria can regrow the cables when severed by space debris.\n\nA space elevator cable rotates along with the rotation of the Earth. Therefore, objects attached to the cable would experience upward centrifugal force in the direction opposing the downward gravitational force. The higher up the cable the object is located, the less the gravitational pull of the Earth, and the stronger the upward centrifugal force due to the rotation, so that more centrifugal force opposes less gravity. The centrifugal force and the gravity are balanced at geosynchronous equatorial orbit (GEO). Above GEO, the centrifugal force is stronger than gravity, causing objects attached to the cable there to pull \"upward\" on it.\n\nThe net force for objects attached to the cable is called the \"apparent gravitational field\". The apparent gravitational field for attached objects is the (downward) gravity minus the (upward) centrifugal force. The apparent gravity experienced by an object on the cable is zero at GEO, downward below GEO, and upward above GEO.\n\nThe apparent gravitational field can be represented this way:\n\nwhere\n\nAt some point up the cable, the two terms (downward gravity and upward centrifugal force) are equal and opposite. Objects fixed to the cable at that point put no weight on the cable. This altitude (r) depends on the mass of the planet and its rotation rate. Setting actual gravity equal to centrifugal acceleration gives:\nOn Earth, this distance is above the surface, the altitude of geostationary orbit.\n\nOn the cable \"below\" geostationary orbit, downward gravity would be greater than the upward centrifugal force, so the apparent gravity would pull objects attached to the cable downward. Any object released from the cable below that level would initially accelerate downward along the cable. Then gradually it would deflect eastward from the cable. On the cable \"above\" the level of stationary orbit, upward centrifugal force would be greater than downward gravity, so the apparent gravity would pull objects attached to the cable \"upward\". Any object released from the cable \"above\" the geosynchronous level would initially accelerate \"upward\" along the cable. Then gradually it would deflect westward from the cable.\n\nHistorically, the main technical problem has been considered the ability of the cable to hold up, with tension, the weight of itself below any given point. The greatest tension on a space elevator cable is at the point of geostationary orbit, above the Earth's equator. This means that the cable material, combined with its design, must be strong enough to hold up its own weight from the surface up to . A cable which is thicker in cross section at that height than at the surface could better hold up its own weight over a longer length. How the cross section area tapers from the maximum at to the minimum at the surface is therefore an important design factor for a space elevator cable.\n\nTo maximize the usable excess strength for a given amount of cable material, the cable's cross section area would need to be designed for the most part in such a way that the stress (i.e., the tension per unit of cross sectional area) is constant along the length of the cable. The constant-stress criterion is a starting point in the design of the cable cross section as it changes with altitude. Other factors considered in more detailed designs include thickening at altitudes where more space junk is present, consideration of the point stresses imposed by climbers, and the use of varied materials. To account for these and other factors, modern detailed cross section designs seek to achieve the largest \"safety margin\" possible, with as little variation over altitude and time as possible. In simple starting-point designs, that equates to constant-stress.\n\nIn the constant-stress case, the cross-section follows this differential equation:\nor\nor\nwhere\n\nThe value of \"g\" is given by the first equation, which yields:\n\nthe variation being taken between \"r\" (ground) and \"r\" (geostationary).\n\nBetween these two points, this quantity can be expressed as:\nor\nwhere formula_1 is the ratio between the centrifugal force on the equator and the gravitational force.\n\nTo compare materials, the \"specific strength\" of the material for the space elevator can be expressed in terms of the \"characteristic length\", or \"free breaking length\": the length of an un-tapered cylindrical cable at which it will break under its own weight under constant gravity. For a given material, that length is formula_2, where formula_3, formula_4 and formula_5 are as defined above.\n\nThe free breaking length needed is given by the equation\nFor a space elevator with a material with formula_6, the section at the synchronous orbit needs to be \"formula_7\" times as much as at ground level. For a space elevator with a material with formula_8, the section at the synchronous orbit needs to be \"formula_9\" times as much as at ground level.\n\nThere are a variety of space elevator designs. Almost every design includes a base station, a cable, climbers, and a counterweight. Earth's rotation creates upward centrifugal force on the counterweight. The counterweight is held down by the cable while the cable is held up and taut by the counterweight. The base station anchors the whole system to the surface of the Earth. Climbers climb up and down the cable with cargo.\n\nModern concepts for the base station/anchor are typically mobile stations, large oceangoing vessels or other mobile platforms. Mobile base stations would have the advantage over the earlier stationary concepts (with land-based anchors) by being able to maneuver to avoid high winds, storms, and space debris. Oceanic anchor points are also typically in international waters, simplifying and reducing cost of negotiating territory use for the base station.\n\nStationary land based platforms would have simpler and less costly logistical access to the base. They also would have an advantage of being able to be at high altitude, such as on top of mountains. In an alternate concept, the base station could be a tower, forming a space elevator which comprises both a compression tower close to the surface, and a tether structure at higher altitudes. Combining a compression structure with a tension structure would reduce loads from the atmosphere at the Earth end of the tether, and reduce the distance into the Earth's gravity field the cable needs to extend, and thus reduce the critical strength-to-density requirements for the cable material, all other design factors being equal.\n\nA space elevator cable would need to carry its own weight as well as the additional weight of climbers. The required strength of the cable would vary along its length. This is because at various points it would have to carry the weight of the cable below, or provide a downward force to retain the cable and counterweight above. Maximum tension on a space elevator cable would be at geosynchronous altitude so the cable would have to be thickest there and taper carefully as it approaches Earth. Any potential cable design may be characterized by the taper factor – the ratio between the cable's radius at geosynchronous altitude and at the Earth's surface.\n\nThe cable would need to be made of a material with a large tensile strength/density ratio. For example, the Edwards space elevator design assumes a cable material with a specific strength of at least 100,000 kN/(kg/m). This value takes into consideration the entire weight of the space elevator. An untapered space elevator cable would need a material capable of sustaining a length of of its own weight \"at sea level\" to reach a geostationary altitude of without yielding. Therefore, a material with very high strength and lightness is needed.\n\nFor comparison, metals like titanium, steel or aluminium alloys have breaking lengths of only 20–30 km. Modern fibre materials such as kevlar, fibreglass and carbon/graphite fibre have breaking lengths of 100–400 km. Nanoengineered materials such as carbon nanotubes and, more recently discovered, graphene ribbons (perfect two-dimensional sheets of carbon) are expected to have breaking lengths of 5000–6000 km at sea level, and also are able to conduct electrical power.\n\nFor a space elevator on Earth, with its comparatively high gravity, the cable material would need to be stronger and lighter than currently available materials. For this reason, there has been a focus on the development of new materials that meet the demanding specific strength requirement. For high specific strength, carbon has advantages because it is only the 6th element in the periodic table. Carbon has comparatively few of the protons and neutrons which contribute most of the dead weight of any material. Most of the interatomic bonding forces of any element are contributed by only the outer few electrons. For carbon, the strength and stability of those bonds is high compared to the mass of the atom. The challenge in using carbon nanotubes remains to extend to macroscopic sizes the production of such material that are still perfect on the microscopic scale (as microscopic defects are most responsible for material weakness).\n\nIn 2014, diamond nanothreads were first synthesized. Since they have strength properties similar to carbon nanotubes, diamond nanothreads were quickly seen as candidate cable material as well.\n\nA space elevator cannot be an elevator in the typical sense (with moving cables) due to the need for the cable to be significantly wider at the center than at the tips. While various designs employing moving cables have been proposed, most cable designs call for the \"elevator\" to climb up a stationary cable.\n\nClimbers cover a wide range of designs. On elevator designs whose cables are planar ribbons, most propose to use pairs of rollers to hold the cable with friction.\n\nClimbers would need to be paced at optimal timings so as to minimize cable stress and oscillations and to maximize throughput. Lighter climbers could be sent up more often, with several going up at the same time. This would increase throughput somewhat, but would lower the mass of each individual payload.\nThe horizontal speed, i.e. due to orbital rotation, of each part of the cable increases with altitude, proportional to distance from the center of the Earth, reaching low orbital speed at a point approximately 66 percent of the height between the surface and geostationary orbit, or a height of about 23,400 km. A payload released at this point would go into a highly eccentric elliptical orbit, staying just barely clear from atmospheric reentry, with the periapsis at the same altitude as LEO and the apoapsis at the release height. With increasing release height the orbit would become less eccentric as both periapsis and apoapsis increase, becoming circular at geostationary level.\nWhen the payload has reached GEO, the horizontal speed is exactly the speed of a circular orbit at that level, so that if released, it would remain adjacent to that point on the cable. The payload can also continue climbing further up the cable beyond GEO, allowing it to obtain higher speed at jettison. If released from 100,000 km, the payload would have enough speed to reach the asteroid belt.\n\nAs a payload is lifted up a space elevator, it would gain not only altitude, but horizontal speed (angular momentum) as well. The angular momentum is taken from the Earth's rotation. As the climber ascends, it is initially moving slower than each successive part of cable it is moving on to. This is the Coriolis force: the climber \"drags\" (westward) on the cable, as it climbs, and slightly decreases the Earth's rotation speed. The opposite process would occur for descending payloads: the cable is tilted eastward, thus slightly increasing Earth's rotation speed.\n\nThe overall effect of the centrifugal force acting on the cable would cause it to constantly try to return to the energetically favorable vertical orientation, so after an object has been lifted on the cable, the counterweight would swing back toward the vertical like an inverted pendulum. Space elevators and their loads would be designed so that the center of mass is always well-enough above the level of geostationary orbit to hold up the whole system. Lift and descent operations would need to be carefully planned so as to keep the pendulum-like motion of the counterweight around the tether point under control.\n\nClimber speed would be limited by the Coriolis force, available power, and by the need to ensure the climber's accelerating force does not break the cable. Climbers would also need to maintain a minimum average speed in order to move material up and down economically and expeditiously. At the speed of a very fast car or train of it will take about 5 days to climb to geosynchronous orbit.\n\nBoth power and energy are significant issues for climbers—the climbers would need to gain a large amount of potential energy as quickly as possible to clear the cable for the next payload.\n\nVarious methods have been proposed to get that energy to the climber:\n\nWireless energy transfer such as laser power beaming is currently considered the most likely method, using megawatt powered free electron or solid state lasers in combination with adaptive mirrors approximately wide and a photovoltaic array on the climber tuned to the laser frequency for efficiency. For climber designs powered by power beaming, this efficiency is an important design goal. Unused energy would need to be re-radiated away with heat-dissipation systems, which add to weight.\n\nYoshio Aoki, a professor of precision machinery engineering at Nihon University and director of the Japan Space Elevator Association, suggested including a second cable and using the conductivity of carbon nanotubes to provide power.\n\nSeveral solutions have been proposed to act as a counterweight:\nExtending the cable has the advantage of some simplicity of the task and the fact that a payload that went to the end of the counterweight-cable would acquire considerable velocity relative to the Earth, allowing it to be launched into interplanetary space. Its disadvantage is the need to produce greater amounts of cable material as opposed to using just anything available that has mass.\n\nAn object attached to a space elevator at a radius of approximately 53,100 km would be at escape velocity when released. Transfer orbits to the L1 and L2 Lagrangian points could be attained by release at 50,630 and 51,240 km, respectively, and transfer to lunar orbit from 50,960 km.\n\nAt the end of Pearson's cable, the tangential velocity is 10.93 kilometers per second (6.79 mi/s). That is more than enough to escape Earth's gravitational field and send probes at least as far out as Jupiter. Once at Jupiter, a gravitational assist maneuver could permit solar escape velocity to be reached.\n\nA space elevator could also be constructed on other planets, asteroids and moons.\n\nA Martian tether could be much shorter than one on Earth. Mars' surface gravity is 38 percent of Earth's, while it rotates around its axis in about the same time as Earth. Because of this, Martian stationary orbit is much closer to the surface, and hence the elevator could be much shorter. Current materials are already sufficiently strong to construct such an elevator. Building a Martian elevator would be complicated by the Martian moon Phobos, which is in a low orbit and intersects the Equator regularly (twice every orbital period of 11 h 6 min).\n\nOn the near side of the Moon, the strength-to-density required of the tether of a lunar space elevator exists in currently available materials. A lunar space elevator would be about long. Since the Moon does not rotate fast enough, there is no effective lunar-stationary orbit, but the Lagrangian points could be used. The near side would extend through the Earth-Moon L1 point from an anchor point near the center of the visible part of Earth's Moon.\n\nOn the far side of the Moon, a lunar space elevator would need to be very long—more than twice the length of an Earth elevator—but due to the low gravity of the Moon, could also be made of existing engineering materials.\n\nRapidly spinning asteroids or moons could use cables to eject materials to convenient points, such as Earth orbits; or conversely, to eject materials to send a portion of the mass of the asteroid or moon to Earth orbit or a Lagrangian point. Freeman Dyson, a physicist and mathematician, has suggested using such smaller systems as power generators at points distant from the Sun where solar power is uneconomical.\n\nA space elevator using presently available engineering materials could be constructed between mutually tidally locked worlds, such as Pluto and Charon or the components of binary asteroid 90 Antiope, with no terminus disconnect, according to Francis Graham of Kent State University. However, spooled variable lengths of cable must be used due to ellipticity of the orbits.\n\nThe construction of a space elevator would need reduction of some technical risk. Some advances in engineering, manufacturing and physical technology are required. Once a first space elevator is built, the second one and all others would have the use of the previous ones to assist in construction, making their costs considerably lower. Such follow-on space elevators would also benefit from the great reduction in technical risk achieved by the construction of the first space elevator.\n\nPrior to the work of Edwards in 2000 most concepts for constructing a space elevator had the cable manufactured in space. That was thought to be necessary for such a large and long object and for such a large counterweight. Manufacturing the cable in space would be done in principle by using an asteroid or Near-Earth object for source material. These earlier concepts for construction require a large preexisting space-faring infrastructure to maneuver an asteroid into its needed orbit around Earth. They also required the development of technologies for manufacture in space of large quantities of exacting materials.\n\nSince 2001, most work has focused on simpler methods of construction requiring much smaller space infrastructures. They conceive the launch of a long cable on a large spool, followed by deployment of it in space. The spool would be initially parked in a geostationary orbit above the planned anchor point. A long cable would be dropped \"downward\" (toward Earth) and would be balanced by a mass being dropped \"upward\" (away from Earth) for the whole system to remain on the geosynchronous orbit. Earlier designs imagined the balancing mass to be another cable (with counterweight) extending upward, with the main spool remaining at the original geosynchronous orbit level. Most current designs elevate the spool itself as the main cable is paid out, a simpler process. When the lower end of the cable is long enough to reach the surface of the Earth (at the equator), it would be anchored. Once anchored, the center of mass would be elevated more (by adding mass at the upper end or by paying out more cable). This would add more tension to the whole cable, which could then be used as an elevator cable.\n\nOne plan for construction uses conventional rockets to place a \"minimum size\" initial seed cable of only 19,800 kg. This first very small ribbon would be adequate to support the first 619 kg climber. The first 207 climbers would carry up and attach more cable to the original, increasing its cross section area and widening the initial ribbon to about 160 mm wide at its widest point. The result would be a 750-ton cable with a lift capacity of 20 tons per climber.\n\nFor early systems, transit times from the surface to the level of geosynchronous orbit would be about five days. On these early systems, the time spent moving through the Van Allen radiation belts would be enough that passengers would need to be protected from radiation by shielding, which would add mass to the climber and decrease payload.\n\nA space elevator would present a navigational hazard, both to aircraft and spacecraft. Aircraft could be diverted by air-traffic control restrictions. All objects in stable orbits that have perigee below the maximum altitude of the cable that are not synchronous with the cable would impact the cable eventually, unless avoiding action is taken. One potential solution proposed by Edwards is to use a movable anchor (a sea anchor) to allow the tether to \"dodge\" any space debris large enough to track.\n\nImpacts by space objects such as meteoroids, micrometeorites and orbiting man-made debris pose another design constraint on the cable. A cable would need to be designed to maneuver out of the way of debris, or absorb impacts of small debris without breaking.\n\nWith a space elevator, materials might be sent into orbit at a fraction of the current cost. As of 2000, conventional rocket designs cost about US$25,000 per kilogram (US$11,000 per pound) for transfer to geostationary orbit. Current space elevator proposals envision payload prices starting as low as $220 per kilogram ($100 per pound), similar to the $5–$300/kg estimates of the Launch loop, but higher than the $310/ton to 500 km orbit quoted to Dr. Jerry Pournelle for an orbital airship system.\n\nPhilip Ragan, co-author of the book \"Leaving the Planet by Space Elevator\", states that \"The first country to deploy a space elevator will have a 95 percent cost advantage and could potentially control all space activities.\"\n\nThe International Space Elevator Consortium (ISEC) was formed to promote the development, construction, and operation of a space elevator as \"a revolutionary and efficient way to space for all humanity\". It was formed after the Space Elevator Conference in Redmond, Washington in July 2008 and became an affiliate organization with the National Space Society in August 2013.\n\nISEC coordinates with the two other major societies focusing on space elevators: the Japanese Space Elevator Association and EuroSpaceward. ISEC supports symposia and presentations at the International Academy of Astronautics and the International Astronautical Federation Congress each year. The organization published two issues of a peer-reviewed journal on space elevators called \"CLIMB\".\n\nISEC also conducts one-year studies focusing on individual topics. The process involves experts for one year of discussions on the topic of choice and culminates in a draft report that is presented and reviewed at the ISEC Space Elevator conference workshop. This review of the major conclusions allows input from space elevator enthusiasts as well as other experts. Topics that have concluded are: 2010 - Space Elevator Survivability, Space Debris Mitigation, 2012 - Space Elevator Concept of Operations, 2013 - Design Consideration for Tether Climbers, 2014 - Space Elevator Architectures and Roadmaps. 2015 - Design Characteristics of a Space Elevator Earth Port, 2017 - Design Considerations for the Space Elevator Apex Anchor and GEO Node.\nThe conventional current concept of a \"Space Elevator\" has evolved from a static compressive structure reaching to the level of GEO, to the modern baseline idea of a static tensile structure anchored to the ground and extending to well above the level of GEO. In the current usage by practitioners (and in this article), a \"Space Elevator\" means the Tsiolkovsky-Artsutanov-Pearson type as considered by the International Space Elevator Consortium. This conventional type is a static structure fixed to the ground and extending into space high enough that cargo can climb the structure up from the ground to a level where simple release will put the cargo into an orbit.\n\nSome concepts related to this modern baseline are not usually termed a \"Space Elevator\", but are similar in some way and are sometimes termed \"Space Elevator\" by their proponents. For example, Hans Moravec published an article in 1977 called \"A Non-Synchronous Orbital Skyhook\" describing a concept using a rotating cable. The rotation speed would exactly match the orbital speed in such a way that the tip velocity at the lowest point was zero compared to the object to be \"elevated\". It would dynamically grapple and then \"elevate\" high flying objects to orbit or low orbiting objects to higher orbit.\n\nThe original concept envisioned by Tsiolkovsky was a compression structure, a concept similar to an aerial mast. While such structures might reach space (100 km, 62 mi), they are unlikely to reach geostationary orbit. The concept of a Tsiolkovsky tower combined with a classic space elevator cable (reaching above the level of GEO) has been suggested. Other ideas use very tall compressive towers to reduce the demands on launch vehicles. The vehicle is \"elevated\" up the tower, which may extend as high as above the atmosphere, and is launched from the top. Such a tall tower to access near-space altitudes of has been proposed by various researchers.\n\nOther concepts for non-rocket spacelaunch related to a space elevator (or parts of a space elevator) include an orbital ring, a pneumatic space tower, a space fountain, a launch loop, a skyhook, a space tether, and a buoyant \"SpaceShaft\".\n\n"}
{"id": "140447", "url": "https://en.wikipedia.org/wiki?curid=140447", "title": "Stanford torus", "text": "Stanford torus\n\nThe Stanford torus is a proposed NASA design for a space habitat capable of housing 10,000 to 140,000 permanent residents.\n\nThe Stanford torus was proposed during the 1975 NASA Summer Study, conducted at Stanford University, with the purpose of exploring and speculating on designs for future space colonies (Gerard O'Neill later proposed his Island One or Bernal sphere as an alternative to the torus). \"Stanford torus\" refers only to this particular version of the design, as the concept of a ring-shaped rotating space station was previously proposed by Wernher von Braun and Herman Potočnik.\n\nIt consists of a torus, or doughnut-shaped ring, that is 1.8 km in diameter (for the proposed 10,000 person habitat described in the 1975 Summer Study) and rotates once per minute to provide between 0.9g and 1.0g of artificial gravity on the inside of the outer ring via centrifugal force.\n\nSunlight is provided to the interior of the torus by a system of mirrors, including a large non-rotating primary solar mirror.\n\nThe ring is connected to a hub via a number of \"spokes\", which serve as conduits for people and materials travelling to and from the hub. Since the hub is at the rotational axis of the station, it experiences the least artificial gravity and is the easiest location for spacecraft to dock. Zero-gravity industry is performed in a non-rotating module attached to the hub's axis.\n\nThe interior space of the torus itself is used as living space, and is large enough that a \"natural\" environment can be simulated; the torus appears similar to a long, narrow, straight glacial valley whose ends curve upward and eventually meet overhead to form a complete circle. The population density is similar to a dense suburb, with part of the ring dedicated to agriculture and part to housing.\n\nThe torus would require nearly 10 million tons of mass. Construction would use materials extracted from the Moon and sent to space using a mass driver. A mass catcher at L2 would collect the materials, transporting them to L5 where they could be processed in an industrial facility to construct the torus. Only materials that could not be obtained from the Moon would have to be imported from Earth. Asteroid mining was an alternative source of materials.\n\n\n"}
{"id": "433041", "url": "https://en.wikipedia.org/wiki?curid=433041", "title": "Technocapitalism", "text": "Technocapitalism\n\nTechnocapitalism (a portmanteau word combining \"technology\" and \"capitalism\") refers to changes in capitalism associated with the emergence of new technology sectors, the power of corporations, and new forms of organization.\n\nLuis Suarez-Villa, in his 2009 book \"Technocapitalism: A Critical Perspective on Technological Innovation and Corporatism\" argues that it is a new version of capitalism that generates new forms of corporate organization designed to exploit \"intangibles\" such as creativity and new knowledge. The new organizations, which he refers to as \"experimentalist organizations\" are deeply grounded in technological research, as opposed to manufacturing and services production. They are also heavily dependent on the corporate appropriation of research outcomes as intellectual property.\n\nThis approach is further developed by Suarez-Villa in his 2012 book \"Globalization and Technocapitalism: The Political Economy of Corporate Power and Technological Domination\", in which he relates the emergence of technocapitalism to globalization and to the growing power of technocapitalist corporations. Taking into account the new relations of power introduced by the corporations that control technocapitalism, he considers new forms of accumulation involving intangibles—such as creativity and new knowledge—along with intellectual property and technological infrastructure. This perspective on globalization—and the effect of technocapitalism and its corporations—also takes into account the growing global importance of intangibles, the inequalities created between nations at the vanguard of technocapitalism and those that are not, the increasing importance of brain-drain flows between nations, and the rise of what he refers to as a \"techno-military-corporate\" complex that is rapidly replacing the old military-industrial complex of the second half of the 20th century. \n\nThe concept behind technocapitalism is part of a line of thought that relates science and technology to the evolution of capitalism. At the core of this idea of the evolution of capitalism is that science and technology are not divorced from society—or that they exist in a vacuum, or in a separate reality of their own—out of reach of social action and human decision. Science and technology are part of society, and they are subject to the priorities of capitalism as much as any other human endeavor, if not more so. Prominent scientists in the early 20th century, such as John Bernal, posited that science has a social function, and cannot be seen as something apart from society. Other scientists at that time, such as John Haldane, related science to social philosophy, and showed how critical approaches to social analysis are very relevant to science, and to our understanding of the need for science. In our time, this line of thought has encouraged philosophers such as Andrew Feenberg to adopt and apply a critical theory approach to technology and science, providing many important insights on how scientific and technological decisions—and their outcomes—are shaped by society, and by capitalism and its institutions.\n\nThe term \"technocapitalism\" has been used by one author to denote aspects and ideas that diverge sharply from those explained above. Dinesh D'Souza, writing about Silicon Valley in an article, used the term to describe the corporate environment and venture capital relationships in a high tech-oriented local economy. His approach to the topic was consonant with that of business journals and the corporate management literature. Some newspaper articles have also used the term occasionally and in a very general sense, to denote the importance of advanced technologies in the economy.\n"}
{"id": "6327661", "url": "https://en.wikipedia.org/wiki?curid=6327661", "title": "Technology adoption life cycle", "text": "Technology adoption life cycle\n\nThe technology adoption lifecycle is a sociological model that describes the adoption or acceptance of a new product or innovation, according to the demographic and psychological characteristics of defined adopter groups. The process of adoption over time is typically illustrated as a classical normal distribution or \"bell curve\". The model indicates that the first group of people to use a new product is called \"innovators\", followed by \"early adopters\". Next come the early majority and late majority, and the last group to eventually adopt a product are called \"Laggards\" or \"phobics.\" For example, a phobic may only use a cloud service when it is the only remaining method of performing a required task, but the phobic may not have an in-depth technical knowledge of how to use the service.\n\nThe demographic and psychological (or \"psychographic\") profiles of each adoption group were originally specified by the North Central Rural Sociology Committee, Subcommittee for the Study of the Diffusion of Farm Practices, by agricultural researchers Beal and Bohlen in 1957.\nThe report summarized the categories as:\n\nThe model has subsequently been adapted for many areas of technology adoption in the late 20th century.\n\nThe model has spawned a range of adaptations that extend the concept or apply it to specific domains of interest.\n\nIn his book \"Crossing the Chasm\", Geoffrey Moore proposes a variation of the original lifecycle. He suggests that for discontinuous innovations, which may result in a Foster disruption based on s-curve, there is a gap or chasm between the first two adopter groups (innovators/early adopters), and the vertical markets.\n\nDisruption as it is used today are of the Clayton M. Christensen variety. These disruptions are not s-curve based. \n\nIn educational technology, Lindy McKeown has provided a similar model (a pencil metaphor) describing the ICT uptake in education. In medical sociology, Carl May has proposed normalization process theory that shows how technologies become embedded and integrated in health care and other kinds of organisation.\n\nWenger, White and Smith, in their book \"Digital habitats: Stewarding technology for communities\", talk of technology stewards: people with sufficient understanding of the technology available and the technological needs of a community to steward the community through the technology adoption process.\n\nRayna and Striukova (2009) propose that the choice of initial market segment has crucial importance for crossing the chasm, as adoption in this segment can lead to a cascade of adoption in the other segments. This initial market segment has, at the same time, to contain a large proportion of visionaries, to be small enough for adoption to be observed from within the segment and from other segment and be sufficiently connected with other segments. If this is the case, the adoption in the first segment will progressively cascade into the adjacent segments, thereby triggering the adoption by the mass-market.\n\nOne way to model product adoption is to understand that people's behaviours are influenced by their peers and how widespread they think a particular action is. For many format-dependent technologies, people have a non-zero payoff for adopting the same technology as their closest friends or colleagues. If two users both adopt product A, they might get a payoff \"a\" > 0; if they adopt product B, they get \"b\" > 0. But if one adopts A and the other adopts B, they both get a payoff of 0.\n\nA threshold can be set for each user to adopt a product. Say that a node v in a graph has d neighbors: then v will adopt product A if a fraction p of its neighbors is greater than or equal to some threshold. For example, if v's threshold is 2/3, and only one of its two neighbors adopts product A, then v will not adopt A. Using this model, we can deterministically model product adoption on sample networks.\n\nThe technology adoption lifecycle is a sociological model that is an extension of an earlier model called \"the diffusion process\", which was originally published in 1957 by Joe M. Bohlen, George M. Beal and Everett M. Rogers at Iowa State University and which was originally published only for its application to agriculture and home economics.\nbuilding on earlier research conducted there by Neal C. Gross and Bryce Ryan. Their original purpose was to track the purchase patterns of hybrid seed corn by farmers.\n\nBeal, Rogers and Bohlen together developed a model called \"the diffusion process\" and later, Everett Rogers generalized the use of it in his widely acclaimed book 1962 \"Diffusion of Innovations\" (now in its fifth edition), describing how new ideas and technologies spread in different cultures. Others have since used the model to describe how innovations spread between states in the U.S.\n\n"}
{"id": "504357", "url": "https://en.wikipedia.org/wiki?curid=504357", "title": "User-centered design", "text": "User-centered design\n\nUser-centered design (UCD) or user-driven development (UDD) is a framework of processes (not restricted to interfaces or technologies) in which usability goals, user characteristics, environment, tasks and workflow of a product, service or process are given extensive attention at each stage of the design process. User-centered design can be characterized as a multi-stage problem-solving process that not only requires designers to analyze and envision the way users are likely to consume a product, but also to validate their assumptions with regard to the user behavior in real world tests. These tests are conducted with/without actual users during each stage of the process from requirements, pre-production models and post production, completing a circle of proof back to and ensuring that \"development proceeds with the user as the center of focus.\" Such testing is necessary as it is often very difficult for the designers of a product to understand intuitively what a first-time user of their design experiences, and what each user's learning curve may look like. User-centered design is common in the design industry and when used is considered to lead to increased product usefulness and usability.\n\nThe chief difference from other product design philosophies is that user-centered design tries to optimize the product around how users can, want, or need to use the product, rather than forcing the users to change their behavior to accommodate the product. The users thus stand in the center of two concentric circles. The inner circle includes the context of the product, objectives of developing it and the environment it would run in. The outer circle involves more granular details of task detail, task organization, and task flow.\n\nThe term user-centered design was coined in Donald A. Norman's research laboratory in University of California, San Diego. The concept became widely popular after the publication of the book \"User-Centered System Design: New Perspectives on Human-Computer Interaction\" in 1986. The concept gained further attention and acceptance in his seminal book \"The Design of Everyday Things\" (originally called \"The Psychology of Everyday Things\"). In the book, Norman describes the psychology behind what he deems 'good' and 'bad' design through examples. He exalts the importance of design in our everyday lives, and the consequences of errors caused by bad designs. The books also include principles of building well-designed products. His recommendations are based on the needs of the user, leaving aside what he considers secondary issues like aesthetics. The main highlights of these are:\n\n\nNorman's overly reductive approach in the previous texts was readdressed by him later in his own publication \"Emotional Design\"..\n\nFor example, the user-centered design process can help software designers to fulfill the goal of a product engineered for their users. User requirements are considered right from the beginning and included into the whole product cycle. These requirements are noted and refined through investigative methods including: ethnographic study, contextual inquiry, prototype testing, usability testing and other methods. Generative methods may also be used including: card sorting, affinity diagramming and participatory design sessions. In addition, user requirements can be inferred by careful analysis of usable products similar to the product being designed.\n\nHere are principles that will ensure a design is user centered:\n\n\nThe goal of the User-Centered design is to make products which have very high usability. This includes how convenient the product is in terms of its usage, manageability, effectiveness and how well the product is mapped to the user requirements. Below are the general phases of User-Centered Design process:\nIn the next steps, the above procedure is repeated to further finish the product. These phases are general approaches and factors like design goals, team and their timeline, and environment in which the product is developed, determine the appropriate phases for a project and their order. You can either follow a waterfall model, agile model or any other software engineering practice.\n\nUCD asks questions about users and their tasks and goals, then uses the findings to make decisions about development and design.\nUCD of a web site, for instance, seeks to answer the following questions:\n\nAs examples of UCD viewpoints, the essential elements of UCD of a web site are considerations of visibility, accessibility, legibility and language.\n\nVisibility helps the user construct a mental model of the document. Models help the user predict the effect(s) of their actions while using the document. Important elements (such as those that aid navigation) should be emphatic. Users should be able to tell from a glance what they can and cannot do with the document.\n\nUsers should be able to find information quickly and easily throughout the document, regardless of its length. Users should be offered various ways to find information (such as navigational elements, search functions, table of contents, clearly labeled sections, page numbers, color-coding, etc.). Navigational elements should be consistent with the genre of the document. ‘Chunking' is a useful strategy that involves breaking information into small pieces that can be organized into some type meaningful order or hierarchy. The ability to skim the document allows users to find their piece of information by scanning rather than reading. Bold and italic words are often used.\n\nText should be easy to read: Through analysis of the rhetorical situation, the designer should be able to determine a useful font style. Ornamental fonts and text in all capital letters are hard to read, but italics and bolding can be helpful when used correctly. Large or small body text is also hard to read. (Screen size of 10-12 pixel sans serif and 12-16 pixel serif is recommended.)\nHigh figure-ground contrast between text and background increases legibility. Dark text against a light background is most legible.\n\nDepending on the rhetorical situation, certain types of language are needed. Short sentences are helpful, as are well-written texts used in explanations and similar bulk-text situations. Unless the situation calls for it, jargon or technical terms should not be used. Many writers will choose to use active voice, verbs (instead of noun strings or nominals), and simple sentence structure.\n\nA user-centered design is focused around the rhetorical situation. The rhetorical situation shapes the design of an information medium. There are three elements to consider in a rhetorical situation: Audience, Purpose, and Context.\n\nThe audience is the people who will be using the document. The designer must consider their age, geographical location, ethnicity, gender, education, etc.\n\nThe purpose is what the document targets or what problem the document is trying to address.\n\nThe context is the circumstances surrounding the situation. The context often answers the question: What situation has prompted the need for this document? Context also includes any social or cultural issues that may surround the situation.\n\nThere are a number of tools that are used in the analysis of user-centered design, mainly: personas, scenarios, and essential use cases.\n\nDuring the UCD process, a Persona representing the user may be created. A persona is a user archetype used to help guide decisions about product features, navigation, interactions, and even visual design. In most cases, personas are synthesized from a series of ethnographic interviews with real people, then captured in 1-2 page descriptions that include behavior patterns, goals, skills, attitudes, and environment, with a few fictional personal details to bring the persona to life.\n\nFor each product, or sometimes for each set of tools within a product, there is a small set of personas, one of whom is the primary focus for the design. There are also what's called a secondary persona, where the character is not the main target of the design, but their needs should be met and problems solved if possible. They exist to help account for further possible problems and difficulties that may occur even though the primary persona is satisfied with their solution. There is also an anti-persona, which is the character that the design is specifically not made for.\n\nPersonas are useful in the sense that they create a common shared understanding of the user group for which the design process is built around. Also, they help to prioritize the design considerations by providing a context of what the user needs and what functions are simply nice to add and have. They can also provide a human face and existence to a diversified and scattered user group, and can also create some empathy and add emotions when referring to the users. However, since personas are a generalized perception of the primary stakeholder group from collected data, the characteristics may be too broad and typical, or too much of an \"average Joe\". Sometimes, personas can have stereotypical properties also, which may hurt the entire design process. Overall, personas can be a useful tool to be used by designers to make informed design decisions around, opposed to referring to a set of data or a wide range of individuals.\n\nA scenario created in the UCD process is a fictional story about the \"daily life of\" or a sequence of events with the primary stakeholder group as the main character. Typically, a persona that was created earlier is used as the main character of this story. The story should be specific of the events happening that relate to the problems of the primary stakeholder group, and normally the main research questions the design process is built upon. These may turn out to be a simple story about the daily life of an individual, but small details from the events should imply details about the users, and may include emotional or physical characteristics. There can be the \"best-case scenario\", where everything works out best for the main character, the \"worst-case scenario\", where the main character experiences everything going wrong around him or her, and an \"average-case scenario\", which is the typical life of the individual, where nothing really special or really depressing occurs, and the day just moves on.\n\nScenarios create a social context in which the personas exist, and also create an actual physical world, instead of imagining a character with internal characteristics from gathered data and nothing else; there is more action involved in the persona's existence. A scenario is also more easily understood by people, since it is in the form of a story, and is easier to follow. Yet, like the personas, these scenarios are assumptions made by the researcher and designer, and is also created from a set of organized data. Some even say such scenarios are unrealistic to real life occurrences. Also, it is difficult to explain and inform low level tasks that occur, like the thought process of the persona before acting.\n\nIn short, a use case describes the interaction between an individual and the rest of the world. Each use case describes an event that may occur for a short period of time in real life, but may consist of intricate details and interactions between the actor and the world. It is represented as a series of simple steps for the character to achieve his or her goal, in the form of a cause-and effect scheme. Use cases are normally written in the form of a chart with two columns: first column labelled actor, second column labelled world, and the actions performed by each side written in order in the respective columns. The following is an example of a use case for performing a song on a guitar in front of an audience.\n\nThe interaction between actor and the world is an act that can be seen in everyday life, and we take them as granted and don't think too much about the small detail that needs to happen in order for an act like performing a piece of music to exist. It is similar to the fact that when speaking our mother tongue, we don't think too much about grammar and how to phrase words; they just come out since we are so used to saying them. The actions between an actor and the world, notably, the primary stakeholder (user) and the world in this case, should be thought about in detail, and hence use cases are created to understand how these tiny interactions occur.\n\nAn essential use case is a special kind of use case, also called an \"abstract use case.\" Essential use cases describe the essence of the problem, and deals with the nature of the problem itself. While writing use cases, no assumptions about unrelated details should be made. In additions, the goals of the subject should be separated from the process and implementation to reach that particular goal. Below is an example of an essential use case with the same goal as the former example.\n\nUse cases are useful because they help identify useful levels of design work. They allow the designers to see the actual low level processes that are involved for a certain problem, which makes the problem easier to handle, since certain minor steps and details the user makes are exposed. The designers' job should take into consideration of these small problems in order to arrive at a final solution that works. Another way to say this is that use cases breaks a complicated task into smaller bits, where these bits are useful units. Each bit completes a small task, which then builds up to the final bigger task. Like writing code on a computer, it is easier to write the basic smaller parts and make them work first, and then put them together to finish the larger more complicated code, instead to tackling the entire code from the very beginning.\n\nThe first solution is less risky because if something goes wrong with the code, it is easier to look for the problem in the smaller bits, since the segment with the problem will be the one that does not work, while in the latter solution, the programmer may have to look through the entire code to search for a single error, which proves time consuming. The same reasoning goes for writing use cases in UCD. Lastly, use cases convey useful and important tasks where the designer can see which one are of higher importance than others. Some drawbacks of writing use cases include the fact that each action, by the actor or the world, consist of little detail, and is simply a small action. This may possibly lead to further imagination and different interpretation of action from different designers.\n\nAlso, during the process, it is really easy to oversimplify a task, since a small task from a larger task may consist of even smaller tasks. Picking up a guitar may involve thinking of which guitar to pick up, which pick to use, and think about where the guitar is located first. These tasks may then be divided into smaller tasks, such as first thinking of what colour of guitar fits the place to perform the piece, and other related details. Tasks may be split further down into even tinier tasks, and it is up to the designer to determine what is a suitable place to stop splitting up the tasks. Tasks may not only be oversimplified, they may also be omitted in whole, thus the designer should be aware of all the detail and all the key steps that are involved in an event or action when writing use cases.\n\n\n"}
{"id": "24950820", "url": "https://en.wikipedia.org/wiki?curid=24950820", "title": "War and Peace in the Global Village", "text": "War and Peace in the Global Village\n\nWar and Peace in the Global Village by Marshall McLuhan and Quentin Fiore is a collage of images and text that illustrates the effects of electronic media and new technology on man. Marshall McLuhan used James Joyce's \"Finnegans Wake\" as a major inspiration for this study of war throughout history as an indicator as to how war may be conducted in the future. (1st Ed.: Bantam, NY; reissued by Gingko Press, 2001 ),\n\nJoyce's \"Wake\" is claimed to be a gigantic cryptogram which reveals a cyclic pattern for the whole history of man through its Ten Thunders. Each \"thunder\" below is a 100-character portmanteau of other words to create a statement he likens to an effect that each technology has on the society into which it is introduced. In order to glean the most understanding out of each, the reader must break the portmanteau into separate words (and many of these are themselves portmanteaus of words taken from multiple languages other than English) and speak them aloud for the spoken effect of each word. There is much dispute over what each portmanteau truly denotes.\n\nMcLuhan claims that the ten thunders in \"Wake\" represent different stages in the history of man:\n"}
{"id": "57534427", "url": "https://en.wikipedia.org/wiki?curid=57534427", "title": "XP-PEN", "text": "XP-PEN\n\nXP-Pen is founded in Japan in 2005, it specializes in graphics tablets, pen display monitors, light pads, stylus pens and digital graphical products. In 2008 they established an office in Taiwan. In 2013, XP-Pen Technology Co. was founded in the United States. In 2015 they opened their office in Shenzhen, China. \n\nIn December 2017, they were invited to DreamWorks campus in Glendale California. in October 2017, they exhibited in Stan Lee Comic Con during the Halloween weekend. In July 2017, they took part in Los Angeles' 25th Anime Expo.\n\nArtist series display\nXP-Pen supplies drivers for Windows 7, 8, 10 and Mac 10.8 and above.\n"}
