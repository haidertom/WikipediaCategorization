{"id": "20441402", "url": "https://en.wikipedia.org/wiki?curid=20441402", "title": "Alliance Ground Surveillance", "text": "Alliance Ground Surveillance\n\nAlliance Ground Surveillance (AGS) is a NATO programme to acquire an airborne ground surveillance capability (Multi-Platform Radar Technology Insertion Program on the Northrop Grumman RQ-4 Global Hawk).\n\nIn a similar fashion as with Strategic Airlift Capability the program is run by 15 NATO member states: Bulgaria, Czech Republic, Denmark, Estonia, Germany, Italy, Latvia, Lithuania, Luxembourg, Norway, Poland, Romania, Slovakia, Slovenia and the United States. AGS is scheduled to reach initial operational capability by the end of 2017. The Main Operating Base will be located at Sigonella Air Base, Italy.\n\nFirst Global Hawk Block 40 UAVs destined for the NATO AGS program rolled off Northrop Grumman production line in Palmdale Ca , 4 June 2015 1 arrived at Edwards on Dec. 19, 2015 completing its first flight. the rest stayed in plant 42 located in Palmdale CA\n\nIn July 2017, the USAF assigned the Mission Designation Series (MDS) of RQ-4D to the NATO AGS air vehicle.\n\n\n"}
{"id": "49449678", "url": "https://en.wikipedia.org/wiki?curid=49449678", "title": "Ami Dar", "text": "Ami Dar\n\nAmi Dar (born January 7, 1961) is the Founder and Executive Director of Idealist.org. Idealist serves more than 120,000 organizations around the world and has more than 1.4 million visitors every month between the English site (idealist.org), and the Spanish site (idealistas.org). \n\nDar was born January 7, 1961, in Jerusalem, Israel, the eldest of three children, to a school teacher mother and diplomat father. He grew up in Peru and Mexico, and it was in Mexico City where he first became aware of the contrast of wealth and poverty around him, which started him on a path of dedication to social justice. \n\nIn 1976, Dar and his family returned to Israel and from 1979 - 1982, he completed his mandatory service as a paratrooper. During this time Dar had an insight that fundamentally changed his thinking about borders and humanity and how labels define and separate people, which is often referred to as the \"sock sharers story.\" \n\nIn 1988 Dar joined Aladdin Knowledge Systems, a software company based in Tel Aviv. From 1988 to 1992 he served as the International Marketing Manager. In 1992 he was named President and he relocated to New York City to establish their North American branch. \n\nBy 1995 he had founded an early iteration of Idealist, The Contact Center Network sponsored meeting spaces in several communities where people could connect with neighbors who might share interests and ideas for local action. In 1996 Dar began calling the online network Idealist.org. \n\nIn January of 2018 the Idealist team launched Idealists of the World which has over 23,300 members world wide. The Idealists of the World participate in global Idealist Days which occur when the day and the month are the same number. The first global Idealist Day occurred on March 3rd, 2018 (3/3) with \"75 events mobilizing nearly 500 people around the world, and 3000 people in the Idealists of the World Facebook group.\" The philosophy behind Idealist Day is rooted in Ami Dar's vision of a world progressing towards positive social change. This is accomplished by people making connections online and in person, bridging divides to collaborate together for a better tomorrow. Regarding this vision Dar has stated,I can't help noticing that all over the world, behind every label and stereotype, there are people who share some basic values. And I can’t help thinking that if these people could somehow work together, the world would be a very different place. What are these values? Treating others the way we’d like to be treated is a good start. But we can go beyond that. For example, I believe that in every country and every culture there are many people who would agree with this sentence:\n\n\"“Working with others, in a spirit of generosity and mutual respect, I want to help build a world where all people can lead free and dignified lives.”\"\n\nThe Stern Family Fund awarded Dar a $100,000 Public Interest Pioneer grant (2000). \n\nDar was named an Ashoka Fellow (2004). Fellows are leading social entrepreneurs recognized to have innovative solutions to social problems and the potential to change patterns across society. They demonstrate unrivaled commitment to bold new ideas and prove that compassion, creativity, and collaboration are tremendous forces for change. Ashoka Fellows work in over 60 countries around the globe in every area of human need.\n\nHe is a Board Member Emeritus of the Nonprofit Technology Network (NTEN), the largest community of nonprofit professionals transforming technology into social change.\n\nDar was named \"Time Magazine's\" Philanthropy Innovator (2005).\n\nHe was recognized in the \"Nonprofit Times: Power and Influence\", 50 Most Influential People in 2000, 2002, 2003, 2004, and 2005.\n\nDar received Duke University's Fuqua School of Business, CASE Leadership in Social Entrepreneurship Award (2006).\n\nDar wrote the Forward to \"Nonprofit Management 101: A Complete and Practical Guide for Leaders and Professionals.\" Wiley.com. May 03, 2011.\n\nHe gave the Commencement Address for City University of New York (CUNY) School of Professional Studies in June 23, 2011.\n\nIn 2012, Dar was profiled in Forbes magazine. \n\nDar's career was profiled by Bloomberg in 2014.\n"}
{"id": "2293952", "url": "https://en.wikipedia.org/wiki?curid=2293952", "title": "Ancient technology", "text": "Ancient technology\n\nDuring the growth of the ancient civilizations, ancient technology was the result from advances in engineering in ancient times. These advances in the history of technology stimulated societies to adopt new ways of living and governance.\n\nThis article includes the advances in technology and the development of several engineering arts in historic times before the Middle Ages, which began after the fall of the Western Roman Empire in AD 476, the death of Justinian I in the 6th century, the coming of Islam in the 7th century, or the rise of Charlemagne in the 8th century. For technologies developed in medieval societies, see Medieval technology and Inventions in medieval Islam.\n\nA significant number of inventions were developed in the Islamic world; a geopolitical region that has at times extended from al-Andalus and Africa in the west to the Indian subcontinent and Malay Archipelago in the east. Many of these inventions had direct implications for Fiqh related issues.\n\nThey were one of the first Bronze age people in the world. Early on they used copper, bronze and gold, and later they used iron. Palaces were decorated with hundreds of kilograms of these very expensive metals. Also, copper, bronze, and iron were used for armor as well as for different weapons such as swords, daggers, spears, and maces.\n\nAccording to the assyriologist Stephanie Dalley, the earliest pump was the Archimedes' screw, first used by Sennacherib, King of Assyria, for the water systems at the Hanging Gardens of Babylon and Nineveh in the 7th century BCE. This attribution, however, is refuted by the historian of ancient water-lifting devices Olseon in the same paper, who still credits, as well as most other scholars, Archimedes with the invention.\n\nPerhaps the most important advance made by the Mesopotamians was the invention of writing by the Sumerians. With the invention of writing came the first recorded laws called the Code of Hammurabi as well as the first major piece of literature called the Epic of Gilgamesh.\n\nAlthough archaeologists don't know for sure who invented the wheel, the oldest wheel discovered was found in Mesopotamia. It is likely the Sumer first used the wheel in making pottery in 3500BC and used it for their chariots in around 3200 BC.\n\nThe Mesopotamians used a sexagesimal number system with the base 60 (like we use base 10). They divided time up by 60s including a 60-second minute and a 60-minute hour, which we still use today. They also divided up the circle into 360 degrees. They had a wide knowledge of mathematics including addition, subtraction, multiplication, division, quadratic and cubic equations, and fractions. This was important in keeping track of records as well as in some of their large building projects. The Mesopotamians had formulas for figuring out the circumference and area for different geometric shapes like rectangles, circles, and triangles. Some evidence suggests that they even knew the Pythagorean Theorem long before Pythagoras wrote it down. They may have even discovered the number for pi in figuring the circumference of a circle.\n\nBabylonian astronomy was able to follow the movements of the stars, planets, and the Moon. Application of advanced math predicted the movements of several planets. By studying the phases of the Moon, the Mesopotamians created the first calendar. It had 12 lunar months and was the predecessor for both the Jewish and Greek calendars.\n\nBabylonian medicine used logic and recorded medical history to be able to diagnose and treat illnesses with various creams and pills. Mesopotamians had two kinds of medical practices, magical and physical. Unlike today they would use both on the same patient. They didn’t see it as one as good and one as bad.\n\nThe Mesopotamians made many technological discoveries. They were the first to use the potter's wheel to make better pottery, they used irrigation to get water to their crops, they used bronze metal (and later iron metal) to make strong tools and weapons, and used looms to weave cloth from wool.\n\nFor later medieval technologies developed in the Mesopotamian region, now known as Iraq, see Inventions in medieval Islam.\n\nThe Egyptians invented and used many simple machines, such as the ramp to aid construction processes. They were among the first to extract gold by large-scale mining using fire-setting, and the first recognisable map, the Turin papyrus shows the plan of one such mine in Nubia.\n\nEgyptian paper, made from papyrus, and pottery were mass-produced and exported throughout the Mediterranean basin. The wheel, however, did not arrive until foreign invaders introduced the chariot. They developed Mediterranean maritime technology including ships and lighthouses. Early construction techniques utilized by the Ancient Egyptians made use of bricks composed mainly of clay, sand, silt, and other minerals. These constructs would have been vital in flood control and irrigation, especially along the Nile delta.\n\nFor later technologies in Ptolemaic Egypt, Roman Egypt, and Arab Egypt, see Ancient Greek technology, Roman technology and Inventions in medieval Islam respectively.\n\nTechnology in Africa has a history stretching to the beginning of the human species, stretching back to the first evidence of tool use by hominid ancestors in the areas of Africa where humans are believed to have evolved. Africa saw the advent of some of the earliest ironworking technology in the Aïr Mountains region of what is today Niger and the erection of some of the world's oldest monuments, pyramids and towers in Egypt, Nubia, and North Africa. In Nubia and ancient Kush, glazed quartzite and building in brick was developed to a greater extent than in Egypt. Parts of the East African Swahili Coast saw the creation of the world's oldest carbon steel creation with high-temperature blast furnaces created by the Haya people of Tanzania.\n\nThe history of science and technology in India dates back to the earliest civilizations of the world. The Indus Valley civilization yields evidence of mathematics, hydrography, metrology, and sewage collection and disposal being practiced by its inhabitants.\n\nThe Indus Valley Civilization, situated in a resource-rich area, is notable for its early application of city planning and sanitation technologies. Cities in the Indus Valley offer some of the first examples of closed gutters, public baths, and communal granaries. The Takshashila University was an important seat of learning in the ancient world. It was the center of education for scholars from all over Asia. Many Greek, Persian and Chinese students studied here under great scholars including Kautilya, Panini, Jivaka, and Vishnu Sharma.\nThe ancient system of medicine in India, Ayurveda was a significant milestone in Indian history. It mainly uses herbs as medicines. Its origins can be traced back to origin of Atharvaveda. The Sushruta Samhita (400 BC) by Sushruta has details about performing cataract surgery, plastic surgery, etc.\n\nAncient India was also at the forefront of seafaring technology - a panel found at Mohenjo-daro, depicts a sailing craft. Ship construction is vividly described in the Yukti Kalpa Taru, an ancient Indian text on Shipbuilding. (The Yukti Kalpa Taru had been translated and published by Prof. Aufrecht in his 'Catalogue of Sanskrit Manuscripts').\n\nIndian construction and architecture, called 'Vaastu Shastra', suggests a thorough understanding or materials engineering, hydrology, and sanitation. Ancient Indian culture was also pioneering in its use of vegetable dyes, cultivating plants including indigo and cinnabar. Many of the dyes were used in art and sculpture. The use of perfumes demonstrates some knowledge of chemistry, particularly distillation and purification processes.\n\nThe history of science and technology in China shows significant advances in science, technology, mathematics, and astronomy. The first recorded observations of comets, solar eclipses, and supernovae were made in China. Traditional Chinese medicine, acupuncture and herbal medicine were also practiced. The Four Great Inventions of China: the compass, gunpowder, papermaking, and printing were among the most important technological advances, only known in Europe by the end of the Middle Ages.\n\nAccording to the Scottish researcher Joseph Needham, the Chinese made many first-known discoveries and developments. Major technological contributions from China include early seismological detectors, matches, paper, the double-action piston pump, cast iron, the iron plough, the multi-tube seed drill, the suspension bridge, natural gas as fuel, the magnetic compass, the raised-relief map, the propeller, the crossbow, the south-pointing chariot, and gunpowder. Other Chinese discoveries and inventions from the Medieval period, according to Joseph Needham's research, include: block printing and movable type, phosphorescent paint, and the spinning wheel.\n\nThe solid-fuel rocket was invented in China about 1150 AD, nearly 200 years after the invention of black powder (which acted as the rocket's fuel). At the same time that the age of exploration was occurring in the West, the Chinese emperors of the Ming Dynasty also sent ships, some reaching Africa. But the enterprises were not further funded, halting further exploration and development. When Ferdinand Magellan's ships reached Brunei in 1521, they found a wealthy city that had been fortified by Chinese engineers, and protected by a breakwater. Antonio Pigafetta noted that much of the technology of Brunei was equal to Western technology of the time. Also, there were more cannons in Brunei than on Magellan's ships, and the Chinese merchants to the Brunei court had sold them spectacles and porcelain, which were rarities in Europe.\n\nThe Qanat, a water management system used for irrigation, originated in Iran before the Achaemenid period of Persia. The oldest and largest known qanat is in the Iranian city of Gonabad which, after 2,700 years, still provides drinking and agricultural water to nearly 40,000 people.\n\nIn the 7th century AD, Persians in Afghanistan developed the first practical windmills. For later medieval technologies developed in Islamic Persia, see Inventions in medieval Islam.\n\nAncient Greek technology developed at an unprecedented speed during the 5th century BC, continuing up to and including the Roman period, and beyond. Some inventions that are credited to the ancient Greeks are the following: the gear, screw, bronze casting techniques, water clock, water organ(hydraulis), torsion siege engine, and the use of steam to operate some experimental machines and toys. Many of these inventions occurred late in the Greek period, often inspired by the need to improve weapons and tactics in war.\n\nGreek and Hellenistic engineers invented many technologies and improved upon pre-existing technologies, particularly during the Hellenistic period. Heron of Alexandria invented a basic steam engine and demonstrated knowledge of mechanic and pneumatic systems. Archimedes invented several machines. The Greeks were unique in pre-industrial times in their ability to combine scientific research with the development of new technologies. One example is the Archimedean screw; this technology was first conceptualized in mathematics, then built. Other technologies invented by Greek scientists include the ballistae, the piston pump, and primitive analog computers like the Antikythera mechanism. Greek architects were responsible for the first true domes, and were the first to explore the Golden ratio and its relationship with geometry and architecture.\n\nApart from Hero of Alexandria's steam aeolipile, Hellenistic technicians were the first to invent watermills and windwheels, making them global pioneers in three of the four known means of non-human propulsion prior to the Industrial Revolution (the fourth being sails). However, only water power was used extensively in antiquity.\n\nOther Greek inventions include torsion catapults, pneumatic catapults, crossbows, cranes, rutways, organs, the keyboard mechanism, gears, differential gears, screws, refined parchment, showers, dry docks, diving bells, odometer and astrolabes. In architecture, Greek engineers constructed monumental lighthouses such as the Pharos and devised the first central heating systems. The Tunnel of Eupalinos is the earliest tunnel in history which has been excavated with a scientific approach from both ends.\n\nAutomata like vending machines, automatic doors and many other ingenious devices were first built by Hellenistic engineers as Ctesibius, Philo of Byzantium and Heron. Greek technological treatises were scrupuously studied and copied by later Byzantine, Arabic and Latin European scholars and provided much of the foundation for further technological advances in these civilizations.\n\nRoman technology supported Roman civilization and made the expansion of Roman commerce and Roman military possible over nearly a thousand years. The Roman Empire had an advanced set of technology for their time. Some of the Roman technology in Europe may have been lost during the turbulent eras of Late Antiquity and the Early Middle Ages. Roman technological feats in many different areas like: civil engineering, construction materials, transport technology, and some inventions such as the mechanical reaper went unmatched until the 19th century.\nRomans developed an intensive and sophisticated agriculture, expanded upon existing iron working technology, created laws providing for individual ownership, advanced stonemasonry technology, advanced road-building (exceeded only in the 19th century), military engineering, civil engineering, spinning and weaving and several different machines like the Gallic reaper that helped to increase productivity in many sectors of the Roman economy. They also developed water power through building aqueducts on a grand scale, using water not just for drinking supplies but also for irrigation, powering water mills and in mining. They used drainage wheels extensively in deep underground mines, one device being the reverse overshot water-wheel. They were the first to apply hydraulic mining methods for prospecting for metal ores, and for extracting those ores from the ground when found using a method known as hushing.\n\nRoman engineers build monumental arches, amphitheatres, aqueducts, public baths, true arch bridges, harbours, dams, vaults and domes on a very large scale across their Empire. Notable Roman inventions include the book (Codex), glass blowing and concrete. Because Rome was located on a volcanic peninsula, with sand which contained suitable crystalline grains, the concrete which the Romans formulated was especially durable. Some of their buildings have lasted 2000 years, to the present day. Roman society had also carried over the design of a door lock with tumblers and springs from Greece. Like many other aspects of innovation and culture that were carried on from Greece to Rome, the lines between where each one originated from have become skewed over time. These mechanisms were highly sophisticated and intricate for the era.\n\nRoman civilization was highly urbanized by pre-modern standards. Many cities of the Roman Empire had over 100,000 inhabitants with the capital Rome being the largest metropolis of antiquity. Features of Roman urban life included multistory apartment buildings called insulae, street paving, public flush toilets, glass windows and floor and wall heating. The Romans understood hydraulics and constructed fountains and waterworks, particularly aqueducts, which were the hallmark of their civilization. They exploited water power by building water mills, sometimes in series, such as the sequence found at Barbegal in southern France and suspected on the Janiculum in Rome. Some Roman baths have lasted to this day. The Romans developed many technologies which were apparently lost in the Middle Ages, and were only fully reinvented in the 19th and 20th centuries. They also left texts describing their achievements, especially Pliny the Elder, Frontinus and Vitruvius.\n\nOther less known Roman innovations include cement, boat mills, arch dams and possibly tide mills.\n\nLacking suitable beasts of burden and inhabiting domains often too mountainous or boggy for wheeled transport, the ancient civilizations of the Americas did not develop wheeled transport or the mechanics associated with animal power. Nevertheless, they produced advanced engineering including above ground and underground acqeducts, quake-proof masonry, artificial lakes, dykes, 'fountains,' pressurized water, road ways and complex terracing. Equally, gold-working commenced early in Peru (2000 BCE), and eventually copper, tin, lead and bronze were used. Although metallurgy did not spread to Mesoamerica until the Middle Ages, it was employed here and in the Andes for sophisticated alloys and gilding. The Native Americans developed a complex understanding of the chemical properties or utility of natural substances, with the result that a majority of the world's early medicinal drugs and edible crops, many important adhesives, paints, fibres, plasters, and other useful items were the products of these civilizations. Perhaps the best-known Mesoamerican invention was rubber, which was used to create rubber bands, rubber bindings, balls, syringes, 'raincoats,' boots, and waterproof insulation on containers and flasks.\n\n\n"}
{"id": "26139453", "url": "https://en.wikipedia.org/wiki?curid=26139453", "title": "Azimuth Systems", "text": "Azimuth Systems\n\nAzimuth Systems is a privately held company located near Boston, Massachusetts. The company’s primary products include wireless channel emulators and wireless test equipment for LTE, WiMAX, 2G/3G cellular and Wi-Fi networks.\n\nIn 2009 Azimuth Systems wrote a White paper entitled \"Improving 4G Wireless Broadband Product Design through Effective Channel Emulation Testing\".\n\nIn April 2010, Azimuth Systems was awarded the \"Best in Test\" award from \"Test & Measurement World\" Magazine. Also, in 2009, Azimuth Systems was awarded the \"4G Wireless Evolution LTE Visionary Award\"by TMC.net. In September 2016, Test solutions major Anritsu Corporation acquired Azimuth Systems.\n\n"}
{"id": "2572586", "url": "https://en.wikipedia.org/wiki?curid=2572586", "title": "Black level", "text": "Black level\n\nVideo black level is defined as the level of brightness at the darkest (black) part of a visual image or the level of brightness at which no light is emitted from a screen, resulting in a pure black screen.\n\nVideo displays generally need to be calibrated so that the displayed black is true to the black information in the video signal. If the black level is not correctly adjusted, visual information in a video signal could be displayed as black, or black information could be displayed as above black information (gray). \n\nThe voltage of the black level varies across different television standards. PAL sets the black level the same as the blanking level, while NTSC sets the black level approximately 54 mV above the blanking level. \n\nUser misadjustment of black level on monitors is common. It results in darker colors having their hue changed, it affects contrast, and in many cases causes some of the image detail to be lost.\n\nBlack level is set by displaying a testcard image and adjusting display controls. With CRT displays:\n\nIn digital video black level usually means the range of RGB values in video signal, which can be either [0..255] (typical of a computer output) or [16..235] (standard for video).\n\n"}
{"id": "5150737", "url": "https://en.wikipedia.org/wiki?curid=5150737", "title": "Cell of origin", "text": "Cell of origin\n\nCell of origin (COO) is a mobile-positioning technique for finding a caller's cell (the basic geographical coverage unit of a cellular telephone system) location.\n\nCrude COO positioning considers the location of the base station to be the location of the caller. This is not very accurate, as the majority of mobile network cells are projected from an antenna with a spread of 120° (i.e. three mounted on a mast to give complete coverage) giving a signal coverage area with the base station at one corner, rather than the centre. Omnidirectional cells may be used in rural locations (which typically have large ranges and hence uncertain locations for phones within them) and in cities (where they may have ranges of a few hundred metres). The underlying issue is that mobile phone networks are optimised for capacity and call handling rather than locating phones.\n\nMost commercially implemented systems rely on 'enhanced' COO. In the GSM system this relies on the fact that the phones constantly measure the signal strength from the closest 6 base stations and lock on to the strongest signal (the reality is slightly more complex than this and includes parameters that each individual network can optimise, including signal quality and variability. Most networks endeavour to optimise for minimum power consumption, but the overall effect approximates to each phone locking onto the strongest signal).\n\nAll networks generate 'splash maps' predicting signal coverage when planning and managing their networks. These maps can be processed to analyse the area which will be dominated by each base station and to approximate each area by a circle (the actual area of coverage may not be exactly where predicted... and in any case will be an irregular shape, rather than a circle).\n\nIn practice a network offering location services to third parties will present an API to which queries can be sent by validated users, to which a reply will be sent comprising the centre of a circle and a radius representing the expected error (the size of the circle in which the phone is expected to be).\n\nFor this reason, when precision is important COO is often used in conjunction with some other technology, such as the Global Positioning System (GPS) or Time of Arrival (TOA).\n\nCOO is the only positioning technique that is widely used in wireless networks and is used for Phase 1 of 911 service in the United States. \n\nLocation service using COO have been adopted by the emergency services in many countries. Commercial services have been slower to take off than many in the industry expected. One of the first services to make widespread use of COO based mobile location was the Zingo taxi hailing system, launched in London in 2003\n"}
{"id": "18010409", "url": "https://en.wikipedia.org/wiki?curid=18010409", "title": "Collaborative journalism", "text": "Collaborative journalism\n\nCollaborative journalism is a mode of journalism where multiple reporters or news organizations, without affiliation to a common parent organization, report on and contribute news items to a news story together. It is practiced by both professional and amateur reporters. It is not to be mixed up with citizen journalism.\n\nCollaborative journalism involves the aggregation of information from numerous individuals or organizations into a single news story. Information is gathered through research or reporting, or added when readers examine, comment and build upon existing stories. Stories from the mainstream media are often built upon. Depending on the system of collaboration, individuals may also provide feedback or vote on whether an article is newsworthy. A single collaborative news story, therefore, may encompass multiple authors, varying articles, and ranged perspectives.\n\nProfessional and amateur reporters may work together to develop collaborative news articles, or mainstream media sites may gather amateur blog posts to complement reporting.\n\nCollaborative journalists either contribute directly to stories, sometimes through a wiki-style collaboration platform, or build upon the story externally, often through personal blogs. Collaborative journalists develop or examine a story one piece at a time. This contrasts the deadline and completion-centered nature of traditional media. A story is built upon continually, and a popular story may receive daily updates. Through combined authorship, collaborative journalism is thought by some to offer an increased independence of thought and experience unavailable to traditional media.\n\nSuccessful collaborative journalism projects require a participatory community with respect for content. Ross Mayfield, CEO of SocialText, has commented on wiki-style collaborative journalism:\n\n\"Most user-generated content isn't content, but conversation. Cultivating community is a decided practice. It boils down to the social contract you make with your readers-turned-writers. If they trust that their effort and words will be appropriated appropriately, while providing social incentives for participation, it can very well work.\"\n\nCollaborative journalism emerged through the internet in the early 2000s, and developed gradually through various online outlets. As examples, Wikinews was founded in 2003, and NewsVine in 2005.\n\nThe Panama Papers project may be the largest example of a journalistic consortium to date. It began sometime in 2015 (date?) when Bastian Obermayer, an investigative reporter with the south German newspaper Süddeutsche Zeitung, was contacted by an anonymous source and offered the trove of 11.5 million electronic documents from Mossack Fonseca, the world’s fourth biggest offshore law firm detailing a web of secret offshore deals and loans worth billions of dollars, and details of tax avoidance designs in numerous countries. The newspaper's editors decided they could not handle the massive volume of information alone and initiated a collaborative journalistic consortium including more than 140 journalists and the International Consortium of Investigative Journalists, a project of the nonprofit Center for Public Integrity.\n\nThe European Investigative Collaborations (EIC) working with \"over 60 journalists in 14 countries\" published a \"series of articles called \"Football Leaks\"—the \"largest leak in sports history\". \"Football Leaks\" \"led to the prosecution of football superstar Cristiano Ronaldo and coach Jose Mourinho.\" EIC was established in the fall of 2015 with founding members that include \"Der Spiegel\", \"El Mundo\", \"Médiapart\", the Romanian Centre for Investigative Journalism (CRJI), and \"Le Soir\".\n\n\n\"Link journalism\", a phrase coined by Scott Karp in 2008, is \"a form of collaborative journalism in which a news story's writer provides external links within the story to reporting or other sources on the web.\" These links are meant to complement, enhance, or add context to the original reporting. Jeff Jarvis, from the Graduate School of Journalism's new media program at the City University of New York, has said that link journalism creates a \"new architecture of news.\"\n\nCollaborative journalism has been implemented in several different ways. Wikinews, the \"free-content online news source,\" lets any user edit or create a news story, similar in style to Wikipedia. Several mainstream news sites have adopted a collaborative journalism approach toward news, through use of news aggregation. The Washington Post has developed a political site which links to related content from other news sites. NBC links to local newspapers, radio broadcasts, online videos, and blogs on its local television stations' sites. The sites do not separate articles written by NBC staff and links to outside sources.\nThe New York Times has introduced a \"Times Extra\" website feature which acts posts links to outside news sites. Commenting on the launch of \"Times Extra\", Marc Frons, CTO for Digital Operations at the New York Times, said:\n\n“In the past, I think many news organizations were afraid to link to other Web sites out of fear that they might be sending people to an unreliable source or that their readers would never return. But those fears were largely misplaced and we’ve seen a much more open policy when it comes to pointing readers at useful content elsewhere on the Web.\"\nOther sites exhibit collaborative journalism through aggregation. On the site NewsVine, for example, wire stories from the Associated Press complement user-generated stories and blog posts. Reddit and other news aggregation sites may also act as collaborative journalism sites, depending on where content originates.\n\nDue to the increase in collaborative journalism, several organizations have begun to offer grants or awards for these types of projects. For example, Online Journalism Awards (launched in May 2000) added a new award category for collaborations and partnerships. Clean Energy Wire offers grants for collaborative journalism projects on the topic of energy or the climate. The annual Hostwriter Prize awards money to support pitches and published collaborative projects by journalists.\n\nCollaborative journalism has received some criticism:\n"}
{"id": "28393571", "url": "https://en.wikipedia.org/wiki?curid=28393571", "title": "Comparison of dosimeters", "text": "Comparison of dosimeters\n\nThe following table compares a features of dosimeters.\n"}
{"id": "8725303", "url": "https://en.wikipedia.org/wiki?curid=8725303", "title": "Comparison of high definition optical disc formats", "text": "Comparison of high definition optical disc formats\n\nThis article compares the technical specifications of multiple high definition formats, including HD DVD and Blu-ray Disc; two mutually incompatible, high definition optical disc formats that, beginning in 2006, attempted to improve upon and eventually replace the DVD standard. The two formats remained in a format war until February 19, 2008 when Toshiba, HD DVD's creator, announced plans to cease development, manufacturing and marketing of HD DVD players and recorders.\n\nOther high-definition optical disc formats were attempted, including the multi-layered red-laser Versatile Multilayer Disc and a Chinese made format called EVD. Both appear to have been abandoned by their respective developers.\n\na These maximum storage capacities apply to currently released media as of January 2012. First two layers of Blu-ray have a 25 GB capacity, but the triple layer disc adds a further 50 GB making 100 GB total. The fourth layer adds a further 28 GB. <br>\nb All HD DVD players are required to decode the two primary channels (left and right) of any Dolby TrueHD track; however, every Toshiba made stand-alone HD DVD player released thus far decodes 5.1 channels of TrueHD.<br>\nc On November 1, 2007 Secondary video and audio decoder became mandatory for new Blu-ray Disc players when the Bonus View requirement came into effect. However, players introduced to the market before this date can continue to be sold without Bonus View.<br>\nd There are some differences in the implementation of Dolby Digital Plus (DD+) on the two formats. On Blu-ray Disc, DD+ can only be used to extend a primary Dolby Digital (DD) 5.1 audiotrack. In this method 640 kbit/s is allocated to the primary DD 5.1 audiotrack (which is independently playable on players that do not support DD+), and up to 1 Mbit/s is allocated for the DD+ extension. The DD+ extension is used to replace the rear channels of the DD track with higher fidelity versions, along with adding additional channels for 6.1/7.1 audiotracks. On HD DVD, DD+ is used to encode all channels (up to 7.1), and no legacy DD track is required since all HD DVD players are required to decode DD+.<br>\ne On PAL DVDs, 24 frame per second content is stored as 50 interlaced frames per second and gets replayed 4% faster. This process can be reversed to retrieve the original 24 frame per second content. On NTSC DVDs, 24 frame per second content is stored as 60 interlaced frames per second using a process called 3:2 pulldown, which if done properly can also be reversed.<br>\nf As of July 2008, about 66.7% of Blu-ray discs are region free and 33.3% use region codes.<br>\ng DVD supports any valid MPEG-2 refresh rate as long as it is packaged with metadata converting it to 576i50 or 480i60, This metadata takes the form of REPEAT_FIRST_FIELD instructions embedded in the MPEG-2 stream itself, and is a part of the MPEG-2 standard. HD DVD is the only high-def disc format that can decode 1080p25 while Blu-ray and HD DVD can both decode 1080p24 and 1080p30. 1080p25 content can only be presented on Blu-ray as 1080i50.<br>\nh Linear PCM is the only lossless audio codec that is mandatory for both HD DVD and Blu-ray disc players, only HD DVD players are required to decode two lossless sound formats and those are Linear PCM and Dolby TrueHD. Dolby TrueHD and DTS-HD Master Audio have become sound format of choice for many studios on their Blu-ray titles but ever since Blu-ray won the format war, it has not become clear if they are now Mandatory for all new Blu-ray disc players since the end of the format war.\n\nBlu-ray Disc has a higher maximum disc capacity than HD DVD (50 GB vs. 30 GB for a double layered disc). In September 2007 the DVD Forum approved a preliminary specification for the triple-layer 51GB HD DVD (ROM only) disc though Toshiba never stated whether it was compatible with existing HD DVD players. In September 2006 TDK announced a prototype Blu-ray Disc with a capacity of 200GB. TDK was also the first to develop a Blu-ray prototype with a capacity of 100GB in May 2005. In October 2007 Hitachi developed a Blu-ray prototype with a capacity of 100GB. Hitachi has stated that current Blu-ray drives would only require a few firmware updates in order to play the disc.\n\nThe first 50 GB dual-layer Blu-ray Disc release was the movie \"Click\", which was released on October 10, 2006. As of July 2008, over 95% of Blu-ray movies/games are published on 50 GB dual layer discs with the remainder on 25 GB discs. 85% of HD DVD movies are published on 30 GB dual layer discs, with the remainder on 15 GB discs.\n\nThe choice of video compression technology (codec) complicates any comparison of the formats. Blu-ray Disc and HD DVD both support the same three video compression standards: MPEG-2, VC-1 and AVC, each of which exhibits different bitrate/noise-ratio curves, visual impairments/artifacts, and encoder maturity. Initial Blu-ray Disc titles often used MPEG-2 video, which requires the highest average bitrate and thus the most space, to match the picture quality of the other two video codecs. As of July 2008 over 70% of Blu-ray Disc titles have been authored with the newer compression standards: AVC and VC-1. HD DVD titles have used VC-1 and AVC almost exclusively since the format's introduction. Warner Bros., which used to release movies in both formats prior to June 1, 2007, often used the same encode (with VC-1 codec) for both Blu-ray Disc and HD DVD, with identical results. In contrast, Paramount used different encodings: initially MPEG-2 for early Blu-ray Disc releases, VC-1 for early HD DVD releases, and eventually AVC for both formats.\n\nWhilst the two formats support similar audio codecs, their usage varies. Most titles released on the Blu-ray format include Dolby Digital tracks for each language in the region, a DTS-HD Master Audio track for all 20th Century Fox and Sony Pictures and many upcoming Universal titles, Dolby TrueHD for Disney and Sony Pictures and some Paramount and Warner titles, and for many Blu-ray titles a Linear PCM track for the primary language. On the other hand, most titles released on the HD DVD format include Dolby Digital Plus tracks for each language in the region, and some also include a Dolby TrueHD track for the primary language.\n\nBoth Blu-ray Disc and HD DVD have two main options for interactivity (on-screen menus, bonus features, etc.).\n\nHD DVD's Standard Content is a minor change from standard DVD's subpicture technology, while Blu-ray's BDMV is completely new. This makes transitioning from standard DVD to Standard Content HD DVD relatively simple —for example, Apple's DVD Studio Pro has supported authoring Standard Content since version 4.0.3. For more advanced interactivity Blu-ray disc supports BD-J while HD DVD supports Advanced Content.\n\nBlu-ray Discs contain their data relatively close to the surface (less than 0.1 mm) which combined with the smaller spot size presents a problem when the surface is scratched as data would be destroyed. To overcome this, TDK, Sony, and Panasonic each have developed a proprietary scratch resistant surface coating. TDK trademarked theirs as Durabis, which has withstood direct abrasion by steel wool and marring with markers in tests.\n\nHD DVD uses traditional material and has the same scratch and surface characteristics of a regular DVD. The data is at the same depth (0.6 mm) as DVD as to minimize damage from scratching. As with DVD the construction of the HD DVD allows for a second side of either HD DVD or DVD.\n\nA study performed by Home Media Magazine (August 5, 2007) concluded that HD DVDs and Blu-ray discs are essentially equal in production cost. Quotes from several disc manufacturers for 25,000 units of HD DVDs and Blu-rays revealed a price differential of only 5-10 cents. (Lowest price: 90 cents versus 100 cents. Highest price: $1.45 versus $1.50.) Another study performed by Wesley Tech (February 9, 2007) arrived at a similar conclusion. Quotes for 10,000 discs show that a 15 gigabyte HD DVD costs $11,500 total, and 25 gigabyte Blu-ray or a 30 gigabyte HD DVD costs $13,000 total. For larger quantities of 100,000 units, the 30 gigabyte HD DVD was more expensive than the 25 gigabyte Blu-ray ($1.55 versus $1.49).\n\nAt the Consumer Electronics Show, on , Warner Bros. introduced a hybrid technology, Total HD, which would reportedly support both formats on a single disc. The new discs were to overlay the Blu-ray and HD DVD layers, placing them respectively and beneath the surface. The Blu-ray top layer would act as a two-way mirror, reflecting just enough light for a Blu-ray reader to read and an HD DVD player to ignore.\n\nLater that year, however, in September 2007, Warner President Ron Sanders said that the technology was on hold due to Warner being the only company who would publish on it.\n\nOne year after the original announcement, on 4 January 2008, Warner Bros. stated that it would support the Blu-ray format exclusively beginning on 1 June 2008, which, along with the demise of HD DVD the following month, ended development of hybrid discs permanently.\n\nThe primary copy protection system used on both formats is the Advanced Access Content System (AACS). Other copy protection systems include:\n\nThe Blu-ray specification and all currently available players support region coding. As of July 2008 about 66.7% of Blu-ray Disc titles are region-free and 33.3% use region codes.\n\nThe HD DVD specification had no region coding, so a HD DVD from anywhere in the world will work in any player. The DVD Forum's steering committee discussed a request from Disney to add it, but many of the 20 companies on the committee actively opposed it.\n\nSome film titles that were exclusive to Blu-ray in the United States such as Sony's \"xXx\", Fox's \"\" and \"The Prestige\", were released on HD DVD in other countries due to different distribution agreements; for example, \"The Prestige\" was released outside the U.S. by once format-neutral studio Warner Bros. Pictures. Since HD DVDs had no region coding, there are no restrictions playing foreign-bought HD DVDs in an HD DVD player.\n"}
{"id": "35969410", "url": "https://en.wikipedia.org/wiki?curid=35969410", "title": "Comparison of space station cargo vehicles", "text": "Comparison of space station cargo vehicles\n\nA number of different spacecraft have been used to carry cargo to and from space stations.\n\n"}
{"id": "56449548", "url": "https://en.wikipedia.org/wiki?curid=56449548", "title": "Cosmos (standard)", "text": "Cosmos (standard)\n\nCOSMOS stands for \"COSMetic Organic and Natural Standard\", which sets certification requirements for organic and natural cosmetics products in the Europe. The standard is recognized globally by the cosmetic industry. By adhering to specific guidelines, cosmetics marketers can use COSMOS signatures, which are registered trademarks, on packaging to confirm the products meet minimum industry requirements to be considered organic or natural.\n\nIn 2002 five European organisations responsible for setting organic and natural cosmetics standards met at a trade show to share ideas for broader standards to be used globally. These five COSMOS members are:\n\nOver 1,600 manufacturers who sell over 25,000 products across over 45 countries follow the standard, according to Cosmos-standard.org. About 85% of the certified cosmetics industry uses COSMOS signatures on its products.\n\nAlthough the five members differed on certain standards separately, they were able to smooth out differences to create a harmonised international standard that was first published in 2010. At this time the five members formed a non-profit international association overseeing the standard. In June 2010 the COSMOS-standard AISBL was awarded Royal Assent from Belgian authorities. The documents published with the standard include:\n\n\nThere are four main certification signatures that comprise the COSMOS-standard, which are for ORGANIC, NATURAL, COSMOS CERTIFIED and COSMOS APPROVED products. Here are six steps to gaining approval of product labeling within the certification process:\n\n\nManufacturers and marketers are only allowed to use COSMOS terms and signatures for products authorized by the certification body. The certification body must be identified on product labels if it is not clearly mentioned anywhere else on the product. In cases in which the label size restricts product labeling, the certification body may allow flexibility as long as the product maintains the general principles of the Labeling Guide. The firm must at least mention the nature of the certification (such as organic or natural) and the identity of the certification body.\n\nMarketers are allowed to use the COSMOS terms and signatures on company letterhead and websites under certain conditions. All the products of a brand must be certified organic in order for the company to make the claim they are \"COSMOS ORGANIC certified.\" Otherwise, the firm must be clear that only specific products have been certified organic. In other words, the use of the terms and signatures must not be misleading to the consumer.\n\nEssentially, the labelling must clearly and accurately describe the product, which must comply with the standard. The marketer must avoid listing ingredients or naming the product in a way that implies it contains certain ingredients that are not present. Any use or branding of the term \"organic,\" for example, must comply with the organic standard and not be confusing to the consumer.\n\nThe labeling must not confuse the terms \"organic\" and \"natural,\" which have separate definitions and certifications based on the way the products and ingredients are processed. If a brand sells several organic products and a few natural products, they must make it clear in their labeling and marketing the differences. The firm must also be clear if some of its products have no certification at all. In other words, in order for a company to promote itself as \"COSMOS ORGANIC certified,\" its entire range of products must meet the organic standard and be certified.\n\nCompanies are not allowed to use logos or seals that may mislead customers into believing the products are COSMOS certified.\n\n"}
{"id": "6890125", "url": "https://en.wikipedia.org/wiki?curid=6890125", "title": "Data auditing", "text": "Data auditing\n\nData auditing is the process of conducting a data audit to assess how company's data is fit for given purpose. This involves profiling the data and assessing the impact of poor quality data on the organization's performance and profits.\n"}
{"id": "58230778", "url": "https://en.wikipedia.org/wiki?curid=58230778", "title": "Dave's Redistricting", "text": "Dave's Redistricting\n\nDave's Redistricting is an online web app created by Dave Bradlee that allows anyone to redistrict a U.S. state's legislative districts.\n\nAccording to Bradlee, the software was designed to \"put power in people's hands,\" and so that they \"can see how the process works, so it's a little less mysterious than it was 10 years ago.\" Bradlee has noticed that many citizens are taking this process seriously and using his app to create legitimate redistricting maps that could be put in place. Some websites have called Bradlee the pioneer and cause of the rise of do-it-yourself redistricting.\n\nDave's Redistricting has frequently been mentioned as a resource that can be used to combat gerrymandering, given that the public has free access to it.\n\nPolitical science firms such as FiveThirtyEight have used the website to draw examples of gerrymandered districts.\n\nUsers can redraw the congressional districts for all 50 states with a given Cook PVI. With the use of PVI, any state can knowingly be gerrymandered to favor one political party over the other.\n\n"}
{"id": "3979717", "url": "https://en.wikipedia.org/wiki?curid=3979717", "title": "Ducks demo", "text": "Ducks demo\n\nThe Ducks demo was a tech demo that demonstrated the capabilities of the PlayStation 2 at E3 2000 and the PlayStation 3 at E3 2005. In the PlayStation 2 demo, only one duck is shown interacting with the water in the bathtub. In the PlayStation 3 demo, there are many ducks interacting with each other and their environment, as a visual representation of the leap in processing abilities from the PlayStation 2 Sony executives promise the PlayStation 3 will deliver. The Ducks demo was the basis for \"Super Rub 'a' Dub\" which is a downloadable title for the PS3 produced by SCEE.\n\nIn the demo, Phil Harrison shows the crowd a small bathtub with some rubber ducks in it. He uses these to demonstrate the real-time water ripples and dynamics. He then uses two toy pirate ships to demonstrate cloth dynamics. To show the Cell's processing power, he adds a huge amount of ducks to the tub (all reacting as they would in real-life, due to the advanced physics engine). Phil comments dryly that \"this is only possible with the use of LOD technology, for those of you that don't know, LOD stands for \"Lots of Ducks\"\n\nPhil Harrison then invites the creator of EyeToy to show the new technology the PlayStation 3 can use. Plugging a standard PlayStation 2 EyeToy into the PlayStation 3, he picks up two ordinary cups and proceeds to move the cups in a scooping motion, which scoops up the water pictured on screen. (animated and reacting as real water would).\n\nThe ducks demo was the basis for \"Super Rub 'a' Dub\" which is a downloadable title for the PS3 produced by SCEE.\n\n"}
{"id": "23021037", "url": "https://en.wikipedia.org/wiki?curid=23021037", "title": "E-OTD", "text": "E-OTD\n\nEnhanced Observed Time Difference (E-OTD) is a standard for the location of mobile telephones. The location method works by multilateration. The standardisation was first carried out for GSM by the GSM standard committees (T1P1.5 and ETIS) in LCS Release 98 and Release 99. The standardisation was continued for 3G and WCDMA mobile telephones by 3GPP.\n\nConceptually, the method is similar to U-TDOA, however, it involves time difference measurements being made in the handset rather than the network, and a mechanism to pseudo-synchronise the network. The handset makes an observation of the time difference of arrival of signals from two different base stations. These observations are known as Observed Time Difference (OTD). The handset measures the OTD between a number of different base stations. If the base stations were synchronised, then a single OTD defines a hyperbolic locus. A second, independent OTD, for which one of the observed base stations is spatially distinct from those in the first OTD, would provide a second hyperbolic locus, and the intersection of the two loci gives an estimate of the location of the mobile. If more than two independent OTDs are available, then the measurements can be combined to yield a more accurate measurement.\n\nHowever, GSM and 3G networks are not necessarily synchronised, so further information is needed. The E-OTD standard provides a method for pseudo-synchronisation. A Location Measurement Unit (LMU) can be used to estimate the transmission time offset between two base stations. This measurement is known as the Real Time Difference (RTD). The RTD for two base stations can then be subtracted from the OTD for the same two base stations to produce the Geometric Time Difference (GTD). The GTD is the time difference that would have been measured by the mobile if the network was perfectly synchronised. Accordingly, the application of the RTD provides a pseudo-synchronisation.\n\nAn LMU is a receiver that is placed in a position in the network that is able to report the RTDs of a number of different base stations. If the base station clocks are not synchronised to a common source, then it is necessary to continuously update the RTDs, as the time offsets will be changing due to the clock drift in each base station.\n\nThe deployment of LMUs can be expensive, and so is a drawback of E-OTD. However, a 2003 paper describes a method of operating E-OTD without LMUs, and presents results of an operational trial. In essence, if there are sufficient independent OTD measurements such that the equation system is over-determined, then the additional information can be used to estimate the RTDs.\n\nE-OTD was considered for the Enhanced 911 mandate, but ultimately was not a successful contender for this application. An active proponent and developer of E-OTD was Cambridge Positioning Systems (CPS). In 2007, CPS was acquired by CSR. In 2009, CSR merged with SIRF.\n\nBecause E-OTD requires a software modification to be included in the mobile phone, E-OTD positioning system has been less commonly used than the U-TDOA positioning system.\n\n\n"}
{"id": "12769634", "url": "https://en.wikipedia.org/wiki?curid=12769634", "title": "Ephemeralization", "text": "Ephemeralization\n\nEphemeralization, a term coined by R. Buckminster Fuller, is the ability of technological advancement to do \"more and more with less and less until eventually you can do everything with nothing,\" that is, an accelerating increase in the efficiency of achieving the same or more output (products, services, information, etc.) while requiring less input (effort, time, resources, etc.). Fuller's vision was that ephemeralization will result in ever-increasing standards of living for an ever-growing population despite finite resources. The concept has been embraced by those who argue against Malthusian philosophy.\n\nFuller uses Henry Ford's assembly line as an example of how ephemeralization can continuously lead to better products at lower cost with no upper bound on productivity. Fuller saw ephemeralization as an inevitable trend in human development. The progression was from \"compression\" to \"tension\" to \"visual\" to \"abstract electrical\" (i.e., nonsensorial radiation, such as radio waves, x rays, etc.).\n\nLength measurement technologies in human development, for example, started with a compressive measure, such as a ruler. The compressive technique reached an upper limit with a rod. For longer measures, a tensive measure such as a string or rope was used. This reached an upper limit with sagging of the string. Next was a surveyor’s telescope (visual). This reached an upper limit with curvature of the earth. Next was radio triangulation (abstract electrical). The technological progression is a continuing increase in length-measuring ability per pound of instrument, with no apparent upper limit according to Fuller.\n\nFrancis Heylighen and Alvin Toffler have written that ephemeralization, though it may increase our power to solve physical problems, can make non-physical problems worse. According to Heylighen and Toffler, increasing system complexity and information overload make it difficult and stressful for the people who must control the ephemeralized systems. This can negate the advantages of ephemeralization.\n\nThe solution proposed by Heylighen is the integration of human intelligence, computer intelligence, and coordination mechanisms that direct an issue to the cognitive resource (document, person, or computer program) most fit to address it. This requires a distributed, self-organizing system, formed by all individuals, computers and the communication links that connect them. The self-organization can be achieved by algorithms. According to Heylighen, the effect is to superpose the contributions of many different human and computer agents into a collective map that may link the cognitive and physical resources relatively efficiently. The resulting information system could react relatively rapidly and adaptively to requests for guidance or changes in the situation.\n\nIn Heylighen's view, the system could frequently be fed with new information from its myriad human users and computer agents, which it would take into account to offer the human users a list of the best possible approaches to achieve tasks. Heylighen believes near-optimization could be achieved both at the level of the individual who makes the request, and at the level of society which attempts to minimize the conflicts between the desires of its different members and to aim at long term, global progress while as much as possible protecting individual liberty and privacy.\n\n\n"}
{"id": "43751582", "url": "https://en.wikipedia.org/wiki?curid=43751582", "title": "Equivalent input", "text": "Equivalent input\n\nEquivalent input (also input-referred or input-related), is a method of referring to the signal or noise level at the output of a system as if it were an input to the same system. This is accomplished by removing all signal changes (e.g. amplifier gain, transducer sensitivity, etc.) to get the units to match the input.\n\nA microphone converts acoustical energy to electrical energy. Microphones have some level of electrical noise at their output. This noise may have contributions from random diaphragm movement, thermal noise, or a dozen other sources, but those can all be thought of as an imaginary acoustic noise source injecting sound into the (now noiseless) microphone. The units on this noise are no longer volts, but units of sound pressure (pascals or dBSPL), which can be directly compared to the desired sound pressure inputs.\n\nA device which uses a microphone may be susceptible to electromagnetic interference which causes sonic artifacts. The problem is not in the microphone, but the interference level can be \"related\" back to the input to compare to the level of typical inputs to see how audible the artifact is.\n"}
{"id": "54473092", "url": "https://en.wikipedia.org/wiki?curid=54473092", "title": "Euronorm", "text": "Euronorm\n\nA Euronorm is an international technical standard that has been recognized as applicable by the European Union. They may be identical to international standards of the ISO or IEC, or have editorial or technical content changes for applicability in the European Union, with changes annexed to the international standard, or may be originated by a European standards organization. Organizations recognized by EU regulations to establish standards are CEN, CENELEC and ETSI. A Euronorm becomes the equivalent of a national standard in all member countries, and replaces any prior conflicting national standard.\n"}
{"id": "46583121", "url": "https://en.wikipedia.org/wiki?curid=46583121", "title": "Existential risk from artificial general intelligence", "text": "Existential risk from artificial general intelligence\n\nExistential risk from artificial general intelligence is the hypothesis that substantial progress in artificial general intelligence (AGI) could someday result in human extinction or some other unrecoverable global catastrophe. For instance, the human species currently dominates other species because the human brain has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes \"superintelligent\", then this new superintelligence could become powerful and difficult to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.\n\nThe likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science. Once the exclusive domain of science fiction, concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as Stephen Hawking, Bill Gates, and Elon Musk.\n\nOne source of concern is that a sudden and unexpected \"intelligence explosion\" might take an unprepared human race by surprise. For example, in one scenario, the first-generation computer program found able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months of massively parallel processing time. The second-generation program is expected to take three months to perform a similar chunk of work, on average; in practice, doubling its own capabilities may take longer if it experiences a mini-\"AI winter\", or may be quicker if it undergoes a miniature \"AI Spring\" where ideas from the previous generation are especially easy to mutate into the next generation. In this scenario the system undergoes an unprecedently large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas. More broadly, examples like arithmetic and Go show that progress from human-level AI to superhuman ability is sometimes extremely rapid.\n\nA second source of concern is that controlling a superintelligent machine (or even instilling it with human-compatible values) may be an even harder problem than naïvely supposed. Some AGI researchers believe that a superintelligence would naturally resist attempts to shut it off, and that preprogramming a superintelligence with complicated human values may be an extremely difficult technical task. In contrast, skeptics such as Facebook's Yann LeCun argue that superintelligent machines will have no desire for self-preservation.\n\n\"\", the standard undergraduate AI textbook, assesses that superintelligence \"might mean the end of the human race\": \"Almost any technology has the potential to cause harm in the wrong hands, but with (superintelligence), we have the new problem that the wrong hands might belong to the technology itself.\" Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:\n\n\nAI systems uniquely add a third difficulty: the problem that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic \"learning\" capabilities may cause it to \"evolve into a system with unintended behavior\", even without the stress of new unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself, but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would not only need to be \"bug-free\", but it would need to be able to design successor systems that are also \"bug-free\".\n\nAll three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence labeled as \"malfunctioning\" correctly predicts that humans will attempt to shut it off, and successfully deploys its superintelligence to outwit such attempts.\n\nCiting major advances in the field of AI and the potential for AI to have enormous long-term benefits or costs, the 2015 Open Letter on Artificial Intelligence stated:\n\nThis letter was signed by a number of leading AI researchers in academia and industry, including AAAI president Thomas Dietterich, Eric Horvitz, Bart Selman, Francesca Rossi, Yann LeCun, and the founders of Vicarious and Google DeepMind.\n\nIn 1863 Darwin among the Machines, an essay by Samuel Butler stated: \n\nButler developed this into \"The Book of the Machines\", three chapters of \"Erewhon\", published anonymously in 1872. \nIn 1965, I. J. Good originated the concept now known as an \"intelligence explosion\":\n\nOccasional statements from scholars such as Alan Turing, from I. J. Good himself, and from Marvin Minsky expressed philosophical concerns that a superintelligence could seize control, but contained no call to action. In 2000, computer scientist and Sun co-founder Bill Joy penned an influential essay, \"Why The Future Doesn't Need Us\", identifying superintelligent robots as a high-tech dangers to human survival, alongside nanotechnology and engineered bioplagues.\n\nIn 2009, experts attended a private conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any sort of autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They concluded that self-awareness as depicted in science fiction is probably unlikely, but that there were other potential hazards and pitfalls. The New York Times summarized the conference's view as 'we are a long way from Hal, the computer that took over the spaceship in \"\"'\n\nNick Bostrom was the first person to suggest that an artificial general intelligence might deliberately exterminate humankind, and invention of artificial general intelligence is the explanation for the Fermi paradox. Bostrom's 2014 book om the artificial general intelligence question stimulated discussion. By 2015, public figures such as physicists Stephen Hawking and Nobel laureate Frank Wilczek, computer scientists Stuart J. Russell and Roman Yampolskiy, In April 2016, \"Nature\" warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control — and their interests might not align with ours.\"\n\nA superintelligent machine would be as alien to humans as human thought processes are to cockroaches. Such a machine may not have humanity's best interests at heart; it is not obvious that it would even care about human welfare at all. If superintelligent AI is possible, and if it is possible for a superintelligence's goals to conflict with basic human values, then AI poses a risk of human extinction. A \"superintelligence\" (a system that exceeds the capabilities of humans in every relevant endeavor) can outmaneuver humans any time its goals conflict with human goals; therefore, unless the superintelligence decides to allow humanity to coexist, the first superintelligence to be created will inexorably result in human extinction.\nThere is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore superintelligence is physically possible. In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal. The emergence of superintelligence, if or when it occurs, may take the human race by surprise, especially if some kind of intelligence explosion occurs. Examples like arithmetic and Go show that machines have already reached superhuman levels of competency in certain domains, and that this superhuman competence can follow quickly after human-par performance is achieved. One hypothetical intelligence explosion scenario could occur as follows: An AI gains an expert-level capability at certain key software engineering tasks. (It may initially lack human or superhuman capabilities in other domains not directly relevant to engineering.) Due to its capability to recursively improve its own algorithms, the AI quickly becomes superhuman; just as human experts can eventually creatively overcome \"diminishing returns\" by deploying various human capabilities for innovation, so too can the expert-level AI use either human-style capabilities or its own AI-specific capabilities to power through new creative breakthroughs. The AI then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field, including scientific creativity, strategic planning, and social skills. Just as the current-day survival of the gorillas is dependent on human decisions, so too would human survival depend on the decisions and goals of the superhuman AI.\n\nSome humans have a strong desire for power; others have a strong desire to help less fortunate humans. The former is a likely attribute of any sufficiently intelligent system; the latter cannot be assumed. Almost any AI, no matter its programmed goal, would rationally prefer to be in a position where nobody else can switch it off without its consent: A superintelligence will naturally gain self-preservation as a subgoal as soon as it realizes that it can't achieve its goal if it's shut off. Unfortunately, any compassion for defeated humans whose cooperation is no longer necessary would be absent in the AI, unless somehow preprogrammed in. A superintelligent AI will not have a natural drive to aid humans, for the same reason that humans have no natural desire to aid AI systems that are of no further use to them. (Another analogy is that humans seem to have little natural desire to go out of their way to aid viruses, termites, or even gorillas.) Once in charge, the superintelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems \"just to be on the safe side\" or for building additional computers to help it calculate how to best accomplish its goals.\n\nThus, the argument concludes, it is likely that someday an intelligence explosion will catch humanity unprepared, and that such an unprepared-for intelligence explosion may result in human extinction or a comparable fate.\n\nWhile there is no standardized terminology, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve the AI's set of goals, or \"utility function\". The utility function is a mathematical algorithm resulting in a single objectively-defined answer, not an English statement. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\"; however, they do not know how to write a utility function for \"maximize human flourishing\", nor is it currently clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function. AI researcher Stuart Russell writes:\n\nDietterich and Horvitz echo the \"Sorcerer's Apprentice\" concern in a \"Communications of the ACM\" editorial, emphasizing the need for AI systems that can fluidly and unambiguously solicit human input as needed.\n\nThe first of Russell's two concerns above is that autonomous AI systems may be assigned the wrong goals by accident. Dietterich and Horvitz note that this is already a concern for existing systems: \"An important aspect of any AI system that interacts with people is that it must reason about what people \"intend\" rather than carrying out commands literally.\" This concern becomes more serious as AI software advances in autonomy and flexibility. For example, in 1982, an AI named Eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable. The evolution resulted in a winning process that cheated: rather than create its own concepts, the winning process would steal credit from other processes.\n\nThe Open Philanthropy Project summarizes arguments to the effect that misspecified goals will become a much larger concern if AI systems achieve general intelligence or superintelligence. Bostrom, Russell, and others argue that smarter-than-human decision-making systems could arrive at more unexpected and extreme solutions to assigned tasks, and could modify themselves or their environment in ways that compromise safety requirements.\n\nIsaac Asimov's Three Laws of Robotics are one of the earliest examples of proposed safety measures for AI agents. Asimov's laws were intended to prevent robots from harming humans. In Asimov's stories, problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans. Citing work by Eliezer Yudkowsky of the Machine Intelligence Research Institute, Russell and Norvig note that a realistic set of rules and goals for an AI agent will need to incorporate a mechanism for learning human values over time: \"We can't just give a program a static utility function, because circumstances, and our desired responses to circumstances, change over time.\"\n\nMark Waser of the Digital Wisdom Institute recommends eschewing optimizing goal-based approaches entirely as misguided and dangerous. Instead, he proposes to engineer a coherent system of laws, ethics and morals with a top-most restriction to enforce social psychologist Jonathan Haidt's functional definition of morality: \"to suppress or regulate selfishness and make cooperative social life possible\". He suggests that this can be done by implementing a utility function designed to always satisfy Haidt’s functionality and aim to generally increase (but not maximize) the capabilities of self, other individuals and society as a whole as suggested by John Rawls and Martha Nussbaum.\n\nWhile current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify it, a sufficiently advanced, rational, \"self-aware\" AI might resist any changes to its goal structure, just as Gandhi would not want to take a pill that makes him want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and be able to prevent itself being \"turned off\" or being reprogrammed with a new goal.\n\nThere are some goals that almost any artificial intelligence might rationally pursue, like acquiring additional resources or self-preservation. This could prove problematic because it might put an artificial intelligence in direct competition with humans.\n\nCiting Steve Omohundro's work on the idea of instrumental convergence and \"basic AI drives\", Russell and Peter Norvig write that \"even if you only want your program to play chess or prove theorems, if you give it the capability to learn and alter itself, you need safeguards.\" Highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially, as competitors for limited resources. Building in safeguards will not be easy; one can certainly say in English, \"we want you to design this power plant in a reasonable, common-sense way, and not build in any dangerous covert subsystems\", but it's not currently clear how one would actually rigorously specify this goal in machine code.\n\nIn dissent, evolutionary psychologist Steven Pinker argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\" Computer scientists Yann LeCun and Stuart Russell disagree with one another whether superintelligent robots would have such AI drives; LeCun states that \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\", while Russell argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"\n\nOne common belief is that any superintelligent program created by humans would be subservient to humans, or, better yet, would (as it grows more intelligent and learns more facts about the world) spontaneously \"learn\" a moral truth compatible with human values and would adjust its goals accordingly. However, Nick Bostrom's \"orthogonality thesis\" argues against this, and instead states that, with some technical caveats, more or less any level of \"intelligence\" or \"optimization power\" can be combined with more or less any ultimate goal. If a machine is created and given the sole purpose to enumerate the decimals of formula_1, then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary. The machine may utilize all physical and informational resources it can to find every decimal of pi that can be found. Bostrom warns against anthropomorphism: A human will set out to accomplish his projects in a manner that humans consider \"reasonable\", while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, and may instead only care about the completion of the task.\n\nWhile the orthogonality thesis follows logically from even the weakest sort of philosophical \"is-ought distinction\", Stuart Armstrong argues that even if there somehow exist moral facts that are provable by any \"rational\" agent, the orthogonality thesis still holds: it would still be possible to create a non-philosophical \"optimizing machine\" capable of making decisions to strive towards some narrow goal, but that has no incentive to discover any \"moral facts\" that would get in the way of goal completion.\n\nOne argument for the orthogonality thesis is that some AI designs appear to have orthogonality built into them; in such a design, changing a fundamentally friendly AI into a fundamentally unfriendly AI can be as simple as prepending a onto its utility function. A more intuitive argument is to examine the strange consequences if the orthogonality thesis were false. If the orthogonality thesis is false, there exists some simple but \"unethical\" goal G such that there cannot exist any efficient real-world algorithm with goal G. This means if a human society were highly motivated (perhaps at gunpoint) to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail; that there cannot exist any pattern of reinforcement learning that would train a highly efficient real-world intelligence to follow the goal G; and that there cannot exist any evolutionary or environmental pressures that would evolve highly efficient real-world intelligences following goal G.\n\nSome dissenters, like Michael Chorost (writing in Slate), argue instead that \"by the time (the AI) is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\" Chorost argues that \"a (dangerous) A.I. will need to desire certain states and dislike others... Today's software lacks that ability—and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels.\"\n\nPart of the disagreement about whether a superintelligent machine would behave morally may arise from a terminological difference. Outside of the artificial intelligence field, \"intelligence\" is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning. At an extreme, if morality is part of the definition of intelligence, then by definition a superintelligent machine would behave morally. However, in the field of artificial intelligence research, while \"intelligence\" has many overlapping definitions, none of them make reference to morality. Instead, almost all current \"artificial intelligence\" research focuses on creating algorithms that \"optimize\", in an empirical way, the achievement of an arbitrary goal.\n\nTo avoid anthropomorphism or the baggage of the word \"intelligence\", an advanced artificial intelligence can be thought of as an impersonal \"optimizing process\" that strictly takes whatever actions are judged most likely to accomplish its (possibly complicated and implicit) goals. Another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function; this choice is then outputted, regardless of any extraneous ethical concerns.\n\nIn science fiction, an AI, even though it has not been programmed with human emotions, often spontaneously experiences those emotions anyway: for example, Agent Smith in The Matrix was influenced by a \"disgust\" toward humanity. This is fictitious anthropomorphism: in reality, while an artificial intelligence could perhaps be deliberately programmed with human emotions, or could develop something similar to an emotion as a means to an ultimate goal \"if\" it is useful to do so, it would not spontaneously develop human emotions for no purpose whatsoever, as portrayed in fiction.\n\nOne example of anthropomorphism would be to believe that your PC is angry at you because you insulted it; another would be to believe that an intelligent robot would naturally find a woman sexually attractive and be driven to mate with her. Scholars sometimes claim that others' predictions about an AI's behavior are illogical anthropomorphism. An example that might initially be considered anthropomorphism, but is in fact a logical statement about AI behavior, would be the Dario Floreano experiments where certain robots spontaneously evolved a crude capacity for \"deception\", and tricked other robots into eating \"poison\" and dying: here a trait, \"deception\", ordinarily associated with people rather than with machines, spontaneously evolves in a type of convergent evolution. According to Paul R. Cohen and Edward Feigenbaum, in order to differentiate between anthropomorphization and logical prediction of AI behavior, \"the trick is to know enough about how humans and computers think to say \"exactly\" what they have in common, and, when we lack this knowledge, to use the comparison to \"suggest\" theories of human thinking or computer thinking.\"\n\nThere is universal agreement in the scientific community that an advanced AI would not destroy humanity out of human emotions such as \"revenge\" or \"anger.\" The debate is, instead, between one side which worries whether AI might destroy humanity as an incidental action in the course of progressing towards its ultimate goals; and another side which believes that AI would not destroy humanity at all. Some skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power; proponents accuse some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms.\n\nSome sources argue that the ongoing weaponization of artificial intelligence could constitute a catastrophic risk. James Barrat, documentary filmmaker and author of \"Our Final Invention\", says in a Smithsonian interview, \"Imagine: in as little as a decade, a half-dozen companies and nations field computers that rival or surpass human intelligence. Imagine what happens when those computers become expert at programming smart computers. Soon we'll be sharing the planet with machines thousands or millions of times more intelligent than we are. And, all the while, each generation of this technology will be weaponized. Unregulated, it will be catastrophic.\"\n\nOpinions vary both on \"whether\" and \"when\" artificial general intelligence will arrive. At one extreme, AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\"; obviously this prediction failed to come true. At the other extreme, roboticist Alan Winfield claims the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical, faster than light spaceflight. Optimism that AGI is feasible waxes and wanes, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when AGI would arrive was 2040 to 2050, depending on the poll.\n\nSkeptics who believe it is impossible for AGI to arrive anytime soon, tend to argue that expressing concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about the impact of AGI, because of fears it could lead to government regulation or make it more difficult to secure funding for AI research, or because it could give AI research a bad reputation. Some researchers, such as Oren Etzioni, aggressively seek to quell concern over existential risk from AI, saying \"(Elon Musk) has impugned us in very strong language saying we are unleashing the demon, and so we're answering.\"\n\nIn 2014 Slate's Adam Elkus argued \"our 'smartest' AI is about as intelligent as a toddler—and only when it comes to instrumental tasks like information recall. Most roboticists are still trying to get a robot hand to pick up a ball or run around without falling over.\" Elkus goes on to argue that Musk's \"summoning the demon\" analogy may be harmful because it could result in \"harsh cuts\" to AI research budgets.\n\nThe Information Technology and Innovation Foundation (ITIF), a Washington, D.C. think-tank, awarded its Annual Luddite Award to \"alarmists touting an artificial intelligence apocalypse\"; its president, Robert D. Atkinson, complained that Musk, Hawking and AI experts say AI is the largest existential threat to humanity. Atkinson stated \"That's not a very winning message if you want to get AI funding out of Congress to the National Science Foundation.\" \"Nature\" sharply disagreed with the ITIF in an April 2016 editorial, siding instead with Musk, Hawking, and Russell, and concluding: \"It is crucial that progress in technology is matched by solid, well-funded research to anticipate the scenarios it could bring about... If that is a Luddite perspective, then so be it.\" In a 2015 \"Washington Post\" editorial, researcher Murray Shanahan stated that human-level AI is unlikely to arrive \"anytime soon\", but that nevertheless \"the time to start thinking through the consequences is now.\"\n\nSome scholars have proposed hypothetical scenarios intended to concretely illustrate some of their concerns.\n\nFor example, Bostrom in \"Superintelligence\" expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because: Bostrom suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents — a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars infer a broad lesson — the smarter the AI, the safer it is: Large and growing industries, widely seen as key to national economic competitiveness and military security, work with prestigious scientists who have built their careers laying the groundwork for advanced artificial intelligence. \"AI researchers have been working to get to human-level artificial intelligence for the better part of a century: of course there is no real prospect that they will now suddenly stop and throw away all this effort just when it finally is about to bear fruit.\" The outcome of debate is preordained; the project is happy to enact a few safety rituals, but only so long as they don't significantly slow or risk the project. \"And so we boldly go — into the whirling knives.\"\n\nIn Tegmark's \"Life 3.0\", a corporation's \"Omega team\" creates an extremely powerful AI able to moderately improve its own source code in a number of areas, but after a certain point the team chooses to publicly downplay the AI's ability, in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI in a box where it is mostly unable to communicate with the outside world, and tasks it to flood the market through shell companies, first with Amazon Turk tasks and then with producing animated films and TV shows. While the public is aware that the lifelike animation is computer-generated, the team keeps secret that the high-quality direction and voice-acting are also mostly computer-generated, apart from a few third-world contractors unknowingly employed as decoys; the team's low overhead and high output effectively make it the world's largest media empire. Faced with a cloud computing bottleneck, the team also tasks the AI with designing (among other engineering tasks) a more efficient datacenter and other custom hardware, which they mainly keep for themselves to avoid competition. Other shell companies make blockbuster biotech drugs and other inventions, investing profits back into the AI. The team next tasks the AI with astroturfing an army of pseudonymous citizen journalists and commentators, in order to gain political influence to use \"for the greater good\" to prevent wars. The team faces risks that the AI could try to escape via inserting \"backdoors\" in the systems it designs, via hidden messages in its produced content, or via using its growing understanding of human behavior to persuade someone into letting it free. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.\n\nIn contrast, top physicist Michio Kaku, an AI risk skeptic, posits a deterministically positive outcome. In \"Physics of the Future\" he asserts that \"It will take many decades for robots to ascend\" up a scale of consciousness, and that in the meantime corporations such as Hanson Robotics will likely succeed in creating robots that are \"capable of love and earning a place in the extended human family\".\n\nThe thesis that AI could pose an existential risk provokes a wide range of reactions within the scientific community, as well as in the public at large.\n\nIn 2004, law professor Richard Posner wrote that dedicated efforts for addressing AI can wait, but that we should gather more information about the problem in the meanwhile.\n\nMany of the opposing viewpoints share common ground. The Asilomar AI Principles, which contain only the principles agreed to by 90% of the attendees of the Future of Life Institute's Beneficial AI 2017 conference, agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\" AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane Terminator pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\" Conversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic Martin Ford states that \"I think it seems wise to apply something like Dick Cheyney's famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low — but the implications are so dramatic that it should be taken seriously\"; similarly, an otherwise skeptical \"Economist\" stated in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".\n\nDuring a 2016 Wired interview of President Barack Obama and MIT Media Lab's Joi Ito, Ito stated: Obama added:\n\nHillary Clinton stated in \"What Happened\":\nMany of the scholars who are concerned about existential risk believe that the best way forward would be to conduct (possibly massive) research into solving the difficult \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?\n\nA 2017 email survey of researchers with publications at the 2015 NIPS and ICML machine learning conferences asked them to evaluate Russell's concerns about AI risk. 5% said it was \"among the most important problems in the field,\" 34% said it was \"an important problem\", 31% said it was \"moderately important\", whilst 19% said it was \"not important\" and 11% said it was \"not a real problem\" at all.\n\nThe thesis that AI poses an existential risk, and that this risk is in need of much more attention than it currently commands, has been endorsed by many figures; perhaps the most famous are Elon Musk, Bill Gates, and Stephen Hawking. The most notable AI researcher to endorse the thesis is Stuart J. Russell. Endorsers sometimes express bafflement at skeptics: Gates states he \"can't understand why some people are not concerned\", and Hawking criticized widespread indifference in his 2014 editorial: \n\nThe thesis that AI can pose existential risk also has many strong detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God; at an extreme, Jaron Lanier argues that the whole concept that current machines are in any way intelligent is \"an illusion\" and a \"stupendous con\" by the wealthy.\n\nMuch of existing criticism argues that AGI is unlikely in the short term: computer scientist Gordon Bell argues that the human race will already destroy itself before it reaches the technological singularity. Gordon Moore, the original proponent of Moore's Law, declares that \"I am a skeptic. I don't believe (a technological singularity) is likely to happen, at least for a long time. And I don't know why I feel that way.\" Baidu Vice President Andrew Ng states AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"\n\nSome AI and AGI researchers may be reluctant to discuss risks, worrying that policymakers do not have sophisticated knowledge of the field and are prone to be convinced by \"alarmist\" messages, or worrying that such messages will lead to cuts in AI funding. \"Slate\" notes that some researchers are dependent on grants from government agencies such as DARPA.\n\nIn a YouGov poll of the public for the British Science Association, about a third of survey respondents said AI will pose a threat to the long term survival of humanity. Referencing a poll of its readers, Slate's Jacob Brogan stated that \"most of the (readers filling out our online survey) were unconvinced that A.I. itself presents a direct threat.\" Similarly, a SurveyMonkey poll of the public by USA Today found 68% thought the real current threat remains \"human intelligence\"; however, the poll also found that 43% said superintelligent AI, if it were to happen, would result in \"more harm than good\", and 38% said it would do \"equal amounts of harm and good\".\n\nAt some point in an intelligence explosion driven by a single AI, the AI would have to become vastly better at software innovation than the best innovators of the rest of the world; economist Robin Hanson is skeptical that this is possible.\n\nIn The Atlantic, James Hamblin points out that most people don't care one way or the other, and characterizes his own gut reaction to the topic as: \"Get out of here. I have a hundred thousand things I am concerned about at this exact moment. Do I seriously need to add to that a technological singularity?\" In a 2015 Wall Street Journal panel discussion devoted to AI risks, IBM's Vice-President of Cognitive Computing, Guruduth S. Banavar, brushed off discussion of AGI with the phrase, \"it is anybody's speculation.\" Geoffrey Hinton, the \"godfather of deep learning\", noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but stated that he continues his research because \"the prospect of discovery is too \"sweet\"\".\n\nThere is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise, and probably futile. Skeptics argue that regulation of AI would be completely valueless, as no existential risk exists. Almost all of the scholars who believe existential risk exists, agree with the skeptics that banning research would be unwise: in addition to the usual problem with technology bans (that organizations and individuals can offshore their research to evade a country's regulation, or can attempt to conduct covert research), regulating research of artificial intelligence would pose an insurmountable 'dual-use' problem: while nuclear weapons development requires substantial infrastructure and resources, artificial intelligence research can be done in a garage. Instead of trying to regulate technology itself, some scholars suggest to rather develop common norms including requirements for the testing and\ntransparency of algorithms, possibly in combination with some form of warranty.\n\nOne rare dissenting voice calling for some sort of regulation on artificial intelligence is Elon Musk. According to NPR, the Tesla CEO is \"clearly not thrilled\" to be advocating for government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid... As they should be.\" In response, politicians express skepticism about the wisdom of regulating a technology that's still in development. Responding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO Brian Krzanich argues that artificial intelligence is in its infancy and that it's too early to regulate the technology.\n\nInstitutions such as the Machine Intelligence Research Institute, the Future of Humanity Institute, the Future of Life Institute, the Centre for the Study of Existential Risk, and the Center for Human-Compatible AI are currently involved in mitigating existential risk from advanced artificial intelligence, for example by research into friendly artificial intelligence.\n\n"}
{"id": "54620481", "url": "https://en.wikipedia.org/wiki?curid=54620481", "title": "FIRST Global", "text": "FIRST Global\n\nFIRST Global (For Inspiration and Recognition of Science and Technology) is a trade name for a nonprofit organization, the International First Committee Association. It promotes STEM education and careers for youth through Olympics-style robotics competitions called the FIRST Global Challenge. It was founded by Dean Kamen in 2016 as an expansion of FIRST, an organization with similar objectives. \n\nFIRST Global is a trade name for the International First Committee Association, a nonprofit corporation based in Manchester, New Hampshire, with a 501(c)3 designation from the IRS.\n\nIt was founded by the co-founder of FIRST, Dean Kamen, with the objective of promoting STEM education and careers in the developing world through Olympics-style robotics competitions.\n\nEach year, FIRST Global announces the host city of the next year's Challenge during that year's closing ceremony. For example, during the closing ceremony of the 2017 FIRST Global Challenge, entrepreneur Ricardo Salinas, a founding member of FIRST Global, announced that Mexico City would host the 2018 FIRST Global Challenge.\nThe 2017 FIRST Global Challenge was held in Washington, D.C., from July 17–19, and the challenge was the use of robots to separate different colored balls, representing clean water and impurities in water, symbolizing the Engineering Grand Challenge (based on the Millennium Development Goal) of improving access to clean water in the developing world. Around 160 teams composed of 15- to 18-year-olds from 157 countries participated, and around 60% of teams were created or led by young women. Six continental teams also participated.\n\nThe 2018 FIRST Global Challenge was held in Mexico City from August 15-18. The 2018 Challenge is called Energy Impact and explores the impact of various types of energy on the world and how they can be made more sustainable. In the challenge, robots work together in teams of three to give cubes to human players, turn a crank, and score cubes in goals in order to generate electrical power. The challenge is based on three Engineering Grand Challenges; making solar energy affordable, making fusion energy a reality, and creating carbon sequestration methods.\n\nThe Global STEM Corps is a FIRST Global initiative that connects qualified volunteer mentors with students in developing countries to prepare them for competitions.\n"}
{"id": "49361513", "url": "https://en.wikipedia.org/wiki?curid=49361513", "title": "Flowtite Technology", "text": "Flowtite Technology\n\nFlowtite Technology AS is a Norwegian technology company, owned by Amiantit Group. It develops GRP pipe manufacturing technology and designs tailor-made manufacturing equipment under the brand name \"Flowtite\". Its predecessor, Vera Fabrikker, was the first company in the world to utilize a continuous filament winding machine for production of glassfibre-reinforced plastic pipes (GRP pipes) and to invent the corresponding manufacturing process, commonly known as continuous filament winding process or Drostholm process.\n\nThe history of Flowtite Technology can be traced back to 1929 when the owners of Jotun established a manufacturing plant for vegetable oils, called Vera Fabrikker, in Sandefjord, Norway. In May 1966 engineers from Vera Fabrikker met with Frede Hilmar Drostholm at an exhibition in Copenhagen, Denmark. The company Drostholm Ltd presented a prototype of a continuous winding machine for production of GRP pipes — an invention of Drostholm’s partner, Peder Ulrik Poulsen. Vera Fabrikker purchased this machine with a goal to develop a process enabling the machine to produce GRP pipes and tank shells. After a year of unsuccessful experimenting, a second invention was made — a steel band was added to the machine and in 1968 Vera Fabrikker started producing GRP pipes and tanks. In 1971 the GRP technology was bought by Owens Corning, USA. Two years later, in 1977, Owens Corning together with Jotun established a new company — Veroc Technology AS (the last two letters in Veroc stand for Owens Corning). It was responsible for the development of GRP pipes and tanks, as well as manufacturing and installing production equipment for the licensed GRP pipe producers all over the world. In 1993 Owens Corning took over Veroc Technology 100%. Later, in 1998, the company name was changed to Flowtite Technology AS, as it is known today. In 2001 Flowtite Technology AS was acquired by the Saudi Arabian Amiantit Company (Amiantit Group) and became its main technology centre in 2006.\n\nThe brand Flowtite and its logo was created in the 1970s by Owens Corning, when the company produced GRP pipes under the trademark Flowtite. The trademark was first registered in the USA in 1975, then progressively across the world. It is composed of the words “flow” and “tite” (a dialect version of “tight”) and represents a steady movement of liquids through a compact and sealed medium. Two circles of the Flowtite logotype symbolize a pipe containing water flow.\n\nFlowtite pipes can only be produced with a custom-designed production equipment, which has a basic filament winding machine as a basis. It consists of a continuous steel band mandrel, supported by beams installed circumferentially to form a cylindrical shape. The mandrel moves continuously in a spiral path towards the exit assembly. As the mandrel rotates, all composite materials are metered onto it in precise amounts, forming one layer after another. As a pipe is being formed on the mandrel, it moves forward to the curing and cutting areas and the finished product comes out at the end of one continuous production process.\n\nFlowtite GRP pipes are used in various industries and multiple applications, such as:\n\n"}
{"id": "21182020", "url": "https://en.wikipedia.org/wiki?curid=21182020", "title": "History of paper", "text": "History of paper\n\nPaper, a thin unwoven material made from milled plant fibers, is primarily used for writing, artwork, and packaging; it is commonly white. The first papermaking process was documented in China during the Eastern Han period (25–220 CE), traditionally attributed to the court official Cai Lun. During the 8th century, Chinese papermaking spread to the Islamic world, where pulp mills and paper mills were used for papermaking and money making. By the 11th century, papermaking was brought to Europe. By the 13th century, papermaking was refined with paper mills utilizing waterwheels in Spain. Later European improvements to the papermaking process came in the 19th century with the invention of wood-based papers.\n\nAlthough precursors such as papyrus and amate existed in the Mediterranean world and pre-Columbian Americas, respectively, these materials are not defined as true paper. Nor is true parchment considered paper; used principally for writing, parchment is heavily prepared animal skin that predates paper and possibly papyrus. In the twentieth century with the advent of plastic manufacture some plastic \"paper\" was introduced, as well as paper-plastic laminates, paper-metal laminates, and papers infused or coated with different products that give them special properties. \n\nThe word \"paper\" is etymologically derived from \"papyrus\", Ancient Greek for the \"Cyperus papyrus\" plant. Papyrus is a thick, paper-like material produced from the pith of the \"Cyperus papyrus\" plant which was used in ancient Egypt and other Mediterranean societies for writing long before paper was used in China.\n\nPapyrus is prepared by cutting off thin ribbon-like strips of the interior of the \"Cyperus papyrus\", and then laying out the strips side-by-side to make a sheet. A second layer is then placed on top, with the strips running at right angle to the first. The two layers are the pounded together into a sheet. The result is very strong, but has an uneven surface, especially at the edges of the strips. When used in scrolls, repeated rolling and unrolling causes the strips to come apart again, typically along vertical lines. This effect can be seen in many ancient papyrus documents.\n\nPaper contrasts with papyrus in that the plant material is broken down through maceration or disintegration before the paper is pressed. This produces a much more even surface, and no natural weak direction in the material which falls apart over time.\n\nArchaeological evidence of papermaking predates the traditional attribution given to Cai Lun, an imperial eunuch official of the Han dynasty (202 BCE – CE 220), thus the exact date or inventor of paper can not be deduced. The earliest extant paper fragment was unearthed at Fangmatan in Gansu province, and was likely part of a map, dated to 179–141 BCE. Fragments of paper have also been found at Dunhuang dated to 65 BCE and at Yumen pass, dated to 8 BCE.\n\n\"Cai Lun's\" invention, recorded hundreds of years after it took place, is dated to 105 CE. The innovation is a type of paper made of mulberry and other bast fibres along with fishing nets, old rags, and hemp waste which reduced the cost of paper production, which prior to this, and later, in the West, depended solely on rags.\n\nDuring the Shang (1600–1050 BCE) and Zhou (1050–256 BCE) dynasties of ancient China, documents were ordinarily written on bone or bamboo (on tablets or on bamboo strips sewn and rolled together into scrolls), making them very heavy, awkward, and hard to transport. The light material of silk was sometimes used as a recording medium, but was normally too expensive to consider. The Han dynasty Chinese court official Cai Lun (c. 50–121 CE) is credited as the inventor of a method of papermaking (inspired by wasps and bees) using rags and other plant fibers in 105 CE. However, the discovery of specimens bearing written Chinese characters in 2006 at Fangmatan in north-east China's Gansu Province suggests that paper was in use by the ancient Chinese military more than 100 years before Cai, in 8 BCE, and possibly much earlier as the map fragment found at the Fangmatan tomb site dates from the early 2nd century BCE. It therefore would appear that \"Cai Lun's contribution was to improve this skill systematically and scientifically, fix a recipe for papermaking\".\n\nThe record in the \"Twenty-Four Histories\" says\n\nThe production process may have originated from the practice of pounding and stirring rags in water, after which the matted fibres were collected on a mat. The bark of paper mulberry was particularly valued and high quality paper was developed in the late Han period using the bark of \"tan\" (檀; sandalwood). In the Eastern Jin period a fine bamboo screen-mould treated with insecticidal dye for permanence was used in papermaking. After printing was popularized during the Song dynasty the demand for paper grew substantially. In the year 1101, 1.5 million sheets of paper were sent to the capital.\n\nAmong the earliest known uses of paper was padding and wrapping delicate bronze mirrors according to archaeological evidence dating to the reign of Emperor Wu of Han from the 2nd century BCE. Padding doubled as both protection for the object as well as the user in cases where poisonous \"medicine\" were involved, as mentioned in the official history of the period. Although paper was used for writing by the 3rd century CE, paper continued to be used for wrapping (and other) purposes. Toilet paper was used in China from around the late 6th century. In 589, the Chinese scholar-official Yan Zhitui (531–591) wrote: \"Paper on which there are quotations or commentaries from Five Classics or the names of sages, I dare not use for toilet purposes\". An Arab traveler who visited China wrote of the curious Chinese tradition of toilet paper in 851, writing: \"... [the Chinese] do not wash themselves with water when they have done their necessities; but they only wipe themselves with paper\".\n\nDuring the Tang dynasty (618–907) paper was folded and sewn into square bags to preserve the flavor of tea. In the same period, it was written that tea was served from baskets with multi-colored paper cups and paper napkins of different size and shape. During the Song dynasty (960–1279) the government produced the world's first known paper-printed money, or banknote (\"see Jiaozi and Huizi\"). Paper money was bestowed as gifts to government officials in special paper envelopes. During the Yuan dynasty (1271–1368), the first well-documented Europeans in Medieval China, the Venetian merchant Marco Polo remarked how the Chinese burned paper effigies shaped as male and female servants, camels, horses, suits of clothing and armor while cremating the dead during funerary rites.\n\nAccording to Timothy Hugh Barrett, paper played a pivotal role in early Chinese written culture, and a \"strong reading culture seems to have developed quickly after its introduction, despite political fragmentation.\" Indeed the introduction of paper had immense consequences for the book world. It meant books would no longer have to be circulated in small sections or bundles, but in their entirety. Books could now be carried by hand rather than transported by cart. As a result individual collections of literary works increased in the following centuries.\n\nTextual culture seems to have been more developed in the south by the early 5th century, with individuals owning collections of several thousand scrolls. In the north an entire palace collection might have been only a few thousand scrolls in total. By the early 6th century, scholars in both the north and south were capable of citing upwards of 400 sources in commentaries on older works. A small compilation text from the 7th century included citations to over 1,400 works.\n\nThe personal nature of texts was remarked upon by a late 6th century imperial librarian. According to him, the possession of and familiarity with a few hundred scrolls was what it took to be socially accepted as an educated man. \n\nAccording to Endymion Wilkinson, one consequence of the rise of paper in China was that \"it rapidly began to surpass the Mediterranean empires in book production.\" During the Tang dynasty, China became the world leader in book production. In addition the gradual spread of woodblock printing from the late Tang and Song further boosted their lead ahead of the rest of the world.\n\nHowever despite the initial advantage afforded to China by the paper medium, by the 9th century its spread and development in the middle east had closed the gap between the two regions. Between the 9th to early 12th centuries, libraries in Cairo, Baghdad, and Cordoba held collections larger than even the ones in China, and dwarfed those in Europe. From about 1500 the maturation of paper making and printing in Southern Europe also had an effect in closing the gap with the Chinese. The Venetian Domenico Grimani's collection numbered 15,000 volumes by the time of his death in 1523. After 1600, European collections completely overtook those in China. The Bibliotheca Augusta numbered 60,000 volumes in 1649 and surged to 120,000 in 1666. In the 1720s the Bibliotheque du Roi numbered 80,000 books and the Cambridge University 40,000 in 1715. After 1700, libraries in North America also began to overtake those of China, and toward the end of the century, Thomas Jefferson's private collection numbered 4,889 titles in 6,487 volumes. The European advantage only increased further into the 19th century as national collections in Europe and America exceeded a million volumes while a few private collections, such as that of Lord Action, reached 70,000.\n\nPaper became central to the three arts of China – poetry, painting, and calligraphy. In later times paper constituted one of the 'Four Treasures of the Scholar's Studio,' alongside the brush, the ink, and the inkstone.\n\nAfter its origin in central China, the production and use of paper spread steadily. It is clear that paper was used at Dunhuang by 150 CE, in Loulan in the modern-day province of Xinjiang by 200, and in Turpan by 399. Paper was concurrently introduced in Japan sometime between the years 280 and 610.\n\nPaper spread to Vietnam in the 3rd century.\n\nPaper spread to Korea in the 4th century.\n\nPaper spread to Japan in the 5th century.\n\nPaper spread to India in the 7th century. However, the use of paper was not widespread there until the 12th century.\n\nAfter the defeat of the Chinese in the Battle of Talas in 751 (present day Kyrgyzstan), the invention spread to the Middle East.\n\nThe legend goes, the secret of papermaking was obtained from two Chinese prisoners from the Battle of Talas, which led to the first paper mill in the Islamic world being founded in Samarkand in Sogdia (modern-day Uzbekistan). There was a tradition that Muslims would release their prisoners if they could teach ten Muslims any valuable knowledge. There are records of paper being made at Gilgit in Pakistan by the sixth century, in Samarkand by 751, in Baghdad by 793, in Egypt by 900, and in Fes, Morocco around 1100.\n\nThe laborious process of paper making was refined and machinery was designed for bulk manufacturing of paper. Production began in Baghdad, where a method was invented to make a thicker sheet of paper, which helped transform papermaking from an art into a major industry. The use of water-powered pulp mills for preparing the pulp material used in papermaking, dates back to Samarkand in the 8th century, though this should not be confused with paper mills (see \"Paper mills\" section below). The Muslims also introduced the use of trip hammers (human- or animal-powered) in the production of paper, replacing the traditional Chinese mortar and pestle method. In turn, the trip hammer method was later employed by the Chinese. Historically, trip hammers were often powered by a water wheel, and are known to have been used in China as long ago as 40 BCE or maybe even as far back as the Zhou Dynasty (1050 BCE–221 BCE).\n\nBy the 9th century, Muslims were using paper regularly, although for important works like copies of the revered Qur'an, vellum was still preferred. Advances in book production and bookbinding were introduced.\nIn Muslim countries they made books lighter—sewn with silk and bound with leather-covered paste boards; they had a flap that wrapped the book up when not in use. As paper was less reactive to humidity, the heavy boards were not needed.\nBy the 12th century in Marrakech in Morocco a street was named \"Kutubiyyin\" or book sellers which contained more than 100 bookshops.\n\nIn 1035 a Persian traveler visiting markets in Cairo noted that vegetables, spices and hardware were wrapped in paper for the customers after they were sold.\nSince the First Crusade in 1096, paper manufacturing in Damascus had been interrupted by wars, but its production continued in two other centres. Egypt continued with the thicker paper, while Iran became the center of the thinner papers. Papermaking was diffused across the Islamic world, from where it was diffused further west into Europe. Paper manufacture was introduced to India in the 13th century by Muslim merchants, where it almost wholly replaced traditional writing materials.\n\nThe oldest known paper document in the West is the Mozarab Missal of Silos from the 11th century, probably using paper made in the Islamic part of the Iberian Peninsula. They used hemp and linen rags as a source of fiber. The first recorded paper mill in the Iberian Peninsula was in Xàtiva in 1056.\nPapermaking reached Europe as early as 1085 in Toledo and was firmly established in Xàtiva, Spain by 1150. It is clear that France had a paper mill by 1190, and by 1276 mills were established in Fabriano, Italy and in Treviso and other northern Italian towns by 1340. Papermaking then spread further northwards, with evidence of paper being made in Troyes, France by 1348, in Holland sometime around 1340–1350, in Mainz, Germany in 1320, and in Nuremberg by 1390 in a mill set up by Ulman Stromer. This was just about the time when the woodcut printmaking technique was transferred from fabric to paper in the old master print and popular prints. There was a paper mill in Switzerland by 1432 and the first mill in England was set up by John Tate in 1490 near Stevenage in Hertfordshire, but the first commercially successful paper mill in Britain did not occur before 1588 when John Spilman set up a mill near Dartford in Kent. During this time, paper making spread to Poland by 1491, to Austria by 1498, to Russia by 1576, to the Netherlands by 1586, to Denmark by 1596, and to Sweden by 1612.\n\nArab prisoners who settled in a town called Borgo Saraceno in the Italian Province of Ferrara introduced Fabriano artisans in the Province of Ancona the technique of making paper by hand. At the time they were renowned for their wool-weaving and manufacture of cloth. Fabriano papermakers considered the process of making paper by hand an art form and were able to refine the process to successfully compete with parchment which was the primary medium for writing at the time. They developed the application of stamping hammers to reduce rags to pulp for making paper, sizing paper by means of animal glue, and creating watermarks in the paper during its forming process. The Fabriano used glue obtained by boiling scrolls or scraps of animal skin to size the paper; it is suggested that this technique was recommended by the local tanneries. The introduction of the first European watermarks in Fabriano was linked to applying metal wires on a cover laid against the mould which was used for forming the paper.\n\nThey adapted the waterwheels from the fuller's mills to drive a series of three wooden hammers per trough. The hammers were raised by their heads by cams fixed to a waterwheel's axle made from a large tree trunk.\n\nIn the Americas, archaeological evidence indicates that a similar bark-paper writing material was used by the Mayans no later than the 5th century CE. Called \"amatl\" or \"amate\", it was in widespread use among Mesoamerican cultures until the Spanish conquest. The earliest sample of amate was found at Huitzilapa near the Magdalena Municipality, Jalisco, Mexico, belonging to the shaft tomb culture. It is dated to 75 BCE.\n\nThe production of amate is much more similar to paper than papyrus. The bark material is soaked in water, or in modern methods boiled, so that it breaks down into a mass of fibres. They are then laid out in a frame and pressed into sheets. It is a true paper product in that the material is not in its original form, but the base material has much larger fibres than those used in modern papers. As a result, amate has a rougher surface than modern paper, and may dry into a sheet with hills and valleys as the different length fibres shrink.\n\nEuropean papermaking spread to the Americas first in Mexico by 1575 and then in Philadelphia by 1690.\n\nThe use of human and animal powered mills was known to Chinese and Muslim papermakers. However, evidence for water-powered paper mills is elusive among both prior to the 11th century. The general absence of the use of water-powered paper mills in Muslim papermaking prior to the 11th century is suggested by the habit of Muslim authors at the time to call a production center not a \"mill\", but a \"paper manufactory\".\n\nDonald Hill has identified a possible reference to a water-powered paper mill in Samarkand, in the 11th-century work of the Persian scholar Abu Rayhan Biruni, but concludes that the passage is \"too brief to enable us to say with certainty\" that it refers to a water-powered paper mill. This is seen by Halevi as evidence of Samarkand first harnessing waterpower in the production of paper, but notes that it is not known if waterpower was applied to papermaking elsewhere across the Islamic world at the time. Burns remains sceptical, given the isolated occurrence of the reference and the prevalence of manual labour in Islamic papermaking elsewhere prior to the 13th century.\n\nClear evidence of a water-powered paper mill dates to 1282 in the Spanish Kingdom of Aragon. A decree by the Christian king Peter III addresses the establishment of a royal \"molendinum\", a proper hydraulic mill, in the paper manufacturing centre of Xàtiva. The crown innovation was operated by the Muslim Mudéjar community in the Moorish quarter of Xàtiva,though it appears to have been resented by sections of the local Muslim papermakering community; the document guarantees them the right to continue the way of traditional papermaking by beating the pulp manually and grants them the right to be exempted from work in the new mill. Paper making centers began to multiply in the late 13th century in Italy, reducing the price of paper to one sixth of parchment and then falling further; paper making centers reached Germany a century later.\n\nThe first paper mill north of the Alps was established in Nuremberg by Ulman Stromer in 1390; it is later depicted in the lavishly illustrated \"Nuremberg Chronicle\". From the mid-14th century onwards, European paper milling underwent a rapid improvement of many work processes.\n\nBefore the industrialisation of the paper production the most common fibre source was recycled fibres from used textiles, called rags. The rags were from hemp, linen and cotton. A process for removing printing inks from recycled paper was invented by German jurist Justus Claproth in 1774. Today this method is called deinking. It was not until the introduction of wood pulp in 1843 that paper production was not dependent on recycled materials from ragpickers.\n\nAlthough cheaper than vellum, paper remained expensive, at least in book-sized quantities, through the centuries, until the advent of steam-driven paper making machines in the 19th century, which could make paper with fibres from wood pulp. Although older machines predated it, the Fourdrinier papermaking machine became the basis for most modern papermaking. Nicholas Louis Robert of Essonnes, France, was granted a patent for a continuous paper making machine in 1799. At the time he was working for Leger Didot with whom he quarrelled over the ownership of the invention. Didot sent his brother-in-law, John Gamble, to meet Sealy and Henry Fourdrinier, stationers of London, who agreed to finance the project. Gamble was granted British patent 2487 on 20 October 1801. With the help particularly of Bryan Donkin, a skilled and ingenious mechanic, an improved version of the Robert original was installed at Frogmore Paper Mill, Hertfordshire, in 1803, followed by another in 1804. A third machine was installed at the Fourdriniers' own mill at Two Waters. The Fourdriniers also bought a mill at St Neots intending to install two machines there and the process and machines continued to develop.\n\nHowever, experiments with wood showed no real results in the late 18th century and at the start of the 19th century. By 1800, Matthias Koops (in London, England) further investigated the idea of using wood to make paper, and in 1801 he wrote and published a book titled \"Historical account of the substances which have been used to describe events, and to convey ideas, from the earliest date, to the invention of paper.\" His book was printed on paper made from wood shavings (and adhered together). No pages were fabricated using the pulping method (from either rags or wood). He received financial support from the royal family to make his printing machines and acquire the materials and infrastructure needed to start his printing business. But his enterprise was short lived. Only a few years following his first and only printed book (the one he wrote and printed), he went bankrupt. The book was very well done (strong and had a fine appearance), but it was very costly.\n\nThen in the 1830s and 1840s, two men on two different continents took up the challenge, but from a totally new perspective. Both Friedrich Gottlob Keller and Charles Fenerty began experiments with wood but using the same technique used in paper making; instead of pulping rags, they thought about pulping wood. And at about the same time, by mid-1844, they announced their findings. They invented a machine which extracted the fibres from wood (exactly as with rags) and made paper from it. Charles Fenerty also bleached the pulp so that the paper was white. This started a new era for paper making. By the end of the 19th-century almost all printers in the western world were using wood in lieu of rags to make paper.\n\nTogether with the invention of the practical fountain pen and the mass-produced pencil of the same period, and in conjunction with the advent of the steam driven rotary printing press, wood based paper caused a major transformation of the 19th century economy and society in industrialized countries. With the introduction of cheaper paper, schoolbooks, fiction, non-fiction, and newspapers became gradually available by 1900. Cheap wood based paper also meant that keeping personal diaries or writing letters became possible and so, by 1850, the clerk, or writer, ceased to be a high-status job.\n\nThe original wood-based paper was acidic due to the use of alum and more prone to disintegrate over time, through processes known as slow fires. Documents written on more expensive rag paper were more stable. Mass-market paperback books still use these cheaper mechanical papers (see below), but book publishers can now use acid-free paper for hardback and trade paperback books.\n\nDetermining the provenance of paper is a complex process that can be done in a variety of ways. The easiest way is using a known sheet of paper as an exemplar. Using known sheets can produce an exact identification. Next, comparing watermarks with those contained in catalogs or trade listings can yield useful results. Inspecting the surface can also determine age and location by looking for distinct marks from the production process. Chemical and fiber analysis can be used to establish date of creation and perhaps location.\n\n\n"}
{"id": "27761763", "url": "https://en.wikipedia.org/wiki?curid=27761763", "title": "IPhone (1st generation)", "text": "IPhone (1st generation)\n\nThe iPhone is the first smartphone model designed and marketed by Apple Inc. After years of rumors and speculation, it was officially announced on January 9, 2007, and was later released in the United States on June 29, 2007. It featured quad-band GSM cellular connectivity with GPRS and EDGE support for data transfer.\n\nOn June 9, 2008, Apple announced its successor, the iPhone 3G. After this announcement, the first-generation iPhone became referred to by some sources as the iPhone 2G due to the fact that it was the only iPhone to solely support 2G-speed networks. The original iPhone has not received software updates from Apple since iPhone OS (now iOS) 3.1.3.\n\nSince June 2013, the original iPhone is considered \"vintage\" by some service providers in the US, and \"obsolete\" in Apple retail stores and all other regions. Apple does not service vintage or obsolete products, and replacement parts for obsolete products are not available to service providers.\n\nIn 2005, Apple CEO Steve Jobs conceived an idea of using a multi-touch touchscreen to interact with a computer in a way in which he could directly type onto the display. He decided that it needed to have a triple layered touch screen, a very new and high tech technology at the time. This helped out with removing the physical keyboard and mouse, the same as a tablet computer. Jobs recruited a group of Apple engineers to investigate the idea as a side project. When Jobs reviewed the prototype and its user interface, he conceived a second idea of implementing the technology onto a mobile phone. The whole effort was called Project Purple 2 and began in 2005.\n\nApple created the device during a secretive and unprecedented collaboration with AT&T, formerly Cingular Wireless. The development cost of the collaboration was estimated to have been $150 million over a thirty-month period. Apple rejected the \"design by committee\" approach that had yielded the Motorola ROKR E1, a largely unsuccessful collaboration with Motorola. Instead, Cingular Wireless gave Apple the liberty to develop the iPhone's hardware and software in-house.\n\nThe original iPhone was introduced by Steve Jobs on January 9, 2007 in a keynote address at the Macworld Conference & Expo held in Moscone West in San Francisco, California. In his address, Jobs said, \"This is a day, that I have been looking forward to for two and a half years\", and that \"today, Apple is going to reinvent the phone.\" Jobs introduced the iPhone as a combination of three devices: a \"widescreen iPod with touch controls\"; a \"revolutionary mobile phone\"; and a \"breakthrough Internet communicator\".\n\nSix weeks before the iPhone was to be released, the plastic screen was replaced with a glass one, after Jobs was upset that the screen of the prototype he was carrying in his pocket had been scratched by his keys. The quick switch led to a bidding process for a manufacturing contractor that was won by Foxconn, which had just opened up a new wing of its Shenzhen factory complex specifically for this bid.\n\nIn February 2007, LG Electronics accused Apple of \"copying\" their LG Prada phone, which was introduced around the same time as iPhone.\n\nThe iPhone was released in the United States on June 29, 2007 at the price of $499 for the 4 GB model and $599 for the 8 GB model, both requiring a 2-year contract. Thousands of people were reported to have waited outside Apple and AT&T retail stores days before the device's launch; many stores reported stock shortages within an hour of availability. To avoid repeating the problems of the PlayStation 3 launch, which caused burglaries and even a shooting, off-duty police officers were hired to guard stores overnight.\n\nIt was later made available in the United Kingdom, France, Germany, Portugal, the Republic of Ireland, and Austria in November 2007.\n\nSix out of ten Americans surveyed said they knew the iPhone was coming before its release.\n\nThe iPhone's main competitors in both consumer and business markets were considered to be the LG Prada, LG Viewty, Samsung Ultra Smart F700, Nokia N95, Nokia E61i, Palm Treo 750, Palm Centro, HTC Touch, Sony Ericsson W960 and BlackBerry.\n\nThe iPod Touch, a touchscreen device with the media and internet abilities and interface of the iPhone but without the ability to connect to a cellular network for phone functions or internet access, was released on September 5, 2007. At the same time, Apple significantly dropped the price of the 8 GB model (from $599 to $399, still requiring a 2-year contract with AT&T) while discontinuing the 4 GB model. Apple sold the one millionth iPhone five days later, or 74 days after the release. After receiving \"hundreds of emails...upset\" about the price drop, Apple gave store credit to early adopters.\n\nA 16 GB model was released on February 5, 2008 for $499, the original launch price of the 4 GB model. Apple released an SDK on March 6, 2008, allowing developers to create the apps that would be available starting in iPhone OS version 2.0, a free upgrade for iPhone users. On June 9, Apple announced the iPhone 3G, which began shipping July 11. The original iPhone was discontinued 4 days later; total sales volume came to 6,124,000 units.\n\nDuring release, the iPhone was marketed as running \"OS X\". The name of the operating system was revealed as iPhone OS with the release of the iPhone SDK. The original iPhone supported three major versions of the operating system before it was discontinued: iPhone OS 1, 2, and 3. However, the full iPhone OS 3 feature set was not supported, and the last update the original iPhone received was iPhone OS 3.1.3\n\nThe original operating system for the original iPhone was iPhone OS 1, marketed as OS X, and included Visual Voicemail, multi-touch gestures, HTML email, Safari web browser, threaded text messaging, and YouTube. However, many features like MMS, apps, and copy and paste were not supported at release, leading hackers jailbreaking their phones to add these features. Official software updates slowly added these features.\n\niPhone OS 2 was released on July 11, 2008, around the same time as the release of the iPhone 3G, and introduced third-party applications, Microsoft Exchange support, push e-mail, and other enhancements.\n\niPhone OS 3 was released on June 17, 2009, and introduced copy and paste functionality, Spotlight search for the home screen, and new features for the YouTube app. iPhone OS 3 was available for the original iPhone as well as the iPhone 3G. However, not all features of iPhone OS 3 were supported on the original iPhone.\n\niPhone OS 3.1.3 was the last version of iPhone OS (now iOS) to be released for the original iPhone.\n\nOnly four writers were given review models of the original iPhone: David Pogue of \"The New York Times\", Walt Mossberg of \"The Wall Street Journal\", Steven Levy of \"Newsweek\", and Ed Baig of \"USA Today\". \"The New York Times\" and \"The Wall Street Journal\" published positive, but cautious, reviews of the iPhone, their primary criticisms being the relatively slow speed of the AT&T's 2.5G EDGE network and the phone's inability to connect using 3G services. \"The Wall Street Journal\"s technology columnist, Walt Mossberg, concluded that \"despite some flaws and feature omissions, the iPhone is, on balance, a beautiful and breakthrough handheld computer.\" \"Time\" magazine named it the Invention of the Year in 2007.\n\n\"Mobile Gazette\" reported that whilst the iPhone has many impressive points, it equally has many bad ones too, noting the lack of 3G, MMS, third-party applications, and its weak camera without autofocus and flash.\n\n\n"}
{"id": "4724116", "url": "https://en.wikipedia.org/wiki?curid=4724116", "title": "ISO 15926", "text": "ISO 15926\n\nThe ISO 15926 is a standard for data integration, sharing, exchange, and hand-over between computer systems.\n\nThe title, \"Industrial automation systems and integration—Integration of life-cycle data for process plants including oil and gas production facilities\", is regarded too narrow by the present ISO 15926 developers. Having developed a generic data model and Reference Data Library for process plants, it turned out that this subject is already so wide, that actually any state information may be modelled with it.\n\nIn 1991 a European Union ESPRIT-, named ProcessBase, started. The focus of this research project was to develop a data model for lifecycle information of a facility that would suit the requirements of the process industries. At the time that the project duration had elapsed, a consortium of companies involved in the process industries had been established: EPISTLE (European Process Industries STEP Technical Liaison Executive). Initially individual companies were members, but later this changed into a situation where three national consortia were the only members: PISTEP (UK), POSC/Caesar (Norway), and USPI-NL (Netherlands). (later PISTEP merged into POSC/Caesar, and USPI-NL was renamed to USPI).\n\nEPISTLE took over the work of the ProcessBase project. Initially this work involved a standard called ISO 10303-221 (referred to as \"STEP AP221\"). In that AP221 we saw, for the first time, an Annex M with a list of standard instances of the AP221 data model, including types of objects. These standard instances would be for reference and would act as a knowledge base with knowledge about the types of objects.\nIn the early nineties EPISTLE started an activity to extend Annex M to become a library of such object classes and their relationships: STEPlib. In the STEPlib activities a group of approx. 100 domain experts from all three member consortia, spread over the various expertises (e.g. Electrical, Piping, Rotating equipment, etc.), worked together to define the \"core classes\".\n\nThe development of STEPlib was extended with many additional classes and relationships between classes and published as Open Source data. Furthermore, the concepts and relation types from the AP221 and ISO 15926-2 data models were also added to the STEPlib dictionary. This resulted in the development of Gellish English, whereas STEPlib became the Gellish English dictionary. Gellish English is a structured subset of natural English and is a modeling language suitable for knowledge modeling, product modeling and data exchange. It differs from conventional modeling languages (meta languages) as used in information technology as it not only defines generic concepts, but also includes an English dictionary. The semantic expression capability of Gellish English was significantly increased by extending the number of relation types that can be used to express knowledge and information.\n\nFor modelling-technical reasons POSC/Caesar proposed another standard than ISO 10303, called ISO 15926. EPISTLE (and ISO) supported that proposal, and continued the modelling work, thereby writing Part 2 of ISO 15926. This Part 2 has official ISO IS (International Standard) status since 2003.\n\nPOSC/Caesar started to put together their own RDL (Reference Data Library). They added many specialized classes, for example for ANSI (American National Standards Institute) pipe and pipe fittings. Meanwhile, STEPlib continued its existence, mainly driven by some members of USPI. Since it was clear that it was not in the interest of the industry to have two libraries for, in essence, the same set of classes, the Management Board of EPISTLE decided that the core classes of the two libraries shall be merged into Part 4 of ISO 15926. This merging process has been finished. Part 4 should act as reference data for part 2 of ISO 15926 as well as for ISO 10303-221 and replaced its Annex M. On June 5, 2007 ISO 15926-4 was signed off as a TS (Technical Specification).\n\nIn 1999 the work on an earlier version of Part 7 started. Initially this was based on XML Schema (the only useful W3C Recommendation available then), but when Web Ontology Language (OWL) became available it was clear that provided a far more suitable environment for Part 7. Part 7 passed the first ISO ballot by the end of 2005, and an implementation project started. A formal ballot for TS (Technical Specification) was planned for December 2007. However, it was decided then to split Part 7 into more than one part, because the scope was too wide.\n\nISO 15926 has eleven parts (as of June 2009):\n\n\nThe model and the library are suitable for representing lifecycle information about technical installations and their components.\n\nThey can also be used for defining the terms used in product catalogs in e-commerce. Another, more limited, use of the standard is as a reference classification for harmonization purposes between shared databases and product catalogues that are not based on ISO 15926.\n\nThe purpose of ISO 15926 is to provide a Lingua Franca for computer systems, thereby integrating the information produced by them. Although set up for the process industries with large projects involving many parties, and involving plant operations and maintenance lasting decades, the technology can be used by anyone willing to set up a proper vocabulary of reference data in line with Part 4.\n\nIn Part 7 the concept of Templates is introduced. These are semantic constructs, using Part 2 entities, that represent a small piece of information. These constructs then are mapped to more efficient classes of n-ary relations that interlink the Nodes that are involved in the represented information.\n\nIn Part 8 the data model of Part 2 is mapped to OWL, and so are, in concept, the Reference Data of Part 4 and the templates of Part 7. For validation and reasoning purposes all are represented in First-Order Logic as well.\n\nIn Part 9 these Node and Template instances are stored in Façades. A Façade is an RDF quad store, set up to a standard schema and an API. Any Façade only stores the data for which the Façade owner is responsible.\n\nEach participating computer system maps its data from its internal format to such ISO-standard Node and Template instances. These are stored in a System Façade, each system its own Façade.\n\nData can be \"handed over\" from one Façade to another in cases where data custodianship is handed over (e.g. from a contractor to a plant owner, or from a manufacturer to the owners of the manufactured goods). Hand-over can be for a part of all data, whilst maintaining full referential integrity.\n\nFaçades can be set up for the consolidation of data by handing over data produced by various participating computer systems and stored in their System Façades. Examples are: a Façade for a project discipline, a project, a plant).\n\nDocuments are user-definable. They are defined in XML Schema and they are, in essence, only a structure containing cells that make reference to instances of Templates. This represents a view on all lifecycle data: since the data model is a 4D (space-time) model, it is possible to present the data that was valid at any given point in time, thus providing a true historical record. It is expected that this will be used for Knowledge Mining.\n\nData can be queried by means of SPARQL. In any implementation a restricted number of Façades can be involved, with different access rights. This is done by means of creating a CPF Server (= Confederation of Participating Façades). An Ontology Browser allows for access to one or more Façades in a given CPF, depending on the access rights.\n\nThere are a number of projects working on the extension of the ISO 15926 standard in different application areas.\n\nWithin the application of Capital Intensive projects, some cooperating implementation projects are running:\n\n\nFinalised projects include:\n\n\nThe Norwegian Oil Industry Association (OLF) has decided to use ISO 15926 (also known as the Oil and Gas Ontology) as the instrument for integrating data across disciplines and business domains for the Upstream Oil and Gas industry. It is seen as one of the enablers of what has been called the next (or second) generation of Integrated operations, where a better integration across companies is the goal.\n\nThe following projects are currently running (May 2009):\n\n\nFinalised projects include:\n\n\nOne of the main requirements was (and still is) that the scope of the data model covers the entire lifecycle of a facility (e.g. oil refinery) and its components (e.g. pipes, pumps and their parts, etc.). Since such a facility over such a long time entails many different types of activities on a myriad of different objects it became clear that a generic and data-driven data model would be required.\n\nA simple example will illustrate this. There are thousands of different types of physical objects in a facility (pumps, compressors, pipes, instruments, fluids, etc). Each of these has many properties. If all combinations would be modelled in a \"hard-coded\" fashion, the number of combinations would be staggering, and unmanageable.\n\nThe solution is a \"template\" that represents the semantics of: \"This object has a property of X yyyy\" (where yyyy is the unit of measure). Any instance of that template refers to the applicable reference data:\n\nWithout being able to make reference to those classes, via the Internet, it will be impossible to express this information.\n\n"}
{"id": "165331", "url": "https://en.wikipedia.org/wiki?curid=165331", "title": "Kinescope", "text": "Kinescope\n\nKinescope , shortened to kine , also known as telerecording in Britain, is a recording of a television program on motion picture film, directly through a lens focused on the screen of a video monitor. The process was pioneered during the 1940s for the preservation, re-broadcasting and sale of television programmes before the use of commercial broadcast-quality videotape became prevalent for these purposes.\n\nTypically, the term can refer to the process itself, the equipment used for the procedure (a 16 mm or 35 mm movie camera mounted in front of a video monitor, and synchronized to the monitor's scanning rate), or a film made using the process. Kinescopes were the only practical way to preserve live television broadcasts prior to the introduction of videotape in 1956. A small number of theatrically released feature films have also been produced as kinescopes.\n\nThe term \"kinescope\" originally referred to the cathode ray tube used in television receivers, as named by inventor Vladimir K. Zworykin in 1929. Hence, the recordings were known in full as kinescope films or kinescope recordings.  RCA was granted a trademark for the term (for its cathode ray tube) in 1932; it voluntarily released the term to the public domain in 1950.\n\nThe General Electric laboratories in Schenectady, New York experimented with making still and motion picture records of television images in 1931.\n\nThere is some evidence to suggest that the BBC experimented with filming the output of the television monitor before its television service was suspended in 1939 due to the outbreak of World War II. A BBC executive, Cecil Madden, later recalled filming a production of \"The Scarlet Pimpernel\" in this way, only for film director Alexander Korda to order the burning of the negative as he owned the film rights to the book, which he felt had been infringed. However, the evidence for this is purely anecdotal, and indeed there is no written record of any BBC Television production of \"The Scarlet Pimpernel\" during the 1936–1939 period. The incident is, however, dramatised in Jack Rosenthal's 1986 television play \"The Fools on the Hill\".\n\nSome of the surviving live transmissions of the Nazi German television station Fernsehsender Paul Nipkow, dating as far back as the 1930s, were recorded by pointing a 35mm camera to a receiver's screen, although most surviving Nazi live television programs such as the 1936 Summer Olympics (not to confuse with the cinematic footage made during the same event by Leni Riefenstahl for her film \"Olympia\"), a number of Nuremberg Rallies, or official state visits (such as Benito Mussolini's) were shot directly on 35mm instead and transmitted over the air as a television signal, with only a two minutes' delay from the original event, by means of the so-called \"Zwischenfilmverfahren\" (see intermediate film system) from an early outside broadcast van on the site.\n\nAccording to a 1949 film produced by RCA, silent films had been made of early experimental telecasts during the 1930s. The films were produced by aiming a camera at television monitors - at a speed of eight frames per second, resulting in somewhat jerky reproductions of the images. By the mid-1940s, RCA and NBC were refining the filming process and including sound; the images were less jerky but still somewhat fuzzy.\n\nBy early 1946, television cameras were being attached to American guided missiles to aid in their remote steering. Films were made of the television images they transmitted for further evaluation of the target and the missile's performance.\n\nThe first known surviving example of the telerecording process in Britain is from October 1947, showing the singer Adelaide Hall performing at the RadiOlympia event. Hall sings \"Chi-Baba, Chi-Baba (My Bambino Go to Sleep)\" and \"I Can't Give You Anything But Love\", as well as accompanying herself on ukulele and dancing. When the show was originally broadcast on BBC TV it was 60 minutes in length and also included performances from Winifred Atwell, Evelyn Dove, Cyril Blake and his Calypso Band, Edric Connor and Mable Lee, and was produced by Eric Fawcett. The six-minute footage of Miss Hall is all that survives of the show.\n\nFrom the following month, the wedding of Princess Elizabeth to Prince Philip also survives, as do various early 1950s productions such as \"It is Midnight, Dr Schweitzer\", \"The Lady from the Sea\" and the opening two episodes of \"The Quatermass Experiment\", although in varying degrees of quality. A complete 7-hour set of telerecordings of Queen Elizabeth II's 1953 coronation also exists. \n\nIn the era before satellite communications, kinescopes were used to distribute live events such as a royal wedding as quickly as possible to other countries of the Commonwealth that had started a television service. A Royal Air Force aircraft would fly the telerecording from the UK to Canada, where it would be broadcast over the whole North American network.\n\nEven after the introduction of videotape, the BBC and the ITV companies made black and white kinescopes of selected programs for international sales, and continued to do so until the early 1970s by which time programs were being videotaped in color. Most, if not all, videotapes from the 405-line era have long since been wiped as have many from the introduction of 625-line video to the early days of color. Consequently, the majority of British shows that still exist before the introduction of color, and a number thereafter, do so in the form of these telerecordings. A handful of shows, including some episodes of \"Doctor Who\" and most of the first series of \"Adam Adamant Lives!\", were deliberately telerecorded for ease of editing rather than being videotaped.\n\nIn September 1947, Eastman Kodak introduced the Eastman Television Recording Camera, in cooperation with DuMont Laboratories, Inc. and NBC, for recording images from a television screen under the trademark \"Kinephoto\". Prior to the introduction of videotape in 1956, kinescopes were the only way to record television broadcasts, or to distribute network television programs that were broadcast live from New York or other originating cities, to stations not connected to the network, or to stations that wished to show a program at a time different than the network broadcast. Although the quality was less than desirable, television programs of all types from prestigious dramas to regular news shows were handled in this manner.\n\nNBC, CBS, and DuMont set up their main kinescope recording facilities in New York City, while ABC chose Chicago. By 1951, NBC and CBS were each shipping out some 1,000 16mm kinescope prints each week to their affiliates across the United States, and by 1955 that number had increased to 2,500 per week for CBS. By 1954 the television industry’s film consumption surpassed that of all of the Hollywood studios combined.\n\nAfter the network of coaxial cable and microwave relays carrying programs to the West Coast was completed in September 1951, CBS and NBC instituted a \"hot kinescope\" process in 1952, where shows being performed in New York were transmitted west, filmed on two kinescope machines in 35 mm negative and 16 mm reversal film (the latter for backup protection) in Los Angeles, rushed to film processing, and then transmitted from Los Angeles three hours later for broadcast in the Pacific Time Zone.\n\nIn September 1956, NBC began making color \"hot kines\" of some of its color programs using a lenticular film process which, unlike color negative film, could be processed rapidly using standard black-and-white methods.\n\nEven after the introduction of Quadruplex videotape machines in 1956 removed the need for \"hot kines\", the television networks continued to use kinescopes in the \"double system\" method of videotape editing. It was impossible to slow or freeze frame a videotape at that time, so the unedited tape would be copied to a kinescope, and edited conventionally. The edited kinescope print was then used to conform the videotape master. More than 300 videotaped network series and specials used this method over a 12-year period, including the fast-paced \"Rowan & Martin's Laugh-In\".\n\nWith the variable quality of Kinescopes, networks looked towards alternative methods to replace them with a higher degree of quality.\n\nPrograms originally shot with film cameras (as opposed to kinescopes) were also used in television’s early years, although they were generally considered inferior to the big-production \"live\" programs because of their lower budgets and loss of immediacy.\n\nIn 1951, the stars and producers of the Hollywood-based television series \"I Love Lucy\", Desi Arnaz and Lucille Ball, decided to film the show directly onto 35 mm film using the three-camera system, instead of broadcasting it live. Normally, a live program originating from Los Angeles would be performed live in the late afternoon for the Eastern Time Zone, and seen on a kinescope three hours later in the Pacific Time Zone. But as an article in \"American Cinematographer\" explained,\n\nThe \"I Love Lucy\" decision introduced reruns to most of the American television audience, and set a pattern for the syndication of TV shows after their network runs (and later, for first-run airings via syndication).\n\nThe program director of the DuMont Television Network, James L. Caddigan, devised an alternative — the Electronicam. In this, all the studio TV cameras had built-in 35 mm film cameras which shared the same optical path. An Electronicam technician threw switches to mark the film footage electronically, identifying the camera \"takes\" called by the director. The corresponding film segments from the various cameras then were combined by a film editor to duplicate the live program. The \"Classic 39\" syndicated episodes of \"The Honeymooners\" were filmed using Electronicam (as well as the daily five-minute syndicated series \" Les Paul & Mary Ford At Home\" in 1954–55), but with the introduction of a practical videotape recorder only one year away, the Electronicam system never saw widespread use. The DuMont network did not survive into the era of videotape, and in order to gain clearances for its programs, was heavily dependent on kinescopes, which it called Teletranscriptions.\n\nAttempts were made for many years to take television images, convert them to film via kinescope, then project them in theaters for paying audiences. In the mid-1960s, Producer/entrepreneur H. William \"Bill\" Sargent, Jr. used conventional analog Image Orthicon video camera tube units, shooting in the B&W 819-line interlaced 25fps French video standard, using modified high-band quadruplex VTRs to record the signal. The promotors of Electronovision (not to be confused with Electronicam) gave the impression that this was a new system created from scratch, using a high-tech name (and avoiding the word kinescope) to distinguish the process from conventional film photography. Nonetheless, the advances in picture quality were, at the time, a major step ahead. By capturing more than 800 lines of resolution at 25 frame/s, raw tape could be converted to film via kinescope recording with sufficient enhanced resolution to allow big-screen enlargement. The 1960s productions used Marconi image orthicon video cameras, which have a characteristic white \"glow\" around black objects (and a corresponding black glow around white objects), which was a defect of the pickup. Later vidicon and plumbicon video camera tubes produced much cleaner, more accurate pictures.\n\nIn 1951, singer Bing Crosby’s company Bing Crosby Enterprises made the first experimental magnetic video recordings; however, the poor picture quality and very high tape speed meant it would be impractical to use. In 1956, Ampex introduced the first commercial Quadruplex videotape recorder, followed in 1958 by a color model. Offering high quality and instant playback at a much lower cost, Quadruplex tape quickly replaced kinescope as the primary means of recording television broadcasts.\n\nU.S. television networks continued to make kinescopes of their daytime dramas available (many of which still aired live into the late 1960s) as late as 1969 for their smaller network affiliates that did not yet have videotape capability but wished to time-shift the network programming. Some of these programs aired up to two weeks after their original dates, particularly in Alaska and Hawaii. Many episodes of programs from the 1960s survive only through kinescoped copies. The last 16 mm kinescopes of television programs ended in the late 1970s, as video tape recorders became more affordable.\n\nIn Australia, kinescopes were still being made of some evening news programs as late as 1977, if they were recorded at all. A recording of a 1975 episode of Australian series \"This Day Tonight\" is listed on the National Archives of Australia website as a kinescope, while surviving episodes of the 1978 drama series \"The Truckies\" also exist as kinescopes, indicating that the technology was still being used by ABC at that point.\n\nIn later years, film and television producers were often reluctant to include kinescope footage in anthologies because of its inherent inferior quality. While it is true that kinescopes did look inferior to live transmissions in the 1950s, it was due to the industry's technical limitations at that time. Even the best live transmission could look hazy by the time it reached the home viewer. Advances in broadcast technology soon allowed for a wider gray scale in black-and-white, and a fuller spectrum of colors, making kinescopes a perfectly viable commodity. This was demonstrated in the feature film \"Ten from Your Show of Shows\", a compilation of Sid Caesar kinescopes released to theaters. Reviewers were astonished at how good the kinescoped image looked on a large screen. Kinescopes have since lost their stigma of inferiority, and are commonly consulted today for archival purposes.\n\nUp until the early 1960s, much of the BBC's output, and British television in general, was broadcast live, and telerecordings would be used to preserve a programme for repeat showings, which had previously required the entire production being performed live for a second time. In the UK, telerecordings continued to be made after the advent of commercial broadcast videotape from 1958 as they possessed several distinct advantages, particularly for overseas program sales. Firstly, they were cheaper, easier to transport and more durable than video. Secondly, they could be used in any country regardless of the television broadcasting standard, which was not true of videotape. Thirdly, the system could be used to make black and white copies of color programs for sale to television stations who were not yet broadcasting in color.\n\nThe telerecording system could be of a very high quality, easily reproducing the full detail of the television picture. One side effect of the system was that it removed the 'fluid' look of interlaced video and 'filmized' the picture.\n\nThe system was largely used for black and white reproduction. Although some color telerecordings were made, they were generally in the minority as by the time color programs were widely needed for sale, video standards conversion was easier and higher quality and the price of videotape had become much reduced. Before videotape became the exclusive transmission format during the early to mid-1980s, any (color) video recordings used in documentaries or filmed program inserts were usually transferred onto film.\n\nIn the 1950s a home telerecording kit was introduced in Britain, allowing enthusiasts to make 16 mm film recordings of television programs . The major drawback, apart from the short duration of a 16 mm film magazine, was that a large opaque frame had to be placed in front of the TV set in order to block out any stray reflections, making it impossible to watch the set normally while filming. It is not known if any recordings made using this equipment still exist.\n\nBritish broadcasters used telerecordings for domestic purposes well into the 1960s, with 35 mm being the film gauge usually used as it produced a higher quality result. For overseas sales, 16 mm film would be used, as it was cheaper. Although domestic use of telerecording in the UK for repeat broadcasts dropped off sharply after the move to color in the late 1960s, 16 mm black and white film telerecordings were still being offered for sale by British broadcasters well into the 1970s.\n\nTelerecording was still being used internally at the BBC in the 1980s too, to preserve copies for posterity of programs which were not necessarily of the highest importance, but which nonetheless their producers wanted to be preserved. If there were no videotape machines available on a given day, then a telerecording would be made. There is evidence to suggest that the children's magazine program \"Blue Peter\" was occasionally being telerecorded as late as 1985. After this point, however, cheap domestic videotape formats such as VHS could more easily be used to keep a back-up reference copy of a program.\n\nAnother occasional use of telerecording into the late 1980s was by documentary makers working in 16 mm film who wished to include a videotape-sourced excerpt in their work, although such use was again rare.\n\nIn other territories, film telerecordings were stopped being produced after the introduction of videotape. In Czechoslovakia, the first videotape recorders (Machtronics MVR-15) were introduced in 1966, but soon were replaced by the Ampex 2\" Quadruplex in 1967. Most of the programs, like TV dramas, were recorded on video, but only a few programmes continued to be telerecorded onto 16mm film. The last telerecording was produced in 1969 and soon after all programmes were recorded on video only.\n\nKinescopes were intended to be used for immediate rebroadcast, or for an occasional repeat of a prerecorded program; thus, only a small fraction of kinescope recordings remain today. Many television shows are represented by only a handful of episodes, such as with the early television work of comedian Ernie Kovacs, and the original version of \"Jeopardy!\" hosted by Art Fleming.\n\nAnother purpose of Kinescopes involved satisfying show sponsors. Kinescopes sometimes would be sent to the advertising agency for the sponsor of a show so that the ad agency could determine whether or not the sponsor's ads appeared properly. Due to this practice, some kinescopes have actually been discovered in the storage areas of some of these older advertising agencies or in the storage areas of the program sponsors themselves.\n\nKinescopes were also used for some live television programs, like \"Captain Kangaroo\", when back-to-back episodes were made in a day for different time zones.\n\nTelerecordings form an important part of British television heritage, preserving what would otherwise have been lost. Nearly every pre-1960s British television programme in the archives is in the form of a telerecording, along with the vast majority of existing 1960s output. Videotape was expensive and could be wiped and re-used; film was cheaper, smaller, and in practice more durable. Only a very small proportion of British television from the black and white era survives at all; perhaps 5% from the 1953-58 period and 8-10% from the 1960s.\n\nMany recovered programmes, particularly those made by the BBC, have been returned as telerecordings by foreign broadcasters or private film collectors from the 1980s onwards, as the BBC has taken stock of the large gaps in its archive and sought to recover as much of the missing material as possible. Many of these surviving telerecorded programmes, such as episodes of \"Doctor Who\", \"Steptoe and Son\" and \"Till Death Us Do Part\" continue to be transmitted on satellite television stations such as UKTV Gold, and many such programmes have been released on VHS and DVD.\n\nIn late 2008 the BBC transmitted an episode of \"Dad's Army\" after the original colour had been restored to the only surviving monochrome film recording of \"Room at the Bottom\".\n\nNTSC television images are scanned at roughly 60 Hz, with two interlaced fields per frame, displayed at 30 frames per second.\n\nA kinescope must be able to:\n\nIn kinescoping an NTSC signal, 525 lines are broadcast in one frame. A 35 mm or 16 mm camera exposes one frame of film for every one frame of television (525 lines), and moving a new frame of film into place during the time equivalent of one field of television (131.25 lines). In the British 405-line television system, the French 819-line television system and the European 625-line television system, television ran at 25 frames—or more correctly, 50 fields—per second, so the film camera would also be run at 25 frames per second rather than the cinematic film standard of 24 frames.\n\nTherefore, in order to maintain successful kinescope photography, a camera must expose one frame of film for \"exactly\" 1/30th or 1/25th of a second, the time in which one frame of video is transmitted, and move to another frame of film within the small interval of 1/120 of a second. In some instances, this was accomplished through means of an electronic shutter which cuts off the TV image at the end of every set of visible lines.\n\nMost U.S. kinescope situations, however, utilized a mechanical shutter, revolving at 24 revolutions per second. This shutter had a closed angle of 72° and an open angle of 288°, yielding the necessary closed time of 1/120 of a second and open time 1/30 of a second. Using this shutter, in 1 second of video (60 fields equaling 30 frames), 48 television fields (totaling to 24 frames of video) would be captured on 24 frames of film, and 12 additional fields would be omitted as the shutter closed and the film advanced.\n\nBecause television is a field- rather than frame-based system, however, not all the information in the picture can be retained on film in the same way as it can on videotape. The time taken physically to move the film on by one frame and stop it so that the gate can be opened to expose a new frame of film to the two fields of television picture is much longer than the vertical blanking interval between these fields—so the film is still moving when the start of the next field is being displayed on the television screen. It is not possible to accelerate the film fast enough to get it there in time without destroying the perforations in the film stock—and the larger the film gauge used, the worse the problem becomes.\n\nThe problem of adapting the way the image is either displayed or captured on film, to get around the above, was solved in various different ways as time went on—improving the quality of the image.\n\nThe 72°/288° shutter and the systematic loss of 12 fields per second were not without its side effects. In going from 30 frame/s to 24 frame/s, the camera photographed \"part\" of some fields. The juncture on the film frame where these part-fields met was called a \"splice\".\n\nIf the timing was accurate, the splice was invisible. However, if the camera and television were out of phase, a phenomenon known as \"shutter bar\" or \"banding\" took place. If the shutter was slow in closing, overexposure resulted where the part-fields joined and the \"shutter bar\" took the form of a white line. If the shutter closed too soon, underexposure took place and the line was black. The term \"banding\" referred to the phenomenon occurring on the screen as two bars.\n\nA simpler system less prone to breakdown was to suppress one of the two fields in displaying the television picture. This left the time in which the second field was displayed for the film camera to advance the film by one frame, which proved enough. This method was also called 'Skip field' recording.\n\nThis method had several disadvantages. In missing out every second field of video, half the information of the picture was lost on such recordings. The resulting film consisted of fewer than 200 lines of picture information and as a result the line structure was very apparent; the missing field information also made movement look very 'jerky'.\n\nA development on the suppressed field system was to display the image from one of the fields at a much higher intensity on the television screen during the time when the film gate was closed, and then capture the image as the second field was being displayed. By adjusting the intensity of the first field, it was possible to arrange it so that the luminosity of the phosphor had decayed to exactly match that of the second field, so that the two appeared to be at the same level and the film camera captured both.This method came to be preferred.\n\nAnother technique developed by the BBC known as 'spot wobble' involved the addition of an extremely high frequency but low voltage sine wave to the vertical deflection plate of the television screen, which changed the moving 'spot' through which the television picture was displayed into an elongated oval. While this made the image slightly blurred, it removed the visible line structure and resulted in a better image. It also prevented moiré patterns appearing when the resulting film was re-broadcast on television and the lines of the recording did not match the scan lines.\n\nThe first successful procedure was to use the Mechau film projector mechanism in reverse. The Mechau system used a synchronised rotating mirror to display each frame of a film in sequence without the need for a gate. When reversed, a high-quality television monitor was set up in place of the projection screen, and unexposed film stock is run through at the point where the lamp was illuminating the film.\n\nThis procedure had the advantage of capturing both fields of the frame on a film, but it was difficult to keep the mirrors running at the right speed and all the equipment adjusted correctly, which often resulted in poor quality output. An additional problem was that the whole procedure took place in an open room and it was known for insects to settle on the screen which were then permanently present on the film recording. The Mechau film magazine only held enough for nine minutes so two recorders were needed to run in sequence in order to record anything longer.\n\nLenses did not need a great depth of field, but had to be capable both of producing a very sharp image with high resolution of a flat surface and of doing so at high speed. In order to keep from light fall-off on the perimeter of the lens, a coated lens was preferable. 40 mm or 50 mm lenses were usually used with 16mm in calibrated mounts. Focus was checked by examining a print yielded under a microscope.\n\nIn order to record half-hour programs without interruption, magazines were designed which accommodated a load of 1,200 feet for 16 mm film. Stations recording on 35 mm utilized 6,000 foot magazines for one hour of continuous recording.\n\nThe camera could be equipped with sound recording to place the soundtrack and picture on the same film for single system sound recording. More commonly, the alternative double system, whereby the soundtrack was recorded on an optical recorder or magnetic dubber in sync with the camera, yielded a better quality sound track and greatly facilitated editing.\n\nKinescope tubes intended for photographic use were coated with phosphors rich in blue and ultra-violet radiations. This permitted the use of positive type emulsions for photographing in spite of their slow film speeds. The brightness range of kinescope tubes were about 1 to 30.\n\nKinescope images were capable of great flexibility. The operator could make the image brighter or darker, adjust contrast, width and height, turn left, right or upside down, and positive or negative.\n\nSince kinescopes were able to produce a negative picture, direct positive recordings could be made by simply photographing a negative image on the kinescope tube. When making a negative film, in order for final prints to be in the correct emulsion position, the direction of the image was reversed on the television. This applied only when double system sound was used.\n\nFor kinescopes, 16 mm film was the common choice by most studios because of the lower cost of stock and film processing, but in the larger network markets, it was not uncommon to see 35 mm kinescopes, particularly for national rebroadcast. By law, all film supplied to TV stations, both 16 mm and 35 mm had to be on a non-flammable, safety film base.\n\nFor U.S. video recording, fine grain positive stock was the most common used because of its low cost and high resolution yield. Of the fine grain stocks, the following were recommended by film manufacturers:\n\nVideotape engineer Frederick M. Remley wrote of kinescope recordings,\n\nBecause each field is sequential in time to the next, a kinescope film frame that captured two interlaced fields at once often showed a ghostly fringe around the edges of moving objects, an artifact not as visible when watching television directly at 50 or 60 fields per second.\n\nSome kinescopes filmed the television pictures at the same frame rate of 30 full frames per second, resulting in more faithful picture quality than those that recorded at 24 frames per second. The standard was later changed for color TV to 59.94 fields/s. or 29.97 frame/s. when color TV was invented. \n\nIn the era of early color TV, the chroma information included in the video signal filmed could cause visible artifacts. It was possible to filter the chroma out, but this was not always done. Consequently, the color information was included (but not in color) in the black & white film image. Using modern computing techniques, the color may now be recovered, a process known as color recovery.\n\nIn recent years, the BBC has introduced a video process called \"VidFIRE\", which can restore kinescope recordings to their original frame rate by interpolating video fields between the film frames.\n\nCertain performers or production companies would require that a kinescope be made of every television program. Such is the case with performers Jackie Gleason and Milton Berle, for whom nearly complete program archives exist. As Jackie Gleason’s program was broadcast live in New York, the show was kinescoped for later rebroadcast for the West Coast. Per his contract, he would receive one copy of each broadcast, which he kept in his vault, and only released them to the public (on home video) shortly before his death in 1987.\n\nMilton Berle sued NBC late in his life, believing the kinescopes of a major portion of his programs were lost. However, the programs were later found in a warehouse in Los Angeles.\n\nMark Goodson-Bill Todman Productions, the producers of such TV game shows as \"What's My Line?\", had a significant portion of their output recorded on both videotape and kinescopes. These programs are rebroadcast on the American cable TV’s Game Show Network.\n\nAll of the NBC Symphony Orchestra telecasts with Arturo Toscanini, from 1948 to 1952, were preserved on kinescopes and later released on VHS and LaserDisc by RCA and on DVD by Testament. The original audio from the kinescopes, however, was replaced with high fidelity sound that had been recorded simultaneously either on transcription discs or magnetic tape.\n\nIn the mid-90s, Edie Adams, wife of Ernie Kovacs, claimed that so little value was given to the kinescope recordings of the DuMont Television Network that after the network folded in 1956 its entire archive was dumped into upper New York bay. Today however, efforts are made to preserve the few surviving DuMont kinescopes, with the UCLA Film and Television Archive having collected over 300 for preservation.\n\nIn September 2010, a kinescope of game 7 of the 1960 World Series was found in the wine cellar of Bing Crosby. The game was thought lost forever, but was preserved due to Crosby's superstition about watching the game live. The film was transferred to DVD and was broadcast on the MLB Network shortly afterwards.\n\nBecause videotape records at fifty interlaced fields per second and telerecordings at twenty-five progressive frames per second, videotaped programmes that exist now only as telerecordings have lost their characteristic \"live video\" look and the motion now looks filmic. One solution to this problem is VidFIRE, an electronic process to restore video-type motion.\n\nEarly Australian television drama series were recorded as kinescopes, such as \"Autumn Affair\" and \"Emergency\", along with variety series like \"The Lorrae Desmond Show\". Kinescopes continued to be made after video-tape was introduced to Australia; most existing episodes of the 1965-1967 children's series \"Magic Circle Club\" are kinescopes (per listings for episodes on National Film and Sound Archive website)\n\n\n"}
{"id": "7983699", "url": "https://en.wikipedia.org/wiki?curid=7983699", "title": "Knowledge divide", "text": "Knowledge divide\n\nThe knowledge divide is the gap in standards of living between those who can find, create, manage, process, and disseminate information or knowledge, and those who are impaired in this process. According to a 2005 UNESCO World Report, the rise in the 21st century of a global information society has resulted in the emergence of knowledge as a valuable resource, increasingly determining who has access to power and profit. The rapid dissemination of information on a potentially global scale as a result of new information media and the globally uneven ability to assimilate knowledge and information has resulted in potentially expanding gaps in knowledge between individuals and nations.\n\nIn the 21st century, the emergence of the knowledge society becomes pervasive. The transformations of world's economy and of each society have a fast pace. Together with information and communication technologies (ICT) these new paradigms have the power to reshape the global economy. In order to keep pace with innovations, to come up with new ideas, people need to produce and manage knowledge. This is why knowledge has become essential for all societies.\n\nAccording to UNESCO and the World Bank, knowledge gaps between nations may occur due to the varying degrees by which individual nations incorporate the following elements:\n\n\nThe information and ICT systems that support knowledge are very important. This is why digitization is viewed closely related to knowledge. Scientists generally agree that there is a digital divide, recently different reports also showed the existence of knowledge divide.\n\nThe creation and effective use of knowledge are increasingly related to the development of an ICT infrastructure. Without ICT, it is impossible to have an infrastructure able to process the huge flow of information required in an advanced economy. In particular, without adequate technical support, it is difficult to develop and use e-learning and electronic documents to overcome time and space constraints.\n\nThe digital divide is, however, but one important part of the larger knowledge divide. As UNESCO states, \"closing the digital divide will not suffice to close the knowledge divide, for access to useful, relevant knowledge is more than simply a matter of infrastructure—it depends on training, cognitive skills and regulatory frameworks geared towards access to contents.\"\n\nIn the book Digital Dead End, Virginia Eubanks criticizes the way that the digital divide is generally thought of as a division between haves and have-nots, where the solution is distribution. This over simplistic depiction obscures the fact that often social and structural inequality is at the root of the divide. According to a study done by Eubanks with women of the YWCA, the women of the community \"insisted that have-nots possess many different kinds of crucial information and skills.\" In other words, it is not simply knowledge of the technology itself that is the issue but the structural system based on perpetuating the status quo in which the haves \"hoard\" knowledge.\n\nFirst, it was noticed that a great difference exists between the North and the South (rich countries vs. poor countries). The development of knowledge depends on spreading Internet and computer technology and also on the development of education in these countries. If a country has attained a higher literacy level then this will result in having higher level of knowledge.\nIndeed, UNESCO's report details many social issues in knowledge divide related to globalization. There was noticed a knowledge divide with respect to\n\n\n\n"}
{"id": "300543", "url": "https://en.wikipedia.org/wiki?curid=300543", "title": "Knowledge gap hypothesis", "text": "Knowledge gap hypothesis\n\nThe knowledge gap hypothesis explains that knowledge, like other forms of wealth, is often differentially distributed throughout a social system. Specifically, the hypothesis predicts that \"as the infusion of mass media information into a social system increases, segments of the population with higher socioeconomic status tend to acquire this information at a faster rate than the lower status segments, so that the gap in knowledge between these segments tends to increase rather than decrease\". Phillip J. Tichenor, then Associate Professor of Journalism and Mass Communication, George A. Donohue, Professor of Sociology, and Clarice N. Olien, Instructor in Sociology – three University of Minnesota researchers – first proposed the knowledge gap hypothesis in 1970.\n\nAlthough first formally articulated in 1970, Tichenor, Donohue, and Olien note that the knowledge gap hypothesis has been implicit throughout the mass communication literature.\n\nIndeed, research published as early as the 1920s had already begun to examine the influence of individual characteristics on people's media content preferences. For example, Gray and Munroe identified education – still used today as an operationalization of socioeconomic status in knowledge gap research (see, e.g., Hwang and Jeong, 2009) – as a significant and positive correlate of a person's tendency to prefer \"serious\" (rather than non-serious) print content.\n\nPopular belief, however, held that such differences in preferences might be diminished by the advent of radio, which required neither the special skill nor the exertion of reading (Lazarsfeld, 1940). Guglielmo Marconi, inventor of the wireless telegraph, even believed that the radio would \"make war impossible, because it will make war ridiculous\" (Narodny, 1912, p. 145). Interested in whether radio had attenuated these individual differences in content preferences, Paul Lazarsfeld, head of the Office of Radio Research at Columbia University, set out to examine whether (1) the total amount of time that people listened to the radio and (2) the type of content they listened to correlated with their socioeconomic status. Not only did Lazarsfeld's data indicate people of lower socioeconomic status tended to listen to more radio programming, but also they were simultaneously less likely to listen to \"serious\" radio content. Contrary to popular belief at the time, then, the widespread adoption of the radio seems to have had little, if any, effect on a person's tendency to prefer specific types of content.\n\nFurther evidence supporting the knowledge gap hypothesis came from Star and Hughes (1950) analysis of efforts to inform Cincinnati adults about the United Nations. Like Gray and Munroe (1929) and Lazarsfeld (1940) before them, Star and Hughes found that while the campaign was successful in reaching better-educated people, those with less education virtually ignored the campaign. Additionally, after realizing that the highly educated people reached by the campaign also tended to be more interested in the topic, Star and Hughes suggested that knowledge, education, and interest may be interdependent.\n\nBased on observations implicit in mass communication research, Tichenor, Donohue, and Olien (1970) define the knowledge gap hypothesis as follows:\n\nAdditionally, Tichenor, Donohue, and Olien suggest 5 reasons why the knowledge gap should exist: \nailor their conte\n\nGiven the preceding information, the knowledge gap hypothesis can be expressed using the following set of related propositions:\n\nThe knowledge gap hypothesis can be operationalized both for cross-sectional and time-series appropriate research. For cross-sectional research, the knowledge gap hypothesis expects that \"\"at any given time\", there should be a higher correlation between acquisition of knowledge and education for topics highly publicized in the media than for topics less highly publicized. Tichenor, Donohue, and Olien (1970) tested this hypothesis using an experiment in which participants were asked to read and discuss two news stories of varying publicity. The results of the experiment support the hypothesis because correlations between education and understanding were significant for high publicity stories but not significant for low publicity stories.\n\nFor time-series research, the knowledge gap hypothesis expects that \"\"over time\", acquisition of knowledge of a heavily publicized topic will proceed at a faster rate among better educated persons than among those with less education.\" Tichenor, Donohue, and Olien (1970) tested this hypothesis using public opinion surveys gathered between 1949 and 1965 measuring whether participants believed humans would reach the Moon in the foreseeable future. During the 15-year span, belief among grade-school educated people increased only about 25 percentage points while belief among college educated people increased more than 60 percentage points, a trend consistent with the hypothesis.\n\nAlthough by the mid-1970s extensive data supported the existence of a knowledge gap among low and high socioeconomic status individuals, Donohue, Tichenor, and Olien (1975) sought to refine the hypothesis to determine under what conditions the knowledge gap might be attenuated or even eliminated. To this end, they examined survey data on national and local issues from probability samples of 16 Minnesota communities gathered between 1969 and 1975. Donohue and colleagues identified three variables that weakened the knowledge gap:\n\nAt least two narrative reviews and one meta-analysis of knowledge gap hypothesis research exist. Gaziano conducted two narrative reviews, one of 58 articles with relevant data in 1983 and the other of 39 additional studies in 1997. Gaziano writes, \"the most consistent result is the presence of knowledge differentials, regardless of topic, methodological, or theoretical variations, study excellence, or other variables and conditions\" (1997, p. 240). Evidence from several decades, Gaziano concludes, underscores the enduring character of knowledge gaps and indicates that they transcend topics and research settings.\n\nBecause narrative reviews examine significance tests rather than effect sizes, Hwang and Jeong (2009) conducted a meta-analysis of 46 knowledge gap studies. Consistent with Gaziano's results, however, Hwang and Jeong found constant knowledge gaps across time.\n\nIn 2010 Elizabeth Corley and Dietram Scheufele conducted a study to investigate the widening knowledge gap with the example of nanotechnology. On the whole, public opinion research has shown that respondents with higher socioeconomic status (SES) acquire new information at a higher rate than low SES respondents. Their previous analyses of two large national surveys conducted in 2004 and 2007 found that respondents with at least a college degree displayed an increase in knowledge levels between 2004 and 2007 while respondents with education levels of less than a high school diploma had a significant decrease in nanotechnology knowledge levels. These results stress that the group that is most in need of help, low SES bracket, have not been helped through communication efforts and their nanotechnology knowledge levels have decreased over time.\n\nCorley and Scheufele investigated a wide range of factors that may help to close knowledge gaps, including mass media. The researchers found that the number of days a week that respondents spent online was significantly correlated to knowledge levels about nanotechnology. Therefore, internet use helped those with less formal education to catch up to their counterparts.\n\nThe emergence of the Internet, and more specifically Web 2.0, may be playing a role in closing the knowledge gap. In fact, Corley and Scheufele explain that \"the internet may finally live up to the hype … as a tool for creating a more informed citizenry by serving as a \"leveler\" of knowledge gaps.\" (2010, p. 2) This is widely due to the fact that information on Web 2.0 is written in layman's terms. The content is created by those individuals who have an understanding of the information, but who are also able to tailor the articles towards a more general audience.\n\nStill, the knowledge gap may still exist even with the emergence of Web 2.0. The disenfranchised group, in this situation, the group with lower SES, must still be motivated to get the information to close the gap. Also, information about a given subject must be given. Without the content being provided, Web 2.0 will not be much of a help. However, if the content is provided, Web 2.0 has allowed the readers to be more interactive and talk with others online, through discussion boards, forums and blogs. The results of the research conducted by Corley and Scheufele are a clear call to action for researchers to investigate non-traditional ways of connecting with lay audiences about emerging technologies.\n\nOverall, studies show the introduction of Web 2.0 may help in closing the knowledge gap because the content that traditionally those with lower SES could not reach, can now be understood because it is written in layman's terms. Web 2.0 has helped because:\n\n\nThere are now three existing competing hypotheses: 1) Media Malaise hypothesis (that predicts a general negative effect), 2) the Virtuous Circle hypothesis (that predicts a general positive effect), and 3) the Differential Effect hypothesis (that predicts a positive effect from newspapers, and a null or negative effect from television)\" (Fraile, 2011). Three types of media outlets have been used to examine the media effects on knowledge gap: 1) Television – knowledge gap between lower and higher education groups are greater among light television users compared to heavy television users (Eveland, 2000), 2) Newspaper – the exposure to newspaper can potentially reinforce the knowledge gap in politics for different SES groups since reading newspaper requires literacy ability to effectively understand the information (Jerit \"et al.\", 2006), while other studies suggest that exposure to newspaper actually slightly decreases the knowledge gap rather than increasing it (Eveland, 2000), and 3) Internet - internet exposure increases public's general knowledge in health issues (Shim, 2008).\n\n"}
{"id": "48050093", "url": "https://en.wikipedia.org/wiki?curid=48050093", "title": "Learning nugget", "text": "Learning nugget\n\nLearning nuggets is a standalone mini learning activity, usually less than 5 minutes in length, that would vary in size and scope that learners undertake in a particular context in order to attain specific learning outcomes A learning nugget task will take a prescribed length of time and may, or may not be assessed. Nuggets should be designed with a particular approach to learning and teaching in mind (Conole & Fill, 2005)\n\nLearning nuggets are the essential elements of the Subscription Learning approach. In this context, the learning happens through a stream of intermittent nuggets which involves a variety of learning-related events which include \"content presentation, diagnostics, scenario-based questions, job aids, reflection questions, assignments, discussions, etc. The nuggets are delivered to the learner in many format like email, text message, smart-phone notifications, or any other form of prompting. They are designed to be delivered on predetermined intervals to support learning. The series of learning nuggets are called learning threads. For utmost effective learning, sending a learning nugget could be dynamically triggered by many factors like learners' leaning need, results of a learning assessment or learners' performance.\n\nFujitsu and MIT described some examples of Learning Nuggets as follows:\n\n\n"}
{"id": "1496061", "url": "https://en.wikipedia.org/wiki?curid=1496061", "title": "List of Unified Modeling Language tools", "text": "List of Unified Modeling Language tools\n\nThis article compares UML tools. UML tools are software applications which support some functions of the Unified Modeling Language.\n"}
{"id": "7697949", "url": "https://en.wikipedia.org/wiki?curid=7697949", "title": "List of technology centers", "text": "List of technology centers\n\nThis is a list of technology centers throughout the world. Governmental planners and business networks like to use the name \"silicon\" or \"valley\" to describe their own areas as a result of the success of Silicon Valley in California. Nevertheless, there are a few qualitative differences between these places, and metrics may be applied to measure their dominance.\n\nThese metrics include:\n\n\nCameroon\n\nEgypt\n\nKenya\n\nMauritius\n\nMorocco\n\nSouth Africa\n\nZambia\n\nBolivia\n\nBrazil\n\nCanada\nChile\nGuatemala\n\nMexico\n\nUnited States\n\nChina\n\nHong Kong\n\nIndia\n\nIran\n\n\nIsrael\n\nJapan\n\nMalaysia\n\nMyanmar\n\n\nPakistan\n\nPhilippines\n\nQatar\n\nSaudi Arabia\n\nSingapore\n\nSouth Korea\n\nTaiwan\n\n\nThailand\n\nUnited Arab Emirates\n\nVietnam\n\n\nAustria\n\nBelarus\n\nCzech Republic\n\nFinland\n\nFrance\nGermany\n\nHungary\nIreland\n\nItaly\n\nNetherlands\n\nPortugal\n\nRussia\n\nRomania\n\nSlovakia\n\nSpain\n\nSweden\n\nTurkey\n\nUnited Kingdom\n\n\nUkraine\n\n\nThe following list contains places with \"Silicon\" names, that is, places with nicknames inspired by the \"Silicon Valley\" nickname given to part of the San Francisco Bay Area:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUkraine\n\n\n\n\n\n\n"}
{"id": "9402865", "url": "https://en.wikipedia.org/wiki?curid=9402865", "title": "List of thermal conductivities", "text": "List of thermal conductivities\n\nIn heat transfer, the thermal conductivity of a substance, \"k\", is an intensive property that indicates its ability to conduct heat.\n\nThermal conductivity is often measured with laser flash analysis. Alternative measurements are also established.\n\nMixtures may have variable thermal conductivities due to composition. Note that for gases in usual conditions, heat transfer by advection (caused by convection or turbulence for instance) is the dominant mechanism compared to conduction.\n\nThis table shows thermal conductivity in SI units of watts per metre-kelvin (W·m·K). Some measurements use the imperial unit BTUs per foot per hour per degree Fahrenheit ( = \n\nThis concerns materials at atmospheric pressure and around .\n\n"}
{"id": "7986300", "url": "https://en.wikipedia.org/wiki?curid=7986300", "title": "MEETin", "text": "MEETin\n\nMEETin.org, or the MEETin group, is a web-based social community dedicated to providing a casual social environment for people without charging membership fees, or otherwise profiting from events. Founded in Washington, DC in 2003, the MEETin group now spans around 90 cities worldwide and has over 90,000 members.\n\nThe MEETin group was established by Mikey Heard on 14 March 2003, in Washington, DC, to connect people socially in the city. As the DC chapter became increasingly active, another 16 chapters were founded in other US cities, which included many of the largest MEETin chapters to this day. Although MEETin began as a Yahoo Group, Heard soon constructed a new web site to deal with the expansion in membership and locations. This web site now defines the identity of the MEETin group and the infrastructure is still very much in use today.\n\nThe MEETin \"Vibe\" represents the general culture and attitude of the MEETin group. Each chapter is run by local volunteers with the main purpose of bringing people together to find new friends in their city. The MEETin group also help connect cities around the globe, and allows members to quickly re-establish their social network across various MEETin cities. As an integral part of the MEETin Vibe, members are encouraged to organize and participate in social events in any given MEETin city, without the pressures of business networking or \"pick-up\" scenes. Further, members are also forbidden to organize events from which they can profit, either financially or by gain of goods or services.\n\nMEETin has been recognized in both the \"Washington Post\" and \"Business Week\".\n\nEvents can be organized by any MEETin member, or co-hosted by two members. Leaders will assist in hosting of events as necessary. Interest groups can also be established on each local MEETin chapter to facilitate specially-catered events, such as film viewing, dinner nights, and book clubs. It was specifically designed to not be a dating service. Events are created by any member by posting it on the local group's web site, and events have no fees or dues.\n"}
{"id": "40277431", "url": "https://en.wikipedia.org/wiki?curid=40277431", "title": "Ministry of Science and Technology (Bangladesh)", "text": "Ministry of Science and Technology (Bangladesh)\n\nThe Ministry of Science and Technology (; \"Bijñāna ō prayukti Montronaloya\") (abbreviated as MOST) is a ministry of the government of Bangladesh which coordinates science and technology activities in Bangladesh.\n\nThe principle agenda of the Ministry of Science and Technology is socio-economic development of Bangladesh through research, development, extension and successful utilization of Science and Technology.\n\n\n"}
{"id": "51682311", "url": "https://en.wikipedia.org/wiki?curid=51682311", "title": "Multefire", "text": "Multefire\n\nMulteFire™ is an LTE-based technology that operates standalone in unlicensed and shared spectrum, including the global 5 GHz band. Based on 3GPP Release 13 and 14, MulteFire technology supports Listen-Before-Talk for fair co-existence with Wi-Fi and other technologies operating in the same spectrum. It supports private LTE and neutral host deployment models. Target vertical markets include industrial IoT, enterprise, cable, and various other vertical markets.\n\nThe MulteFire Release 1.0 specification was developed by the MulteFire Alliance, an independent, diverse and international member-driven consortium. Release 1.0 was published to MulteFire Alliance members in January 2017 and was made publicly available in April 2017. The MulteFire Alliance is currently working on Release 1.1 which will add further optimizations for IoT and new spectrum bands.\n\nAccording to Harbor Research in its published white paper, the market opportunity for private LTE networks for industrial and commercial IoT will reach $118.5 billion in 2023. It also reported that the total addressable revenue for Enterprise markets deploying private and neutral host LTE with MulteFire will reach $5.7 billion by 2025.  \n\nThe MulteFire Alliance has grown to more than 40 members. Its board members include Boingo Wireless, CableLabs, Ericsson, Huawei, Intel, Nokia, Qualcomm and SoftBank. The organization is open to any company with an interest in advancing LTE and cellular technology in unlicensed and shared spectrum.  \n\n"}
{"id": "41497", "url": "https://en.wikipedia.org/wiki?curid=41497", "title": "PCS switching center", "text": "PCS switching center\n\nPCS switching center: In personal communications service, a facility that (a) supports access-independent call control/service control, and connection control (switching) functions, and (b) is responsible for interconnection of access and network systems to support end-to-end services. \n\n\"Note 1:\" The PCS switching center represents a collection of one or more network elements. \n\n\"Note 2:\" The term \"center\" does not imply a physical location.\n"}
{"id": "46735444", "url": "https://en.wikipedia.org/wiki?curid=46735444", "title": "Polaris Networks", "text": "Polaris Networks\n\nPolaris Networks is a privately held company founded in 2003 and located in San Jose, California. It focuses on developing networking protocol software, and its products primarily include wireless protocol test tools and emulators for 3GPP LTE networks.\n\nIn 2012, CERN selected the xTCA Test Tools developed by Polaris Networks for the internal testing of their xTCA systems, including those of the Large Hadron Collider.\n\nIn April 2013, Polaris Networks announced the cloud-based deployment of their NetEPC, a carrier-grade EPC which combines the functionality of the MME, SGW, PGW, HSS and PCRF into a single high-availability platform. And in June 2013, the Public Safety Communications Research Program used the Polaris Networks NetEPC to demonstrate deployable LTE at the Public Safety Broadband Stakeholder Conference in Westminster, Colorado. In June 2018, Polaris Networks and Nemergent Solutions completed interoperability tests between Polaris Networks’ NetEPC and Nemergent’s Mission Critical Services (MCS) application server.\n\n"}
{"id": "45047140", "url": "https://en.wikipedia.org/wiki?curid=45047140", "title": "QoS Class Identifier", "text": "QoS Class Identifier\n\nQoS Class Identifier (QCI) is a mechanism used in 3GPP Long Term Evolution (LTE) networks to ensure bearer traffic is allocated appropriate Quality of Service (QoS). Different bearer traffic requires different QoS and therefore different QCI values. QCI value 9 is typically used for the default bearer of a UE/PDN for non privileged subscribers.\n\nTo ensure that bearer traffic in LTE networks is appropriately handled, a mechanism is needed to classify the different types of bearers into different classes, with each class having appropriate QoS parameters for the traffic type. Examples of the QoS parameters include Guaranteed Bit Rate (GBR) or non-Guaranteed Bit Rate (non-GBR), Priority Handling, Packet Delay Budget and Packet Error Loss rate. This overall mechanism is called QCI.\n\nThe QoS concept as used in LTE networks is class-based, where each bearer type is assigned one QoS Class Identifier (QCI) by the network. The QCI is a scalar that is used within the access network (namely the eNodeB) as a reference to node specific parameters that control packet forwarding treatment, for example scheduling weight, admission thresholds and link-layer protocol configuration.\n\nThe QCI is also mapped to transport network layer parameters in the relevant Evolved Packet Core (EPC) core network nodes (for example, the PDN Gateway (P-GW), Mobility Management Entity (MME) and Policy and Charging Rules Function (PCRF)), by preconfigured QCI to Differentiated Services Code Point (DSCP) mapping. \n\nAccording to 3GPP TS 23.203, 9 QCI values in Rel-8 (13 QCIs Rel-12, 15 QCIs Rel-14) are standardized and associated with QCI characteristics in terms of packet forwarding treatment that the bearer traffic receives edge-to-edge between the UE and the P-GW. Scheduling priority, resource type, packet delay budget and packet error loss rate are the set of characteristics defined by the 3GPP standard and they should be understood as guidelines for the pre-configuration of node specific parameters to ensure that applications/services mapped to a given QCI receive the same level of QoS in multi-vendor environments as well as in roaming scenarios. The QCI characteristics are not signalled on any interface.\n\nThe following table illustrates the standardized characteristics as defined in the 3GPP TS 23.203 standard \"Policy and Charging Control Architecture\".\n\nEvery QCI (GBR and Non-GBR) is associated with a Priority level. Priority level 0.5 is the highest Priority level. If congestion is encountered, the lowest Priority level traffic would be the first to be discarded.\n\nQCI-65, QCI-66, QCI-69 and QCI-70 were introduced in 3GPP TS 23.203 Rel-12.\n\nQCI-75 and QCI-79 were introduced in 3GPP TS 23.203 Rel-14.\n\n"}
{"id": "45636259", "url": "https://en.wikipedia.org/wiki?curid=45636259", "title": "SRVCC", "text": "SRVCC\n\nSingle Radio Voice Call Continuity (SRVCC) provides an interim solution for handing over VoLTE (Voice over LTE) to 2G/3G networks. The voice calls on LTE network are meant to be packet switched calls which use IMS system to be made. To make it inter operable with existing networks, these calls are to be handed over to Circuit switched calls in GSM/WCDMA networks. QoS is ensured by SRVCC operators for calls made.\n\n3GPP also standardized SRVCC to provide easy handovers from LTE network to GSM/UMTS network.\n"}
{"id": "56797234", "url": "https://en.wikipedia.org/wiki?curid=56797234", "title": "SS Australasia", "text": "SS Australasia\n\nThe Australasia was a wooden hulled steamship that sank on October 18, 1896 in Lake Michigan near the town of Sevastopol, Door County, Wisconsin, United States, after burning off Cana Island. On July 3, 2013 the wreck of the \"Australasia\" was added to the National Register of Historic Places.\n\nThe \"Australasia\" (Official number 106302) was built in 1884 in West Bay City, Michigan by the shipyard owned by Captain James Davidson. She was built for the Davidson Steamship Company which was also owned by Captain Davidson. At a length of the \"Australasia\" was one of the largest wooden ships ever built; her beam was wide and her cargo hold was deep. She was powered by a fore and aft compound engine which was fueled by two coal burning Scotch marine boilers. She had a gross register tonnage of 1829.32 tons and a net register tonnage of 1539.20 tons.\n\nOn September 17, 1884 the \"Australasia\" was launched as hull number #9. At the time of her launch the \"Australasia\" was the largest wooden hulled ship in the world. Because of her enormous size the \"Australasia\" needed iron cross bracing, an iron keelson, iron plates, and several iron arches to increase her strength.\n\nShe was used to haul bulk cargoes such as iron ore, coal, grain and sometimes salt. She could carry these cargoes so efficiently that she earned a fortune for her owners at a time when small, less versatile wooden vessels were quickly being replaced by larger, and stronger iron or steel vessels. Just like all ships owned by Captain Davidson, the \"Australasia\" used to tow a wooden schooner barge.\n\nOn October 17, 1896, the \"Australasia\" was bound from Lake Erie to Milwaukee, Wisconsin, carrying 2,200 tons of soft coal. At around 6:00 p.m. near Baileys Harbor, the crew of the \"Australasia\" discovered \"a fire beneath the texas on the main deck\". They attempted to fight the blaze but failed. The crew abandoned the \"Australasia\" before she reached Jacksonport, Wisconsin. At 10:30 p.m., the \"Australasia\" was about four hours off Jacksonport when the tugboat \"John Leathem\" came upon the struggling steamer. The \"Leathem\" began towing the \"Australasia\" to shore, but the hawser connecting them kept burning through. At 9:00 a.m. on October 18, 1896, the crew of the \"Leathem\" gave up trying to salvage her and instead dragged her onto the beach in of water south of Cave Point. Her crew decided to scuttle her; they did this by ramming a hole in the \"Australasia\"&apos;s side with the \"Leathem\"&apos;s stem. She was left heading north by northwest. She burned until the night of October 18, 1896.\n\nThe \"Australasia\" was declared a total loss. Much of her cargo of soft coal and machinery was salvaged, however her hull was beyond repair and was abandoned. Today her lower hull lies mostly buried in sand 15 to 20 feet of water off Whitefish Dunes State Park. Because most of her hull remains buried in sand, there is the possibility that different hull sections may be uncovered which may reveal more significant information about her construction. Not a trace of her cargo is visible on the site of her wreck, but traces of coal is visible on a beach nearby. The wreck of the \"Australasia\" is rarely visited by divers which means that very little site disturbance to the site has occurred. Close by are the wrecks of several other ships including the early steel freighter \"Lakeland\", the large wooden bulk carrier \"Frank O'Connor, the wooden steamer \"Louisiana\" which was lost during the Great Lakes Storm of 1913, the schooner \"Christina Nilsson\" and the steamboat \"Joys\".\n"}
{"id": "28743", "url": "https://en.wikipedia.org/wiki?curid=28743", "title": "Slide rule", "text": "Slide rule\n\nThe slide rule, also known colloquially in the United States as a slipstick, is a mechanical analog computer. The slide rule is used primarily for multiplication and division, and also for functions such as exponents, roots, logarithms and trigonometry, but typically not for addition or subtraction. Though similar in name and appearance to a standard ruler, the slide rule is not meant to be used for measuring length or drawing straight lines.\n\nSlide rules exist in a diverse range of styles and generally appear in a linear or circular form with a standardized set of markings (scales) essential to performing mathematical computations. Slide rules manufactured for specialized fields such as aviation or finance typically feature additional scales that aid in calculations common to those fields.\n\nAt its simplest, each number to be multiplied is represented by a length on a sliding ruler. As the rulers each have a logarithmic scale, it is possible to align them to read the sum of the logarithms, and hence calculate the product of the two numbers.\n\nThe Reverend William Oughtred and others developed the slide rule in the 17th century based on the emerging work on logarithms by John Napier. Before the advent of the electronic calculator, it was the most commonly used calculation tool in science and engineering. The use of slide rules continued to grow through the 1950s and 1960s even as computers were being gradually introduced; but around 1974 the handheld electronic scientific calculator made them largely obsolete and most suppliers left the business.\n\nIn its most basic form, the slide rule uses two logarithmic scales to allow rapid multiplication and division of numbers. These common operations can be time-consuming and error-prone when done on paper. More elaborate slide rules allow other calculations, such as square roots, exponentials, logarithms, and trigonometric functions.\n\nScales may be grouped in decades, which are numbers ranging from 1 to 10 (i.e. 10 to 10). Thus single decade scales C and D range from 1 to 10 across the entire width of the slide rule while double decade scales A and B range from 1 to 100 over the width of the slide rule.\n\nIn general, mathematical calculations are performed by aligning a mark on the sliding central strip with a mark on one of the fixed strips, and then observing the relative positions of other marks on the strips. Numbers aligned with the marks give the approximate value of the product, quotient, or other calculated result.\n\nThe user determines the location of the decimal point in the result, based on mental estimation. Scientific notation is used to track the decimal point in more formal calculations. Addition and subtraction steps in a calculation are generally done mentally or on paper, not on the slide rule.\n\nMost slide rules consist of three linear strips of the same length, aligned in parallel and interlocked so that the central strip can be moved lengthwise relative to the other two. The outer two strips are fixed so that their relative positions do not change.\n\nSome slide rules (\"duplex\" models) have scales on both sides of the rule and slide strip, others on one side of the outer strips and both sides of the slide strip (which can usually be pulled out, flipped over and reinserted for convenience), still others on one side only (\"simplex\" rules). A sliding with a vertical alignment line is used to find corresponding points on scales that are not adjacent to each other or, in duplex models, are on the other side of the rule. The cursor can also record an intermediate result on any of the scales.\n\nA logarithm transforms the operations of multiplication and division to addition and subtraction according to the rules formula_1 and formula_2.\nMoving the top scale to the right by a distance of formula_3, by matching the beginning of the top scale with the label formula_4 on the bottom, aligns each number formula_5, at position formula_6 on the top scale, with the number at position formula_7 on the bottom scale. Because formula_8, this position on the bottom scale gives formula_9, the product of formula_4 and formula_5. For example, to calculate 3×2, the 1 on the top scale is moved to the 2 on the bottom scale. The answer, 6, is read off the bottom scale where 3 is on the top scale. In general, the 1 on the top is moved to a factor on the bottom, and the answer is read off the bottom where the other factor is on the top. This works because the distances from the \"1\" are proportional to the logarithms of the marked values:\n\nOperations may go \"off the scale;\" for example, the diagram above shows that the slide rule has not positioned the 7 on the upper scale above any number on the lower scale, so it does not give any answer for 2×7. In such cases, the user may slide the upper scale to the left until its right index aligns with the 2, effectively dividing by 10 (by subtracting the full length of the C-scale) and then multiplying by 7, as in the illustration below:\n\nHere the user of the slide rule must remember to adjust the decimal point appropriately to correct the final answer. We wanted to find 2×7, but instead we calculated (2/10)×7=0.2×7=1.4. So the true answer is not 1.4 but 14. Resetting the slide is not the only way to handle multiplications that would result in off-scale results, such as 2×7; some other methods are:\n\nMethod 1 is easy to understand, but entails a loss of precision. Method 3 has the advantage that it only involves two scales.\n\nThe illustration below demonstrates the computation of 5.5/2. The 2 on the top scale is placed over the 5.5 on the bottom scale. The 1 on the top scale lies above the quotient, 2.75. There is more than one method for doing division, but the method presented here has the advantage that the final result cannot be off-scale, because one has a choice of using the 1 at either end.\n\nIn addition to the logarithmic scales, some slide rules have other mathematical functions encoded on other auxiliary scales. The most popular were trigonometric, usually sine and tangent, common logarithm (log) (for taking the log of a value on a multiplier scale), natural logarithm (ln) and exponential (\"e\") scales. Some rules include a Pythagorean scale, to figure sides of triangles, and a scale to figure circles. Others feature scales for calculating hyperbolic functions. On linear rules, the scales and their labeling are highly standardized, with variation usually occurring only in terms of which scales are included and in what order:\n\nThe Binary Slide Rule manufactured by Gilson in 1931 performed an addition and subtraction function limited to fractions.\n\nThere are single-decade (C and D), double-decade (A and B), and triple-decade (K) scales. To compute formula_12, for example, locate x on the D scale and read its square on the A scale. Inverting this process allows square roots to be found, and similarly for the powers 3, 1/3, 2/3, and 3/2. Care must be taken when the base, x, is found in more than one place on its scale. For instance, there are two nines on the A scale; to find the square root of nine, use the first one; the second one gives the square root of 90.\n\nFor formula_13 problems, use the LL scales. When several LL scales are present, use the one with \"x\" on it. First, align the leftmost 1 on the C scale with x on the LL scale. Then, find \"y\" on the C scale and go down to the LL scale with \"x\" on it. That scale will indicate the answer. If \"y\" is \"off the scale,\" locate formula_14 and square it using the A and B scales as described above. Alternatively, use the rightmost 1 on the C scale, and read the answer off the next higher LL scale. For example, aligning the rightmost 1 on the C scale with 2 on the LL2 scale, 3 on the C scale lines up with 8 on the LL3 scale.\n\nTo extract a cube root using a slide rule with only C/D and A/B scales, align 1 on the B cursor with the base number on the A scale (taking care as always to distinguish between the lower and upper halves of the A scale). Slide the cursor until the number on the D scale which is against 1 on the C cursor is the same as the number on the B cursor which is against the base number on the A scale. (Examples: A 8, B 2, C 1, D 2; A 27, B 3, C 1, D 3.)\n\nThe S, T, and ST scales are used for trig functions and multiples of trig functions, for angles in degrees.\n\nFor angles from around 5.7 up to 90 degrees, sines are found by comparing the S scale with C (or D) scale; though on many closed-body rules the S scale relates to the A scale instead, and what follows must be adjusted appropriately. The S scale has a second set of angles (sometimes in a different color), which run in the opposite direction, and are used for cosines. Tangents are found by comparing the T scale with the C (or D) scale for angles less than 45 degrees. For angles greater than 45 degrees the CI scale is used. Common forms such as formula_15 can be read directly from \"x\" on the S scale to the result on the D scale, when the C-scale index is set at \"k\". For angles below 5.7 degrees, sines, tangents, and radians are approximately equal, and are found on the ST or SRT (sines, radians, and tangents) scale, or simply divided by 57.3 degrees/radian. Inverse trigonometric functions are found by reversing the process.\n\nMany slide rules have S, T, and ST scales marked with degrees and minutes (e.g. some Keuffel and Esser models, late-model Teledyne-Post Mannheim-type rules). So-called \"decitrig\" models use decimal fractions of degrees instead.\n\nBase-10 logarithms and exponentials are found using the L scale, which is linear. Some slide rules have a Ln scale, which is for base e. Logarithms to any other base can be calculated by reversing the procedure for calculating powers of a number. For example, log2 values can be determined by lining up either leftmost or rightmost 1 on the C scale with 2 on the LL2 scale, finding the number whose logarithm is to be calculated on the corresponding LL scale, and reading the log2 value on the C scale.\n\nSlide rules are not typically used for addition and subtraction, but it is nevertheless possible to do so using two different techniques.\n\nThe first method to perform addition and subtraction on the C and D (or any comparable scales) requires converting the problem into one of division. For addition, the quotient of the two variables plus one times the divisor equals their sum:\n\nFor subtraction, the quotient of the two variables minus one times the divisor equals their difference:\n\nThis method is similar to the addition/subtraction technique used for high-speed electronic circuits with the logarithmic number system in specialized computer applications like the Gravity Pipe (GRAPE) supercomputer and hidden Markov models.\n\nThe second method utilizes a sliding linear L scale available on some models. Addition and subtraction are performed by sliding the cursor left (for subtraction) or right (for addition) then returning the slide to 0 to read the result.\n\nThe width of the slide rule is quoted in terms of the nominal width of the scales. Scales on the most common \"10-inch\" models are actually 25 cm, as they were made to metric standards, though some rules offer slightly extended scales to simplify manipulation when a result overflowed. Pocket rules are typically 5 inches. Models a couple of metres wide were sold to be hung in classrooms for teaching purposes.\n\nTypically the divisions mark a scale to a precision of two significant figures, and the user estimates the third figure. Some high-end slide rules have magnifier cursors that make the markings easier to see. Such cursors can effectively double the accuracy of readings, permitting a 10-inch slide rule to serve as well as a 20-inch.\n\nVarious other conveniences have been developed. Trigonometric scales are sometimes dual-labeled, in black and red, with complementary angles, the so-called \"Darmstadt\" style. Duplex slide rules often duplicate some of the scales on the back. Scales are often \"split\" to get higher accuracy.\n\nCircular slide rules come in two basic types, one with two cursors, and another with a free dish and one cursor. The dual cursor versions perform multiplication and division by holding a fast angle between the cursors as they are rotated around the dial. The onefold cursor version operates more like the standard slide rule through the appropriate alignment of the scales.\n\nThe basic advantage of a circular slide rule is that the widest dimension of the tool was reduced by a factor of about 3 (i.e. by π). For example, a 10 cm circular would have a maximum precision approximately equal to a 31.4 cm ordinary slide rule. Circular slide rules also eliminate \"off-scale\" calculations, because the scales were designed to \"wrap around\"; they never have to be reoriented when results are near 1.0—the rule is always on scale. However, for non-cyclical non-spiral scales such as S, T, and LL's, the scale width is narrowed to make room for end margins.\n\nCircular slide rules are mechanically more rugged and smoother-moving, but their scale alignment precision is sensitive to the centering of a central pivot; a minute 0.1 mm off-centre of the pivot can result in a 0.2 mm worst case alignment error. The pivot, however, does prevent scratching of the face and cursors. The highest accuracy scales are placed on the outer rings. Rather than \"split\" scales, high-end circular rules use spiral scales for more complex operations like log-of-log scales. One eight-inch premium circular rule had a 50-inch spiral log-log scale. Around 1970, an inexpensive model from B. C. Boykin (Model 510) featured 20 scales, including 50-inch C-D (multiplication) and log scales. The RotaRule featured a friction brake for the cursor.\n\nThe main disadvantages of circular slide rules are the difficulty in locating figures along a dish, and limited number of scales. Another drawback of circular slide rules is that less-important scales are closer to the center, and have lower precisions. Most students learned slide rule use on the linear slide rules, and did not find reason to switch.\n\nOne slide rule remaining in daily use around the world is the E6B. This is a circular slide rule first created in the 1930s for aircraft pilots to help with dead reckoning. With the aid of scales printed on the frame it also helps with such miscellaneous tasks as converting time, distance, speed, and temperature values, compass errors, and calculating fuel use. The so-called \"prayer wheel\" is still available in flight shops, and remains widely used. While GPS has reduced the use of dead reckoning for aerial navigation, and handheld calculators have taken over many of its functions, the E6B remains widely used as a primary or backup device and the majority of flight schools demand that their students have some degree of proficiency in its use.\n\nProportion wheels are simple circular slide rules used in graphic design to calculate aspect ratios. Lining up the original and desired size values on the inner and outer wheels will display their ratio as a percentage in a small window. They are not as common since the advent of computerized layout, but \n\nIn 1952, Swiss watch company Breitling introduced a pilot's wristwatch with an integrated circular slide rule specialized for flight calculations: the Breitling Navitimer. The Navitimer circular rule, referred to by Breitling as a \"navigation computer\", featured airspeed, rate/time of climb/descent, flight time, distance, and fuel consumption functions, as well as kilometer—nautical mile and gallon—liter fuel amount conversion functions.\nThere are two main types of cylindrical slide rules: those with helical scales such as the Fuller, the Otis King and the Bygrave slide rule, and those with bars, such as the Thacher and some Loga models. In either case, the advantage is a much longer scale, and hence potentially greater precision, than afforded by a straight or circular rule.\nTraditionally slide rules were made out of hard wood such as mahogany or boxwood with cursors of glass and metal. At least one high precision instrument was made of steel.\n\nIn 1895, a Japanese firm, Hemmi, started to make slide rules from bamboo, which had the advantages of being dimensionally stable, strong, and naturally self-lubricating. These bamboo slide rules were introduced in Sweden in September, 1933, and probably only a little earlier in Germany. Scales were made of celluloid, plastic, or painted aluminium. Later cursors were acrylics or polycarbonates sliding on Teflon bearings.\n\nAll premium slide rules had numbers and scales engraved, and then filled with paint or other resin. Painted or imprinted slide rules were viewed as inferior because the markings could wear off. Nevertheless, Pickett, probably America's most successful slide rule company, made all printed scales. Premium slide rules included clever catches so the rule would not fall apart by accident, and bumpers to protect the scales and cursor from rubbing on tabletops. The recommended cleaning method for engraved markings is to scrub lightly with steel-wool. For painted slide rules, use diluted commercial window-cleaning fluid and a soft cloth.\n\nThe slide rule was invented around 1620–1630, shortly after John Napier's publication of the concept of the logarithm. In 1620 Edmund Gunter of Oxford developed a calculating device with a single logarithmic scale; with additional measuring tools it could be used to multiply and divide. In c. 1622, William Oughtred of Cambridge combined two handheld Gunter rules to make a device that is recognizably the modern slide rule. Like his contemporary at Cambridge, Isaac Newton, Oughtred taught his ideas privately to his students. Also like Newton, he became involved in a vitriolic controversy over priority, with his one-time student Richard Delamain and the prior claims of Wingate. Oughtred's ideas were only made public in publications of his student William Forster in 1632 and 1653.\n\nIn 1677, Henry Coggeshall created a two-foot folding rule for timber measure, called the Coggeshall slide rule, expanding the slide rule's use beyond mathematical inquiry.\n\nIn 1722, Warner introduced the two- and three-decade scales, and in 1755 Everard included an inverted scale; a slide rule containing all of these scales is usually known as a \"polyphase\" rule.\n\nIn 1815, Peter Mark Roget invented the log log slide rule, which included a scale displaying the logarithm of the logarithm. This allowed the user to directly perform calculations involving roots and exponents. This was especially useful for fractional powers.\n\nIn 1821, Nathaniel Bowditch, described in the \"American Practical Navigator\" a \"sliding rule\" that contained scales trigonometric functions on the fixed part and a line of log-sines and log-tans on the slider used to solve navigation problems.\n\nIn 1845, Paul Cameron of Glasgow introduced a nautical slide rule capable of answering navigation questions, including right ascension and declination of the sun and principal stars.\n\nA more modern form of slide rule was created in 1859 by French artillery lieutenant Amédée Mannheim, \"who was fortunate in having his rule made by a firm of national reputation and in having it adopted by the French Artillery.\" It was around this time that engineering became a recognized profession, resulting in widespread slide rule use in Europe–but not in the United States. There, Edwin Thacher's cylindrical rule took hold after 1881. The duplex rule was invented by William Cox in 1891, and was produced by Keuffel and Esser Co. of New York.\n\nAstronomical work also required precise computations, and, in 19th-century Germany, a steel slide rule about two meters long was used at one observatory. It had a microscope attached, giving it accuracy to six decimal places..\n\nThroughout the 1950s and 1960s, the slide rule was the symbol of the engineer's profession in the same way the stethoscope is that of the medical profession.\n\nGerman rocket scientist Wernher von Braun bought two \"Nestler\" slide rules in the 1930s. Ten years later he brought them with him when he moved to the U.S. after World War II to work on the American space effort. Throughout his life he never used any other slide rule. He used his two Nestlers while heading the NASA program that landed a man on the moon in July 1969.\n\nAluminium Pickett-brand slide rules were carried on Project Apollo space missions. The model N600-ES owned by Buzz Aldrin that flew with him to the moon on Apollo 11 was sold at auction in 2007. The model N600-ES taken along on Apollo 13 in 1970 is owned by the National Air and Space Museum.\n\nSome engineering students and engineers carried ten-inch slide rules in belt holsters, a common sight on campuses even into the mid-1970s. Until the advent of the pocket digital calculator, students also might keep a ten- or twenty-inch rule for precision work at home or the office while carrying a five-inch pocket slide rule around with them.\n\nIn 2004, education researchers David B. Sher and Dean C. Nataro conceived a new type of slide rule based on \"prosthaphaeresis\", an algorithm for rapidly computing products that predates logarithms. However, there has been little practical interest in constructing one beyond the initial prototype.\n\nSlide rules have often been specialized to varying degrees for their field of use, such as excise, proof calculation, engineering, navigation, etc., but some slide rules are extremely specialized for very narrow applications. For example, the John Rabone & Sons 1892 catalog lists a \"Measuring Tape and Cattle Gauge\", a device to estimate the weight of a cow from its measurements.\n\nThere were many specialized slide rules for photographic applications; for example, the actinograph of Hurter and Driffield was a two-slide boxwood, brass, and cardboard device for estimating exposure from time of day, time of year, and latitude.\n\nSpecialized slide rules were invented for various forms of engineering, business and banking. These often had common calculations directly expressed as special scales, for example loan calculations, optimal purchase quantities, or particular engineering equations. For example, the Fisher Controls company distributed a customized slide rule adapted to solving the equations used for selecting the proper size of industrial flow control valves.\n\nPilot balloon slide rules were used by meteorologists in weather services to determine the upper wind velocities from an ascending hydrogen or helium filled pilot balloon.\n\nIn World War II, bombardiers and navigators who required quick calculations often used specialized slide rules. One office of the U.S. Navy actually designed a generic slide rule \"chassis\" with an aluminium body and plastic cursor into which celluloid cards (printed on both sides) could be placed for special calculations. The process was invented to calculate range, fuel use and altitude for aircraft, and then adapted to many other purposes.\n\nThe E6-B is a circular slide rule used by pilots and navigators.\n\nCircular slide rules to estimate ovulation dates and fertility are known as \"wheel calculators\".\n\nThe importance of the slide rule began to diminish as electronic computers, a new but rare resource in the 1950s, became more widely available to technical workers during the 1960s. (See History of computing hardware (1960s–present).)\n\nAnother step away from slide rules was the introduction of relatively inexpensive electronic desktop scientific calculators. The first included the Wang Laboratories LOCI-2, introduced in 1965, which used logarithms for multiplication and division; and the Hewlett-Packard HP 9100A, introduced in 1968. Both of these were programmable and provided exponential and logarithmic functions; the HP had trigonometric functions (sine, cosine, and tangent) and hyperbolic trigonometric functions as well. The HP used the CORDIC (coordinate rotation digital computer) algorithm, which allows for calculation of trigonometric functions using only shift and add operations. This method facilitated the development of ever smaller scientific calculators.\n\nAs with mainframe computing, the availability of these machines did not significantly affect the ubiquitous use of the slide rule until cheap hand held scientific electronic calculators became available in the mid-1970s, at which point, it rapidly declined. \nThe pocket-sized Hewlett-Packard HP-35 scientific calculator was the first handheld device of its type, but it cost US$395 in 1972. This was justifiable for some engineering professionals but too expensive for most students. \nBy 1975, basic four-function electronic calculators could be purchased for less than $50, and, by 1976, the TI-30 scientific calculator was sold for less than $25.\n\nMost people find slide rules difficult to learn and use. Even during their heyday, they never caught on with the general public. Addition and subtraction are not well-supported operations on slide rules and doing a calculation on a slide rule tends to be slower than on a calculator. This led engineers to use mathematical equations that favored operations that were easy on a slide rule over more accurate but complex functions, these approximations could lead to inaccuracies and mistakes. On the other hand, the spatial, manual operation of slide rules cultivates in the user an intuition for numerical relationships and scale that people who have used only digital calculators often lack. A slide rule will also display all the terms of a calculation along with the result, thus eliminating uncertainty about what calculation was actually performed.\n\nA slide rule requires the user to separately compute the order of magnitude of the answer in order to position the decimal point in the results. For example, 1.5 × 30 (which equals 45) will show the same result as 1,500,000 × 0.03 (which equals 45,000). This separate calculation is less likely to lead to extreme calculation errors, but forces the user to keep track of magnitude in short-term memory (which is error-prone), keep notes (which is cumbersome) or reason about it in every step (which distracts from the other calculation requirements).\n\nThe typical arithmetic precision of a slide rule is about three significant digits, compared to many digits on digital calculators. As order of magnitude gets the greatest prominence when using a slide rule, users are less likely to make errors of false precision.\n\nWhen performing a sequence of multiplications or divisions by the same number, the answer can often be determined by merely glancing at the slide rule without any manipulation. This can be especially useful when calculating percentages (e.g. for test scores) or when comparing prices (e.g. in dollars per kilogram). Multiple speed-time-distance calculations can be performed hands-free at a glance with a slide rule. Other useful linear conversions such as pounds to kilograms can be easily marked on the rule and used directly in calculations.\n\nBeing entirely mechanical, a slide rule does not depend on grid electricity or batteries. However, mechanical imprecision in slide rules that were poorly constructed or warped by heat or use will lead to errors.\n\nMany sailors keep slide rules as backups for navigation in case of electric failure or battery depletion on long route segments. Slide rules are still commonly used in aviation, particularly for smaller planes. They are being replaced only by integrated, special purpose and expensive flight computers, and not general-purpose calculators. The E6B circular slide rule used by pilots has been in continuous production and remains available in a variety of models. Some wrist watches designed for aviation use still feature slide rule scales to permit quick calculations. The Citizen Skyhawk AT is a notable example.\n\nEven today, some people prefer a slide rule over an electronic calculator as a practical computing device. Others keep their old slide rules out of a sense of nostalgia, or collect them as a hobby.\n\nA popular collectible model is the Keuffel & Esser \"Deci-Lon\", a premium scientific and engineering slide rule available both in a ten-inch (25 cm) \"regular\" (\"Deci-Lon 10\") and a five-inch \"pocket\" (\"Deci-Lon 5\") variant. Another prized American model is the eight-inch (20 cm) Scientific Instruments circular rule. Of European rules, Faber-Castell's high-end models are the most popular among collectors.\n\nAlthough a great many slide rules are circulating on the market, specimens in good condition tend to be expensive. Many rules found for sale on are damaged or have missing parts, and the seller may not know enough to supply the relevant information. Replacement parts are scarce, expensive, and generally available only for separate purchase on individual collectors' web sites. The Keuffel and Esser rules from the period up to about 1950 are particularly problematic, because the end-pieces on the cursors, made of celluloid, tend to chemically break down over time.\n\nThere are still a handful of sources for brand new slide rules. The Concise Company of Tokyo, which began as a manufacturer of circular slide rules in July 1954, continues to make and sell them today. In September 2009, on-line retailer ThinkGeek introduced its own brand of straight slide rules, described as \"faithful replica[s]\" that are \"individually hand tooled\". These are no longer available in 2012. In addition, Faber-Castell has a number of slide rules still in inventory, available for international purchase through their web store. Proportion wheels are still used in graphic design.\n\nVarious slide rule simulator apps are available for Android and iOS-based smart phones and tablets.\n\nSpecialized slide rules such as the E6B used in aviation, and gunnery slide rules used in laying artillery are still used though no longer on a routine basis. These rules are used as part of the teaching and instruction process as in learning to use them the student also learns about the principles behind the calculations, it also allows the student to be able to use these instruments as a back up in the event that the modern electronics in general use fail.\n\n"}
{"id": "28303167", "url": "https://en.wikipedia.org/wiki?curid=28303167", "title": "Social information seeking", "text": "Social information seeking\n\nSocial information seeking (SIS) is a field of research that involves studying situations, motivations, and methods for people seeking and sharing information in participatory online social sites, such as Yahoo! Answers, Answerbag, WikiAnswers and Twitter as well as building systems for supporting such activities. Highly related topics involve traditional and virtual reference services, information retrieval, information extraction, and knowledge representation.\n\nSocial information seeking is often materialized in online question-answering (QA) websites, which are driven by a community. Such QA sites have emerged in the past few years as an enormous market, so to speak, for the fulfillment of information needs. Estimates of the volume of questions answered are difficult to come by, but it is likely that the number of questions answered on social/community QA (cQA) sites far exceeds the number of questions answered by library reference services, which until recently were one of the few institutional sources for such question answering. cQA sites make their content – questions and associated answers submitted on the site – available on the open web, and indexable by search engines, thus enabling web users to find answers provided for previously asked questions in response to new queries.\n\nThe popularity of such sites have been increasing dramatically for the past several years. Major sites that provide a general platform for questions of all types include Yahoo! Answers, Answerbag and Quora. While other sites that focus on particular fields; for example, StackOverflow (computing). StackOverflow has 3.45 million questions, 1.3 million users and over 6.86 million answers since July 2008 while Quora has 437 thousand questions, 264 thousand users and 979 thousand answers.\n\nSocial Q&A or cQA, according to Shah et al., consists of three components: a mechanism for users to submit questions in natural language, a venue for users to submit answers to questions, and a community built around this exchange. Viewed in that light, online communities have performed a question answering function perhaps since the advent of Usenet and Bulletin Board Systems, so in one sense cQA is nothing new. Websites dedicated to cQA, however, have emerged on the web only within the past few years: the first cQA site was the Korean Naver Knowledge iN, launched in 2002, while the first English-language CQA site was Answerbag, launched in April 2003. Despite this short history, however, cQA has already attracted a great deal of attention from researchers investigating information seeking behaviors, selection of resources, social annotations, user motivations, comparisons with other types of question answering services, and a range of other information-related behaviors.\n\nSome of the interesting and important research questions in this area include:\n\n\nShah et al. provide a detailed research agenda for social Q&A. A new book by Shah presents a more recent and comprehensive information pertaining to SIS.\n\nFriendsourcing is an important component of social question and answering, including how to route questions to friends or others who will most likely answer the question. The important questions include what people’s behaviors are in social networks, especially what kinds of questions people ask from their social networks and how different question types affect the frequency, speed and quality of answers they receive.\n\nMorris et al. (2010) conducted a survey of question and answering within social networks with 624 people, and gathered detailed data about the behavior of Q&A, including frequency, types of questions and answers, and motivations. They found that half (50.6%) of respondents reported having used their status messages to ask a question, which indicated that Q&A on social networks is popular. Also, the types of questions people asked include recommendation, opinion, factual knowledge, rhetorical, etc. And motivations for asking include trust, asking subjective questions, etc. Their analysis also explored the relationships between answer speed and quality, questions’ property and participants’ property. Only a very small portion (6.5%) of the questions were answered, but the 89.3% of the respondents were satisfied with the response time they experienced even though there’s a discrepancy between that and expectation. Also, the responses gathered via social networks appear to be very valuable. Their findings implied design for search tools that could combine the speed and breadth of traditional search engines with the trustworthiness, personalization, and the high engagement of social media Q&A.\n\nPaul et al. (2011) did a study on question and answering on Twitter, and found that out of the 1152 questions they examined, the most popular question types asked on Twitter were rhetorical (42%) and factual (16%). Surprisingly, along with entertainment (29%) and technology (29%) questions, people asked personal and health-related questions (11%). Only 18.7% questions received response, while a handful of questions received a high number of responses. The larger the askers’ network, the more responses she received; however, posting more tweets or posting more frequently did not increase chances of receiving a response. Most often the “follow” relationship between asker and answerer was one-way. Paul et al. also examined what factors of the askers would increase the chance of getting a response and found that more relevant responses are received when there is a mutual relationship between askers and answerers. Intuitively, we would expect this, as mutual relationship would indicate stronger tie strength and hence, more number of relevant answers.\n\nExisting social Q&A services can be characterized from the three perspectives, by the definition of social Q&A as a service involving (1) a method for presenting information needs, (2) a place for responding to information need, and (3) participation as a community.\n\nThese social networks support various friendsourcing behavior, provide information benefits that often times traditional search tools cannot, and also may reinforce social bonds through the process. However, there are many questions and limitations that may prevent people from asking questions on their social networks. For example, they may feel uncomfortable asking questions that are too private, might not want to cost too much other people’s time and effort, or might feel the burden of social debts.\n\nRzeszotarski and Morris (2014) took a novel approach to explore the perceived social costs of friendsourcing on Twitter via monetary choices. They modeled friendsourcing costs across users, and compared it with crowdsourcing on Amazon Mechanical Turk. Their findings suggested interesting design considerations for minimizing social cost by building a hybrid system combining friendsourcing and crowdsourcing with microtask markets.\n\nSometimes, only asking question from people’s own social networks or friends is not enough. If the question is obscure or time sensitive, no members of their social networks may know the answer. For example, this person’s friends might not have expertise in providing evaluations for a specific model of digital camera. Also asking the current wait time for security at the local airport might not be possible if none of this person’s friends are currently at the airport.\n\nNichols and Kang (2012) leveraged Twitter for question and answering with targeted strangers by taking advantage of its public accessibility. In their approach, they mined the public status updates posted on Twitter to find strangers with potentially useful information, and send questions to these strangers to collect responses. As a feasibility study, they collected information regarding response rate, and response time. 42% of users responded to questions from strangers, and 44% of the responses arrived within 30 minutes.\n\nAnother important and unique component of social Q&A system is that it is a community which allows members to form relationships and bonds, so that their behavior in these social Q&A services will also add to their social capital.\n\nGray et al. (2013) explored how bridging social capital, question type and relational closeness influence the perceived usefulness and satisfaction of information obtained through questions asked on Facebook. Their results indicated that bridging social capital could positively predict the perceived utility of the acquired information, meaning that information exchanges on social networks is an effective way of social capital conversion. Also, useful answers are more likely to be received from weak ties than strong ties.\n\nIn order to recommend the most appropriate users to provide answers in a social network, we need to find approaches to detect users' authority in a social network. In the field of information retrieval, there has been a trend of research investigating ways to detect users' authority effectively and accurately in a social network.\n\nCha et al. investigate possible metrics for determining authority users on popular social network Twitter. They propose the following three simple network-based metrics and discuss their usefulness in determining a user's influence.\n\nAn initial analysis of the three aforementioned metrics showed that the users with the highest indegrees and the users with the highest retweet/mention counts were not the same. The top 1% of users by indegree are shown to have very low correlation with the same percentile of users by retweets and by mentions. This implies that follower count is not useful in determining whether a user's tweets get retweeted or whether the other users engage with them.\n\nPal et al. designed features to measure a user's authority on a certain topic. For example, retweet impact refers to how many times a certain user has been retweeted on a certain topic. The impact is dampened by a factor measuring how many times the user had been retweeted by a unique author to avoid the cases when a user has fans who retweet regardless of the content. They first used a clustering approach to find the target cluster which has the highest average score across all features, and used a ranking algorithm to find the most authoritative users within the cluster.\n\nWith these authority detection methods, social Q&A could be more effective in providing accurate answers to askers.\n\nMajor figures:\n"}
{"id": "38728116", "url": "https://en.wikipedia.org/wiki?curid=38728116", "title": "Stauroscope", "text": "Stauroscope\n\nA stauroscope is an optical instrument used in determining the position of the planes of light-vibration in sections of crystals. The word comes from Greek for cross + scope. It was invented by Wolfgang Franz von Kobell in 1855.\n"}
{"id": "16017237", "url": "https://en.wikipedia.org/wiki?curid=16017237", "title": "Style guide", "text": "Style guide\n\nA style guide (or manual of style) is a set of standards for the writing and design of documents, either for general use or for a specific publication, organization, or field. (It is often called a style sheet, though that term has other meanings.)\n\nA style guide establishes and enforces style to improve communication. To do that, it ensures consistency within a document and across multiple documents and enforces best practice in usage and in language composition, visual composition, orthography and typography. For academic and technical documents, a guide may also enforce the best practice in ethics (such as authorship, research ethics, and disclosure), pedagogy (such as exposition and clarity), and compliance (technical and regulatory).\n\nStyle guides are common for general and specialized use, for the general reading and writing audience, and for students and scholars of various academic disciplines, medicine, journalism, the law, government, business, and specific industries. House style refers to the internal style manual of a particular publisher or organization.\n\nStyle guides vary widely in scope and size.\n\nThis variety in scope and length is enabled by the cascading of one style over another, in a way analogous to how styles cascade in web development and in desktop publishing (e.g., how inline styles in HTML cascade over CSS styles).\n\nA short style guide is often called a \"style sheet\". A comprehensive guide tends to be long and is often called a \"style manual\" or \"manual of style\" (\"MOS\" or \"MoS\"). In many cases, a project such as one book, journal, or monograph series typically has a short style sheet that cascades over the somewhat larger style guide of an organization such as a publishing company, whose content is usually called \"house style\". Most house styles, in turn, cascade over an \"industry-wide or profession-wide style manual\" that is even more comprehensive. Some examples of these industry style guides include the following: \n\nFinally, these reference works cascade over the orthographic norms of the language in use (for example, English orthography for English-language publications). This, of course, may be subject to national variety such as the different varieties of American English and British English.\n\nSome style guides focus on specific topic areas such as graphic design, including typography. Website style guides cover a publication's visual and technical aspects along with text.\n\nStyle guides that cover usage may suggest ways of describing people that avoid racism, sexism, and homophobia. Guides in specific scientific and technical fields cover nomenclature, which specifies names or classifying labels that are preferred because they are clear, standardized, and ontologically sound (e.g., taxonomy, chemical nomenclature, and gene nomenclature).\n\nMost style guides are revised periodically to accommodate changes in conventions and usage. The frequency of updating and the revision control are determined by the subject matter. For style manuals in reference work format, new editions typically appear every 1 to 20 years. For example, the AP Stylebook is revised annually, and the Chicago, APA, and ASA manuals are in their 17th, 6th, and 4th editions, respectively. Many house styles and individual project styles change more frequently, especially for new projects.\n\nSeveral basic style guides for technical and scientific communication have been defined by international standards organizations. One example is ISO 215 \"Documentation — Presentation of contributions to periodicals and other serials\".\n\nThe European Union publishes an \"Interinstitutional style guide\"—encompassing 24 languages across the European Union. This manual is \"obligatory\" for all those employed by the institutions of the EU who are involved in preparing EU documents and works. The Directorate-General for Translation of the European Commission publishes its own \"English Style Guide\", intended primarily for English-language authors and translators, but aiming to serve a wider readership as well.\n\n\nGeneral\n\nJournalism\n\nLaw\n\n\n\nIn the United States, most public-facing corporate communication and journalism writing is written with styles following \"The Associated Press Stylebook\". Book publishers and authors of journals requiring reference sections generally choose the Chicago Manual of Style, while scholarly writing often follows the \"MLA Style Manual and Guide to Scholarly Publishing\". One of the most popular grammar guides used in third-person writing is \"The Elements of Style\". The Associated Press Stylebook is written to be used together with The Elements of Style to provide a very complete grammar and English style reference with no conflicts.\n\n\n\n\n\n\n\nDespite the near uniform use of the Bluebook, nearly every state has appellate court rules that specify citation methods and writing styles specific to that state - and the Supreme Court of the United States has its own citation method. However, in most cases these are derived from the Bluebook.\n\nThere are also several other citation manuals available to legal writers in wide usage in the United States. Virtually all large law firms maintain their own citation manual and several major publishers of legal texts (West, Lexis-Nexis, Hein, \"et al.\") maintain their own systems.\n\n\n\n\n\nGuidelines for citing web content also appear in comprehensive style guides such as Oxford/Hart, Chicago and MLA.\n\n"}
{"id": "29816", "url": "https://en.wikipedia.org/wiki?curid=29816", "title": "Technology", "text": "Technology\n\nTechnology (\"science of craft\", from Greek , \"techne\", \"art, skill, cunning of hand\"; and , \"-logia\") is the collection of techniques, skills, methods, and processes used in the production of goods or services or in the accomplishment of objectives, such as scientific investigation. Technology can be the knowledge of techniques, processes, and the like, or it can be embedded in machines to allow for operation without detailed knowledge of their workings.\n\nThe simplest form of technology is the development and use of basic tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food, and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times, including the printing press, the telephone, and the Internet, have lessened physical barriers to communication and allowed humans to interact freely on a global scale.\n\nTechnology has many effects. It has helped develop more advanced economies (including today's global economy) and has allowed the rise of a leisure class. Many technological processes produce unwanted by-products known as pollution and deplete natural resources to the detriment of Earth's environment. Innovations have always influenced the values of a society and raised new questions of the ethics of technology. Examples include the rise of the notion of efficiency in terms of human productivity, and the challenges of bioethics.\n\nPhilosophical debates have arisen over the use of technology, with disagreements over whether technology improves the human condition or worsens it. Neo-Luddism, anarcho-primitivism, and similar reactionary movements criticize the pervasiveness of technology, arguing that it harms the environment and alienates people; proponents of ideologies such as transhumanism and techno-progressivism view continued technological progress as beneficial to society and the human condition.\n\nThe use of the term \"technology\" has changed significantly over the last 200 years. Before the 20th century, the term was uncommon in English, and it was used either to refer to the description or study of the useful arts or to allude to technical education, as in the Massachusetts Institute of Technology (chartered in 1861).\n\nThe term \"technology\" rose to prominence in the 20th century in connection with the Second Industrial Revolution. The term's meanings changed in the early 20th century when American social scientists, beginning with Thorstein Veblen, translated ideas from the German concept of \"\" into \"technology.\" In German and other European languages, a distinction exists between \"technik\" and \"technologie\" that is absent in English, which usually translates both terms as \"technology.\" By the 1930s, \"technology\" referred not only to the study of the industrial arts but to the industrial arts themselves.\n\nIn 1937, the American sociologist Read Bain wrote that \"technology includes all tools, machines, utensils, weapons, instruments, housing, clothing, communicating and transporting devices and the skills by which we produce and use them.\" Bain's definition remains common among scholars today, especially social scientists. Scientists and engineers usually prefer to define technology as applied science, rather than as the things that people make and use. More recently, scholars have borrowed from European philosophers of \"technique\" to extend the meaning of technology to various forms of instrumental reason, as in Foucault's work on technologies of the self (\"techniques de soi\").\n\nDictionaries and scholars have offered a variety of definitions. The \"Merriam-Webster Learner's Dictionary\" offers a definition of the term: \"the use of science in industry, engineering, etc., to invent useful things or to solve problems\" and \"a machine, piece of equipment, method, etc., that is created by technology.\" Ursula Franklin, in her 1989 \"Real World of Technology\" lecture, gave another definition of the concept; it is \"practice, the way we do things around here.\" The term is often used to imply a specific field of technology, or to refer to high technology or just consumer electronics, rather than technology as a whole. Bernard Stiegler, in \"Technics and Time, 1\", defines technology in two ways: as \"the pursuit of life by means other than life,\" and as \"organized inorganic matter.\"\n\nTechnology can be most broadly defined as the entities, both material and immaterial, created by the application of mental and physical effort in order to achieve some value. In this usage, technology refers to tools and machines that may be used to solve real-world problems. It is a far-reaching term that may include simple tools, such as a crowbar or wooden spoon, or more complex machines, such as a space station or particle accelerator. Tools and machines need not be material; virtual technology, such as computer software and business methods, fall under this definition of technology. W. Brian Arthur defines technology in a similarly broad way as \"a means to fulfill a human purpose.\"\n\nThe word \"technology\" can also be used to refer to a collection of techniques. In this context, it is the current state of humanity's knowledge of how to combine resources to produce desired products, to solve problems, fulfill needs, or satisfy wants; it includes technical methods, skills, processes, techniques, tools and raw materials. When combined with another term, such as \"medical technology\" or \"space technology,\" it refers to the state of the respective field's knowledge and tools. \"State-of-the-art technology\" refers to the high technology available to humanity in any field.\nTechnology can be viewed as an activity that forms or changes culture. Additionally, technology is the application of math, science, and the arts for the benefit of life as it is known. A modern example is the rise of communication technology, which has lessened barriers to human interaction and as a result has helped spawn new subcultures; the rise of cyberculture has at its basis the development of the Internet and the computer. Not all technology enhances culture in a creative way; technology can also help facilitate political oppression and war via tools such as guns. As a cultural activity, technology predates both science and engineering, each of which formalize some aspects of technological endeavor.\n\nThe distinction between science, engineering, and technology is not always clear. Science is systematic knowledge of the physical or material world gained through observation and experimentation. Technologies are not usually exclusively products of science, because they have to satisfy requirements such as utility, usability, and safety.\n\nEngineering is the goal-oriented process of designing and making tools and systems to exploit natural phenomena for practical human means, often (but not always) using results and techniques from science. The development of technology may draw upon many fields of knowledge, including scientific, engineering, mathematical, linguistic, and historical knowledge, to achieve some practical result.\n\nTechnology is often a consequence of science and engineering, although technology as a human activity precedes the two fields. For example, science might study the flow of electrons in electrical conductors by using already-existing tools and knowledge. This new-found knowledge may then be used by engineers to create new tools and machines such as semiconductors, computers, and other forms of advanced technology. In this sense, scientists and engineers may both be considered technologists; the three fields are often considered as one for the purposes of research and reference.\n\nThe exact relations between science and technology in particular have been debated by scientists, historians, and policymakers in the late 20th century, in part because the debate can inform the funding of basic and applied science. In the immediate wake of World War II, for example, it was widely considered in the United States that technology was simply \"applied science\" and that to fund basic science was to reap technological results in due time. An articulation of this philosophy could be found explicitly in Vannevar Bush's treatise on postwar science policy, \"Science – The Endless Frontier\": \"New products, new industries, and more jobs require continuous additions to knowledge of the laws of nature ... This essential new knowledge can be obtained only through basic scientific research.\" In the late-1960s, however, this view came under direct attack, leading towards initiatives to fund science for specific tasks (initiatives resisted by the scientific community). The issue remains contentious, though most analysts resist the model that technology simply is a result of scientific research.\n\nThe use of tools by early humans was partly a process of discovery and of evolution. Early humans evolved from a species of foraging hominids which were already bipedal, with a brain mass approximately one third of modern humans. Tool use remained relatively unchanged for most of early human history. Approximately 50,000 years ago, the use of tools and complex set of behaviors emerged, believed by many archaeologists to be connected to the emergence of fully modern language.\n\nHominids started using primitive stone tools millions of years ago. The earliest stone tools were little more than a fractured rock, but approximately 75,000 years ago, pressure flaking provided a way to make much finer work.\n\nThe discovery and utilization of fire, a simple energy source with many profound uses, was a turning point in the technological evolution of humankind. The exact date of its discovery is not known; evidence of burnt animal bones at the Cradle of Humankind suggests that the domestication of fire occurred before 1 Ma; scholarly consensus indicates that \"Homo erectus\" had controlled fire by between 500 and 400 ka. Fire, fueled with wood and charcoal, allowed early humans to cook their food to increase its digestibility, improving its nutrient value and broadening the number of foods that could be eaten.\n\nOther technological advances made during the Paleolithic era were clothing and shelter; the adoption of both technologies cannot be dated exactly, but they were a key to humanity's progress. As the Paleolithic era progressed, dwellings became more sophisticated and more elaborate; as early as 380 ka, humans were constructing temporary wood huts. Clothing, adapted from the fur and hides of hunted animals, helped humanity expand into colder regions; humans began to migrate\nout of Africa by 200 ka and into other continents such as Eurasia.\n\nHuman's technological ascent began in earnest in what is known as the Neolithic Period (\"New Stone Age\"). The invention of polished stone axes was a major advance that allowed forest clearance on a large scale to create farms. This use of polished stone axes increased greatly in the Neolithic, but were originally used in the preceding Mesolithic in some areas such as Ireland. Agriculture fed larger populations, and the transition to sedentism allowed simultaneously raising more children, as infants no longer needed to be carried, as nomadic ones must. Additionally, children could contribute labor to the raising of crops more readily than they could to the hunter-gatherer economy.\n\nWith this increase in population and availability of labor came an increase in labor specialization. What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations, such as Sumer, is not specifically known; however, the emergence of increasingly hierarchical social structures and specialized labor, of trade and war amongst adjacent cultures, and the need for collective action to overcome environmental challenges such as irrigation, are all thought to have played a role.\n\nContinuing improvements led to the furnace and bellows and provided, for the first time, the ability to smelt and forge of gold, copper, silver, and lead native metals found in relatively pure form in nature. The advantages of copper tools over stone, bone, and wooden tools were quickly apparent to early humans, and native copper was probably used from near the beginning of Neolithic times (about 10 ka). Native copper does not naturally occur in large amounts, but copper ores are quite common and some of them produce metal easily when burned in wood or charcoal fires. Eventually, the working of metals led to the discovery of alloys such as bronze and brass (about 4000 BCE). The first uses of iron alloys such as steel dates to around 1800 BCE.\n\nMeanwhile, humans were learning to harness other forms of energy. The earliest known use of wind power is the sailing ship; the earliest record of a ship under sail is that of a Nile boat dating to the 8th millennium BCE. From prehistoric times, Egyptians probably used the power of the annual flooding of the Nile to irrigate their lands, gradually learning to regulate much of it through purposely built irrigation channels and \"catch\" basins. The ancient Sumerians in Mesopotamia used a complex system of canals and levees to divert water from the Tigris and Euphrates rivers for irrigation.\n\nAccording to archaeologists, the wheel was invented around 4000 BCE probably independently and nearly simultaneously in Mesopotamia (in present-day Iraq), the Northern Caucasus (Maykop culture) and Central Europe. Estimates on when this may have occurred range from 5500 to 3000 BCE with most experts putting it closer to 4000 BCE. The oldest artifacts with drawings depicting wheeled carts date from about 3500 BCE; however, the wheel may have been in use for millennia before these drawings were made. More recently, the oldest-known wooden wheel in the world was found in the Ljubljana marshes of Slovenia.\n\nThe invention of the wheel revolutionized trade and war. It did not take long to discover that wheeled wagons could be used to carry heavy loads. The ancient Sumerians used the potter's wheel and may have invented it. A stone pottery wheel found in the city-state of Ur dates to around 3429 BCE, and even older fragments of wheel-thrown pottery have been found in the same area. Fast (rotary) potters' wheels enabled early mass production of pottery, but it was the use of the wheel as a transformer of energy (through water wheels, windmills, and even treadmills) that revolutionized the application of nonhuman power sources. The first two-wheeled carts were derived from travois and were first used in Mesopotamia and Iran in around 3000 BCE.\n\nThe oldest known constructed roadways are the stone-paved streets of the city-state of Ur, dating to circa 4000 BCE and timber roads leading through the swamps of Glastonbury, England, dating to around the same time period. The first long-distance road, which came into use around 3500 BCE, spanned 1,500 miles from the Persian Gulf to the Mediterranean Sea, but was not paved and was only partially maintained. In around 2000 BCE, the Minoans on the Greek island of Crete built a fifty-kilometer (thirty-mile) road leading from the palace of Gortyn on the south side of the island, through the mountains, to the palace of Knossos on the north side of the island. Unlike the earlier road, the Minoan road was completely paved.\n\nAncient Minoan private homes had running water. A bathtub virtually identical to modern ones was unearthed at the Palace of Knossos. Several Minoan private homes also had toilets, which could be flushed by pouring water down the drain. The ancient Romans had many public flush toilets, which emptied into an extensive sewage system. The primary sewer in Rome was the Cloaca Maxima; construction began on it in the sixth century BCE and it is still in use today.\n\nThe ancient Romans also had a complex system of aqueducts, which were used to transport water across long distances. The first Roman aqueduct was built in 312 BCE. The eleventh and final ancient Roman aqueduct was built in 226 CE. Put together, the Roman aqueducts extended over 450 kilometers, but less than seventy kilometers of this was above ground and supported by arches.\n\nInnovations continued through the Middle Ages with innovations such as silk, the horse collar and horseshoes in the first few hundred years after the fall of the Roman Empire. Medieval technology saw the use of simple machines (such as the lever, the screw, and the pulley) being combined to form more complicated tools, such as the wheelbarrow, windmills and clocks. The Renaissance brought forth many of these innovations, including the printing press (which facilitated the greater communication of knowledge), and technology became increasingly associated with science, beginning a cycle of mutual advancement. The advancements in technology in this era allowed a more steady supply of food, followed by the wider availability of consumer goods.\nStarting in the United Kingdom in the 18th century, the Industrial Revolution was a period of great technological discovery, particularly in the areas of agriculture, manufacturing, mining, metallurgy, and transport, driven by the discovery of steam power. Technology took another step in a second industrial revolution with the harnessing of electricity to create such innovations as the electric motor, light bulb, and countless others. Scientific advancement and the discovery of new concepts later allowed for powered flight and advancements in medicine, chemistry, physics, and engineering. The rise in technology has led to skyscrapers and broad urban areas whose inhabitants rely on motors to transport them and their food supply. Communication was also greatly improved with the invention of the telegraph, telephone, radio and television. The late 19th and early 20th centuries saw a revolution in transportation with the invention of the airplane and automobile.\nThe 20th century brought a host of innovations. In physics, the discovery of nuclear fission has led to both nuclear weapons and nuclear power. Computers were also invented and later miniaturized utilizing transistors and integrated circuits. Information technology subsequently led to the creation of the Internet, which ushered in the current Information Age. Humans have also been able to explore space with satellites (later used for telecommunication) and in manned missions going all the way to the moon. In medicine, this era brought innovations such as open-heart surgery and later stem cell therapy along with new medications and treatments.\n\nComplex manufacturing and construction techniques and organizations are needed to make and maintain these new technologies, and entire industries have arisen to support and develop succeeding generations of increasingly more complex tools. Modern technology increasingly relies on training and education – their designers, builders, maintainers, and users often require sophisticated general and specific training. Moreover, these technologies have become so complex that entire fields have been created to support them, including engineering, medicine, and computer science, and other fields have been made more complex, such as construction, transportation, and architecture.\n\nGenerally, technicism is the belief in the utility of technology for improving human societies. Taken to an extreme, technicism \"reflects a fundamental attitude which seeks to control reality, to resolve all problems with the use of scientific–technological methods and tools.\" In other words, human beings will someday be able to master all problems and possibly even control the future using technology. Some, such as Stephen V. Monsma, connect these ideas to the abdication of religion as a higher moral authority.\n\nOptimistic assumptions are made by proponents of ideologies such as transhumanism and singularitarianism, which view technological development as generally having beneficial effects for the society and the human condition. In these ideologies, technological development is morally good.\n\nTranshumanists generally believe that the point of technology is to overcome barriers, and that what we commonly refer to as the human condition is just another barrier to be surpassed.\n\nSingularitarians believe in some sort of \"accelerating change\"; that the rate of technological progress accelerates as we obtain more technology, and that this will culminate in a \"Singularity\" after artificial general intelligence is invented in which progress is nearly infinite; hence the term. Estimates for the date of this Singularity vary, but prominent futurist Ray Kurzweil estimates the Singularity will occur in 2045.\n\nKurzweil is also known for his history of the universe in six epochs: (1) the physical/chemical epoch, (2) the life epoch, (3) the human/brain epoch, (4) the technology epoch, (5) the artificial intelligence epoch, and (6) the universal colonization epoch. Going from one epoch to the next is a Singularity in its own right, and a period of speeding up precedes it. Each epoch takes a shorter time, which means the whole history of the universe is one giant Singularity event.\n\nSome critics see these ideologies as examples of scientism and techno-utopianism and fear the notion of human enhancement and technological singularity which they support. Some have described Karl Marx as a techno-optimist.\n\nOn the somewhat skeptical side are certain philosophers like Herbert Marcuse and John Zerzan, who believe that technological societies are inherently flawed. They suggest that the inevitable result of such a society is to become evermore technological at the cost of freedom and psychological health.\n\nMany, such as the Luddites and prominent philosopher Martin Heidegger, hold serious, although not entirely, deterministic reservations about technology (see \"The Question Concerning Technology\"). According to Heidegger scholars Hubert Dreyfus and Charles Spinosa, \"Heidegger does not oppose technology. He hopes to reveal the essence of technology in a way that 'in no way confines us to a stultified compulsion to push on blindly with technology or, what comes to the same thing, to rebel helplessly against it.' Indeed, he promises that 'when we once open ourselves expressly to the essence of technology, we find ourselves unexpectedly taken into a freeing claim.' What this entails is a more complex relationship to technology than either techno-optimists or techno-pessimists tend to allow.\"\n\nSome of the most poignant criticisms of technology are found in what are now considered to be dystopian literary classics such as Aldous Huxley's \"Brave New World\", Anthony Burgess's \"A Clockwork Orange\", and George Orwell's \"Nineteen Eighty-Four\". In Goethe's \"Faust\", Faust selling his soul to the devil in return for power over the physical world is also often interpreted as a metaphor for the adoption of industrial technology. More recently, modern works of science fiction such as those by Philip K. Dick and William Gibson and films such as \"Blade Runner\" and \"Ghost in the Shell\" project highly ambivalent or cautionary attitudes toward technology's impact on human society and identity.\n\nThe late cultural critic Neil Postman distinguished tool-using societies from technological societies and from what he called \"technopolies,\" societies that are dominated by the ideology of technological and scientific progress to the exclusion or harm of other cultural practices, values, and world-views.\n\nDarin Barney has written about technology's impact on practices of citizenship and democratic culture, suggesting that technology can be construed as (1) an object of political debate, (2) a means or medium of discussion, and (3) a setting for democratic deliberation and citizenship. As a setting for democratic culture, Barney suggests that technology tends to make ethical questions, including the question of what a good life consists in, nearly impossible because they already give an answer to the question: a good life is one that includes the use of more and more technology.\n\nNikolas Kompridis has also written about the dangers of new technology, such as genetic engineering, nanotechnology, synthetic biology, and robotics. He warns that these technologies introduce unprecedented new challenges to human beings, including the possibility of the permanent alteration of our biological nature. These concerns are shared by other philosophers, scientists and public intellectuals who have written about similar issues (e.g. Francis Fukuyama, Jürgen Habermas, William Joy, and Michael Sandel).\n\nAnother prominent critic of technology is Hubert Dreyfus, who has published books such as \"On the Internet\" and \"What Computers Still Can't Do\".\n\nA more infamous anti-technological treatise is \"\", written by the Unabomber Ted Kaczynski and printed in several major newspapers (and later books) as part of an effort to end his bombing campaign of the techno-industrial infrastructure. There are also subcultures that disapprove of some or most technology, such as self-identified off-gridders.\n\nThe notion of appropriate technology was developed in the 20th century by thinkers such as E.F. Schumacher and Jacques Ellul to describe situations where it was not desirable to use very new technologies or those that required access to some centralized infrastructure or parts or skills imported from elsewhere. The ecovillage movement emerged in part due to this concern.\n\n\"This section mainly focuses on American concerns even if it can reasonably be generalized to other Western countries. \"\n\nIn his article, Jared Bernstein, a Senior Fellow at the Center on Budget and Policy Priorities, questions the widespread idea that automation, and more broadly, technological advances, have mainly contributed to this growing labor market problem.\nHis thesis appears to be a third way between optimism and skepticism. Essentially, he stands for a neutral approach of the linkage between technology and American issues concerning unemployment and declining wages.\n\nHe uses two main arguments to defend his point.\nFirst, because of recent technological advances, an increasing number of workers are losing their jobs. Yet, scientific evidence fails to clearly demonstrate that technology has displaced so many workers that it has created more problems than it has solved. Indeed, automation threatens repetitive jobs but higher-end jobs are still necessary because they complement technology and manual jobs that \"requires flexibility judgment and common sense\" remain hard to replace with machines. Second, studies have not shown clear links between recent technology advances and the wage trends of the last decades.\n\nTherefore, according to Bernstein, instead of focusing on technology and its hypothetical influences on current American increasing unemployment and declining wages, one needs to worry more about \"bad policy that fails to offset the imbalances in demand, trade, income, and opportunity.\"\n\nFor people who use both the Internet and mobile devices in excessive quantities it is likely for them to experience fatigue and over exhaustion as a result of disruptions in their sleeping patterns. Continuous studies have shown that increased BMI and weight gain are associated with people who spend long hours online and not exercising frequently. Heavy Internet use is also displayed in the school lower grades of those who use it in excessive amounts. It has also been noted that the use of mobile phones whilst driving has increased the occurrence of road accidents — particularly amongst teen drivers. Statistically, teens reportedly have fourfold the amount of road traffic incidents as those who are 20 years or older, and a very high percentage of adolescents write (81%) and read (92%) texts while driving. In this context, mass media and technology have a negative impact on people, on both their mental and physical health.\n\nThomas P. Hughes stated that because technology has been considered as a key way to solve problems, we need to be aware of its complex and varied characters to use it more efficiently. What is the difference between a wheel or a compass and cooking machines such as an oven or a gas stove? Can we consider all of them, only a part of them, or none of them as technologies?\n\nTechnology is often considered too narrowly; according to Hughes, \"Technology is a creative process involving human ingenuity\". This definitio's emphasis on creativity avoids unbounded definitions that may mistakenly include cooking \"technologies,\" but it also highlights the prominent role of humans and therefore their responsibilities for the use of complex technological systems.\n\nYet, because technology is everywhere and has dramatically changed landscapes and societies, Hughes argues that engineers, scientists, and managers have often believed that they can use technology to shape the world as they want. They have often supposed that technology is easily controllable and this assumption has to be thoroughly questioned. For instance, Evgeny Morozov particularly challenges two concepts: \"Internet-centrism\" and \"solutionism.\" Internet-centrism refers to the idea that our society is convinced that the Internet is one of the most stable and coherent forces. Solutionism is the ideology that every social issue can be solved thanks to technology and especially thanks to the internet. In fact, technology intrinsically contains uncertainties and limitations. According to Alexis Madrigal's review of Morozov's theory, to ignore it will lead to \"unexpected consequences that could eventually cause more damage than the problems they seek to address.\" Benjamin R. Cohen and Gwen Ottinger also discussed the multivalent effects of technology.\n\nTherefore, recognition of the limitations of technology, and more broadly, scientific knowledge, is needed – especially in cases dealing with environmental justice and health issues. Ottinger continues this reasoning and argues that the ongoing recognition of the limitations of scientific knowledge goes hand in hand with scientists and engineers’ new comprehension of their role. Such an approach of technology and science \"[require] technical professionals to conceive of their roles in the process differently. [They have to consider themselves as] collaborators in research and problem solving rather than simply providers of information and technical solutions.\"\n\nTechnology is properly defined as any application of science to accomplish a function. The science can be leading edge or well established and the function can have high visibility or be significantly more mundane, but it is all technology, and its exploitation is the foundation of all competitive advantage.\n\nTechnology-based planning is what was used to build the US industrial giants before WWII (e.g., Dow, DuPont, GM) and it is what was used to transform the US into a superpower. It was not economic-based planning.\n\nThe use of basic technology is also a feature of other animal species apart from humans. These include primates such as chimpanzees, some dolphin communities, and crows. Considering a more generic perspective of technology as ethology of active environmental conditioning and control, we can also refer to animal examples such as beavers and their dams, or bees and their honeycombs.\n\nThe ability to make and use tools was once considered a defining characteristic of the genus Homo. However, the discovery of tool construction among chimpanzees and related primates has discarded the notion of the use of technology as unique to humans. For example, researchers have observed wild chimpanzees utilising tools for foraging: some of the tools used include leaf sponges, termite fishing probes, pestles and levers. West African chimpanzees also use stone hammers and anvils for cracking nuts, as do capuchin monkeys of Boa Vista, Brazil.\n\nTheories of technology often attempt to predict the future of technology based on the high technology and science of the time. As with all predictions of the future, however, technology's is uncertain.\n\nIn 2005, futurist Ray Kurzweil predicted that the future of technology would mainly consist of an overlapping \"GNR Revolution\" of genetics, nanotechnology and robotics, with robotics being the most important of the three.\n\n"}
{"id": "17163802", "url": "https://en.wikipedia.org/wiki?curid=17163802", "title": "Technology dynamics", "text": "Technology dynamics\n\nTechnology dynamics is broad and relatively new scientific field that has been developed in the framework of the postwar science and technology studies field. It studies the process of technological change. Under the field of Technology Dynamics the process of technological change is explained by taking into account influences from \"internal factors\" as well as from \"external factors\". Internal factors relate technological change to unsolved technical problems and the established modes of solving technological problems and external factors relate it to various (changing) characteristics of the social environment, in which a particular technology is embedded.\n\nFor the last three decades, it has been argued that technology development is neither an autonomous process, determined by the \"inherent progress\" of human history, nor a process completely determined by external conditions like the prices of the resources that are needed to operate (develop) a technology, as it is theorized in neoclassical economic thinking. In mainstream neoclassical economic thinking, technology is seen as an exogenous factor: at the moment a technology is required, the most appropriate version can be taken down from the shelf based on costs of labor, capital and eventually raw materials.\n\nConversely, modern technology dynamics studies generally advocate that technologies are not \"self-evident\" or market-demanded, but are the upshot of a particular path of technology development and are shaped by social, economic and political factors. in this sense, technology dynamics aims at overcoming distinct \"internal\" and \"external\" points of views by presenting co-evolutionary approach regarding technology development.\n\nIn general, technology dynamics studies, besides giving a \"thick description\" of technology development, uses constructivist viewpoints emphasizing that technology is the outcome of particular social context. Accordingly, Technology Dynamics emphasizes the significance and possibility of regaining social control of technology, and also provides mechanisms needed to adapt to and steer the development of certain technologies. In that respect, it uses insights from retrospective studies to formulate hypotheses of a prospective nature on technology development of emerging technologies, besides formulating prescriptive policy recommendations.\n\nAn important feature of relevant theories of technological change therein is that they underline the quasi-evolutionary character of technological change: change based on technological variation and social selection in which technological knowledge, systems and institutions develop in interaction with each other. Processes of 'path dependence' are crucial in explaining technological change.\n\nFollowing these lines, there have been different approaches and concepts used under the field of technology dynamics.\n\n\nBased on the analysis of the various perspectives, one can aim at developing interventions in the dynamics of a technology. Some approaches have been developed targeting on interventions in technological change:\n\n\n\n"}
{"id": "1175103", "url": "https://en.wikipedia.org/wiki?curid=1175103", "title": "Technology governance", "text": "Technology governance\n\nTechnology governance means the governance, i.e., the steering between the different sectors—state, business, and NGOs—of the development of technology. The concept is based on the notion of innovation and of techno-economic paradigm shifts according to the theories of Joseph A. Schumpeter, Christopher Freeman, Carlota Perez, etc. \n\nThe idea is that certain periods in economic development are dominated by a paradigm-leading technology that influences through finance mechanisms, organizational change, greater returns, etc. Also, the economic and social sphere to such an extent that an entire paradigm is coined by them. Currently, that paradigm-leading technology is information and communications technology (ICT). According to Innovation Theory (and empirical findings as well), states, regions, or other communities do not automatically promote, or even go into, these key technologies, but rather need to be guided there by active intervention, by the State sector (in close cooperation and coordination). Technology governance, (as a field of scholarly inquiry and academic instruction) is therefore about how this is theoretically and practically done, which institutions are optimal for such a venue, how the education of administrators have to look like if they are going to have the administrative capacity to deal with such matters, etc.\n\nTechnology governance is a public policy concept; it is not to be confused with inner-corporate arrangements of organisation (corporate governance) and IT arrangements, sometimes called \"Information Technology Governance\".\n\n"}
{"id": "38798417", "url": "https://en.wikipedia.org/wiki?curid=38798417", "title": "Technoself studies", "text": "Technoself studies\n\nTechnoself studies, commonly referred to as TSS, is an emerging, interdisciplinarity domain of scholarly research dealing with all aspects of human identity in a technological society focusing on the changing nature of relationships between the human and technology. As new and constantly changing experiences of human identity emerge due to constant technological change, technoself studies seeks to map and analyze these mutually influential developments with a focus on identity, rather than technical developments. Therefore, the self is a key concept of TSS. The term \"technoself\", advanced by Luppicini (2013), broadly denotes evolving human identity as a result of the adoption of new technology, while avoiding ideological or philosophical biases inherent in other related terms including cyborg, posthuman, transhuman, techno-human, beman (also known as bio-electric human), digital identity, avatar, and homotechnicus though Luppicini acknowledges that these categories \"capture important aspects of human identity\". Technoself is further elaborated and explored in Luppicini's \"Handbook of Research on Technoself: Identity in a Technological Environment\".\n\nTechnoself evolved from early groundwork in identity studies, philosophy of mind, and cognitive science. René Descartes is often credited as one of the first identity theorists of Modernity to question the material world and the certainty of knowledge from the self. Despite heavy criticism, the question he posed regarding the necessary relation between the mind and body is still considered a prevalent theme in contemporary discussions of identity and technology. Another major development in identity studies came from early social psychology, sociology and psychoanalysis. Beginning with Freud, the psychoanalytic tradition shed some light on the dynamics of identity and personality development. Erving Goffman expanded the inquiry of identity with his dramaturgical theory, which emphasized the centrality of the social realm and the notion of self-presentation to identity. Later, Foucault further expanded the area of inquiry by contemplating how technologies could facilitate the emergence of new ways of relating to oneself.\n\nThe most entrenched area of technoself studies is revolved around ontological considerations and conceptualizations of technoself. The effort to identify the essence of human being is frequent in philosophical circles and is entrenched within emerging theoretical scholarship on technoself. DeGrazia's (2005) examination on identify/numerical identity to shed light on the ethics of human enhancement. According to DeGrazia, human identity is divided into two parts: 1) numerical identity (concerns the continuity of an individual as the same object over time or across procedure), and 2) narrative identity (concerns the changes in self-perception experienced by an individual over time). By dividing human identity into two parts, DeGrazia is facilitating a discussion on the ethics of human enhancements. Meanwhile, Croon Fors(2012) research on the entanglement of the self and digitalization have helped frame ontological considerations related to the conceptualization of technoself studies. Furthermore, the changing nature of identity is a common theme within technoself studies. As a result, this has given way for scholars to analyze questions such as: How are advances in sensing technologies, biometrics, and genetics changing the way we define and recognize identity? How are technologies changing the way people define themselves and present themselves in society? These types of questions are being heavily analyzed as the conceptualization of identity is changing rapidly.\n\nCentral to the understanding of the development of technoself studies as a field of research is the idea that human identity is shaped by the adoption of new technologies and the relationship between humans and technology. Advancements in digital technology have recently forced researchers to consider the conception of the self in relation to the increasing reliance of society on the use of technologies (such as cellphones, tablets, and social media) in daily tasks in peoples' personal and professional lives. New technologies, particularly computer-mediated communication tools, have raised questions related to identity in relationship to privacy issues, virtual identity boundaries, online fraud, citizen surveillance, etc. These issues come as our perspective on technology shifts from one of functionality to one of interaction. According to John Lester, in the future, \"we won't simply enjoy using our tools, we will come to care for them\".\n\nThe noeme is a term coined in 2011 by biogerontologist Marios Kyriazis, and it denotes a \"combination of a distinct physical brain function and that of an outsourced virtual one\". A noeme is the intellectual \"networked presence\" of an individual within the global brain, a meaningful synergy between each individual human, their social interactions and artificial agents, globally connected to other noemes through digital communications technology. Kyriazis further clarifies that: \"\"...The noeme is structurally coupled with its medium, i.e. the computer/internet. It continuously generates its own organisation and specifies its operation and content. As a self-organising system it adjusts to external influences and reinvents itself in order to adapt to its environment i.e. it reproduces (self-replicates) horizontally in a process that can be termed 'noemic reproduction'. This digital intellectual manifestation of a person, if successful, will lead to others copying it, thus noemes are replicating... A noeme is not just a collection of single ideas or solitary intellectual achievements. It is the total sum of all individual cognitive efforts and active information-sharing accomplishments of a person, the intellectual standing of a person within the Global brain.\"\n\nA cyborg (cybernetic organism) is a term referring to individuals with \"both biological and artificial parts.\" Cyborgs are known as being half-human, half machine organisms, due to the fact that they are always connected with technology. This term, which was coined in 1960 by Manfred Clynes, refers to and acknowledges those beings whose abilities have been enhanced due to the presence and advancement of technology. The notion of cyborg has played a part in breaking down boundaries between humans and non-humans living within a technologically advanced society. For example, those who have installed pacemakers, hearing aids, artificial body parts, cochlear implants as well as other technologies that may aid in enhancing an organisms abilities and capacities to perform, either physically or mentally.\nHugh Herr, an American rock climber, engineer, and biophysicist, has successfully invented the next generation of cyborg (bionic limbs and robotic prosthetics). As the head of the Media Lab's Biomechatronics group in MIT, he shared his experience and presented the team achievement first time in a TED talk show.\n\nTranshuman is a concept that emerged as a result of the transhumanist movement which is centred around the notion of improving the abilities of human beings mainly through both 'scientific and technical means.' Unlike the posthuman concept, the notion of transhuman is based on human augmentation but does not commit itself to positing a new separate species. The philosophy of transhumanism was developed in the 1990s by British philosopher Max More who articulated the principles of transhumanism as a futurist philosophy. However, the transhuman philosophy has also been subject to scrutiny by prominent scholars such as Francis Fukuyama.\n\nPosthuman is a concept that aims towards signifying and characterizing a fresh and enhanced type of being. This organism is highly representative of a being that embraces drastic capabilities that exceed current human capabilities that are presently defining human beings. This posthuman state of identity has mainly resulted from the advancement of technological presence. According to Luppicini, posthuman capabilities \"suggest a new type of being over and above human. This compromises the neutrality needed for a clear conception of human identity in the face of human-technological integration.\" This concept aims towards enabling a brighter future concerned with gaining a better perception of the world through various viewpoints.\n\nHomo technicus is a term \"first coined by Galvin in 2003 to help refine the definition of human beings to more accurately reflect the evolving condition of human beings intertwined within advancing technological society\". It refers to the notion that human beings are technological by nature and evolve simultaneously with technology. Galvin states in his article titled \"On Technoethics\", \"mankind cannot do away with the technical dimension, going even to the\npoint of considering this part of its constitution: mankind is technical by nature. Technology is not an\naddition to man but is, in fact, one of the ways in which mankind distinguishes itself from animals.\" Luppicini builds upon the concept of homo technicus in his book \"Handbook of Research on Technoself: Identity in a Technological Society\". Luppicini feels that the notion of homo technicus contributes to the conception of humans as technoselves in two ways. First it helps to solidify the idea of technology as being a key component in defining humans and society and secondly it demonstrates the importance of technology as a human creation that aligns with human values. He further goes onto explain that human interactions with the material world around them helps to create meaning and this unique way of creating meaning has affected how humans have evolved as a species.\n\nNote: the term homo technicus was coined earlier than 2003. For instance, it was used by Russell E. Willis in his 1990 PhD dissertation for Emory University: Toward a Theological Ethics of Technology: An analysis in Dialogue with Jacques Ellul, James Gustafson, and the Philosophy of Technology. It was later used by Willis in \"Complex Responsibility in an Age of Technology,\" in Living Responsibly in Community, ed. Fredrick E. Glennon, et al. (University Press of America, 1997): 251ff. In both publications homo technicus is offered as a model for the responsible self in an age of pervasive technology.\n\nAvatars represent the individual, the individual's alter ego, or character(s) within virtual environments controlled by a human user. Avatars provide a unique opportunity to experiment with one's identity construction within virtual worlds (Turkle, 1995) and to do so with others\". Examples of avatars can include personas in online games or virtual life simulations such as Second Life.\n\nA new hybrid form of creature that results from an intertwinement between human and machine.\n\nA techno-sapien would be a slang term for a human being who is familiar and comfortable with technology. Someone who has the latest gadgets and electronic machinery would be techno-sapien.\n\nDigital identity is the data that uniquely describes a person or a thing and contains information about the subject's relationships. The social identity that an internet user establishes through digital identities in cyberspace is referred to as online identity.\n\nThe areas of focus in TSS are: philosophical inquiry and theoretical framing, digital identity and virtual life, human enhancement technologies, and their regulation. These areas of study have been influenced by extensive research, and input from an editorial advisory and review board over the course of several years.\n\nDigital identity and virtual life looks at how individuals explore, develop and represent their identities in online, virtual, or mediated environments. Research on virtual life and digital identities is concerned not only with how individuals relate to their own mediated identities, but also with how they relate to those of others. With the current popularity of social networking service sites, it is no surprise that TSS scholars have also begun studying the effects that such constant and mediated social connections have on identity. Topics that fall under this category have included intellectual disability, gender identity, and mass media in sport.\n\nCritical areas of research include: how individuals treat the identity of others in an online space; how people use media to develop and project their identity; and how digital representation can alter life meaning and identity (Luppicini, 2013). Such research examines the advantages and disadvantages of online life and digital identity construction.\n\nAreas of digital identity and virtual life have become quite popular, e.g. online avatars. Scholars are now focused on the role avatars play in identity exploration, priming behaviours, and self-presentation. Other research looks at the use of communication technologies by immigrant individuals as part of a digital diaspora. These scholars examine a trend in which diasporic immigrants who feel disconnected from their cultural identities have turned to digital technologies as a way to reconnect.\n\nThe term \"technoself\" is often used interchangeably with \"virtual self\". In this case, technoself is used to refer to a virtual manifestation of one's self. The ability to project one's self into a virtual world allows users to control their appearance and personality. Users are able to customize their virtual identity and craft a persona to their liking. The malleability of online identities allows users to not only create their own virtual self, but also to continually change and mold their online selves in ways impossible to do with their real identities. Users can edit and change their virtual selves' appearance and behavior to control other users' perception of them.\n\nThe ability to create and change your identity in this way, is due to anonymity. Anonymity is a paramount and dynamic feature of virtual social interaction within the online public sphere. As individuals are not required to reveal their real identity, they are able to explore new and undiscovered aspects of themselves. In this expansion of the self, anonymous individuals may try on various identities which break traditional social norms, without fear of retribution or judgment. This contributes to the creation of 'super-selves', through which individuals may amplify aspects of their projected identities in order to form an ideal expression of the self. The fact that the vast majority of virtual encounters are anonymous in nature allows a 'strangers on a train' phenomenon to\ntake place. Through invented and unknown personae, individuals are able to engage in self-disclosure, transvestism, and fantasies. However, this freedom may not be absolute, as there are many risks in participating in an online community, including identity theft and the potential linkage between anonymous and manifest identities. Anonymity also may have legal ramifications, making it difficult for law enforcement to maintain control over online communities. Tracking down online law-breakers is difficult when their identity is unknown. Anonymity also frees individuals so that they are able to behave in socially undesirable and harmful ways, which can result in forms of hate speech and cruel online behaviour. Lastly, anonymity also diminishes the integrity of information, and as a result, diminished the overall trust of online environment.\n\nMany online users choose to attempt anonymity through the use of avatars. Users associate themselves with avatars as digital representatives within a duplicated and simulated virtual community. The user's body is essentially plugged in within the avatar world, thereby creating the illusion of infinite \"space\" behind the computer screen. As a result, they provide the opportunity for users to manipulate their worlds and the spaces and objects with which they interact. Participation in online communities has resulted in the creation of a virtual economy based on the semantic value of digital products. This form of online consumerism is centered on the creation of avatars as extensions of the self. The purchase of symbolic goods for these avatars relates to the emotional and social value that the user holds for these items. These products may indicate roles or personality traits of players within a community and consist primarily of task oriented and nonfunctional items.\n\nLuppicini argues that the rise of online life creates serious questions on the advantages and disadvantages of online communities along with the challenges to online identity construction (Turkle, 1999). He notes the negative influence of the impersonality of virtual communities on offline interaction and the consequence of Internet addiction. Sherry Turkle states: \"We discovered the network – the world of connectivity – to be uniquely suited to the overworked and overscheduled life it makes possible. And now we look to the network to defend us against loneliness even as we want it to control the intensity of our connections\".\n\nComputer networking and smart technologies such as radio frequency identification (RFID), geographical information systems (GIS), and global positioning systems (GPS) are providing new tools and techniques for monitoring individuals and their behavior. The rise in these types of technologies has raised concerns over the invasion of privacy, and the misuse of information. That is because the networked identity of technoselves can be exploited by third parties who may want to gain access and control over personal information. Moreover, the implications of the sophisticated technologies for identifying and tracking people, the storage of this data, and the governmental use of surveillance to track suspicious types of people are significant issues in privacy/surveillance and TSS. The availability of related technologies (e.g. EyeTap, Memoto) to individuals (as opposed to governments or commercial interests) has also led to the phenomenon dubbed sousveillance, whereby individuals track or record authorities' activities either online or in real environments.\n\nLeading scholars in the study of surveillance include David Lyon and Mark Andrejevic. In addition to contributing to the advent of citizen journalism, the proliferation of sousveillance technologies has suggested a number of legal/regulatory, ethical, and social implications for democratic and consumer rights. A dramatic illustration of these concerns comes from University of Toronto Professor Steve Mann, a privacy rights advocate and pioneering engineer of such technologies. After being allegedly assaulted in a French McDonald's restaurant for wearing an augmented-reality digital eye glass device, Mann was, ironically, allegedly denied access to McDonald's own surveillance camera footage. This led to Mann's coinage of the term \"McVeillance\" for instances of surveillance/sousveillance double standards and to his contribution the proposal of the Mann-Wassell law in the New York legislature.\n\nHuman enhancement technology (HET) is the study of tools that would better and improve a human being's way of life. It seeks to advance and progress what humans already do within their normal lives. However, customarily it seeks to aid any illnesses and weaknesses in the body. Popular topics within this new area of study include, sex reassignment surgery, mood enhancers, genomics, and neuroenhancement. Enhancement within the workplace is a new topic of discussion, while the workplace should be adapting to the various types of human impairment, it seems that improving the workers is of more concern to corporations. Through the use of cyborg prosthetics one can assemble themselves in their own vision, any disfigurement or handicap can possibly disappear. Within the evolution of cyborg prosthetics a human is able to physically grasp things more easily, allowing more of the population to engage in whatever they choose. A large aspect of this technology stems from the ability to determine who may and may not benefit, as well as how access to these new technologies should be controlled.\n\nHuman bodies can now not only be improved upon through natural means, but through the effects of technology. This new form of enhancement is connected with what humans perceive of themselves, and as to how their own identity is created. A human operates based on their abilities; these capabilities are the factors and characteristics that create a personality. The augmentation of these aptitudes leads to a new human, who has a renewed sense of who they are. The term 'free to be me' is closely related to this new form of enhancement, wherein technological enhancements can be either cosmetic or reconstructive. Through the incorporation of medicine and technology \"...cosmetic surgery then becomes a technology through which the body is normalized and homogenized as much as enhanced\". A proper example relating to human enhancement and cyborgs could be the recently convicted Oscar Pistorius. In past years, Pistorius fought with the International Olympic Committee to have a place in the hurdles events of the 2012 Olympics in London. The controversy surrounding Pistorius extended to his artificial legs, and how they compared to the natural human anatomy; did Pistorius have an unfair advantage over his competitors? The ruling was left up to a scientific analysis of his legs and running stride which ultimately lead to his participation in the Olympic Games. Therefore, we've come to a time where decisions, and thus human panels, need to determine what is human, what is natural, and what is artificial.\n\nRights and privacy issues over human enhancement technology has given rise to challenging topics within technoself studies. For example, the consideration of ethical policies and guidelines in the deployment of HET is an emerging topic within TSS. Further, the question of access to HET, and where we draw the line between necessary therapeutic technologies, and frivolous human enhancement are being raised in TSS. Therefore, the emerging topic regarding the rights and privacy over HET is of great interest within TSS. Popular HET's topics in recent research academia include: Sex (re assignment) (Diamond & Sigmundson, 1997; Zucker, 2002), mood enhancers (Rabin, 2006), cognitive enhancers (Walker, 2008), genomics (Zwart, 2009), and neuroenhancement (Northoff,1996). A second line of inquiry explores social, legal, and ethical aspects of human enhancement and possible threats to human dignity that could arise from the implementation of human enhancements\n(Bostrom, 2005).\n\nSome critiques engage a discussion between the development of HET and the socio-economic environment. Francis Fukuyama, an American political scientist concerns about the future of HET might cause the extension of contradiction between the rich and poor within comparatively rich, industrialized nations because HET is likely to be a luxury product. At the moment, HET seems to be hard to be mainstream in public health services due to the price, which creates a deeper distinctions among those who can afford the technology and those who will remain disabled.\n\nTranshuman thought focuses on beliefs held that the fundamental transformation of the human condition will be through the development of various technology, which will eventually eliminate human aging and will enhance human capacities, both physical and mental. Believers in this theory think that the future of human development will see a new intelligent species that will be enhanced by the technological advances. They use these technological advances to approach various issues regarding the human experience, like morality and health issues. They see this convergence happening through the support of current technologies and the vision of technology in the future; using these advances to eventually make humans more than human, enhanced through this technology. Their central argument is that humans need to be able to choose whether or not this technology is used by them or not.\n\nThis theory expands on the notion of technoself, as transhumanism poses what to many who hold these beliefs is the natural evolution of the human condition. Many look to the history of technological advancements as proof that these future advancements are possible, at least in theory.\n\nAvatars are a visual representation of a user in an online environment. This representation may be an accurate physical representation of the user, or may be completely different. This online representation may affect the offline self. Pena and his colleagues explored a phenomenon known as the \"Proteus effect\" wherein \"avatars can prime negative attitudes and cognition in desktop virtual settings\". They conducted a study that demonstrated how the appearance and affiliations of an individual's online avatar can alter the individual's offscreen personality and attitudes. Pena's group used virtual group discussions to gauge the aggressiveness of individuals using avatars wearing black cloaks versus their control group counterparts wearing white and found more aggressive intentions and attitudes in the black cloak group.\n\nSimilar results were found in a second study that used Thematic Apperception Test studies to determine the differences between values and attitudes of a control group and a group using a Ku Klux Klan (KKK)-associated avatar. Individuals using the KKK-associated avatars were less affiliative and displayed more negative thoughts than the control group. Further support for Pena et al.'s work can be found in other studies that yielded similar results: \"Yee and Bailenson found that, in an immersive 3D environment, participants using avatars with more attractive faces walked closer and disclosed more information when compared to those using avatars with less attractive faces. In addition...participants using taller avatars tended to negotiate more forcefully in comparison to those using shorter avatars.\" A growing body of evidence supports how our online personas can affect our offline self; altering our attitudes and values.\n\nOnline anonymity is commonly described using the phrase \"On the Internet, nobody knows you're a dog\". Online anonymity allows users to present different versions of themselves in online environments. Unconstrained by physical limitations, users are free to choose and construct their virtual form(s) and identities. Virtual spaces which foster such freedom and anonymity therefore allow users to depart from the expectations, norms, and behaviours of their daily lives. It can be said that this unlimited freedom of anonymous expression allows for the transfer of real world suppressed emotions to the online domain. However, if one continually chooses to express their true self anonymously online as opposed to in the real world via face to face interaction, which realm would be more 'real'? As extreme as this scenario may seem, one could say the suppression of norms and natural expression would deem the physical self the avatar, and the online avatar the true self.\n\nA user's online identity is a social identity that represents the user in the online environment, allowing a user a high level of control over their identity in a way that differs from the offline world. Turkle found that the level of control over creating an online identity also extends to the intensity of connections made in such virtual spaces, as users may engage and disengage at will. Dervin and Abbas note that Turkle, in her early work was \"one of the first to show how anonymity 'provides ample room for individuals to express unexplored parts of themselves' more easily than in face-to-face interaction\". Within this notion of being free in online anonymity, technoself studies also looks at what the element of hiding does to us. Turkle suggests that, \"our networked life allows us to hide from each other, even as we are tethered to each other\". Technoself studies explores what these profiles do to the human unconscious. While people are \"exposing\" themselves, they question their level of exposure and sharing compared to extent in what they are truly hiding in reality.\nFurthermore, when creating online profiles, people risk others' perceptions of the information shared and if they receive the messages that the sender intended. Without verbal communication misperceptions, messages can alter identity or personal development.\n\nAvatars can be an important element of the online presentation of the user. In many cases, \"avatars in blogging were created to accurately reflect their owners' physical appearance, lifestyle and preferences. By contrast, participants in the dating and gaming treatments accentuated certain aspects of their avatar to reflect the tone and perceived expectations of the context\". In other words, individuals often emphasize or downplay certain characteristics depending upon the context of their online interactions. These inconsistencies tend to be trivial, however. For instance, men tend to mildly exaggerate their height, while women often underestimate their weight. This is typically not an attempt to mislead others but to be as honest as possible while still presenting themselves in the best light.\n\nAccording to Vasalou & Joinson, although various online forums may present people with the opportunity to create (an) alternate persona(s), they typically choose to create an avatar or represent themselves in a way that is consistent with reality: \"In having equal access to everyday artifacts and fantasy options, participants were inclined to draw on existing self-views rather than grasping the opportunity to explore other personas\". Furthermore, Vasalou and Joinson also claim that, in the context of online communication, high self-awareness (as demonstrated by an avatar largely consistent with an individual's offline persona), contributes to a higher rate of interpersonal communication.\n\nOne consequence of online anonymity and creating false identities is the ability to \"catfish\". Catfishing is a recent internet phenomenon, of manipulating, deceiving and luring people into relationships, through creating an online fictional persona. In many cases these deceptions are used to create romantic or intimate affairs. Since the affair happens entirely through technology, one is able to hide their true identity and carry on the relationship through their made up character. The majority of these incidences happen through social media sites, such as Facebook, and internet dating sites where people are already looking for love, and therefore can be easily manipulated by peoples personas and deceptions.\n\nNew directions and opportunities in technoself research involving personalized robots and social integration of artificial creatures is becoming an increasing reality. Considering the work of pioneering computer scientists and robotics experts such a Rodney Brooks and Hiroshi Ishiguro, human interaction with personal and social robots reached mainstream audiences beginning with the popularization of robotic dolls and pets for children. Research by Sherry Turkle examines many of the effects of these social robots on children, middle and elderly. There are also robots for adults aimed at therapeutic (technotherapy), personal, and social applications (Paro Phobot, Roxxxy, etc.). These types of therapeutic robots are used in nursing homes and hospitals, with the purpose of creating an environment where one can nurture and communicate with an animal. This allows people in a lonely or isolated environment the ability to have something to care for and interact with that is also able to respond and interact back. This has shown to provide happiness and a larger sense of purpose for the individuals, even if for a short period of time. With personalized robots and the social integration of artificial intelligence, technoself is developing in children through relationships with robotic pets and related robotic technologies based on animals, objects, or people (Tamagotchi, Furby, AIBO, etc.). Current areas of interest in this topic are reported in Melson (2012), which provide helpful insights into children's views about robot pets, children's relationship with robotic pets and, conceptualizations of self-identity within child-robot relationships. Other research is focusing more on personalized robots for adults. If the trend towards the personalization of robots and social integration of artificial creatures continues, it is expected that this research will become more prevalent. David Levy, the artificial intelligence researcher in University of Maastricht contains the forecast of robot and human relationship in his thesis, \"Intimate Relationships with Artificial Partners\". In his interview Forecast: Sex and Marriage with Robots by 2050 with \"LiveScience\", Levy says :\"My forecast is that around 2050, the state of Massachusetts will be the first jurisdiction to legalize marriages with robots\". are real life experience suggesting that humans can develop an psychological level relationship with artificial subjects, even if the subject itself is not in any physical shape. Judith Newman wrote an article on \"New York Times\" about the relationship between the Siri system and her 13-year-old son who has autism. Newman says his son develops a close relationship with the system and learning to show affection to it even though he knows Siri is not 'real'. Newman suggest that Siri could be a potential companion to those children who have a hard time to communicate with people. Duggan (2016) describes how users already form relationships with technology that share many of the features of relationships between humans. These relationships have important implications for the future of healthcare as interactive technology increasingly replaces roles traditionally filled by humans.\n\nHuman enhancement regulation, governance, and legal concerns has become another growing concern for the opportunity of TSS research. According to Saner and Geelen (2012), there is one framework to guide technoself governance which distinguishes six different approaches to which emerging technologies may affect human identity:\n\nLuppicini posits that this sort of model could \"prove invaluable for guiding future decision making directed at the framing of HET regulation debates, as well as leveraging strategic planning and decision making concerning HET adaption standards.\" Technoethics relates to the ethical considerations concerning technology in society.Human enhancement improves aspects of human function and may temporarily or permanently overcome the limitations of the human body through natural or artificial means. The consequences of such technological alterations implies ethical questions such as the unfair physical and mental categorization of certain individuals. Therefore, further consideration will need to be associated with ethical questions surrounding the evolution of technology. With growing trends of artificial intelligence and technological devices, such as Google Glass, stricter regulation will be necessary. Furthermore, Elon Musk recently stated that \"We need to be careful with AI (artificial intelligence). Potentially more dangerous than nukes\", meaning that there may be need to worry about the evolution of technology, and specifically how humans employ it to their benefit.\n\n"}
{"id": "43380", "url": "https://en.wikipedia.org/wiki?curid=43380", "title": "Trebuchet", "text": "Trebuchet\n\nA trebuchet (French \"trébuchet\") is a type of catapult, a common type of siege engine which uses a swinging arm to throw a projectile.\n\nThe traction trebuchet, also referred to as a mangonel at times, first appeared in Ancient China during the 4th century BC as a siege weapon. It spread westward, probably by the Avars, and was adopted by the Byzantines in the mid 6th century AD. It uses manpower to swing the arm.\n\nThe later counterweight trebuchet, also known as the counterpoise trebuchet, uses a counterweight to swing the arm. It appeared in both Christian and Muslim lands around the Mediterranean in the 12th century, and made its way back to China via Mongol conquests in the 13th century.\n\nThe trebuchet is a compound machine that makes use of the mechanical advantage of a lever to throw a projectile. They are typically large constructions (up to in height or more) made primarily of wood, usually reinforced with metal, leather, rope, and other materials. They are usually immobile and must be assembled on-site, possibly making use of local lumber with only key parts brought with the army to the site of the siege or battle.\n\nA trebuchet consists primarily of a long beam attached by an axle suspended high above the ground by a stout frame and base, such that the beam can rotate vertically through a wide arc (typically over 180°). A sling is attached to one end of the beam to hold the projectile. The projectile is thrown when the beam is quickly rotated by applying force to the opposite end of the beam. The mechanical advantage is primarily obtained by having the projectile end of the beam much longer than the opposite end where the force is applied – usually four to six times longer.\n\nCounterweight trebuchets are powered by gravity; potential energy is stored by slowly raising an extremely heavy box (typically filled with stones, sand, or lead) attached by a hinged connection to the shorter end of the beam, and releasing it on command. Traction trebuchets are human powered; on command, men pull ropes attached to the shorter end of the trebuchet beam. The difficulties of coordinating the pull of many men together repeatedly and predictably makes counterweight trebuchets preferable for the larger machines, though they are more complicated to engineer. Further increasing their complexity is that either winches or treadwheels, aided by block and tackle, are typically required to raise the more massive counterweights. So while counterweight trebuchets require significantly fewer men to operate than traction trebuchets, they require significantly more time to reload. In a long siege, reload time may not be a critical concern.\n\nWhen the trebuchet is loosed, the force causes rotational acceleration of the beam around the axle (the fulcrum of the lever). These factors multiply the acceleration transmitted to the throwing portion of the beam and its attached sling. The sling starts rotating with the beam, but rotates farther (typically about 360°) and therefore faster, transmitting this increased speed to the projectile. The length of the sling increases the mechanical advantage, and also changes the trajectory so that, at the time of release from the sling, the projectile is traveling in the desired speed and angle to give it the range to hit the target. Adjusting the sling's release point is the primary means of fine-tuning the range, as the rest of the trebuchet's actions are difficult to adjust after construction.\n\nThe rotation speed of the throwing beam increases smoothly until it reaches maximum rotation speed. Then the arm continues to rotate, slowing, coming to rest at the end of the rotation rather smoothly as momentum is transferred to the sling and its projectile. This is unlike the violent sudden stop inherent in the action of other siege engine designs such as the onager, which loses energy thereby. This key difference also makes the trebuchet much more durable, allowing for larger and more powerful machines.\n\nA trebuchet projectile can be almost anything, even debris, corpses, or incendiaries, but is typically a large stone. Dense stone, or even metal, specially worked to be round and smooth, gives the best range and predictability. When attempting to breach enemy walls, it is important to use materials that will not shatter on impact; projectiles were sometimes brought from distant quarries to get the desired properties.\n\nThe traction trebuchet, also referred to as a mangonel in some sources, is thought to have originated in ancient China. Torsion-based siege weapons such as the ballista and onager are not known to have been used in China.\n\nThe first recorded use of traction trebuchets was in ancient China. They were probably used by the Mohists as early as 4th century BC, descriptions of which can be found in the \"Mojing\" (compiled in the 4th century BC). In Chapter 14 of the \"Mojing\", the traction trebuchet is described hurling hollowed out logs filled with burning charcoal at enemy troops. The trebuchet was carried westward by the Avars and appeared next in the eastern Mediterranean by the late 6th century AD, where it replaced torsion powered siege engines such as the ballista and onager due to its simpler design and faster rate of fire. The Byzantines adopted the traction trebuchet possibly as early as 587, the Persians in the early 7th century, and the Arabs in the second half of the 7th century. The Franks and Saxons adopted the weapon in the 8th century.\n\nWest of China, the traction trebuchet remained the primary siege weapon until the 12th century when it was replaced by the counterweight trebuchet. In China the traction trebuchet continued to be used until the counterweight trebuchet was introduced during the Mongol conquest of the Song dynasty. In 617 Li Mi (Sui dynasty) constructed 300 trebuchets for his assault on Luoyang, in 621 Li Shimin did the same at Luoyang, and onward into the Song dynasty when in 1161, trebuchets operated by Song dynasty soldiers fired bombs of lime and sulphur against the ships of the Jin dynasty navy during the Battle of Caishi.\n\nThe term \"traction trebuchet\" is a modern invention and was not used by contemporary users of the weapon. The term was created mainly to distinguish it from the \"onager\", a torsion powered artillery weapon which is often confused with another name for the traction trebuchet, the \"mangonel\", a generic medieval term for stone throwing artillery. Confusion between the onager and mangonel in terminology has led some historians today to use \"traction trebuchet\" instead. The mangonel is called \"al-manjanīq\" in Arabic. In China the traction trebuchet was called the \"pào\" (砲).\n\nThe hand-trebuchet () was a staff sling mounted on a pole using a lever mechanism to propel projectiles. Basically a one-man traction trebuchet, it was used by emperor Nikephoros II Phokas around 965 to disrupt enemy formations in the open field. It was also mentioned in the Taktika of general Nikephoros Ouranos (c. 1000), and listed in \"De obsidione toleranda\" (author anonymous) as a form of artillery.\n\nAccording to Paul E. Chevedden, a hybrid trebuchet existed that used both counterweight and human propulsion. However no illustrations or descriptions of the device exist from the time when they were supposed to have been used. The entire argument for the existence of hybrid trebuchets rests on accounts of increasingly more effective siege weapons. Peter Purton suggests that this was simply because the machines became larger. The earliest depiction of a hybrid trebuchet is dated to 1462, when trebuchets had already become obsolete due to cannons.\n\nThe earliest known description and illustration of a counterweight trebuchet comes from a commentary on the conquests of Saladin by Mardi ibn Ali al-Tarsusi in 1187.\n\nThe earliest solid reference to counterweight trebuchets in European sources dates to the siege of Castelnuovo Bocca d'Adda in 1199. They were used in Germany from around 1205, in England at least by 1217, and in Iberia shortly after 1218. By the 1230s the counterweight trebuchet was a common item in siege warfare.\n\nPaul E. Chevedden argues that counterweight trebuchets appeared even earlier in Europe based on what might have been counterweight trebuchets in earlier sources. The 12th century Byzantine historian Niketas Choniates may have been referring to a counterweight trebuchet when he described one equipped with a windlass, which is only useful to counterweight machines, at the siege of Zevgminon in 1165. At the Siege of Nicaea in 1097 the Byzantine emperor Alexios I Komnenos reportedly invented new pieces of heavy artillery which deviated from the conventional design and made a deep impression on everyone. Possible references to counterweight trebuchets also appear for the second siege of Tyre in 1124, where the crusaders reportedly made use of \"great trebuchets\". Chevedden argues that given the references to new and better trebuchets that by the 1120–30s, the counterweight trebuchet was being used in a variety of places by different peoples such as the crusader states, the Normans of Sicily and the Seljuks.\n\nCounterweight trebuchets do not appear with certainty in Chinese historical records until about 1268 when the Mongols laid siege to Fancheng and Xiangyang. After failing to take the twin cities of Fancheng and Xiangyang for several years, collectively known as the Siege of Fancheng and Xiangyang, the Mongol army brought in two Persian engineers to build hinged counterweight trebuchets. Known as the Huihui trebuchet (回回砲, where \"huihui\" is a loose slang referring to any Muslims), or Xiangyang trebuchet (襄陽砲) because they were first encountered in that battle. Ismail and Al-aud-Din arrived travelled to South China from Iraq and mangonels and trebuchets for the siege. Chinese and Muslim engineers operated artillery and siege engines for the Mongol armies. The counterweight trebuchet, known as the Muslim trebuchet in China, replaced the traction version after its introduction in the late 13th century. Its greater range was however, somewhat countered by the fact that it had to be constructed close to the site of the siege unlike traction trebuchets, which were easier to take apart and put back together again where necessary.\n\nThe counterweight trebuchet remained in use in China for roughly two centuries, at which point it was well on its way to obsolescence.\n\nThe couillard is a smaller version of a counterweight trebuchet with a single frame instead of the usual double \"A\" frames. The counterweight is split into two halves to avoid hitting the center frame.\n\nWith the introduction of gunpowder, the trebuchet began to lose its place as the siege engine of choice to the cannon. Trebuchets were still used both at the siege of Burgos (1475–1476) and siege of Rhodes (1480). One of the last recorded military uses was by Hernán Cortés, at the 1521 siege of the Aztec capital Tenochtitlán. Accounts of the attack note that its use was motivated by the limited supply of gunpowder. The attempt was reportedly unsuccessful: the first projectile landed on the trebuchet itself, destroying it.\n\nMost trebuchet use in recent centuries has been for recreational or educational, rather than military purposes. New machines have been constructed and old ones restored by living history enthusiasts, for historical re-enactments, and use in other historical celebrations. As their construction is substantially simpler than modern weapons, trebuchets also serve as the object of engineering challenges.\n\nThe trebuchet's technical constructions were lost at the beginning of the 16th century. In 1984, the French engineer Renaud Beffeyte made the first modern reconstruction of a trebuchet, based on documents from 1324.\n\nThe largest currently-functioning trebuchet in the world is the 22-tonne machine at Warwick Castle, England, constructed in 2005. Based on historical designs, it stands tall and throws missiles typically 36 kg (80 lbs) up to . The trebuchet gained significant interest from numerous news sources when in 2015 a burning missile fired from the siege engine struck and damaged a Victorian-era boathouse situated at the River Avon close by, inadvertently demonstrating the weapon's power. It is built on the design of a similar trebuchet at Middelaldercentret in Denmark. In 1989, Middelaldercentret became the first place in the modern era to have a working trebuchet.\n\nTrebuchets compete in one of the classifications of machines used to hurl pumpkins at the annual pumpkin chucking contest held in Sussex County, Delaware, U.S. The record-holder in that contest for trebuchets is the Yankee Siege II from New Hampshire, which at the 2013 WCPC Championship tossed a pumpkin 2835.8 ft (864.35 metres). The , trebuchet flings the standard pumpkins, specified for all entries in the WCPC competition.\n\nA large trebuchet has recently been tested in Belfast as part of the set for the television series \"Game of Thrones.\"\n\nAlthough rarely used as a weapon today, trebuchets maintain the interest of professional and hobbyist engineers. One modern technological development, especially for the competitive pumpkin-hurling events, is the \"floating arms\" design. Instead of using the traditional axle fixed to a frame, these devices are mounted on wheels that roll on a track parallel to the ground, with a counterweight that falls directly downward upon release, allowing for greater efficiency by increasing the proportion of energy transferred to the projectile.\n\nIn 2013, during the Syrian civil war, rebels were filmed using a trebuchet in the Battle of Aleppo. The trebuchet was used to project explosives at government troops.\n\nIn 2014, during the Hrushevskoho street riots in Ukraine, rioters used an improvised trebuchet to throw bricks and molotov cocktails at the Berkut.\n\n\n"}
{"id": "12286920", "url": "https://en.wikipedia.org/wiki?curid=12286920", "title": "W74", "text": "W74\n\nThe W74 was an experimental American nuclear artillery shell.\n\nResponding to a 1969 United States Army request for a replacement for the W48 155 mm artillery shell, the Los Alamos National Laboratory started development of the W74. However, by mid 1973 the program was discontinued without producing a weapon for general production. The primary reasons for the cancellation of the W74 were the obsolete technology used and high cost. The weapon only employed fission, had no improvements in accuracy, and had no capability to be further developed as an \"enhanced radiation\" (neutron) device; the result was a weapon that did not provide new capabilities, all with a per shell cost of $425,000 (1973).\n\nThe W75 was the 8 inch (203mm) equivalent of the W74. It was viewed by many as being an obsolete system, expensive to deploy and not worth pursuing. Development was started in 1973 at the Livermore Lab and terminated two years later.\n\n"}
{"id": "55533945", "url": "https://en.wikipedia.org/wiki?curid=55533945", "title": "Woz U", "text": "Woz U\n\nWoz U is a tech education platform launched by Apple co-founder Steve Wozniak that focuses on both students and companies. Woz U is Arizona-based with plans to launch physical locations for learning in more than 30 cities across the globe. Although the company is registered in Arizona, it works through its Texas-based partner, Southern Careers Institute. Less than a week after the launch announcement, the company came under scrutiny by state regulators about the lack of state accreditation.\n\nIn fall of 2018, a CBS News investigation of Woz U cast some doubts on the professionalism of the expensive curriculum. CBS interviewed two dozen current and former students and employees, who shared their dissatisfaction with the content quality, such as documentation typos leading to confusing program errors, while some promised live lectures were actually recorded and out-of-date. One student described the 33-week online program as \"a $13,000 e-book\". A former \"enrollment counselor\" described a high-pressure sales environment, which the company denied. In a prepared statement, Woz U president Chris Coleman admitted the documentation errors and said quality control efforts were being implemented, and said curriculum was reviewed by Wozniak. The founder declined interview requests, then dodged a reporter's unannounced appearance at a conference.\n\n"}
