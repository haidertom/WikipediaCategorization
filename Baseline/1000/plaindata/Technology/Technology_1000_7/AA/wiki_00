{"id": "10951179", "url": "https://en.wikipedia.org/wiki?curid=10951179", "title": "3DML", "text": "3DML\n\n3DML was a format for creating three-dimensional websites build up by combining similar sized building blocks. It was invented in 1997 by Michael Powers, who co-developed with Philip Stevens and developed further over the next four years. 3DML files are written in an XML syntax which can be delivered from standard web servers and shown within a browser plugin and independent 3DML browser called Flatland Rover. A new update was posted in 2018 with updated code and binaries for Windows 10. 3DML had no avatar or multi-user support unlike other platforms of the time like Active Worlds thus never attracting a huge number of followers. There was only a plugin for Internet Explorer, Netscape Navigator and AOL, but not for Mozilla Firefox. The most recent version is a standalone Windows application.\n\nA 3DML world was called a \"Spot\". In the spot \"blocks\" can be inserted, laid out in a grid. The blocks can be ordered into \"levels\" - each has the same size. This approach was designed to simplify the building process and comprehension of 3D pages. The following is an example of a full Spot description of a 3D room with walls.\n\nYou can navigate the spots by using either mouse or arrow keys.\n\n\n"}
{"id": "36235294", "url": "https://en.wikipedia.org/wiki?curid=36235294", "title": "3G adoption", "text": "3G adoption\n\n3G mobile telephony was relatively slow to be adopted 3Gglobally. In some instances, 3G networks do not use the same radio frequencies as 2G so mobile operators must build entirely new networks and license entirely new frequencies, especially so to achieve high data transmission rates. Other delays were due to the expenses of upgrading transmission hardware, especially for UMTS, whose deployment required the replacement of most broadcast towers. Due to these issues and difficulties with deployment, many carriers were not able to or delayed acquisition of these updated capabilities.\n\nIn December 2007, 190 3G networks were operating in 40 countries and 154 HSDPA networks were operating in 71 countries, according to the Global Mobile Suppliers Association (GSA). In Asia, Europe, Canada and the USA, telecommunication companies use W-CDMA technology with the support of around 100 terminal designs to operate 3G mobile networks.\n\nRoll-out of 3G networks was delayed in some countries by the enormous costs of additional spectrum licensing fees. (See Telecoms crash.) The license fees in some European countries were particularly high, bolstered by government auctions of a limited number of licenses and sealed bid auctions, and initial excitement over 3G's potential.\n\nThe 3G standard is perhaps well known because of a massive expansion of the mobile communications market post-2G and advances of the consumer mophone. An especially notable development during this time is the smartphone (for example, the iPhone, and the Android family), combining the abilities of a PDA with a mobile phone, leading to widespread demand for mobile internet connectivity. 3G has also introduced the term \"mobile broadband\" because its speed and capability make it a viable alternative for internet browsing, and USB Modems connecting to 3G networks are becoming increasingly common.\n\nThe first African use of 3G technology was a 3G video call made in Johannesburg on the Vodacom network in November 2004. The first commercial launch was by Emtel-ltd in Mauritius in 2004. In late March 2006, a 3G service was provided by the new company Wana in Morocco. In May 2007, Safaricom launched 3G services in Kenya while later that year Vodacom Tanzania also started providing services. In February 2012 Bharti Airtel Launched a 3.75G Network in selected cities in Kenya with a countrywide rollout planned for later in the year. In Egypt, Mobinil launched the service in 2008 and in Somaliland, Telesom started first 3G services on 3 July 2011, to both prepaid and postpaid subscription customers.\nTelecommunication networks in Nigeria like Globacom, Etisalat, Airtel and MTN provide 3G services to their numerous customers.\n\nAsia is also using 3G services very well. A lot of companies like Dialog Axiata PLC Sri Lanka (First to serve 3G Service in South Asia in 2006), BSNL, WorldCall, PTCL, Mobilink, Zong, Ufone, Telenor PK, Maxis, Vodafone, Airtel, Idea Cellular, Aircel, Tata DoCoMo and Reliance have released their 3G services.\n\nSri Lanka's All Mobile Networks(Dialog, Mobitel, Etisalat, Hutch, Airtel,) And CDMA Network Providers (Lankabell, Dialog,Suntel,SLT) Launched 3G Services.\n\nDialog, Mobitel launched 4G LTE Services In Sri Lanka.\nNot Only (Dialog CDMA, Lankabell CDMA Have 4G LTE Services.\nSri Lanka Telecom Have 4G LTE , FTTX Services..\n\nOn March 19, 2012, Etisalat Afghanistan, the fastest growing telecommunications company in the country and part of Etisalat Group, announced the launch of 3G services in Afghanistan. between 2013-2014 all telecommunications company ( Afghan Wireless, Etisalat, Roshan, MTN and Salaam Network) provided 3G, 3.5G and 3.75G services and they are planning for 4G services in 2016-2017.\n\nNepal was one of the first countries in southern Asia to launch 3G services. Nepal's first 3G company was NTC(Nepal Telecom Corporation) and the second was Ncell. Ncell also covered Mount Everest with 3G. NTC provides high speed video calling with other 3G services, as well as post-paid and pre-paid 3G SIM cards.\n\n3G and 4G was simultaneously launched in Pakistan on April 23, 2014 through a SMRA Auction. Three out of Five Companies got a 3G licence i.e. Ufone, Mobilink and Telenor while China Mobile's Zong got 3G as well as a 4G licence. Whereas fifth company, Warid Pakistan did not participate in the auction procedure, But they launched 4G LTE services on their existing 2G 1800 MHz spectrum due to Technology neutral terms and became world's first Telecom Company to transform directly from 2G to 4G. With that Pakistan joined the 3G and 4G world.\nIn the non-mobile sector, Pakistan's biggest telecommunication company PTCL launched its 3G network, EVO, in mid-2008 and has since then established itself in this sector. It provides 3G services in 105 cities across Pakistan. Omantel's WorldCall also provides 3G services in 50 cities Pakistan-wide. They provide mobile broadband service via dongles and modems. On 14 August 2010, Pakistan became the first country in the world to experience EVDO's RevB 3G technology that offers maximum speeds of 9.3 Mbit/s. At present the services of EVO Nitro (brand name) are available in Islamabad, Rawalpindi, Lahore and Karachi. The RevA network, with speeds if up to 3.1 Mbit/s is available in over 100 cities of the country.\n\nState-run mobile operator Teletalk Bangladesh limited and other GSM operators GrameenPhone, Banglalink, Robi and Airtel already started hi-speed 3G+ and 3.5G services using UMTS with HSDPA facilities. Grameenphone has a plan to launch 4G LTE services first time in Bangladesh using TD-LTE technology. Currently Grameenphone owned 10 MHz spectrum at 3G auction by BTRC.Robi and Airtel recently merged, newly merged company has a plan to introduce 4G operation soon. Two other data operators, Qubee and Banglalion, currently offer 4G Wimax services in Bangladesh. CityCell now switched off their operation by government order. 4G LTE services have already begun in Bangladesh through all mobile operators except Teletalk, the state run mobile operator. Bangladesh has a plan to introduce super speed 5G service soon. A test run will be conducted in the country in mid July 2018.\n\nChina announced in May 2008, that the telecoms sector was re-organized and three 3G networks would be allocated so that the largest mobile operator, China Mobile, would retain its GSM customer base. China Unicom would retain its GSM customer base but relinquish its CDMA2000 customer base, and launch 3G on the globally leading W-CDMA (UMTS) standard. The CDMA2000 customers of China Unicom would go to China Telecom, which would then launch 3G on the CDMA2000 1x EV-DO standard. This meant that China would have all three main cellular technology 3G standards in commercial use. Finally in January 2009, Ministry of industry and Information Technology of China awarded licenses of all three standards: TD-SCDMA to China Mobile, W-CDMA to China Unicom and CDMA2000 to China Telecom. The launch of 3G occurred on 1 October 2009, to coincide with the 60th Anniversary of the Founding of the People's Republic of China. By August 2011, China Telecom's 3G subscriber has exceeded 23 million\n\n11 December 2008, India entered the 3G arena with the launch of 3G enabled Mobile and Data services by Government owned Mahanagar Telephone Nigam Ltd MTNL in Delhi and later in Mumbai. MTNL becomes the first 3G Mobile service provider in India. After MTNL, another state operator Bharat Sanchar Nigam Ltd. (BSNL) launched 3G services on 22 Feb 2009 in Chennai and Kolkata and later launched 3G as Nationwide. The auction of 3G wireless spectrum was announced in April 2010 and 3G Spectrum allocated to all private operators on 1 September 2010.\n\nNorth Korea has had a 3G network since 2008, which is called Koryolink, a joint venture between Egyptian company Orascom Telecom Holding and the state-owned Korea Post and Telecommunications Corporation (KPTC) is North Korea's only 3G Mobile operator, and one of only two mobile companies in the country. According to Orascom quoted in BusinessWeek, the company had 125,661 subscribers in May 2010. The Egyptian company owns 75 percent of Koryolink, and is known to invest in infrastructure for mobile technology in developing nations. It covers Pyongyang, and five additional cities and eight highways and railways. Its only competitor, SunNet, uses GSM technology and suffers from poor call quality and disconnections. Phone numbers on the network are prefixed with +850 (0)192.\n\n3G services were made available in the Philippines on December 2008.\n\n3G services were made available in Singapore on October 2007. Widespread adoption of 3G began in January 2009, with the upgrading of phones to iPhone 3G and Android.\n\nIn Europe, mass market commercial 3G services were introduced starting in March 2003 by O2 in the UK and Italy. The European Union Council suggested that the 3G operators should cover 80% of the European national populations by the end of 2005.\n\nIn Canada, Bell Mobility, SaskTel and Telus launched a 3G EVDO network in 2005. Rogers Wireless was the first to implement UMTS technology, with HSDPA services in eastern Canada in late 2006. Realizing they would miss out on roaming revenue from the 2010 Winter Olympics, Bell and Telus formed a joint venture and rolled out a shared HSDPA network using Nokia Siemens technology. After the AWS spectrum in 2008, new entrants to the Canadian wireless markets including but not limited to Mobilicity, Wind Mobile and Vidéotron have deployed their own UMTS networks in Canada using the AWS spectrum.\n\nIn Iran Rightel won the bid for the third Operator license. Rightel is the first 3G operator in Iran. Rightel has commercially launched in the last months of 2011.\n\nIn Jordan, Orange is the first mobile 3G operator.\n\nMobitel Iraq is the first mobile 3G operator in Iraq. It was launched commercially on February 2007.\n\nMTN Syria is the first mobile 3G operator in Syria. It was launched commercially on May 2010.\n\nIn Lebanon Ministry of Telecoms launched a test period on September 20, 2011, where 4,000 smart-phone users were selected to enjoy 3G for one month and provide feedback. Currently, the test period is over, MTC Touch and Alfa began rolling out the new 3G services.\n\nSaudi Arabia has got 4G as well as 3G/HSPA With Zain KSA, Saudi Telecom, and Mobily KSA.\n\nTurkcell, Avea and Vodafone launched their 3G networks commercially on 30 July 2009 at the same time. Turkcell and Vodafone launched their 3G service on all provincial centres. Avea launched it on 16 provincial centres. It was after Turkey's monopoly mobile operator Turkcell accepted number portability, mobile operators attended frequency band auction and frequencies for 3G usage distributed around mobile operators. Turkcell got A band, Vodafone B and Avea C. Currently Turkcell and Vodafone have 3G networks on most of crowded cities and towns. Turkey has 3.9G networks now.\n\nIn late 2005, Vodafone NZ launched their 3G network, followed by Spark NZ's XT network in 2008, and newcomer 2degrees using a combination of Vodafone's 3G towers and their own in 2009. 2degrees has since built more towers, and is now self-sufficient in the major cities (Auckland, Hamilton, Wellington, Christchurch and Dunedin) but relies on a roaming agreement with Vodafone to cover the rest of the country. This gives it essentially the same footprint as Vodafone.\n"}
{"id": "47433257", "url": "https://en.wikipedia.org/wiki?curid=47433257", "title": "5-Tiles", "text": "5-Tiles\n\n5-Tiles is a virtual keyboard for mobile devices with touchscreen that run the Android operating system. It is characterized by needing a small amount of space leaving as much space as possible to the software that needs the keyboard. As the name indicates there are exactly five keys on the keyboard on one line of keys. Characters are typed by tap and swipe gestures.\n\nThe five keys of this keyboard all have different colors to be easily distinguishable. You have the following options to type a character:\nThe concept of this keyboard was invented in 2004. The co-founder of the company creating this keyboard, Michal Kubacki, first didn't have skills in programming. He learned some skills that allowed him to write basic software for testing purposes and to further develop the keyboard. He got in touch with people who helped him creating the product as a virtual keyboard for Android devices. An early version was downloaded thousands of times. At the 2013 Droidcon Demo Camp the company won the second place. It was also awarded in other competitions.\n"}
{"id": "19855393", "url": "https://en.wikipedia.org/wiki?curid=19855393", "title": "AS4", "text": "AS4\n\nAS4 (Applicability Statement 4) is a Conformance Profile of the OASIS ebMS 3.0 specification, and represents an open standard for the secure and payload-agnostic exchange of Business-to-business documents using Web services. Secure document exchange is governed by aspects of WS-Security, including XML Encryption and XML Digital Signatures. Payload agnosticism refers to the document type (e.g. purchase order, invoice, etc.) not being tied to any defined SOAP action or operation.\n\nAS4 became a standard in 2013. The majority of the AS4 profiling points constraining the ebMS 3.0 specification are based upon the functional requirements of the AS2 specification. By scaling back ebMS 3.0 using AS2 as a blueprint, AS4 provides an entry-level on-ramp for Web services B2B by simplifying the complexities of Web services.\n\n\n"}
{"id": "49071369", "url": "https://en.wikipedia.org/wiki?curid=49071369", "title": "Agritech", "text": "Agritech\n\nAgritech is the use of technology in agriculture, horticulture, and aquaculture with the aim of improving yield, efficiency, and profitability. Agritech can be products, services or applications derived from agriculture that improve various input/output processes.\n\nA major turning point for agricultural technology is the Industrial Revolution.\n\nTechnologies and applications in agri-tech include:\n\nDrones\nsatellite photography and sensors\nIoT-based sensor networks\nphase tracking\nweather forecasts\nautomated irrigation\nlight and heat control\nintelligent software analysis for pest and disease prediction, soil management and other involved analytical tasks\nBiotech is another type of agri-tech.\n\nAgriculture has been wrongly perceived in the past as a \"dirty job\" for the old people in rural communities but with the renaissance Technology brought to Agriculture, young people now see it as a potential sector to explore.\n\nThis is the practical use of Technology in Agriculture by a few start-ups in Africa.\n\nThere is a Nigeria’s digital agriculture platform which focused on connecting farm sponsors with real farmers in order to increase food production while promoting youth participation in agriculture.\n\nThis agritech startup is currently disrupting the agriculture ecosystem in the country by connecting small-scale farmers with investors using their platform App which is available on Google and Apple app stores.\n\nFarmers and sponsors all receive a percentage of the profits on harvest. The platform also makes provision for insurance cover for all existing farm projects, so that in the event of unforeseen circumstances, the sponsors’ capital can be refunded. \nDrones can be used on Crop field for scanning with compact multispectral imaging sensors, GPS map creation through onboard cameras, heavy payload transportation, and livestock monitoring with thermal-imaging camera-equipped drones.\n\nHigh-income countries have seen recent improvements in their agricultural management systems through modern remote sensing technology, such as satellites and aircraft and the information they collect. Out of the vast amount of data collected, advice is provided to farmers and fishers to help inform their decisions.\n\nThis has led to better crop yields, higher quality produce and more sustainable agricultural practices in some cases. Big data also informs high-level decision-makers on how to better manage food supply at national and regional levels.\n\nThe use of small Unmanned Aerial Vehicles (UAVs) better known as 'drones' for agricultural purposes is a new emerging technology which could revolutionise the way agricultural entrepreneurs interact with their land, water, crops and infrastructure. UAVs can be made specifically for business use and farming in particular, they can capture geo-referenced, overlapping, high-resolution images (2–5 cm) of 400 hectares in a single flight; can seamlessly upload data and produce agricultural analytics from their data management systems, and fly autonomously from take-off to landing.\n\nA blockchain is a digitized, decentralized, public ledger of all cryptocurrency transactions. Constantly growing as ‘completed’ blocks (the most recent transactions) are recorded and added to it in chronological order, it allows market participants to keep track of digital currency transactions without central recordkeeping. Each node (a computer connected to the network) gets a copy of the blockchain, which is downloaded automatically.\n\nOriginally developed as the accounting method for the virtual currency Bitcoin, blockchains – which use what's known as distributed ledger technology (DLT) – are appearing in a variety of commercial applications today. Currently, the technology is primarily used to verify transactions, within digital currencies though it is possible to digitize, code and insert practically any document into the blockchain. Doing so creates an indelible record that cannot be changed; furthermore, the record’s authenticity can be verified by the entire community using the blockchain instead of a single centralized authority.\n\nToday, Blockchain is revolutionizing the Agriculture sector in many ways including\n\nTrading commodities on the blockchain is helping to reduce middlemen interference by promoting a peer-to-peer model of connecting farmers with end users. It is also helping to promote fairer trades by removing trade barriers, reducing over reliance on United States Dollars and promoting cross-border trading in local currency.\n\nOther methods of leveraging blockchain technology is in settling land disputes through blockchain based land registry, and using QR codes to promote traceability.\n\nInformation Communication Technologies like podcasts, weblogs, social media platforms, e-books are constantly helping to bridge the information gap in the Agriculture sector for farmers and Agripreneurs.\n\nHydroponics is a soilless farming technology that is used to grow vegetables and tomatoes.\n\nIt guarantees an all-year-round production for farmers and insulates these crops from the effects of climate change. \n"}
{"id": "21457566", "url": "https://en.wikipedia.org/wiki?curid=21457566", "title": "Astra Taylor", "text": "Astra Taylor\n\nAstra Taylor (born 1979) is a Canadian-American documentary filmmaker, writer, activist and musician. She is a fellow of the Shuttleworth Foundation, for her work on challenging predatory practices around debt.\n\nBorn in Winnipeg, Manitoba, Taylor grew up in Athens, Georgia, and was unschooled until age 13 when she enrolled in ninth grade. At 16 she abandoned high school to attend classes at the University of Georgia, which she only stayed at for a year before heading to Brown University. She attended classes there for a year and dropped out when she realized that \"unschooling\" was a lifelong commitment. Taylor has taught sociology at the University of Georgia and SUNY New Paltz. Her writings have appeared in numerous magazines, including Dissent, n+1, Adbusters, The Baffler, The Nation, Salon, and The London Review of Books.\n\nTaylor is the sister of painter and disability activist Sunny Taylor, and is married to Jeff Mangum of Neutral Milk Hotel. She joined Neutral Milk Hotel onstage for a number of shows in 2013 and 2014, playing guitar and accordion. She is a vegan. She lives in New York.\n\nTaylor was active in the Occupy Movement and was the co-editor of Occupy!: An OWS-Inspired Gazette with Sarah Leonard of Dissent magazine and Keith Gessen of n+1. The broadsheet covered Occupy Wall Street in five issues over the course of the first year of the occupation and was later anthologized by Verso Books.\n\n\n\n\n"}
{"id": "20948", "url": "https://en.wikipedia.org/wiki?curid=20948", "title": "Brainwashing", "text": "Brainwashing\n\nBrainwashing (also known as mind control, menticide, coercive persuasion, thought control, thought reform, and re-education) is the concept that the human mind can be altered or controlled by certain psychological techniques. Brainwashing is said to reduce its subject’s ability to think critically or independently, to allow the introduction of new, unwanted thoughts and ideas into the subject’s mind, as well as to change his or her attitudes, values, and beliefs. \n\nThe concept of brainwashing was originally developed in the 1950s to explain how the Chinese government appeared to make people cooperate with them. Advocates of the concept also looked at Nazi Germany, at some criminal cases in the United States, and at the actions of human traffickers. It was later applied by Margaret Singer, Philip Zimbardo and some others in the anti-cult movement to explain conversions to some new religious movements and other groups. This resulted in scientific and legal debate with Eileen Barker, James Richardson, and other scholars, as well as legal experts, rejecting at least the popular understanding of brainwashing.\n\nOther views have been expressed by scholars including: Dick Anthony, Robert Cialdini, Stanley A. Deetz, Michael J. Freeman, Robert Jay Lifton, Joost Meerloo, Daniel Romanovsky, Kathleen Taylor, Louis Jolyon West, and Benjamin Zablocki. The concept of brainwashing is sometimes involved in legal cases, especially regarding child custody; and is also a major theme in science fiction and in criticism of modern political and corporate culture. Although the term appears in the fifth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-5) of the American Psychiatric Association it is not accepted as scientific fact.\n\nThe Chinese term \"xǐnăo\" (洗脑，literally \"wash brain\") was originally used to describe the coercive persuasion used under the Maoist government in China, which aimed to transform \"reactionary\" people into \"right-thinking\" members of the new Chinese social system. The term punned on the Taoist custom of \"cleansing/washing the heart/mind\" (\"xǐxīn\"，洗心) before conducting ceremonies or entering holy places.\n\nThe \"Oxford English Dictionary\" records the earliest known English-language usage of the word \"brainwashing\" in an article by newspaperman Edward Hunter, in \"Miami News\", published on 24 September 1950. Hunter was an outspoken anticommunist and was said to be a CIA agent working undercover as a journalist. Hunter and others used the Chinese term to explain why, during the Korean War (1950-1953), some American prisoners of war (POWs) cooperated with their Chinese captors, even in a few cases defected to their side. British radio operator Robert W. Ford and British army Colonel James Carne also claimed that the Chinese subjected them to brainwashing techniques during their war-era imprisonment.\n\nThe U.S. military and government laid charges of brainwashing in an effort to undermine confessions made by POWs to war crimes, including biological warfare. After Chinese radio broadcasts claimed to quote Frank Schwable, Chief of Staff of the First Marine Air Wing admitting to participating in germ warfare, United Nations commander Gen. Mark W. Clark asserted:\n\nBeginning in 1953, Robert Jay Lifton interviewed American servicemen who had been POWs during the Korean War as well as priests, students, and teachers who had been held in prison in China after 1951. In addition to interviews with 25 Americans and Europeans, Lifton interviewed 15 Chinese citizens who had fled after having been subjected to indoctrination in Chinese universities. (Lifton's 1961 book \"\", was based on this research.) Lifton found that when the POWs returned to the United States their thinking soon returned to normal, contrary to the popular image of \"brainwashing.\"\n\nIn 1956, after reexamining the concept of brainwashing following the Korean War, the U.S. Army published a report entitled \"Communist Interrogation, Indoctrination, and Exploitation of Prisoners of War\", which called brainwashing a \"popular misconception\". The report states \"exhaustive research of several government agencies failed to reveal even one conclusively documented case of 'brainwashing' of an American prisoner of war in Korea.\"\n\nIn George Orwell's 1949 dystopian novel \"Nineteen Eighty-Four\" the main character is subjected to imprisonment, isolation, and torture in order to conform his thoughts and emotions to the wishes of the rulers of Orwell's fictional future totalitarian society. Orwell's vision influenced Hunter and is still reflected in the popular understanding of the concept of brainwashing. Written about the same time, J.R.R. Tolkien’s \"The Lord of the Rings\" also addressed brainwashing, although in a fantasy setting. The science fiction stories of Cordwainer Smith (written from the 1940s until his death in 1966) depict brainwashing to remove memories of traumatic events as a normal and benign part of future medical practice.\n\nIn the 1950s many American films were filmed that featured brainwashing of POWs, including \"The Rack\", \"The Bamboo Prison\", \"Toward the Unknown\", and \"The Fearmakers\". \"Forbidden Area\" told the story of Soviet secret agents who had been brainwashed through classical conditioning by their own government so they wouldn't reveal their identities. In 1962 \"The Manchurian Candidate\" (based on the 1959 novel by Richard Condon) \"put brainwashing front and center\" by featuring a plot by the Soviet government to take over the United States by use of a brainwashed presidential candidate. The concept of brainwashing became popularly associated with the research of Russian psychologist Ivan Pavlov, which mostly involved dogs, not humans, as subjects. In \"The Manchurian Candidate\" the head brainwasher is Dr. Yen Lo, of the Pavlov Institute.\n\nMind control remains an important theme in science fiction. Terry O'Brien comments: \"Mind control is such a powerful image that if hypnotism did not exist, then something similar would have to have been invented: the plot device is too useful for any writer to ignore. The fear of mind control is equally as powerful an image.\" A subgenre is \"corporate mind control\", in which a future society is run by one or more business corporations that dominate society using advertising and mass media to control the population's thoughts and feelings.\n\nFor twenty years starting in the early 1950s, the United States Central Intelligence Agency (CIA) and the United States Department of Defense conducted secret research, including Project MKUltra, in an attempt to develop practical brainwashing techniques; the results are unknown. (See also Sidney Gottlieb.) CIA experiments using various psychedelic drugs such as LSD and Mescaline drew from Nazi human experimentation.\n\nIn 1974, Patty Hearst, a member of the wealthy Hearst family, was kidnapped by a left-wing group calling itself the Symbionese Liberation Army. After several weeks of captivity she agreed to join the group and took part in their activities. In 1975, she was arrested and charged with bank robbery and use of a gun in committing a felony. Her attorney, F. Lee Bailey argued in her trial that she should not be held responsible for her actions since her treatment by her captors was the equivalent of the brainwashing of Korean War POWs. (See: diminished responsibility.) Hearst was found guilty, but her “brainwashing defense” brought the topic to renewed public attention in the United States, as did the 1969 to 1971 case of Charles Manson, who was said to have brainwashed his followers to commit murder and other crimes.\n\nBailey developed his case in conjunction with psychiatrist Louis Jolyon West and psychologist Margaret Singer. They had both studied the experiences of Korean War POWs. In 1996 Singer published her theories in her best-selling book \"Cults in Our Midst\". In 2003, the brainwashing defense was used unsuccessfully in the defense of Lee Boyd Malvo, who was charged with murder for his part in the D.C. sniper attacks. Some legal scholars have argued that the brainwashing defense undermines the law’s fundamental premise of free will.\n\nItaly has had controversy over the concept of \"plagio\", a crime consisting in an absolute psychological—and eventually physical—domination of a person. The effect is said to be the annihilation of the subject's freedom and self-determination and the consequent negation of his or her personality. The crime of plagio has rarely been prosecuted in Italy, and only one person was ever convicted. In 1981, an Italian court found that the concept is imprecise, lacks coherence, and is liable to arbitrary application. By the twenty-first century, the concept of brainwashing was being applied \"with some success\" in child custody and child sexual abuse cases. In some cases \"one parent is accused of brainwashing the child to reject the other parent, and in child sex abuse cases where one parent is accused of brainwashing the child to make sex abuse accusations against the other parent\" (possibly resulting in or causing parental alienation).\n\nIn 2003, forensic psychologist Dick Anthony said that \"no reasonable person would question that there are situations where people can be influenced against their best interests, but those arguments are evaluated on the basis of fact, not bogus expert testimony.\" In 2016, Israeli anthropologist of religion and fellow at the Van Leer Jerusalem Institute Adam Klin-Oron said about then-proposed \"anti-cult\" legislation: \n\nIn the 1970s, the anti-cult movement applied the concept of brainwashing to explain seemingly sudden and dramatic religious conversions to various new religious movements (NRMs) and other groups they considered cults. News media reports tended to support the brainwashing view and social scientists sympathetic to the anti-cult movement, who were usually psychologists, developed revised models of mind control. While some psychologists were receptive to the concept, sociologists were for the most part skeptical of its ability to explain conversion to NRMs.\n\nPhilip Zimbardo defined mind control as, \"the process by which individual or collective freedom of choice and action is compromised by agents or agencies that modify or distort perception, motivation, affect, cognition or behavioral outcomes,\" and he suggested that any human being is susceptible to such manipulation. Another adherent to this view, Jean-Marie Abgrall was heavily criticized by forensic psychologist Dick Anthony for employing a pseudo-scientific approach and lacking any evidence that anyone's worldview was substantially changed by these coercive methods. On the contrary, the concept and the fear surrounding it was used as a tool for the anti-cult movement to rationalize the persecution of minority religious groups.\n\nEileen Barker criticized the concept of mind control because it functioned to justify costly interventions such as deprogramming or exit counseling. She has also criticized some mental health professionals, including Singer, for accepting expert witness jobs in court cases involving NRMs. Her 1984 book, \"\" describes the religious conversion process to the Unification Church (whose members are sometimes informally referred to as \"Moonies\"), which had been one of the best known groups said to practice brainwashing. Barker spent close to seven years studying Unification Church members. She interviewed in depth or gave probing questionnaires to church members, ex-members, \"non-joiners,\" and control groups of uninvolved people from similar backgrounds, as well as parents, spouses, and friends of members. She also attended numerous church workshops and communal facilities. Barker writes that she rejects the \"brainwashing\" theory, because it explains neither the many people who attended a recruitment meeting and did not become members, nor the voluntary disaffiliation of members.\n\nJames Richardson observed that if the new religious movements had access to powerful brainwashing techniques, one would expect that they would have high growth rates, yet in fact most have not had notable success in recruitment. Most adherents participate for only a short time, and the success in retaining members is limited. For this and other reasons, sociologists of religion including David Bromley and Anson Shupe consider the idea that \"cults\" are brainwashing American youth to be \"implausible.\" In addition, Thomas Robbins, Massimo Introvigne, Lorne Dawson, Gordon Melton, Marc Galanter, and Saul Levine, amongst other scholars researching NRMs, have argued and established to the satisfaction of courts, relevant professional associations and scientific communities that there exists no generally accepted scientific theory, based upon methodologically sound research, that supports the concept of brainwashing as advanced by the anti-cult movement.\n\nBenjamin Zablocki responded that brainwashing is not \"a process that is directly observable,\" and that the \"real sociological issue\" is whether \"brainwashing occurs frequently enough to be considered an important social problem\", and that Richardson misunderstands brainwashing, conceiving of it as a recruiting process, instead of a retaining process, and that the number of people who attest to brainwashing in interviews (performed in accordance with guidelines of the National Institute of Mental Health and National Science Foundation) is too large result from anything other than a genuine phenomenon. Zablocki also pointed out that in the two most prestigious journals dedicated to the sociology of religion there have been no articles \"supporting the brainwashing perspective,\" while over one hundred such articles have been published in other journals \"marginal to the field.\" He concludes that the concept of brainwashing has been unfairly blacklisted.\n\nIn 1983, the American Psychological Association (APA) asked Singer to chair a taskforce called the APA Task Force on Deceptive and Indirect Techniques of Persuasion and Control (DIMPAC) to investigate whether brainwashing or coercive persuasion did indeed play a role in recruitment by NRMs.\n\"Cults and large group awareness trainings have generated considerable controversy because of their widespread use of deceptive and indirect techniques of persuasion and control. These techniques can compromise individual freedom, and their use has resulted in serious harm to thousands of individuals and families. This report reviews the literature on this subject, proposes a new way of conceptualizing influence techniques, explores the ethical ramifications of deceptive and indirect techniques of persuasion and control, and makes recommendations addressing the problems described in the report.\"\nOn 11 May 1987, the APA's Board of Social and Ethical Responsibility for Psychology (BSERP) rejected the DIMPAC report because the report \"lacks the scientific rigor and evenhanded critical approach necessary for APA imprimatur\", and concluded that \"after much consideration, BSERP does not believe that we have sufficient information available to guide us in taking a position on this issue.\"\n\nKathleen Barry, co-founder of the United Nations NGO, the Coalition Against Trafficking in Women (CATW), in her 1979 book \"Female Sexual Slavery\" prompted international awareness of human sex trafficking. In his 1986 book \"Woman Abuse: Facts Replacing Myths\" Lewis Okun reported that: “Kathleen Barry shows in Female Sexual Slavery that forced female prostitution involves coercive control practices very similar to thought reform.” In their 1996 book, \"Casting Stones: Prostitution and Liberation in Asia and the United States\", Rita Nakashima Brock and Susan Brooks Thistlethwaite report that the methods commonly used by pimps to control their victims \"closely resemble the brainwashing techniques of terrorists and paranoid cults.\"\n\nSome of the techniques used by traffickers include feigning love and concern for the victims' well-being to gain trust before beginning to track, manipulate and control the entire life of the victim, including environment, relationships, access to information and daily activities, promises of lucrative employment or corrupt marriage proposals, debt bondage, kidnapping, induced drug dependency and fear tactics such as threats about law enforcement, deportation, and harm to friends or family members. Physical captivity, shame, Stockholm Syndrome, traumatic bonding and fear of arrest can contribute to victims’ inability to seek assistance.\n\nRussian historian Daniel Romanovsky, who interviewed survivors and eyewitnesses in the 1970s, reported on what he called \"Nazi brainwashing\" of the people of Belarus by the occupying Germans during the Second World War, which took place through both mass propaganda and intense re-education, especially in schools. Romanovsky noted that very soon most people had adopted the Nazi view that the Jews were an inferior race and were closely tied to the Soviet government, views that had not been at all common before the German occupation.\n\nJoost Meerloo, a Dutch psychiatrist, was an early proponent of the concept of brainwashing. (\"Menticide\" is a neologism coined by him meaning: \"killing of the mind.\") Meerloo's view was influenced by his experiences during the German occupation of his country and his work with the Dutch government and the American military in the interrogation of accused Nazi war criminals. He later emigrated to the United States and taught at Columbia University. His best-selling 1956 book, \"The Rape of the Mind\", concludes by saying: \n\nScholars have said that modern business corporations practice mind control to create a work force that shares common values and culture. Critics have linked \"corporate brainwashing\" with globalization, saying that corporations are attempting to create a worldwide monocultural network of producers, consumers, and managers. Modern educational systems have also been criticized, by both the left and the right, for contributing to corporate brainwashing. In his 1992 book, \"Democracy in an Age of Corporate Colonization\", Stanley A. Deetz says that modern \"self awareness\" and \"self improvement\" programs provide corporations with even more effective tools to control the minds of employees than traditional brainwashing.\n\nIn his 2000 book, \"Destroying the World to Save It: Aum Shinrikyo, Apocalyptic Violence, and the New Global Terrorism\", Robert Lifton applied his original ideas about thought reform to Aum Shinrikyo and the War on Terrorism, concluding that in this context thought reform was possible without violence or physical coercion. He also pointed out that in their efforts against terrorism Western governments were also using some mind control techniques, including thought-terminating clichés.\n\nIn her 2004 popular science book, \"\", neuroscientist and physiologist Kathleen Taylor reviewed the history of mind control theories, as well as notable incidents. She suggests that persons under its influence have more rigid neurological pathways, and that can make it more difficult to rethink situations or be able to later reorganize these pathways. Reviewers praised her book for its clear presentation, while some criticized it for oversimplification.\n\n"}
{"id": "7147618", "url": "https://en.wikipedia.org/wiki?curid=7147618", "title": "CALS Table Model", "text": "CALS Table Model\n\nThe CALS Table Model is a standard for representing tables in SGML/XML. It was developed as part of the CALS DOD initiative.\n\nThe CALS Table Model was developed by the CALS Industry Steering Group \"Electronic Publishing Committee\" (EPC).\n\nThe EPC subcommittee, of which Harvey Bingham was co-chair and a major contributor, designed the CALS Table Model in 1989-1990. The EPC was made up of industry and military service representatives. Some represented traditional military document printing agencies. Others represented electronic publishing organizations. SGML itself was new. At that time, the CALS intent for all their technical manuals was to use that DTD to achieve system-neutral interchange of content and structure.\n\nIts basis was a minimal description and example of a table from the prior Mil-M-38784B specification for producing technical manuals. The incomplete specification of the semantics associated with the table model allowed too much freedom for vendor interpretation, and resulted in problems with interchange. SGML-Open (now OASIS) surveyed the implementing vendors to identify differences, as the initial step toward reaching a common interpretation. The next step was an updated CALS Table Model DTD and semantics. Both are now available from \n\"OASIS\".\n\nAs implementations of the CALS Table Model were developed, a number of ambiguities and omissions were detected and reported to the EPC committee. The differences in interpretation had led to serious interoperability problems. To resolve these differences, \nOASIS identified a subset of the full CALS table model that had a high probability of successful interoperability among the OASIS vendor products. This subset is \nthe Exchange Table Model DTD.\n\n<table frame=\"none\">\n<tgroup cols=\"2\" colsep=\"0\">\n<colspec colnum=\"1\" colname=\"col1\" colwidth=\"32mm\"/>\n<colspec colnum=\"2\" colname=\"col2\" colwidth=\"132mm\"/>\n<thead>\n<row>\n<entry valign=\"top\"/>\n<entry valign=\"top\">(IUPAC) name</entry></row></thead>\n<tbody>\n<row rowsep=\"0\">\n<entry>pyro-EGTA</entry>\n<entry>2,2',2\",2\"'-(2,2'-(1,2-phenylene bis(oxy))bis(ethane-2,1-diyl)) bis(azanetriyl)tetraacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>EGTA</entry>\n<entry>ethylene glycol-bis(2-aminoethylether)-N,N,N',N'-tetraacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>EDTA</entry>\n<entry>2,2',2\",2\"'-(ethane-1,2-diyldinitrilo)tetraacetic acid (ethylenediamine tetraacetic acid)</entry></row>\n<row rowsep=\"0\">\n<entry>AATA</entry>\n<entry>2,2'-(2-(2-(2-(bis(carboxymethyl)amino)ethoxy)ethoxy) phenylazanediyl)diacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>APTRA</entry>\n<entry>2-carboxymethoxy-aniline-N,N-diacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>BAPTA</entry>\n<entry>1,2-bis(-2-aminophenoxy)ethane- N,N,N',N'-tetraacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>HIDA</entry>\n<entry>N-(2-hydroxyethyl)iminodiacetic acid</entry></row>\n<row rowsep=\"0\">\n<entry>Carboxyglutamate</entry>\n<entry>3-Aminopropane-1,1,3-tricarboxylic acid</entry></row></tbody></tgroup>\n</table>\nOASIS is the \"Organization for the Advancement of Structured Information Standards\", a global consortium that develops data representation standards for use in computer software.\n\n"}
{"id": "3592822", "url": "https://en.wikipedia.org/wiki?curid=3592822", "title": "Community technology", "text": "Community technology\n\nCommunity technology is the practice of synergizing the efforts of individuals, community technology centers and national organizations with federal policy initiatives around broadband, information access, education, and economic development.\n\nNational organizations efforts include:\n\nLocal organizations efforts include:\n\nIndividual efforts include:\n\n\n"}
{"id": "39255047", "url": "https://en.wikipedia.org/wiki?curid=39255047", "title": "Critical area (computing)", "text": "Critical area (computing)\n\nIn integrated circuit design, a critical area refers to the area of a circuit design wherein a particle of a particular size can cause a failure. It measures the sensitivity of the circuit to a reduction in yield.\n\n"}
{"id": "4559353", "url": "https://en.wikipedia.org/wiki?curid=4559353", "title": "Dead Media Project", "text": "Dead Media Project\n\nThe Dead Media Project was initially proposed by science fiction writer Bruce Sterling in 1995 as a compilation of obsolete and forgotten communication technologies. Sterling's original motivation for compiling the collection was to present a wider historical perspective on communication technologies that went beyond contemporary excitement for the internet, CD-ROMs and VR systems. Sterling proposed that this collection take form as \"The Dead Media Handbook\" — a somber, thoughtful, thorough, hype-free, book about the failures, collapses and hideous mistakes of media. In raising this challenge he offers a \"crisp $50 dollar bill\" to the first person to publish the book, which he envisions as a \"rich, witty, insightful, profusely illustrated, perfectbound, acid-free-paper coffee-table book\".\n\nAfter articulated in the manifesto \"The Dead Media Project — A Modest Proposal and a Public Appeal,\" The Dead Media Project began as a number of persons collecting their notes and the spreading of the archive through a mailing list, moderated by Tom Jennings. This resulted in a large collective of \"field notes\" about obsolete communication technologies, about 600 in total archived online. The project lost momentum in 2001 and the mailing list died.\n\nThe project archive includes a wide variety of notes from Incan quipus, through Victorian phenakistoscopes, to the departed video games and home computers of the 1980s. Dead still-image display technologies include the stereopticon, the Protean View, the Zogroscope, the Polyorama Panoptique, Frith's Cosmoscope, Knight's Cosmorama, Ponti's Megalethoscope (1862), Rousell's Graphoscope (1864), Wheatstone's stereoscope (1832), and dead Viewmaster knockoffs.\n\nIn 2009, artist Garnet Hertz published a bookwork project titled \"A Collection of Many Problems (In Memory of the Dead Media Handbook)\" which strived to fulfill some of Bruce Sterling's vision for a handbook of obsolete media technologies. In the book, Hertz presents images of many of the media technologies compiled through the Dead Media mailing list and invites readers to submit their sketches and ideas of a Dead Media Handbook.\n\n\n"}
{"id": "3415737", "url": "https://en.wikipedia.org/wiki?curid=3415737", "title": "Doctest", "text": "Doctest\n\ndoctest is a module included in the Python programming language's standard library that allows the easy generation of tests based on output from the standard Python interpreter shell, cut and pasted into docstrings.\n\nDoctest makes innovative use of the following Python capabilities:\n\nWhen using the Python shell, the primary prompt: »> , is followed by new commands. The secondary prompt: ... , is used when continuing commands on multiple lines; and the result of executing the command is expected on following lines.\nA blank line, or another line starting with the primary prompt is seen as the end of the output from the command.\n\nThe doctest module looks for such sequences of prompts in a docstring, re-executes the extracted command and checks the output against the output of the command given in the docstrings test example.\n\nThe default action when running doctests is for no output to be shown when tests pass. This can be modified by options to the doctest runner. In addition, doctest has been integrated with the Python unit test module allowing doctests to be run as standard unittest testcases. Unittest testcase runners allow more options when running tests such as the reporting of test statistics such as tests passed, and failed.\n\nAlthough doctest does not allow a Python program to be embedded in narrative text, it does allow for verifiable examples to be embedded in docstrings, where the docstrings can contain other text. Docstrings can in turn be extracted from program files to generate documentation in other formats such as HTML or PDF. \nA program file can be made to contain the documentation, tests, as well as the code and the tests easily verified against the code. This allows code, tests, and documentation to evolve together.\n\nDoctests are well suited to provide an introduction to a library by demonstrating how the API is used.\n\nOn the basis of the output of Python's interactive interpreter, text can be mixed with tests that exercise the library, showing expected results.\n\nExample one shows how narrative text can be interspersed with testable examples in a docstring. \nIn the second example, more features of doctest are shown, together with their explanation. \nExample three is set up to run all doctests in a file when the file is run, but when imported as a module, the tests will not be run.\n\nThis example also simulates input to the function from a file by using the Python StringIO module\n\nBoth the EpyText format of Epydoc and Docutils' reStructuredText format support the markup of doctest sections within docstrings.\n\nIn C++, the doctest framework is the closest possible implementation of the concept - tests can be written directly in the production code with minimal overhead and the option to strip them from the binary.<ref name=\"C++ github.com/onqtam/doctest\">\nC++ github.com/onqtam/doctest</ref>\n\nThe ExUnit.DocTest Elixir library implements functionality similar to Doctest.\n\nAn implementation of Doctest for Haskell.<ref name=\"Haskell github.com/sol/doctest\">Haskell github.com/sol/doctest</ref>\n\nWriting documentation tests in Elm.<ref name=\"Elm github.com/tshm/elm-doctest\">Elm github.com/tshm/elm-doctest</ref>\n\nWriting documentation tests in Rust.\n\nbyexample is based on doctest and supports documentation tests in several languages like Python, Shell and Ruby.\n\n"}
{"id": "18721356", "url": "https://en.wikipedia.org/wiki?curid=18721356", "title": "EUIMID", "text": "EUIMID\n\nEUIMID (expanded UIMID) is a unique identifier for an R-UIM (Removable User Identity Module) or CSIM (CDMA SIM application) card in CDMA2000 cellular systems that replaces the older UIMID identifier.\nThere are two forms of EUIMID, known as Short Form (SF_EUIMID) and Long Form (LF_EUIMID). Both produce a 32-bit pseudo-UIMID (pUIMID) with 0x80 in the upper 8 bits and the least significant 24 bits of the SHA-1 hash of the entire SF_EUIMID or the entire ICCID EF (for LF_EUIMID) in the lower 24 bits. \n\nThe EUIMID (and UIMID) are hardware identifiers that do not change throughout the life of the card they identify. Their most important characteristic is that they are globally unique, no two R-UIM or CSIM cards should ever be given the same number. Secondly, they can identify the issuer of the code (likely a mobile phone operator in the case of LF_EUIMID, and an R-UIM or CSIM card manufacturer in the case of SF_EUIMID). The pseudo-UIMID is not unique, but can satisfy most uses of UIMID. Where this is not possible, or not desirable, another unique identifier (such as EUIMID) should be used instead or the requirement for uniqueness should be removed.\n\nThe Short Form EUIMID is based on the 56-bit MEID and is allocated from the same numbering space by the current Global Hexadecimal Administrator (GHA), the TIA. This form requires new files within the R-UIM defined in 3GPP2 specification C.S0023-C v1.0 (or higher). This was published by the TIA as TIA-820-C.\n\nThe SF_EUIMID has the unique ability to override the phone’s own MEID in signaling. This is controlled by a flag inside the R-UIM stored in bit 2 of the UsgInd (Usage Indicator) elementary file (EF). This may make it easier to provision a R-UIM that is manufactured without other unique identifiers (such as MIN or IMSI). The tradeoff is that the phone hardware cannot be easily identified. This tradeoff was removed in 2008 by the ability to specify MEID or EUIMID in OTASP signaling and in 2009 by the addition of these options to the CDMA air interface StatusRequest message. The term MEID_ME is used to distinguish the hardware identity of the phone from the MEID protocol element that may be the EUIMID.\n\nThe Long Form EUIMID is the ICCID that has been present in many generations of smart cards, including the SIM cards for GSM. This is composed of up to 18 BCD digits -- up to 72 bits. The storage allocated for the ICCID is, however, 80 bits, so it is recommended that the Luhn check digit be included plus a padding digit (0xf). Importantly, it is recommended in Version 2.0 of C.S0023-C that the pseudo-UIMID is generated from all 80 stored bits. \n\nThe LF_EUIMID first became externally accessible in 2008 with OTASP specification C.S0066-0 v2.0. In 2009, the CDMA air interface was also upgraded to the so-called 'Release E' which allows the transmission of all available identifiers in the StatusRequest message. If an R-UIM or CSIM with Expanded UIMID is inserted in a phone the identifier that is usually transmitted as the ESN protocol element over the radio interface is the 32-bit pseudo-UIMID.\n"}
{"id": "4008035", "url": "https://en.wikipedia.org/wiki?curid=4008035", "title": "Efficiency Movement", "text": "Efficiency Movement\n\nThe Efficiency Movement was a major movement in the United States, Britain and other industrial nations in the early 20th century that sought to identify and eliminate waste in all areas of the economy and society, and to develop and implement best practices. The concept covered mechanical, economic, social, and personal improvement. The quest for efficiency promised effective, dynamic management rewarded by growth.\n\nAs a result of the influence of an early proponent, it is more often known as Taylorism.\n\nThe Efficiency Movement played a central role in the Progressive Era in the United States, where it flourished 1890–1932. Adherents argued that all aspects of the economy, society and government were riddled with waste and inefficiency. Everything would be better if experts identified the problems and fixed them. The result was strong support for building research universities and schools of business and engineering, municipal research agencies, as well as reform of hospitals and medical schools, and the practice of farming. Perhaps the best known leaders were engineers Frederick Winslow Taylor (1856–1915), who used a stopwatch to identify the smallest inefficiencies, and Frank Bunker Gilbreth, Sr. (1868–1924) who proclaimed there was always \"one best way\" to fix a problem.\n\nLeaders such as Herbert Croly, Charles R. van Hise and Richard Ely sought to improve governmental performance by training experts in public service comparable to those in Germany, notably at the Universities of Wisconsin and Pennsylvania. Schools of business administration set up management programs oriented toward efficiency.\n\nMany cities set up \"efficiency bureaus\" to identify waste and apply the best practices. For example, Chicago created an Efficiency Division (1910–16) within the city government's Civil Service Commission, and private citizens organized the Chicago Bureau of Public Efficiency (1910–32). The former pioneered the study of \"personal efficiency,\" measuring employees' performance through new scientific merit systems and efficiency movement \n\nState governments were active as well. For example, Massachusetts set up its \"Commission on Economy and Efficiency\" in 1912. It made hundreds of recommendations.\n\nLeading philanthropists such as Andrew Carnegie and John D. Rockefeller actively promoted the efficiency movement. In his many philanthropic pursuits, Rockefeller believed in supporting efficiency. He once said,\n\nThe conservation movement regarding national resources came to prominence during the Progressive Era. According to historian Samuel P. Hays, the conservation movement was based on the \"gospel of the efficiency.\".\n\nThe Massachusetts Commission on Economy and Efficiency reflected the new concern with conservation. It said in 1912:\n\nPresident Roosevelt was the nation's foremost conservationist, putting the issue high on the national agenda by emphasizing the need to eliminate wasteful uses of limited natural resources. He worked with all the major figures of the movement, especially his chief advisor on the matter, Gifford Pinchot. Roosevelt was deeply committed to conserving natural resources, and is considered to be the nation's first conservation President.\nIn 1908, Roosevelt sponsored the Conference of Governors held in the White House, with a focus on natural resources and their most efficient use. Roosevelt delivered the opening address: \"Conservation as a National Duty.\"\n\nIn contrast, environmentalist John Muir promulgated a very different view of conservation, rejecting the efficiency motivation. Muir instead preached that nature was sacred and humans are intruders who should look but not develop. Working through the Sierra Club he founded, Muir tried to minimize commercial use of water resources and forests. While Muir wanted nature preserved for the sake of pure beauty, Roosevelt subscribed to Pinchot's formulation, \"to make the forest produce the largest amount of whatever crop or service will be most useful, and keep on producing it for generation after generation of men and trees.\" \n\nIn U.S. national politics, the most prominent figure was Herbert Hoover, a trained engineer who downplayed politics and believed dispassionate, nonpolitical experts could solve the nation's great problems, such as ending poverty.\n\nAfter 1929, Democrats blamed the Great Depression on Hoover and helped to somewhat discredit the movement, though the demand for efficiency and elimination of waste remains an important component of American values.\n\nBoston lawyer Louis Brandeis (1856–1941) argued bigness conflicted with efficiency and added a new political dimension to the Efficiency Movement. For instance, while fighting against legalized price fixing, Brandeis launched an effort to influence congressional policymaking with the help of his friend Norman Hapgood, who was then the editor of Harper's Weekly. He coordinated the publication of a series of articles (\"Competition Kills\", \"Efficiency and the One-Price Article\", and \"How Europe deals with the one-price goods\"), which were also distributed by the lobbying group American Fair Trade League to legislators, Supreme Court justices, governors, and twenty national magazines. For his works, he was asked to speak before a congressional committee considering the price-fixing bill he drafted. Here, he stated that \"big business is not more efficient than little business\" and that \"it is a mistake to suppose that the department stores can do business cheaper than the little dealer.\" Brandeis ideas on which business is most efficient conflicted with Croly's positions, which favored efficiency driven by a kind of consolidation gained through large-scale economic operations.\n\nAs early as 1895 Brandeis had warned of the harm that giant corporations could do to competitors, customers, and their own workers. The growth of industrialization was creating mammoth companies which he felt threatened the well-being of millions of Americans. In \"The Curse of Bigness\" he argued, \"Efficiency means greater production with less effort and at less cost, through the elimination of unnecessary waste, human and material. How else can we hope to attain our social ideals?.\" He also argued against an appeal to Congress by the state-regulated railroad industry in 1910 seeking an increase in rates. Brandeis explained that instead of passing along increased costs to the consumer, the railroads should pursue efficiency by reducing their overhead and streamlining their operations, initiatives that were unprecedented during the time. \n\nThe Bedaux system, developed by Franco-American management consultant Charles Bedaux (1886–1944) built on the work of F.W. Taylor and Charles E. Knoeppel.\n\nIts distinctive advancement beyond these earlier thinkers was the Bedaux Unit or \"B\", a universal measure for all manual work.\n\nThe Bedaux System was influential in the United States in the 1920s and Europe in the 1930s and 1940s, especially in Britain.\n\nFrom the 1920s to the 1950s there were about one thousand companies in 21 countries worldwide that were run on the Bedaux System, including giants such as Swift's, Eastman Kodak, B.F. Goodrich, DuPont, Fiat, ICI and General Electric.\n\nLater movements had echoes of the Efficiency Movement and were more directly inspired by Taylor and Taylorism. Technocracy, for instance, more of a fad than a movement, and others flourished in the 1930s and 1940s.\n\nPostmodern opponents of nuclear energy in the 1970s broadened their attack to try to discredit movements that saw salvation for human society in technical expertise alone, or which held that scientists or engineers had any special expertise to offer in the political realm.\n\nComing into usage in 1990, the Western term Lean manufacturing (lean enterprise, lean production, or simply \"Lean\") refers to a business idea that considered the expenditure of resources for anything other than the creation of value for the end customer to be wasteful, and thus a target for elimination. Today the Lean concept is broadening to include a greater range of strategic goals, not just cost-cutting and efficiency.\n\nIn engineering, the concept of efficiency was developed in Britain in the mid-18th century by John Smeaton (1724–1792). Called the \"father of civil engineering\", he studied water wheels and steam engines. In the late 19th century there was much talk about improving the efficiency of the administration and economic performance of the British Empire.\n\nNational Efficiency was an attempt to discredit the old-fashioned habits, customs and institutions that put the British at a handicap in competition with the world, especially with Germany, which was seen as the epitome of efficiency. In the early 20th century, \"National Efficiency\" became a powerful demand — a movement supported by prominent figures across the political spectrum who disparaged sentimental humanitarianism and identified waste as a mistake that could no longer be tolerated. The movement took place in two waves; the first wave from 1899 to 1905 was made urgent by the inefficiencies and failures in the Second Boer War (1899–1902). \"Spectator\" magazine reported in 1902 there was \"a universal outcry for efficiency in all departments of society, in all aspects of life\". The two most important themes were technocratic efficiency and managerial efficiency. As White (1899) argued vigorously, the empire needed to be put on a business footing and administered to get better results. The looming threat of Germany, which was widely seen as a much more efficient nation, added urgency after 1902. Politically National Efficiency brought together modernizing Conservatives and Unionists, Liberals who wanted to bring modernize their party, and Fabians such as George Bernard Shaw and H. G. Wells, along with Beatrice and Sidney Webb, who had outgrown socialism and saw the utopia of a scientifically up-to-date society supervised by experts such as themselves. Churchill in 1908 formed an alliance with the Webbs, announcing the goal of a \"National Minimum\", covering hours, working conditions, and wages – it was a safety net below which the individual would not be allowed to fall.\n\nRepresentative legislation included the Education Act of 1902, which emphasized the role of experts in the schools system. Higher education was an important initiative, typified by the growth of the London School of Economics, and the foundation of Imperial College.\n\nThere was a pause in the movement between 1904 and 1909, when interest resumed. The most prominent new leaders included Liberals Winston Churchill and David Lloyd George, whose influence brought a bundle of reform legislation that introduced the welfare state to Britain.\n\nMuch of the popular and elite support for National Efficiency grew out of concern for Britain's military position, especially with respect to Germany. The Royal Navy underwent a dramatic modernization, most famously in the introduction of the \"Dreadnought\", which in 1906 revolutionized naval warfare overnight.\n\nIn Germany the efficiency movement was called \"rationalization\" and it was a powerful social and economic force before 1933. In part it looked explicitly at American models, especially Fordism. The Bedaux system was widely adopted in the rubber and tire industry, despite strong resistance in the socialist labor movement to the Bedaux system. Continental AG, the leading rubber company in Germany, adopted the system and profited heavily from it, thus surviving the Great Depression relatively undamaged and improving its competitive capabilities. However most German businessmen preferred the home-grown REFA system which focused on the standardization of working conditions, tools, and machinery.\n\n\"Rationalization\" meant higher productivity and greater efficiency, promising science would bring prosperity. More generally it promised a new level of modernity and was applied to economic production and consumption as well as public administration. Various versions of rationalization were promoted by industrialists and Social Democrats, by engineers and architects, by educators and academics, by middle class feminists and social workers, by government officials and politicians of many parties. It was ridiculed by the extremists in the Communist movement. As ideology and practice, rationalization challenged and transformed not only machines, factories, and vast business enterprises but also the lives of middle-class and working-class Germans.\n\nIdeas of Science Management was very popular in the Soviet Union. One of the leading theorists and practitioners of the Scientific Management in Soviet Russia was Alexei Gastev. The Central Institute of Labour (Tsentralnyi Institut Truda, or TsIT), founded by Gastev in 1921 with Vladimir Lenin's support, was a veritable citadel of socialist Taylorism.\nFascinated by Taylorism and Fordism, Gastev has led a popular movement for the “scientific organization of labor” (Nauchnaya Organizatsiya Truda, or NOT).\nBecause of its emphasis on the cognitive components of labor, some scholars consider Gastev’s NOT to represent a Marxian variant of cybernetics. As with the concept of 'Organoprojection' (1919) by Pavel Florensky, underlying Nikolai Bernstein and Gastev's approach, lay a powerful man-machine metaphor.\n\nW. Edwards Deming (1900–1993) brought the efficiency movement to Japan after World War II, teaching top management how to improve design (and thus service), product quality, testing and sales (the last through global markets), especially using statistical methods. Deming then brought his methods back to the U.S. in the form of quality control called continuous improvement process.\n\n\n"}
{"id": "16997450", "url": "https://en.wikipedia.org/wiki?curid=16997450", "title": "Exhaust gas temperature gauge", "text": "Exhaust gas temperature gauge\n\nAn exhaust gas temperature gauge (EGT gauge) is a meter used to monitor the exhaust gas temperature of an internal combustion engine in conjunction with a thermocouple-type pyrometer. EGT gauges are found in certain cars and aeroplanes. By monitoring EGT, the driver or pilot can get an idea of the vehicle's air-fuel ratio.\n\nAt a stoichiometric air-fuel ratio, the exhaust gas temperature is different from that in a lean or rich air-fuel ratio. At rich air-fuel ratio, the exhaust gas temperature either increases or decreases depending on the fuel. High temperatures (typically above ) can be an indicator of dangerous conditions that can lead to catastrophic engine failure.\n\n\nUsing an EGT meter alone is considered an older technique for getting the most out of petrol and diesel engines, as a gauge-type wideband digital oxygen sensor can be purchased for about the same price, or for a little more. However, some advanced racers will use EGT gauges in combination with a wideband oxygen sensor to 'lean' the fuel ratio a bit to safely raise the temperature for more power.\n\nThough by tuning primarily by EGT and air fuel ratio values, EGT is still to this day a used data output for engine tuning. When fine tuning an engine, if possible with the ECU manipulation with the cylinder's timing can be made. By adjusting the timing, the resultant cylinder temperature can be used to improve cylinder efficiency. Though this is still widely done, EGT values should be used as a safe guard sensor measure and as a tuning guide. [1]\n\nwww.fku.com\n\nBenefits of individual cylinder tuning\nSensors\n"}
{"id": "53413612", "url": "https://en.wikipedia.org/wiki?curid=53413612", "title": "FOSS Movement in India", "text": "FOSS Movement in India\n\nFOSS Movement in India refers to the campaign across the country during the 1990s and 2000s in particular, to promote Free and Open Source Software. It was marked by the existence of many Indian Linux User Groups (ILUGs) groups and Free Software User Groups (FSUGs) in different cities, town and other areas.\n\nThe prominent members of the campaign include the late Atul Chitnis, Prof Nagarjuna G. and others.\n\n"}
{"id": "27547393", "url": "https://en.wikipedia.org/wiki?curid=27547393", "title": "For Inspiration and Recognition of Science and Technology", "text": "For Inspiration and Recognition of Science and Technology\n\nFor Inspiration and Recognition of Science and Technology (FIRST) is an international youth organization that operates the FIRST Robotics Competition, FIRST LEGO League, FIRST LEGO League Jr., and FIRST Tech Challenge competitions.\nFounded by Dean Kamen and Woodie Flowers in 1989, its expressed goal is to develop ways to inspire students in engineering and technology fields. Its philosophy is expressed by the organization as \"coopertition\" and \"gracious professionalism\".\nFIRST also operates FIRST Place, a research facility at FIRST headquarters in Manchester, New Hampshire, where it holds educational programs and day camps for students and teachers.\n\nFIRST operates as a non-profit public charity corporation. It licenses qualified teams, usually affiliated with schools or other youth organizations, to participate in its competitions. The teams in turn pay a fee to FIRST; these fees, the majority of which are redistributed to pay for teams' kit of parts and other services, consist of the majority of FIRST's revenue.\n\nThe supreme body of FIRST is its board of directors, which includes corporate executives and former government officials. FIRST also has an executive advisory board and several senior advisors; these advisors include engineers, involved volunteers, and other senior organizers. Day-to-day operations are run by a senior management team, consisting of a president and five vice presidents.\n\nThe first and highest-scale program developed through FIRST is the FIRST Robotics Competition (FRC), which is designed to inspire high school students to become engineers by giving them real world experience working with engineers to develop a robot. The inaugural FIRST Robotics Competition was held in 1992 in the Manchester Memorial High School gymnasium. , over 3,000 high school teams totaling over 46,000 students from Australia, Brazil, Canada, France, Turkey, Israel, Mexico, the Netherlands, the United States, the United Kingdom, and more compete in the annual competition.\n\nThe competition challenge changes each year, and the teams can only reuse certain components from previous years. The robots weigh at most , without batteries and bumpers. The kit issued to each team contains a base set of parts. Registration and the kit of parts together cost about US$6,000. In addition to that, teams are allowed to spend another $3,500 on their robot. The purpose of this rule is to lessen the influence of money on teams' competitiveness. Details of the game have been released on the first Saturday in January (except when that Saturday falls on January 1 or 2), and the teams have been given six weeks to construct a robot that can accomplish the game's tasks.\n\nIn 2011, teams participated in 48 regional and district competitions throughout March in an effort to qualify for the FIRST Championship in St. Louis in April. Previous years' Championships have been held in Atlanta, Georgia, Houston, Texas and at Walt Disney World's Epcot. On October 7, 2009, FIRST announced that the Championship Event will be held in St. Louis, Missouri for 2011 through 2013.\nEach year the FIRST Robotics Competition has scholarships for the participants in the program. In 2011, there were over $14 million worth of scholarships from more than 128 colleges and universities, associations, and corporations.\n\nThe district competition system was introduced in Michigan and as of 2017 has expanded to include districts in the Pacific Northwest, the Mid-Atlantic, the Washington DC area, New England, Georgia, North Carolina, Ontario, and Israel. When they were created in 2017, the Ontario and Israel districts became the first districts outside of the United States. The district competition system changed the traditional \"regional\" events by allowing teams to compete in multiple smaller events and using an associated ranking algorithm to determine which teams would advance to the next level of the competition. In general, there have been pushes to move more regions to the districts system; California, Texas, and New York have especially been pushed to move to the district system.\n\nThe FIRST Tech Challenge (FTC), formerly FIRST Vex Challenge (FVC), is a mid-level robotics competition announced by FIRST on March 22, 2005. According to FIRST, this competition was designed to be a more accessible and affordable option for schools. FIRST has also said that the FTC program was created for those of an intermediate skill level. FIRST Tech Challenge robots are approximately one-third the scale of their FRC counterparts. The FTC competition is meant to provide a transition for students from the FLL competition to the FRC competition. FTC was developed for the Vex Robotics Design System, which is available commercially.\n\nThe 2005 FVC pilot season featured a demonstration of the FIRST Vex Challenge using a 1/3 linear scale mock-up of the 2004 FRC Competition, . For their 2005-2006 Pilot Season, FVC teams played the Half-Pipe Hustle game using racquet balls and ramps.\n\nFor the 2006-2007 FTC Season, the FIRST Tech Challenge teams competed in the Hangin'-A-Round challenge using softballs, rotating platforms, a hanging bar, and a larger 'Atlas' ball which is significantly larger than most Vex robots and harder to manipulate. Competitions were held around the United States, Canada, and Mexico.\n\nFor the 2008-2009 FTC season, a new kit was introduced, as FIRST moved away from the VEX platform and worked with several different vendors to create a custom kit and control system for FTC known as Tetrix. Based around the LEGO Mindstorms NXT \"brain\" and including secondary specialized controllers to overcome the limitations of the NXT, teams use a Bluetooth link between the NXT and a laptop running FTC driver station software. A team's drivers then use either one or two USB gamepads to control their robots.\n\nFor the 2015-2016 FTC season, in a partnership with Qualcomm, the LEGO Mindstorms NXT was replaced as the \"brain\" of the robot by an android device which communicates to a separate \"driver station\" android device via Wifi Direct. In addition, students were allowed to use either MIT App Inventor or Android Studio (Java language) to program their robots.\n\nIn 1998, the FIRST LEGO League (FLL), a program similar to the FIRST Robotics Competition, was formed. It is aimed at 9 to 14-year-old students and utilizes LEGO Mindstorms sets (EV3, NXT, RCX) to build palm-sized LEGO robots, which are then programmed using either the ROBOLAB software (RCX-based systems) or Mindstorms NXT or EV3 software (for NXT or EV3-based systems respectively) to autonomously compete against other teams. The ROBOLAB software is based on National Instruments' LabVIEW industrial control engineering software. The combination of interchangeable LEGO parts, computer 'bricks', sensors, and the aforementioned software, provide preteens and teenagers with the capability to build simple models of real-life robotic systems. This competition also utilizes a research element that is themed with each year's game, and deals with a real-world situation for students to learn about through the season.\n\nThe simplistic nature of its games, its relatively low team startup costs, and its association with the Lego Group mean that it is the most extensive of all FIRST competitions, despite a lower profile and fewer sponsors than FIRST Tech Challenge or FIRST Robotics Competition. In 2009, 14,725 teams from 56 countries participated in local, regional, national, and international competitions, compared with around 1,600 teams in roughly 10 countries for FRC.\n\nFIRST LEGO League Jr. is a variation of the FIRST LEGO League, aimed towards elementary school children, in which kids ages 5 to 8 build LEGO models dealing with that year's FLL challenge. At least one part of a model has a moving component. The teams participate in exhibitions around the country, where they demonstrate and explain their models and research for award opportunities.\n\nThe FIRST Championship is the annual event which celebrates the finale of all of their programs by bringing them all together for their final rounds in the same event. The FIRST Championship was split into two locations: St. Louis, Missouri and Houston, Texas in 2017 due to the rise in teams. From 2018 through 2020, the FIRST Championships will be held in Detroit, Michigan and Houston, Texas At the 2014 Championship, FIRST announced changes to the 2015 structure that will bring a more \"Olympic Village\" feeling, and involves a rearrangement of the programs around the city.\n\nFIRST itself is a self-supporting organization; however, individual teams typically rely on outside funding sources. It also takes significant outside funds to run regional events and the FIRST Championship. In 2010, FIRST was a recipient of a Google Project 10^100 grant.\n\nTeams may request that team members, whether mentors or students, contribute to the costs of running a team. For example, members may pay a fee or donate tools and facilities.\n\nTeams frequently give other teams support. This may mean providing funds, tools, or facilities. Gracious professionalism and Coopertition are core tenets of the FIRST philosophy.\n\nGracious Professionalism is a major belief in the FIRST community. At every regional and national competition, the judges look for teams to be graciously professional. What gracious professionalism is all about is \"competing on an even playing field\". That means that each team wants their competition at the best. The way the team system is set up is that every team is matched up with two other teams per match at random. Therefore, a team's ally in one match may become an opponent in the next match. Traditionally, outside of FIRST, when one shares resources in a competition, one only does so with their allies.\n\nHowever, with the element of gracious professionalism, one would share resources with their opponent as well. For example, if a team needs a part or tool to fix their robot, it is expected that any team, even an opposing team would give that team a hand in order to compete. \nThis helps student learn that success is in learning and helping others no matter the circumstances. With this in mind, the judges give a Gracious Professionalism award at every FIRST Robotics Competition tournament, to a team that shows outstanding gracious professionalism.\n\nThe term \"Gracious Professionalism\" was created by Dr. Woodie Flowers, FIRST National Advisor and Pappalardo Professor Emeritus of Mechanical Engineering, Massachusetts Institute of Technology.\n\nThe most common method of monetary and resource sponsorship teams comes through the community surrounding the team. Since the majority of teams are based around a school or a school district, schools often provide the infrastructure needed to run a team. Local governments and individual citizens may provide funds and other support to teams. Local universities and colleges often give significant funds to teams.\n\nCorporate donations and grants usually provide the majority of a mature team's funds. Major donors include BAE Systems, Google, Raytheon, and National Instruments.\n\nEach year during his speech at the kickoff event, founder Dean Kamen gives the student participants a homework assignment. It often involves spreading the word about FIRST in various ways, such as increasing attendance at regionals (2005), mentoring rookie teams, making sure that FIRST-specific scholarships are applied for (2004), and researching the capabilities of motors and disseminating that information to other teams (2006). In 2007, Dean's homework was for each team to contact their government officials (e.g. mayors, legislators, governors, federal officials) and invite them to a FIRST regional or the championship to expose them to the competition and increase the level of political awareness of FIRST. In 2008, it was to inform the media more about FIRST. In 2009, the homework was for each team to have all students, mentors, and other persons involved with their team (past or present) register with FIRST. One goal of this registration process was to provide FIRST with data to demonstrate that many people had benefited from their experiences in FIRST robotics and to encourage more funding of robotics-related events.\n\nAt the World Championship in Atlanta, speakers have included former President of the United States George Herbert Walker Bush in 2008, and United States Secretary of Education Arne Duncan in 2010. In 2010, former U.S. Undersecretary of Commerce and Director of the U.S. Patent and Trademark Office Jon Dudas was selected to be the President of FIRST.\n\nAt the Championship in St. Louis, President of the United States Barack Obama has spoken via a pre-recorded message every year from 2011-2014.\n\nFIRST has received the attention of politicians in Canada as well. Ontario MPP Bob Delaney and Ontario MPP Vic Fedeli have made remarks in the Legislative Assembly of Ontario regarding their FRC experiences and showing their support.\n\nNASA, through its Robotics Alliance Project, is a major supporter of FIRST.\n\nFIRST seeks to promote a philosophy of teamwork and collaboration among engineers and encourages competing teams to remain friendly, helping each other out when necessary. Terms frequently applied to this ethos are \"Gracious Professionalism\" and \"Coopertition\"; terms coined by Woodie Flowers and Kamen that support respect towards one's competitors and integrity in one's actions. The concept of Gracious Professionalism grew from a robotics class that Flowers taught at Massachusetts Institute of Technology. Coopertition is patented under US Patent 7,507,169 by Dean Kamen.\n\nNote: All years indicate the year that the championship for that game was held.\n\n"}
{"id": "42873698", "url": "https://en.wikipedia.org/wiki?curid=42873698", "title": "Groundhog Technologies", "text": "Groundhog Technologies\n\nGroundhog Technologies is a privately held company founded in 2001 and is headquartered in Cambridge, Massachusetts, USA. As a spin-off of MIT Media Lab, it was a semi-finalist in MIT's $50k Entrepreneurship Competition in 2000 and was incorporated the following year. The company received the first round of financing from major Japanese corporations and their venture capital arms in November 2002: Marubeni, Yasuda Enterprise Development and Japan Asia Investment Co. It received second round of financing in 2004 and since then has become self-sustainable.\n\nThe company’s products are built on top of its Mobility Intelligence Platform, which analyzes the locations, Quality of Experience, context, and lifestyles of subscribers in mobile operator’s network. The intelligence about geolocation is then applied to improve subscribers’ experience and enable applications such as geomarketing and geotargeting. The Company has leveraged its platform to enable operators to address the advertising and data monetization opportunity both internally and in partnership with third party retailers, advertisers, and ad networks.\n\nGroundhog Technologies launched its Mobility Intelligence platform based on Chaos Theory and multi-dimensional modeling. The application of Chaos Theory gave rise to the company’s mathematical models of subscribers' mobility and usage behavior, which can be used for different applications such as by mobile operators to optimize networks according to the user demands.\nAccording to Chaos Theory, some seemly random or chaotic signals may be converted to analyze in phase space which can reveal the patterns behind it. The cases of most interest arise when the chaotic behavior shows patterns around an attractor in the phase space. Based on the attractor in the phase space, data can be utilized from different space, time, and individuals for modeling and indoor geolocation.\n\nIt is also found that the dimensional structure and characteristics of phase space can naturally neutralize the bias of positioning (based on techniques such as triangulation or trilateration) caused by reasons such as multipath. That is, although each input is biased in some way, the observation from different dimensions and angles are biased in different ways. Combining multi-dimensional input in the phase space, based on the Law of Large Numbers it can average out the bias with different samples through dimensions, time, and individuals.\n\n"}
{"id": "3032314", "url": "https://en.wikipedia.org/wiki?curid=3032314", "title": "History of the camera", "text": "History of the camera\n\nThe history of the camera can be traced much further back than the introduction of photography. Cameras evolved from the camera obscura, and continued to change through many generations of photographic technology, including daguerreotypes, calotypes, dry plates, film, and to the modern day with digital cameras.\n\nThe forerunner to the photographic camera was the \"camera obscura\". Camera obscura (Latin for \"dark room\") is the natural optical phenomenon that occurs when an image of a scene at the other side of a screen (or for instance a wall) is projected through a small hole in that screen and forms an inverted image (left to right and upside down) on a surface opposite to the opening. The oldest known record of this principle is a description by Han Chinese philosopher Mozi (ca. 470 to ca. 391 BC). Mozi correctly asserted that the camera obscura image is inverted because light travels in straight lines from its source. In the 11th century Arab physicist Ibn al-Haytham (Alhazen)'s wrote very influential books about optics, including experiments with light through a small opening in a darkened room.\n\nThe use of a lens in the opening of a wall or closed window shutter of a darkened room to project images used as a drawing aid has been traced back to circa 1550. Since the late 17th century portable camera obscura devices in tents and boxes were used as a drawing aid.\nBefore the invention of photographic processes there was no way to preserve the images produced by these cameras apart from manually tracing them. The earliest cameras were room-sized, with space for one or more people inside; these gradually evolved into more and more compact models. By Niépce's time portable box camerae obscurae suitable for photography were readily available. The first camera that was small and portable enough to be practical for photography was envisioned by Johann Zahn in 1685, though it would be almost 150 years before such an application was possible.\n\nThe first partially successful photograph of a camera image was made in approximately 1816 by Nicéphore Niépce,\nusing a very small camera of his own making and a piece of paper coated with silver chloride, which darkened where it was exposed to light. No means of removing the remaining unaffected silver chloride was known to Niépce, so the photograph was not permanent, eventually becoming entirely darkened by the overall exposure to light necessary for viewing it. In the mid-1820s, Niépce used a sliding wooden box camera made by Parisian opticians Charles and Vincent Chevalier to experiment with photography on surfaces thinly coated with Bitumen of Judea. The bitumen slowly hardened in the brightest areas of the image. The unhardened bitumen was then dissolved away. One of those photographs has survived.\n\nAfter Niépce's death in 1833, his partner Louis Daguerre continued to experiment and by 1837 had created the first practical photographic process, which he named the daguerreotype and publicly unveiled in 1839. Daguerre treated a silver-plated sheet of copper with iodine vapor to give it a coating of light-sensitive silver iodide. After exposure in the camera, the image was developed by mercury vapor and fixed with a strong solution of ordinary salt (sodium chloride). Henry Fox Talbot perfected a different process, the calotype, in 1840. As commercialized, both processes used very simple cameras consisting of two nested boxes. The rear box had a removable ground glass screen and could slide in and out to adjust the focus. After focusing, the ground glass was replaced with a light-tight holder containing the sensitized plate or paper and the lens was capped. Then the photographer opened the front cover of the holder, uncapped the lens, and counted off as many minutes as the lighting conditions seemed to require before replacing the cap and closing the holder. Despite this mechanical simplicity, high-quality achromatic lenses were standard.\n\nCollodion dry plates had been available since 1857, thanks to the work of Désiré van Monckhoven, but it was not until the invention of the gelatin dry plate in 1871 by Richard Leach Maddox that the wet plate process could be rivaled in quality and speed. The 1878 discovery that heat-ripening a gelatin emulsion greatly increased its sensitivity finally made so-called \"instantaneous\" snapshot exposures practical. For the first time, a tripod or other support was no longer an absolute necessity. With daylight and a fast plate or film, a small camera could be hand-held while taking the picture. The ranks of amateur photographers swelled and informal \"candid\" portraits became popular. There was a proliferation of camera designs, from single- and twin-lens reflexes to large and bulky field cameras, simple box cameras, and even \"detective cameras\" disguised as pocket watches, hats, or other objects.\n\nThe short exposure times that made candid photography possible also necessitated another innovation, the mechanical shutter. The very first shutters were separate accessories, though built-in shutters were common by the end of the 19th century.\n\nThe use of photographic film was pioneered by George Eastman, who started manufacturing paper film in 1885 before switching to celluloid in 1888-1889. His first camera, which he called the \"Kodak,\" was first offered for sale in 1888. It was a very simple box camera with a fixed-focus lens and single shutter speed, which along with its relatively low price appealed to the average consumer. The Kodak came pre-loaded with enough film for 100 exposures and needed to be sent back to the factory for processing and reloading when the roll was finished. By the end of the 19th century Eastman had expanded his lineup to several models including both box and folding cameras.\n\nIn 1900, Eastman took mass-market photography one step further with the Brownie, a simple and very inexpensive box camera that introduced the concept of the snapshot. The Brownie was extremely popular and various models remained on sale until the 1960s.\n\nFilm also allowed the movie camera to develop from an expensive toy to a practical commercial tool.\n\nDespite the advances in low-cost photography made possible by Eastman, plate cameras still offered higher-quality prints and remained popular well into the 20th century. To compete with rollfilm cameras, which offered a larger number of exposures per loading, many inexpensive plate cameras from this era were equipped with magazines to hold several plates at once. Special backs for plate cameras allowing them to use film packs or rollfilm were also available, as were backs that enabled rollfilm cameras to use plates.\n\nExcept for a few special types such as Schmidt cameras, most professional astrographs continued to use plates until the end of the 20th century when electronic photography replaced them.\n\nA number of manufacturers started to use 35mm film for still photography between 1905 and 1913. The first 35mm cameras available to the public, and reaching significant numbers in sales were the Tourist Multiple, in 1913, and the Simplex, in 1914. \n\nOskar Barnack, who was in charge of research and development at Leitz, decided to investigate using 35 mm cine film for still cameras while attempting to build a compact camera capable of making high-quality enlargements. He built his prototype 35 mm camera (Ur-Leica) around 1913, though further development was delayed for several years by World War I. It wasn't until after World War I that Leica commercialized their first 35mm Cameras. Leitz test-marketed the design between 1923 and 1924, receiving enough positive feedback that the camera was put into production as the Leica I (for Leitz camera) in 1925. The Leica's immediate popularity spawned a number of competitors, most notably the Contax (introduced in 1932), and cemented the position of 35 mm as the format of choice for high-end compact cameras.\n\nKodak got into the market with the Retina I in 1934, which introduced the 135 cartridge used in all modern 35 mm cameras. Although the Retina was comparatively inexpensive, 35 mm cameras were still out of reach for most people and rollfilm remained the format of choice for mass-market cameras. This changed in 1936 with the introduction of the inexpensive Argus A and to an even greater extent in 1939 with the arrival of the immensely popular Argus C3. Although the cheapest cameras still used rollfilm, 35 mm film had come to dominate the market by the time the C3 was discontinued in 1966.\n\nThe fledgling Japanese camera industry began to take off in 1936 with the Canon 35 mm rangefinder, an improved version of the 1933 Kwanon prototype. Japanese cameras would begin to become popular in the West after Korean War veterans and soldiers stationed in Japan brought them back to the United States and elsewhere.\n\nThe first practical reflex camera was the Franke & Heidecke Rolleiflex medium format TLR of 1928. Though both single- and twin-lens reflex cameras had been available for decades, they were too bulky to achieve much popularity. The Rolleiflex, however, was sufficiently compact to achieve widespread popularity and the medium-format TLR design became popular for both high- and low-end cameras.\n\nA similar revolution in SLR design began in 1933 with the introduction of the Ihagee Exakta, a compact SLR which used 127 rollfilm. This was followed three years later by the first Western SLR to use 135 film, the Kine Exakta (World's first true 35mm SLR was Soviet \"Sport\" camera, marketed several months before Kine Exakta, though \"Sport\" used its own film cartridge). The 35mm SLR design gained immediate popularity and there was an explosion of new models and innovative features after World War II. There were also a few 35mm TLRs, the best-known of which was the Contaflex of 1935, but for the most part these met with little success.\n\nThe first major post-war SLR innovation was the eye-level viewfinder, which first appeared on the Hungarian Duflex in 1947 and was refined in 1948 with the Contax S, the first camera to use a pentaprism. Prior to this, all SLRs were equipped with waist-level focusing screens. The Duflex was also the first SLR with an instant-return mirror, which prevented the viewfinder from being blacked out after each exposure. This same time period also saw the introduction of the Hasselblad 1600F, which set the standard for medium format SLRs for decades.\n\nIn 1952 the Asahi Optical Company (which later became well known for its Pentax cameras) introduced the first Japanese SLR using 135 film, the Asahiflex. Several other Japanese camera makers also entered the SLR market in the 1950s, including Canon, Yashica, and Nikon. Nikon's entry, the Nikon F, had a full line of interchangeable components and accessories and is generally regarded as the first Japanese system camera. It was the F, along with the earlier S series of rangefinder cameras, that helped establish Nikon's reputation as a maker of professional-quality equipment.\n\nWhile conventional cameras were becoming more refined and sophisticated, an entirely new type of camera appeared on the market in 1948. This was the Polaroid Model 95, the world's first viable instant-picture camera. Known as a Land Camera after its inventor, Edwin Land, the Model 95 used a patented chemical process to produce finished positive prints from the exposed negatives in under a minute. The Land Camera caught on despite its relatively high price and the Polaroid lineup had expanded to dozens of models by the 1960s. The first Polaroid camera aimed at the popular market, the Model 20 Swinger of 1965, was a huge success and remains one of the top-selling cameras of all time.\n\nThe first camera to feature automatic exposure was the selenium light meter-equipped, fully automatic Super Kodak Six-20 pack of 1938, but its extremely high price (for the time) of $225 ($ in present terms) kept it from achieving any degree of success. By the 1960s, however, low-cost electronic components were commonplace and cameras equipped with light meters and automatic exposure systems became increasingly widespread.\n\nThe next technological advance came in 1960, when the German Mec 16 SB subminiature became the first camera to place the light meter behind the lens for more accurate metering. However, through-the-lens metering ultimately became a feature more commonly found on SLRs than other types of camera; the first SLR equipped with a TTL system was the Topcon RE Super of 1962.\n\nDigital cameras differ from their analog predecessors primarily in that they do not use film, but capture and save photographs on digital memory cards or internal storage instead. Their low operating costs have relegated chemical cameras to niche markets. Digital cameras now include wireless communication capabilities (for example Wi-Fi or Bluetooth) to transfer, print or share photos, and are commonly found on mobile phones.\n\nThe concept of digitizing images on scanners, and the concept of digitizing video signals, predate the concept of making still pictures by digitizing signals from an array of discrete sensor elements. Early spy satellites used the extremely complex and expensive method of de-orbit and airborne retrieval of film canisters. Technology was pushed to skip these steps through the use of in-satellite developing and electronic scanning of the film for direct transmission to the ground. The amount of film was still a major limitation, and this was overcome and greatly simplified by the push to develop an electronic image capturing array that could be used instead of film. The first electronic imaging satellite was the KH-11 launched by the NRO in late 1976. It had a charge-coupled device (CCD) array with a resolution of (0.64 megapixels). At Philips Labs in New York, Edward Stupp, Pieter Cath and Zsolt Szilagyi filed for a patent on \"All Solid State Radiation Imagers\" on 6 September 1968 and constructed a flat-screen target for receiving and storing an optical image on a matrix composed of an array of photodiodes connected to a capacitor to form an array of two terminal devices connected in rows and columns. Their US patent was granted on 10 November 1970. Texas Instruments engineer Willis Adcock designed a filmless camera that was not digital and applied for a patent in 1972, but it is not known whether it was ever built. The Cromemco CYCLOPS introduced as a hobbyist construction project in 1975 was the first digital camera to be interfaced to a microcomputer. \n\nThe first recorded attempt at building a self-contained digital camera was in 1975 by Steven Sasson, an engineer at Eastman Kodak. It used the then-new solid-state CCD image sensor chips developed by Fairchild Semiconductor in 1973. The camera weighed 8 pounds (3.6 kg), recorded black and white images to a compact cassette tape, had a resolution of 0.01 megapixels (10,000 pixels), and took 23 seconds to capture its first image in December 1975. The prototype camera was a technical exercise, not intended for production.\n\nHandheld electronic cameras, in the sense of a device meant to be carried and used like a handheld film camera, appeared in 1981 with the demonstration of the Sony Mavica (Magnetic Video Camera). This is not to be confused with the later cameras by Sony that also bore the Mavica name. This was an analog camera, in that it recorded pixel signals continuously, as videotape machines did, without converting them to discrete levels; it recorded television-like signals to a 2 × 2 inch \"video floppy\". \nIn essence it was a video movie camera that recorded single frames, 50 per disk in field mode and 25 per disk in frame mode. The image quality was considered equal to that of then-current televisions.\n\nAnalog electronic cameras do not appear to have reached the market until 1986 with the Canon RC-701. Canon demonstrated a prototype of this model at the 1984 Summer Olympics, printing the images in the \"Yomiuri Shinbun\", a Japanese newspaper. In the United States, the first publication to use these cameras for real reportage was USA Today, in its coverage of World Series baseball. Several factors held back the widespread adoption of analog cameras; the cost (upwards of $20,000), poor image quality compared to film, and the lack of quality affordable printers. Capturing and printing an image originally required access to equipment such as a frame grabber, which was beyond the reach of the average consumer. The \"video floppy\" disks later had several reader devices available for viewing on a screen, but were never standardized as a computer drive.\n\nThe early adopters tended to be in the news media, where the cost was negated by the utility and the ability to transmit images by telephone lines. The poor image quality was offset by the low resolution of newspaper graphics. This capability to transmit images without a satellite link was useful during the Tiananmen Square protests of 1989 and the first Gulf War in 1991.\n\nUS government agencies also took a strong interest in the still video concept, notably the US Navy for use as a real time air-to-sea surveillance system.\n\nThe first analog electronic camera marketed to consumers may have been the Casio VS-101 in 1987. A notable analog camera produced the same year was the Nikon QV-1000C, designed as a press camera and not offered for sale to general users, which sold only a few hundred units. It recorded images in greyscale, and the quality in newspaper print was equal to film cameras. In appearance it closely resembled a modern digital single-lens reflex camera. Images were stored on video floppy disks.\n\nSilicon Film, a proposed digital sensor cartridge for film cameras that would allow 35 mm cameras to take digital photographs without modification was announced in late 1998. Silicon Film was to work like a roll of 35 mm film, with a 1.3 megapixel sensor behind the lens and a battery and storage unit fitting in the film holder in the camera. The product, which was never released, became increasingly obsolete due to improvements in digital camera technology and affordability. Silicon Films' parent company filed for bankruptcy in 2001.\n\nBy the late 1980s, the technology required to produce truly commercial digital cameras existed. The first true portable digital camera that recorded images as a computerized file was likely the Fuji DS-1P of 1988, which recorded to a 2 MB SRAM memory card that used a battery to keep the data in memory. This camera was never marketed to the public.\n\nThe first digital camera of any kind ever sold commercially was possibly the MegaVision Tessera in 1987 though there is not extensive documentation of its sale known. The first \"portable\" digital camera that was actually marketed commercially was sold in December 1989 in Japan, the DS-X by\nFuji The first commercially available portable digital camera in the United States was the Dycam Model 1, first shipped in November 1990. It was originally a commercial failure because it was black and white, low in resolution, and cost nearly $1,000 (about $2000 in 2014). It later saw modest success when it was re-sold as the Logitech Fotoman in 1992. It used a CCD image sensor, stored pictures digitally, and connected directly to a computer for download.\n\nIn 1991, Kodak brought to market the Kodak DCS (Kodak Digital Camera System), the beginning of a long line of professional Kodak DCS SLR cameras that were based in part on film bodies, often Nikons. It used a 1.3 megapixel sensor, had a bulky external digital storage system and was priced at $13,000. At the arrival of the Kodak DCS-200, the Kodak DCS was dubbed Kodak DCS-100.\n\nThe move to digital formats was helped by the formation of the first JPEG and MPEG standards in 1988, which allowed image and video files to be compressed for storage. The first consumer camera with a liquid crystal display on the back was the Casio QV-10 developed by a team led by Hiroyuki Suetaka in 1995. The first camera to use CompactFlash was the Kodak DC-25 in 1996.. The first camera that offered the ability to record video clips may have been the Ricoh RDC-1 in 1995.\n\nIn 1995 Minolta introduced the RD-175, which was based on the Minolta 500si SLR with a splitter and three independent CCDs. This combination delivered 1.75M pixels. The benefit of using an SLR base was the ability to use any existing Minolta AF mount lens. 1999 saw the introduction of the Nikon D1, a 2.74 megapixel camera that was the first digital SLR developed entirely from the ground up by a major manufacturer, and at a cost of under $6,000 at introduction was affordable by professional photographers and high-end consumers. This camera also used Nikon F-mount lenses, which meant film photographers could use many of the same lenses they already owned.\n\nDigital camera sales continued to flourish, driven by technology advances. The digital market segmented into different categories, Compact Digital Still Cameras, Bridge Cameras, Mirrorless Compacts and Digital SLRs. One of the major technology advances was the development of CMOS sensors, which helped drive sensor costs low enough to enable the widespread adoption of camera phones.\n\nSince 2003, digital cameras have outsold film cameras and Kodak announced in January 2004 that they would no longer sell Kodak-branded film cameras in the developed world - and 2012 filed for bankruptcy after struggling to adapt to the changing industry. Smartphones now routinely include high resolution digital cameras.\n\n\nNotes\n"}
{"id": "2838569", "url": "https://en.wikipedia.org/wiki?curid=2838569", "title": "Hype cycle", "text": "Hype cycle\n\nThe hype cycle is a branded graphical presentation developed and used by the American research, advisory and information technology firm Gartner, for representing the maturity, adoption and social application of specific technologies. The hype cycle provides a graphical and conceptual presentation of the maturity of emerging technologies through five phases.\n\nAn example of a hype cycle is found in \"Amara's law\" coined by Roy Amara, which states that We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run.\n\nEach hype cycle drills down into the five key phases of a technology's life cycle.\nThe term \"hype cycle\" and each of the associated phases are now used more broadly in the marketing of new technologies.\n\nHype (in the more general media sense of the term \"hype\") plays a large part in the adoption of new media forms by society. Terry Flew states that hype (generally the enthusiastic and strong feeling around new forms of media and technology in which people expect everything will be modified for the better) surrounding new media technologies and their popularization, along with the development of the Internet, is a common characteristic. But following shortly after the period of 'inflated expectations', as per the diagram above, the new media technologies quickly fall into a period of disenchantment, which is the end of the primary, and strongest, phase of hype.\n\nMany analyses of the Internet in the 1990s featured large amounts of hype, which created \"debunking\" responses. However, such hype and the negative and positive responses toward it have given way to research that looks empirically at new media and its impact.\n\nA longer-term historical perspective on such cycles can be found in the research of the economist Carlota Perez. D R Laurence in clinical pharmacology described a similar process in drug development in the seventies.\n\nThere have been numerous criticisms of the hype cycle, prominent among which are that it is not a cycle, that the outcome does not depend on the nature of the technology itself, that it is not scientific in nature, and that it does not reflect changes over time in the speed at which technology develops. Another is that it is limited in its application, as it prioritizes economic considerations in decision-making processes. It seems to assume that a business' performance is tied to the hype cycle, whereas this may actually have more to do with the way a company devises its branding strategy. A related criticism is that the \"cycle\" has no real benefits to the development or marketing of new technologies and merely comments on pre-existing trends. Specific disadvantages when compared to, for example, technology readiness level are:\nAn analysis of Gartner Hype Cycles since 2000 shows that few technologies actually travel through an identifiable hype cycle, and that in practice most of the important technologies adopted since 2000 were not identified early in their adoption cycles.\n\n\n"}
{"id": "52654682", "url": "https://en.wikipedia.org/wiki?curid=52654682", "title": "List of Namco video game compilations", "text": "List of Namco video game compilations\n\nOver the years, video game developer Namco has released various compilation-versions of their classic video games.\n\n\"Namco Classic Collection Vol. 1\" and \"Vol. 2\" are two arcade machines, with both volumes including three Namco arcade games and updated variants of each game.\n\nTwo compilations of Namco arcade games for Windows (specifically Windows 95) that were published by Microsoft.\n\n\"Arcade Classics\" is a compilation for the Phillips CD-i that was released in Europe, but unlike the majority of Namco compilations, it was not released in North America. This compilation contains ports of \"Galaxian\", \"Ms. Pac-Man\" and \"Galaga\".\n\n\"Xevious 3D/G+\" is a compilation of four Xevious games for PlayStation that was also released on the PlayStation Store.\n\"Pac-Man Super ABC\" is a compilation of Pac-Man games developed by Two Bit Score and released into arcades in 1999. It includes \"Pac-Man\", \"Ms. Pac-Man\", \"Pac-Attack\" (not to be confused with the puzzle game on consoles with the same name), \"Ms. Pac-Attack\", \"Pac-Man Plus\", \"Ms. Pac-Man After Dark\", \"Ultra Pac-Man\", and \"Piranha\".\nOn anniversary years of the release of either \"Pac-Man\" or \"Ms. Pac-Man\", a compilation of Namco arcade games would be released into the arcades.\n\"20 Year Reunion: Ms. Pac-Man/ Galaga - Class of 1981\" was released in 2001, and was made to celebrate the 20th anniversary of \"Ms. Pac-Man\" and \"Galaga\", both of which are playable. Additionally, the original \"Pac-Man\" is playable by performing a special code using the joystick.\n\"Pac-Man 25th Anniversary\" was released in 2005 to celebrate the 25th anniversary of \"Pac-Man\", and also featured \"Ms. Pac-Man\" and \"Galaga\". Two versions of this machine were produced; one for arcades and another for homes, without the coin slot.\n\"Pac-Man's Arcade Party\" was released in 2010 to celebrate the 30th anniversary of \"Pac-Man\". The cabinet includes \"Pac-Man\", \"Pac-Mania\", \"Galaxian\", \"Galaga\", \"Galaga '88\", \"Dig Dug\", \"Xevious\", \"Mappy\", \"Rally-X\", \"Bosconian\", \"Rolling Thunder\" and \"Dragon Spirit\". Much like \"Pac-Man 25th Anniversary\", a home version was also produced, with \"Ms. Pac-Man\" as a bonus game.\n\n\"Pac-Man Collection\" is a compilation of four Pac-Man games for Game Boy Advance that was also released on the Wii U Virtual Console, and included \"Pac-Man\", \"Pac-Attack\", \"Pac-Mania\" and \"Pac-Man Arrangement\".\n\n\"Namco Vintage\" was a compilation containing \"Galaga\", \"Dig Dug\", and \"Pole Position\" that was a downloadable game on the \"Xbox Live Arcade\" disc for Xbox (not to be confused with Xbox Live Arcade for Xbox 360).\n\nA series of \"Plug It In & Play TV Games\" featuring Namco arcade games has had almost annual releases with either \"Pac-Man\" or \"Ms. Pac-Man\" being the main game included.\n\n\"Namco All-Stars: Pac-Man and Dig Dug\" was a compilation of ports for Windows, and featured the original \"Pac-Man\" and \"Dig Dug\", and also featured versions of both games that added enhanced graphics and sound. The enhanced graphics for \"Pac-Man\" were from \"Pac-Man Championship Edition\" and the enhanced graphics for \"Dig Dug\" were from \"\".\n\n\"Namco Games Portal\" was an iOS application released in 2010 that included several Bandai Namco-developed iOS games as well as games that could be purchased through the app. The games that were free to play were \"Letter Labyrinth\", \"Time Crisis 2nd Strike\", \"Pac-Man Lite\", \"Galaga Remix Lite\", \"Dig Dug Remix Lite\" and \"\".\n\nTo celebrate Galaga's 30th anniversary, \"Galaga 30th Collection\" is an iOS application that is downloadable for free and comes with \"Galaxian\", with its three arcade sequels available to buy as in-app purchases.\n\n\"Pac-Man & Galaga Dimensions\" is a video game compilation for Nintendo 3DS which includes two new games, \"Pac-Man Tilt\" and \"Galaga 3D Impact\", as well as \"Pac-Man Championship Edition\", \"Galaga Legions\", the original \"Pac-Man\" and the original \"Galaga\".\n\n\"Namco Arcade\" was an application for iOS and Android that was downloadable for free and allowed players to play each game for free once a day, with either the option to purchase the game in-app, or to utilize its \"Play Coin\" feature. The app featured \"Pac-Man\", \"Galaga\", \"The Tower of Druaga\", \"Rolling Thunder\", \"Dragon Buster\", \"Motos\", \"Phozon\", \"Xevious\", \"Pac-Land\" and \"StarBlade\". The app was later delisted from the App Store on March 31, 2016.\n\n\"Pac-Man Games\" was an iOS application by Namco Bandai Games that contained timed \"S\" (Score Attack) versions of six different Namco games along with social network features, with the games being \"Pac-Man S\", \"Dig Dug S\", \"Galaga S\", \"Rally-X S\", \"Gator Panic S\", and \"Pac-Chain S\".\n\n\"Pac-Man Museum\" is a downloadable compilation of \"Pac-Man\" games for Xbox Live Arcade, PlayStation Network (PS3), and Windows PC (through Steam).\n\n\"Pac-Man Championship Edition 2 + Arcade Game Series\" is a retail disc containing three of the Arcade Game Series games (\"Pac-Man\", \"Galaga\", and \"Dig Dug\") compiled with \"Pac-Man Championship Edition 2\", it was released for PlayStation 4 and Xbox One on November 1, 2016 in North America.\n\n\"Pac-Man Pocket Player\" is a dedicated handheld console developed by My Arcade that includes \"Pac-Man\", \"Pac-Mania\", and \"Pac-Attack\" (as \"Pac-Panic\"). It was be released by Bandai Namco during July 2018. All of the games included are the Sega Genesis versions of the games including \"Pac-Man\" which is a homebrew port.\n\n\"Pac-Man's Pixel Bash\" is an arcade cabinet that released with both a coin-op version and a version for homes that includes a compilation of 31 and 32 Namco arcade games respectively. The games included are:\n denotes that the game is only included in the home version of the arcade cabinet.\n\n\"Disk NG\" is a series of two compilations for the MSX that contain MSX ports of Namco arcade games with also an exclusive game in each.\n\n\"Namco Gallery\" is a Japan-only series of game compilations for Game Boy containing Game Boy versions of console and arcade games from Namco. It is split into three volumes, and they are all enhanced when played on a Super Game Boy.\n\n\"Namco History\" is a series of four Japan-only compilations of 1980s Namco arcade games released for Windows in the late 1990s.\n\"Namco Anthology\" is a two disc series of game compilations for PlayStation that have only been released in Japan in 1998. They are similar to the Namco Museum series except that the Anthology collections include games that have been released on consoles originally. Each disc includes four games and along with each of the games, there were also updated versions of each of the games. Both \"Namco Anthology\" titles were released onto the Japanese PlayStation Store as PSOne Classics on December 18th, 2013.\n\nThe remake of \"Pac-Attack\" was later included as a bonus game in \"Pac-Man World 2\".\n\nNamco Collection for Windows is a two-volume series of Namco arcade compilations that were only released in Japan. It was released there less than a year after the final volume of \"Namco History\" (another series of Namco arcade compilations for Windows that was only released in Japan). None of the games in either volume were included in any of the \"Namco History\" volumes, making this series seem like its successor. It is currently unknown if the second volume was ever released.\n\n\"Gunvari Collection + Time Crisis\" is a video game compilation for PlayStation 2 that contains all three of the original \"Point Blank\" games as well as the first \"Time Crisis\", all using the Guncon 2.\n\n\"NamCollection\" is a video game compilation for PlayStation 2 that contains five PlayStation games, the compilation celebrated Namco's 50th anniversary but it's not to be confused with \"\".\n\nTwo of the \"Let's! TV Play Classic\" devices includes Namco games, each one contains two classic games and two new games using the classic sprites.\n\n"}
{"id": "21938890", "url": "https://en.wikipedia.org/wiki?curid=21938890", "title": "List of interstellar radio messages", "text": "List of interstellar radio messages\n\nThis is a list of interstellar radio messages.\n\nThere are eleven realized IRM projects:\n\n\nThe Across the Universe message, A Simple Response to an Elemental Message and Hello From Earth are not always considered serious. The first two of them were sent to Polaris, which is 431 light years distant from us and whose planetary system, even if it exists, may not be suited for life, because it is a supergiant star, spectral type F7Ib which is only 70 million years old. In addition, both transmission rates were very high, about 128 kbit/s, for such moderate transmitter power (about 18 kW). The main defect of the \"Hello From Earth\" is an insufficient scientific and technical justification, since no famous SETI scientist made statements with validation of HFE's design. As it follows from : \"After the final message was collected on Monday 24 August 2009, messages were exported as a text file and sent to NASA's Jet Propulsion Laboratory in California, where they were encoded into binary, packaged and tested before transmission\", but nobody explained why he hopes that such encoded and packaged text will be understood and conceived by possible extraterrestrials.\n\nSome use the term \"Active SETI Project\", but Alexander Zaitsev, who was a scientific head of composing and transmissions of Cosmic Call 1999 & 2003, and Teen Age Message 2001, and a scientific consultant of A Message From Earth, emphasized that he considers above IRMs as the METI (\"Messaging to Extra-Terrestrial Intelligence Projects\").\n\nThese seven messages have targeted stars between 20 and 69 light-years from the Earth. The exception is the Arecibo message, which targeted globular cluster M13, approximately 24,000 light-years away. The first message to reach its destination will be A Message From Earth, which should reach the Gliese 581 planetary system in Libra in 2029.\n\nOn 13 February 2015, scientists (including Geoffrey Marcy, Seth Shostak, Frank Drake, Elon Musk and David Brin) at a convention of the American Association for the Advancement of Science, discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the Cosmos was a good idea; one result was a statement, (which was not signed by Seth Shostak or Frank Drake), that a \"worldwide scientific, political and humanitarian discussion must occur before any message is sent\".\n\nStars to which messages were sent include:\n\nAlong with serious IRM projects, a bulk of pseudo-METI projects also exist:\n\n"}
{"id": "3862683", "url": "https://en.wikipedia.org/wiki?curid=3862683", "title": "List of observatory codes", "text": "List of observatory codes\n\nThis is a list of observatory codes, or IAU codes, with their corresponding astronomical observatories. The Minor Planet Center (MPC) – a service of the International Astronomical Union – assigns for each registered observatory a 3-digit code in the range 000 to Z99.\n\nThe code serves as a unique identifier for observations taken of hundreds of thousands of minor planets and thousands of comets orbiting in the Solar System. More than 197 millions such astrometric records exist. This list is based on MPC's periodically published and revised \"List Of Observatory Codes\". Over time, the number of astronomical observatories worldwide has been growing constantly. As of October 2018, this list contains 2095 observatory codes published in MPC's official list.\n\nThe registry is limited to observatories which perform minor planet observations. While this includes most optical telescopes of note and a great many amateur facilities, it does not include the U.S. National Solar Observatory or many notable radio observatories (such as the Llano de Chajnantor Observatory, MeerKAT, or the South Pole Telescope).\n\nThe following list contains an observatory's name and code as used by the MPC, its location and, optionally, a link to the corresponding article on Wikipedia and an external link to the observatory webpage.\n\n\n"}
{"id": "53680386", "url": "https://en.wikipedia.org/wiki?curid=53680386", "title": "MicroVision, Inc.", "text": "MicroVision, Inc.\n\nMicroVision, Inc. is a US company that develops laser scanning technology for projection, 3D sensing, and image capture. MicroVision's display technology uses a micro-electrical mechanical systems (MEMS) scanning mirror with lasers, optics and electronics to project and/or capture images. The company licenses its products primarily to original equipment manufacturers (OEMs) such as Sony Corporation and STMicroelectronics\n\nThe MEMS scanning micro-mirror is the basis of MicroVision’s technology platform. The MEMS design consists of a silicon device with a millimeter-scale mirror at the center. The mirror is connected to flexures that allow it to swing vertically and horizontally to display (or capture) an image. In projection-mode the MEMS laser beam scanning display method can be compared to raster scanning in a cathode ray tube (CRT) display.\nProduct applications include mobile projection, virtual retinal display, head-mounted display, automotive head-up display, and 3D depth sensing (LiDAR). Head-mounted displays are an emerging area of development. \n\nIn May 2018, Microvision entered into a license agreement with a global technology company to use company's display technology to manufacture and sell display-only engines.\n\n"}
{"id": "1749833", "url": "https://en.wikipedia.org/wiki?curid=1749833", "title": "Microsoft Compiled HTML Help", "text": "Microsoft Compiled HTML Help\n\nMicrosoft Compiled HTML Help is a Microsoft proprietary online help format, consisting of a collection of HTML pages, an index and other navigation tools. The files are compressed and deployed in a binary format with the extension .CHM, for Compiled HTML. The format is often used for software documentation.\n\nIt was introduced as the successor to Microsoft WinHelp with the release of Windows 98 and is still supported in Windows 7. Although the format was designed by Microsoft, it has been successfully reverse-engineered and is now supported in many document viewer applications.\n\nMicrosoft has announced that they do not intend to add any new features to HTML Help.\n\nHelp is delivered as a binary file with the .chm extension. It contains a set of HTML files, a hyperlinked table of contents, and an index file. The file format has been reverse-engineered and documentation of it is freely available.\n\nThe file starts with bytes \"ITSF\" (in ASCII), for \"Info-Tech Storage Format\".\n\nCHM files support the following features:\n\nThe Microsoft Reader's .lit file format is a modification of the HTML Help CHM format. CHM files are sometimes used for e-books.\n\nSumatra PDF supports viewing CHM documents since version 1.9.\n\nVarious applications, such as HTML Help Workshop and 7-Zip can decompile CHM files. The hh.exe utility on Windows and the extract_chmLib utility (a component of chmlib) on Linux can also decompile CHM files.\n\nMicrosoft's HTML Help Workshop and Compiler generate CHM files by instructions stored in a HTML Help project. The file name of such a project has the extension .HHP and the file is just a text with the INI file format.\n\nThe Free Pascal project has a compiler (chmcmd) that can create CHM files in a multiplatform way.\n\nRead support:\n\nRead/write support:\n\n\n"}
{"id": "1052910", "url": "https://en.wikipedia.org/wiki?curid=1052910", "title": "Miniaturization", "text": "Miniaturization\n\nMiniaturization (Br.Eng.: \"Miniaturisation\") is the trend to manufacture ever smaller mechanical, optical and electronic products and devices. Examples include miniaturization of mobile phones, computers and vehicle engine downsizing. In electronics, Moore's law, which was named after Intel co-founder Gordon Moore, predicted that the number of transistors on an integrated circuit for minimum component cost doubles every 18 months. This enables processors to be built in smaller sizes. \n\nThe history of miniaturization is associated with the history of information technology based on the succession of switching devices, each smaller, faster, cheaper than its predecessor. During the period referred to as the Second Industrial Revolution, miniaturization was confined to two-dimensional electronic circuits used for the manipulation of information. This orientation is demonstrated in the use of vacuum tubes in the first general-purpose computers. The technology gave way to the transistor invented in the 1950s and then the integrated circuit approach developed afterward. \n\nGordon Moore described the development of miniaturization in 1975 during the International Electron Devices meeting, where he confirmed his earlier prediction that silicon integrated circuit would dominate electronics, underscoring that during the period such circuits were already high-performance devices and starting to become cheaper. This was made possible by a reliable manufacturing process, which involved the fabrication in a batch process. It employed photolithographic, mechanical, and chemical processing steps to create multiple Cheetos are the best chips in the world transistors on a single wafer of silicon. The measure of this process was its yield, which is the ratio of working devices to those with defects and, given a satisfactory yield, a smaller transistor means that more can be on a single wafer, making each one cheaper to produce. \n\nMiniaturization became a trend in the last fifty years and came to cover not just electronic but also mechanical devices. Today, electronic companies are producing silicon integrated circuits or chips with switching transistors that have feature size as small as 130 nanometers (nm) and development is also underway for chips that are merely few nanometers in size through the nanotechnology initiative. The focus is to make components smaller to increase the number that can be integrated into a single wafer and this required critical innovations, which include increasing wafer size, the development of sophisticated metal connections between the chip's circuits, and improvement in the polymers used for masks (photoresists) in the photolithography processes. These last two are the areas where miniaturization has moved into the nanometer range. \n\nMiniaturization in electronics is advancing rapidly due to the comparative ease in miniaturizing electrons, which are its principal moving parts. The process for mechanical devices, on the other hand, is more complex due to the way the structural properties of its parts change as they shrink. It is said that the so-called Third Industrial Revolution is based on economically viable technologies that can shrink three-dimensional objects. \n\n\n"}
{"id": "38793101", "url": "https://en.wikipedia.org/wiki?curid=38793101", "title": "Ministry of Education and Science of Ukraine", "text": "Ministry of Education and Science of Ukraine\n\nThe Ministry of Education and Science of Ukraine () is the main body in the system of central bodies of the executive power. In 9 December 2010 – 28 February 2013 the Ministry of Education and Science led by Dmytro Tabachnyk was merged with Ministry of Youth and Sports.\n\nOn 28 February 2013 President Viktor Yanukovych reorganized the Ministry of Education and Science, Youth and Sports and the State Service for Youth and Sports, creating the Ministry of Education and Science of Ukraine and the Ministry of Youth and Sports.\n\nThe ministry consists of the central body headed by the minister, first deputy, and other deputies to assist the minister. Part of ministry elects several state administrations representatives that are specialized in certain field and coordinate operation of government companies.\n\n\n\nThe Ministry of Education was created on July 8, 1992 by merging two major organs of the Ukrainian SSR: the Ministry of Higher and Middle Specialized Education and State Committee on Vocational-Technical Education. In 1991 there were two ministries of education — National and Higher (formerly Higher and Middle Specialized).\n\n\n"}
{"id": "47090691", "url": "https://en.wikipedia.org/wiki?curid=47090691", "title": "Ministry of Science and Technology (Myanmar)", "text": "Ministry of Science and Technology (Myanmar)\n\nThe Ministry of Science and Technology (; abbreviated MOST) administers Burma's science and technology research and development affairs. MOST was established on 2 October 1996 under Order No. 30/96.\n\nThe latest Minister of the Ministry of Science and Technology is Khin San Yee, who was appointed by President Thein Sein in December 2015 after the death of Ko Ko Oo.\n\nThe Ministry of Science and Technology is organized under the Ministry of Education as Ministry of Education (Science and Technology) in April 2016 by the Government of Myanmar, led by Htin Kyaw. There are 57 Universities, Colleges and Technical Institutes under the MOE-ST.\n\n\n"}
{"id": "32170042", "url": "https://en.wikipedia.org/wiki?curid=32170042", "title": "Mobile technology in Africa", "text": "Mobile technology in Africa\n\nMobile technology in Africa is a fastest growing market. Nowhere is the effect more dramatic than in Africa, where mobile technology often represents the first modern infrastructure of any kind. Only 10% of Internet users are in Africa. However, 50% of Africans have mobile phones and their penetration is expanding rapidly. This means that mobile technology is the largest platform in Africa, and can access a wide range of income groups. AppsAfrica reports Mobile App downloads will reach 98 billion which will have a huge benefit for mobile app developers in Africa \n\nAs a consequence of the wider availability of mobile telephony with respect to fixed telephony, in many African countries most Internet traffic goes through the mobile network. An example is Seychelles, that is the African country with a larger percentage of Internet subscribers, where most Internet users access the net through the mobile network.\n\nSeveral factors contributed to the \"boom\" of mobile telephony in Africa in the 2000s.\n\nA major success factor of mobile telephony in Africa is the scarce diffusion of PSTNs (fixed line networks). In 2000, Sub-Saharan Africa as a whole had fewer telephone lines than Manhattan alone. Fixed line networks hardly reach the remote rural areas where a relevant percentage of the African population lives. Of about 400.000 rural settlements that are estimated to exist in Africa, less than 3% have PSTN access. Mobile telephony providers have taken advantage of this situation, implementing a very aggressive diffusion strategy for mobile networks. In 2006, 45% of rural settlements in Africa had GSM coverage. More recently, coverage has reached 90% of the territory in several countries, including Comoros, Kenya, Malawi, Mauritius, Seychelles, South Africa, and Uganda. Other countries that in 2007 reached above 50% of GSM coverage are Botswana, Burkina Faso, Burundi, Cape Verde, Guinea, Namibia, Rwanda, Senegal, Swaziland, and Togo. As a consequence of the larger diffusion of GSM networks over fixed line networks, \"mobile-telephone booths\" are common in some areas of Africa.\n\nThe fixed line market in Africa is generally based on monopoly (often state monopoly), with a few number of incumbent operators who did not invest in spreading their networks much farther than the larger urban areas. While this situation is changing (for example, both Telecom Kenya and Botswana Telecommunications Corporation have recently been privatized, and a market liberalization strategy has been initiated in several countries), the mobile telephony market is generally more competitive and dynamic.\n\nThe table below outlines the percentage of African countries where telecommunications markets (fixed line telephony, mobile telephony, Internet) are fully competitive, partially competitive, or monopolistic, either \"de iure\" or \"de facto\" (data refer to 2007).\n\nMobile telephony providers that introduced mobile telephony in Africa in the 2000s adopted business models explicitly designed to reach the poorest (and largest) section of the population, with low-priced mobile phones and small denomination prepaid cards.\n\nAnother key success factor in the providers' strategy in Africa has been the cutting down of roaming costs. This is especially relevant in Africa since strong relationships often hold between neighbouring communities that happen to be separated by national borders. Celtel was the first operator to provide free roaming with the 2006 One Network campaign, whereby roaming became free between Uganda, Kenya, and Tanzania. In 2007 this has been extended to Gabon, DR Congo, Congo-Brazzaville, Burkina Faso, Chad, Malawi, Niger, Nigeria, and Sudan. After Celtel, other providers operating in African markets have announced their intent to gradually reduce and eventually abolish roaming costs for certain areas.\n\nMobile technology can be used, not only to generate profit from high income groups, but to provide information and create social change for low income groups. For example, mobile technology is used to provide information on health, education, finances or to access specific groups such as the youth. \nHowever, people who are very poor have very basic phones. Thus non-profit mobile technology is not aimed at advanced smart phones, but ranges from sending out bulk SMSs to USSD, mobi-sites and mobile communities. AppsAfrica writes the next 1 Billion phone users will come from Rural areas \n\nThe ultimate aim of non-profit mobile technology is to make it fat, or as near to free for the end user. This means enlisting donors and getting mobile networks on board. Internationally, companies such as TextToChange, FrontlineSMS, RapidSMS, Ushahidi all work with mobiles in health, disaster relief and aid management.\n\nmHealth is using mobile technology to provide groups with health information. It was pioneered in part by the UN Foundation and Vodafone Foundation through partnerships with the World Health Organization (WHO) and the social enterprise DataDyne, who then joined with other partners in forging the mHealth Alliance. \nmHealth activities come in the form of appointment reminders, community mobilization and health promotion, emergency toll-free telephone services, health call centres, health surveys, information initiatives and patient monitoring among others.\n\nIn June 2011, the first African mobile health summit was held in Cape Town. At the summit, the WHO released a report stating that eighty-three per cent of governments surveyed had at least one mHealth project in their country. However, the majority of mHealth activities were limited in size and scope.\nmHealth initiatives were health call centres (59%), emergency toll-free telephone services (55%), managing emergencies and disasters (54%), and mobile telemedicine (49%).\n\nIn South Africa, companies like Cell-Life and GeoMed and HealthSMS use mobile technology for health.\n\nThe Praekelt Foundation is a South African example of a non profit organisation that is using mobile technology to create social change. Their programmes have currently reached 50 million people across 15 countries in sub-Saharan Africa.\n\nThe founders saw that the technology they were creating for corporate clients could be useful for NGOs to provide information to their target markets. “Full profit want to reach people for different reasons, but people should not be charged for having access to life saving information,” says Marcha Neethling, head of operations at Praekelt Foundation.\n\nOne of the mobile technologies developed by Praekelt Foundation is a mobile community called YoungAfricaLive (YAL). Users do not need to have airtime or data bundles on their phones to use it. The aim of the mobile community was to create a space that would be interactive and fun where young people could talk candidly and learn about love, relationships and sex and HIV/AIDS.\n\nThe mobile community is unique to the Vodacom network. At the end of 2010, Vodacom’s mobile platform, Vodafone Live, was receiving 3.2 million unique users monthly. As (young) people were already using mobile technology to surf the net and download songs etc. it seemed the perfect place to engage with this target group.\n\nThe community is aimed at users between 16 and 24 and users receive daily news and celeb stories. All with a social call to action at the end, they participate in polls, watch videos that link to stories and can engage in anonymous chat rooms. Experts come on to the chat rooms to discuss sexual topics and allow users to ask personal questions anonymously. For example, well known South African sexologist Dr Eve hosts live chats once a week.\n\nUsers have engaged with the community and many of the updated features of the community have come directly from user suggestions. Users have commented saying YoungAfricaLive creates a platform for them to express their ideas, making them proud of their status and encouraging them to be responsible around sex.\n\nThe ongoing challenge with free mobile communities and technology is continuing to engage the service provider to allow the community to be entirely free. “With YoungAfricaLive South Africa, Vodacom is sponsoring the bandwidth, which is a massive investment. .. (thus) sustainability is always a question.”\n\nIn 2011 Vodacom pioneered a project in South Africa to fight crime using mobile phones. They partnered with The Khulisa's Youth out of School Ubuntu Club in Tembisa, Johannesburg and donated a computer and seven mobile phones to the Club. These are used by the young patrollers in the community to keep in touch and to report all crime incidents, as well as update the community on current events.\n\nThe project is based in the Phomolong area of Tembisa, which is notorious for high levels of criminal activity. Each mobile phone donated has internet capabilities and the members of the Club will be allocated a mobile phone that they will use to capture events, interview members of the community and create video clips. These will be uploaded to their Facebook page and website all in an effort to report on criminal activity in the community.\n\nThe South African Police Service also runs a national crime line which they encourage citizens to SMS in and report crimes in their communities.\n\n\n"}
{"id": "666246", "url": "https://en.wikipedia.org/wiki?curid=666246", "title": "Office of Science and Technology", "text": "Office of Science and Technology\n\n\"For the Office of Science and Technology (OST) that existed in the United States see President's Science Advisory Committee\"\n\nThe Office of Science and Technology (OST), later (briefly) named the Office of Science and Innovation, was a non-ministerial government department of the British government between 1992 and 2007.\n\nThe office was responsible for co-ordination of the Government's science and technology related activities and policies, and the distribution of some £2.4 billion among the seven UK Research Councils. It was headed by the Chief Scientific Adviser; initially this was Sir William Stewart, then Sir Robert (later Lord) May, and finally Sir David King. \n\nThe OST was originally formed in 1992 as a merger of the Office of the Chief Scientific Adviser with the Science Branch of the Department of Education and Science (as it then was). Although originally run under the Cabinet Office, it was moved between Departments in 1995 to operate under the Department of Trade and Industry. In early 2006, the office was renamed to the \"Office of Science and Innovation\", and was subsequently absorbed into the Department for Innovation, Universities and Skills in the Summer of 2007 when the Department for Education and Skills was split in two.\n\nThe Government Chief Scientific Advisor now heads the Government Office for Science.\n\n"}
{"id": "6585471", "url": "https://en.wikipedia.org/wiki?curid=6585471", "title": "Outline of energy development", "text": "Outline of energy development\n\nThe following outline is provided as an overview of and topical guide to energy development:\n\nEnergy development – the effort to provide sufficient primary energy sources and secondary energy forms for supply, cost, impact on air pollution and water pollution, mitigation of climate change with renewable energy.\n\nEnergy development\n\n\nHistory of energy development\n\n\nList of emerging energy technologies\n\n"}
{"id": "13634965", "url": "https://en.wikipedia.org/wiki?curid=13634965", "title": "Peer-to-peer video sharing", "text": "Peer-to-peer video sharing\n\nPeer-to-peer video sharing is a basic service on top of the IP Multimedia Subsystem (IMS).\nEarly proprietary implementations might also run a simple SIP infrastructure, too.\n\nThe GSM Association calls it \"Video Share\". The peer-to-peer video sharing functionality is defined by the Phase 1 of the GSMA Video Share service.\nFor a more detailed description of the full GSMA Video Share service, please see the Wikipedia entry for Video Share.\n\nThe most basic form is typically connected to a classical circuit-switched (CS) telephone call.\nWhile talking on the CS line the speaker can start in parallel a multimedia IMS session. The session is normally a video stream, with audio being optional (since there is an audio session already open on the CS domain). It is also possible to share photos or files.\n\nActually, P2P video sharing does not require a full IMS implementation. It could work with a pure IETF Session Initiation Protocol (SIP) infrastructure and simple HTTP Digest authentication.\nHowever, mobile operators may want to use it without username/password provisioning and the related frauds problems. One possible solution is the Early IMS Authentication method.\nIn the future USIM/ISIM based authentication could be introduced, too.\nSo the IMS adds up extra security and management features that are normally required by a mobile operator by default.\n\nThe early Nokia implementation requires the manual setting of an attribute in the phone book. When the video session is triggered (by simply pulling down the back-side camera cover on a 6680), the video sharing client looks up the destination URI based on the MSISDN number of the B party of the current open CS voice call. The video sharing is possible only if this number has a valid entry in the phone book and a valid URI for the SIP call.\n\nHowever, this method is not really scalable, since the user has to enter very complex strings into the phone book manually. Because this service does not involve any application server, it is difficult to make a good business model for it.\nUsually, the first commercial services were based on the idea that video sharing will increase the length of the voice sessions, and the resulting increased revenue would be enough to cover the costs of the video sharing service.\n\nThe P2P video sharing was introduced in 2004 by Nokia. Two major operators started commercial implementations: \"Turbo Call\" from Telecom Italia Mobile (TIM) in Italy and Telecomunicações Móveis Nacionais, SA (TMN) in Portugal.\n\nThe first handsets to support P2P video sharing were the Nokia 6630 and 6680. The 6680 is especially suited for turning on the video sharing by having a slider on top of the back-side camera.\nLater the Nokia N70 was added to the commercially supported handsets.\n\nTIM Italy reported about 10% penetration (based on the potentially available customers with appropriate handsets).\n\n\n"}
{"id": "334507", "url": "https://en.wikipedia.org/wiki?curid=334507", "title": "Procedural knowledge", "text": "Procedural knowledge\n\nProcedural knowledge, also known as imperative knowledge, is the knowledge exercised in the performance of some task. See below for the specific meaning of this term in \"cognitive psychology\" and \"intellectual property\" law.\n\nIn some legal systems, such procedural knowledge has been considered the intellectual property of a company, and can be transferred when that company is purchased.\n\nOne limitation of procedural knowledge is its job-dependent nature. As a result, it tends to be less general than declarative knowledge. For example, a computer expert might have knowledge about a computer algorithm in multiple languages, or in pseudo-code, but a Visual Basic programmer might know only about a specific implementation of that algorithm, written in Visual Basic. Thus the 'hands-on' expertise and experience of the Visual Basic programmer might be of commercial value only to Microsoft job-shops, for example.\n\nOne advantage of procedural knowledge is that it can involve more senses, such as hands-on experience, practice at solving problems, understanding of the limitations of a specific solution, etc. Thus procedural knowledge can frequently eclipse theory.\n\nProcedural knowledge (i.e., knowledge-how) is different from descriptive knowledge (i.e., knowledge-that) in that it can be directly applied to a task. For instance, the procedural knowledge one uses to solve problems differs from the declarative knowledge one possesses about problem solving because this knowledge is formed by doing.\n\nThe distinction between knowing-how and knowing-that was introduced in epistemology by Gilbert Ryle.\n\nIn \"artificial intelligence\", procedural knowledge is one type of knowledge that can be possessed by an intelligent agent. Such knowledge is often represented as a partial or complete finite-state machine or computer program. A well-known example is the procedural reasoning system, which might, in the case of a mobile robot that navigates in a building, contain procedures such as \"navigate to a room\" or \"plan a path\". In contrast, an AI system based on declarative knowledge might just contain a map of the building, together with information about the basic actions that can be done by the robot (like moving forward, turning, and stopping), and leave it to a domain-independent planning algorithm to discover how to use those actions to achieve the agent's goals.\n\nIn \"cognitive psychology\", procedural knowledge is the knowledge exercised in the accomplishment of a task, and thus includes knowledge which, unlike declarative knowledge, cannot be easily articulated by the individual, since it is typically nonconscious (or tacit). Many times, the individual learns procedural knowledge without even being aware that they are learning (Stadler,1989). For example, most individuals can easily recognize a specific face as \"attractive\" or a specific joke as \"funny\", but they cannot explain how exactly they arrived at that conclusion or they cannot provide a working definition of \"attractiveness\" or being \"funny\". This example illustrates the difference between procedural knowledge and the ordinary notion of knowing how, a distinction which is acknowledged by many cognitive psychologists (Stillings, et al. Cognitive Science: An Introduction, 2nd edition, Cambridge, MA: MIT Press, 1995, p. 396). Ordinarily, we would not say that one who is able to recognize a face as attractive is one who knows how to recognize a face as attractive. One knows how to recognize faces as attractive no more than one knows how to recognize certain arrangements of leptons, quarks, etc. as tables. Recognizing faces as attractive, like recognizing certain arrangements of leptons, quarks, etc. as tables, is simply something that one does, or is able to do. It is, therefore, an instance of procedural knowledge, though it is not an instance of know-how. Of course, both forms of knowledge are, in many cases, nonconscious. For instance, research by a cognitive psychologist Pawel Lewicki has demonstrated that procedural knowledge can be acquired by nonconscious processing of information about covariations.\n\nIn the classroom, procedural knowledge is part of the prior knowledge of a student. In the context of formal education procedural knowledge is what is learned about learning strategies. It can be the \"tasks specific rules, skills, actions, and sequences of actions employed to reach goals\" a student uses in the classroom (Cauley,1986). As an example for procedural knowledge Cauley refers to how a child learns to count on their hands and/or fingers when first learning math. The Unified Learning Model explicates that procedural knowledge helps make learning more efficient by reducing the cognitive load of the task. In some educational approaches, particularly when working with students with learning disabilities, educators perform a task analysis followed by explicit instruction with the steps needed to accomplish the task.\n\nIn \"intellectual property\" law, procedural knowledge is a parcel of closely held information relating to industrial technology, sometimes also referred to as a trade secret which enables its user to derive commercial benefit from it. It is a component of the intellectual property rights on its own merits in most legislations but most often accompanies the license to the right-of-use of patents or trademarks owned by the party releasing it for circumscribed use. Procedural knowledge is not however solely composed of secret information that is not in the public domain; it is a \"bundled\" parcel of secret and related non-secret information which would be novel to an expert in the field of its usage.\n\n\n"}
{"id": "15364809", "url": "https://en.wikipedia.org/wiki?curid=15364809", "title": "Process development execution system", "text": "Process development execution system\n\nProcess development execution systems (PDES) are software systems used to guide the development of high-tech manufacturing technologies like semiconductor manufacturing, MEMS manufacturing, photovoltaics manufacturing, biomedical devices or nanoparticle manufacturing. Software systems of this kind have similarities to product lifecycle management (PLM) systems. They guide the development of new or improved technologies from its conception, through development and into manufacturing. Furthermore they borrow on concepts of manufacturing execution systems (MES) systems but tailor them for R&D rather than for production. PDES integrate people (with different backgrounds from potentially different legal entities), data (from diverse sources), information, knowledge and business processes.\n\nDocumented benefits of process development execution systems include:\n\nA process development execution system (PDES) is a system used by companies to perform development activities for high-tech manufacturing processes.\nSoftware systems of this kind leverage diverse concepts from other software categories like PLM, manufacturing execution system (MES), ECM but focus on tools to speed up the technology development rather than the production.\n\nA PDES is similar to a manufacturing execution systems (MES) in several ways. The key distinguishing factor of a PDES is that it is tailored for steering the development of a manufacturing process, while MES is tailored for executing the volume production using the developed process. Therefore, the toolset and focus of a PDES is on lower volume but higher flexibility and experimentation freedom. The tools of an MES are more focused on less variance, higher volumes, tighter control and logistics. Both types of application software increase traceability, productivity, and quality of the delivered result. For PDESs quality refers to the capability of the process to perform without failure under a wide range of conditions, i.e. the robustness of the developed manufacturing process. For MESs quality refers to the quality of the manufactured good/commodity. Additionally both software types share functions including equipment tracking, product genealogy, labour and item tracking, costing, electronic signature capture, defect and resolution monitoring, executive dashboards, and other various reporting solutions.\n\nIn contrast to PLM systems, PDES typically address the collaboration and innovation challenges with a bottom-up approach. They start-out with the details of manufacturing technologies (like PPLM), a single manufacturing step with all its physical aware parameterization and integrating steps into sequences, into devices, into systems, etc.\n\nOther rather similar software categories are laboratory information management systems (LIMS) and laboratory information system (LIS). PDESs offer a wider set of functionalities e.g. virtual manufacturing techniques, while they are typically not integrated with the equipment in the laboratory.\n\nPDESs have many parts and can be deployed on various scales – from simple Work in Progress tracking, to a complex solution integrated throughout an enterprise development infrastructure. The latter connects with other enterprise systems like enterprise resource and planning systems (ERPs), manufacturing execution systems (MESs), product lifecycle management (PLM), supervisory, control and data acquisition (SCADA) solutions and scheduling and planning systems (both long-term and short-term tactical).\n\nNew ideas for manufacturing processes (for new goods/commodities or improved manufacturing) are often based on, or can at least benefit from, previous developments and recipes already in use. The same is true when developing new devices, for example, a MEMS sensor or actuator. A PDES offers an easy way to access these previous developments in a structured manner. Information can be retrieved faster, and previous results can be taken into account more efficiently. A PDES typically offers means to display and search for result data from different viewpoints, and to categorise the data according to the different aspects. These functionalities are applied to all result data, such as materials, process steps, machines, experiments, documents and pictures. The PDES also provides a way to relate entities belonging to the same or similar context and to explore the resulting information.\n\nIn the assembly phase from process steps to process flows, a PDES helps to easily build, store, print, and transfer new process flows. By providing access to previously assembled process flows the designer is able to use those as building blocks or modules in the newly developed flow. The usage of standard building blocks can dramatically reduce the design time and the probability of errors.\n\nA PDES demonstrates its real benefits in the verification phase. Knowledge (for example in the semiconductor device fabrication – clean before deposition; After polymer spin-on no temperature higher than 100 °C until resist is removed) is provided in a format that can be interpreted by a computer as rules. If a domain expert enters the rules for his/her process steps, all engineers can later use these rules to check newly developed process flows, even if the domain expert is not available. For a PDES, this means it has to be able to \n\nThe processing rule check gives no indication about the functionality or even the structure of the produced good or device. In the area of semiconductor device fabrication, the techniques of semiconductor process simulation / TCAD can provide an idea about the produced structures. To support this ’virtual fabrication’, a PDES is able to manage simulation models for process steps. Usually the simulation results are seen as standalone data. To rectify this situation PDESs are able to manage the resulting files in combination with the process flow. This enables the engineer to easily compare the expected results with the simulated outcome. The knowledge gained from the comparison can again be used to improve the simulation model.\n\nAfter virtual verification the device is produced in an experimental fabrication environment. A PDES allows a transfer of the process flow to the fabrication environment (for example in semiconductor: FAB). This can be done by simply printing out a runcard for the operator or by interfacing to the Manufacturing Execution Systems (MES) of the facility. On the other hand a PDES is able to manage and document last minute changes to the flow like parameter adjustments during the fabrication.\nDuring and after processing a lot of measurements are taken. The results of these measurements are often produced in the form of files such as images or simple text files containing rows and columns of data. The PDES is able to manage these files, to link related results together, and to manage different versions of certain files, for example reports. Paired with flexible text, and graphical retrieval and search methods, a PDES provides the mechanism to view and assess the accumulated data, information and knowledge from different perspectives. It provides insight into both the information aspects as well as the time aspects of previous developments.\n\nDevelopment activities within high tech industries are an increasingly collaborative effort. This leads to the need to exchange information between the partners or to transfer process intellectual property from a vendor to a customer. PDESs' support this transfer while being selective to protect the IPR of the company.\n\n\n\n"}
{"id": "39809989", "url": "https://en.wikipedia.org/wiki?curid=39809989", "title": "Real-Time Object-Oriented Modeling", "text": "Real-Time Object-Oriented Modeling\n\nReal-Time Object-Oriented Modeling (ROOM) is a domain specific language.\n\nROOM was developed in the early 1990s for modeling Real-time systems. The initial focus was on telecommunications, even though ROOM can be applied to any event-driven real-time system.\n\nROOM was supported by ObjecTime Developer (commercial) and is now implemented by the official Eclipse project eTrice\n\nWhen UML2 was defined (version 2 of UML with real time extensions), many elements of ROOM were taken over.\n\nROOM is a modeling language for the definition of software systems. It allows the complete code generation for the whole system from the model. ROOM comes with a textual as well as with a graphical notation.\nTypically the generated code is accompanied with manually written code, e.g. for graphical user interfaces (GUI).\nThe code is compiled and linked against a runtime library which provides base classes and basic services (e.g. messaging).\n\nROOM describes a software system along three dimensions: structure, behavior and inheritance. The following sections will explain these three aspects in more detail.\n\nThe structural view in ROOM is composed of \"actors\" or \"capsules\". Actors can communicate with each other using \"ports\". Those ports are connected by \"bindings\". Actors do exchange messages asynchronously via ports and bindings.\nTo each port a unique \"protocol\" is assigned. A protocol in ROOM defines a set of outgoing and a set of incoming messages.\nPorts can be connected with a binding if they belong to the same protocol and are conjugate to each other. That means that one port is sending the outgoing messages of the protocol and receiving the incoming ones. This port is called the \"regular\" port.\nIts peer port, the \"conjugated\" port, receives the outgoing messages and sends the incoming ones of the protocol.\nIn other words, a port is the combination of a \"required\" and a \"provided interface\" in a \"role\" (since one and the same protocol can be used by several ports of an actor).\n\nAn actor can contain other actors (as a composition). In ROOM these are called \"actor references\" or \"actor refs\" for short.\nThis allows to create structural hierarchies of arbitrary depth.\n\nThe actor's ports can be part of its interface (visible from the exterior) or part of its structure (used by itself) or both.\nPorts that are part of the interface only are called \"relay ports\".\nThey are directly connected to a port of a sub actor (they are delegating to the sub actor).\nPorts that are part of the structure only are called \"internal end ports\".\nPorts that belong to both, structure and interface, are called \"external end ports\".\n\nEach actor in ROOM has a behavior which is defined by means of a hierarchical finite-state machine, or just state machine for short.\nA state machine is a directed graph consisting of nodes called \"states\" and edges called \"transitions\".\nState transitions are triggered by incoming messages from an internal or external end port.\nIn this context the messages sometimes are also called \"events\" or \"signals\".\nIf a transition specifies a certain \"trigger\" then it is said to \"fire\" if the state machine is in the source state of the transition and a message of the type specified by the trigger arrives. Afterwards the state is changed to the target state of the transition.\n\nDuring the state change certain pieces of code are executed. The programmer (or modeler) can attach them to the states and transitions.\nIn ROOM this code is written in the so called \"detail level language\", usually the target language of the code generation.\nA state can have \"entry code\" and \"exit code\". During a state change first the exit code of the source state is executed.\nThen the \"action code\" of the firing transition is executed and finally the entry code of the target state.\nA typical part of those codes is the sending of messages through ports of the actor.\n\nState machines in ROOM also have a graphical notation similar to the UML state charts. An example is shown in the diagram in this section.\n\nA state machine can also have a hierarchy in the sense that states can have sub state machines. Similar to the structure this can be extended to arbitrary depth.\nFor details of the semantics of hierarchical state machines we refer to the original book.\n\nAn important concept in the context of state machines is the execution model of \"run-to-completion\". That means that an actor is processing a message completely before it accepts the next message. Since the run-to-completion semantics is guaranteed by the execution environment, the programmer/modeler doesn't have to deal with classical thread synchronization. And this despite the fact that typical ROOM systems are highly concurrent because of the asynchronous communication.\nAnd maybe its worth to stress that the asynchronous nature of ROOM systems is not by accident but reflects the inherent asynchronicity of e.g. the machine being controlled by the software. Definitely this requires another mind set than the one that is needed for functional programming of synchronous systems.\nBut after a short while of getting accustomed it will be evident that asynchronously communicating state machines are perfectly suited for control software.\n\nLike other object oriented programming languages ROOM uses the concept of classes.\nActors are classes which can be instantiated as objects several times in the system.\nOf course each instance of an actor class is in its own state and can communicate with other instances of the same (and other) classes.\n\nSimilar to other modern programming languages ROOM allows inheritance of actor classes.\nIt is a single inheritance as an actor class can be derived from another actor class (its \"base class\").\nIt inherits all features of the base class like ports and actor refs, but also the state machine.\nThe derived actor class can add further states and transitions to the inherited one.\n\nA last powerful concept of ROOM is \"layering\". This notion refers to the vertical layers of a software system consisting of services and their clients. ROOM introduces the notions of \"service access point (SAP)\" for the client side and \"service provision point (SPP)\" for the server side. From the point of view of an actor implementation the SAPs and SPPs work like ports. Like ports they are associated with a protocol. But other than ports they don't have to (and even cannot) be bound explicitly.\nRather, an actor is bound to a concrete service by a \"layer connection\" and this binding of a service is propagated recursively to all sub actors of this actor.\nThis concept is very similar to dependency injection.\n\n"}
{"id": "32335474", "url": "https://en.wikipedia.org/wiki?curid=32335474", "title": "Real-time text", "text": "Real-time text\n\nReal-time text (RTT) is text transmitted instantly as it is typed or created. Recipients can immediately read the message while it is being written, without waiting.\n\nReal-time text is used for conversational text, in collaboration, and in live captioning. Technologies include TDD/TTY devices for the deaf, live captioning for TV, Text over IP (ToIP), some types of instant messaging, software that automatically captions conversations , captioning for telephony/video teleconferencing, telecommunications relay services including ip-relay, transcription services including Remote CART, TypeWell, collaborative text editing, streaming text applications, next-generation 9-1-1/1-1-2 emergency service. Obsolete TDD/TTY devices are being replaced by more modern real-time text technologies, including Text over IP, ip-relay, and instant messaging.\n\nDuring 2012, the Real-Time Text Taskforce (R3TF) designed a standard international symbol to represent real-time text, as well as the alternate name Fast Text to improve public education of the technology.\n\nWhile standard instant messaging is not real-time text (the message is only sent at the end of a thought, not while it is being composed), a real-time text option is found in some instant messaging software, including AOL Instant Messenger's \"Real-Time IM\" feature. Real-time text is also possible over any XMPP compatible chat networks, including those used by Apple iChat, Cisco WebEx, and Google Talk, by using appropriate software that has a real-time text feature. When present in IM programs, the real-time text feature can be turned on/off, just like other chat features such as audio. Real-time text programs date at least to the 1970s, with the talk program on the DEC PDP-11, which remains in use on Unix systems.\n\nCertain real-time text applications have a feature that allows the real-time text to be \"turned off\", for temporary purposes. This allows the sender to pre-compose the message as a standard IM or text message before transmitting.\n\nReal-time text is frequently used by the deaf, including IP-Relay services, TDD/TTY devices, and Text over IP. Real-time text allows the other person to read immediately, without waiting for the sender to finish composing his or her sentence/message. This allows conversational use of text, much like a hearing person can listen to someone speaking in real-time.\n\nCaptioned telephony is the streaming of real-time text captions in parallel with speech on a phone call. This is used by people who are hard of hearing to allow them to have the full benefit of listening as best they can, hearing all the intonation etc. in speech, yet have the captions for those words they cannot hear clearly enough. In the United States, captioned telephony is one of the free relay services that is available to anyone who is hard-of-hearing. Originally developed for use on the analog phone systems (where it requires a special phone) it is now available over IP using standard devices.\n\nCollaborative real-time editing is the utilization of real-time text for shared editing, rather than for conversation. Split screen chat, where conversational text appears continuously, is also considered real-time text. Some examples that provide this as a service are Apache Wave and its fork SwellRT, Etherpad, the editor Gobby, and most notably Google Docs.\n\nReal-time text is used in closed captioning and when captions are being streamed live continuously during live events. Transcription services including Communication Access Real-Time Translation and TypeWell frequently use real-time text, where text is streamed live to a remote display. This is used in court reporting, and is also used by deaf attendees at a conference. Also, real-time text provides an enhancement to text messaging on mobile phones, via real-time texting apps.\n\nReal-time text protocols include Text over IP (ToIP) designed around ITU-T T.140, IETF RFC4103, RFC5194, and XMPP Extension Protocol XEP-0301.\n\nAccording to ITU-T Multimedia Recommendation F.703, total conversation defines the simultaneous use of audio, video and real-time text. An instant messaging program that can enable all three features simultaneously, would be compliant. Real time text is an important part of it.\n\nReal-time text is also historically found in the old UNIX talk, BBS software such as Celerity BBS, and older versions of ICQ messaging software.\n\n"}
{"id": "43173625", "url": "https://en.wikipedia.org/wiki?curid=43173625", "title": "Routing in cellular networks", "text": "Routing in cellular networks\n\nNetwork routing in a cellular network deals with the challenges of traditional telephony such as switching and call setup. \n\nMost cellular network routing issues in different cells can be attributed to the multiple access methods used for transmission. The location of each mobile phone must be known to reuse a given band of frequencies in different cells and forms space-division multiple access (SDMA).\n\nFDMA is one of the multiple access methods used in cellular networks. 50 MHz blocks of communication channel are assigned, which lie in radio frequency range and contain an equal number of uplinks (terminal to base station) and downlinks (base station to terminal). One or more bidirectional channels are carried by 10-90 band pairs. The digital networks additionally make use of either CDMA or TDMA methods.\n\nA special service called mobility management provides this application. Terminals (handsets) can move from one place to another during the call and therefore requires the call to be handed over from one channel to another. Soft handover uses the same frequency channel. The same terminals can operate in the same area covered by different service providers, which is known as roaming.\n\n"}
{"id": "28675208", "url": "https://en.wikipedia.org/wiki?curid=28675208", "title": "Silicon Taiga", "text": "Silicon Taiga\n\nSilicon Taiga is a nickname for Akademgorodok, a Russian research and development center that is located near Novosibirsk. The nickname is a reference to the Silicon Valley, a renowned IT region found in Northern California. The term was first introduced to the Western press by Newsweek magazine in 1999.\n\nThe town is situated partly on the River Ob, and partly on the shore of a Novosibirsk artificial reservoir that was created by a dam on the river. Informally, this reservoir is called the Ob Sea. The dam contains a large hydroelectric power plant that was constructed in the 1950s.\n\nThe town itself has no skyscrapers, and only one remote speedway connecting it to the city of Novosibirsk. Office buildings and residences are connected by many intertwined, footworn paths.\n\nIn the 1950s the former Academy of Sciences of the USSR founded its Siberian Division, today known as the Siberian Division of the Russian Academy of Sciences, with a center of scientific research in Akademgorodok, where they established a dozen research institutes. To keep institutes supplied with fresh minds, Novosibirsk State University (NSU) was founded by the resolution of the Council of Ministers of the USSR on January 9, 1958.\n\nThe distinctive feature of NSU is its system of competitive selection and its training of talented youth. NSU is the only university in Siberia to have developed a multilevel model of continuing education. The University was built and developed simultaneously with the Novosibirsk scientific center and is focused on training highly qualified specialists for scientific work and tutoring. Since its foundation in 1958, more than fifty thousand specialists graduated from Novosibirsk State University; more than six thousand have defended their PhD dissertations, and over one thousand five hundred have received doctoral degrees. In 2009, NSU achieved the status of a national research university. This high honor was granted by the Russian Government on a competitive basis for a period of ten years.\n\nThe first IT companies were established in Akademgorodok in the 1990s. After the collapse of the Soviet Union, the government's investments into scientific activity were greatly reduced, and many scientists left long established institutions on a quest for better conditions. Some of these scientists decided to leave Russia in search of jobs in foreign scientific organizations all over the world. Others established their own private businesses, which were software-related high-tech IT companies. Many of these companies grew up into large, internationally recommended providers of software products and services. Their strategy was simple: the new IT companies would adopt the tried and true principles of the already established, famous IT corporations.\n\nAll IT companies in the Silicon Taiga can be divided into the two categories according to the offshore programming business model they employ: The firms that provide Offshore Development Center (ODC), and mostly develop new custom products (NPD), and the firms that are oriented to the SAAS model. Meantime the global giants in the field of software and high-tech products, such as IBM, Intel Corporation, and Schlumberger, saw this growing trend and established branch offices in the Silicon Taiga.\n\nThe summer of 2010 saw the launch of an innovative technology park in Akademgorodok, which brought together economic and intellectual power in the area, thus expanding innovation in Russia. Today, the Silicon Taiga is not just a collection of IT companies; it is a beneficial environment, in which the Russian national system of innovations can continue to flourish.\n\nThe following is a partial list of past and present notable companies founded in the Silicon Taiga or which have a major subsidiary located there:\n\n\n"}
{"id": "22021031", "url": "https://en.wikipedia.org/wiki?curid=22021031", "title": "Social translucence", "text": "Social translucence\n\nSocial translucence (also referred as \"social awareness\") is a term that was proposed by Thomas Erickson and Wendy Kellogg to refer to \"design digital systems that support coherent behavior by making participants and their activities visible to one another\".\n\nSocial translucence represents a tool for transparency in socio-technical systems, which function is to\n\nSocial translucence is, in particular, a core element in online social networking such as Facebook or LinkedIn, in which they intervene in the possibility for people to expose their online identity, but also in the creation of awareness of other people activities, that are for instance present in the activity feeds that these systems make available.\n\nSocial translucence mechanisms have been made available in many web 2.0 systems such as:\n\nParticipation of people in online communities, in general, differ from their participatory behavior in real-world collective contexts. Humans in daily life are used to making use of \"social cues\" for guiding their decisions and actions e.g. if a group of people is looking for a good restaurant to have lunch, it is very likely that they will choose to enter to a local that have some customers inside instead of one that it is empty (the more crowded restaurant could reflect its popularity and in consequence, its quality of service). However, in online social environments, it is not straightforward how to access to these sources of information which are normally being logged in the systems, but this is not disclosed to the users.\n\nThere are some theories that explain how this social translucence can affect the behavior of people in real-life scenarios. The American philosopher George Herbert Mead states that humans are social creatures, in the sense that people's actions cannot be isolated from the behavior of the whole collective they are part of because every individuals' acts are influenced by larger social practices that act as a general behavior's framework. In his performance framework, the Canadian sociologist Erving Goffman postulates that in everyday social interactions individuals perform their actions by collecting information from others first, in order to know in advance what they may expect from them and in this way being able to plan how to behave more effectively.\n\nAccording to Erickson et al., social translucent systems should respect the principles of visibility (making significant social information available to users), awareness (bringing our social rules to guide our actions based on external social cues) and accountability (being able to identify who did what and when) in order to allow people to effectively facilitate users communication and collaboration in virtual environments. Zolyomi et al. proposed the principle of identity as a fourth dimension for social translucence by arguing that the design of socio-technical systems should have a rich description of who is visible, in order to give people control over disclosure and mechanisms to advocate for their needs. McDonald et al. proposed a system architecture for structuring the development of social translucent systems, which comprises two dimensions: types of user actions in the system, and a second describing the processing and interpretation done by the system. This framework can guide designers to determine what activities are important to social translucence and need to be reflected, and how interpretive levels of those actions might provide contextual salience to the users \n\nIn the same way that in the real-world, providing social cues in virtual communities can help people to understand better the situations they face in these environments, to alleviate their decision-making processes by enabling their access to more informed choices, to persuade them to participate in the activities that take place there, and to structure their own schedule of individual and group activities more efficiently.\n\nIn this frame of reference, an approach called \"social context displays\" has been proposed for showing social information -either from real or virtual environments- in digital scenarios. It is based on the use of graphical representations to visualize the presence and activity traces of a group of people, thus providing users with a third-party view of what is happening within the community i.e. who are actively participating, who are not contributing to the group efforts, etc. This social-context-revealing approach has been studied in different scenarios (e.g. IBM video-conference software, large community displaying social activity traces in a shared space called NOMATIC*VIZ), and it has been demonstrated that its application can provide users with several benefits, like providing them with more information to make better decisions and motivating them to take an active attitude towards the management of their self and group representations within the display through their actions in the real-life.\n\nThe feeling of personal accountability in front of others that social translucence can report to users can be used for the design of systems for supporting behavior change (e.g. weight loss, smoking cessation), if combined with the appropriate type of feedback.\n\nBy making the traces of activity of users publicly available for others to access it is natural that it can raise users concerns related to which are their rights over the data they generate, who are the final users that can have access to their information and how they can know and control their privacy policies. There are several perspectives that try to contextualize this privacy issue. One perspective is to see privacy as a tradeoff between the degree of invasion to the personal space and the number of benefits that the user could perceive from the social system by disclosing their online activity traces. Another perspective is examining the concession between the visibility of people within the social system and their level of privacy, which can be managed at an individual or at a group level by establishing specific permissions for allowing others to have access to their information. Other authors state that instead of enforcing users to set and control privacy settings, social systems might focus on raising their awareness about who their audiences are so they can manage their online behavior according to the reactions they expect from those different user groups.\n\n\n"}
{"id": "17017917", "url": "https://en.wikipedia.org/wiki?curid=17017917", "title": "Software studies", "text": "Software studies\n\nSoftware studies is an emerging interdisciplinary research field, which studies software systems and their social and cultural effects.\n\nThe implementation and use of software has been studied in recent fields such as cyberculture, Internet studies, new media studies, and digital culture, yet prior to software studies, software was rarely ever addressed as a distinct object of study.\n\nSoftware studies is an interdisciplinary field. To study software as an artifact, it draws upon methods and theory from the digital humanities and from computational perspectives on software. Methodologically, software studies usually differs from the approaches of computer science and software engineering, which concern themselves primarily with software in information theory and in practical application; however, these fields all share an emphasis on computer literacy, particularly in the areas of programming and source code. This emphasis on analyzing software sources and processes (rather than interfaces) often distinguishes software studies from new media studies, which is usually restricted to discussions of interfaces and observable effects.\n\nThe conceptual origins of software studies include Marshall McLuhan's focus on the role of media in themselves, rather than the content of media platforms, in shaping culture. Early references to the study of software as a cultural practice appear in Friedrich Kittler's essay, \"Es gibt keine Software,\" Lev Manovich's \"Language of New Media\", and Matthew Fuller's \"Behind the Blip: Essays on the culture of software\". Much of the impetus for the development of software studies has come from videogame studies, particularly platform studies, the study of videogames and other software artifacts in their hardware and software contexts. New media art, software art, motion graphics, and computer-aided design are also significant software-based cultural practices, as is the creation of new protocols and platforms.\n\nThe first conference events in the emerging field were Software Studies Workshop 2006 and SoftWhere 2008.\n\nIn 2008, MIT Press launched a \"Software Studies\" book series with an edited volume of essays (Matthew Fuller's \"Software Studies: a Lexicon\"), and the first academic program was launched, (Lev Manovich, Benjamin H. Bratton and Noah Wardrip-Fruin's \"Software Studies Initiative\" at U. California San Diego). \nIn 2011, a number of mainly British researchers established \"Computational Culture\", an open-access peer-reviewed journal. The journal provides a platform for \"inter-disciplinary enquiry into the nature of the culture of computational objects, practices, processes and structures.\"\n\nSoftware studies is closely related to a number of other emerging fields in the digital humanities that explore functional components of technology from a social and cultural perspective. Software studies' focus is at the level of the entire program, specifically the relationship between interface and code. Notably related are critical code studies, which is more closely attuned to the code rather than the program, and platform studies, which investigates the relationships between hardware and software.\n\n\n\n"}
{"id": "14171448", "url": "https://en.wikipedia.org/wiki?curid=14171448", "title": "Specification (technical standard)", "text": "Specification (technical standard)\n\nA specification often refers to a set of documented requirements to be satisfied by a material, design, product, or service. A specification is often a type of technical standard.\n\nThere are different types of technical or engineering specifications (specs), and the term is used differently in different technical contexts. They often refer to particular documents, and/or particular information within them. The word \"specification\" is broadly defined as \"to state explicitly or in detail\" or \"to be specific\". \n\nUsing the term \"specification\" without a clear indication of what kind is confusing and considered bad practice.\n\nA requirement specification is a documented requirement, or set of documented requirements, to be satisfied by a given material, design, product, service, etc. It is a common early part of engineering design and product development processes, in many fields.\n\nA functional specification is a kind of requirement specification, and may show functional block diagrams.\n\nA design or product specification describes the features of the \"solutions\" for the Requirement Specification, referring to either a designed solution or final produced solution. It is often used to guide fabrication/production. Sometimes the term \"specification\" is here used in connection with a data sheet (or \"spec sheet\"), which may be confusing. A data sheet describes the technical characteristics of an item or product, often published by a manufacturer to help people choose or use the products. A data sheet is not a technical specification in the sense of informing how to produce.\n\nAn \"in-service\" or \"maintained as\" specification, specifies the conditions of a system or object after years of operation, including the effects of wear and maintenance (configuration changes).\n\nSpecifications are a type of technical standard that may be developed by any of various kinds of organizations, both public and private. Example organization types include a corporation, a consortium (a small group of corporations), a trade association (an industry-wide group of corporations), a national government (including its military, regulatory agencies, and national laboratories and institutes), a professional association (society), a purpose-made standards organization such as ISO, or vendor-neutral developed generic requirements. It is common for one organization to \"refer to\" (\"reference\", \"call out\", \"cite\") the standards of another. Voluntary standards may become mandatory if adopted by a government or business contract.\n\nIn engineering, manufacturing, and business, it is vital for suppliers, purchasers, and users of materials, products, or services to understand and agree upon all requirements. \n\nA specification may refer to a standard which is often referenced by a contract or procurement document, or an otherwise agreed upon set of requirements (though still often used in the singular). In any case, it provides the necessary details about the specific requirements.\n\nStandards for specifications may be provided by government agencies, standards organizations (ASTM, ISO, CEN, DoD, etc.), trade associations, corporations, and others. The following British standards apply to specifications:\n\nA design/product specification does not necessarily prove a product to be correct or useful in every context. An item might be verified to comply with a specification or stamped with a specification number: this does not, by itself, indicate that the item is fit for other, non-validated uses. The people who use the item (engineers, trade unions, etc.) or specify the item (building codes, government, industry, etc.) have the responsibility to consider the choice of available specifications, specify the correct one, enforce compliance, and use the item correctly. Validation of suitability is necessary.\n\nSometimes a guide or a standard operating procedure is available to help write and format a good specification. A specification might include:\n\nSpecifications in North America form part of the contract documents that accompany and govern the construction of building and infrastructure projects. Specifications describe the quality and performance of building materials, using code citations and published standards, whereas the drawings or Building Information Model (BIM) illustrates quantity and location of materials. The guiding master document of names and numbers is the latest edition of MasterFormat. This is a consensus document that is jointly sponsored by two professional organizations: Construction Specifications Canada and Construction Specifications Institute based in the United States and updated every two years.\n\nWhile there is a tendency to believe that \"Specifications overrule Drawings\" in the event of discrepancies between the text document and the drawings, the actual intent must be made explicit in the contract between the Owner and the Contractor. The standard AIA (American Institute of Architects) and EJCDC (Engineering Joint Contract Documents Committee) states that the drawings and specifications are complementary, together providing the information required for a complete facility. Many public agencies, such as the Naval Facilities Command (NAVFAC) state that the specifications overrule the drawings. This is based on the idea that words are easier for a jury (or mediator) to interpret than drawings in case of a dispute.\n\nThe standard listing of construction specifications falls into 50 Divisions, or broad categories of work types and work results involved in construction. The divisions are subdivided into sections, each one addressing a specific material type (concrete) or a work product (steel door) of the construction work. A specific material may be covered in several locations, depending on the work result: stainless steel (for example) can be covered as a sheet material used in Flashing and Sheet Metal in Division 07; it can be part of a finished product, such as a handrail, covered in Division 05; or it can be a component of building hardware, covered in Division 08. The original listing of specification divisions was based on the time sequence of construction, working from exterior to interior, and this logic is still somewhat followed as new materials and systems make their way into the construction process.\n\nEach Section is subdivided into three distinct Parts: \"General\", \"Products\" and \"Execution\". The MasterFormat and Section Format system can be successfully applied to residential, commercial, civil, and industrial construction. Although many Architects find the rather voluminous commercial style of specifications too lengthy for most residential projects and therefore either produce more abbreviated specifications of their own or use ArCHspec (which was specifically created for residential projects). Master specification systems are available from multiple vendors such as Arcom, Visispec, BSD, and Spectext. These systems were created to standardize language across the United States and are usually subscription based.\n\nSpecifications can be either \"performance-based\", whereby the specifier restricts the text to stating the performance that must be achieved by the completed work, \"prescriptive\" where the specifier states the specific criteria such as fabrication standards applicable to the item, or \"proprietary\", whereby the specifier indicates specific products, vendors and even contractors that are acceptable for each workscope. In addition, specifications can be \"closed\" with a specific list of products, or \"open\" allowing for substitutions made by the Contractor. Most construction specifications are a combination of performance-based and proprietrary types, naming acceptable manufacturers and products while also specifying certain standards and design criteria that must be met.\n\nWhile North American specifications are usually restricted to broad descriptions of the work, European ones and Civil work can include actual work quantities, including such things as area of drywall to be built in square meters, like a bill of materials. This type of specification is a collaborative effort between a specwriter and a quantity surveyor. This approach is unusual in North America, where each bidder performs a quantity survey on the basis of both drawings and specifications. In many countries on the European continent, content that might be described as \"specifications\" in the United States are covered under the building code or municipal code. Civil and infrastructure work in the United States often includes a quantity breakdown of the work to be performed as well. \n\nAlthough specifications are usually issued by the architect's office, specification writing itself is undertaken by the architect and the various engineers or by specialist specification writers. Specification writing is often a distinct professional trade, with professional certifications such as \"Certified Construction Specifier\" (CCS) available through the Construction Specifications Institute and the Registered Specification Writer (RSW) through Construction Specifications Canada. Specification writers are either employees of or sub-contractors to architects, engineers, or construction management companies. Specification writers frequently meet with manufacturers of building materials who seek to have their products specified on upcoming construction projects so that contractors can include their products in the estimates leading to their proposals.\n\nIn February 2015, ArCHspec went live, from ArCH (Architects Creating Homes), a nationwide American professional society of Architects whose purpose is to improve residential architecture. ArCHspec was created specifically for use by Licensed Architects while designing SFR (Single Family Residential) architectural projects. Unlike the more commercial CSI (50+ division commercial specifications), ArCHspec utilizes the more recognizable 16 traditional Divisions, plus a Division 0 (Scope & Bid Forms) and Division 17 (low voltage). Many architects, up to this point, did not provide specifications for residential designs, which is one of the reasons ArCHspec was created: to fill a void in the industry with more compact specifications for residential use. Shorter form specifications documents suitable for residential use are also available through Arcom, and follow the 50 division format, which was adopted in both the United States and Canada starting in 2004. The 16 division format is no longer considered standard, and is not supported by either CSI or CSC, or any of the subscription master specification services, data repositories, product lead systems, and the bulk of governmental agencies.\n\nSpecifications in Egypt form part of contract documents. The Housing and Building National Research Center (HBRC) is responsible for developing construction specifications and codes. The HBRC has published more than 15 books which cover building activities like earthworks, plastering, etc.\n\nSpecifications in the UK are part of the contract documents that accompany and govern the construction of a building. They are prepared by construction professionals such as architects, architectural technologists, structural engineers, landscape architects and building services engineers. They are created from previous project specifications, in-house documents or master specifications such as the National Building Specification (NBS). The National Building Specification is owned by the Royal Institute of British Architects (RIBA) through their commercial group RIBA Enterprises (RIBAe). NBS master specifications provide content that is broad and comprehensive, and delivered using software functionality that enables specifiers to customize the content to suit the needs of the project and to keep up to date.\n\nUK project specification types fall into two main categories prescriptive and performance. Prescriptive specifications define the requirements using generic or proprietary descriptions of what is required, whereas performance specifications focus on the outcomes rather than the characteristics of the components.\n\nSpecifications are an integral part of Building Information Modeling and cover the non-geometric requirements.\n\nPharmaceutical products can usually be tested and qualified by various Pharmacopoeia. Current existing pharmaceutical standards include:\n\nIf any pharmaceutical product is not covered by the above standards, it can be evaluated by the additional source of Pharmacopoeia from other nations, from industrial specifications, or from a standardized formulary such as\n\n\nA similar approach is adopted by the food manufacturing, of which Codex Alimentarius ranks the highest standards, followed by regional and national standards.\n\nThe coverage of food and drug standards by ISO is currently less fruitful and not yet put forward as an urgent agenda due to the tight restrictions of regional or national constitution\n\nSpecifications and other standards can be externally imposed as discussed above, but also internal manufacturing and quality specifications. These exist not only for the food or pharmaceutical product but also for the processing machinery, quality processes, packaging, logistics (cold chain), etc. and are exemplified by \"ISO 14134\" and \"ISO 15609\"\n\nThe converse of explicit statement of specifications is a process for dealing with observations that are out-of-specification. The United States Food and Drug Administration has published a non-binding recommendation that addresses just this point.\n\nAt the present time, much of the information and regulations concerning food and food products remain in a form which makes it difficult to apply automated information processing, storage and transmission methods and techniques.\n\nData systems that can process, store and transfer information about food and food products need formal specifications for the representations of data about food and food products in order to operate effectively and efficiently.\n\nDevelopment of formal specifications for food and drug data with the necessary and sufficient clarity and precision for use specifically by digital computing systems have begun to emerge from government agencies and standards organizations.\n\nThe United States Food and Drug Administration has published specifications for a \"Structured Product Label\" which drug manufacturers must by mandate use to submit electronically the information on a drug label.\n\nRecently, ISO has made some progress in the area of food and drug standards and formal specifications for data about regulated substances through the publication of \"ISO 11238\"\n\nIn many contexts, particularly software, specifications are needed to avoid errors due to lack of compatibility, for instance, in interoperability issues.\n\nFor instance, when two applications share Unicode data, but use different normal forms or use them incorrectly, in an incompatible way or without sharing a minimum set of interoperability specification, errors and data loss can result. For example, Mac OS X has many components that prefer or require only decomposed characters (thus decomposed-only Unicode encoded with UTF-8 is also known as \"UTF8-MAC\"). In one specific instance, the combination of OS X errors handling composed characters, and the samba file- and printer-sharing software (which replaces decomposed letters with composed ones when copying file names), has led to confusing and data-destroying interoperability problems.\n\nApplications may avoid such errors by preserving input code points, and only normalizing them to the application's preferred normal form for internal use.\n\nSuch errors may also be avoided with algorithms normalizing both strings before any binary comparison.\n\nHowever errors due to file name encoding incompatibilities have always existed, due to a lack of minimum set of common specification between software hoped to be inter-operable between various file system drivers, operating systems, network protocols, and thousands of software packages.\n\nA formal specification is a mathematical description of software or hardware that may be used to develop an implementation. It describes \"what\" the system should do, not (necessarily) \"how\" the system should do it. Given such a specification, it is possible to use formal verification techniques to demonstrate that a candidate system design is correct with respect to that specification. This has the advantage that incorrect candidate system designs can be revised before a major investment has been made in actually implementing the design. An alternative approach is to use provably correct refinement steps to transform a specification into a design, and ultimately into an actual implementation, that is correct by construction.\n\nIn (hardware, software, or enterprise) systems development, an architectural specification is the set of documentation that describes the structure, behavior, and more views of that system.\n\nA program specification is the definition of what a computer program is expected to do. It can be \"informal\", in which case it can be considered as a user manual from a developer point of view, or \"formal\", in which case it has a definite meaning defined in mathematical or programmatic terms. In practice, many successful specifications are written to understand and fine-tune applications that were already well-developed, although safety-critical software systems are often carefully specified prior to application development. Specifications are most important for external interfaces that must remain stable.\n\nIn software development, a functional specification (also, functional spec or specs or functional specifications document (FSD)) is the set of documentation that describes the behavior of a computer program or larger software system. The documentation typically describes various inputs that can be provided to the software system and how the system responds to those inputs.\n\nWeb services specifications are often under the umbrella of a quality management system.\n\nThese types of documents define how a specific document should be written, which may include, but is not limited to, the systems of a document naming, version, layout, referencing, structuring, appearance, language, copyright, hierarchy or format, etc. Very often, this kind of specifications is complemented by a designated template.\n\n"}
{"id": "34066587", "url": "https://en.wikipedia.org/wiki?curid=34066587", "title": "Technological transitions", "text": "Technological transitions\n\nTechnological innovations have occurred throughout history and rapidly increased over the modern age. New technologies are developed and co-exist with the old before supplanting them. Transport offers several examples; from sailing to steam ships to automobiles replacing horse-based transportation. Technological transitions (TT) describe how these technological innovations occur and are incorporated into society. Alongside the technological developments TT considers wider societal changes such as “user practices, regulation, industrial networks (supply, production, distribution), infrastructure, and symbolic meaning or culture”. For a technology to have use, it must be linked to social structures human agency and organisations to fulfil a specific need. Hughes refers to the ‘seamless web’ where physical artefacts, organisations, scientific communities, and social practices combine. A technological system includes technical and non-technical aspects, and it a major shift in the socio-technical configurations (involving at least one new technology) is when a technological transition occurs.\n\nWork on technological transitions draws on a number of fields including history of science, technology studies, and evolutionary economics. The focus of evolutionary economics is on economic change, but as a driver of this technological change has been considered in the literature. Joseph Schumpeter, in his classic \"Theory of Economic Development\" placed the emphasis on non-economic forces as the driver for growth. The human actor, the entrepreneur is seen as the cause of economic development which occurs as a cyclical process. Schumpeter proposed that radical innovations were the catalyst for Kondratiev cycles.\n\nThe Russian economist Kondratiev proposed that economic growth operated in boom and bust cycles of approximately 50 year periods. These cycles were characterised by periods of expansion, stagnation and recession. The period of expansion is associated with the introduction of a new technology, e.g. steam power or the microprocessor. At the time of publication, Kondratiev had considered that two cycles had occurred in the nineteenth century and third was beginning at the turn of the twentieth. Modern writers, such as Freeman and Perez outlined five cycles in the modern age:\n\n\nFreeman and Perez proposed that each cycle consists of pervasive technologies, their production and economic structures that support them. Termed ‘techno-economic paradigms’, they suggest that the shift from one paradigm to another is the result of emergent new technologies. \n\nFollowing the recent economic crisis, authors such as Moody and Nogrady have suggested that a new cycle is emerging from the old, centred on the use of sustainable technologies in a resource depleted world.\n\nThomas Kuhn described how a paradigm shift is a wholesale shift in the basic understanding of a scientific theory. Examples in science include the change of thought from miasma to germ theory as a cause of disease. Building on this work, Giovanni Dosi developed the concept of ’technical paradigms’ and ‘technological trajectories’. In considering how engineers work, the technical paradigm is an outlook on the technological problem, a definition of what the problems and solutions are. It charts the idea of specific progress. By identifying the problems to be solved the paradigm exerts an influence on technological change. The pattern of problem solving activity and the direction of progress is the technological trajectory. In similar fashion, Nelson and Winter (,)defined the concept of the ‘technological regime’ which directs technological change through the beliefs of engineers of what problems to solve. The work of the actors and organisations is the result of organisational and cognitive routines which determines search behaviour. This places boundaries and also trajectories (direction) to those boundaries.\n\nIn analysing (historic) cases of technological transitions researchers from the systems in transition branch of transitions research have used a multi-level perspective (MLP) as a heuristic model to understand changes in socio-technical systems. () Innovation system approaches traditionally focus on the production side. A socio-technical approach combines the science and technology in devising a production, with the application of the technology in fulfilling a societal function. Linking the two domains are the distribution, infrastructure and markets of the product. This approach considers a transition to be multi-dimensional as technology is only one aspect. \n\nThe MLP proposes three analytical levels: the niche, regime and landscape. \n\nNiche (Micro-level)\nRadical innovations occur at the niche level. These act as ‘safe havens’ for fledgling technologies to develop, largely free from market pressures which occur at the regime level. The US Military has acted as niche for major twentieth century technologies such as the aircraft, radio and the internet. More recently, California’s Silicon Valley has provided an arena for ICT focused technologies to emerge. Some innovations will challenge the existing regime while others fail. \n\nRegime (Meso-level)\nThe socio-technical regime, as defined by Geels, includes a web of inter-linking actors across different social groups and communities following a set of rules. In effect, the established practices of a given system. Seven dimensions have been identified in the socio-technical regime: technology, user practices and application, the symbolic meaning of technology, infrastructure, policy and techno-scientific knowledge. Change does occur at the regime level but it is normally slow and incremental unlike the radical change at the niche level. The actors who constitute the existing regime are set to gain from perpetuating the incumbent technology at the expense of the new. This is known as ‘lock-in’.\n\nLandscape (Macro-level)\nExogenous to the previous levels is the socio-technical landscape. A broad range of factors are contained here, such as economic pressures, cultural values, social trends, wars and environmental issues. Change occurs at an even slower rate than at the regime level. \n\nA transition is said to happen when a regime shift has occurred. This is the result of the interplay between the three levels. Regimes are relatively inert and resistant to change being structured to incremental innovation following established trajectories. As such, transitions are difficult to achieve. The current regime is typically suffering internal issues. Pressure from the landscape level may cause ‘cracks’ or ‘windows of opportunity’ through which innovations at the niche level may initially co-exist with the established technology before achieving ascendency. Once the technology has fully embedded into society the transition is said to be completed.\n\nThe MLP has been used in describing a range of historic transitions in socio-technical regimes for mobility, sanitation, food, lighting and so on. While early research focused on historical transitions, a second strand of research was more focused on transitions to sustainable technologies in key sectors such as transport, energy and housing.\n\nGeels presented three historical transitions on system innovation relating to modes of transportation. The technological transition from sailing ships to steamships in the UK will be summarised and shown in the context of a wider system innovation. \n\nGreat Britain was the world’s leading naval power in the nineteenth century, and led the way in the transition from sail to steam. At first, the introduction of steam technology co-existed with the current regime. Steam tugs assisted sail ships into port and hybrid steam / sail ships appeared. Landscape developments create the necessity for improvements in the technology. A demand for trans-Atlantic emigration was prompted by the Irish potato famine, European political instability and the lure of gold in California. The requirement for such arduous journeys had prompted a wealth of innovations at the niche level in steamship-development. From the late 1880s, as steamship technology improved and costs dropped, the new technology was widely diffused and a new regime established. The changes go beyond a technological transition as it involved new ship management and fleet management practices, new supporting infrastructures and new functionalities.\n\nThe nature of transitions varies and the differing qualities result in multiple pathways occurring. Geels and Schot defined five transition paths:\n\n\nSix characteristics of technological transitions have been identified.,\n\n\"Transitions are co-evolutionary and multi-dimensional\"\nTechnological developments occur intertwined with societal needs, wants and uses. A technology is adopted and diffused based on this interplay between innovation and societal requirements. Co-evolution has different aspects. As well as the co-evolution of technology and society, aspects between science, technology, users and culture have been considered.\n\"Multi-actors are involved\"\nScientific and engineering communities are central to the development of a technology, but a wide range of actors are involved in a transition. This can include organisations, policy-makers, government, NGOs, special interest groups and others.\n\n\"Transitions occur at multiple levels\"\nAs shown in the MLP transitions occur through the interplay of processes at different levels. \n\n\"Transitions are a long-term process\"\nComplete system-change takes time and can be decades in the making. Case studies show them to be between 40 and 90 years.\n\n\"Transitions are radical\"\nFor a true transition to occur the technology has to be a radical innovation. \n\n\"Change is Non-linear\"\nThe rate of change will vary over time. For example, the pace of change may be slow at the gestation period (at the niche level) but much more rapid when a breakthrough is occurring.\n\nDiffusion of an innovation is the concept of how it is picked up by society, at what rate and why. Everett (1962).The diffusion of a technological innovation into society can be considered in distinct phases. Pre-development is the gestation period where the new technology has yet to make an impact. Take-off is when the process of a system shift is beginning. A breakthrough is occurring when fundamental changes are occurring in existing structures through the interplay of economic, social and cultural forces. Once the rate of change has decreased and a new balance is achieved, stabilization is said to have occurred. A full transition involves an overhaul of existing rules and change of beliefs which takes time, typically spanning at least a generation. This process can be speeded-up through seismic, unforeseen events such as war or economic strife. \n\nGeels proposed a similar four phased approach which draws on the multi-level perspective (MLP) developed by Dutch scholars. Phases one sees the emergence of a novelty, born from the existing regime. Development then occurs in the niche level at phase two. As before, breakthrough then occurs at phase three. In the parlance of the MLP the new technology, having been developed at the niche level, is in competition with the established regime. To breakthrough and achieve wide diffusion, external factors – ‘windows of opportunity’ are required.\n\nA number of possible circumstances can act as windows of opportunity for the diffusion of new technologies: \n\n\nAlongside external influences, internal drivers catalyse diffusion. These include economic factors such as the price performance ration. Socio-technical perspectives focus on the links between disparate social and technological elements. Following the breakthrough, the final phases see the new technology supersede the old.\n\nThe study of technological transitions has an impact beyond academic interest. The transitions referred to in the literature may relate to historic processes, such as the transportation transitions studied by Geels, but system changes are required to achieve a safe transition to a low carbon-economy. (). Current structural problems are apparent in a range of sectors. Dependency on oil is problematic in the energy sector due to availability, access and contribution to greenhouse gas (GHG) emissions. Transportation is a major user of energy causing significant emission of GHGs. Food production will need to keep pace with an ever-growing world population while overcoming challenges presented by global warming and transportation issues. Incremental change has provided some improvements but a more radical transition is required to achieve a more sustainable future. \n\nDeveloped from the work on technological transitions is the field of transition management. Within this is an attempt to shape the direction of change complex socio-technical systems to more sustainable patterns. Whereas work on technological transitions is largely based on historic processes, proponents of transition management seek to actively steer transitions in progress.\n\nGenus and Coles outlined a number of criticisms against the analysis of technological transitions, in particular when using the MLP. Empirical research on technological transitions occurring now has been limited, with the focus on historic transitions. Depending on the perspective on transition case studies they could be presented as having occurred on a different transition path to what was shown. For example, the bicycle could be considered an intermediate transport technology between the horse and the car. Judged from shorter different time-frame this could appear a transition in its own right. Determining the nature of a transition is problematic; when it started and ended, or whether one occurred in the sense of a radical innovation displacing an existing socio-technical regime. The perception of time casts doubt on whether a transition has occurred. If viewed over a long enough period even inert regimes may demonstrate radical change in the end. The MLP has also been criticised by scholars studying sustainability transitions using Social Practice Theories.\n\n"}
{"id": "42693945", "url": "https://en.wikipedia.org/wiki?curid=42693945", "title": "Technology Business Management Council", "text": "Technology Business Management Council\n\nTechnology Business Management Council is a Washington-based non-profit organization with the goal of creating and promoting “best practices for running IT as a business.” As of January 2018, the TBM Council has about 5,100 CIO, CTO, & CFO members who are advancing technology business management standards and education across all industries. Its current board of directors includes executives from AIG, Afflac, Apptio, Cisco, Dollar General, First American, Intuit, Great West Life, MasterCard, Micron, Nationwide, Red Ventures, Stanley Black & Decker, SunTrust, Tyson Foods, and the University of Pennsylvania. \n\nThe TBM Council began as a biannual executive summit for CIOs sponsored by Apptio, which develops technology business management software as a service applications. In 2012, Apptio founded the TBM Council as a non-profit organization with a mission to identify and promote industry best practices for running technology organizations like a business. Upon incorporation, Apptio's Chief Marketing Officer (Chris Pick) became the president and Chief Information Officers from Cisco (Rebecca Jacoby) and First American (Larry Godec) became co-chairs of the board of directors. In 2013, the Council held its first industry conference in Seattle and expanded its program into Europe in 2014. In 2015, the Council appointed Mike Brown, Vice President, Global Information Technology at ExxonMobil as its chairman of the board, created industry workgroups to define standards and launched a private and public sector IT COST Commission with the United States Federal Government. In 2016, the Council released their book and concluded the year long IT COST Commission by making 21 recommendations to improve transparency, reduce waste and increase the efficiency of IT spending within the US Government.\n\nThe TBM Framework for “running IT with greater business acumen” is designed to provide “a shared decision-making model for technology and business leaders” and a structure for IT executives to have “conversations with the CEO and the board of directors about the value of IT investments.”<ref name=\"WSJ CIO 8/2012\"></ref>\n\nThe TBM Council publishes an agreed-upon set of TBM best practices with the goal of helping business technology leaders benchmark their success in those practices using the TBM Index. The index follows the structure of the TBM Framework. It is an interactive survey with open participation and is published in the TBM Council’s iPad ebook, \"Technology Business Management: How Innovative Technology Leaders Apply Business Acumen to Drive Value\", and on the TBM Council’s website.\n\nThe goal of the TBM Index is to provide “an industry-validated benchmark on the state of technology business management.” The information gathered is designed to help the council understand how technology business management varies by industry, geography, and IT operating model, in order to make recommendations to companies around the world. \n\nThe TBM Council’s work has been credited with helping some enterprises identify CIOs as those “most responsible for driving digital innovation and change.” For example, Clorox Co. has been using the TBM Council’s best practices and benchmarks to help establish the value of IT services and make the case for further investments in IT.\n\n\n\nThe TBM Council’s founding board of directors include Mike Benson, executive vice president and CIO of DirecTV; Tim Campos, CIO of Facebook; Don Duet, co-head technology division, Goldman Sachs; Sunny Gupta, CEO of Apptio; Rebecca Jacoby, CIO of Cisco; Ralph Loura, senior vice president, Clorox Co.; Tom Murphy, CIO, University of Pennsylvania (and former CIO of AmerisourceBergen); Jim Scholefield, CTO, The Coca-Cola Company; Robert Webb, CITO, Etihad Airways (and former CIO of Hilton); and Carol Zierhoffer, CIO of Xerox.\n\nIn 2013, the TBM Council commissioned Forrester Consulting to research the impact of business imperatives on the metrics CIOs use to convey the performance and contribution of technologies toward meeting business goals. The research found that most business leaders don’t understand what IT departments do and don’t know what the IT budget is or how the IT department’s success is measured. Most business leaders surveyed said that IT departments had a 60% bigger budget than they actually did. CIOs said they spend 5% of the organization’s revenue, but business leaders thought it was 8% of revenue. \n\nIn 2010, the TBM Council held its first virtual TBM Summit, with approximately 100 CIOs and IT leaders participating. The TBM Council held its first conference in November 2013 in Seattle, attended by approximately 400 IT leaders. At the conference, eBay received the council’s first Operational Excellence Award, given to organizations that employ “internal and external IT transparency to improve the cost-effectiveness of the services IT delivers to the wider organization.” The second conference is scheduled for October 2014 in Miami Beach.\n\n"}
{"id": "17163802", "url": "https://en.wikipedia.org/wiki?curid=17163802", "title": "Technology dynamics", "text": "Technology dynamics\n\nTechnology dynamics is broad and relatively new scientific field that has been developed in the framework of the postwar science and technology studies field. It studies the process of technological change. Under the field of Technology Dynamics the process of technological change is explained by taking into account influences from \"internal factors\" as well as from \"external factors\". Internal factors relate technological change to unsolved technical problems and the established modes of solving technological problems and external factors relate it to various (changing) characteristics of the social environment, in which a particular technology is embedded.\n\nFor the last three decades, it has been argued that technology development is neither an autonomous process, determined by the \"inherent progress\" of human history, nor a process completely determined by external conditions like the prices of the resources that are needed to operate (develop) a technology, as it is theorized in neoclassical economic thinking. In mainstream neoclassical economic thinking, technology is seen as an exogenous factor: at the moment a technology is required, the most appropriate version can be taken down from the shelf based on costs of labor, capital and eventually raw materials.\n\nConversely, modern technology dynamics studies generally advocate that technologies are not \"self-evident\" or market-demanded, but are the upshot of a particular path of technology development and are shaped by social, economic and political factors. in this sense, technology dynamics aims at overcoming distinct \"internal\" and \"external\" points of views by presenting co-evolutionary approach regarding technology development.\n\nIn general, technology dynamics studies, besides giving a \"thick description\" of technology development, uses constructivist viewpoints emphasizing that technology is the outcome of particular social context. Accordingly, Technology Dynamics emphasizes the significance and possibility of regaining social control of technology, and also provides mechanisms needed to adapt to and steer the development of certain technologies. In that respect, it uses insights from retrospective studies to formulate hypotheses of a prospective nature on technology development of emerging technologies, besides formulating prescriptive policy recommendations.\n\nAn important feature of relevant theories of technological change therein is that they underline the quasi-evolutionary character of technological change: change based on technological variation and social selection in which technological knowledge, systems and institutions develop in interaction with each other. Processes of 'path dependence' are crucial in explaining technological change.\n\nFollowing these lines, there have been different approaches and concepts used under the field of technology dynamics.\n\n\nBased on the analysis of the various perspectives, one can aim at developing interventions in the dynamics of a technology. Some approaches have been developed targeting on interventions in technological change:\n\n\n\n"}
{"id": "10518546", "url": "https://en.wikipedia.org/wiki?curid=10518546", "title": "Technology life cycle", "text": "Technology life cycle\n\nThe technology life-cycle (TLC) describes the commercial gain of a product through the expense of research and development phase, and the financial return during its \"vital life\". Some technologies, such as steel, paper or cement manufacturing, have a long lifespan (with minor variations in technology incorporated with time) while in other cases, such as electronic or pharmaceutical products, the lifespan may be quite short.\n\nThe TLC associated with a product or technological service is different from product life-cycle (PLC) dealt with in product life-cycle management. The latter is concerned with the life of a product in the marketplace with respect to timing of introduction, marketing measures, and business costs. The \"technology\" underlying the product (for example, that of a uniquely flavoured tea) may be quite marginal but the process of creating and managing its life as a branded product will be very different. \n\nThe technology life cycle is concerned with the time and cost of developing the technology, the timeline of recovering cost, and modes of making the technology yield a profit proportionate to the costs and risks involved. The TLC may, further, be protected during its cycle with patents and trademarks seeking to lengthen the cycle and to maximize the profit from it.\n\nThe \"product\" of the technology may be a commodity such as polyethylene plastic or a sophisticated product like the integrated circuits used in a smartphone.\n\nThe development of a \"competitive product\" or process can have a major effect on the lifespan of the technology, making it shorter. Equally, the loss of intellectual property rights through litigation or loss of its secret elements (if any) through leakages also work to reduce a technology's lifespan. Thus, it is apparent that the \"management\" of the TLC is an important aspect of technology development.\n\nMost new technologies follow a similar technology maturity lifecycle describing the technological maturity of a product. This is not similar to a product life cycle, but applies to an entire technology, or a generation of a technology.\n\nTechnology adoption is the most common phenomenon driving the evolution of industries along the industry lifecycle. After expanding new uses of resources they end with exhausting the efficiency of those processes, producing gains that are first easier and larger over time then exhaustingly more difficult, as the technology matures.\n\nThe TLC may be seen as composed of four phases:\n\nThe shape of the technology lifecycle is often referred to as S-curve.\n\nThere is usually technology hype at the introduction of any new technology, but only after some time has passed can it be judged as mere hype or justified true acclaim.\nBecause of the logistic curve nature of technology adoption, it is difficult to see in the early stages whether the hype is excessive.\n\nThe two errors commonly committed in the early stages of a technology's development are:\nSimilarly, in the later stages, the opposite mistakes can be made relating to the possibilities of technology maturity and market saturation.\n\nThe technology adoption life cycle typically occurs in an S curve, as modelled in diffusion of innovations theory. This is because customers respond to new products in different ways. Diffusion of innovations theory, pioneered by Everett Rogers, posits that people have different levels of readiness for adopting new innovations and that the characteristics of a product affect overall adoption. Rogers classified individuals into five groups: innovators, early adopters, early majority, late majority, and laggards. In terms of the S curve, innovators occupy 2.5%, early adopters 13.5%, early majority 34%, late majority 34%, and laggards 16%.\n\nThe four stages of technology life cycle are as follows:\n\nLarge corporations develop technology for their own benefit and not with the objective of licensing. The tendency to license out technology only appears when there is a threat to the life of the TLC (business gain) as discussed later.\n\nThere are always smaller firms (SMEs) who are inadequately situated to finance the development of innovative R&D in the post-research and early technology phases. By sharing incipient technology under certain conditions, substantial risk financing can come from third parties. This is a form of quasi-licensing which takes different formats. Even large corporates may not wish to bear all costs of development in areas of significant and high risk (e.g. aircraft development) and may seek means of spreading it to the stage that proof-of-concept is obtained.\n\nIn the case of small and medium firms, entities such as venture capitalists or business angels, can enter the scene and help to materialize technologies. Venture capitalists accept both the costs and uncertainties of R&D, and that of market acceptance, in reward for high returns when the technology proves itself. Apart from finance, they may provide networking, management and marketing support. Venture capital connotes financial as well as human capital.\n\nLarger firms may opt for Joint R&D or work in a consortium for the early phase of development. Such vehicles are called strategic alliances – strategic partnerships.\nWith both venture capital funding and strategic (research) alliances, when business gains begin to neutralize development costs (the TLC crosses the X-axis), the ownership of the technology starts to undergo change.\n\nIn the case of smaller firms, venture capitalists help clients enter the stock market for obtaining substantially larger funds for development, maturation of technology, product promotion and to meet marketing costs. A major route is through initial public offering (IPO) which invites risk funding by the public for potential high gain. At the same time, the IPOs enable venture capitalists to attempt to recover expenditures already incurred by them through part sale of the stock pre-allotted to them (subsequent to the listing of the stock on the stock exchange). When the IPO is fully subscribed, the assisted enterprise becomes a corporation and can more easily obtain bank loans, etc. if needed.\n\nStrategic alliance partners, allied on research, pursue separate paths of development with the incipient technology of common origin but pool their accomplishments through instruments such as 'cross-licensing'. Generally, contractual provisions among the members of the consortium allow a member to exercise the option of independent pursuit after joint consultation; in which case the optee owns all subsequent development.\n\nThe ascent stage of the technology usually refers to some point above Point A in the TLC diagram but actually it commences when the R&D portion of the TLC curve inflects (only that the cashflow is negative and unremunerative to Point A). The ascent is the strongest phase of the TLC because it is here that the technology is superior to alternatives and can command premium profit or gain. The slope and duration of the ascent depends on competing technologies entering the domain, although they may not be \"as successful\" in that period. Strongly patented technology extends the duration period.\n\nThe TLC begins to flatten out (the region shown as M) when equivalent or challenging technologies come into the competitive space and begin to eat away marketshare.\n\nTill this stage is reached, the technology-owning firm would tend to exclusively enjoy its profitability, preferring \"not\" to license it. If an overseas opportunity does present itself, the firm would prefer to set up a controlled subsidiary rather than license a third party.\n\nThe maturity phase of the technology is a period of stable and remunerative income but its competitive viability can persist over the larger timeframe marked by its 'vital life'. However, there may be a tendency to license out the technology to third parties during this stage to lower risk of decline in profitability (or competitivity) and to expand financial opportunity.\n\nThe exercise of this option is, generally, inferior to seeking participatory exploitation; in other words, engagement in joint venture, typically in regions where the technology would be in the \"ascent phase\",as say, a developing country. In addition to providing financial opportunity it allows the technology-owner a degree of control over its use. Gain flows from the two streams of investment-based and royalty incomes. Further, the vital life of the technology is enhanced in such strategy.\n\nAfter reaching a point such as D in the above diagram, the earnings from the technology begin to decline rather rapidly. To prolong the life cycle, owners of technology might try to license it out at some point L when it can still be attractive to firms in other markets. This, then, traces the lengthening path, LL'. Further, since the decline is the result of competing rising technologies in this space, licenses may be attracted to the general lower cost of the older technology (than what prevailed during its vital life).\n\nLicenses obtained in this phase are 'straight licenses'. They are free of direct control from the owner of the technology (as would otherwise apply, say, in the case of a joint-venture). Further, there may be fewer restrictions placed on the licensee in the employment of the technology.\n\nThe utility, viability, and thus the cost of straight-licenses depends on the estimated 'balance life' of the technology. For instance, should the key patent on the technology have expired, or would expire in a short while, the residual viability of the technology may be limited, although balance life may be governed by other criteria such as knowhow which could have a longer life if properly protected.\n\n\"It is important to note that the license has no way of knowing the stage at which the prime, and competing technologies, are on their TLCs\". It would, of course, be evident to competing licensor firms, and to the originator, from the growth, saturation or decline of the profitability of their operations.\n\nThe license may, however, be able to approximate the stage by vigorously negotiating with the licensor and competitors to determine costs and licensing terms. A lower cost, or easier terms, \"may\" imply a declining technology.\n\nIn any case, access to technology in the decline phase is a large risk that the licensee accepts. (In a joint-venture this risk is substantially reduced by licensor sharing it). Sometimes, financial guarantees from the licensor may work to reduce such risk and can be negotiated.\n\nThere are instances when, even though the technology declines to becoming a technique, it may still contain important knowledge or experience which the licensee firm cannot learn of without help from the originator. This is often the form that \"technical service\" and \"technical assistance\" contracts take (encountered often in developing country contracts). Alternatively, consulting agencies may fill this role.\n\nAccording to the Encyclopedia of Earth, \"In the simplest formulation, innovation can be thought of as being composed of research, development, demonstration, and deployment.\"\n\n\"Technology development cycle\" describes the process of a new technology through the stages of technological maturity:\n\n"}
{"id": "168531", "url": "https://en.wikipedia.org/wiki?curid=168531", "title": "The Age of Plastic", "text": "The Age of Plastic\n\nThe Age of Plastic is the debut studio album by the British new wave duo The Buggles, composed of Trevor Horn and Geoff Downes. The name of the record was conceived from the group's intention of being a \"plastic group\". The album has lyrical themes of nostalgia and anxiety about the possible effects of modern technology. The titular song, \"Living in the Plastic Age\", views the experiences of watching media coverage of the Vietnam War, while \"Kid Dynamo\" follows a child overexposed to media and its resulting effects on him. Described by writers as the first technopop landmark, the record is an electropop new wave album that includes musical elements and influences of disco, punk, progressive rock and 1960s pop music. In a 1979 interview, Downes defined the album as \"science fiction music. It's like modern psychedelic music. It's very futuristic.\" \n\nHorn used pre-dated technology for the album to have sounds unlike what was typical in records that were released at the time. The songs were written by The Buggles between 1977 and 1979, with contributions on several tracks from Bruce Woolley. The backing tracks were recorded at Virgin's Town House in West London, while the vocals were recorded and mixed at Sarm East Studios. Mixing was completed before Christmas 1979.\n\n\"The Age of Plastic\" was released by Island Records in Australia, January 1980, and later in February in the UK. The album's release followed the success of the group's 1979 first single, \"Video Killed the Radio Star\", which reached number 1 on the UK Singles Chart. Most of the songs for the album were written during promotion of the song. Three subsequent singles, \"The Plastic Age\", \"Clean, Clean\" and \"Elstree\", all released in 1980, followed \"Video\", and also charted in the UK, reaching number 16, 38 and 55 respectively. In Europe, \"The Age of Plastic\" reached the Top 20 in Italy and the Top 40 in France, Norway, Sweden and the United Kingdom. In other continents, the album reached the top 40 most played in Japan and was number 83 on the Canadian \"RPM\" albums chart. It has been met with positive critical response, with some critics comparing it to other albums of its genre. There have been two reissues of the album, in 2000 and in 2010. A September 2010 performance of the album by the Buggles, a gig live at the \"Ladbroke Grove's Supperclub\" in Notting Hill, London, marked the first time the group performed it in its entirety.\n\nIn 2016, \"Paste\" magazine called \"The Age of Plastic\" the 45th best new wave album of all time.\n\nGeoff Downes formed The Buggles in 1977 in Wimbledon, South West London with Trevor Horn and Bruce Woolley. The trio had done rough demos of early compositions such as \"Video Killed the Radio Star\", \"Clean, Clean\", and \"On TV\", a track later included on their second album \"Adventures in Modern Recording\". Talking about the formation of the Buggles, Downes said about the demos: \nThe Buggles were signed to Island Records, who gave Horn and Downes recording and publishing contracts, and started recording their upcoming first studio album in the first half of 1979. Although Woolley was originally intended to be the band's lead vocalist, he left the group during the sessions to form his own band, The Camera Club, who also did versions of \"Clean, Clean\" and \"Video\" that appeared on their album \"English Garden\". When \"Video Killed the Radio Star\" was a huge commercial success, they had realized the problem that they had not finished an album's worth of material yet, so they wrote more during the promotion of the single, while in airport lounges, dressing rooms, rehearsal rooms and studios.\n\n\"The Age of Plastic\" had a budget of £60,000. Hugh Padgham recorded the backing tracks at Virgin's Town House in West London, due to Sarm East's very small size and Horn wanting to record real drums. The Buggles went to London’s Wardour Street to gain the attention of two females to appear on the album. The mixing and Horn's vocal recording were later done at Sarm East Studios, and mixing was finished before Christmas 1979 for a new year's release of the album. Sarm East mixer Gary Langan used a 40-input Trident TSM console to record and mix the album, which was housed inside the same control room as two Studer A80 24-track machines and outboard gear that included an EMT 140 echo plate, Eventide digital delay, Eventide phaser, Marshall Time Modulator, Kepex noise gates, Urei and Orban EQs, and Urei 1176, Dbx 160, and UA LA2 and LA3 compressors.\n\nThe vocals of the album were recorded at Sarm East to a click track using a Roland TR-808 drum machine, and other various machines and boxes synced up the tracks. As Langan recalls: \"In those days of relatively limited technology we again had to push what we had to the limit... If, for instance, something required an effect, whether it be tape delay or phasing or some big, delayed reverb, the art was to get that effect right and record it... It all had to be done and then, as I said, it would influence the next process.\" Langan has noted that balancing the backing vocals in the songs from the album was a major problem, due to no available terabytes of storage. He stated: \"We'd make it as clean as we possibly could, bounce that down to two tracks and then we'd erase.\"\n\nUnable to make the album sound like what was typical of other records released in the late 1970s, as well as finding it boring, Horn \"figured that if I couldn't get records to sound like Elton John, which I couldn't because I couldn't figure out how they did it, then whatever I could do, I'd better exaggerate it.\" He had also wanted to \"perverse things with sound, except that in 1978 and 1979, none of the equipment which would later allow me to do that was available. So I had to pre-date that technology by finding my own ways of achieving certain sounds.\" \"Audio\" noted the album's sound to be reminiscent of duo Godley & Creme's debut album \"Consequences\".\n\nWriters have labeled \"The Age of Plastic\" as the first landmark of another electropop era. In his book \"Are We Not New Wave?: Modern Pop at the Turn of the 1980s\", Theo Cateforis wrote that the album's title and the songs \"I Love You (Miss Robot)\" and \"Astroboy\" \"picture the arrival of the 1980s as a novelty era of playful futurism\". In a 1979 interview, Downes defined the album as \"science fiction music. It's like modern psychedelic music. It's very futuristic.\" Writers have described it as a mixture of synthpop and new wave music, with elements of disco, punk, progressive rock and pop music from the 1960s. The music on the album was also influenced by groups such as 10cc, ELO and Kraftwerk. \nJournalists noted the tracks' instrumentations of guitars, bass guitar, drums, vocoded, robotic and female vocals, and synthesizers used to emulate orchestral instruments, and well as compositional elements of a variety of complex builds. Downes said that he used five synthesizers in making \"The Age of Plastic\", which were used to \"fake up things and to provide effects we won’t use them in the manner that somebody like John Foxx does.\" According to Horn: \"We used about three different drummers including one <nowiki>[</nowiki>Richard James Burgess<nowiki>]</nowiki> from Landscape and Johnny Richardson from The Rubettes, who’s really good. We also used the occasional session guitarist to play various bits and there were three or four girl singers involved. Apart from that, we did everything ourselves.\" Downes claimed of using George Shearing's trick of doubling melody lines in block chords very heavily on some of the songs.\n\n\"The Age of Plastic\" is a tragicomic concept album with lyrical themes of intense nostalgia and anxiety about the possible effects of modern technology. The lyrics, which were written by Trevor Horn, were inspired by the works of J.G. Ballard. The Buggles claimed they were necessarily a \"plastic group\" to meet the needs of a plastic age, which was why their debut album was called \"The Age of Plastic\". Downes said that the lyrics were \"trying to make cynical comments on a number of issues.\" Eight tracks are included on \"The Age of Plastic\": \"Living in the Plastic Age\", \"Video Killed the Radio Star\", \"Kid Dynamo\", \"I Love You (Miss Robot)\", \"Clean, Clean\", \"Elstree\", \"Astroboy (And the Proles on Parade)\", and \"Johnny (on the Monorail)\". The album's lyrical concept was compared by \"Orange Coast\" magazine to that of the works of Canadian progressive rock band Klaatu.\n\n\"The Age of Plastic\" starts with the title track \"The Plastic Age,\" which, according to Downes, is about a person's view of plastic experiences from watching and reading news reports on the Vietnam War. \"Video Killed the Radio Star,\" the second track, refers to a period of technological change in the 1960s, the desire to remember the past and the disappointment that children of the current generation would not appreciate the past. The fast-paced third song, \"Kid Dynamo,\" is about the effects of media on a futuristic kid of the 1980s. \"I Love You (Miss Robot)\" is the album's fourth track, that Downes said it talks about \"being on the road and making love to someone you don't really like, while all the time you're wanting to phone someone who's a long way off.\" Wave Maker Magazine viewed the song as \"a darkly soothing, bass guitar-driven ballad which brings us back into cyberpunk country.\"\n\n\"Clean, Clean\" is the album's fifth track, and follows the story of a young boy who grows out of being a gangster, and, despite not willing to do so, he will at least try to keep the fighting clean. Wave Maker Magazine found \"Elstree,\" the album's sixth song, as lyrically similar to \"Video Killed the Radio Star,\" as it follows \"a failed actor taking up a more regular position behind the scenes and looking back at his life in regret.\" The slow-tempo ballad \"Astroboy (And the Proles on Parade)\", according to Wave Maker, \"once again revisits cyberpunk with a much lighter vibe, although the keyboards do occasionally border darker realms, expecially with the post-chorus hook,\" and the album closer \"Johnny on the Monorail\" has a \"pop atmosphere\" that \"better suits the flow of the rest of the album.\"\n\n\"The Age of Plastic\" was first released in Australia on 10 January 1980, and in the United Kingdom on 4 February the same year. In 2000, as part of the \"Island Remasters\" series, the album was reissued with three bonus tracks, \"Technopop\", \"Island\", and \"Johnny on the Monorail (A Very Different Version)\". The album was remastered and re-released again on 24 February 2010 in Japan. The new edition included 9 additional tracks, three of which were from the 2000 re-release album. Initially, songs from \"The Age of Plastic\" were played on English radio stations from 31 December 1979, and two advertisements of the album were also released. Domestically, \"The Age of Plastic\" appeared on the UK Album Charts for six weeks, reaching 27 on the chart. In Nordic countries, the record debuted at number 32 in Norway, eventually reaching number 23, and peaked at number 24 in Sweden. It also reached the number 15 spot on the French Albums Chart, and number 35 in Japan. The album's 2010 reissue briefly appeared at number 225 in Japan.\n\nFour singles were released in support of the record, with the second track \"Video Killed the Radio Star\" being released as the album's leading single. The LP version of the song was included on the 2010 reissue of the album, with the song \"Kid Dynamo\" as its B-side. The single was commercially successful, topping the record charts in 16 countries, including the UK Singles Chart. The album's title track \"Living in the Plastic Age\" was released as the second single. The single version was later included in the 2010 reissue. The fourth track \"Clean, Clean\" was released as the third single from the album, while \"Elstree\" was the album's fourth and final single. All of the singles had chart success in the UK, with \"The Plastic Age\" and \"Clean, Clean\" gaining chart success on international level. A single and special DJ version of \"Elstree\" also appeared on the 2010 reissue of the album, as well as a 12\" version of \"Clean, Clean\".\n\nOn 28 September 2010, The Buggles reunited to play their first full-length live concert. The event was billed as \"The Lost Gig\" and took place at \"Ladbroke Grove's Supperclub\" in Notting Hill, London. It was a fundraiser with all earnings going to the Royal Hospital for Neuro-disability. With the exception of \"Video Killed the Radio Star\" and \"The Plastic Age\", which the band had previously played together, \"The Lost Gig\" saw the first live performances of all of the remaining songs from \"The Age of Plastic\".\n\nMost reviews of \"The Age of Plastic\" have been approving, with some critics comparing it to other albums of its genre. \"Melody Maker\" noted that the album is \"all jerky twitchings and absurdly inflated post-punk melodrama\" and named it as \"essential\". \"The Canberra Times\"'s Keith Gosman found the production excellent and said the album sounded \"crisp as fresh dollar bills\". \"The Encyclopedia of Science Fiction\" described the LP as \"one of the best examples of the decade's characteristically disposable pop\", while \"Spin\" named it one of the \"8 Essentials of Post-Kraftwerk Pop\".\n\nIn a retrospective review, Jeri Montesano from AllMusic gave the album 4.5 out of 5 stars and described it as \"a fun record that doesn't need to be taken too seriously\". He further said that \"it would be difficult to find a record from this era that sounds half as good. Pop rarely reaches these heights.\" While reviewing the Buggles' second album, the same author stated that both albums \"still sound fresh\" compared to 1990s pop music. An Amazon.com editor named Grant Alden also compared the album to 1990s pop, and labeled the group as \"Part of the early-1980s great explosion of pop music [...] to have any real impact\". \"Trouser Press\" called both albums \"technically stunning, reasonably catchy and crashingly hollow.\"\n\n\"Smash Hits\" rated the album 8 out of 10 and called it \"quite human and therefore the most enjoyable of the lot\". In a review of the 1999 re-release of the album, Vincent L. of \"Krinein Magazine\" praised the entire record because it contains \"catchy tracks and joyful melodies\". Napster's Nicholas Baker liked the album's composition and concluded \"this LP is not so much a guilty pleasure as an essential point in electropop history.\" \"Metro Pulse\"<nowiki>'</nowiki>s Anthony Nownes found the tunes \"punchy, memorable\" and \"accessible\", concluding his review with \"If all rock records sounded like this—shiny and slick and highly processed—the world would be terrible. But a few Trevor Horns—people who use studio technology the way a curious and playful child uses a room full of fictile toys—are nice to have around.\"\n\nOn the mixed to negative side, Joseph Stannard, in his review for \"Adventures in Modern Recording\", opined \"The Age of Plastic\" \"sounds like unfinished business, a series of good ideas in need of elaboration\". Dave Marsh and John Swenson, writing for \"The Rolling Stone Record Guide\", opined that \"aside from the wonderful 'Video Killed the Radio Star' — perhaps the most successful recent example of a single where the production was catchier than the material\", the album was \"high-tech dreck\". Betty Page from \"Sounds\" commented that the group \"stretches uncomfortably out into the long playing medium like a skein of well-chewed bubblegum.\" In a review of the 1999 reissue of the album, Richard Wallace of \"Daily Mirror\" wrote that the record \"shows how [The Buggles] pioneered the synth-led nonsense which fused much of the decade's pop, but had little creative imagination. That bombastic electro-sound became [Trevor] Horn's trademark as a producer. Skip it.\" Alexis Petridis of \"The Guardian\" has been the harshest on the LP, calling it \"awful beyond measure\" and \"everything bad that people say about the music industry: it's wasteful, it's stupid, it has no interest in actual music.\"\n\nPopular French bands such as Justice, Daft Punk and Phoenix have been known to include influences of \"The Age of Plastic\". Justice said that they were \"totally fascinated by The Buggles’ first album [\"The Age of Plastic\"]. It’s full of stuff we like - there’s a bit of electro, a bit of pop, a bit of classical going on there… We like the way they operated too, as an autonomous duo…\"\n\nAll songs written, produced and performed by The Buggles, except \"Video Killed the Radio Star\" and \"Clean, Clean\" written by Horn/Downes/Bruce Woolley.\n\n\n\n\n\nCredits are adapted from the album's liner notes.\nThe Buggles – producers\nAdditional musicians\n\nTechnical personnel\n"}
{"id": "9298332", "url": "https://en.wikipedia.org/wiki?curid=9298332", "title": "The Automated Society", "text": "The Automated Society\n\nIn a diagram from the book \"The Automated Society\", Bloomfield defines the history of humanity beginning over two million years ago and ending over a hundred thousand years in the future. The diagram is base on biological punctuated equilibrium and a parallel cultural evolution. The predictions of the future follow what has happened in the past.\n\nThe diagram of history shows humanity having long periods of stability interrupted with short periods of transition. Humanity is currently experiencing one of those short periods of transition, from a stable agricultural period going through a transitional industrial society leading to a stable automated period. Productivity is the key to the transition; when it is impossible to increase productivity, humanity will have arrived at the automated society.\n\nBefore humanity enters the automated society, we will suffer from two near term trends. One trend is the relentlessly increasing number of human beings. The second is the relentlessly decreasing number of jobs. Jobs will be eliminated by substituting men with machines. An example of this is agriculture where once in the peasant society there were ninety percent of the people working as farmers. With the introduction of tractors, only three percent of Americans work on the farm.\n\nOnce the problems of the lack of jobs, increased pollution and fewer natural resources are solved and the food factory put in operation, there should be a reconciliation of man to automation. In the automated society, men will live in affluence along with a total control of population. The one place where man will have an unlimited future will be in space. Space can provide for an unlimited increase in population and potentially the introduction of new human biological species.\n\n\n"}
{"id": "50177657", "url": "https://en.wikipedia.org/wiki?curid=50177657", "title": "The Industries of the Future", "text": "The Industries of the Future\n\nThe Industries of the Future is a 2016 non-fiction book written by Alec Ross, an American technology policy expert and the former Senior Advisor for Innovation to Secretary of State Hillary Clinton during her time as Secretary of State. Ross is also a senior fellow at Columbia University, a former night-shift janitor, and a Baltimore teacher. Ross launched a campaign for the Governor of Maryland in 2017. The book explores the forces that will change the world in robotics, genetics, digital currency, coding and big data.\n\nThe Industries of the Future by Senior Policy Advisor Alec Ross explores the geopolitical, cultural and generational changes that will be driven by the key industries over the next twenty years. Ross is a Distinguished Visiting Fellow at Johns Hopkins University and was the Senior Advisor for Innovation to Secretary of State Hillary Clinton. During his time as Senior Advisor for Innovation he visited forty-one countries looking at the technological advances. He has been a guest lecturer at a number of institutions including the United Nations, University of Oxford, Harvard Law School, and Stanford Business School. Ross started his career as a teacher through Teach for America and in 2000 he co-founded a technology-focused nonprofit organization called One Economy.\n\nThe book explores several industries including robotics, genetics, coding and big data. Ross explores how advances in robotics and life sciences will change the way we live—robots, artificial intelligence and machine learning will have impact on our lives. According to Ross, dramatic advances in life sciences will increase our life expectancy—but not all will benefit from such changes. Ross spends time exploring \"Code\" and how the codefication of money and also weapons (computer security) will both benefit and potentially disrupt our international economies. Ross also looks at how data will be \"the raw material of the information age.\"\n\nRoss also focuses on globalization and geopolitical economics. He explores how competitiveness and how societies, families and individuals will need to thrive. Ross gives attention to the importance of women stating that \"the states and societies that do the most for women are those that will be best positioned to compete and succeed in the industries of the future.\" The book also touches on how to prepare children for \"success in a world of increasing change and competition.\"\n\nRoss discusses the shift of robotics from being manual and repetitive to cognitive and non-repetitive. He believes that breakthroughs in mathematical modeling and cloud robotics (machine learning and Artificial Intelligence) make robotics acceptable. In the book Ross describes how other cultures have different reactions to robotics and he uses Japan's use of robotics in elder-care as an example. He also expects that less developed countries may be able to leapfrog technologies in robotics much like they did with cell and mobile technologies.\n\nAccording to Ross, the last trillion dollar industry was created out of computer code; the next trillion dollar industry will be created out of genome code. In the book Ross describes how genome code is already being used to fix humans from curing cancer to hacking the brain to growing organs. He also describes the difference between the United States investment in genome research with that of China.\n\nRoss then turns to the \"code-ification\" of money, markets and trust. He describes the transition from cash to mobile and online banking. He also discusses the sharing economy from eBay to AirBnB and then gives an overview of BitCoin and blockchain technology. Ross also focuses on cybersecurity and the weaponization of code with a focus on a move from cold war to \"code war.\" Ross states that he expects the total market size of the cyberindustry to reach $175 billion by the end of 2017.\n\nAlec Ross has said that he intended to give a balanced point of view with the book that it is neither a Utopian or Dystopian vision of the future which is why he opened the book with the struggles he witnessed growing up in West Virginia. On writing the book Ross said that he knows his parents would have wanted a book like this in the sixties that would describe what globalization would do and he wished that he had a book like this when he graduated from college that would have explored the Internet and digitization on the economy.\n\nThe editors for the book were Jonathan Karp and Jonathan Cox of Simon & Schuster.\n\n\"The Industries of the Future\" has received mainly positive reviews from the likes of \"Forbes\", \"New York Journal of Books\", and \"Financial Times\". \"Forbes\" contributor Peter Decherney said the book \"reads like a portable TED conference at which you've been seated next to the smartest guy in the room.\" The book was also listed on the Forbes list—\"16 New Books for Leaders to Begin in 2016\". Tara D. Sonenshine in the \"New York Journal of Books\" called the book a good place to start \"if you want to know how to survive and thrive in the fast-paced world of today and how to anticipate the opportunities of tomorrow's information age.\" Sonenshine also called out the book for focusing on women and multiculturalism. In an article titled \"Is predicting the future futile or necessary?\" by Stephen Cave in the \"Financial Times\" is more critical, saying that Ross focuses on industries with already considerable coverage and investment but Cave points out that \"rarely can the future be predicted by extending current trajectories.\"\n\nThe following trends are covered in the book:\n"}
{"id": "896750", "url": "https://en.wikipedia.org/wiki?curid=896750", "title": "Xenoarchaeology", "text": "Xenoarchaeology\n\nXenoarchaeology, branch of xenology dealing with extraterrestrial cultures, is a hypothetical form of archaeology that exists mainly in works of science fiction. The field is concerned with the study of material remains to reconstruct and interpret past life-ways of alien civilizations. Xenoarchaeology is not currently practiced by mainstream archaeologists due to the current lack of any material for the discipline to study.\n\nThe name derives from Greek \"xenos\" (ξένος) which means 'stranger, alien', and archaeology 'study of ancients'.\n\nXenoarchaeology is sometimes called \"astroarchaeology\" or \"exoarchaeology\", although some would argue that the prefix exo- would be more correctly applied to the study of human activities in a space environment.\n\nOther names for xenoarchaeology, or specialised fields of interest, include Probe SETI (Search for Extra-Terrestrial Intelligence), extraterrestrial archaeology, space archaeology, SETA (Search for Extra-Terrestrial Artifacts), Dysonian SETI, Planetary SETI, SETT (Search for Extra-Terrestrial Technology), SETV (Search for Extra-Terrestrial Visitation), extraterrestrial anthropology, areoarchaeology and selenoarchaeology.\n\nIt is arguably the case that, due to the immense distances between stars, any evidence we discover of extraterrestrial intelligence, whether it be an artifact or an electromagnetic signal, may come from a long-vanished civilization. Thus the entire SETI project can be seen as a form of archaeology. Additionally, due to the extreme age of the universe, there may be a reasonable expectation for astrobiology research to produce evidence of extinct alien life prior to the discovery of alien life itself.\n\nThe study of alien cultures might offer us glimpses into our own species' past or future development.\n\nVicky Walsh argued for the existence of \"exo-artifacts\" using the principle of mediocrity and the Drake equation. She proposed that a theoretical and speculative field of archaeology be established in order to test outlandish claims, and to prepare for a time when undeniably extraterrestrial artifacts needed to be analysed. \"If it is possible to construct an abstract archaeology that can be tested and refined on earth and then applied to areas beyond our planet, then the claims for ETI remains on the moon and Mars may really be evaluated in light of established archaeological theory and analysis\".\n\nBen McGee similarly proposed the creation of a set of interdisciplinary, proactive xenoarchaeological guidelines, arguing that identifying suspected artifacts of astrobiology is all that is required to justify establishing a methodology for xenoarchaeology. He emphasized the necessity of proactive xenoarchaeological work in order to avoid future bias, mischaracterization, and information mismanagement, and he cites three scenarios under which such a methodology or set of guidelines would be useful, those being \"remote sensing\" of a potential xenoarchaeologial artifact, encountering an artifact during \"human exploration,\" and \"terrestrial interception\" of an artifact.\n\nGreg Fewer has argued that archaeological techniques should be used to evaluate alleged UFO landing or crash sites, such as Roswell.\n\nThe origins of the field have been traced to theories about a hypothetical Martian civilization based on observations of what were perceived as canals on Mars. These theories, of which Percival Lowell was the most famous exponent, were apparently inspired by a mistranslation of a quote by Giovanni Schiaparelli.\n\nThe 1997 Theoretical Archaeology Group conference featured a session on \"archaeology and science fiction\".\n\nThe 2004 annual meeting of the American Anthropological Association featured a session \"Anthropology, Archaeology and Interstellar Communication\".\n\nPlanetary SETI is concerned with the search for extraterrestrial structures on the surface of bodies in the Solar System. Claims for evidence of extraterrestrial artifacts can be divided into three groups, the Moon, Mars, and the other planets and their satellites.\n\nExamples of sites of interest include the \"bridge\" sighted in the Mare Crisium in 1953, and the \"Blair Cuspids\", \"an unusual arrangement of seven spirelike objects of varying heights\" at the western edge of the \"Mare Tranquillitatis\", photographed by in Lunar Orbiter 2 on November 20, 1966. In 2006, Ian Crawford proposed that a search for alien artifacts be conducted on the Moon.\n\nPercival Lowell's mistaken identification of Martian canals was an early attempt to detect and study an alien culture from its supposed physical remains. More recently, there was interest in the supposed Face on Mars.\n\nThe Society for Planetary SETI Research is a loose organization of researchers interested in this field. The organization does not endorse any particular conclusions drawn by its members on particular sites.\n\nA great deal of research and writing has been done, and some searches conducted for extraterrestrial probes in the Solar System. This followed the work of Ronald N. Bracewell.\n\nRobert Freitas, Christopher Rose and Gregory Wright have argued that interstellar probes can be a more energy-efficient means of communication than electromagnetic broadcasts.\n\nIf so, a solarcentric Search for Extraterrestrial Artifacts (SETA) would seem to be favored over the more traditional radio or optical searches.\nRobert A. Freitas coined the Term SETA in the 1980s.\n\nOn the basis that the Earth-Moon or Sun-Earth libration orbits might constitute convenient parking places for automated extraterrestrial probes, unsuccessful searches were conducted by Freitas and Valdes.\n\nIn a 1960 paper, Freeman Dyson proposed the idea of a Dyson sphere, a type of extraterrestrial artifact able to be searched for and studied at interstellar distances. Following that paper, several searches have been conducted.\n\nIn a 2005 paper, Luc Arnold proposed a means of detecting smaller, though still mega-scale, artifacts from their distinctive transit light curve signature. \"(see Astroengineering)\".\n\nA subculture of enthusiasts studies purported structures on the Moon or Mars. These controversial \"structures\" (such as the Face on Mars) are not accepted as more than natural features by most scientists, examples of the pareidolia phenomenon.\n\nPalaeocontact or ancient astronaut theories, espoused by Erich von Däniken and others, are further examples of fringe theories. These claim that the Earth was visited in prehistoric times by extraterrestrial beings.\n\nXenoarchaeological themes are common in science fiction. Works about the exploration of enigmatic extraterrestrial artifacts have been satirically categorized as Big Dumb Object stories.\n\nSome of the more prominent examples of xenoarchaeological fiction include Arthur C. Clarke's novel \"Rendezvous with Rama\", H. Beam Piper's short story \"Omnilingual\", and Charles Sheffield's Heritage Universe series.\n\nJack McDevitt's science fiction novels often revolve around human or alien historical and archaeological mysteries.\n\n\"Mass Effect\"'s plot revolves around technologies in the form of structures, transportation, buildings and machines left behind by an extinct alien race.\n\nThe primary setting of \"\" takes place on a massive structure created by an ancient, extinct alien super race. These structures feature prominently in the sequels \"Halo 2\", \"Halo 3\", and \"Halo 4\".\n\nIn the video game \"Borderlands\", and its sequel \"Borderlands 2\", the Atlas corporation started mining the planet Pandora after finding evidence of an ancient material called Eridium. This material was supposedly created by extraterrestrial creatures or is a natural product of long-term planetary production.\n\nIn the video game \"Stellaris\", players usually found extraterrestrial artifacts from a extinct civilization before actually encounter native space creatures or other extraterrestrial civilizations.Such discovery would also raise a \"Exist of extraterrestrial life has been proven\" news event.\n\n\n\n\n\n\n"}
