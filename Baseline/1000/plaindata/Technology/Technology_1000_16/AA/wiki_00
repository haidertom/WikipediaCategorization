{"id": "43993501", "url": "https://en.wikipedia.org/wiki?curid=43993501", "title": "American Boat and Yacht Council", "text": "American Boat and Yacht Council\n\nThe American Boat and Yacht Council is a non-profit organization which sets standards for the safe construction and maintenance of boats in the United States. It is a standards development member of the American National Standards Institute. Founded in 1954, it is currently chaired by Kenneth Weinbrecht of Ocean-Bay Marine Services, Inc.. It sets standards on items such as required electrical cable color coding, size and construction. Some standards, such as bilge pump requirements, are legally binding.\n\n"}
{"id": "54247317", "url": "https://en.wikipedia.org/wiki?curid=54247317", "title": "Appian Corporation", "text": "Appian Corporation\n\nAppian Corporation is a cloud computing company currently headquartered in Reston, Virginia, part of the Dulles Technology Corridor. \n\nAppian sells a Platform as a Service for building enterprise software applications. It competes in the Low-Code Development , Business Process Management , and Case Management markets.\n\nAppian was named the 2018 Washington D.C. area \"Top Workplace\" by The Washington Post.\n\nAppian was founded in 1999 by Matthew Calkins with co-founders Michael Beckley, Marc Wilson, and Robert Kramer. \n\nIn 2001, Appian developed Army Knowledge Online. At the time, it was regarded as “the world's largest intranet\".\n\nIn 2004, Appian released Appian Enterprise, its first Business Process Management Suite (BPMS) software platform.\n\nIn 2014, Appian received $37.5 million in secondary investments from New Enterprise Associates.\n\nBy 2016, Appian’s software platform had evolved to include broader aspects of application development with a focus on low-code development.\n\nOn May 25, 2017, Appian became a publicly-traded company, trading under the symbol APPN on the NASDAQ Global Exchange\n\nIn 2018, Appian announced it will move its headquarters to Tysons Corner, Virginia\n"}
{"id": "6936288", "url": "https://en.wikipedia.org/wiki?curid=6936288", "title": "Baseball telecasts technology", "text": "Baseball telecasts technology\n\nThe following is a chronological list of the technological advancements of Major League Baseball television broadcasts:\n\nOn August 26, the first ever Major League Baseball telecast (the Brooklyn Dodgers vs. Cincinnati Reds from Ebbets Field) aired by W2XBS, an experimental station in New York City which would ultimately become WNBC-TV.\n\nRed Barber called the game without the benefit of a monitor and with only two cameras capturing the game. One camera was on Barber and the other was behind the plate. Barber had to guess from which light was on and where it pointed.\n\nIn 1939, baseball games were usually covered by one camera providing a point-of-view along the third base line.\n\n\nOn August 11, 1951, WCBS-TV in New York City televised the first baseball game (in which the Boston Braves beat the Brooklyn Dodgers by the score of 8-1) in color.\n\nOn October 3 of that year, NBC aired the first coast-to-coast baseball telecast as the Brooklyn Dodgers were beaten by the New York Giants in the final game of a playoff series by the score of 5-4 (off Bobby Thomson's now-legendary home run).\n\n\n1955 marked the first time that the World Series was televised in color.\n\n\n\nOn July 23, 1962, Major League Baseball had its first satellite telecast (via Telstar Communications). The telecast included portion of a contest between the Chicago Cubs vs. the Philadelphia Phillies from Wrigley Field with Jack Brickhouse commentating on WGN-TV.\n\nBy 1969, the usage of chroma key (in which the commentators would open a telecast by standing in front of a greenscreen composite of the stadiums' crowds) became a common practice for baseball telecasts.\n\n\n\nIn the bottom of the 12th inning of Game 6 of the 1975 World Series at Boston's Fenway Park, Red Sox catcher Carlton Fisk was facing Cincinnati Reds pitcher Pat Darcy. Fisk then hit a pitch down the left field line that appeared to be heading to foul territory. The enduring image of Fisk jumping and waving the ball fair as he made his way to first base is arguably one of baseball's greatest moments. The ball struck the foul pole, giving the Red Sox a 7-6 win and forcing a seventh and deciding game of the Fall Classic. During this time, cameramen covering baseball games were instructed to follow the flight of the ball; reportedly, Fisk's reaction was only being recorded because NBC cameraman Lou Gerard, positioned inside Fenway's scoreboard at the base of the left-field Green Monster wall, had become distracted by a large rat. This play was perhaps the most important catalyst in getting camera operators to focus most of their attention on the players themselves.\n\nOn July 6, 1983, NBC televised the All-Star Game out of Chicago's Comiskey Park. During the telecast, special guest analyst, Don Sutton helped introduce NBC's new pitching tracking device dubbed The NBC Tracer. \"The NBC Tracer\" was a stroboscopic comet tail showing the path of a pitch to the catcher's glove. For instance, \"The NBC Tracer\" helped track a Dave Stieb curveball among others.\n\nIn 1985, NBC's telecast of the All-Star Game out of the Metrodome in Minnesota was the first program to be broadcast in stereo by a television network.\n\n\nFor the 1987 World Series between the Minnesota Twins and St. Louis Cardinals, ABC utilized 12 cameras and nine tape machines. This includes cameras positioned down the left field line, on the roof of the Metrodome, and high above third base.\n\nIn 1990, CBS took over from both ABC and NBC as Major League Baseball's national, over-the-air television provider. They in the process brought along their telestration technology that they dubbed CBS Chalkboard. \"CBS Chalkboard\" made its debut eight years earlier during CBS' coverage of Super Bowl XVI.\n\nFor CBS' coverage of the 1992 All-Star Game, they introduced Basecam, a lipstick-size camera, inside first base.\n\nDuring CBS' coverage of the 1993 World Series, umpires were upset with the overhead replays being televised by CBS. Dave Phillips, the crew chief, said just prior to Game 2 that the umpires want \"CBS to be fair with their approach.\"\n\nRick Gentile, the senior vice president for production of CBS Sports, said that Richie Phillips, the lawyer for the Major League Umpires Association, tried to call the broadcast booth during Saturday's game, but the call was not put through. Richie Phillips apparently was upset when Dave Phillips called the Philadelphia Phillies' Ricky Jordan out on strikes in the fourth inning, and a replay showed the pitch to be about 6 inches outside.\n\nNational League President Bill White, while using a CBS headset in the broadcast booth during Game 1, was overheard telling Gentile and the producer Bob Dekas: \n\n\n\nOn July 8, 1997, Fox televised its first ever All-Star Game (out of Jacobs Field in Cleveland). For this particular game, Fox introduced \"Catcher-Cam\" in which a camera was affixed to the catchers' masks in order to provide unique perspectives of the action around home plate. Catcher-Cam soon would become a regular fixture in Fox's baseball broadcasts.\n\nIn addition to Catcher-Cam, other innovations (some of which have received more acclaim than others) that Fox has provided for baseball telecasts have been:\n\nFor a Saturday afternoon telecast of a Los Angeles Dodgers/Chicago Cubs game at Wrigley Field on August 26, 2000, Fox aired a special \"Turn Back the Clock\" broadcast to commemorate the 61st anniversary of the first televised baseball game. The broadcast started with a re-creation of the television technology of 1939, with play-by-play announcer Joe Buck working alone with a single microphone, a single black-and-white camera, and no graphics; then, each subsequent half-inning would see the broadcast \"jump ahead in time\" to a later era, showing the evolving technologies and presentation of network baseball coverage through the years.\n\n\n\nIn October 2002, Fox televised the first ever World Series to be shown in high definition.\n\n\n\nStarting in 2004, some TBS telecasts (mostly Fridays or Saturdays) became more enhanced. The network decided to call it Braves TBS Xtra. Enhancements included catcher cam, \"Xtra Motion\", which featured the type of pitch and movement, also \"leadOff Line\". It would also show features with inside access to players.\n\nIn October 2004, Fox started airing all Major League Baseball postseason broadcasts (including the League Championship Series and World Series) in high definition. Fox also started airing the Major League Baseball All-Star Game in HD the following year. At the same time, the FoxBox and graphics are upgraded.\n\n\n\nFor their 2007 Division Series coverage, TBS debuted various new looks, such as the first live online views from cameras in dugouts and ones focused on pitchers. TBS also introduced a graphic that creates sort of a rainbow to trace the arc of pitches on game replays. The graphic was superimposed in the studio so analysts like Cal Ripken, Jr. for instance, could take virtual cuts at pitches thrown in games.\n\nDuring their 2009 playoff coverage, TBS displays their \"PitchTrax\" graphic full-time during at-bats (with the center field camera only) during the high-definition version of the broadcast in the extreme right-hand corner of the screen.\n\nMeanwhile, for their own 2009 playoff coverage, Fox announced that they would occasionally include this stat on replays: Speed of pitches as they leave pitchers' hands as well as their speed when they cross home plate.\n\n\nWith the start of the 2011 postseason, TBS planned to introduce the following\n\nThe screen on TBS's standard definition feed now airs a letterboxed version of the native HD feed to match Fox's default widescreen SD presentation, allowing the right side pitch tracking graphic to be seen by SD viewers.\n\nFor the 2011 World Series, Fox debuted infrared technology that's designed to pinpoint heat made by a ball making contact — with, say, bats, face masks, players' bodies — and mark the spot for viewers by making it glow. During Game 1, Fox used \"Hot Spot\" to show that a batted ball was fouled off Texas Rangers batter Adrián Beltré's foot.\n\nFox's 2012 World Series coverage would include a camera whose replays could generate as many as 20,000 frames per second, the most ever seen on Fox—and up from about 60 frames per second on regular replays. The camera would allow viewers \"to see the ball compress\" when batted, similar to how cameras now show golf balls getting compressed when struck. The technology for the camera originated with the U.S. military looking at replays of missile impacts.\n\n\n"}
{"id": "18983683", "url": "https://en.wikipedia.org/wiki?curid=18983683", "title": "CBEFF", "text": "CBEFF\n\nCBEFF (Common Biometric Exchange Formats Framework) was developed from 1999 to 2000 by the CBEFF Development Team (NIST) and the BioAPI Consortium. This standard provides the ability for different biometric devices and applications to exchange biometric information between system components efficiently. In order to support biometric technologies in a common way the CBEFF structure describes a set of necessary data elements.\n\nThe CBEFF Basic Structure consists of a single SBH (Standard Biometric Header) followed by a BSMB (Biometric Specific Memory Block) and an optional SB (Signature Block).\n\nThe CBEFF Nested Structure consists of a Root Header, optional Sub-Headers, CBEFF Basic Structure(s), and an optional SB (Signature Block).\n\n"}
{"id": "2771086", "url": "https://en.wikipedia.org/wiki?curid=2771086", "title": "Change machine", "text": "Change machine\n\nA change machine is a vending machine that accepts large denominations of currency and returns an equal amount of currency in smaller bills or coins. Typically these machines are used to provide coins in exchange for paper currency, in which case they are also often known as bill changers.\n\nIn the US, these devices are typically seen in the vicinity of machines that will not accept paper currency. This can be in a parking facility that has parking meters, in laundromats, or near vending machines that lack bill validators and don't accept paper currency.\n\nBefore the advent of \"coinless\" slot machines, casinos would sometimes have change machines that would accept paper currency and return coins or tokens that could be used in the machines. A similar arrangement has often been found at video arcades.\n\nIn some cases, a machine may subtract a small amount (e.g. 5 cents) as a surcharge for the transaction.\n\n"}
{"id": "1446835", "url": "https://en.wikipedia.org/wiki?curid=1446835", "title": "Communication with extraterrestrial intelligence", "text": "Communication with extraterrestrial intelligence\n\nCommunication with extraterrestrial intelligence (a.k.a. CETI) is a branch of the search for extraterrestrial intelligence that focuses on composing and deciphering interstellar messages that theoretically, could be understood by another technological civilization. This field of study once was known as \"exosemiotics\". The best-known CETI experiment of its kind was the 1974 Arecibo message composed by Frank Drake.\n\nThere are multiple independent organizations and individuals engaged in CETI research; the generic application of abbreviations CETI and SETI (search for extraterrestrial intelligence) in this article should not be taken as referring to any particular organization (such as the SETI Institute).\n\nCETI research has focused on four broad areas: mathematical languages, pictorial systems such as the Arecibo message, algorithmic communication systems (ACETI), and computational approaches to detecting and deciphering \"natural\" language communication. There remain many undeciphered writing systems in human communication, such as Linear A, discovered by archeologists. Much of the research effort is directed at how to overcome similar problems of decipherment that arise in many scenarios of interplanetary communication.\n\nOn 13 February 2015, scientists (including Douglas Vakoch, David Grinspoon, Seth Shostak, and David Brin) at an annual meeting of the American Association for the Advancement of Science, discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the cosmos was a good idea. That same week, a statement was released, signed by many in the SETI community, that a \"worldwide scientific, political, and humanitarian discussion must occur before any message is sent\". On 28 March 2015, a related essay was written by Seth Shostak and published in \"The New York Times\".\n\nIn the 19th century there were many books and articles about the possible inhabitants of other planets. Many people believed that intelligent beings might live on the Moon, Mars, and Venus.\n\nSince travel to other planets was not possible at that time, some people suggested ways to signal the extraterrestrials even before radio was discovered. Carl Friedrich Gauss is often credited with an 1820 proposal suggested that a giant triangle and three squares, the Pythagoras, could be drawn on the Siberian tundra. The outlines of the shapes would have been ten-mile-wide strips of pine forest, the interiors could be rye or wheat. Joseph Johann Littrow proposed in 1819 to use the Sahara as a sort of blackboard. Giant trenches several hundred yards wide could delineate twenty-mile-wide shapes. Then the trenches would be filled with water, and then enough kerosene could be poured on top of the water to burn for six hours. Using this method, a different signal could be sent every night.\n\nMeanwhile, other astronomers were looking for signs of life on other planets. In 1822, Franz von Gruithuisen thought he saw a giant city and evidence of agriculture on the moon, but astronomers using more powerful instruments refuted his claims. Gruithuisen also believed he saw evidence of life on Venus. Ashen light had been observed on Venus, and he postulated that it was caused by a great fire festival put on by the inhabitants to celebrate their new emperor. Later he revised his position, stating that the Venusians could be burning their rainforest to make more farmland.\n\nBy the late 1800s, the possibility of life on the moon was put to rest. Astronomers at that time believed in the Kant-Laplace hypothesis, which stated that the farthest planets from the sun are the oldest—therefore Mars was more likely to have advanced civilizations than Venus. Subsequent investigations focused on contacting Martians. In 1877 Giovanni Schiaparelli announced he had discovered \"canali\" (\"channels\" in Italian, which occur naturally, and mistranslated as \"canals\", which are artificial) on Mars—this was followed by thirty years of Mars enthusiasm. Eventually the Martian canals proved illusory.\n\nThe inventor Charles Cros was convinced that pinpoints of light observed on Mars and Venus were the lights of large cities. He spent years of his life trying to get funding for a giant mirror with which to signal the Martians. The mirror would be focused on the Martian desert, where the intense reflected sunlight could be used to burn figures into the Martian sand.\n\nInventor Nikola Tesla mentioned many times during his career that he thought his inventions such as his Tesla coil, used in the role of a \"resonant receiver\", could communicate with other planets, and that he even had observed repetitive signals of what he believed were extraterrestrial radio communications coming from Venus or Mars in 1899. These \"signals\" turned out to be terrestrial radiation, however.\n\nAround 1900, the Guzman Prize was created; the first person to establish interplanetary communication would be awarded 100,000 francs under one stipulation: Mars was excluded because Madame Guzman thought communicating with Mars would be too easy to deserve a prize.\n\nPublished in 1960 by Hans Freudenthal, \"Lincos: Design of a Language for Cosmic Intercourse\", expands upon Astraglossa to create a general-purpose language derived from basic mathematics and logic symbols. Several researchers have expanded further upon Freudenthal's work. A dictionary resembling Lincos was featured in the Carl Sagan novel \"Contact\" and film adaptation.\n\nPublished in 1963 by Lancelot Hogben, \"Astraglossa\" is an essay describing a system for combining numbers and operators in a series of short and long pulses. In Hogben's system, short pulses represent numbers, while trains of long pulses represent symbols for addition, subtraction, etc.\n\nIn the 1985 science fiction novel \"Contact\", Carl Sagan explored in some depth how a message might be constructed to allow communication with an alien civilization, using prime numbers as a starting point, followed by various universal principles and facts of mathematics and science.\n\nSagan also edited a nonfiction book on the subject. An updated collection of articles on the same topic was published in 2011.\n\nIn 2016, McGill University Linguistics Professor, Jessica Coon, spoke with Business Insider about how 2016 sci-fi blockbuster, \"Arrival\", properly portrayed how humans might actually communicate with aliens. To create this language, film producers consulted with Wolfram Research Founder and CEO, Stephen Wolfram - creator of the computer programming language known as the Wolfram Language - and his son, Christopher. Together, they helped analyze approximately 100 logograms that ultimately served as the basis for the alien language utilized throughout the film. This work, along with many other thoughts with regard to artificial intelligence communication has been documented in an interview published by Space.com. During production, Wolfram's personal copy of \"Lincos: Design of a Language for Cosmic Intercourse\" was also on set.\n\nPublished in 1992 by Carl Devito and Richard Oehrle, \"A language based on the fundamental facts of science\" is a paper describing a language similar in syntax to Astraglossa and Lincos, but which builds its vocabulary around known physical properties.\n\nIn 2010, Michael W. Busch created a general-purpose binary language later used in the Lone Signal project to transmit crowdsourced messages to extraterrestrial intelligence (METI). This was followed by an attempt to extend the syntax used in the Lone Signal hailing message to communicate in a way that, while neither mathematical nor strictly logical, was nonetheless understandable given the prior definition of terms and concepts in the Lone Signal hailing message.\nPictorial communication systems seek to describe fundamental mathematical or physical concepts via simplified diagrams sent as bitmaps. These messages presume that the recipient has similar visual capabilities and can understand basic mathematics and geometry. A common critique of these systems is that they presume a shared understanding of special shapes, which may not be the case with a species with substantially different vision, and therefore a different way of interpreting visual information. For instance, an arrow representing the movement of some object could be interpreted as a weapon firing.\n\nThe two Pioneer plaques were launched on Pioneer 10 and Pioneer 11 in 1972 and 1973, depicting the location of the Earth in the galaxy and the solar system, and the form of the human body.\n\nLaunched in 1977, the Voyager probes carried two golden records that were inscribed with diagrams depicting the human form, our solar system, and its location. Also included were recordings of images and sounds from Earth.\n\nThe Arecibo message, transmitted in 1974, was a 1679 pixel image with 73 rows and 23 columns. It shows the numbers one through ten, the atomic numbers of hydrogen, carbon, nitrogen, oxygen, and phosphorus, the formulas for the sugars and bases in the nucleotides of DNA, the number of nucleotides in DNA, the double helix structure of DNA, a figure of a human being and its height, the population of Earth, a diagram of our solar system, and an image of the Arecibo telescope with its diameter.\n\nThe \"Cosmic Call\" messages consisted of a few digital sections - \"Rosetta Stone\", copy of Arecibo Message, Bilingual Image Glossary, the Braastad message, as well as text, audio, video, and other image files submitted for transmission by everyday people around the world. The \"Rosetta Stone\" was composed by Stephane Dumas and Yvan Dutil and represents a multi-page bitmap that builds a vocabulary of symbols representing numbers and mathematical operations. The message proceeds from basic mathematics to progressively more complex concepts, including physical processes and objects (such as a hydrogen atom). The message is designed with a noise resistant format and characters that make it resistant to alteration by noise. These messages were transmitted in 1999 and 2003 from Evpatoria Planetary Radar under scientific guidance of Alexander L. Zaitsev. Richard Braastad coordinated the overall project.\n\nStar systems to which messages were sent, are the following:\n\nThe \"Teen-Age Message\", composed by Russian scientists (Zaitsev, Gindilis, Pshenichner, Filippova) and teens, was transmitted from the 70-m dish of Evpatoria Deep Space Center to six star systems resembling that of the Sun on August 29 and September 3 and 4, 2001. The message consists of three parts:\n\nSection 1 represents a coherent-sounding radio signal with slow Doppler wavelength tuning to imitate transmission from the Sun's center. This signal was transmitted in order to help extraterrestrials detect the TAM and diagnose the radio propagation effect of the interstellar medium.\n\nSection 2 is analog information representing musical melodies performed on the theremin. This electric musical instrument produces a quasi-monochromatic signal, which is easily detectable across interstellar distances. There were seven musical compositions in the First Theremin Concert for Aliens. The 14-minute analog transmission of the theremin concert would take almost 50 hours by digital means; see The First Musical Interstellar Radio Message.\n\nSection 3 represents a well-known Arecibo-like binary digital information: the logotype of the TAM, bilingual Russian and English greeting to aliens, and image glossary.\n\nStar systems to which the message was sent are the following:\n\nThe Cosmic Call-2 message contained text, images, video, music, the Dutil/Dumas message, a copy of the 1974 Arecibo message, BIG = Bilingual Image Glossary, the AI program \"Ella\", and the Braastad message.\n\nAlgorithmic communication systems are a relatively new field within CETI. In these systems, which build upon early work on mathematical languages, the sender describes a small set of mathematic and logic symbols that form the basis for a rudimentary programming language that the recipient can run on a virtual machine. Algorithmic communication has a number of advantages over static pictorial and mathematical messages, including: localized communication (the recipient can probe and interact with the programs within a message, without transmitting a reply to the sender and then waiting years for a response), forward error correction (the message might contain algorithms that process data elsewhere in the message), and the ability to embed proxy agents within the message. In principle, a sophisticated program when run on a fast enough computing substrate, may exhibit complex behavior and perhaps, intelligence.\n\n\"CosmicOS\", designed by Paul Fitzpatrick at MIT, describes a virtual machine that is derived from lambda calculus.\n\n\"Logic Gate Matrices\" (a.k.a. LGM), developed by Brian McConnell, describes a universal virtual machine that is constructed by connecting coordinates in an n-dimensional space via mathematics and logic operations, for example: (1,0,0) <-- (OR (0,0,1) (0,0,2)). Using this method, one may describe an arbitrarily complex computing substrate as well as the instructions to be executed on it.\n\nThis research focuses on the event that we receive a signal or message that is either not directed at us (eavesdropping) or one that is in its natural communicative form. To tackle this difficult, but probable scenario, methods are being developed that first, will detect if a signal has structure indicative of an intelligent source, categorize the type of structure detected, and then decipher its content: from its physical level encoding and patterns to the parts-of-speech that encode internal and external ontologies.\n\nPrimarily, this structure modeling focuses on the search for generic human and inter-species language universals to devise computational methods by which language may be discriminated from non-language and core structural syntactic elements of unknown languages may be detected. Aims of this research include: contributing to the understanding of language structure and the detection of intelligent language-like features in signals, to aid the search for extraterrestrial intelligence.\n\nThe problem goal is therefore to separate language from non-language without dialogue, and learn something about the structure of language in the passing. The language may not be human (animals, aliens, computers...), the perceptual space may be unknown, and we cannot presume human language structure, but must begin somewhere. We need to approach the language signal from a naive viewpoint, in effect, increasing our ignorance and assuming as little as possible.\n\nIf a sequence can be tokenized, that is, separated into \"words\", an unknown human language may be distinguished from many other data sequences by the frequency distribution of the tokens. Human languages conform to a Zipfian distribution, while many (but not all) other data sequences do not. It has been proposed that an alien language also might conform to such a distribution (). When displayed in a log-log graph of frequency vs. rank, this distribution would appear as a somewhat straight line with a slope of approximately -1. SETI scientist Laurance Doyle explains that the slope of a line that represents individual tokens in a stream of tokens may indicate whether the stream contains linguistic or other structured content. If the line angles at 45°, the stream contains such content. If the line is flat, it does not.\n\n\nSome researchers have concluded that in order to communicate with extraterrestrial species, humanity must first try to communicate with Earth's intelligent animal species. John C. Lilly worked on with interspecies communication by teaching dolphins English (successful with rhythms, not with understandability, given their different mouth/blowhole shapes). He practiced various disciplines of spirituality and also ingested psychedelic drugs such as LSD and (later) ketamine in the company of dolphins. He tried to determine whether he could communicate non-verbally with dolphins, and also tried to determine if some extraterrestrial radio signals are intelligent communications. Similarly, Laurance Doyle, Robert Freitas and Brenda McCowan compare the complexity of cetacean and human languages to help determine whether a specific signal from space is complex enough to represent a message that needs to be decoded.\n\n\n"}
{"id": "36826605", "url": "https://en.wikipedia.org/wiki?curid=36826605", "title": "Comparison of satellite buses", "text": "Comparison of satellite buses\n\nThis page includes a list of satellite buses, of which multiple similar artificial satellites have been, or are being, built to the same model of structural frame, propulsion, spacecraft power and intra-spacecraft communication. Only commercially available (in present or past) buses are included, thus excluding series-produced proprietary satellites operated only by their makers.\n\n\n\nIt is not clear from the sources if the Spacebus 100 satellite bus is still on offer.\n"}
{"id": "40948", "url": "https://en.wikipedia.org/wiki?curid=40948", "title": "Configuration management", "text": "Configuration management\n\nConfiguration management (CM) is a systems engineering process for establishing and maintaining consistency of a product's performance, functional, and physical attributes with its requirements, design, and operational information throughout its life. \nThe CM process is widely used by military engineering organizations to manage changes throughout the system lifecycle of complex systems, such as weapon systems, military vehicles, and information systems. Outside the military, the CM process is also used with IT service management as defined by ITIL, and with other domain models in the civil engineering and other industrial engineering segments such as roads, bridges, canals, dams, and buildings.\n\nCM applied over the life cycle of a system provides visibility and control of its performance, functional, and physical attributes. CM verifies that a system performs as intended, and is identified and documented in sufficient detail to support its projected life cycle. The CM process facilitates orderly management of system information and system changes for such beneficial purposes as to revise capability; improve performance, reliability, or maintainability; extend life; reduce cost; reduce risk and liability; or correct defects. The relatively minimal cost of implementing CM is returned many fold in cost avoidance. The lack of CM, or its ineffectual implementation, can be very expensive and sometimes can have such catastrophic consequences such as failure of equipment or loss of life.\n\nCM emphasizes the functional relation between parts, subsystems, and systems for effectively controlling system change. It helps to verify that proposed changes are systematically considered to minimize adverse effects. Changes to the system are proposed, evaluated, and implemented using a standardized, systematic approach that ensures consistency, and proposed changes are evaluated in terms of their anticipated impact on the entire system. CM verifies that changes are carried out as prescribed and that documentation of items and systems reflects their true configuration. A complete CM program includes provisions for the storing, tracking, and updating of all system information on a component, subsystem, and system basis.\n\nA structured CM program ensures that documentation (e.g., requirements, design, test, and acceptance documentation) for items is accurate and consistent with the actual physical design of the item. In many cases, without CM, the documentation exists but is not consistent with the item itself. For this reason, engineers, contractors, and management are frequently forced to develop documentation reflecting the actual status of the item before they can proceed with a change. This reverse engineering process is wasteful in terms of human and other resources and can be minimized or eliminated using CM.\n\nConfiguration Management originated in the United States Department of Defense in the 1950s as a technical management discipline for hardware material items—and it is now a standard practice in virtually every industry. The CM process became its own technical discipline sometime in the late 1960s when the DoD developed a series of military standards called the \"480 series\" (i.e., MIL-STD-480, MIL-STD-481 and MIL-STD-483) that were subsequently issued in the 1970s. In 1991, the \"480 series\" was consolidated into a single standard known as the MIL–STD–973 that was then replaced by MIL–HDBK–61 pursuant to a general DoD goal that reduced the number of military standards in favor of industry technical standards supported by standards developing organizations (SDO). This marked the beginning of what has now evolved into the most widely distributed and accepted standard on CM, ANSI–EIA–649–1998. Now widely adopted by numerous organizations and agencies, the CM discipline's concepts include systems engineering (SE), Integrated Logistics Support (ILS), Capability Maturity Model Integration (CMMI), ISO 9000, Prince2 project management method, COBIT, Information Technology Infrastructure Library (ITIL), product lifecycle management, and Application Lifecycle Management. Many of these functions and models have redefined CM from its traditional holistic approach to technical management. Some treat CM as being similar to a librarian activity, and break out change control or change management as a separate or stand alone discipline.\n\nCM is the practice of handling changes systematically so that a system maintains its integrity over time. CM implements the policies, procedures, techniques, and tools that manage, evaluate proposed changes, track the status of changes, and maintain an inventory of system and support documents as the system changes. CM programs and plans provide technical and administrative direction to the development and implementation of the procedures, functions, services, tools, processes, and resources required to successfully develop and support a complex system. During system development, CM allows program management to track requirements throughout the life-cycle through acceptance and operations and maintenance. As changes inevitably occur in the requirements and design, they must be approved and documented, creating an accurate record of the system status. Ideally the CM process is applied throughout the system lifecycle. Most professionals mix up or get confused with Asset management (AM), where it inventories the assets on hand. The key difference between CM and AM is that the former does not manage the financial accounting aspect but on service that the system supports.\n\nThe CM process for both hardware- and software-configuration items comprises five distinct disciplines as established in the MIL–HDBK–61A\nand in ANSI/EIA-649. These disciplines are carried out as policies and procedures for establishing baselines and for performing a standard change-management process. The IEEE 12207 process IEEE 12207.2 also has these activities and adds \"Release management and delivery\".\n\nThe five disciplines are:\n\n\nThe traditional software configuration management (SCM) process is looked upon by practitioners as the best solution to handling changes in software projects. It identifies the functional and physical attributes of software at various points in time, and performs systematic control of changes to the identified attributes for the purpose of maintaining software integrity and traceability throughout the software development life cycle.\n\nThe SCM process further defines the need to trace changes, and the ability to verify that the final delivered software has all of the planned enhancements that are supposed to be included in the release. It identifies four procedures that must be defined for each software project to ensure that a sound SCM process is implemented. They are:\n\nThese terms and definitions change from standard to standard, but are essentially the same.\n\n\nThe Information Technology Infrastructure Library (ITIL) specifies the use of a Configuration management system (CMS) or Configuration management database (CMDB) as a means of achieving industry best practices for Configuration Management. CMDBs are used to track Configuration Items (CIs) and the dependencies between them, where CIs represent the things in an enterprise that are worth tracking and managing, such as but not limited to computers, software, software licenses, racks, network devices, storage, and even the components within such items.\n\nThe benefits of a CMS/CMDB includes being able to perform functions like root cause analysis, impact analysis, change management, and current state assessment for future state strategy development. Example systems, commonly identifies themselves as IT Service Management (ITSM) systems, include FreshService, ServiceNow and Samanage.\n\nFor information assurance, CM can be defined as the management of security features and assurances through control of changes made to hardware, software, firmware, documentation, test, test fixtures, and test documentation throughout the life cycle of an information system. CM for information assurance, sometimes referred to as Secure Configuration Management, relies upon performance, functional, and physical attributes of IT platforms and products and their environments to determine the appropriate security features and assurances that are used to measure a system configuration state. For example, configuration requirements may be different for a network firewall that functions as part of an organization's Internet boundary versus one that functions as an internal local network firewall.\n\nConfiguration management is used to maintain an understanding of the status of complex assets with a view to maintaining the highest level of serviceability for the lowest cost. Specifically, it aims to ensure that operations are not disrupted due to the asset (or parts of the asset) overrunning limits of planned lifespan or below quality levels.\n\nIn the military, this type of activity is often classed as \"mission readiness\", and seeks to define which assets are available and for which type of mission; a classic example is whether aircraft on board an aircraft carrier are equipped with bombs for ground support or missiles for defense.\n\nConfiguration management can be used to maintain OS configuration files. Example systems include Ansible, Bcfg2, CFEngine, Chef, Otter, Puppet, Quattor, SaltStack, and Vagrant. Many of these systems utilize Infrastructure as Code to define and maintain configuration.\n\nA theory of configuration maintenance was worked out by Mark Burgess, with a practical implementation on present day computer systems in the software CFEngine able to perform real time repair as well as preventive maintenance.\n\nUnderstanding the \"as is\" state of an asset and its major components is an essential element in preventive maintenance as used in maintenance, repair, and overhaul and enterprise asset management systems.\n\nComplex assets such as aircraft, ships, industrial machinery etc. depend on many different components being serviceable. This serviceability is often defined in terms of the amount of usage the component has had since it was new, since fitted, since repaired, the amount of use it has had over its life and several other limiting factors. Understanding how near the end of their life each of these components is has been a major undertaking involving labor-intensive record keeping until recent developments in software.\n\nMany types of component use electronic sensors to capture data which provides live condition monitoring. This data is analyzed on board or at a remote location by computer to evaluate its current serviceability and increasingly its likely future state using algorithms which predict potential future failures based on previous examples of failure through field experience and modeling. This is the basis for \"predictive maintenance\".\n\nAvailability of accurate and timely data is essential in order for CM to provide operational value and a lack of this can often be a limiting factor. Capturing and disseminating the operating data to the various support organizations is becoming an industry in itself.\n\nThe consumers of this data have grown more numerous and complex with the growth of programs offered by original equipment manufacturers (OEMs). These are designed to offer operators guaranteed availability and make the picture more complex with the operator managing the asset but the OEM taking on the liability to ensure its serviceability.\n\nA number of standards support or include configuration management, including:\n\n\nMore recently configuration management has been applied to large construction projects which can often be very complex and have a huge amount of details and changes that need to be documented. Construction agencies such as the Federal Highway Administration have used configuration management for their infrastructure projects. There are construction-based configuration management tools that aim to document change orders and RFIs in order to ensure a project stays on schedule and on budget. These programs can also store information to aid in the maintenance and modification of the infrastructure when it is completed. One such application, ccsNet, was tested in a case study funded by the Federal Transportation Administration (FTA) in which the efficacy of configuration management was measured through comparing the approximately 80% complete construction of the Los Angeles County Metropolitan Transit Agency (LACMTA) 1st and 2nd segments of the Red Line, a $5.3 billion rail construction project. This study yielded results indicating a benefit to using configuration management on projects of this nature.\n\n"}
{"id": "50403904", "url": "https://en.wikipedia.org/wiki?curid=50403904", "title": "Decapping", "text": "Decapping\n\nDecapping (decapsulation) or delidding of an integrated circuit is the process of removing the protective cover of a microchip so that the contained die is revealed for visual inspection of the micro circuitry imprinted on the die. This process is typically done in order to debug a manufacturing problem with the chip, or possibly to copy information from the device. Modern integrated circuits can be encapsulated in plastic, ceramic, or epoxy.\n\nDecapping is usually carried out by chemical etching of the covering, laser cutting, or mechanical removal of the cover using a milling machine. The process can be either destructive or non-destructive.\n\n"}
{"id": "1449552", "url": "https://en.wikipedia.org/wiki?curid=1449552", "title": "Dew warning", "text": "Dew warning\n\nA dew warning, also known as a dew alarm or dew signal, is an error indication on VCRs and camcorders if the VCR/camcorder develops dew inside the unit from being exposed to extreme temperature and/or humidity changes.\n\nThe presence of moisture between the tape and the rotating head drum increases friction which prevents correct operation and can cause damage to both the recording device and the tape. In extreme cases, if the dew sensor fails to function and stop the video recorder, moisture can cause the tape to stick to the spinning video head. This can pull a large amount of tape from the cassette before the head drum stops spinning. The tape will be extensively damaged, the video heads will often become clogged, and the mechanism may be unable to eject the cassette.\n\nThe dew sensor itself is mounted very close to the video head drum. Contrary to how one might expect this to behave, the sensor increases its resistance when moisture is present. Poor contacts on the sensor can therefore be a cause of random dew sensor warnings.\n\nUsually, a \"DEW\" indicator or error code lights up on the display of most VCRs/camcorders, and on some, a buzzer may sound.\n"}
{"id": "9917025", "url": "https://en.wikipedia.org/wiki?curid=9917025", "title": "Digital strategy", "text": "Digital strategy\n\nA digital strategy is a form of strategic management and a business answer or response to a digital question, often best addressed as part of an overall business strategy. A digital strategy is often characterized by the application of new technologies to existing business activity and/or a focus on the enablement of new digital capabilities to their business (such as those created by the Information Age and often as a result of advancements in digital technologies such as computers, data, telecommunications, Internet, etc.). As is the case with its business strategy parent, a digital strategy can be formulated and implemented through a variety of different approaches. Formulation often includes the process of specifying an organization's vision, goals, opportunities and related activities in order to maximize the business benefits of digital initiatives to an organization. These can range from an enterprise focus, which considers the broader opportunities and risks digital can create and often includes customer intelligence, collaboration, new product/market exploration, sales and service optimization, enterprise technology architectures and processes, innovation and governance; to more marketing and customer-focused efforts such as web sites, mobile, eCommerce, social, site and search engine optimization, and advertising.\n\nThe digital strategy is part of the business strategy and experts maintain that it cannot be effective or successful if built independently. It is argued that it represents how the business strategy is influenced by leveraging digital resources to create differential value. In the process, it reshapes traditional organizational strategies into modular, distributed, cross-functional, and global business processes.\n\nThere, are numerous approaches to conducting digital strategy, but at their core, all go through four steps:\n\nWithin each of those stages, a number of techniques and analyses may be employed.\n\nIncludes one-on-one interviews, group interviews and workshops with a company's senior management, marketing and sales, operations and service stakeholders with a goal of understanding the business strategy, challenges and opportunities, products, organization, processes, supply chain and vendors, distributors, customers, and competitive landscape, as well as the potential role of their online assets.\n\nIncludes evaluations of a company's main competitors and potential substitutes with the goal of understanding a company's strengths and weaknesses relative to their competitors and potential substitutes. While this often includes steps found in traditional marketing competitor analysis, such as products, prices, etc. Competitor analysis includes two unique items:\n\nAn analysis of a company's financial data (which may include everything from public financial statements to private ERP data) with the goal of understanding the financial impact (positive and negative) that certain changes would have on a company.\n\nASSIMPLER Blueprinting - The Business Blueprinting of the organization is designed based on the ASSIMPLER framework. ASSIMPLER stands for Availability, Scalability, Security, Interoperability, Maintainability, Performance, Low cost of ownership, Extendability and Reliability - applied to business services and processes. The framework helps model the business expectations and challenges to be addressed through the Digital Strategy.\n\nIncludes one-on-one interviews and focus groups with a company's external stakeholders, with a goal of understanding external stakeholders behaviours, needs, goals and perceptions of the company and their industry both in the broadest business context as well as specifically online. In addition to standard marketing strategy methodologies and questions (quantitative and qualitative), external stakeholder interviews for Digital Strategy may include usability testing, an analysis of how effectively external stakeholders can use the online assets developed by a company for their intended purposes. In digital strategy this is used to uncover usability barriers that may prevent the online vision being achieved.\n\nAn analysis of external stakeholder behaviors in their environment, for example: field observations of shoppers in a store. In addition to standard ethnographic research, digital strategy research may include video recording of an external stakeholder using their computers or specific computer applications or web sites.\n\nAn analysis of the usage patterns of a company's online assets with the goal of better understanding external stakeholder behavior as well as identifying strengths and weakness of the company's current online offerings. This may include understanding how many people are visiting a web site, what are the most popular pages, what are the most popular paths, where are people coming from, where do they drop off, how long do they stay, etc.\n\nA specific methodology for web analytics where the company's online assets are modeled as a sales funnel, with a visit or impression representing a new lead, a certain page or action in the web site considered a conversion (such as a user hitting the purchase confirmation page) and specific pages in the web site representing specific stages of the sales funnel. The goal of the analysis is to provide insight into the overall conversion rate as well as the key weak points of the funnel (the stages in which the largest percentages of users drop out of the funnel). This may also involve analysis of a company's search engine optimization situation and changes in online traffic pathways.\n\nAn analysis of a company's customer databases and information repositories with the goal of segmenting customers into homogeneous groups across one or more dimensions of behavior, demographics, value, product or marketing message affinity, etc. In digital strategy this often includes the online customer registration database which companies use to provide access to their customer specific, protected areas.\n\nAn analysis of a customer's behavior (such as their purchase or service behavior) that looks across all of the different channels, in which customers interact with a company's products or information. There are lots of different ways to do this; a representative example would be, a company focuses on the customer purchase process (how a customer becomes aware of a product, how a customer develops the intent to purchase a product, and how a customer actually purchases the product). The analysis would look at which channels (for example: phone, catalog, retail store, web site, 3rd party search engine, etc.) a customer uses at which stage of the purchase process, attempting to understand why each channel is used, each channel's relative attribution and evaluating the company's strengths or weaknesses in that particular channel for the particular stage of the process.\n\nAn approach to the collection of external stakeholder feedback in a quantitative manner from a large population. In digital strategy, surveys may be used to validate or invalidate key questions raised in more qualitative exercises such as external stakeholder interviews and focus groups. Depending on the breadth of the survey population and the degree of variation within the population, survey results may be segmented to form homogeneous groups across one or more dimensions of behavior, demographics, value, product or marketing message affinity, etc. Surveys are often conducted online using web surveys, e-mail lists, or 3rd party panels, although phone surveys or other offline methods may sometimes be used when there are questions as to the online savviness of a particular target population.\n\nA spreadsheet with supporting documentation that quantifies the investments and returns over time, resulting from the execution of the online strategy. The Business plan also defines the Key Performance Indicators (KPIs) that will be used to measure and evaluate the success of the online strategy.\n\nA design of a technical architecture which will meet the needs of the business vision and conform to the business plan and roadmap. This is often done as a gap analysis where the current technical architecture is assessed. A future technical architecture, which meets the needs of the online vision, is designed. The gaps between the current state and future state are identified, and a series of initiatives or projects to fill those gaps are developed and sequenced.\n\nSimilar to a technical assessment, organizational and process assessments look at the changes that need to be made to an organization and its processes in order to achieve the online vision. They may involve a series of business process reengineering projects focused on the areas of an organization most affected by the online initiatives.\n\nA way of prioritizing various initiatives by comparing their cost of implementation with their expected business benefits. This is often done by creating a two by two matrix where cost of implementation runs along the x-axis (from high cost to low cost) and expected business benefit runs along the y-axis, from low benefit to high benefit. Individual initiatives or projects are then plotted on the matrix in terms of their calculated costs and benefits. Priorities are then determined according to which projects will provide the greatest benefit for the lowest cost.\n\nA plan detailing the allocation of media spending across online media (such as search engine marketing, banner advertising, and affiliate marketing) usually as part of the customer acquisition or retention elements of the digital strategy. Since the late 2000s, social media has become increasingly important in engaging with customers both for marketing and customer support purposes, especially benefiting smaller businesses.\n\nGraphical representations or an outline of key ideas or processes of the digital strategy. These are often created in order to better communicate a key concept or to build excitement among stakeholders when building consensus or socializing a digital strategy.\n\nA high-level project plan which details the durations and dependencies of all the initiatives in the digital strategy. The roadmap will often include checkpoints to assess the progress and success of the digital strategy, over time.\n\nA description of the key performance indicators used to measure the effectiveness of the digital strategy as well as the process for collecting and sharing this information. The measurement plan usually covers the financial, operational, and e-business metrics and their relationships.\n\nThe organizational structure, roles, and process description of the operational entity that will manage the initiatives in a digital strategy. The governance model describes who is responsible for what, how decisions are made, how issues are escalated, and how information on the performance of the projects is communicated within the organization.\n\nAs of 2007, a trend in digital strategy is the use of personas as a framework for using customer information to prioritize online initiatives. Personas are character sketches which represent a typical member of one customer segment and highlights their needs, goals and behaviors. Because it is representative of a customer segment, it allows decision makers to prioritize various features based on the needs of the segment. Because it is a character sketch, it is sometimes easier for decision makers to internalize the key needs of the segment than it would be by reading large quantities of information. A typical approach is to create the segment based on customer analysis such as customer interviews, ethnographic research, and statistical surveys. Then assemble key decision makers or stakeholders, present the findings of the personas, and use them to kick start a brainstorming session around different online initiatives which can meet the personas needs and goals.\n\nHistorically, execution of a business or digital strategy is done as a big bang, with large initiatives such as site redesigns and transactional systems taking 6–12 months to develop and often an additional 6–12 months before they deliver any results. As of 2007, a trend has emerged where companies adopt a more iterative approach to rolling out their strategies, one which leverages a series of smaller tests, which are carefully measured and analyzed and used to modify or optimize the digital strategy. An example of this test-measure-optimize-scale approach is that a company might take some key pages on their site and test a number of versions of those pages with different marketing messages, design approaches, user experience optimizations, navigation optimizations, and even new features and functions using a multivariate or A/B test. The company would then identify the page which had the best combination of changes in terms of some key business metric (such as conversion), analyzing the results to understand which changes were most instrumental in affecting the high conversion rate, and applying those learnings to future pages and future tests (conversion optimization). \n\nThe advantage of this approach is that in the long run, it tends to be more successful in delivering business results, because each step is measured and adjusted for. In addition, it tends to favor smaller (less risky, less expensive) steps rather than larger (more risky, more expensive) initiatives before getting the payback. The disadvantage is that over time this approach tends to converge on a solution (local optimum), not necessarily the best solution (global optimum) that might have been reached if a company starts from scratch instead of building each step on the previous one. Another disadvantage is that although this solution tends to favor smaller, more incremental changes, there is often a larger up front cost to setting up all the measurement systems and staffing a company with the right analysts and change processes to react to these tests in a timely and effective manner. As a result, companies often adopt a mix of big bang efforts augmented by some smaller, more iterative efforts as part of their overall strategy. A person who is primarily focused on digital strategy may be referred to as a digital architect or digital strategist and a person who executes a digital strategy may be referred to as a digital marketing engineer.\n\nThere are also several challenges when developing and implementing a digital strategy. This include human factors, particularly knowledge, skills, and attitudes, which impede full engagement.\n\nThe two terms, \"digital strategy\" and \"online strategy\", tend to be thrown out somewhat interchangeably. However, there is some consensus around the differences between digital strategy and online strategy. According to some, digital strategy refers to the strategy a company takes to become a digital company, where digital connotes deeper interactions with their customers, more customized and personalized offerings and interactions, data driven decision making, and organizational models and processes that are more reactive to changes in the company's environment. Online is a subset of digital, as there may be digital assets which are not online. In this context, a company may use the term online strategy to be limited to the development of plans to deploy their online assets to maximize business results and digital strategy to be the more transformative step of changing the organization, although the latter is also referred as a digital transformation strategy. \n\n"}
{"id": "900498", "url": "https://en.wikipedia.org/wiki?curid=900498", "title": "Early adopter", "text": "Early adopter\n\nAn early adopter (sometimes misspelled as \"early adapter\" or \"early adaptor\") or lighthouse customer is an early customer of a given company, product, or technology. The term originates from Everett M. Rogers' \"Diffusion of Innovations\" (1962).\n\nTypically this will be a customer who, in addition to using the vendor's product or technology, will also provide considerable and candid feedback to help the vendor refine its future product releases, as well as the associated means of distribution, service, and support. \n\nThe relationship is synergistic, with the customer having early (and sometimes unique, or at least uniquely early) access to an advantageous new product or technology, but he or she also serves as a kind of guinea pig.\n\nIn exchange for being an early adopter, and thus being exposed to the problems, risks, and annoyances common to early-stage product testing and deployment, the lighthouse customer is sometimes given especially attentive vendor assistance and support, even to the point of having personnel at the customer's work site to assist with implementation.\nThe customer is sometimes given preferential pricing, terms, and conditions, although new technology is often very expensive, so the early adopter still often pays quite a lot.\n\nThe vendor, on the other hand, benefits from receiving early\nrevenues, and also from a lighthouse customer's endorsement and assistance in further developing the product and its go-to-market mechanisms. Acquiring lighthouse customers is a common step in new product development and implementation. The real-world focus that this type of relationship can bring to a vendor can be extremely valuable.\n\nEarly adoption does come with pitfalls: early versions of products may be buggy and/or prone to malfunction. Furthermore, more efficient, and sometimes less expensive, versions of the product usually appear a few months after the initial release (Apple iPhone). The trend of new technology costing more at release is often referred to as the \"early adopter tax\".\n\n"}
{"id": "361569", "url": "https://en.wikipedia.org/wiki?curid=361569", "title": "English writing style", "text": "English writing style\n\nAn English writing style is a way of using the English language.\n\nThe style of a piece of writing is the way in which features of the language are used to convey meaning, typically but not always within the constraints of more widely accepted conventions of usage, grammar, and spelling.\n\nAn individual's writing style may be a very personal thing. Organizations that employ writers or commission written work from individuals may require that writers conform to a standardized style defined by the organization. This allows a consistent readability of composite works produced by many authors, and promotes usability of, for example, references to other cited works.\n\nIn many kinds of professional writing aiming for effective transfer of information, adherence to a standardised style of writing helps readers make sense of what the writer is presenting. Many standardised styles are documented in style guides. Some styles are more widely used, others restricted to a particular journal. Adherence to no particular style is also a style in its own right; some may think it undesirable, others not.\n\nAll writing has some style, even if the author is not thinking about a personal style. It is important to understand that style reflects meaning. For instance, if a writer wants to express a sense of euphoria, he or she might write in a style overflowing with expressive modifiers. Some writers use styles that are very specific, for example in pursuit of an artistic effect. Stylistic rule-breaking is exemplified by the poet. An example is E. E. Cummings, whose writing consists mainly of only lower case letters, and often uses unconventional typography, spacing, and punctuation. Even in non-artistic writing, every person who writes has his or her own personal style.\n\nMany large publications define a house style to be used throughout the publication, a practice almost universal among newspapers and well-known magazines. These styles can cover the means of expression and sentence structures, such as those adopted by \"Time\". They may also include features peculiar to a publication; the practice at \"The Economist\", for example, is that articles are rarely attributed to an individual author. General characteristics have also been prescribed for different categories of writing, such as in journalism, the use of SI units, or questionnaire construction.\n\nUniversity students, especially graduate students, are encouraged to write papers in an approved style. This practice promotes readability and ensures that references to cited works are noted in a uniform way. Typically, students are encouraged to use a style commonly adopted by journals publishing articles in the field of study. The list of \"Style Manuals & Guides\", from the University of Memphis Libraries, includes thirty academic style manuals that are currently in print, and twelve that are available on-line. Citation of referenced works is a key element in academic style.\n\nThe requirements for writing and citing articles accessed on-line may sometimes differ from those for writing and citing printed works. Some of the details are covered in \"The Columbia Guide to Online Style\".\n\n\n\n"}
{"id": "8312842", "url": "https://en.wikipedia.org/wiki?curid=8312842", "title": "Extended newsvendor model", "text": "Extended newsvendor model\n\nExtended newsvendor models are variations of the classic newsvendor model involving production capacity constraints, multiple products, multiple production cycles, demand dependent selling price, etc. that appear in the Operations management literature.\n\n"}
{"id": "2265023", "url": "https://en.wikipedia.org/wiki?curid=2265023", "title": "Field emission gun", "text": "Field emission gun\n\nA field emission gun is a type of electron gun in which a sharply pointed Müller-type emitter is held at several kilovolts negative potential relative to a nearby electrode, so that there is sufficient potential gradient at the emitter surface to cause field electron emission. Emitters are either of cold-cathode type, usually made of single crystal tungsten sharpened to a tip radius of about 100 nm, or of the Schottky type, in which thermionic emission is enhanced by barrier lowering in the presence of a high electric field. Schottky emitters are made by coating a tungsten tip with a layer of zirconium oxide, which has the unusual property of increasing in electrical conductivity at high temperature. \n\nIn electron microscopes, a field emission gun is used to produce an electron beam that is smaller in diameter, more coherent and with up to three orders of magnitude greater current density or brightness than can be achieved with conventional thermionic emitters such as tungsten or lanthanum hexaboride ()-tipped filaments. The result in both scanning and transmission electron microscopy is significantly improved signal-to-noise ratio and spatial resolution, and greatly increased emitter life and reliability compared with thermionic devices. \n"}
{"id": "36172555", "url": "https://en.wikipedia.org/wiki?curid=36172555", "title": "Growth of wind power in the United States", "text": "Growth of wind power in the United States\n\nWind power has been growing in the United States since the 1970s.\n\nThe growth of wind power in the United States has been enhanced by a production tax credit (PTC), which pays 2.3¢/kWh for the first 10 years. Development has been off and on again due to the expiration and late renewal of the PTC. It expired at the end of 2013, only to be restored in December 2014. In 2015 it was renewed but is phased out over a five-year period. One of the goals set by the Department of Energy is to increase the number of states which have over 1,000 MW of wind power to 15 by 2018. By the end of 2015 there were 17. Another goal was to increase the number of states with at least 100 MW to 30 by 2010. As of 2011, there are 29. The 2015 vision for wind calls for 10% generation by wind by 2020 (113 GW), 30% by 2030 (224 GW), and 35% by 2050 (404 GW). It calls for 40 states to each have at least 1 GW, and wind power in all 50 states. Larger wind turbines has expanded the commercial viability of wind to all 50 states. Analysts expect 25 GW more between 2016-18.\n\nSource:\n\nSource:\n\nThe United States uses approximately 4 million GWh/year.\n"}
{"id": "3189819", "url": "https://en.wikipedia.org/wiki?curid=3189819", "title": "History of candle making", "text": "History of candle making\n\nCandle making was developed independently in many places throughout history. \nCandles were used by the early Greeks to honour the goddess Artemis' birth on the sixth day of every lunar month.\n\nCandles were made by the Romans beginning about 500 BC. These were true dipped candles and made from tallow. Evidence for candles made from whale fat in China dates back to the Qin Dynasty (221–206 BC). In India, wax from boiling cinnamon was used for temple candles.\n\nIn parts of Europe, the Middle-East and Africa, where lamp oil made from olives was readily available, candle making remained unknown until the early middle-ages. Candles were primarily made from tallow and beeswax in ancient times, but have been made from spermaceti, purified animal fats (stearin) and paraffin wax in recent centuries.\n\nThe early Greeks used candles to honour the goddess Artemis' birth on the sixth day of every lunar month.\n\nRomans began making true dipped candles from tallow, beginning around 500 BC. While oil lamps were the most widely used source of illumination in Roman Italy, candles were common and regularly given as gifts during Saturnalia.\n\nThe mausoleum of Qin Shi Huang (259–210 BC), contained candles made from whale fat. The word zhú was used as candle during the Warring States period (403–221 BC); some excavated bronzewares from that era feature a pricket thought to hold a candle.\n\nThe Han Dynasty (202 BC – 220 AD) \"Jizhupian\" dictionary of about 40 BC hints at candles being made of beeswax, while the \"Book of Jin\" (compiled in 648) covering the Jin Dynasty (265–420) makes a solid reference to the beeswax candle in regards to its use by the statesman Zhou Yi (d. 322). An excavated earthenware bowl from the 4th century AD, located at the Luoyang Museum, has a hollowed socket where traces of wax were found. Generally these Chinese candles were molded in paper tubes, using rolled rice paper for the wick, and wax from an indigenous insect that was combined with seeds.\n\nWax from boiling cinnamon was used for temple candles in India. Yak butter was used for candles in Tibet\n\nThere is a fish called the eulachon or \"candlefish\", a type of smelt which is found from Oregon to Alaska. During the 1st century AD, indigenous people from this region used oil from this fish for illumination. A simple candle could be made by putting the dried fish on a forked stick and then lighting it.\n\nAfter the collapse of the Roman empire, trading disruptions made olive oil, the most common fuel for oil lamps, unavailable throughout much of Europe. As a consequence, candles became more widely used. By contrast, in North Africa and the Middle East, candle-making remained relatively unknown due to the availability of olive oil.\n\nCandles were commonplace throughout Europe in the Middle Ages. Candle makers (known as chandlers) made candles from fats saved from the kitchen or sold their own candles from within their shops. The trade of the chandler is also recorded by the more picturesque name of \"smeremongere\", since they oversaw the manufacture of sauces, vinegar, soap and cheese. The popularity of candles is shown by their use in Candlemas and in Saint Lucy festivities.\n\nTallow, fat from cows or sheep, became the standard material used in candles in Europe. The unpleasant smell of tallow candles is due to the glycerine they contain. The smell of the manufacturing process was so unpleasant that it was banned by ordinance in several European cities. Beeswax was discovered to be an excellent substance for candle production without the unpleasant odour, but remained restricted in usage for the rich and for churches and royal events, due to their great expense.\n\nIn England and France, candle making had become a guild craft by the 13th century. The Tallow Chandlers Company of London was formed in about 1300 in London, and in 1456 was granted a coat of arms. The Wax Chandlers Company dating from about 1330, acquired its charter in 1484. By 1415, tallow candles were used in street lighting. The first candle mould comes from the 15th century in Paris.\n\nWith the growth of the whaling industry in the 18th century, spermaceti, an oil that comes from a cavity in the head of the sperm whale, became a widely used substance for candle making. The spermaceti was obtained by crystallizing the oil from the sperm whale and was the first candle substance to become available in mass quantities. Like beeswax, spermaceti wax did not create a repugnant odor when burned, and produced a significantly brighter light. It was also harder than either tallow or beeswax, so it would not soften or bend in the summer heat. The first \"standard candles\" were made from spermaceti wax.\n\nBy 1800, an even cheaper alternative was discovered. Colza oil, derived from Brassica campestris, and a similar oil derived from rapeseed, yielded candles that produce clear, smokeless flames. The French chemists Michel Eugène Chevreul (1786–1889) and Joseph-Louis Gay-Lussac (1778–1850) patented stearin in 1825. Like tallow, this was derived from animals, but had no glycerine content.\n\nThe manufacture of candles became an industrialised mass market in the mid 19th century. In 1834, Joseph Morgan, a pewterer from Manchester, England, patented a machine that revolutionised candle making. It allowed for continuous production of molded candles by using a cylinder with a moveable piston to eject candles as they solidified. This more efficient mechanized production produced about 1,500 candles per hour, (according to his patent \". . with three men and five boys [the machine] will manufacture two tons of candle in twelve hours\"). This allowed candles to become an easily affordable commodity for the masses.\nAt this time, candlemakers also began to fashion wicks out of tightly braided (rather than simply twisted) strands of cotton. This technique makes wicks curl over as they burn, maintaining the height of the wick and therefore the flame. Because much of the excess wick is incinerated, these are referred to as \"self-trimming\" or \"self-consuming\" wicks.\n\nIn 1848 James Young established the world’s first oil refinery at the Alfreton Ironworks in Riddings, Derbyshire. Two paraffin wax candles were made from the naturally occurring paraffin wax present in the oil and these candles illuminated a lecture at the Royal Institution by Lyon Playfair. In the mid-1850s, James Young succeeded in distilling paraffin wax from coal and oil shales at Bathgate in West Lothian and developed a commercially viable method of production. The Paraffin wax was processed by distilling residue left after crude petroleum was refined. \n\nParaffin could be used to make inexpensive candles of high quality. It was a bluish-white wax, burned cleanly, and left no unpleasant odor, unlike tallow candles. A drawback to the substance was that early coal- and petroleum-derived paraffin waxes had a very low melting point. The introduction of stearin, discovered by Michel Eugène Chevreul, solved this problem. Stearin is hard and durable, with a convenient melting range of 54–72.5 °C (129.2–162.5 °F). By the end of the 19th century, most candles being manufactured consisted of paraffin and stearic acid.\n\nBy the late 19th century, Price's Candles, based in London was the largest candle manufacturer in the world. The company traced its origins back to 1829, when William Wilson invested in 1,000 acres (4 km²) of coconut plantation in Sri Lanka. His aim was to make candles from coconut oil. Later he tried palm oil from palm trees. An accidental discovery swept all his ambitions aside when his son George Wilson, a talented chemist, distilled the first petroleum oil in 1854. George also pioneered the implementation of the technique of steam distillation, and was thus able to manufacture candles from a wide range of raw materials, including skin fat, bone fat, fish oil and industrial greases.\n\nIn America, Syracuse, New York developed into a global center for candle manufacturing from the mid-nineteenth century. Manufacturers included Will & Baumer, Mack Miller, Muench Kruezer, and Cathedral Candle Company.\n\nDespite advances in candle making, the candle industry declined rapidly upon the introduction of superior methods of lighting, including kerosene and lamps and the 1879 invention of the incandescent light bulb.\n\nFrom this point on, candles came to be marketed as more of a decorative item. Candles became available in a broad array of sizes, shapes and colors, and consumer interest in scented candles began to grow. During the 1990s, new types of candle waxes were being developed due to an unusually high demand for candles. Paraffin, a by-product of oil, was quickly replaced by new waxes and wax blends due to rising costs.\n\nCandle manufacturers looked at waxes such as soy, palm and flax-seed oil, often blending them with paraffin in hopes of getting the performance of paraffin with the price benefits of the other waxes. The creation of unique wax blends, now requiring different fragrance chemistries and loads, placed pressure for innovation on the candle wick manufacturing industry to meet performance needs with the often tougher to burn formulations. \n"}
{"id": "4999816", "url": "https://en.wikipedia.org/wiki?curid=4999816", "title": "History of military technology", "text": "History of military technology\n\nThe military funding of science has had a powerful transformative effect on the practice and products of scientific research since the early 20th century. Particularly since World War I, advanced science-based technologies have been viewed as essential elements of a successful military.\n\nWorld War I is often called \"the chemists’ war\", both for the extensive use of poison gas and the importance of nitrates and advanced high explosives. Poison gas, beginning in 1915 with chlorine from the powerful German dye industry, was used extensively by the Germans and the British ; over the course of the war, scientists on both sides raced to develop more and more potent chemicals and devise countermeasures against the newest enemy gases. Physicists also contributed to the war effort, developing wireless communication technologies and sound-based methods of detecting U-boats, resulting in the first tenuous long-term connections between academic science and the military.\n\nWorld War II marked a massive increase in the military funding of science, particularly physics. In addition to the Manhattan Project and the resulting atomic bomb, British and American work on radar was widespread and ultimately highly influential in the course of the war; radar enabled detection of enemy ships and aircraft, as well as the radar-based proximity fuze. Mathematical cryptography, meteorology, and rocket science were also central to the war effort, with military-funded wartime advances having a significant long-term effect on each discipline. The technologies employed at the end—jet aircraft, radar and proximity fuzes, and the atomic bomb—were radically different from pre-war technology; military leaders came to view continued advances in technology as the critical element for success in future wars. The advent of the Cold War solidified the links between military institutions and academic science, particularly in the United States and the Soviet Union, so that even during a period of nominal peace military funding continued to expand. Funding spread to the social sciences as well as the natural sciences, and whole new fields, such as digital computing, were born of military patronage. Following the end of the Cold War and the dissolution of the Soviet Union, military funding of science has decreased substantially, but much of the American military-scientific complex remains in place.\n\nThe sheer scale of military funding for science since World War II has instigated a large body of historical literature analyzing the effects of that funding, especially for American science. Since Paul Forman’s 1987 article “Behind quantum electronics: National security as a basis for physical research in the United State, 1940-1960,” there has been an ongoing historical debate over precisely how and to what extent military funding affected the course of scientific research and discovery. Forman and others have argued that military funding fundamentally redirected science—particularly physics—toward applied research, and that military technologies predominantly formed the basis for subsequent research even in areas of basic science; ultimately the very culture and ideals of science were colored by extensive collaboration between scientists and military planners. An alternate view has been presented by Daniel Kevles, that while military funding provided many new opportunities for scientists and dramatically expanded the scope of physical research, scientists by-and-large retained their intellectual autonomy.\n\nWhile there were numerous instances of military support for scientific work before the 20th century, these were typically isolated instances; knowledge gained from technology was generally far more important for the development of science than scientific knowledge was to technological innovation. Thermodynamics, for example, is a science partly born from military technology: one of the many sources of the first law of thermodynamics was Count Rumford’s observation of the heat produced by boring cannon barrels. Mathematics was important in the development of the Greek catapult and other weapons, but analysis of ballistics was also important for the development of mathematics, while Galileo tried to promote the telescope as a military instrument to the military-minded Republic of Venice before turning it to the skies while seeking the patronage of the Medici court in Florence. In general, craft-based innovation, disconnected from the formal systems of science, was the key to military technology well into the 19th century.\nEven craft-based military technologies were not generally produced by military funding. Instead, craftsmen and inventors developed weapons and military tools independently and actively sought the interest of military patrons afterward. Following the rise of engineering as a profession in the 18th century, governments and military leaders did try to harness the methods of both science and engineering for more specific ends, but frequently without success. In the decades leading up to the French Revolution, French artillery officers were often trained as engineers, and military leaders from this mathematical tradition attempted to transform the process of weapons manufacture from a craft-based enterprise to an organized and standardized system based on engineering principles and interchangeable parts (pre-dating the work of Eli Whitney in the U.S.). During the Revolution, even natural scientists participated directly, attempting to create “weapons more powerful than any we possess” to aid the cause of the new French Republic, though there were no means for the revolutionary army to fund such work. Each of these efforts, however, was ultimately unsuccessful in producing militarily useful results. A slightly different outcome came from the longitude prize of the 18th century, offered by the British government for an accurate method of determining a ship’s longitude at sea (essential for the safe navigation of the powerful British navy): intended to promote—and financially reward—a scientific solution, it was instead won by a scientific outsider, the clockmaker John Harrison. However, the naval utility of astronomy did help increase the number of capable astronomers and focus research on developing more powerful and versatile instruments.\n\nThrough the 19th century, science and technology grew closer together, particularly through electrical and acoustic inventions and the corresponding mathematical theories. The late 19th and early 20th centuries witnessed a trend toward military mechanization, with the advent of repeating rifles with smokeless powder, long-range artillery, high explosives, machine guns, and mechanized transport along with telegraphic and later wireless battlefield communication. Still, independent inventors, scientists and engineers were largely responsible for these drastic changes in military technology (with the exception of the development of battleships, which could only have been created through organized large-scale effort).\n\nWorld War I marked the first large-scale mobilization of science for military purposes. Prior to the war, the American military ran a few small laboratories as well as the Bureau of Standards, but independent inventors and industrial firms predominated. Similarly in Europe, military-directed scientific research and development was minimal. The powerful new technologies that led to trench warfare, however, reversed the traditional advantage of fast-moving offensive tactics; fortified positions supported by machine guns and artillery resulted in high attrition but strategic stalemate. Militaries turned to scientists and engineers for even newer technologies, but the introduction of tanks and aircraft had only a marginal impact; the use of poison gas made a tremendous psychological impact, but decisively favored neither side. The war ultimately turned on maintaining adequate supplies of materials, a problem also addressed by military-funded science—and, through the international chemical industry, closely related to the advent of chemical warfare.\n\nThe Germans introduced gas as a weapon in part because naval blockades limited their supply of nitrate for explosives, while the massive German dye industry could easily produce chlorine and organic chemicals in large amounts. Industrial capacity was completely mobilized for war, and Fritz Haber and other industrial scientists were eager to contribute to the German cause; soon they were closely integrated into the military hierarchy as they tested the most effective ways of producing and delivering weaponized chemicals. Though the initial impetus for gas warfare came from outside the military, further developments in chemical weapon technology might be considered military-funded, considering the blurring of lines between industry and nation in Germany.\nFollowing the first chlorine attack by the Germans in May 1915, the British quickly moved to recruit scientists for developing their own gas weapons. Gas research escalated on both sides, with chlorine followed by phosgene, a variety of tear gases, and mustard gas. A wide array of research was conducted on the physiological effects of other gases, such and hydrogen cyanide, arsenic compounds, and a host of complex organic chemicals. The British built from scratch what became an expansive research facility at Porton Down, which remains a significant military research institution into the 21st century. Unlike many earlier military-funded scientific ventures, the research at Porton Down did not stop when the war ended or an immediate goal was achieved. In fact, every effort was made to create an attractive research environment for top scientists, and chemical weapons development continued apace—though in secret—through the interwar years and into World War II. German military-backed gas warfare research did not resume until the Nazi era, following the 1936 discovery of tabun, the first nerve agent, through industrial insecticide research.\n\nIn the United States, the established tradition of engineering was explicitly competing with the rising discipline of physics for World War I military largess. A host of inventors, led by Thomas Edison and his newly created Naval Consulting Board, cranked out thousands of inventions to solve military problems and aid the war effort, while academic scientists worked through the National Research Council (NRC) led by Robert Millikan. Submarine detection was the most important problem that both the physicists and inventors hoped to solve, as German U-boats were decimating the crucial naval supply lines from the U.S. to England. Edison’s Board produced very few useful innovations, but NRC research resulted in a moderately successful sound-based methods for locating submarines and hidden ground-based artillery, as well as useful navigational and photographic equipment for aircraft. Because of the success of academic science in solving specific military problems, the NRC was retained after the war’s end, though it gradually decoupled from the military.\n\nMany industrial and academic chemists and physicists came under military control during the Great War, but post-war research by the Royal Engineers Experimental Station at Porton Down and the continued operation of the National Research Council were exceptions to the overall pattern; wartime chemistry funding was a temporary redirection of a field largely driven by industry and later medicine, while physics grew closer to industry than to the military. The discipline of modern meteorology, however, was largely built from military funding. During World War I, the French civilian meteorological infrastructure was largely absorbed into the military. The introduction of military aircraft during the war as well as the role of wind and weather in the success or failure of gas attacks meant meteorological advice was in high demand. The French army (among others) created its own supplementary meteorological service as well, retraining scientists from other fields to staff it. At war's end, the military continued to control French meteorology, sending weathermen to French colonial interests and integrating weather service with the growing air corps; most of the early-twentieth century growth in European meteorology was the direct result of military funding. World War II would result in a similar transformation of American meteorology, initiating a transition from an apprenticeship system for training weathermen (based on intimate knowledge of local trends and geography) to the university-based, science-intensive system that has predominated since.\n\nIf World War I was the chemists’ war, World War II was the physicists’ war. As with other total wars, it is difficult to draw a line between military funding and more spontaneous military-scientific collaboration during World War II. Well before the Invasion of Poland, nationalism was a powerful force in the German physics community (see Deutsche Physik); the military mobilization of physicists was all but irresistible after the rise of National Socialism. German and Allied investigations of the possibility of a nuclear bomb began in 1939 at the initiative of civilian scientists, but by 1942 the respective militaries were heavily involved. The German nuclear energy project had two independent teams, a civilian-controlled team under Werner Heisenberg and a military-controlled led by Kurt Diebner; the latter was more explicitly aimed at producing a bomb (as opposed to a power reactor) and received much more funding from the Nazis, though neither was ultimately successful.\n\nIn the U.S., the Manhattan Project and other projects of the Office of Scientific Research and Development resulted in a much more extensive military-scientific venture, the scale of which dwarfed previous military-funded research projects. Theoretical work by a number of British and American scientists resulted in significant optimism about the possibility of a nuclear chain reaction. As the physicists convinced military leaders of the potential of nuclear weapons, funding for actual development was ratcheted up rapidly. A number of large laboratories were created across the United States for work on different aspects of the bomb, while many existing facilities were reoriented to bomb-related work; some were university-managed while others were government-run, but all were ultimately funded and directed by the military. The May 1945 surrender of Germany, the original intended target for the bomb, did virtually nothing to slow the project’s momentum. After Japan’s surrender immediately following the atomic bombings of Hiroshima and Nagasaki, many scientists returned to academia or industry, but the Manhattan Project infrastructure was too large—and too effective—to be dismantled wholesale; it became the model for future military-scientific work, in the U.S. and elsewhere.\n\nOther wartime physics research, particularly in rocketry and radar technology, was less significant in popular culture but much more significant for the outcome of the war. German rocketry was driven by the pursuit of \"Wunderwaffen\", resulting in the V-2 ballistic missile; the technology as well as the personal expertise of the German rocketry community was absorbed by the U.S. and the U.S.S.R. rocket programs after the war, forming the basis of long-term military funded rocketry, ballistic missile, and later space research. Rocket science was only beginning to make impact by the final years of the war. German rockets created fear and destruction in London, but had only modest military significance, while air-to-ground rockets enhanced the power of American air strikes; jet aircraft also went into service by the end of the war. Radar work before and during the war provided even more of an advantage for the Allies. British physicists pioneered long-wave radar, developing an effective system for detecting incoming German air forces. Work on potentially more precise short-wave radar was turned over to the U.S.; several thousand academic physicists and engineers not participating the Manhattan Project did radar work, particularly at MIT and Stanford, resulting in microwave radar systems that could resolve more detail in incoming flight formations. Further refinement of microwave technology led to proximity fuzes, which greatly enhanced the ability of the U.S. Navy to defend against Japanese bombers. Microwave production, detection and manipulation also formed the technical foundation to complement the institutional foundation of the Manhattan Project in much post-war defense research.\n\nIn the years immediately following World War II, the military was by far the most significant patron of university science research in the U.S., and the national labs also continued to flourish. After two years in political limbo (but with work on nuclear power and bomb manufacture continuing apace) the Manhattan Project became a permanent arm of the government as the Atomic Energy Commission. The Navy—inspired by the success of military-directed wartime research—created its own R&D organization, the Office of Naval Research, which would preside over an expanded long-term research program at Naval Research Laboratory as well as fund a variety of university-based research. Military money following up the wartime radar research led to explosive growth in both electronics research and electronics manufacturing. The Air Force became an independent service branch from the Army and established its own research and development system, and the Army followed suit (though it was less invested in academic science than the Navy or Air Force). Meanwhile, the perceived communist menace of the Soviet Union caused tensions—and military budgets—to escalate rapidly.\n\nThe Department of Defense primarily funded what has been broadly described as “physical research,” but to reduce this to merely chemistry and physics is misleading. Military patronage benefited a large number of fields, and in fact helped create a number of the modern scientific disciplines. At Stanford and MIT, for example, electronics, aerospace engineering, nuclear physics, and materials science—all physics, broadly speaking—each developed in different directions, becoming increasingly independent of parent disciplines as they grew and pursued defense-related research agendas. What began as interdepartmental laboratories became the centers for graduate teaching and research innovation thanks to the broad scope of defense funding. The need to keep up with corporate technology research (which was receiving the lion’s share of defense contracts) also prompted many science labs to establish close relationships with industry.\n\nThe complex histories of computer science and computer engineering were shaped, in the first decades of digital computing, almost entirely by military funding. Most of the basic component technologies for digital computing were developed through the course of the long-running Whirlwind-SAGE program to develop an automated radar shield. Virtually unlimited funds enabled two decades of research that only began producing useful technologies by the end of the 50s; even the final version of the SAGE command and control system had only marginal military utility. More so than with previously-established disciplines receiving military funding, the culture of computer science was permeated with a Cold War military perspective. Indirectly, the ideas of computer science also had a profound effect on psychology, cognitive science and neuroscience through the mind-computer analogy.\n\nThe history of earth science and the history of astrophysics were also closely tied to military purposes and funding throughout the Cold War. American geodesy, oceanography, and seismology grew from small sub-disciplines in into full-fledged independent disciplines as for several decades, virtually all funding in these fields came from the Department of Defense. A central goal that tied these disciplines together (even while providing the means for intellectual independence) was the figure of the Earth, the model of the earth’s geography and gravitation that was essential for accurate ballistic missiles. In the 1960s, geodesy was the superficial goal of the satellite program CORONA, while military reconnaissance was in fact a driving force. Even for geodetic data, new secrecy guidelines worked to restrict collaboration in a field that had formerly been fundamentally international; the Figure of the Earth had geopolitical significance beyond questions of pure geoscience. Still, geodesists were able to retain enough autonomy and subvert secrecy limitations enough to make use of the findings of their military research to overturn some of the fundamental theories of geodesy. Like geodesy and satellite photography research, the advent of radio astronomy had a military purpose hidden beneath official astrophysical research agenda. Quantum electronics permitted both revolutionary new methods of analyzing the universe and—using the same equipment and technology—the monitoring of Soviet electronic signals.\n\nMilitary interest in (and funding of) seismology, meteorology and oceanography was in some ways a result of the defense-related payoffs of physics and geodesy. The immediate goal of funding in these fields was to detect clandestine nuclear testing and track fallout radiation, a necessary precondition for treaties to limit the nuclear weapon technology earlier military research had created. In particular, the feasibility of monitoring underground nuclear explosions was crucial to the possibility of a comprehensive rather than Partial Nuclear Test Ban Treaty. But the military-funded growth of these disciplines continued even when no pressing military goals were driving them; as with other natural sciences, the military also found value in having ‘scientists on tap’ for unforeseen future R&D needs.\n\nThe biological sciences were also affected by military funding, but, with the exception of nuclear physics-related medical and genetic research, largely indirectly. The most significant funding sources for basic research before the rise of the military-industrial-academic complex were philanthropic organizations such as the Rockefeller Foundation. After World War II (and to some extent before), the influx of new industrial and military funding opportunities for the physical sciences prompted philanthropies to divest from physics research—most early work in high-energy physics and biophysics had been the product of foundation grants—and refocus on biological and medical research.\n\nThe social sciences also found limited military support from the 1940s to the 1960s, but much defense-minded social science research could be—and was—pursued without extensive military funding. In the 1950s, social scientists tried to emulate the interdisciplinary organizational success of the physical sciences’ Manhattan Project with the synthetic behavioral science movement. Social scientists actively sought to promote their usefulness to the military, researching topics related to propaganda (put to use in Korea), decision making, the psychological and sociological causes and effects of communism, and a broad constellation of other topics of Cold War significance. By the 1960s, economists and political scientists offered up modernization theory for the cause of Cold War nation-building; modernization theory found a home in the military in the form of Project Camelot, a study of the process of revolution, as well as in the Kennedy administration’s approach to the Vietnam War. Project Camelot was ultimately canceled because of the concerns it raised about scientific objectivity in the context of such a politicized research agenda; though natural sciences were not yet susceptible to implications of the corrupting influence of military and political factors, the social sciences were.\n\nHistorian Paul Forman, in his seminal 1987 article, proposed that not only had military funding of science greatly expanded the scope and significance of American physics, it also initiated \"a qualitative change in its purposes and character.\" Historians of science were beginning to turn to the Cold War relationship between science and the military for detailed study, and Forman’s “distortionist critique” (as Roger Geiger has described it) served to focus the ensuing debates. Forman and others (e.g., Robert Seidel, Stuart Leslie, and for the history of the social sciences, Ron Robin) view the influx of military money and the focus on applied rather than basic research as having had, at least partially, a negative impact on the course of subsequent research. In turn, critics of the distortionist thesis, beginning with Daniel Kevles, deny that the military \"seduced American physicists from, so to speak, a 'true basic physics'.\" Kevles, as well as Geiger, instead view the effects of military funding relative to such funding simply being absent—rather than put to alternate scientific use. Most recent scholarship has moved toward a tempered version of Forman's thesis, in which scientists retained significant autonomy despite the radical changes brought about by military funding.\n\n\n \n"}
{"id": "16531537", "url": "https://en.wikipedia.org/wiki?curid=16531537", "title": "Industrious Revolution", "text": "Industrious Revolution\n\nThe Industrious Revolution is the title given to a period of time, usually given as between 1600 and 1800 that led up to the Industrial Revolution. It is a term first coined by the Japanese demographic historian Akira Hayami , and accepted by other historians to help further explain the advent of the Industrial Revolution. Much of this theory deals with the spending behaviours of families in the period. It also deals with the production and consumption of goods. In fact, Industrious Revolutions are often characterized by a rise in demand for \"market-supplied goods\", which will minimize the value of domestic goods, before the ultimate consumption. Industrious Revolutions often occur during a period where labour wages have stagnated or decreased. The theory of a pre-industrial Industrious Revolution is contested within the history community.\n\nHayami introduced the concept of Industrious Revolution in a Japanese-language work published in 1967. It was coined to compare the labour-intensive technologies of Tokugawa Japan (1603-1868) with the capital-intensive technologies of Britain's Industrial Revolution. Hayami observed that the two countries took different paths due to the different mix of factor endowments (capital for Britain and labour for Japan). He introduced Industrious Revolution to describe the Japanese developmental trajectory, which - lacking the British capital - exploited the benefits of increasing labour absorption. \n\nThis proposed Industrious Revolution does not aim to replace the Industrial Revolution in history; rather it is designed to supplement it. By revamping the history of the period directly preceding the Industrial Revolution, some historians hope to ensure that people get a better understanding of a particular aspect of the Early Modern Period.\nThe basic picture painted of the pre-Industrial Revolution is that the Industrial Revolution was the result of a surplus of money and crops, which led to the development of new technology. This new technology eventually developed into factories. The Industrious Revolution addresses this belief, saying instead, that the overwhelming desire for more goods directly preceded the Industrial Revolution. The theory states that during the Industrious Revolution there was an increase in demand for goods, but that supply did not rise as quickly.\nEventually, some achievements of industry and agriculture, as well as the decisions made by households, helped to increase the supply, as well as the demand for goods. These behaviours, when combined constitute an Industrious Revolution. A quick summation of the differences between the Industrious Revolution and the Industrial Revolution is that the former is concerned with demand, and the latter is supply based. The right mindset to a productional economy and world may have increased the supply of technology, but they would have had little impact on invention without a demand for new techniques. \nThe theory of an Industrious Revolution, as put forward by historian Jan de Vries, claims that there were two parts to the Industrious Revolution. First, there was a reduction of leisure time as the utility of monetary income rose. Second, the focus of labour shifted from goods and services to marketable goods. \n\nSome within the field of cultural history have theorized that a further motive for an industrious revolution is a shift in the view on what it takes for an individual to be considered independent. Independence was equated with land ownership prior to an industrious revolution. Wage earners were granted a degraded and dependent status. Typically, as in the case of pre-industrial Great Britain and America, these dependent sorts were deemed unworthy of full citizenship rights. Industrious revolutions shift the meaning of independence from land-holding to earning a competency. A wage earner could now be considered independent if he/she earned enough money to support a household. Hard work was viewed as essential for reaching this goal.\n\nOne of the suggested hallmarks of an Industrious Revolution is that of increased work days. However, according to historians Gregory Clark and Ysbrand Van Der Werf, there has been no information found to suggest an increase in work days, in the time period between the Medieval Period and the nineteenth century. These records even indicate that before 1750, some people were working three hundred days per year. Even in the period preceding the Industrial Revolution people were working at least two hundred and ninety days in a year. So, this information would demonstrate that there was very little increase in days worked. Since increase in work loads is one of the suggested hallmarks of an Industrious Revolution, this would work against the theory.\nClark and Van Der Werf have also examined the output of a couple of English industries. For one, they looked at records of the saw mills in England. Between 1300 and 1800, the period directly preceding and following the proposed Industrious Revolution, the estimated amount of lumber sawed increased about eighty percent. However, this increase in lumber sawed can be attributed to new technologies, and not in fact the influence of an Industrious Revolution. In contrast, they mention the threshing industry. Unlike the lumber sawing business, this industry shows “clear downward movement” in threshing rates, after which there are no longer any trends. This information would help to disprove the idea of an Industrious Revolution, since, as it has been put forth, there is no universal trend displaying an increase of work habits.\n\nIn a later work, Hayami cited that de Vries and other theorists' interpretations did not use the term in the same way he does. Hayami noted that these saw Industrious Revolution and Industrial Revolution as a continuum while the original idea considers the two revolutions as opposing concepts. Hayami also stressed that the term explained how the Japanese became industrious for some reason at one point and that eventually they will no longer be. \n\nPrior the proposed era of the Industrious Revolution most goods were produced either by household or by guilds. There were many households involved in the production of marketable goods. Most of what was produced by these households were things that involved cloth- textiles, clothing, as well as art, and tapestries. These would be produced by the households, or by their respective guilds. It was even possible for guilds and merchants to outsource into more rural areas, to get some of the work done. These merchants would bring the raw materials to the workers, who would then, using the supplied materials, make the goods. For example, young girls would be hired to make silk, because they were the only people believed to have hands dexterous enough to make the silk properly. Other occupations such as knitting, a job that was never organized into guilds, could easily be done within the household.\n\nThe income of the household became dependent upon the quality and the quantity of everyone's work. Even if people were not working for an individual guild they could still supply and make items not controlled by the guilds. These would be small, but necessary items like wooden dishes, or soaps. So, basically, much of production was done by, or for, guilds. This would indicate that much of what was done was not done for one individual household, but for a larger group or organization. \n\nPrior to the Industrious Revolution, the household was the major site of production, and could be comparable to a factory. However, things were to change a bit during the Industrious Revolution. If the theory is to be believed, then there was a shift in the running of the household. The everyday goods and products used by the household would slowly shift from mostly homemade, to mostly \"commercially produced goods\". At the same time, the women would be more than likely to be able to attain jobs outside of the household. This is also seen within the context of the Industrial Revolution, where women would often find small jobs to help supplement their husband's wages. This would demonstrate the gradual movement away from the household as a centre of production.\n\nThe patterns of consumption present in England at this time period were similar to what one might find in Europe, as well as in some of the Ottoman territories. This is unsurprising, however, since most of Europe and the Ottoman Empire were all connected through trade. Through these trade connections, people were able to buy many of the luxury goods that they desired. Very common amongst the nobility was the idea of Conspicuous consumption. This can be traced back, even into the Middle Ages. People, the nobility especially, had trade connections throughout Europe, and many of them would use these connections to buy the works of art, etc. that they desired. This does not extend only to the rich however: even Medieval peasants enjoyed imported luxury items; there is evidence to suggest that some English peasants drank imported French wine.\n\nPeople were consuming products well before the Industrious Revolution. There were stock exchanges all over Europe, even London. People were also clearly making products for consumption, as the large number of guilds existing within Europe at that point would suggest. What would cause the period between 1600 and 1800 to be labeled an Industrious Revolution, by looking at the patterns of consumption, is the rise in demand for these products. So, what would set this period in time apart would be more demands for luxury items. Especially those items that could not be produced in the homes or by the guilds. During the proposed Industrious Revolution, this demand for luxury items would be greater than the supply could accommodate. A rise in the rate at which people were consuming goods, especially when combined with other factors of the times, could have potentially heralded the Industrious Revolution.\n\n"}
{"id": "203614", "url": "https://en.wikipedia.org/wiki?curid=203614", "title": "Information mapping", "text": "Information mapping\n\nInformation mapping is a research-based method for writing clear and user focused information, based on the audience's needs and the purpose of the information. The method is applied primarily to designing and developing business and technical communications. It is used as a content standard within organizations throughout the world.\n\nThe information mapping method is a research-based methodology used to analyze, organize and present information based on an audience's needs and the purpose of the information. The method applies to all subject matter and media technology. Information mapping has close ties to information visualization, information architecture, graphic design, information design, data analysis, experience design, graphic user interface design, and knowledge management systems.\n\nInformation mapping provides a number of tools for analyzing, organizing and presenting information.\n\nSome of Robert E. Horn's best-known work was his development of the theory of information types. Horn identified six types of information that account for nearly all the content of business and technical communications. The types categorize elements according to their purpose for the audience:\nThe information mapping method proposes six principles for organizing information so that it is easy to access, understand, and remember:\nDocuments written according to information mapping have a modular structure. They consist of clearly outlined information units (\"maps\" and \"blocks\") that take into account how much information a reader is able to assimilate.\n\nThere is an essential difference between an information unit and the traditional text paragraph. A \"block\" is limited to a single topic and consists of a single type of information. Blocks are grouped into \"maps\", and each map consists only of relevant blocks. The hierarchical approach to structuring information greatly facilitates electronic control of content via content management systems and knowledge management systems.\n\nThe information mapping method offers advantages to writers and readers, as well as to an entire organization.\n\nInformation mapping offers these advantages for writers:\n\nInformation mapping offers these advantages for readers:\n\nAlso an entire organization can benefit from using a content standard like information mapping if the method is used with the following objectives in mind:\n\nInformation mapping was developed in the late 20th century by Robert E. Horn, a researcher in the cognitive and behavioral sciences. Horn was interested in visual presentation of information to improve accessibility, comprehension and performance. Horn's development of the information mapping method has won him recognition from the International Society for Performance Improvement and the Association for Computing Machinery.\n\nMany independent studies have confirmed that applying the information mapping method to business and technical communications results in quicker, easier access to information, improved comprehension and enhanced performance. It also facilitates repurposing for publication in different formats.\n\nDoubts have been raised over the strength of the research Horn uses to justify some of his principles. For instance, his chunking principle requires lists, paragraphs, sub-sections and sections in a document to contain no more than 7±2 chunks of information. Horn does not state where he got this principle, but an Information Mapping website stated that the principle is \"based on George A. Miller's 1956 research\". Miller did write a paper in 1956 called \"The Magical Number Seven, Plus or Minus Two: Some Limits on our Capacity for Processing Information\", but its relevance to writing is tenuous. Miller himself said that his research had nothing to do with writing. Insisting that lists, paragraphs, sub-sections and sections throughout a document contain no more than 7±2 chunks of information paradoxically assumes that the size of what is not read in a document can influence a reader's ability to comprehend what they do read.\n\n\n"}
{"id": "4758015", "url": "https://en.wikipedia.org/wiki?curid=4758015", "title": "Instruments used in general surgery", "text": "Instruments used in general surgery\n\nSurgical instruments can be generally divided into five classes by function. These classes are:\n\nInstruments used in general surgery are:\n"}
{"id": "46550108", "url": "https://en.wikipedia.org/wiki?curid=46550108", "title": "Legal technology", "text": "Legal technology\n\nLegal technology, also known as Legal Tech, refers to the use of technology and software to provide legal services. Legal Tech companies are generally startups founded with the purpose of disrupting the traditionally conservative legal market. According to TechCrunch, as of December 2014, \"legal technology is booming, with companies attempting to disrupt the legal space at every level and from every angle\" and Forbes noted in February 2015 that there were “hundreds of legal startups popping up all over the US and Europe”.\n\nLegal technology traditionally referred to the application of technology and software to help law firms with practice management, document storage, billing, accounting and electronic discovery. Since 2011, Legal Tech has evolved to be associated more with technology startups disrupting the practice of law by giving people access to online software that reduces or in some cases eliminates the need to consult a lawyer, or by connecting people with lawyers more efficiently through online marketplaces and lawyer-matching websites.\n\nThe legal industry is widely seen to be conservative and traditional, with Law Technology Today noting that \"in 50 years, the customer experience at most law firms has barely changed\". Reasons for this include the fact law firms face weaker cost-cutting incentives than other professions (since they pass disbursements directly to their client) and are seen to be risk averse (as a minor technological error could have significant financial consequences for a client).\n\nHowever, the growth of the hiring by businesses of in-house counsel and their increasing sophistication, together with the development of email, has led to clients placing increasing cost and time pressure on their lawyers. In addition, there are increasing incentives for lawyers to become technologically competent, with the American Bar Association voting in August 2012 to amend the Model Rules of Professional Conduct to require lawyers to keep abreast of \"the benefits and risks associated with relevant technology\", and the saturation of the market leading many lawyers to look for cutting-edge ways to compete. The exponential growth in the volume of documents (mostly email) that must be reviewed for litigation cases has greatly accelerated the adoption of technology used in eDiscovery, with elements of machine language and artificial intelligence being incorporated and cloud-based services being adopted by law firms.\n\nInvestment in Legal Tech is predominantly focused in the United States with more than $254 million invested in 2014 in the United States, however there is significant growth worldwide.\n\nStanford Law School has started CodeX, the Center for Legal Informatics, an interdisciplinary research center, which also incubates companies started by law students and computer scientists. Some companies that have come out of the program include Lex Machina and Legal.io.\n\nTraditional areas of Legal Tech include:\n\nMore recent areas of growth in Legal Tech focus on:\n\nLegal research:\n\n\nSeed round:\nSeries A and beyond:\n\nOther legal technology companies:\n\n"}
{"id": "5100302", "url": "https://en.wikipedia.org/wiki?curid=5100302", "title": "List of DVD manufacturers", "text": "List of DVD manufacturers\n\nThis aims to be a complete list of DVD manufacturers.\n\nThis list is not necessarily complete or up to date - if you see a manufacturer that should be here but isn't (or one that shouldn't be here but is), please update the page accordingly. This list is only a list of brand names for DVDs and not an actual\nmanufacturers list\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "44417219", "url": "https://en.wikipedia.org/wiki?curid=44417219", "title": "List of Taiwanese inventions and discoveries", "text": "List of Taiwanese inventions and discoveries\n\nThis is a list of inventions by people who were born in Taiwan or current citizens of Taiwan.\n\n\n"}
{"id": "25338858", "url": "https://en.wikipedia.org/wiki?curid=25338858", "title": "List of largest video screens", "text": "List of largest video screens\n\nThis is a list of the largest video-capable screens in the world.\n\n\n"}
{"id": "7822632", "url": "https://en.wikipedia.org/wiki?curid=7822632", "title": "MTV-1", "text": "MTV-1\n\nThe MTV-1 Micro TV was the second model of a near pocket-sized television. The first was the Panasonic IC model TR-001 introduced in 1970. The MTV-1 was developed by Clive Sinclair (Sinclair Radionics Ltd). It was shown to the public at trade shows in London and Chicago in January, 1977, and released for sale in 1978. Development spanned 10 years and included a cash infusion of (about ) from the UK government in 1976.\n\nThe MTV-1 used a German AEG Telefunken black-and-white, electrostatic cathode ray tube (CRT), the smallest CRT built into a commercially available product, and included a rechargeable 4-AA-cell NiCad battery pack. It measured and weighed . It was able to receive either PAL or NTSC transmissions on VHF or UHF. A Welsh company, Wolsey Electronics, manufactured it for Sinclair. Custom ICs made by Texas Instruments and Sinclair contributed to its small size and low power consumption.\n\nThe original (about ) price tag proved to be too high to sell many of them, and Sinclair lost over in 1978, eventually selling its remaining inventory to liquidators at greatly reduced prices.\n\nThe MTV-1B, released later in 1978 at the much lower price of , was able to receive only British and South African UHF PAL signals.\n\n\n"}
{"id": "10550813", "url": "https://en.wikipedia.org/wiki?curid=10550813", "title": "Ministry of Earth Sciences", "text": "Ministry of Earth Sciences\n\nThe Ministry of Earth Sciences was formed in the year 2006 from a merger of the India Meteorological Department (IMD), the National Centre for Medium Range Weather Forecasting (NCMRWF), the Indian Institute of Tropical Meteorology (IITM), Pune, and Earth Risk Evaluation Centre (EREC), and the Ministry of Ocean Development.\n\nCurrently, the ministry is headed by Dr. Harsh Vardhan.\n\nThe Ministry’s mandate is to look after Atmospheric Sciences, Ocean Science & Technology and Seismology in an integrated manner.\n\n\n\nAll institutions under ESSO are connected through National Knowledge Network (NKN) and its Common User Group (CUG).\n\nAdithya HPC located at IITM, Pune is one of the largest computation facility in India.\n\n\n"}
{"id": "13113721", "url": "https://en.wikipedia.org/wiki?curid=13113721", "title": "Ministry of Education and Science of Georgia", "text": "Ministry of Education and Science of Georgia\n\nThe Ministry of Education and Science of Georgia () is a governmental body responsible for education system and children's services in Georgia. Ministry of Education works under the Minister of Education and Science of Georgia. The ministry is located on Uzandze street in Tbilisi in a historical building built in Mauritanic style. Tamar Sanikidze's resignation was announced on 3 June 2016. Her proposed replacement is Aleksandre Jejelava. \n\n"}
{"id": "31766044", "url": "https://en.wikipedia.org/wiki?curid=31766044", "title": "Mobile collaboration", "text": "Mobile collaboration\n\nMobile collaboration is a technology-based process of communicating using electronic assets and accompanying software designed for use in remote locations. Newest generation hand-held electronic devices feature video, audio, and telestration (on-screen drawing) capabilities broadcast over secure networks, enabling multi-party conferencing in real time (although real time communication is not a strict requirement of mobile collaboration and may not be applicable or practical in many collaboration scenarios).\n\nDiffering from traditional video conferencing, mobile collaboration utilizes wireless, cellular and broadband technologies enabling effective collaboration independent of location. Where traditional video conferencing has been limited to boardrooms, offices, and lecture theatres, recent technological advancements have extended the capabilities of video conferencing for use with discreet, hand-held mobile devices, permitting true mobile collaborative possibilities.\n\nThe scope of mobile collaboration takes into account a number of elements that continue to evolve in their sophistication and complexity: video, audio and telestration capabilities, conferencing and telepresence systems, collaboration tools, transmission technologies, and mobility.\n\nCisco Systems predicts \"two-thirds of the world's mobile data traffic will be video by 2015.\" The Unified Communications Interoperability Forum (UCIF), a non-profit alliance of technology vendors states that \"one important driver for the growth of UC (unified communications) is mobility and the remote worker. No segment is growing faster than mobile communications, and virtually every smart phone will be equipped with video chat, IM, directory, and other UC features within a few years.\"\n\nTo date, the use of mobile collaboration technology extends to industries as diverse as manufacturing, energy, healthcare, insurance, government and public safety. Mobile collaboration allows multiple users in multiple locations the ability to synergistically combine their input while working towards the resolution of problems or issues in today’s complex work environments. This can be done in real time with advanced video, audio and telestrator capabilities, comparable to working together in the same room but without the associated expense and downtime typically involved in getting the experts to remote locations.\n\nManufacturers of all kinds use mobile collaboration technology in a number of ways. Recent trends in globalization and outsourcing in particular, have meant that companies need to communicate with employees, suppliers, and customers the world over. The flexibility of hand-held mobile collaboration devices allow real-time communication to take place at any location where products are being designed, built, and inspected, such as an automotive assembly plant a continent away. Improved communication through mobile collaboration affects many aspects of complex manufacturing such as production line maintenance, supply chain management and equipment field service.\n\nCompanies in the energy sector face unique challenges due to, for example, the vast distances between a head office and the remote, harsh environment of an offshore oil rig, as well as the often inadequacies or absence of necessary transmission networks. Recent advancements in mobile collaboration technology and transmission networks are making it possible for employees in these situations to collaborate in secure and reliable ways with colleagues thousands of miles away. The use of mobile collaboration in the energy sector is enabling companies to conduct remote inspections, safety audits, maintenance, repair and overhaul work, as well as IT/communication infrastructure troubleshooting.\n\nAlthough telemedicine technology has been in use for a number of years in the healthcare sector, mobile collaboration technology extends these capabilities to locations now reachable through the use of hand-held devices such as a remote community, long-term care facility, or a patient’s home. Healthcare professionals in multiple locations can together view, discuss, and assess patient issues. The use of mobile collaboration technology within the healthcare sector has the potential to improve the quality and access to care, while making its delivery more cost-effective.\n\nMobile collaboration technology might also be used for remote education. From one on one tutoring to large classes it has many uses. Homeschooling could really benefit from this technology as you participate in a lecture from anywhere in the world. Most useful you can record your classes or lectures and review them. Internet schools, including higher education, will most certainly also benefit from this development in mobile education. Though these methods are not widely used they are quite useful and most likely will become widely popular.\n\nMobile collaboration between franchiser and franchisee allows modern technology to be used to allow a better flow of communications similar to face-to-face, albeit remotely via video/voice media such as smartphones, tablets, iPhones, iPads, etc. to be collectively used without requiring one party to travel to another location. This in turn reduces travel time and expenses not to mention better and quicker modes of communication.\n\nFranchisers who have several hundred franchisees find it an absolute must.\n\n"}
{"id": "481601", "url": "https://en.wikipedia.org/wiki?curid=481601", "title": "Mobile network operator", "text": "Mobile network operator\n\nA mobile network operator or MNO, also known as a wireless service provider, wireless carrier, cellular company, or mobile network carrier, is a provider of wireless communications services that owns or controls all the elements necessary to sell and deliver services to an end user including radio spectrum allocation, wireless network infrastructure, back haul infrastructure, billing, customer care, provisioning computer systems and marketing and repair organizations.\n\nIn addition to obtaining revenue by offering retail services under its own brand, an MNO may also sell access to network services at wholesale rates to mobile virtual network operators.\n\nA key defining characteristic of a mobile network operator is that an MNO must own or control access to a radio spectrum license from a regulatory or government entity. A second key defining characteristic of an MNO is that an MNO must own or control the elements of the network infrastructure necessary to provide services to subscribers over the licensed spectrum.\n\nA mobile network operator typically also has the necessary provisioning, billing and customer care computer systems and the marketing, customer care and engineering organizations needed to sell, deliver and bill for services. However, an MNO can outsource any of these systems or functions and still be considered a mobile network operator.\n\n"}
{"id": "7842528", "url": "https://en.wikipedia.org/wiki?curid=7842528", "title": "Mobile social network", "text": "Mobile social network\n\nMobile social networking (MSN) is social networking where individuals with similar interests converse and connect with one another through their mobile phone and/or tablet. Much like web-based social networking, mobile social networking occurs in virtual communities.\n\nMany web-based social networking sites, such as Facebook and Twitter, have created mobile applications to give their users instant and real-time access from anywhere they have access to the Internet. Additionally, native mobile social networks have been created, such as Instagram, Foursquare, and Strava, to allow communities to be built around mobile functionality.\n\nMore and more, the line between mobile and web is being blurred as mobile apps use existing social networks to create native communities and promote discovery, and web-based social networks take advantage of mobile features and accessibility.\n\nAs mobile web evolved from proprietary mobile technologies and networks, to full mobile access to the Internet, the distinction changed to the following types:\n\n\nWhile mobile and web-based social networking systems often work symbiotically to spread content, increase accessibility, and connect users, consumers are increasingly spending their attention on native apps compared to web browsers.\n\nThe evolution of social networking on mobile networks started in 1999 with basic chatting and texting services. With the introduction of various technologies in mobile networks, social networking has reached an advance level over four generations.\n\nTechnologies used in this generation are application-based, pre-installed on mobile handsets. Features include text-only chat via chat rooms. The people who used these services were anonymous. The services of this generation's mobile social networks can be used on a pay-as-you-go or subscription-to-service basis.\n\nThe introduction of 3G and camera phones added many features such as uploading photos, mobile search for person based on profile, and contacting/flirting with another person anonymously. Regional distributions of these features include Japan, Korea, Australia, Western Europe and US. The applications are mostly useful for dating purposes. The services of this generation's mobile social networks can be used on a pay-as-you-go or subscription-to-service basis.\n\nThe experiments for this generation mobile social networks started in 2006. It was adopted widely in 2008/2009. This generation brought tremendous changes and made mobile social networks as a part of daily life. The features include a richer user experience, automatic publishing to web profile and status updates, some Web 2.0 features, search by group/join by interests, alerts, location-based services and content sharing (especially music). Technologies include WAP 2.0, Java on the server, MMS, and voice capture. Applications introduced were customized with general interests such as music and mobile-specific content distribution. Regional distributions of this generation of mobile social networks include Japan, Korea, Western Europe, and North America. Advertising and ad-supported content become increasingly important. The services in this generation can be used with pay-as-you-go plans; subscription-based plans were still popular as networks increased their scale to become content distribution platforms.\n\nFourth generation began in 2008 and reached in 2010. All the features in third generation are advanced in this generation of social mobile networks. The features of this generation include the features of the third generation, the ability to hide/mask one's presence, asynchronous video conversation, multi-point audio chat conversation with one button, and multiplayer mobile gaming. Technologies which made these features possible are Web 2.0 widgets, Flash Lite, OpenSocial, and Open Handset Alliance. The business model of previous generations continued along with virtual currency – the purchase and trade of virtual goods.\n\nIn parallel to the increase of various technologies in mobile networks, the number of hours spent per adult on mobile devices per day has increased dramatically since 2008. As of 2014, mobile devices have surpassed desktop/laptops as the most used device per day for internet usage. A steady increase of mobile application usage over the past few years has contributed to the rise of mobile social networks, as well as to the diversity of usage of mobile social networks.\n\nAs the use of mobile social networks has increased, the location-based services within the mobile social network has also been increasing. Social network service companies now provide more location-based services for customers' wide use of the mobile devices and their convenience.\n\nMobile social networking sites allow users to create a profile, send and receive messages via phone or computer and visit an online version of a mobile site. There were different models which were adapted by different networking sites. Most of these sites have many unique features or special functions, but the main function of the site remains the same as other services. All these sites are categorized according to the following business models and usage.\n\nSimilar to there being many online social networking sites, such as Facebook and Twitter, there are just as many social network on mobile devices. They offer vast number of functions including multimedia posts, photo sharing, and instant messaging. Most of these mobile apps offer free international calling and texting capabilities. Today, social networking apps are not just for the social aspect, but are frequently used for professional aspects as well, such as LinkedIn, which is still constantly growing. Along with sharing multimedia posts and instant messaging, social networks are commonly used to connect immigrants in a new country. While the thought of moving to a new country may be intimidating for many, social media can be used to connect immigrants of the same land together to make assimilation a little less stressful.\n\nThis model is focused on the ability to send short, text-based messages to an individual, group of close friends, or even a large group of classmates, simultaneously. This category enables messages to reach the right people as quickly as possible. Many messaging apps are very popular, maybe even more than classical texting. Some social network platforms, such as Facebook, have their own native messaging applications, similar to Facebook Messenger. Different countries have a certain messenger that is predominant, like China with WeChat, Korea with KakaoTalk, and the US with WhatsApp.\n\nThis can be viewed as an advanced version of the messenger category. In addition to text messages, audio and video files can be transmitted among a group, such as Skype or Oovoo, which are forms of online video chatting. In the case of Instagram and Vine, photos and videos of personal lives are shared to either friends or to the public. Similarly, Pinterest is used to share photos, but on a more community level. The largest media sharing app today is YouTube, which allows people post videos and share with the public. Many of these services store media content online for easy storage and access.\n\nSome mobile social networks, such as Yelp, FourSquare and YikYak, allow users to search for local venues. Many of these apps publish crowd-sourced reviews and tips about restaurants, shops, places of interest and more. Yelp and FourSquare also personalizes each user's database according to their latest search and interest to make searching more efficient.\n\nThis model is about connecting people through both multi-player games and competitive single-player games. Mobile devices are always increasing their capacity for graphics performance and computing power, making them capable gaming devices. The leader in this category is Zynga, creators of \"Farmville\" and \"Words with Friends\", though it has suffered a decline. \"Hearthstone\" is another popular mobile game where players use monster and spell cards to fight each other. Many games also introduce the idea of having another player as an \"ally\" during game play. For example, in \"Naruto Blazing\", players can choose one person from a set of players to be on their team while fighting enemies throughout the game. Mobile social networks can also connect people outside of the mobile environment. \"Pokemon Go\" incorporated augmented reality to allow players to catch Pokemon while together physically while outside. Players can also battle each other at gyms in various locations in the world. Facebook has also integrated games through its chat messenger. For example, friends can play chess by sending \"@fbchess play\" to the other person or basketball by sending a basketball emoji and clicking on the emoji.\n\nThese are location-based apps that allow users to create a profile and are matched with those who have similar interests. Some of these sites use radar to ping a user if there is a matching single profile within a certain distance. Tinder was the first dating app that started the trend and has one of the largest user base. Other dating apps include Coffee Meets Bagel and OkCupid. These sites are marked with serious security measures, so that no personal details are released without the user's consent. However, there still has been several dangerous incidents that rose questions of whether Tinder-like apps are safe and should be kept around.\n\nMusic apps connect people by sharing playlists and being able to see what other people are listening to. Spotify, a very popular music site, is also used to social networking in a sense that people can see what their friends are listening to at the moment as well. Users can also follow certain artists or even friends that they want to, which is a form of “liking” a post on Facebook. Other social media music apps include radio stations like Pandora and last.fm.\n\nRecently, mobile social networks has also been used to motivate individuals to stick to their fitness and health goals. These social networks either work as a form of encouragement by rewarding the individual when they have accomplish a goal, or as a form of punishment by disciplining those who failed to accomplish their goal through a monetary cost or social pressure. An example of this network is PACT. In PACT, individuals make a weekly goal to exercise more or eat healthier and set a monetary amount that you will pay if you don’t succeed. Using the app, you can prove that you were at the gym through GPS or that you ate healthy meals by uploading pictures of the meal. If you succeed in your goal, you earn cash paid by members who didn’t keep to their goals.\n\nStrava is another mobile social network application that lets you keep track of your activities using GPS and analyze your performance through metrics such as speed and distance. Using the social network, you can meet other individuals who are also into the same activities as you and find out about new track routes or challenges or other athletic content.\n\nMobile commerce, or m-commerce, is a branch of e-commerce, that is available in a form of apps and mobile sites. In some apps like letgo, it is easy for the buyer to talk to the seller about specifics about the product or negotiate the price and this assumes a form of social networking. It also narrows the search down by cities and topics to make it more efficient. Some major e-commerce sites, such as Amazon and eBay, are also available in apps, so that people can shop anytime and anywhere. Particularly, however, some mobile social media networks add m-commerce functionalities to their applications. For instance, there is the case of the Facebook Marketplace where people can sell and purchase products through their mobile devices. Many e-commerce and m-commerce applications are also increasingly developed to interface with other applications such as mobile payment, banking, and ticketing applications so customers can easily pay or accept payments. There is the case of Instagram, which in 2018 became open to merchants using the Shopify platform. \n\nMobile payment social networks such as Venmo and Square Cash allow for person-to-person money transfer between family and friends, with a swipeable feed of payment details similar to Facebook's News Feed.\n\nThere are still many functionalities of mobile social networks that do not fall into any specific categories. One is collecting rebates. Ibotta allows users to submit receipts to collect rebates for items they have purchased. They also added in a teamwork element where you can work with your friends to reach goals such as redeeming a certain amount of dollars or redeeming a certain amount of rebates for monetary rewards. Observers are also expecting the mobile social computing platform to be pivotal in the emergence of new technologies due to its rich sensing capabilities. For instance, it can augment real-world experiences, allowing mobile devices to bridge the physical world and the Internet.\n\nThe rise of the digital age has made social media a lasting trend, Facebook is still the leader of social networks, were initially as web-based and then extended towards access via mobile browsers and smartphone apps. Compared with Twitter, Instagram, and Pinterest, Facebook continues to dominate the social media world. As of the fourth quarter of 2015, 823 million Facebook users accessed the social network exclusively through mobile device, exceed from 526 million users in the previous year. In 2016, there was Instagram started as mobile and later developed into web-based platforms as well. In 2016, there was practically 1.6 billion active users around the world. Moreover, in the United States, a study named the usage of the most popular mobile social networking percentages showed that social media audiences spent a total of 230 billion minutes on Facebook in 2014, 80% higher than Instagram. Until January 2016, 52% of users in North America accessed social media through mobile when the global mobile social penetration rate was 27%. The report in 2017 showed that around 1 billion users will visit Facebook via mobile devices and during this year, the US market plays a significant role with nearly 80% of Facebook users using mobile devices to access their accounts. Facebook mobile advertising revenue account for 10 billion dollars and occupy for 74% of revenue in total. It shows that by 2018, more than 75% of the Facebook users worldwide will access the service via their mobile phones.\n\nSafety issues (including security, privacy, and trust) in mobile social networks are concerned about the condition of being protected against different types of failure, damage, error, accidents, harm or any other non-desirable event, while mobile carriers contact each other in mobile environments. However, lack of a protective infrastructure in these networks has turned them in to convenient targets for various perils. This is the main impulse why MSNs carry disparate and intricate safety concerns and embrace divergent safety challenging problems.\n\nThere has been cases where a user was caused bodily harm through mobile social media. For example, Kurt Eichenwald was sent a tweet with a flashing animated image by another user who knew that Eichenwald had epilepsy, causing a seizure. As a result of these dangers, many mobile social networks such as Twitter and Facebook have implemented various methods for protecting user safety such as removing harmful users, detecting malware, and verifying a user's identity; however, these policies are still in the workings.\n\nOther than online safety issues, the evolution of mobile devices has also introduced new offline, or physical, safety concerns. The distractions caused by mobile social networks have cause numerous accidents due to the user not paying attention to their surroundings. According to the National Safety Council, nearly 330,000 injuries occur each year from accidents where the driver was texting while driving. The safety issues caused by distractions from mobile social network became more prominent after the release of Pokemon Go in July 2016. In Pokemon Go, users can catch pokemon while walking around outdoors on their phones. While this game has positive impacts such as getting players to exercise, increasing museum and theme park visitors, and helping single people find dates, it has also led to more accidents. In California, two men fell over 40 feet from an ocean bluff while playing Pokemon Go. In Auburn, a driver went off the road and hit a tree because he was playing Pokemon Go while driving. In Pittsburgh, a teenager crossed a highway to catch a pokemon and was hit by a car because she was distracted.\n\nAnother safety concern arose from mobile dating applications such as Tinder and Hinge, where single people are matched up with each other to go on dates. These environments make it much easier for criminals to commit crimes such as rape and murder because it is difficult for users to completely know the other person before agreeing to meet them face to face. In England and Wales, there were 204 reported crimes due to Tinder or Grindr in 2014. This number rose to 412 in 2015. On November 23, 2016, Stephen Port was convicted for rape and murder of 4 men who he met on the Grindr. In April 2016, Ingrid Lyne was murdered and her accused murderer was a man she had met on a dating app. While there are many dangers to meeting people online, it has also successfully helped single people find love and marriage. Increasing the safety procedures regarding mobile dating applications is an ongoing work by the police force and by the developers of these mobile applications.\n\nWhile Japan, Korea, and China have a higher usage rate of mobile social networks compared to other western countries, the United States is a prevalent user of mobile social networks. The US has a population of 303.82 million people and a mobile penetration of 72% with 219.73 million mobile subscribers in 2008. Informa forecasts the number of mobile subscribers to rise to 243.27 million by 2013.\n\nThe mobile data market in the US is at a developed stage of growth where non-messaging data revenues account for 20% of US operators' overall data revenues. In September 2012, the CTIA (Cellular Telephone Industries Association) announced that data service revenues rose 40% to US $14.8 billion. The CTIA announced that SMS usage had maintained its strong growth.\n\nSocial networking once began in the online space, but it has rapidly spread to mobile platforms. Currently, consumption of mobile internet usage is being driven by mobile social networking. Data shows that the US has 220.14 million online internet users which is 72.5% of the population. Flat-rate data plans have been prevalent in the US for a number of years but the customer adoption of mobile internet was slow until 2008. However, the introduction of the iPhone has definitely increased the market for mobile internet. iPhones have transformed the mobile social network market, and today there is numerous mobile development for social network apps.\n\nThe US mobile social networking market experienced steady growth in 2008 with 6.4 million mobile social network users. Since then, the number of mobile users has continued to grow and below is graph forecasting the growth until 2013. According to Statista.com, the most popular social media networking app as of 2016 is Facebook at 123.55 million monthly users, surpassing the next most popular app, Facebook Messenger, which has 97.86 million monthly users.\n\n\n\n"}
{"id": "26274846", "url": "https://en.wikipedia.org/wiki?curid=26274846", "title": "Multi-band device", "text": "Multi-band device\n\nIn telecommunications, a multi-band device (including dual-band, tri-band, quad-band and penta-band devices) is a communication device (especially a mobile phone) that supports multiple radio frequency bands. All devices which have more than one channel use multiple frequencies; a band however is a group of frequencies containing many channels. Multiple bands in mobile devices support roaming between different regions where different standards are used for mobile telephone services. Where the bands are widely separated in frequency, parallel transmit and receive signal path circuits must be provided, which increases the cost, complexity and power demand of multi-band devices.\n\nThe term quad-band describes a device that supports four frequency bands: 850 and 1900 MHz, mostly used in the Americas (ITU region 2), and the 900 and 1800 MHz bands used elsewhere. Most GSM/UMTS phones support all four bands, while most CDMA2000/1xRTT phones (mostly North America and voice transmission only) do not, and so are considered only dual-band devices. A few phones support both of the domestic frequencies but only one foreign one for limited roaming, making them tri-band phones.\n\nThe term penta-band describes a device that supports a fifth frequency band, commonly the 2100 MHz band in much of the world. The Advanced Wireless Services (AWS) 1700 MHz band also seeing increased usage.\n\nIn the United States only, the two largest carriers are instead implementing 4G LTE in the 700 MHz band, which was reallocated from TV broadcasting during the DTV transition. TV stations were forced to move to lower UHF and even far worse VHF frequencies with poorer mobile TV and even regular terrestrial TV performance, because the 700 MHz band has better radio propagation characteristics that allow mobile phone signal to penetrate deeper into buildings with less attenuation than the 1700 MHz or 2100 MHz bands.\n\nAT&T Mobility devices use former TV channel 55 nationwide (purchased from Qualcomm's defunct MediaFLO pay TV service), and also channel 56 in densely populated areas such as California and the Northeast Corridor. Verizon Wireless holds frequencies just above TV channel 51, which is still in use, causing adjacent-channel interference that is preventing the carrier from using them until the planned top-down spectrum repacking occurs. It more recently acquired higher blocks within the former TV band, which it plans to use first.\n\n"}
{"id": "2165622", "url": "https://en.wikipedia.org/wiki?curid=2165622", "title": "Neophile", "text": "Neophile\n\nNeophile or Neophiliac, a term popularised by cult writer Robert Anton Wilson, is a personality type characterized by a strong affinity for novelty. The term was used earlier by Christopher Booker in his book The Neophiliacs (1969), and by J. D. Salinger in his short story Hapworth 16, 1924 (1965).\n\nNeophiles/Neophiliacs have the following basic characteristics:\n\n\nA neophile is distinct from a revolutionary in that anyone might become a revolutionary if pushed far enough by the reigning authorities or social norms, whereas neophiles are revolutionaries by nature. Their intellectual abhorrence of tradition and repetition usually bemoans a deeper emotional need for constant novelty and change. The meaning of neophile approaches and is not mutually exclusive to the term visionary, but differs in that a neophile actively seeks first-hand experience of novelty rather than merely pontificating about it.\n\nThe opposite of a neophile is a neophobe; a person with an aversion to novelty and change. Wilson observes that neophobes tend to regard neophiles, especially extreme ones, with fear and contempt, and to brand them with titles such as \"witch,\" \"satanist,\" \"heretic,\" etc. He also speculates in his Prometheus Rising series of books that the industrial revolution and related enlightenment represents one of the first periods of history in which neophiles were a dominant force in society. Neophiles accelerate change because they like it that way.\n\nOpen-source advocate and programmer Eric S. Raymond observes that this personality is especially prevalent in certain fields of expertise; in business, these are primarily computer science and other areas of high technology. Raymond speculates that the rapid progress of these fields (especially computers) is a result of this. A neophile's love of novelty is likely to lead them into subjects outside of the normal areas of human interest. Raymond observes a high concentration of neophiles in or around what he calls \"leading edge subcultures\" such as science fiction fandom, neo-paganism, transhumanism, etc. as well as in or around nontraditional areas of thought such as fringe philosophy or the occult. Raymond observes that most neophiles have roving interests and tend to be widely well-read.\n\nThere is more than one type of neophile. There are social neophiles (the extreme social butterfly), intellectual neophiles (the revolutionary philosopher and the technophile), and physical/kinetic neophiles (the extreme sports enthusiast). These tendencies are not mutually exclusive, and might exist simultaneously in the same individual.\n\nThe word \"neophilia\" has particular significance in Internet and hacker culture. \"The New Hacker's Dictionary\" gave the following definition to neophilia:\n\nThe trait of being excited and pleased by novelty. Common among most hackers, SF fans, and members of several other connected leading-edge subcultures, including the pro-technology 'Whole Earth' wing of the ecology movement, space activists, many members of Mensa, and the Discordian/neo-pagan underground (see geek). All these groups overlap heavily and (where evidence is available) seem to share characteristic hacker tropisms for science fiction, music.\n\nResearch has uncovered a possible link between certain predisposition to some kind of neophilia and increased levels of the enzyme monoamine oxidase A.\n\n\n"}
{"id": "17037094", "url": "https://en.wikipedia.org/wiki?curid=17037094", "title": "Non-rocket spacelaunch", "text": "Non-rocket spacelaunch\n\nNon-rocket spacelaunch refers to concepts for launch into space where some or all of the needed speed and altitude are provided by something other than rockets, or by other than expendable rockets. A number of alternatives to expendable rockets have been proposed. In some systems such as a combination launch system, skyhook, rocket sled launch, rockoon, or air launch, a rocket would be part, but only part of the system used to reach orbit.\n\nPresent-day launch costs are very high – $2,500 to $25,000 per kilogram from Earth to low Earth orbit (LEO). As a result, launch costs are a large percentage of the cost of all space endeavors. If launch can be made cheaper the total cost of space missions will be reduced. \nDue to the exponential nature of the rocket equation, providing even a small amount of the velocity to LEO by other means has the potential of greatly reducing the cost of getting to orbit.\n\nLaunch costs in the hundreds of dollars per kilogram would make possible many proposed large-scale space projects such as space colonization, space-based solar power and terraforming Mars.\n\nIn this usage, the term \"static\" is intended to convey the understanding that the structural portion of the system has no internal moving parts.\n\nA space tower is a tower that would reach outer space. To avoid an immediate need for a vehicle launched at orbital velocity to raise its perigee, a tower would have to extend above the edge of space (above the 100 km Kármán line), but a far lower tower height could reduce atmospheric drag losses during ascent. If the tower went all the way to geosynchronous orbit at approximately 36,000 km, or 22,369 miles, objects released at such height could then drift away with minimal power and would be in a circular orbit. The concept of a structure reaching to geosynchronous orbit was first conceived by Konstantin Tsiolkovsky.\nThe original concept envisioned by Tsiolkovsky was a compression structure. \nBuilding a compression structure from the ground up proved an unrealistic task as there was no material in existence with enough compressive strength to support its own weight under such conditions.\nOther ideas use very tall compressive towers to reduce the demands on launch vehicles. The vehicle is \"elevated\" up the tower, which may extend above the atmosphere and is launched from the top. Such a tall tower to access near-space altitudes of has been proposed by various researchers.\n\nThe height is limited by materials, with higher structures possible if the structure tapers (i.e. the upper parts are narrower than the bottom), but with current construction techniques, cost increases exponentially with construction height. Buckling may be a failure mode before exceeding a material's nominal compressive yield strength (though designs such as with a truss may help compensate), but, aside from that and aside from design against weather, the theoretical scale height of a structure is the allowable load of its material divided by the product of density and local gravitational acceleration, where needed material cross-section increases by a factor of \"e\" (2.718...) over each scale height.\n\nFor common plain carbon steel under a typical allowable stress limit, its scale height is ≈ 1.635 kilometer. A 4.9-kilometer-high tower (3 × its scale height) of such would accordingly mass at least 20 times the weight supported at its top (as \"e\" ≈ 20). In contrast, an example of a more expensive high-performance aerospace material, Amoco T300/ERL1906 carbon composite, has a scale height of 54 kilometers at a safety factor of 2, though construction challenges, including wind loading, would apply. Earth's atmosphere has approximately 50% of its mass under 6 kilometers elevation, 90% below 16 kilometers, and 99% below 30 kilometers of altitude.\n\nNatural mountains reach up to 9 km altitude. As of 2018, the tallest man-made structure is the Burj Khalifa, which is 829.8 m tall. A tower or other high-altitude facility could form one component of a launch system, such as being the base station of a space elevator, or a support pillar for the distal part of a mass driver or the \"gun barrel\" of a space gun.\n\nTensile structures for non-rocket spacelaunch are proposals to use long, very strong cables (known as tethers) to lift a payload into space. Tethers can also be used for changing orbit once in space.\n\nOrbital tethers can be tidally locked (skyhook) or rotating (rotovators). They can be designed (in theory) to pick up the payload when the payload is stationary or when the payload is hypersonic (has a high but not orbital velocity).\n\nEndo-atmospheric tethers can be used to transfer kinetics (energy and momentum) between large conventional aircraft (subsonic or low supersonic) or other motive force and smaller aerodynamic vehicles, propelling them to hypersonic velocities without exotic propulsion systems.\n\nA skyhook is a theoretical class of orbiting tether propulsion intended to lift payloads to high altitudes and speeds. Proposals for skyhooks include designs that employ tethers spinning at hypersonic speed for catching high speed payloads or high altitude aircraft and placing them in orbit.\n\nA space elevator is a proposed type of space transportation system. Its main component is a ribbon-like cable (also called a tether) anchored to the surface and extending into space above the level of geosynchronous orbit. As the planet rotates, the centrifugal force at the upper end of the tether counteracts gravity, and keeps the cable taut. Vehicles can then climb the tether and reach orbit without the use of rocket propulsion.\n\nSuch a cable could be made out of any material able to support itself under tension by tapering the cable's diameter sufficiently quickly as it approached the Earth's surface. On Earth, with its relatively strong gravity, current materials are not sufficiently strong and light. With conventional materials, the taper ratio would need to be very large, increasing the total launch mass to a fiscally infeasible degree. However, carbon nanotube or boron nitride nanotube based materials have been proposed as the tensile element in the tether design. Their measured strengths are high compared to their linear densities. They hold promise as materials to make an Earth-based space elevator possible.\n\nLandis and Cafarelli suggested that a tension structure (\"space elevator\") extending downward from geosynchronous orbit could be combined with the compression structure (\"Tsiolkovski tower\") extending upward from the surface, forming the combined structure reaching geosynchronous orbit from the surface, and having structural advantages over either one individually.\n\nThe space elevator concept is also applicable to other planets and celestial bodies. For locations in the Solar System with weaker gravity than Earth's (such as the Moon or Mars), the strength-to-density requirements aren't as great for tether materials. Currently available materials (such as Kevlar) could serve as the tether material for elevators there.\n\nAn endo-atmospheric tether uses the long cable within the atmosphere to provide some or all of the velocity needed to reach orbit. The tether is used to transfer kinetics (energy and momentum) from a massive, slow end (typically a large subsonic or low supersonic aircraft) to a hypersonic end through aerodynamics or centripetal action. The \"Kinetics Interchange TEther (KITE) Launcher\" is one proposed endo-atmospheric tether.\n\nA space fountain is a proposed form of space elevator that does not require the structure to be in geosynchronous orbit, and does not rely on tensile strength for support. In contrast to the original space elevator design (a tethered satellite), a space fountain is a tremendously tall tower extending up from the ground. Since such a tall tower could not support its own weight using traditional materials, massive pellets are projected upward from the bottom of the tower and redirected back down once they reach the top, so that the force of redirection holds the top of the tower aloft.\n\nAn orbital ring is a concept for a space elevator that consists of a ring in low Earth orbit that rotates at slightly above orbital speed, and has fixed tethers hanging down to the ground.\n\nThe first design of an orbital ring offered by A. Yunitsky in 1982.\n\nIn the 1982 Paul Birch JBIS design of an orbital ring system, a rotating cable is placed in a low Earth orbit, rotating at slightly faster than orbital speed. Not in orbit, but riding on this ring, supported electromagnetically on superconducting magnets, are ring stations that stay in one place above some designated point on Earth. Hanging down from these ring stations are short space elevators made from cables with high tensile-strength-to-mass ratio. Birch claimed that the ring stations, in addition to holding the tether, could accelerate the orbital ring eastwards, causing it to precess around Earth. If it were possible to make the precession rate large enough – once per day, the rate of rotation of the Earth – the ring would be \"geostationary\" without having to be at the normal geostationary altitude or even in the equatorial plane.\n\nA launch loop or Lofstrom loop is a design for a belt-based maglev orbital launch system that would be around 2000 km long and maintained at an altitude of up to 80 km (50 mi). Vehicles weighing 5 metric tons would be electromagnetically accelerated on top of the cable which forms an acceleration track, from which they would be projected into Earth orbit or even beyond. The structure would constantly need around 200 MW of power to keep it in place.\n\nThe system is designed to be suitable for launching humans for space tourism, space exploration and space colonization with a maximum of 3 g acceleration. Some other Launch Loops are developed in.\n\nOne proposed design is a freestanding tower composed of high strength material (e.g. kevlar) tubular columns inflated with a low density gas mix, and with dynamic stabilization systems including gyroscopes and \"pressure balancing\".<ref name='hdl.handle.net/10315/2587'> (P. 7.)</ref> Suggested benefits in contrast to other space elevator designs include avoiding working with the great lengths of structure involved in some other designs, construction from the ground instead of orbit, and functional access to the entire range of altitudes within the design's practical reach. The design presented is \"at 5 km altitude and extending to 20 km above sea level\", and the authors suggest that \"the approach may be further scaled to provide direct access to altitudes above 200 km\".\n\nA major difficulty of such a tower is buckling since it is a long slender construction.\n\nWith any of these projectile launchers, the launcher gives a high velocity at, or near, ground level. In order to achieve orbit, the projectile must be given enough extra velocity to punch through the atmosphere, unless it includes an additional propulsion system (such as a rocket). Also, the projectile needs either an internal or external means to perform orbital insertion. The designs below fall into three categories, electrically driven, chemically driven, and mechanically driven.\n\nElectrical launch systems include mass drivers, railguns, and coilguns. All of these systems use the concept of a stationary launch track which uses some form of linear electrical motor to accelerate a projectile.\n\nA mass driver is basically a very long and mainly horizontally aligned launch track or tunnel for space launch, curved upwards at the end. The concept was proposed by Arthur C. Clarke in 1950, and was developed in more detail by Gerard K. O'Neill, working with the Space Studies Institute, focusing on the use of a mass driver for launching material from the Moon.\n\nA mass driver uses some sort of repulsion to keep a payload separated from the track or walls. Then it uses a linear motor (an alternating-current motor such as in a coil gun, or a homopolar motor as in a railgun) to accelerate the payload to high speeds. After leaving the launch track, the payload would be at its launch velocity.\n\nStarTram is a proposal to launch vehicles directly to space by accelerating them with a mass driver. Vehicles would float by maglev repulsion between superconductive magnets on the vehicle and the aluminum tunnel walls while they were accelerated by AC magnetic drive from aluminum coils. The power required would probably be provided by superconductive energy storage units distributed along the tunnel. Vehicles could coast up to low or even geosynchronous orbital height; then a small rocket motor burn would be required to circularize the orbit.\n\nCargo-only Generation 1 systems would accelerate at 10–20 Gs and exit from a mountain top. While not suitable for passengers, they could put cargo into orbit for $40 per kilogram, 100 times cheaper than rockets.\n\nPassenger-capable Generation 2 systems would accelerate for a much longer distance at 2 Gs. The vehicles would enter the atmosphere at an altitude of 20 km from an evacuated tunnel restrained by Kevlar tethers and supported by magnetic repulsion between superconducting cables in the tunnel and on the ground. For both Gen 1–2 systems, the mouth of the tube would be open during vehicle acceleration, with air kept out by magnetohydrodynamic pumping.\n\nA space gun is a proposed method of launching an object into outer space using a large gun, or cannon. Science fiction writer Jules Verne proposed such a launch method in \"From the Earth to the Moon\", and in 1902 a movie, \"A Trip to the Moon\", was adapted.\n\nHowever, even with a \"gun barrel\" through both the Earth's crust and troposphere, the g-forces required to generate escape velocity would still be more than what a human tolerates. Therefore, the space gun would be restricted to freight and ruggedized satellites. Also, the projectile needs either an internal or external means to stabilize on orbit.\n\nGun launch concepts do not always use combustion. In pneumatic launch systems, a projectile is accelerated in a long tube by air pressure, produced by ground-based turbines or other means. In a light-gas gun, the pressurant is a gas of light molecular weight, to maximize the speed of sound in the gas.\n\nIn the 1990s, John Hunter of Quicklaunch proposed use of a 'Hydrogen Gun' to launch unmanned payloads to orbit for less than the regular launch costs.\n\nA ram accelerator also uses chemical energy like the \"space gun\" but it is entirely different in that it relies on a jet-engine-like propulsion cycle utilizing ramjet and/or scramjet combustion processes to accelerate the projectile to extremely high speeds.\n\nIt is a long tube filled with a mixture of combustible gases with a frangible diaphragm at either end to contain the gases. The projectile, which is shaped like a ram jet core, is fired by another means (e.g., a space gun, discussed above) supersonically through the first diaphragm into the end of the tube. It then burns the gases as fuel, accelerating down the tube under jet propulsion. Other physics come into play at higher velocities.\n\nA blast wave accelerator is similar to a \"space gun\" but it differs in that rings of explosive along the length of the barrel are detonated in sequence to keep the accelerations high. Also, rather than just relying on the pressure behind the projectile, the blast wave accelerator specifically times the explosions to squeeze on a tail cone on the projectile, as one might shoot a pumpkin seed by squeezing the tapered end.\n\nIn a slingatron, projectiles are accelerated along a rigid tube or track that typically has circular or spiral turns, or combinations of these geometries in two or three dimensions. A projectile is accelerated in the curved tube by propelling the entire tube in a small-amplitude circular motion of constant or increasing frequency without changing the orientation of the tube, i.e. the entire tube gyrates but does not spin. An everyday example of this motion is stirring a beverage by holding the container and moving it in small horizontal circles, causing the contents to spin, without spinning the container itself.\n\nThis gyration continually displaces the tube with a component along the direction of the centripetal force acting on the projectile, so that work is continually done on the projectile as it advances through the machine. The centripetal force experienced by the projectile is the accelerating force, and is proportional to the projectile mass.\n\nIn air launch, a carrier aircraft carries the space vehicle to high altitude and speed before release. This technique was used on the suborbital X-15 and SpaceshipOne vehicles, and for the Pegasus orbital launch vehicle.\n\nThe main disadvantages are that the carrier aircraft tends to be quite large, and separation within the airflow at supersonic speeds has never been demonstrated, thus the boost given is relatively modest.\n\nA spaceplane is an aircraft designed to pass the edge of space. It combines some features of an aircraft with some of a spacecraft. Typically, it takes the form of a spacecraft equipped with aerodynamic surfaces, one or more rocket engines, and sometimes additional airbreathing propulsion as well.\n\nEarly spaceplanes were used to explore hypersonic flight (e.g. X-15).\n\nSome air-breathing engine-based designs (cf X-30) such as aircraft based on scramjets or pulse detonation engines could potentially achieve orbital velocity or go some useful way to doing so; however, these designs still must perform a final rocket burn at their apogee to circularize their trajectory to avoid returning to the atmosphere. Other, reusable turbojet-like designs like Skylon which uses precooled jet engines up to Mach 5.5 before employing rockets to enter orbit appears to have a mass budget that permits a larger payload than pure rockets while achieving it in a single stage.\n\nBalloons can raise the initial altitude of rockets. However, balloons have relatively low payload (although see the Sky Cat project for an example of a heavy-lift balloon intended for use in the lower atmosphere), and this decreases even more with increasing altitude.\n\nThe lifting gas could be helium or hydrogen. Helium is not only expensive in large quantities but is also a nonrenewable resource. This makes balloons an expensive launch assist technique. Hydrogen could be used as it has the advantage of being cheaper and lighter than helium, but the disadvantage of also being highly flammable. Rockets launched from balloons, known as \" rockoons\", have been demonstrated but to date, only for suborbital (\"sounding rocket\") missions. The size of balloon that would be required to lift an orbital launch vehicle would be extremely large.\n\nOne prototype of a balloon launch platform has been made by JP Aerospace as \"Project Tandem\", although it has not been used as a rocket launch vehicle. JP Aerospace further proposes a hypersonic, lighter than air upper stage. A Spanish company, zero2infinity, is officially developing a launcher system called bloostar based on the rockoon concept, expected to be operational by 2018.\n\nGerard K. O'Neill proposed that by using very large balloons it may be possible to construct a space port in the stratosphere. Rockets could launch from it or a mass driver could accelerate payloads into the orbit. This has the advantage that most (about 90%) of the atmosphere is below the space port.\n\nA SpaceShaft is a proposed version of an atmospherically buoyant structure that would serve as a system to lift cargo to near-space altitudes, with platforms distributed at several elevations that would provide habitation facilities for long term human operations throughout the mid-atmosphere and near-space altitudes. For space launch, it would serve as a non-rocket first stage for rockets launched from the top.\n\nSeparate technologies may be combined. In 2010, NASA suggested that a future scramjet aircraft might be accelerated to 300 m/s (a solution to the problem of ramjet engines not being startable at zero airflow velocity) by electromagnetic or other sled launch assist, in turn air-launching a second-stage rocket delivering a satellite to orbit.\n\nAll forms of projectile launchers are at least partially hybrid systems if launching to low Earth orbit, due to the requirement for orbit circularization, at a minimum entailing several percent of total delta-v to raise perigee (e.g. a tiny rocket burn), or in some concepts much more from a rocket thruster to ease ground accelerator development.\n\nSome technologies can have exponential scaling if used in isolation, making the effect of combinations be of counter-intuitive magnitude. For instance, 270 m/s is under 4% of the velocity of low Earth orbit, but a NASA study estimated that Maglifter sled launch at that velocity could increase the payload of a conventional ELV rocket by 80% when also having the track go up a 3000‑meter mountain.\n\nForms of ground launch limited to a given maximum acceleration (such as due to human g-force tolerances if intended to carry passengers) have the corresponding minimum launcher length scale not linearly but with velocity squared. Tethers can have even more non-linear, exponential scaling. The tether-to-payload mass ratio of a space tether would be around 1:1 at a tip velocity 60% of its characteristic velocity but becomes more than 1000:1 at a tip velocity 240% of its characteristic velocity. For instance, for anticipated practicality and a moderate mass ratio with current materials, the HASTOL concept would have the first half (4 km/s) of velocity to orbit be provided by other means than the tether itself.\n\nA proposal to use a hybrid system combining a mass driver for initial lofting followed by additive thrust by a series of ground-based lasers sequenced according to wavelength was proposed by Mashall Savage in the book \"\" as one of the core theses of the book, but the idea has not been pursued to any notable degree. Savage's specific proposals proved to be infeasible on both engineering and political grounds, and while the difficulties could be overcome, the group Savage founded, now called the Living Universe Foundation, has been unable to raise significant funds for research.\n\nCombining multiple technologies would in itself be an increase to complexity and development challenges, but reducing the performance requirements of a given subsystem may allow reduction in its individual complexity or cost. For instance, the number of parts in a liquid-fueled rocket engine may be two orders of magnitude less if pressure-fed rather than pump-fed if its delta-v requirements are limited enough to make the weight penalty of such be a practical option, or a high-velocity ground launcher may be able to use a relatively moderate performance and inexpensive solid fuel or hybrid small motor on its projectile. Assist by non-rocket methods may compensate against the weight penalty of making an orbital rocket reusable. Though suborbital, the first private manned spaceship, SpaceShipOne had reduced rocket performance requirements due to being a combined system with its air launch.\n\n\n"}
{"id": "27240903", "url": "https://en.wikipedia.org/wiki?curid=27240903", "title": "Online vetting", "text": "Online vetting\n\nOnline vetting, also known as cyber-vetting is increasingly being used by potential employers and other acquaintances to vet people's online presence or \"internet reputation\" (\"netrep\") on social network services such as Facebook, MySpace, Twitter, Bebo, and LinkedIn. Employers may check profiles, posts, and photographs for indications that the candidate is unsuitable.\n\nA survey in 2007 found that half of UK employees would be outraged if their employers looked up information about them on social networking sites, and 56% thought it would be unethical. Employer surveys found that between approximately 20-67% of employers conduct internet searches, including of social networking sites, and that some have turned down applicants as a result of their searches. 21% of colleges and universities surveyed said they looked at the social networking of prospective students, usually for those applying for scholarships and other limited awards and programmes. Prospective political appointees to the Obama administration were asked to list all their blog posts, any emails, text messages, and instant messages that could suggest a conflict of interest or public source of embarrassment, the URLs of any sites that featured them in a personal or professional capacity, and all of their online aliases.\n\nJob applicants have been refused due to criticising previous employers and discussing company information online, as well as for posting provocative and inappropriate photographs, drinking or drug use, poor communication skills, making discriminatory comments, and lying about qualifications. Several companies offer online reputation management services, including helping to remove embarrassing information from websites.\n\nAnother recent research findings conducted with recruiters published in 2017 listed three primary function of a cybervetting process:\n\n\nThe cybervetting is so contradictory that most of the participants didn’t acknowledge its usage regardless the familiarity they have with this theme. Only a few number of participants talked openly about having cybervetting current candidates.\n\nLegal experts have warned human resources departments about vetting prospective employees online, due to the possibility of discrimination and the unreliability of this information. The chairman of the UK Children's Charities' Coalition on Internet Safety argued that it was \"possibly illegal, but certainly unethical\". While the Information Commissioner's Office advised that just looking at information on someone's social networking profiles would not be illegal, an employment law specialist noted that under the Data Protection Act 1998, processing and storing the information or using it to make discriminatory decisions could be.\n\nAge discrimination might result from such a practice, due to the age profile of users of social networking sites. Failed candidates may be able to use discrimination legislation to ask about vetting operations and even ask for IT records to check access to social networks. In the US, vetting using social networking sites risks breaching the Fair Credit Reporting Act (FCRA), which requires employers to gain the consent of applicants before doing a background check, state laws that limit the consideration of off-duty conduct in making employment decisions, and any searches risk breaching prohibitions against commercial use contained in the terms of service of the social networking sites.\n\nIn 2006, a trainee teacher at a high school in Pennsylvania was denied her teaching degree after her supervisor found a picture she posted on MySpace captioned \"Drunken pirate\" and deemed it \"unprofessional\". She sued, arguing that by acting on the basis of her legal out-of-hours behavior Millersville University had breached her First Amendment rights, but a federal district court ruled that the photograph was not \"protected speech\" under the First Amendment.\n\n\n\n"}
{"id": "27352320", "url": "https://en.wikipedia.org/wiki?curid=27352320", "title": "Open-source appropriate technology", "text": "Open-source appropriate technology\n\nOpen-source appropriate technology (OSAT) is appropriate technology developed through the principles of the open-design movement. OSAT refers to, on the one hand, technology designed with special consideration to the environmental, ethical, cultural, social, political, and economic aspects of the community it is intended for. On the other hand, OSAT is developed in the open and licensed in such a way as to allow their designs to be used, modified and distributed freely.\n\nOpen source is a development method for appropriate technology that harnesses the power of distributed peer review and transparency of process. is an example of open-source appropriate technology. There anyone can both learn how to make and use AT free of concerns about patents. At the same time anyone can also add to the collective open-source knowledge base by contributing ideas, observations, experimental data, deployment logs, etc. It has been claimed that the potential for open-source-appropriate technology to drive applied sustainability is enormous. The built in continuous peer-review can result in better quality, higher reliability, and more flexibility than conventional design/patenting of technologies. The free nature of the knowledge also obviously provides lower costs, particularly for those technologies that do not benefit to a large degree from scale of manufacture. Finally, OSAT also enables the end to predatory intellectual property lock-in. This is particularly important in the context of technology focused on relieving suffering and saving lives in the developing world.\n\nThe \"open-source\" model can act as a driver of sustainable development. Reasons include:\n\n\nFor solutions, many researchers, companies, and academics do work on products meant to assist sustainable development. Vinay Gupta has suggested that those developers agree to three principles:\n\n\nThe ethics of information sharing in this context has been explored in depth.\n\n\n\n\nAppropriate technology is designed to promote decentralized, labor-intensive, energy-efficient and environmentally sound businesses. Carroll Pursell says that the movement declined from 1965 to 1985, due to an inability to counter advocates of agribusiness, large private utilities, and multinational construction companies. Recently (2011), several barriers to OSAT deployment have been identified:\n\n"}
{"id": "55049050", "url": "https://en.wikipedia.org/wiki?curid=55049050", "title": "Outcome Health", "text": "Outcome Health\n\nOutcome Health is a healthcare technology company founded by Rishi Shah and valued at $5.6 billion in May 2017. It is registered in Delaware as ContextMedia Health LLC.\n\nIn May 2017, a funding round with Goldman Sachs, CapitalG, Pritzker Group, and others invested $600 million in Outcome Health, giving it a $5.6 billion valuation. This is the largest single funding round in Chicago since Groupon in 2011, when it raised $950 million in its fifth funding round.\n\nAccording to a report in The Wall Street Journal unnamed former employees and advertisers accused the company of overcharging their customers for advertisements and misquoting third-party analyses and falsifying documents on the ads' performance. According to the accusations, Outcome Health reported that the ads appeared on more video screens than they had installed. Lanny Davis, a company spokesperson, responded by saying a law firm had been hired to \"review allegations about certain employees’ conduct that have been raised internally.\"\n\nAs of January 2018, Outcome Health decided to settle outstanding investor lawsuits in exchange for having Shah and Agarwal step down.\n\nIn June 2018, Matt McNally, former chief media officer at Publicis Health, was announced as the company's new CEO. \n\n\n\n\n\n\nhttps://www.wsj.com/articles/outcome-healths-investors-receive-subpoenas-from-justice-department-1510276020\n"}
{"id": "7531185", "url": "https://en.wikipedia.org/wiki?curid=7531185", "title": "Outline of space exploration", "text": "Outline of space exploration\n\nThe following outline is provided as an overview of and topical guide to space exploration:\n\nSpace exploration – use of astronomy and space technology to explore outer space. Physical exploration of space is conducted both by human spaceflights and by robotic spacecraft.\n\nSpace exploration\n\n\n\n\n\nLunar (the Moon)\nSun\nMercury\nVenus\nMars\nOuter Solar System\nBeyond the Solar System\n\n\n\n\n\n"}
{"id": "811550", "url": "https://en.wikipedia.org/wiki?curid=811550", "title": "Over-the-air programming", "text": "Over-the-air programming\n\nOver-the-Air programming (OTA) refers to various methods of distributing new software, configuration settings, and even updating encryption keys to devices like cellphones, set-top boxes or secure voice communication equipment (encrypted 2-way radios). One important feature of OTA is that one central location can send an update to all the users, who are unable to refuse, defeat, or alter that update, and that the update applies immediately to everyone on the channel. A user could \"refuse\" OTA but the \"channel manager\" could also \"kick them off\" the channel automatically.\n\nIn the context of the mobile content world these include over-the-air service provisioning (OTASP), over-the-air provisioning (OTAP) or over-the-air parameter administration (OTAPA), or provisioning handsets with the necessary settings with which to access services such as WAP or MMS.\n\nAs mobile phones accumulate new applications and become more advanced, OTA configuration has become increasingly important as new updates and services come on stream. OTA via SMS optimizes the configuration data updates in SIM cards and handsets and enables the distribution of new software updates to mobile phones or provisioning handsets with the necessary settings with which to access services such as WAP or MMS. OTA messaging provides remote control of mobile phones for service and subscription activation, personalization and programming of a new service for mobile operators and telco third parties.\n\nVarious standardization bodies were established to help develop, oversee, and manage OTA. One of them is the Open Mobile Alliance (OMA).\n\nMore recently, with the new concepts of Wireless Sensor Networks and the Internet of Things, where the networks consist of hundreds or thousands of nodes, OTA is taken to a new direction: for the first time OTA is applied using unlicensed frequency bands (868 MHz, 900 MHz, 2400 MHz) and with low consumption and low data rate transmission using protocols such as 802.15.4 and ZigBee.\n\nMotes are often located in places that are either remote or difficult to access. As an example, Libelium has implemented a smart and easy-to-use OTA programming system for ZigBee WSN devices. This system enables firmware upgrades without the need of physical access, saving time and money if the nodes must be re-programmed.\n\nOn modern mobile devices such as smartphones, an over-the-air update may refer simply to a software update that is distributed over Wi-Fi or mobile broadband using a function built into the operating system, with the \"over-the-air\" aspect referring to its use of wireless internet instead of requiring the user to connect the device to a computer via USB to perform the update.\n\nFirmware updates are available for download from the OTA service.\n\nThe OTA mechanism requires the existing software and hardware of the target device to support the feature, namely the receipt and installation of new software received via the wireless network from the provider.\n\nNew software is transferred to the phone, installed, and put into use. It is often necessary to turn the phone off and back on for the new programming to take effect, though many phones will automatically perform this action.\n\nDepending on implementation, OTA software delivery can be initiated upon action, such as a call to the provider's customer support system or other dialable service, or can be performed automatically. Typically it is done via the former method to avoid service disruption at an inconvenient time, but this requires subscribers to manually call the provider. Often, a carrier will send a broadcast SMS text message to all subscribers (or those using a particular model of phone) asking them to dial a service number to receive a software update.\n\nVerizon Wireless in the U.S. provides a number of OTA functions to its subscribers via the *228 service code. Option 1 updates phone configuration, option 2 updates the PRL. Similarly Voitel Wireless and StraightTalk, which both use Verizon network, use *22890 service code to program Verizon based wireless phones. Interop Technologies provides a number of nationwide wireless operators in the US with an SS7 Based Over-the-Air device management solution. This solution allows operators to manage wireless device functionality including renumbering handsets, updating phone settings, applications and subscriber data and adjusting PRL to manage cost structures.\n\nTo provision parameters in a mobile device OTA, the device needs to have a provisioning client capable of receiving, processing and setting the parameters. For example, a Device Management client in a device may be capable of receiving and provisioning applications, or connectivity parameters.\n\nIn general, the term OTA implies the use of wireless mechanisms to send provisioning data or update packages for firmware or software updates to a mobile device — this is so that the user does not have to go to a store or a service center to have applications provisioned, parameters changed or firmware or software updated. Non-OTA options for a user are a) to go to a store and seek help b) use a PC and a cable to connect to the device and change settings on a device, add software to device, etc.\n\nThere are a number of standards that describe OTA functions. One of the first was the GSM 03.48 series.\nThe ZigBee suite of standards includes the ZigBee Over-the-Air Upgrading Cluster which is part of the ZigBee Smart Energy Profile and provides an interoperable (vendor-independent) way of updating device firmware.\n\nOTA is similar to firmware distribution methods used by other mass-produced consumer electronics, such as cable modems, which use TFTP as a way to remotely receive new programming, thus reducing the amount of time spent by both the owner and the user of the device on maintenance.\n\nOver-the-air provisioning (OTAP) is also available in wireless environments (though it is disabled by default for security reasons). It allows an access point (AP) to discover the IP address of its controller. When enabled, the controller tells the other APs to include additional information in the Radio Resource Management Packets (RRM) that would assist a new access point in learning of the controller. It is sent in plain text however, which would make it vulnerable to sniffing. That's why it is disabled by default.\n\n"}
{"id": "57914571", "url": "https://en.wikipedia.org/wiki?curid=57914571", "title": "PFCP", "text": "PFCP\n\nPacket Forwarding Control Protocol (PFCP) is a 3GPP protocol used on the Sx/N4 interface between the control plane and the user plane function, specified in TS 29.244. It is one of the main protocols introduced in the 5G Next Generation Mobile Core Network (aka 5GC), but also used in the 4G/LTE EPC to implement the Control and User Plane Separation (CUPS). PFCP and the associated interfaces seek to formalize the interactions between different types of functional elements used in the Mobile Core Networks as deployed by most operators providing 4G, as well as 5G, services to mobile subscribers. These 2 types of components are: \n\n\nPFCP's scope is similar to that of OpenFlow, however it was engineered to serve the particular use-case of Mobile Core Networks. PFCP lacks the same general-purpose targets, describing well the 3GPP-specific functional basic blocks of packet forwarding used in 2G, 3G, 4G/LTE, Non-3GPP WiFi and 5G networks.\n\nAlbeit similar to GTP in concepts and implementation, PFCP is complementary to it. It provides the control means for a signaling component of the Control-Plane to manage packet processing and forwarding performed by an User-Plane component. Typical EPC or 5G Packet Gateways are split by the protocol in 2 functional parts, allowing for a more natural evolution and scalability.\n\nThe PFCP protocol is used on the following 3GPP mobile core interfaces:\n\n\nNote: Sxa and Sxb can be combined, in case a merged SGW/PGW is implemented.\n\nThe Control-Plane functional element (e.g. PGW-C, SMF) controls the packet processing and forwarding in the User-Plane functional elements (e.g. PGW-U, UPF), by establishing, modifying or deleting PFCP Sessions. Per session multiple PDRs, FARs, QERs, URR and/or BARs are sent.\n\nHere are the main concepts used, organized in their logical association model:\n\n\nIEs are defined either as having a proprietary encoding, or as grouped. Grouped IEs are simply a list of other IEs, encoded one after the other like in the PFCP Message Payload.\n\nIE Types 0..32767 are 3GPP specific and do not have an Enterprise-ID set. IE Types 32768..65535 can be used by custom implementation and the Enterprise-ID must be set to the IANA SMI Network Management Private Enterprise Codes of the issuing party.\n\nVery similar to GTP-C, PFCP uses UDP. Port 8805 is reserved. \n\nFor reliability, a similar re-transmission strategy as for GTP-C is employed, lost messages being sent N1-times at T1-intervals. Transactions are identified by the 3-byte long Sequence Number, the IP address and port of the communication peer.\n\nThe protocol includes an own Heart-beat Request/Response model, which allows monitoring the availability of communication peers and detecting restarts (by use of a Recovery-Timestamp Information Element).\n\nFor User-Plane packet exchanges between the Control and User Plane functional elements, GTP-U for the Sx-u interface, or alternatively a simpler UDP or Ethernet encapsulation for the N4-u interface (to be confirmed, as standards are still incomplete).\n\n"}
{"id": "38370599", "url": "https://en.wikipedia.org/wiki?curid=38370599", "title": "Quantum vacuum thruster", "text": "Quantum vacuum thruster\n\nA quantum vacuum thruster (QVT or Q-thruster) is a theoretical system that uses the same principles and equations of motion that a conventional plasma thruster would use, namely magnetohydrodynamics (MHD), to make predictions about the behavior of the propellant. However, rather than using a conventional plasma as a propellant, a QVT uses the quantum vacuum fluctuations of the zero-point field. If QVT systems were to truly work they would eliminate the need to carry any propellant, as the system uses the quantum vacuum to assist with thrust. It would also allow for much higher specific impulses for QVT systems compared to other spacecraft as they would be limited only by their power supply’s energy storage densities. Harold White's Advanced Propulsion Physics Laboratory (NASA Eagleworks) suggests that their RF cavity may be an example of a quantum vacuum thruster (QVT or Q-thruster).\n\nThe name and concept is controversial. In 2008, Yu Zhu and others at China's Northwestern Polytechnical University claimed to measure thrust from such a thruster, but called it a \"microwave thruster without propellant\" working on quantum principles. In 2011 it was mentioned as something to be studied by Harold G. White and his team at NASA's Eagleworks Laboratories, who were working with a prototype of such a thruster. Other physicists, such as Sean M. Carroll and John Baez, dismiss it because the quantum vacuum as currently understood is not a plasma and does not possess plasma-like characteristics.\n\nA vacuum can be viewed not as empty space but as the combination of all zero-point fields. According to quantum field theory the universe is made up of matter fields whose quanta are fermions (e.g. electrons and quarks) and force fields, whose quanta are bosons (i.e. photons and gluons). All these fields have some intrinsic zero-point energy. Describing the quantum vacuum, a \"Physics Today\" article cited by the NASA team describes this ensemble of fields as \"a turbulent sea, roiling with waves associated with a panoply of force-mediating fields such as the photon and Higgs fields\". Given the equivalence of mass and energy expressed by Einstein's E = mc, any point in space that contains energy can be thought of as having mass to create particles. Virtual particles spontaneously flash into existence and annihilate each other at every point in space due to the energy of quantum fluctuations. Many real physical effects attributed to these vacuum fluctuations have been experimentally verified, such as spontaneous emission, Casimir force, Lamb shift, magnetic moment of the electron and Delbrück scattering; these effects are usually called \"radiative corrections\".\nThe Casimir effect is a weak force between two uncharged conductive plates caused by the zero-point energy of the vacuum. It was first observed experimentally by Lamoreaux (1997) and results showing the force have been repeatedly replicated. Several scientists including White have highlighted that a net thrust can indeed be induced on a spacecraft via the related \"dynamical Casimir effect\". The dynamic Casimir effect was observed experimentally for the first time in 2011 by Wilson et al. In the dynamical Casimir effect electromagnetic radiation is emitted when a mirror is accelerated through space at relativistic speeds. When the speed of the mirror begins to match the speed of the photons, some photons become separated from their virtual pair and so do not get annihilated. Virtual photons become real and the mirror begins to produce light. This is an example of Unruh radiation. A publication by Feigel (2004) raised the possibility of a Casimir-like effect that transfers momentum from zero-point quantum fluctuations to matter, controlled by applied electric and magnetic fields. These results were debated in a number of follow up papers in particular van Tiggelen et al. (2006) found no momentum transfer for homogeneous fields, but predict a very small transfer for a Casimir-like field geometry. This cumulated with Birkeland & Brevik (2007) who showed that electromagnetic vacuum fields can cause broken symmetries (anisotropy) in the transfer of momentum or, put another way, that the extraction of momentum from electromagnetic zero-point fluctuations is possible in an analogous way that the extraction of energy is possible from the Casimir effect. Birkeland & Brevik highlight that momentum asymmetries exist throughout nature and that the artificial stimulation of these by electric and magnetic fields have already been experimentally observed in complex liquids. This relates to the Abraham–Minkowski controversy, a long theoretical and experimental debate that continues to the current time. It is widely recognized that this controversy is an argument about definition of the interaction between matter and fields. \nIt has been argued that momentum transfer between matter and electromagnetic fields relating to the Abraham-Minikowski issue would allow for propellant-less drives.\n\nA QVT system seeks to make use of this predicted Casimir-like momentum transfer. It is argued that when the vacuum is exposed to crossed electric and magnetic fields (i.e. E and B-fields), it will induce a drift of the entire vacuum plasma which is orthogonal to that of the applied E x B fields. In a 2015 paper White highlighted that the presence of ordinary matter is predicted to cause an energy perturbation in the surrounding quantum vacuum such that the local vacuum state has a different energy density when compared with the \"empty\" cosmological vacuum energy state. This suggests the possibility of modelling the vacuum as a dynamic entity as opposed to it being an immutable and non-degradable state. White models of the perturbed quantum vacuum around a hydrogen atom as a Dirac vacuum consisting of virtual electron-positron pairs. Given the nontrivial variability in local energy densities resulting from virtual pair production, he suggests the tools of magnetohydrodynamics (MHD) can be used to model the quasiclassical behavior of the quantum vacuum as a plasma.\n\nWhite compares changes in vacuum energy density induced by matter to the hypothetical chameleon field or quintessence currently being discussed in the scientific literature. It is claimed the existence of a “chameleon” field whose mass is dependent on the local matter density may be an explanation for dark energy. A number of notable physicists, such as Sean Carroll, see the idea of a dynamical vacuum energy as the simplest and best explanation for dark energy. Evidence for quintessence would come from violations of Einstein's equivalence principle and variation of the fundamental constants ideas which are due to be tested by the Euclid telescope which is set to launch in 2020.\n\nSystems utilizing Casimir effects have thus far been shown to only create very small forces and are generally considered one-shot devices that would require a subsequent energy to recharge them (i.e. Forward's \"vacuum fluctuation battery\"). The ability of systems to use the zero-point field continuously as a source of energy or propellant is much more contentious (though peer-reviewed models have been proposed). There is debate over which formalisms of quantum mechanics apply to propulsion physics under such circumstances, the more refined Quantum Electrodynamics (QED), or the relatively undeveloped and controversial Stochastical Quantum Electrodynamics (SED). SED describes electromagnetic energy at absolute zero as a stochastic, fluctuating zero-point field. In SED the motion of a particle immersed in the stochastic zero-point radiation field generally results in highly nonlinear behaviour. Quantum effects emerge as a result of permanent matter-field interactions not possible to describe in QED The typical mathematical models used in classical electromagnetism, quantum electrodynamics (QED) and the standard model view electromagnetism as a U(1) gauge theory, which topologically restricts any complex nonlinear interaction. The electromagnetic vacuum in these theories is generally viewed as a linear system with no overall observable consequence. For many practical calculations zero-point energy is dismissed by fiat in the mathematical model as a constant that may be canceled or as a term that has no physical effect.\n\nThe 2016 NASA paper highlights that stochastic electrodynamics (SED) allows for a pilot-wave interpretation of quantum mechanics. Pilot-wave interpretations of quantum mechanics are a family of deterministic nonlocal theories distinct from other more mainstream interpretations such as the Copenhagen interpretation and Everett's many-worlds interpretation. Pioneering experiments by Couder and Fort beginning in 2006 have shown that macroscopic classical pilot-waves can exhibit characteristics previously thought to be restricted to the quantum realm. Hydrodynamic pilot-wave analogs have been able to duplicate the double slit experiment, tunneling, quantized orbits, and numerous other quantum phenomena and as such pilot-wave theories are experiencing a resurgence in interest. Coulder and Fort note in their 2006 paper that pilot-waves are nonlinear dissipative systems sustained by external forces. A dissipative system is characterized by the spontaneous appearance of symmetry breaking (anisotropy) and the formation of complex, sometimes chaotic or emergent, dynamics where interacting fields can exhibit long range correlations. In SED the zero point field (ZPF) plays the role of the pilot wave that guides real particles on their way. Modern approaches to SED consider wave and particle-like quantum effects as well-coordinated emergent systems that are the result of speculated sub-quantum interactions with the zero-point field\n\nSome notable physicists have found the Q-thruster concept to be implausible. For example, mathematical physicist John Baez has criticized the reference to \"quantum vacuum virtual plasma\" noting that: \"There's no such thing as 'virtual plasma' \". Noted Caltech theoretical physicist Sean M. Carroll has also affirmed this statement, writing \"[t]here is no such thing as a ‘quantum vacuum virtual plasma,’...\". In addition, Lafleur found that quantum field theory predicts no net force, implying that the measured thrusts are unlikely to be due to quantum effects. However, Lafleur noted that this conclusion was based on the assumption that the electric and magnetic fields were homogeneous, whereas certain theories posit a small net force in inhomogeneous vacuums.\n\nNotably, the violation of energy and momentum conservation laws have been heavily criticized. In a presentation at Nasa Ames Research Centre in November 2014, Harold White addressed the issue of conservation of momentum by stating that the Q-thruster conserves momentum by creating a wake or anisotropic state in the quantum vacuum. White indicated that once false positives were ruled out, Eagleworks would explore the momentum distribution and divergence angle of the quantum vacuum wake using a second Q-thruster to measure the quantum vacuum wake. In a paper published in January 2014, White proposed to address the conservation of momentum issue by stating that the Q-thruster pushes quantum particles (electrons/positrons) in one direction, whereas the Q-thruster recoils to conserve momentum in the other direction. White stated that this principle was similar to how a submarine uses its propeller to push water in one direction, while the submarine recoils to conserve momentum. Hence, the violations of fundamental laws of physics can be avoided.\n\nA number of physicists have suggested that a spacecraft or object may generate thrust through its interaction with the quantum vacuum. For example, Fabrizio Pinto in a 2006 paper published in the \"Journal of the British Interplanetary Society\" noted it may be possible to bring a cluster of polarisable vacuum particles to a hover in the laboratory and then to transfer thrust to a macroscopic accelerating vehicle. Similarly, Jordan Maclay in a 2004 paper titled \"A Gedanken Spacecraft that Operates Using the Quantum Vacuum (Dynamic Casimir Effect)\" published in the scientific journal \"Foundations of Physics\" noted that it is possible to accelerate a spacecraft based on the dynamic Casimir effect, in which electromagnetic radiation is emitted when an uncharged mirror is properly accelerated in vacuum. Similarly, Puthoff noted in a 2010 paper titled \"Engineering the Zero-Point Field and Polarizable Vacuum For Interstellar Flight\" published in the \"Journal of the British Interplanetary Society\" noted that it may be possible that the quantum vacuum might be manipulated so as to provide energy/thrust for future space vehicles. Likewise, researcher Yoshinari Minami in a 2008 paper titled \"Preliminary Theoretical Considerations for Getting Thrust via Squeezed Vacuum\" published in the \"Journal of the British Interplanetary Society\" noted the theoretical possibility of extracting thrust from the excited vacuum induced by controlling squeezed light. In addition, Alexander Feigel in a 2009 paper noted that propulsion in quantum vacuum may be achieved by rotating or aggregating magneto-electric nano-particles in strong perpendicular electrical and magnetic fields.\n\nHowever, according to Puthoff, although this method can produce angular momentum causing a static disk (known as a Feynman disk) to begin to rotate, it cannot induce linear momentum due to a phenomenon known as \"hidden momentum\" that cancels the ability of the proposed E×B propulsion method to generate linear momentum. However, some recent experimental and theoretical work by van Tiggelen and colleagues suggests that linear momentum may be transferred from the quantum vacuum in the presence of an external magnetic field.\n\nIn 2013, the Eagleworks team tested a device called the Serrano Field Effect Thruster, built by Gravitec Inc. at the request of Boeing and DARPA. The Eagleworks team has theorized that this device is a Q-thruster. The thruster consists of a set of circular dielectrics sandwiched between electrodes; its inventor describes it device as producing thrust through a preselected shaping of an electric field. Gravitec Inc. alleges that in 2011 they tested the \"asymmetrical capacitor\" device in a high vacuum several times and have ruled out ion wind or electrostatic forces as an explanation for the thrust produced. In February through June 2013, the Eagleworks team evaluated the SFE test article in and out of a Faraday Shield and at various vacuum conditions. Thrust was observed in the ~1–20 N/kW range. The magnitude of the thrust scaled approximately with the cube of the input voltage (20–110 μN). As of 2015, the researchers have not published a peer-reviewed paper detailing the results of this experiment.\n\nUsing a torsion pendulum, White's team claimed to have measured 30–50 \"μ\"N of thrust from a microwave cavity resonator designed by Guido Fetta in an attempt at propellant-less propulsion. Using the same measurement equipment, a non-zero force was also measured on a \"null\" resonator that was not designed to experience any such force, which they suggest hints at \"interaction with the quantum vacuum virtual plasma\". All measurements were performed at atmospheric pressure, presumably in contact with air, and with no analysis of systematic errors, except for the use of an RF load without the resonant cavity interior as a control device. In early 2015, Paul March from that team made new results public, claiming positive experimental force measurements with a torsional pendulum in a hard vacuum: about 50 µN with 50 W of input power at 5.0×10 torr, and new null-thrust tests. The claims of the team have not yet been published in a peer-reviewed journal, only as a conference paper in 2013.\n\nYu Zhu previously claimed to have measured anomalous thrust arising from a similar device, using power levels roughly 100 times greater, and measuring thrust roughly 1000 times greater.\n\nAs of 2015, Eagleworks is attempting to gather performance data to support the development of a Q-thruster engineering prototype for reaction-control-system applications in the force range of 0.1–1 N with a corresponding input electrical power range of 0.3–3 kW. The group plans to begin by testing a refurbished test article to improve the historical performance of a 2006 experiment that attempted to demonstrate the Woodward effect. The photograph shows the test article and the plot diagram shows the thrust trace from a 500g load cell in experiments performed in 2006.\n\nThe group hopes that testing the device on a high-fidelity torsion pendulum (1–4 μN at 10–40 W) will unambiguously demonstrate the feasibility of this concept. The team is maintaining a dialogue with the ISS national labs office for an on-orbit detailed test objective (DTO) to test the Q-thrusters operation in the vacuum and weightlessness of outer space.\n\n\n"}
{"id": "47395801", "url": "https://en.wikipedia.org/wiki?curid=47395801", "title": "Readdle", "text": "Readdle\n\nReaddle is a Ukrainian mobile application development company. The company research and development is based in Odesa, Ukraine. The operation is mostly built around the App Store, cumulatively generating over 100 million downloads. The company's two main products are PDF Expert and Spark.\n"}
{"id": "23441092", "url": "https://en.wikipedia.org/wiki?curid=23441092", "title": "Rocket sled launch", "text": "Rocket sled launch\n\nA rocket sled launch, also known as \"ground based launch assist\", \"catapult launch assist\", and \"sky ramp launch\", is a proposed method for launching space vehicles. With this concept the launch vehicle is supported by an eastward pointing rail or maglev track that goes up the side of a mountain while an externally applied force is used to accelerate the launch vehicle to a given velocity. Using an externally applied force for the initial acceleration reduces the propellant the launch vehicle needs to carry to reach orbit. This allows the launch vehicle to carry a larger payload and reduces the cost of getting to orbit. When the amount of velocity added to the launch vehicle by the ground accelerator becomes great enough, single-stage-to-orbit flight with a reusable launch vehicle becomes possible.\n\nFor hypersonic research in general, tracks at Holloman Air Force Base have tested, as of 2011, small rocket sleds moving at up to (Mach 8.5).\n\nEffectively a 'sky ramp' would make the most expensive, first stage of a rocket fully reusable since the sled is returned to its starting position, to be refueled and may be reused in the order of hours after use. Present launch vehicles have performance-driven costs of thousands of dollars per kilogram of dry weight; sled launch would aim to reduce performance requirements and amortize hardware expenses over frequent, repeated launches. Designs for mountain based inclined rail 'rocket' sleds often use jet engines or rockets to accelerate the spacecraft mounted on it. Electromagnetic methods (such as Bantam, Maglifter, and StarTram) are another technique investigated to accelerate a rocket before launch, potentially scalable to greater rocket masses and velocities than air launch.\n\nNASA studies have shown that the Space Shuttle used more than a third of its fuel just to reach . If a rocket were already moving at launch, with corresponding reduced propellant needs, a greater fraction of liftoff mass could have been payload and hardware.\n\nDue to factors including the exponential nature of the rocket equation and higher propulsive efficiency than if a rocket takes off stationary, a NASA Maglifter study estimated that a launch of an ELV rocket from a 3000-meter altitude mountain peak could increase payload to LEO by 80% compared to the same rocket from a conventional launch pad. Mountains of such height are available within the mainland U.S. for the easiest logistics, or nearer to the Equator for a little more gain from Earth's rotation. Among other possibilities, a larger single-stage-to-orbit (SSTO) could be reduced in liftoff mass by 35% with such launch assist, dropping to 4 instead of 6 engines in one case considered.\n\nAt an anticipated efficiency close to 90%, electrical energy consumed per launch of a 500-ton rocket would be around 30 GJ, 8000 kilowatt hours (each kilowatt-hour costing a few cents at the current cost of electricity in the United States), aside from any additional losses in energy storage. It is a system with low marginal costs dominated by initial capital costs Although a fixed site, it was estimated to provide a substantial net payload increase for a high portion of the varying launch azimuths needed by different satellites, with rocket maneuvering during the early stage of post-launch ascent (an alternative to adding electric propulsion for later orbital inclination change). Maglev guideway costs were estimated as $10 – $20 million per mile in the 1994 study, which had anticipated annual maglev maintenance costs on the order of 1% of capital costs.\n\nRocket sled launch helps a vehicle gain altitude, and proposals commonly involve the track curving up a mountain. Advantages to any launch system that starts from high altitudes include reduce gravity drag (the cost of lifting fuel in a gravity well). The thinner air will reduce air resistance and allow more efficient engine geometries. Rocket nozzles have different shapes (expansion ratios) to maximize thrust at different air pressures. (Though NASA's aerospike engine for the Lockheed Martin X-33 was designed to change geometry to remain efficient at a variety of different pressures, the aerospike engine had added weight and complexity; X-33 funding was canceled in 2001; and other benefits from launch assist would remain even if aerospike engines reached flight testing).\n\nFor example, the air is 39% thinner at 2500 meters. The more efficient rocket plume geometry and the reduced air friction allows the engine to be 5% more efficient per amount of fuel burned.\n\nAnother advantage to high altitude launches is that it eliminates the need to throttle back the engine when the \"Max Q\" limit is attained. Rockets launched in thick atmosphere can go so fast that air resistance may cause structural damage. Engines are throttled back when Max Q is reached, until the rocket is high enough that they can resume full power. The Atlas V 551 gives an example of this. It reaches its Max Q at 30,000 feet. Its engine is throttled back to 60% thrust for 30 seconds. This reduced acceleration adds to the gravity drag the rocket must overcome. Additionally, space craft engines concerned with Max Q are more complex as they must be throttled during launch.\n\nA launch from high altitude need not throttle back at Max Q as it starts above the thickest portion of the Earth's atmosphere.\n\nDebora A. Grant and James L. Rand in: \"The Balloon Assisted Launch System - A Heavy Lift Balloon\" wrote: \"It was established some time ago that a ground launched rocket capable of reaching 20 km would be able to reach an altitude of almost 100km if it was launched from 20km.\" They suggest that small rockets are lifted above the majority of the atmosphere by balloon in order to avoid the problems discussed above.\n\nRocket sleds at China Lake testing ground have reached Mach 4 while carrying 60,000 kg masses. A sled track that gave a Mach 2 or greater launch assist could reduce the fuel to orbit by 40% or more, while helping counter the weight penalty when aiming to make a fully reusable launch vehicle. Angled at 55 degrees to vertical, a track on a tall mountain could allow a single stage to orbit reusable vehicle with no new technology.\n\n\n"}
{"id": "3618556", "url": "https://en.wikipedia.org/wiki?curid=3618556", "title": "Roy Rosenzweig Center for History and New Media", "text": "Roy Rosenzweig Center for History and New Media\n\nRoy Rosenzweig Center for History and New Media (RRCHNM), formerly the Center for History and New Media (CHNM) is a research institution in the George Mason University in Fairfax County, Virginia specializing in history and information technology. It was established by Roy Rosenzweig in 1994 to research and use digital media and information technology in historical research, education, digital tools and resources, digital preservation, and outreach.\n\nFollowing the September 11, 2001 attacks, the Center for History and New Media in partnership with the American Social History Project at the City University of New York organized the September 11 Digital Archive with funding from the Alfred P. Sloan Foundation. With the September 11 Digital Archive, CHNM and ASHP utilized electronic media to collect, preserve, and present the past, with a digital repository of material including more than 150,000 first-hand accounts, emails, images, and other digital materials. This project has inspired a new project, the Hurricane Digital Memory Bank, which is collecting the stories and digital artefacts related to the Hurricane Katrina, Rita, and Wilma. CHNM continues to explore methods, tools, and technologies for archiving and preserving information, data, and documents digitally.\n\nThe Center for History and New Media worked in partnership with the American Social History Project (ASHP) at the City University of New York, to develop an online resource directed at American History teachers, along with online resources about the French Revolution. More recent projects have focused on developing online educational resources about World History, and a project on historical thinking, in partnership with the Smithsonian Institution’s National Museum of American History. CHNM is also involved with educational outreach with teachers in Virginia school districts.\n\nCHNM has also developed a number of online databases and other resources for historians and history teachers, including a listing of 1,200 history departments worldwide, a practical guide to Digital History, a collection of essays on history and new media. Another online database is Making the History of 1989, which chronicles the fall of communism in Eastern Europe. Created in collaboration with the German Historical Institute and the National Endowment for the Humanities, the 1989 Project is powered by CHNM's Omeka software, and includes resources for teachers and students, ranging from lesson plans to an archive of primary source materials.\n\nCHNM collaborated with the Jewish Women's Archive on Katrina's Jewish Voices, a virtual archive of stories, images, and reflections about the New Orleans and Gulf Coast Jewish communities before and after Hurricane Katrina.\n\nCHNM is responsible for the development of two notable open source software projects: Zotero and Omeka. Zotero is reference management software which is used by academics to read and cite the academic literature. Omeka is a content management system that uses the Dublin Core metadata standard to build digital collections and publish digital exhibits. Both projects are free, and reflect CHNM's dedication to democratizing the practice of history.\n\nThe Center for History and New Media also distributes a set of free additional digital tools for historians and teachers, including Web Scrapbook, Survey Builder, Scribe (a note taking application designed with historians in mind), Poll Builder, H-Bot (an automated historical fact finder), and Syllabus Finder, which allows you to find and compare syllabi from thousands of universities and colleges on any topic, using the Google search engine.\n\nIn 2017, with financial support of the Andrew W. Mellon Foundation, CHNM released Tropy, a free and open-source desktop knowledge organization application to manage and describe photographs of research materials.\n\nProjects like Zotero provide tools for historians to research and analyze the past. But will digital media change the nature of scholarly argument, communication, and publication? In order to encourage experimentation in this arena, American Quarterly in collaboration with the American Studies Crossroads Project and CHNM organized an experiment in hypertext publishing. Four essays, covering such diverse topics as photos, as legal evidence, the Spanish–American War in film, early comic strips, and Arnold Schwarzenegger, offer contrasting approaches to using digital media for scholarly presentations.\n\nImaging the French Revolution is another experiment in digital scholarship. In a series of essays, seven scholars analyze forty-two images of crowds and crowd violence in the French Revolution. Offering the most relevant examples and comments from an on-line forum, those same scholars consider issues of interpretation, methodology, and the impact of digital media on scholarship.\n\nFinally, Interpreting the Declaration of Independence by Translation is a roundtable of historians brought together to discuss the translation and reception of the Declaration of Independence in Japan, Mexico, Russia, China, Poland, Italy, Germany, Spain, and Israel. In addition to these reflections, the site includes actual translations of the Declaration into several different languages and \"re-translations\" back into English to illustrate the effects of translation on how a key historical document has been understood.\n\nCHNM has also developed some projects with an explicit focus on broad, public audiences. Gulag: Many Days, Many Lives is a web-based exhibit funded by NEH and being developed in collaboration with the Gulag Museum in Perm, Russia, will provide a multifaceted consideration of the human struggle for survival in the Gulag, the brutal and often lethal Soviet system of forced labor concentration camps. History News Network features articles, placing current events in historical perspective, written by historians of all political persuasions.\n\nOn April 15, 2011, the Center for History and New Media became the Roy Rosenzweig Center for History and New Media, in memory of its founder.\n\n\n\n"}
{"id": "9881376", "url": "https://en.wikipedia.org/wiki?curid=9881376", "title": "Sleepers, Wake!", "text": "Sleepers, Wake!\n\nSleepers, Wake! Technology and the Future of Work is a book written by Barry Jones, originally published in 1982 and reprinted many times. A revised and updated edition was published in 1995.\n\nBased on the premise that technologically advanced nations are currently passing through a post-industrial or information revolution, Jones analyzes the unique threats and opportunities of the sudden rise in information to the field such as manufacturing, service employment, and basic income.\n\nJones argues that science and technology have changed the quality, length, and direction of life in the past century far more than politics, education, ideology, or religion. Therefore, inventors such as Thomas Edison and Henry Ford have shaped human experience more broadly and enduringly than Lenin and Hitler.\n\nSome of the book's key points, such as the claim that technological innovation is a major component of economic growth, are more widely accepted now than in 1982. But, to quote Barry Jones himself, \"The central thesis was that people were going to be living longer, far longer, \"but it was possible that they would be working a good deal less\".\" \n\nDue to the rising issues for the labour force, Jones proposed the need to assist workers in income support and choosing to stay or leave the workforce. However, Jones noted in the 1990 edition that the Labor government did not pursue the idea of basic income when it won office in 1983.\n\n\"Sleepers, Wake!\" analyzes the major changes in the workforce and presents the possible political programs to assist the society in profiting from the technological advancements.\n\nThe fourth edition uses 1991 Commonwealth census data as confirmation of his thesis about changes in the labour force.\n\nBarry Jones was Australia's Minister for Science in the Hawke government from 1983 to 1990.\n"}
{"id": "41773", "url": "https://en.wikipedia.org/wiki?curid=41773", "title": "Systems control", "text": "Systems control\n\nSystems control, in a communications system, is the control and implementation of a set of functions that:\n\n"}
{"id": "48678928", "url": "https://en.wikipedia.org/wiki?curid=48678928", "title": "Technology readiness", "text": "Technology readiness\n\nTechnology readiness refers to people’s propensity to embrace and use new technologies for accomplishing goals in home life and at work. The construct can be viewed as an overall state of mind resulting from a gestalt of mental enablers and inhibitors that collectively determine a person’s predisposition to use new technologies.\n\nThe Technology Readiness Index can be viewed as a variation on the Technology acceptance model (TAM). It is a multidimensional psychographic construct, offering a way to segment online customers based upon their underlying positive and negative technology beliefs.\n\nTechnology readiness has four underlying dimensions:\n\nWhile optimism and innovativeness are contributors to technology readiness, discomfort and insecurity are inhibitors.\n"}
{"id": "5910689", "url": "https://en.wikipedia.org/wiki?curid=5910689", "title": "Timeline of United States inventions (after 1991)", "text": "Timeline of United States inventions (after 1991)\n\nA timeline of United States inventions (after 1991) encompasses the ingenuity and innovative advancements of the United States within a historical context, dating from the Contemporary era to the present day, which have been achieved by inventors who are either native-born or naturalized citizens of the United States. Patent protection secures a person's right to his or her first-to-invent claim of the \"original\" invention in question, highlighted in Article I, Section 8, Clause 8 of the United States Constitution which gives the following enumerated power to the United States Congress:\nIn 1641, the first patent in North America was issued to Samuel Winslow by the General Court of Massachusetts for a new method of making salt. On April 10, 1790, President George Washington signed the Patent Act of 1790 (1 Stat. 109) into law which proclaimed that patents were to be authorized for \"any useful art, manufacture, engine, machine, or device, or any improvement therein not before known or used.\" On July 31, 1790, Samuel Hopkins of Pittsford, Vermont became the first person in the United States to file and to be granted a patent for an improved method of \"Making Pot and Pearl Ashes.\" The Patent Act of 1836 (Ch. 357, 5 Stat. 117) further clarified United States patent law to the extent of establishing a patent office where patent applications are filed, processed, and granted, contingent upon the language and scope of the claimant's invention, for a patent term of 14 years with an extension of up to an additional 7 years. However, the Uruguay Round Agreements Act of 1994 (URAA) changed the patent term in the United States to a total of 20 years, effective for patent applications filed on or after June 8, 1995, thus bringing United States patent law further into conformity with international patent law. The modern-day provisions of the law applied to inventions are laid out in Title 35 of the United States Code (Ch. 950, sec. 1, 66 Stat. 792).\n\nFrom 1836 to 2011, the United States Patent and Trademark Office (USPTO) has granted a total of 7,861,317 patents relating to several well-known inventions appearing throughout the timeline below.\n\n1992 Spinner (wheel)\n\n1994 CMOS image sensor\n\nA CMOS image sensor (complementary metal-oxide semiconductor) is an image sensor consisting of an integrated circuit containing an array of pixel sensors, each pixel containing a photodetector and an active amplifier. Starting at the same point, they have to convert light into electrons by using the CMOS process. CMOS image sensors can be found in digital SLR cameras, embedded web-cams, video cameras, automotive safety systems, swallowable-pill cameras, toys and video games, and wireless video-security networks. The renowned American physicist and engineer Eric Fossum invented the CMOS image sensor while working at NASA's Jet Propulsion Laboratory in Pasadena, California. On January 28, 1994, Fossum filed U.S. patent #5,471,515, which was issued to him on November 28, 1995.\n\n1994 DNA computing\n\nDNA computing uses DNA, biochemistry and molecular biology, instead of the traditional silicon-based computer technologies. DNA computing, or, more generally, molecular computing, is a fast-developing interdisciplinary area. Research and development in this area concerns theory, experiments and applications of DNA computing. DNA computing is fundamentally similar to parallel computing in that it takes advantage of the many different molecules of DNA to try many different possibilities at once. Leonard Adleman of the University of Southern California initially pioneered this field in 1994. Adleman demonstrated a proof-of-concept use of DNA as a form of computation which solved the seven-point Hamiltonian path problem.\n\n1994 Segway PT\n\nThe Segway PT is a two-wheeled, self-balancing, zero-emission, electric vehicle used for \"personal transport\". Segways have had success in niche markets such as transportation for police departments, military bases, warehouses, corporate campuses or industrial sites, as well as in tourism. The earliest patent resembling the modern Segway PT, U.S. patent #6,357,544, was filed on May 27, 1994 and issued to Dean Kamen on December 30, 1997. Kamen introduced his invention to the public in 2001.\n\n1994 Quantum cascade laser\n\nA quantum cascade laser is a sliver of semiconductor material about the size of a tick. Inside, electrons are constrained within layers of gallium and aluminum compounds, called quantum wells are nanometers thick, much smaller than the thickness of a hair. Electrons jump from one energy level to another, rather than moving smoothly between levels and tunnel from one layer to the next going \"through\" rather than \"over\" energy barriers separating the wells. When the electrons jump, they emit photons of light. The quantum cascade laser was co-invented by Alfred Y. Cho, Claire F. Gmachl, Federico Capasso, Deborah Sivco, Albert Hutchinson, and Alessandro Tredicucci at Bell Laboratories in 1994. On April 4, 1994, the Bell Labs team filed U.S. patent #5,457,709 that was issued on October 10, 1995.\n\n1995 Bose–Einstein condensate\n\n1995 Screenless hammer mill\n\n1995 Scroll wheel\n\n1995 JavaScript\n\n1996 Adobe Flash\n\n1996 Bait car\n\n1997 Virtual reality therapy\n\n1998 HVLS fan\n\n1999 Torino scale\n\n1999 Phase-change incubator\n\n1999 Bowtie cotter pin\n\n1999 iBOT\n\n2002 SERF\n\n2003 Fermionic condensate\n\nA fermionic condensate is a superfluid phase formed by fermionic particles at low temperatures. The first atomic fermionic condensate was invented by Deborah S. Jin in 2003.\n\n2003 Slingshot (water vapor distillation system)\n\nSlingshot is a portable water purification device powered by a stirling engine running on a combustible fuel source. The size of a dorm fridge, the Slingshot is claimed to be capable of turning any water source such as urine or saltwater into drinking water. Dean Kamen invented the Slingshot. Kamen filed U.S. patent # 7,340,879 on November 13, 2003 for the device which was issued on March 11, 2008.\n\n2007 Nanowire battery\n\nA nanowire battery is a lithium-ion battery consisting of a stainless steel anode covered in silicon nanowires. Silicon, which stores ten times more lithium than graphite, allows a far greater energy density on a steel anode, thus reducing the mass of the battery. The high surface area further allows for fast charging and discharging. The practicality of nanowire batteries is reasoned that a laptop computer that runs on a regular lithium-ion battery for two hours could potentially operate up to 20 hours using a nanowire battery without recharging, which would be a considerable advantage for many people resulting in energy conservation and cost savings. The nanowire battery was co-invented in 2007 by Chinese-American Dr. Yi Cui, an assistant professor of materials science and engineering along with his colleagues at Stanford University.\n\n2008 Bionic contact lens\n\nA bionic contact lens is a digital contact lens worn directly on the human eye which in the future, scientists believe could one day serve as a useful virtual platform for activities such as surfing the World Wide Web, superimposing images on real-world objects, playing video games for entertainment, and for monitoring patients' medical conditions. The bionic contact lens is a form of nanotechnology and microfabrication constructed of light emitting diodes, an antenna, and electronic circuit wiring. The bionic contact lens is the 2008 creation of Iranian-American Babak Parviz, an electrical engineer at the University of Washington (UW) in Seattle.\n\nTimelines of United States inventions\n\nRelated topics\n\n\n"}
{"id": "3154072", "url": "https://en.wikipedia.org/wiki?curid=3154072", "title": "Tracking system", "text": "Tracking system\n\nA tracking system is used for the observing of persons or objects on the move and supplying a timely ordered sequence of location data for further processing.\n\nIn virtual space technology, a tracking system is generally a system capable of rendering virtual space to a human observer while tracking the observer's coordinates. For instance, in dynamic virtual auditory space simulations, a real-time head tracker provides feedback to the central processor, allowing for selection of appropriate head-related transfer functions at the estimated current position of the observer relative to the environment.\n\nThere are myriads of tracking systems. Some are 'lag time' indicators, that is, the data is collected after an item has passed a point for example a bar code or choke point or gate. Others are 'real-time' or 'near real-time' like Global Positioning Systems (GPS) depending on how often the data is refreshed. There are bar-code systems which require a person to scan items and automatic identification (RFID auto-id). For the most part, the tracking worlds are composed of discrete hardware and software systems for different applications. That is, bar-code systems are separate from Electronic Product Code (EPC) systems, GPS systems are separate from active real time locating systems or RTLS for example, a passive RFID system would be used in a warehouse to scan the boxes as they are loaded on a truck - then the truck itself is tracked on a different system using GPS with its own features and software. The major technology “silos” in the supply chain are:\n\nIndoors assets are tracked repetitively reading e.g. a barcode, any passive and active RFID and feeding read data into Work in Progress models (WIP) or Warehouse Management Systems (WMS) or ERP software. The readers required per choke point are meshed auto-ID or hand-held ID applications.\n\nHowever tracking could also be capable of providing monitoring data without binding to a fixed location by using a cooperative tracking capability, e.g. an RTLS.\n\nOutdoors mobile assets of high value are tracked by choke point,\n802.11, Received Signal Strength Indication (RSSI), Time Delay on Arrival (TDOA), active RFID or GPS Yard Management; feeding into either third party yard management software from the provider or to an existing system. Yard Management Systems (YMS) couple location data collected by RFID and GPS systems to help supply chain managers to optimize utilization of yard assets such as trailers and dock doors. YMS systems can use either active or passive RFID tags.\n\nFleet management is applied as a tracking application using GPS and composing tracks from subsequent vehicle's positions. Each vehicle to be tracked is equipped with a GPS receiver and relays the obtained coordinates via cellular or satellite networks to a home station. Fleet management is required by:\n\nOne such use of the RFID technology is in tracking IDs of students. Using GPS IDs would resolve the decreasing attendance in schools by monitoring the whereabouts of students when they did not attend class (Jensen, 2008). It is also used to efficiently check attendance. Perks of this tracking system is allowing students to check out library books buy food in the cafeterias (Jensen, 2008). The GPS IDs also act as a security measure to monitor any unwanted visitors or an emergency locator if a student cannot be found (Jensen, 2008). In the Spring Independent School District, students have been using for many years in check that students are staying in school during the day. Since they have instigated the system, attendance has increased thus schooling funding has increased as well (Jensen, 2008).\n\nRecently, debates over the Fourth Amendment have come up. Conservative students wish to keep their privacy and forbid to wear tracking devices, especially hackers can break into these systems to find out students’ information. Since many schools, such as those in the Spring Independent School District, require students to wear the tracking IDs, students argue that it is an immediate violation of their privacy (Jensen, 2008). Yet, the Fourth Amendment is not violated in these cases since students are not tracked in their homes (Warner, 2007). Each school’s decision over GPS IDs varies as states develop laws against these IDs in schools and as students protest for their privacy rights.\n\nLocation-based services or LBS is a term that is derived from the telematics and telecom world. The combination of A-GPS, newer GPS and cellular locating technology is what has enabled the latest “LBS” for handsets and PDAs. Line of sight is not necessarily required for a location fix. This is a significant advantage in certain applications since a GPS signal can still be lost indoors. As such, A-GPS enabled cell phones and PDAs can be located indoors and the handset may be tracked more precisely. This enables non-vehicle centric applications and can bridge the indoor location gap, typically the domain of RFID and RTLS systems, with an off the shelf cellular device.\n\nCurrently, A-GPS enabled handsets are still highly dependent on the LBS carrier system, so handset device choice and application requirements are still not apparent. Enterprise system integrators need the skills and knowledge to correctly choose the pieces that will fit the application and geography.\n\nRegardless of the tracking technology, for the most part the end-users just want to locate themselves or wish to find points of interest. The reality is that there is no \"one size fits all\" solution with locating technology for all conditions and applications.\n\nApplication of tracking is a substantial basis for vehicle tracking in fleet management, asset management, individual navigation, social networking, or mobile resource management and more. Company, group or individual interests can benefit from more than one of the offered technologies depending on the context.\n\nGPS has global coverage but can be hindered by line-of-sight issues caused by buildings and urban canyons. RFID is excellent and reliable indoors or in situations where close proximity to tag readers is feasible, but has limited range and still requires costly readers. RFID stands for Radio Frequency Identification. This technology uses electromagnetic waves to receive the signal from the targeting object to then save the location on a reader that can be looked at through specialized software (Warner, 2007).\n\nRTLS are enabled by Wireless LAN systems (according to IEEE 802.11) or other wireless systems (according to IEEE 802.15) with multilateration. Such equipment is suitable for certain confined areas, such as campuses and office buildings. RTLS require system-level deployments and server functions to be effective.\n\n\n"}
