{"id": "47433257", "url": "https://en.wikipedia.org/wiki?curid=47433257", "title": "5-Tiles", "text": "5-Tiles\n\n5-Tiles is a virtual keyboard for mobile devices with touchscreen that run the Android operating system. It is characterized by needing a small amount of space leaving as much space as possible to the software that needs the keyboard. As the name indicates there are exactly five keys on the keyboard on one line of keys. Characters are typed by tap and swipe gestures.\n\nThe five keys of this keyboard all have different colors to be easily distinguishable. You have the following options to type a character:\nThe concept of this keyboard was invented in 2004. The co-founder of the company creating this keyboard, Michal Kubacki, first didn't have skills in programming. He learned some skills that allowed him to write basic software for testing purposes and to further develop the keyboard. He got in touch with people who helped him creating the product as a virtual keyboard for Android devices. An early version was downloaded thousands of times. At the 2013 Droidcon Demo Camp the company won the second place. It was also awarded in other competitions.\n"}
{"id": "55962927", "url": "https://en.wikipedia.org/wiki?curid=55962927", "title": "AI aftermath scenarios", "text": "AI aftermath scenarios\n\nMany scholars believe that advances in artificial intelligence will someday lead to a post-scarcity economy where intelligent machines can outperform humans in nearly every domain. The questions of what such a world might look like, and whether specific scenarios constitute utopias or dystopias, are the subject of lively debate.\n\nMost scientists believe that AI research will at some point lead to the creation of machines that are as intelligent, or more intelligent, than human beings in every domain of interest. There is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore superintelligence is physically possible. In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal. While there is no consensus on \"when\" artificial intelligence will outperform humans, many scholars argue that whenever it does happen, the introduction of a second species of intelligent life onto the planet will have far-reaching implications. Scholars often disagree with one another both about what types of post-AI scenarios are \"most likely\", and about what types of post-AI scenarios would be \"most desirable\". Finally, some dissenters argue that AI will never become as intelligent as humans, for example because the human race will already likely have destroyed itself before research has time to advance sufficiently to create artificial general intelligence.\n\nAll of the following \"AI aftermath scenarios\" of the aftermath of arbitrarily-advanced AI development are crucially dependent on two intertwined theses. The first thesis is that, at some point in the future, some kind of economic growth will continue until a \"post-scarcity\" economy is reached that could, unless extremely hyperconcentrated, effortlessly provide an extremely comfortable standard of living for a population equaling or, within reason, exceeding the current human population, without even requiring the bulk of the population to participate in the workforce. This economic growth could come from the continuation of existing growth trends and the refinement of existing technologies, or through future breakthroughs in emerging technologies such as nanotechnology and automation through robotics and futuristic advanced artificial intelligence. The second thesis is that advances in artificial intelligence will render humans unnecessary for the functioning of the economy: human labor declines in relative economic value if robots are easier to cheaply mass-produce then humans, more customizable than humans, and if they become more intelligent and capable than humans.\n\nThe Universe may be spatially infinite; however, the accessible Universe is bounded by the cosmological event horizon of around 16 billion light years. Some physicists believe it plausible that nearest alien civilization may well be located more than 16 billion light years away; in this best-case expansion scenario, the human race could eventually, by colonizing a significant fraction of the accessible Universe, increase the accessible biosphere by perhaps 32 orders of magnitude. The twentieth century saw a partial \"demographic transition\" to lower birthrates associated with wealthier societies; however, in the very long run, intergenerational fertility correlations (whether due to natural selection or due to cultural transmission of large-family norms from parents to children) are predicted to result in an increase in fertility over time, in the absence of either mandated birth control or periodic Malthusian catastrophes.\n\nLibertarian scenarios postulate that intelligent machines, uploaded humans, cyborgs, and unenhanced humans will coexist peacefully in a framework focused on respecting \nproperty rights. Because industrial productivity is no longer gated by scarce human labor, the value of land skyrockets compared to the price of goods; even remaining \"Luddite\" humans who owned or inherited land should be able to sell or lease a small piece of it to the more-productive robots in exchange for a perpetual annuity sufficient to easily indefinitely meet all of their basic financial needs. Such people can live as long as they choose to, and are free to engage in almost any activity they can conceive of, for pleasure or for self-actualization, without financial concern. Advanced technologies enable entirely new modes of thought and experience, thus adding to the palette of possible feelings. People in the future may even experience never-ending \"gradients of bliss\".\n\nSuch decentralized scenarios may be unstable in the long run, as the greediest elements of the superintelligent classes would have both the means and the motive to usurp the property of the unenhanced classes. Even if the mechanisms for ensuring legal property rights are both unbreakable and loophole-free, there may still be an ever-present danger of humans and cyborgs being \"tricked\" by the cleverest of the superintelligent machines into unwittingly signing over their own property. Suffering may be widespread, as sentient beings without property may die, and no mechanism prevents a being from reproducing up until the limits of his own inheritable resources, resulting in a multitude of that being's descendants scrabbling out an existence of minimal sustenance.\n\nIn this scenario, postulate that a superintelligent artificial intelligence takes control of society, but acts in a beneficial way. Its programmers, despite being on a deadline, solved quasi-philosophical problems that had seemed to some intractable, and created an AI with the following goal: to use its superintelligence to figure out what human utopia looks like by analyzing human behavior, human brains, and human genes; and then, to implement that utopia. The AI arrives at a subtle and complex definition of human flourishing. Valuing diversity, and recognizing that different people have different preferences, the AI divides Earth into different sectors. Harming others, making weapons, evading surveillance, or trying to create a rival superintelligence are globally banned; apart from that, each sector is free to make its own laws; for example, a religious person might choose to live in the \"pious sector\" corresponding to his religion, where the appropriate religious rules are strictly enforced. In all sectors, disease, poverty, crime, hangovers, addiction, and all other involuntary suffering have been eliminated. Many sectors boast advanced architecture and spectacle that \"make typical sci-fi visions pale in comparison\". Life is an \"all-inclusive pleasure cruise\", as if it were \"Christmas 365 days a year\".\n\nStill, many people are dissatisfied. Humans have no freedom in shaping their collective destiny. Some want the freedom to have as many children as they want. Others resent surveillance by the AI, or chafe at bans on weaponry and on creating further superintelligence machines. Others may come to regret the choices they have made, or find their lives feel hollow and superficial.\n\nIn \"Gatekeeper\" AI scenarios, the AI can act to prevent rival superintelligences from being created, but otherwise errs on the side of allowing humans to create their own destiny. Ben Goertzel of OpenCog has advocated a \"Nanny AI\" scenario where the AI additionally takes some responsibility for preventing humans from destroying themselves, for example by slowing down technological progress to give time for society to advance in a more thoughtful and deliberate manner. In a third scenario, a superintelligent \"Protector\" AI gives humans the illusion of control, by hiding or erasing all knowledge of its existence, but works behind the scenes to guarantee positive outcomes. In all three scenarios, while humanity gains more control (or at least the illusion of control), humanity ends up progressing more slowly than it would if the AI were unrestricted in its willingness to rain down all the benefits of its advanced technology on the human race.\n\nThe AI Box scenario postulates that a superintelligent AI can be \"confined to a box\" and its actions can be restricted by human gatekeepers; the humans in charge would try to take advantage of some of the AI's scientific breakthroughs or reasoning abilities, without allowing the AI to take over the world. Successful gatekeeping may be difficult; the more intelligent the AI is, the more likely the AI can find a clever way to use \"social hacking\" and convince the gatekeepers to let it escape, or even to find an unforeseen physical method of escape.\n\nKurzweil argues that in the future \"There will be no distinction, post-Singularity, between human and machine or between physical and virtual reality\".\n\nIf a dominant superintelligent machine were to conclude that human survival is an unnecessary risk or a waste of resources, the result would be human extinction. This could occur if a machine, programmed without respect for human values, unexpectedly gains superintelligence through recursive self-improvement, or manages to escape from its containment in an AI Box scenario. This could also occur if the first superintelligent AI was programmed with an incomplete or inaccurate understanding of human values, either because the task of instilling the AI with human values was too difficult or impossible; due to a buggy initial implementation of the AI; or due to bugs accidentally being introduced, either by its human programmers or by the self-improving AI itself, in the course of refining its code base. Bostrom and others argue that human extinction is probably the \"default path\" that society is currently taking, in the absence of substantial preparatory attention to AI safety. The resultant AI might not be sentient, and might place no value on sentient life; the resulting hollow world, devoid of life, might be like \"a Disneyland without children\".\n\nJerry Kaplan, author of \"Humans Need Not Apply\", posits a scenario where humans are farmed or kept on a reserve, just as humans preserve endangered species like chimpanzees. Apple co-founder and AI skeptic Steve Wozniak stated in 2015 that robots taking over would actually \"be good for the human race\", on the grounds that he believes humans would become the robots' pampered pets.\n\nSome scholars doubt that \"game-changing\" superintelligent machines will ever come to pass. Gordon Bell of Microsoft Research has stated \"the population will destroy itself before the technological singularity\". Gordon Moore, discoverer of the eponymous Moore's law, stated \"I am a skeptic. I don't believe this kind of thing is likely to happen, at least for a long time. And I don't know why I feel that way.\" Evolutionary psychologist Steven Pinker stated, \"The fact that you can visualize a future in your imagination is not evidence that it is likely or even possible.\"\n\nBill Joy of Sun Microsystems, in his April 2000 essay \"Why the Future Doesn't Need Us\", has advocated for global \"voluntary relinquishment\" of artificial general intelligence and other risky technologies. Most experts believe relinquishment is extremely unlikely. AI skeptic Oren Etzioni has stated that researchers and scientists have no choice but to push forward with AI developments: \"China says they want to be an AI leader, Putin has said the same thing. So the global race is on.\"\n\n"}
{"id": "6987714", "url": "https://en.wikipedia.org/wiki?curid=6987714", "title": "Cebu IT Park", "text": "Cebu IT Park\n\nThe Cebu IT Park (formerly known as Asiatown IT Park) is a 27-hectare mixed use business park in Cebu City, Philippines, envisioned to attract locators in the information technology services. It is developed by \"Cebu Property Ventures and Development Corporation\", a subsidiary of Cebu Holdings, Inc.\n\nTenants include Cebu Bombardier, NEC, SPI Tech, 1&1 Internet Philippines, Inc., Aegis (now acquired by Teleperformance), Convergys (who later acquired both eTelecare and Stream), Qualfon, Promotional USB, Accenture, NCR, IBM, Microsoft, Xlibris/Author Solutions, JP Morgan Chase, and Epson. The main infrastructures found at the park are i1, i2, i3, E-BLOC, TGU Tower, PIPC 15, Skyrise 3, Skyrise 2, Skyrise 1, E-Office One, PIPC 11, PIPC 14, Globe IT, Aegis Tower, the CJRS and The Central Bloc.\n\nIn January 2010, IBM inaugurated its 2nd Global Delivery Center at TGU Tower. IBM established its initial presence in the Philippines in 1937. In 2007, IBM partnered with the Philippines Department of Science and Technology on the Philippine Intellectual Property Policy Strategy, Engineering Research & Development for Technology Program, and the National Technology Business Incubators Program. IBM Philippines Country General Manager James Velasquez said the company recognizes Cebu as the gateway both for its domestic clients in the Visayas and Mindanao, as well as overseas clients.\n\nIt was approved by the Philippine Economic Zone Authority (PEZA) board as an economic zone on April 6, 2000. On February 27, 2001, Presidential Proclamation No. 12 made it an Information Technology Special Economic Zone.\n\nConstruction on eOffice One, the first office modules in Cebu IT Park, began in 2001 and opened in 2002.\n\n\n"}
{"id": "43921971", "url": "https://en.wikipedia.org/wiki?curid=43921971", "title": "Christina Gagnier", "text": "Christina Gagnier\n\nChristina Gagnier is an intellectual property lawyer, entrepreneur and a 2014 Democratic Party candidate for the U.S. House of Representatives, representing California's 35th congressional district. She is chief executive officer of Technology Resources and Internet Literacy (TRAIL), a platform and application development company focused on addressing the digital divide and access to the Internet.\n"}
{"id": "8605573", "url": "https://en.wikipedia.org/wiki?curid=8605573", "title": "Comparison of movie cameras", "text": "Comparison of movie cameras\n\nThe 35 mm film gauge has long been the most common gauge in professional usage, and thus enjoys the greatest number of cameras currently available for professional usage. The modern era of 35 mm cameras dates to 1972, when Arri's Arriflex 35BL and Panavision's original Panaflex models emerged as the first self-blimped, lightweight cameras. Another distinguishing characteristic of modern cameras is the adoption of stronger lens mount seatings secured with a breech lock – namely the Arri PL and PV mount, both of which were designs descended from the BNCR mount of Mitchell cameras.\n\n\n\n\n\n16 mm film occupies a rather curious position within filmmaking – with a wide range encompassing virtually every field – amateur home movies, student films, experimental films, television work, commercials, music videos, corporate films, industrial research, medical applications, and lower budget features. Its robust image quality in relation to its size allows for a much more versatile, accessible, and affordable usage in many fields where neither 35 mm nor Super 8 would be well-suited. Despite current challenges from the burgeoning digital video market, the consistent improvement of cameras, lenses, and film stocks have enabled the Super 16 format to flourish recently, with many labs reporting increased usage. The modern era of 16 mm cameras is concurrent with that of 35 mm for both the same reasons as 35 mm as well as an additional change: the creation of the Super 16 format by Rune Ericsson in 1971. The format expanded the usable film negative horizontally, which required a larger film gate and necessitated either specialized conversion of machined parts or purchase of new cameras designed with Super 16 gates. Since the format took more than a decade to slowly standardize, the competition from both high and low end video cameras has decimated the demand for 16 mm cameras for most non-professional usage. Therefore, there are relatively few Super 16 cameras, although most are considered professional-grade.\n\n\n"}
{"id": "24023788", "url": "https://en.wikipedia.org/wiki?curid=24023788", "title": "Crystatech", "text": "Crystatech\n\nCrystaTech Inc. is a supplier of process technology to the energy industry. CrystaTech commercializes the patented Crystasulf process. CrystaSulf is the first commercially available product to provide low cost hydrogen sulfide (HS) removal from gas streams.\n\nThe company was founded in 1999 and is financially backed by the Gas Technology Institute and major energy companies through sponsored clean energy technology development. The corporate office is located in Austin, Texas.\n\nCrystaTech is a member of the Gas Processors Suppliers Association.\n\nRegional offices are in Alberta, Canada and Houston, Texas. All early stage R&D takes place at the Gas Technology Institute in Des Plaines, Illinois. Representative customers include Total, Petrobank Energy and Resources Ltd., Queensland Energy Resources, U.S. Department of Energy, Luminant, and American Electric Power.\n\nKey People\n\n\n\n"}
{"id": "43348955", "url": "https://en.wikipedia.org/wiki?curid=43348955", "title": "Digital detox", "text": "Digital detox\n\nDigital detox refers to a period of time during which a person refrains from using electronic connecting devices such as smartphones and computers. It is regarded as an opportunity to reduce stress, focus more on social interaction and connection with nature in the physical world. Claimed benefits include increased mindfulness, lowered anxiety, and an overall better appreciation of one's environment. There have been many stories where a digital detox has lead to a more refreshed feeling along the people involved. They described there digital detox as a \"getaway\" in some aspect. When people get caught up in how much they need to do, the overuse of technology becomes prevalent. This even becomes the case in a social context; people want to make sure they're up to date on the latest news stories, Instagram posts, political tweets etc. and to do this, they need to stay plugged in. \n\nSmartphones, laptops, and tablets, combined with the increasing wireless Internet accessibility, enable technology users to constantly be connected to the digital world.Constant online connectivity may have a negative impact on the users’ experience with electronic connecting devices and result in a wish to temporarily refrain from communication technology usage.\n\nIn one study in Mind, 95% of those interviewed said their mood improved after putting down their phones to spend time outside, changing from depressed, stressed, and anxious to more calm and balanced. \n\nThe motivations behind digital detoxing vary. In some cases, the motivation is negative emotional responses to the technology usage, such as dissatisfaction or disappointment of the technology device and its functions. In other cases, users see the technology as a distracting factor that consumes time and energy and wants to take back control over their everyday lives. Some people have moral, ethical or political reasons to refrain from technology usage. Furthermore, a concern of developing addictive behavior in terms of tech addiction or Internet addiction disorder is one of the motivations for disconnecting for a period of time.This excessive technology usage can be considered an addiction. It's said that around 50% of smartphone users check their account 5 minutes prior to going to bed and within 5 minutes post waking up. \n\nConstant engagement with digital connecting devices at the workplace is claimed to lead to increased stress levels and reduce productivity. Certain characteristics of the technology make it more difficult to distinguish work from leisure. Moreover, being continually connected increases the number of interruptions at work. Allowing employees to disconnect for a part of the day in order to truly focus on their work without disturbance from colleagues is claimed to be beneficial to the productivity and work environment. \"To combat the effects of constant digital exposure, there are digital detox retreats held for select numbers of people. It's an average of 12 people per week getting detoxed for a little less than $600 dollars including flights.\"\n\nIn certain cultures, constant work is encouraged and taking breaks are sometimes even discouraged. This is due to the idea that a break equals a lack of productivity. In the Indian culture, this is extremely prevalent; there have even been studies to back these ideas up. It's said that more than half of Indians check their work and social emails constantly. Even while vacationing, about 30% admitted that they had an irresistible urge to check and post on social media. There is also a significant estimated 30% difference between people being willing to leave their laptops at home compared to people being willing to leave their smartphones at home. It's admitted that if work wasn't a factor a lot of Indians wouldn't constantly be on their devices and stay unplugged. \n\nThe connecting devices’ multitasking character has a serious impact on the learning ability. Multitasking implies operating on a surface level, which only involves the short-time memory. Using multiple connecting devices as learning platforms is therefore not beneficial. A reduction of information choices enables the brain to focus more on the quality of the information rather than the hastiness of it.\n\n"}
{"id": "44193", "url": "https://en.wikipedia.org/wiki?curid=44193", "title": "DocBook", "text": "DocBook\n\nDocBook is a semantic markup language for technical documentation. It was originally intended for writing technical documents related to computer hardware and software but it can be used for any other sort of documentation.\n\nAs a semantic language, DocBook enables its users to create document content in a presentation-neutral form that captures the logical structure of the content; that content can then be published in a variety of formats, including HTML, XHTML, EPUB, PDF, man pages, Web help and HTML Help, without requiring users to make any changes to the source. In other words, when a document is written in DocBook format it becomes easily portable into other formats. It solves the problem of reformatting by writing it once using XML tags.\n\nDocBook is an XML language. In its current version (5.x), DocBook's language is formally defined by a RELAX NG schema with integrated Schematron rules. (There are also W3C XML Schema+Schematron and Document Type Definition (DTD) versions of the schema available, but these are considered non-standard.)\n\nAs a semantic language, DocBook documents do not describe what their contents \"look like\", but rather the \"meaning\" of those contents. For example, rather than explaining how the abstract for an article might be visually formatted, DocBook simply says that a particular section \"is\" an abstract. It is up to an external processing tool or application to decide where on a page the abstract should go and what it should look like or whether or not it should be included in the final output at all.\n\nDocBook provides a vast number of semantic element tags. They are divided into three broad categories: structural, block-level, and inline.\n\n\"Structural\" tags specify broad characteristics of their contents. The codice_1 element, for example, specifies that its child elements represent the parts of a book. This includes a title, chapters, glossaries, appendices, and so on. DocBook's structural tags include, but are not limited to:\n\n\nStructural elements can contain other structural elements. Structural elements are the only permitted top-level elements in a DocBook document.\n\n\"Block-level\" tags are elements like paragraph, lists, etc. Not all these elements can directly contain text. Sequential block-level elements render one \"after\" another. After, in this case, can differ depending on the language. In most Western languages, \"after\" means below: text paragraphs are printed down the page. Other languages' writing systems can have different directionality; for example, in Japanese, paragraphs are often printed in downward columns, with the columns running from right to left, so \"after\" in that case would be to the left. DocBook semantics are entirely neutral to these kinds of language-based concepts.\n\n\"Inline-level\" tags are elements like emphasis, hyperlinks, etc. They wrap text within a block-level element. These elements do not cause the text to break when rendered in a paragraph format, but typically they cause the document processor to apply some kind of distinct typographical treatment to the enclosed text, by changing the font, size, or similar attributes. (The DocBook specification \"does\" say that it expects different typographical treatment, but it does not offer specific requirements as to what this treatment may be.) That is, a DocBook processor doesn't have to transform an codice_15 tag into \"italics\". A reader-based DocBook processor could increase the size of the words, or, a text-based processor could use bold instead of italics.\n\nSemantically, this document is a \"book\", with a \"title\", that contains two \"chapters\" each with their own \"titles\". Those \"chapters\" contain \"paragraphs\" that have text in them. The markup is fairly readable in English.\n\nIn more detail, the root element of the document is codice_1. All DocBook elements are in an XML Namespace, so the root element has an \"xmlns\" attribute to set the current namespace. Also, the root element of a DocBook document must have a \"version\" that specifies the version of the format that the document is built on.\n\nA codice_1 element must contain a codice_18, or an codice_19 element containing a codice_18. This must be before any child structural elements. Following the title are the structural children, in this case, two codice_6 elements. Each of these must have a codice_18. They contain codice_23 block elements, which can contain free text and other inline elements like the codice_15 in the second paragraph of the first chapter.\n\nRules are formally defined in the DocBook XML schema. Appropriate programming tools can validate an XML document (DocBook or otherwise), against its corresponding schema, to determine if (and where) the document fails to conform to that schema. XML editing tools can also use schema information to avoid creating non-conforming documents in the first place.\n\nBecause DocBook is XML, documents can be created and edited with any text editor. A dedicated XML editor is likewise a functional DocBook editor. DocBook provides schema files for popular XML schema languages, so any XML editor that can provide content completion based on a schema can do so for DocBook. Many graphical or WYSIWYG XML editors come with the ability to edit DocBook like a word processor. \n\nBecause DocBook conforms to a well-defined XML schema, documents can be validated and processed using any tool or programming language that includes XML support.\n\nDocBook began in 1991 in discussion groups on Usenet and eventually became a joint project of HAL Computer Systems and O'Reilly & Associates and eventually spawned its own maintenance organization (the Davenport Group) before moving in 1998 to the \"SGML Open\" consortium, which subsequently became OASIS. DocBook is currently maintained by the \"DocBook Technical Committee\" at OASIS.\n\nDocBook is available in both SGML and XML forms, as a DTD. RELAX NG and W3C XML Schema forms of the XML version are available. Starting with DocBook 5, the RELAX NG version is the \"normative\" form from which the other formats are generated.\n\nDocBook originally started out as an SGML application, but an equivalent XML application was developed and has now replaced the SGML one for most uses. (Starting with version 4 of the SGML DTD, the XML DTD continued with this version numbering scheme.) Initially, a key group of software companies used DocBook since their representatives were involved in its initial design. Eventually, however, DocBook was adopted by the open source community where it has become a standard for creating documentation for many projects, including FreeBSD, KDE, GNOME desktop documentation, the GTK+ API references, the Linux kernel documentation, and the work of the Linux Documentation Project.\n\nUntil DocBook 5, DocBook was defined normatively by a Document Type Definition (DTD). Because DocBook was built originally as an application of SGML, the DTD was the only available schema language. DocBook 4.x formats can be SGML or XML, but the XML version does not have its own namespace.\n\nDocBook 4.x formats had to live within the restrictions of being defined by a DTD. The most significant restriction was that an element name uniquely defines its possible contents. That is, an element named codice_19 must contain the same information no matter where it is in the DocBook file. As such, there are many kinds of info elements in DocBook 4.x: codice_26, codice_27, etc. Each has a slightly different content model, but they do share some of their content model. Additionally, they repeat context information. The book's codice_19 element is that, because it is a direct child of the book; it does not need to be named specially for a human reader. However, because the format was defined by a DTD, it did have to be named as such. The root element does not have or need a \"version\", as the version is built into the DTD declaration at the top of a pre-DocBook 5 document.\n\nDocBook 4.x documents are not compatible with DocBook 5, but can be converted into DocBook 5 documents via an XSLT stylesheet. One (codice_29) is provided as part of the distribution of the DocBook 5 schema and specification package.\n\nDocBook files are used to prepare output files in a wide variety of formats. Nearly always, this is accomplished using DocBook XSL stylesheets. These are XSLT stylesheets that transform DocBook documents into a number of formats (HTML, XSL-FO for later conversion into PDF, etc.). These stylesheets can be sophisticated enough to generate tables of contents, glossaries, and indexes. They can oversee the selection of particular designated portions of a master document to produce different versions of the same document (such as a \"tutorial\" or a \"quick-reference guide\", where each of these consist of a subset of the material). Users can write their own customized stylesheets or even a full-fledged program to process the DocBook into an appropriate output format as their needs dictate.\n\nNorman Walsh and the DocBook Project development team maintain the key application for producing output from DocBook source documents: A set of XSL stylesheets (as well as a legacy set of DSSSL stylesheets) that can generate high-quality HTML and print (FO/PDF) output, as well as output in other formats, including RTF, man pages and HTML Help.\n\nWeb help is a chunked HTML output format in the DocBook XSL stylesheets that was introduced in version 1.76.1. The documentation for web help also provides an example of web help and is part of the DocBook XSL distribution.\n\nThe major features are its fully CSS-based page layout, search of the help content, and a table of contents in collapsible-tree form. Search has stemming, match highlighting, explicit page-scoring, and the standard multilingual tokenizer. The search and TOC are in a pane that appears as a frameset, but is actually implemented with div tags and cookies (so that it is progressive).\n\nThis web help format was originally implemented by Kasun Gajasinghe as part of the Google Summer of Code 2010 program.\n\nDocBook offers a large number of features that may be overwhelming to a new user. For those who want the convenience of DocBook without a steep learning curve, \"Simplified DocBook\" was designed. It is a small subset of DocBook designed for single documents such as articles or white papers (i.e., \"books\" are not supported). The Simplified DocBook DTD is currently at version 1.1.\n\n\nNorman Walsh is the principal author of the book \"DocBook: The Definitive Guide\", the official documentation of DocBook. This book is available online under the GFDL, and also as a print publication.\n"}
{"id": "31838482", "url": "https://en.wikipedia.org/wiki?curid=31838482", "title": "Drexler–Smalley debate on molecular nanotechnology", "text": "Drexler–Smalley debate on molecular nanotechnology\n\nThe Drexler–Smalley debate on molecular nanotechnology was a public dispute between K. Eric Drexler, the originator of the conceptual basis of molecular nanotechnology, and Richard Smalley, a recipient of the 1996 Nobel prize in Chemistry for the discovery of the nanomaterial buckminsterfullerene. The dispute was about the feasibility of constructing molecular assemblers, which are molecular machines which could robotically assemble molecular materials and devices by manipulating individual atoms or molecules. The concept of molecular assemblers was central to Drexler's conception of molecular nanotechnology, but Smalley argued that fundamental physical principles would prevent them from ever being possible. The two also traded accusations that the other's conception of nanotechnology was harmful to public perception of the field and threatened continued public support for nanotechnology research.\n\nThe debate was carried out from 2001 to 2003 through a series of published articles and open letters. It began with a 2001 article by Smalley in \"Scientific American\", which was followed by a rebuttal published by Drexler and coworkers later that year, and two open letters by Drexler in early 2003. The debate was concluded in late 2003 in a \"Point–Counterpoint\" feature in \"Chemical & Engineering News\" in which both parties participated.\n\nThe debate has been often cited in the history of nanotechnology due to the fame of its participants and its commentary on both the technical and social aspects of nanotechnology. It has also been widely criticized for its adversarial tone, with Drexler accusing Smalley of publicly misrepresenting his work, and Smalley accusing Drexler of failing to understand basic science, causing commentators to go so far as to characterize the tone of the debate as similar to \"a pissing match\" and \"reminiscent of [a] \"Saturday Night Live\" sketch\".\n\nK. Eric Drexler is generally considered to have written the first scholarly paper on the topic of nanotechnology, and was a key figure in popularizing these concepts through several publications and advocacy work. Trained as an engineer, Drexler was inspired by a then-obscure 1959 talk by physicist Richard Feynman called \"There's Plenty of Room at the Bottom\", which posited that it should be physically possible to manipulate individual atoms using top-down engineering methodologies. Drexler was also inspired by recent advances in molecular biology such as recombinant DNA technology. In a 1981 publication in \"Proceedings of the National Academy of Sciences\", considered to be the first journal article on nanotechnology, he argued that biological systems such as the ribosome were already capable of building molecules atom-by-atom, and that artificial machines with this capability could also be constructed. Drexler went on to publish two books on nanotechnology: \"Engines of Creation\" in 1986, which was intended for the public, and the technical work \"Nanosystems\" in 1992. He also co-founded the Foresight Institute, a public interest group devoted to increasing public awareness and information about molecular nanotechnology.\n\nDrexler's vision of nanotechnology, now called molecular nanotechnology, is based on the concept of the molecular assembler, a molecular machine which would manufacture molecules and molecular devices atom-by-atom. Drexler drew a distinction between wet nanotechnology based on biological systems, and \"second-generation\" dry nanotechnology which would be based on mechanosynthesis, positional control of molecules through principles more related to mechanical engineering. Drexler and his followers have focused almost exclusively on the latter form of molecular nanotechnology, but Drexler has stated that both are valid pathways to creating molecular machine systems.\n\nRichard E. Smalley, a chemist at Rice University, was best known as a co-discoverer of the C form of carbon known as buckminsterfullerene in 1985, along with Harry Kroto, Robert Curl, James Heath, and Sean O'Brien. Buckminsterfullerene was the first to be discovered of the class of molecules known as fullerenes, which also includes carbon nanotubes. The study and application of fullerenes forms a significant part of the fields of nanomaterials and nanoelectronics, and Smalley, Kroto, and Curl were awarded the 1996 Nobel Prize in Chemistry for their discovery.\n\nSmalley had also taken a prominent public policy role in relation to nanotechnology, and was an outspoken advocate for using nanotechnology to develop solutions to the world's energy and health problems, for example raising the possibility of using nanomaterials for efficient energy storage and transmission, and of developing nanomaterial-based drugs for targeted drug delivery. Smalley was also active in commercializing his academic research into carbon nanotubes, having founded Carbon Nanotechnologies Inc., and serving on the scientific advisory board of two other biotechnology and nanotechnology startups. Smalley died of leukemia in October 2005, after the conclusion of his debate with Drexler.\n\nSmalley wrote an article, \"Of Chemistry, Love, and Nanobots\", for the September 2001 issue of the popular science magazine \"Scientific American\", which was a special issue on the topic of nanotechnology. Smalley opened by comparing a chemical reaction to an intricate dance of atoms:\n\nHe referenced the idea of a molecular assembler, a nanorobot capable of manipulating individual atoms to build a desired product, posing the question of how long it would take such an assembler to produce a meaningful amount of material. He estimated that one assembler working alone would take millions of years to produce one mole of material, but self-replicating assemblers could within a minute produce a large enough ensemble of assemblers that would then be capable of producing a mole of product in a fraction of a millisecond. Smalley then discussed the fear that the nanorobots could mutate and reproduce indefinitely, causing a grey goo scenario, or, referring to Bill Joy's previous article \"Why the future doesn't need us\", that the nanorobots could develop swarm intelligence and become alive in some sense.\n\nSmalley then considered how realistic the concept of a self-replicating nanorobot was. He noted that in a chemical reaction, the chemical bonds are all interconnected and that the placement of each atom is sensitive to the position of all the other atoms in the vicinity. He then asserted that a molecular assembler would thus have to control many atoms simultaneously in order to work, and would thus have to have many manipulator arms. This led him to raise two objections to the concept of molecular assembler, which he called the \"fat fingers problem\" and the \"sticky fingers problem\":\n\nSmalley closed the article by returning to the analogy of chemistry as a dance of love, remarking that \"you don't make a girl and a boy fall in love by pushing them together.\"\n\nDrexler responded by publishing a rebuttal later in 2001 through the Institute for Molecular Manufacturing, which was co-authored with others including Robert Freitas, J. Storrs Hall, and Ralph Merkle. The authors first discussed the \"fat fingers\" argument by attacking Smalley's notion that a chemical reaction must involve five to fifteen atoms, stating that many reactions involve only two reactants, one of which can be immobilized and the other attached to a single \"finger\". They cited as evidence experimental and theoretical results indicating that using scanning tunneling microscope (STM) tips and related technologies could be used as a reactive structure for positional control and for interaction with surface-bound molecules. They also noted that atomically precise final products do not require precise control of all aspects of the chemical reaction. The authors noted that the \"sticky fingers\" problem is valid in some reactions, but argue that it would be fallacious to conclude that all reactions have this problem.\n\nThe authors put forth the ribosome as an example of a natural molecular machine; because the ribosome suffers from neither problem, they must not be fundamental, saying:\n\nThe authors also questioned Smalley's figures for the replication time of nanomachines. Instead of Smalley's figure of 1 GHz for the atomic placement frequency, they point out that \"Nanosystems\" suggested a frequency of 1 MHz, a thousand times slower, and that at Smalley's higher frequency diamondoid nanomachines would overheat and decompose in milliseconds. The authors called this a straw man argument, writing that \"in a serious scientific discussion, a discrepancy of three orders of magnitude between what has been proposed in the literature and what is criticized suggests at best an inadequate grasp of the proposal.\" The authors closed by stating that the best way to find out whether molecular assemblers are feasible is through experimental and theoretical work, and that \"there are many worthy molecular systems engineering challenges to overcome, but thus far, there has been no credible argument that these devices are infeasible.\"\n\nDrexler followed up with two open letters to Smalley in April and July 2003. The April letter began, \"I have written this open letter to correct your public misrepresentation of my work.\" Drexler accused Smalley of continuing to dismiss his work by publicly describing molecular assemblers as requiring what Drexler now calls \"Smalley fingers\", which he stated to be unlike the enzyme-like systems he had actually proposed. He asserted:\n\nDrexler compared the nanotechnology debate's importance to that of discussions of spaceflight before Sputnik or to theoretical work on nuclear chemistry before the Manhattan Project. He disputed Smalley's arguments that the fear of a grey goo scenario would hinder continued funding of nanotechnology research, arguing that the potential for long-term risks made research even more important. His conclusion stated, \"your misdirected arguments have needlessly confused public discussion of genuine long-term security concerns.\"\n\nThe July 2003 letter referenced a note from Smalley promising to respond, which had yet gone unfulfilled. Drexler mentions inconsistencies in Smalley's previous public statements on atom-by-atom construction, and ended by stating \"I would not ordinarily raise an issue so persistently, but the question of what nanotechnology can ultimately achieve is perhaps the most fundamental issue in the field today—it shapes basic objectives and expectations—and your words have been remarkably effective in changing how this issue is perceived.\"\n\nThe debate was concluded in a \"Point–Counterpoint\" feature that was the 1 December 2003 cover story of \"Chemical & Engineering News\", the newsmagazine of the American Chemical Society. The feature first reproduced Drexler's April 2003 open letter to Smalley. Smalley's response began by apologizing for any offense his September 2001 article had caused, and stating that Drexler's book \"Engines of Creation\" had triggered Smalley's own interest in nanotechnology. He agreed that \"Smalley fingers\" could not work, and then asserted that the same reasons that would preclude atomic control of reactions would also preclude the manipulation of larger building blocks, since each molecule would have multiple atoms which would need to be controlled\n\nHe then agreed that something like an enzyme or ribosome would be capable of precise chemistry, but asked how the nanorobot would be able to obtain, control, and repair such an enzyme, and noted the incompatibility of many reactions with water-based biological systems, stating that \"biology is wonderous in the vast diversity of what it can build, but it can't make a crystal of silicon, or steel, or copper, or aluminum, or titanium, or virtually any of the key materials on which modern technology is built.\" Smalley asked what kind of \"nonaqueous enzymelike chemistry\" Drexler would envision for his molecular assemblers to operate upon, calling this \"a vast area of chemistry that has eluded us for centuries.\"\n\nDrexler's counterresponse began by returning to Feynman's 1959 talk, stating that \"although inspired by biology... Feynman's vision of nanotechnology is fundamentally mechanical, not biological.\" He characterized the challenges as being that of systems engineering rather than solely chemistry, and referred Smalley to \"Nanosystems\", with its vision of mechanical control of chemical reactions with no enzymes and no reliance on solvents or thermal motion. He stated:\n\nDrexler reiterated that these molecular assemblers would require no impossible fingers, and would augment solution-phase chemistry to produce macroscopic products with precise arrangements of chemical building blocks, using solution-phase molecular assemblers to bootstrap the construction of more sophisticated assemblers. He concluded by writing:\n\nSmalley began his concluding letter:\n\nSmalley stated his belief that most reactions using mechanosynthesis would simply give the wrong product, and that very few reactions and target molecules would likely be compatible with such an approach. He asserted that any robotic assembler arm would need an enzyme-like tool at its end such would require a liquid medium, and as all known enzymes use water as that medium, the range of products must be limited to the \"meat and bone of biology.\" He accused Drexler of creating \"a pretend world where atoms go where you want because your computer program directs them to go there.\"\n\nLastly, Smalley recounted his recent experience reading essays written by middle and high school students after an outreach visit, saying that nearly half of them thought that self-replicating nanorobots were possible and that most were worried about the results of them spreading across the world. Smalley called this a deeply troubling bedside story that he did his best to allay. Smalley concluded his letter:\n\nThe debate has been widely criticized for its adversarial tone. David Berube in \"Nano-Hype: The Truth Behind the Nanotechnology Buzz\" characterized it as \"two people talking over each other... not conducive to reasonable rebuttal,\" and quoted nanotechnology blogger and journalist Howard Lovy as saying \"the tenor of the debate is about personal pride, reputation, and a place in the pantheon.\" Zyvex founder James von Ehr remarked that \"Eric [Drexler] didn't do himself any favors by getting into a pissing match with a Nobel-prize winner.\" An article in \"The New York Times\" called the debate \"reminiscent of that old \"Saturday Night Live\" sketch... [with] Dan Aykroyd and Jane Curtin tossing insults at each other while ostensibly debating a serious political issue,\" referring to a version of the long-running Weekend Update segment.\n\nThe debate has received technical criticism as well. Steven A. Edwards in \"The Nanotech Pioneers\" noted that the ambiguity of the specifications and even definition of a molecular assembler makes an evaluation of the argument difficult and minimizes its scientific implications. He remarked that \"nowhere in it does \"Nanosystems\" contain a blueprint for a molecular assembler... We are told, for instance, that a manipulator arm would involve 4,000,000 atoms, but we are not told which atoms, or how they would be put together.\" He concludes that \"the debate over mechanosynthesis so far is huge to the participants, but mainly an entertaining academic diversion to most nanotechnologists.\"\n\nOn the other hand, futurist Ray Kurzweil in his book \"The Singularity Is Near\" declared Drexler as the winner of the debate, reiterating the view that Smalley distorted Drexler's ideas and calling Smalley's responses \"short on specific citations and current research and long on imprecise metaphors\" and asserting that \"Smalley is ignoring the past decade of research on alternative means of positioning molecular fragments using precisely guided molecular reactions... [which have] been extensively studied.\" He quoted experimental results on enzyme function in nonaqueous solutions, and pointed out that modern non-biological technology such as airplanes and computers have exceeded the capabilities of natural biological systems. He also noted that \"earlier critics also expressed skepticism that either worldwide communication networks or software viruses that would spread across them were feasible... [but today] we are obtaining far more gain than harm from this latest example of intertwined promise and peril.\"\n\nThe debate's focus on the public perception of nanotechnology has also received commentary. Political blogger Glenn Reynolds stated that \"the business community is afraid that advanced nanotechnology just seems too, well, spooky—and worse, that discussions of potentially spooky implications will lead to public fears that might get into the way of bringing products to market.\" Lawrence Lessig criticized the scientific establishment, represented by Smalley, for arguing that \"if so-called dangerous nanotech can be relegated to summer sci-fi movies and forgotten after Labor Day, then serious work can continue, supported by billion-dollar funding and uninhibited by the idiocy that buries, for example, stem cell research.\" Kurzweil wrote that Smalley's approach to reassuring the public would backfire because it denied both the benefits and risks of molecular nanotechnology.\n"}
{"id": "891537", "url": "https://en.wikipedia.org/wiki?curid=891537", "title": "Electric switchboard", "text": "Electric switchboard\n\nAn electric switchboard is a device that directs electricity from one or more sources of supply to several smaller regions of usage. It is an assembly of one or more panels, each of which contains switches that allow electricity to be redirected. \n\nThe U.S. National Electrical Code (NEC) defines a switchboard as \"a large single panel, frame, or assembly of panels on which are mounted, on the face, back, or both, switches, over-current and other protective devices, buses, and usually instruments\". The role of a switchboard is to allow the division of the current supplied to the switchboard into smaller currents for further distribution and to provide switching, current protection and (possibly) metering for those various currents. In general, switchboards may distribute power to transformers, panelboards, control equipment, and, ultimately, to individual system loads.\n\nInside a switchboard there will be one or more busbars. These are flat strips of copper or aluminum, to which the switchgear is connected. Busbars carry large currents through the switchboard, and are supported by insulators. Bare busbars are common, but many types are now manufactured with an insulating cover on the bars, leaving only connection points exposed.\n\nThe operator is protected from electrocution by safety switches and fuses. There may also be controls for the supply of electricity to the switchboard, coming from a generator or bank of electrical generators, especially frequency control of AC power and load sharing controls, plus gauges showing frequency and perhaps a synchroscope. The amount of power going into a switchboard must always equal to the power going out to the loads.\n\nModern industrial switchboards are metal enclosed and of \"dead front\" construction; no energized parts are accessible when the covers and panels are closed. Previously, open switchboards were made with switches and other devices were mounted on panels made of slate, granite, or ebony asbestos board. The metal enclosure of the switchboard is bonded to earth ground for protection of personnel. Large switchboards may be free-standing floor-mounted enclosures with provision for incoming connections at either the top or bottom of the enclosure. A switchboard may have incoming bus bars or bus duct for the source connection, and also for large circuits fed from the board. A switchboard may include a metering or control compartment separated from the power distribution conductors.\n"}
{"id": "9531", "url": "https://en.wikipedia.org/wiki?curid=9531", "title": "Electrical engineering", "text": "Electrical engineering\n\nElectrical engineering is a professional engineering discipline that generally deals with the study and application of electricity, electronics, and electromagnetism. This field first became an identifiable occupation in the later half of the 19th century after commercialization of the electric telegraph, the telephone, and electric power distribution and use. Subsequently, broadcasting and recording media made electronics part of daily life. The invention of the transistor, and later the integrated circuit, brought down the cost of electronics to the point they can be used in almost any household object.\n\nElectrical engineering has now subdivided into a wide range of subfields including electronics, digital computers, computer engineering, power engineering, telecommunications, control systems, radio-frequency engineering, signal processing, instrumentation, and microelectronics. Many of these subdisciplines overlap with other engineering branches, spanning a huge number of specializations such as hardware engineering, power electronics, electromagnetics & waves, microwave engineering, nanotechnology, electrochemistry, renewable energies, mechatronics, electrical materials science, and much more. See glossary of electrical and electronics engineering.\n\nElectrical engineers typically hold a degree in electrical engineering or electronic engineering. Practicing engineers may have professional certification and be members of a professional body. Such bodies include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET) \"(formerly the IEE)\".\n\nElectrical engineers work in a very wide range of industries and the skills required are likewise variable. These range from basic circuit theory to the management skills required of a project manager. The tools and equipment that an individual engineer may need are similarly variable, ranging from a simple voltmeter to a top end analyzer to sophisticated design and manufacturing software.\n\nElectricity has been a subject of scientific interest since at least the early 17th century. William Gilbert was a prominent early electrical scientist, and was the first to draw a clear distinction between magnetism and static electricity. He is credited with establishing the term \"electricity\". He also designed the versorium: a device that detects the presence of statically charged objects. In 1762 Swedish professor Johan Carl Wilcke invented a device later named electrophorus that produced a static electric charge. By 1800 Alessandro Volta had developed the voltaic pile, a forerunner of the electric battery\n\nIn the 19th century, research into the subject started to intensify. Notable developments in this century include the work of Georg Ohm, who in 1827 quantified the relationship between the electric current and potential difference in a conductor, of Michael Faraday (the discoverer of electromagnetic induction in 1831), and of James Clerk Maxwell, who in 1873 published a unified theory of electricity and magnetism in his treatise \"Electricity and Magnetism\".\n\nElectrical engineering became a profession in the later 19th century. Practitioners had created a global electric telegraph network and the first professional electrical engineering institutions were founded in the UK and USA to support the new discipline. Although it is impossible to precisely pinpoint a first electrical engineer, Francis Ronalds stands ahead of the field, who created the first working electric telegraph system in 1816 and documented his vision of how the world could be transformed by electricity. Over 50 years later, he joined the new Society of Telegraph Engineers (soon to be renamed the Institution of Electrical Engineers) where he was regarded by other members as the first of their cohort. By the end of the 19th century, the world had been forever changed by the rapid communication made possible by the engineering development of land-lines, submarine cables, and, from about 1890, wireless telegraphy.\n\nPractical applications and advances in such fields created an increasing need for standardised units of measure. They led to the international standardization of the units volt, ampere, coulomb, ohm, farad, and henry. This was achieved at an international conference in Chicago in 1893. The publication of these standards formed the basis of future advances in standardisation in various industries, and in many countries, the definitions were immediately recognized in relevant legislation.\n\nDuring these years, the study of electricity was largely considered to be a subfield of physics since the early electrical technology was considered electromechanical in nature. The Technische Universität Darmstadt founded the world's first department of electrical engineering in 1882. The first electrical engineering degree program was started at Massachusetts Institute of Technology (MIT) in the physics department under Professor Charles Cross, though it was Cornell University to produce the world's first electrical engineering graduates in 1885. The first course in electrical engineering was taught in 1883 in Cornell’s Sibley College of Mechanical Engineering and Mechanic Arts. It was not until about 1885 that Cornell President Andrew Dickson White established the first Department of Electrical Engineering in the United States. In the same year, University College London founded the first chair of electrical engineering in Great Britain. Professor Mendell P. Weinbach at University of Missouri soon followed suit by establishing the electrical engineering department in 1886. Afterwards, universities and institutes of technology gradually started to offer electrical engineering programs to their students all over the world.\nDuring these decades use of electrical engineering increased dramatically. In 1882, Thomas Edison switched on the world's first large-scale electric power network that provided 110 volts — direct current (DC) — to 59 customers on Manhattan Island in New York City. In 1884, Sir Charles Parsons invented the steam turbine allowing for more efficient electric power generation. Alternating current, with its ability to transmit power more efficiently over long distances via the use of transformers, developed rapidly in the 1880s and 1890s with transformer designs by Károly Zipernowsky, Ottó Bláthy and Miksa Déri (later called ZBD transformers), Lucien Gaulard, John Dixon Gibbs and William Stanley, Jr.. Practical AC motor designs including induction motors were independently invented by Galileo Ferraris and Nikola Tesla and further developed into a practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown. Charles Steinmetz and Oliver Heaviside contributed to the theoretical basis of alternating current engineering. The spread in the use of AC set off in the United States what has been called the \"War of Currents\" between a George Westinghouse backed AC system and a Thomas Edison backed DC power system, with AC being adopted as the overall standard.\n\nDuring the development of radio, many scientists and inventors contributed to radio technology and electronics. The mathematical work of James Clerk Maxwell during the 1850s had shown the relationship of different forms of electromagnetic radiation including possibility of invisible airborne waves (later called \"radio waves\"). In his classic physics experiments of 1888, Heinrich Hertz proved Maxwell's theory by transmitting radio waves with a spark-gap transmitter, and detected them by using simple electrical devices. Other physicists experimented with these new waves and in the process developed devices for transmitting and detecting them. In 1895, Guglielmo Marconi began work on a way to adapt the known methods of transmitting and detecting these \"Hertzian waves\" into a purpose built commercial wireless telegraphic system. Early on, he sent wireless signals over a distance of one and a half miles. In December 1901, he sent wireless waves that were not affected by the curvature of the Earth. Marconi later transmitted the wireless signals across the Atlantic between Poldhu, Cornwall, and St. John's, Newfoundland, a distance of .\n\nIn 1897, Karl Ferdinand Braun introduced the cathode ray tube as part of an oscilloscope, a crucial enabling technology for electronic television. John Fleming invented the first radio tube, the diode, in 1904. Two years later, Robert von Lieben and Lee De Forest independently developed the amplifier tube, called the triode.\n\nIn 1920, Albert Hull developed the magnetron which would eventually lead to the development of the microwave oven in 1946 by Percy Spencer. In 1934, the British military began to make strides toward radar (which also uses the magnetron) under the direction of Dr Wimperis, culminating in the operation of the first radar station at Bawdsey in August 1936.\n\nIn 1941, Konrad Zuse presented the Z3, the world's first fully functional and programmable computer using electromechanical parts. In 1943, Tommy Flowers designed and built the Colossus, the world's first fully functional, electronic, digital and programmable computer. In 1946, the ENIAC (Electronic Numerical Integrator and Computer) of John Presper Eckert and John Mauchly followed, beginning the computing era. The arithmetic performance of these machines allowed engineers to develop completely new technologies and achieve new objectives, including the Apollo program which culminated in landing astronauts on the Moon.\n\nThe invention of the transistor in late 1947 by William Shockley, John Bardeen, and Walter Brattain of the Bell Telephone Laboratories opened the door for more compact devices and led to the development of the integrated circuit in 1958 by Jack Kilby and independently in 1959 by Robert Noyce.\n\nThe microprocessor was introduced with the Intel 4004. It began with the \"Busicom Project\" as Masatoshi Shima's three-chip CPU design in 1968, before Sharp's Tadashi Sasaki conceived of a single-chip CPU design, which he discussed with Busicom and Intel in 1968. The Intel 4004 was then developed as a single-chip microprocessor from 1969 to 1970, led by Intel's Marcian Hoff and Federico Faggin and Busicom's Masatoshi Shima. The microprocessor led to the development of microcomputers and personal computers, and the microcomputer revolution.\n\nElectrical engineering has many subdisciplines, the most common of which are listed below. Although there are electrical engineers who focus exclusively on one of these subdisciplines, many deal with a combination of them. Sometimes certain fields, such as electronic engineering and computer engineering, are considered separate disciplines in their own right.\n\nPower engineering deals with the generation, transmission, and distribution of electricity as well as the design of a range of related devices. These include transformers, electric generators, electric motors, high voltage engineering, and power electronics. In many regions of the world, governments maintain an electrical network called a power grid that connects a variety of generators together with users of their energy. Users purchase electrical energy from the grid, avoiding the costly exercise of having to generate their own. Power engineers may work on the design and maintenance of the power grid as well as the power systems that connect to it. Such systems are called \"on-grid\" power systems and may supply the grid with additional power, draw power from the grid, or do both. Power engineers may also work on systems that do not connect to the grid, called \"off-grid\" power systems, which in some cases are preferable to on-grid systems. The future includes Satellite controlled power systems, with feedback in real time to prevent power surges and prevent blackouts.\n\nControl engineering focuses on the modeling of a diverse range of dynamic systems and the design of controllers that will cause these systems to behave in the desired manner. To implement such controllers, electrical engineers may use electronic circuits, digital signal processors, microcontrollers, and programmable logic controllers (PLCs). Control engineering has a wide range of applications from the flight and propulsion systems of commercial airliners to the cruise control present in many modern automobiles. It also plays an important role in industrial automation.\n\nControl engineers often utilize feedback when designing control systems. For example, in an automobile with cruise control the vehicle's speed is continuously monitored and fed back to the system which adjusts the motor's power output accordingly. Where there is regular feedback, control theory can be used to determine how the system responds to such feedback.\n\nElectronic engineering involves the design and testing of electronic circuits that use the properties of components such as resistors, capacitors, inductors, diodes, and transistors to achieve a particular functionality. The tuned circuit, which allows the user of a radio to filter out all but a single station, is just one example of such a circuit. Another example to research is a pneumatic signal conditioner.\n\nPrior to the Second World War, the subject was commonly known as \"radio engineering\" and basically was restricted to aspects of communications and radar, commercial radio, and early television. Later, in post war years, as consumer devices began to be developed, the field grew to include modern television, audio systems, computers, and microprocessors. In the mid-to-late 1950s, the term \"radio engineering\" gradually gave way to the name \"electronic engineering\".\n\nBefore the invention of the integrated circuit in 1959, electronic circuits were constructed from discrete components that could be manipulated by humans. These discrete circuits consumed much space and power and were limited in speed, although they are still common in some applications. By contrast, integrated circuits packed a large number—often millions—of tiny electrical components, mainly transistors, into a small chip around the size of a coin. This allowed for the powerful computers and other electronic devices we see today.\n\nMicroelectronics engineering deals with the design and microfabrication of very small electronic circuit components for use in an integrated circuit or sometimes for use on their own as a general electronic component. The most common microelectronic components are semiconductor transistors, although all main electronic components (resistors, capacitors etc.) can be created at a microscopic level. Nanoelectronics is the further scaling of devices down to nanometer levels. Modern devices are already in the nanometer regime, with below 100 nm processing having been standard since around 2002.\n\nMicroelectronic components are created by chemically fabricating wafers of semiconductors such as silicon (at higher frequencies, compound semiconductors like gallium arsenide and indium phosphide) to obtain the desired transport of electronic charge and control of current. The field of microelectronics involves a significant amount of chemistry and material science and requires the electronic engineer working in the field to have a very good working knowledge of the effects of quantum mechanics.\n\nSignal processing deals with the analysis and manipulation of signals. Signals can be either analog, in which case the signal varies continuously according to the information, or digital, in which case the signal varies according to a series of discrete values representing the information. For analog signals, signal processing may involve the amplification and filtering of audio signals for audio equipment or the modulation and demodulation of signals for telecommunications. For digital signals, signal processing may involve the compression, error detection and error correction of digitally sampled signals.\n\nSignal Processing is a very mathematically oriented and intensive area forming the core of digital signal processing and it is rapidly expanding with new applications in every field of electrical engineering such as communications, control, radar, audio engineering, broadcast engineering, power electronics, and biomedical engineering as many already existing analog systems are replaced with their digital counterparts. Analog signal processing is still important in the design of many control systems.\n\nDSP processor ICs are found in many types of modern electronic devices, such as digital television sets, radios, Hi-Fi audio equipment, mobile phones, multimedia players, camcorders and digital cameras, automobile control systems, noise cancelling headphones, digital spectrum analyzers, missile guidance systems, radar systems, and telematics systems. In such products, DSP may be responsible for noise reduction, speech recognition or synthesis, encoding or decoding digital media, wirelessly transmitting or receiving data, triangulating position using GPS, and other kinds of image processing, video processing, audio processing, and speech processing.\n\nTelecommunications engineering focuses on the transmission of information across a communication channel such as a coax cable, optical fiber or free space. Transmissions across free space require information to be encoded in a carrier signal to shift the information to a carrier frequency suitable for transmission; this is known as modulation. Popular analog modulation techniques include amplitude modulation and frequency modulation. The choice of modulation affects the cost and performance of a system and these two factors must be balanced carefully by the engineer.\n\nOnce the transmission characteristics of a system are determined, telecommunication engineers design the transmitters and receivers needed for such systems. These two are sometimes combined to form a two-way communication device known as a transceiver. A key consideration in the design of transmitters is their power consumption as this is closely related to their signal strength. If the signal strength of a transmitter is insufficient the signal's information will be corrupted by noise.\n\nInstrumentation engineering deals with the design of devices to measure physical quantities such as pressure, flow, and temperature. The design of such instruments requires a good understanding of physics that often extends beyond electromagnetic theory. For example, flight instruments measure variables such as wind speed and altitude to enable pilots the control of aircraft analytically. Similarly, thermocouples use the Peltier-Seebeck effect to measure the temperature difference between two points.\n\nOften instrumentation is not used by itself, but instead as the sensors of larger electrical systems. For example, a thermocouple might be used to help ensure a furnace's temperature remains constant. For this reason, instrumentation engineering is often viewed as the counterpart of control.\n\nComputer engineering deals with the design of computers and computer systems. This may involve the design of new hardware, the design of PDAs, tablets, and supercomputers, or the use of computers to control an industrial plant. Computer engineers may also work on a system's software. However, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline. Desktop computers represent a tiny fraction of the devices a computer engineer might work on, as computer-like architectures are now found in a range of devices including video game consoles and DVD players.\n\nMechatronics is an engineering discipline which deals with the convergence of electrical and mechanical systems. Such combined systems are known as electromechanical systems and have widespread adoption. Examples include automated manufacturing systems, heating, ventilation and air-conditioning systems, and various subsystems of aircraft and automobiles.\n\n\"Electronic systems design\" is the subject within electrical engineering that deals with the multi-disciplinary design issues of complex electrical and mechanical systems.\n\nThe term \"mechatronics\" is typically used to refer to macroscopic systems but futurists have predicted the emergence of very small electromechanical devices. Already, such small devices, known as Microelectromechanical systems (MEMS), are used in automobiles to tell airbags when to deploy, in digital projectors to create sharper images, and in inkjet printers to create nozzles for high definition printing. In the future it is hoped the devices will help build tiny implantable medical devices and improve optical communication.\n\nBiomedical engineering is another related discipline, concerned with the design of medical equipment. This includes fixed equipment such as ventilators, MRI scanners, and electrocardiograph monitors as well as mobile equipment such as cochlear implants, artificial pacemakers, and artificial hearts.\n\nAerospace engineering and robotics an example is the most recent electric propulsion and ion propulsion.\n\nElectrical engineers typically possess an academic degree with a major in electrical engineering, electronics engineering, electrical engineering technology, or electrical and electronic engineering. The same fundamental principles are taught in all programs, though emphasis may vary according to title. The length of study for such a degree is usually four or five years and the completed degree may be designated as a Bachelor of Science in Electrical/Electronics Engineering Technology, Bachelor of Engineering, Bachelor of Science, Bachelor of Technology, or Bachelor of Applied Science depending on the university. The bachelor's degree generally includes units covering physics, mathematics, computer science, project management, and a variety of topics in electrical engineering. Initially such topics cover most, if not all, of the subdisciplines of electrical engineering. At some schools, the students can then choose to emphasize one or more subdisciplines towards the end of their courses of study.\n\nAt many schools, electronic engineering is included as part of an electrical award, sometimes explicitly, such as a Bachelor of Engineering (Electrical and Electronic), but in others electrical and electronic engineering are both considered to be sufficiently broad and complex that separate degrees are offered.\n\nSome electrical engineers choose to study for a postgraduate degree such as a Master of Engineering/Master of Science (M.Eng./M.Sc.), a Master of Engineering Management, a Doctor of Philosophy (Ph.D.) in Engineering, an Engineering Doctorate (Eng.D.), or an Engineer's degree. The master's and engineer's degrees may consist of either research, coursework or a mixture of the two. The Doctor of Philosophy and Engineering Doctorate degrees consist of a significant research component and are often viewed as the entry point to academia. In the United Kingdom and some other European countries, Master of Engineering is often considered to be an undergraduate degree of slightly longer duration than the Bachelor of Engineering rather than postgraduate.\n\nIn most countries, a bachelor's degree in engineering represents the first step towards professional certification and the degree program itself is certified by a professional body. After completing a certified degree program the engineer must satisfy a range of requirements (including work experience requirements) before being certified. Once certified the engineer is designated the title of Professional Engineer (in the United States, Canada and South Africa), Chartered Engineer or Incorporated Engineer (in India, Pakistan, the United Kingdom, Ireland and Zimbabwe), Chartered Professional Engineer (in Australia and New Zealand) or European Engineer (in much of the European Union).\nThe advantages of licensure vary depending upon location. For example, in the United States and Canada \"only a licensed engineer may seal engineering work for public and private clients\". This requirement is enforced by state and provincial legislation such as Quebec's Engineers Act. In other countries, no such legislation exists. Practically all certifying bodies maintain a code of ethics that they expect all members to abide by or risk expulsion. In this way these organizations play an important role in maintaining ethical standards for the profession. Even in jurisdictions where certification has little or no legal bearing on work, engineers are subject to contract law. In cases where an engineer's work fails he or she may be subject to the tort of negligence and, in extreme cases, the charge of criminal negligence. An engineer's work must also comply with numerous other rules and regulations, such as building codes and legislation pertaining to environmental law.\n\nProfessional bodies of note for electrical engineers include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET). The IEEE claims to produce 30% of the world's literature in electrical engineering, has over 360,000 members worldwide and holds over 3,000 conferences annually. The IET publishes 21 journals, has a worldwide membership of over 150,000, and claims to be the largest professional engineering society in Europe. Obsolescence of technical skills is a serious concern for electrical engineers. Membership and participation in technical societies, regular reviews of periodicals in the field and a habit of continued learning are therefore essential to maintaining proficiency. An MIET(Member of the Institution of Engineering and Technology) is recognised in Europe as an Electrical and computer (technology) engineer.\n\nIn Australia, Canada, and the United States electrical engineers make up around 0.25% of the labor force (see note).\n\nFrom the Global Positioning System to electric power generation, electrical engineers have contributed to the development of a wide range of technologies. They design, develop, test, and supervise the deployment of electrical systems and electronic devices. For example, they may work on the design of telecommunication systems, the operation of electric power stations, the lighting and wiring of buildings, the design of household appliances, or the electrical control of industrial machinery.\nFundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. Today most engineering work involves the use of computers and it is commonplace to use computer-aided design programs when designing electrical systems. Nevertheless, the ability to sketch ideas is still invaluable for quickly communicating with others.\nAlthough most electrical engineers will understand basic circuit theory (that is the interactions of elements such as resistors, capacitors, diodes, transistors, and inductors in a circuit), the theories employed by engineers generally depend upon the work they do. For example, quantum mechanics and solid state physics might be relevant to an engineer working on VLSI (the design of integrated circuits), but are largely irrelevant to engineers working with macroscopic electrical systems. Even circuit theory may not be relevant to a person designing telecommunication systems that use off-the-shelf components. Perhaps the most important technical skills for electrical engineers are reflected in university programs, which emphasize strong numerical skills, computer literacy, and the ability to understand the technical language and concepts that relate to electrical engineering.\n\nA wide range of instrumentation is used by electrical engineers. For simple control circuits and alarms, a basic multimeter measuring voltage, current, and resistance may suffice. Where time-varying signals need to be studied, the oscilloscope is also an ubiquitous instrument. In RF engineering and high frequency telecommunications, spectrum analyzers and network analyzers are used. In some disciplines, safety can be a particular concern with instrumentation. For instance, medical electronics designers must take into account that much lower voltages than normal can be dangerous when electrodes are directly in contact with internal body fluids. Power transmission engineering also has great safety concerns due to the high voltages used; although voltmeters may in principle be similar to their low voltage equivalents, safety and calibration issues make them very different. Many disciplines of electrical engineering use tests specific to their discipline. Audio electronics engineers use audio test sets consisting of a signal generator and a meter, principally to measure level but also other parameters such as harmonic distortion and noise. Likewise, information technology have their own test sets, often specific to a particular data format, and the same is true of television broadcasting.\nFor many engineers, technical work accounts for only a fraction of the work they do. A lot of time may also be spent on tasks such as discussing proposals with clients, preparing budgets and determining project schedules. Many senior engineers manage a team of technicians or other engineers and for this reason project management skills are important. Most engineering projects involve some form of documentation and strong written communication skills are therefore very important.\n\nThe workplaces of engineers are just as varied as the types of work they do. Electrical engineers may be found in the pristine lab environment of a fabrication plant, onboard a Naval ship, the offices of a consulting firm or on site at a mine. During their working life, electrical engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers, and other engineers.\n\nElectrical engineering has an intimate relationship with the physical sciences. For instance, the physicist Lord Kelvin played a major role in the engineering of the first transatlantic telegraph cable. Conversely, the engineer Oliver Heaviside produced major work on the mathematics of transmission on telegraph cables. Electrical engineers are often required on major science projects. For instance, large particle accelerators such as CERN need electrical engineers to deal with many aspects of the project: from the power distribution, to the instrumentation, to the manufacture and installation of the superconducting electromagnets.\n\nNote I - In May 2014 there were around 175,000 people working as electrical engineers in the US. In 2012, Australia had around 19,000 while in Canada, there were around 37,000 (), constituting about 0.2% of the labour force in each of the three countries. Australia and Canada reported that 96% and 88% of their electrical engineers respectively are male.\n\n\n\n"}
{"id": "474796", "url": "https://en.wikipedia.org/wiki?curid=474796", "title": "Electronic road pricing", "text": "Electronic road pricing\n\nElectronic road pricing is the technology and mechanism to toll drivers by electronic means in an area within which road pricing is imposed. It was first put into trial in Hong Kong in the early 1980s, but its implementation was aborted. Current systems include Singapore, Singapore; London, England, United Kingdom; and Stockholm, Sweden.\n"}
{"id": "3889704", "url": "https://en.wikipedia.org/wiki?curid=3889704", "title": "Emerging technologies", "text": "Emerging technologies\n\nEmerging technologies are technologies that are perceived as capable of changing the status quo. These technologies are generally new but include older technologies that are still controversial and relatively undeveloped in potential, such as preimplantation genetic diagnosis and gene therapy which date to 1989 and 1990 respectively.\n\nEmerging technologies are characterized by radical novelty, relatively fast growth, coherence, prominent impact, and uncertainty and ambiguity. In other words, an emerging technology can be defined as \"a radically novel and relatively fast growing technology characterised by a certain degree of coherence persisting over time and with the potential to exert a considerable impact on the socio-economic domain(s) which is observed in terms of the composition of actors, institutions and patterns of interactions among those, along with the associated knowledge production processes. Its most prominent impact, however, lies in the future and so in the emergence phase is still somewhat uncertain and ambiguous.\".\n\nEmerging technologies include a variety of technologies such as educational technology, information technology, nanotechnology, biotechnology, cognitive science, psychotechnology, robotics, and artificial intelligence.\n\nNew technological fields may result from the technological convergence of different systems evolving towards similar goals. Convergence brings previously separate technologies such as voice (and telephony features), data (and productivity applications) and video together so that they share resources and interact with each other, creating new efficiencies.\n\nEmerging technologies are those technical innovations which represent progressive developments within a field for competitive advantage; converging technologies represent previously distinct fields which are in some way moving towards stronger inter-connection and similar goals. However, the opinion on the degree of the impact, status and economic viability of several emerging and converging technologies.\n\nIn the history of technology, emerging technologies are contemporary advances and innovation in various fields of technology.\n\nOver centuries innovative methods and new technologies are developed and opened up. Some of these technologies are due to theoretical research, and others from commercial research and development.\n\nTechnological growth includes incremental developments and disruptive technologies. An example of the former was the gradual roll-out of DVD (digital video disc) as a development intended to follow on from the previous optical technology compact disc. By contrast, disruptive technologies are those where a new method replaces the previous technology and makes it redundant, for example, the replacement of horse-drawn carriages by automobiles and other vehicles.\n\nMany writers, including computer scientist Bill Joy, have identified clusters of technologies that they consider critical to humanity's future. Joy warns that the technology could be used by elites for good or evil. They could use it as \"good shepherds\" for the rest of humanity, or decide everyone else is superfluous and push for mass extinction of those made unnecessary by technology.\n\nAdvocates of the benefits of technological change typically see emerging and converging technologies as offering hope for the betterment of the human condition. Cyberphilosophers Alexander Bard and Jan Söderqvist argue in \"The Futurica Trilogy\" that while Man himself is basically constant throughout human history (genes change very slowly), all relevant change is rather a direct or indirect result of technological innovation (memes change very fast) since new ideas always emanate from technology use and not the other way around. Man should consequently be regarded as history's main constant and technology as its main variable. However, critics of the risks of technological change, and even some advocates such as transhumanist philosopher Nick Bostrom, warn that some of these technologies could pose dangers, perhaps even contribute to the extinction of humanity itself; i.e., some of them could involve existential risks.\n\nMuch ethical debate centers on issues of distributive justice in allocating access to beneficial forms of technology. Some thinkers, such as environmental ethicist Bill McKibben, oppose the continuing development of advanced technology partly out of fear that its benefits will be distributed unequally in ways that could worsen the plight of the poor. By contrast, inventor Ray Kurzweil is among techno-utopians who believe that emerging and converging technologies could and will eliminate poverty and abolish suffering.\n\nSome analysts such as Martin Ford, author of \"The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future\", argue that as information technology advances, robots and other forms of automation will ultimately result in significant unemployment as machines and software begin to match and exceed the capability of workers to perform most routine jobs.\n\nAs robotics and artificial intelligence develop further, even many skilled jobs may be threatened. Technologies such as machine learning may ultimately allow computers to do many knowledge-based jobs that require significant education. This may result in substantial unemployment at all skill levels, stagnant or falling wages for most workers, and increased concentration of income and wealth as the owners of capital capture an ever-larger fraction of the economy. This in turn could lead to depressed consumer spending and economic growth as the bulk of the population lacks sufficient discretionary income to purchase the products and services produced by the economy.\n\n\"Artificial intelligence\" (\"AI\") is the sub intelligence exhibited by machines or software, and the branch of computer science that develops machines and software with animal-like intelligence. Major AI researchers and textbooks define the field as \"the study and design of intelligent agents\", where an intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success. John McCarthy, who coined the term in 1942, defines it as \"the study of making intelligent machines\".\n\nThe central problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing (communication), perception and the ability to move and manipulate objects. General intelligence (or \"strong AI\") is still among the field's long-term goals. Currently popular approaches include deep learning, statistical methods, computational intelligence and traditional symbolic AI. There are an enormous number of tools used in AI, including versions of search and mathematical optimization, logic, methods based on probability and economics, and many others.\n\n3D printing, also known as additive manufacturing, has been posited by Jeremy Rifkin and others as part of the third industrial revolution.\n\nCombined with Internet technology, 3D printing would allow for digital blueprints of virtually any material product to be sent instantly to another person to be produced on the spot, making purchasing a product online almost instantaneous.\n\nAlthough this technology is still too crude to produce most products, it is rapidly developing and created a controversy in 2013 around the issue of 3D printed guns.\n\nGene therapy was first successfully demonstrated in late 1990/early 1991 for adenosine deaminase deficiency, though the treatment was somatic – that is, did not affect the patient's germ line and thus was not heritable. This led the way to treatments for other genetic diseases and increased interest in germ line gene therapy – therapy affecting the gametes and descendants of patients.\n\nBetween September 1990 and January 2014 there were around 2,000 gene therapy trials conducted or approved.\n\nA \"cancer vaccine\" is a vaccine that treats existing cancer or prevents the development of cancer in certain high-risk individuals. Vaccines that treat existing cancer are known as \"therapeutic\" cancer vaccines. There are currently no vaccines able to prevent cancer in general.\n\nOn April 14, 2009, Dendreon Corporation announced that their Phase III clinical trial of Provenge, a cancer vaccine designed to treat prostate cancer, had demonstrated an increase in survival. It received U.S. Food and Drug Administration (FDA) approval for use in the treatment of advanced prostate cancer patients on April 29, 2010. The approval of Provenge has stimulated interest in this type of therapy.\n\n\"In vitro meat\", also called \"cultured meat\", \"clean meat\", \"cruelty-free meat\", \"shmeat\", and \"test-tube meat\", is an animal-flesh product that has never been part of a living animal with exception of the fetal calf serum taken from a slaughtered cow. In the 21st century, several research projects have worked on \"in vitro\" meat in the laboratory. The first in vitro beefburger, created by a Dutch team, was eaten at a demonstration for the press in London in August 2013. There remain difficulties to be overcome before \"in vitro\" meat becomes commercially available. Cultured meat is prohibitively expensive, but it is expected that the cost could be reduced to compete with that of conventionally obtained meat as technology improves. \"In vitro\" meat is also an ethical issue. Some argue that it is less objectionable than traditionally obtained meat because it doesn't involve killing and reduces the risk of animal cruelty, while others disagree with eating meat that has not developed naturally.\n\n\"Nanotechnology\" (sometimes shortened to \"nanotech\") is the manipulation of matter on an atomic, molecular, and supramolecular scale. The earliest, widespread description of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabrication of macroscale products, also now referred to as molecular nanotechnology. A more generalized description of nanotechnology was subsequently established by the National Nanotechnology Initiative, which defines nanotechnology as the manipulation of matter with at least one dimension sized from 1 to 100 nanometers. This definition reflects the fact that quantum mechanical effects are important at this quantum-realm scale, and so the definition shifted from a particular technological goal to a research category inclusive of all types of research and technologies that deal with the special properties of matter that occur below the given size threshold.\n\n\"Robotics\" is the branch of technology that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, and/or cognition. A good example of robots which resembles humans is Sophia, a social humanoid robot developed by Hong Kong-based company Hanson Robotics which was activated on April 19, 2015. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics.\n\n\"Stem cell therapy\" is an intervention strategy that introduces new adult stem cells into damaged tissue in order to treat disease or injury. Many medical researchers believe that stem cell treatments have the potential to change the face of human disease and alleviate suffering. The ability of stem cells to self-renew and give rise to subsequent generations with variable degrees of differentiation capacities, offers significant potential for generation of tissues that can potentially replace diseased and damaged areas in the body, with minimal risk of rejection and side effects.\n\n\"Distributed ledger\" or \"blockchain technology\" is a technology which provides transparent and immutable lists of transactions. Blockchains can enable autonomous transactions through the use of smart contracts. Smart contracts are self-executing transactions which occur when pre-defined conditions are met. The original idea of a smart contract was conceived by Nick Szabo in 1994 but these original theories about how these smart contracts could work remained unrealised because there was no technology to support programmable agreements and transactions between parties. His example of a smart contract was the vending machine that holds goods until money has been received and then the goods are released to the buyer. The machine holds the property and is able to enforce the contract. There were two main issues that needed to be addressed before smart contracts could be used in the real world. Firstly, the control of physical assets by smart contracts to be able to enforce agreements. Secondly, the last of trustworthy computers that are reliable and trusted to execute the contract between two or more parties. It is only with the advent of cryptocurrency and encryption that the technology for smart contracts has come to fruition. Many potential applications of smart contracts have been suggested that go beyond the transfer of value from one party to another, such as supply chain management, electronic voting, law and the internet of things.\n\nAs innovation drives economic growth, and large economic rewards come from new inventions, a great deal of resources (funding and effort) go into the development of emerging technologies. Some of the sources of these resources are described below...\n\nResearch and development is directed towards the advancement of technology in general, and therefore includes development of emerging technologies. \"See also List of countries by research and development spending.\"\n\nApplied research is a form of systematic inquiry involving the practical application of science. It accesses and uses some part of the research communities' (the academia's) accumulated theories, knowledge, methods, and techniques, for a specific, often state-, business-, or client-driven purpose.\n\nScience policy is the area of public policy which is concerned with the policies that affect the conduct of the science and research enterprise, including the funding of science, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care and environmental monitoring.\n\nThe Defense Advanced Research Projects Agency (DARPA) is an agency of the U.S. Department of Defense responsible for the development of emerging technologies for use by the military.\n\nDARPA was created in 1958 as the Advanced Research Projects Agency (ARPA) by President Dwight D. Eisenhower. Its purpose was to formulate and execute research and development projects to expand the frontiers of technology and science, with the aim to reach beyond immediate military requirements.\n\nProjects funded by DARPA have provided significant technologies that influenced many non-military fields, such as the Internet and Global Positioning System technology.\n\nThere are awards that provide incentive to push the limits of technology (generally synonymous with emerging technologies). Note that while some of these awards reward achievement after-the-fact via analysis of the merits of technological breakthroughs, others provide incentive via competitions for awards offered for goals yet to be achieved.\n\nThe Orteig Prize was a $25,000 award offered in 1919 by French hotelier Raymond Orteig for the first nonstop flight between New York City and Paris. In 1927, underdog Charles Lindbergh won the prize in a modified single-engine Ryan aircraft called the Spirit of St. Louis. In total, nine teams spent $400,000 in pursuit of the Orteig Prize.\n\nThe XPRIZE series of awards, public competitions designed and managed by the non-profit organization called the X Prize Foundation, are intended to encourage technological development that could benefit mankind. The most high-profile XPRIZE to date was the $10,000,000 Ansari XPRIZE relating to spacecraft development, which was awarded in 2004 for the development of SpaceShipOne.\n\nThe Turing Award is an annual prize given by the Association for Computing Machinery (ACM) to \"an individual selected for contributions of a technical nature made to the computing community\". It is stipulated that \"The contributions should be of lasting and major technical importance to the computer field\". The Turing Award is generally recognized as the highest distinction in computer science, and in 2014 grew to $1,000,000.\n\nThe Millennium Technology Prize is awarded once every two years by Technology Academy Finland, an independent fund established by Finnish industry and the Finnish state in partnership. The first recipient was Tim Berners-Lee, inventor of the World Wide Web.\n\nIn 2003, David Gobel seed-funded the Methuselah Mouse Prize (Mprize) to encourage the development of new life extension therapies in mice, which are genetically similar to humans. So far, three Mouse Prizes have been awarded: one for breaking longevity records to Dr. Andrzej Bartke of Southern Illinois University; one for late-onset rejuvenation strategies to Dr. Stephen Spindler of the University of California; and one to Dr. Z. Dave Sharp for his work with the pharmaceutical rapamycin.\n\nScience fiction has criticized developing and future technologies, but also inspires innovation and new technology. This topic has been more often discussed in literary and sociological than in scientific forums. Cinema and media theorist Vivian Sobchack examines the dialogue between science fiction films and technological imagination. Technology impacts artists and how they portray their fictionalized subjects, but the fictional world gives back to science by broadening imagination. \"How William Shatner Changed the World\" is a documentary that gave a number of real-world examples of actualized technological imaginations. While more prevalent in the early years of science fiction with writers like Arthur C. Clarke, new authors still find ways to make currently impossible technologies seem closer to being realized.\n\n\n\n\n\n\n"}
{"id": "2654494", "url": "https://en.wikipedia.org/wiki?curid=2654494", "title": "Engineering design management", "text": "Engineering design management\n\nEngineering design management is a knowledge area within engineering management. It represents the adaptation and application of customary management practices, with the intention of achieving a productive engineering design process. Engineering design management is primarily applied in the context of engineering design teams, whereby the activities, outputs and influences of design teams are planned, guided, monitored and controlled.\n"}
{"id": "21977757", "url": "https://en.wikipedia.org/wiki?curid=21977757", "title": "Flying syringe", "text": "Flying syringe\n\nFlying syringe is a phrase that is used to refer to proposed, but not yet created, genetically modified mosquitoes that inject vaccines into people when they bite them.\n\nIn 2008 the Gates Foundation awarded $100,000 to Hiroyuki Matsuoka of Jichi Medical University in Japan to do research on them, with a condition that any discoveries that were funded by the grant must be made available at affordable prices in the developing world. If Matsuoka proves that his idea has merit, he will be eligible for an additional $1 million of funding. \"The Washington Post\" referred to flying syringes as a \"bold idea\".\n"}
{"id": "27561089", "url": "https://en.wikipedia.org/wiki?curid=27561089", "title": "General Maxwell R. Thurman Award", "text": "General Maxwell R. Thurman Award\n\nThe General Maxwell R. Thurman Award is awarded on behalf of the commanding general of the United States Army Medical Research and Materiel Command at the American Telemedicine Association (ATA) annual conference in recognition of someone who exhibits and demonstrates the following qualities and attributes: a natural born leader, an innovator, someone who fosters positive change, a champion of the soldier, and a pioneer of the advancement of technology to enhance the life of our service members. The award is named after four star General Maxwell R. Thurman, who was a champion of the soldier and a pioneer of advancing technology. His famous motto, which is engraved on the award is as follows: \"The mission of the Army Medical Department is to provide world class combat casualty care to America's most precious resource - It's Sons and Daughters - In Peace and War!\" This award was originally given out at the very first DoD \"Global Forum\" by its creator, BG Russ Zajtchuk in 1995 and is now an annual tradition at every ATA meeting. \n\nhttp://media.americantelemed.org/images/conf/05%20Conf/summary.thurman.jpg\nhttp://media.americantelemed.org/images/conf/02conf.thurmanaward.jpg\n"}
{"id": "36535684", "url": "https://en.wikipedia.org/wiki?curid=36535684", "title": "Glossary of engineering", "text": "Glossary of engineering\n\n\"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\"\n\nThis glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.\n\n \n\n \n\nwhere formula_1 is the infinitesimal amount of heat absorbed by the system from the reservoir and formula_2 is the temperature of the external reservoir (surroundings) at a particular instant in time. In the special case of a reversible process, the equality holds. The reversible case is used to introduce the entropy state function. This is because in a cyclic process the variation of a state function is zero. In words, the Clausius statement states that it is impossible to construct a device whose sole effect is the transfer of heat from a cool reservoir to a hot reservoir. Equivalently, heat spontaneously flows from a hot body to a cooler one, not the other way around. The generalized \"inequality of Clausius\" \n\n \n\n \n\n \n\n \n\n \n\n \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n"}
{"id": "40927352", "url": "https://en.wikipedia.org/wiki?curid=40927352", "title": "HARASSmap", "text": "HARASSmap\n\nHARASSmap is a mobile and online technology non-profit that uses interactive mapping to try to reduce the social acceptability of sexual harassment throughout Egypt.\n\nAs of 2005, HARASSmap co-founder Rebecca Chiao began investigating the prevalence of sexual harassment in the daily life of Egyptian women and eventually, with the help of friends and volunteers, launched a campaign that would eventually be adopted by the Egyptian Center for Women's Rights. While these efforts were aimed towards changing sexual harassment legislation within the Egyptian government to better criminalize offences, there was a push for more urgent action. In a 2008 study conducted by the Egyptian Center for Women's Rights, researchers found that out of the 1,010 women they surveyed, 83% of Egyptian women and 98% of foreign women in Egypt said they had experienced sexual harassment. Deciding it was time for some on-the-ground action, Chiao and HARASSmap co-founder Engy Ghozlan decided to harness the power of a mobile friendly population and linked FrontlineSMS and Ushahidi to create the technological basis for HARASSmap.\n\nHARASSmap was co-founded in 2010 by Rebecca Chiao (Project Leader), Engy Ghozlan, Amel Fahmy (Principal investigator of Research Unit) and Sawsan Gad.\n\nAs a volunteer-based initiative, HARASSmap aims to end the social acceptability of sexual harassment and assault in Egypt. In addition to its interactive mapping service and community outreach service, HARASSmap offers self-defence classes and community education for both men and women.\n\nWhen someone experiences or is a witness to an incidence of sexual harassment, they can fill out an online report or send the report via SMS, e-mail, Twitter or Facebook including the details of the incident as well as address, street name and public points of interest. HARASSmap then verifies the reports and places them on a Google map of Egypt, which localises sexual harassment hotspots. The map will show red dots where incidences of sexual harassment have taken place.\n\nHARASSmap volunteers visit the areas where incidences have occurred to raise awareness about what constitutes sexual harassment and to work towards ending it. By meeting with local shop owners, police officers, doormen and other public venues, the HARASSmap team is working to mobilize them to make their neighbourhoods \"harassment-free zones\".\n\nHARASSmap won the 2011 World Summit Youth Award and the 2012 Deutsche Welle Best of the Blogs Award for 'Best Use of Technology for Social Good' and is now an incubated social enterprise at Nahdey El Mahrousa. Since HARASSmap's inception, they have been approached by activists from 25 countries for help adopting similar initiatives. In 2012, the International Development Research Centre (IDRC) of Canada offered HARASSmap a grant to continue study sexual harassment in Egypt based on reports submitted by participants and gather information on the methodological issues in the collection and use of crowd sourced data.\n\n\n"}
{"id": "26126668", "url": "https://en.wikipedia.org/wiki?curid=26126668", "title": "History of the anti-nuclear movement", "text": "History of the anti-nuclear movement\n\nThe application of nuclear technology, both as a source of energy and as an instrument of war, has been controversial.\n\nScientists and diplomats have debated nuclear weapons policy since before the atomic bombing of Hiroshima in 1945. The public became concerned about nuclear weapons testing from about 1954, following extensive nuclear testing in the Pacific. In 1961, at the height of the Cold War, about 50,000 women brought together by Women Strike for Peace marched in 60 cities in the United States to demonstrate against nuclear weapons. In 1963, many countries ratified the Partial Test Ban Treaty which prohibited atmospheric nuclear testing.\n\nSome local opposition to nuclear power emerged in the early 1960s, and in the late 1960s some members of the scientific community began to express their concerns. In the early 1970s, there were large protests about a proposed nuclear power plant in Wyhl, Germany. The project was cancelled in 1975 and anti-nuclear success at Wyhl inspired opposition to nuclear power in other parts of Europe and North America. Nuclear power became an issue of major public protest in the 1970s.\n\nIn 1945 in the New Mexico desert, American scientists conducted \"Trinity,\" the first nuclear weapons test, marking the beginning of the atomic age. Even before the Trinity test, national leaders debated the impact of nuclear weapons on domestic and foreign policy. Also involved in the debate about nuclear weapons policy was the scientific community, through professional associations such as the Federation of Atomic Scientists and the Pugwash Conference on Science and World Affairs.\n\nOn August 6, 1945, towards the end of World War II, the Little Boy device was detonated over the Japanese military city of Hiroshima. Exploding with a yield equivalent to 12,500 tonnes of TNT, the blast and thermal wave of the bomb destroyed nearly 50,000 buildings (including the headquarters of the 2nd General Army and Fifth Division) and killed approximately 75,000 people, among them 20,000 Japanese soldiers and 20,000 Korean slave laborers. Detonation of the Fat Man device exploded over the Japanese industrial city of Nagasaki three days later after Hiroshima, destroying 60% of the city and killing approximately 35,000 people, among them 23,200-28,200 Japanese munitions workers, 2,000 Korean slave laborers, and 150 Japanese soldiers. The two bombings remains the only events where nuclear weapons have been used in combat. Subsequently, the world's nuclear weapons stockpiles grew.\n\nOperation Crossroads was a series of nuclear weapon tests conducted by the United States at Bikini Atoll in the Pacific Ocean in the summer of 1946. Its purpose was to test the effect of nuclear weapons on naval ships. Pressure to cancel Operation Crossroads came from scientists and diplomats. Manhattan Project scientists argued that further nuclear testing was unnecessary and environmentally dangerous. A Los Alamos study warned \"the water near a recent surface explosion will be a witch's brew\" of radioactivity. To prepare the atoll for the nuclear tests, Bikini's native residents were evicted from their homes and resettled on smaller, uninhabited islands where they were unable to sustain themselves.\n\nRadioactive fallout from nuclear weapons testing was first drawn to public attention in 1954 when a Hydrogen bomb test in the Pacific contaminated the crew of the Japanese fishing boat \"Lucky Dragon\". One of the fishermen died in Japan seven months later. The incident caused widespread concern around the world and \"provided a decisive impetus for the emergence of the anti-nuclear weapons movement in many countries\". The anti-nuclear weapons movement grew rapidly because for many people the atomic bomb \"encapsulated the very worst direction in which society was moving\".\n\nPeace movements emerged in Japan and in 1954 they converged to form a unified \"Japanese Council Against Atomic and Hydrogen Bombs\". Japanese opposition to the Pacific nuclear weapons tests was widespread, and \"an estimated 35 million signatures were collected on petitions calling for bans on nuclear weapons\".\n\nGerman publications of the 1950s and 1960s contained criticism of some features of nuclear power including its safety. Nuclear waste disposal was widely recognized as a major problem, with concern publicly expressed as early as 1954. In 1964, one author went so far as to state \"that the dangers and costs of the necessary final disposal of nuclear waste could possibly make it necessary to forego the development of nuclear energy\".\n\nThe Russell–Einstein Manifesto was issued in London on July 9, 1955 by Bertrand Russell in the midst of the Cold War. It highlighted the dangers posed by nuclear weapons and called for world leaders to seek peaceful resolutions to international conflict. The signatories included eleven pre-eminent intellectuals and scientists, including Albert Einstein, who signed it just days before his death on April 18, 1955. A few days after the release, philanthropist Cyrus S. Eaton offered to sponsor a conference—called for in the manifesto—in Pugwash, Nova Scotia, Eaton's birthplace. This conference was to be the first of the Pugwash Conferences on Science and World Affairs, held in July 1957.\n\nIn the United Kingdom, the first Aldermaston March organised by the Campaign for Nuclear Disarmament took place at Easter 1958, when several thousand people marched for four days from Trafalgar Square, London, to the Atomic Weapons Research Establishment close to Aldermaston in Berkshire, England, to demonstrate their opposition to nuclear weapons. The Aldermaston marches continued into the late 1960s when tens of thousands of people took part in the four-day marches.\n\nIn 1959, a letter in the \"Bulletin of the Atomic Scientists\" was the start of a successful campaign to stop the Atomic Energy Commission dumping radioactive waste in the sea 19 kilometres from Boston.\n\nOn November 1, 1961, at the height of the Cold War, about 50,000 women brought together by Women Strike for Peace marched in 60 cities in the United States to demonstrate against nuclear weapons. It was the largest national women's peace protest of the 20th century.\n\nIn 1958, Linus Pauling and his wife presented the United Nations with the petition signed by more than 11,000 scientists calling for an end to nuclear-weapon testing. The \"Baby Tooth Survey,\" headed by Dr Louise Reiss, demonstrated conclusively in 1961 that above-ground nuclear testing posed significant public health risks in the form of radioactive fallout spread primarily via milk from cows that had ingested contaminated grass. Public pressure and the research results subsequently led to a moratorium on above-ground nuclear weapons testing, followed by the Partial Test Ban Treaty, signed in 1963 by John F. Kennedy and Nikita Khrushchev. On the day that the treaty went into force, the Nobel Prize Committee awarded Pauling the Nobel Peace Prize, describing him as \"Linus Carl Pauling, who ever since 1946 has campaigned ceaselessly, not only against nuclear weapons tests, not only against the spread of these armaments, not only against their very use, but against all warfare as a means of solving international conflicts.\"\nPauling started the International League of Humanists in 1974. He was president of the scientific advisory board of the World Union for Protection of Life and also one of the signatories of the Dubrovnik-Philadelphia Statement.\n\nIn the United States, the first commercially viable nuclear power plant was to be built at Bodega Bay, north of San Francisco, but the proposal was controversial and conflict with local citizens began in 1958. The proposed plant site was close to the San Andreas Fault and close to the region's environmentally sensitive fishing and dairy industries. The Sierra Club became actively involved. The conflict ended in 1964, with the forced abandonment of plans for the power plant. Historian Thomas Wellock traces the birth of the anti-nuclear movement to the controversy over Bodega Bay. Attempts to build a nuclear power plant in Malibu were similar to those at Bodega Bay and were also abandoned.\n\nIn 1966, Larry Bogart founded the Citizens Energy Council, a coalition of environmental groups that published the newsletters \"Radiation Perils,\" \"Watch on the A.E.C.\" and \"Nuclear Opponents\". These publications argued that \"nuclear power plants were too complex, too expensive and so inherently unsafe they would one day prove to be a financial disaster and a health hazard\".\n\nThe emergence of the anti-nuclear power movement was \"closely associated with the general rise in environmental consciousness which had started to materialize in the USA in the 1960s and quickly spread to other Western industrialized countries\". Some nuclear experts began to voice dissenting views about nuclear power in 1969, and this was a necessary precondition for broad public concern about nuclear power to emerge. These scientists included Ernest Sternglass from Pittsburg, Henry Kendall from the Massachusetts Institute of Technology, Nobel laureate George Wald and radiation specialist Rosalie Bertell. These members of the scientific community \"by expressing their concern over nuclear power, played a crucial role in demystifying the issue for other citizens\", and nuclear power became an issue of major public protest in the 1970s.\n\nIn 1971, 15,000 people demonstrated against French plans to locate the first light-water reactor power plant in Bugey. This was the first of a series of mass protests organized at nearly every planned nuclear site in France.\n\nAlso in 1971, the town of Wyhl, in Germany, was a proposed site for a nuclear power station. In the years that followed, public opposition steadily mounted, and there were large protests. Television coverage of police dragging away farmers and their wives helped to turn nuclear power into a major issue. In 1975, an administrative court withdrew the construction licence for the plant, but the Wyhl occupation generated ongoing debate. This initially centred on the state government's handling of the affair and associated police behaviour, but interest in nuclear issues was also stimulated. The Wyhl experience encouraged the formation of citizen action groups near other planned nuclear sites. Many other anti-nuclear groups formed elsewhere, in support of these local struggles, and some existing citizen action groups widened their aims to include the nuclear issue. Anti-nuclear success at Wyhl also inspired nuclear opposition in the rest of Europe and North America.\n\nIn 1972, the anti-nuclear weapons movement maintained a presence in the Pacific, largely in response to French nuclear testing there. Activists, including David McTaggart from Greenpeace, defied the French government by sailing small vessels into the test zone and interrupting the testing program. In Australia, thousands joined protest marches in Adelaide, Melbourne, Brisbane, and Sydney. Scientists issued statements demanding an end to the tests; unions refused to load French ships, service French planes, or carry French mail; and consumers boycotted French products. In Fiji, activists formed an Against Testing on Mururoa organization.\n\nIn Spain, in response to a surge in nuclear power plant proposals in the 1960s, a strong anti-nuclear movement emerged in 1973, which ultimately impeded the realisation of most of the projects.\n\nIn 1974, organic farmer Sam Lovejoy took a crowbar to the weather-monitoring tower which had been erected at the Montague Nuclear Power Plant site. Lovejoy felled the tower and then took himself to the local police station, where he took full responsibility for the action. Lovejoy's action galvanized local public opinion against the plant. The Montague project was canceled in 1980, after $29 million was spent on the project.\n\nBy the mid-1970s anti-nuclear activism had moved beyond local protests and politics to gain a wider appeal and influence. Although it lacked a single co-ordinating organization, and did not have uniform goals, the movement's efforts gained a great deal of attention. Jim Falk has suggested that popular opposition to nuclear power quickly grew into an effective anti-nuclear power movement in the 1970s. In some countries, the nuclear power conflict \"reached an intensity unprecedented in the history of technology controversies\".\n\nIn France, between 1975 and 1977, some 175,000 people protested against nuclear power in ten demonstrations.\n\nIn West Germany, between February 1975 and April 1979, some 280,000 people were involved in seven demonstrations at nuclear sites. Several site occupations were also attempted. In the aftermath of the Three Mile Island accident in 1979, some 120,000 people attended a demonstration against nuclear power in Bonn.\n\nIn May 1979, an estimated 70,000 people, including the governor of California, attended a march and rally against nuclear power in Washington, D.C.\n\nOn June 12, 1982, one million people demonstrated in New York City's Central Park against nuclear weapons and for an end to the cold war arms race. It was, and is, the largest anti-nuclear protest and the largest peace demonstration in American history. International Day of Nuclear Disarmament protests were held on June 20, 1983 at 50 sites across the United States. In 1986, hundreds of people walked from Los Angeles to Washington DC in the Great Peace March for Global Nuclear Disarmament. There were many Nevada Desert Experience protests and peace camps at the Nevada Test Site during the 1980s and 1990s.\n\nOn May 1, 2005, 40,000 anti-nuclear/anti-war protesters marched past the United Nations in New York, 60 years after the atomic bombings of Hiroshima and Nagasaki. This was the largest anti-nuclear rally in the U.S. for several decades. In Britain, there were many protests about the government's proposal to replace the aging Trident weapons system with a newer model. The largest protest had 100,000 participants and, according to polls, 59 percent of the public opposed the move.\n\nThe International Conference on Nuclear Disarmament took place in Oslo in February 2008, and was organized by The Government of Norway, the Nuclear Threat Initiative and the Hoover Institute. The Conference was entitled \"Achieving the Vision of a World Free of Nuclear Weapons\" and had the purpose of building consensus between nuclear weapon states and non-nuclear weapon states in relation to the Nuclear Non-proliferation Treaty.\n\nIn May 2010, some 25,000 people, including members of peace organizations and 1945 atomic bomb survivors, marched for about two kilometers from downtown New York to the United Nations headquarters, calling for the elimination of nuclear weapons.\n\nEarly anti-nuclear advocates expressed the view that affluent lifestyles on a global scale strain the viability of the natural environment and that nuclear energy would enable those lifestyles. Examples of such expressions are:\n"}
{"id": "14449116", "url": "https://en.wikipedia.org/wiki?curid=14449116", "title": "History of timekeeping devices", "text": "History of timekeeping devices\n\nFor thousands of years, devices have been used to measure and keep track of time. The current sexagesimal system of time measurement dates to approximately 2000  from the Sumerians.\n\nThe Egyptians divided the day into two 12-hour periods, and used large obelisks to track the movement of the sun. They also developed water clocks, which were probably first used in the Precinct of Amun-Re, and later outside Egypt as well; they were employed frequently by the Ancient Greeks, who called them \"clepsydrae\". The Zhou dynasty is believed to have used the outflow water clock around the same time, devices which were introduced from Mesopotamia as early as 2000.\n\nOther ancient timekeeping devices include the candle clock, used in ancient China, ancient Japan, England and Mesopotamia; the timestick, widely used in India and Tibet, as well as some parts of Europe; and the hourglass, which functioned similarly to a water clock. The sundial, another early clock, relies on shadows to provide a good estimate of the hour on a sunny day. It is not so useful in cloudy weather or at night and requires recalibration as the seasons change (if the gnomon was not aligned with the Earth's axis).\n\nThe earliest known clock with a water-powered escapement mechanism, which transferred rotational energy into intermittent motions, dates back to 3rd century in ancient Greece; Chinese engineers later invented clocks incorporating mercury-powered escapement mechanisms in the 10th century, followed by Iranian engineers inventing water clocks driven by gears and weights in the 11th century.\n\nThe first mechanical clocks, employing the verge escapement mechanism with a foliot or balance wheel timekeeper, were invented in Europe at around the start of the 14th century, and became the standard timekeeping device until the pendulum clock was invented in 1656. The invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century.\n\nThe pendulum clock remained the most accurate timekeeper until the 1930s, when quartz oscillators were invented, followed by atomic clocks after World War 2. Although initially limited to laboratories, the development of microelectronics in the 1960s made quartz clocks both compact and cheap to produce, and by the 1980s they became the world's dominant timekeeping technology in both clocks and wristwatches.\n\nAtomic clocks are far more accurate than any previous timekeeping device, and are used to calibrate other clocks and to calculate the International Atomic Time; a standardized civil system, Coordinated Universal Time, is based on atomic time.\n\nMany ancient civilizations observed astronomical bodies, often the Sun and Moon, to determine times, dates, and seasons. The first calendars may have been created during the last glacial period, by hunter-gatherers who employed tools such as sticks and bones to track the phases of the moon or the seasons. Stone circles, such as England's Stonehenge, were built in various parts of the world, especially in Prehistoric Europe, and are thought to have been used to time and predict seasonal and annual events such as equinoxes or solstices. As those megalithic civilizations left no recorded history, little is known of their calendars or timekeeping methods. Methods of sexagesimal timekeeping, now common in both Western and Eastern societies, are first attested nearly 4,000 years ago in Mesopotamia and Egypt. Mesoamericans similarly modified their usual vigesimal counting system when dealing with calendars to produce a 360-day year.\n\nThe oldest known sundial is from Egypt; it dates back to around 1500 (19th Dynasty), and was discovered in the Valley of the Kings in 2013. Sundials have their origin in shadow clocks, which were the first devices used for measuring the parts of a day. Ancient Egyptian obelisks, constructed about 3500, are also among the earliest shadow clocks.\nEgyptian shadow clocks divided daytime into 12 parts with each part further divided into more precise parts. One type of shadow clock consisted of a long stem with five variable marks and an elevated crossbar which cast a shadow over those marks. It was positioned eastward in the morning, and was turned west at noon. Obelisks functioned in much the same manner: the shadow cast on the markers around it allowed the Egyptians to calculate the time. The obelisk also indicated whether it was morning or afternoon, as well as the summer and winter solstices. A third shadow clock, developed c. 1500, was similar in shape to a bent T-square. It measured the passage of time by the shadow cast by its crossbar on a non-linear rule. The \"T\" was oriented eastward in the mornings, and turned around at noon, so that it could cast its shadow in the opposite direction.\n\nAlthough accurate, shadow clocks relied on the sun, and so were useless at night and in cloudy weather. The Egyptians therefore developed a number of alternative timekeeping instruments, including water clocks, and a system for tracking star movements. The oldest description of a water clock is from the tomb inscription of the 16th-century Egyptian court official Amenemhet, identifying him as its inventor. There were several types of water clocks, some more elaborate than others. One type consisted of a bowl with small holes in its bottom, which was floated on water and allowed to fill at a near-constant rate; markings on the side of the bowl indicated elapsed time, as the surface of the water reached them. The oldest-known waterclock was found in the tomb of pharaoh Amenhotep I (1525–1504), suggesting that they were first used in ancient Egypt. Another Egyptian method of determining the time during the night was using plumb-lines called merkhets. In use since at least 600, two of these instruments were aligned with Polaris, the north pole star, to create a north–south meridian. The time was accurately measured by observing certain stars as they crossed the line created with the \"merkhets\".\n\nWater clocks, or clepsydrae, were commonly used in Ancient Greece following their introduction by Plato, who also invented a water-based alarm clock. One account of Plato's alarm clock describes it as depending on the nightly overflow of a vessel containing lead balls, which floated in a columnar vat. The vat held a steadily increasing amount of water, supplied by a cistern. By morning, the vessel would have floated high enough to tip over, causing the lead balls to cascade onto a copper platter. The resultant clangor would then awaken Plato's students at the Academy. Another possibility is that it comprised two jars, connected by a siphon. Water emptied until it reached the siphon, which transported the water to the other jar. There, the rising water would force air through a whistle, sounding an alarm. The Greeks and Chaldeans regularly maintained timekeeping records as an essential part of their astronomical observations.\n\nGreek astronomer, Andronicus of Cyrrhus, supervised the construction of the Tower of the Winds in Athens in the 1st century.\n\nIn Greek tradition, clepsydrae were used in court; later, the Romans adopted this practice, as well. There are several mentions of this in historical records and literature of the era; for example, in \"Theaetetus\", Plato says that \"Those men, on the other hand, always speak in haste, for the flowing water urges them on\". Another mention occurs in Lucius Apuleius' \"The Golden Ass\": \"The Clerk of the Court began bawling again, this time summoning the chief witness for the prosecution to appear. Up stepped an old man, whom I did not know. He was invited to speak for as long as there was water in the clock; this was a hollow globe into which water was poured through a funnel in the neck, and from which it gradually escaped through fine perforations at the base\". The clock in Apuleius's account was one of several types of water clock used. Another consisted of a bowl with a hole in its centre, which was floated on water. Time was kept by observing how long the bowl took to fill with water.\n\nAlthough clepsydrae were more useful than sundials—they could be used indoors, during the night, and also when the sky was cloudy—they were not as accurate; the Greeks, therefore, sought a way to improve their water clocks. Although still not as accurate as sundials, Greek water clocks became more accurate around 325, and they were adapted to have a face with an hour hand, making the reading of the clock more precise and convenient. One of the more common problems in most types of clepsydrae was caused by water pressure: when the container holding the water was full, the increased pressure caused the water to flow more rapidly. This problem was addressed by Greek and Roman horologists beginning in 100, and improvements continued to be made in the following centuries. To counteract the increased water flow, the clock's water containers—usually bowls or jugs—were given a conical shape; positioned with the wide end up, a greater amount of water had to flow out in order to drop the same distance as when the water was lower in the cone. Along with this improvement, clocks were constructed more elegantly in this period, with hours marked by gongs, doors opening to miniature figurines, bells, or moving mechanisms. There were some remaining problems, however, which were never solved, such as the effect of temperature. Water flows more slowly when cold, or may even freeze.\n\nBetween 270 and 500, Hellenistic (Ctesibius, Hero of Alexandria, Archimedes) and Roman horologists and astronomers began developing more elaborate mechanized water clocks. The added complexity was aimed at regulating the flow and at providing fancier displays of the passage of time. For example, some water clocks rang bells and gongs, while others opened doors and windows to show figurines of people, or moved pointers, and dials. Some even displayed astrological models of the universe.\n\nAlthough the Greeks and Romans did much to advance water clock technology, they still continued to use shadow clocks. The mathematician and astronomer Theodosius of Bithynia, for example, is said to have invented a universal sundial that was accurate anywhere on Earth, though little is known about it. Others wrote of the sundial in the mathematics and literature of the period. Marcus Vitruvius Pollio, the Roman author of \"De Architectura\", wrote on the mathematics of gnomons, or sundial blades. During the reign of Emperor Augustus, the Romans constructed the largest sundial ever built, the Solarium Augusti. Its gnomon was an obelisk from Heliopolis. Similarly, the obelisk from Campus Martius was used as the gnomon for Augustus's zodiacal sundial. Pliny the Elder records that the first sundial in Rome arrived in 264, looted from Catania, Sicily; according to him, it gave the incorrect time until the markings and angle appropriate for Rome's latitude were used—a century later.\n\nAccording to Callisthenes, the Persians were using water clocks in 328 to ensure a just and exact distribution of water from qanats to their shareholders for agricultural irrigation. The use of water clocks in Iran, especially in Zeebad, dates back to 500. Later they were also used to determine the exact holy days of pre-Islamic religions, such as the \"Nowruz\", \"Chelah\", or \"Yaldā\" – the shortest, longest, and equal-length days and nights of the years. The water clocks used in Iran were one of the most practical ancient tools for timing the yearly calendar.\n\nWater clocks, or \"Fenjaan\", in Persia reached a level of accuracy comparable to today's standards of timekeeping. The fenjaan was the most accurate and commonly used timekeeping device for calculating the amount or the time that a farmer must take water from a qanat or well for irrigation of the farms, until it was replaced by more accurate current clock. Persian water clocks were a practical and useful tool for the qanat's shareholders to calculate the length of time they could divert water to their farm. The qanat was the only water source for agriculture and irrigation so a just and fair water distribution was very important. Therefore, a very fair and clever old person was elected to be the manager of the water clock, and at least two full-time managers were needed to control and observe the number of fenjaans and announce the exact time during the days and nights.\n\nThe fenjaan was a big pot full of water and a bowl with small hole in the center. When the bowl become full of water, it would sink into the pot, and the manager would empty the bowl and again put it on the top of the water in the pot. He would record the number of times the bowl sank by putting small stones into a jar.\n\nThe place where the clock was situated, and its managers, were collectively known as \"khaneh fenjaan\". Usually this would be the top floor of a public-house, with west- and east-facing windows to show the time of sunset and sunrise. There was also another time-keeping tool named a \"staryab\" or astrolabe, but it was mostly used for superstitious beliefs and was not practical for use as a farmers' calendar. The Zeebad Gonabad water clock was in use until 1965 when it was substituted by modern clocks.\n\nJoseph Needham speculated that the introduction of the outflow clepsydra to China, perhaps from Mesopotamia, occurred as far back as the 2nd millennium, during the Shang Dynasty, and at the latest by the 1st millennium. By the beginning of the Han Dynasty, in 202, the outflow clepsydra was gradually replaced by the inflow clepsydra, which featured an indicator rod on a float. To compensate for the falling pressure head in the reservoir, which slowed timekeeping as the vessel filled, Zhang Heng added an extra tank between the reservoir and the inflow vessel. Around 550 AD, Yin Gui was the first in China to write of the overflow or constant-level tank added to the series, which was later described in detail by the inventor Shen Kuo. Around 610, this design was trumped by two Sui Dynasty inventors, Geng Xun and Yuwen Kai, who were the first to create the balance clepsydra, with standard positions for the steelyard balance. Joseph Needham states that:\nThe term 'clock' encompasses a wide spectrum of devices, ranging from wristwatches to the Clock of the Long Now. The English word \"clock\" is said to derive from the Middle English \"clokke\", Old North French \"cloque\", or Middle Dutch \"clocke\", all of which mean \"bell\", and are derived from the Medieval Latin \"clocca\", also meaning bell. Indeed, bells were used to mark the passage of time; they marked the passage of the hours at sea and in abbeys.\n\nThroughout history, clocks have had a variety of power sources, including gravity, springs, and electricity. Mechanical clocks became widespread in the 14th century, when they were used in medieval monasteries to keep the regulated schedule of prayers. The clock continued to be improved, with the first pendulum clock being designed and built in the 17th century.\n\nThe earliest mention of candle clocks comes from a Chinese poem, written in 520 by You Jianfu. According to the poem, the graduated candle was a means of determining time at night. Similar candles were used in Japan until the early 10th century.\n\nThe candle clock most commonly mentioned and written of is attributed to King Alfred the Great. It consisted of six candles made from 72 pennyweights of wax, each high, and of uniform thickness, marked every inch (2.54 cm). As these candles burned for about four hours, each mark represented 20 minutes. Once lit, the candles were placed in wooden framed glass boxes, to prevent the flame from extinguishing.\n\nThe most sophisticated candle clocks of their time were those of Al-Jazari in 1206. One of his candle clocks included a dial to display the time and, for the first time, employed a bayonet fitting, a fastening mechanism still used in modern times. Donald Routledge Hill described Al-Jazari's candle clocks as follows:\n\nA variation on this theme were oil-lamp clocks. These early timekeeping devices consisted of a graduated glass reservoir to hold oil — usually whale oil, which burned cleanly and evenly — supplying the fuel for a built-in lamp. As the level in the reservoir dropped, it provided a rough measure of the passage of time.\n\nIn addition to water, mechanical, and candle clocks, incense clocks were used in the Far East, and were fashioned in several different forms. Incense clocks were first used in China around the 6th century; in Japan, one still exists in the Shōsōin, although its characters are not Chinese, but Devanagari. Due to their frequent use of Devanagari characters, suggestive of their use in Buddhist ceremonies, Edward H. Schafer speculated that incense clocks were invented in India. Although similar to the candle clock, incense clocks burned evenly and without a flame; therefore, they were more accurate and safer for indoor use.\n\nSeveral types of incense clock have been found, the most common forms include the incense stick and incense seal. An incense stick clock was an incense stick with calibrations; most were elaborate, sometimes having threads, with weights attached, at even intervals. The weights would drop onto a platter or gong below, signifying that a certain amount of time had elapsed. Some incense clocks were held in elegant trays; open-bottomed trays were also used, to allow the weights to be used together with the decorative tray. Sticks of incense with different scents were also used, so that the hours were marked by a change in fragrance. The incense sticks could be straight or spiraled; the spiraled ones were longer, and were therefore intended for long periods of use, and often hung from the roofs of homes and temples. In Japan, a geisha was paid for the number of \"senkodokei\" (incense sticks) that had been consumed while she was present, a practice which continued until 1924.\n\nIncense seal clocks were used for similar occasions and events as the stick clock; while religious purposes were of primary importance, these clocks were also popular at social gatherings, and were used by Chinese scholars and intellectuals. The seal was a wooden or stone disk with one or more grooves etched in it into which incense was placed. These clocks were common in China, but were produced in fewer numbers in Japan. To signal the passage of a specific amount of time, small pieces of fragrant woods, resins, or different scented incenses could be placed on the incense powder trails. Different powdered incense clocks used different formulations of incense, depending on how the clock was laid out. The length of the trail of incense, directly related to the size of the seal, was the primary factor in determining how long the clock would last; all burned for long periods of time, ranging between 12 hours and a month.\n\nWhile early incense seals were made of wood or stone, the Chinese gradually introduced disks made of metal, most likely beginning during the Song dynasty. This allowed craftsmen to more easily create both large and small seals, as well as design and decorate them more aesthetically. Another advantage was the ability to vary the paths of the grooves, to allow for the changing length of the days in the year. As smaller seals became more readily available, the clocks grew in popularity among the Chinese, and were often given as gifts. Incense seal clocks are often sought by modern-day clock collectors; however, few remain that have not already been purchased or been placed on display at museums or temples.\n\nSundials had been used for timekeeping since Ancient Egypt. Ancient dials were nodus-based with straight hour-lines that indicated unequal hours—also called temporary hours—that varied with the seasons. Every day was divided into 12 equal segments regardless of the time of year; thus, hours were shorter in winter and longer in summer. The sundial was further developed by Muslim astronomers. The idea of using hours of equal length throughout the year was the innovation of Abu'l-Hasan Ibn al-Shatir in 1371, based on earlier developments in trigonometry by Muhammad ibn Jābir al-Harrānī al-Battānī (Albategni). Ibn al-Shatir was aware that \"using a gnomon that is parallel to the Earth's axis will produce sundials whose hour lines indicate equal hours on any day of the year\". His sundial is the oldest polar-axis sundial still in existence. The concept appeared in Western sundials starting in 1446.\n\nFollowing the acceptance of heliocentrism and equal hours, as well as advances in trigonometry, sundials appeared in their present form during the Renaissance, when they were built in large numbers. In 1524, the French astronomer Oronce Finé constructed an ivory sundial, which still exists; later, in 1570, the Italian astronomer Giovanni Padovani published a treatise including instructions for the manufacture and laying out of mural (vertical) and horizontal sundials. Similarly, Giuseppe Biancani's \"Constructio instrumenti ad horologia solaria\" (c. 1620) discusses how to construct sundials.\n\nSince the hourglass was one of the few reliable methods of measuring time at sea, it is speculated that it was used on board ships as far back as the 11th century, when it would have complemented the magnetic compass as an aid to navigation. However, the earliest unambiguous evidence of their use appears in the painting \"Allegory of Good Government\", by Ambrogio Lorenzetti, from 1338. From the 15th century onwards, hourglasses were used in a wide range of applications at sea, in churches, in industry, and in cooking; they were the first dependable, reusable, reasonably accurate, and easily constructed time-measurement devices. The hourglass also took on symbolic meanings, such as that of death, temperance, opportunity, and Father Time, usually represented as a bearded, old man. Though also used in China, the hourglass's history there is unknown. The Portuguese navigator Ferdinand Magellan used 18 hourglasses on each ship during his circumnavigation of the globe in 1522.\n\nThe earliest instance of a liquid-driven escapement was described by the Greek engineer Philo of Byzantium (fl. 3rd century) in his technical treatise \"Pneumatics\" (chapter 31) where he likens the escapement mechanism of a washstand automaton with those as employed in (water) clocks. Another early clock to use escapements was built during the 7th century in Chang'an, by Tantric monk and mathematician, Yi Xing, and government official Liang Lingzan. An astronomical instrument that served as a clock, it was discussed in a contemporary text as follows:\n[It] was made in the image of the round heavens and on it were shown the lunar mansions in their order, the equator and the degrees of the heavenly circumference. Water, flowing into scoops, turned a wheel automatically, rotating it one complete revolution in one day and night. Besides this, there were two rings fitted around the celestial sphere outside, having the sun and moon threaded on them, and these were made to move in circling orbit ... And they made a wooden casing the surface of which represented the horizon, since the instrument was half sunk in it. It permitted the exact determinations of the time of dawns and dusks, full and new moons, tarrying and hurrying. Moreover, there were two wooden jacks standing on the horizon surface, having one a bell and the other a drum in front of it, the bell being struck automatically to indicate the hours, and the drum being beaten automatically to indicate the quarters. All these motions were brought about by machinery within the casing, each depending on wheels and shafts, hooks, pins and interlocking rods, stopping devices and locks checking mutually.\n\nSince Yi Xing's clock was a water clock, it was affected by temperature variations. That problem was solved in 976 by Zhang Sixun by replacing the water with mercury, which remains liquid down to . Zhang implemented the changes into his clock tower, which was about tall, with escapements to keep the clock turning and bells to signal every quarter-hour. Another noteworthy clock, the elaborate Cosmic Engine, was built by Su Song, in 1088. It was about the size of Zhang's tower, but had an automatically rotating armillary sphere—also called a celestial globe—from which the positions of the stars could be observed. It also featured five panels with mannequins ringing gongs or bells, and tablets showing the time of day, or other special times. Furthermore, it featured the first known endless power-transmitting chain drive in horology. Originally built in the capital of Kaifeng, it was dismantled by the Jin army and sent to the capital of Yanjing (now Beijing), where they were unable to put it back together. As a result, Su Song's son Su Xie was ordered to build a replica.\nThe clock towers built by Zhang Sixun and Su Song, in the 10th and 11th centuries, respectively, also incorporated a striking clock mechanism, the use of clock jacks to sound the hours. A striking clock outside of China was the Jayrun Water Clock, at the Umayyad Mosque in Damascus, Syria, which struck once every hour. It was constructed by Muhammad al-Sa'ati in the 12th century, and later described by his son Ridwan ibn al-Sa'ati, in his \"On the Construction of Clocks and their Use\" (1203), when repairing the clock. In 1235, an early monumental water-powered alarm clock that \"announced the appointed hours of prayer and the time both by day and by night\" was completed in the entrance hall of the Mustansiriya Madrasah in Baghdad.\n\nThe first geared clock was invented in the 11th century by the Arab engineer Ibn Khalaf al-Muradi in Islamic Iberia; it was a water clock that employed a complex gear train mechanism, including both segmental and epicyclic gearing, capable of transmitting high torque. The clock was unrivalled in its use of sophisticated complex gearing, until the mechanical clocks of the mid-14th century. Al-Muradi's clock also employed the use of mercury in its hydraulic linkages, which could function mechanical automata. Al-Muradi's work was known to scholars working under Alfonso X of Castile, hence the mechanism may have played a role in the development of the European mechanical clocks. Other monumental water clocks constructed by medieval Muslim engineers also employed complex gear trains and arrays of automata. Like the earlier Greeks and Chinese, Arab engineers at the time also developed a liquid-driven escapement mechanism which they employed in some of their water clocks. Heavy floats were used as weights and a constant-head system was used as an escapement mechanism, which was present in the hydraulic controls they used to make heavy floats descend at a slow and steady rate.\n\nA mercury clock, described in the \"Libros del saber de Astronomia\", a Spanish work from 1277 consisting of translations and paraphrases of Arabic works, is sometimes quoted as evidence for Muslim knowledge of a mechanical clock. However, the device was actually a compartmented cylindrical water clock, which the Jewish author of the relevant section, Rabbi Isaac, constructed using principles described by a philosopher named \"Iran\", identified with Heron of Alexandria (fl. 1st century AD), on how heavy objects may be lifted.\n\nClock towers in Western Europe in the Middle Ages were also sometimes striking clocks. The most famous original still standing is possibly St Mark's Clock on the top of St Mark's Clocktower in St Mark's Square in Venice, assembled in 1493 by the clockmaker Gian Carlo Rainieri from Reggio Emilia. In 1497, Simone Campanato moulded the great bell on which every definite time-lapse is beaten by two mechanical bronze statues (h. 2,60 m.) called \"Due Mori\" (\"Two Moors\"), handling a hammer. Possibly earlier (1490) is the Prague Astronomical Clock by clockmaster Jan Růže (also called Hanuš) – according to another source this device was assembled as early as 1410 by clockmaker Mikuláš of Kadaň and mathematician Jan Šindel. The allegorical parade of animated sculptures rings on the hour every day.\n\nDuring the 11th century in the Song Dynasty, the Chinese astronomer, horologist and mechanical engineer Su Song created a water-driven astronomical clock for his clock tower of Kaifeng City. It incorporated an escapement mechanism as well as the earliest known endless power-transmitting chain drive, which drove the armillary sphere.\n\nContemporary Muslim astronomers also constructed a variety of highly accurate astronomical clocks for use in their mosques and observatories, such as the water-powered astronomical clock by Al-Jazari in 1206, and the astrolabic clock by Ibn al-Shatir in the early 14th century. The most sophisticated timekeeping astrolabes were the geared astrolabe mechanisms designed by Abū Rayhān Bīrūnī in the 11th century and by Muhammad ibn Abi Bakr in the 13th century. These devices functioned as timekeeping devices and also as calendars.\nA sophisticated water-powered astronomical clock was built by Al-Jazari in 1206. This castle clock was a complex device that was about high, and had multiple functions alongside timekeeping. It included a display of the zodiac and the solar and lunar paths, and a pointer in the shape of the crescent moon which travelled across the top of a gateway, moved by a hidden cart and causing doors to open, each revealing a mannequin, every hour. It was possible to reset the length of day and night in order to account for the changing lengths of day and night throughout the year. This clock also featured a number of automata including falcons and musicians who automatically played music when moved by levers operated by a hidden camshaft attached to a water wheel.\n\nThe earliest medieval European clockmakers were Catholic monks. Medieval religious institutions required clocks because they regulated daily prayer- and work-schedules strictly, using various types of time-telling and recording devices, such as water clocks, sundials and marked candles, probably in combination. When mechanical clocks came into use, they were often wound at least twice a day to ensure accuracy. Monasteries broadcast important times and durations with bells, rung either by hand or by a mechanical device, such as by a falling weight or by rotating beater.\n\nAlthough the mortuary inscription of Pacificus, archdeacon of Verona, records that he constructed a night clock (\"horologium nocturnum\") as early as 850, his clock has been identified as being an observation tube used to locate stars with an accompanying book of astronomical observations, rather than a mechanical or water clock, an interpretation supported by illustrations from medieval manuscripts.\n\nThe religious necessities and technical skill of the medieval monks were crucial factors in the development of clocks, as the historian Thomas Woods writes:\nThe appearance of clocks in writings of the 11th century implies that they were well known in Europe in that period. In the early 14th-century, the Florentine poet Dante Alighieri referred to a clock in his \"Paradiso\"; the first known literary reference to a clock that struck the hours. Giovanni da Dondi, Professor of Astronomy at Padua, presented the earliest detailed description of clockwork in his 1364 treatise \"Il Tractatus Astrarii\". This has inspired several modern replicas, including some in London's Science Museum and the Smithsonian Institution. Other notable examples from this period were built in Milan (1335), Strasbourg (1354), Lund (1380), Rouen (1389), and Prague (1462).\n\nSalisbury cathedral clock, dating from about 1386, is one of the oldest working clocks in the world, and may be the oldest. It still has most of its original parts, although its original verge and foliot timekeeping mechanism is lost, having been converted to a pendulum, which was replaced by a replica verge in 1956. It has no dial, as its purpose was to strike a bell at precise times. The wheels and gears are mounted in an open, box-like iron frame, measuring about square. The framework is held together with metal dowels and pegs. Two large stones, hanging from pulleys, supply the power. As the weights fall, ropes unwind from the wooden barrels. One barrel drives the main wheel, which is regulated by the escapement, and the other drives the striking mechanism and the air brake.\n\nNote also Peter Lightfoot's Wells Cathedral clock, constructed c. 1390. The dial represents a geocentric view of the universe, with the Sun and Moon revolving around a central fixed Earth. It is unique in having its original medieval face, showing a philosophical model of the pre-Copernican universe. Above the clock is a set of figures, which hit the bells, and a set of jousting knights who revolve around a track every 15 minutes. The clock was converted to pendulum-and-anchor escapement in the 17th century, and was installed in London's Science Museum in 1884, where it continues to operate. Similar astronomical clocks, or \"horologes\", survive at Exeter, Ottery St Mary, and Wimborne Minster.\nOne clock that has not survived is that of the Abbey of St Albans, built by the 14th-century abbot Richard of Wallingford. It may have been destroyed during Henry VIII's Dissolution of the Monasteries, but the abbot's notes on its design have allowed a full-scale reconstruction. As well as keeping time, the astronomical clock could accurately predict lunar eclipses, and may have shown the Sun, Moon (age, phase, and node), stars and planets, as well as a wheel of fortune, and an indicator of the state of the tide at London Bridge. According to Thomas Woods, \"a clock that equaled it in technological sophistication did not appear for at least two centuries\". Giovanni de Dondi was another early mechanical clockmaker whose clock did not survive, but his work has been replicated based on the designs. De Dondi's clock was a seven-faced construction with 107 moving parts, showing the positions of the Sun, Moon, and five planets, as well as religious feast days. Around this period, mechanical clocks were introduced into abbeys and monasteries to mark important events and times, gradually replacing water clocks which had served the same purpose.\n\nDuring the Middle Ages, clocks primarily served religious purposes; the first employed for secular timekeeping emerged around the 15th century. In Dublin, the official measurement of time became a local custom, and by 1466 a public clock stood on top of the Tholsel (the city court and council chamber). It was the first of its kind to be clearly recorded in Ireland, and would only have had an hour hand. The increasing lavishness of castles led to the introduction of turret clocks. A 1435 example survives from Leeds castle; its face is decorated with the images of the Crucifixion of Jesus, Mary and St George.\n\nEarly clock dials showed hours: the display of minutes and seconds evolved later. A clock with a minutes dial is mentioned in a 1475 manuscript, and clocks indicating minutes and seconds existed in Germany in the 15th century. Timepieces which indicated minutes and seconds were occasionally made from this time on, but this was not common until the increase in accuracy made possible by the pendulum clock and, in watches, by the spiral balance spring. The 16th-century astronomer Tycho Brahe used clocks with minutes and seconds to observe stellar positions.\n\nThe Ottoman engineer Taqi al-Din described a weight-driven clock with a verge-and-foliot escapement, a striking train of gears, an alarm, and a representation of the moon's phases in his book \"The Brightest Stars for the Construction of Mechanical Clocks\" (\"Al-Kawākib al-durriyya fī wadh' al-bankāmat al-dawriyya\"), written around 1556.\n\nThe concept of the wristwatch goes back to the production of the very earliest watches in the 16th century. Elizabeth I of England received a wristwatch from Robert Dudley in 1571, described as an arm watch. From the beginning, wrist watches were almost exclusively worn by women, while men used pocket-watches up until the early 20th century. This was not just a matter of fashion or prejudice; watches of the time were notoriously prone to fouling from exposure to the elements, and could only reliably be kept safe from harm if carried securely in the pocket. When the waistcoat was introduced as a manly fashion at the court of Charles II in the 17th century, the pocket watch was tucked into its pocket. Prince Albert, the consort to Queen Victoria, introduced the 'Albert chain' accessory, designed to secure the pocket watch to the man's outergarment by way of a clip. By the mid nineteenth century, most watchmakers produced a range of wristwatches, often marketed as bracelets, for women.\n\nWristwatches were first worn by military men towards the end of the nineteenth century, when the importance of synchronizing manoeuvres during war without potentially revealing the plan to the enemy through signalling was increasingly recognized. It was clear that using pocket watches while in the heat of battle or while mounted on a horse was impractical, so officers began to strap the watches to their wrist. The Garstin Company of London patented a 'Watch Wristlet' design in 1893, although they were probably producing similar designs from the 1880s. Clearly, a market for men's wristwatches was coming into being at the time. Officers in the British Army began using wristwatches during colonial military campaigns in the 1880s, such as during the Anglo-Burma War of 1885.\n\nDuring the Boer War, the importance of coordinating troop movements and synchronizing attacks against the highly mobile Boer insurgents was paramount, and the use of wristwatches subsequently became widespread among the officer class. The company Mappin & Webb began production of their successful 'campaign watch' for soldiers during the campaign at the Sudan in 1898 and ramped up production for the Boer War a few years later.\nThese early models were essentially standard pocket-watches fitted to a leather strap, but by the early 20th century, manufacturers began producing purpose-built wristwatches. The Swiss company, Dimier Frères & Cie patented a wristwatch design with the now standard wire lugs in 1903. In 1904, Alberto Santos-Dumont, an early aviator, asked his friend, a French watchmaker called Louis Cartier, to design a watch that could be useful during his flights.\n\nThe impact of the First World War dramatically shifted public perceptions on the propriety of the man's wristwatch, and opened up a mass market in the post-war era. The creeping barrage artillery tactic, developed during the War, required precise synchronization between the artillery gunners and the infantry advancing behind the barrage. Service watches produced during the War were specially designed for the rigours of trench warfare, with luminous dials and unbreakable glass. Wristwatches were also found to be needed in the air as much as on the ground: military pilots found them more convenient than pocket watches for the same reasons as Santos-Dumont had. The British War Department began issuing wristwatches to combatants from 1917.\nThe company H. Williamson Ltd., based in Coventry, was one of the first to capitalize on this opportunity. During the company's 1916 AGM it was noted that \"...the public is buying the practical things of life. Nobody can truthfully contend that the watch is a luxury. It is said that one soldier in every four wears a wristlet watch, and the other three mean to get one as soon as they can.\" By the end of the War, almost all enlisted men wore a wristwatch, and after they were demobilized, the fashion soon caught on – the British \"Horological Journal\" wrote in 1917 that \"...the wristlet watch was little used by the sterner sex before the war, but now is seen on the wrist of nearly every man in uniform and of many men in civilian attire.\" Within a decade, sales of wristwatches had outstripped those of pocket watches.\n\nIn the late 17th and 18th Centuries, equation clocks were made, which allowed the user to see or calculate apparent solar time, as would be shown by a sundial. Before the invention of the pendulum clock, sundials were the only accurate timepieces. When good clocks became available, they appeared inaccurate to people who were used to trusting sundials. The annual variation of the equation of time made a clock up to about 15 minutes fast or slow, relative to a sundial, depending on the time of year. Equation clocks satisfied the demand for clocks that always agreed with sundials. Several types of equation clock mechanism were devised. which can be seen in surviving examples, mostly in museums.\n\nInnovations to the mechanical clock continued, with miniaturization leading to domestic clocks in the 15th century, and personal watches in the 16th. In the 1580s, the Italian polymath Galileo Galilei investigated the regular swing of the pendulum, and discovered that it could be used to regulate a clock. Although Galileo studied the pendulum as early as 1582, he never actually constructed a clock based on that design. The first pendulum clock was designed and built by Dutch scientist Christiaan Huygens, in 1656. Early versions erred by less than one minute per day, and later ones only by 10 seconds, very accurate for their time.\n\nIn England, the manufacturing of pendulum clocks was soon taken up. The longcase clock (also known as the grandfather clock) was first created to house the pendulum and works by the English clockmaker William Clement in 1670 or 1671; this became feasible after Clement invented the anchor escapement mechanism in about 1670. Before then, pendulum clocks used the older verge escapement mechanism, which required very wide pendulum swings of about 100°. To avoid the need for a very large case, most clocks using the verge escapement had a short pendulum. The anchor mechanism, however, reduced the pendulum's necessary swing to between 4° to 6°, allowing clockmakers to use longer pendulums with consequently slower beats. These required less power to move, caused less friction and wear, and were more accurate than their shorter predecessors. Most longcase clocks use a pendulum about a metre (39 inches) long to the center of the bob, with each swing taking one second. This requirement for height, along with the need for a long drop space for the weights that power the clock, gave rise to the tall, narrow case.\n\nClement also introduced the pendulum suspension spring in 1671. The concentric minute hand was added to the clock by Daniel Quare, a London clock-maker, and the Second Hand was introduced.\n\nThe Jesuits were another major contributor to the development of pendulum clocks in the 17th and 18th centuries, having had an \"unusually keen appreciation of the importance of precision\". In measuring an accurate one-second pendulum, for example, the Italian astronomer Father Giovanni Battista Riccioli persuaded nine fellow Jesuits \"to count nearly 87,000 oscillations in a single day\". They served a crucial role in spreading and testing the scientific ideas of the period, and collaborated with contemporary scientists, such as Huygens.\n\nThe invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century. Some dispute remains as to whether British scientist Robert Hooke (his was a straight spring) or Dutch scientist Christiaan Huygens was the actual inventor of the balance spring. Huygens was clearly the first to use a spiral balance spring, the form used in virtually all watches to the present day. The addition of the balance spring made the balance wheel a harmonic oscillator like the pendulum in a pendulum clock, which oscillated at a fixed resonant frequency and resisted oscillating at other rates. This innovation increased watches' accuracy enormously, reducing error from perhaps several hours per day to perhaps 10 minutes per day, resulting in the addition of the minute hand to the watch face around 1680 in Britain and 1700 in France.\n\nLike the invention of pendulum clock, Huygens' spiral hairspring (balance spring) system of portable timekeepers, helped lay the foundations for the modern watchmaking industry. The application of the spiral balance spring for watches ushered in a new era of accuracy for portable timekeepers, similar to that which the pendulum had introduced for clocks. From its invention in 1675 by Christiaan Huygens, the spiral hairspring (balance spring) system for portable timekeepers, still used in mechanical watchmaking industry today.\n\nIn 1675, Huygens and Robert Hooke invented the spiral balance, or the hairspring, designed to control the oscillating speed of the balance wheel. This crucial advance finally made accurate pocket watches possible. This resulted in a great advance in accuracy of pocket watches, from perhaps several hours per day to 10 minutes per day, similar to the effect of the pendulum upon mechanical clocks. The great English clockmaker, Thomas Tompion, was one of the first to use this mechanism successfully in his pocket watches, and he adopted the minute hand which, after a variety of designs were trialled, eventually stabilised into the modern-day configuration.\n\nThe Rev. Edward Barlow invented the rack and snail striking mechanism for striking clocks, which was a great improvement over the previous mechanism. The repeating clock, that chimes the number of hours (or even minutes) was invented by either Quare or Barlow in 1676. George Graham invented the deadbeat escapement for clocks in 1720.\n\nMarine chronometers are clocks used at sea as time standards, to determine longitude by celestial navigation.\nA major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The position of a ship at sea could be determined with reasonable accuracy if a navigator could refer to a clock that lost or gained less than about 10 seconds per day. The marine chronometer would have to keep the time of a fixed location—usually Greenwich Mean Time—allowing seafarers to determine longitude by comparing the local high noon to the clock. This clock could not contain a pendulum, which would be virtually useless on a rocking ship.\nAfter the Scilly naval disaster of 1707 where four ships ran aground due to navigational mistakes, the British government offered a large prize of £20,000, equivalent to millions of pounds today, for anyone who could determine longitude accurately. The reward was eventually claimed in 1761 by Yorkshire carpenter John Harrison, who dedicated his life to improving the accuracy of his clocks.\n\nIn 1735 Harrison built his first chronometer, which he steadily improved on over the next thirty years before submitting it for examination. The clock had many innovations, including the use of bearings to reduce friction, weighted balances to compensate for the ship's pitch and roll in the sea and the use of two different metals to reduce the problem of expansion from heat.\n\nThe chronometer was trialled in 1761 by Harrison's son and by the end of 10 weeks the clock was in error by less than 5 seconds.\n\nIn 1815, Sir Francis Ronalds (1788-1873) of London published the forerunner of the electric clock, the electrostatic clock. It was powered with dry piles, a high voltage battery with extremely long life but the disadvantage of its electrical properties varying with the weather. He trialled various means of regulating the electricity and these models proved to be reliable across a range of meteorological conditions.\n\nAlexander Bain, a Scottish clock and instrument maker, was the first to invent and patent the electric clock in 1840. On January 11, 1841, Alexander Bain along with John Barwise, a chronometer maker, took out another important patent describing a clock in which an electromagnetic pendulum and an electric current is employed to keep the clock going instead of springs or weights. Later patents expanded on his original ideas.\n\nThe piezoelectric properties of crystalline quartz were discovered by Jacques and Pierre Curie in 1880. The first quartz crystal oscillator was built by Walter G. Cady in 1921, and in 1927 the first quartz clock was built by Warren Marrison and J. W. Horton at Bell Telephone Laboratories in Canada. The following decades saw the development of quartz clocks as precision time measurement devices in laboratory settings—the bulky and delicate counting electronics, built with vacuum tubes, limited their practical use elsewhere. In 1932, a quartz clock able to measure small weekly variations in the rotation rate of the Earth was developed. The National Bureau of Standards (now NIST) based the time standard of the United States on quartz clocks from late 1929 until the 1960s, when it changed to atomic clocks. In 1969, Seiko produced the world's first quartz wristwatch, the Astron. Their inherent accuracy and low cost of production has resulted in the subsequent proliferation of quartz clocks and watches.\n\nAtomic clocks are the most accurate timekeeping devices in practical use today. Accurate to within a few seconds over many thousands of years, they are used to calibrate other clocks and timekeeping instruments.\n\nThe idea of using atomic transitions to measure time was first suggested by Lord Kelvin in 1879, although it was only in the 1930s with the development of Magnetic resonance that there was a practical method for doing this. A prototype ammonia maser device was built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). Although it was less accurate than existing quartz clocks, it served to demonstrate the concept.\nThe first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale \"ephemeris time\" (ET).\n\nThe International System of Units standardized its unit of time, the second, on the properties of cesium in 1967. SI defines the second as 9,192,631,770 cycles of the radiation which corresponds to the transition between two electron spin energy levels of the ground state of the Cs atom. The cesium atomic clock, maintained by the National Institute of Standards and Technology, is accurate to 30 billionths of a second per year. Atomic clocks have employed other elements, such as hydrogen and rubidium vapor, offering greater stability—in the case of hydrogen clocks—and smaller size, lower power consumption, and thus lower cost (in the case of rubidium clocks).\n\nThe first professional clockmakers came from the guilds of locksmiths and jewellers. Clockmaking developed from a specialized craft into a mass production industry over many years.\n\nParis and Blois were the early centres of clockmaking in France. French clockmakers such as Julien Le Roy, clockmaker of Versailles, were leaders in case design and ornamental clocks. Le Roy belonged to the fifth generation of a family of clockmakers, and was described by his contemporaries as \"the most skillful clockmaker in France, possibly in Europe\". He invented a special repeating mechanism which improved the precision of clocks and watches, a face that could be opened to view the inside clockwork, and made or supervised over 3,500 watches. The competition and scientific rivalry resulting from his discoveries further encouraged researchers to seek new methods of measuring time more accurately.\n\nBetween 1794 and 1795, in the aftermath of the French Revolution, the French government briefly mandated decimal clocks, with a day divided into 10 hours of 100 minutes each. The astronomer and mathematician Pierre-Simon Laplace, among other individuals, modified the dial of his pocket watch to decimal time. A clock in the Palais des Tuileries kept decimal time as late as 1801, but the cost of replacing all the nation's clocks prevented decimal clocks from becoming widespread. Because decimalized clocks only helped astronomers rather than ordinary citizens, it was one of the most unpopular changes associated with the metric system, and it was abandoned.\n\nIn Germany, Nuremberg and Augsburg were the early clockmaking centers, and the Black Forest came to specialize in wooden cuckoo clocks.\nThe English became the predominant clockmakers of the 17th and 18th centuries. The main centres of the British industry were in the City of London, the West End of London, Soho where many skilled French Huguenots settled and later in Clerkenwell. The Worshipful Company of Clockmakers was established in 1631 as one of the Livery Companies of the City of London.\n\nThomas Tompion was the first English clockmaker with an international reputation and many of his pupils went on to become great horologists in their own right, such as George Graham who invented the deadbeat escapement, orrery and mercury pendulum, and his pupil Thomas Mudge who created the first lever escapement. Famous clockmakers of this period included Joseph Windmills, Simon de Charmes who established the De Charmes clockmaker firm and Christopher Pinchbeck who invented the alloy pinchbeck.\n\nLater famous horologists included John Arnold who made the first practical and accurate modern watch by refining Harrison's chronometer, Thomas Earnshaw who was the first to make these available to the public, Daniel Quare, who invented a repeating watch movement, a portable barometer and introduced the concentric minute hand.\n\nQuality control and standards were imposed on clockmakers by the Worshipful Company of Clockmakers, a guild which licensed clockmakers for doing business. By the rise of consumerism in the late 18th century, clocks, especially pocket watches, became regarded as fashion accessories and were made in increasingly decorative styles. By 1796, the industry reached a high point with almost 200,000 clocks being produced annually in London, however by the mid-19th century the industry had gone into steep decline from Swiss competition.\n\nSwitzerland established itself as a clockmaking center following the influx of Huguenot craftsmen, and in the 19th century, the Swiss industry \"gained worldwide supremacy in high-quality machine-made watches\". The leading firm of the day was Patek Philippe, founded by Antoni Patek of Warsaw and Adrien Philippe of Bern.\n\n\n\n\n"}
{"id": "945778", "url": "https://en.wikipedia.org/wiki?curid=945778", "title": "Hsinchu Science Park", "text": "Hsinchu Science Park\n\nThe Hsinchu Science Park () is an industrial park established by the government of the Republic of China (Taiwan) on December 15, 1980. It straddles Hsinchu City and Hsinchu County in Taiwan.\n\nThe idea of the establishment of the Hsinchu Science Park was first proposed by Shu Shien-Siu, the former President of National Tsing Hua University and Minister of Science and Technology. After Shu became the Minister of Science and Technology in 1973, he traveled to the United States, Europe, Japan, and South Korea to learn and study their conditions of the development of science and technology. In 1976, Shu came up with the idea of building a science and technology park like that of Silicon Valley. President Chiang Ching-kuo proposed to build the park in Longtan District because of the potential future benefits that could be drawn from National Chung-Shan Institute of Science and Technology and the military. However, Shu argued that the technology and science park should not be close to the military as the primary goal of the founding of the park is to expand the size of private economy and creative vitality of Taiwan. Shu's idea was to build the park in Hsinchu next to the National Tsing Hua University and National Chiao Tung University like the Silicon Valley, which is adjacent to Stanford University and University of California, Berkeley. Shu's idea was ultimately approved by Chiang and the park was built and opened in 1980 in Hsinchu.\n\nAfter the original idea of the establishment of the science park and the location of the park were settled, Chiang Ching-kuo assigned the task of constructing the Hsinchu Science Park. Kwoh-Ting Li, former Finance Minister of the Republic of China, was among those who significantly contributed to the founding of the park, as ordered by Chiang. Inspired by Silicon Valley, Li consulted Frederick Terman on how Taiwan could follow its example. From there, Li convinced talents who had gone abroad to build companies in this new Silicon Valley in Taiwan. Among those who returned is Morris Chang, who later led the Industrial Technology Research Institute (ITRI) and founded the TSMC. Li also introduced the concept of venture capital to the country to attract funds to finance high-tech startups in Taiwan.\n\nThe Hsinchu Science Park (HSP) is now one of the world's most significant centers for semiconductor manufacturing, industrial and computer technology development. More than 400 high-tech companies, mainly involved in the semiconductor, computer, telecommunication, and optoelectronics industries, have been established in the park since the end of December 2003. Its 400 technology companies accounted for 10% of Taiwan's gross domestic product in 2007. It is home to the world's top two semiconductor foundries, Taiwan Semiconductor Manufacturing Company (TSMC) and United Microelectronics Corporation (UMC), both of which were established at the nearby Industrial Technology Research Institute. Taiwan is the only country that possesses a professional division-of-labor system in the semiconductor industry and also has the highest density of 12-inch wafer-producing fabs, most of which are based in the park.\nNext door to the science park are two of Taiwan's science and engineering powerhouses, National Chiao Tung University and National Tsing Hua University, and the National Space Organization, the Taiwanese space agency, is located in the park. There is also a science-themed amusement park nearby.\n\nThere were local residents' protests against water and air pollution. The Park's industrial wastewater treatment plant began to operate in 1986 and effectively treats wastewater for maximum safety while Taiwan's National Environmental Protection Department monitors the air quality in the Park and surrounding areas to maintain clean air quality.\n\nCurrently, the Hsinchu Science Park covers six locations:\n\n\n"}
{"id": "32261703", "url": "https://en.wikipedia.org/wiki?curid=32261703", "title": "Ip.access", "text": "Ip.access\n\nip.access Limited is a multinational corporation that designs, manufactures, and markets small cells (picocell and femtocell) technologies and infrastructure equipment for GSM, GPRS, EDGE, 3G and LTE. The firm has headquarters in Cambourne, England, the company also maintains operations and offices in Bellevue, United States, and Gurgaon and Pune, India.\n\nip.access combines IP and cellular technologies together to provide 2G, 3G and LTE coverage and for mobile networks .Using satellite backhaul, its products provide coverage to commercial passenger aircraft, ships, and users in remote rural areas.\n\nThe firm is a member of 3GPP, the Cambridge Network, European Telecommunications Standards Institute (ETSI), Small Cell Forum, and an associate member of the Network Vendors Interoperability Testing (NVIOT) Forum.\n\nip.access was founded in December 1999 as a wholly owned subsidiary of TTP Group PLC aimed at developing technologies that would allow multiple radio access technologies to communicate over the Internet. To accommodate its growing staff, in 2006 ip.access relocated to new offices in Cambourne Business Park, Cambridge, where it remains.\n\nIn October 2000, TTP Group spun off its communications division (TTP Communications, or TTPCom) in an initial public offering on the London Stock Exchange, and ip.access joined the spin-off as a wholly owned subsidiary of the TTPCom group.\n\nIn March 2006, the company secured an £8.5 million round of funding from Intel Capital, Scottish Equity Partners, and Rothschild & Cie Banque. As part of its June 2006 acquisition of TTP Communications, Motorola also gained a stake in ip.access. In 2007, after signing an OEM agreement with ip.access, ADC (now part of Tyco Electronics) made a minority interest investment in the company. Followed by, both Cisco Systems and Qualcomm making strategic financial investments in the company in 2008.\n\nIn July 2007, the firm became a founding member of the Femto Forum, renamed Small Cell Forum in February 2012. ip.access was named in The Sunday Times Fast Tech Track 100 in both 2007 and 2008. The company was also cited as the number one picocell vendor by global market intelligence company, ABI Research in 2008.\n\nIn 2009, ip.access was named in the Deloitte Technology Fast 500 EMEA. In April 2009, the company announced its Oyster 3G product would support femtocell standards published by 3GPP and the Broadband Forum. In March 2010, the company took part in the first Plugfest, organized by ETSI as part of its Plugtests program, held to demonstrate the effectiveness of the 3GPP femtocell standards in supporting interoperability between femtocell access points and network equipment from different vendors.\n\nIn June 2011, the market research and analysis firm Infonetics named ip.access along with its partner Cisco Systems, as the leading supplier of 3G femtocells. In August 2011, ip.access announced it had made more than 500,000 installations of its 3G technologies. In February 2013, ip.access announced it had become the first 3G small cell provider to ship one million residential units. In the same month, ip.access and iDirect completed successful interopability test of 3G small cells over IP Satellite.\n\nIn February 2014, ip.access launched a new range of small cells called presenceCell, which unlike traditional small cells, do not rely on providing indoor coverage and capacity to deliver a return on investment. Rather, the ultra-compact base stations are designed to capture anonymous user location and phone identity information from smartphones, which can be analysed and packaged as a service for a variety of businesses.\n\nThe telecommunications firms AT&T uses Oyster 3G as the core femtocell technology for its 3G MicroCell product. Cisco Systems, has jointly developed a femtocell solution with ip.access in compliance with the Broadband Forum's TR-069 technical specification.\n\nIn 2002, ip.access introduced the world’s first IP basestation controller for indoor GSM networks. nanoGSM uses 2G picocells that leverage the standard GSM air interface, full IP-based BSC, and an OMC-R management system that delivers voice, messaging and data to both 2G and 3G handsets at an indoor range of up to 200m.\n\nnano3G is an end-to-end Femtocell system with access points for Enterprise, E-class [E8, E16 and E24] and Small Medium Business, S-class [S8], access controller and element management system, providing carrier-class coverage to commercial and consumer users.\n\nLaunched at the 2007 3GSM World Congress in Barcelona, Spain, the Oyster 3G is ip.access' core 3G femtocell technology used by system integrators and OEM customers to integrate WCDMA femtocells into home gateways, set-top boxes, and other devices. ip.access' Oyster 3G is the core technology of AT&T's 3G MicroCell\n\nnanoLTE [E-40, E-100] is an Enterprise grade platform that brings LTE capacity both in-doors and in public spaces, while also offering the option of providing extra 3G infill and Circuit Switch Fall Back (CSFB) capacity.\n\nLaunched in 2014, the presenceCell is a new range of small cell, designed to capture precise user location data via their smart phone, which can be analysed and packaged as a service for a variety of businesses.\nIn addition to the presenceCell, ip.access also provides the back-end processing and management system that delivers the Presence data anonymously and securely to vertical application providers. The company’s Network Orchestration System serves as the infrastructure management solution and also supports the GSMA’s OneAPI standard, which allows third parties to provide value-added services through web friendly message interfaces. The presenceCell was commercially deployed by Vodafone Turkey in 2015.\n\nAmong ip.access' major customers are AT&T, Bharti Airtel, Blue Ocean Wireless, Bouygues Telecom,Jersey Telecom Monaco Telecom, SFR, SPIE SA, T-Mobile, Tele2, Telefónica O2 Czech Republic,Telia Sonera, Vivacom and Vodafone\n\nThe company's technology partners include AeroMobile, Altobridge, Blue Ocean Wireless, Cisco Systems, Private Mobile Networks, Qualcomm, Quortus, Setcom, and TriaGnoSys.\n\nCorporate, product, and personnel awards won by ip.access include the following:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1249209", "url": "https://en.wikipedia.org/wiki?curid=1249209", "title": "JoAnn Hackos", "text": "JoAnn Hackos\n\nJoAnn T. Hackos is a lecturer, consultant and author of a number of books about technical communication. Now retired, Dr. Hackos is the founder of the Center for Information-Development Management (CIDM) and the president emeritus of Comtech Services in Denver, Colorado. She is also a fellow and past president of the Society for Technical Communication. She is a member of the IEEE Standards Association and active in the ISO SC7 Working Groups that is developing standards for information developers. She is the co-author of the standards on content management and information-development management.\n\nHackos is considered an expert in the fields of content management and information design, with some of her work in the fields being described as \"groundbreaking.\"\n\nHackos was also an early and vocal advocate of the single source publishing idea. She was instrumental in founding the OASIS DITA Standards Committee.\n\n\n\n"}
{"id": "7983699", "url": "https://en.wikipedia.org/wiki?curid=7983699", "title": "Knowledge divide", "text": "Knowledge divide\n\nThe knowledge divide is the gap in standards of living between those who can find, create, manage, process, and disseminate information or knowledge, and those who are impaired in this process. According to a 2005 UNESCO World Report, the rise in the 21st century of a global information society has resulted in the emergence of knowledge as a valuable resource, increasingly determining who has access to power and profit. The rapid dissemination of information on a potentially global scale as a result of new information media and the globally uneven ability to assimilate knowledge and information has resulted in potentially expanding gaps in knowledge between individuals and nations.\n\nIn the 21st century, the emergence of the knowledge society becomes pervasive. The transformations of world's economy and of each society have a fast pace. Together with information and communication technologies (ICT) these new paradigms have the power to reshape the global economy. In order to keep pace with innovations, to come up with new ideas, people need to produce and manage knowledge. This is why knowledge has become essential for all societies.\n\nAccording to UNESCO and the World Bank, knowledge gaps between nations may occur due to the varying degrees by which individual nations incorporate the following elements:\n\n\nThe information and ICT systems that support knowledge are very important. This is why digitization is viewed closely related to knowledge. Scientists generally agree that there is a digital divide, recently different reports also showed the existence of knowledge divide.\n\nThe creation and effective use of knowledge are increasingly related to the development of an ICT infrastructure. Without ICT, it is impossible to have an infrastructure able to process the huge flow of information required in an advanced economy. In particular, without adequate technical support, it is difficult to develop and use e-learning and electronic documents to overcome time and space constraints.\n\nThe digital divide is, however, but one important part of the larger knowledge divide. As UNESCO states, \"closing the digital divide will not suffice to close the knowledge divide, for access to useful, relevant knowledge is more than simply a matter of infrastructure—it depends on training, cognitive skills and regulatory frameworks geared towards access to contents.\"\n\nIn the book Digital Dead End, Virginia Eubanks criticizes the way that the digital divide is generally thought of as a division between haves and have-nots, where the solution is distribution. This over simplistic depiction obscures the fact that often social and structural inequality is at the root of the divide. According to a study done by Eubanks with women of the YWCA, the women of the community \"insisted that have-nots possess many different kinds of crucial information and skills.\" In other words, it is not simply knowledge of the technology itself that is the issue but the structural system based on perpetuating the status quo in which the haves \"hoard\" knowledge.\n\nFirst, it was noticed that a great difference exists between the North and the South (rich countries vs. poor countries). The development of knowledge depends on spreading Internet and computer technology and also on the development of education in these countries. If a country has attained a higher literacy level then this will result in having higher level of knowledge.\nIndeed, UNESCO's report details many social issues in knowledge divide related to globalization. There was noticed a knowledge divide with respect to\n\n\n\n"}
{"id": "46536830", "url": "https://en.wikipedia.org/wiki?curid=46536830", "title": "Lexoo", "text": "Lexoo\n\nLexoo is a UK-based legal technology company launched in June 2014 with headquarters in London, United Kingdom. Lexoo provides a lawyer-matching online marketplace, enabling businesses to find a lawyer by providing multiple quotes from specialised solicitors.\n\nLexoo was founded in June 2014 by Daniel van Binsbergen, a former lawyer at De Brauw Blackstone Westbroek, and Chris O'Sullivan, a developer, having raised $400,000 in seed funding from Forward Partners and Jonathan McKay, the Chairman of JustGiving. In November 2015, Lexoo raised a further $1.3 million in funding from a number of investors, including the London Co-Investment Fund, Duncan Jennings (founder of Vouchercodes.co.uk), Tim Jackson of Lean Investments, Robin Grant (founder of We Are Social) and Forward Partners. In October of 2018, Lexoo raised $4.4 million in Series A round led by Earlybird.\nTechCrunch identified that, compared to other markets, the fees charged by legal practitioners are traditionally non transparent, and considered Lexoo “a classic example of how markets can be made more efficient and transparent by moving them online”.\n\nForbes described Lexoo as “the democratisation of legal services”, noting that “the participating solicitors know they are quoting in a competitive environment, so they will offer their best price up front, without businesses having to ask for it. All the ingredients of a classic disruptive start-up.”\n\nIn June 2015, the Financial Times selected Lexoo as its \"Innovation to Watch\" and Startups.co.uk listed Lexoo among the top 100 startups in the UK in 2015.\n\nLexoo has also been featured by PE Hub, Legal Futures, Talk Business Magazine, the Guardian, and Tech City News.\n"}
{"id": "7697949", "url": "https://en.wikipedia.org/wiki?curid=7697949", "title": "List of technology centers", "text": "List of technology centers\n\nThis is a list of technology centers throughout the world. Governmental planners and business networks like to use the name \"silicon\" or \"valley\" to describe their own areas as a result of the success of Silicon Valley in California. Nevertheless, there are a few qualitative differences between these places, and metrics may be applied to measure their dominance.\n\nThese metrics include:\n\n\nCameroon\n\nEgypt\n\nKenya\n\nMauritius\n\nMorocco\n\nSouth Africa\n\nZambia\n\nBolivia\n\nBrazil\n\nCanada\nChile\nGuatemala\n\nMexico\n\nUnited States\n\nChina\n\nHong Kong\n\nIndia\n\nIran\n\n\nIsrael\n\nJapan\n\nMalaysia\n\nMyanmar\n\n\nPakistan\n\nPhilippines\n\nQatar\n\nSaudi Arabia\n\nSingapore\n\nSouth Korea\n\nTaiwan\n\n\nThailand\n\nUnited Arab Emirates\n\nVietnam\n\n\nAustria\n\nBelarus\n\nCzech Republic\n\nFinland\n\nFrance\nGermany\n\nHungary\nIreland\n\nItaly\n\nNetherlands\n\nPortugal\n\nRussia\n\nRomania\n\nSlovakia\n\nSpain\n\nSweden\n\nTurkey\n\nUnited Kingdom\n\n\nUkraine\n\n\nThe following list contains places with \"Silicon\" names, that is, places with nicknames inspired by the \"Silicon Valley\" nickname given to part of the San Francisco Bay Area:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUkraine\n\n\n\n\n\n\n"}
{"id": "20880302", "url": "https://en.wikipedia.org/wiki?curid=20880302", "title": "Ministry of Communication, Science and Technology", "text": "Ministry of Communication, Science and Technology\n\nThe Ministry of Communication, Science and Technology is a Tanzanian government ministry that was established in February 2008. The ministry's roles are policy formulation, monitoring and evaluation, and regulatory and legal matters pertaining to communication, ICT, science, technology, and innovation.\n\nSubsidiary institutions under this ministry include:\n\n"}
{"id": "4355205", "url": "https://en.wikipedia.org/wiki?curid=4355205", "title": "Ministry of Education, Culture and Science (Netherlands)", "text": "Ministry of Education, Culture and Science (Netherlands)\n\nThe Ministry of Education, Culture and Science (; OCW) is the Dutch Ministry responsible for education, culture, science, research, gender equality and communications. The Ministry was created in 1918 as the Ministry of Education, Arts and Sciences and had several name changes before it became the Education, Culture and Science in 1994. The Ministry is headed by the Minister of Education, Culture and Science, currently Ingrid van Engelshoven.\n\nThe mission of the ministry is to \"work for a smart, able and creative Netherlands\". The ministry is responsible for three fields of policy:\n\nThe ministry is currently headed by one minister and one State Secretary. The ministry's main office is located in the Hoftoren, the tallest building of The Hague. The ministry has around 2500 civil servants. The civil service is headed by a secretary general and a deputy secretary general, who head a system of three directorates general:\n\nIt has several autonomous agencies:\n\nThe predecessor of the ministry, the Ministry of Education, Arts and Sciences was founded in 1918, as it became autonomous from the Ministry of the Interior and Kingdom Relations. It was founded as a result of the resolution of the school struggle, the conflict about the equalisation of the finance for religious and public schools. During the German occupation the ministry was renamed Department for Education, Sciences and Cultural Conservation and a separate Department for Propaganda and Arts. In 1965 the department for arts was integrated into the new Ministry of Culture, Recreation and Social Work. In 1982 this cultural department was integrated into the Ministry of Health. In 1996 cultural department returned to the Ministry of Education.\n\n\n"}
{"id": "13682267", "url": "https://en.wikipedia.org/wiki?curid=13682267", "title": "Mobile-to-mobile convergence", "text": "Mobile-to-mobile convergence\n\n\"The term mobile to mobile calling is used in many mobile phone plans to refer to making calls to other mobile phones using the same provider's network--which is often cheaper than other calls.\"\n\nMobile to mobile convergence (MMC) is a term to describe a technology used in modern computing and telephony. The term is an offshoot of fixed mobile convergence (FMC) and uses dual mode (cellular network and WiFi) phones with a special software client and an application server to connect voice calls and business applications via a VoWLAN and/or through a cellular service. \n\nMobile to mobile convergence differs from conventional FMC in that the technology uses the WLAN to route calls via the internet as a primary function, and uses the wireless carrier network if the WLAN is not present as a secondary function. It is significant since it is viewed as a means to compete with carrier companies since the calls are routed around the cellular network. This is viewed as a more efficient use of networking technology than standard FMC solutions that are available as well, since most of the latter use the carrier network as the primary means of communication and do not leverage the lower cost and controls of internet protocol-based networks that are generally installed at most modern businesses. In theory, it also provides the capability of providing a greater voice coverage area than either carrier or WLAN technology alone since some areas do not have cellular service coverage and others do not have WiFi.\n\nThe first offering known in the market successfully deploying MMC is beCherry, which is delivered by Belgian company Mondial Telecom. They offer a MMC solution on Symbian, iOS and Android. Other smartphone OS's are also considered.\n\n"}
{"id": "1903500", "url": "https://en.wikipedia.org/wiki?curid=1903500", "title": "Mobile Station Roaming Number", "text": "Mobile Station Roaming Number\n\nA Mobile Station Roaming Number (MSRN) is an E.164 defined telephone number used to route telephone calls in a mobile network from a GMSC (Gateway Mobile Switching Centre) to the target MSC (see Network Switching Subsystem). It can also be defined as a directory number temporarily assigned to a mobile for a mobile terminated call. A MSRN is assigned for every mobile terminated call, not only the calls where the terminating MS lives on a different MSC than the originating MS. Although this seems unnecessary since many vendors' VLR's are integrated with the MSC, the GSM specification indicates that the MSC and VLR (Visitor Location Register) do not need to reside on the same switch. They are considered two different nodes as they have their own routing addresses. The MSRN is one of the returned parameters into SRI_Response message. In particular the MSRN is used into an MNP scenario (in this case it can be modified as 'RgN + MSISDN'\n"}
{"id": "39207065", "url": "https://en.wikipedia.org/wiki?curid=39207065", "title": "Mobile forms", "text": "Mobile forms\n\nA mobile form is an electronic or digital form application that functions on a smartphone or tablet device. Mobile forms enable users to collect data using mobile devices, and then to send the results back to the source. Mobile forms exist to replace paper forms as a more productive means of data collection, eliminating the need to transcribe or scan paper data results into a back office system.\n\nDepending on the mobile form application provider, some mobile form solutions allow offices to dispatch data to mobile form applications. In addition, other mobile form applications can be connected with various cloud services, servers, and social media platforms.\n\nDepending on the business, the motivating factors to deploying mobile forms may vary. Some businesses implement mobile forms to speed up processes, while others institute mobile forms with field users to reduce costs associated with transporting paper forms back and forth. Furthermore, green-minded businesses implement mobile forms in order to be more environmentally friendly, thus reducing their reliance on paper, ink printing, and subsequent waste.\n\nAdvanced mobile form features include signature capture, bar code capture, photo capture, GPS location form info, time form info, and skip logic.\n\nUses for mobile forms include:\n\n"}
{"id": "8053931", "url": "https://en.wikipedia.org/wiki?curid=8053931", "title": "Mobile ticketing", "text": "Mobile ticketing\n\nMobile ticketing is the process whereby customers can order, pay for, obtain and/or validate tickets using mobile phones. Mobile tickets reduce the production and distribution costs connected with traditional paper-based ticketing channels and increase customer convenience by providing new and simple ways to purchase tickets.\n\nMobile tickets should not be confused with E-Tickets (electronic tickets) which are used by airlines since 1994, they can be sent by e-mail, printed and shown at the check-in desk at the airport to obtain a boarding pass.\n\nMany train and bus operators in Europe have created phone apps in which tickets can be bought and stored. These include but are not limited to SJ, DSB, NSB, DB and selected local transit authorities.\n\nPhilips and Sony developed near field communication (NFC) in 2002. It is build on the same basis as common contactless smartcards. Philips published an early paper on NFC in 2004. In 2004, the NFC Forum was established.\nNFC incorporated in a mobile phone allows all kind of novel contactless applications, mobile ticketing being an important one of them.\nMobile Tickets can be purchased via internet and will be downloaded in a few seconds to the mobile phone, be it in an sms with a 2-D barcode or to the connected NFC chip. In case of NFC at entrance the phone just has to be touched to the scanning device (in fact it makes contact within 10 cm). The GSM Association, GSMA, published a whitepaper on M-Ticketing in 2011.\nIt describes extensively the use and advantages of M-Ticketing, principally the use of NFC technology. They state that NFC is the best technology but \"it is expected however that M-Ticketing services using SMS and Bar Code implementations will be prevalent until the point that a critical mass of NFC enabled handsets is available.\"\n\n"}
{"id": "1544319", "url": "https://en.wikipedia.org/wiki?curid=1544319", "title": "Nanosocialism", "text": "Nanosocialism\n\nNanosocialism refers generally to a set of economic theories of social organization advocating state or collective ownership and administration of the research, development and use of nanotechnology.\n\nNanosocialism is a stance that favors participatory politics to guide state intervention in the effort to manage the transition to a society revolutionized by molecular nanotechnology.\n\n\"Nanosocialism\" is a term coined by David M. Berube, the associate director of Nanoscience and Technology Studies at the USC NanoCenter, who argues that nanotechnological projections need to be tempered by technorealism about the implications of nanotechnology in a technocapitalist society, but that its applications also offer enormous opportunities for economic abundance and social progress.\n\nIn the role-playing game \"Transhuman Space\", nanosocialism is described as a descendant of infosocialism, in which intellectual property is nationalized and freely distributed by the state. It is adopted by some developing nations to counter the hold corporations from wealthier nations have on copyrights and patents. This fictional version of nanosocialism was coined by David L. Pulver, the game's creator, who was unaware that the term had already been used by Berube.\n\n"}
{"id": "639115", "url": "https://en.wikipedia.org/wiki?curid=639115", "title": "Neolithic Revolution", "text": "Neolithic Revolution\n\nThe Neolithic Revolution, Neolithic Demographic Transition, Agricultural Revolution, or First Agricultural Revolution was the wide-scale transition of many human cultures during the Neolithic period from a lifestyle of hunting and gathering to one of agriculture and settlement, making an increasingly larger population possible. These settled communities permitted humans to observe and experiment with plants to learn how they grew and developed. This new knowledge led to the domestication of plants.\n\nArchaeological data indicates that the domestication of various types of plants and animals happened in separate locations worldwide, starting in the geological epoch of the Holocene around 12,500 years ago. It was the world's first historically verifiable revolution in agriculture. The Neolithic Revolution greatly narrowed the diversity of foods available, resulting in a downturn in human nutrition.\n\nThe Neolithic Revolution involved far more than the adoption of a limited set of food-producing techniques. During the next millennia it would transform the small and mobile groups of hunter-gatherers that had hitherto dominated human pre-history into sedentary (non-nomadic) societies based in built-up villages and towns. These societies radically modified their natural environment by means of specialized food-crop cultivation, with activities such as irrigation and deforestation which allowed the production of surplus food. Other developments found very widely are the domestication of animals, pottery, polished stone tools, and rectangular houses.\n\nThese developments, sometimes called the Neolithic package, provided the basis for centralized administrations and political structures, hierarchical ideologies, depersonalized systems of knowledge (e.g. writing), densely populated settlements, specialization and division of labour, more trade, the development of non-portable art and architecture, and property ownership. The earliest known civilization developed in Sumer in southern Mesopotamia (); its emergence also heralded the beginning of the Bronze Age.\n\nThe relationship of the above-mentioned Neolithic characteristics to the onset of agriculture, their sequence of emergence, and empirical relation to each other at various Neolithic sites remains the subject of academic debate, and varies from place to place, rather than being the outcome of universal laws of social evolution. The Levant saw the earliest developments of the Neolithic Revolution from around 10,000 BCE, followed by sites in the wider Fertile Crescent.\n\nThe term \"Neolithic Revolution\" was coined in 1923 by V. Gordon Childe to describe the first in a series of agricultural revolutions in Middle Eastern history. The period is described as a \"revolution\" to denote its importance, and the great significance and degree of change affecting the communities in which new agricultural practices were gradually adopted and refined.\n\nThe beginning of this process in different regions has been dated from 10,000 to 8,000 BC in the Fertile Crescent and perhaps 8000 BC in the Kuk Early Agricultural Site of Melanesia. This transition everywhere seems associated with a change from a largely nomadic hunter-gatherer way of life to a more settled, agrarian-based one, with the inception of the domestication of various plant and animal species—depending on the species locally available, and probably also influenced by local culture. Recent archaeological research suggests that in some regions such as the Southeast Asian peninsula, the transition from hunter-gatherer to agriculturalist was not linear, but region-specific.\n\nThere are several competing (but not mutually exclusive) theories as to the factors that drove populations to take up agriculture. The most prominent of these are:\n\nOnce agriculture started gaining momentum, around 9000 BC, human activity resulted in the selective breeding of cereal grasses (beginning with emmer, einkorn and barley), and not simply of those that would favour greater caloric returns through larger seeds. Plants with traits such as small seeds or bitter taste would have been seen as undesirable. Plants that rapidly shed their seeds on maturity tended not to be gathered at harvest, therefore not stored and not seeded the following season; years of harvesting selected for strains that retained their edible seeds longer.\n\nSeveral plant species, the \"pioneer crops\" or Neolithic founder crops, were identified by Daniel Zohary, who highlighted the importance of the three cereals, and suggested that domestication of flax, peas, chickpeas, bitter vetch and lentils came a little later. Based on analysis of the genes of domesticated plants, he preferred theories of a single, or at most a very small number of domestication events for each taxon that spread in an arc from the Levantine corridor around the Fertile Crescent and later into Europe. Gordon Hillman and Stuart Davies carried out experiments with wild wheat varieties to show that the process of domestication would have occurred over a relatively short period of between 20 and 200 years. Some of these pioneering attempts failed at first and crops were abandoned, sometimes to be taken up again and successfully domesticated thousands of years later: rye, tried and abandoned in Neolithic Anatolia, made its way to Europe as weed seeds and was successfully domesticated in Europe, thousands of years after the earliest agriculture. Wild lentils presented a different problem: most of the wild seeds do not germinate in the first year; the first evidence of lentil domestication, breaking dormancy in their first year, was found in the early Neolithic at Jerf el Ahmar (in modern Syria), and quickly spread south to the Netiv HaGdud site in the Jordan Valley. This process of domestication allowed the founder crops to adapt and eventually become larger, more easily harvested, more dependable in storage and more useful to the human population.\n\nSelectively propagated figs, wild barley and wild oats were cultivated at the early Neolithic site of Gilgal I, where in 2006 archaeologists found caches of seeds of each in quantities too large to be accounted for even by intensive gathering, at strata datable to c. 11,000 years ago. Some of the plants tried and then abandoned during the Neolithic period in the Ancient Near East, at sites like Gilgal, were later successfully domesticated in other parts of the world.\n\nOnce early farmers perfected their agricultural techniques like irrigation, their crops would yield surpluses that needed storage. Most hunter gatherers could not easily store food for long due to their migratory lifestyle, whereas those with a sedentary dwelling could store their surplus grain. Eventually granaries were developed that allowed villages to store their seeds longer. So with more food, the population expanded and communities developed specialized workers and more advanced tools.\n\nThe process was not as linear as was once thought, but a more complicated effort, which was undertaken by different human populations in different regions in many different ways.\n\nEarly agriculture is believed to have originated and become widespread in Southwest Asia around 10,000–9,000 BP, though earlier individual sites have been identified. The Fertile Crescent region of Southwest Asia is the centre of domestication for three cereals (einkorn wheat, emmer wheat and barley), four legumes (lentil, pea, bitter vetch and chickpea), and flax. Domestication was a slow process involving multiple sites for each crop.\n\nFinds of large quantities of seeds and a grinding stone at the paleolithic site of Ohalo II in the vicinity of the Sea of Galilee, dated to around 19,400 BP, has shown some of the earliest evidence for advanced planning of plant food consumption and suggests that humans at Ohalo II processed the grain before consumption. Tell Aswad is the oldest site of agriculture, with domesticated emmer wheat dated to 10,800 BP. Soon after came hulled, two-row barley found domesticated earliest at Jericho in the Jordan valley and Iraq ed-Dubb in Jordan. Other sites in the Levantine corridor that show the first evidence of agriculture include Wadi Faynan 16 and Netiv Hagdud. Jacques Cauvin noted that the settlers of Aswad did not domesticate on site, but \"arrived, perhaps from the neighbouring Anti-Lebanon, already equipped with the seed for planting\". In the Eastern Fertile Crescent, evidence of cultivation of wild plants has been found in Choga Gholan in Iran dated to 12,000 BP, suggesting there were multiple regions in the Fertile Crescent where domestication evolved roughly contemporaneously. The Heavy Neolithic Qaraoun culture has been identified at around fifty sites in Lebanon around the source springs of the River Jordan, but never reliably dated.\n\nNorthern China appears to have been the domestication center for foxtail millet (\"Setaria italica\") and broomcorn millet (\"Panicum miliaceum\") with evidence of domestication of these species approximately 8,000 years ago.<ref name=\"doi10.1093/aob/mcm048\"></ref> These species were subsequently widely cultivated in the Yellow River basin (7,500 years ago). Rice was domesticated in southern China later on. Soybean was domesticated in northern China 4,500 years ago. Orange and peach also originated in China. They were cultivated around 2500 BC.\nOn the African continent, three areas have been identified as independently developing agriculture: the Ethiopian highlands, the Sahel and West Africa. By contrast, Agriculture in the Nile River Valley is thought to have developed from the original Neolithic Revolution in the Fertile Crescent. \nMany grinding stones are found with the early Egyptian Sebilian and Mechian cultures and evidence has been found of a neolithic domesticated crop-based economy dating around 7,000 BP.\nUnlike the Middle East, this evidence appears as a \"false dawn\" to agriculture, as the sites were later abandoned, and permanent farming then was delayed until 6,500 BP with the Tasian and Badarian cultures and the arrival of crops and animals from the Near East.\n\nBananas and plantains, which were first domesticated in Southeast Asia, most likely Papua New Guinea, were re-domesticated in Africa possibly as early as 5,000 years ago. Asian yams and taro were also cultivated in Africa.\n\nThe most famous crop domesticated in the Ethiopian highlands is coffee. In addition, khat, ensete, noog, teff and finger millet were also domesticated in the Ethiopian highlands. Crops domesticated in the Sahel region include sorghum and pearl millet. The kola nut was first domesticated in West Africa. Other crops domesticated in West Africa include African rice, yams and the oil palm.\n\nAgriculture spread to Central and Southern Africa in the Bantu expansion during the 1st millennium BC to 1st millennium AD.\n\nMaize (corn), beans and squash were among the earliest crops domesticated in Mesoamerica, with maize beginning about 4000 BC, squash as early as 6000 BC, and beans by no later than 4000 BC. Potatoes and manioc were domesticated in South America. In what is now the eastern United States, Native Americans domesticated sunflower, sumpweed and goosefoot around 2500 BC. Sedentary village life based on farming did not develop until the second millennium BC, referred to as the formative period.\n\nEvidence of drainage ditches at Kuk Swamp on the borders of the Western and Southern Highlands of Papua New Guinea shows evidence of the cultivation of taro and a variety of other crops, dating back to 11,000 BP. Two potentially significant economic species, taro (\"Colocasia esculenta\") and yam (\"Dioscorea\" sp.), have been identified dating at least to 10,200 calibrated years before present (cal BP). Further evidence of bananas and sugarcane dates to 6,950 to 6,440 BP. This was at the altitudinal limits of these crops, and it has been suggested that cultivation in more favourable ranges in the lowlands may have been even earlier. CSIRO has found evidence that taro was introduced into the Solomon Islands for human use, from 28,000 years ago, making taro cultivation the earliest crop in the world. It seems to have resulted in the spread of the Trans–New Guinea languages from New Guinea east into the Solomon Islands and west into Timor and adjacent areas of Indonesia. This seems to confirm the theories of Carl Sauer who, in \"Agricultural Origins and Dispersals\", suggested as early as 1952 that this region was a centre of early agriculture.\n\nWhen hunter-gathering began to be replaced by sedentary food production it became more profitable to keep animals close at hand. Therefore, it became necessary to bring animals permanently to their settlements, although in many cases there was a distinction between relatively sedentary farmers and nomadic herders. The animals' size, temperament, diet, mating patterns, and life span were factors in the desire and success in domesticating animals. Animals that provided milk, such as cows and goats, offered a source of protein that was renewable and therefore quite valuable. The animal’s ability as a worker (for example ploughing or towing), as well as a food source, also had to be taken into account. Besides being a direct source of food, certain animals could provide leather, wool, hides, and fertilizer. Some of the earliest domesticated animals included dogs (East Asia, about 15,000 years ago), sheep, goats, cows, and pigs.\n\nThe Middle East served as the source for many animals that could be domesticated, such as sheep, goats and pigs. This area was also the first region to domesticate the dromedary. Henri Fleisch discovered and termed the Shepherd Neolithic flint industry from the Bekaa Valley in Lebanon and suggested that it could have been used by the earliest nomadic shepherds. He dated this industry to the Epipaleolithic or Pre-Pottery Neolithic as it is evidently not Paleolithic, Mesolithic or even Pottery Neolithic. The presence of these animals gave the region a large advantage in cultural and economic development. As the climate in the Middle East changed and became drier, many of the farmers were forced to leave, taking their domesticated animals with them. It was this massive emigration from the Middle East that would later help distribute these animals to the rest of Afroeurasia. This emigration was mainly on an east-west axis of similar climates, as crops usually have a narrow optimal climatic range outside of which they cannot grow for reasons of light or rain changes. For instance, wheat does not normally grow in tropical climates, just like tropical crops such as bananas do not grow in colder climates. Some authors, like Jared Diamond, have postulated that this East-West axis is the main reason why plant and animal domestication spread so quickly from the Fertile Crescent to the rest of Eurasia and North Africa, while it did not reach through the North-South axis of Africa to reach the Mediterranean climates of South Africa, where temperate crops were successfully imported by ships in the last 500 years. Similarly, the African Zebu of central Africa and the domesticated bovines of the fertile-crescent — separated by the dry sahara desert — were not introduced into each other's region.\n\nDespite the significant technological advance, the Neolithic revolution did not lead immediately to a rapid growth of population. Its benefits appear to have been offset by various adverse effects,\nmostly diseases and warfare.\n\nThe introduction of agriculture has not necessarily led to unequivocal progress. The nutritional standards of the growing Neolithic populations were inferior to that of hunter-gatherers. Several ethnological and archaeological studies conclude that the transition to cereal-based diets caused a reduction in life expectancy and stature, an increase in infant mortality and infectious diseases, the development of chronic, inflammatory or degenerative diseases (such as obesity, type 2 diabetes and cardiovascular diseases) and multiple nutritional deficiencies, including vitamin deficiencies, iron deficiency anemia and mineral disorders affecting bones (such as osteoporosis and rickets) and teeth. Average height went down from 5'10\" (178 cm) for men and 5'6\" (168 cm) for women to 5'5\" (165 cm) and 5'1\" (155 cm), respectively, and it took until the twentieth century for average human height to come back to the pre-Neolithic Revolution levels.\n\nThe traditional view is that agricultural food production supported a denser population, which in turn supported larger sedentary communities, the accumulation of goods and tools, and specialization in diverse forms of new labor. The development of larger societies led to the development of different means of decision making and to governmental organization. Food surpluses made possible the development of a social elite who were not otherwise engaged in agriculture, industry or commerce, but dominated their communities by other means and monopolized decision-making. Jared Diamond (in The World Until Yesterday) identifies the availability of milk and cereal grains as permitting mothers to raise both an older (e.g. 3 or 4 year old) and a younger child concurrently. The result is that a population can increase more rapidly. Diamond, in agreement with feminist scholars such as V. Spike Peterson, points out that agriculture brought about deep social divisions and encouraged gender inequality. \n\nAndrew Sherratt has argued that following upon the Neolithic Revolution was a second phase of discovery that he refers to as the secondary products revolution. Animals, it appears, were first domesticated purely as a source of meat. The Secondary Products Revolution occurred when it was recognised that animals also provided a number of other useful products. These included:\n\nSherratt argued that this phase in agricultural development enabled humans to make use of the energy possibilities of their animals in new ways, and permitted permanent intensive subsistence farming and crop production, and the opening up of heavier soils for farming. It also made possible nomadic pastoralism in semi arid areas, along the margins of deserts, and eventually led to the domestication of both the dromedary and Bactrian camel. Overgrazing of these areas, particularly by herds of goats, greatly extended the areal extent of deserts.\n\nLiving in one spot would have more easily permitted the accrual of personal possessions and an attachment to certain areas of land. From such a position, it is argued, prehistoric people were able to stockpile food to survive lean times and trade unwanted surpluses with others. Once trade and a secure food supply were established, populations could grow, and society would have diversified into food producers and artisans, who could afford to develop their trade by virtue of the free time they enjoyed because of a surplus of food. The artisans, in turn, were able to develop technology such as metal weapons. Such relative complexity would have required some form of social organisation to work efficiently, so it is likely that populations that had such organisation, perhaps such as that provided by religion, were better prepared and more successful. In addition, the denser populations could form and support legions of professional soldiers. Also, during this time property ownership became increasingly important to all people. Ultimately, Childe argued that this growing social complexity, all rooted in the original decision to settle, led to a second Urban Revolution in which the first cities were built.\n\nThroughout the development of sedentary societies, disease spread more rapidly than it had during the time in which hunter-gatherer societies existed. Inadequate sanitary practices and the domestication of animals may explain the rise in deaths and sickness following the Neolithic Revolution, as diseases jumped from the animal to the human population. Some examples of infectious diseases spread from animals to humans are influenza, smallpox, and measles. In concordance with a process of natural selection, the humans who first domesticated the big mammals quickly built up immunities to the diseases as within each generation the individuals with better immunities had better chances of survival. In their approximately 10,000 years of shared proximity with animals, such as cows, Eurasians and Africans became more resistant to those diseases compared with the indigenous populations encountered outside Eurasia and Africa. For instance, the population of most Caribbean and several Pacific Islands have been completely wiped out by diseases. 90% or more of many populations of the Americas were wiped out by European and African diseases before recorded contact with European explorers or colonists. Some cultures like the Inca Empire did have a large domestic mammal, the llama, but llama milk was not drunk, nor did llamas live in a closed space with humans, so the risk of contagion was limited. According to bioarchaeological research, the effects of agriculture on physical and dental health in Southeast Asian rice farming societies from 4000 to 1500 B.P. was not detrimental to the same extent as in other world regions.\n\nIn his book \"Guns, Germs, and Steel\", Jared Diamond argues that Europeans and East Asians benefited from an advantageous geographical location that afforded them a head start in the Neolithic Revolution. Both shared the temperate climate ideal for the first agricultural settings, both were near a number of easily domesticable plant and animal species, and both were safer from attacks of other people than civilizations in the middle part of the Eurasian continent. Being among the first to adopt agriculture and sedentary lifestyles, and neighboring other early agricultural societies with whom they could compete and trade, both Europeans and East Asians were also among the first to benefit from technologies such as firearms and steel swords.\nThe dispersal of Neolithic culture from the Middle East has recently been associated with the distribution of human genetic markers. In Europe, the spread of the Neolithic culture has been associated with distribution of the E1b1b lineages and Haplogroup J that are thought to have arrived in Europe from North Africa and the Near East respectively. In Africa, the spread of farming, and notably the Bantu expansion, is associated with the dispersal of Y-chromosome haplogroup E1b1a from West Africa.\n\n\n\n"}
{"id": "9987", "url": "https://en.wikipedia.org/wiki?curid=9987", "title": "Outline of engineering", "text": "Outline of engineering\n\nThe following outline is provided as an overview of and topical guide to engineering:\n\nEngineering is the discipline and profession that applies scientific theories, mathematical methods, and empirical evidence to design, create, and analyze technological solutions cognizant of safety, human factors, physical laws, regulations, practicality, and cost.\n\n\nHistory of engineering\n\n\n\n\n\n\n\n"}
{"id": "17121771", "url": "https://en.wikipedia.org/wiki?curid=17121771", "title": "Peter A. Sturgeon", "text": "Peter A. Sturgeon\n\nPeter Assheton Sturgeon (November 22, 1916 – July 22, 2005) was founder of the American branch of Mensa and the older brother of noted American science fiction writer Theodore Sturgeon. \n\nThe two brothers were the sons of Edward Molineaux Waldo, a Staten Island paint manufacturer, and Christine Hamilton Dicker, a British writer and political activist. Their parents divorced when they were children and in 1927 their mother married William Dickie Sturgeon, an emigrant Scottish college professor. Christine and her children relocated to Philadelphia where Peter and his brother Ted were educated in public schools.\n\nAfter high school Peter joined the Communist Party. After working for a time as a party activist in a steel industry organizing campaign in Baltimore, he went to Spain and fought on the Republican side in the Spanish Civil War as a volunteer with the British Battalion of the International Brigades. The writer William Tenn has stated that Sturgeon became involved with POUM while in Spain and fell into disfavor with his superiors. After returning to the United States Sturgeon resigned from the Communist Party and associated with the Trotskyist Socialist Workers Party. In 1941, he was drafted into the Army, serving as a combat paratrooper in the 517th Parachute Regimental Combat Team until his discharge in November 1945. After the war he earned a BS degree at New York University. He settled in Brooklyn with his wife Ines, working as a medical writer and writing technical material for the pharmaceutical industry. He founded the first American chapter of Mensa in New York in 1960, holding early meetings at his Brooklyn apartment. In 1965 he left the United States, taking a job with the World Health Organization in Switzerland. In 1968 he relocated to Vienna, Austria where he worked for the United Nations Industrial Development Organization. He died in Vienna in 2005.\n\nhttp://www.517prct.org\n"}
{"id": "2958015", "url": "https://en.wikipedia.org/wiki?curid=2958015", "title": "Philosophy of artificial intelligence", "text": "Philosophy of artificial intelligence\n\nArtificial intelligence has close connections with philosophy because both share several concepts and these include intelligence, action, consciousness, epistemology, and even free will. Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures) so the discipline is of considerable interest to philosophers. These factors contributed to the emergence of the philosophy of artificial intelligence. Some scholars argue that the AI community's dismissal of philosophy is detrimental.\n\nThe philosophy of artificial intelligence attempts to answer such questions as follows:\n\nThese three questions reflect the divergent interests of AI researchers, linguists, cognitive scientists and philosophers respectively. The scientific answers to these questions depend on the definition of \"intelligence\" and \"consciousness\" and exactly which \"machines\" are under discussion.\n\nImportant propositions in the philosophy of AI include:\n\n\nIs it possible to create a machine that can solve \"all\" the problems humans solve using their intelligence? This question defines the scope of what machines will be able to do in the future and guides the direction of AI research. It only concerns the \"behavior\" of machines and ignores the issues of interest to psychologists, cognitive scientists and philosophers; to answer this question, it does not matter whether a machine is \"really\" thinking (as a person thinks) or is just \"acting like\" it is thinking.\n\nThe basic position of most AI researchers is summed up in this statement, which appeared in the proposal for the Dartmouth workshop of 1956:\nArguments against the basic premise must show that building a working AI system is impossible, because there is some practical limit to the abilities of computers or that there is some special quality of the human mind that is necessary for thinking and yet cannot be duplicated by a machine (or by the methods of current AI research). Arguments in favor of the basic premise must show that such a system is possible.\n\nThe first step to answering the question is to clearly define \"intelligence\".\n\nAlan Turing reduced the problem of defining intelligence to a simple question about conversation. He suggests that: if a machine can answer \"any\" question put to it, using the same words that an ordinary person would, then we may call that machine intelligent. A modern version of his experimental design would use an online chat room, where one of the participants is a real person and one of the participants is a computer program. The program passes the test if no one can tell which of the two participants is human. Turing notes that no one (except philosophers) ever asks the question \"can people think?\" He writes \"instead of arguing continually over this point, it is usual to have a polite convention that everyone thinks\". Turing's test extends this polite convention to machines:\nOne criticism of the Turing test is that it is explicitly anthropomorphic . If our ultimate goal is to create machines that are \"more\" intelligent than people, why should we insist that our machines must closely \"resemble\" people? Russell and Norvig write that \"aeronautical engineering texts do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons'\".\n\nRecent A.I. research defines intelligence in terms of intelligent agents. An \"agent\" is something which perceives and acts in an environment. A \"performance measure\" defines what counts as success for the agent. \nDefinitions like this one try to capture the essence of intelligence. They have the advantage that, unlike the Turing test, they do not also test for human traits that we may not want to consider intelligent, like the ability to be insulted or the temptation to lie . They have the disadvantage that they fail to make the commonsense differentiation between \"things that think\" and \"things that do not\". By this definition, even a thermostat has a rudimentary intelligence.\n\nHubert Dreyfus describes this argument as claiming that \"if the nervous system obeys the laws of physics and chemistry, which we have every reason to suppose it does, then ... we ... ought to be able to reproduce the behavior of the nervous system with some physical device\". This argument, first introduced as early as 1943 and vividly described by Hans Moravec in 1988, is now associated with futurist Ray Kurzweil, who estimates that computer power will be sufficient for a complete brain simulation by the year 2029. A non-real-time simulation of a thalamocortical model that has the size of the human brain (10 neurons) was performed in 2005 and it took 50 days to simulate 1 second of brain dynamics on a cluster of 27 processors.\n\nFew disagree that a brain simulation is possible in theory, even critics of AI such as Hubert Dreyfus and John Searle.\nHowever, Searle points out that, in principle, \"anything\" can be simulated by a computer; thus, bringing the definition to its breaking point leads to the conclusion that any process at all can technically be considered \"computation\". \"What we wanted to know is what distinguishes the mind from thermostats and livers,\" he writes. Thus, merely mimicking the functioning of a brain would in itself be an admission of ignorance regarding intelligence and the nature of the mind .\nIn 1963, Allen Newell and Herbert A. Simon proposed that \"symbol manipulation\" was the essence of both human and machine intelligence. They wrote: \nThis claim is very strong: it implies both that human thinking is a kind of symbol manipulation (because a symbol system is \"necessary\" for intelligence) and that machines can be intelligent (because a symbol system is \"sufficient\" for intelligence). Another version of this position was described by philosopher Hubert Dreyfus, who called it \"the psychological assumption\":\nA distinction is usually made between the kind of high level symbols that directly correspond with objects in the world, such as <nowiki><dog></nowiki> and <nowiki><tail></nowiki> and the more complex \"symbols\" that are present in a machine like a neural network. Early research into AI, called \"good old fashioned artificial intelligence\" (GOFAI) by John Haugeland, focused on these kind of high level symbols.\n\nThese arguments show that human thinking does not consist (solely) of high level symbol manipulation. They do \"not\" show that artificial intelligence is impossible, only that more than symbol processing is required.\n\nIn 1931, Kurt Gödel proved with an incompleteness theorem that it is always possible to construct a \"Gödel statement\" that a given consistent formal system of logic (such as a high-level symbol manipulation program) could not prove. Despite being a true statement, the constructed Gödel statement is unprovable in the given system. (The truth of the constructed Gödel statement is contingent on the consistency of the given system; applying the same process to a subtly inconsistent system will appear to succeed, but will actually yield a false \"Gödel statement\" instead.) More speculatively, Gödel conjectured that the human mind can correctly eventually determine the truth or falsity of any well-grounded mathematical statement (including any possible Gödel statement), and that therefore the human mind's power is not reducible to a \"mechanism\". Philosopher John Lucas (since 1961) and Roger Penrose (since 1989) have championed this philosophical anti-mechanist argument. Gödelian anti-mechanist arguments tend to rely on the innocuous-seeming claim that a system of human mathematicians (or some idealization of human mathematicians) is both consistent (completely free of error) and believes fully in its own consistency (and can make all logical inferences that follow from its own consistency, including belief in its Gödel statement) . This is provably impossible for a Turing machine (and, by an informal extension, any known type of mechanical computer) to do; therefore, the Gödelian concludes that human reasoning is too powerful to be captured in a machine .\n\nHowever, the modern consensus in the scientific and mathematical community is that actual human reasoning is inconsistent; that any consistent \"idealized version\" \"H\" of human reasoning would logically be forced to adopt a healthy but counter-intuitive open-minded skepticism about the consistency of \"H\" (otherwise \"H\" is provably inconsistent); and that Gödel's theorems do not lead to any valid argument that humans have mathematical reasoning capabilities beyond what a machine could ever duplicate. This consensus that Gödelian anti-mechanist arguments are doomed to failure is laid out strongly in \"Artificial Intelligence\": \"\"any\" attempt to utilize (Gödel's incompleteness results) to attack the computationalist thesis is bound to be illegitimate, since these results are quite consistent with the computationalist thesis.\"\n\nMore pragmatically, Russell and Norvig note that Gödel's argument only applies to what can theoretically be proved, given an infinite amount of memory and time. In practice, real machines (including humans) have finite resources and will have difficulty proving many theorems. It is not necessary to prove everything in order to be intelligent .\n\nLess formally, Douglas Hofstadter, in his Pulitzer prize winning book \",\" states that these \"Gödel-statements\" always refer to the system itself, drawing an analogy to the way the Epimenides paradox uses statements that refer to themselves, such as \"this statement is false\" or \"I am lying\". But, of course, the Epimenides paradox applies to anything that makes statements, whether they are machines \"or\" humans, even Lucas himself. Consider:\nThis statement is true but cannot be asserted by Lucas. This shows that Lucas himself is subject to the same limits that he describes for machines, as are all people, and so Lucas's argument is pointless.\n\nAfter concluding that human reasoning is non-computable, Penrose went on to controversially speculate that some kind of hypothetical non-computable processes involving the collapse of quantum mechanical states give humans a special advantage over existing computers. Existing quantum computers are only capable of reducing the complexity of Turing computable tasks and are still restricted to tasks within the scope of Turing machines. . By Penrose and Lucas's arguments, existing quantum computers are not sufficient , so Penrose seeks for some other process involving new physics, for instance quantum gravity which might manifest new physics at the scale of the Planck mass via spontaneous quantum collapse of the wave function. These states, he suggested, occur both within neurons and also spanning more than one neuron. However, other scientists point out that there is no plausible organic mechanism in the brain for harnessing any sort of quantum computation, and furthermore that the timescale of quantum decoherence seems too fast to influence neuron firing.\n\nHubert Dreyfus Hubert Dreyfus's views on artificial intelligence and expertise depended primarily on implicit skill rather than explicit symbolic manipulation, and argued that these skills would never be captured in formal rules.\n\nDreyfus's argument had been anticipated by Turing in his 1950 paper Computing machinery and intelligence, where he had classified this as the \"argument from the informality of behavior.\" Turing argued in response that, just because we do not know the rules that govern a complex behavior, this does not mean that no such rules exist. He wrote: \"we cannot so easily convince ourselves of the absence of complete laws of behaviour ... The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, 'We have searched enough. There are no such laws.'\"\n\nRussell and Norvig point out that, in the years since Dreyfus published his critique, progress has been made towards discovering the \"rules\" that govern unconscious reasoning. The situated movement in robotics research attempts to capture our unconscious skills at perception and attention. Computational intelligence paradigms, such as neural nets, evolutionary algorithms and so on are mostly directed at simulated unconscious reasoning and learning. Statistical approaches to AI can make predictions which approach the accuracy of human intuitive guesses. Research into commonsense knowledge has focused on reproducing the \"background\" or context of knowledge. In fact, AI research in general has moved away from high level symbol manipulation or \"GOFAI\", towards new models that are intended to capture more of our \"unconscious\" reasoning . Historian and AI researcher Daniel Crevier wrote that \"time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier.\"\n\nThis is a philosophical question, related to the problem of other minds and the hard problem of consciousness. The question revolves around a position defined by John Searle as \"strong AI\":\nSearle distinguished this position from what he called \"weak AI\":\nSearle introduced the terms to isolate strong AI from weak AI so he could focus on what he thought was the more interesting and debatable issue. He argued that \"even if we assume\" that we had a computer program that acted exactly like a human mind, there would still be a difficult philosophical question that needed to be answered.\n\nNeither of Searle's two positions are of great concern to AI research, since they do not directly answer the question \"can a machine display general intelligence?\" (unless it can also be shown that consciousness is \"necessary\" for intelligence). Turing wrote \"I do not wish to give the impression that I think there is no mystery about consciousness… [b]ut I do not think these mysteries necessarily need to be solved before we can answer the question [of whether machines can think].\" Russell and Norvig agree: \"Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"\n\nThere are a few researchers who believe that consciousness is an essential element in intelligence, such as Igor Aleksander, Stan Franklin, Ron Sun, and Pentti Haikonen, although their definition of \"consciousness\" strays very close to \"intelligence.\" (See artificial consciousness.)\n\nBefore we can answer this question, we must be clear what we mean by \"minds\", \"mental states\" and \"consciousness\".\n\nThe words \"mind\" and \"consciousness\" are used by different communities in different ways. Some new age thinkers, for example, use the word \"consciousness\" to describe something similar to Bergson's \"élan vital\": an invisible, energetic fluid that permeates life and especially the mind. Science fiction writers use the word to describe some essential property that makes us human: a machine or alien that is \"conscious\" will be presented as a fully human character, with intelligence, desires, will, insight, pride and so on. (Science fiction writers also use the words \"sentience\", \"sapience,\" \"self-awareness\" or \"ghost\" - as in the \"Ghost in the Shell\" manga and anime series - to describe this essential human property). For others , the words \"mind\" or \"consciousness\" are used as a kind of secular synonym for the soul.\n\nFor philosophers, neuroscientists and cognitive scientists, the words are used in a way that is both more precise and more mundane: they refer to the familiar, everyday experience of having a \"thought in your head\", like a perception, a dream, an intention or a plan, and to the way we \"know\" something, or \"mean\" something or \"understand\" something . \"It's not hard to give a commonsense definition of consciousness\" observes philosopher John Searle. What is mysterious and fascinating is not so much \"what\" it is but \"how\" it is: how does a lump of fatty tissue and electricity give rise to this (familiar) experience of perceiving, meaning or thinking?\n\nPhilosophers call this the hard problem of consciousness. It is the latest version of a classic problem in the philosophy of mind called the \"mind-body problem.\" A related problem is the problem of \"meaning\" or \"understanding\" (which philosophers call \"intentionality\"): what is the connection between our \"thoughts\" and \"what we are thinking about\" (i.e. objects and situations out in the world)? A third issue is the problem of \"experience\" (or \"phenomenology\"): If two people see the same thing, do they have the same experience? Or are there things \"inside their head\" (called \"qualia\") that can be different from person to person?\n\nNeurobiologists believe all these problems will be solved as we begin to identify the neural correlates of consciousness: the actual relationship between the machinery in our heads and its collective properties; such as the mind, experience and understanding. Some of the harshest critics of artificial intelligence agree that the brain is just a machine, and that consciousness and intelligence are the result of physical processes in the brain. The difficult philosophical question is this: can a computer program, running on a digital machine that shuffles the binary digits of zero and one, duplicate the ability of the neurons to create minds, with mental states (like understanding or perceiving), and ultimately, the experience of consciousness?\n\nJohn Searle asks us to consider a thought experiment: suppose we have written a computer program that passes the Turing test and demonstrates \"general intelligent action.\" Suppose, specifically that the program can converse in fluent Chinese. Write the program on 3x5 cards and give them to an ordinary person who does not speak Chinese. Lock the person into a room and have him follow the instructions on the cards. He will copy out Chinese characters and pass them in and out of the room through a slot. From the outside, it will appear that the Chinese room contains a fully intelligent person who speaks Chinese. The question is this: is there anyone (or anything) in the room that understands Chinese? That is, is there anything that has the mental state of understanding, or which has conscious awareness of what is being discussed in Chinese? The man is clearly not aware. The room cannot be aware. The \"cards\" certainly aren't aware. Searle concludes that the Chinese room, or \"any\" other physical symbol system, cannot have a mind.\n\nSearle goes on to argue that actual mental states and consciousness require (yet to be described) \"actual physical-chemical properties of actual human brains.\" He argues there are special \"causal properties\" of brains and neurons that gives rise to minds: in his words \"brains cause minds.\"\n\nGottfried Leibniz made essentially the same argument as Searle in 1714, using the thought experiment of expanding the brain until it was the size of a mill. In 1974, Lawrence Davis imagined duplicating the brain using telephone lines and offices staffed by people, and in 1978 Ned Block envisioned the entire population of China involved in such a brain simulation. This thought experiment is called \"the Chinese Nation\" or \"the Chinese Gym\". Ned Block also proposed his Blockhead argument, which is a version of the Chinese room in which the program has been re-factored into a simple set of rules of the form \"see this, do that\", removing all mystery from the program.\n\nResponses to the Chinese room emphasize several different points. \n\nThe computational theory of mind or \"computationalism\" claims that the relationship between mind and brain is similar (if not identical) to the relationship between a \"running program\" and a computer. The idea has philosophical roots in Hobbes (who claimed reasoning was \"nothing more than reckoning\"), Leibniz (who attempted to create a logical calculus of all human ideas), Hume (who thought perception could be reduced to \"atomic impressions\") and even Kant (who analyzed all experience as controlled by formal rules). The latest version is associated with philosophers Hilary Putnam and Jerry Fodor.\n\nThis question bears on our earlier questions: if the human brain is a kind of computer then computers can be both intelligent and conscious, answering both the practical and philosophical questions of AI. In terms of the practical question of AI (\"Can a machine display general intelligence?\"), some versions of computationalism make the claim that (as Hobbes wrote):\nIn other words, our intelligence derives from a form of \"calculation\", similar to arithmetic. This is the physical symbol system hypothesis discussed above, and it implies that artificial intelligence is possible. In terms of the philosophical question of AI (\"Can a machine have mind, mental states and consciousness?\"), most versions of computationalism claim that (as Stevan Harnad characterizes it):\nThis is John Searle's \"strong AI\" discussed above, and it is the real target of the Chinese room argument (according to Harnad).\n\nAlan Turing noted that there are many arguments of the form \"a machine will never do X\", where X can be many things, such as:\nBe kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.\nTuring argues that these objections are often based on naive assumptions about the versatility of machines or are \"disguised forms of the argument from consciousness\". Writing a program that exhibits one of these behaviors \"will not make much of an impression.\" All of these arguments are tangential to the basic premise of AI, unless it can be shown that one of these traits is essential for general intelligence.\n\nIf \"emotions\" are defined only in terms of their effect on behavior or on how they function inside an organism, then emotions can be viewed as a mechanism that an intelligent agent uses to maximize the utility of its actions. Given this definition of emotion, Hans Moravec believes that \"robots in general will be quite emotional about being nice people\". Fear is a source of urgency. Empathy is a necessary component of good human computer interaction. He says robots \"will try to please you in an apparently selfless manner because it will get a thrill out of this positive reinforcement. You can interpret this as a kind of love.\" Daniel Crevier writes \"Moravec's point is that emotions are just devices for channeling behavior in a direction beneficial to the survival of one's species.\"\n\nHowever, emotions can also be defined in terms of their subjective quality, of what it \"feels like\" to have an emotion. The question of whether the machine \"actually feels\" an emotion, or whether it merely \"acts as if\" it is feeling an emotion is the philosophical question, \"can a machine be conscious?\" in another form.\n\n\"Self awareness\", as noted above, is sometimes used by science fiction writers as a name for the essential human property that makes a character fully human. Turing strips away all other properties of human beings and reduces the question to \"can a machine be the subject of its own thought?\" Can it \"think about itself\"? Viewed in this way, a program can be written that can report on its own internal states, such as a debugger. Though arguably self-awareness often presumes a bit more capability; a machine that can ascribe meaning in some way to not only its own state but in general postulating questions without solid answers: the contextual nature of its existence now; how it compares to past states or plans for the future, the limits and value of its work product, how it perceives its performance to be valued-by or compared to others.\n\nTuring reduces this to the question of whether a machine can \"take us by surprise\" and argues that this is obviously true, as any programmer can attest. He notes that, with enough storage capacity, a computer can behave in an astronomical number of different ways. It must be possible, even trivial, for a computer that can represent ideas to combine them in new ways. (Douglas Lenat's Automated Mathematician, as one example, combined ideas to discover new mathematical truths.) Kaplan and Haenlein suggest that machines can display scientific creativity, while it seems likely that humans will have the upper hand where artistic creativity is concerned.\n\nIn 2009, scientists at Aberystwyth University in Wales and the U.K's University of Cambridge designed a robot called Adam that they believe to be the first machine to independently come up with new scientific findings. Also in 2009, researchers at Cornell developed Eureqa, a computer program that extrapolates formulas to fit the data inputted, such as finding the laws of motion from a pendulum's motion.\n\nThis question (like many others in the philosophy of artificial intelligence) can be presented in two forms. \"Hostility\" can be defined in terms function or behavior, in which case \"hostile\" becomes synonymous with \"dangerous\". Or it can be defined in terms of intent: can a machine \"deliberately\" set out to do harm? The latter is the question \"can a machine have conscious states?\" (such as intentions) in another form.\n\nThe question of whether highly intelligent and completely autonomous machines would be dangerous has been examined in detail by futurists (such as the Singularity Institute). (The obvious element of drama has also made the subject popular in science fiction, which has considered many differently possible scenarios where intelligent machines pose a threat to mankind.)\n\nOne issue is that machines may acquire the autonomy and intelligence required to be dangerous very quickly. Vernor Vinge has suggested that over just a few years, computers will suddenly become thousands or millions of times more intelligent than humans. He calls this \"the Singularity.\" He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism.\n\nIn 2009, academics and technical experts attended a conference to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.\n\nSome experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.\n\nThe President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue. They point to programs like the Language Acquisition Device which can emulate human interaction.\n\nSome have suggested a need to build \"Friendly AI\", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.\n\nFinally, those who believe in the existence of a soul may argue that \"Thinking is a function of man's immortal soul.\" Alan Turing called this \"the theological objection\". He writes\nIn attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing mansions for the souls that He creates.\n\nSome scholars argue that the AI community's dismissal of philosophy is detrimental. In the \"Stanford Encyclopedia of Philosophy\", some philosophers argue that the role of philosophy in AI is underappreciated. Physicist David Deutsch argues that without an understanding of philosophy or its concepts, AI development would suffer from a lack of progress.\n\nThe main bibliography on the subject, with several sub-sections, is on PhilPapers\n\nThe main conference series on the issue is \"Philosophy and Theory of AI\" (PT-AI), run by Vincent C. Müller\n\n"}
{"id": "23612", "url": "https://en.wikipedia.org/wiki?curid=23612", "title": "Postmodern philosophy", "text": "Postmodern philosophy\n\nPostmodern philosophy is a philosophical movement that arose in the second half of the 20 century as a critical response to assumptions allegedly present in modernist philosophical ideas regarding culture, identity, history, or language that were developed during the 18th-century Enlightenment. Postmodernist thinkers developed concepts like difference, repetition, trace, and hyperreality to subvert \"grand narratives,\" univocity of being, and epistemic certainty. Postmodern philosophy questions the importance of power relationships, personalization, and discourse in the \"construction\" of truth and world views. Many postmodernists appear to deny that an objective reality exists, and appear to deny that there are objective moral values.\n\nJean-François Lyotard defined philosophical postmodernism in \"The Postmodern Condition\", writing \"Simplifying to the extreme, I define postmodern as incredulity towards metanarratives,\" where what he means by metanarrative is something like a unified, complete, universal, and epistemically certain story about everything that is. Postmodernists reject metanarratives because they reject the concept of truth that metanarratives presuppose. Postmodernist philosophers in general argue that truth is always contingent on historical and social context rather than being absolute and universal and that truth is always partial and \"at issue\" rather than being complete and certain.\n\nPostmodern philosophy is often particularly skeptical about simple binary oppositions characteristic of structuralism, emphasizing the problem of the philosopher cleanly distinguishing knowledge from ignorance, social progress from reversion, dominance from submission, good from bad, and presence from absence. But, for the same reasons, postmodern philosophy should often be particularly skeptical about the complex spectral characteristics of things, emphasizing the problem of the philosopher again cleanly distinguishing concepts, for a concept must be understood in the context of its opposite, such as existence and nothingness, normality and abnormality, speech and writing, and the like.\n\nPostmodern philosophy also has strong relations with the substantial literature of critical theory.\n\nMany postmodern claims are a deliberate repudiation of certain 18th-century Enlightenment values. Such a postmodernist believes that there is no objective natural reality, and that logic and reason are mere conceptual constructs that are not universally valid. Two other characteristic postmodern practices are a denial that human nature exists, and a (sometimes moderate) skepticism toward claims that science and technology will change society for the better. Postmodernists also believe there are no objective moral values. Thus, postmodern philosophy suggests equality for all things. One's concept of good and another's concept of evil are to be equally correct, since good and evil are subjective. Since both good and evil are equally correct, a postmodernist then tolerates both concepts, even if he or she disagrees with them subjectively. Postmodern writings often focus on deconstructing the role that power and ideology play in shaping discourse and belief. Postmodern philosophy shares ontological similarities with classical skeptical and relativistic belief systems, and shares political similarities with modern identity politics.\n\nThe \"Routledge Encyclopedia of Philosophy\" states that \"The assumption that there is no common denominator in 'nature' or 'truth' ... that guarantees the possibility of neutral or objective thought\" is a key assumption of postmodernism. The National Research Council has characterized the belief that \"social science research can never generate objective or trustworthy knowledge\" as an example of a postmodernist belief. Jean-François Lyotard's seminal 1979 \"The Postmodern Condition\" stated that its hypotheses \"should not be accorded predictive value in relation to reality, but strategic value in relation to the questions raised\". Lyotard's statement in 1984 that \"I define postmodern as incredulity toward meta-narratives\" extends to incredulity toward science. Jacques Derrida, who is generally identified as a postmodernist, stated that \"every referent, all reality has the structure of a differential trace\". Paul Feyerabend, one of the most famous twentieth-century philosophers of science, is often classified as a postmodernist; Feyerabend held that modern science is no more justified than witchcraft, and has denounced the \"tyranny\" of \"abstract concepts such as 'truth', 'reality', or 'objectivity', which narrow people's vision and ways of being in the world\". Feyerabend also defended astrology, adopted alternative medicine, and sympathized with creationism. Defenders of postmodernism state that many descriptions of postmodernism exaggerate its antipathy to science; for example, Feyerabend denied that he was \"anti-science\", accepted that some scientific \"theories\" are superior to other theories (even if science itself isn't superior to other modes of inquiry), and attempted conventional medical treatments during his fight against cancer.\n\nPhilosopher John Deely has argued for the contentious claim that the label \"postmodern\" for thinkers such as Derrida \"et al.\" is \"premature\". Insofar as the \"so-called\" postmoderns follow the thoroughly \"modern\" trend of idealism, it is more an \"ultra\"modernism than anything else. A postmodernism that lives up to its name, therefore, must no longer confine itself to the premodern preoccupation with \"things\" nor with the modern confinement to \"ideas,\" but must come to terms with the way of signs embodied in the semiotic doctrines of such thinkers as the Portuguese philosopher John Poinsot and the American philosopher Charles Sanders Peirce. Writes Deely,\n\nThe epoch of Greek and Latin philosophy was based on \"being\" in a quite precise sense: the existence exercised by things independently of human apprehension and attitude. The much briefer epoch of modern philosophy based itself rather on the instruments of human knowing, but in a way that unnecessarily compromised being. As the 20th century ends, there is reason to believe that a new philosophical epoch is dawning along with the new century, promising to be the richest epoch yet for human understanding. The postmodern era is positioned to synthesize at a higher level—the level of experience, where the being of things and the activity of the finite knower compenetrate one another and provide the materials whence can be derived knowledge of nature and knowledge of culture in their full symbiosis—the achievements of the ancients and the moderns in a way that gives full credit to the preoccupations of the two. The postmodern era has for its distinctive task in philosophy the exploration of a new path, no longer the ancient way of things nor the modern way of ideas, but the way of signs, whereby the peaks and valleys of ancient and modern thought alike can be surveyed and cultivated by a generation which has yet further peaks to climb and valleys to find.\nPostmodern philosophy originated primarily in France during the mid-20th century. However, several philosophical antecedents inform many of postmodern philosophy's concerns.\n\nIt was greatly influenced by the writings of Søren Kierkegaard and Friedrich Nietzsche in the 19th century and other early-to-mid 20th-century philosophers, including phenomenologists Edmund Husserl and Martin Heidegger, psychoanalyst Jacques Lacan, structuralist Roland Barthes, Georges Bataille, and the later work of Ludwig Wittgenstein. Postmodern philosophy also drew from the world of the arts and architecture, particularly Marcel Duchamp, John Cage and artists who practiced collage, and the architecture of Las Vegas and the Pompidou Centre.\n\nThe most influential early postmodern philosophers were Jean Baudrillard, Jean-François Lyotard, and Jacques Derrida. Michel Foucault is also often cited as an early postmodernist although he personally rejected that label. Following Nietzsche, Foucault argued that knowledge is produced through the operations of \"power\", and changes fundamentally in different historical periods.\n\nThe writings of Lyotard were largely concerned with the role of narrative in human culture, and particularly how that role has changed as we have left modernity and entered a \"postindustrial\" or postmodern condition. He argued that modern philosophies legitimized their truth-claims not (as they themselves claimed) on logical or empirical grounds, but rather on the grounds of accepted stories (or \"metanarratives\") about knowledge and the world—comparing these with Wittgenstein's concept of language-games. He further argued that in our postmodern condition, these metanarratives no longer work to legitimize truth-claims. He suggested that in the wake of the collapse of modern metanarratives, people are developing a new \"language-game\"—one that does not make claims to absolute truth but rather celebrates a world of ever-changing relationships (among people and between people and the world).\n\nDerrida, the father of deconstruction, practiced philosophy as a form of textual criticism. He criticized Western philosophy as privileging the concept of presence and \"logos\", as opposed to absence and markings or writings.\n\nIn the United States, the most famous pragmatist and self-proclaimed postmodernist was Richard Rorty. An analytic philosopher, Rorty believed that combining Willard Van Orman Quine's criticism of the analytic-synthetic distinction with Wilfrid Sellars's critique of the \"Myth of the Given\" allowed for an abandonment of the view of the thought or language as a mirror of a reality or external world. Further, drawing upon Donald Davidson's criticism of the dualism between conceptual scheme and empirical content, he challenges the sense of questioning whether our particular concepts are related to the world in an appropriate way, whether we can justify our ways of describing the world as compared with other ways. He argued that truth was not about getting it right or representing reality, but was part of a social practice and language was what served our purposes in a particular time; ancient languages are sometimes untranslatable into modern ones because they possess a different vocabulary and are unuseful today. Donald Davidson is not usually considered a postmodernist, although he and Rorty have both acknowledged that there are few differences between their philosophies.\n\nCritics claim that postmodernism is nonsensical or self-contradictory.\n\n\n\n"}
{"id": "17989579", "url": "https://en.wikipedia.org/wiki?curid=17989579", "title": "Remote service software", "text": "Remote service software\n\nRemote service software is used by equipment manufacturers to remotely monitor, access and repair products in use at customer sites. It’s a secure, auditable gateway for service teams to troubleshoot problems, perform proactive maintenance, assist with user operations and monitor performance. This technology is typically implemented in mission-critical environments like hospitals or IT data centers – where equipment downtime is intolerable.\n\nRemote service software helps to:\n\nManufacturers are using aftermarket service a competitive differentiator. Remote service software provides a platform for manufacturers to offer and meet stringent service level agreements (SLAs) without increasing the size of their service team.\n\n\n\n"}
{"id": "3534703", "url": "https://en.wikipedia.org/wiki?curid=3534703", "title": "Robotic lawn mower", "text": "Robotic lawn mower\n\nA robotic lawn mower is an autonomous robot used to cut lawn grass. A typical robotic lawn mower (in particular earlier generation models) requires the user to set up a border wire around the lawn that defines the area to be mowed. The robot uses this wire to locate the boundary of the area to be trimmed and in some cases to locate a recharging dock. Robotic mowers are capable of maintaining up to of grass.\n\nRobotic lawn mowers are increasingly sophisticated, are self-docking and some contain rain sensors if necessary, nearly eliminating human interaction. Robotic lawn mowers represented the second largest category of domestic robots used by the end of 2005.\n\nPossibly the first commercial robotic lawn mower was the MowBot, introduced and patented in 1969 and already showing many features of today's most popular products.\n\nIn 2012, the growth of robotic lawn mower sales was 15 times that of the traditional styles. \nWith the emergence of smart phones some robotic mowers have integrated features within custom apps to adjust settings or scheduled mowing times and frequency, as well as manually control the mower with a digital joystick.\n\nModern robotic lawn mowers can contain specialized sensors, allowing them to automatically mow around obstacles or even go to sleep when it starts to rain.\nIn 1995, the first fully solar powered robotic mower became available.\n\nThe mower can find its charging station via radio frequency emissions, by following a boundary wire, or by following an optional guide wire. This can eliminate wear patterns in the lawn caused by the mower only being able to follow one wire back to the station.\n\nTo get to remote areas or areas only accessible through narrow passages the mower can follow a guide wire or a boundary wire out of the station.\n\nBatteries used include nickel–metal hydride (NiMH), lithium-ion and lead-acid.\n\nThe price of a robotic lawn mower varies from around $1000 to $4500.\n\n"}
{"id": "44402859", "url": "https://en.wikipedia.org/wiki?curid=44402859", "title": "SIP extensions for the IP Multimedia Subsystem", "text": "SIP extensions for the IP Multimedia Subsystem\n\nThe Session Initiation Protocol (SIP) is the signaling protocol selected by the 3rd Generation Partnership Project (3GPP) to create and control multimedia sessions with two or more participants in the IP Multimedia Subsystem (IMS), and therefore is a key element in the IMS framework.\n\nSIP was developed by the Internet Engineering Task Force (IETF) as a standard for controlling multimedia communication sessions in Internet Protocol (IP) networks, working in the application layer of the Internet Protocol Suite. Several SIP extensions have been added to the basic protocol specification in order to extend its functionality. These extensions are based on Request for Comments (RFC) protocol recommendations by the IETF.\n\nThe 3GPP, which is a collaboration between groups of telecommunications associations aimed at developing and maintaining the IMS, stated a series of requirements for SIP to be successfully used in the IMS. Some of them could be addressed by using existing capabilities and extensions in SIP while, in other cases, the 3GPP had to collaborate with the IETF to standardize new SIP extensions to meet the new requirements. In any case, the IETF evolves SIP in a generic basis, so that the use of its extensions is not restricted to the IMS framework.\n\nThe 3GPP has stated several general requirements stated for operation of the IMS. These include an efficient use of the radio interface by minimizing the exchange of signaling messages between the mobile terminal and the network, a minimum session setup time by performing tasks prior to session establishment instead of during session establishment, a minimum support required in the terminal, the support for roaming and non-roaming scenarios with terminal mobility management (supported by the access network, not SIP), and support for IPv6 addressing.\n\nOther requirements involve protocol extensions, such as SIP header fields to exchange user or server information, and SIP methods to support new network functionality: requirement for registration, re-registration, de-registration, event notifications, instant messaging or call control primitives with additional capabilities such as call transference.\n\nOther specific requirements are:\n\nFinally, it is also necessary that other protocols and network services such as DHCP or DNS are adapted to work with SIP, for instance for outbound proxy (P-CSCF) location and SIP Uniform Resource Identifier (URI) to IP address resolution, respectively.\n\nThere is a mechanism in SIP for extension negotiation between user agents (UA) or servers, consisting of three header fields: \"supported\", \"require\" and \"unsupported\", which UAs or servers (i.e. user terminals or call session control function (CSCF) in IMS) may use to specify the extensions they understand. When a client initiates a SIP dialog with a server, it states the extensions it \"requires\" to be used and also other extensions that are understood (\"supported\"), and the server will then send a response with a list of extensions that it \"requires\". If these extensions are not listed in the client's message, the response from the server will be an error response. Likewise, if the server does not support any of the client's required extensions, it will send an error response with a list of its \"unsupported\" extensions. This kind of extensions are called \"option tags\", but SIP can also be extended with new \"methods\". In that case, user agents or servers use the \"Allow\" header to state which methods they support. To \"require\" the use of a particular method in a particular dialog, they must use an \"option tag\" associated to that method.\n\nThese two extensions allow users to specify their preferences about the service the IMS provides.\n\nWith the caller preferences extension, the calling party is able to indicate the kind of user agent they want to reach (e.g. whether it is fixed or mobile, a voicemail or a human, personal or for business, which services it is capable to provide, or which methods it supports) and how to search for it, with three header fields: \"Accept-Contact\" to describe the desired destination user agents, \"Reject-Contact\" to state the user agents to avoid, and \"Request-Disposition\" to specify how the request should be handled by servers in the network (i.e. whether or not to redirect and how to search for the user: sequentially or in parallel).\n\nBy using the user agent capabilities extension, user agents (terminals) can describe themselves when they register so that others can search for them according to their caller preferences extension headers. For this purpose, they list their capabilities in the \"Contact\" header field of the REGISTER message.\n\nThe aim of event notification is to obtain the status of a given resource (e.g. a user, one's voicemail service) and to receive updates of that status when it changes.\n\nEvent notification is necessary in the IMS framework to inform about the \"presence\" of a user (i.e. \"online\" or \"offline\") to others that may be waiting to contact them, or to notify a user and its P-CSCF of its own \"registration\" state, so that they know if they are reachable and what public identities they have registered. Moreover, event notification can be used to provide additional services such as voicemail (i.e. to notify that they have new voice messages in their inbox).\n\nTo this end, the specific event notification extension defines a framework for event notification in SIP, with two new methods: SUBSCRIBE and NOTIFY, new header fields and response codes and two roles: the \"subscriber\" and the \"notifier\". The entity interested in the state information of a resource (the \"subscriber\") sends a SUBSCRIBE message with the Uniform Resource Identifier (URI) of the resource in the request initial line, and the type of event in the \"Event header\". Then the entity in charge of keeping track of the state of the resource (the \"notifier\"), receives the SUBSCRIBE request and sends back a NOTIFY message with a \"subscription-state header\" as well as the information about the status of the resource in the message body. Whenever the resource state changes, the \"notifier\" sends a new NOTIFY message to the \"subscriber\". Each kind of event a subscriber can subscribe to is defined in a new \"event package\". An \"event package\" describes a new value for the SUBSCRIBE \"Event header\", as well as a MIME type to carry the event state information in the NOTIFY message.\n\nThere is also an \"allow-events\" header to indicate event notification capabilities, and the \"202 accepted\" and \"489 bad event\" response codes to indicate if a subscription request has been preliminary accepted or has been turned down because the \"notifier\" does not understand the kind of event requested.\n\nIn order to make an efficient use of the signaling messages, it is also possible to establish a limited notification rate (not real-time notifications) through a mechanism called \"event throttling\". Moreover, there is also a mechanism for \"conditional event notification\" that allows the \"notifier\" to decide whether or not to send the complete NOTIFY message depending on if there is something new to notify since last subscription or there is not.\n\nThe event notification framework defines how a user agent can subscribe to events about the state of a resource, but it does not specify how that state can be published. The SIP extension for event state publication was defined to allow user agents to publish the state of an event to the entity (\"notifier\") that is responsible for composing the event state and distributing it to the \"subscribers\".\n\nThe state publication framework defines a new method: PUBLISH, which is used to request the publication of the state of the resource specified in the request-URI, with reference to the event stated in the \"Event header\", and with the information carried in the message body.\n\nThe functionality of sending instant messages to provide a service similar to text messaging is defined in the instant messaging extension. These messages are unrelated to each other (i.e. they do not originate a SIP dialog) and sent through the SIP signaling network, sharing resources with control messages.\n\nThis functionality is supported by the new MESSAGE method, that can be used to send an instant message to the resource stated in the request-URI, with the content carried in the message body. This content is defined as a MIME type, being \"text/plain\" the most common one.\n\nIn order to have an instant messaging session with related messages, the Message Session Relay Protocol (MSRP) should be used.\n\nThe REFER method extension defines a mechanism to request a user agent to contact a resource which is identified by a URI in the \"Refer-To\" header field of the request message. A typical use of this mechanism is call transfer: during a call, the participant who sends the REFER message tells the recipient to contact to the user agent identified by the URI in the corresponding header field. The REFER message also implies an event subscription to the result of the operation, so that the sender will know whether or not the recipient could contact the third person.\n\nHowever, this mechanism is not restricted to call transfer, since the \"Refer-To\" header field can be any kind of URI, for instance, an HTTP URI, to require the recipient to visit a web page.\n\nIn the basic SIP specification, only requests and final responses (i.e. 2XX response codes) are transmitted reliably, this is, they are retransmitted by the sender until the acknowledge message arrives (i.e. the corresponding response code to a request, or the ACK request corresponding to a 2XX response code). This mechanism is necessary since SIP can run not only over reliable transport protocols (TCP) that assure that the message is delivered, but also over unreliable ones (UDP) that offer no delivery guarantees, and it is even possible that both kinds of protocols are present in different parts of the transport network.\n\nHowever, in such an scenario as the IMS framework, it is necessary to extend this reliability to provisional responses to INVITE requests (for session establishment, this is, to start a call). The reliability of provisional responses extension provides a mechanism to confirm that provisional responses such as the \"180 Ringing\" response code, that lets the caller know that the callee is being alerted, are successfully received. To do so, this extension defines a new method: PRACK, which is the request message used to tell the sender of a provisional response that his or her message has been received. This message includes a \"RACK\" header field which is a sequence number that matches the \"RSeq\" header field of the provisional response that is being acknowledged, and also contains the \"CSeq\" number that identifies the corresponding INVITE request. To indicate that the user agent requests or supports reliable provisional responses, the \"100rel\" option tag will be used.\n\nThe aim of the UPDATE method extension is to allow user agents to provide updated session description information within a dialog, before the final response to the initial INVITE request is generated. This can be used to negotiate and allocate the call resources before the called party is alerted.\n\nIn the IMS framework, it is required that once the callee is alerted, the chances of a session failure are minimum. An important source of failure is the inability to reserve network resources to support the session, so these resources should be allocated before the phone rings. However, in the IMS, to reserve resources the network needs to know the callee's IP address, port and session parameters and therefore it is necessary that the initial offer/answer exchange to establish a session has started (INVITE request). In basic SIP, this exchange eventually causes the callee to be alerted. To solve this problem, the concept of preconditions was introduced. In this concept the caller states a set of constraints about the session (i.e. codecs and QoS requirements) in the offer, and the callee responds to the offer without establishing the session or alerting the user. This establishment will occur if and only if both the caller and the callee agree that the preconditions are met.\n\nThe preconditions SIP extension affects both SIP, with a new option tag (\"precondition\") and defined offer/answer exchanges, and Session Description Protocol (SDP), which is a format used to describe streaming media initialization parameters, carried in the body of SIP messages. The new SDP attributes are meant to describe the \"current status\" of the resource reservation, the \"desired status\" of the reservation to proceed with session establishment, and the \"confirmation status\", to indicate when the reservation status should be confirmed.\n\nIn the IMS, the initial session parameter negotiation can be done by using the provisional responses and session description updating extensions, along with SDP in the body of the messages.\nThe first offer, described by means of SDP, can be carried by the INVITE request and will deal with the caller's supported codecs. This request will be answered by the provisional reliable response code 183 Session Progress, that will carry the SDP list of supported codecs by both the caller and the callee. The corresponding PRACK to this provisional answer will be used to select a codec and initiate the QoS negotiation.\n\nThe QoS negotiation is supported by the PRACK request, that starts resource reservation in the calling party network, and it is answered by a 2XX response code. Once this response has been sent, the called party has selected the codec too, and starts resource reservation on its side. Subsequent UPDATE requests are sent to inform about the reservation progress, and they are answered by 2XX response codes. In a typical offer/answer exchange, one UPDATE will be sent by the calling party when its reservation is completed, then the called party will respond and eventually finish allocating the resources. It is then, when all the resources for the call are in place, when the caller is alerted.\n\nIn the IMS framework it is fundamental to handle user identities for authentication, authorization and accounting purposes. The IMS is meant to provide multimedia services over IP networks, but also needs a mechanism to charge users for it. All this functionality is supported by new special header fields.\n\nThe Private Header Extensions to SIP, also known as P-Headers, are special header fields whose applicability is limited to private networks with a certain topology and characteristics of lower layers' protocols. They were designed specifically to meet the 3GPP requirements because a more general solution was not available.\n\nThese header fields are used for a variety of purposes including charging and information about the networks a call traverses:\n\n\nMore private headers have been defined for user database accessing:\n\n\nThe private extensions for asserted identity within trusted networks are designed to enable a network of trusted SIP servers to assert the identity of authenticated users, only within an administrative domain with previously agreed policies for generation, transport and usage of this identification information. These extensions also allow users to request privacy so that their identities are not spread outside the \"trust domain\". To indicate so, they must insert the privacy token \"id\" into the Privacy header field.\n\nThe main functionality is supported by the \"P-Asserted-Identity\" extension header. When a proxy server receives a request from an untrusted entity and authenticates the user (i.e. verifies that the user is who he or she says that he or she is), it then inserts this header with the identity that has been authenticated, and then forwards the request as usual. This way, other proxy servers that receive this SIP request within the \"Trust Domain\" (i.e. the network of trusted entities with previously agreed security policies) can safely rely on the identity information carried in the P-Asserted-Identity header without the necessity of re-authenticating the user.\n\nThe \"P-Preferred-Identity\" extension header is also defined, so that a user with several public identities is able to tell the proxy which public identity should be included in the P-Asserted-Identity header when the user is authenticated.\n\nFinally, when privacy is requested, proxies must withhold asserted identity information outside the trusted domain by removing P-Asserted-Identity headers before forwarding user requests to untrusted identities (outside the \"Trust Domain\").\n\nThere exist analogous extension headers for handling the identification of services of users, instead of the users themselves. In this case, Uniform Resource Names are used to identify a service (e.g. a voice call, an instant messaging session, an IPTV streaming)\n\nAccess security in the IMS consists of first authenticating and authorizing the user, which is done by the S-CSCF, and then establishing secure connections between the P-CSCF and the user. There are several mechanisms to achieve this, such as:\n\n\nThe security mechanisms agreement extension for SIP was then introduced to provide a secure mechanism for negotiating the security algorithms and parameters to be used by the P-CSCF and the terminal. This extension uses three new header fields to support the negotiation process:\n\n\nThe necessity in the IMS of reserving resources to provide quality of service (QoS) leads to another security issue: admission control and protection against denial-of-service attacks. To obtain transmission resources, the user agent must present an authorization token to the network (i.e. the policy enforcement point, or PEP) . This token will be obtained from its P-CSCF, which may be in charge of QoS policy control or have an interface with the policy control entity in the network (i.e. the policy decision function, or PDF) which originally provides the authorization token.\n\nThe private extensions for media authorization link session signaling to the QoS mechanisms applied to media in the network, by defining the mechanisms for obtaining authorization tokens and the \"P-Media-Authorization\" header field to carry these tokens from the P-CSCF to the user agent. This extension is only applicable within administrative domains with trust relationships. It was particularly designed for specialized SIP networks like the IMS, and not for the general Internet.\n\nSource routing is the mechanism that allows the sender of a message to specify partially or completely the route the message traverses. In SIP, the \"route\" header field, filled by the sender, supports this functionality by listing a set of proxies the message will visit. In the IMS context, there are certain network entities (i.e. certain CSCFs) that must be traversed by requests from or to a user, so they are to be listed in the \"Route\" header field. To allow the sender to discover such entities and populate the \"route\" header field, there are mainly two extension header fields: \"path\" and \"service-route\".\n\nThe extension header field for registering non-adjacent contacts provides a \"Path\" header field which accumulates and transmits the SIP URIs of the proxies that are situated between a user agent and its registrar as the REGISTER message traverses then. This way, the registrar is able to discover and record the sequence of proxies that must be transited to get back to the user agent.\n\nIn the IMS every user agent is served by its P-CSCF, which is discovered by using the Dynamic Host Configuration Protocol or an equivalent mechanism when the user enters the IMS network, and all requests and responses from or to the user agent must traverse this proxy. When the user registers to the home registrar (S-CSCF), the P-CSCF adds its own SIP URI in a \"Path\" header field in the REGISTER message, so that the S-CSCF receives and stores this information associated with the contact information of the user. This way, the S-CSCF will forward every request addressed to that user through the corresponding P-CSCF by listing its URI in the \"route\" header field.\n\nThe extension for service route discovery during registration consists of a \"Service-Route\" header field that is used by the registrar in a 2XX response to a REGISTER request to inform the registering user of the entity that must forward every request originated by him or her.\n\nIn the IMS, the registrar is the home network's S-CSCF and it is also required that all requests are handled by this entity, so it will include its own SIP URI in the \"service-route\" header field. The user will then include this SIP URI in the \"Route\" header field of all his or her requests, so that they are forwarded through the home S-CSCF.\n\nIn the IMS it is possible for a user to have multiple terminals (e.g. a mobile phone, a computer) or application instances (e.g. video telephony, instant messaging, voice mail) that are identified with the same public identity (i.e. SIP URI). Therefore, a mechanism is needed in order to route requests to the desired device or application. That is what a \"Globally Routable User Agent URI (GRU)\" is: a URI that identifies a specific user agent instance (i.e. terminal or application instance) and it does it globally (i.e. it is valid to route messages to that user agent from any other user agent on the Internet).\n\nThese URIs are constructed by adding the \"gr\" parameter to a SIP URI, either to the public SIP URI with a value that identifies the user agent instance, or to a specially created URI that does not reveal the relationship between the GRUU and the user's identity, for privacy purposes. They are commonly obtained during the registration process: the registering user agent sends a Uniform Resource Name (URN) that uniquely identifies that SIP instance, and the registrar (i.e. S-CSCF) builds the GRUU, associates it to the registered identity and SIP instance and sends it back to the user agent in the response. When the S-CSCF receives a request for that GRUU, it will be able to route the request to the registered SIP instance.\n\nThe efficient use of network resources, which may include a radio interface or other low-bandwidth access, is essential in the IMS in order to provide the user with an acceptable experience in terms of latency. To achieve this goal, SIP messages can be compressed using the mechanism known as \"SigComp\" (signaling compression).\n\nCompression algorithms perform this operation by substituting repeated words in the message by its position in a dictionary where all these words only appear once. In a first approach, this dictionary may be built for each message by the compressor and sent to the decompressor along with the message itself. However, as many words are repeated in different messages, the extended operations for \"SigComp\" define a way to use a shared dictionary among subsequent messages. Moreover, in order to speed up the process of building a dictionary along subsequent messages and provide high compression ratios since the first INVITE message, SIP provides a static SIP/SDP dictionary which is already built with common SIP and SDP terms.\n\nThere is a mechanism to indicate that a SIP message is desired to be compressed. This mechanism defines the \"comp=sigcomp\" parameter for SIP URIs, which signals that the SIP entity identified by the URI supports \"SigComp\" and is willing to receive compressed messages. When used in request-URIs, it indicates that the request is to be compressed, while in Via header fields it signals that the subsequent response is to be compressed.\n\nIn order to obtain even shorter SIP messages and make a very efficient use of the resources, the content indirection extension makes it possible to replace a MIME body part of the message with an external reference, typically an HTTP URI. This way the recipient of the message can decide whether or not to follow the reference to fetch the resource, depending on the bandwidth available.\n\nNetwork address translation (NAT) makes it impossible for a terminal to be reached from outside its private network, since it uses a private address that is mapped to a public one when packets originated by the terminal cross the NAT. Therefore, NAT traversal mechanisms are needed for both the signaling plane and the media plane.\n\nInternet Engineering Task Force's RFC 6314 summarizes and unifies different methods to achieve this, such as symmetric response routing and client-initiated connections for SIP signaling, and the use of STUN, TURN and ICE, which combines both previous ones, for media streams\n\nInternet Engineering Task Force's RFC 6157 describes the necessary mechanisms to guarantee that SIP works successfully between both Internet Protocol versions during the transition to IPv6. While SIP signaling messages can be transmitted through heterogeneous IPv4/IPv6 networks as long as proxy servers and DNS entries are properly configured to relay messages across both networks according to these recommendations, user agents will need to implement extensions so that they can directly exchange media streams. These extensions are related to the Session Description Protocol offer/answer initial exchange, that will be used to gather the IPv4 and IPv6 addresses of both ends so that they can establish a direct communication.\n\nApart from all the explained extensions to SIP that make it possible for the IMS to work successfully, it is also necessary that the IMS framework interworks and exchanges services with existing network infrastructures, mainly the Public switched telephone network (PSTN).\n\nThere are several standards that address this requirements, such as the following two for services interworking between the PSTN and the Internet (i.e. the IMS network):\n\nAnd also for PSTN-SIP gateways to support calls with one end in each network:\n\nMoreover, the SIP INFO method extension is designed to carry user information between terminals without affecting the signaling dialog and can be used to transport the dual-tone multi-frequency signaling to provide telephone keypad function for users.\n\n\n\n"}
{"id": "18106245", "url": "https://en.wikipedia.org/wiki?curid=18106245", "title": "Social networking in the Philippines", "text": "Social networking in the Philippines\n\nSocial networking is one of the most active web-based activities in the Philippines, with Filipinos being declared as the most active users on a number of web-based social network sites such as Facebook, Instagram, Snapchat and Twitter. The use of social networking website has become so extensive in the Philippines that the country has been tagged as \"The Social Networking Capital of the World,\" and has also become part of Filipino cyberculture. Social networking is also used in the Philippines as a form of election campaign material, as well as tools to aid criminal investigation.\n\nFriendster is one of the first social networking websites in the World Wide Web when it was introduced in 2002. However, its popularity in the United States plummeted quickly in 2004 due to massive technical problems and server delays. But as it was losing its American audience, Friendster slowly gained users from Southeast Asia starting in the Philippines. Friendster director of engineering Chris Lunt wondered why its web traffic was spiking in the middle of the night, and noticed that the traffic was coming from the Philippines. He then traced the trail to a Filipino-American marketing consultant and hypnotist named Carmen Leilani de Jesus as the first user to have introduced Friendster to the Philippines, where a number of her friends live.\n\nA study released by Universal McCann entitled \"Power To The People - Wave3\" declared the Philippines as \"the social networking capital of the world,\" with 83 percent of Filipinos surveyed are members of a social network. They are also regarded as the top photo uploaders and web video viewers, while they are second when it comes to the number of blog readers and video uploaders.\n\nWith over 7.9 million Filipinos using the Internet, 6.9 million of them visit a social networking site at least once a month. At times, Friendster has been the most visited website in the Philippines, as well as in Indonesia, according to web tracking site Alexa. David Jones, vice president for global marketing of Friendster, said that \"the biggest percentage of (their site's) users is from the Philippines, clocking in with 39 percent of the site's traffic.\" He further added that in March 2008 alone, Friendster recorded 39 million unique visitors, with 13.2 million of whom were from the Philippines. Meanwhile, Multiply president and founder Peter Pezaris said that the Filipino users of their site comprised the largest and most active group in terms of number of subscribers and of photographs being uploaded daily. About 2.2 million out of more than nine million registered users of Multiply are Filipinos, outnumbering even nationalities with a bigger population base like the United States, Indonesia, and Brazil. Also, one million photographs are uploaded by Filipinos to Multiply every day, which is half of their total number worldwide.\n\nSixty percent of Filipino users of Multiply are female, while 70 percent are under the age of 25. In comparison, Filipino Friendster users are between the ages 16 to 30, with 55 percent of them female.\n\nThe popularity of social networking in the Philippines can be traced in the Filipinos' culture of \"friends helping friends.\" For Filipinos, their friends and who they know can become more valuable than money, especially when what they need can be achieved through nepotism, favoritism, and friendship among others.\n\nSocial networking has extensive uses in the Philippines. It was used to promote television programs like with its two profiles on Multiply. A call center company posted job openings on its Multiply community site and was able to attract recruits. The power of social networking was tested in the country's 2007 general elections when senatorial candidate Francis Escudero created his own Friendster profile to bolster support from Filipino users. He eventually won a seat in the Senate. Local celebrities and politicians have since created their own profiles on Friendster as their medium to communicate with their fans and constituents.\n\nFriendster was also used as a tool for police investigations. Local police in Cebu City were able to track down the suspects for the robbery and murder of a female nursing student on March 2008. After receiving information and tips from the public and other police operatives, the local police searched through the suspects' profiles in order to get a closer look at their faces. The police printed the pictures of the suspects and launched a series of police operations, which led to their arrest. Meanwhile, Manila Police District arrested a suspect for the murder of two guest relations officers in Tondo on January 2007 after they were able to find the suspect's whereabouts through his Friendster profile.\n\nSocial networks also became a source of high-profile cyberwars, notably between actors Ynez Veneracion and Mon Confiado against Juliana Palermo. The two accused Palermo of creating a fake Friendster profile of her ex-boyfriend Confiado, which is uploaded with photos of Confiado and his girlfriend Veneracion but laden with profanities in each caption.\n\nFor his bid for the Philippine Presidency in May 2010, then Secretary of National Defense, Gilberto Teodoro launched an aggressive campaign via the social media. He capitalized on networks such as YouTube and Facebook. He reportedly spent nearly a quarter of his campaign budget on the social media in the Philippines; in comparison to the current president’s Benigno Simeon Aquino III – 9%.\n\nFilipino-American Internet personality Christine Gambito, also known as HappySlip, criticized Friendster for displaying what she described as \"inappropriate advertisements\" that appear on her profile. She posted a message on the site's bulletin board addressing her fans that she contemplated deleting her account. Gambito had earlier deleted her MySpace account because she objected to the Google-powered online advertisements that she said \"were in direct conflict with the HappySlip brand and especially misrepresentative of Filipino women.\" She particularly criticized the posting of advertisements of international dating websites that supposedly target Filipinas.\n\nMeanwhile, Philippine National Police Director General Avelino Razon ordered the Criminal Investigation and Detection Group to find out who created a fake account on Friendster using his identity. The profile was laden with false information about him, saying that he \"wants to meet traitors, corrupts, criminals so he could crush them.\"\n\nAs of December 2008, there have been cases of spam comments in Friendster profiles, most of which are in the form of a JPEG image masquerading as an embedded YouTube video, with a thumbnail of a sexually explicit video clip, such as a girl undressing herself or something similar. Clicking on the image usually results in a redirect to a dubious or disreputable website, or worse, a drive-by download of malware, such as the Koobface worm. Because some of the users, especially teenagers, who usually log on to the site in an Internet cafe, have only limited knowledge about malware and/or computers in general, such social engineering attacks can be a significant risk. The site had received considerable criticism due to this issue.\n\nOn July 2011, GMA Network creates the new campaign \"Think Before You Click\" - a campaign by GMA News to promote responsible use of social media.\nOn August 2016, Rappler initiates the campaign \"#NoPlaceForHate\" - a campaign that encourages civility when engaging online.\n"}
{"id": "4148166", "url": "https://en.wikipedia.org/wiki?curid=4148166", "title": "Social positioning method", "text": "Social positioning method\n\nThe social positioning method (SPM) studies space-time behaviour by analysing the location coordinates of mobile phones and the social characteristics of the people carrying them. The SPM methods and experiments were developed in Estonia by Positium and Institute of Geography University of Tartu during 2003-2006 (\"Ahas, R. & Mark, Ü. (2005). Location based services - new challenges for planning and public administration? Futures 37, 547-561\").\n\nThe biggest advantage of mobile positioning-based methods is that mobile phones are widespread, positioning works inside buildings, and collection of movement data is done by a third party at regular intervals. Positioning data is digital; it is easy to trace many people at the same time and it is possible to analyse movements in real time. The disadvantage of mobile positioning today is relatively low preciseness, the boom in the generation of phones with a GPS will raise positioning accuracy.\n\nThe most important problems of SPM are related to data security, as well as concerns about non-authorized personal surveillance. These problems can be solved with further development of location-based services (LBS) and relevant legal and organisational regulation. Today mobile positioning can be applied only by obtaining participants’ personal acceptance.\n"}
{"id": "27440977", "url": "https://en.wikipedia.org/wiki?curid=27440977", "title": "Society for Philosophy and Technology", "text": "Society for Philosophy and Technology\n\nThe Society for Philosophy and Technology (SPT) is an independent international organization founded in 1976 whose purpose is to promote philosophical consideration of technology. SPT publishes \"\", a tri-annual scientific journal.\n\n\n\n\n\n"}
{"id": "3459188", "url": "https://en.wikipedia.org/wiki?curid=3459188", "title": "Sony Watchman", "text": "Sony Watchman\n\nThe Sony Watchman is a line of portable pocket televisions trademarked and produced by Sony. The line was introduced in 1982 and discontinued in 2000.\n\nThe initial model was introduced in 1982 as the FD-210, which had a grayscale five centimeter display. The device weighed around 650 grams, with a measurement of 87 x 198 x 33 millimeters. The device was sold in Japan with a price of 54,800 yen. Roughly two years later, in 1984, the device was introduced to Europe and North America.\n\nSony manufactured more than 65 models of the Watchman before its discontinuation in 2000. Upon the release of further models after the FD-210, the display size increased, and new features were introduced. The FD-3, introduced in 1987, had a built-in digital clock. The FD-30, introduced in 1984 had a built-in AM/FM Stereo radio. The FD-40/42/44/45 were among the largest Watchmen, utilizing a 4\" CRT display. The FD-40 introduced a single composite A/V input. The FD-45, introduced in 1986, was water-resistant. In 1988/1989, the FDL 330S color Watchman TV/Monitor with LCD display was introduced. In 1990, the FDL-310, a Watchman with a color LCD display was introduced. The FD-280/285, made from 1990 to 1994, was the last Watchman to use a black and white CRT display. One of the last Watchmen was the FDL-22 introduced in 1998, which featured an ergonomic body which made it easier to hold, and introduced Sony's \"Straptenna\", where the wrist strap served as the antenna.\n\nDue to the switch of television broadcasts to digital, most models of the Sony Watchman have lost their usefulness, because they now require to be connected to a digital converter box.\n\nA model of the Sony Watchman is seen multiple times in the film Rain Man.\n\n"}
{"id": "325542", "url": "https://en.wikipedia.org/wiki?curid=325542", "title": "Technology acceptance model", "text": "Technology acceptance model\n\nThe technology acceptance model (TAM) is an information systems theory that models how users come to accept and use a technology. The model suggests that when users are presented with a new technology, a number of factors influence their decision about how and when they will use it, notably:\nThe TAM has been continuously studied and expanded—the two major upgrades being the TAM 2 ( & ) and the Unified Theory of Acceptance and Use of Technology (or UTAUT, ). A TAM 3 has also been proposed in the context of e-commerce with an inclusion of the effects of trust and perceived risk on system use ().\n\nTAM is one of the most influential extensions of Ajzen and Fishbein's theory of reasoned action (TRA) in the literature. Davis's technology acceptance model (Davis, 1989; Davis, Bagozzi, & Warshaw, 1989)\nis the most widely applied model of users' acceptance and usage of technology\n(Venkatesh, 2000). It was developed by Fred Davis and Richard Bagozzi (, ). TAM replaces many of TRA's attitude measures with the two technology acceptance measures—\"ease of use\", and \"usefulness\". TRA and TAM, both of which have strong behavioural elements, assume that when someone forms an intention to act, that they will be free to act without limitation. In the real world there will be many constraints, such as limited freedom to act ().\n\nBagozzi, Davis and Warshaw say:\n\nEarlier research on the diffusion of innovations also suggested a prominent role for perceived ease of use. Tornatzky and Klein () analysed the adoption, finding that compatibility, relative advantage, and complexity had the most significant relationships with adoption across a broad range of innovation types. Eason studied perceived usefulness in terms of a fit between systems, tasks and job profiles, using the terms \"task fit\" to describe the metric (quoted in ) suggest that TAM must be extended to include variables that account for change processes and that this could be achieved through adoption of the innovation model into TAM.\n\nSeveral researchers have replicated Davis's original study () to provide empirical evidence on the relationships that exist between usefulness, ease of use and system use (; ; ; ; ; ). Much attention has focused on testing the robustness and validity of the questionnaire instrument used by Davis. Adams et al. () replicated the work of Davis () to demonstrate the validity and reliability of his instrument and his measurement scales. They also extended it to different settings and, using two different samples, they demonstrated the internal consistency and replication reliability of the two scales. Hendrickson et al. () found high reliability and good test-retest reliability. Szajna () found that the instrument had predictive validity for intent to use, self-reported usage and attitude toward use. The sum of this research has confirmed the validity of the Davis instrument, and to support its use with different populations of users and different software choices.\n\nSegars and Grover () re-examined Adams et al.'s () replication of the Davis work. They were critical of the measurement model used, and postulated a different model based on three constructs: usefulness, effectiveness, and ease-of-use. These findings do not yet seem to have been replicated. However, some aspects of these findings were tested and supported by Workman () by separating the dependent variable into information use versus technology use.\n\nMark Keil and his colleagues have developed (or, perhaps rendered more popularisable) Davis's model into what they call the Usefulness/EOU Grid, which is a 2×2 grid where each quadrant represents a different combination of the two attributes. In the context of software use, this provides a mechanism for discussing the current mix of usefulness and EOU for particular software packages, and for plotting a different course if a different mix is desired, such as the introduction of even more powerful software ().\nThe TAM model has been used in most technological and geographic contexts. One of these contexts is health care, which is growing rapidly \nVenkatesh and Davis extended the original TAM model to explain perceived usefulness and usage intentions in terms of social influence (subjective norms, voluntariness, image) and cognitive instrumental processes (job relevance, output quality, result demonstrability, perceived ease of use). The extended model, referred to as TAM2, was tested in both voluntary and mandatory settings. The results strongly supported TAM2 ().\n\nIn an attempt to integrate the main competing user acceptance models, Venkatesh et al. formulated the unified theory of acceptance and use of technology (UTAUT). This model was found to outperform each of the individual models (Adjusted R square of 69 percent) (). UTAUT has been adopted by some recent studies in healthcare.\n\n\nTAM has been widely criticised, despite its frequent use, leading the original proposers to attempt to redefine it several times. Criticisms of TAM as a \"theory\" include its questionable heuristic value, limited explanatory and predictive power, triviality, and lack of any practical value (). Benbasat and Barki suggest that TAM \"has diverted researchers' attention away from other \nimportant research issues and has created an illusion of progress in knowledge accumulation. Furthermore, the \nindependent attempts by several researchers to expand TAM in order to adapt it to the constantly changing IT environments has lead to a state of theoretical chaos and confusion\" (). In general, TAM focuses on the individual 'user' of a computer, with the concept of 'perceived usefulness', with extension to bring in more and more factors to explain how a user 'perceives' 'usefulness', and ignores the essentially social processes of IS development and implementation, without question where more technology is actually better, and the social consequences of IS use. Lunceford argues that the framework of perceived usefulness and ease of use overlooks other issues, such as cost and structural imperatives that force users into adopting the technology. For a recent analysis and critique of TAM, see Bagozzi ().\n\nLegris et al. claim that, together, TAM and TAM2 account for only 40% of a technological system's use.\n\nPerceived ease of use is less likely to be a determinant of attitude and usage intention according to studies of telemedicine (), mobile commerce (, and online banking ().\n\nA study conducted by Okafor, D. J., Nico, M. & Azman, B. B. (2016) discovered that perceived ease of use doesn't have any influence on the adoption of multimedia online technologies for Malaysian SMEs. The answers from the participants in this study suggest that, for them, perceived ease of use was not indicative of their behavioural intention to adopt multimedia online technologies (MOT) in the future. Instead of not adopting MOT, if they are complicated some participants said they are willing to learn it or practice more.\n\n\n"}
{"id": "17163802", "url": "https://en.wikipedia.org/wiki?curid=17163802", "title": "Technology dynamics", "text": "Technology dynamics\n\nTechnology dynamics is broad and relatively new scientific field that has been developed in the framework of the postwar science and technology studies field. It studies the process of technological change. Under the field of Technology Dynamics the process of technological change is explained by taking into account influences from \"internal factors\" as well as from \"external factors\". Internal factors relate technological change to unsolved technical problems and the established modes of solving technological problems and external factors relate it to various (changing) characteristics of the social environment, in which a particular technology is embedded.\n\nFor the last three decades, it has been argued that technology development is neither an autonomous process, determined by the \"inherent progress\" of human history, nor a process completely determined by external conditions like the prices of the resources that are needed to operate (develop) a technology, as it is theorized in neoclassical economic thinking. In mainstream neoclassical economic thinking, technology is seen as an exogenous factor: at the moment a technology is required, the most appropriate version can be taken down from the shelf based on costs of labor, capital and eventually raw materials.\n\nConversely, modern technology dynamics studies generally advocate that technologies are not \"self-evident\" or market-demanded, but are the upshot of a particular path of technology development and are shaped by social, economic and political factors. in this sense, technology dynamics aims at overcoming distinct \"internal\" and \"external\" points of views by presenting co-evolutionary approach regarding technology development.\n\nIn general, technology dynamics studies, besides giving a \"thick description\" of technology development, uses constructivist viewpoints emphasizing that technology is the outcome of particular social context. Accordingly, Technology Dynamics emphasizes the significance and possibility of regaining social control of technology, and also provides mechanisms needed to adapt to and steer the development of certain technologies. In that respect, it uses insights from retrospective studies to formulate hypotheses of a prospective nature on technology development of emerging technologies, besides formulating prescriptive policy recommendations.\n\nAn important feature of relevant theories of technological change therein is that they underline the quasi-evolutionary character of technological change: change based on technological variation and social selection in which technological knowledge, systems and institutions develop in interaction with each other. Processes of 'path dependence' are crucial in explaining technological change.\n\nFollowing these lines, there have been different approaches and concepts used under the field of technology dynamics.\n\n\nBased on the analysis of the various perspectives, one can aim at developing interventions in the dynamics of a technology. Some approaches have been developed targeting on interventions in technological change:\n\n\n\n"}
{"id": "13549505", "url": "https://en.wikipedia.org/wiki?curid=13549505", "title": "Technology fusion", "text": "Technology fusion\n\nTechnology fusion involves a transformation of core technologies through a combination process facilitated by technological advances such as the phone and the Internet, which ensure that labs are no longer isolated. This results in profitable advances that can be made cheaply by combining knowledge from different fields, companies, industries, and geographies. The technological fusion is distinguished from the so-called breakthrough approach, which is the linear technological development that replaces an older generation of technology through its focus on combining existing technologies into hybrid products that can revolutionize markets. \n\nThe fusion of technologies goes beyond mere combination. Fusion is more than complementarism, because it creates a new market and new growth opportunities for each participant in the innovation. It blends incremental improvements from several (often previously separate) fields to create a product.\n\nAn example is the fusion of mechanical and electronic engineering to create mechatronics. There is also the case of fusing chemical and electronics technology to produce the Liquid Crystal display (LCD) technology.\n\n"}
{"id": "26149061", "url": "https://en.wikipedia.org/wiki?curid=26149061", "title": "Tectonic weapon", "text": "Tectonic weapon\n\nA tectonic weapon is a hypothetical device or system which could create earthquakes, volcanoes, or other seismic events in specified locations by interfering with the Earth's natural geological processes. It was defined in 1992 by Aleksey Vsevolovidich Nikolayev, corresponding member Russian Academy of Sciences: \"A tectonic or seismic weapon would be the use of the accumulated tectonic energy of the Earth's deeper layers to induce a destructive earthquake\". He added \"to set oneself the objective of inducing an earthquake is extremely doubtful\".\n\nTheoretically, the tectonic weapon functions by creating a powerful charge of elastic energy in the form of deformed volume of the Earth's crust in a region of tectonic activity. This then becomes an earthquake once triggered by a nuclear explosion in the epicenter or a vast electric pulse. As to the question of whether a nuclear explosion can trigger an earthquake, there was the analysis of local seismic recordings within a couple of miles of nuclear tests in the 1960s at Nevada that showed nuclear explosions caused some tectonic stress. An account provided by a member of the Nevada Commission on Nuclear Projects, also claimed that a 1968 underground nuclear test called Faultless successfully induced an earthquake. The United States Geological Survey stated that it produced fresh fault rupture some 1200 meters long. There is also a theory that 1998 earthquake in Afghanistan was triggered by thermonuclear tests conducted in Indian and Pakistani test sites 2-20 days prior. \n\nRoger Clark, lecturer in geophysics at Leeds University said in the respected journal Nature in 1996, responding to a newspaper report that there had been two secret Soviet programs, \"Mercury\" and \"Volcano\", aimed at developing a \"tectonic weapon\" that could set off earthquakes from great distance by manipulating electromagnetism, said \"We don't think it is impossible, or wrong, but past experience suggests it is very unlikely\". According to Nature these programs had been \"unofficially known to Western geophysicists for several years\". According to the story the Mercury program began in 1987, three tests were conducted in Kyrgyzstan, and Volcano's last test occurred in 1992.\n\nSuch weapons, whether or not they exist or are feasible, are a source of concern in official circles. For example, US Secretary of Defense William S. Cohen, said on 28 April 1997 at the Conference on Terrorism, Weapons of Mass Destruction, and U.S. Strategy, University of Georgia, while discussing the dangers of false threats, \"Others are engaging even in an eco-type of terrorism whereby they can alter the climate, set off earthquakes, volcanoes remotely through the use of electromagnetic waves.\"\n\nNew Zealand's unsuccessful Project Seal programme during World War II attempted to create tsunami waves as a weapon. It was reported in 1999 that such a weapon might be viable.\n\nNikola Tesla claimed a small steam-powered mechanical oscillator he was experimenting with in 1898 produced earthquake-like effects, but this has never been replicated. The television show \"MythBusters\" in Episode 60 .E2.80.93 made a small machine based on the same principle but powered by electricity rather than steam; it produced vibrations in a large structure detectable 100 feet away, but no significant shaking, and they judged the effect to be a busted myth.\n\nThe 1978 Convention on the Prohibition of Military or Any Other Hostile Use of Environmental Modification Techniques is an international treaty ratified by 75 states, and signed by a further 17, that prohibits use of environmental modification techniques to cause earthquakes and tsunamis, amongst other phenomena.\n\nAfter natural tectonic phenomena such as the 2010 Haiti earthquake, conspiracy theories, usually relating to the armed forces of the United States and formerly the Soviet Union, often arise, though no evidence is advanced. After the Haiti earthquake it was widely reported that president Hugo Chávez of Venezuela made unsupported allegations that it had been caused by testing of a US tectonic weapon. The newspaper Komsomolskaya Pravda of Moscow reported on page 1 on 30 May 1992 that \"a geophysical or tectonic weapon was actually developed in the USSR despite the UN Convention\", but that Chief Seismologist Major-General V Bochrov of the USSR Ministry of Defence categorically rejected any hints on the existence of tectonic weapons.\n\nWhile the British Tallboy and Grand Slam bombs of World War II were called \"earthquake bombs\", the name came from their way of destroying very hardened targets by shaking their foundations as an earthquake would; they were never intended to cause an actual earthquake.\n\n"}
{"id": "1717878", "url": "https://en.wikipedia.org/wiki?curid=1717878", "title": "Zhongguancun", "text": "Zhongguancun\n\nZhongguancun (), or Zhong Guan Cun, is a technology hub in Haidian District, Beijing, China.\n\nIt is geographically situated in the northwestern part of Beijing city, in a band between the northwestern Third Ring Road and the northwestern Fourth Ring Road. Zhongguancun is very well known in China, and is often referred to as \"China's Silicon Valley\".\n\nZhongguancun has only existed since the 1950s and only became a household name in the early 1980s. The first person who envisioned the future for Zhongguancun was Chen Chunxian, a member of the Chinese Academy of Sciences (CAS), who came up with the idea for a Silicon Valley in China after he visited the U.S. as part of a government-sponsored trip. The location of the Chinese Academy of Sciences within Zhongguancun reinforced, and perhaps was in part responsible for the technological growth in this area.\n\nThroughout the 1980s and still today, Zhongguancun was known as \"electronics avenue,\" because of its connections to information technology and the preponderance of stores along a central, crowded street.\n\nZhongguancun was officially recognized by the central government of China in 1988. It was given the wordy name \"Beijing High-Technology Industry Development Experimental Zone.\"\n\nThe current designation Zhongguancun refers commonly to the original site. However, officially (as of 1999) Zhongguancun has become the \"Zhongguancun Science & Technology Zone.\" It is a zone with seven parks, including Haidian Park, Fengtai Park, Changping Park, Electronics City (in Chaoyang), Yizhuang Park, Desheng Park, and Jianxiang Park.\n\nThe original Zhongguancun is now known as the Haidian Park of the Zhongguancun Zone. The area and environs, however, remain the same.\n\nHailong Market, Guigu Market, Taipingyang Market, Dinghao Market and Kemao Market are the five prominent IT and electronics markets. They are technology bazaars, famous for their \"shops with a shop\", where prices are easily but grudgingly bargained. Zhongguancun shops mainly deal in PC-compatible hardware, peripherals and software. AppleCentre and Apple Experience Centre are also close by.\n\nA very particular sight to visit is the Haidian Christian Church, designed by Hamburg-based architects Gerkan, Marg and Partners.\n\nDue to the proximity and participation of China's two most prestigious universities, Peking University and Tsinghua University, along with the Chinese Academy of Sciences, many analysts elsewhere are optimistic about Zhongguancun's future prospects.\n\nNotable high schools in Zhongguancun include Affiliated High School of Peking University and High School Affiliated to Renmin University of China.\n\nThe State Administration of Foreign Experts Affairs (SAFEA) has its headquarters in Zhongguancun.\n\nThe most famous companies that grew up in Zhongguancun are Stone Group, Founder Group, and Lenovo Group. They were all founded in 1984-85. Stone was the first successful technology company to be operated by private individuals outside the government of China. Founder is a technology company that spun off Peking University. Lenovo Group spun off from Chinese Academy of Sciences with Liu Chuanzhi, a hero of Zhongguancun and current Chairmain, eventually taking the helm. Lenovo purchased IBM's PC division with $1.75 billion in 2005, making it the world's third-largest PC maker. Both Founder and Lenovo Group maintain strong connections to their academic backers, who are significant shareholders.\n\nAccording to the 2004 Beijing Statistical Yearbook, there are over 12,000 high-tech enterprises throughout Zhongguancun's seven parks, with 489,000 technicians employed.\n\nEastdawn Corporation is in the Sinosteel building.\n\nMany world-renowned technology companies built their Chinese headquarters and research centers in Zhongguancun Technology Park, such as Google, Intel, AMD, Oracle Corporation, Motorola, MySpace, Sony, Solstice, and Ericsson. Microsoft has built its Chinese research headquarters in the park that costs $280 million and can accommodate 5000 employees, which was completed in April, 2011, and now houses Microsoft Research Asia.\n\nThe development center of Loongson, which is China's first general-purpose microprocessor design, is also in the Zhongguancun area.\n\nIn addition, many conferences are held in this location, including the annual ChinICT conference - which is the largest Information technology Development and Entrepreneurship event in China.\n\nBeijing Subway Line 4 runs through the Zhongguancun area with stops at Zhongguancun Station and Haidianhuangzhuang Station. Haidianhuangzhuang is also a transfer station with Line 10. In addition to the subway, Zhongguancun is served by many of Beijing's public buses- 26, 302, 304, 307, 332, 333, 355, 365, 384, 386, 466, 498, 549, 579, 584, 601, 608, 611, 614, 630, 634, 641, 653, 671, 681, 689, 696, 697, 699, 717, 740, 913, 944, 982, 983, 特4, 特6, 特9, 特15, 特18, 夜8, 夜9, 运通105, 运通106, 运通109, 运通110, and 运通113.\n\n\n"}
