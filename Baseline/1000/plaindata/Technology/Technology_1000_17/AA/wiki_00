{"id": "10951179", "url": "https://en.wikipedia.org/wiki?curid=10951179", "title": "3DML", "text": "3DML\n\n3DML was a format for creating three-dimensional websites build up by combining similar sized building blocks. It was invented in 1997 by Michael Powers, who co-developed with Philip Stevens and developed further over the next four years. 3DML files are written in an XML syntax which can be delivered from standard web servers and shown within a browser plugin and independent 3DML browser called Flatland Rover. A new update was posted in 2018 with updated code and binaries for Windows 10. 3DML had no avatar or multi-user support unlike other platforms of the time like Active Worlds thus never attracting a huge number of followers. There was only a plugin for Internet Explorer, Netscape Navigator and AOL, but not for Mozilla Firefox. The most recent version is a standalone Windows application.\n\nA 3DML world was called a \"Spot\". In the spot \"blocks\" can be inserted, laid out in a grid. The blocks can be ordered into \"levels\" - each has the same size. This approach was designed to simplify the building process and comprehension of 3D pages. The following is an example of a full Spot description of a 3D room with walls.\n\nYou can navigate the spots by using either mouse or arrow keys.\n\n\n"}
{"id": "2641938", "url": "https://en.wikipedia.org/wiki?curid=2641938", "title": "Applied physics", "text": "Applied physics\n\nApplied physics is intended for a particular technological or practical use. It is usually considered as a bridge or connection between physics and engineering.\n\n\"Applied\" is distinguished from \"pure\" by a subtle combination of factors, such as the motivation and attitude of researchers and the nature of the relationship to the technology or science that may be affected by the work. Applied physics is rooted in the fundamental truths and basic concepts of the physical sciences, but is concerned with the utilization of scientific principles in practical devices and systems, and in the application of physics in other areas of science. \n\nIt usually differs from engineering in that an applied physicist may not be designing something in particular, but rather is using physics or conducting physics research with the aim of developing new technologies or solving an engineering problem. This approach is similar to that of applied mathematics. \n\nIn other words, applied physics is rooted in the fundamental truths and basic concepts of the physical sciences but is concerned with the utilization of these scientific principles in practical devices and systems.\n\nApplied physicists can also be interested in the use of physics for scientific research. For instance, the field of accelerator physics can contribute to research in theoretical physics by working with engineers enabling design and construction of high-energy colliders.\n\n"}
{"id": "6367125", "url": "https://en.wikipedia.org/wiki?curid=6367125", "title": "Band brake", "text": "Band brake\n\nA band brake is a primary or secondary brake, consisting of a band of friction material that tightens concentrically around a cylindrical piece of equipment to either prevent it from rotating (a static or \"holding\" brake), or to slow it (a dynamic brake). This application is common on winch drums and chain saws and is also used for some bicycle brakes.\n\nA former application was the locking of gear rings in epicyclic gearing. In modern automatic transmissions this task has been taken over entirely by multiple-plate clutches or multiple-plate brakes.\n\nBand brakes can be simple, compact, rugged, and can generate high force with a light input force. However, band brakes are prone to grabbing or chatter and loss of brake force when hot. These problems are inherent with the design and thus limit where band brakes are a good solution.\n\nOne way to describe the effectiveness of the brake is as formula_1, where formula_2 is the coefficient of friction between band and drum, and formula_3 is the angle of wrap. With a large formula_4, the brake is very effective and requires low input force to achieve high brake force, but is also very sensitive to changes in formula_2. For example, light rust on the drum may cause the brake to \"grab\" or chatter, water may cause the brake to slip, and rising temperatures in braking may cause the coefficient of friction to drop slightly but in turn cause brake force to drop greatly. Using a band material with low formula_2 increases the input force required to achieve a given brake force, but some low-formula_2 materials also have more consistent formula_2 across the range of working temperatures.\n\n"}
{"id": "31798863", "url": "https://en.wikipedia.org/wiki?curid=31798863", "title": "Compact surveillance radar", "text": "Compact surveillance radar\n\nCompact surveillance radar are small lightweight radar systems that have a wide coverage area and are able to track people and vehicles in range and azimuth angle. They weigh less than 10 pounds, consume less than 15 Watts of power and are easily deployed in large numbers. \n\nCompact surveillance radar have the same characteristics of the larger Ground Surveillance Radar(GSR) namely; the ability to track many moving targets simultaneously, all weather day & night operation, wide coverage areas and the ability to track targets and cue cameras automatically.\n\nCSR Manufacturers \n"}
{"id": "28193958", "url": "https://en.wikipedia.org/wiki?curid=28193958", "title": "Comparison of digital SLRs", "text": "Comparison of digital SLRs\n\nFollowing list compares main features of digital single-lens reflex cameras (DSLRs). Order of this list should be firstly by manufacturer alphabetically, secondly from high end to low end models.\n\nKey:\n"}
{"id": "1755384", "url": "https://en.wikipedia.org/wiki?curid=1755384", "title": "Comparison of display technology", "text": "Comparison of display technology\n\nThis is a comparison of various properties of different display technologies.\n\nDifferent display technologies have vastly different temporal characteristics, leading to perceptual differences for motion, flicker, etc.\nThe figure shows a sketch of how different technologies present a single white/grey frame. Time and intensity is not to scale. Notice that some have a fixed intensity, while the illuminated period is variable. This is a kind of pulse-width modulation. Others can vary the actual intensity in response to the input signal.\n\n\n"}
{"id": "8725303", "url": "https://en.wikipedia.org/wiki?curid=8725303", "title": "Comparison of high definition optical disc formats", "text": "Comparison of high definition optical disc formats\n\nThis article compares the technical specifications of multiple high definition formats, including HD DVD and Blu-ray Disc; two mutually incompatible, high definition optical disc formats that, beginning in 2006, attempted to improve upon and eventually replace the DVD standard. The two formats remained in a format war until February 19, 2008 when Toshiba, HD DVD's creator, announced plans to cease development, manufacturing and marketing of HD DVD players and recorders.\n\nOther high-definition optical disc formats were attempted, including the multi-layered red-laser Versatile Multilayer Disc and a Chinese made format called EVD. Both appear to have been abandoned by their respective developers.\n\na These maximum storage capacities apply to currently released media as of January 2012. First two layers of Blu-ray have a 25 GB capacity, but the triple layer disc adds a further 50 GB making 100 GB total. The fourth layer adds a further 28 GB. <br>\nb All HD DVD players are required to decode the two primary channels (left and right) of any Dolby TrueHD track; however, every Toshiba made stand-alone HD DVD player released thus far decodes 5.1 channels of TrueHD.<br>\nc On November 1, 2007 Secondary video and audio decoder became mandatory for new Blu-ray Disc players when the Bonus View requirement came into effect. However, players introduced to the market before this date can continue to be sold without Bonus View.<br>\nd There are some differences in the implementation of Dolby Digital Plus (DD+) on the two formats. On Blu-ray Disc, DD+ can only be used to extend a primary Dolby Digital (DD) 5.1 audiotrack. In this method 640 kbit/s is allocated to the primary DD 5.1 audiotrack (which is independently playable on players that do not support DD+), and up to 1 Mbit/s is allocated for the DD+ extension. The DD+ extension is used to replace the rear channels of the DD track with higher fidelity versions, along with adding additional channels for 6.1/7.1 audiotracks. On HD DVD, DD+ is used to encode all channels (up to 7.1), and no legacy DD track is required since all HD DVD players are required to decode DD+.<br>\ne On PAL DVDs, 24 frame per second content is stored as 50 interlaced frames per second and gets replayed 4% faster. This process can be reversed to retrieve the original 24 frame per second content. On NTSC DVDs, 24 frame per second content is stored as 60 interlaced frames per second using a process called 3:2 pulldown, which if done properly can also be reversed.<br>\nf As of July 2008, about 66.7% of Blu-ray discs are region free and 33.3% use region codes.<br>\ng DVD supports any valid MPEG-2 refresh rate as long as it is packaged with metadata converting it to 576i50 or 480i60, This metadata takes the form of REPEAT_FIRST_FIELD instructions embedded in the MPEG-2 stream itself, and is a part of the MPEG-2 standard. HD DVD is the only high-def disc format that can decode 1080p25 while Blu-ray and HD DVD can both decode 1080p24 and 1080p30. 1080p25 content can only be presented on Blu-ray as 1080i50.<br>\nh Linear PCM is the only lossless audio codec that is mandatory for both HD DVD and Blu-ray disc players, only HD DVD players are required to decode two lossless sound formats and those are Linear PCM and Dolby TrueHD. Dolby TrueHD and DTS-HD Master Audio have become sound format of choice for many studios on their Blu-ray titles but ever since Blu-ray won the format war, it has not become clear if they are now Mandatory for all new Blu-ray disc players since the end of the format war.\n\nBlu-ray Disc has a higher maximum disc capacity than HD DVD (50 GB vs. 30 GB for a double layered disc). In September 2007 the DVD Forum approved a preliminary specification for the triple-layer 51GB HD DVD (ROM only) disc though Toshiba never stated whether it was compatible with existing HD DVD players. In September 2006 TDK announced a prototype Blu-ray Disc with a capacity of 200GB. TDK was also the first to develop a Blu-ray prototype with a capacity of 100GB in May 2005. In October 2007 Hitachi developed a Blu-ray prototype with a capacity of 100GB. Hitachi has stated that current Blu-ray drives would only require a few firmware updates in order to play the disc.\n\nThe first 50 GB dual-layer Blu-ray Disc release was the movie \"Click\", which was released on October 10, 2006. As of July 2008, over 95% of Blu-ray movies/games are published on 50 GB dual layer discs with the remainder on 25 GB discs. 85% of HD DVD movies are published on 30 GB dual layer discs, with the remainder on 15 GB discs.\n\nThe choice of video compression technology (codec) complicates any comparison of the formats. Blu-ray Disc and HD DVD both support the same three video compression standards: MPEG-2, VC-1 and AVC, each of which exhibits different bitrate/noise-ratio curves, visual impairments/artifacts, and encoder maturity. Initial Blu-ray Disc titles often used MPEG-2 video, which requires the highest average bitrate and thus the most space, to match the picture quality of the other two video codecs. As of July 2008 over 70% of Blu-ray Disc titles have been authored with the newer compression standards: AVC and VC-1. HD DVD titles have used VC-1 and AVC almost exclusively since the format's introduction. Warner Bros., which used to release movies in both formats prior to June 1, 2007, often used the same encode (with VC-1 codec) for both Blu-ray Disc and HD DVD, with identical results. In contrast, Paramount used different encodings: initially MPEG-2 for early Blu-ray Disc releases, VC-1 for early HD DVD releases, and eventually AVC for both formats.\n\nWhilst the two formats support similar audio codecs, their usage varies. Most titles released on the Blu-ray format include Dolby Digital tracks for each language in the region, a DTS-HD Master Audio track for all 20th Century Fox and Sony Pictures and many upcoming Universal titles, Dolby TrueHD for Disney and Sony Pictures and some Paramount and Warner titles, and for many Blu-ray titles a Linear PCM track for the primary language. On the other hand, most titles released on the HD DVD format include Dolby Digital Plus tracks for each language in the region, and some also include a Dolby TrueHD track for the primary language.\n\nBoth Blu-ray Disc and HD DVD have two main options for interactivity (on-screen menus, bonus features, etc.).\n\nHD DVD's Standard Content is a minor change from standard DVD's subpicture technology, while Blu-ray's BDMV is completely new. This makes transitioning from standard DVD to Standard Content HD DVD relatively simple —for example, Apple's DVD Studio Pro has supported authoring Standard Content since version 4.0.3. For more advanced interactivity Blu-ray disc supports BD-J while HD DVD supports Advanced Content.\n\nBlu-ray Discs contain their data relatively close to the surface (less than 0.1 mm) which combined with the smaller spot size presents a problem when the surface is scratched as data would be destroyed. To overcome this, TDK, Sony, and Panasonic each have developed a proprietary scratch resistant surface coating. TDK trademarked theirs as Durabis, which has withstood direct abrasion by steel wool and marring with markers in tests.\n\nHD DVD uses traditional material and has the same scratch and surface characteristics of a regular DVD. The data is at the same depth (0.6 mm) as DVD as to minimize damage from scratching. As with DVD the construction of the HD DVD allows for a second side of either HD DVD or DVD.\n\nA study performed by Home Media Magazine (August 5, 2007) concluded that HD DVDs and Blu-ray discs are essentially equal in production cost. Quotes from several disc manufacturers for 25,000 units of HD DVDs and Blu-rays revealed a price differential of only 5-10 cents. (Lowest price: 90 cents versus 100 cents. Highest price: $1.45 versus $1.50.) Another study performed by Wesley Tech (February 9, 2007) arrived at a similar conclusion. Quotes for 10,000 discs show that a 15 gigabyte HD DVD costs $11,500 total, and 25 gigabyte Blu-ray or a 30 gigabyte HD DVD costs $13,000 total. For larger quantities of 100,000 units, the 30 gigabyte HD DVD was more expensive than the 25 gigabyte Blu-ray ($1.55 versus $1.49).\n\nAt the Consumer Electronics Show, on , Warner Bros. introduced a hybrid technology, Total HD, which would reportedly support both formats on a single disc. The new discs were to overlay the Blu-ray and HD DVD layers, placing them respectively and beneath the surface. The Blu-ray top layer would act as a two-way mirror, reflecting just enough light for a Blu-ray reader to read and an HD DVD player to ignore.\n\nLater that year, however, in September 2007, Warner President Ron Sanders said that the technology was on hold due to Warner being the only company who would publish on it.\n\nOne year after the original announcement, on 4 January 2008, Warner Bros. stated that it would support the Blu-ray format exclusively beginning on 1 June 2008, which, along with the demise of HD DVD the following month, ended development of hybrid discs permanently.\n\nThe primary copy protection system used on both formats is the Advanced Access Content System (AACS). Other copy protection systems include:\n\nThe Blu-ray specification and all currently available players support region coding. As of July 2008 about 66.7% of Blu-ray Disc titles are region-free and 33.3% use region codes.\n\nThe HD DVD specification had no region coding, so a HD DVD from anywhere in the world will work in any player. The DVD Forum's steering committee discussed a request from Disney to add it, but many of the 20 companies on the committee actively opposed it.\n\nSome film titles that were exclusive to Blu-ray in the United States such as Sony's \"xXx\", Fox's \"\" and \"The Prestige\", were released on HD DVD in other countries due to different distribution agreements; for example, \"The Prestige\" was released outside the U.S. by once format-neutral studio Warner Bros. Pictures. Since HD DVDs had no region coding, there are no restrictions playing foreign-bought HD DVDs in an HD DVD player.\n"}
{"id": "8605573", "url": "https://en.wikipedia.org/wiki?curid=8605573", "title": "Comparison of movie cameras", "text": "Comparison of movie cameras\n\nThe 35 mm film gauge has long been the most common gauge in professional usage, and thus enjoys the greatest number of cameras currently available for professional usage. The modern era of 35 mm cameras dates to 1972, when Arri's Arriflex 35BL and Panavision's original Panaflex models emerged as the first self-blimped, lightweight cameras. Another distinguishing characteristic of modern cameras is the adoption of stronger lens mount seatings secured with a breech lock – namely the Arri PL and PV mount, both of which were designs descended from the BNCR mount of Mitchell cameras.\n\n\n\n\n\n16 mm film occupies a rather curious position within filmmaking – with a wide range encompassing virtually every field – amateur home movies, student films, experimental films, television work, commercials, music videos, corporate films, industrial research, medical applications, and lower budget features. Its robust image quality in relation to its size allows for a much more versatile, accessible, and affordable usage in many fields where neither 35 mm nor Super 8 would be well-suited. Despite current challenges from the burgeoning digital video market, the consistent improvement of cameras, lenses, and film stocks have enabled the Super 16 format to flourish recently, with many labs reporting increased usage. The modern era of 16 mm cameras is concurrent with that of 35 mm for both the same reasons as 35 mm as well as an additional change: the creation of the Super 16 format by Rune Ericsson in 1971. The format expanded the usable film negative horizontally, which required a larger film gate and necessitated either specialized conversion of machined parts or purchase of new cameras designed with Super 16 gates. Since the format took more than a decade to slowly standardize, the competition from both high and low end video cameras has decimated the demand for 16 mm cameras for most non-professional usage. Therefore, there are relatively few Super 16 cameras, although most are considered professional-grade.\n\n\n"}
{"id": "2030477", "url": "https://en.wikipedia.org/wiki?curid=2030477", "title": "Connections (TV series)", "text": "Connections (TV series)\n\nConnections is a 10-episode documentary television series and 1978 book (\"Connections\", based on the series) created, written, and presented by science historian James Burke. The series was produced and directed by Mick Jackson of the BBC Science and Features Department and first aired in 1978 (UK) and 1979 (USA). It took an interdisciplinary approach to the history of science and invention, and demonstrated how various discoveries, scientific achievements, and historical world events were built from one another successively in an interconnected way to bring about particular aspects of modern technology. The series was noted for Burke's crisp and enthusiastic presentation (and dry humour), historical re-enactments, and intricate working models.\n\nThe popular success of the series led to the production of \"The Day the Universe Changed\" (1985), a similar program but showing a more linear history of several important scientific developments. Years later, the success in syndication led to two sequels, \"Connections\" (1994) and \"Connections\" (1997), both for TLC. In 2004, KCSM-TV produced a program called \"Re-Connections\", consisting of an interview of Burke and highlights of the original series, for the 25th anniversary of the first broadcast in the USA on PBS.\n\n\"Connections\" explores an \"Alternative View of Change\" (the subtitle of the series) that rejects the conventional linear and teleological view of historical progress. Burke contends that one cannot consider the development of any particular piece of the modern world in isolation. Rather, the entire gestalt of the modern world is the result of a web of interconnected events, each one consisting of a person or group acting for reasons of their own motivations (e.g., profit, curiosity, religion) with no concept of the final, modern result to which the actions of either them or their contemporaries would lead. The interplay of the results of these isolated events is what drives history and innovation, and is also the main focus of the series and its sequels.\n\nTo demonstrate this view, Burke begins each episode with a particular event or innovation in the past (usually ancient or medieval times) and traces the path from that event through a series of seemingly unrelated connections to a fundamental and essential aspect of the modern world. For example, the episode \"The Long Chain\" traces the invention of plastics from the development of the fluyt, a type of Dutch cargo ship.\n\nBurke also explores three corollaries to his initial thesis. The first is that, if history is driven by individuals who act only on what they know at the time, and not because of any idea as to where their actions will eventually lead, then predicting the future course of technological progress is merely conjecture. Therefore, if we are astonished by the connections Burke is able to weave among past events, then we will be equally surprised to what the events of today eventually will lead, especially events of which we were not even aware at the time.\n\nThe second and third corollaries are explored most in the introductory and concluding episodes, and they represent the downside of an interconnected history. If history progresses because of the synergistic interaction of past events and innovations, then as history does progress, the number of these events and innovations increases. This increase in possible connections causes the process of innovation to not only continue, but also to accelerate. Burke poses the question of what happens when this rate of innovation, or more importantly 'change' itself, becomes too much for the average person to handle, and what this means for individual power, liberty, and privacy.\n\nLastly, if the entire modern world is built from these interconnected innovations, all increasingly maintained and improved by specialists who required years of training to gain their expertise, what chance does the average citizen without this extensive training have in making an informed decision on practical technological issues, such as the building of nuclear power plants or the funding of controversial projects such as stem cell research? Furthermore, if the modern world is increasingly interconnected, what happens when one of those nodes collapses? Does the entire system follow suit?\n\nThe original 1978 Connections 10-episode documentary television series and had a companion book (\"Connections\", based on the series) created, written, and presented by science historian James Burke. The 1978 Connections companion book was published about the time the middle of the series was airing, so likely was written in parallel to the series and had a post-production editing release. The very popular book was re-released as a work in a 1995 edition, 1998, (relations to sections below is unknown.) and again in 2007 as both hardcover or softcover editions. Since the television series varied in content with each corresponding production run and release, it is likely the companion volumes (as is suggest by the plethora of ISBN codes) are also different works. This 1978 work's coverage deviates in some topics and details being both more in depth and a bit broader, from the lighter coverage of the episodes. It can be found in many libraries. \n\n\n\nAll three \"Connections\" documentaries have been released in their entirety as DVD box sets in the US. The ten episodes of series one were released in Europe (Region 2) on 6 February 2017.\n\nBurke also wrote a series of \"Connections\" articles in \"Scientific American\", and published a book of the same name (1995, ), all built on the same theme of exploring the history of science and ideas, going back and forth through time explaining things on the way and, generally, coming back to the starting point.\n\nBurke produced another documentary series called \"The Day the Universe Changed\" in 1985, which explored man's concept of how the universe worked in a manner similar to the original \"Connections\".\n\n\"\" is a video series launched in 2011 on Kickstarter.com that was inspired by James Burke's \"Connections\". However, it follows concepts rather than inventions through time.\n\n\"Richard Hammond's Engineering Connections\", shown on BBC2, follows a similar format.\n\n\"Connections\", a \"Myst\"-style computer game with James Burke and others providing video footage and voice acting, was released in 1995. It was a runner-up for \"Computer Gaming World\"s award for the best \"Classics/Puzzles\" game of 1995, which ultimately went to \"You Don't Know Jack\". The editors wrote of \"Connections\", \"That you enjoy yourself so much you hardly realize that you're learning is a tribute to the design.\"\n\nA clip from the episode \"Yesterday, Tomorrow and You\" appears in the 2016 video game The Witness.\n\n"}
{"id": "45533018", "url": "https://en.wikipedia.org/wiki?curid=45533018", "title": "CougarTech", "text": "CougarTech\n\nCougarTech, FRC team 2228, is a FIRST Robotics Competition team that was founded in 2007, and is a school-based team from the Honeoye Falls-Lima Central School District in Honeoye Falls, New York. The team also represents the Rush–Henrietta Central School District in competition. CougarTech is a veteran team that prides itself on its coopertition with other teams as well as its teaching of younger teams. During the all important six weeks of build season, the team builds a robot to play the year's new game that FIRST designs each year. During the off season, the team focuses work on public outreach, recruitment, fundraising and sponsorship to support their team and FIRST. \n\n\n"}
{"id": "47887859", "url": "https://en.wikipedia.org/wiki?curid=47887859", "title": "Cycle of quantification/qualification", "text": "Cycle of quantification/qualification\n\nCycle of quantification/qualification (C) is a parameter used in real-time polymerase chain reaction techniques, indicating the cycle number where a PCR amplification curve meets a predefined mathematical criterion. A C may be used for quantification of the target sequence or to determine whether the target sequence is present or not.\n\nTwo criteria to determine the C are used by different thermocyclers:\nCycle of Threshold (CT) is the number of cycles required for the fluorescent signal to cross a given value threshold. Usually, the threshold is set above the baseline, about 10 times the standard deviation of the noise of the baseline, to avoid random effects on the CT. However, the threshold shouln't be set much higher than that to avoid reduced reproducibility due to uncontrolled factors. \nCrossing point (Cp) and Take off point (TOP) are the cycle value of the maximum second derivative of the amplification curve.\n"}
{"id": "300595", "url": "https://en.wikipedia.org/wiki?curid=300595", "title": "Datasheet", "text": "Datasheet\n\nA datasheet, data sheet, or spec sheet is a document that summarizes the performance and other technical characteristics of a product, machine, component (e.g., an electronic component), material, a subsystem (e.g., a power supply) or software in sufficient detail that allows design engineer to understand the role of the component in the overall system. Typically, a datasheet is created by the manufacturer and begins with an introductory page describing the rest of the document, followed by listings of specific characteristics, with further information on the connectivity of the devices. In cases where there is relevant source code to include, it is usually attached near the end of the document or separated into another file.\n\nDepending on the specific purpose, a datasheet may offer an average value, a typical value, a typical range, engineering tolerances, or a nominal value. The type and source of data are usually stated on the datasheet.\n\nA datasheet is usually used for technical communication to describe technical characteristics of an item or product. It can be published by the manufacturer to help people choose products or to help use the products. By contrast, a technical specification is an explicit set of requirements to be satisfied by a material, product, or service.\n\nAn electronic datasheet specifies characteristics in a formal structure that allows the information to be processed by a machine. Such machine readable descriptions can facilitate information retrieval, display, design, testing, interfacing, verification, and system discovery. Examples include transducer electronic data sheets for describing sensor characteristics, and Electronic device descriptions in CANopen or descriptions in markup languages, such as SensorML.\n\nExamples include:\n\nA typical datasheet for an electronic component contains most of the following information: \n\n\nAlthough a datasheet may include a \"typical use\" circuit diagram, as well as programming examples in the case of programmable devices, this sort of information is often published in a separate application note, with a high level of detail.\n\nHistorically, datasheets were typically available in a databook that contained many datasheets, usually grouped by manufacturer or general type. Today, they are also available through the Internet in table form or via downloadable (usually PDF) documents.\n\nA Material Safety Data Sheet (MSDS), Safety Data Sheet (SDS), or Product Safety Data Sheet (PSDS) is an important component of product stewardship and occupational safety and health. These are required by agencies such as OSHA in its Hazard Communication Standard, 29 C.F.R. 1910.1200. It provides workers with ways to allow them to work in a safe manner and gives them physical data (melting point, boiling point, flash point, etc.), toxicity, health effects, first aid, reactivity, storage, disposal, protective equipment, and spill-handling procedures. The MSDSs differ from country to country, as different countries have different regulations. In some jurisdictions, it is compulsory for the SDS to state the chemical's risks, safety, and effect on the environment.\n\nThe SDSs are a commonly used classification for logging information on chemicals, chemical compounds, and chemical mixtures. The SDSs often include the safe use of the chemical and the hazardous nature of the chemical. Anytime chemicals are used these data sheets will be found.\n\nThere is a need to have an internationally recognized symbol when describing hazardous substances. Labels can include hazard symbols such as the European Union standard black diagonal cross on an orange background, used to denote a harmful substance.\n\nThe purpose of an SDS is not so that the general public will have a knowledge of how to read and understand it, but more so that it can be used in an occupational setting to allow workers to be able to work with it.\n\nData sheets and pages are available for specific properties of chemicals in Chemical elements data references: example, Melting points of the elements (data page). Specific materials have technical data in individual sheets such as Ethanol (data page): this includes subjects such as structure and properties, thermodynamic properties, spectral data, vapor pressure, etc. Other chemical data sheets are available from individual producers of chemicals, often on their web pages.\n\nData sheets for automobiles may be described under several names such as features, specs, engineering data, technical summary, etc. They help communicate the technical information about a car to potential buyers and are useful for comparisons with similar cars. They might include: critical inside and outside dimensions, weight, fuel efficiency, engine and drive train, towing capability, safety features and options, warranty, etc.\n\n\n"}
{"id": "41368073", "url": "https://en.wikipedia.org/wiki?curid=41368073", "title": "Department of Science (1972–75)", "text": "Department of Science (1972–75)\n\nThe Department of Science was an Australian government department that existed between December 1972 and June 1975.\n\nInformation about the department's functions and/or government funding allocation could be found in the Administrative Arrangements Orders, the annual Portfolio Budget Statements and in the Department's annual reports.\n\nAccording to the Administrative Arrangements Order issued 19 December 1972, at its creation, the Department was responsible for:\n\nThe Department was an Australian Public Service department, staffed by officials who were responsible to the Minister for Science.\n\nThe Secretary of the Department was Hugh Ennor.\n"}
{"id": "564695", "url": "https://en.wikipedia.org/wiki?curid=564695", "title": "Discrete system", "text": "Discrete system\n\nA discrete system is a system with a countable number of states. Discrete systems may be contrasted with continuous systems, which may also be called analog systems. A final discrete system is often modeled with a directed graph and is analyzed for correctness and complexity according to computational theory. Because discrete systems have a countable number of states, they may be described in precise mathematical models.\n\nA computer is a finite state machine that may be viewed as a discrete system. Because computers are often used to model not only other discrete systems but continuous systems as well, methods have been developed to represent real-world continuous systems as discrete systems. One such method involves sampling a continuous signal at discrete time intervals.\n\n\n"}
{"id": "48587054", "url": "https://en.wikipedia.org/wiki?curid=48587054", "title": "Dochub", "text": "Dochub\n\nDocHub is an online PDF editor and document signing platform. DocHub lets users add text, draw, add signatures and make document templates. DocHub can integrate with Dropbox, Google Drive, Gmail and Box accounts.\n\nAs of April 2017, DocHub has at least 18.1 million registered users. Over 68 million documents have been created on DocHub.\n\n"}
{"id": "23021037", "url": "https://en.wikipedia.org/wiki?curid=23021037", "title": "E-OTD", "text": "E-OTD\n\nEnhanced Observed Time Difference (E-OTD) is a standard for the location of mobile telephones. The location method works by multilateration. The standardisation was first carried out for GSM by the GSM standard committees (T1P1.5 and ETIS) in LCS Release 98 and Release 99. The standardisation was continued for 3G and WCDMA mobile telephones by 3GPP.\n\nConceptually, the method is similar to U-TDOA, however, it involves time difference measurements being made in the handset rather than the network, and a mechanism to pseudo-synchronise the network. The handset makes an observation of the time difference of arrival of signals from two different base stations. These observations are known as Observed Time Difference (OTD). The handset measures the OTD between a number of different base stations. If the base stations were synchronised, then a single OTD defines a hyperbolic locus. A second, independent OTD, for which one of the observed base stations is spatially distinct from those in the first OTD, would provide a second hyperbolic locus, and the intersection of the two loci gives an estimate of the location of the mobile. If more than two independent OTDs are available, then the measurements can be combined to yield a more accurate measurement.\n\nHowever, GSM and 3G networks are not necessarily synchronised, so further information is needed. The E-OTD standard provides a method for pseudo-synchronisation. A Location Measurement Unit (LMU) can be used to estimate the transmission time offset between two base stations. This measurement is known as the Real Time Difference (RTD). The RTD for two base stations can then be subtracted from the OTD for the same two base stations to produce the Geometric Time Difference (GTD). The GTD is the time difference that would have been measured by the mobile if the network was perfectly synchronised. Accordingly, the application of the RTD provides a pseudo-synchronisation.\n\nAn LMU is a receiver that is placed in a position in the network that is able to report the RTDs of a number of different base stations. If the base station clocks are not synchronised to a common source, then it is necessary to continuously update the RTDs, as the time offsets will be changing due to the clock drift in each base station.\n\nThe deployment of LMUs can be expensive, and so is a drawback of E-OTD. However, a 2003 paper describes a method of operating E-OTD without LMUs, and presents results of an operational trial. In essence, if there are sufficient independent OTD measurements such that the equation system is over-determined, then the additional information can be used to estimate the RTDs.\n\nE-OTD was considered for the Enhanced 911 mandate, but ultimately was not a successful contender for this application. An active proponent and developer of E-OTD was Cambridge Positioning Systems (CPS). In 2007, CPS was acquired by CSR. In 2009, CSR merged with SIRF.\n\nBecause E-OTD requires a software modification to be included in the mobile phone, E-OTD positioning system has been less commonly used than the U-TDOA positioning system.\n\n\n"}
{"id": "16750340", "url": "https://en.wikipedia.org/wiki?curid=16750340", "title": "Ebara Corporation", "text": "Ebara Corporation\n\nEbara Corporation is a publicly traded manufacturing company based in Tokyo, Japan which makes environmental and industrial machinery such as pumps and turbines. It is the owner of the Elliott Company in the United States and Sumoto S.r.l. in Italy.\n\nEbara is divided into three main divisions:\n\n\n"}
{"id": "23048878", "url": "https://en.wikipedia.org/wiki?curid=23048878", "title": "EchoStar Mobile", "text": "EchoStar Mobile\n\nEchoStar Mobile was set up in 2008 as Solaris Mobile, a joint venture company between SES and Eutelsat Communications to develop and commercialize the first geostationary satellite systems in Europe for broadcasting video, radio and data to in-vehicle receivers and to mobile devices, such as mobile phones, portable media players and PDAs. In January 2014 all stock in Solaris Mobile was acquired by EchoStar Corporation and in March 2015 the company was renamed EchoStar Mobile.\n\nThe agreement to set up Solaris Mobile was reached in 2006 with the company formed in 2008. SES and Eutelsat – both successful European satellite operators, providing TV and other services from geostationary satellites to millions of cable and direct-to-home viewers – invested €130m in the venture. The services to be developed included video, radio, multimedia data, interactive services, and voice communications, with the primary aim of delivering mobile television any time, anywhere. Its headquarters is in Dublin, in the Republic of Ireland.\n\nSolaris Mobile's first commercial contract was with Italian media publishing group Class Editori, to launch a digital radio service in Italy. A hybrid satellite/terrestrial network will initially be deployed in Milan, in October 2011 and extended across the country in 2012. Solaris claims that the network will enable Italians to access dozens of new digital radio channels broadcasting music, news, entertainment and sports, in their original format with continuity of reception across the entire country, and that the digital audio signal will be complemented with new visual media services such as programme information and traffic data.\n\nThe EU Telecoms Commissioner, Viviane Reding, has commented, \"Mobile satellite services have huge potential: they can enable Europeans to access new communication services, particularly in rural and less populated regions.\"\n\nSolaris Mobile primarily intends to provide mobile TV and interactive services to handheld and vehicle receivers. For in-vehicle use, the mobile satellite receivers could also double as web browsers providing full Internet access, and deliver interactive services such as online reservations, emergency warnings, or toll payments.\n\nSolaris claims the technology brings a \"fully-fledged TV experience\" to mobile television, unavailable with purely terrestrial systems, delivering high-quality live TV, viable at high vehicle speeds, dozens of TV channels per country, and universal coverage.\n\nThe coverage across Europe will also enable the system to be used for situations when other means of communication are not possible, such as gathering data (traffic, weather, pollution) from moving vehicles, and support for emergency and rescue services in isolated regions, under extreme conditions or when terrestrial networks have been compromised.\n\nTo avoid the requirement for mobile phones with S-band reception for the satellite services, Solaris Mobile has developed a 'Pocket Gateway' in conjunction with Finnish company Elekrobit. The Gateway is a compact S-band receiver which decodes DVB-SH transmissions from the Solaris satellite and relays them over WiFi to any compatible handset with a web browser. The Gateway is also planned to be used in vehicles with a roof-mounted antenna for S-band reception with services accessed on passengers' mobile phones. The technology was demonstrated at the GSMA Mobile World Congress in Barcelona in February 2010.\n\nThe Solaris Mobile services use DVB-SH technology to deliver IP based data and media content to handheld and in-vehicle terminals using a hybrid satellite/terrestrial system with satellite transmission serving the whole of Europe and beyond, and terrestrial repeaters for urban and indoor penetration.\n\nThe S-band frequencies used (2.00 GHz) are reserved for the exclusive use of satellite and terrestrial mobile services, and sit alongside the UMTS frequencies already in use across Europe for 3G terrestrial mobile phone services, allowing the reuse of existing cellular towers and antennas, and the simple incorporation of Solaris services in mobile handsets.\n\nHandsets equipped with the first DVB-SH chipsets were successfully demonstrated live at the \"Mobile World Congress\" in Barcelona in February 2008.\n\nSolaris was intended to first use the Eutelsat W2A satellite at 10° east, which contains an S-band payload, and was scheduled for launch in early 2009. However, following the successful launch on April 3, 2009, the S-band payload was found to show \"an anomaly\" which has put in doubt the payload's capability to provide mobile satellite services for Solaris.\n\nFurther testing of the satellite was undertaken to establish its future in the Solaris programme. Investigation of S-band payload has confirmed significant non-compliance from its original specifications. On 1 July 2009, Solaris Mobile filed an insurance claim. The technical findings indicate that the company should be able to offer some, but not all of the services it was planning to offer.\n\nOn June 30, 2008 the European Parliament and the Council adopted the European’s Decision to establish a single selection and authorisation process to ensure a coordinated introduction of mobile satellite services (MSS) in Europe. The selection process was launched in August 2008 and attracted four applications by prospective operators (ICO, Inmarsat, Solaris Mobile, TerreStar).\n\nIn May 2009, the European Commission selected two operators, Inmarsat Ventures and Solaris Mobile, giving these operators \"the right to use the specific radio frequencies identified in the Commission's decision and the right to operate their respective mobile satellite systems\". EU Member States now have to ensure that the two operators have the right to use the specific radio frequencies identified in the Commission's decision and the right to operate their respective mobile satellite systems for 18 years from the selection decision. The operators are compelled to start operations within 24 months from the selection decision.\n\nAlthough the EU’s decision was announced days after the apparent failure of the payload intended to serve Solaris, the company remains confident of \"its ability to meet the commitments made under the European Commission selection process\".\n\nIn May 2010, Solaris Mobile announced that following the granting of spectrum by the European Commission, the company had been actively pursuing licenses from European member states and it had just been granted 18-year licences to operate Mobile Satellite Services in France, Sweden and Germany, to add to existing licenses for Finland, Luxembourg, Italy, and Slovenia.\n\n\n"}
{"id": "8518824", "url": "https://en.wikipedia.org/wiki?curid=8518824", "title": "Extensible Forms Description Language", "text": "Extensible Forms Description Language\n\nExtensible Forms Description Language (XFDL) is a high-level computer language that facilitates defining a form as a single, stand-alone object using elements and attributes from the Extensible Markup Language (XML). Technically, it is a class of XML originally specified in a World Wide Web Consortium (W3C) Note. See Specifications below for links to the current versions of XFDL. XFDL It offers precise control over form layout, permitting replacement of existing business/government forms with electronic documents in a human-readable, open standard.\n\nIn addition to precision layout control, XFDL provides multiple page capabilities, step-by-step guided user experiences, and digital signatures. XFDL also provides a syntax for in-line mathematical and conditional expressions and data validation constraints as well as custom items, options, and external code functions. Current versions of XFDL (see Specifications below) are capable of providing these interactive features via open standard markup languages including XForms, XPath, XML Schema and XML Signatures.\n\nXFDL not only supports multiple digital signatures, but the signatures can apply to specific sections of a form and prevent changes to signed content.\n\nThese advantages to XFDL led large organizations such as the United States Army and Air Force to migrate to XFDL from using forms in other formats. Later, though, the lack of portable software capable of creating XFDL led them to investigate moving away from it. The Army migrated to Adobe fillable PDFs in 2014.\n\n\n\n"}
{"id": "32127696", "url": "https://en.wikipedia.org/wiki?curid=32127696", "title": "Fail-silent system", "text": "Fail-silent system\n\nA fail-silent system is a type of system that either provides the correct service, or provides no service at all (becomes silent).\n\n"}
{"id": "56776732", "url": "https://en.wikipedia.org/wiki?curid=56776732", "title": "Fiber optic display", "text": "Fiber optic display\n\nA fiber optic display is a light-emitting display that uses fiber optics to display images, text, and notification lights. Fiber-optic displays can either be static or dynamic, with the typical lighting source being halogen light bulbs.\n\nStatic fiber optic displays have been commonly used for some types of traffic signals. One common use for static fiber optic displays are lane control lights, which display either a green downward-pointing arrow or a red X to indicate the open/closed status of road lanes.\n\nDynamic fiber optic displays typically display alphanumeric text, and utilize electromechanical shutters to open or close the ends of the fiber strands to display an alphanumeric pixel. These type of displays were commonly used as variable-message signs on highways. Compared to eggcrate displays, dynamic fiber optic displays offered lower energy consumption due to requiring fewer bulbs, and offered improved nighttime legibility. For daytime legibility, they were sometimes combined with flip-disc displays to be reflective in daylight and emissive at night.\n"}
{"id": "31369965", "url": "https://en.wikipedia.org/wiki?curid=31369965", "title": "Frog galvanoscope", "text": "Frog galvanoscope\n\nThe frog galvanoscope was a sensitive electrical instrument used to detect voltage in the late eighteenth and nineteenth centuries. It consists of skinned frog's leg with electrical connections to a nerve. The instrument was invented by Luigi Galvani and improved by Carlo Matteucci.\n\nThe frog galvanoscope, and other experiments with frogs played a part in the dispute between Galvani and Alessandro Volta over the nature of electricity. The instrument is extremely sensitive and continued to be used well into the nineteenth century, even after electromechanical meters came into use.\n\nSynonyms for this device include galvanoscopic frog, frog's leg galvanoscope, frog galvanometer, rheoscopic frog, and frog electroscope. The device is properly called a \"galvanoscope\" rather than \"galvanometer\" since the latter implies accurate measurement whereas a galvanoscope only gives an indication. In modern usage a galvanometer is a sensitive laboratory instrument for measuring current, not voltage. Everyday current meters for use in the field are called ammeters. A similar distinction can be made between electroscopes, electrometers, and voltmeters for voltage measurements.\n\nFrogs were a popular subject of experiment in the laboratories of early scientists. They are small, easily handled, and there is a ready supply. Marcello Malpighi, for instance, used frogs in his study of lungs in the seventeenth century. Frogs were particularly suitable for the study of muscle activity. Especially in the legs, the muscle contractions are readily observed and the nerves are easily dissected out. Another desirable feature for scientists was that these contractions continued after death for a considerable time. Also in the seventeenth century, Leopoldo Caldani and Felice Fontana subjected frogs to electric shocks to test Albrecht von Haller's irritability theory.\n\nLuigi Galvani, a lecturer at the University of Bologna, was researching the nervous system of frogs from around 1780. This research included the muscular response to opiates and static electricity, for which experiments the spinal cord and rear legs of a frog were dissected out together and the skin removed. In 1781, an observation was made while a frog was being so dissected. An electric machine discharged just at the moment one of Galvani's assistants touched the crural nerve of a dissected frog with a scalpel. The frog's legs twitched as the discharge happened. Galvani found that he could make the prepared leg of a frog (see the \"Construction\" section) twitch by connecting a metal circuit from a nerve to a muscle, thus inventing the first frog galvanoscope. Galvani published these results in 1791 in \"De viribus electricitatis\".\n\nAn alternative version of the story of the frog response at a distance has the frogs being prepared for a soup on the same table as a running electric machine. Galvani's wife notices the frog twitch when an assistant accidentally touches a nerve and reports the phenomenon to her husband. This story originates with Jean-Louis Alibert and, according to Piccolino and Bresadola, was probably invented by him.\n\nGalvani, and his nephew Giovanni Aldini, used the frog galvanoscope in their electrical experiments. Carlo Matteucci improved the instrument and brought it to wider attention. Galvani used the frog galvanoscope to investigate and promote the theory of \"animal electricity\", that is, that there was a vital life force in living things that manifested itself as a new kind of electricity. Alessandro Volta opposed this theory, believing that the electricity that Galvani and other proponents were witnessing was due to metal contact electrification in the circuit. Volta's motivation in inventing the voltaic pile (the forerunner of the common zinc–carbon battery) was largely to enable him to construct a circuit entirely with non-biological material to show that the vital force was not necessary to produce the electrical effects seen in animal experiments. Matteucci, in answer to Volta, and to show that metal contacts were not necessary, constructed a circuit entirely out of biological material, including a frog battery. Neither the animal electricity theory of Galvani nor the contact electrification theory of Volta forms part of modern electrical science. However, Alan Hodgkin in the 1930s showed that there is indeed an ionic current flowing in nerves.\n\nMatteucci used the frog galvanoscope to study the relationship of electricity to muscles, including in freshly amputated human limbs. Matteucci concluded from his measurements that there was an electric current continually flowing from the interior, to the exterior of all muscles. Matteucci's idea was widely accepted by his contemporaries, but this is no longer believed and his results are now explained in terms of injury potential.\n\nAn entire frog's hind leg is removed from the frog's body with the sciatic nerve still attached, and possibly also a portion of the spinal cord. The leg is skinned, and two electrical connections are made. These may be made to the nerve and the foot of the frog's leg by wrapping them with metal wire or foil, but a more convenient instrument is Matteucci's arrangement shown in the image. The leg is placed in a glass tube with just the nerve protruding. Connection is made to two different points on the nerve.\n\nAccording to Matteucci, the instrument is most accurate if direct electrical contact with muscle is avoided. That is, connections are made only to the nerve. Matteucci also advises that the nerve should be well stripped and that contacts to it can be made with wet paper in order to avoid using sharp metal probes directly on the nerve.\n\nWhen the frog's leg is connected to a circuit with an electric potential, the muscles will contract and the leg will twitch briefly. It will twitch again when the circuit is broken. The instrument is capable of detecting extremely small voltages, and could far surpass other instruments available in the first half of the nineteenth century, including the electromagnetic galvanometer and the gold-leaf electroscope. For this reason, it remained popular long after other instruments became available. The galvanometer was made possible in 1820 by the discovery by Hans Christian Ørsted that electric currents would deflect a compass needle, and the gold-leaf electroscope was even earlier (Abraham Bennet, 1786). Yet Golding Bird could still write in 1848 that \"the irritable muscles of a frog's legs were no less than 56,000 times more delicate a test of electricity than the most sensitive condensing electrometer.\" The word \"condenser\" used by Bird here means a coil, so named by Johann Poggendorff by analogy with Volta's term for a capacitor.\n\nThe frog galvanoscope can be used to detect the direction of electric current. A frog's leg that has been somewhat desensitised is needed for this. The sensitivity of the instrument is greatest with a freshly prepared leg and then falls off with time, so an older leg is best for this. The response of the leg is greater to currents in one direction than the other and with a suitably desensitised leg it may only respond to currents in one direction. For a current going into the leg from the nerve, the leg will twitch on making the circuit. For a current passing out of the leg, it will twitch on breaking the circuit.\n\nThe major drawback of the frog galvanoscope is that the frog leg frequently needs replacing. The leg will continue to respond for up to 44 hours, but after that a fresh one must be prepared.\n\n"}
{"id": "53431368", "url": "https://en.wikipedia.org/wiki?curid=53431368", "title": "Funzing", "text": "Funzing\n\nFunzing is an online sharing economy marketplace that facilitates people to host and attend events and experiences in their leisure time based on their hobbies, passions or skills. Funzing was founded in Israel in 2014 by \"Avigur Zmora\", the former CEO of Playtech. Funzing is currently active in London, Manchester, Tel Aviv and Singapore.\nThe online community marketplace promotes diversity in people’s free time activities, experiences vary from supper clubs, tours, workshops to one-off lectures and classes.\n\nAccording to co-founder \"Yaron Saghiv\", the idea about the online sharing economy came out of several friends’ frustration at being bored by going to the same places every time they wanted to spend quality free time in a social setting. Thus, they devised the platform which allows anyone with creative potential to showcase their skills to the world while entertaining others and earning some money along the way.\nFunzing was founded in Israel in 2014. It was the brainchild of Avigur Zmora, former CEO of Playtech, who went on to set up the company together with Eran Alon, Noa Moscati and Yaron Saghiv.\nThe initial investment capital to start Funzing was raised from venture capital firm \"Inimiti\" and amounted to $1.5 million.\nCurrently, there are over 2,000 registered hosts on the Funzing system, and more than 35,000 individuals have attended events as of June 2016.\n"}
{"id": "482371", "url": "https://en.wikipedia.org/wiki?curid=482371", "title": "Glass cockpit", "text": "Glass cockpit\n\nA glass cockpit is an aircraft cockpit that features electronic (digital) flight instrument displays, typically large LCD screens, rather than the traditional style of analog dials and gauges. While a traditional cockpit (nicknamed a \"steam cockpit\" within aviation circles) relies on numerous mechanical gauges to display information, a glass cockpit uses several displays driven by flight management systems, that can be adjusted (multi-function display) to display flight information as needed. This simplifies aircraft operation and navigation and allows pilots to focus only on the most pertinent information. They are also popular with airline companies as they usually eliminate the need for a flight engineer, saving costs. In recent years the technology has also become widely available in small aircraft.\n\nAs aircraft displays have modernized, the sensors that feed them have modernized as well. Traditional gyroscopic flight instruments have been replaced by electronic attitude and heading reference systems (AHRS) and air data computers (ADCs), improving reliability and reducing cost and maintenance. GPS receivers are usually integrated into glass cockpits.\n\nEarly glass cockpits, found in the McDonnell Douglas MD-80/90, Boeing 737 Classic, 757 and 767-200/-300, ATR 42, ATR 72 and in the Airbus A300-600 and A310, used Electronic Flight Instrument Systems (EFIS) to display attitude and navigational information only, with traditional mechanical gauges retained for airspeed, altitude, vertical speed, and engine performance. Later glass cockpits, found in the Boeing 737NG, 747-400, 767-400, 777, A320 and later Airbuses, Ilyushin Il-96 and Tupolev Tu-204 have completely replaced the mechanical gauges and warning lights in previous generations of aircraft. While glass cockpit-equipped aircraft throughout the late 20th century still retained analog altimeters, attitude, and airspeed indicators as standby instruments in case the EFIS displays failed, more modern aircraft have been increasingly been using digital standby instruments as well, such as the Integrated standby instrument system.\n\nGlass cockpits originated in military aircraft in the late 1960s and early 1970s; an early example is the Mark II avionics of the F-111D (first ordered in 1967, delivered from 1970–73), which featured a multi-function display.\n\nPrior to the 1970s, air transport operations were not considered sufficiently demanding to require advanced equipment like electronic flight displays. Also, computer technology was not at a level where sufficiently light and powerful circuits were available. The increasing complexity of transport aircraft, the advent of digital systems and the growing air traffic congestion around airports began to change that.\n\nThe Boeing 2707 was one of the earliest commercial aircraft designed with a glass cockpit. Most cockpit instruments were still analog, but CRT displays were to be used for the Attitude indicator and HSI. However, the 2707 was cancelled in 1971 after insurmountable technical difficulties and ultimately the end of project funding by the US government.\n\nThe average transport aircraft in the mid-1970s had more than one hundred cockpit instruments and controls, and the primary flight instruments were already crowded with indicators, crossbars, and symbols, and the growing number of cockpit elements were competing for cockpit space and pilot attention. As a result, NASA conducted research on displays that could process the raw aircraft system and flight data into an integrated, easily understood picture of the flight situation, culminating in a series of flights demonstrating a full glass cockpit system.\n\nThe success of the NASA-led glass cockpit work is reflected in the total acceptance of electronic flight displays beginning with the introduction of the MD-80 in 1979. Airlines and their passengers alike have benefited. The safety and efficiency of flights have been increased with improved pilot understanding of the aircraft's situation relative to its environment (or \"situational awareness\").\n\nBy the end of the 1990s, liquid-crystal display (LCD) panels were increasingly favored among aircraft manufacturers because of their efficiency, reliability and legibility. Earlier LCD panels suffered from poor legibility at some viewing angles and poor response times, making them unsuitable for aviation. Modern aircraft such as the Boeing 737 Next Generation, 777, 717, 747-400ER, 747-8F 767-400ER, 747-8, and 787, Airbus A320 family (later versions), A330 (later versions), A340-500/600, A340-300 (later versions), A380 and A350 are fitted with glass cockpits consisting of LCD units.\n\nThe glass cockpit has become standard equipment in airliners, business jets, and military aircraft. It was fitted into NASA's Space Shuttle orbiters \"Atlantis\", \"Columbia\", \"Discovery\", and \"Endeavour\", and the current Russian Soyuz TMA model spacecraft that was launched in 2002. By the end of the century glass cockpits began appearing in general aviation aircraft as well. In 2003, Cirrus Design's SR20 and SR22 became the first light aircraft equipped with glass cockpits, which they made standard on all Cirrus aircraft. By 2005, even basic trainers like the Piper Cherokee and Cessna 172 were shipping with glass cockpits as options (which nearly all customers chose), as well as many modern aircraft such as the Diamond DA42 twin-engine travel and training aircraft. The Lockheed Martin F-35 Lightning II features a \"panoramic cockpit display\" touchscreen that replaces most of the switches and toggles found in an aircraft cockpit. The civilian Cirrus Vision SF50 now has the same, which they call a \"Perspective Touch\" glass cockpit.\n\nUnlike the previous era of glass cockpits—where designers merely copied the look and feel of conventional electromechanical instruments onto cathode ray tubes—the new displays represent a true departure. They look and behave very similarly to other computers, with windows and data that can be manipulated with point-and-click devices. They also add terrain, approach charts, weather, vertical displays, and 3D navigation images.\n\nThe improved concepts enable aircraft makers to customize cockpits to a greater degree than previously. All of the manufacturers involved have chosen to do so in one way or another—such as using a trackball, thumb pad or joystick as a pilot-input device in a computer-style environment. Many of the modifications offered by the aircraft manufacturers improve situational awareness and customize the human-machine interface to increase safety.\n\nModern glass cockpits might include Synthetic Vision (SVS) or Enhanced Vision systems (EVS). Synthetic Vision systems display a realistic 3D depiction of the outside world (similar to a flight simulator), based on a database of terrain and geophysical features in conjunction with the attitude and position information gathered from the aircraft navigational systems. Enhanced Vision systems add real-time information from external sensors, such as an infrared camera.\n\nAll new airliners such as the Airbus A380, Boeing 787 and private jets such as Bombardier Global Express and Learjet use glass cockpits.\n\nMany modern general aviation aircraft are available with glass cockpits. Systems such as the Garmin G1000 are now available on many new GA aircraft, including the classic Cessna 172. Many small aircraft can also be modified post-production to replace analogue instruments.\n\nGlass cockpits are also popular as a retrofit for older private jets and turboprops such as Dassault Falcons, Raytheon Hawkers, Bombardier Challengers, Cessna Citations, Gulfstreams, King Airs, Learjets, Astras, and many others. Aviation service companies work closely with equipment manufacturers to address the needs of the owners of these aircraft.\n\nToday, smartphones and tablets use mini-applications, or \"apps,\" to remotely control complex devices, by WiFi radio interface. They demonstrate how the \"glass cockpit\" idea is being applied to consumer devices. Applications include toy-grade UAVs which use the display and touch screen of a tablet or smartphone to employ every aspect of the \"glass cockpit\" for instrument display, and fly-by-wire for aircraft control.\n\nThe glass cockpit idea made news in 1980s trade magazines, like Aviation Week & Space Technology, when NASA announced that it would be replacing most of the electro-mechanical flight instruments in the space shuttles with glass cockpit components. The articles mentioned how glass cockpit components had the added benefit of being a few hundred pounds lighter than the original flight instruments and support systems used in the space shuttles.\n\nAs aircraft operation depends on glass cockpit systems, flight crews must be trained to deal with possible failures. The Airbus A320 family has seen fifty incidents where several flight displays were lost.\n\nOn 25 January 2008 United Airlines Flight 731 experienced a serious glass-cockpit blackout, losing half of the ECAM displays as well as all radios, transponders, TCAS, and attitude indicators. The pilots were able to land at Newark Airport without radio contact in good weather and daylight conditions.\n\nAirbus has offered an optional fix, which the US NTSB has suggested to the US FAA as mandatory, but the FAA has yet to make it a requirement. A preliminary NTSB factsheet is available. Due to the possibility of a blackout, glass cockpit aircraft also have an integrated standby instrument system that includes (at a minimum) an artificial horizon, altimeter and airspeed indicator. It is electronically separate from the main instruments and can run for several hours on a backup battery.\n\nIn 2010, the NTSB published a study done on 8,000 general aviation light aircraft. The study found that, although aircraft equipped with glass cockpits had a lower overall accident rate, they also had a larger chance of being involved in a fatal accident. The NTSB Chairman said in response to the study:\n\n"}
{"id": "3442409", "url": "https://en.wikipedia.org/wiki?curid=3442409", "title": "Grey literature", "text": "Grey literature\n\nGrey literature (or gray literature) are materials and research produced by organizations outside of the traditional commercial or academic publishing and distribution channels. Common grey literature publication types include reports (annual, research, technical, project, etc.), working papers, government documents, white papers and evaluations. Organizations that produce grey literature include government departments and agencies, civil society or non-governmental organisations, academic centres and departments, and private companies and consultants.\n\nGrey literature may be made available to the public, or distributed privately within organizations or groups, and may lack a systematic means of distribution and collection. The standard of quality, review and production of grey literature can vary considerably. Grey literature may be difficult to discover, access, and evaluate, but this can be addressed through the formulation of sound search strategies.\n\nWhile a hazy definition of \"grey literature\" had existed previously, the term is generally understood to have been coined by the researcher Charles P. Auger, who wrote \"Use of Reports Literature\" in 1975. The literature he referred to consisted of intelligence reports and notes on atomic research produced in vast quantities by the Allied Forces during World War II. In a conference held by the British Lending Library Division in 1978, Auger used the term \"grey literature\" to describe the concept for the first time. His concepts focused upon a \"vast body of documents\", with \"continuing increasing quantity\", that were characterized by the \"difficulty it presents to the librarian\". Auger described the documentation as having great ambiguity between temporary character and durability, and by a growing impact on scientific research. While acknowledging the challenges of reports literature, he recognized that it held a number of advantages \"over other means of dissemination, including greater speed, greater flexibility and the opportunity to go into considerable detail if necessary\". Auger considered reports a \"half-published\" communication medium with a \"complex interrelationship [to] scientific journals\". In 1989 Auger published the second edition of \"The Documentation of the European Communities: A Guide\", which contained the first usage of the term \"grey literature\" in a published work.\n\nThe \"Luxembourg definition\", discussed and approved at the Third International Conference on Grey Literature in 1997, defined grey literature as \"that which is produced on all levels of government, academics, business and industry in print and electronic formats, but which is not controlled by commercial publishers\". In 2004, at the Sixth Conference in New York City, a postscript was added to the definition for purposes of clarification: grey literature is \"...not controlled by commercial publishers, i.e., where publishing is not the primary activity of the producing body\". This definition is now widely accepted by the scholarly community.\n\nThe U.S. Interagency Gray Literature Working Group (IGLWG), in its \"Gray Information Functional Plan\" of 1995, defined grey literature as \"foreign or domestic open source material that usually is available through specialized channels and may not enter normal channels or systems of publication, distribution, bibliographic control, or acquisition by booksellers or subscription agents\". Thus grey literature is usually inaccessible through relevant reference tools such as databases and indexes, which rely upon the reporting of subscription agents.\n\nOther terms used for this material include: report literature, government publications, policy documents, fugitive literature, nonconventional literature, unpublished literature, non-traditional publications, and ephemeral publications. With the introduction of desktop publishing and the Internet, new terms include: electronic publications, online publications, online resources, open-access research, and digital documents.\n\nThough the concept is difficult to define, the term grey literature is an agreed collective term that researchers and information professionals can use to discuss this distinct but disparate group of resources.\n\nIn 2010, D.J. Farace and J. Schöpfel pointed out that existing definitions of grey literature were predominantly economic, and argued that in a changing research environment, with new channels of scientific communication, grey literature needed a new conceptual framework. They proposed the \"Prague definition\" as follows:\n\nDue to the rapid increase web publishing and access to documents, the focus of grey literature has shifted to quality, intellectual property, curation, and accessibility.\n\nThe term \"grey literature\" acts as a collective noun to refer to a large number of publications types produced by organizations for various reasons. These include: research and project reports, annual or activity reports, theses, conference proceedings, preprints, working papers, newsletters, technical reports, recommendations and technical standards, patents, technical notes, data and statistics, presentations, field notes, laboratory research books, academic courseware, lecture notes, evaluations, and many more. The international network GreyNet maintains an online listing of document types.\n\nOrganizations produce grey literature as a means of encapsulating, storing and sharing information for their own use, and for wider distribution. This can take the form of a record of data and information on a site or project (archaeological records, survey data, working papers); sharing information on how and why things occurred (technical reports and specifications, briefings, evaluations, project reports); describing and advocating for changes to public policy, practice or legislation (white papers, discussion papers, submissions); meeting statutory or other requirements for information sharing or management (annual reports, consultation documents); and many other reasons.\n\nOrganizations are often looking to create the required output, sharing it with relevant parties quickly and easily, without the delays and restrictions of academic journal and book publishing. Often there is little incentive or justification for organizations or individuals to publish in academic journals and books, and often no need to charge for access to organizational outputs. Indeed, some information organizations may be required to make certain information and documents public. On the other hand, grey literature is not necessarily always free, with some resources, such as market reports, selling for thousands of dollars. However, this is the exception and on the whole grey literature, while costly to produce, is usually made available for free.\n\nWhile production and research quality may be extremely high (with organizational reputation vested in the end product), the producing body, not being a formal publisher, generally lacks the channels for extensive distribution and bibliographic control.\n\nInformation and research professionals generally draw a distinction between ephemera and grey literature. However, there are certain overlaps between the two media and they undoubtedly share common frustrations such as bibliographic control issues. Unique written documents such as manuscripts and archives, and personal communications, are not usually considered to fall under the heading of grey literature, although they again share some of the same problems of control and access.\n\nThe relative importance of grey literature is largely dependent on research disciplines and subjects, on methodological approaches, and on the sources they use. In some fields, especially in the life sciences and medical sciences, there has been a traditional preference for only using peer-reviewed academic journals while in others, such as agriculture, aeronautics and the engineering sciences in general, grey literature resources tend to predominate.\n\nIn the last few decades, systematic literature reviews in health and medicine have established the importance of discovering and analyzing grey literature as part of the evidence-base and in order to avoid publication bias.\n\nGrey literature is particularly important as a means of distributing scientific and technical and public policy and practice information. Professionals insist on its importance for two main reasons: research results are often more detailed in reports, doctoral theses and conference proceedings than in journals, and they are distributed in these forms up to 12 or even 18 months before being published elsewhere. Some results simply are not published anywhere else.\n\nIn particular, public administrations and public and industrial research laboratories produce a great deal of “grey” material, often for internal and in some cases “restricted” dissemination. The notion of evidence-based policy has also seen some recognition of the importance of grey literature as part of the evidence-base; however, the term is not yet widely used in public policy and the social sciences more broadly.\n\nFor a number of reasons, discovery, access, evaluation and curation of grey literature pose a number of difficulties.\n\nGenerally, grey literature lacks any strict or meaningful bibliographic control. Basic information such as authors, publication dates and publishing or corporate bodies may not be easily identified. Similarly, the nonprofessional layouts and formats, low print runs and non-conventional channels of distribution make the organized collection of grey literature a challenge compared to journals and books.\n\nAlthough grey literature is often discussed with reference to scientific research, it is by no means restricted to any one field: outside the hard sciences, it presents significant challenges in archaeology where site surveys and excavation reports, containing unique data, have frequently been produced and circulated in informal \"grey\" formats.\n\nSome of the problems of accessing grey literature have decreased since the late 1990s as government, professional, business and university bodies have increasingly published their reports and other official or review documents online. The informal nature of grey literature has meant that it has become more numerous as the technology that allows people to create documentation has improved. Less expensive and more sophisticated printers increased the ease of creating grey literature. And the ability to post documents on the internet has resulted in a tremendous boom. The impact of this trend has been greatly boosted since the early 2000s, as the growth of major search engines has made retrieving grey literature simultaneously easier and more cluttered. Grey reports are thus far more easily found online than they were, often at no cost to access. Most users of reports and other grey documents have migrated to using online copies, and efforts by libraries to collect hard-copy versions have generally declined in consequence.\n\nHowever, many problems remain because originators often fail to produce online reports or publications to an adequate bibliographic standard (often omitting a publication date, for instance). Documents are often not assigned permanent URLs or DOI numbers, or stored in electronic depositories, so that link rot can develop within citations, reference lists, databases and websites. Copyright law and the copyrighted status of many reports inhibits their downloading and electronic storage and there is a lack of large scale collecting of digital grey literature. Securing long-term access to and management of grey literature in the digital era thus remains a considerable problem.\n\nThe amount of digital grey literature now available also poses a problem for finding relevant resources and to be able to assess their credibility and quality given the number of resources now available. At the same time a great deal of grey literature remains hidden, either not made public or not made discoverable via search engines.\n\nVarious databases and libraries collect and make available print and digital grey literature; however, the cost and difficulty of finding and cataloguing grey literature mean that it is still difficult to find large collections. The British Library began collecting print grey literature in the post WWII period and now has an extensive collection of print resources. Australian and New Zealand Policy Online has an extensive collection of grey literature on a wide range of public policy issues, Arxiv is a collection of preprints on physics and other sciences, RePEc is a collection of economics working papers.\n\nMany university libraries provide subject guides that give information on grey literature and suggestions for databases. ROAR and OpenDOAR are directories of Open Access (OA) Institutional Repositories (IR) and subject repositories many of which contain some grey literature.\n\nThe annual International grey literature conference series has been organised since 1993 by the Europe-based organisation GreyNet Research in this field of information has been systematically documented and archived via the International Conference Series on Grey Literature (1993, Vol.1)...(2014, Vol.16)\n\nGreynet also produces a journal on grey literature and has been a key advocate for the recognition and study of grey literature, particularly in library and information sciences. \"The Grey Journal\" (2005, Vol.1)...(2014, Vol.10). (print: , online: ). The Grey Journal appears three times a year—in spring, summer, and autumn. Each issue in a volume is thematic and deals with one or more related topics in the field of grey literature. \"The Grey Journal\" appears both in print and electronic formats. The electronic version on article level is available via EBSCO's LISTA-FT Database (EBSCO Publishing). \"The Grey Journal\" is indexed by Scopus and others.\n\nOn 16 May 2014, the Pisa Declaration on Policy Development for Grey Literature Resources was ratified and published.\n\n"}
{"id": "6837951", "url": "https://en.wikipedia.org/wiki?curid=6837951", "title": "Humanistic informatics", "text": "Humanistic informatics\n\nHumanistic Informatics (also known as Humanities informatics) is one of several names chosen for the study of the relationship between human culture and technology. The term is fairly common in Europe, but is little known in the English-speaking world, though Digital Humanities (also known as Humanities computing) is in many cases roughly equivalent. \n\nHumanistic informatics departments were generally started in the 1990s when universities rarely taught humanities-based approaches to the rapidly developing computerized society. For this reason, the field was quite broadly defined, and included courses in humanities computing, basic introductions to how computers work, historical developments of technology, technology and learning, digital art and literature and digital culture. Today several departments have declared more specialized areas of research, such as digital arts and culture at the University of Bergen, and socio-cultural communication with and without technology at the University of Aalborg.\n\nDigital Humanities is a primary topic, and there are several universities in the US and the UK that have Digital Arts and Humanities research and development centers. One aspect of Digital Humanities that will grow will be the intersection of new digital media and the humanities, particularly in the gaming industry which has developed both casual and serious gaming and game design strategies to foster learning in the humanities and all other academic disciplines. A key principle in all digital interactive media or games is the storyline; the narrative or quest or goal of the game is primary to both literary works and games. Characters and players go on the quest, and playing the game becomes the narrative. Game design principles, also relevant in literature and the fine arts, include visual literacy and empowering players/learners to align with great artitsts and writers who believe in the creative process.\n\n"}
{"id": "37482023", "url": "https://en.wikipedia.org/wiki?curid=37482023", "title": "HusITa", "text": "HusITa\n\nhusITa (Human Services Information Technology Applications) is an international virtual associationand a registered US non-profit organizationestablished with the mission of promoting the ethical and effective use of information technology in the human services. The main focus of husITa, and the claim to expertise of its associates, is situated at the intersection of three core domains: information technology, human services, and social development. husITa pursues its mission through international conferences, publications and research dissemination directed at technology applications and innovations that promote social well-being.\n\nFor much of its early history husITa operated as an informal international network of human service academics and practitioners. One of the outcomes of its first international conferencehusITa1 held in 1987 in Birmingham, Englandwas the establishment of a working group to determine the feasibility of an international body 'to highlight the importance of human service computing, to guide developments, and to foster international co-operation'.\n\nThe working group was composed of Hein de Graaf (Netherlands), Walter LaMendola (USA), Dick Schoech (USA), and Stuart Toole (UK). Initial projects identified by the working group included the development of research agendas, position papers, repositories of information, and promoting a second husITa conference in 1989. Bryan Glastonbury was later added to the group as secretary. The working group met in Colorado, Denver for three days in May 1988 and published a report on the issues that a husITa international organization would need to address.\n\nAlthough the 1988 Denver meeting agreed its objectives, husITa wasn't formally established as an organization for another twelve years. The structure of the formal organization was later agreed to at Denver in 2000. The founding members at the Denver 2000 meeting were: Hein de Graaf, Walter LaMendola, Rob MacFadden, Jo Ann Regan, Jackie Rafferty, Jan Steyaert, Dick Schoech, Stuart Toole, and Victor Savtschenko.\n\nhusITa's objectives (agreed by the 1988 Denver working group) are to:\n\nThe \"Journal of Technology in Human Services\" is the official journal of husITa. Formerly known as \"Computers in Human Services\" it was launched in 1985 as a Haworth Press publication. Dick Schoech, a professor of social work at the University of Texas at Arlington, was its founding editor. The \"Journal of Technology in Human Services\" is a peer-reviewed, refereed journal now published by Taylor & Francis. Its scope includes the potential of information and communication technologies in mental health, developmental disability, welfare, addictions, education, and other human services. The current Editor-in-Chief is Dr. Lauri Goldkind (Associate Professor, Fordham University, USA), with Dr. Chitat Chan (Assistant Professor, Hong Kong Polytechnic University, Hong Kong) serves as the Associate Editor-in-Chief.\n\nhusITa1: husITa's first international conference was held between in September 1987 in Birmingham, England.\n\nhusITa2: \"Computer Technology and Human Services in the 90’s: Advancing Theory and Practice\", June 1991 in New Brunswick, New Jersey.\n\nhusITa3: \"Information Technology and the Quality of Life and Services\", June 1993 in Maastricht, the Netherlands. The same year saw the formation of a husITa Foundation in the Netherlands which continued until its disestablishment in 2003.\n\nhusITa4: \"Information Technology in the Human Services: Dreams and Realities\", June 1996 in Lapland, Finland.\n\nhusITa5: \"Social Services in the Information Society: Closing the GAP\", August–September 1999 in Budapest, Hungary.\n\nhusITa6: \"Technology and Human Services in a Multicultural Society\", September 2001, in Charleston, South Carolina. However, the conference was cut short as a result of the terrorist attacks in the USA on 11 September 2001. A brief husITa board meeting was held, the by-laws were approved, and officers were elected.\n\nhusITa7: \"Digital Inclusion-Building a Digital Inclusive Society\", August 2004 in Hong Kong, China. It had been delayed from its planned date of 2003 due to an outbreak of SARS.\n\nhusITa8: \"Information Technology and Diversity in Human Services: Promoting Strength Through Difference\", August 2007 in Toronto, Canada.\n\nhusITa9: \"ICT as a Context for Human Services\", June 2010 in Hong Kong, China. This event was held in conjunction with the 2010 Joint World Conference on Social Work and Social Development.\n\nhusITa14: \"Sustainable & Ethical use of Technology\". July 9–12, Melbourne, Australia. Held in conjunction with the 2014 Joint World Conference on Social Work, Education, and Social Development.\n\nhusITa was built on the activity of an international network of human service organizations, academics and practitioners in the USA, the UK and the Netherlands. The section below highlights some of the key organizations, people and events.\n\nIn 1978, Gunther R. Geiss, aprofessor of social work at Adelphi University, New York, conducted a survey of US schools of social work. The survey sought to identify faculty members who had used computers in their administrative, teaching or research activities, or who had consulted or participated in the design and development of computer-based information systems. There were over 80 positive responses indicating a wide range of activities and levels of involvement. The survey initiated the development of a system to track and communicate with individuals with expertise in computers and human services.\n\nWalter LaMendola, professor of social work at the University of Denver in Colorado, described an incident during a social work conference in 1979 suggesting early indications of the resistance of some social work professionals to computer use in the human services. This is a theme which has continued throughout the history of technology use in the human services and continues to the present day. Some aspects of this resistance can be considered as a well-founded concern about the ethical issues surrounding human service technology applications. However, other aspects of technology resistance seem to be a less rational form of Neo-Luddism.\n\nGrowing interest in the use of technology in the human services led a group of US human service technology specialists, meeting at a Council of Social Work Education conference in Louisville Kentucky in 1981, to form the Computer Use In Social Services Network (CUSSN). By the end of 1981 the network had over 350 members. The CUSSN newsletter continued in print until 1992 when it was merged with the first academic journal on human service technology \"Computers in Human Services\".\n\nIn 1984 Gunther Geiss was sponsored by the Silberman Fund to organize the Wye Plantation Conference on Human Services Technology. Conference members developed pre-conference position papers via EYES: a centralized email system.\n\nIn 1985, CUSSN developed CUSSNet CUSSNet, a PC and FidoNet based networking system that automatically exchanged emails between members each night during off-peak telephone hours. FidoNet was a PC distributed email, bulletin board, and file sharing system that preceded the Internet. CUSSNet quickly developed nodes in major cities in the US, the UK, and the Netherlands.\n\nThe name husITa (Human Service Information Technology Applications) was coined in 1983 by Walter LaMendola and Brian Klepinger at the University of Denver.\n\nThe Human Service Microcomputer Conference was held in Seattle.\n\nIn 1984 Stuart Toole formed Computer Applications in Social Work (CASW) in the UK to set up and run national conferences and to publish the CASW journal.\n\nBased on the success of the first UK technology conferences, Stuart Toole, Walter LaMendola, and Brian Klepinger agreed to pursue an international conference in 1987: this conference was to become HUSITA's first international conference HUSITA1.\n\nThe CASW journal was later renamed New Technology in the Human Services in [when] and continued in publication under this title until it was closed in 2003.\n\nIn 1986 the second UK conference held on social welfare computing was held.\n\nIn 1985 Bryan Glastonbury, from the University of Southampton published \"Computers in Social Work\", the first major academic text on technology and human services. In the same year the University of Southampton began publishing the journal New Technology in the Human Services under the editorship of Bryan Glastonbury. In the same year the University of Southampton established the Centre for Human Service Technology (CHST) with Jackie Rafferty as Director. In 2007, Jan Steyaert joined as adjunct research professor and together they edited a special issue of the British Journal of Social Work on \"social work in the digital age\".\nThe Centre for Human Service Technology is based at the University of Southampton in England. CHST is an international, multi-disciplinary research center focused on influencing the appropriate use of technology in social work practice and education and researching its implementation and impact.\n\nIn July 1997 the CTI Centre for Human Services held a conference on \"Social Services and Learning Technology\" hosted by the Institute for Health and Community Services at the University of Bournemouth.\n\nIn 2003 the journal New Technology in Human Services ceased publication.\n\nIn 1986 Hein de Graaf, Director of the CREON foundation in the Netherlands, organised the first of a series of three-day gatherings. The CREON foundation was a Dutch foundation for computer research, expertise and support in field of the human services. The gatherings, called WELCOM, were designed to increase the knowledge and understanding of information technology in the Dutch human services. WELCOM1 was held in 1986 in Bussum with Walter LaMendola as the main international speaker. WELCOM2, a smaller Dutch-only conference, took place in 1987, again in Bussum. The next WELCOM event, WELCOM3: a combined conference and fair, was held jointly with HUSITA3 in Maastricht in 1993.\n\nFollowing the success of HUSITA's first international conference held in Birmingham in 1987 the Dutch Ministry of Social Welfare, Health and Cultural Affairs organized an informal meeting of European experts in the field of Information Technology and Human Services. The meeting was one of the outcomes of a feasibility study carried out by the CREON foundation, concerning international cooperation in this field. A main conclusion of this feasibility study was that an international (European) network should be established for the exchange of products, ideas, expertise, experiences and skills with respect to the introduction and use of IT in the human services. As a first step in this approach the informal meeting of experts was organized. The European network of organizations was called ENITH (European Network Information Technology and Human services).\n\nIn 1992 an ENITH3 Expert Meeting on “IT Applications and the Quality of Life and Services” was held in The Netherlands.\n\nIn 1994, from September 21 to September 23 an ENITH4 conference was held in Berlin, Germany. Bernd kolleck chaired this conference.\n\nIn September 1995 a CAUSA5/ENITH5 conference on “The Impact of Information Technology on Social Policy,” was held in Eindhoven, The Netherlands. Jan Steyaert chaired this conference.\n"}
{"id": "6979831", "url": "https://en.wikipedia.org/wiki?curid=6979831", "title": "Instruction creep", "text": "Instruction creep\n\nInstruction creep occurs when instructions increase in number and size over time until they are unmanageable. It can be insidious and damaging to the success of large groups such as corporations, originating from ignorance of the KISS principle and resulting in over-complex (as opposed to \"simplified\") procedures that are often misunderstood, followed with great irritation, or ignored.\n\nInstruction creep is common in complex organizations, where rules and guidelines are created by changing groups of people over extended periods of time. The constant state of flux in such groups often leads them to add or modify instructions, rather than simplifying or consolidating existing ones. This can result in considerable overlap in the message of directives, at the expense of clarity, efficiency, and communication, or even of consistency.\n\nThe fundamental fallacy of instruction creep is believing that people read instructions with the same level of attention and comprehension, regardless of the volume or complexity of those instructions. A byproduct is the advent of many new rules having the deliberate intent to control others via fiat, without considering consensus or collaboration. This tends to antagonize others, even when it appears to the instigators that they are acting with proper intent.\n\n\n"}
{"id": "46536830", "url": "https://en.wikipedia.org/wiki?curid=46536830", "title": "Lexoo", "text": "Lexoo\n\nLexoo is a UK-based legal technology company launched in June 2014 with headquarters in London, United Kingdom. Lexoo provides a lawyer-matching online marketplace, enabling businesses to find a lawyer by providing multiple quotes from specialised solicitors.\n\nLexoo was founded in June 2014 by Daniel van Binsbergen, a former lawyer at De Brauw Blackstone Westbroek, and Chris O'Sullivan, a developer, having raised $400,000 in seed funding from Forward Partners and Jonathan McKay, the Chairman of JustGiving. In November 2015, Lexoo raised a further $1.3 million in funding from a number of investors, including the London Co-Investment Fund, Duncan Jennings (founder of Vouchercodes.co.uk), Tim Jackson of Lean Investments, Robin Grant (founder of We Are Social) and Forward Partners. In October of 2018, Lexoo raised $4.4 million in Series A round led by Earlybird.\nTechCrunch identified that, compared to other markets, the fees charged by legal practitioners are traditionally non transparent, and considered Lexoo “a classic example of how markets can be made more efficient and transparent by moving them online”.\n\nForbes described Lexoo as “the democratisation of legal services”, noting that “the participating solicitors know they are quoting in a competitive environment, so they will offer their best price up front, without businesses having to ask for it. All the ingredients of a classic disruptive start-up.”\n\nIn June 2015, the Financial Times selected Lexoo as its \"Innovation to Watch\" and Startups.co.uk listed Lexoo among the top 100 startups in the UK in 2015.\n\nLexoo has also been featured by PE Hub, Legal Futures, Talk Business Magazine, the Guardian, and Tech City News.\n"}
{"id": "21564241", "url": "https://en.wikipedia.org/wiki?curid=21564241", "title": "Linear model of innovation", "text": "Linear model of innovation\n\nThe Linear Model of Innovation was an early model designed for to understand the relationship of science and technology that begins with basic research that flows into applied research, development and diffusion \n\nIt prioritizes scientific research as the basis of innovation, and plays down the role of later players in the innovation process.\n\nCurrent models of innovation derive from approaches such as Actor-Network Theory, Social shaping of technology and social learning,provide a much richer picture of the way innovation works. Current ideas in Open Innovation and User innovation derive from these later ideas. \n\nIn the 'Phase Gate Model' , the product or services concept is frozen at an early stage to minimize risk. Through enterprise, the innovation process involves a series of sequential phases arranged in a manner that the preceding phase muse be cleared before movie to the next phase. Therefore a project must pass through a gate with the permission of the gatekeeper before moving to the next succeeding phase.\n\nCriteria for passing through each gate is defined beforehand. The gatekeeper examines whether the stated objectives for the preceding phase have been properly met or not and whether desired development has taken place during the preceding phase or not.\n\nTwo versions of the linear model of innovation are often presented:\n\n\nFrom the 1950s to the Mid-1960s, the industrial innovation process was generally perceived as a linear progression from scientific discovery, through technological development in firms, to the marketplace. The stages of the \"Technology Push\" model are: \n\nFrom the Mid 1960s to the Early 1970s, emerges the second-generation Innovation model, referred to as the \"market pull\" model of innovation. According to this simple sequential model, the market was the source of new ideas for directing R&D, which had a reactive role in the process. The stages of the \"market pull \" model are:\n\nThe linear models of innovation supported numerous criticisms concerning the linearity of the models. These models ignore the many feedbacks and loops that occur between the different \"stages\" of the process. Shortcomings and failures that occur at various stages may lead to a reconsideration of earlier steps and this may result in an innovation. A history of the linear model of innovation may be found in Godin \"The Linear Model of Innovation: The Historical Construction of an Analytical Framework\".\n\n\n"}
{"id": "1260665", "url": "https://en.wikipedia.org/wiki?curid=1260665", "title": "List of codecs", "text": "List of codecs\n\nThe following is a list of compression formats and related codecs.\n\n\n\n\n\n\n\nThose codecs are used by many PC games which use voice chats via Microsoft DirectPlay API.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "59024", "url": "https://en.wikipedia.org/wiki?curid=59024", "title": "List of knots", "text": "List of knots\n\nThis list of knots includes many alternate names for common knots and lashings. Knot names have evolved over time and there are many conflicting or confusing naming issues. The overhand knot, for example, is also known as the thumb knot. The figure-eight knot is also known as the savoy knot or the Flemish knot.\n\n\n\n\n\n"}
{"id": "5721896", "url": "https://en.wikipedia.org/wiki?curid=5721896", "title": "List of research parks", "text": "List of research parks\n\nThe following is a list of science park, technology parks and biomedical parks of the world, organized by continent.\n\nASEAN Economic Community\nReport listing all the Economic Zones in the ASEAN Economic Community from UNIDO Viet Nam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are approximately 170 university research parks in North America today.\n\nAlberta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilicon Mallee Adelaide, South Australia\n\n\n"}
{"id": "10280304", "url": "https://en.wikipedia.org/wiki?curid=10280304", "title": "Malaria vaccine", "text": "Malaria vaccine\n\nMalaria vaccine is a vaccine that is used to prevent malaria. The only approved vaccine as of 2015 is RTS,S. It requires four injections, and has a relatively low efficacy. Due to this low efficacy, WHO does not recommend the use of RTS,S vaccine in babies between 6 and 12 weeks of age.\n\nThe vaccine is going to be studied further in Africa in 2018. Research continues into recombinant protein and attenuated whole organism vaccines.\n\nRTS,S (developed by PATH Malaria Vaccine Initiative (MVI) and GlaxoSmithKline (GSK) with support from the Bill and Melinda Gates Foundation) is the most recently developed recombinant vaccine. It consists of the \"P. falciparum\" circumsporozoite protein (CSP) from the pre-erythrocytic stage. The CSP antigen causes the production of antibodies capable of preventing the invasion of hepatocytes and additionally elicits a cellular response enabling the destruction of infected hepatocytes. The CSP vaccine presented problems in trials due to its poor immunogenicity. RTS,S attempted to avoid these by fusing the protein with a surface antigen from hepatitis B, hence creating a more potent and immunogenic vaccine. When tested in trials an emulsion of oil in water and the added adjuvants of monophosphoryl A and QS21 (SBAS2), the vaccine gave protective immunity to 7 out of 8 volunteers when challenged with \"P. falciparum.\"\n\nRTS,S/AS01 (commercial name Mosquirix), was engineered using genes from the outer protein of \"P. falciparum\" malaria parasite and a portion of a hepatitis B virus plus a chemical adjuvant to boost the immune response. Infection is prevented by inducing high antibody titers that block the parasite from infecting the liver. The developers are non-profit In November 2012 a Phase III trial of RTS,S found that it provided modest protection against both clinical and severe malaria in young infants.\n\nIn a bid to accommodate a larger group and guarantee a sustained availability for the general public, GSK applied for a marketing license with the European Medicines Agency (EMA) in July 2014. GSK treated the project as a non-profit initiative, with most funding coming from the Gates Foundation, a major contributor to malaria eradication.\n\nOn 24 July 2015, Mosquirix received a positive opinion from the EMA on the proposal of the vaccine to be used to vaccinate children aged 6 weeks to 17 months outside the European Union.\n\nThe task of developing a preventive vaccine for malaria is a complex process. There are a number of considerations to be made concerning what strategy a potential vaccine should adopt.\n\n\"P. falciparum\" has demonstrated the capability, through the development of multiple drug-resistant parasites, for evolutionary change. The \"Plasmodium\" species has a very high rate of replication, much higher than that actually needed to ensure transmission in the parasite’s life cycle. This enables pharmaceutical treatments that are effective at reducing the reproduction rate, but not halting it, to exert a high selection pressure, thus favoring the development of resistance. The process of evolutionary change is one of the key considerations necessary when considering potential vaccine candidates. The development of resistance could cause a significant reduction in efficacy of any potential vaccine thus rendering useless a carefully developed and effective treatment. \n\nThe parasite induces two main response types from the human immune system. These are anti-parasitic immunity and anti-toxic immunity.\n\n\nTaking this information into consideration an ideal vaccine candidate would attempt to generate a more substantial cell-mediated and antibody response on parasite presentation. This would have the benefit of increasing the rate of parasite clearance, thus reducing the experienced symptoms and providing a level of consistent future immunity against the parasite.\n\nBy their very nature, protozoa are more complex organisms than bacteria and viruses, with more complicated structures and life cycles. This presents problems in vaccine development but also increases the number of potential targets for a vaccine. These have been summarised into the life cycle stage and the antibodies that could potentially elicit an immune response.\n\nThe epidemiology of malaria varies enormously across the globe, and has led to the belief that it may be necessary to adopt very different vaccine development strategies to target the different populations. A Type 1 vaccine is suggested for those exposed mostly to \"P. falciparum\" malaria in sub-Saharan Africa, with the primary objective to reduce the number of severe malaria cases and deaths in infants and children exposed to high transmission rates. The Type 2 vaccine could be thought of as a ‘travellers’ vaccine’, aiming to prevent all cases of clinical symptoms in individuals with no previous exposure. This is another major public health problem, with malaria presenting as one of the most substantial threats to travellers’ health. Problems with the current available pharmaceutical therapies include costs, availability, adverse effects and contraindications, inconvenience and compliance, many of which would be reduced or eliminated entirely if an effective (greater than 85–90%) vaccine was developed. \n\nThe life cycle of the malaria parasite is particularly complex, presenting initial developmental problems. Despite the huge number of vaccines available at the current time, there are none that target parasitic infections. The distinct developmental stages involved in the life cycle present numerous opportunities for targeting antigens, thus potentially eliciting an immune response. Theoretically, each developmental stage could have a vaccine developed specifically to target the parasite. Moreover, any vaccine produced would ideally have the ability to be of therapeutic value as well as preventing further transmission and is likely to consist of a combination of antigens from different phases of the parasite’s development. More than 30 of these antigens are currently being researched by teams all over the world in the hope of identifying a combination that can elicit immunity in the inoculated individual. Some of the approaches involve surface expression of the antigen, inhibitory effects of specific antibodies on the life cycle and the protective effects through immunization or passive transfer of antibodies between an immune and a non-immune host. The majority of research into malarial vaccines has focused on the \"Plasmodium falciparum\" strain due to the high mortality caused by the parasite and the ease of a carrying out in vitro/in vivo studies. The earliest vaccines attempted to use the parasitic circumsporozoite (CS) protein. This is the most dominant surface antigen of the initial pre-erythrocytic phase. However, problems were encountered due to low efficacy, reactogenicity and low immunogenicity. \n\n\nIncreasing the potential immunity generated against \"Plasmodia\" can be achieved by attempting to target multiple phases in the life cycle. This is additionally beneficial in reducing the possibility of resistant parasites developing. The use of multiple-parasite antigens can therefore have a synergistic or additive effect.\n\nOne of the most successful vaccine candidates currently in clinical trials consists of recombinant antigenic proteins to the circumsporozoite protein. (This is discussed in more detail below.)\n\nThe selection of an appropriate system is fundamental in all vaccine development, but especially so in the case of malaria. A vaccine targeting several antigens may require delivery to different areas and by different means in order to elicit an effective response. Some adjuvants can direct the vaccine to the specifically targeted cell type—e.g. the use of Hepatitis B virus in the RTS,S vaccine to target infected hepatocytes—but in other cases, particularly when using combined antigenic vaccines, this approach is very complex. Some methods that have been attempted include the use of two vaccines, one directed at generating a blood response and the other a liver-stage response. These two vaccines could then be injected into two different sites, thus enabling the use of a more specific and potentially efficacious delivery system.\n\nTo increase, accelerate or modify the development of an immune response to a vaccine candidate it is often necessary to combine the antigenic substance to be delivered with an adjuvant or specialised delivery system. These terms are often used interchangeably in relation to vaccine development; however in most cases a distinction can be made. An adjuvant is typically thought of as a substance used in combination with the antigen to produce a more substantial and robust immune response than that elicited by the antigen alone. This is achieved through three mechanisms: by affecting the antigen delivery and presentation, by inducing the production of immunomodulatory cytokines, and by affecting the antigen presenting cells (APC). Adjuvants can consist of many different materials, from cell microparticles to other particulated delivery systems (e.g. liposomes).\n\nAdjuvants are crucial in affecting the specificity and isotype of the necessary antibodies. They are thought to be able to potentiate the link between the innate and adaptive immune responses. Due to the diverse nature of substances that can potentially have this effect on the immune system, it is difficult to classify adjuvants into specific groups. In most circumstances they consist of easily identifiable components of micro-organisms that are recognised by the innate immune system cells. The role of delivery systems is primarily to direct the chosen adjuvant and antigen into target cells to attempt to increase the efficacy of the vaccine further, therefore acting synergistically with the adjuvant.\n\nThere is increasing concern that the use of very potent adjuvants could precipitate autoimmune responses, making it imperative that the vaccine is focused on the target cells only. Specific delivery systems can reduce this risk by limiting the potential toxicity and systemic distribution of newly developed adjuvants.\n\nStudies into the efficacy of malaria vaccines developed to date have illustrated that the presence of an adjuvant is key in determining any protection gained against malaria. A large number of natural and synthetic adjuvants have been identified throughout the history of vaccine development. Options identified thus far for use combined with a malaria vaccine include mycobacterial cell walls, liposomes, monophosphoryl lipid A and squalene.\n\nA completely effective vaccine is not yet available for malaria, although several vaccines are under development. SPf66 a synthetic peptide based vaccine developed by Manuel Elkin Patarroyo team in Colombia was tested extensively in endemic areas in the 1990s, but clinical trials showed it to be insufficiently effective, 28% efficacy in South America and minimal or no efficacy in Africa. Other vaccine candidates, targeting the blood-stage of the parasite's life cycle, have also been insufficient on their own. Several potential vaccines targeting the pre-erythrocytic stage are being developed, with RTS,S showing the most promising results so far.,<ref name=\"doi10.1056/NEJMoa1208394\"></ref>\n\nIn 2015, researchers used a repetitive antigen display technology to engineer a nanoparticle that displayed malaria specific B cell and T cell epitopes. The particle exhibited icosahedral symmetry and carried on its surface up to 60 copies of the RTS,S protein. The researchers claimed that the density of the protein was much higher than the 14% of the GSK vaccine.\n\nThe PfSPZ vaccine is a candidate malaria vaccine developed by Sanaria using radiation-attenuated sporozoites to elicit an immune response. Clinical trials have been promising, with trials taking place in Africa, Europe, and the US protecting over 80% of volunteers. It has been subject to some criticism regarding the ultimate feasibility of large-scale production and delivery in Africa, since it must be stored in liquid nitrogen.\n\nThe PfSPZ vaccine candidate has granted fast track designation by the U.S. Food and Drug Administration in September 2016.\n\nIndividuals who are exposed to the parasite in endemic countries develop acquired immunity against disease and death. Such immunity does not however prevent malarial infection; immune individuals often harbour asymptomatic parasites in their blood. This does, however, imply that it is possible to create an immune response that protects against the harmful effects of the parasite.\n\nResearch shows that if immunoglobulin is taken from immune adults, purified and then given to individuals who have no protective immunity, some protection can be gained.\n\nIn 1967, it was reported that a level of immunity to the Plasmodium berghei parasite could be given to mice by exposing them to sporozoites that had been irradiated by x-rays. Subsequent human studies in the 1970s showed that humans could be immunized against Plasmodium vivax and Plasmodium falciparum by exposing them to the bites of significant numbers of irradiated mosquitos.\n\nFrom 1989 to 1999, eleven volunteers recruited from the United States Public Health Service, United States Army, and United States Navy were immunized against Plasmodium falciparum by the bites of 1001 to 2927 mosquitos that had been irradiated with 15,000 rads of gamma rays from a Co-60 or Cs-137 source. This level of radiation being sufficient to attenuate the malaria parasites so that while they could still enter hepatic cells, they could not develop into schizonts or infect red blood cells. Over a span of 42 weeks, 24 of 26 tests on the volunteers showed that they were protected from malaria infection.\n\n\n"}
{"id": "29923819", "url": "https://en.wikipedia.org/wiki?curid=29923819", "title": "Ministry of National Education (Russian Empire)", "text": "Ministry of National Education (Russian Empire)\n\nThe Ministry of National Education, also translated as Ministry of National Enlightment, was a government ministry in the Russian Empire which oversaw science and education. It was in existence from 1802 to 1817 and from 1824 to 1917. From 1817 to 1824, it was part of the Ministry of Religious Affairs and Public Education.\n\n"}
{"id": "23612695", "url": "https://en.wikipedia.org/wiki?curid=23612695", "title": "Mobile Access Protocol", "text": "Mobile Access Protocol\n\nMAP27\nMobile Access Protocol for MPT 1327 equipment\n\nThis standard specifies an interface between a mobile radio and a data terminal equipment. This\ninterface gives access to and defines network layer procedures for call set-up and data transfer as specified in MPT-8702509549 MPT1327 and MPT1343 or derivations thereof. A conformance test definition is outside the scope of this standard.\n"}
{"id": "11282548", "url": "https://en.wikipedia.org/wiki?curid=11282548", "title": "Mobile virtual private network", "text": "Mobile virtual private network\n\nA mobile virtual private network (mobile VPN or mVPN) is a VPN which is capable of persisting during sessions across changes in physical connectivity, point of network attachment, and IP address. The \"mobile\" in the name refers to the fact that the VPN can change points of network attachment, not necessarily that the mVPN client is a mobile phone or that it is running on a wireless network.\n\nMobile VPNs are used in environments where workers need to keep application sessions open at all times, throughout the working day, as they connect via various wireless networks, encounter gaps in coverage, or suspend-and-resume their devices to preserve battery life. A conventional VPN cannot survive such events because the network tunnel is disrupted, causing applications to disconnect, time out, fail, or even the computing device itself to crash. Mobile VPNs are commonly used in public safety, home care, hospital settings, field service management, utilities and other industries. Increasingly, they are being adopted by mobile professionals and white-collar workers.\n\nA VPN maintains an authenticated, encrypted tunnel for securely passing data traffic over public networks (typically, the Internet.) Other VPN types are IPsec VPNs, which are useful for point-to-point connections when the network endpoints are known and remain fixed; or SSL VPNs, which provide for access through a Web browser and are commonly used by remote workers (telecommuting workers or business travelers).\n\nMakers of mobile VPNs draw a distinction between remote access and mobile environments. A remote-access user typically establishes a connection from a fixed endpoint, launches applications that connect to corporate resources as needed, and then logs off. In a mobile environment, the endpoint changes constantly (for instance, as users roam between different cellular networks or Wi-Fi access points). A mobile VPN maintains a virtual connection to the application at all times as the endpoint changes, handling the necessary network logins in a manner transparent to the user.\n\nThe following are functions common to mobile VPNs.\nSome mobile VPNs offer additional \"mobile-aware\" management and security functions, giving information technology departments visibility and control over devices that may not be on the corporate premises or that connect through networks outside IT's direct control.\nMobile VPNs have found uses in a variety of industries, where they give mobile workers access to software applications.\nIn telecommunication, a mobile VPN is a solution that provides data user mobility and ensures secure network access with predictable performance. Data user mobility is defined as uninterrupted connectivity or the\nability to stay connected and communicate to a possibly remote data network while changing the network access medium or points of attachment.\n\nIn 2001, Huawei launched a product named \"MVPN\". In this case \"MVPN\" had a different meaning from the way that later industry sources would use the term. The Huawei product was focused on delivering a seamless corporate phone system to users whether they were on desktop phones or mobile devices. Although the web page is no longer available, the company advertised that their MVPN had the following advantages over a standard phone system:\n\nTmharay\n\n"}
{"id": "25464184", "url": "https://en.wikipedia.org/wiki?curid=25464184", "title": "ORCID", "text": "ORCID\n\nThe ORCID iD (Open Researcher and Contributor ID) is a nonproprietary alphanumeric code to uniquely identify scientific and other academic authors and contributors. This addresses the problem that a particular author's contributions to the scientific literature or publications in the humanities can be hard to recognize as most personal names are not unique, they can change (such as with marriage), have cultural differences in name order, contain inconsistent use of first-name abbreviations and employ different writing systems. It provides a persistent identity for humans, similar to that created for content-related entities on digital networks by digital object identifiers (DOIs).\n\nThe ORCID organization, ORCID Inc., offers an open and independent registry intended to be the \"de facto\" standard for contributor identification in research and academic publishing. On 16 October 2012, ORCID launched its registry services and started issuing user identifiers.\n\nORCID was first announced in 2009 as a collaborative effort by the research community \"to resolve the author name ambiguity problem in scholarly communication\". The \"Open Researcher Contributor Identification Initiative\" - hence the name ORCID - was created temporarily prior to incorporation. \n\nA prototype was developed on software adapted from that used by Thomson Reuters for its ResearcherID system. ORCID, Inc., was incorporated as an independent nonprofit organization incorporated in August 2010 in Delaware, United States of America, with an international board of directors. Its executive Director, Laure Haak, was appointed in April 2012. From 2016, the board is chaired by Veronique Kiermer of PLOS (the former chair was Ed Pentz of Crossref). ORCID is freely usable and interoperable with other ID systems. ORCID launched its registry services and started issuing user identifiers on 16 October 2012. Formally, ORCID iDs are specified as URIs, for example, the ORCID iD for John Wilbanks is <nowiki>https://orcid.org/0000-0002-4510-0385</nowiki> (both https:// and http:// forms are supported; the former became canonical in November 2017). However, some publishers use the short form, e.g. \"ORCID: 0000-0002-4510-0385\" (as a URN).\n\nORCID iDs are a subset of the International Standard Name Identifier (ISNI), under the auspices of the International Organization for Standardization (as ISO 27729), and the two organizations are cooperating. ISNI will uniquely identify contributors to books, television programmes, and newspapers, and has reserved a block of identifiers for use by ORCID, in the range 0000-0001-5000-0007 to 0000-0003-5000-0001. It is therefore possible for a person to legitimately have both an ISNI and an ORCID iD – effectively, two ISNIs.\n\nBoth ORCID and ISNI use 16-character identifiers, using the digits 0–9, and separated into groups of four by hyphens. The final character, which may also be a letter \"X\" representing the value \"10\" (for example, Nick Jennings' ORCID iD is <nowiki>https://orcid.org/0000-0003-0166-248X</nowiki>, Stephen Hawking's is <nowiki>https://orcid.org/0000-0002-9079-593X</nowiki>) is a MOD 11-2 check digit conforming to the ISO/IEC 7064:2003 standard.\n\nAn ORCID account for a fictitious person, Josiah Carberry, exists as <nowiki>https://orcid.org/0000-0002-1825-0097</nowiki>, for use in testing and as an example in documentation and training material.\n\nThe aim of ORCID is to aid \"the transition from science to e-Science, wherein scholarly publications can be mined to spot links and ideas hidden in the ever-growing volume of scholarly literature\". Another suggested use is to provide each researcher with \"a constantly updated ‘digital curriculum vitae’ providing a picture of his or her contributions to science going far beyond the simple publication list\". The idea is that other organizations will use the open-access ORCID database to build their own services.\n\nIt has been noted in an editorial in \"Nature\" that ORCID, in addition to tagging the contributions that scientists make to papers, \"could also be assigned to data sets they helped to generate, comments on their colleagues’ blog posts or unpublished draft papers, edits of Wikipedia entries and much else besides\".\n\nIn April 2014, ORCID announced plans to work with the Consortia Advancing Standards in Research Administration Information to record and acknowledge contributions to peer review.\n\nIn an open letter dated 1 January 2016 eight publishers, including the Royal Society, the American Geophysical Union, Hindawi, the Institute of Electrical and Electronics Engineers, PLOS, and Science, committed to requiring all authors in their journals to have an ORCID iD.\n\nBy the end of 2013 ORCID had 111 member organizations and over 460,000 registrants. On 15 November 2014, ORCID announced the one-millionth registration. , the number of registered accounts reported by ORCID was 5,641,303. The organizational members include many research institutions such as Caltech and Cornell University, and publishers such as Elsevier, Springer, Wiley and Nature Publishing Group. There are also commercial companies including Thomson Reuters, academic societies and funding bodies.\n\nGrant-making bodies such as the Wellcome Trust (a charitable foundation) have also begun to mandate that applicants for funding provide an ORCID identifier.\n\nIn several countries, consortia, including government bodies as partners, are operating at a national level to implement ORCID. For example, in Italy, seventy universities and four research centres are collaborating under the auspices of the (CRUI) and the National Agency for the Evaluation of the University and Research Institutes (ANVUR), in a project implemented by Cineca, a not-for-profit consortium representing the universities, research institutions, and the Ministry of Education. In Australia, the government's National Health and Medical Research Council (NHMRC) and Australian Research Council (ARC) \"encourage all researchers applying for funding to have an ORCID identifier\". The French scientific article repository HAL is also inviting its users to enter their ORCID number.\n\nIn addition to members and sponsors, journals, publishers, and other services have included ORCID in their workflows or databases. For example, the \"Journal of Neuroscience\", Springer Publishing, the Hindawi Publishing Corporation, Europe PMC, the Japanese National Institute of Informatics's Researcher Name Resolver, Wikipedia, and Wikidata.\n\nSome online services have created tools for exporting data to, or importing data from, ORCID. These include Scopus, Figshare, Thomson Reuters' ResearcherID system, Researchfish, the British Library (for their EThOS thesis catalogue), ProQuest (for their ProQuest Dissertations and Theses service), and Frontiers Loop.\n\nIn October 2015, DataCite, Crossref and ORCID announced that the former organisations would update ORCID records, \"when an ORCID identifier is found in newly registered DOI names\".\n\nThird-party tools allow the migration of content from other services into ORCID, for example Mendeley2ORCID, for Mendeley.\n\nSome ORCID data may also be retrieved as RDF/XML, RDF Turtle, XML or JSON. ORCID uses GitHub as its code repository.\n\n\n"}
{"id": "666246", "url": "https://en.wikipedia.org/wiki?curid=666246", "title": "Office of Science and Technology", "text": "Office of Science and Technology\n\n\"For the Office of Science and Technology (OST) that existed in the United States see President's Science Advisory Committee\"\n\nThe Office of Science and Technology (OST), later (briefly) named the Office of Science and Innovation, was a non-ministerial government department of the British government between 1992 and 2007.\n\nThe office was responsible for co-ordination of the Government's science and technology related activities and policies, and the distribution of some £2.4 billion among the seven UK Research Councils. It was headed by the Chief Scientific Adviser; initially this was Sir William Stewart, then Sir Robert (later Lord) May, and finally Sir David King. \n\nThe OST was originally formed in 1992 as a merger of the Office of the Chief Scientific Adviser with the Science Branch of the Department of Education and Science (as it then was). Although originally run under the Cabinet Office, it was moved between Departments in 1995 to operate under the Department of Trade and Industry. In early 2006, the office was renamed to the \"Office of Science and Innovation\", and was subsequently absorbed into the Department for Innovation, Universities and Skills in the Summer of 2007 when the Department for Education and Skills was split in two.\n\nThe Government Chief Scientific Advisor now heads the Government Office for Science.\n\n"}
{"id": "32768675", "url": "https://en.wikipedia.org/wiki?curid=32768675", "title": "Revel Systems", "text": "Revel Systems\n\nRevel Systems is an iPad-based point of sale system, which was co-founded by Lisa Falzone and Christopher Ciabarra. It is now majority owned by private equity firm Welsh, Carson, Anderson & Stowe.\n\nRevel Systems was founded in 2010 in San Francisco. In May 2011, Revel received $3.7 million in funding from DCM. In 2015 the company announced an investment of approximately $13.5 Million from ROTH Capital Partners, bringing Revel's Series C round to approximately $110 Million. This infusion from ROTH was Revel's C-3 investment round, a follow-up to the Series C-1 round led by Welsh, Carson, Anderson & Stowe (WCAS) in November 2014 and Series C-2 round led by Intuit Inc. in December 2014.\n\nIn 2015, the company announced a strategic partnership with Apple Computers as a member of the Apple Enterprise Mobility Program and in 2014 Revel announced a partnership with Intuit to create Quickbooks Point of Sale Powered by Revel Systems and in Sept 2016 Revel announced a partnership with Shell Global\n\nThe company integrates with third party vendors, and has an open API, allowing others to customize the POS system. Revel released Atlas V2 for the iPad POS in February 2012.\n\nThe Revel Systems headquarters is located in North Beach, San Francisco. An additional office is located in St. Petersburg, Florida. European sales are handled by office in London. Revel's iPad point of sale software focuses on security in order to be a properly licensed system. Revel was the first iPad POS to implement EMV—or \"Chip and Pin\"—Processing in the United States, in January 2013.\n\nSome of Revel’s clients include the following (or franchisees of the following): Shell, Smoothie King, Tully’s Coffee, Little Caesars Pizza, Legends Hospitality, Rocky Mountain Chocolate Factory, Popeye’s Louisiana Kitchen, Illy Coffee, Dairy Queen, Forever Yogurt, and Twistee Treat, among others. Revel has partnered with retail giants Belkin and Goodwill.\n\nIn February 2017 it was announced that Falzone had been replaced as CEO. Lisa Falzone and CTO Chris Ciabarra were removed by majority share holder, investment firm Welsh, Carson, Anderson & Stowe, which now has a majority stake in the company. New CEO Scott Betts was appointed.\n\nRevel Systems' Point of sale system operates on the Apple iPad. The backend can be managed via mobile device or via Web browser. Associated hardware includes: receipt printer, cash drawer, and card swipe. Revel also announced the Revel Ethernet Connect cable in 2015 that allows for a hardwired Ethernet connection to iPads running Revel software.\n\nRevel has several POS systems for the culinary industry such as Kitchen Display System, Drive-through POS, Food Truck POS, and Restaurant POS. Other retail POS systems include Grocery POS, Retail POS, and Quick Service POS. Revel also have systems for large venues including Stadium POS and Events POS.\n\nRevel Systems offers a range of preconfigured hardware to complement its point of sale system. The Apple iPad acts as a business's main POS terminal, or register. Transactions, orders, and various other functions take place on the iPad POS. The iPad Mini is used as a POS terminal for customer-facing kiosks and table-side ordering. The Apple iPod Touch serves as a line-buster, or as a customer-facing display. These terminals work with Epson Printers, wireless routers, access points, cash drawers, card swipes, and barcode scanners to meet a merchant’s needs.\n\nRevel allows for a customizable point of sale solution and integrates with a variety of third party providers. Providers for payment include FirstData, Mercury Payments, LevelUp, Adyen and PayPal. Reporting is provided by companies including Avero, CTUIT, and RTI Connect. Revel’s gift card providers include Givex, Mercury, PlasticPrinters, Synergy, and Valutec. The loyalty and reward program is provided by companies including LevelUp, Punchh, LoyalTree, and Synergy. Revel systems include Facebook and Twitter integration with online ordering options provided by companies including Zuppler, and Shopify. Revel’s Managed Hosting is provided by Singlehop and Softlayer.\n\nCreate menu items in the Management Console and manage your goods with Revel’s inventory management, allowing business owners to keep track of and manage their inventory directly from the backend management console or a mobile device. WiFi Management helps a business establish a strong WiFi network through the use of the ISP provided modem, additional routers and access points. And with Revel’s Always On Mode customers' POS can run uninterrupted offline or on a local network. With the delivery management console, business owners track orders, employees, and deliveries. Customer Relationship Management or CRM, allows businesses to keep track of their customers and customer preferences. With QuickBooks integration, users can export reports into the file type of their choice (including CSV, XLS, and JSON) and import into QuickBooks for accounting purposes. Purchase order generation allows business owners to maintain a database of vendors as well as create, track, and verify purchase orders. Payroll Management provides users with the ability to track and audit labor hours so that employees can be paid correctly and timely. With online ordering, users can integrate their website's online ordering with the Revel POS, allowing their customers to place orders online and set pickup times. With Revel Reporting leverage the Management Console to compare real-time data with historical trends to guide your business decisions.\n\nRevel Systems was included first on the list of Business News Daily's \"Best iPad POS Systems.\" In 2013, Revel Systems was chosen as the Best Retail app in the Business at the Tabby Awards and CEO Lisa Falzone was recognized in Tech Cocktail as one of \"15 Female Entrepreneurs You Should Know About (But Probably Don't).\" In 2015 Lisa Falzone was named on the Fortune 40 Under 40 list and the Forbes list of Eight Rising Stars.\n\n\n\nhttp://mashable.com/2014/11/11/revel-systems-raises-100-million/\n\nhttp://aztechbeat.com/2015/01/sf-based-revel-systems-expand-scottsdale-jobs/\n\nhttp://www.zdnet.com/article/revel-systems-100m-funding-round-sees-australian-expansion/\n\nhttps://www.forbes.com/sites/alexkonrad/2014/11/11/with-100-million-revel-fights-for-point-of-sale/\n\nhttps://www.cnbc.com/id/102171311\n"}
{"id": "28057675", "url": "https://en.wikipedia.org/wiki?curid=28057675", "title": "Skolkovo Innovation Center", "text": "Skolkovo Innovation Center\n\nThe Skolkovo Innovation Center is a high technology business area that is being built at Mozhaysky District in Moscow, Russia. Although historically Russia has been successful with development of science and technology, its lack of entrepreneur spirit led to government intervention of patents and nonproliferation of Russian tech companies beyond the scope of regional service. As corporations and individuals become \"residents\" of the city, with proposed projects and ideas receiving financial assistance. \nSkolkovo was first announced on 12 November 2009 by then Russian President Dmitry Medvedev. The complex is headed by Viktor Vekselberg and co-chaired by former Intel CEO Craig Barrett.\n\nIn March 2010, Vekselberg announced the necessity of developing a special legal order in Skolkovo and emphasized the need to offer a tax holiday lasting 5–7 years.\n\nIn April 2010, Russian Prime Minister Dmitry Medvedev charged the government with working out specific legal, administrative, tax and customs regulations on Skolkovo.\n\nIn May 2010, Dmitry Medvedev introduced two bills regulating working conditions in Skolkovo. The bills were adopted by the State Duma in September of that year and, on 28 September 2010, the President of the Russian Federation signed the bills into federal law.\n\nIn August 2010, Dmitry Medvedev introduced a bill easing migratory policies in regards to Skolkovo.\n\nOn 20 August 2010, a new government decree regulating visas for participants of the Skolkovo project was published. According to this decree, specialized and highly skilled foreign nationals who arrive in Russia with the purpose of securing employment at Skolkovo will be granted a visa for a term of up to 30 days. In the event of successful job placement they can then obtain a work visa for a term of 3 years.\n\nA new highway was opened connecting Skolkovo to the MKAD in June 2010. Railway transport will be available via Belorussky Rail Terminal and Kiyevsky Rail Terminal. A link to Vnukovo International Airport is also planned.\n\nThe innovation center will be financed primarily from the Russian federal budget. The center's 2010 budget was 3.9 Billion RUB. An additional 22 Billion RUB is planned for 2012 and 17.3 Billion RUB in 2013.\n\nSkolkovo includes five \"clusters\" specializing in different areas. These include IT, Energy, Nuclear Technologies, Biomedicine and Space Technologies.\n\nThe IT cluster is tasked with creating an effective model for successful commercialization of IT technologies in Russia. Over 450 companies have signed up for the IT cluster.IT ecosystem includes over 50 fast growing cyber-security startup companies with more than 700 employees in total\n\nThe Energy Efficient Technologies cluster aims to introduce breakthrough technologies focused on the reduction of energy consumption by industrial, housing and municipal infrastructure facilities. Today over 80 companies are on board for the energy efficiency cluster.\n\nThe Nuclear Technologies cluster aims to encourage the competitiveness of nuclear power markets and develop breakthrough technologies and products.\n\nThe strategic goal of this cluster is to create an ecosystem for biomedical innovation. In order to achieve this goal, the best practices of leading biotechnology and biomedical research centers were studied. More than 215 companies have signed on for the Biomedical Technologies cluster.\n\nThe Space Technology and Telecommunications cluster is intended to strengthen Russia's position in the respective industries. The scope of activity is wide: from space tourism to satellite navigation systems. Russian companies aim to increase their market share in this global market, the total volume of which is estimated at $300 billion.\n\nThere are examples of cooperation between the clusters. For example, in 2012 clusters of Information Technologies and Biomedical Technologies organized joint contest on Mobile Diagnostic Device \"Skolkovo M.D.\" and FRUCT was named the contest winner.\n\nThe main elements of The City are the University and a Technopark. The City will also feature a Congress Center, office buildings, laboratories, fitness centers and stores. The City will measure roughly 400 hectares and have a permanent population of 21,000. Employees, including commuters from Moscow and surrounding regions, will comprise about 31,000 people.\n\nAt least 50% of the energy consumed by the city will come from renewable sources. The well-developed water system uses significantly less water by Russian standards without compromising comfort or hygiene. The transport system prioritizes walking and cycling. The use of vehicles with internal combustion engines is prohibited in the city. Energy passive and active buildings that do not require energy from the outside and even produce more energy than they consume will be built at Skolkovo. Household and municipal waste will be disposed of in the most environmentally friendly way possible – leveraging the use of plasma incinerator technology.\n\nIn July 2012, IBM and five leading Russian innovation companies: the Skolkovo Foundation, Rusnano, Rostelecom, Russian Venture Company and ITFY, all signed a collaboration agreement to foster a culture of applied research and commercialization and attract key talent and investment from around the world in the area of microelectronics.\n\nThe agreement will give the Electronics Technology Center access to IBM’s intellectual property for chip design. IBM will also provide cloud computing technologies to form the basis of a new virtual design environment to be used to develop new microelectronic devices such as sensors to be used in smarter infrastructure projects, industry and consumer electronics.\n\nThe cloud will help unite Russia’s dispersed microelectronics development teams and provide access to advanced technologies and best practice and foster global collaboration. Russian chip designers and fabless design houses will be able to access new semiconductor technologies, including automation tools, design kits, libraries and intellectual property. The center will also provide access to a wide variety of semiconductor production processes offered by many different foundries.\n\nThe agreement was signed by Victor Vekselberg, President of the Skolkovo Foundation; Anatoly Chubais, CEO and Chairman of the Executive Board of Rosnano; Alexander Provotorov, President and Chairman of the Management Board of Rostelecom; Igor Agamirzian, CEO of Russian Venture Company; Evgeny Babayan, Chairman of the Board, ITFY; Leonid Svatkov, CEO ITFY; Bruno Di Leo, Senior Vice President IBM; and Kirill Korniliev, Country General Manager, IBM Russia & CIS.\n\nThe ETC will initially focus on microelectronics design; however in the future it may be extended to other fields where cloud computing can support collaborative development projects.\n\nSkolkovo's Open University (OpUS) isn't an educational institution in the typical sense of the word, because graduating students don't receive a diploma. Instead, OpUS is a source of prospective Masters and PhD candidates, for the Skolkovo University of Science and Technology (SkTech), and interns for Skolkovo partner companies. The educational plan of OpUS includes lecture series, master classes and courses by leading scientists, thinkers and practitioners. Students acquire knowledge in the priority research and development areas of Skolkovo (information technology, biomedicine, energetics, space and nuclear technology). In addition, they have an opportunity to gain knowledge in academic and innovative competencies (foresight, forecasting, thinking, projecting), entrepreneur competence, experience in teamwork on projecting and solving inter-disciplinary problems.\n\nOpUS was opened on 21 April 2011 in Moscow. Selection for Winter 2011-2012 students was carried out in Saint Petersburg and Tomsk. There are currently more than 250 students enrolled in OpUS.\n\nInternational partners include:\n\n\n\n"}
{"id": "18106245", "url": "https://en.wikipedia.org/wiki?curid=18106245", "title": "Social networking in the Philippines", "text": "Social networking in the Philippines\n\nSocial networking is one of the most active web-based activities in the Philippines, with Filipinos being declared as the most active users on a number of web-based social network sites such as Facebook, Instagram, Snapchat and Twitter. The use of social networking website has become so extensive in the Philippines that the country has been tagged as \"The Social Networking Capital of the World,\" and has also become part of Filipino cyberculture. Social networking is also used in the Philippines as a form of election campaign material, as well as tools to aid criminal investigation.\n\nFriendster is one of the first social networking websites in the World Wide Web when it was introduced in 2002. However, its popularity in the United States plummeted quickly in 2004 due to massive technical problems and server delays. But as it was losing its American audience, Friendster slowly gained users from Southeast Asia starting in the Philippines. Friendster director of engineering Chris Lunt wondered why its web traffic was spiking in the middle of the night, and noticed that the traffic was coming from the Philippines. He then traced the trail to a Filipino-American marketing consultant and hypnotist named Carmen Leilani de Jesus as the first user to have introduced Friendster to the Philippines, where a number of her friends live.\n\nA study released by Universal McCann entitled \"Power To The People - Wave3\" declared the Philippines as \"the social networking capital of the world,\" with 83 percent of Filipinos surveyed are members of a social network. They are also regarded as the top photo uploaders and web video viewers, while they are second when it comes to the number of blog readers and video uploaders.\n\nWith over 7.9 million Filipinos using the Internet, 6.9 million of them visit a social networking site at least once a month. At times, Friendster has been the most visited website in the Philippines, as well as in Indonesia, according to web tracking site Alexa. David Jones, vice president for global marketing of Friendster, said that \"the biggest percentage of (their site's) users is from the Philippines, clocking in with 39 percent of the site's traffic.\" He further added that in March 2008 alone, Friendster recorded 39 million unique visitors, with 13.2 million of whom were from the Philippines. Meanwhile, Multiply president and founder Peter Pezaris said that the Filipino users of their site comprised the largest and most active group in terms of number of subscribers and of photographs being uploaded daily. About 2.2 million out of more than nine million registered users of Multiply are Filipinos, outnumbering even nationalities with a bigger population base like the United States, Indonesia, and Brazil. Also, one million photographs are uploaded by Filipinos to Multiply every day, which is half of their total number worldwide.\n\nSixty percent of Filipino users of Multiply are female, while 70 percent are under the age of 25. In comparison, Filipino Friendster users are between the ages 16 to 30, with 55 percent of them female.\n\nThe popularity of social networking in the Philippines can be traced in the Filipinos' culture of \"friends helping friends.\" For Filipinos, their friends and who they know can become more valuable than money, especially when what they need can be achieved through nepotism, favoritism, and friendship among others.\n\nSocial networking has extensive uses in the Philippines. It was used to promote television programs like with its two profiles on Multiply. A call center company posted job openings on its Multiply community site and was able to attract recruits. The power of social networking was tested in the country's 2007 general elections when senatorial candidate Francis Escudero created his own Friendster profile to bolster support from Filipino users. He eventually won a seat in the Senate. Local celebrities and politicians have since created their own profiles on Friendster as their medium to communicate with their fans and constituents.\n\nFriendster was also used as a tool for police investigations. Local police in Cebu City were able to track down the suspects for the robbery and murder of a female nursing student on March 2008. After receiving information and tips from the public and other police operatives, the local police searched through the suspects' profiles in order to get a closer look at their faces. The police printed the pictures of the suspects and launched a series of police operations, which led to their arrest. Meanwhile, Manila Police District arrested a suspect for the murder of two guest relations officers in Tondo on January 2007 after they were able to find the suspect's whereabouts through his Friendster profile.\n\nSocial networks also became a source of high-profile cyberwars, notably between actors Ynez Veneracion and Mon Confiado against Juliana Palermo. The two accused Palermo of creating a fake Friendster profile of her ex-boyfriend Confiado, which is uploaded with photos of Confiado and his girlfriend Veneracion but laden with profanities in each caption.\n\nFor his bid for the Philippine Presidency in May 2010, then Secretary of National Defense, Gilberto Teodoro launched an aggressive campaign via the social media. He capitalized on networks such as YouTube and Facebook. He reportedly spent nearly a quarter of his campaign budget on the social media in the Philippines; in comparison to the current president’s Benigno Simeon Aquino III – 9%.\n\nFilipino-American Internet personality Christine Gambito, also known as HappySlip, criticized Friendster for displaying what she described as \"inappropriate advertisements\" that appear on her profile. She posted a message on the site's bulletin board addressing her fans that she contemplated deleting her account. Gambito had earlier deleted her MySpace account because she objected to the Google-powered online advertisements that she said \"were in direct conflict with the HappySlip brand and especially misrepresentative of Filipino women.\" She particularly criticized the posting of advertisements of international dating websites that supposedly target Filipinas.\n\nMeanwhile, Philippine National Police Director General Avelino Razon ordered the Criminal Investigation and Detection Group to find out who created a fake account on Friendster using his identity. The profile was laden with false information about him, saying that he \"wants to meet traitors, corrupts, criminals so he could crush them.\"\n\nAs of December 2008, there have been cases of spam comments in Friendster profiles, most of which are in the form of a JPEG image masquerading as an embedded YouTube video, with a thumbnail of a sexually explicit video clip, such as a girl undressing herself or something similar. Clicking on the image usually results in a redirect to a dubious or disreputable website, or worse, a drive-by download of malware, such as the Koobface worm. Because some of the users, especially teenagers, who usually log on to the site in an Internet cafe, have only limited knowledge about malware and/or computers in general, such social engineering attacks can be a significant risk. The site had received considerable criticism due to this issue.\n\nOn July 2011, GMA Network creates the new campaign \"Think Before You Click\" - a campaign by GMA News to promote responsible use of social media.\nOn August 2016, Rappler initiates the campaign \"#NoPlaceForHate\" - a campaign that encourages civility when engaging online.\n"}
{"id": "17017917", "url": "https://en.wikipedia.org/wiki?curid=17017917", "title": "Software studies", "text": "Software studies\n\nSoftware studies is an emerging interdisciplinary research field, which studies software systems and their social and cultural effects.\n\nThe implementation and use of software has been studied in recent fields such as cyberculture, Internet studies, new media studies, and digital culture, yet prior to software studies, software was rarely ever addressed as a distinct object of study.\n\nSoftware studies is an interdisciplinary field. To study software as an artifact, it draws upon methods and theory from the digital humanities and from computational perspectives on software. Methodologically, software studies usually differs from the approaches of computer science and software engineering, which concern themselves primarily with software in information theory and in practical application; however, these fields all share an emphasis on computer literacy, particularly in the areas of programming and source code. This emphasis on analyzing software sources and processes (rather than interfaces) often distinguishes software studies from new media studies, which is usually restricted to discussions of interfaces and observable effects.\n\nThe conceptual origins of software studies include Marshall McLuhan's focus on the role of media in themselves, rather than the content of media platforms, in shaping culture. Early references to the study of software as a cultural practice appear in Friedrich Kittler's essay, \"Es gibt keine Software,\" Lev Manovich's \"Language of New Media\", and Matthew Fuller's \"Behind the Blip: Essays on the culture of software\". Much of the impetus for the development of software studies has come from videogame studies, particularly platform studies, the study of videogames and other software artifacts in their hardware and software contexts. New media art, software art, motion graphics, and computer-aided design are also significant software-based cultural practices, as is the creation of new protocols and platforms.\n\nThe first conference events in the emerging field were Software Studies Workshop 2006 and SoftWhere 2008.\n\nIn 2008, MIT Press launched a \"Software Studies\" book series with an edited volume of essays (Matthew Fuller's \"Software Studies: a Lexicon\"), and the first academic program was launched, (Lev Manovich, Benjamin H. Bratton and Noah Wardrip-Fruin's \"Software Studies Initiative\" at U. California San Diego). \nIn 2011, a number of mainly British researchers established \"Computational Culture\", an open-access peer-reviewed journal. The journal provides a platform for \"inter-disciplinary enquiry into the nature of the culture of computational objects, practices, processes and structures.\"\n\nSoftware studies is closely related to a number of other emerging fields in the digital humanities that explore functional components of technology from a social and cultural perspective. Software studies' focus is at the level of the entire program, specifically the relationship between interface and code. Notably related are critical code studies, which is more closely attuned to the code rather than the program, and platform studies, which investigates the relationships between hardware and software.\n\n\n\n"}
{"id": "4605995", "url": "https://en.wikipedia.org/wiki?curid=4605995", "title": "Sord IS-11", "text": "Sord IS-11\n\nThe Sord IS-11 was an A4-size, lightweight, portable Z80-based computer. The IS-11 ('IS' stands for 'Integrated Software') had no operating system, but came with built-in word processing, spreadsheet, file manager and communication software.\n\nThe machine was manufactured by Sord Computer Corporation and released in 1983. It was later followed by the IS-11B and IS-11C.\n\nThe IS-11 had a CMOS version of the Z80A running at 3.4 MHz with 32-64 KiB NVRAM and 64 KiB ROM. The monochrome non-back-lit LCD screen allowed for 40 characters × 8 lines or 256 × 64 pixels. Data was stored on built-in microcassette recorder (128kb, 2000 baud).\n\n\n"}
{"id": "773714", "url": "https://en.wikipedia.org/wiki?curid=773714", "title": "São José dos Campos", "text": "São José dos Campos\n\nSão José dos Campos (, meaning Saint Joseph of the Fields) is a major city and the seat of the municipality of the same name in the state of São Paulo, Brazil. One of the leading industrial and research centers with emphasis in aerospace sciences in Latin America, the city is located in the Paraíba Valley, between the two most active production and consumption regions in the country, São Paulo ( from the city) and Rio de Janeiro (). It is the main city of the Metropolitan Region of Vale do Paraíba e Litoral Norte. A native of São José dos Campos is called a \"joseense\" ().\n\nSource: City administration website (in Portuguese) \n\n\nThe municipality comprises three districts: São José dos Campos — the city itself, (also the seat), Eugênio de Melo and São Francisco Xavier. The last one is known for its natural sites and ecotourism.\n\nThe district of São José dos Campos is also subdivided into 2 subdistricts (São José dos Campos and Santana do Paraíba).\n\nHowever, for administrative purposes, the city is composed of 7 urban regions: Center, North, South, West, East, Southeast and São Francisco Xavier.\n\nHighlands predominate in the northern region of the municipality with altitudes ranging from . The northern border of the municipality lies over the Serra da Mantiqueira Mountains (\"Mantiqueira Range\"), with some peaks reaching over 2000 meters (6500 ft.). The highest point in the municipality is known as 'Pico do Selado' at an altitude of 2082 meters.\n\nIn the urban area, there are rolling plateaus and hills.\nThe lowest elevation in the city (and also in the municipality) is found in the \"Paraíba do Sul\" river, at an elevation of 550 m.\n\nThe municipality is bounded at the south by the 'Serra do Jambeiro' mountains, with an elevation of about 900m.\n\n\nThe municipality holds the São Francisco Xavier Environmental Protection Area, established in 2002.\nIt contains part of the Mananciais do Rio Paraíba do Sul Environmental Protection Area, created in 1982 to protect the sources of the Paraiba do Sul river.\n\nThe climate of the city can be, perhaps, best described as a mix between the subtropical climate of southern parts of the country, the tropical climate of most of the country and the subtropical highland climate (in Portuguese: clima tropical de altitude) of neighbouring mountainous regions.\n\nTechnically, the city has a humid subtropical climate with significant less precipitation during winters (transitioning between \"Cfa\" and \"Cwa\" climatic types in the Köppen classification). Winters are very mild, with average temperature in the coldest month of 17 °C. Summers are not excessively hot, with average temperature of the hotttest month of 24 °C. With global warming it is very likely that the city's climate will transition to a true tropical climate (type 'Aw' in the Köppen climate classification) in the near future.\n\nFrosts are very rare, happening on average once per decade. There's no record of snow precipitation in the city.\n\nThe peaks at the northern border of the municipality (some over 2000 m high) due to altitude have a colder Cwb/Cfb climates, with very occasional snowfalls (about once per decade).\n\nThe origins of São José dos Campos lie at the end of the 16th Century when Jesuits founded a cattle farm, \"Aldeia do Rio Comprido\". The farm was created through a concession of settlements around 1590 to the Society of Jesus. The farm was located on the banks of the Rio Comprido, natural division between São José and the city of Jacareí today.\n\nThe farm status was an artifice to hide a religious outpost, one of the several Jesuit Reductions in Brazil, known for their resistance to enslavement, from the Portuguese expedition leaders and indigenous people hunters, known as the Bandeirantes.\n\nOn September 10, 1611, the local was officially recognized and the farmers precluded from utilizing the Natives as slaves. However, a turmeric conflict between farmers and the religious led to the expulsion of the Jesuits in 1640 from the region and the consequent dispersion of the mission.\n\nNevertheless, the Jesuits returned and reestablished a new settlement, where the current city center is spotted. It was about northeast of the previous mission, on a higher plain with a privileged view above a geological depression, which guaranteed security against invasions and floods. Again, despite being a new mission, it was officially treated as a cattle farm.\n\nThe initial urbanization plan is attributed to the Jesuit priest Manoel de Leão, whose main occupation was really to be an administrator of the community.\n\nIn 1692, documents named the village as \"Residência do Paraíba do Sul\"; in 1696 as \"Residência de São José\".\n\nAt the beginning of the gold mining economic cycle in Brazil, the settlement goes through serious difficulties due to the exit of labor to the mines.\n\nAfter the definitive expulsion of Jesuits from the Portuguese Empire in 1759, all the religious order's assets, such as farms, colleges and villages were taken under the Portuguese Crown's custody. The governor, D. Luis Antonio Botelho Mourão, had as a priority to turn these new assets into productive units and increase tax collection. For that, Boutelho Mourão successfully requested authorization from the Viceroy to create civil parishes, known as \"freguesias\", and to change the fiscal status of villages to the category of Vila (town).\n\nThen, on July 27, 1767, São José reached the official status of town, with a hall and a pillory, passing over the status of civil parish; and the name Vila de São José do Paraíba was formalized. But for many years it maintained the same rural characteristics. The main difficulty was the fact that the \"Estrada Real\" (Royal Road) passed by its limits, far from the village.\n\nIn the middle of the 19th century, the village of São José do Paraíba had demonstrated some signs of economic growth through the development of agriculture. Cotton production evolved rapidly in the region, exported to the English textile industry. The production reached a peak in 1864.\n\nIn the same year, on April 22, the town became the seat of a municipality, acquiring finally, in 1871, the current name of São José dos Campos, followed by the creation of a judiciary district in 1872. Almost simultaneously, there was development of coffee crops in Paraíba Valley, which started to take off in 1870.\n\nIn 1886, after the opening of the Estrada de Ferro Central do Brasil railway (1877), the coffee production peaked. Then started to decay, running steady until the 1930s.\n\nThe call for the municipality of São José dos Campos for the treatment of pulmonary tuberculosis by sanatoriums became noticed at the\n\nbeginning of last century, due to its supposedly favorable climate conditions. The city became to be known as \"the Sanatorium City\". The country's then-largest hospital, the Vicentina Aranha Sanitarium, was opened in town in 1924, and in 1935 the municipality was officially recognized as a health retreat.\n\nWith the advent of antibiotics in the 1940s, tuberculosis begins to be treated anywhere, thus ending the healthcare advantage carried out by São José, whereas the establishment of industries was about just to start.\n\nThe industrialization process of the municipality takes hold from the installation of the \"Technological Institute of Aeronautics \" in 1950 and also with the opening of the Dutra Highway (BR-116), thus making possible a faster connection between Rio de Janeiro and São Paulo and cutting into the urban area of São José dos Campos. Altogether, these factors allowed the municipality to make strides towards fulfilling its scientific and technological potential.\n\nSão José dos Campos has a \"strong mayoral\" system in which the mayor is vested with extensive executive powers, as it is in all municipalities in Brazil. The mayor is elected to a four-year term by universal voting. The City Council is elected every four years with the mayor. The current mayor is Felicio Ramuth, from the political party PSDB and was elected in 2016.\n\nThe State of São Paulo is divided politically and administratively into 15 regions. São José dos Campos is the seat and the name of the 3rd Administrative Region, which includes the North Coast of São Paulo state and the Paraíba Valley.\n\nThe region comprises 39 municipalities with sharp contrasts. São José dos Campos is a densely populated city, with approximately 2,100 inhabitants/square kilometers in urban area, whereas the quiet municipality of São José do Barreiro has only seven inhabitants/square kilometer. There are both highly industrialized cities and the others in the region are focused on agriculture and tourism. São José dos Campos is well known as the \"Capital do Vale\" which means that São José dos Campos is the most important city of the Paraíba Valley.\n\nIt is one of the state's most dynamic areas, the fourth one in terms of population density, and covers 11.3% of the state's territory. The main municipalities are São José dos Campos, Taubaté, Jacareí, Guaratingueta, Caraguatatuba, Campos do Jordão, São Sebastião, Lorena, Pindamonhangaba, Ubatuba and Caçapava.\n\n\n\nAll the major hypermarkets, supermarket chains and discount and department stores are in city. The largest malls are:\n\n\nBesides those malls, the most important commercial centers include:\n\n\nAnd newer areas such as:\n\n\nThe Vila Ema district has sites for nightlife including bars and restaurants.\n\nSeveral fairs and expositions are done at the two pavilions located in the city.\n\nA list of notable sites:\n\n\n\nA Japanese garden is open for visits within the Santos-Dumont Park, celebrating the sister cities.\n\nAlthough São José is an industrial center, the city still preserves green areas and quiet town districts. Around 62% of the area from the municipality is characterized as an environmental preservation area. On the oustskirts of the urban area, Augusto Ruschi Ecology Reserve has many local plant species. The natural reserve has , being a government protect area for the local flora. São Francisco Xavier is a community the offers many of those attributes as well.\n\nFurthermore, there is easy access to the mountain cities (Campos do Jordão, Santo Antônio do Pinhal) and to the beaches of the Northern Coast of São Paulo.\n\nThe city has three parks and several sports and country clubs. Tenis Clube and Associação Esportiva São José have hosted the 35th Banana Bowl International Tennis Federation Juniors Circuit in 2005 and 2006. \n\nA soccer stadium, called Estádio Martins Pereira, is the home ground of São José Esporte Clube, a professional soccer team.\n\nNotable teams from the city:\n\nThere are 19 movie theaters and 2 theaters including one inside the Univap - University of Vale do Paraíba.\n\nThe Brazilian National Institute for Space Research (INPE) has its headquarters in São José dos Campos. It coordinates intensive research and development in areas such as Earth observation, space sciences and space technologies. Also the Brazilian General Command for Aerospace Technology (CTA) has its facilities in the city. There are 53 secondary schools, 54 primary schools and 109 preschools.\n\n\n\n\nThe city has two bus stations, having lines to cities in all regions of the country and international routes to Argentina, Paraguay, Uruguay and Chile.\n\nSão José dos Campos boasts an extensive bus system. Operated by three companies (Expresso Maringa, Julio Simões and Viação Capital do Vale), these lines serve nearly all areas in the city with 319 buses. São José dos Campos also has an alternative system with minivans to supplement the regular buses.\n\nThe city uses a ring road system, that interconnects it to important national and state highways:\n\n\n\nThe city is also served by a railway (the former Central do Brasil), administered by MRS Logística, which today only carries freight.\n\nThe ports of São Sebastião and Santos can be reached by the highways SP-099, SP-155, and BR-101. The transportation of cargo to the domestic and foreign markets is made through both ports.\n\nThe São José dos Campos Airport (IATA: SJK, ICAO: SBSJ) has a heavy passenger flow, mainly business trips during weekdays, and it is an important connection between São Paulo and Rio de Janeiro. With a runway, the airport also serves people who come to visit the tourist city of Campos de Jordão. The airlines Azul Brazilian Airlines and TRIP Linhas Aéreas fly from São José dos Campos to several destinations and presently are the only commercial airlines operating on São José dos Campos.\n\nIt is also used for the transportation of cargo from the several industries located in the so-called Cone Leste Paulista (\"São Paulo's East Cone\"). Infraero and the Federal Revenue Agency are also introducing a new concept called airport-industry, that will offer fiscal incentives and fast importation and exportation procedures. The municipality has also a Customs Station for the Hinterland (dry port), controlled by the Federal Revenue Agency.\n\nSão José dos Campos receives natural gas from two gas pipelines, and large companies such as General Motors, Kodak, Monsanto and Embraer are among the main users. The city is the third largest in the country referring to the distribution net of natural gas for residential use. \n\nIt has also a large network of fiber optics, with broadband services covering 75% of the city. There is 1 telephone for each 3 inhabitants and a vast service network of cellular telephones. \n\nThe municipality cultivates different crops: rice, tomato, potatoes, orange and many vegetables; cattle are raised for beef and milk supply. There are also farms for production of eggs and chicken. \n\nIn contrast to the rural town in 1950s, today São José is an important manufacturing center and holds a large array of industries. Over 1,251 industries are in the municipality and nearby 47,000 inhabitants work for industries. The three main industries are automotive, oil/petrochemical and aerospace. There are significant pharmaceutical, consumer durables, chemical, and telecommunication companies in the city.\n\nIt is also known as the \"Brazilian aeronautics capital\" because it is home of CTA and one of the biggest aircraft manufacturers in the world, Embraer.\n\nIn 2014 São José dos Campos ranked as the 5th largest exporter, by value, of all Brazilian municipalities exporting $4.6B (USD) worth of materials. In that year, between aircraft and aircraft parts categories, São José Dos Campos exported $3.57B (USD) or 81.9% of the total exports of the municipality.\n\nSince the 1990s, the local economy has been evolving in a different direction. The manufacturing economy has been downsized or replaced by tertiary and quaternary sectors of industries.\n\nFor instance, the Entrepreneurial District of Chacaras Reunidas concentrates companies of micro, small and medium size, which are mainly the result of downsizing from old large local industries. Yet even though most of these are industries, these companies provide service as well.\n\nTwo technological parks and five (one in project) business incubators have been created within universities or industrial facilities.\n\nThere are incubators with technological start-up companies installed at Univap and at Henrique Lage Refinery of Petrobrás. The CTA houses other incubator, Incubaero, specialized in the aeronautical field.\n\nUnivap features a technological park with capacity for around 40 small to medium-sized innovating companies in the areas of materials, electronics and telecommunications, information technology, aerospace, energy, environment control, biotechnology, bioinformatics, chemical engineering, and software among others. A new technological park, managed by the municipality and the state government of São Paulo, will house two new think tanks: the Institute for Technological Research (IPT) and the ItecBio (Instituto de Tecnologias Biomédicas).\n\nAs a result of its geographical location, the city became an important distribution center, having several logistics providers. Activities like purchasing, transport, planning and warehousing have employed many people recently.\n\nCommerce and real estate ventures have developed in the last years, reflecting the changes in the economy. For instance, the largest shopping mall in the region was an old manufacturing facility. Serving the region's population of approximately one million, the city is the regional hub for shopping and services for the Vale do Paraíba, the northern coast of São Paulo and southern Minas Gerais.\n\n"}
{"id": "10456882", "url": "https://en.wikipedia.org/wiki?curid=10456882", "title": "Technological fix", "text": "Technological fix\n\nA technological fix, technical fix, technological shortcut or solutionism refers to the attempt of using engineering or technology to solve a problem (often created by earlier technological interventions).\n\nSome references define technological fix as an \"attempt to repair the harm of a technology by modification of the system\", that might involve modification of the machine and/or modification of the procedures for operating and maintaining it.\n\nTechnological fixes are inevitable in modern technology. It has been observed that many technologies, although invented and developed to solve certain perceived problems, often create other problems in the process, known as externalities. In other words, there would be modification of the basic hardware, modification of techniques and procedures, or both.\n\nTechnological fix is the idea that all problems can find solutions in better and new technologies. It now is used as a dismissive phrase to describe cheap, quick fixes by using inappropriate technologies; these fixes often create more problems than they solve, or give people a sense that they have solved the problem.\n\nIn the contemporary context, technological fix is sometimes used to refer to the idea of using data and intelligent algorithms to supplement and improve human decision making in hope that this would result in ameliorating the bigger problem. One critic, Evgeny Morozov defines this as \"Recasting all complex social situations either as neat problems with definite, computable solutions or as transparent and self-evident processes that can be easily optimized--if only the right algorithms are in place.\" While some criticizes this approach to the issues of today as detrimental to efforts to truly solve these problems, opponents finds merits in such approach to technological improvement of our society as complements to existing activists and policy efforts.\n\nAn example of the criticism is how policy makers may be tempted to think that installing smart energy monitors would help people conserve energy better, thus improving global warming, rather than focusing on the arduous process of passing laws to tax carbon, etc. Another example is thinking of obesity as a lifestyle choice of eating high caloric foods and not exercising enough, rather than viewing obesity as more of a social and class problem where individuals are predisposed to eat certain kind of foods (due to the lack of affordable health-supporting food in urban food deserts), to lack optimally evidence-based health behaviors, and lack of proper health care to mitigate behavioral outcomes.\n\nThe technological fix for climate change is an example of the use of technology to restore the environment. This can be seen through various different strategies such as: geo-engineering and renewable energy.\n\nGeo-engineering is referred as \"the artificial modification of Earth's climate systems through two primary ideologies, Solar Radiation Management (SRM) and Carbon Dioxide Removal (CDR)\". Different schemes, projects and technologies have been designed to tackle the effects of climate change, usually by removing CO2 from the air as seen by Klaus Lackner's invention of a Co2 prototype, or by limiting the amount of sunlight that reaches the Earth's surface, by space mirrors. However, \"critics by contrast claim that geoengineering isn't realistic – and may be a distraction from reducing emissions.\" It has been argued that geo-engineering is an adaptation to global warming. It allows TNC's, humans and governments to avoid facing the facts that global warming is a crisis that needs to be dealt with head-on by reducing emissions and implementing green technologies, rather than developing ways to control the environment and ultimately allow Greenhouse Gases to continue to be released into the atmosphere.\n\nRenewable Energy is also another example of a technological fix, as technology is being used in attempts to reduce and mitigate the effects of global warming. Renewable Energy refers to technologies that has been designed to be eco-friendly and efficient for the well-being of the Earth. They are generally regarded as infinite energy sources, which means they will never run out, unlike fossil fuels such as oil and coal, which are finite sources of energy. They additionally release no green house gases such as carbon dioxide, which is harmful for the planet as it depletes the ozone layer. Examples of renewable energy can be seen by wind turbines, solar energy such as solar panels and kinetic energy from waves. These energies are regarded as a technological fix as they have been designed and innovated to overcome issues with energy insecurity, as well as to help protect Earth from the harmful emissions released from non-renewable energy sources, and thus overcome global warming. It is also known that such technologies will in turn require their own technological fixes. For example, some types of solar energy have local impacts on ambient temperature, which can be a hazard to birdlife.\n\nIt has been made explicit within society that the world's population is rapidly increasing, with the \"UNICEF estimating that an average of 353,000 babies are born each day around the world.\" Therefore, it is expected that the production of food will not be able to progress and develop to keep up with the needs of species. Ester Boserup highlighted in 1965 that when the human population increases and food production decreases, an innovation will take place. This can be demonstrated in the technological development of hydroponics and genetically modified crops.\nHydroponics is an example of a technological fix. It demonstrates the ability for humans to recognise a problem within society, such as the lack of food for an increasing population, and therefore attempt to fix this problem with the development of an innovative technology. Hydroponics is a method of food production to increase productivity, in an \"artificial environment.\" The soil is replaced by a mineral solution that is left around the plant roots. Removing the soil allows a greater crop yield, as there is less chance of soil-born diseases, as well as being able to monitor plant growth and mineral concentrations. This innovative technology to yield more food reflects the ability for humans to develop their way out of a problem, portraying a technological fix.\n\nGenetically modified organism (GMO) reflect the use of technology to innovate our way out of a problem such as the lack of food to cater for the growing population, demonstrating a technological fix. GM crops can create many advantages, such as higher food fields, added vitamins and increased farm profits. Depending on the modifications, they may also introduce the problem of increasing resistance to pesticides and herbicides, which may inevitably precipitate the need for further fixes in the future.\n\nGolden rice is one example of a technological fix. It demonstrates the ability for humans to develop and innovate themselves out of problems, such as the deficiency of vitamin A in Taiwan and Philippines, in which the World Health Organization reported that about 250 million preschool children are affected by. Through the technological development of GM Crops, scientists were able to develop golden rice that can be grown in these countries with genetically higher levels of beta-carotene (a precursor of vitamin A). This enables healthier and fulfilling lifestyles for these individuals and consequently helps to reduce the deaths caused by the deficiency.\n\nExternalities refer to the unforeseen or unintended consequences of technology. It is evident that everything new and innovative can potentially have negative effects, especially if it is a new area of development. Although technologies are invented and developed to solve certain perceived problems, they often create other problems in the process.\n\nDDT was initially use by the Military in World War II to control a range of different illnesses, varying from Malaria to the bubonic plague and body lice. Due to the efficiency of DDT, it was soon adopted as a farm pesticide to help maximise crop yields to consequently cope with the rising populations food demands post WWII. This pesticide proved to be extremely effective in killing bugs and animals on crops, and was often referred as the \"wonder-chemical.\" However, despite being banned for over forty years, we are still facing the externalities of this technology. It was found that DDT had major health impacts on both humans and animals. It was found that DDT accumulated within the fatty cells of both humans and animals and therefore highlights that technological fixes have their negatives as well as positives. \n\nGlobal warming can be a natural phenomenon that occurs in long (geologic) cycles. However, it has been found that man-made ozone layer depletion and the release of Greenhouse Gases through industry, and traffic, causes the earth to heat up to unnatural temperatures. This is causing externalities on the environment, such as melting icecaps, shifting biomes and extinction of many aquatic species, through ocean acidification and changing temperatures.\n\nAutomobiles with internal combustion engines have revolutionised civilisation and technology. However, whilst the technology was new and innovative, helping to connect places through the ability of transport, it was not recognised at the time that burning fossil fuels, such as coal and oil, inside the engines would release pollutants. This is an explicit example of an externality caused by a technological fix, as the problems caused from the development of the technology was not recognised at the time.\n\nHigh-tech megaprojects are large scale and require huge sums of investment and revenue to be created. Examples of these high technologies are Dams, nuclear power plants, and airports. They usually cause externalities on other factors such as the environment, are highly expensive, and are top-down governmental plans. \nThe Three Gorges Dam is an example of a high-tech technological fix. The creation of the multi-purpose navigation hydropower and flood control scheme was designed to fix the issues with flooding whilst providing efficient, clean renewable hydro-electric power in China. The Three Gorges Dam is the world's largest power station in terms of installed capacity (22,500 MW). The dam is the largest operating hydroelectric facility in terms of annual energy generation, generating 83.7 TWh in 2013 and 98.8 TWh in 2014, while the annual energy generation of the Itaipú Dam in Brazil and Paraguay was 98.6 TWh in 2013 and 87.8 in 2014. It was estimated to have cost over £25 billion. There have been many externalies from this technology, such as the extinction of the Chinese River Dolphin, an increase in pollution, as the river can no longer 'flush' itself, and over 4 million locals being displaced in the area.\n\nIs usually small-scale and cheap technologies that are usually seen in developing countries. The capital to build and create these technologies are usually low, yet labour is high. Local expertise can be used to maintain these technologies making them very quick and effective to build and repair. An example of an intermediate technology can be seen by water wells, rain barrels and pumpkin tanks.\n\nTechnology that suits the level of income, skills and needs of the people. Therefore, this factor encompasses both high and low technologies.\n\nAn example of this can be seen by developing countries that implement technologies that suit their expertise, such as rain barrels and hand pumps. These technologies are low costing and can be maintained by local skills, making them affordable and efficient. However, to implement rain barrels in a developed country would not be appropriate, as it would not suit the technological advancement apparent in these countries. Therefore, appropriate technological fixes take into consideration the level of development within a country before implementing them.\n\nMichael and Joyce Huesemann caution against the hubris of large-scale techno-fixes In the book \"Techno-Fix: Why Technology Won't Save Us Or the Environment\" they show why negative unintended consequences of science and technology are inherently unavoidable and unpredictable, why counter-technologies or techno-fixes are no lasting solutions, and why modern technology in current context does not promote sustainability but instead collapse. Naomi Klein is a prominent opponent of the view that simply technological fixes will solve our problems. She explained her concerns in her book \"\" and states that technical fixes for climate change such as geoengineering bring significant risks as \"we simply don't know enough about the Earth system to be able to re-engineer it safely\". According to her the proposed technique of dimming the rays of the sun with sulphate-spraying helium balloons in order to mimic the cooling effect on the atmosphere of large volcanic eruptions for instance is highly dangerous and such schemes will surely be attempted if abrupt climate change gets seriously under way. Various experts and environmental groups have also come forward with their concerns over views and approaches that look for techno fixes as solutions and warn that those would be \"misguided, unjust, profoundly arrogant and endlessly dangerous\" approaches as well as over the prospect of a technological 'fix' for global warming, however impractical, causing lessened political pressure for a real solution.\n\n"}
{"id": "2089284", "url": "https://en.wikipedia.org/wiki?curid=2089284", "title": "Telecentre", "text": "Telecentre\n\nA telecentre is a public place where people can access computers, the Internet, and other digital technologies that enable them to gather information, create, learn, and communicate with others while they develop essential digital skills. Telecentres exist in almost every country, although they sometimes go by a different names including public internet access center (PIAP), village knowledge center, infocenter, community technology center (CTC), community multimedia center (CMC), multipurpose community telecentre (MCT), Common/Citizen Service Centre (CSC) and school-based telecentre. While each telecentre is different, their common focus is on the use of digital technologies to support community, economic, educational, and social development—reducing isolation, bridging the digital divide, promoting health issues, creating economic opportunities, and reaching out to youth for example.\n\nThe telecentre movement’s origins can be traced to Europe's telecottage and Electronic Village Halls (originally in Denmark) and Community Technology Centers (CTCs) in the United States, both of which emerged in the 1980s as a result of advances in computing. At a time when computers were available but not yet a common household good, public access to computers emerged as a solution. Today, although home ownership of computers is widespread in the United States and other industrialized countries, there remains a need for free public access to computing, whether it is in CTCs, telecottages or public libraries to ensure that everyone has access to technologies that have become essential.\n\nThere are also CTCs located in the state of New South Wales, Australia, that provide technology, resources, training and educational programs to communities in regional, rural and remote areas. \n\nBeyond the differences in names, public ICT access centers are diverse, varying in the clientele they serve, the services they provide, as well as their business or organizational model. Around the world, some telecentres include NGO-sponsored, local government, commercial, school-based, and university-related In the United States and other countries, public access to the Internet in libraries may also be considered within the “telecentre concept”, especially when the range of services offered is not limited to pure access but also includes training end-users. Each type has advantages and disadvantages when considering attempts to link communities with ICTs and to bridge the digital divide. Among the various types: \n\n\nIt is estimated that 40% of the world's population has less than US$ 20 per year available to spend on ICT. In Brazil, the poorest 20% of the population counts with merely US$9 per year to spend on ICT (US$ 0.75 per month). In Mexico, the poorest 20% of the society counts with an estimated US$ 35 per year (US$ 3 per month). For Latin America it is estimated that the borderline between ICT as a necessity good and ICT as a luxury good is roughly around the \"magical number\" of US$10 per person per month, or US$120 per year.\n\nIn the 1990s, international development institutions such as Canada’s International Development Research Centre (IDRC) and UNESCO, sponsored the deployment of many telecentres in developing countries. Both IDRC and UNESCO are still very involved in the telecentre movement. The former telecentre.org programme at IDRC was transferred to the telecentre.org Foundation in the Philippines in March 2010 and continues to support networks of telecentres around the world. UNESCO continues to support the growth of community multimedia centers (CMCs), which, unlike most other telecentres, have a local community radio, television or other media component.\n\nIn light of the rapidly evolving technologies that support telecentres and in light of the increased penetration of mobile technologies (i.e., cell phones), the telecentre model needs to continuously evolve in order to remain relevant and to continue to address the changing needs of the communities they serve. As mobile communication technologies become more pervasive around the world, including in rural areas, the telecentres may no longer need to provide phone services, yet they may still be very relevant in terms of access to web-enabled e-government services, e-Learning, and basic Internet communication needs (email and web browsing).\n\nAmong the various sustainability considerations:\n\n\"Evolving models\" — since the local demand for information and communication services is evolving, the telecentre models need to evolve as well. Franchises and other approaches to linking and networking telecentres are proving to be popular.\n\n\"Evolving technologies\" — wireless connectivity technologies, beyond VSAT (known to be expensive) are being explored in many communities around the world. These technologies provide new opportunities for connecting communities through telecentres and eventually at the individual household level.\n\n\"Evolving services\" — the types of services that telecentres can and should provide is also rapidly evolving. As the fields of eGovernment, eHealth, e-Learning, eCommerce are evolving and maturing in many countries, telecentres need to take advantage of opportunities to extend the benefits to the community at large, through their public access. Some governments are pursuing the deployment of telecentres precisely as a means of ensuring that larger segments of the population are able to access government services and information through electronic channels.\n\n\"Community stakeholders\" - identifying leaders among the community who champion the concept of shared services through telecentre mode, play a crucial role as a bridge between the telecentre operator and hesitant villagers. Indeed, There is a maturing period during which community leaders have to invest constant efforts to drive changes of behaviour in the adoption of innovations.\n\n\"Community involvement\" is required however, at the initial phase of the telecentre set up, starting with the site selection and creating a sort of empathy and feeling of empowerment. Furthermore, the telecentre should be well rooted in the socio-cultural context of the community.\n\nThe telecentres of today and of the future are networked telecentres, or telecentres of the 2.0 generation. Increasingly, telecentres are not operating as independent, isolated entities but as members of a network. At times, the network takes the form of a franchise. In other circumstances, the network is much more informal.\n\nOne such regional network targeted towards Asia-Pacific is, the Asia-Pacific Telecentre Network.\n\nIn the United States, more than 1,000 community technology centers where organized under the leadership of CTCnet, a nonprofit association headquartered in Washington, D.C.. CTCs are also organized under the banner of state organizations, such as the Ohio Community Computing Network, or city programs such the City of Seattle Community Technology Program. and Austin FreeNet.\n\nFor more information on telecentre networks, visit telecentre.org. An overview of telecentre networks can also be found in Chapter 7 of Making the Connection: Scaling Telecentres for Development.\n\nAdditional information about concept of community telecentres can also be found in the online book .\n\nAdditional information about the practice of building and sustaining telecentres can be found in this page on Telecentre Sustainability.\n\nAdditional information about the social, political, economic, and technical problems and challenges facing the development and sustainability of telecentres can be found at Telecenters.\n\nThere is a growing research and analytical literature on telecentres and other community based technology initiatives and approaches particularly within the context of Community informatics as an academic discipline and through the Journal of Community Informatics.\n\n\n\n"}
{"id": "9237787", "url": "https://en.wikipedia.org/wiki?curid=9237787", "title": "The Californian Ideology", "text": "The Californian Ideology\n\n\"The Californian Ideology\" is a 1995 essay by English media theorists Richard Barbrook and Andy Cameron of the University of Westminster. Barbrook describes it as a \"critique of dotcom neoliberalism\". In the essay, Barbrook and Cameron argue that the rise of networking technologies in Silicon Valley in the 1990s was linked to American neoliberalism and a paradoxical hybridization of beliefs from the political left and right in the form of hopeful technological determinism.\n\nThe original essay was published in \"Mute\" magazine in 1995 and later appeared on the \"nettime\" Internet mailing list for debate. A final version was published in \"Science as Culture\" in 1996. The critique has since been revised in several different versions and languages.\n\nAndrew Leonard of \"Salon\" called Barbrook & Cameron's work \"one of the most penetrating critiques of neo-conservative digital hypesterism yet published.\" Louis Rossetto, former editor and publisher of \"Wired\" magazine, called it an \"anal retentive attachment to failed 19th century social and economic analysis\".\n\nDuring the 1990s, members of the entrepreneurial class in the information technology industry in Silicon Valley vocally promoted an ideology that combined the ideas of Marshall McLuhan with elements of radical individualism, libertarianism, and neoliberal economics, using publications like \"Wired\" magazine to promulgate their ideas. This ideology mixed New Left and New Right beliefs together based on their shared interest in anti-statism, the counterculture of the 1960s, and techno-utopianism.\n\nProponents believed that in a post-industrial, post-capitalist, knowledge-based economy, the exploitation of information and knowledge would drive growth and wealth creation while diminishing the older power structures of the state in favor of connected individuals in virtual communities.\n\nCritics contend that the Californian Ideology has strengthened the power of corporations over the individual and has increased social stratification, and remains distinctly Americentric. Barbrook argues that members of the digerati who adhere to the Californian Ideology, embrace a form of reactionary modernism. According to Barbrook, \"American neo-liberalism seems to have successfully achieved the contradictory aims of reactionary modernism: economic progress and social immobility. Because the long-term goal of liberating everyone will never be reached, the short-term rule of the digerati can last forever.\"\n\nAccording to Fred Turner, sociologist Thomas Streeter of the University of Vermont notes that the Californian Ideology appeared as part of a pattern of Romantic individualism with Stewart Brand as a key influence. Adam Curtis connects the origins of the Californian Ideology to the Objectivist philosophy of Ayn Rand.\n\nWhile in general agreement with Barbrook & Cameron's central thesis, David Hudson of \"Rewired\" takes issue with their portrayal of \"Wired\" magazine's position as representative of every viewpoint in the industry. \"What Barbrook is saying between the lines is that the people with their hands on the reins of power in all of the wired world...are guided by an utterly skewed philosophical construct.\" Hudson maintains that there is not one, but a multitude of different ideologies at work.\n\nAndrew Leonard of \"Salon\" calls the essay \"a lucid lambasting of right-wing libertarian digerati domination of the Internet\" and \"one of the most penetrating critiques of neo-conservative digital hypesterism yet published.\" Leonard also notes the \"vitriolic\" response from Louis Rossetto, former editor and publisher of \"Wired\" magazine.\n\nRossetto issued a rebuttal to the original version published in \"Mute\" magazine, calling it \"a totally out-to-lunch excursion\", \"a descent into the kind of completely stupid comments on race in America that only smug Europeans can even attempt\", an \"utterly laughable Marxist/Fabian kneejerk that there is such a thing as the info-haves and have-nots – this is equivalent to a 1948 Mute whining that there were TV-haves and have-nots because television penetration had yet to become universal\", an \"anal retentive attachment to failed 19th century social and economic analysis and bromides is what allows you to claim that the laughable French Minitel system is a success\" and finally, \"a profound ignorance of economics\".\n\nGary Kamiya, also of \"Salon\", recognized the validity of the main points in the essay, but like Rossetto, Kamiya attacked Barbrook & Cameron's \"ludicrous academic-Marxist claim that high-tech libertarianism somehow represents a recrudescence of racism.\"\n\nArchitecture historian Kazys Varnelis of Columbia University found that in spite of the privatization advocated by the Californian Ideology, the economic growth of Silicon Valley and California were \"made possible only due to exploitation of the immigrant poor and defense funding...government subsidies for corporations and exploitation of non-citizen poor: a model for future administrations.\"\n\nIn the 2011 documentary, \"All Watched Over by Machines of Loving Grace\", Curtis concludes that the Californian Ideology failed to live up to its claims:\n\n\n\n\n"}
{"id": "1918246", "url": "https://en.wikipedia.org/wiki?curid=1918246", "title": "United Media", "text": "United Media\n\nUnited Media was a large editorial column and comic strip newspaper syndication service based in the United States, owned by the E. W. Scripps Company, that operated from 1978 to 2011. It syndicated 150 comics and editorial columns worldwide. Its core businesses were the United Feature Syndicate and the Newspaper Enterprise Association.\n\nE. W. Scripps started his newspaper career in the 1885, and owned 22 newspapers by 1910. In 1897, he created two companies, the Scripps-McRae Press Association and the Scripps News Association. In 1907, he combined a number of news providers into United Press Associations as a rival to Associated Press.\n\nOn June 2, 1902, the new Newspaper Enterprise Association (NEA), based in Cleveland, Ohio, started as a news report service for different Scripps-owned newspapers. It started selling content to non-Scripps owned newspapers in 1907, and by 1909, it became a more general syndicate, offering comics, pictures and features as well. At that time, it had some 100 features available.\n\nUnited Feature Syndicate was formed in 1919. It became a dominant player in the syndication market in the early 1930s. In March 1930, United Features acquired the Metropolitan Newspaper Service (ostensibly from the Bell Syndicate). And in late February 1931, Scripps acquired the \"New York World\", which controlled the syndication arms of the Pulitzer company: World Feature Service and Press Publishing Co. (which unlike other syndicates were owned by the paper rather than being separate entities). An April 1933 article in \"Fortune\" described United Feature as one of the \"Big Four\" American syndicates (along with King Features Syndicate, Chicago Tribune Syndicate, and the Bell Syndicate). United Features and NEA both became successful distributors of newspaper comics in the 1930s.\n\nIn 1972, United Features Syndicate acquired and absorbed the North American Newspaper Alliance and the Bell-McClure Syndicate into its operations.\n\nIn May 1978 Scripps merged United Features and NEA to form United Media Enterprises.\n\nIn 1994, Jim Davis's company, Paws, Inc., purchased the rights to \"Garfield\" (including the strips from 1978 to 1993) from United Feature. The strip is currently distributed by Universal Press Syndicate, while rights for the strip remain with Paws.\n\nOn June 3, 2010, United Media sold their licensing arm, along with the rights to \"Peanuts\", to Iconix Brand Group.\n\nOn February 24, 2011, United Media struck a distribution deal with Universal Uclick (now known as Andrews McMeel Syndication) for syndication of the company's 150 comic strip and news features, which became effective on June 1 of that year. Of the more than 40 comic strips United Media transferred to Universal Uclick, about 75% of them were United Features strips (as opposed to Newspaper Enterprise Association strips).\n\nWhile United Media effectively ceased to exist, Scripps still maintains copyrights and intellectual property rights.\n\n\n\n\n\nThese are published on GoComics:\n\n\n\n"}
