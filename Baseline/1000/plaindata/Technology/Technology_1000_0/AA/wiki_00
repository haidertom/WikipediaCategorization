{"id": "4645594", "url": "https://en.wikipedia.org/wiki?curid=4645594", "title": "Advanced Technology Leisure Application Simulator", "text": "Advanced Technology Leisure Application Simulator\n\nThe Advanced Technology Leisure Application Simulator, or ATLAS, is a large hydraulic motion simulator. It was designed, as the name implies, for the theme park industry. The ATLAS is a product of Rediffusion Simulation in Sussex, England, now owned by Thales Group and known as Thales Training & Simulation. Disney filed multiple patents on their variant of the device, including US Utility Patent #5161104 .\n\nThe ATLAS was derived from military flight simulation technology. It uses six hydraulic actuators to provide a broad range of movement.\n\nIn the later half of the 1980s, Walt Disney Imagineering bought and refined this technology for two theme park attractions; Star Tours at Disneyland (and later Disney's Hollywood Studios, Tokyo Disneyland, and Disneyland Paris) and Body Wars at Epcot. The technology was also used in 2016 for the Iron Man Experience at Hong Kong Disneyland. The Disney attractions feature large, 40-person cabins hidden from outside view, arranged lengthwise with four or six simulators per installation. There are four simulators at Disneyland's Star Tours and EPCOT's Body Wars, while the remaining Star Tours installations have six. Body Wars is now defunct and the simulators have been removed from the building in the years since the closure of the Wonders of Life pavilion.\n"}
{"id": "32294777", "url": "https://en.wikipedia.org/wiki?curid=32294777", "title": "Alphagov", "text": "Alphagov\n\nAlphagov was the project name of the experimental prototype website built by the Government Digital Service and launched on 11 May 2011 by the UK Cabinet Office that was open for public comment for two months in order to judge the feasibility of a single domain for British Government web services.\n\nLaunched in response to the report by Martha Lane Fox \"Directgov 2010 and Beyond: Revolution Not Evolution\" that was published in November 2010. Alphagov sought to act as a proof of concept for the way citizens could interact with Government through a series of useful online tools, where they were more useful than published content alone.\n\nAs well as improving the 'citizen experience' of using government web services online the project also identified the potential for £64 million in yearly savings on central government's annual £128 million current web publishing bill.\n\nThis initial consultation period was completed in June 2011. A beta version was then created, which led to the launch of GOV.UK.\n\n\n"}
{"id": "49481503", "url": "https://en.wikipedia.org/wiki?curid=49481503", "title": "Amazon Simple Notification Service", "text": "Amazon Simple Notification Service\n\nAmazon Simple Notification Service (SNS) is a notification service provided as part of Amazon Web Services since 2010. It provides a low-cost infrastructure for the mass delivery of messages, predominantly to mobile users.\n\nFrom the sender's viewpoint, SNS acts as a single message bus that can message to a variety of devices and platforms, from the Kindle Fire to Baidu. A single code interface can address all of these equally, or message formats can be tailored to the particular needs of each platform.\n\nSNS can also deliver messages by SMS to 200+ countries.\n\nSNS uses the publish/subscribe model for push delivery of messages. Recipients subscribe to one or more 'topics' within SNS. Typically this is hidden from the user as an internal part of a mobile app. Receipt of a message may also be hidden from the user: this service is largely aimed at the internal processing of specific apps, rather than as a generic email substitute. A game might receive bonus level announcements or unlock keys for in-game purchases by this route. A ticket booking app could use it for confirmation vouchers, boarding passes or notifications of a delay to a flight.\n\nCosts (2016) are quoted as $1.00 to send one million mobile notifications. Pricing varies according to the delivery mechanism: there is a charge to request a message ($0.50 /million) and a varying charge for how (and if) the recipient chooses to receive it. HTTP is cheapest for retrieval ($0.60 /million), email is around 30 times this ($20 /million) and SMS considerably (hundreds of times) more ($7,500 /million). Like most cloud services, initial access costs are kept low and there are no sign-up or subscription charges.\n"}
{"id": "3135176", "url": "https://en.wikipedia.org/wiki?curid=3135176", "title": "B2MML", "text": "B2MML\n\nB2MML or Business To Manufacturing Markup Language is an XML implementation of the ANSI/ISA-95 family of standards (ISA-95), known internationally as IEC/ISO 62264. B2MML consists of a set of XML schemas written using the World Wide Web Consortium's XML Schema language (XSD) that implement the data models in the ISA-95 standard.\n\nB2MML is meant to be a common data definition to link ERP and supply chain management systems with manufacturing systems such as Industrial Control Systems and Manufacturing Execution Systems.\n\nB2MML is published by the Manufacturing Enterprise Solutions Association (MESA).\n\n"}
{"id": "4934", "url": "https://en.wikipedia.org/wiki?curid=4934", "title": "Basic English", "text": "Basic English\n\nBasic English is an English-based controlled language created by linguist and philosopher Charles Kay Ogden as an international auxiliary language, and as an aid for teaching English as a second language. Basic English is, in essence, a simplified subset of regular English. It was presented in Ogden's book \"Basic English: A General Introduction with Rules and Grammar\" (1930).\n\nOgden's Basic, and the concept of a simplified English, gained its greatest publicity just after the Allied victory in World War II as a means for world peace. Although Basic English was not built into a program, similar simplifications have been devised for various international uses. Ogden's associate I. A. Richards promoted its use in schools in China. More recently, it has influenced the creation of Voice of America's Special English for news broadcasting, and Simplified Technical English, another English-based controlled language designed to write technical manuals.\n\nWhat survives today of Ogden's Basic English is the basic 850-word list used as the beginner's vocabulary of the English language taught worldwide, especially in Asia.\n\nOgden tried to simplify English while keeping it normal for native speakers, by specifying grammar restrictions and a controlled small vocabulary which makes an extensive use of paraphrasing. Most notably, Ogden allowed only 18 verbs, which he called \"operators\". His \"General Introduction\" says \"There are no 'verbs' in Basic English\", with the underlying assumption that, as noun use in English is very straightforward but verb use/conjugation is not, the elimination of verbs would be a welcome simplification.\n\nOgden's word lists include only word roots, which in practice are extended with the defined set of affixes and the full set of forms allowed for any available word (noun, pronoun, or the limited set of verbs). The 850 core words of Basic English are found in Wiktionary's Basic English word list. This core is theoretically enough for everyday life. However, Ogden prescribed that any student should learn an additional 150-word list for everyday work in some particular field, by adding a list of 100 words particularly useful in a general field (e.g., science, verse, business, etc.), along with a 50-word list from a more specialised subset of that general field, to make a basic 1000-word vocabulary for everyday work and life.\n\nMoreover, Ogden assumed that any student already should be familiar with (and thus may only review) a core subset of around 200 \"international\" words. Therefore, a first-level student should graduate with a core vocabulary of around 1200 words. A realistic general core vocabulary could contain 2000 words (the core 850 words, plus 200 international words, and 1000 words for the general fields of trade, economics, and science). It is enough for a \"standard\" English level. This 2000 word vocabulary represents \"what any learner should know\". At this level students could start to move on their own.\n\nOgden's and Voice of America's serve as dictionaries for the Simple English Wikipedia.\n\nThe word use of Basic English is similar to full English, but the rules are much simpler, and there are fewer exceptions. Not all meanings of each word are allowed.\n\nOgden's rules of grammar for Basic English help people use the 850 words to talk about things and events in a normal way.\n\n\nLike all international auxiliary languages (or IALs), Basic English may be criticised as inevitably based on personal preferences, and thus, paradoxically, inherently divisive. Moreover, like all natural language based IALs, Basic is subject to criticism as unfairly biased towards the native speaker community.\n\nAs a teaching aid for English as a Second Language, Basic English has been criticised for the choice of the core vocabulary and for its grammatical constraints.\n\nIn 1944, readability expert Rudolf Flesch published an article in \"Harper's Magazine\", \"How Basic is Basic English?\" in which he claimed, \"It's not basic, and it's not English.\" The essence of his complaint is that the vocabulary is too restricted, and, as a result, the text ends up being awkward and more difficult than necessary. He also argues that the words in the Basic vocabulary were arbitrarily selected, and notes that there had been no empirical studies showing that it made language simpler.\n\nIn the novel \"The Shape of Things to Come\", published in 1933, H. G. Wells depicted Basic English as the lingua franca of a new elite that after a prolonged struggle succeeds in uniting the world and establishing a totalitarian world government. In the future world of Wells' vision, virtually all members of humanity know this language.\n\nFrom 1942 to 1944 George Orwell was a proponent of Basic English, but in 1945 he became critical of universal languages. Basic English later inspired his use of Newspeak in \"Nineteen Eighty-Four\".\n\nEvelyn Waugh criticized his own 1945 novel \"Brideshead Revisited\", which he had previously called his magnum opus, in the preface of the 1959 reprint: \"It <nowiki>[</nowiki>World War II<nowiki>]</nowiki> was a bleak period of present privation and threatening disaster—the period of soya beans and Basic English—and in consequence the book is infused with a kind of gluttony, for food and wine, for the splendours of the recent past, and for rhetorical and ornamental language that now, with a full stomach, I find distasteful.\"\n\nIn his story \"Gulf\", science fiction writer Robert A. Heinlein used a constructed language called Speedtalk, in which every Basic English word is replaced with a single phoneme, as an appropriate means of communication for a race of genius supermen.\n\n\n\n"}
{"id": "30272815", "url": "https://en.wikipedia.org/wiki?curid=30272815", "title": "Change.org", "text": "Change.org\n\nChange.org is a petition website operated by for-profit Change.org, Inc., an American certified B corporation which claims to have over 240 million users and hosts sponsored campaigns for organizations. The company is headquartered in San Francisco, California. The website serves to facilitate petitions by the general public.\n\nPreviously corporations including Virgin America, and organizations such as Amnesty International and the Humane Society, paid the site to host and promote their petitions. Change.org's stated mission is to \"empower people everywhere to create the change they want to see.\" Popular topics of Change.org petitions are economic and criminal justice, human rights, education, environmental protection, animals rights, health, and sustainable food.\n\nChange.org was launched on May 28, 2002, by current chief executive Ben Rattray, with the support of founding chief technology officer Mark Dimas, Darren Haas, and Adam Cheyer. As of February 2012, the site had 100 employees with offices on four continents. By the end of 2012, Rattray stated \"he plans to have offices in 20 countries and to operate in several more languages, including Arabic and Chinese.\" In May 2013, the company announced a $15 million round of investment led by Omidyar Network and said it has 170 staff members in 18 countries.\n\nIn 2011, Change.org claimed it was the subject of a distributed denial of service attack by \"Chinese hackers\", and that the alleged attack was apparently related to its petition to the Chinese government to release artist Ai Weiwei. In 2011, there was a proposal to merge the Spanish-speaking counterpart website \"Actuable\" into \"Change.org\". It took place in 2012 when they approved the voluntary union of \"Actuable\" users into the \"Change.org\" platform.\n\nIn 2012, Arizona State University decided to block access to Change.org in response to a petition created by student Eric Haywood protesting \"rising tuition costs at the school\". University officials claimed that \"Change.org is a spam site\" and the blocking was conducted \"to protect the use of our limited and valuable network resources for legitimate academic, research, and administrative uses\".\n\nIt was reported on April 5, 2012, that Change.org hit 10 million members, and was the fastest-growing social action platform on the web. At that time, they were receiving 500 new petitions per day. On May 13, 2012, \"The Guardian\", BBC News and other sources reported that Change.org would launch a UK-specific platform for petitions, placing Change.org in competition with 38 Degrees, a British not-for-profit political-activism organization.\n\nAn August 2013 \"Fast Company's\" article reported that Change.org would soon begin featuring petition recipients, saying, \"For the first time, companies will be able to post a public response to any given petition (currently, they can only respond to the person who started the campaign). They will also be able to create their own Decision Maker page, which will show all petitions against them, the number of signatures gathered, and their statuses.\" In summer 2017, a petition on change.org called for /r/incels (incel named after an abbreviation for \"involuntary celibate\") to be banned for inciting violence against women.\n\nOn November 12th, 2018, a petition entitled “Contra o aumento de salário aos ministros do STF” exceed the record of signatures with 2,6 million. Much more than \"Aprovem o Pedido de Impeachment da Presidente Dilma\" with 2,23 million.\n\nOn March 8, 2012, a petition entitled \"Prosecute the killer of our son, 17-year-old Trayvon Martin\" was posted on Change.org. The petition received over 2.2 million signatures – at that time the largest number of signatures for any campaign in Change.org's history. The petition called for the prosecution of George Zimmerman, a neighborhood watch volunteer, who, on February 26, 2012, shot and killed Trayvon Martin in Sanford, Florida, a suburb of Orlando. Zimmerman said he was acting in self-defense, and was set free without being charged. On April 11, 2012, Zimmerman was arrested and charged with second-degree murder. He stood trial in the months of June and July and was acquitted of all charges on July 13, 2013.\n\nOn October 1, 2011, Molly Katchpole, a \"22 year old nanny with two jobs\" in Washington D.C., started a petition on Change.org \"asking Bank of America and their CEO Bryan Moynihan to drop its unexpected new $5/month banking fee\" for debit card customers. Less than one month later, 300,000 signatures were collected. The petition was widely cited as a contributing cause for the bank formally announcing to drop the new banking fee. U.S. President Barack Obama signed the petition; U.S. Senator Dick Durbin, the Democratic senator from Illinois, responded to Bank of America and the petition on Twitter. It may have contributed to the U.S. Congress deciding to \"look at legislation for out-of-control banking fees\".\n\nIn December 2011, a fourth-grade class in Brookline, Massachusetts, launched the \"Lorax Petition Project\" through Change.org requesting Universal Studios to include more of an environmental message on its website and trailer for its upcoming film, \"The Lorax\", a classic Dr. Seuss children's story. The website and trailer lacked the important message from the book, \"to help the environment\". The petition collected over 57,000 signatures, and on January 26, 2012, the studio updated the website \"with the environmental message the kids had requested\".\n\nOn the morning of February 2, 2012, Stef Gray, a 23-year-old graduate in New York, held a news conference at the Washington offices of Sallie Mae where she presented the results of her Change.org, Sallie Mae, the \"nation's largest private student-loan provider\" petition, which had received about 77,000 signers. That afternoon the company changed its forbearance fee policy.\n\nIn November 2013, someone calling himself \"John Doe\" of Arlington, Texas launched a petition against changes made to the YouTube commenting system by Google. The changes force YouTube users to create an account on Google+ and also removes the \"reply\" mechanism on comments unless they were posted on Google+. This petition received over 100,000 signatures in less than a week, and over 200,000 within two weeks. The petition garnered more than 241,000 signatures, but failed to change Google's stance on the matter. However, the changes to YouTube did not have the intended effect – namely, increasing spam instead of curbing it – and in mid-2015, the commenting system was altered to allow users to post comments without signing into Google+.\n\nIn November 2013, Aaron Thompson from Tuscaloosa, Alabama started a petition, directed at Seth MacFarlane to bring back Brian Griffin on the TV series \"Family Guy\", after he was briefly killed off in the Season 12 episode \"Life Of Brian\". Thompson's petition gained 30,000 signatures within 36 hours. The character was brought back to the show a few episodes later.\n\nIn September 2014, Karol Wilcox of Hayti, Missouri started a petition against the planned execution of Beau, a two-and-a-half-year-old dog in Dyersburg, Tennessee, for allegedly killing a duck on his owner's property. By November, this petition had gained over 540,000 signatures. The petition worked and the dog was spared.\n\nAfter the 2016 United States presidential election, in which Donald Trump was declared President-elect of the United States, there were mass protests. As part of these protests, one California man started a change.org petition on November 10, 2016, which called for electors in states that Trump won to become faithless electors and cast their vote for Hillary Clinton instead at state Electoral College meetings. The petition acquired over 4 million signatures by November 14, 2016, only 4 days after it started. By November 23, 2016, it had gotten 4.5 million signatures. The petition ultimately failed as, on December 19, 2016, Trump officially gained the presidency with 304 electors. The petition closed with 4.9 million signatures, the highest in change.org history to date.\n\nOn November 3, 2017, following sexual assault claims made against actor Kevin Spacey, Netflix fired the actor from, and stalled the production of, the sixth and final season of the television series in which Spacey had starred in on the network, \"House of Cards\". Following Spacey's dismissal, a petition created on November 2, 2017, calling for Spacey to be replaced by actor Kevin James as a post-plastic surgery Frank Underwood began gaining a rapid number of supporters; this petition has gained media notability since its inception, gaining 50,000 supporters within eight days.\n\nIn wake of the Logan Paul suicide video controversy, user \"... - .- -.-- .- .-.. .. ...- .\" (\"stayalive\" in Morse code) created a petition entitled “Delete Logan Paul’s YouTube Channel\", having received more than 520,000 signatures as of 15 January 2018. While numerous other petitions have been created for the same purpose, none have received as much attention.\n\nOn 10 March 2015, the political blogger Guido Fawkes, whose real name is Paul Staines, started a petition to reinstate Jeremy Clarkson, BBC co-host of TV series \"Top Gear\". This followed the BBC's decision to suspend him over a \"fracas\" involving a producer on the show. The petition gained over 500,000 signatures within 24 hours, making it the fastest growing petition to date for the site, while having the servers at Change.org in the UK regularly become unresponsive due to the high demand. It had gained over 1,000,000 signatures by 20 March 2015 and it was delivered to the BBC.\n\nIn August 2014, Erica Perry from Vancouver, BC, started a petition asking Centerplate, a large food and beverage corporation serving entertainment venues in North America and the UK, to fire its then-CEO Desmond \"Des\" Hague after the public release of security camera footage allegedly showing Hague abusing a young Doberman Pinscher in an elevator. In response to Centerplate not taking action after the incident other than releasing a statement of apology from Hague, and an agreement by Hague to commit to perform certain charitable acts, the petition called for Centerplate to fire Hague. On September 2, 2014, after the petition had received over 190,000 signatures, Hague resigned from his position as CEO of Centerplate.\n\nIn February 2016, 50 petitions have exceeded 100,000 signatories. A petition against the \"Loi El Khomri\", a labor law project by the French Labor Minister Myriam El Khomri has over 1 million signatures, a record for change.org in France.\n\nIn 2012, Philip Matesanz, a 21-year-old German university student, started a petition to allow third-party recording tools for YouTube. The petition garnered more than 4.3 million signatures. This was the largest number of signatures in the history of Change.org until the record was beaten in November 2016.\n\nIn February 2013, over one million people, around two percent of the total population of Spain, had signed the petition calling for the entire Spanish government to resign. The call was motivated by an unprecedented corruption scandal involving the majority of key leaders of the People's Party.\n\nThe petition to have death row convict Mary Jane Veloso released was the fastest ever growing petition from the Philippines with over 250,000 signatories from over 125 countries.\n\nThe petition of \"Affordable internet for all\" was started by an Indian blogger, YouTuber Amit Bhavani. To raise the voice against increasing prices of internet. The petition reached 75,000+ sign ups within 48 hours.\n\nThe petition of \"Justice for Asifa: 8yr old girl raped and murdered in Kathua, J&K \" got 2 million supporters with over 1.5 million signatures.\n\nThe petition of Kumar Ajitabh's kidnapping case has got 7K supporters (#FindAjitabh)\n\nThe petition of banning, TV show, Pehredaar Piya Ki received support in the form of 1 lakh people due to promoting child marriage.\n\nThe petition against \"Australian Citizenship Eligibility Changes April 2017\" is ongoing with over 75,000 signatures.\n\nIn 2018 an anonymous creator of a Facebook community built on hatred for people riding bikes started a change.org anti-cycling petition that has grown in a short time to reach over 100,000 signatures. Allegedly, there is evidence many of the names on the petition are fake. Change.org advised they were not able to provide any information to substantiate any such claims of fake names being used.\n\nAfter Snapchat released their new update in February 2018, Nic Rumsey from Australia created a petition to remove this update after a huge number of people provided feedback to the effect that they were not enjoying the update. The petition has over 1,100,000 signatures as of February 16, 2018.\n\nA petition commenced in October 2018, gaining over 300,000 signatures, to prevent a proposal to use the Sydney Opera House as a promotional billboard. Racing NSW proposed using the sails of the Sydney Opera House to promote the Everest horse race at Sydney's Royal Randwick on 13 October 2018.\n\nAfter two earthquakes hit Central Mexico on September 7th and September 19th, 2017, there were different petitions to force the \"Instituto Nacional Electoral\" (\"National Electoral Institute\"), the Mexican Senate, and President Enrique Peña Nieto to donate most or all of the money destined for the upcoming 2018 general elections be redirected to victims of the natural disaster in Mexico City and neighbor states of Morelos, Chiapas, Oaxaca, and Puebla. All petitions together sum the number of more than 3 million signatures. The petition of Alfredo Aguirre from Monterrey,Mèxico reached 1,000,000+ sign ups within 24 hours. It was a new world record of Changeorg's history.\n\nChange.org makes revenue through a subscription membership model, people promoting petitions on the site, and crowdfunding.\n\nChange.org members contribute monthly to sustain the technology and the small teams of campaigners who coach and support petition starters. The majority of the company's revenue is advertising - individuals and organisations who start or sign petitions then chip in to promote those petitions to other site visitors.\n\nTo date, Change.org has raised $50 million to fuel its growth from mission-aligned investors in business, technology and the media. In 2017 an investment round driven by Reid Hoffman helped drive the shift to the current business model.\n\nThe website previously made revenue by running advertisements called sponsored campaigns for advocacy organizations such as Amnesty International \nand list-building services to partner organizations.\nIn May 2013 the website started \"crowd-promoted petitions\" that allowed a signatory to promote the petition by paying $5 to $1000 at the final stage of petition signing.\n\nMassive Australian based anti-cycling petition is full of false names, claims Bicycle Queensland CEO. Anne Savage said the body had received information that many of the names were created by electronic “bots”. “We have evidence that the majority of names on that petition are false names,” she said.\n\nUnder certain conditions, signatures and other private information including email addresses can be found by search engines. Change.org operates a system for signature hiding, which works only if the user has an account on Change.org. Conversely, the platform has been criticized for not providing enough information on who has signed a petition; for instance a means of verifying that a petition protesting a politician has been signed by his or her constituents or that the signatures are genuine at all.\n\nThere has been debate and criticism around the fact that Change.org is a for-profit business despite using the .org domain suffix rather than the commercial .com. The site has been accused of fooling its users and hiding the fact that it is \"a for-profit entity that has an economic incentive to get people to sign petitions\".\n\nChange.org spokesperson Charlotte Hill countered this criticism in a September 2013 article in \"Wired\", saying, \"We are a mission-driven social enterprise, and while we bring in revenue, we reinvest 100% of that revenue back into our mission of empowering ordinary people. It's not just that we're not yet making a profit – it's that we are decidedly not for-profit.\" \n\nIn 2012, the site dropped most of the restrictions it previously placed on paid content. Internal documents began referring to \"clients\" and \"partners\" as \"advertisers\" and stated that \"only advertisers strictly identified as 'hate groups' are to be barred.\" As a result, Change.org was accused of encouraging astroturfing and abandoning the progressive user base from which it initially gained traction. Additional controversy arose when the employee who initially leaked the documents was fired. Of the users who lost interest in the site after this change, a number of them expressed difficulty in being removed from Change.org mailing lists.\n\nChange.org has also been accused of selling the personal data provided by the users to third-party companies that hire its services, gaining money at the expense of the users.\n\nTopics for Change.org petitions have grown to include disagreement with the Academy Awards and removing milk from certain types of coffee. The authors of these petitions have been criticized for focusing on first world problems. Further debate over the content of petitions came in November 2014 when Martin Daubney called some of them \"bizarre\" and stated that the site was being used to promote censorship. In response, the Change.org communication director John Coventry defended the wide range of petitions, saying that \"people make an informed choice in what they want to support.\" The following week saw criticism alleging that petitions about the media receive more attention than petitions about \"saving 'actual' lives.\"\n\n"}
{"id": "10371312", "url": "https://en.wikipedia.org/wiki?curid=10371312", "title": "Clymer repair manual", "text": "Clymer repair manual\n\nClymer repair manuals are vehicle repair manuals that often focus on powersport vehicles such as motorcycles, all-terrain vehicles, personal water craft and snowmobiles. Clymer also has several books dedicated to small engines and \"outdoor power equipment\" such as leaf blowers, chainsaws and other lawn and garden power equipment.\n\nClymer repair manuals are named after their creator Floyd Clymer, who is described in the Motorcycle Hall of Fame as:\n\n\"a pioneer in the sport of motorcycling. He was a racer, a motorcycle dealer and distributor, a magazine publisher, a racing promoter, an author and a motorcycle manufacturer.\"\nClymer repair manuals are categorized as an aftermarket product or non-OEM. Unlike OEM manuals, Clymer repair manuals are written specifically for the do it yourself as well as the professional and experienced mechanic. OEM manuals are often designed for a professional technician, who often has at their disposal an array of specialized tools, equipment and knowledge.\n\nOne valuable step in the creation process of a Clymer repair manuals is the complete disassembly and reassembly of the machine. This helps the writers for Clymer provide easy-to-follow instructions that allow the manual user to safely and efficiently service and repair their machine. This high level of detail sets Clymer apart from its factory \"OEM\" counterparts.\n\nIn 2013, Haynes Publishing Group acquired Clymer repair manuals from Penton Media.\n\nClymer currently has over three hundred repair manuals that cover thousands of models. Some of the most popular models are the Honda TRX ATVs, International Harvester Farm Tractors, BMW K1200 Series, Harley-Davidson FLH & FLT Twin Cam 88 & 103 models and the MerCruiser Stern Drive Marine Engines.\n\nHere are some of the manufacturers covered in the Clymer library:\n\n\n\n"}
{"id": "18010409", "url": "https://en.wikipedia.org/wiki?curid=18010409", "title": "Collaborative journalism", "text": "Collaborative journalism\n\nCollaborative journalism is a mode of journalism where multiple reporters or news organizations, without affiliation to a common parent organization, report on and contribute news items to a news story together. It is practiced by both professional and amateur reporters. It is not to be mixed up with citizen journalism.\n\nCollaborative journalism involves the aggregation of information from numerous individuals or organizations into a single news story. Information is gathered through research or reporting, or added when readers examine, comment and build upon existing stories. Stories from the mainstream media are often built upon. Depending on the system of collaboration, individuals may also provide feedback or vote on whether an article is newsworthy. A single collaborative news story, therefore, may encompass multiple authors, varying articles, and ranged perspectives.\n\nProfessional and amateur reporters may work together to develop collaborative news articles, or mainstream media sites may gather amateur blog posts to complement reporting.\n\nCollaborative journalists either contribute directly to stories, sometimes through a wiki-style collaboration platform, or build upon the story externally, often through personal blogs. Collaborative journalists develop or examine a story one piece at a time. This contrasts the deadline and completion-centered nature of traditional media. A story is built upon continually, and a popular story may receive daily updates. Through combined authorship, collaborative journalism is thought by some to offer an increased independence of thought and experience unavailable to traditional media.\n\nSuccessful collaborative journalism projects require a participatory community with respect for content. Ross Mayfield, CEO of SocialText, has commented on wiki-style collaborative journalism:\n\n\"Most user-generated content isn't content, but conversation. Cultivating community is a decided practice. It boils down to the social contract you make with your readers-turned-writers. If they trust that their effort and words will be appropriated appropriately, while providing social incentives for participation, it can very well work.\"\n\nCollaborative journalism emerged through the internet in the early 2000s, and developed gradually through various online outlets. As examples, Wikinews was founded in 2003, and NewsVine in 2005.\n\nThe Panama Papers project may be the largest example of a journalistic consortium to date. It began sometime in 2015 (date?) when Bastian Obermayer, an investigative reporter with the south German newspaper Süddeutsche Zeitung, was contacted by an anonymous source and offered the trove of 11.5 million electronic documents from Mossack Fonseca, the world’s fourth biggest offshore law firm detailing a web of secret offshore deals and loans worth billions of dollars, and details of tax avoidance designs in numerous countries. The newspaper's editors decided they could not handle the massive volume of information alone and initiated a collaborative journalistic consortium including more than 140 journalists and the International Consortium of Investigative Journalists, a project of the nonprofit Center for Public Integrity.\n\nThe European Investigative Collaborations (EIC) working with \"over 60 journalists in 14 countries\" published a \"series of articles called \"Football Leaks\"—the \"largest leak in sports history\". \"Football Leaks\" \"led to the prosecution of football superstar Cristiano Ronaldo and coach Jose Mourinho.\" EIC was established in the fall of 2015 with founding members that include \"Der Spiegel\", \"El Mundo\", \"Médiapart\", the Romanian Centre for Investigative Journalism (CRJI), and \"Le Soir\".\n\n\n\"Link journalism\", a phrase coined by Scott Karp in 2008, is \"a form of collaborative journalism in which a news story's writer provides external links within the story to reporting or other sources on the web.\" These links are meant to complement, enhance, or add context to the original reporting. Jeff Jarvis, from the Graduate School of Journalism's new media program at the City University of New York, has said that link journalism creates a \"new architecture of news.\"\n\nCollaborative journalism has been implemented in several different ways. Wikinews, the \"free-content online news source,\" lets any user edit or create a news story, similar in style to Wikipedia. Several mainstream news sites have adopted a collaborative journalism approach toward news, through use of news aggregation. The Washington Post has developed a political site which links to related content from other news sites. NBC links to local newspapers, radio broadcasts, online videos, and blogs on its local television stations' sites. The sites do not separate articles written by NBC staff and links to outside sources.\nThe New York Times has introduced a \"Times Extra\" website feature which acts posts links to outside news sites. Commenting on the launch of \"Times Extra\", Marc Frons, CTO for Digital Operations at the New York Times, said:\n\n“In the past, I think many news organizations were afraid to link to other Web sites out of fear that they might be sending people to an unreliable source or that their readers would never return. But those fears were largely misplaced and we’ve seen a much more open policy when it comes to pointing readers at useful content elsewhere on the Web.\"\nOther sites exhibit collaborative journalism through aggregation. On the site NewsVine, for example, wire stories from the Associated Press complement user-generated stories and blog posts. Reddit and other news aggregation sites may also act as collaborative journalism sites, depending on where content originates.\n\nDue to the increase in collaborative journalism, several organizations have begun to offer grants or awards for these types of projects. For example, Online Journalism Awards (launched in May 2000) added a new award category for collaborations and partnerships. Clean Energy Wire offers grants for collaborative journalism projects on the topic of energy or the climate. The annual Hostwriter Prize awards money to support pitches and published collaborative projects by journalists.\n\nCollaborative journalism has received some criticism:\n"}
{"id": "31798863", "url": "https://en.wikipedia.org/wiki?curid=31798863", "title": "Compact surveillance radar", "text": "Compact surveillance radar\n\nCompact surveillance radar are small lightweight radar systems that have a wide coverage area and are able to track people and vehicles in range and azimuth angle. They weigh less than 10 pounds, consume less than 15 Watts of power and are easily deployed in large numbers. \n\nCompact surveillance radar have the same characteristics of the larger Ground Surveillance Radar(GSR) namely; the ability to track many moving targets simultaneously, all weather day & night operation, wide coverage areas and the ability to track targets and cue cameras automatically.\n\nCSR Manufacturers \n"}
{"id": "8022654", "url": "https://en.wikipedia.org/wiki?curid=8022654", "title": "DVD6C", "text": "DVD6C\n\nThe DVD6C Licensing Agency or DVD6C Licensing Group is an industry consortium which licenses a portfolio of patents required to produce DVD discs, players, drives, recorders, decoders, and encoders.\n\nThe group comprises 9 members: Hitachi, JVC, Matsushita (Panasonic), Mitsubishi, Sanyo, Sharp, Toshiba, Warner Home Video and Samsung.\n\n\n"}
{"id": "6890125", "url": "https://en.wikipedia.org/wiki?curid=6890125", "title": "Data auditing", "text": "Data auditing\n\nData auditing is the process of conducting a data audit to assess how company's data is fit for given purpose. This involves profiling the data and assessing the impact of poor quality data on the organization's performance and profits.\n"}
{"id": "55796083", "url": "https://en.wikipedia.org/wiki?curid=55796083", "title": "Digital divide by country", "text": "Digital divide by country\n\nThe digital divide is an economic and social inequality with regard to access to, use of, or impact of information and communication technologies (ICT). Factors causing the divide can vary depending on the country and culture, as can the potential solutions for minimizing or closing the divide.\n\nThe following is a list of countries that have a digital divide along with contributing factors and steps the country is taking to resolve the issue.\n\nThe digital divide in Argentina is the term used to describe the technological and connectivity gap between different demographics and regions, specifically the gaps present in the country of Argentina. The availability and access to the Internet in Argentina shows how the trends of a digital divide stay constant, even across different countries and cultures. Age is one of the main factors, and Argentina's statistics show a common disparity between ages. According to TNS Infratest, the age group with the highest rate of daily internet users in 2016 were users under 25. 91% of that age demographic were polled as being daily internet users. In contrast, the age group with the lowest rate were users over 55, with only 68% of that demographic being daily internet users. The older one is, the less likely they are to use Internet, and these statistics reflect that. The way that users are able to access the Internet can also show the status of Internet and how easily one can utilize it. In Argentina, 48% of users preferred to use a smartphone or tablet to a computer, 29% used them equally, and 16% preferred a computer to a smartphone or tablet.\n\nThe quality of life in Argentina is among one of the highest in South and Latin America, according to InterNations. The children are well educated in school, and they are given numerous opportunities to pursue afterwards. The Government created a project named \"Conectar Igualdad\" (Connecting Equality) where they distribute netbooks to students and teachers in primary school, special needs schools, and state institutes. Since the project began nearly 4,000,000 netbooks have been dispersed amongst them to 400 different schools and 7,500 different teachers. The program works in three parts: an internet connection to Wi-Fi, an established internet connection through a reliable network (giving students Internet access anywhere, even at home), and making use of a syntonizer for Digital TV, allowing students access to interactive live programs. In 2011, the Government launched another program named \"Escuelas de Innovacion\" (Schools of Innovation). The main goal is to guarantee students better access to education and technology. The Government of Argentina creates new and innovative projects each year for students and teachers to have greater access to the Internet. It plays an important role in their education and is a great opportunity for students and teachers to close the gap in the digital divide.\n\nImmorality and ICT holds are the major contributor to stunt in Africa's development. The geography factor focuses more on how an individuals' location put them at an advantage or disadvantage to compete with the digital age. However, only a handful on people and communities are being represented. Underdeveloped geographical locations, like the continent of Africa serves as one of the underrepresented minorities. In a study conducted in 2011, they estimate that internet access is only available to roughly fourteen percent of the African population. This means that while the world’s population is only composed of fifteen percent of Africans, around six percent of that subscribe to the internet. \n\nThe digital divide in Bangladesh is a gap in the population of Arshads who can access information and communication technologies (ICTs) and those without proper access or skills.\n\nInformation and communication technologies use in Bangladesh began to grow in 2005 with the introduction of market liberalization.\n\nThe government has been trying to help increase ICT availability to all Bangladeshis by reducing taxes on the VSAT and creating special projects to support the spread of ICTs. In 2006 the largest mobile phone company in Bangladesh, GrameenPhone Ltd, introduced Community Information Centers (CICs), which are each equipped with two computers, printers, digital cameras and web cameras that can be used by the community. \"There are over 500 CICs operating in 450 subdistricts in Bangladesh\". Within the community, there is research suggesting that men benefit more from ICTs than women due to Bangladesh's gender discrimination. Bangladesh has the potential to grow and gain more access to ICTs if they continue to reduce government limitations and grow their economy.\n\nThe education system in Bangladesh is divided up into three stages of schooling: primary school, secondary school, and a level of more formal education. Primary school lasts for five years for children aged 6 to 10, secondary school lasts for seven years, and the formal education level can last anywhere from two to six years. Government policies have been implanted since 2000 that has caused the Information Technology (IT) industry to grow in Bangladesh. Students started going to school in order to become a part of this growing workforce, taking lessons in English language, analytical thinking, and software development. The youth of Bangladesh are becoming educated in the technology available, which is a step in ending Bangladesh’s part in the global digital divide. A country can have access to technology but if the people do not understand how to use the technology than there is no difference between having it or not. Satisfaction and gratification from using the technology are key points in bridging the gap because the sense of accomplishment give the people confidence to use the technology.\n\nThe digital divide in Canada is impacted by several factors that include the differences in the availability of online connectivity resources in different locations across the country, levels of digital literacy, and economic levels.\n\nInternet Censorship\n\nSites in which allow for the regular and free interaction between Chinese citizens, generally come with some restrictions. This will include social media sites, forums, and other sites which are heavy on user interactivity. Google has also been heavily restricted in China and this includes all of its different features.\n\nThe digital divide in Colombia refers to the absence of Information and Communication Technology (ICT) and how it affects Colombian individuals and its society as a whole. The main lapse of technology and information lies in the physical access realm of things, where it is lacking tremendously. Though Internet in Colombia has made progress compared to recent years, the scholary part of how ICTs are used are still in question. With this being said, government officials have made certain that access to ICTs be a priority in their country. In addition to physical access, literature review and types of internet use have also been main points of focus in regards to a solution for digital divide in Colombia.\n\nThere is a difference between material or physical access and actual use of ICTs. \"According to Hargittai (2008),\ndifferences in the uses of ICTs have important implications for life outcomes\". \"Rojas and Puig-i-abril (2009) found that the most prevalent activities of Internet users in Colombia were checking e-mail, consuming entertainment content, chatting with friends, and consuming news and information.\" There are different levels of \"use\" including, non-use, low use, and frequent use while also taking into consideration the opportunities taken by the users. There is a difference in those who go online just for fun and those who use the internet for the improvement of themselves. \n(International Journal of Communication (19328036)) (Digital Divide in Colombia: The Role of Motivational and Material Access in the Use and Types of Use of ICTs).\n\nOne digital divide study displayed physical access as \"having\"/\"not having\" access to the internet. Those who are less fortunate have been shown to lack educational advantage, due to the fact of not having access to internet as common as those who are more fortunate in wealth. Communities that are more technological advanced offers their communities to expand their knowledge and be able to communicate with other worlds. The intellectual advantage is common for countries, such as Colombia, where the poor communities are lacking the opportunities offered to those who are on the other side of town with more advantage. Poor communities are focusing on trying to make ends meet, and working hard labor without being offered to make a change to better themselves. The internet plays an important role in this, due to the fact that the internet provides opportunities to explore new jobs and to widen individuals' knowledge. In certain areas where the communities are wealthy, the internet is provided at the access of their hands. Being able to access the internet so easily, gives privileged individuals the chance to go beyond their expectations and get their point out into the world.\n\nThere is no singular cause of the Digital Divide in Colombia. A number of different factors each contribute to the variance of access to technology within Colombia. These factors include location, economic class, education, and others.\n\nIn Colombia, those located in large cities are 69% more likely to use the Internet. The high correlation between the use of ICTs and location in large industrial cities is one of the most visible examples of digital divide in Colombia. People located in rural parts of Colombia are significantly less likely to use ICTs and the internet regularly for various reasons. One leading cause is simply because people living in rural parts of the country tend to be less educated which is negatively correlated with technology use. The pastoral parts of Colombia are often much less developed than the large cities. Often, these rustic areas lack the technology and internet providers needed to further bridge the digital divide between large and small cities. A study of the Digital Divide in Colombia concerning residency included a population in which 21.6% of respondents lived in a small or medium-sized city (<1,000,000 inhabitants), while 78.4% lived in larger urban centers. This study showed that the vast majority of ICT usage occurred in large urban areas.\n\nThe importance of technology and its relationship with economic development was synthesized by Solow (1987a) when he stated that “technology remains the dominant engine of growth, with human capital investment in second place.” Concerning economic development and moves to improve it, it is an understood phenomena that countries with the highest availability and use of technology are the countries with the best economies. Technology has not advanced in all societies as far as many thought it would be according to today’s societal expectations. This lack of advancement mainly occurs in communities or organizations whose organizational context differentiates from a more skillful and high value profile to a context of autonomy. “In non-industrial contexts, non-computerized societies and organizations maintain more traditional routines, because they simultaneously face the absorption of techniques and instruments, while copying their preexisting idiosyncrasies, environments, and routines. In these cases change is perceived as expensive, time consuming and risky, producing sentiments that facilitate phobic, indifferent or stereotyped attitudes towards technology”. These preconceptions towards technology play a small role in the digital divide in Colombia. Among society’s who are not on the same accord in regards to technology and its benefits, attitudes towards technology show a distinct display of the amount of technological dependence a community has.\n\nThe two ideas of cultural preservation and cultural modernization create a divide amongst the world of industry and the world of agriculture. The industry field often involves research and development. Societies involving research and development often require skill in many different areas with products that's value is highly regarded. Both the qualitative and quantitative aspects of the products creates a need for enhanced work material, which oftentimes is the latest and fastest technology. Industrially, in Colombia this is understood and industries strive, strengthening the different industries and corporations all over Colombia. Colombia’s somewhat, partial digital divide stems from areas of Colombia where locals are resistant to change. In 2003, Avgerou found this resistance to change to be due to the uncertainties of technology among the rural population. Gille referred to what was called the Technological System. This idea of Gilles suggested that technology would eventually transform social life all together by connecting technologies and everyday productive routines. Communities often adopt things that are for the betterment of the entire community. Rural populations and communities are often close knit and every one often shares the same views. The idea of technology is not necessarily one that everyone is going to agree with, especially when a community has done with out considerable updates to their prior owned technology for a long period of time. People admire their culture and want to see that it stay the same, with no flaws or inconsistencies. If the technological system adopted by the community is inconsistent or non-competitive, the host is limited by these gaps and builds and inefficient technical rationality. The combination of reverence of community and culture, and possibilities of technological gaps delays technological advancement into many developing communities and this creates a digital divide. While information technology is thriving in the developed parts of Colombia with industries and corporations using it to their economic advantage, the rural and developing side of Colombia still has some ground to cover. This delayed progress of information technology can be mended through information technology programs that teach and enable developing communities how to use technology. Progress can also be made through built trust and reassurance through the government about modernized technology.\n\nEducation is oftentimes viewed as the most important aspect of a society. The relationship between quantitative and qualitative productions of learning and the effort invested in the process, measured with respect to the goals set by the educative institution, the available resources and the needs from the environment, educational productivity is defined. Though education levels tend to depend largely on strength of the governments economy, other factors, including educational programs and community involvement can have an impactful effect. The country of Colombia is primarily rural and is mainly represented by remote areas. The general population of Colombia perceives the Internet as not useful and the purchasing power of Colombians is limited. This displays that access to technology is not necessarily essential and, or available. Communities from developing territories end up producing superficial changes in their tortuous transition to the new ICT paradigm, unable to keep up the pace in developing computer skills. The problem is not just the access to tools; it includes the construction of a compatible social, cultural and economic logic (Avgerou, 2003), that, due to the resistance to change from some of the local stakeholders, turns into a complex and slow process. It involves sacrificing some of the distinctive particularities of the community.\n\nThe CPE (Country Program Evaluation) allows the transfer of technology to rural areas to be a much smoother one by training teachers to properly use technological information. This knowledge is taken by teachers and aimed to increase levels of incorporation, adaptation, and integration of technologies as required for achievement of sustainable growth in Colombia, ensuring increased productivity and competitiveness while consolidating the quality of the Colombian educational system. Overall, Colombia’s information technology presence in public schools are relatively normal. Areas of Colombia where poor educational infrastructure is combined with an economy focused on natural resources and its exploitation, are areas with school systems that focus on technical training concerning the production of basic goods rather than long term knowledge and innovation skills. In regards to higher education and universities in Colombia information technology infrastructure is growing at an alarming rate. From possessing updated technology in the classroom to technological information possessed by those in charge, ICT’s (Information Communication Technology) in the Colombian school system is currently at paramount.\n\nIn 2010, President Juan Manuel Santos launched \"Vive Digital,\" a government initiative aimed to create jobs, improve economic growth and development, and—most importantly—reduce poverty. \"Vive Digital\" does this all through the embracing of financial opportunities rooted in the production and utilization of technology. Within the past four years, Vive Digital has changed the digital landscape in Colombia. The largest impact has been on the poorest citizens. Colombia has gone from 3.1 million Internet broadband connections in 2010 to 9.9 million in mid-2014, and Internet penetration for small and medium enterprises (SMEs) increased from 7 percent in 2010 to more than 60 percent in 2014. Another initiative the Colombian government has served the people of Colombia with is the National Broadband Policy of the Ministerio de Tecnologías de la Información y las Comunicaciones (Ministry of Information Technology and Communications) which has increased the percentage of Colombian municipalities connected to the Internet from 17 percent in 2010 to 96 percent today.\n\n\"By 2018, the government hopes to have 63 percent of the country connected to broadband. And according to 2013 GSMA mobile economy figures, there are already 43.9 million mobile connections and 24 million mobile users in a country whose 47 million people give it the third largest population in Latin America and third largest Spanish-speaking population in the world.\"\n\nIn May 2014, Hughes Network Systems LLC, one of the leading broadband solutions and providing services in the world, was granted a contract for an HX broadband satellite system and terminals by HISPASAT Colombia. This contract came as the result of the \"Vive Digital\" government initiative. Hughes Network's increased presence within Colombia has exponentially increased the use of information and communication technologies in Colombia.\n\nThere are two main issues with the digital divide in Colombia that is being researched on. Those two are: how the information is being delivered and, how the information that is being delivered throughout the academic community is working and communicating. The solutions to these issues is getting everyone to focus on the main idea that the way they are communicating with the community is important and vital to civilization. It is important to send out accurate information, and not inform the community with false accusations. Research has shown that the delivery of information has become a vital aspect to Colombia and it includes the way everyone lives their daily lives. Researchers are focusing on the wide spectrum of how information is being spread, whether it is via internet, digital broadcast, mobile communication, or social media networks. With all of the research, it has also been noted to inform the community with the production of interpersonal communication and improvement in technological developments. The goal of the research is to improve the digital divide for Colombia's community to provide an adequate form of communication via the participating communities.\n\nThe gender digital divide in Middle Eastern countries causes women to have far less access to the internet than men. Statistically, women are held at lower standards in the Middle East, which is a reason for this digital division. Males are using the internet at a 47.7% rate with women at only 39.4%.\n\nThe digital divide in Ethiopia has caused a massive lack of information for many poorer individuals, however, the greatest failure relates to education. Between intense government regulation as well as economic factors, it is nearly impossible for the average person to access the internet. With such a gross lack of access to technology and the internet, education is stunted and children are forced to rely on archaic, biased, and often unreliable sources of information. \n\nThe CIA notes that only 15% of the Ethiopian population is connected to the Internet placing the country at 107 of the 217 countries ranked in the study.\n\nIn 2016, the Ethiopian government blocked all social media sites after an anonymous user posted copies of university entrance exams online. In an interview with Reuters News Agency, Mohammed Seid at the Office for Government Communications Affairs, said only social media sites were blocked but BBC News received conflicting reports from Ethiopian citizens who claimed that they experienced problems with both mobile networks and fixed line Internet services.\n\nRoughly 40% of schools in Ethiopia have computers. Of that 40% most of the schools are located in Ethiopia'a capital, Addis Ababa. This in turn has created a divide between schools in urban cities like Addis Ababa and schools in rural areas. Most schools that are connected to the Internet have limited access and only use it for email that's solely for administrators. Many universities and higher learning institutions in Ethiopia have computers but the average ratio of students to computer is about 10:1. Only 15% of private universities in Ethiopia participate in Electronic Distance Education which is also known as online school. For many students, online education is a more convenient alternative to pursuing higher education because of their location.\n\nThere are two major initiatives aimed at bridging the digital divide gap. SchoolNet Ethiopia is a joint initiative between Ministry of Education and the United Nations Development Programme that has equipped 181 schools with 15 networked computer labs. The Distance Learning initiative backed by The Ministry of Education has been working with the Indira Gandhi National Open University in India to provide master's degrees to students from the University of Addis Adaba in Ethiopia.\n\nIn a world almost completely reliant on technology and access to the internet, the digital divide in Ethiopia poses a major problem for its citizens and the effects are nearly detrimental in the current global economy. Without access to the internet, Ethiopia cannot compete with the rest of the world and this has caused many gradual effects. These include lack of education, continued economic depression, and a gross lack of transparency in the Ethiopian government, as well as many other smaller, trickle down effects. \n\nOne of the greatest effects the digital divide has one Ethiopia is the lack of education. Approximately 39% of adult Ethiopians can read and a mere 55% of school aged children actually attend school full-time. With an essentially non-existent public education system in this poverty stricken country, the current lack of digital access does not help this matter. An improved access to the internet could help better educate Ethiopian children and with decreased costs as well.\n\nAnother effect of the gross disparity in digital access is the economic impact on Ethiopia's economy. Ethiopia is currently one of 19 remaining countries on earth without a true stock exchange. Without access to capital and investments, Ethiopia’s economy is growing at a snail pace of 5.4% in 2017. The digital divide plays a major factor in this because without access to technology the economy cannot keep up with the rest of the world.  \n\nThe Ethiopian government has a very well established reputation of being suppressive and corrupt throughout the world, and the digital divide also impact this. The few citizens with access to the internet who participate in the media are often heavily censored. With access to the internet, freedom of speech would be more easily accessible and less censored. The Ethiopian government would benefit from this because with more fair media coverage there would be less government corruption thus making Ethiopia a more stable country to invest in.\n\nEthiopian women experience more barriers to overcoming this digital divide than men, resulting in a gender gap. Many of these barriers are socially constructed through discriminating gender roles. Women's time and mobility are restricted by the expectation placed on them to bear the most household chores and management, keeping them too busy for educational or leisurely use of technology. Technological devices such as radios or movie showings are gendered as masculine tools and toys that are unfit for women. The restriction on time and mobility placed on women to work at home also cuts down on female participation in education, making them less comfortable with the idea of using computers and mobile phones because they believe themselves to not be educated enough to understand the devices or software. The risks of exploitation or exposure to pornography or harassment online also discourage the use of technology.\n\nWomen could benefit from measures to increase their access to technology and close the digital divide between the genders. Access to technology and the internet would provide a wealth of information that is vital to entrepreneurship and practicing democracy. Policies and laws that discriminate against women and other social and political inequalities can be discussed and changed by allowing more effective means of communication and organization for those who fight for women's rights.\n\nThe Digital Divide refers to the gap in exposure to technology needed to access the internet and online resources amongst a population. This gap leads to a decrease in the standard of living for those without access to technology while the standard of living for those who do have access to technology increases exponentially. The global digital divide has been an issue for hundreds of years and is far from a new phenomenon. Currently, in most countries, the digital divide is becoming notably less wide due to an increase in availability of affordable electronic devices capable of accessing the internet and has led to an extraordinary increase in competition amongst technological markets. An increase in competition has resulted in increased exposure to new technology with internet access in even the lowest income areas. Despite the increased exposure to technology, there are still other elements of technological access that contribute to the gap. These disparities continue to contribute to the gap that divides nations and prevents historically underprivileged groups from meeting the societal standards that will allow them to thrive in current society.\n\nThe three digital divides in France are characterized as generational, social and cultural differences, according to a report released by the Center for Strategic Analysis in the French government. The generational gap generally affects the elderly since they are the most unfamiliar with newer technologies. The social divide affects the poor since they are less likely to have the budget for technological devices. The cultural divide affects the least educated of the French since they have fewer opportunities for properly utilizing technology.\n\nA survey released by the centre de recherche pour l'étude et l'observation des conditions de vie reported that 17% of the people in France do not have a home computer, 19% do not have any access to the internet, and 23% have no internet access at home. They also reported that only 1 in 3 people aged 70 or older can be considered internet users.\n\nIn France, internet accessibility is not equally distributed among the entire population. Factors that determine internet accessibility in the French community are age, education, and income. Higher internet availability was found among French residents younger than 30 years of age, completed some level of higher education, and are currently employed or in school.\n\nA decrease in internet accessibility correlates with increasing age in France. Those younger than 30 have been recorded to have the highest access to internet. French students retain the highest accessibility. After 30 years of age, accessibility declines. French 59 years and older retain the lowest internet accessibility.\n\nLess than 17% of people over the age of 75 have a computer at their home. In comparison, more than 90% of people aged 15 to 24 have a home computer. This is quite a large divide with senior citizens who make up about 20% of the population.\n\nHigher education correlates to increasing internet accessibility. There is a sharp division between primary and secondary education. Secondary education graduates have four times the accessibility to internet versus those with only primary education. Post-secondary education degrees continue to show an exponential increase in internet accessibility.\n\nHouseholds with higher income levels reflect an increased internet accessibility. Residents with higher salaries are more likely to be able to afford internet connection. The trend persists between employed and unemployed residents. The employed population is 40% more likely to have access to internet.\n\nOf the individuals with lowest incomes in France only 34% have a home computer and only 28% have internet connection. In contrast, 91% of the individuals with considerable wealth have a home computer and 87% have internet connection.\n\nA language system is normally included in information and communication technologies. Certain languages are more prominent in ICT's than others. Data collected in 2007 states that 45% of information on the internet is in English while 4.41% is in French. A separate study published by UNESCO in 2009 compared the amount of Wikipedia articles available per language. 2,259,431 articles (23.078%) were available in English and 629,004 articles (6.425%) were available in French. Of the total population in France, only 39% speak English.\n\nFrance has made significant impact on closing the digital divide. Technological advancements and government funded programs have led to several innovative solutions and a narrowed gap between social classes within the nation regarding internet access and technological availability.\n\nIn 2004 the French government began a program to curb the nation’s digital divide and starting offering a computer with high speed internet access to 1.2 million of its poorest citizens for just 1 euro a day. Mainly to ensure they would have access to the growing number of government services available online. Prime Minister Villepin announced the plan after a meeting of the interministerial committee for the Information Society. A similar project also launched in 2004 sought to put internet connected computers into the hands of university students, also for 1 euro a day, and between September 2004 and September 2005 the number of students with laptops rose from 8 to 22 percent.\n\nThe French Center for Strategic Analysis released several recommendations for bridging the digital divide in 2011. Since many digital divides exist they have decided to take political actions to close the gaps. They also want to use public information campaigns to educate people. For poor and uneducated households they will work to lower the costs associated with network access and also help with the cost of various types of devices for people to access those networks. There is an effort to try and better integrate older people into the digital society, by offering them assistance and education when it comes to participating in the digital society. They will integrate the use of digital technologies in education to highlight good internet practices and to reduce the inequalities in schools. There is a push to showcase 'digital natives', which are those who were born into the digital world, adapting new social behaviors because of the way they integrate new interactive technology into their lives.\n\nThe Minitel System was one of France's earliest attempts to bridge the gap caused by the digital divide. The Minitel System, influenced greatly by the French government, was introduced in 1983 and \"laid the groundwork for France's computerized future\". The system provided French telephone users with access to online databases through their own personal phone line for no extra charge. The videotex service was accessed through a text-interface monitor and keyboard. Initially the system provided access only to things like phone books; however, in little time, users would be able to access numerous online services that allowed them to do things like view and pay bills and online shop. France Telecom made the service more user friendly, bundling users’ online purchases and telephone bills together. The Mintier System continued to grow and was able to provide access to \"more than 20,000 online services before the World Wide Web even got off the ground\". By the end of the 1980s, \"every adult living in France had access to the network\". The system proved to be very successful and continued to provide online content to French phone users until its demise in 2012.\n\nOne in four European internet users access the internet only outside of their home; this means that 1/4 of the current European internet users would not have access to the internet at all if it weren't for places with free public access to the internet such as government funded programs like libraries. The libraries of France are considered to be largely influenced by their political intentions to level the success and education levels amongst the social classes. These libraries are strategically placed in low income areas and on the outskirts of big cities where incomes tend to be lower and crime and unemployment rates tend to be higher. Unlike the uNited States and comparable nations, French libraries have extended hours and are open on all days of the week. Librarians working in these facilities generally have an aspiration to help those of struggling communities enhance their education and learn to use new technology. These facilities have proven to be helpful in bridging the gap between high and low income social classes by exposing French citizens of low income areas to new technology, providing them with access to the internet, and thus lowering unemployment rates.\n\nGermany's digital divide is impacted by several factors that include age, gender, family structure, education, ethnicity, and motivation. There are still areas in Germany that lack access to high-speed internet and large cities tend to have more access to the internet than rural communities. In 2006, the (N)Onliner Atlas recorded that only 51% of small rural communities (pop. < 5,000) were Internet users, eleven points less than their urban counterparts in large cities (pop. > 500,000), where Internet usage rates averaged 62%\n\nThe digital divide in Japan is the disparity of access to the Internet by the population of Japan. Multiple factors influence this divide. Cultural aspects of Japanese people contribute to the digital divide in the nation. Contributing cultural factors to the digital divide present in Japan are closing over time. Groups shown to be improving access include women and the elderly. 97.1% of the households in Japan have Internet access at home while 81% of the households in Japan have personal computers.\n\nThe 2015 population census of Japan was released, under 15 years of age are about 16 mil which is 12.6% of the total population. 76 mil of the population is the age 15–64 years of age which is 60.7% of the total population and 3.3mil are 65 years and higher are 26.6%.\n\nAccording to the Japanese Statistics Bureau, MPHPA, and World Internet Project Japan, in 2001, 44% of the population was online, and 41% of that group were females. Additionally, figures point to declining disparity among internet access in various socioeconomic classes and gender. Additionally, figures show that younger Japanese individuals use the internet as compared to older Japanese individuals. All gaps are shown to be narrowing. A study produced at the University of Buffalo shows that the Japanese consider advanced ICTs a common commodity, not as necessary for advancement.\n\nThe Philippine Information Literacy website, which had an article based on a forum hosted by ASEAN and Japan about media and information literacy, held a two-day event that discussed “how the youth should cope with the rapidly growing world online—particularly the rise of social media—and how they could protect themselves from its hazards.” It also informed the audience about the advantages of technology and how telecommunications has taken over. \n\nJapan is divided into territorial divisions and depending on which geographic area that you live in, and everyone either has internet or no one has internet.\n\nAccording to the Ministry of Internal Affairs and Communications, there are a total of 39 “Zero Broadband Areas” in Japan. This is where there is no broadband access for even a single household within the district. This means Japanese citizens cannot get broadband even if they wanted to because there is no broadband infrastructure in their neighborhood. For some territorial divisions the coverage is up to 99.8% of the people in this area have access to broadband. This leads to a digital divide, but the divide is more drastic at the ends of the curve depending on where you live.\n\nThe Ministry of Internal Affairs and Communications is aiming to improve broadband penetration in remote areas of Japan, but businesses aren’t exactly queuing up to provide expensive infrastructure to hook up remote islands and mountain villages. Japan has more than 4000 islands, 260 of which are inhabited. Japan’s land is also 73% mountainous or hilly, and whilst the population is concentrated mainly in the alluvial plains and coastal areas, there are significant populations living in remote valleys deep in the mountains mainly engaged in agriculture or forestry. It is difficult to justify building out a fiber-based infrastructure to a remote island, or to a dead end mountain ravine community miles from anywhere.\n\nThe internet users in Japan are increasing at an exponential rate but there are still areas demographically do not receive optimal broadband satellite communications in Japan. A possible solution for this is to launch more satellites for those areas. Globally separated into categories of 58% female(most pronounced in Africa, Arab states and Asia-Pacific), 60% rural,poor, illiterate and elderly are offline. ITU provides idea/plans for these focus groups to increase the amount of internet users. Rural areas need to implement a low-cost, given the lower household income is lower in these areas, another idea for rural areas is to provide public buildings with internet access.\n\nThe digital divide in Malaysia is impacted by several factors, which includes age, location, and wealth. Malaysia's status as a non-fully developed country has impacted the availability of technology and the internet, as the lack of access to information communication technology may cause the country to fall even further behind in the progress of worldwide technology if this issue is not addressed and mended.\n\nIn Mexico, half of the population does not have access to the internet. In 2017, 50.4 % of Mexican households had access to the internet and 45.4 % had a computer. The digital divide in Mexico is related to low income, education, lack of proper infrastructure, and geographical location.\n\nWealthier Mexicans are more likely than poorer Mexicans to have access to the internet. Because of certain dominant players monopolizing the telecommunication sector in Mexico, the majority of the Mexican population cannot afford the expensive services of television and internet.\n\nFormal schooling is scarce in the poor regions of Mexico leaving many school-aged children without internet access. Those who have only received an elementary education are four times less likely to access or use the internet. In Mexico, 33% of primary and 48% of secondary schools have access to the internet and report having one computer. The government has addressed the issue by providing technologies that provide the capability to access the internet, but teachers and students are not instructed on how to use them.\n\nMexico's lack of proper fiber optic infrastructure causes less wireless broadband penetration in the country, which in turn effects the number of internet users in Mexico. Fiber optic infrastructure is found more in urban than rural areas. Urban internet usage is significantly higher than rural internet usage. In big cities such as Mexico City, citizens doubled the national average in terms of computer ownership. In 2000, the southern border states of Mexico had half as many phones per capita as the northern states.\n\nThe digital divide is an issue for countries like Morocco who are trying to provide improved internet availability to its people. While internet is available to a majority of people in city centers, rural locations are still without access. There exists some restrictions on telecommunication services and limited service providers to choose from. This acts as a block for Moroccan internet users because the lack of competition can become costly. The cost associated with internet services also prevents those who are more economically disadvantaged from accessing the same types of services.\n\nSome of the main limitations for access are education, gender, and age. People with lower education are less likely to access technology or further their professionalism in the information technology field. Moroccan women can often find that they do not have the same sort of opportunities to technology professions and internet usage. The United Nations Development Fund for Women (UNIFEM) was formed to increase opportunities for women and a project called \"Achieving E-Quality in the IT Sector in Morocco\" has been set up for women to be presented with, increasing their professionalism. Ten universities currently participate in this project in an active effort to close the gender gap within the digital divide.\n\nThere is a discernible age gap between individuals who are active on social media and the internet and those who are not. Those aged 14–25 tend to be more familiar with the different social trends and access them much faster than the older generations. A large attribution to this is the lack of literacy in many adults. In 2010, 56% of Morocco's population was illiterate. While children able to go to school are learning to read, write and use various forms of technology; many of their parents do not have access to these same resources and may not have had access while they were growing up. The largest city of Morocco, Casablanca, has a population of 3,544,498 as of 2016. Around 57.6% of the people throughout Morocco had access to the internet in 2016 while the rural population was 39.32%. With a little over half of the population having access to internet many adults that lack literacy skills are currently living in rural areas with minimal access and usage of the internet, yet for their children they have the opportunity to occasionally access the internet through schooling.\n\nThe government of Morocco has become increasingly involved in closing the digital divide being experienced by the country. Morocco lags behind in the digital world and steps are actively being taken to fix this. One plan that has been put in place is called “Maroc Numeric 2013”. This plan was initiated in 2013 in an attempt to get Morocco more involved in the digital world as well as provide some protections for internet users. This plan has the potential to boost the nation's economy by creating jobs. The plans passed in government legislation also work to reduce some of the costs associated with internet connection and usage by ensuring that internet services can operate on a free market with little to no control unlike the newspapers and television stations of the region. Despite efforts by the Moroccan government there is still some gap in digital connection among its people. The wealthier population still has higher internet access than those that are poorer. There has been some improvement with the help of free market competition to make access to the internet more readily available to all people.\n\nAccording to Samuel Lee, Fabian Seiderer and Lida Bteddini, one potential solution could be increasing and improving the lines of communication between the Moroccan government and it’s people who have expressed much interest in being involved with legal legislation and having more access to knowledge online. A final solution might be updating the original plan, “Digital Morocco 2013” that was put in place in an attempt to bridge the digital divide. While the plan has led to some improvement, connectivity and usage of the internet is still limited to the more educated and urban regions of the Moroccan kingdom.\n\nThe digital divide in Myanmar is impacted by gender and economic class. Wealthy and powerful persons tend to have better access than those who are poor and men typically have more opportunities to get involved in the tele-infrastructure business. The tele-infrastructure in Myanmar is relatively weak compared to those in surrounding countries. The internet supply doesn't meet the increasing demand; hence the digital divide.\n\nMyanmar is a country with low income level and most internet access is on cellphones, because of the low simcard cost. The people who live above the poverty line have all kinds of modern-day access unlike the average or poor who struggle in the everyday necessities. In 2009 it was recorded that 0.94 percent of Myanmar's population had cell phone subscriptions, due to low trade and economic power. The people who live above the poverty line have all kinds of modern-day access unlike the average or poor who struggle in the everyday necessities. In 2016 Myanmar's Gross Domestic Product (GDP) averaged $1,275.02 US dollars, which contrasts with that of the United States at an average of $57,466.79 dollars.\n\nThe amount of internet users in Myanmar has increased significantly in the last few years. In 2014, there were 2 million internet users in Myanmar. That number, however, increased to 39 million users by 2016. The cause of this drastic increase in internet users was caused mainly by the massive increase in SIM card sales recorded through May 2016. This combined with a more aggressive network roll-out by mobile providers to other regions, a decline in SIM card prices, the lowered prices of voice and data plans, and the lowered cost of mobile phones all led to this major increase in mobile sales and subscriptions over that two-year period. Because this market is beginning to reach its maturity stage, significant slowdown is predicted to occur by 2022.\n\nNearly 30% of women are less likely to have access to a mobile device in Myanmar. The cell phone is the main source of internet access in the country. Because of this deficit in internet access between the genders there is a Gender Digital Divide in Myanmar. There is also a significant wage gap between men and woman in addition to limited leadership positions available to be filled. Due to this woman are not as equally represented and often listed as a dependent. This gap contributes to the lack of resources, furthering the lack of digital activity for women. Currently Myanmar is working to close the gap by using more ICT’s in daily life in order to educate women. However, due to the economic difference between to genders, women are less likely to have access to ICT’s. \"IREX’s Tech Age Girls (TAG) program is addressing this gap in opportunity, providing more than 100 young women in Myanmar with the technology and leadership skills they need to achieve their goals and become agents of change.\"\n\nAccording to mid-September MIS Report from Nepal Telecommunication Authority, 53 percent of the Nepal population uses or has access to the internet. Some of the reasons for this divide are factors such as location, age, and education. Prior to the 1950s Nepal had closed itself off due to geopolitical reasons, however after the Ran Regime was abolished in 1951 the country began to adopt and develop many modern sciences within the government. A 2011 study has shown that while things like age and education levels did impact Internet usage, things such as income levels and telephone ownership did not and that over half of Nepal's Internet users apply the Internet to educational purposes rather than commercial or personal purposes. Lack of technology literacy in Nepal is also a major contributing factor to the widening of the divide.\n\nThe Nepal government usually focuses their resources on important necessities such as clean water, roads, and healthcare, as the country is still poor and developing and most of their population lives in rural parts of Nepal. People living in these rural areas are less likely to have access to technology, Internet, or phone lines.\n\nNepali is the national language of Nepal and is spoken by 44.6% of the Nepalese people. However, despite this, Nepal is home to 123 various languages. English is included in this and is spoken fluently at an estimated 2-10%. This mixture of diverse languages contributes to the digital divide as most computer content is written in English, making technology inaccessible to non-English speaking people who could otherwise afford and use a computer.\n\nThere have been some attempts to close or lessen the digital divide in Nepal, such as Mahabir Pun's Nepal Wireless Networking Project, which was launched with the intent to help bring wireless technologies. Some areas are also holding classes in digital literacy in order to help people in rural areas gain knowledge and experience with technology. The courses will provide the Nepalese with skills that they are able to utilize in their schooling and careers. The widening digital divide in Nepal has also prompted the government to step in, due to worries that the lack of digital literacy will have a negative impact on the country and its economy. Officials are working towards closing the gap by introducing new programs that will give people the ability to utilize technology more easily than before. Large brand name companies are also making efforts to eliminate the digital divide in Nepal. Microsoft Nepal is providing resources to improve digital literacy as well as increasing access to technology.\n\nThe digital divide in Nigeria is impacted by education, lack of electrical infrastructure, income, and urban drift, as well as a variety of other social and political factors contribute to Nigeria's growing digital divide. There have been efforts to reduce the digital divide by both government agencies and technology corporations.\n\nPakistan is one of the South Asian countries that does have internet access. Internet access in Pakistan began in the 1990s and has continued to grow over the past decades. In fact, Pakistan has about 32 million internet users. However, according to data collected by the World Bank, Pakistan has an overall population of about 193 million people. In addition, Pakistan also has about 15 million people who access the internet from their mobile devices. Pakistan has 5 broadband internet providers and 10 DSL. Broadband use through computers is the number one way that people access the internet in Pakistan. The second most popular way that people of Pakistan access the internet is through their cellular devices. Even though there are about 32 million internet user compared to a large population, Internet in Pakistan is ranked 20th in the world.\n\nPakistan has five major cell phone providers. The five are (in order from most to fewest subscribers): Jazz 51.88 million; Telenor 39.37 million; Zong 27.71 million; Ufone 18.46 million; and Warid 12 million. Recently in May 2017, the Pakistani government decided to lower tariffs and taxes on cellular services as well as on mobile phones. The reasons for this decision included years of pressure from the nation's top mobile operators and a World Bank report pointing out Pakistan's high taxing of telecom services in the region.\n\nIn rural Pakistan, cell service use is divided along gender lines. Karin Astrid Siegmann pointed out this disparity; 40 percent of female users have to ask permission from the male owners to make calls. Cell phones being in the hands of females is viewed unfavorably in Pakistani culture. According to one participant in the study, from the Muzzafargarh district in rural Pakistan, \"Women don't even know how to dial a number.\" Additionally, the percentage of women owning a cell phone is 36% compared to men at 78% overall.\n\nBesides gender, there is a distinct divide among castes. Newer castes rank much more highly on the Digital Access Index (DAI) than do the older castes. The same study compared two political parties. One political party, called the PTI (Pakistan Tehreek-e-Insaf), was composed of newer castes (Khan, Hashmi, Alvi, and Qureshi). The Muslim League is composed of older castes. The PTI outperformed the Pakistan Muslim League in number of foreign visitors to its web site, with a combined percentage of 12.7%, and in websites linking to it with 450, compared to the Muslim League's 168 (See Table 6 page 354). Ahsan Abdullah elaborates the important background of these findings: \"Members of the new caste traditionally have not been farmers; for example, members of the Sheikh caste are traditionally traders, and members of the Syed caste traditionally hold religious offices, and hence they have to be better educated as compared to the old caste members. The old caste members, who are traditionally farmers, require tacit knowledge more than education to be successful. This observation is supported by the higher literacy rate in new castes as compared to the old caste members, with Pathan, Sheikh, and Syed being the top three educated castes among the 12 castes considered.\"\n\nBesides Private efforts, Public efforts put forth by the Pakistani government would help bridge the digital divide. There is an ambitious undertaking called the Universal Services Fund which would aim to provide broadband coverage to the whole nation by 2018.\n\nIn the Philippines about 47 to 50% of the population can and has access to the Internet. Initially the Philippines only had BBS (Bulletin board system) access, however after March 29, 1994 the Philippine Network Foundation (PHNet) connected the country to the web via Sprint. As of 2010, 29.3 million Filipinos were using the internet. The digital divide is impacted by several factors that includes income and education. Jim-yong Kim, president of the World Bank Group, has stated that “We must continue to connect everyone and leave no one behind because the cost of lost opportunities is enormous. But for digital dividends to be widely shared among all parts of society, countries also need to improve their business climate, invest in people’s education and health, and promote good governance.”\n\nBased on Philippines government research, there is a noticeable rise of Internet use in the Philippines after it was first introduced on March 29, 1994. “They were connected to the internet via SprintLink”, this changed the Philippines culturally and politically. Social media is a leading motive for Internet use in the Philippines, but Internet use also plays a big part in their political communications. The Philippine presidential election of Estrada is where Internet use for politics started to take form. Due to the protests, Filipinos used Internet to display charges against Estrada.\n\nThe Philippines was the only country in the Southeast Asia region that had a declining youth literacy rate between the years of 1990 and 2004, according to data from the United Nations. This prompted major school reforms and in 2012, a K-12 school curriculum was introduced which included a year of kindergarten and two senior school years. The Department of Education in the Philippines (DepEd) goal for students who graduate from the K-12 curriculum is for these students to understand technology works and how they can benefit from it.\n\nThe curriculum of this program focuses on developing students to be comfortable with using computers and technology. Some examples are that in grade 4 students will learn how to use the basic functions of the computer, including the use of the internet and emailing, in safe ways. In grade 6, students will begin taking art classes to learn how to use technology for digital paintings and graphic designs. In grade 7, students will begin learning how to academically use the internet for journals and searching for academic sources for essays and other similar assignments. In the higher upper level grades, the curriculum will include schools that specialize in computer education as well as science and technology. The Philippines being able to close in on the technology gap after being so behind in the 90's and early 2000's is going to very much improve the graduation rate of the students now that they can study at home and use their sources to their advantage through tablets and computers instead of only being able to access their school work in the classroom.\n\nThe digital divide in Saudi Arabia is seen through their Internet usage statistics 47.5% of Saudi Arabians use the internet compared to 78.2% of the United States. Of the 47.5%, 83.87% of Saudi households use the internet and only 10% of these households have one internet user. Saudi Arabia, as of 2017, has 96% of women using the internet and 88% of men. In addition, both females and males use the internet almost equally.\n\nSouth Africa's Digital Divide is being bridged by programs like Isifundo. This program provides computer literacy for South Africans who would otherwise not receive any training. They set up centers that provide access to computers and the internet. Their Facebook keeps people up to date with computer related news and other helpful programs in South Africa.\n\nThe digital divide in South Korea is mainly caused by the unevenness of economic, regional, physical, or social opportunities, leading to marginalized persons not receiving the benefits that technology can bring. The lack of adaptation to the informatization of social services, such as administration and welfare, results in limited opportunities for basic daily life and social participation. South Korea's information gap was initially due to economic reasons and the difference in the initial cost for using the Internet or PC, but recently there has been a gap between the users of the information according to the degree of utilization of information. As the information society rapidly developed, the distribution of the Internet quickly accelerated in Korea, dividing people into two groups, people who are well adapted to the changes and those who are more familiar with the previous media. Although the percentage of local population with internet access is high in Korea, the average rate of internet usage is 99.9% for the young and 64.3% for the elderly.\n\nAs transiting society from industrial to de-industrial, information and knowledge can be substituted with capital and product in important social matter. \n\nHowever, the nature of information and knowledge, through commercialization process in capitalism, could make a greater problem than supposition. So, information and knowledge is different from previous product( shoes, clothes, food so on) in that information and knowledge’s cyclic process from creating and application to extinction is most fast. Also, those alteration in quantity and quality is unpredictable. From the nature of information and knowledge, those could make enormous social inequality(ex. class, state, sex, education, region etc.) Furthermore, a global effect feature, one of the information and knowledge’s nature, can deepen global inequality and further sharpen the 2080 society, called Pareto's law.\n\nThe most basic reason is the distribution of hardware. The most basic hardware in the information era is a computer, which creates an information gap between those who have difficulties buying a computer and those who do not. According to the '2016 Information Gap Index and Survey' in South Korea, which conducted by the Ministry of Information and Communication (MIC) on 6,300 people nationwide, while only 53.2% of the underprivileged people have personal computer, the average proportion of whole people are nearly 83%. Computer penetration rates are affected by many correlating variables such as region, education, and income and these variables can issue a complex impact. To overcome this difference, OLPC (One Laptop per child) is developing and also distributing $100 computer. In addition, a project for the free use of the internet such as the fund router project is in progress, but it is not widely available in Korea. As 'ubiquitous' becomes more and more popular, the distribution of small hardware is expected to grow even more and the digital divide is expected to deepen further.\n\nEducation is the most relevant part of income in Korea. Children whose lower income earners have less educational opportunities than higher incom earners. For this reason, the children of low educated are more likely to be a low educated students than those of highly educated people, and this phenomenon leads to a vicious cycle of social inequality. Because the information era is built on capitalism, this vicious circle is likely to lead to current society as well. Education is an important reason of digital divide. Without education on information society, it leads to poverty of information which causes to the economic discrimination. Most of all, considering the characteristics of knowledge which is explosively expanding, if education can not keep up with the pace of the informational change, it can cause education gap to be more severe. In order to solve this problem, continuous education is much more required rather than shot term education.\n\nThailand's digital divide is impacted by its status as a developing country within Southeast Asia as well as several other factors that include income, choice of technologies, and socioeconomic factors. ICT development and mobile penetration are strongly correlated with economic growth and social benefits.\n\nThe digital divide in the United States has decreased since it was initially detected, however there are still portions of the country and certain groups that have limited or no access. Groups impacted by the digital divide can include certain income brackets, ethnicities, and the less educated. There is also a gap between rural and non-rural areas in America. Rural Americans have made large gains in adopting digital technology in recent years, but they remain less likely than nonrural adults to have home broadband, smartphones and other devices. This is not necessarily because of a financial issue, but because of the lack of access and the poor internet connection due to the lack of towers and phone lines. Mobile technology use among rural adults has risen rapidly, however they are still leaps and bounds behind Americans in larger cities that have more access. As of 2016, approximately 11.5% of the total U.S. population did not have internet access. Out of the 324,118,787 Americans, there were 286,942,362 total internet users (88.5%).\n\nThe digital divide in Vietnam stems from sociopolitical, economic, and technological issues, but over the last decade the country has made great strides in providing large-scale Internet access and more lax restrictions in order to bridge this gap. Though the majority of Vietnam is rural, more than half of the country's population has access to the Internet. Despite these limitations on technology, organizations around the world are working directly with the people of Vietnam to close the digital divide.\n\nVietnam has a population of 96 million people, and 34.9% of the population is urban. Despite this, 53% of the country’s population currently has access to the Internet. According to a study conducted by market research company Statista in 2016, more than 91 percent of daily Internet users were between the ages of 25 and 34. However, despite the expansion in access, Internet quality remains poor in rural areas, and Vietnam ranks 16th among other Asian countries with the most Internet users. Vietnam also has a slower average data transfer speed than neighbor countries. Compared to Singapore’s average of 16.5 Mbit/s, Vietnam’s average is 5.0 Mbit/s.\n\nFor individuals with access to the Internet, government censorship becomes another obstacle because the three Internet service providers in Vietnam (FPT Telecom, Viet Nam Post and Telecommunications Corporation, and Viettel) are owned by the government and military. The Vietnamese government restricts user access to websites that are critical of the government or feature politically sensitive content, as well as the websites of select human rights organizations. In 2016, Facebook and Instagram were temporarily blocked in an effort by the government to crack down on social media due to rising unrest in Hanoi and Ho Chi Minh City. Hundreds of citizens protested in city streets regarding the government’s delayed response to an environmental disaster in which millions of dead fish washed ashore. It was believed by locals to be the fault of Formosa Plastics.\n\nIn 2011, The World Bank extended a project for an additional two years in an attempt to complete the process of installing software and conducting training. This technology and training would potentially allow for the expansion of small businesses, enhance current operating systems and disseminate information on a larger scale. Due to the slow Internet speeds in Vietnam, obtaining information was difficult. However, this attempt at closing the digital divide was unsuccessful and expensive, totaling 106.97 million US dollars. Although this project was unsuccessful, organizations are still working to solve the issue of the digital divide.\n"}
{"id": "43683079", "url": "https://en.wikipedia.org/wiki?curid=43683079", "title": "Distributed Processing Technology", "text": "Distributed Processing Technology\n\nDistributed Processing Technology (DPT) was founded in 1977, in Maitland, Florida. DPT was an early pioneer in computer storage technology, popularizing the use of disk caching in the 1980s and 1990s. DPT was the first company to design, manufacture and sell microprocessor-based intelligent caching disk controllers to the OEM computer market. Prior to DPT, disk caching technology had been implemented in proprietary hardware in mainframe computing to improve the speed of disk access.\n\nDPT's products popularized the use of disk caching in the 1980s. According to Bill Brothers, Unix product manager at the Santa Cruz Operation (SCO), a computer operating system vendor, \"The kind of performance those guys (DPT) produce is phenomenal. It's unlike any other product on the market.\"\n\nDPT was founded by Steve Goldman, who served as the President and Chief Executive Officer until DPT was acquired by Adaptec in November 1999.\n\n"}
{"id": "202311", "url": "https://en.wikipedia.org/wiki?curid=202311", "title": "E-services", "text": "E-services\n\nE-services (electronic services) are services which use of information and communication technologies (ICTs). The three main components of e-services are- service provider, service receiver and the channels of service delivery (i.e., technology). For example, as concerned to public e-service, public agencies are the service provider and citizens as well as businesses are the service receiver. The channel of service delivery is the third requirement of e-service. Internet is the main channel of e-service delivery while other classic channels (e.g. telephone, call center, public kiosk, mobile phone, television) are also considered.\n\nSince its inception in the late 1980s in Europe and formal introduction in 1993 by the US Government, the term ‘E-Government’ has now become one of the recognized research domains especially in the context of public policy and now has been rapidly gaining strategic importance in public sector modernization. E-service is one of the branches of this domain and its attention has also been creeping up among the practitioners and researchers.\n\nE-service (or eservice) is a highly generic term, usually referring to ‘The provision of services via the Internet (the prefix 'e' standing for ‘electronic’, as it does in many other usages), thus e-Service may also include e-Commerce, although it may also include non-commercial services (online), which is usually provided by the government.’ (Irma Buntantan & G. David Garson, 2004: 169-170; Muhammad Rais & Nazariah, 2003: 59, 70-71).\n\n\"E-Service constitutes the online services available on the Internet, whereby a valid transaction of buying and selling (procurement) is possible, as opposed to the traditional websites, whereby only descriptive information are available, and no online transaction is made possible.\" (Jeong, 2007).\n\nLu (2001) identifies a number of benefits for e-services, some of these are:\n\n\n\nThe term ‘e-service’ has many applications and can be found in many disciplines. The two dominant application areas of e-services are\n\nE-business (or e-commerce): e-services mostly provided by businesses or [NGO|non-government organizations] (NGOs) (private sector).\n\nE-government: e-services provided by government to citizens or business (public sector is the supply side). The use and description of the e-service in this page will be limited to the context of e-government only where of the e-service is usually associated with prefix “public”: Public e-services. In some cases, we will have to describe aspects that are related to both fields like some conferences or journals which cover the concept of “e-Service” in both domains of e-government and e-business.[example: www.eserviceforyou.com]\n\nDepending on the types of services, there are certain functionalities required in the certain layers of e-service architectural framework, these include but are not limited to – Data layer (data sources), processing layers (customer service systems, management systems, data warehouse systems, integrated customer content systems), exchange layer (Enterprise Application Integration– EAI), interaction layer ( integrating e-services), and presentation layer (customer interface through which the web pages and e-services are linked).\n\nMeasuring service quality and service excellence are important in a competitive organizational environment. The SERVQUAL- service quality model is one of the widely used tools for measuring quality of the service on various aspects. The five attributes of this model are: reliability, responsiveness, assurance, tangibles, and empathy. The following table summarizes some major of these:\n\nThe LIRNEasia study on benchmarking national telecom regulator websites focuses on content than on accessibility and ease of use, unlike the other studies mentioned here. Websites are increasingly important portals to government agencies, especially in the context of information society reforms. Stakeholders, including businesses, investors and even the general public, are interested in information produced by these agencies, and websites can help to increase their transparency and accountability. The quality of its website also demonstrates how advanced a regulatory agency is.\n\nSome major cost factors are (Lu, 2001):\n\n\nInformation technology is a powerful tool for accelerating economic development. Developing countries have focused on the development of ICT during the last two decades and as a result, it has been recognized that ICT is critical to economy and is as a catalyst of economic development. So, in recent years there seems to have been efforts for providing various e-services in many developing countries since ICT is believed to offer considerable potential for the sustainable development of e-Government and as a result, e-Services.\n\nMany government agencies in developed countries have taken progressive steps toward the web and ICT use, adding coherence to all local activities on the Internet, widening local access and skills, opening up interactive services for local debates, and increasing the participation of citizens on promotion and management of the territory(Graham and Aurigi, 1997).\n\nBut the potential for e-government in developing countries remains largely unexploited, even though. ICT is believed to offer considerable potential for the sustainable development of e-government. Different human, organizational and technological factors,\nissues and problems pertain in these countries, requiring focused studies and appropriate approaches. ICT, in general, is referred to as an “enabler”, but on the other hand, it should also be regarded as a challenge and a peril in itself. The organizations, public or private, which ignore the potential value and use of ICT may suffer pivotal competitive disadvantages. Nevertheless, some e-government initiatives have flourished in developing countries too, e.g. Brazil, India, Chile, etc. What the experience in these countries shows, is that governments in the developing world can effectively exploit and appropriate the benefits of ICT, but e-government success entails the accommodation of certain unique conditions, needs and obstacles. The adaptive challenges of e-government go far beyond technology, they call for organizational structures and skills, new forms of leadership, transformation of public-private partnerships (Allen et al., 2001).\n\nFollowing are a few examples regarding e-services in some developing countries:\n\nBangladesh first e-service system is National E-Service System ([http://www.eservice.gov.bd/[ NESS]]) and 2nd e-Service For you [http://eserviceforyou.com/[eserviceforyou.com]]. \n\nOnly a decade after emerging from the fastest genocide of the 20th Century, Rwanda, a small country in Eastern Central Africa,\nhas become one of the continent’s leaders in, and model on, bridging the digital divide through e-government. Rwanda has undergone a rapid turnaround from one of the most technologically deficient countries only a decade ago to a country\nwhere legislative business is conducted online and wireless access to the Internet is available anywhere in the country. This is\npuzzling when viewed against the limited progress made in other comparable developing countries, especially those located in the\nsame region, sub-Saharan Africa, where the structural and institutional constraints to e-government diffusion are similar.\n\nIn South Africa, there continues to be high expectations of government in respect to improved delivery of service and of closer consultation with citizens. Such expectations are not unique to this country, and in this regard there is a need for governments to recognise that the implementation of e-government systems and e-services affords them the opportunity to enhance service delivery and good governance. The implementation of e-Government has been widely acclaimed in that it provides new impetus to deliver services quickly and efficiently (Evans & Yen, 2006:208). In recognition of these benefits, various arms of the South African government have embarked on a number of e-government programmes for example the Batho Pele portal, SARS e-filing, the e-Natis system, electronic processing of grant applications from remote sites, and a large number of departmental information websites. Also a number of well publicised e-government ventures such as the latter, analysts and researchers consider the state of e-government in South Africa to be at rudimentary stages. There are various factors\nwhich collectively contribute to such an assessment. Amongst these, key factors relate to a lack of a clear strategy to facilitate uptake and adoption of e-government services as well as evaluation frameworks to assess expectations of citizens who are one of the primary user groups of these services.\n\nE-Services is one of the pilot projects under the Electronic Government Flagship within the Multimedia Super Corridor (MSC) initiative. With E-Services, one can now conduct transactions with Government agencies, such as the Road Transport Department (RTD) and private utility companies such as Tenaga Nasional Berhad (TNB) and Telekom Malaysia Berhad (TM) through various convenient channels such as the eServices kiosks and internet. No more queuing, traffic jams or bureaucratic hassles and one can now conduct transaction at one’s own convenience. Also, Electronic Labour Exchange (ELX)is one stop-centre for labor market information, as supervised by the Ministry of Human Resource (MOHR), to enable employers and job seekers to communicate on the same platform.\n\ne-Syariah is the seventh project under the Electronic Government flagship application of the Multimedia Super Corridor (MSC). A case management system that integrates the processes related to management of cases for the Syariah Courts.\n\nIn America, citizens have many options and opportunities to follow and understand government actions through e-government. Government 2.0 (Gov. 2.0) is currently in place to bring the people and governments together to learn new information, increase government transparency, and better means for communicating to one another. Gov. 2.0 offers increased citizen participation through on-line applications such as social media and other apps. Through the internet and websites such as USA.gov, an individual can perform actions such as contacting elected officials, find information about the work force such as retirement plans and labor laws, learn about money and consumer issues such as taxes, loans, and welfare, learn about citizenship and obtaining a visa or passport, and other topics such as health and welfare, education, and environmental issues.\n\nE-commerce is another growing E-service in the United States for both big and small businesses. E-commerce sales are projected to grow 10 to 12 percent annually. Amazon.com is the largest on-line marketplace in the country with annual sales of $79 billion. Wal-Mart is also a widely popular retailer. They have grown their business by having electronic services. Wal-Mart’s sales for E-commerce in 2015 was roughly $13 billion. Apple develops and sells a wide variety of technological goods and services such as cell phones, music players, and computers. Apple’s sales for E-commerce in 2015 was $12 billion. E-services allows businesses to reach new clientele and offer new services. Companies such as eBay and Etsy have achieved great success, with eBay posting a net income in 2016 of nearly $9 billion and Esty claiming roughly $200 million in profits from nearly $2 billion sales. The majority or eBay's business is conducted in the United States but it does a great deal of international business including the United Kingdom and Germany. The global reach of Etsy is seen in nearly every country in the world with 31% of gross merchandise sales occurring outside of the United States.\n\nChina’s recent realization of the continuing growth of internet usage has caused the government to recognize the need to expand their E-government services. Some steps the government wants to take in order to increase their E-government services are to develop more online functions, use government sites to integrate on-line services, have supplementary open data available to citizens to further government transparency, and to combine services from local and country-wide governments for convenience. China’s plan of action to incorporate the internet into everyday business and grow the economy is known as “Internet Plus.” The government plans to have this plan in full effect by 2025 to be the main driving force for economic and social improvements. Internet Plus will help to grow the job market as the government plans to use local citizens for development, and to generate more areas dedicated to technological growth such as Zhongguancun.\n\nBecause of the large population, China has the most internet and cell phone users in the world.(consider rewording) This causes a need for technological growth and a demand for increased E-services. In 2016, Chinese consumers spent more money for on-line goods and services than the United States and United Kingdom combined. There is(are) a wide variety of reasons as to why E-commerce flourishes in China including easy access to mobile internet, low cost of shipping, and a vast selection of cheap, unbranded products. Alibaba is China’s largest on-line marketplace with an annual revenue stream of $16 billion. Its services are globally available in Russia and Brazil through AliExpress. Tencent is another internet company with an annual revenue income of $16 billion. Tencent is used mainly for instant messaging but has other applications as well including mobile games and other digital content. By the end of 2015, Tencent’s WeChat messaging app reached around 700 million users. The biggest competitor for Tencent is Facebook’s WhatsApp. Baidu Is the most visited website in the country and it is used as a search engine and has an annual revenue of $10 billion. In March 2016, there were roughly 663 million users. Google challenges Baidu as the major internet search engines in the world. Huawei is a tech company that produces phones, tablets, and develops the equipment used in fixed-line networks. Huawei has an annual revenue income of $61 billion. It is currently located throughout 100 countries worldwide and in 2015, it filed 3,898 patent applications, more than any other country in the world. The biggest competitors to Huawei is Apple and Samsung.\n\nThe future of e-service is bright but some challenges remain. There are some challenges in e-service, as Sheth & Sharma (2007) identify, are:\n\n\nThe first challenge and primary obstacle to the e-service platform will be penetration of the internet. In some developing countries, the access to the internet is limited and speeds are also limited. In these cases firms and customers will continue to use traditional platforms. The second issue of concern is fraud on the internet. It is anticipated that the fraud on the e-commerce internet space costs $2.8 billion. Possibility of fraud will continue to reduce the utilization of the internet. The third issue is of privacy. Due to both spyware and security holes in operating systems, there is concern that the transactions that consumers undertake have privacy limitations. For example, by stealthily following online activities, firms can develop fairly accurate descriptions of customer profiles. Possibility of privacy violations will reduce the utilizations of the internet. The final issue is that e-service can also become intrusive as they reduce time and location barriers of other forms of contract. For example, firms can contact people through mobile devices at any time and at any place. Customers do not take like the intrusive behavior and may not use the e-service platform. (Heiner and lyer, 2007)\n\nA considerable amount of research efforts already exists on the subject matter exploring different aspects of e-service and e-service delivery ; one worth noting effort is Rowley’s study (2006) who did a review study on the e-service literature. The key finding of his study is that there is need to explore dimensions of e-service delivery not focusing only on service quality “In order to understand e-service experiences it is necessary to go beyond studies of e-service quality dimensions and to also take into account the inherent characteristics of e-service delivery and the factors that differentiate one service experience from another.”\n\nSome of the major keywords of e-service as found in the e-government research are as follows:\n\nUser acceptance of technology is defined according to Morris (1996, referred by Wu 2005, p. 1) as “the demonstrable willingness within a user group to employ information technology for the tasks it is designed to support”. This definition can be brought into the context of e-service where acceptance can be defined as the users’ willingness to use e-service or the willingness to decide when and how to use the e-service.\n\nUsers’ ability to access to the e-service is important theme in the previous literature. For example, Huang (2003) finds that most of the websites in general fail to serve users with disabilities. Recommendation to improve accessibility is evident in previous literature including Jaeger (2006) who suggests the following to improve e-services’ accessibility like: design for accessibility from the outset of website development, Involve users with disabilities in the testing of the site …Focus on the benefits of an accessible Web site to all users.\n\nAccording to Grönlund et al. (2007), for a simple e-service, the needs for knowledge and skills, content and procedures are considerably less. However, in complicated services there are needed to change some prevailed skills, such as replacing verbal skills with skill in searching for information online.\n\nThis theme is concerned with establishing standards for measuring e-services or the best practices within the field. This theme also includes the international benchmarking of e-government services (UN reports, EU reports); much critic has been targeting these reports being incomprehensive and useless. According to Bannister (2007) “… benchmarks are not a reliable tool for measuring real e-government progress. Furthermore, if they are poorly designed, they risk distorting government policies as countries may chase the benchmark rather than looking at real local and national needs”\n\nDigital divide is considered one of the main barriers to implementing e-services; some people do not have means to access the e-services and some others do not know how to use the technology (or the e-service). According to Helbig et al. (2009), “we suggest E-Government and the digital divide should be seen as complementary social phenomena (i.e., demand and supply). Moreover, a serious e-government digital divide is that services mostly used by social elites.\"\n\nMost of the reports and the established criteria focus on assessing the services in terms of infrastructure and public policies ignoring the citizen participation or e-readiness. According to by Shalini (2009), “the results of the research project reveal that a high index may be only indicating that a country is e-ready in terms of ICT infrastructure and info-structure, institutions, policies, and political commitment, but it is a very poor measure of the e-readiness of citizens. To summarize the findings, it can be said that Mauritius is ready but the Mauritians are not”\n\n``E-readiness, as the Economist Intelligence Unit defines, is the measure of a country’s ability to leverage digital channels for communication, commerce and government in order to further economic and social development. Implied in this measure is the extent to which the usage of communications devices and Internet services creates efficiencies for business and citizens, and the extent to which this usage is leveraged in the development of information and communications technology (ICT) industries. In general terms, the definition of e-readiness is relative, for instance depending on a country in question's priorities and perspective.\n\nAs opposed to effectiveness, efficiency is focused on the internal competence within the government departments when delivering e-services. There is a complaint that researchers focus more on effectiveness “There is an emerging trend seemingly moving away from the efficiency target and focusing on users and governance outcome. While the latter is worthwhile, efficiency must still remain a key priority for eGovernment given the budget constraints compounded in the future by the costs of an ageing population. Moreover, efficiency gains are those that can be most likely proven empirically through robust methodologies”\n\nSecurity is the most important challenge that faces the implementation of e-services because without a guarantee of privacy and security citizens will not be willing to take up e-government services. These security concerns, such as hacker attacks and the theft of credit card information, make governments hesitant to provide public online services. According to the GAO report of 2002 “security concerns present one of the toughest challenges to extending the reach of e-government.The rash of hacker attacks, Web page defacing, and credit card information being posted on electronic bulletin boards can make many federal agency officials—as well as the general public—reluctant to conduct sensitive government transactions involving personal or financial data over the Internet.” By and Large, Security is one of the major challenges that faces the implementation and development of electronic services. people want to be assured that they are safe when they are conducting online services and that their information will remain secure and confidential\n\nAxelsson et al. (2009) argue that the stakeholder concept-which was originally used in private firms-, can be used in public setting and in the context of e-government. According to them, several scholars have discussed the use of the stakeholder theory in public settings. The stakeholder theory suggests that need to focus on all the involved stakeholder s when designing the e-service; not only on the government and citizens.\n\nCompared to Accessibility, There is sufficient literature that addresses the issue of usability; researchers have developed different models and methods to measure the usability and effectiveness of eGovernment websites. However, But still there is call to improve these measures and make it more compressive\n\n``The word usability has cropped up a few times already in this unit. In the context of biometric identification, usability referred to the smoothness of enrollment and other tasks associated with setting up an identification system. A system that produced few false matches during enrollment of applicants was described as usable. Another meaning of usability is related to the ease of use of an interface. Although this meaning of the term is often used in the context of computer interfaces, there is no reason to confine it to computers.´´\n\nThe perceived effectiveness of e-service can be influenced by public’s view of the social and cultural implications of e-technologies and e-service.\n\nImpacts on individuals’ rights and privacy – as more and more companies and government agencies use technology to collect, store, and make accessible data on individuals, privacy concerns have grown. Some companies monitor their employees' computer usage patterns in order to assess individual or workgroup performance. Technological advancements are also making it much easier for businesses, government and other individuals to obtain a great deal of information about an individual without their knowledge. There is a growing concern that access to a wide range of information can be dangerous within politically corrupt government agencies.\n\nImpact on Jobs and Workplaces - in the early days of computers, management scientists anticipated that computers would replace human decision-makers. However, despite significant technological advances, this prediction is no longer a mainstream concern. At the current time, one of the concerns associated with computer usage in any organization (including governments) is the health risk – such as injuries related to working continuously on a computer keyboard. Government agencies are expected to work with regulatory groups in order to avoid these problems.\n\nPotential Impacts on Society – despite some economic benefits of ICT to individuals, there is evidence that the computer literacy and access gap between the haves and have-nots may be increasing. Education and information access are more than ever the keys to economic prosperity, yet access by individuals in different countries is not equal - this social inequity has become known as the digital divide.\n\nImpact on Social Interaction – advancements in ICT and e-Technology solutions have enabled many government functions to become automated and information to be made available online. This is a concern to those who place a high value on social interaction.\n\nInformation Security - technological advancements allow government agencies to collect, store and make data available online to individuals and organizations. Citizens and businesses expect to be allowed to access data in a flexible manner (at any time and from any location). Meeting these expectations comes at a price to government agencies where it concerns managing information – more specifically, ease of access; data integrity and accuracy; capacity planning to ensure the timely delivery of data to remote (possibly mobile) sites; and managing the security of corporate and public information.\n\nThe benefits of e-services in advancing businesses efficiency and in promoting good governance are huge; recognizing the importance of these benefits has resulted in number of international awards that are dedicated to recognize the best designed e-services. In the section, we will provide description of some international awards\n\nEuropean eGovernment Awards program started 2003 to recognize the best online public service in Europe. The aim of Awards is to encourage the deployment of e-services and to bring the attention to best practices in the field. The winners of the |4th European eGovernment Awards were announced in the award ceremony that took place at the 5th Ministerial eGovernment Conference on 19 November 2009 (Sweden); the winners in their respective categories are:\n\n\nSultan Qaboos Award for excellence in eGovernance (Started 2009) The award has five categories: Best eContent, Best eService, Best eProject, eEconomy, eReadiness.\n\neGovernment Excellence Awards (Started 2007) The program has three categories: Government Awards: Best eContent, Best eService, Best eProject, eEconomy, eEducation, eMaturity Business Awards: Best ICT solution Provider, eEconomy, eEducation Citizen Awards: Best eContent, eCitizen.\n\nPhilippines e-Service Awards (Started 2001) Categories: Outstanding Client Application of the Year, Outstanding Customer Application of the year, Groundbreaking Technology of the Year, Most Progressive Homegrown Company of the Year.\n\nThere are some journals particularly interested for “e-Service “. Some of these are:\n\nMajor conferences considering e-service as one of the themes are:\n\n\n"}
{"id": "18905981", "url": "https://en.wikipedia.org/wiki?curid=18905981", "title": "Electromagnetic source imaging", "text": "Electromagnetic source imaging\n\nElectromagnetic Source Imaging is a functional imaging technique, which uses Electroencephalography and/or Magnetoencephalography measurements to map functional areas of the Cerebral cortex.\n\n"}
{"id": "12776956", "url": "https://en.wikipedia.org/wiki?curid=12776956", "title": "Engineer's Ring", "text": "Engineer's Ring\n\nThe Engineer's Ring is a ring worn by members of the United States Order of the Engineer, a fellowship of engineers who must be a certified Professional Engineer or graduated from an accredited engineering program (or be within one academic year of graduation to participate). The ring is usually a stainless steel band worn on the little finger of the dominant hand. This is so that it makes contact with all work done by the engineer. Rings used to be cast in iron in the most unattractive and simple form to show the nature of work. The ring is symbolic of the oath taken by the wearer, and symbolizes the unity of the profession in its goal of benefitting mankind. The stainless steel from which the ring is made depicts the strength of the profession.\n\nStarting in 1970, it is inspired by the original Canadian Iron Ring ceremony that started in 1922. Engineers receive the ring after taking an oath known as The Obligation of The Engineer, during a ring ceremony. Only those who have met the standards of professional engineering training or experience are able to accept the Obligation, which is voluntarily received for life.\n\nThe required oath, taken immediately before accepting the Engineer's Ring, is known as \"The Obligation of the Engineer\" and is as follows:\n\n\n"}
{"id": "2654494", "url": "https://en.wikipedia.org/wiki?curid=2654494", "title": "Engineering design management", "text": "Engineering design management\n\nEngineering design management is a knowledge area within engineering management. It represents the adaptation and application of customary management practices, with the intention of achieving a productive engineering design process. Engineering design management is primarily applied in the context of engineering design teams, whereby the activities, outputs and influences of design teams are planned, guided, monitored and controlled.\n"}
{"id": "25828273", "url": "https://en.wikipedia.org/wiki?curid=25828273", "title": "Explosive-driven ferroelectric generator", "text": "Explosive-driven ferroelectric generator\n\nAn explosive-driven ferroelectric generator (EDFEG, explosively pumped ferroelectric generator, EPFEG, or FEG) is a compact pulsed power generator, a device used for generation of short high-voltage high-current pulse. The energies available are fairly low, in the range of single joules, the voltages range in tens of kilovolts to over 100 kV, and the powers range in hundreds of kilowatts to megawatts. They are suitable for delivering high voltage pulses to high-impedance loads and can directly drive radiating circuits.\n\nECFEGs operate by releasing the electrical charge stored in the poled crystal structure of a suitable ferroelectric material, e.g. PZT, by an intense mechanical shock. They are a kind of phase transition generators.\n\nThe structure of an EDFEG is generally a block of a suitable high explosive, accelerating a metal plate into a target made of ferroelectric material.\n\nFEGs find multiple uses due to their compact character; charging banks of capacitors, initiation of slapper detonator arrays in nuclear weapons and other devices, driving nuclear fusion reactions, powering pulsed neutron generators, seed power sources for stronger pulse generators (e.g. EPFCGs), electromagnetic pulse generators, electromagnetic weapons, vector inversion generators, etc.\n\nA 2.4 megawatt HERF generator (an EDFEG with a pulse forming network directly driving a dipole antenna) with peak output frequency at 21.4 MHz was demonstrated.\n\n\n"}
{"id": "573752", "url": "https://en.wikipedia.org/wiki?curid=573752", "title": "Green paper", "text": "Green paper\n\nIn the European Union, the Commonwealth countries, Hong Kong and the United States, a green paper is a tentative government report and consultation document of policy proposals for debate and discussion. A green paper represents the best that the government can propose on the given issue, but, remaining uncommitted, it is able without loss of face to leave its final decision open until it has been able to consider the public reaction to it. Green papers may result in the production of a white paper. They may be considered as grey literature.\n\nA green paper in Canada, like a white paper, is an official government document. Green papers tend to be statements not of policy already determined, but of propositions put before the whole nation for discussion. They are produced early in the policy-making process, while ministerial proposals are still being formulated. Many white papers in Canada have been, in effect, green papers, while at least one green paper—that on immigration and population in 1975—was released for public debate after the government had already drafted legislation.\n\nSimilarly, in the UK, green papers are official consultation documents produced by the government for discussion both inside and outside Parliament, for instance when a government department is considering introducing a new law.\n\nA green paper released by the European Commission is a discussion document intended to stimulate debate and launch a process of consultation, at European level, on a particular topic. A green paper usually presents a range of ideas and is meant to invite interested individuals or organizations to contribute views and information. It may be followed by a white paper, an official set of proposals that is used as a vehicle for their development into law.\n\nA major review of defence policy in Australia culminated in a white paper issued in December 2000. Prior to this, a discussion paper was released in June 2000. This discussion paper was in nature what is known as a green paper (and was sometimes referred to as such).\n\nThe purpose of the 2008 EU green paper on copyright was to foster a debate on how knowledge for research, science and education can best be disseminated in the online environment. The green paper, which was published on 16 July 2008, aimed to set out a number of issues connected with the role of copyright in the \"knowledge economy\" and intended to launch a consultation on these issues (see this document). The EU asked for answers and comments to be submitted up to 30 November 2008.\n\n\n"}
{"id": "871236", "url": "https://en.wikipedia.org/wiki?curid=871236", "title": "Hanshin Industrial Region", "text": "Hanshin Industrial Region\n\nThe is one of the largest industrial regions in Japan. Its name comes from the \"on\"-reading of the kanji used to abbreviate the names of Osaka (大阪) and Kobe (神戸), the two largest cities in the megalopolis. The GDP of this area (Osaka and Kobe) is $341 billion, one of the world's most productive regions.\n2014 Osaka and Kyoto's GDP per capita (PPP) was US$35,902.\n\nFacilities:\n\nLaboratories, research institutes:\n\nFacilities:\n\n\n\n\nLaboratories, research institutes:\n\nFacilities: and research institutes:\n\nFacilities:\n\nLaboratories, research institutes:\n\nFacilities:\n\n\n\n\nLaboratories, research institutes:\n\nFacilities:\n\nLaboratories, research institutes:\n\nFacilities:\n\nLaboratories, research institutes:\n\n\n"}
{"id": "9703269", "url": "https://en.wikipedia.org/wiki?curid=9703269", "title": "History of multitrack recording", "text": "History of multitrack recording\n\nMultitrack recording of sound is the process in which sound and other electro-acoustic signals are captured on a recording medium such as magnetic tape, which is divided into two or more audio tracks that run parallel with each other. Because they are carried on the same medium, the tracks stay in perfect synchronisation, while allowing multiple sound sources to be recorded asynchronously. The first system for creating stereophonic sound (using telephone technology) was demonstrated by Clément Ader in Paris in 1881. The pallophotophone, invented by Charles A. Hoxie and first demonstrated in 1922, recorded optically on 35 mm film, and some versions used a format of as many as twelve tracks in parallel on each strip. The tracks were recorded one at a time in separate passes and were not intended for later mixdown or stereophony; as with later half-track and quarter-track monophonic tape recording, the multiple tracks simply multiplied the maximum recording time possible, greatly reducing cost and bulk. British EMI engineer Alan Blumlein patented systems for recording stereophonic sound and surround sound on disc and film in 1933. The history of modern multitrack audio recording using magnetic tape began in 1943 with the invention of stereo tape recording, which divided the recording head into two tracks.\n\nThe next major development in multitrack recording came in the mid-1950s, when the Ampex corporation devised the concept of 8-track recording, utilizing its \"Sel-Sync\" (Selective Synchronous) recording system, and sold the first such machine to musician Les Paul. However, for the next 35 years, multitrack audio recording technology was largely confined to specialist radio, TV and music recording studios, primarily because multitrack tape machines were both very large and very expensive - the first Ampex 8-track recorder, installed in Les Paul's home studio in 1957, cost a princely US$10,000 - roughly three times the US average yearly income in 1957, and equivalent to around $90,000 in 2016. However, this situation changed radically in 1979 with the introduction of the TASCAM Portastudio, which used the inexpensive compact audio cassette as the recording medium, making good-quality 4-track (and later 8-track) multitrack recording available to the average consumer for the first time. Ironically, by the time the Portastudio had become popular, electronics companies were already introducing digital audio recording systems, and by the 1990s, computer-based digital multitrack recording systems such as Pro Tools and Cubase were being adopted by the recording industry, and soon became standard. By the early 2000s, rapid advances in home computing and digital audio software were making digital multitrack audio recording systems available to the average consumer, and high-quality digital multitrack recording systems like GarageBand were being included as a standard feature on home computers.\nStereo sound recording on tape was perfected in 1943 by German audio engineers working for the AEG corporation. Around 250 stereo tape recordings were made during this period (of which only three have survived), but the technology remained a closely guarded secret within Germany until the end of World War II. After the war, American audio engineer John T. Mullin and the Ampex corporation pioneered the commercial development of tape recording in the USA, and the technology was rapidly taken up by radio and the music industry due to its superior sound fidelity and because tape - being a linear recording medium - could be easily edited, by physically cutting and splicing the tape, to remove unwanted elements and create a 'perfect' recording. 2-track tape recording was rapidly adopted for modern music in the 1950s because it enabled signals from two or more separate microphones to be recorded simultaneously, enabling stereophonic recordings to be made and edited conveniently, which in turn facilitated the rapid expansion of the consumer high-fidelity (\"HiFi\") market. Stereo (either true binaural two-microphone stereo or multimixed) quickly became the norm for commercial classical recordings and radio broadcasts, although many pop music and jazz recordings continued to be issued in monophonic sound until the late 1960s.\n\nMuch of the credit for the development of multitrack recording goes to guitarist, composer and technician Les Paul, who lent his name to Gibson's first solid-body electric guitar. His experiments with tapes and recorders in the early 1950s led him to order the first custom-built eight-track recorder from Ampex, and his pioneering recordings with his then wife, singer Mary Ford. But it was Patti Page who was the first vocalist to record her own voice, sound on sound, with a song called 'Confess', in 1947: Bill Putnam, an engineer for Mercury Records, was able to overdub Page's voice, due to his well-known use of technology. Thus, Page became the first pop artist to overdub her vocals on a song. This was months before Les Paul and Mary Ford had their first multi-voiced release. Paul was the first to make use of the technique of multitracking to record separate elements of a musical piece asynchronously — that is, separate elements could be recorded at different times. Paul's technique enabled him to listen to the tracks he had already taped and record new parts in time alongside them. In 1963, solo jazz pianist, Bill Evans, recorded Conversations with Myself, an innovative solo album using the unconventional (in jazz solo recordings) technique of overdubbing over himself, in effect creating a two-piano duet of jazz improvisations.\n\nDespite Ampex having created the first 8-track tape machines for Les Paul and Atlantic Records, multitrack recording was taken up in a more limited way in the industry via 3-track recorders. These proved extremely useful for popular music, since they enabled backing music to be recorded on two tracks (either to allow the overdubbing of separate parts, or to create a full stereo backing track) while the third track was reserved for the lead vocalist. Three-track recorders remained in widespread commercial use until the mid-1960s and many famous pop recordings — including many of Phil Spector's so-called \"Wall of Sound\" productions and early Motown hits — were taped on 3-track recorders.\n\nThe next evolution was 4-track recording, which was the studio standard through the mid 1960s. Many of the most famous recordings by The Beatles and The Rolling Stones were recorded on 4-track, and the engineers at London's Abbey Road Studios became particularly adept at the technique called \"reduction mixes\" in the UK and \"bouncing down\" in the United States, in which multiple tracks were recorded onto one 4-track machine and then mixed together and transferred (bounced down) to one track of a second 4-track machine. In this way, it was possible to record literally dozens of separate tracks and combine them into finished recordings of great complexity.\n\nBy the mid-1960s, the ready availability of the most up-to-date multitrack recorders - which were by then standard equipment in the leading Los Angeles recording studios - enabled Brian Wilson of The Beach Boys to become one of the first pop producers to exploit the huge potential of multitrack recording. During the group's most innovative period of music-making, from 1964 to 1967, Wilson developed elaborate techniques for assembling the band's songs, which combined elements captured on both 4-track and 8-track recorders, as well as making extensive use of tape editing. By 1964, Wilson's increasingly complex arrangements had far outstripped the group's limited musical abilities - singer-guitarist Carl Wilson was the only group member who regularly contributed to these tracking sessions - so Wilson began routinely recording all the instrumental backing tracks for his songs using the team of top-rank professional studio musicians who came to be known as \"The Wrecking Crew\". For the group's landmark \"Pet Sounds\" album in 1965, Wilson recorded all the album's elaborate backing tracks using The Wrecking Crew and other session musicians, while the Beach Boys were away touring; the session musicians typically performed these instrumental tracks as ensemble performances, which were recorded and mixed live, direct to a 4-track recorder. When the other Beach Boys returned from touring, they moved to Columbia's Hollywood studio, which was equipped with the latest 8-track technology; by this time, Wilson and his engineers had 'reduced' the pre-recorded 4-track backing tracks to a mono mix, which was then dubbed onto one track of the 8-track master tape; Wilson then recorded the vocal tracks, assigning one individual track to each of the six vocalists (including soon-to-be permanent member Bruce Johnston), leaving the eighth track available for final 'sweetening' elements, such as additional vocal or instrumental touches, and lastly, all these elements were mixed down to the mono master tape. Nearly all of the Beach Boys' 4-track and 8-track masters from this period are preserved in Capitol's archive, allowing the label to release several expansive boxed sets of this music; \"The Pet Sounds Sessions\" (1997), includes nearly all the separate backing and vocal tracks from the album, as well as new stereo mixes of all the songs, while the 9-CD \"The Smile Sessions\" (2011) features a wide cross-section of the huge amount of instrumental and vocal material (totalling around 50 hours of recordings) that was recorded for the group's never-completed 1967 \"magnum opus\", \"Smile\".\n\nAll of the Beatles classic mid-1960s recordings, including the albums \"Revolver\" and \"Sgt Pepper's Lonely Hearts Club Band\", were recorded in this way. There were limitations, however, because of the build-up of noise during the bouncing-down process, and the Abbey Road engineers are still justly famed for the ability to create dense multitrack recordings while keeping background noise to a minimum.\n\n4-track tape also led to a related development, quadraphonic sound, in which each of the four tracks was used to simulate a complete 360-degree surround sound. A number of albums including Pink Floyd's \"The Dark Side of the Moon\" and Mike Oldfield's \"Tubular Bells\" were released both in stereo and quadrophonic format in the 1970s, but 'quad' failed to gain wide commercial acceptance. Although it is now sometimes considered a gimmick, it was the direct precursor of the surround sound technology that has become standard in many modern home theater systems.\n\nIn a professional audio setting today, such as a recording studio, audio engineers may use 64 tracks or more for their recordings, utilizing one or more tracks for each instrument played.\n\nThe combination of the ability to edit via tape splicing, and the ability to record multiple tracks, revolutionized studio recording. It became common studio recording practice to record on multiple tracks, and mix down afterward. The convenience of tape editing and multitrack recording led to the rapid adoption of magnetic tape as the primary technology for commercial musical recordings. Although 33⅓ rpm and 45 rpm vinyl records were the dominant consumer format, recordings were customarily made first on magnetic tape, then transferred to disc, with Bing Crosby leading the way in the adoption of this method in the United States.\n\nThe original Ampex 8-track recorder (not to be confused with 8-track tape endless-loop cartridge players), model 5258, was an internal Ampex project. It was based on an Ampex 1\" data recorder transport with modified Ampex model 350 electronics. It stood over tall and weighed . 8 tracks were chosen because that was the number of recording tracks with guard tracks that would fit on a recording tape, the widest tape available at the time.\n\nThe first of the Ampex 8-track recorders was sold to Les Paul for $10,000 in 1957 and was installed in his home recording studio by David Sarser. It became known as the \"Octopus\".\n\nThe second Ampex model 5258 8-track was sold to Atlantic Records at Tom Dowd's insistence in late 1957. Atlantic was the first record company to use a multi-track (as opposed to stereo or 3-track) recorder in their studio.\n\nMulti-track recording differs from overdubbing and sound on sound because it records separate signals to individual tracks. Sound on sound which Les Paul invented adds a new performance to an existing recording by placing a second playback head in front of the erase head to play back the existing track before erasing it and re-recording a new track.\n\nMulti-track recorders also differ from early stereo and three track recorders that were available at the time in that they can record individual tracks while preserving the other tracks. The original multi-channel recorders could only record all tracks at once.\n\nThe earliest multitrack recorders were analog magnetic tape machines with two or three tracks. Elvis Presley was first recorded on multitrack during 1957, as RCA's engineers were testing their new machines. Buddy Holly's last studio session in 1958 employed three-track, resulting in his only stereo releases not to include overdubs. The new three-track system allowed the lead vocal to be recorded on a dedicated track, while the remaining two tracks could be used to record the backing tracks in full stereo.\n\nFrank Zappa experimented in the early 1960s with a custom built 5-track recorder built by engineer Paul Buff in his Pal Recording Studio in Rancho Cucamonga, California. Buff later went on to work in larger Hollywood studios. However, recorders with four or more tracks were restricted mainly to major American recording studios until the mid-to-late 1960s, mainly because of import restrictions and the high cost of the technology. In England, pioneering independent producer Joe Meek produced all of his innovative early 1960s recordings using monophonic and two-track recorders. Like Meek, EMI house producer George Martin was considered an innovator for his use of two-track as a means to making better mono records, carefully balancing vocals and instruments; Abbey Road Studios installed Telefunken four-track machines in 1959 and 1960 (replaced in 1965 by smaller, more durable Studer machines), but The Beatles would not have access to them until late 1963, and all recordings prior to their first world hit single \"I Want to Hold Your Hand\" (1964) were made on two-track machines.\n\nThe artistic potential of the multitrack recorder came to the attention of the public in the 1960s, when artists such as the Beatles and the Beach Boys began to multitrack extensively, and from then on virtually all popular music was recorded in this manner. The technology developed very rapidly during these years. At the start of their careers, the Beatles and Beach Boys each recorded live to mono, two-track (the Beatles), or three-track (the Beach Boys); by 1965 they used multitracking to create pop music of unprecedented complexity.\n\nThe Beach Boys' acclaimed 1966 LP \"Pet Sounds\" relied on multitrack recorders for its innovative production. Brian Wilson pretaped all the instrumental backing tracks with a large ensemble, recording the performances live, direct to a four-track recorder. These four-track backing tapes were then 'dubbed down' to one track of an eight-track tape. Six of the remaining seven tracks were then used to individually record the vocals of each member of The Beach Boys, and the eighth track was reserved for any final 'sweetening' overdubs of instruments or voices.\n\n3M introduced the 1-inch eight-track version of their model M-23 recorder in 1966, probably the first mass-produced machine of this format. It remained in production until 1970 and was used by many top studios worldwide including Abbey Road Studios in London. Both Pete Townshend and John Lennon had 3M 8-track machines in their home project studios c. 1969-1970. Ampex began mass production of their competing 1-inch eight-track MM1000 in 1967. One of the first 8-track machines in Los Angeles was built by Scully Recording Instruments of Bridgeport, Connecticut and installed at American Recorders in late 1967. The debut album by Steppenwolf was recorded there and was released in January 1968.\n\nBecause The Beatles did not gain access to eight-track recorders until 1968, their groundbreaking \"Sgt Pepper's Lonely Hearts Club Band\" LP (1967) was created using pairs of four-track machines; the group also used vari-speed (also called pitch shift) to achieve unique sounds, and they were the first group in the world to use an important offshoot of multitrack recording, the Automatic Double Tracking (ADT) system invented by Abbey Road staff engineer Ken Townsend in 1966.\n\nOther artists began experimenting with multitrack's possibilities also, with the Music Machine (of \"Talk Talk\" fame) recording on a custom-built ten-track setup, and Pink Floyd collaborating with former Beatles recording engineer Norman \"Hurricane\" Smith, who produced their first albums.\n\nThe first eight-track recorder in the UK was built by Scully and installed at London's Advision Studios in early 1968. Among the first eight-track recordings made there were the single \"Dogs\" by The Who and the album \"My People Were Fair and Had Sky in Their Hair... But Now They're Content to Wear Stars on Their Brows\" by the band Tyrannosaurus Rex. Trident Studios obtained their first eight-track recorder soon afterward. It was during The Beatles' recording of their \"White Album\" sessions of late 1968 that EMI's Abbey Road Studios finally had eight-track recorders installed, until then the group went to Trident to record with eight-tracks. The Beatles used eight-track to record portions of the \"White Album\", the single \"Hey Jude\" and the later \"Abbey Road\".\n\nOther western countries also lagged well behind the USA – in Australia, the largest local recording label, Festival Records, did not install a four-track recorder until late 1966; the first eight-track recorders did not appear there until the late 1960s.\n\nIn 1967 Ampex built its first prototype 16-track professional audio recorder at the request of Mirasound Studios in New York City. This machine used reels of 2-inch tape on a modified tape transport system originally built for video recording. In 1968 it introduced the 16-track production model MM-1000, the first commercially available 16-track machine. Machines of this size are difficult to move and costly to maintain. Prices were very high, typically $10,000 to $30,000 U.S. dollars.\n\nOne of the first 16-track recorders was installed at CBS Studios in New York City where it was used to record the second album by \"Blood, Sweat & Tears\" released in December 1968. The Grateful Dead released their first 16-track recordings \"Aoxomoxoa\" in June 1969 and \"Live/Dead\" in November 1969. TTG Studios in Los Angeles built its own 16-track machine in 1968. This was used on Frank Zappa's album \"Hot Rats\" released in October 1969. \"Volunteers\" by Jefferson Airplane was released in November 1969. The back of the Jefferson Airplane album cover includes a picture of the 16-track MM-1000.\n\nAdvision and Trident were also among the first in the U.K. to install 16-track machines. Trident installed its first 16-track machine in late 1969. \"After The Flood\", a song from the Van der Graaf Generator album \"The Least We Can Do Is Wave To Each Other\", was recorded at this studio on 16 tracks in December 1969. Production of 16-track machines boomed and the number of studios worldwide using these machines exploded during 1970 and 1971. By the end of 1971 there were at least 21 studios in London using 16-track recorders in conjunction with Dolby noise reduction. Groups using Trident at this time also included Genesis and David Bowie as well as Queen who experimented with multitracking extensively most prominently on their albums Queen II and \"A Night at the Opera\".\n\nAustralia's first sixteen-track recorder was installed at Armstrong's Studios in Melbourne in 1971; Festival installed Australia's first 24-track recorder at its Sydney studio in 1974. During the 1970s, sixteen, twenty-four, and thirty-two tracks became common in professional studios, with recording tape reaching two and three inches (5.08 cm - 7.62 cm) wide. The so-called \"golden age\" of large format professional analog recorders would last into the 1990s when the technology was mostly replaced with digital tape machines, and later on, computer systems using hard disk drives instead of tape. Some music producers and musicians still prefer working with the sound of vintage analog recording equipment despite the additional costs and difficulties involved.\n\nLarge format analog multitrack machines can have up to 24 tracks on a tape two inches wide which is the widest analog tape that is generally available. Prototype machines, by MCI in 1978, using 3\" tape for 32 tracks never went into production, though Otari made a 32 track 2\" MX-80. A few studios still operate large format analog recorders, though much of the time their use is only to copy sounds onto a modern digital format. Maintaining these machines has become increasingly difficult as new parts are rarely available. New tape is still available but prices have risen significantly in recent years.\n\nIn 1972 TEAC marketed their consumer 4-channel quadraphonic tape recorders for use as home multitrack recorders. The result were the popular TEAC 2340 and 3340 models. Both used ¼ inch tape. The 2340 ran at either 3¾ or 7½ inches per second and used 7-inch reels while the 3340 ran at 7½ or 15 inches per second and used 10½ inch reels. The 2340 was priced at under U.S. $1,000 making it very popular for home use.\n\nThe advent of the compact audio cassette (developed in 1963) ultimately led to affordable, portable four-track machines such as the Tascam Portastudio which debuted in 1979. Cassette-based machines could not provide the same audio quality as reel-to-reel machines, but served as a useful tool for professional and semi-pro musicians in making song demos. The Portastudio had a revolutionary effect on the emerging punk rock genre, because it enabled young bands to make recordings without signing to a record label. In the early years of punk, many bands self-produced their own recordings and sold them at gigs and by putting advertisements in underground zines. Bruce Springsteen's 1982 album \"Nebraska\" was made this way, with Springsteen choosing the album's earlier demo versions over the later studio recordings.\n\nThe familiar tape cassette was designed to accommodate four channels of audio – in a commercially recorded cassette these four tracks would normally constitute the stereo channels (each consisting of two tracks) for both 'sides' of the cassette – in a four-track cassette recorder all four tracks of a cassette are utilized together, often with the tape running at twice the normal speed (3¾ instead of 1⅞ inches per second) for increased fidelity. A separate signal can be recorded on to each of four tracks. (As such, the four-track machine does not utilize the two separate sides of the cassette in the conventional sense; if the cassette is inserted the other way round, all four tracks play in reverse.) As with professional machines, two or more tracks can be bounced down to one. When recording is complete, the volume level of each track is optimized, electronic effects such as reverb are added to certain tracks where desired, each track is separately 'panned' to the desired point in the stereo field and the resulting stereo signal is mixed down to a separate stereo machine (such as a conventional cassette recorder).\n\nBy the early 1970s, Thomas Stockham of Sound Stream Digital, created the first practical use of pulse code modulation, also called PCM digital recording, for high fidelity purposes. The first to be released were rereleased cleaned up versions of acoustic recordings made by the great tenor, Enrico Caruso. Early computer algorithms were used in the process of cleaning up the scratchy old 78 RPM records. The process could not be done in \"real time\" as the early computers were not very powerful or fast, compared to 2010-era computers. All of the data had to be stored on linear digital tape and then played back, in real time. The actual ingest from the 78 RPM records to the digital tape was also done in real-time. The computer processing to clean up the surface noise, pops and scratches took the early computers quite some time to process.\n\nBy the late 1970s, 3M introduced the first digital multi-track recorder. It utilized 1-inch wide tape and recorded 32 tracks. Unlike analog tape, edits could not be accomplished with a grease pencil, razor blades and splicing tape. So a secondary 4 track editing & mix down recorder was also created with an electronically-controlled edit controller to make effective digital edits. This early system used a 16-bit digital \"word\". The only converters of the day were 12 bit & 4 bit. So two were cascaded/daisy-chained to create the necessary 16-bit \"word\" for 96 DB of dynamic range. The signal was then sampled faster than any other digital recordings made up until that time at 50,000 times per second (50 kHz). It was known to be the best sounding of all the later digital multi-track recorders because their use of 50 kHz sampling did not become the industry standards later established as 44.1 kHz for CD's & 48 kHz for digital video.\n\nThe accepted world standard was created by Sony along with Philips. Sony created a 24 track digital recorder and Mitsubishi Corporation created a different 32 track digital recorder. The Mitsubishi recorded their data differently and it could be edited, the old-fashioned analog way, with a razor blade and splicing tape. The Sony used 1/2 inch tape whereas the Mitsubishi used 1 inch wide tape. So the first recordings that were released produced on the 3M, 32 track digital recorder were still analog vinyl releases, since the CD had yet to be invented. These professional linear tape digital recorders established the \"DASH\" format meaning, \" Digital Audio Stationary Head\". By the time the other manufacturers released their digital multi-track recorders, the CD had already been developed. The sampling rate dictates the upper range of the frequency response, whereas the bit depth dictates the dynamic range and signal to noise ratios.\n\nStarting in 1992, the ALESIS Corporation, a company which made digital drum machines and inexpensive analog audio mixers introduced the first multitrack, eight track, project studio, digital 8 track machine. It was named the ADAT, after the earlier 2 track digital recorders of the time known as DAT (Digital Audio Tape), which were based upon a small spinning head, similar to a consumer video recorder. The ADAT machine recorded its data in an already well-established consumer format based on VHS videotape recorder technology. Eight separate data tracks were recorded within the same bandwidth it took to record a TV show on a home video recorder (VHS). Numerous machines could be electronically locked together with a single cable. You could plug in enough 8 track machines together to create one giant 128 track machine. And like the professional studio recorders before it, a large full function remote control was also available.\n\nThe following year, the TEAC/TASCAM Corporation, introduced their DA-88. Those used the smaller 8 mm video format tapes. Those recorded four duplexed pairs of data tracks and would require a \"read before write\" function for overdubbing purposes of adjacent tracks. A full size remote and remote metering was also made available. Later units introduced by both companies provided for higher bit depths such as 20 & 24 bit. These machines like the early home studio TEAC's before them, slashed the prices of professional digital multitrack recording. It changed the recording industry forever.\n\nBy the late 1990s, dedicated multitrack recorders faded with the introduction of the Macintosh operating system and Windows operating systems in personal computers. Some of the first companies jumping on board with this technology were New England Digital and Digidesign, from the US & Fairlight, from Australia. Through the 1990s, multitrack recorders became digital, using a variety of technologies and media types. These including digital tape format (such as ADAT), or in some cases Minidiscs.\n\nSome of the leading providers of multitrackers were Tascam (hard drive or cassette based), Alesis (ADAT digital tape based), Roland/Boss (hard drive based), Fostex (hard drive based), Yamaha (hard drive based), and Korg.\n\nA highly competitive market and rapidly falling costs for this equipment has made it common to find multitrack recording technology outside a typical recording studio.\n\nThe first software-based digital multi-track recorder, called Deck, was released in 1990. The core engine technology and much of the user interface was programmed and designed by Josh Rosen, Mats Myrberg and John Dalton from a small San Francisco based company. They formed the platform upon which Pro Tools was built in 1991. The same technology lay behind the 1992 release of Cubase Audio, the first version to offer audio support in addition to MIDI sequencing capabilities.\n\nWhile hardware costs have fallen the power of the personal computer have increased, so that in the 2010s, a good quality home computer is sufficiently powerful to serve as a complete multitrack recorder, if a band or performer has a USB microphone or a regular microphone and a sound card adapter, using inexpensive hardware and software. Since 2012, the multitracking software GarageBand is offered as a free download for all of Apple's new computers or $4.99 for older models. GarageBand added to the many free or under $100 solutions available to the Windows platform that run on less expensive but often more powerful hardware. In a price range between $150 and usually under $1000, superior software mimicking complex recording studios that once could cost a hundred thousand dollars, or more, are also available However solutions this powerful are not needed for most applications. This is a far cry from the days when multitrack recorders cost thousands of dollars and few people could afford them. In early (circa) 2000 the availability of low cost CakeWalk for windows provided opportunities for many to get a start in multi-track digital recording on the Windows for around $50 to $100. After hardware became more powerful, more capabilities became available, including more digital channels and plug in effects.\n\nIn the 2010s, the availability of inexpensive software coupled with low cost hardware solutions, have enabled many singer-songwriters to self-produce their first recordings without paying high fees for a professional recording studio or audio engineer. The most popular DAW today are: Pro Tools, Reaper, Live, Cubase, Logic and Digital Performer.\n\n"}
{"id": "32261703", "url": "https://en.wikipedia.org/wiki?curid=32261703", "title": "Ip.access", "text": "Ip.access\n\nip.access Limited is a multinational corporation that designs, manufactures, and markets small cells (picocell and femtocell) technologies and infrastructure equipment for GSM, GPRS, EDGE, 3G and LTE. The firm has headquarters in Cambourne, England, the company also maintains operations and offices in Bellevue, United States, and Gurgaon and Pune, India.\n\nip.access combines IP and cellular technologies together to provide 2G, 3G and LTE coverage and for mobile networks .Using satellite backhaul, its products provide coverage to commercial passenger aircraft, ships, and users in remote rural areas.\n\nThe firm is a member of 3GPP, the Cambridge Network, European Telecommunications Standards Institute (ETSI), Small Cell Forum, and an associate member of the Network Vendors Interoperability Testing (NVIOT) Forum.\n\nip.access was founded in December 1999 as a wholly owned subsidiary of TTP Group PLC aimed at developing technologies that would allow multiple radio access technologies to communicate over the Internet. To accommodate its growing staff, in 2006 ip.access relocated to new offices in Cambourne Business Park, Cambridge, where it remains.\n\nIn October 2000, TTP Group spun off its communications division (TTP Communications, or TTPCom) in an initial public offering on the London Stock Exchange, and ip.access joined the spin-off as a wholly owned subsidiary of the TTPCom group.\n\nIn March 2006, the company secured an £8.5 million round of funding from Intel Capital, Scottish Equity Partners, and Rothschild & Cie Banque. As part of its June 2006 acquisition of TTP Communications, Motorola also gained a stake in ip.access. In 2007, after signing an OEM agreement with ip.access, ADC (now part of Tyco Electronics) made a minority interest investment in the company. Followed by, both Cisco Systems and Qualcomm making strategic financial investments in the company in 2008.\n\nIn July 2007, the firm became a founding member of the Femto Forum, renamed Small Cell Forum in February 2012. ip.access was named in The Sunday Times Fast Tech Track 100 in both 2007 and 2008. The company was also cited as the number one picocell vendor by global market intelligence company, ABI Research in 2008.\n\nIn 2009, ip.access was named in the Deloitte Technology Fast 500 EMEA. In April 2009, the company announced its Oyster 3G product would support femtocell standards published by 3GPP and the Broadband Forum. In March 2010, the company took part in the first Plugfest, organized by ETSI as part of its Plugtests program, held to demonstrate the effectiveness of the 3GPP femtocell standards in supporting interoperability between femtocell access points and network equipment from different vendors.\n\nIn June 2011, the market research and analysis firm Infonetics named ip.access along with its partner Cisco Systems, as the leading supplier of 3G femtocells. In August 2011, ip.access announced it had made more than 500,000 installations of its 3G technologies. In February 2013, ip.access announced it had become the first 3G small cell provider to ship one million residential units. In the same month, ip.access and iDirect completed successful interopability test of 3G small cells over IP Satellite.\n\nIn February 2014, ip.access launched a new range of small cells called presenceCell, which unlike traditional small cells, do not rely on providing indoor coverage and capacity to deliver a return on investment. Rather, the ultra-compact base stations are designed to capture anonymous user location and phone identity information from smartphones, which can be analysed and packaged as a service for a variety of businesses.\n\nThe telecommunications firms AT&T uses Oyster 3G as the core femtocell technology for its 3G MicroCell product. Cisco Systems, has jointly developed a femtocell solution with ip.access in compliance with the Broadband Forum's TR-069 technical specification.\n\nIn 2002, ip.access introduced the world’s first IP basestation controller for indoor GSM networks. nanoGSM uses 2G picocells that leverage the standard GSM air interface, full IP-based BSC, and an OMC-R management system that delivers voice, messaging and data to both 2G and 3G handsets at an indoor range of up to 200m.\n\nnano3G is an end-to-end Femtocell system with access points for Enterprise, E-class [E8, E16 and E24] and Small Medium Business, S-class [S8], access controller and element management system, providing carrier-class coverage to commercial and consumer users.\n\nLaunched at the 2007 3GSM World Congress in Barcelona, Spain, the Oyster 3G is ip.access' core 3G femtocell technology used by system integrators and OEM customers to integrate WCDMA femtocells into home gateways, set-top boxes, and other devices. ip.access' Oyster 3G is the core technology of AT&T's 3G MicroCell\n\nnanoLTE [E-40, E-100] is an Enterprise grade platform that brings LTE capacity both in-doors and in public spaces, while also offering the option of providing extra 3G infill and Circuit Switch Fall Back (CSFB) capacity.\n\nLaunched in 2014, the presenceCell is a new range of small cell, designed to capture precise user location data via their smart phone, which can be analysed and packaged as a service for a variety of businesses.\nIn addition to the presenceCell, ip.access also provides the back-end processing and management system that delivers the Presence data anonymously and securely to vertical application providers. The company’s Network Orchestration System serves as the infrastructure management solution and also supports the GSMA’s OneAPI standard, which allows third parties to provide value-added services through web friendly message interfaces. The presenceCell was commercially deployed by Vodafone Turkey in 2015.\n\nAmong ip.access' major customers are AT&T, Bharti Airtel, Blue Ocean Wireless, Bouygues Telecom,Jersey Telecom Monaco Telecom, SFR, SPIE SA, T-Mobile, Tele2, Telefónica O2 Czech Republic,Telia Sonera, Vivacom and Vodafone\n\nThe company's technology partners include AeroMobile, Altobridge, Blue Ocean Wireless, Cisco Systems, Private Mobile Networks, Qualcomm, Quortus, Setcom, and TriaGnoSys.\n\nCorporate, product, and personnel awards won by ip.access include the following:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "27774511", "url": "https://en.wikipedia.org/wiki?curid=27774511", "title": "Lakes District Technocity", "text": "Lakes District Technocity\n\nThe Lakes District Technocity(established in 2004) is a science park located on the campus of Süleyman Demirel University. The technocity is a full member of International Association of Science Parks. The science park occupies 112000 squaremeters area.\n\nThe activities of the technocity include Energy and Renewable Energy, Internet Technology and Services / E-Business, Plasma Technology, and Environment Technologies. \n\nAs of 2010, 57% of firms on the tecnocity involved in the area of computer software industry.\n\nThe technocity made its first export(plasma sensors to Saudi Arabia) in 2008.\n\nThe following entities are shareholders of the park:\n\n"}
{"id": "22280100", "url": "https://en.wikipedia.org/wiki?curid=22280100", "title": "List of DVD authoring applications", "text": "List of DVD authoring applications\n\nThe following applications can be used to create playable DVDs.\nFree software implementations often lack features such as encryption and region coding due to licensing restrictions issues, and depending on the demands of the DVD producer, may not be considered suitable for mass-market use. \n\n\n\n\n"}
{"id": "24529515", "url": "https://en.wikipedia.org/wiki?curid=24529515", "title": "List of DVD recordable manufacturers", "text": "List of DVD recordable manufacturers\n\nThis aims to be a complete list of DVD recordable manufacturers.\nThis list is not necessarily complete or up to date - if you see a manufacturer that should be here but isn't (or one that shouldn't be here but is), please update the page accordingly.\n\n\n\n\n\n\n\n"}
{"id": "10107030", "url": "https://en.wikipedia.org/wiki?curid=10107030", "title": "List of cameras supporting a raw format", "text": "List of cameras supporting a raw format\n\nThe following digital cameras allow photos to be taken and saved in at least one raw image format. Some cameras support more than one, usually a proprietary format and Digital Negative (DNG).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following cameras allow audio and video to be shot in at least one raw (often similar to raw image format, but also with proprietary YCbCr and RGB coding and/or lossy wavelet compression) format.\n\nWith proprietary ArriRaw and HDMI or HD-SDI uncompressed video format.\n\n\n\nUnofficially, with the use of Magic Lantern software, the following EOS cameras can record RAW video:\n\n\n\n\nWith proprietary REDCODE raw format with lossy compression for video.\n\n\n\nSome Nikon Coolpix cameras which are not advertised as supporting a RAW image format can actually produce usable raw files if switched to a maintenance mode. Note that switching to this mode can invalidate a camera's guarantee. Nikon models with this capability:\n\nE700, E800, E880, E900, E950, E990, E995, E2100, E2500, E3700, E4300, E4500.\n\nSome Canon PowerShot cameras with DiGiC II and certain DiGiC III image processors which are not advertised as supporting a RAW format can actually produce usable raw files with an unofficial open-source firmware add-on by some users.\n\nThe Nokia N900 mobile phone has an add on app \"Fcam\", which allows capture and saving of RAW files in Adobe's DNG format (along with other advanced features usually found in DSLRs).\nIn 2013, Nokia launched Nokia Lumia 1520 and Nokia Lumia 1020 smartphones with DNG RAW format.\n\nSamsung Galaxy Note 5, Note 7, Galaxy S6 Edge+, S7 and S7 Edge also support RAW image capture. Not to mention LG G4, LG G5, iPhone 6s (Plus), iPhone SE, iPhone 7 (Plus) iPad Pro and some other modern phones - OnePlus One, OnePlus Two, OnePlus 3(T), etc.\n\n\n"}
{"id": "10280304", "url": "https://en.wikipedia.org/wiki?curid=10280304", "title": "Malaria vaccine", "text": "Malaria vaccine\n\nMalaria vaccine is a vaccine that is used to prevent malaria. The only approved vaccine as of 2015 is RTS,S. It requires four injections, and has a relatively low efficacy. Due to this low efficacy, WHO does not recommend the use of RTS,S vaccine in babies between 6 and 12 weeks of age.\n\nThe vaccine is going to be studied further in Africa in 2018. Research continues into recombinant protein and attenuated whole organism vaccines.\n\nRTS,S (developed by PATH Malaria Vaccine Initiative (MVI) and GlaxoSmithKline (GSK) with support from the Bill and Melinda Gates Foundation) is the most recently developed recombinant vaccine. It consists of the \"P. falciparum\" circumsporozoite protein (CSP) from the pre-erythrocytic stage. The CSP antigen causes the production of antibodies capable of preventing the invasion of hepatocytes and additionally elicits a cellular response enabling the destruction of infected hepatocytes. The CSP vaccine presented problems in trials due to its poor immunogenicity. RTS,S attempted to avoid these by fusing the protein with a surface antigen from hepatitis B, hence creating a more potent and immunogenic vaccine. When tested in trials an emulsion of oil in water and the added adjuvants of monophosphoryl A and QS21 (SBAS2), the vaccine gave protective immunity to 7 out of 8 volunteers when challenged with \"P. falciparum.\"\n\nRTS,S/AS01 (commercial name Mosquirix), was engineered using genes from the outer protein of \"P. falciparum\" malaria parasite and a portion of a hepatitis B virus plus a chemical adjuvant to boost the immune response. Infection is prevented by inducing high antibody titers that block the parasite from infecting the liver. The developers are non-profit In November 2012 a Phase III trial of RTS,S found that it provided modest protection against both clinical and severe malaria in young infants.\n\nIn a bid to accommodate a larger group and guarantee a sustained availability for the general public, GSK applied for a marketing license with the European Medicines Agency (EMA) in July 2014. GSK treated the project as a non-profit initiative, with most funding coming from the Gates Foundation, a major contributor to malaria eradication.\n\nOn 24 July 2015, Mosquirix received a positive opinion from the EMA on the proposal of the vaccine to be used to vaccinate children aged 6 weeks to 17 months outside the European Union.\n\nThe task of developing a preventive vaccine for malaria is a complex process. There are a number of considerations to be made concerning what strategy a potential vaccine should adopt.\n\n\"P. falciparum\" has demonstrated the capability, through the development of multiple drug-resistant parasites, for evolutionary change. The \"Plasmodium\" species has a very high rate of replication, much higher than that actually needed to ensure transmission in the parasite’s life cycle. This enables pharmaceutical treatments that are effective at reducing the reproduction rate, but not halting it, to exert a high selection pressure, thus favoring the development of resistance. The process of evolutionary change is one of the key considerations necessary when considering potential vaccine candidates. The development of resistance could cause a significant reduction in efficacy of any potential vaccine thus rendering useless a carefully developed and effective treatment. \n\nThe parasite induces two main response types from the human immune system. These are anti-parasitic immunity and anti-toxic immunity.\n\n\nTaking this information into consideration an ideal vaccine candidate would attempt to generate a more substantial cell-mediated and antibody response on parasite presentation. This would have the benefit of increasing the rate of parasite clearance, thus reducing the experienced symptoms and providing a level of consistent future immunity against the parasite.\n\nBy their very nature, protozoa are more complex organisms than bacteria and viruses, with more complicated structures and life cycles. This presents problems in vaccine development but also increases the number of potential targets for a vaccine. These have been summarised into the life cycle stage and the antibodies that could potentially elicit an immune response.\n\nThe epidemiology of malaria varies enormously across the globe, and has led to the belief that it may be necessary to adopt very different vaccine development strategies to target the different populations. A Type 1 vaccine is suggested for those exposed mostly to \"P. falciparum\" malaria in sub-Saharan Africa, with the primary objective to reduce the number of severe malaria cases and deaths in infants and children exposed to high transmission rates. The Type 2 vaccine could be thought of as a ‘travellers’ vaccine’, aiming to prevent all cases of clinical symptoms in individuals with no previous exposure. This is another major public health problem, with malaria presenting as one of the most substantial threats to travellers’ health. Problems with the current available pharmaceutical therapies include costs, availability, adverse effects and contraindications, inconvenience and compliance, many of which would be reduced or eliminated entirely if an effective (greater than 85–90%) vaccine was developed. \n\nThe life cycle of the malaria parasite is particularly complex, presenting initial developmental problems. Despite the huge number of vaccines available at the current time, there are none that target parasitic infections. The distinct developmental stages involved in the life cycle present numerous opportunities for targeting antigens, thus potentially eliciting an immune response. Theoretically, each developmental stage could have a vaccine developed specifically to target the parasite. Moreover, any vaccine produced would ideally have the ability to be of therapeutic value as well as preventing further transmission and is likely to consist of a combination of antigens from different phases of the parasite’s development. More than 30 of these antigens are currently being researched by teams all over the world in the hope of identifying a combination that can elicit immunity in the inoculated individual. Some of the approaches involve surface expression of the antigen, inhibitory effects of specific antibodies on the life cycle and the protective effects through immunization or passive transfer of antibodies between an immune and a non-immune host. The majority of research into malarial vaccines has focused on the \"Plasmodium falciparum\" strain due to the high mortality caused by the parasite and the ease of a carrying out in vitro/in vivo studies. The earliest vaccines attempted to use the parasitic circumsporozoite (CS) protein. This is the most dominant surface antigen of the initial pre-erythrocytic phase. However, problems were encountered due to low efficacy, reactogenicity and low immunogenicity. \n\n\nIncreasing the potential immunity generated against \"Plasmodia\" can be achieved by attempting to target multiple phases in the life cycle. This is additionally beneficial in reducing the possibility of resistant parasites developing. The use of multiple-parasite antigens can therefore have a synergistic or additive effect.\n\nOne of the most successful vaccine candidates currently in clinical trials consists of recombinant antigenic proteins to the circumsporozoite protein. (This is discussed in more detail below.)\n\nThe selection of an appropriate system is fundamental in all vaccine development, but especially so in the case of malaria. A vaccine targeting several antigens may require delivery to different areas and by different means in order to elicit an effective response. Some adjuvants can direct the vaccine to the specifically targeted cell type—e.g. the use of Hepatitis B virus in the RTS,S vaccine to target infected hepatocytes—but in other cases, particularly when using combined antigenic vaccines, this approach is very complex. Some methods that have been attempted include the use of two vaccines, one directed at generating a blood response and the other a liver-stage response. These two vaccines could then be injected into two different sites, thus enabling the use of a more specific and potentially efficacious delivery system.\n\nTo increase, accelerate or modify the development of an immune response to a vaccine candidate it is often necessary to combine the antigenic substance to be delivered with an adjuvant or specialised delivery system. These terms are often used interchangeably in relation to vaccine development; however in most cases a distinction can be made. An adjuvant is typically thought of as a substance used in combination with the antigen to produce a more substantial and robust immune response than that elicited by the antigen alone. This is achieved through three mechanisms: by affecting the antigen delivery and presentation, by inducing the production of immunomodulatory cytokines, and by affecting the antigen presenting cells (APC). Adjuvants can consist of many different materials, from cell microparticles to other particulated delivery systems (e.g. liposomes).\n\nAdjuvants are crucial in affecting the specificity and isotype of the necessary antibodies. They are thought to be able to potentiate the link between the innate and adaptive immune responses. Due to the diverse nature of substances that can potentially have this effect on the immune system, it is difficult to classify adjuvants into specific groups. In most circumstances they consist of easily identifiable components of micro-organisms that are recognised by the innate immune system cells. The role of delivery systems is primarily to direct the chosen adjuvant and antigen into target cells to attempt to increase the efficacy of the vaccine further, therefore acting synergistically with the adjuvant.\n\nThere is increasing concern that the use of very potent adjuvants could precipitate autoimmune responses, making it imperative that the vaccine is focused on the target cells only. Specific delivery systems can reduce this risk by limiting the potential toxicity and systemic distribution of newly developed adjuvants.\n\nStudies into the efficacy of malaria vaccines developed to date have illustrated that the presence of an adjuvant is key in determining any protection gained against malaria. A large number of natural and synthetic adjuvants have been identified throughout the history of vaccine development. Options identified thus far for use combined with a malaria vaccine include mycobacterial cell walls, liposomes, monophosphoryl lipid A and squalene.\n\nA completely effective vaccine is not yet available for malaria, although several vaccines are under development. SPf66 a synthetic peptide based vaccine developed by Manuel Elkin Patarroyo team in Colombia was tested extensively in endemic areas in the 1990s, but clinical trials showed it to be insufficiently effective, 28% efficacy in South America and minimal or no efficacy in Africa. Other vaccine candidates, targeting the blood-stage of the parasite's life cycle, have also been insufficient on their own. Several potential vaccines targeting the pre-erythrocytic stage are being developed, with RTS,S showing the most promising results so far.,<ref name=\"doi10.1056/NEJMoa1208394\"></ref>\n\nIn 2015, researchers used a repetitive antigen display technology to engineer a nanoparticle that displayed malaria specific B cell and T cell epitopes. The particle exhibited icosahedral symmetry and carried on its surface up to 60 copies of the RTS,S protein. The researchers claimed that the density of the protein was much higher than the 14% of the GSK vaccine.\n\nThe PfSPZ vaccine is a candidate malaria vaccine developed by Sanaria using radiation-attenuated sporozoites to elicit an immune response. Clinical trials have been promising, with trials taking place in Africa, Europe, and the US protecting over 80% of volunteers. It has been subject to some criticism regarding the ultimate feasibility of large-scale production and delivery in Africa, since it must be stored in liquid nitrogen.\n\nThe PfSPZ vaccine candidate has granted fast track designation by the U.S. Food and Drug Administration in September 2016.\n\nIndividuals who are exposed to the parasite in endemic countries develop acquired immunity against disease and death. Such immunity does not however prevent malarial infection; immune individuals often harbour asymptomatic parasites in their blood. This does, however, imply that it is possible to create an immune response that protects against the harmful effects of the parasite.\n\nResearch shows that if immunoglobulin is taken from immune adults, purified and then given to individuals who have no protective immunity, some protection can be gained.\n\nIn 1967, it was reported that a level of immunity to the Plasmodium berghei parasite could be given to mice by exposing them to sporozoites that had been irradiated by x-rays. Subsequent human studies in the 1970s showed that humans could be immunized against Plasmodium vivax and Plasmodium falciparum by exposing them to the bites of significant numbers of irradiated mosquitos.\n\nFrom 1989 to 1999, eleven volunteers recruited from the United States Public Health Service, United States Army, and United States Navy were immunized against Plasmodium falciparum by the bites of 1001 to 2927 mosquitos that had been irradiated with 15,000 rads of gamma rays from a Co-60 or Cs-137 source. This level of radiation being sufficient to attenuate the malaria parasites so that while they could still enter hepatic cells, they could not develop into schizonts or infect red blood cells. Over a span of 42 weeks, 24 of 26 tests on the volunteers showed that they were protected from malaria infection.\n\n\n"}
{"id": "45821282", "url": "https://en.wikipedia.org/wiki?curid=45821282", "title": "MessagEase", "text": "MessagEase\n\nMessagEase is an input method and virtual keyboard for touchscreen devices. It relies on a new entry system designed by Saied B. Nesbat, formatted as a 3x3 matrix keypad where users may press or swipe up, down, left, right, or diagonally to access all keys and symbols. It is a keyboard that was designed for devices like cell phones, mimicking the early cell phones' limited number of 12 keys.\n\nThe most common letters (the large letters in the illustration below) are accessed by a tap. Less common letters are accessed by a slide. Example: Tapping the center square generates an 'o'. Sliding to the left from the same square generates a 'c'. A green trail shows the path of the finger. The keyboard supports multiple user dictionaries, used for word prediction and correction.\n\nThe software is developed and patented by ExIdeas, based in Belmont, California. It was first released in 2002 for the Palm, along with a paper in 2003.\n\nThe keyboard layout has a 3x3 matrix that allows for full-text entry. The letter placement is optimized for minimal movement distance between letters, allowing for faster typing.\n\nThe layout is 67% faster than a standard QWERTY software keyboard, and 31% faster than a multi-tap keyboard, when typing is modeled with Fitt's law.\n\nThe 9 most frequent letters in English texts: ETAONRISH, are placed on the keyboard so they can be accessed on a single click.\n\nThe next 17 less frequent letters: DLFCMUGYPWBVKJXQZ, are placed as to be triggered by a single move of the finger from or to the central key (O) (except for Z which is centered around the 'E' key together with some punctuation characters). For example, the letter V is typed by dragging the finger from A to O, and the letter D by moving from O to E.\n\nThe moves producing special characters, which includes 38 characters including accents and punctuation marks, are displayed on a complete keyboard showing up when the user drags the space bar upwards.\n\nThis is not an alternate keyboard in the sense that the key pair moves are valid on both keyboard. It is rather a mnemonic help, which is normally hidden to avoid overwhelming the user with spurious information.\n\nA small vertical bar on the right gives direct access to the cut/copy/paste operations, the numeric keypad, the uppercase/lowercase control, as well the usual F1-F12 control keys. This is also commanded by moving the finger from one cell to an other.\n\nA set of small movements makes the life of the typist easier, like drawing a small circle or a back and forth movement to write a letter uppercase, or prolonging the movement to put accent on letters.\n\nThe keyboard can be resized to fit the need of the user, and is also provided in a double sized version with the numeric keypad on the side of the alphabetic keypad.\n\nThe keyboard is currently available for Android devices, iOS devices and the Apple Watch.\n\n\"Currently supported languages:\"\nMessagEase was released in 2002 for the Palm. It was also originally a competitor to the T9 predictive input method, on a 12-button phone, with 9 number buttons. In this first iteration, each of the 9 primary characters needed to be pressed twice in a row, and secondary characters were entered by first pressing the main button, and then pressing one of the remaining 8 buttons. In this first iteration, because many letters required two presses, it was not significantly faster than the Multi-tap input method.\n\nMessagEase is now exclusively for touch screens, and no longer has physical 12-button support. All characters are now entered by tapping or swiping.\n\n"}
{"id": "55886058", "url": "https://en.wikipedia.org/wiki?curid=55886058", "title": "Mobile Telecommunications Company of Esfahan", "text": "Mobile Telecommunications Company of Esfahan\n\nMobile Telecommunication Company of Esfahan (شرکت مخابرات سیار اصفهان, MTCE) also known as Espadan was a mobile network operator in Iran.\n\nFollowing the agreements reached during the Iranian President Hashemi Rafsanjani's visit to Malaysia in 1994, and according to the laws of attracting and supporting foreign investment, in 29 May 2001 the license for launching the first prepaid mobile telephone network in Iran with an initial capacity of 20 thousand subscribers was granted to Celcom for 15 years, and the Malaysian Technology Resources Industry Company received permission to use GSM 24.18 Mbit/s bandwidth within the Isfahan province.\n\nIn 2005 Celcom sold its stakes to TM International(Telekom Malaysia).\n\nIn 2008 MTCE bought the WiMAX license from Iranian Communications Regulatory Authority (CRA).\n\nIn 2011 MTCE went broke and deactivated all sim cards following merger with Mobile Telecommunications Company of Iran.\n\nAxiata Group Berhad (49%) | Telecommunications Company of Esfahan (49%) | Iran Telecom Industries (2%)\n"}
{"id": "10133876", "url": "https://en.wikipedia.org/wiki?curid=10133876", "title": "Mobile architecture", "text": "Mobile architecture\n\nIn the past computers needed to be disconnected from their internal network if they needed to be taken or moved anywhere. \"Mobile architecture\" allows maintaining this connection whilst during transit. Each day the number of mobile devices is increasing, mobile architecture is the pieces of technology needed to create a rich, connected user experience. \nCurrently there is a lack of uniform interoperability plans and implementation. There is a lack of common industry view on architectural framework. This increases costs and slows down 3rd party mobile development. An open approach is required across all industries to achieve same end results and services.\n\n\nThe basic and detail architecture of the Mobile device consists of Hardware and Software architecture. The main hardware components\nof the mobile phone is the application processor that controls all other components of the device such as display, keypad, power, audio , video etc. The radio signals are handled by base band\nprocessor which in turn communicates with other processors to use their functionality. Power and audio processor controls the functioning of speaker and microphone with the help of application\nprocessor. Subscriber Identification Module (SIM) contains the details about the subscriber.\n\n\nA consortium of companies are pushing for products and services to be based on open, global standards, protocols and interfaces and are not locked to proprietary technologies. The applications layer to be bearer agnostic (examples: GSM, GPRS, EDGE, CDMA, UMTS). The architecture framework and service enablers will be independent of operating systems. There will be support for inseparability of applications and platforms, seamless geographic and inter-generational roaming.\n\n\nAlso:\nMobile Architecture – architecture, a tipe of building or building components;(transform, move, adapt, interact)\n"}
{"id": "3809811", "url": "https://en.wikipedia.org/wiki?curid=3809811", "title": "Mobile search", "text": "Mobile search\n\nMobile search is an evolving branch of information retrieval services that is centered on the convergence of mobile platforms and mobile phones, or that it can be used to tell information about something and other mobile devices. Web search engine ability in a mobile form allows users to find mobile content on websites which are available to mobile devices on mobile networks. As this happens mobile content shows a media shift toward mobile multimedia. Simply put, mobile search is not just a spatial shift of PC web search to mobile equipment, but is witnessing more of treelike branching into specialized segments of mobile broadband and mobile content, both of which show a fast-paced evolution.\n\n\"Competition for the US mobile search market promises to be fierce, thanks to the large US online ad market and strong pushes by portals. By 2019, mobile ad spending will rise to $65.87 billion, or 72.2% of total digital ad spend\", according to a leading market research firm; eMarketer. Depending on a researcher's particular bias toward telecom, Web or technology factors, the published forecasts for global mobile search vary from $1.5 billion by 2011 (from Informa Telecoms & Media) to over $11 billion by 2008 (according to Piper Jaffray).\n\nMobile search is important for the usability of mobile content for the same reasons as internet search engines became important to the usability of internet content. Early internet content was largely provided by portals such as Netscape. As the depth of available content grew, portals were unable to provide total coverage. As a result, Internet web search engines such as Google and AltaVista proved popular as a way of allowing users to find the increasingly specialist content they were looking for. In an international journal article, 'Exploring the logic of mobile search', Westlund, Gómez-Barroso, Compañó, and Feijóo(2011) outline a thorough review of research on mobile search usage, and also present an in-depth study of user patterns. They conclude that mobile search has started to change mobile media consumption patterns radically. They also emphasize that future developments of mobile search must be sensitive to the mobile logic.\n\nWithin the broad umbrella of mobile search (the ability to browse for mobile specific content), there are a range of services. Given the relative immaturity of the market, not all of these can be expected to become the industry standards.\n\nMost major search engines have implemented a mobile optimized version of their products that take into consideration bandwidth and form factor limitations of the mobile platform. For example, Google has launched a mobile-friendly version of their search engine. The algorithms for mobile search engine results are thought to be evolving and aspects such as location and predictive searching will become increasingly important. Google just released its latest 160 page Full Search Quality Raters Guide with a new emphasis on Mobile and Usefulness.\n\nThese services allow a user to text a question to a central database and receive a reply using text. A usage example would be a user that wants to know the answer to a very specific question but is not in front of his/her computer. Most mobile 'Q&A' services are powered by human researchers and are therefore a type of organic search engine. A new approach by AskMeNow and MobileBits is to use Semantic Web technology to automate the process.\n\nThis service is known by different names dependent on country and operator. It can also be known as 'Find My Nearest' or 'Mobile Yellow Pages' services. The basics of the services allow users to find local services in the vicinity of their current location. The services often use location-based technology to pinpoint exactly where the user currently is. An example of usage would be a user looking for a local cab or taxi company after a night out. Services also usually come with a map and directions to help the user. An example is the service offered by Yell in the UK which is powered by MobilePeople's technology.\n\nThese services offer users recommendations on what they should do next. An example would be recommending a user a similar ringtone to the one that s/he has just browsed for. They operate, in a mobile context, in a similar way to the recommendation engines provided by internet retail shops such as Amazon.com. An example of real usage is the Directory Enquiries (DQ) service operated by Orange in the UK. Callers to the Orange landline DQ service are given the business and residential numbers they have requested verbally by an operator. In addition, Orange sends the information in text format to the users mobile phone. The information contains a text reminder of the requested information as well as links to local businesses, services and other interesting information in the local area that the user has searched on.\n\nThese services provide the indexing structure to the portals provided by mobile operators. They index the content already on the operators' portal but also provide users access to mobile specific content that is available outside the confines of the portal.\n\nA new category of mobile search tool that is emerging is one in which a pre-selected set of possible search content is downloaded in advance by a mobile user and then allows for a final internet search step. An example of such search tools is the Worldport Navigator for the iPhone, which provides users with a push-button experience of selecting from thousands of human-screened and categorized Web selections in three or four seconds, without the need for text entry, search, result review, or page-scrolling.\n\n"}
{"id": "51682311", "url": "https://en.wikipedia.org/wiki?curid=51682311", "title": "Multefire", "text": "Multefire\n\nMulteFire™ is an LTE-based technology that operates standalone in unlicensed and shared spectrum, including the global 5 GHz band. Based on 3GPP Release 13 and 14, MulteFire technology supports Listen-Before-Talk for fair co-existence with Wi-Fi and other technologies operating in the same spectrum. It supports private LTE and neutral host deployment models. Target vertical markets include industrial IoT, enterprise, cable, and various other vertical markets.\n\nThe MulteFire Release 1.0 specification was developed by the MulteFire Alliance, an independent, diverse and international member-driven consortium. Release 1.0 was published to MulteFire Alliance members in January 2017 and was made publicly available in April 2017. The MulteFire Alliance is currently working on Release 1.1 which will add further optimizations for IoT and new spectrum bands.\n\nAccording to Harbor Research in its published white paper, the market opportunity for private LTE networks for industrial and commercial IoT will reach $118.5 billion in 2023. It also reported that the total addressable revenue for Enterprise markets deploying private and neutral host LTE with MulteFire will reach $5.7 billion by 2025.  \n\nThe MulteFire Alliance has grown to more than 40 members. Its board members include Boingo Wireless, CableLabs, Ericsson, Huawei, Intel, Nokia, Qualcomm and SoftBank. The organization is open to any company with an interest in advancing LTE and cellular technology in unlicensed and shared spectrum.  \n\n"}
{"id": "2637837", "url": "https://en.wikipedia.org/wiki?curid=2637837", "title": "People Finder Interchange Format", "text": "People Finder Interchange Format\n\nPeople Finder Interchange Format (PFIF) is a widely used open data standard for information about missing or displaced people. PFIF was designed to enable information sharing among governments, relief organizations, and other survivor registries to help people find and contact their family and friends after a disaster.\n\nPFIF is extended from XML. It consists of person records, which contain identifying information about a person, and note records, which contain comments and updates on the status and location of a person. Each note is attached to one person. PFIF defines the set of fields in these records and an XML-based format to store or transfer them. PFIF XML records can be embedded in Atom feeds or RSS feeds.\n\nPFIF allows different repositories of missing person data to exchange and aggregate their records. Every record has a unique identifier, which indicates the domain name of the original repository where the record was created. The unique record identifier is preserved as the record is copied from one repository to another. For example, any repository that receives a copy of a given person can publish a note attached to that person, and even as the note and person are copied to other repositories, they remain traceable to their respective original sources.\n\nWithin three days after the 2001 September 11 attacks, people were using over 25 different online forums and survivor registries to report and check on their family and friends.\nOne of the first and largest of these was the survivor registry at safe.millennium.berkeley.edu, which was created by graduate students Ka-Ping Yee and Miriam Walker and hosted on the Millennium computer cluster at UC Berkeley. To reduce the confusion caused by the proliferation of different websites, the Berkeley survivor registry began collecting data from several of the other major sites into one searchable database. Because the information was formatted differently from site to site, each site required manual effort and custom programming to download and incorporate its data.\n\nAfter Hurricane Katrina displaced hundreds of thousands of people in 2005, online survivor registries again appeared on many different websites. A large volunteer effort called the Katrina PeopleFinder Project worked to gather and manually re-enter this information into one searchable database provided by Salesforce.com. An organizer of the project, David Geilhufe, put out a call for technical help to create a data standard that would enable survivor registries to aggregate and share information with each other via automated means. Working with Katrina volunteers Kieran Lal and Jonathan Plax and the CiviCRM team, Yee drafted the first specification for People Finder Interchange Format, which was released on September 4, 2005 as PFIF 1.0.\nPFIF 1.1, with some small corrections, was released on September 5. The Salesforce.com database added support for PFIF; Yahoo! and Google also launched searchable databases of Katrina survivors that exchanged information using PFIF.\n\nThe next major use of PFIF occurred after the 2010 Haiti earthquake when Google launched Google Person Finder, which used a data model based on PFIF and exchanged data with CNN, the New York Times, the National Library of Medicine, and other survivor registries using PFIF. However, PFIF 1.1 had made US-specific assumptions that were not applicable to Haiti. Released on January 26, 2010, PFIF 1.2 added fields for a person's home country and international postal code, and fields for sex, age, date of birth, status, and links between duplicate records for the same person.\n\nPFIF 1.3, released in March 2011, addressed the privacy of personal information by adding a field to specify an expiry date on each person record and setting out requirements for data deletion. PFIF 1.3 also moved away from the US-specific assumption of a first and last name by adding one field for a person's full name.\n\nPFIF 1.4, released in May 2012, renamed the name fields to \"given_name\" and \"last_name\", added a field for alternate names, added a field for linking to personal profiles on other websites, and added support for multiple photos per person.\n\nThe following websites and software projects implement PFIF:\n\n"}
{"id": "334507", "url": "https://en.wikipedia.org/wiki?curid=334507", "title": "Procedural knowledge", "text": "Procedural knowledge\n\nProcedural knowledge, also known as imperative knowledge, is the knowledge exercised in the performance of some task. See below for the specific meaning of this term in \"cognitive psychology\" and \"intellectual property\" law.\n\nIn some legal systems, such procedural knowledge has been considered the intellectual property of a company, and can be transferred when that company is purchased.\n\nOne limitation of procedural knowledge is its job-dependent nature. As a result, it tends to be less general than declarative knowledge. For example, a computer expert might have knowledge about a computer algorithm in multiple languages, or in pseudo-code, but a Visual Basic programmer might know only about a specific implementation of that algorithm, written in Visual Basic. Thus the 'hands-on' expertise and experience of the Visual Basic programmer might be of commercial value only to Microsoft job-shops, for example.\n\nOne advantage of procedural knowledge is that it can involve more senses, such as hands-on experience, practice at solving problems, understanding of the limitations of a specific solution, etc. Thus procedural knowledge can frequently eclipse theory.\n\nProcedural knowledge (i.e., knowledge-how) is different from descriptive knowledge (i.e., knowledge-that) in that it can be directly applied to a task. For instance, the procedural knowledge one uses to solve problems differs from the declarative knowledge one possesses about problem solving because this knowledge is formed by doing.\n\nThe distinction between knowing-how and knowing-that was introduced in epistemology by Gilbert Ryle.\n\nIn \"artificial intelligence\", procedural knowledge is one type of knowledge that can be possessed by an intelligent agent. Such knowledge is often represented as a partial or complete finite-state machine or computer program. A well-known example is the procedural reasoning system, which might, in the case of a mobile robot that navigates in a building, contain procedures such as \"navigate to a room\" or \"plan a path\". In contrast, an AI system based on declarative knowledge might just contain a map of the building, together with information about the basic actions that can be done by the robot (like moving forward, turning, and stopping), and leave it to a domain-independent planning algorithm to discover how to use those actions to achieve the agent's goals.\n\nIn \"cognitive psychology\", procedural knowledge is the knowledge exercised in the accomplishment of a task, and thus includes knowledge which, unlike declarative knowledge, cannot be easily articulated by the individual, since it is typically nonconscious (or tacit). Many times, the individual learns procedural knowledge without even being aware that they are learning (Stadler,1989). For example, most individuals can easily recognize a specific face as \"attractive\" or a specific joke as \"funny\", but they cannot explain how exactly they arrived at that conclusion or they cannot provide a working definition of \"attractiveness\" or being \"funny\". This example illustrates the difference between procedural knowledge and the ordinary notion of knowing how, a distinction which is acknowledged by many cognitive psychologists (Stillings, et al. Cognitive Science: An Introduction, 2nd edition, Cambridge, MA: MIT Press, 1995, p. 396). Ordinarily, we would not say that one who is able to recognize a face as attractive is one who knows how to recognize a face as attractive. One knows how to recognize faces as attractive no more than one knows how to recognize certain arrangements of leptons, quarks, etc. as tables. Recognizing faces as attractive, like recognizing certain arrangements of leptons, quarks, etc. as tables, is simply something that one does, or is able to do. It is, therefore, an instance of procedural knowledge, though it is not an instance of know-how. Of course, both forms of knowledge are, in many cases, nonconscious. For instance, research by a cognitive psychologist Pawel Lewicki has demonstrated that procedural knowledge can be acquired by nonconscious processing of information about covariations.\n\nIn the classroom, procedural knowledge is part of the prior knowledge of a student. In the context of formal education procedural knowledge is what is learned about learning strategies. It can be the \"tasks specific rules, skills, actions, and sequences of actions employed to reach goals\" a student uses in the classroom (Cauley,1986). As an example for procedural knowledge Cauley refers to how a child learns to count on their hands and/or fingers when first learning math. The Unified Learning Model explicates that procedural knowledge helps make learning more efficient by reducing the cognitive load of the task. In some educational approaches, particularly when working with students with learning disabilities, educators perform a task analysis followed by explicit instruction with the steps needed to accomplish the task.\n\nIn \"intellectual property\" law, procedural knowledge is a parcel of closely held information relating to industrial technology, sometimes also referred to as a trade secret which enables its user to derive commercial benefit from it. It is a component of the intellectual property rights on its own merits in most legislations but most often accompanies the license to the right-of-use of patents or trademarks owned by the party releasing it for circumscribed use. Procedural knowledge is not however solely composed of secret information that is not in the public domain; it is a \"bundled\" parcel of secret and related non-secret information which would be novel to an expert in the field of its usage.\n\n\n"}
{"id": "24431427", "url": "https://en.wikipedia.org/wiki?curid=24431427", "title": "Procedure (term)", "text": "Procedure (term)\n\nA procedure is a document written to support a \"policy directive\". A procedure is designed to describe who, what, where, when, and why by means of establishing corporate accountability in support of the implementation of a \"policy\". \n\nThe \"how\" is further documented by each organizational unit in the form of \"work instructions\" which aims to further support a procedure by providing greater detail.\n\nAs an example, suppose a manufacturing facility established a policy that all overtime shall be approved. A procedure can be created to establish “Who” can approve overtime (ranks, roles & responsibilities), \"What\" forms or systems must be used, \"Where\" they are located, \"When\" overtime is applicable, and \"Why\" (i.e., the management directive established via a \"policy\"). The output of a procedure is an input to a \"work instruction\" (i.e., a set of actions or operations which must be executed in the same manner in order to achieve intended results under the same circumstances. (For example, in the latter example, the output of the procedure could be further broken down into a work instruction to describe \"how\" a manager/employee should access the systems for approving/reviewing overtime (e.g., first click on this hyperlink, second press this button, third choose these fields, and fourth click approve/reject).\n\nUSAGE EXAMPLES\n\n•The insurance company provides a step-by-step guide to its dispute and complaint procedures for all policy holders to access.\n\n•The Doctor surprised me when he told me that he had used a new procedure to repair the tear in the tendon of my leg.\n\n•In a work place that has complex interactions among employees, it is important to specify and implement a procedure for core processes.\n\nIn telecommunications, a SOP (Standard Operating Procedure) is specifically designed to describe and guide multiple iterations of the same procedure over a broad number of locations, on multiple occasions, and over an open period of time until such SOP is updated or discontinued. Also used heavily in the telecommunications industry, a MOP (Method of Procedure) differs from a SOP in that it contains specific directives for a particular activity, on a particular date, for a specific location, piece of equipment, or circumstance. In today's business model, wherein telecom providers can be both \"provider\" and \"user\", most \"user\" organizations require a MOP from the service provider whenever an activity has the potential to cause a traffic-affecting outage. \n\nThe industry standard is <50ms of traffic interruption. If a \"switch hit\" or traffic interruption is 50 ms or less, it is \"transparent\" to the bit stream carrying the traffic, and is therefore considered \"hitless\" and non-traffic affecting.\n\n"}
{"id": "9240851", "url": "https://en.wikipedia.org/wiki?curid=9240851", "title": "Renaissance technology", "text": "Renaissance technology\n\nRenaissance technology is the set of European artifacts and inventions which span the Renaissance period, roughly the 14th century through the 16th century. The era is marked by profound technical advancements such as the printing press, linear perspective in drawing, patent law, double shell domes and Bastion fortresses. Sketchbooks from artisans of the period (Taccola and Leonardo da Vinci, example) give a deep insight into the mechanical technology then known and applied.\n\nRenaissance science spawned the Scientific Revolution; science and technology began a cycle of mutual advancement.\n\nSome important Renaissance technologies, including both innovations and improvements on existing techniques:\n\nSome of the technologies were the arquebus and the musket.\n\nThe technologies that developed in Europe during the second half of the 15th century were commonly associated by authorities of the time with a key theme in Renaissance thought: the rivalry of the Moderns and the Ancients. Three inventions in particular — the printing press, firearms, and the nautical compass — were indeed seen as evidence that the Moderns could not only compete with the Ancients, but had surpassed them, for these three inventions allowed modern people to communicate, exercise power, and finally travel at distances unimaginable in earlier times.\n\nCrank and connecting rod\nThe crank and connecting rod mechanism which converts circular into reciprocal motion is of utmost importance for the mechanization of work processes; it is first attested for Roman water-powered sawmills. During the Renaissance, its use is greatly diversified and mechanically refined; now connecting-rods are also applied to double compound cranks, while the flywheel is employed to get these cranks over the 'dead-spot'. Early evidence of such machines appears, among other things, in the works of the 15th-century engineers Anonymous of the Hussite Wars and Taccola. From then on, cranks and connecting rods become an integral part of machine design and are applied in ever more elaborate ways: Agostino Ramelli's \"The Diverse and Artifactitious Machines\" of 1588 depicts eighteen different applications, a number which rises in the 17th-century \"Theatrum Machinarum Novum\" by Georg Andreas Böckler to forty-five.\n\nPrinting press\nThe invention of the printing press by the German goldsmith Johannes Gutenberg (1398–1468) is widely regarded as the single most important event of the second millennium, and is one of the defining moments of the Renaissance. The Printing Revolution which it sparks throughout Europe works as a modern \"agent of change\" in the transformation of medieval society.\n\nThe mechanical device consists of a screw press modified for printing purposes which can produce 3,600 pages per workday, allowing the mass production of printed books on a proto-industrial scale. By the start of the 16th century, printing presses are operating in over 200 cities in a dozen European countries, producing more than twenty million volumes. By 1600, their output had risen tenfold to an estimated 150 to 200 million copies, while Gutenberg book printing spread from Europe further afield.\n\nThe relatively free flow of information transcends borders and induced a sharp rise in Renaissance literacy, learning and education; the circulation of (revolutionary) ideas among the rising middle classes, but also the peasants, threatens the traditional power monopoly of the ruling nobility and is a key factor in the rapid spread of the Protestant Reformation. The dawn of the Gutenberg Galaxy, the era of mass communication, is instrumental in fostering the gradual democratization of knowledge which sees for the first time modern media phenomena such as the press or bestsellers emerging. The prized incunables, which are testimony to the aesthetic taste and high proficient competence of Renaissance book printers, are one lasting legacy of the 15th century.\n\nParachute\nThe earliest known parachute design appears in an anonymous manuscript from 1470s Renaissance Italy; it depicts a free-hanging man clutching a crossbar frame attached to a conical canopy. As a safety measure, four straps run from the ends of the rods to a waist belt. Around 1485, a more advanced parachute was sketched by the polymath Leonardo da Vinci in his \"Codex Atlanticus\" (fol. 381v), which he scales in a more favorable proportion to the weight of the jumper. Leonardo's canopy was held open by a square wooden frame, altering the shape of the parachute from conical to pyramidal. The Venetian inventor Fausto Veranzio (1551–1617) modifies da Vinci's parachute sketch by keeping the square frame, but replacing the canopy with a bulging sail-like piece of cloth. This he realized decelerates the fall more effectively. Claims that Veranzio successfully tested his parachute design in 1617 by jumping from a tower in Venice cannot be substantiated; since he was around 65 years old at the time, it seems unlikely.\n\nMariner's astrolabe\n\nThe earliest recorded uses of the astrolabe for navigational purposes are by the Portuguese explorers Diogo de Azambuja (1481), Bartholomew Diaz (1487/88) and Vasco da Gama (1497/98) during their sea voyages around Africa.\n\nDry dock\n\nWhile dry docks were already known in Hellenistic shipbuilding, these facilities were reintroduced in 1495/96, when Henry VII of England ordered one to be built at the Portsmouth navy base.\n\nFloating dock\nThe earliest known description of a floating dock comes from a small Italian book printed in Venice in 1560, titled \"Descrittione dell'artifitiosa machina\". In the booklet, an unknown author asks for the privilege of using a new method for the salvaging of a grounded ship and then proceeds to describe and illustrate his approach. The included woodcut shows a ship flanked by two large floating trestles, forming a roof above the vessel. The ship is pulled in an upright position by a number of ropes attached to the superstructure.\nLifting tower\n\nA lifting tower was used to great effect by Domenico Fontana to relocate the monolithic Vatican obelisk in Rome. Its weight of 361 t was far greater than any of the blocks the Romans are known to have lifted by cranes.\nMining, machinery and chemistry\nA standard reference for the state of mechanical arts during the Renaissance is given in the mining engineering treatise \"De re metallica\" (1556), which also contains sections on geology, mining and chemistry. \"De re metallica\" was the standard chemistry reference for the next 180 years.\n\nNewspaper\nThe newspaper is an offspring of the printing press from which the press derives its name. The 16th century sees a rising demand for up-to-date information which can not be covered effectively by the circulating hand-written newssheets. For \"gaining time\" from the slow copying process, Johann Carolus of Strassburg is the first to publish his German-language \"Relation\" by using a printing press (1605). In rapid succession, further German newspapers are established in Wolfenbüttel (\"Avisa Relation oder Zeitung\"), Basel, Frankfurt and Berlin. From 1618 onwards, enterprising Dutch printers take up the practice and begin to provide the English and French market with translated news. By the mid-17th century it is estimated that political newspapers which enjoyed the widest popularity reach up to 250,000 readers in the Holy Roman Empire, around one quarter of the literate population.\n\nAir-gun\n\nIn 1607 Bartolomeo Crescentio described an air-gun equipped with a powerful spiral spring, a device so complex that it must have had predecessors. In 1610 Mersenne spoke in detail of \"sclopeti pneumatici constructio\", and four years later Wilkins wrote enthusiastically of \"that late ingenious invention the wind-gun\" as being \"almost equall to our powder-guns\". In the 1650s Otto von Guericke, famed for his experiments with vacua and pressures, built the \"Madeburger Windbuchse\", one of the technical wonders of its time.\n\nCranked Archimedes' screw\n\nThe German engineer Konrad Kyeser equips in his \"Bellifortis\" (1405) the Archimedes' screw with a crank mechanism which soon replaces the ancient practice of working the pipe by treading.\n\nCranked reel\n\nIn the textile industry, cranked reels for winding skeins of yarn were introduced in the early 15th century.\n\nBrace\n\nThe earliest carpenter's braces equipped with a U-shaped grip, that is with a compound crank, appears between 1420 and 1430 in Flanders.\n\nCranked well-hoist\n\nThe earliest evidence for the fitting of a well-hoist with cranks is found in a miniature of c. 1425 in the German \"Hausbuch of the Mendel Foundation\".\n\nPaddle wheel boat powered by crank and connecting rod mechanism\n\nWhile paddle wheel boats powered by manually turned crankshafts were already conceived of by earlier writers such as Guido da Vigevano and the Anonymous Author of the Hussite Wars, the Italian Roberto Valturio much improves on the design in 1463 by devising a boat with five sets of parallel cranks which are all joined to a single power source by one connecting rod; the idea is also taken up by his compatriot Francesco di Giorgio.\n\nRotary grindstone with treadle\n\nEvidence for rotary grindstones operated by a crank handle goes back to the Carolingian \"Utrecht Psalter\". Around 1480, the crank mechanism is further mechanized by adding a treadle.\n\nGeared hand-mill\n\nThe geared hand-mill, operated either with one or two cranks, appears in the 15th century.\n\nGrenade musket\n\nTwo 16th-century German grenade muskets working with a wheellock mechanism are on display in the Bayerisches Nationalmuseum, Munich.\n\nThe revived scientific spirit of the age can perhaps be best exemplified by the voluminous corpus of technical drawings which the artist-engineers left behind, reflecting the wide variety of interests the Renaissance Homo universalis pursued. The establishment of the laws of linear perspective by Brunelleschi gave his successors, such as Taccola, Francesco di Giorgio Martini and Leonardo da Vinci, a powerful instrument to depict mechanical devices for the first time in a realistic manner. The extant sketch books give modern historians of science invaluable insights into the standards of technology of the time. Renaissance engineers showed a strong proclivity to experimental study, drawing a variety of technical devices, many of which appeared for the first time in history on paper.\n\nHowever, these designs were not always intended to be put into practice, and often practical limitations impeded the application of the revolutionary designs. For example, da Vinci's ideas on the conical parachute or the winged flying machine were only applied much later. While earlier scholars showed a tendency to attribute inventions based on their first pictorial appearance to individual Renaissance engineers, modern scholarship is more prone to view the devices as products of a technical evolution which often went back to the Middle Ages.\n\n\n"}
{"id": "50044340", "url": "https://en.wikipedia.org/wiki?curid=50044340", "title": "Riovic", "text": "Riovic\n\nRiovic is a financial technology company that owns and operates a technology platform that enables private investors to invest in insurance risks.\n\nIn September 2015 the company officially launched a money transfer service called RiovicPay to help people access cross-border money remittance services on holidays and after hours. The service was rebranded to AfricanRemit. It then released the its on-demand platform for financial services, labeled the \"Uber of Finance\" in November 2015. The platform provided financial advisers, crowd investing and Peer-to-peer insurance.\n\nIn 2016 the company was labelled the Lloyd's of FinTech and successfully launched this revised model to eliminate insurance companies in the insurance value by directly connecting brokers and consumers with risk capital. Also labelled the \"Uber of Insurance\", Riovic pioneered \"private investor backed insurance\" where a private investor (or group of private investors) essentially steps into the financial shoes of the insurer, accepting a stream of certain cash flows in exchange for an uncertain future liability.\n\nRiovic has a collaboration with New Zealand's PeerCover and was admitted into South African Rand Merchant Investment Holdings' (RMIH) incubator for next generation financial services companies (similar to Level39), which provides the company with working space. It is also a member of Facebook's FBStart accelerator programme.\n\nIn April 2016, it got nominated as the only insurtech company in the African FinTech Awards and also go listed on the African FinTech 100 list.\n"}
{"id": "43173625", "url": "https://en.wikipedia.org/wiki?curid=43173625", "title": "Routing in cellular networks", "text": "Routing in cellular networks\n\nNetwork routing in a cellular network deals with the challenges of traditional telephony such as switching and call setup. \n\nMost cellular network routing issues in different cells can be attributed to the multiple access methods used for transmission. The location of each mobile phone must be known to reuse a given band of frequencies in different cells and forms space-division multiple access (SDMA).\n\nFDMA is one of the multiple access methods used in cellular networks. 50 MHz blocks of communication channel are assigned, which lie in radio frequency range and contain an equal number of uplinks (terminal to base station) and downlinks (base station to terminal). One or more bidirectional channels are carried by 10-90 band pairs. The digital networks additionally make use of either CDMA or TDMA methods.\n\nA special service called mobility management provides this application. Terminals (handsets) can move from one place to another during the call and therefore requires the call to be handed over from one channel to another. Soft handover uses the same frequency channel. The same terminals can operate in the same area covered by different service providers, which is known as roaming.\n\n"}
{"id": "7612776", "url": "https://en.wikipedia.org/wiki?curid=7612776", "title": "SDEP", "text": "SDEP\n\nThe SDEP (Street events Data Exchange Protocol) comprises an XML data schema and web service WSDL for exchanging information about streetworks, roadworks, and street events between systems.\n\nElgin was funded by the UK NeSDS Government e-Standards Programme to conduct a consultation and convene meetings to define the requirements of a common data exchange protocol for streetworks registers and other systems handling street events data. SDEP was developed to allow the open exchange of such data between back office systems used by local authorities to manage their highway networks in order to enable e-Government and streetworks co-ordination.\n\nThe SDEP consultation group comprised ELGIN (Chair), Mayrise Ltd., Symology Ltd., Pitney Bowes Inc., Exor Corporation (Bentley Systems), Office of the Deputy Prime Minister and Transport for London, with the National Traffic Control Centre in an observing capacity.\n\n"}
{"id": "3602041", "url": "https://en.wikipedia.org/wiki?curid=3602041", "title": "Short code", "text": "Short code\n\nShort codes, or short numbers, are short digit sequences, significantly shorter than telephone numbers, that are used to address messages in the Multimedia Messaging System (MMS) and short message service (SMS) systems of mobile network operators. In addition to messaging, they may be used in abbreviated dialing.\n\nShort codes are designed to be easier to read and remember than telephone numbers. Short codes are unique to each operator at the technological level. Even so, providers generally have agreements to avoid overlaps. In some countries, such as the United States, some classes of numbers are inter-operator (U.S. inter-operator numbers are called common short codes).\n\nShort codes are widely used for value-added services such as charity donations, mobile services, ordering ringtones, and television program voting. Messages sent to a short code can be billed at a higher rate than a standard SMS and may even subscribe a customer to a recurring monthly service that will be added to the customer's mobile phone bill until the user texts, for example, the word \"STOP\" to terminate the service.\n\nShort codes are often associated with automated services. An automated program can handle the response and typically requires the sender to start the message with a command word or prefix. The service then responds to the command appropriately.\n\nIn ads or in other printed material where a provider has to provide both a prefix and the short code number, the advertisement will typically follow this format:\n\n\n\n\n\n\n\n\n\n\n\nEthiopia\n\nCodes are four digits in length and start with 8, like 8xxx. Although the telecom sector in Ethiopia is controlled by the government, short code services are outsourced to the private sector. The short codes are used mostly for fundraising, lottery and polling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "47763930", "url": "https://en.wikipedia.org/wiki?curid=47763930", "title": "Specification for human interface for semiconductor manufacturing equipment", "text": "Specification for human interface for semiconductor manufacturing equipment\n\nThis specification is usually called SEMI E95-0200 standard. It was originally published in February 2000, and the latest technical revision is SEMI E95-1101.\n\nThis standard addresses the area of processing content with the direct intention of developing common software standards, so that problems involving operator training, operation specifications, and efficient development can be resolved more easily.\n\nSemiconductor Equipment and Materials International\n"}
{"id": "3042147", "url": "https://en.wikipedia.org/wiki?curid=3042147", "title": "Subpage", "text": "Subpage\n\nA subpage usually refers to a lower level web page in a website or wiki.\n\nThe subpage feature has been disabled in the main namespace (article namespace) of the English Wikipedia. But subpages are used in Wikiversity, where the structure permits contributions by students at any academic level, and in Wikibooks, where chapters are subpages of a book.\n\n"}
{"id": "41773", "url": "https://en.wikipedia.org/wiki?curid=41773", "title": "Systems control", "text": "Systems control\n\nSystems control, in a communications system, is the control and implementation of a set of functions that:\n\n"}
{"id": "1219401", "url": "https://en.wikipedia.org/wiki?curid=1219401", "title": "Technical communication", "text": "Technical communication\n\nTechnical communication is a means to convey scientific,knowledge engineering, and technique or other technical information. Individuals in a variety of contexts and with varied professional credentials engage in technical communication. Some individuals are designated as technical communicators or technical writers. These individuals use a set of methods to research, document, and present technical processes or products. Technical communicators may put the information they capture into paper documents, web pages, computer-based training, digitally stored text, audio, video, and other media. The Society for Technical Communication defines the field as any form of communication that focuses on technical or specialized topics, communicates specifically by using technology or provides instructions on how to do something. More succinctly, the Institute of Scientific and Technical Communicators defines technical communication as factual communication, usually about products and services. The European Association for Technical Communication briefly defines technical communication as \"the process of defining, creating and delivering information products for the safe, efficient and effective use of products (technical systems, software, services)\".\n\nWhatever the definition of technical communication, the overarching goal of the practice is to create easily accessible information for a specific audience.\n\nTechnical communicators generally tailor information to a specific audience, which may be subject matter experts, consumers, end users, etc. Technical communicators often work collaboratively to create deliverables that include online help, user manuals, classroom training guides, computer-based training, white papers, specifications, industrial videos, reference cards, data sheets, journal articles, and patents.\nTechnical domains can be of any kind, including the soft and hard sciences, high technology including computers and software and consumer electronics. Technical communicators often work with a range of specific Subject-matter experts (SMEs) on these educational projects.\n\nTechnical communication jobs include the following:\nAPI writer, e-learning author, information architect, technical content developer, technical editor, technical illustrator, technical trainer, technical translator, technical writer, usability expert, user experience designer, and user interface designer. Other jobs available to technical communicators include digital strategist, marketing specialist, and content manager.\n\nIn 2015, the European Association for Technical Communication published a competence framework for the professional field of technical communication.\n\nMuch like technology and the world economy, technical communication as a profession has evolved over the last half-century. In a nutshell, technical communicators take the physiological research of a project and apply it to the communication process itself.\n\nTechnical communication is a task performed by specialized employees or consultants. For example, a professional writer may work with a company to produce a user manual. Some companies give considerable technical communication responsibility to other technical professionals—such as programmers, engineers, and scientists. Often, a professional technical writer edits such work to bring it up to modern technical communication standards.\n\nTo begin the documentation process, technical communicators identify the audience and their information needs. The technical communicator researches and structures the content into a framework that can guide detailed development. As the body of information comes together, the technical communicator ensures that the intended audience can understand the content and retrieve the information they need. This process, known as the writing process, has been a central focus of writing theory since the 1970s, and some contemporary textbook authors apply it to technical communication. Technical communication is important to most professions, as a way to contain and organize information and maintain accuracy.\n\nThe technical writing process is based on Cicero's 5 canons of rhetoric, and can be divided into six steps:\n\n\nAll technical communication serves a particular purpose—typically to communicate ideas and concepts to an audience, or instruct an audience in a particular task. Technical communication professionals use various techniques to understand the audience and, when possible, test content on the target audience. For example, if bank workers don't properly post deposits, a technical communicator would review existing instructional material (or lack thereof), interview bank workers to identify conceptual errors, interview subject matter experts to learn the correct procedures, author new material that instructs workers in the correct procedures, and test the new material on the bank workers.\n\nSimilarly, a sales manager who wonders which of two sites is better for a new store might ask a marketing professional to study the sites and write a report with recommendations. The marketing professional hands the report off to a technical communicator (in this case, a technical editor or technical writer), who edits, formats, and sometimes elaborates the document in order to make the marketing professional's expert assessment usable to the sales manager. The process is not one of knowledge transfer, but the accommodation of knowledge across fields of expertise and contexts of use. This is the basic definition of technical communication.\n\nAudience type affects many aspects of communication, from word selection and graphics use to style and organization. Most often, to address a particular audience, a technical communicator must consider what qualities make a text useful (capable of supporting a meaningful task) and usable (capable of being used in service of that task). A non-technical audience might misunderstand or not even read a document that is heavy with jargon—while a technical audience might crave detail critical to their work. Busy audiences often don't have time to read entire documents, so content must be organized for ease of searching—for example by frequent headings, white space, and other cues that guide attention. Other requirements vary according to particular audience's needs.\n\nTechnical communication in the government is particular and detailed. Depending on the segment of government (and country), the government component must follow distinct specifications. Information changes continuously and technical communications (technical manuals, interactive electronic technical manuals, technical bulletins, etc.) must be updated.\n\nTechnical communicators must collect all information that each document requires. They may collect information through primary (first-hand) research—or secondary research, using information from existing work by other authors. Technical communicators must acknowledge all sources they use to produce their work. To this end, technical communicators typically distinguish quotations, paraphrases, and summaries when taking notes.\n\nBefore writing the initial draft, the technical communicator organizes ideas in a way that makes the document flow well. Once each idea is organized, the writer organizes the document as a whole—accomplishing this task in various ways: chronological: used for documents that involve a linear process, such as a step-by-step guide that describes how to accomplish something; parts of an object: Used for documents that describe the parts of an object, such as a graphic showing the parts of a computer (keyboard, monitor, mouse, etc.); simple to complex (or vice versa): starts with easy ideas and gradually goes into complex ideas; specific to general: starts with many ideas, then organizes the ideas into sub-categories; general to specific: starts with a few categories of ideas, then goes deeper\n\nAfter organizing the whole document, the writer typically creates a final outline that shows the document structure. Outlines make the writing process easier and save the author time.\n\nAfter the outline is complete, the writer begins the first draft, following the outline's structure. Setting aside blocks of an hour or more, in a place free of distractions, helps the writer maintain a flow. Most writers prefer to wait until the draft is complete before any revising so they don't break their flow. Typically, the writer should start with the easiest section, and write the summary only after the body is drafted.\n\nThe ABC (\"a\"bstract, \"b\"ody, and \"c\"onclusion) format can be used when writing a first draft of some document types. The abstract describes the subject, so that the reader knows what the document covers. The body is the majority of the document and covers topics in depth. Lastly, the conclusion section restates the document's main topics. The ABC format can also apply to individual paragraphs—beginning with a topic sentence that states the paragraph's topic, followed by the topic, and finally, a concluding sentence.\n\nOnce the initial draft is laid out, editing and revising can be done to fine-tune the draft into a final copy. Four tasks transform the early draft into its final form, suggested by Pfeiffer and Boogard:\n\nIn this step, the writer revises the draft to elaborate on topics that need more attention, shorten other sections—and relocate certain paragraphs, sentences, or entire topics.\n\nGood style makes writing more interesting, appealing, and readable. In general, the personal writing style of the writer is not evident in technical writing. Modern technical writing style relies on attributes that contribute to clarity: headings, lists, graphics; generous white space, short sentences, present tense, simple nouns, active voice (though some scientific applications still use the passive voice), second and third person as required\n\nTechnical writing as a discipline usually requires that a technical writer use a style guide. These guides may relate to a specific project, product, company, or brand. They ensure that technical writing reflects formatting, punctuation, and general stylistic standards that the audience expects. In the United States, many consider the \"Chicago Manual of Style\" the bible for general technical communication. Other style guides have their adherents, particularly for specific industries—such as the \"Microsoft Style Guide\" in some information technology settings.\n\nAt this point, the writer performs a \"mechanical edit\", checking the document for grammar, punctuation, common word confusions, passive voice, overly long sentences, etc.\n\n\n"}
{"id": "1544179", "url": "https://en.wikipedia.org/wiki?curid=1544179", "title": "Technogaianism", "text": "Technogaianism\n\nTechnogaianism (a portmanteau word combining \"techno-\" for technology and \"gaian\" for Gaia philosophy) is a bright green environmentalist stance of active support for the research, development and use of emerging and future technologies to help restore Earth's environment. Technogaians argue that developing safe, clean, alternative technology should be an important goal of environmentalists.\n\nThis point of view is different from the default position of radical environmentalists and a common opinion that all technology necessarily degrades the environment, and that environmental restoration can therefore occur only with reduced reliance on technology. Technogaians argue that technology gets cleaner and more efficient with time. They would also point to such things as hydrogen fuel cells to demonstrate that developments do not have to come at the environment's expense. More directly, they argue that such things as nanotechnology and biotechnology can directly reverse environmental degradation. Molecular nanotechnology, for example, could convert garbage in landfills into useful materials and products, while biotechnology could lead to novel microbes that devour hazardous waste.\n\nWhile many environmentalists still contend that \"most\" technology is detrimental to the environment, technogaians point out that it has been in humanity's best interests to exploit the environment mercilessly until fairly recently. This sort of behaviour follows accurately to current understandings of evolutionary systems, in that when new factors (such as foreign species or mutant subspecies) are introduced into an ecosystem, they tend to maximise their own resource consumption until either, \"a)\" they reach an equilibrium beyond which they cannot continue unmitigated growth, or \"b)\" they become extinct. In these models, it is \"completely impossible\" for such a factor to totally destroy its host environment, though they may precipitate major ecological transformation before their ultimate eradication. Technogaians believe humanity has currently reached just such a threshold, and that the only way for human civilization to continue advancing is to accept the tenets of technogaianism and limit future exploitive exhaustion of natural resources and minimize further unsustainable development or face the widespread, ongoing mass extinction of species. The destructive effects of modern civilization can be mitigated by technological solutions, such as using nuclear power. Furthermore, technogaians argue that only science and technology can help humanity be aware of, and possibly develop counter-measures for, risks to civilization, humans and planet Earth such as a possible impact event. \n\nSociologist James Hughes mentions Walter Truett Anderson, author of \"To Govern Evolution: Further Adventures of the Political Animal\", as an example of a technogaian political philosopher; argues that technogaianism applied to environmental management is found in the reconciliation ecology writings such as Michael Rosenzweig's \"Win-Win Ecology: How The Earth's Species Can Survive In The Midst of Human Enterprise\"; and considers Bruce Sterling's Viridian design movement to be an exemplary technogaian initiative.\n\nThe theories of English writer Fraser Clark may be broadly categorised as technogaian. Clark advocated \"balancing the hippie right brain with the techno left brain\". The idea of combining technology and ecology were extrapolated at length by a South African eco-anarchist project in the 1990s. The Kagenna Magazine project aimed to combine technology, art and ecology in an emerging movement that could restore the balance between human and nature.\n\nGeorge Dvorsky suggests the sentiment of technogaianism is to heal the Earth, use sustainable technology, and create ecologically diverse environments. Dvorsky argues that defensive counter measures could be designed to counter the harmful effects of asteroid impacts, earthquakes, and volcanic eruptions. Dvorksky also suggest that genetic engineering could be used to reduce the environmental impact humans have on the earth.\n\nTechnology facilities the sampling, testing and monitoring of various environments and ecosystems. NASA uses space-based observations to conduct research on solar activity, sea level rise, the temperature of the atmosphere and the oceans, the state of the ozone layer, air pollution, and changes in sea ice and land ice.\n\nClimate engineering is a technogaian method that uses two categories of technologies- carbon dioxide removal and solar radiation management. Carbon dioxide removal addresses a cause of climate change by removing one of the greenhouse gases from the atmosphere. Solar radiation management attempts to offset effects of greenhouse gases by causing the Earth to absorb less solar radiation.\n\nEarthquake engineering is a technogaian method concerned with protecting society and the natural and man-made environment from earthquakes by limiting the seismic risk to acceptable levels.\nAnother example of a technogaian practice is an artificial closed ecological system used to test if and how people could live and work in a closed biosphere, while carrying out scientific experiments. It is in some cases used to explore the possible use of closed biospheres in space colonization, and also allows the study and manipulation of a biosphere without harming Earth's. The most advanced technogaian proposal is the \"terraforming\" of a planet, moon, or other body by deliberately modifying its atmosphere, temperature, or ecology to be similar to those of Earth in order to make it habitable by humans.\n\nS. Matthew Liao, professor of philosophy and bioethics at New York University, claims that the human impact on the environment could be reduced by genetically engineering humans to have, a smaller stature, an intolerance to eating meat, and an increased ability to see in the dark, thereby using less lighting. Liao argues that human engineering is less risky than geoengineering.\n\nGenetically modified foods have reduced the amount of herbicide and insecticide needed for cultivation. The development of glyphosate-resistant (Roundup Ready) plants has changed the herbicide use profile away from more environmentally persistent herbicides with higher toxicity, such as atrazine, metribuzin and alachlor, and reduced the volume and danger of herbicide runoff.\n\nAn environmental benefit of Bt-cotton and maize is reduced use of chemical insecticides. A PG Economics study concluded that global pesticide use was reduced by 286,000 tons in 2006, decreasing the environmental impact of herbicides and pesticides by 15%. A survey of small Indian farms between 2002 and 2008 concluded that Bt cotton adoption had led to higher yields and lower pesticide use. Another study concluded insecticide use on cotton and corn during the years 1996 to 2005 fell by of active ingredient, which is roughly equal to the annual amount applied in the EU. A Bt cotton study in six northern Chinese provinces from 1990 to 2010 concluded that it halved the use of pesticides and doubled the level of ladybirds, lacewings and spiders and extended environmental benefits to neighbouring crops of maize, peanuts and soybeans.\n\n"}
{"id": "361239", "url": "https://en.wikipedia.org/wiki?curid=361239", "title": "Technological utopianism", "text": "Technological utopianism\n\nTechnological utopianism (often called techno-utopianism or technoutopianism) is any ideology based on the premise that advances in science and technology could and should bring about a utopia, or at least help to fulfill one or another utopian ideal.\n\nA techno-utopia is therefore an ideal society, in which laws, government, and social conditions are solely operating for the benefit and well-being of all its citizens, set in the near- or far-future, as advanced science and technology will allow these ideal living standards to exist; for example, post-scarcity, transformations in human nature, the avoidance or prevention of suffering and even the end of death.\n\nTechnological utopianism is often connected with other discourses presenting technologies as agents of social and cultural change, such as technological determinism or media imaginaries.\n\nDouglas Rushkoff, a leading theorist on technology and cyberculture claims that technology gives everyone a chance to voice their own opinions, fosters individualistic thinking, and dilutes hierarchy and power structures by giving the power to the people. He says that the whole world is in the middle of a new Renaissance, one that is centered on technology and self-expression. However, Rushkoff makes it clear that “people don’t live their lives behind a desk with their hands on a keyboard” \n\nA tech-utopia does not disregard any problems that technology may cause, but strongly believes that technology allows mankind to make social, economic, political, and cultural advancements. Overall, Technological Utopianism views technology’s impacts as extremely positive.\n\nIn the late 20th and early 21st centuries, several ideologies and movements, such as the cyberdelic counterculture, the Californian Ideology, transhumanism, and singularitarianism, have emerged promoting a form of techno-utopia as a reachable goal. Cultural critic Imre Szeman argues technological utopianism is an irrational social narrative because there is no evidence to support it. He concludes that it shows the extent to which modern societies place faith in narratives of progress and technology overcoming things, despite all evidence to the contrary.\n\nKarl Marx believed that science and democracy were the right and left hands of what he called the move from the realm of necessity to the realm of freedom. He argued that advances in science helped delegitimize the rule of kings and the power of the Christian Church.\n\n19th-century liberals, socialists, and republicans often embraced techno-utopianism. Radicals like Joseph Priestley pursued scientific investigation while advocating democracy. Robert Owen, Charles Fourier and Henri de Saint-Simon in the early 19th century inspired communalists with their visions of a future scientific and technological evolution of humanity using reason. Radicals seized on Darwinian evolution to validate the idea of social progress. Edward Bellamy’s socialist utopia in \"Looking Backward\", which inspired hundreds of socialist clubs in the late 19th century United States and a national political party, was as highly technological as Bellamy’s imagination. For Bellamy and the Fabian Socialists, socialism was to be brought about as a painless corollary of industrial development.\n\nMarx and Engels saw more pain and conflict involved, but agreed about the inevitable end. Marxists argued that the advance of technology laid the groundwork not only for the creation of a new society, with different property relations, but also for the emergence of new human beings reconnected to nature and themselves. At the top of the agenda for empowered proletarians was \"to increase the total productive forces as rapidly as possible\". The 19th and early 20th century Left, from social democrats to communists, were focused on industrialization, economic development and the promotion of reason, science, and the idea of progress.\n\nSome technological utopians promoted eugenics. Holding that in studies of families, such as the Jukes and Kallikaks, science had proven that many traits such as criminality and alcoholism were hereditary, many advocated the sterilization of those displaying negative traits. Forcible sterilization programs were implemented in several states in the United States.\n\nH.G. Wells in works such as \"The Shape of Things to Come\" promoted technological utopianism.\n\nThe horrors of the 20th century – namely fascist dictatorships and the world wars – caused many to abandon optimism. The Holocaust, as Theodor Adorno underlined, seemed to shatter the ideal of Condorcet and other thinkers of the Enlightenment, which commonly equated scientific progress with social progress.\n\nA movement of techno-utopianism began to flourish again in the dot-com culture of the 1990s, particularly in the West Coast of the United States, especially based around Silicon Valley. The Californian Ideology was a set of beliefs combining bohemian and anti-authoritarian attitudes from the counterculture of the 1960s with techno-utopianism and support for libertarian economic policies. It was reflected in, reported on, and even actively promoted in the pages of \"Wired\" magazine, which was founded in San Francisco in 1993 and served for a number years as the \"bible\" of its adherents.\n\nThis form of techno-utopianism reflected a belief that technological change revolutionizes human affairs, and that digital technology in particular – of which the Internet was but a modest harbinger – would increase personal freedom by freeing the individual from the rigid embrace of bureaucratic big government. \"Self-empowered knowledge workers\" would render traditional hierarchies redundant; digital communications would allow them to escape the modern city, an \"obsolete remnant of the industrial age\".\n\nSimilar forms of \"digital utopianism\" has often entered in the political messages of party and social movements that point to the Web or more broadly to new media as harbingers of political and social change. Its adherents claim it transcended conventional \"right/left\" distinctions in politics by rendering politics obsolete. However, techno-utopianism disproportionately attracted adherents from the libertarian right end of the political spectrum. Therefore, techno-utopians often have a hostility toward government regulation and a belief in the superiority of the free market system. Prominent \"oracles\" of techno-utopianism included George Gilder and Kevin Kelly, an editor of \"Wired\" who also published several books.\n\nDuring the late 1990s dot-com boom, when the speculative bubble gave rise to claims that an era of \"permanent prosperity\" had arrived, techno-utopianism flourished, typically among the small percentage of the population who were employees of Internet startups and/or owned large quantities of high-tech stocks. With the subsequent crash, many of these dot-com techno-utopians had to rein in some of their beliefs in the face of the clear return of traditional economic reality.\n\nIn the late 1990s and especially during the first decade of the 21st century, technorealism and techno-progressivism are stances that have risen among advocates of technological change as critical alternatives to techno-utopianism. However, technological utopianism persists in the 21st century as a result of new technological developments and their impact on society. For example, several technical journalists and social commentators, such as Mark Pesce, have interpreted the WikiLeaks phenomenon and the United States diplomatic cables leak in early December 2010 as a precursor to, or an incentive for, the creation of a techno-utopian transparent society. Cyber-utopianism, first coined by Evgeny Morozov, is another manifestation of this, in particular in relation to the Internet and social networking.\n\nBernard Gendron, a professor of philosophy at the University of Wisconsin–Milwaukee, defines the four principles of modern technological utopians in the late 20th and early 21st centuries as follows:\n\n\nRushkoff presents us with multiple claims that surround the basic principles of Technological Utopianism:\n\n\nCritics claim that techno-utopianism's identification of social progress with scientific progress is a form of positivism and scientism. Critics of modern libertarian techno-utopianism point out that it tends to focus on \"government interference\" while dismissing the positive effects of the regulation of business. They also point out that it has little to say about the environmental impact of technology and that its ideas have little relevance for much of the rest of the world that are still relatively quite poor (see global digital divide).\n\nIn his 2010 study \"System Failure: Oil, Futurity, and the Anticipation of Disaster\", Canada Research Chairholder in cultural studies Imre Szeman argues that technological utopianism is one of the social narratives that prevent people from acting on the knowledge they have concerning the effects of oil on the environment.\n\nIn a controversial article \"Techno-Utopians are Mugged by Reality\", Wall Street Journal explores the concept of the violation of free speech by shutting down social media to stop violence. As a result of British cities being looted consecutively, Prime British Minister David Cameron argued that the government should have the ability to shut down social media during crime sprees so that the situation could be contained. A poll was conducted to see if Twitter users would prefer to let the service be closed temporarily or keep it open so they can chat about the famous television show X-Factor. The end report showed that every Tweet opted for X-Factor. The negative social effects of technological utopia is that society is so addicted to technology that we simply can't be parted even for the greater good. While many Techno-Utopians would like to believe that digital technology is for the greater good, it can also be used negatively to bring harm to the public.\n\nOther critics of a techno-utopia include the worry of the human element. Critics suggest that a techno-utopia may lessen human contact, leading to a distant society. Another concern is the amount of reliance society may place on their technologies in these techno-utopia settings. These criticisms are sometimes referred to as a technological anti-utopian view or a techno-dystopia.\n\nEven today, the negative social effects of a technological utopia can be seen. Mediated communication such as phone calls, instant messaging and text messaging are steps towards a utopian world in which one can easily contact another regardless of time or location. However, mediated communication removes many aspects that are helpful in transferring messages. As it stands today, most text, email, and instant messages offer fewer nonverbal cues about the speaker’s feelings than do face-to-face encounters. This makes it so that mediated communication can easily be misconstrued and the intended message is not properly conveyed. With the absence of tone, body language, and environmental context, the chance of a misunderstanding is much higher, rendering the communication ineffective. In fact, mediated technology can be seen from a dystopian view because it can be detrimental to effective interpersonal communication. These criticisms would only apply to messages that are prone to misinterpretation as not every text based communication requires contextual cues. The limitations of lacking tone and body language in text based communication are likely to be mitigated by video and augmented reality versions of digital communication technologies.\n\n\n"}
{"id": "1714828", "url": "https://en.wikipedia.org/wiki?curid=1714828", "title": "Theory of operation", "text": "Theory of operation\n\nA theory of operation is a description of how a device or system should work. It is often included in documentation, especially maintenance/service documentation, or a user manual. It aids troubleshooting by providing the troubleshooter with a mental model of how the system is supposed to work. The troubleshooter can then more easily identify discrepancies, to aid diagnosis of problem.\n\nIBM Redbooks are \"Theories of operation\" of their products.\n\n"}
{"id": "27417475", "url": "https://en.wikipedia.org/wiki?curid=27417475", "title": "Timeline of Russian innovation", "text": "Timeline of Russian innovation\n\nTimeline of Russian Innovation encompasses key events in the history of technology in Russia, starting from the Early East Slavs and up to the Russian Federation.\n\nThe entries in this timeline fall into the following categories:\n\n\nThis timeline examines scientific and medical discoveries, products and technologies introduced by various peoples of Russia and its predecessor states, regardless of ethnicity, and also lists inventions by naturalized immigrant citizens. Certain innovations achieved by a national operation may also may be included in this timeline, in cases where the Russian side played a major role in such projects.\n\n\n\n\n\n\n\nLapta\n\nZvonnitsa\n\nAnbur script The alphabet was introduced by a Russian missionary, Stepan Khrap, also known as Saint Stephen of Perm (Степан Храп, св. Стефан Пермский) in 1372. The name Abur is derived from the names of the first two characters: An and Bur. The alphabet derived from Cyrillic and Greek, and Komi tribal signs, the latter being similar in the appearance to runes or siglas poveiras, because they were created by incisions, rather than by usual writing. The alphabet was in use until the 17th century, when it was superseded by the Cyrillic script. Abur was also used as cryptographic writing for the Russian language.\n\n1376 Sarafan\n\nBardiche\nBoyar hat\n\nGulyay-gorod\n\nUkha\n\nRussian oven\n\n\nRassolnik\nc. 1430 Russian vodka\n\nKokoshnik (architecture)\n1510s Tented roof masonry\n\n1530 Middle Muscovite\n\nRussian abacus\n\n1550 Streltsy\n1552 Battery-tower\n1561 \"Saint Basil's Cathedral\" \n\n1566 \"Great Abatis Line\" \n1586 \"Tsar Cannon\" \n\nBochka roof\nGorodki\n\nRoller coaster\nBird of Happiness\n\nDymkovo toy\n\nTroika\n\n1630 Late Muscovite Russian architecture characterized by many large cathedral-type churches with five onion-like cupolas, surrounding them with tents of bell towers and aisles.\n\n1659 Khokhloma\n\n1679 Circle of fifths\n1685 Tula pryanik\n\n1688 Balalaika\n\nGlass-holder \n\n1693 \n\nTable-glass\n1704 Decimal currency\n\n1717 Metal lathe compound slide\n\n1718 Yacht club\n\n\n1725 Rebar\n\n1732 Cast iron cupola / Lightning rod \n\n1733 \"Peter and Paul Cathedral\" \n1735 \"Tsar Bell\" \n1739 Ice palace\n\n1741 Quick-firing gun\n\n\n1754 Coaxial rotor / Model helicopter\n 1756 Law of Mass Conservation\n\n1757 Licorne (Russian field gun)\n\n1761 Atmosphere of Venus\n\n1762 Off-axis reflecting telescope\n\n1770 \"Amber Room\" \n\n1770 \"Thunder Stone\" \n\n1776 Orenburg shawl\n\n1778 Russian samovar\n\n1784 Orlov Trotter\nRussian guitar\n\nValenki\n\n1793 Screw drive elevator\n\n1795 Fedoskino miniature / Russian lacquer art\n\n1796 Peaked cap\n\n1802 Modern powdered milk\n\n1802 Continuous electric arc\n\n1805 Droshky any of various 2 or 4 wheeled, horse-drawn, public carriages (early taxicabs).\n1811 Sailor cap\n\n1812 Electric telegraph\n\n1812 Naval mine\n\n1814 Beehive frame\n1820 Antarctica\n\n1820s Russian Revival architecture is the generic term for a number of different movements within Russian architecture that arose in second quarter of the 19th century and was an eclectic melding of pre-Peterine Russian architecture and elements of Byzantine architecture.\n\n1820 Monorail\n\n1825 Zhostovo painting\n\n1828 Electromagnetic telegraph\n\n1829 Industrial production process of sunflower oil\n\n1829 Three bolt diving equipment\n\n1832 Data recording equipment\n\n1833 Lenz's law\n\n1835 Centrifugal fan\n\n1838 Electrotyping\n\n1839 Electric boat\n\n1839 Galvanoplastic sculpture\n1847 Field anesthesia\n\n1848 Modern oil well\n1850s Neo-Byzantine architecture in the Russian Empire emerged in the 1850s and became an officially endorsed preferred architectural style for church construction during the reign of Alexander II of Russia (1855–1881), replacing the Russo-Byzantine style of Konstantin Thon.\n\n1851 \"Struve Geodetic Arc\" \n\n1851 Russian Railway Troops\n\n1854 Modern field surgery\n\n1854 Stereo camera\n\n1857-1861 Theory of chemical structure\n\n1857 Radiator\n\n1858 \"Saint Isaac's Cathedral\" \n\n1859 Aluminothermy\n1860s Russian salad\n\n1861 Beef Stroganoff\n\n1864 Modern icebreaker\n\n1868 Grow light\n\n1869 Hectograph\n\n1869 Periodic table of the elements\n\nGymnasterka\n\n1872 Electric lamp\n\n1872 Aldol reaction\n\n1873 Odhner Arithmometer\n\n1873 Armored cruiser\n\n1874 Headlamp\n\n1875 Railway electrification system\n\n1876 AC transformer\n\n1876 Yablochkov candle\n\n1877 Torpedo boat tender\n\n1877 Tracked wagon\n\n1878 Cylindrical oil tank\n\n1879 Modern oil tanker\n\n1880s Winogradsky column\n\n1888s Three-phase electric power\n\n1880 Vitamins\n\n1880 Electric tram\n\n1881 Carbon arc welding\n\n1883 \"Cathedral of Christ the Saviour\" \n\n1884 \"Electric submarine\" \n\n1888 Caterpillar farm tractor\n\n1888 Shielded metal arc welding\n\n1888 Solar cell (based on the outer photoelectric effect)\n\n1889 Three-phase induction motor\n\n1889 Three-phase transformer\n\n1889 \"Mosin–Nagant rifle\" \n1890 Matryoshka doll\n\n1890 Powered exoskeleton\n\n1890 Chemosynthesis\n\n1891 Thermal chemical cracking\n\n1891 Long-distance transmission of three-phase electric power\n1891 Three-phase hydroelectric power plant\n\n1892 Viruses\n\n1894 Nephoscope\n\n1895 Lightning detector / Radio receiver\n\n1896 Thin-shell structure\n\n1896 Tensile structure\n\n1896 Hyperboloid structure\n\n1897 Gridshell\n\n1898 Polar icebreaker\n\n1899 Radiation pressure\n\nMstyora miniature\n\n1901 Classical conditioning\n\n1901 Chromatography\n\n1902 Fire fighting foam\n\n1903 Theoretical foundations of spaceflight\n\n1903 Cytoskeleton\n\n1903 Motor ship\n\n1904 Radio jamming\n\n1904 Foam extinguisher\n\n1905 Auscultatory blood pressure measurement\n\n1905 Korotkov sounds\n\n1905 Insubmersibility\n\n1906 Electric seismometer\n\n1907 Aerosani / Snowmobile\n\n1907 Pulsejet\n\n1907 Bayan\n\n1907 \"Church of the Savior on Blood\" \n1910 Polybutadiene\n\n1910 Montage (filmmaking) or Kuleshov Effect (by Lev Kuleshov)\n\n1910 Non-Aristotelian logic\nBy Nikolai Vasilyev\n\n1911 Knapsack parachute\n\n1911 Television\n\n1911 Stanislavski's system \n\n1913 Zaum\n\n1913 Airliner\n\n1913 Half-track\n\n1914 Aerobatics\n\n1914 Gyrocar\n\n1914 Tachanka\n\n1914 Strategic bomber\n\n1914 Aerial ramming\n\n1915 Activated charcoal gas mask\n\n1915 Vezdekhod\n\n1915 \"Tsar Tank\" \n\n1916 \"Trans-Siberian Railway\" \n\n1916 Optophonic piano\n\n1917 Socialist realism\n\n1918 Air ioniser\n\n1918 Budenovka\n\n1918 Ushanka\n\n1918 Jet pack (not built)\n\n1919 Film school\n\n1919 Theremin\n\n1919 Constructivism (art)\n\n1920s Constructivist architecture\n\n1921 Aerial refueling\n\n1923 Iconoscope\n\n1923 Palekh miniature\n\n1924 Flying wing\n\n1924 Optophonic Piano\n\n1924 Stem cells\n\n1924 Primordial soup hypothesis\n\n1925 Interlaced video\n\n1926 Graphical sound\n\n1927 Light-emitting diode\n\n1927 \"Polikarpov Po-2 biplane\" \n\n1928 Gene pool\n\n1928 Rabbage\n\n1929 Cadaveric blood transfusion\n\n1929 Kinescope\n\n1929 Pobedit\n\n1929 Teletank / Military robot\nSpring-loaded camming device\n\nAbalakov thread climbing device\n\nElectric rocket motor\n\n1930s Modern ship hull design\n\n1930 Blood bank\n\n1930 Single lift-rotor helicopter\n\n1930 Paratrooping\n\n1931 Pressure suit\n\n1931 Hypergolic rocket propellants\n\n1931 Rhythmicon / Drum machine\n\n1931 Flame tank\n\n1932 Postconstructivism\n\n1932 Postal code\n\n1932 Children's railway\n\n1932 Terpsitone\n\n1932 Underwater welding\n\n1933 Human kidney transplant\n\n1933 Sampling theorem\n\n1933 Tandem rotor helicopter\n\n1933 Stalinist architecture\n\n1934 \"Tupolev ANT-20\" \n\n1934 Cherenkov detector\n\n1935 Kirza\n\n1935 \"Moscow Metro\" \n\n1935 Kremlin stars\n\n1936 Acoustic microscopy\n\n1936 Airborne firefighting\n\n1937 Artificial heart\n\n1937 Modern evolutionary synthesis\n\n1937 Superfluidity\n\n1937 Drag chute\n\n1937 Manned drifting ice station\n\n1937 Welded sculpture\n\n1937 Fire-fighting sport\n\n1938 Deep column station\n\n1938 Sambo\n\n1939 Kirlian photography\n\n1939 Vought-Sikorsky VS-300\n\n1939 \"Ilyushin Il-2\" \n\n1939 Self-propelled multiple rocket launcher\n1940s Ballast cleaner\n\n1940s TRIZ\n\n1940s Sikorsky R-4\n\n1940 \"T-34 tank\" \n\n1941 Competitive rhythmic gymnastics\n\n1941 Maksutov telescope\n\n1941 Degaussing\n\n1942 Winged tank\n\n1942 Gramicidin S\n\n1944 Microtron\n\n1944 EPR spectroscopy\n\n1945 \"T-54/55 tank\" \n\n1945 Passive resonant cavity bug\n\n1946 Heart-lung transplant\n\n1947 Modern multistage rocket \n\n1947 \"MiG-15\" \n\n1947 \"AK-47\" \n\n1947 Lung transplant\n\n1947 Light beam microphone\n\n1949 \"Staged combustion cycle\" \n\n1949 \"Reactive armour\" \n1950s Head transplant\n\n1950s Magnetotellurics\n\n1950 MESM\n\n1950 Berkovich tip\n\n1951 Belousov–Zhabotinsky reaction\n\n1951 Explosively pumped flux compression generator\n\n1952 Masers\n\n1952 Seven Sisters (Moscow)\n\n1952 Carbon nanotubes\n\n1952 Anthropometric cosmetology or Ilizarov apparatus\n\n1954 Nuclear power plant\n\n1955 \"MiG-21\" \n\n1955 Ballistic missile submarine\n\n1955 Fast-neutron reactor\n\n1955 \"Leningrad Metro\" \n\n1955 Tokamak\n\n1957 ANS synthesizer\n\n1957 Synchrophasotron\n\n1957 Spaceport\n\n1957 Intercontinental ballistic missile\n\n1957 Orbital space rocket\n\n1957 Artificial satellite\n\n1957 Space capsule\n\n1957 \"Raketa hydrofoil\" \n\n1958 Modern ternary computer\n\n1959 Nuclear icebreaker\n\n1959 Space probe\n\n1959 Missile boat\n\n1959 Kleemenko cycle\n\n1959 Staged combustion cycle\n1960s Rocket boots\n\n1960 Reentry capsule\n\n1961 Human spaceflight\n\n1961 \"RPG-7\" \n\n1961 Lawrencium\n\n1961 Anti-ballistic missile\n\n1961 Space food\n\n1961 Space suit\n\n1961 \"Tsar Bomb\" \n\n1961 Platform screen doors\n\n1961 Ekranoplan\n\n1961 \"Mil Mi-8\" \n\n1962 Detonation nanodiamond\n\n1962 AVL tree datastructure\n\n1962 3D holography\n\n1962 Modern stealth technology\n\n1963 Oxygen cocktail\n\n1964 Rutherfordium\n\n1964 \"Druzhba pipeline\" \n\n1964 Plasma propulsion engine\n\n1964 Kardashyov scale\n\n1965 Extra-vehicular activity\n\n1965 Molniya orbit satellite\n\n1965 Voitenko compressor\n\n1965 \"Proton rocket\" \n\n1965 Air-augmented rocket\n\n1966 Nobelium\n\n1966 Lander spacecraft\n\n1966 Orbiter\n\n1966 Regional jet\n\n1966 Caspian Sea Monster\n\n1966 \"Soyuz rocket\" \n\n1966 Orbital module\n\n1967 Space toilet\n\n1967 \"Ostankino Tower\" \n\n1967 \"The Motherland Calls\" \n\n1967 Computer for operations with functions\n\n1967 Automated space docking\n\n1967 Venus lander\n\n1968 Dubnium\n\n1968 \"Mil Mi-12\" \n\n1968 Supersonic transport\n\n1969 Comet 67P/Churyumov–Gerasimenko\n\n1969 Intercontinental Submarine-launched ballistic missile\n1970s Semiconductor Heterostructures\n\n1970s Radial keratotomy\n\n1970 Excimer laser\n\n1970 Robotic sample return\n\n1970 Space rover\n\n1971 Space station\n\n1971 Kaissa (chess program)\n\n1972 Hall effect thruster\n\n1972 Mil Mi-24\n\n1972 Nuclear desalination\n\n1973 Reflectron\n\n1973 Skull crucible\n\n1974 Electron cooling\n\n1975 Underwater assault rifle\n\n1975 \"Arktika class icebreaker\" \n\n1975 Androgynous Peripheral Attach System\n\n1976 Close-in weapon system\n\n1976 Mobile ICBM\n\n1977 Vertical launching system\n\n1977 \"Kirov class battlecruiser\" \n\n1978 Unmanned resupply spacecraft\n\n1978 Active protection system\n\n1979 Space-based radio telescope\nKalina cycle \n\n1980s EHF therapy\n\n1980 \"Typhoon class submarine\" \n\n1981 Quantum dot\n\n1981 \"Tupolev Tu-160\" \n\n1982 Helicopter ejection seat\n\n1984 Tetris\n\n1986 Modular space station\n\n1987 \"MIR submersible\" \n\n1987 \"RD-170 rocket engine\" \n\n1988 \"Buran\" \n\n1988 \"An-225\" \n\n1989 \"Kola Superdeep Borehole\" \n\n1989 Supermaneuverability\n\n1989 \"Tupolev Tu-155\"\n1989-1991 BARS apparatus\n\n1991 Thermoplan\n\n1991 Scramjet\n\nRD-180 Engine\n\n1992 Znamya (space mirror)\n\n1992 Nuclotron\n\n1993 RAR\n\n1997 Two-level single-vault transfer station\n\n1998 \"Beriev Be-200\" \n\n1998 Submarine-launched spacecraft\n\n1999 7z\n\n1999 Sea Launch\n\n1999 Flerovium\n2000s Heterotransistor \n\n2000 Livermorium\n\n2000 Abstract state machine\n\n2001 Space tourism\n\n2001 \"Mirny Mine\" \n\n2001 Superconducting nanowire single-photon detector\n\n2003 \"Park Pobedy metro escalators\" \n\n2003 Nihonium\n\n2003 Moscovium\n\n2004 Graphene\n\n2005 Orbitrap\n\n2006 Oganesson\n\n2007 \"NS 50 Let Pobedy\" \n\n2007 \"Father of all bombs\" \n\n2008 Denisovans\n\n2010 \"Chatroulette\" \n\n2010 \"Tennessine\" \n\n2011 \"Nuclear power station barge\" \n\n2011 \"Nord Stream\" \n\n2011 \"Spektr-R\" \n2012 \"Russky Island Bridge\" \n\n2016 \"T-14 Armata\" \n\n"}
{"id": "1850361", "url": "https://en.wikipedia.org/wiki?curid=1850361", "title": "User assistance", "text": "User assistance\n\nUser assistance is a general term for guided assistance to a user of a software product. The phrase incorporates all forms of help available to a user. Assistance can also automatically perform procedures or step users through the procedure, depending on the question that the user asked. The term is broader than online help, and includes procedural and tutorial information.\n\nUser assistance provides information to help a person to interact with software. This can include describing the user interface, but also focuses on how to help the user to best apply the software capabilities to their needs. User assistance can be considered a component of the broader category of user experience.\n\nUser assistance employs a number of devices including help, wizards, tutorials, printed manuals (and their PDF equivalents), and user interface text. User assistance professionals also contribute to enterprise knowledge bases and content management systems.\n\nEffective user assistance development requires a variety of communication skills. These include writing, editing, task analysis, and subject matter expert (SME) interviewing. Since the user assistance profession is directly involved with software development, the discipline often requires an understanding of UI design, usability testing, localization, testing, quality assurance, instructional design, scripting or programming, and accessibility.\n\n\"For information related to this topic, see Instruction manual (computer and video games)\"\n\nA traditional form of user assistance is a user manual, which is distributed either with the product in paper form or electronically. Typical features of a user manual include installation procedures, a guide to how to use the software, as well as a disclaimer stating the licensing status of the software. Details of a helpline may also be available.\n\n\"For more information on this topic, see Online help\"\n\n\"For information related to this topic, see Helpline\"\n\n\n"}
{"id": "36451156", "url": "https://en.wikipedia.org/wiki?curid=36451156", "title": "VocaLink", "text": "VocaLink\n\nVocalink is a payment systems company headquartered in the United Kingdom, created in 2007 from the merger between Voca and LINK. It designs, builds and operates the UK payments infrastructure, which underpins the provision of the Bacs payment system, the Direct Debit system, the UK ATM LINK switching platform covering 65,000 ATMs and the UK Faster Payments systems.\n\nVocalink processes over 90% of UK salaries, more than 70% of household bills and 98% of state benefits. In 2013 the company processed over 10.5 billion UK payments with a value of over £5 trillion. In July 2016 MasterCard purchased a 92% stake in the company, with the remainder to be held by UK banks for a period of three years.\n\nIn 1968, the Joint Stock Banks Clearing Committee chaired by Dennis Gladwell, set-up the Inter-Bank Computer Bureau to modernise the existing paper-based standing order system. Secure electronic funds transfer between banks was introduced from later that year, significantly reducing both the processing time and human error associated with paper-based transactions, particularly bulk payments. In 1971 the company adopted the name \"Bankers Automated Clearing Services Limited\", soon shortened to Bacs, which was formally adopted as the company's name in 1985.\n\nIn November 1998 HM Treasury commissioned a review of competition within the UK banking sector, to be chaired by Sir Don Cruickshank. Reporting in March 2000, The Cruickshank Report of competition within the UK banking sector recommended that:\n\nIn response, on 1 December 2003, Bacs Payment Schemes Ltd (BPSL) was split from Bacs Limited. BPSL was established as a not for profit company with members from the banking industry, the purpose of which is to promote the use of automated payment schemes and govern the rules of the Bacs scheme. Bacs Limited owns the infrastructure to run the Bacs scheme. Bacs Limited was permitted to continue to use the Bacs name for one year, becoming Voca Limited on 12 October 2004.\n\nLINK Interchange Network Ltd was formed in 1985 to create interoperability between ATMs across the United Kingdom. It became an international network in the 1990s through connection with the MasterCard and Visa networks. In October 2002 LINK launched the first service that used ATMs as a retail channel, enabling the facility to top-up a mobile phone at an ATM, bringing banking and mobile phones together for the consumer.\n\nIn 2005 a joint proposal from Voca and LINK was selected to deliver the payment-processing infrastructure for the Faster Payments Service, a near real-time interbank transfer for internet and telephone banking.\n\nAfter forming a strong working partnership, which brought Voca's bulk processing together with LINK's real-time payment switching, the companies agreed to merge on 2 July 2007 to form VocaLink. Since its launch in 2008, over 3 billion real-time faster payment transactions have been processed by VocaLink.\n\nVocaLink operates its services out of two data centers based in the UK; based in Harrogate and Dunstable.\n\nBacs provides two payment products to consumers:\nVocaLink provides the underlying BacsTEL-IP infrastructure for Bacs, which processes over 5.6 billion of clearing and settlement of automated payments a year, with a value of £4.2 trillion.\n\nVocaLink provides the switching infrastructure behind LINK, the busiest ATM switching system in the world which switches over 3.5 billion card-initiated transactions from 130 million enabled cards annually.\n\nVocaLink designed, built and operates the infrastructure for the Faster Payments Service on behalf of the Faster Payments Scheme. Running in parallel with Bacs and CHAPS, launched in 2008 it enables interbank transfers in real time. Since 2008, over three billion Faster Payments transactions have been securely processed by VocaLink.\n\nVocaLink provides the underlying technology to Paym, a mobile payment system for the UK developed by the Payments Council. Recipients are identified by their mobile phone number instead of bank details such as sort code and account number. It was launched through participating banks and building societies in April 2014, and once users opt-in will by the end of 2014 support 9 out of 10 UK bank accounts.\n\nVocaLink has developed Zapp, a function that will reside within mobile banking apps to allow users to make real-time payments to retailers when shopping online or in-store. The service will be open to all financial institutions, merchants, acquirers and consumers, when it launches in 2015.\n\nA Zapp payment works through secure digital 'tokens', which means that customers don’t reveal any of their financial details to retailers. This also means that merchants do not need to store card details.\n\nZapp has been developed by VocaLink who operate the UK payments infrastructure. VocaLink processes over 90% of UK salaries, more than 70% of household bills and 98% of state benefits. In 2013 VocaLink processed over 10 billion transactions with a value of over £5 trillion.\n\nZapp is backed by four UK high street banks, meaning 18m consumer accounts are potentially Zapp-enabled: HSBC, Nationwide, Metro Bank, Santander.\n\nOn 12 July 2010 through EAPS, LINK opened all UK cash machines to the similar German Girocard scheme operated by Zentraler Kreditausschuss. Similarly the VocaLink agreement with the Pulse network allows Discover card and Diners Club International cardholders to use LINK ATMs. VocaLink also provides a gateway service to Visa, MasterCard and China Union Pay.\n\nBetween 2007 and 2012, VocaLink operated a Single Euro Payments Area clearing and settlement mechanism, participating as a member of the European Automated Clearing House Association. The service was discontinued as a consequence of the low participation of VocaLink's shareholder UK banks into the platform.\n\nIn May 2008, VocaLink signed a deal with the Swedish Automated Clearing House Bankgirot, to outsource part of the Swedish payments system. This was the first time that the processing of a national payments scheme has been transferred to a non-domestic player.\n\nIn March 2014, in partnership with BCS Information Systems, VocaLink launched an immediate payments service, Fast And Secure Transfers (FAST) in Singapore, enabling fourteen Singaporean banks to offer the ability to transfer funds between bank accounts in real time.\n\nThe VocaLink Take Home Pay Index tracks monthly take home pay levels in the UK. Compiled using data captured by VocaLink from the salary payments of over two hundred FTSE 350 companies and six hundred public sector organisations, the indices are split by broad sector group: manufacturing; services; public sector; FTSE 350.\n\n\n"}
