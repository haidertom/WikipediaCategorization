{"id": "32329761", "url": "https://en.wikipedia.org/wiki?curid=32329761", "title": "Analytica (software)", "text": "Analytica (software)\n\nAnalytica is a visual software package developed by Lumina Decision Systems for creating, analyzing and communicating quantitative decision models. As a modeling environment, it is interesting in the way it combines hierarchical influence diagrams for visual creation and view of models, intelligent arrays for working with multidimensional data, Monte Carlo simulation for analyzing risk and uncertainty, and optimization, including linear and nonlinear programming. Its design, especially its influence diagrams and treatment of uncertainty, is based on ideas from the field of decision analysis. As a computer language, it is notable in combining a declarative (non-procedural) structure for referential transparency, array abstraction, and automatic dependency maintenance for efficient sequencing of computation.\n\nAnalytica models are organized as influence diagrams. Variables (and other objects) appear as nodes of various shapes on a diagram, connected by arrows that provide a visual representation of dependencies. Analytica influence diagrams may be hierarchical, in which a single \"module\" node on a diagram represents an entire submodel.\n\nHierarchical influence diagrams in Analytica serve as a key organizational tool. Because the visual layout of an influence diagram matches these natural human abilities both spatially and in the level of abstraction, people are able to take in far more information about a model's structure and organization at a glance than is possible with less visual paradigms, such as spreadsheets and mathematical expressions. Managing the structure and organization of a large model can be a significant part of the modeling process, but is substantially aided by the visualization of influence diagrams.\n\nInfluence diagrams also serve as a tool for communication. Once a quantitative model has been created and its final results computed, it is often the case that an understanding of how the results are obtained, and how various assumptions impact the results, is far more important than the specific numbers computed. The ability of a target audience to understand these aspects is critical to the modeling enterprise. The visual representation of an influence diagram quickly communicates an understanding at a level of abstraction that is normally more appropriate than detailed representations such as mathematical expressions or cell formulae. When more detail is desired, users can drill down to increasing levels of detail, speeded by the visual depiction of the model's structure.\n\nThe existence of an easily understandable and transparent model supports communication and debate within an organization, and this effect is one of the primary benefits of investing in quantitative model building. When all interested parties are able to understand a common model structure, debates and discussions will often focus more directly on specific assumptions, can cut down on \"cross-talk\", and therefore lead to more productive interactions within the organization. The influence diagram serves as a graphical representation that can help to make models accessible to people at different levels.\n\nAnalytica uses index objects to track the dimensions of multidimensional arrays. An index object has a name and a list of elements. When two multidimensional values are combined, for example in an expression such as\n\nwhere \"Revenue\" and \"Expenses\" are each multidimensional, Analytica repeats the profit calculation over each dimension, but recognizes when same dimension occurs in both values and treats it as the same dimension during the calculation, in a process called \"intelligent array abstraction\". Unlike most programming languages, there is no inherent ordering to the dimensions in a multidimensional array. This avoids duplicated formulas and explicit FOR loops, both common sources of modeling errors. The simplified expressions made possible by intelligent array abstraction allow the model to be more accessible, interpretable, and transparent.\n\nAnother consequence of intelligent array abstraction is that new dimensions can be introduced or removed from an existing model, without requiring changes to the model structure or changes to variable definitions. For example, while creating a model, the model builder might assume a particular variable, for example \"discount_rate\", contains a single number. Later, after constructing a model, a user might replace the single number with a table of numbers, perhaps \"discount_rate\" broken down by \"Country\" and by \"Economic_scenario\". These new divisions may reflect the fact that the effective discount rate is not the same for international divisions of a company, and that different rates are applicable to different hypothetical scenarios. Analytica automatically propagates these new dimensions to any results that depend upon \"discount_rate\", so for example, the result for \"Net present value\" will become multidimensional and contain these new dimensions. In essence, Analytica repeats the same calculation using the discount rate for each possible combination of \"Country\" and \"Economic_scenario\".\n\nThis flexibility is important when exploring computation tradeoffs between the level of detail, computation time, available data, and overall size or dimensionality of parametric spaces. Such adjustments are common after models have been fully constructed as a way of exploring \"what-if\" scenarios and overall relationships between variables.\n\nIncorporating uncertainty into model outputs helps to provide more realistic and informative projections. Uncertain quantities in Analytica can be specified using a distribution function. When evaluated, distributions are sampled using either Latin hypercube or Monte Carlo sampling, and the samples are propagated through the computations to the results. The sampled result distribution and summary statistics can then be viewed directly (mean, fractile bands, probability density function (PDF), cumulative distribution function (CDF)), Analytica supports collaborative decision analysis and probability management through the use of the SIPMath(tm) standard.\n\nSystem dynamics is an approach to simulating the behaviour of complex systems over time. It deals with feedback loops and time delays on the behaviour of the entire system. The Dynamic() function in Analytica allows definition of variables with cyclic dependencies, such as feedback loops. It expands the influence diagram notation, which does not normally allow cycles. At least one link in each cycle includes a time lag, depicted as a gray influence arrow to distinguish it from standard black arrows without time lags.\n\nAnalytica includes a general language of operators and functions for expressing mathematical relationships among variables. Users can define functions and libraries to extend the language.\n\nAnalytica has several features as a programming language designed to make it easy to use for quantitative modeling: It is a visual programming language, where users view programs (or \"models\") as influence diagrams, which they create and edit visually by adding and linking nodes. It is a declarative language, meaning that a model declares a definition for each variable without specifying an execution sequence as required by conventional imperative languages. Analytica determines a correct and efficient execution sequence using the dependency graph. It is a referentially transparent functional language, in that execution of functions and variables have no side effects i.e. changing other variables. Analytica is an array programming language, where operations and functions generalize to work on multidimensional arrays.\n\nAnalytica has been used for policy analysis, business modeling, and risk analysis. Areas in which Analytica has been applied include energy, health and pharmaceuticals, \nenvironmental risk and emissions policy analysis, wildlife management,\necology,\nclimate change, technology and defense,\nstrategic financial planning, \nR&D planning and portfolio management, \nfinancial services, \naerospace, manufacturing and environmental health impact assessment.\n\nThe Analytica software runs on Microsoft Windows operating systems. Three editions (Professional, Enterprise, Optimizer) each with more functions and cost, are purchased by users interested in building models. A free edition is available, called Analytica Free 101, which allows you to build medium to moderate sized models of up to 101 user objects.. Free 101 also allows you to view models with more than 101 objects, change inputs, and compute results, which enables free sharing of models for review. A more capable but non-free Power Player enables users to save inputs and utilize database connections. The Analytica Cloud Player allows you to share models over the web and lets users access and run via a web browser.\n\nThe most recent release of Analytica is version 5.1, released in May 2018.\n\nAnalytica's predecessor, called \"Demos\", grew from the research on tools for policy analysis by Max Henrion as a PhD student and later professor at Carnegie Mellon University between 1979 and 1990. Henrion founded Lumina Decision Systems in 1991 with Brian Arnold. Lumina continued to develop the software and apply it to environmental and public policy analysis applications. Lumina first released Analytica as a product in 1996.\n"}
{"id": "1118832", "url": "https://en.wikipedia.org/wiki?curid=1118832", "title": "Arbitrarily large", "text": "Arbitrarily large\n\nIn mathematics, the phrases arbitrarily large, arbitrarily small, and arbitrarily long are used in statements such as:\n\nwhich is shorthand for:\n\n\"Arbitrarily large\" is not equivalent to \"sufficiently large\". For instance, while it is true that prime numbers can be arbitrarily large since there are an infinite number of them, it is not true that all sufficiently large numbers are prime. \"Arbitrarily large\" does not mean \"infinitely large\" because although prime numbers can be arbitrarily large, an infinitely large prime does not exist since all prime numbers (as well as all other integers) are finite.\n\nIn some cases, phrases such as \"P(\"x\") is true for arbitrarily large \"x\"\" are used primarily for emphasis, as in \"P(\"x\") is true for all \"x\", no matter how large \"x\" is.\" In these cases, the phrase \"arbitrarily large\" does not have the meaning indicated above but is in fact logically synonymous with \"all.\"\n\nTo say that there are \"arbitrarily long arithmetic progressions of prime numbers\" does not mean that there exists any infinitely long arithmetic progression of prime numbers (there is not), nor that there exists any particular arithmetic progression of prime numbers that is in some sense \"arbitrarily long\", but rather that no matter how large a number \"n\" is, there exists some arithmetic progression of prime numbers of length at least \"n\".\n\nThe statement \"ƒ(\"x\") is non-negative for arbitrarily large \"x\".\" could be rewritten as:\n\nUsing \"sufficiently large\" instead yields:\n\n"}
{"id": "28045698", "url": "https://en.wikipedia.org/wiki?curid=28045698", "title": "Cannonball problem", "text": "Cannonball problem\n\nIn the mathematics of figurate numbers, the cannonball problem asks which numbers are both square and square pyramidal. The problem can be stated as: given a square arrangement of cannonballs, for what size squares can these cannonballs also be arranged into a square pyramid.\nEquivalently, which squares can be represented as the sum of consecutive squares, starting from 1?\n\nWhen cannonballs are stacked within a square frame, the number of balls is a square pyramidal number; Thomas Harriot gave a formula for this number around 1587, answering a question posed to him by Sir Walter Raleigh on their expedition to America. \nÉdouard Lucas formulated the cannonball problem as a Diophantine equation\nor\nand conjectured that the only solutions are \"N\" = 1, \"M\" = 1, and \"N\" = 24, \"M\" = 70. It was not until 1918 that G. N. Watson found a proof for this fact, using elliptic functions. More recently, elementary proofs have been published.\nThe solution \"N\" = 24, \"M\" = 70. can be used for constructing the Leech Lattice. The result has relevance to the bosonic string theory in 26 dimensions.\n\nAlthough it is possible to tile a geometric square with unequal squares, it is not possible to do so with a solution to the cannonball problem. The squares with side lengths from 1 to 24 have areas equal to the square with side length 70, but they cannot be arranged to tile it.\n\nThe only numbers that are simultaneously triangular and square pyramidal, are 1, 55, 91, and 208335..\n\nThere are no numbers (other than the trivial solution 1) that are both tetrahedral and square pyramidal.\n\n"}
{"id": "45715624", "url": "https://en.wikipedia.org/wiki?curid=45715624", "title": "Compound of octahedra", "text": "Compound of octahedra\n\nA compound of octahedra may be:\n"}
{"id": "47790980", "url": "https://en.wikipedia.org/wiki?curid=47790980", "title": "Control variable (programming)", "text": "Control variable (programming)\n\nA control variable in computer programming is a program variable that is used to regulate the flow of control of the program.\n\nIn definite iteration, control variables are variables which are successively assigned (or bound to) values from a predetermined sequence of values.\n\nIn some programming languages control variables are just ordinary variables used for manipulating the program flow. This is the case of C, Fortran, and Pascal, which allow for control variables to have their values changed within the loop body. However, some languages have special rules for control variables. In Ada, for instance, the control variable of the for loop must remain constant within the loop body.\n"}
{"id": "51363226", "url": "https://en.wikipedia.org/wiki?curid=51363226", "title": "Convolution quotient", "text": "Convolution quotient\n\nIn mathematics, a convolution quotient is to the operation of convolution as a quotient of integers is to multiplication. Convolution quotients were introduced by , and their theory is sometimes called \"Mikusiński's operational calculus\". For two functions \"ƒ\", \"g\", the pair (\"ƒ\", \"g\") has the same convolution quotient as the pair (\"h\" * \"ƒ\",\"h\" * \"g\").\n\nConvolution quotients are used in an approach to making Dirac's delta function and other generalized functions logically rigorous.\n\n"}
{"id": "26661047", "url": "https://en.wikipedia.org/wiki?curid=26661047", "title": "Counting rods", "text": "Counting rods\n\nCounting rods () are small bars, typically 3–14 cm long, that were used by mathematicians for calculation in ancient East Asia. They are placed either horizontally or vertically to represent any integer or rational number.\n\nThe written forms based on them are called rod numerals. They are a true positional numeral system with digits for 1–9 and a blank for 0, from the Warring states period (circa 475 BCE) to the 16th century.\n\nChinese arithmeticians used counting rods well over two thousand years ago. In 1954 forty-odd counting rods of the Warring States period (5th century BCE to 221 BCE) were found in Zuǒjiāgōngshān (左家公山) Chu Grave No.15 in Changsha, Hunan.\n\nIn 1973 archeologists unearthed a number of wood scripts from a tomb in Hubei dating from the period of the Han dynasty (206 BCE to 220 CE). On one of the wooden scripts was written: \"当利二月定算\". This is one of the earliest examples of using counting-rod numerals in writing.\n\nIn 1976 a bundle of Western Han-era (202 BCE to 9 CE) counting rods made of bones was unearthed from Qianyang County in Shaanxi. The use of counting rods must predate it; Sunzi ( 544 to 496 BCE), a military strategist at the end of Spring and Autumn period of 771 BCE to 5th century BCE, mentions their use to make calculations to win wars before going into the battle; Laozi (died 531 BCE), writing in the Warring States period, said \"a good calculator doesn't use counting rods\". The \"Book of Han\" (finished 111 CE) recorded: \"they calculate with bamboo, diameter one fen, length six cun, arranged into a hexagonal bundle of two hundred seventy one pieces\".\n\nAt first, calculating rods were round in cross-section, but by the time of the Sui dynasty (581 to 618 CE) mathematicians used triangular rods to represent positive numbers and rectangular rods for negative numbers.\n\nAfter the abacus flourished, counting rods were abandoned except in Japan, where rod numerals developed into a symbolic notation for algebra.\n\nCounting rods represent digits by the number of rods, and the perpendicular rod represents five. To avoid confusion, vertical and horizontal forms are alternately used. Generally, vertical rod numbers are used for the position for the units, hundreds, ten thousands, etc., while horizontal rod numbers are used for the tens, thousands, hundred thousands etc. It is written in \"Sunzi Suanjing\" that \"one is vertical, ten is horizontal\".\n\nRed rods represent positive numbers and black rods represent negative numbers. Ancient Chinese clearly understood negative numbers and zero (leaving a blank space for it), though they had no symbol for the latter. The Nine Chapters on the Mathematical Art, which was mainly composed in the first century CE, stated \"(when using subtraction) subtract same signed numbers, add different signed numbers, subtract a positive number from zero to make a negative number, and subtract a negative number from zero to make a positive number\". Later, a go stone was sometimes used to represent zero.\n\nThis alternation of vertical and horizontal rod numeral form is very important to understanding written transcription of rod numerals on manuscripts correctly. For instance, in Licheng suanjin, 81 was transcribed as , and 108 was transcribed as ; it is clear that the latter clearly had a blank zero on the \"counting board\" (i.e., floor or mat), even though on the written transcription, there was no blank. In the same manuscript, 405 was transcribed as , with a blank space in between for obvious reasons, and could in no way be interpreted as \"45\". In other words, transcribed rod numerals may not be positional, but on the counting board, they are positional. is an exact image of the counting rod number 405 on a table top or floor.\n\nThe value of a number depends on its physical position on the counting board. A 9 at the rightmost position on the board stands for 9. Moving the batch of rods representing 9 to the left one position (i.e., to the tens place) gives 9[] or 90. Shifting left again to the third position (to the hundreds place) gives 9[][] or 900. Each time one shifts a number one position to the left, it is multiplied by 10. Each time one shifts a number one position to the right, it is divided by 10. This applies to single-digit numbers or multiple-digit numbers.\n\nSong dynasty mathematician Jia Xian used hand-written Chinese decimal orders 步十百千萬 as rod numeral place value, as evident from a facsimile from a page of Yongle Encyclopedia. He arranged 七萬一千八百二十四 as\n\nHe treated the Chinese order numbers as place value markers, and 七一八二四 became place value decimal number. He then wrote the rod numerals according to their place value:\n\nIn Japan, mathematicians put counting rods on a counting board, a sheet of cloth with grids, and used only vertical forms relying on the grids. An 18th-century Japanese mathematics book has a checker counting board diagram, with the order of magnitude symbols \"千百十一分厘毛“(thousand, hundred, ten, unit, tenth, hundredth, thousandth).\n\nExamples:\nRod numerals are a positional numeral system made from shapes of counting rods. Positive numbers are written as they are and the negative numbers are written with a slant bar at the last digit. The vertical bar in the horizontal forms 6–9 are drawn shorter to have the same character height.\n\nA circle (〇) is used for 0. Many historians think it was imported from Indian numerals by Gautama Siddha in 718, but some think it was created from the Chinese text space filler \"□\", and others think that the Indians acquired it from China, because it resembles a Confucian philosophical symbol for \"nothing\".\n\nIn the 13th century, Southern Song mathematicians changed digits for 4, 5, and 9 to reduce strokes. The new horizontal forms eventually transformed into Suzhou numerals. Japanese continued to use the traditional forms.\n\nExamples:\nIn Japan, Seki Takakazu developed the rod numerals into symbolic notation for algebra and drastically improved Japanese mathematics. After his period, the positional numeral system using Chinese numeral characters was developed, and the rod numerals were used only for the plus and minus signs. \n\nA fraction was expressed with rod numerals as two rod numerals one on top of another (without any other symbol, like the modern horizontal bar).\n\nThe method for using counting rods for mathematical calculation was called \"rod calculation\" or rod calculus (筹算). Rod calculus can be used for a wide range of calculations, including finding the value of , finding square roots, cube roots, or higher order roots, and solving a system of linear equations.\n\nBefore the introduction of written zero, there was no way to distinguish 10007 and 107 in written forms except by inserting a bigger space between 1 and 7, and so rod numerals were used only for doing calculations with counting rods. Once written zero came into play, the rod numerals had become independent, and their use indeed outlives the counting rods, after its replacement by abacus. One variation of horizontal rod numerals, the Suzhou numerals is still in use for book-keeping and in herbal medicine prescription in Chinatowns in some parts of the world.\n\nUnicode 5.0 includes counting rod numerals in their own block in the Supplementary Multilingual Plane (SMP) from U+1D360 to U+1D37F. The code points for the horizontal digits 1–9 are U+1D360 to U+1D368 and those for the vertical digits 1–9 are U+1D369 to U+1D371. The former are called \"unit digits\" and the latter are called \"tens digits\", which is opposite of the convention described above. Zero should be represented by U+3007 (〇, ideographic number zero) and the negative sign should be represented by U+20E5 (combining reverse solidus overlay). As these were recently added to the character set and since they are included in the SMP, font support may still be limited.\n\n\nFor a look of the ancient counting rods, and further explanation, you can visit the sites\n"}
{"id": "16175342", "url": "https://en.wikipedia.org/wiki?curid=16175342", "title": "De divina proportione", "text": "De divina proportione\n\nDe divina proportione (\"On the Divine Proportion\") is a book on mathematics written by Luca Pacioli and illustrated by Leonardo da Vinci, composed around 1498 in Milan and first printed in 1509. Its subject was mathematical proportions (the title refers to the golden ratio) and their applications to geometry, visual art through perspective, and architecture. The clarity of the written material and Leonardo's excellent diagrams helped the book to achieve an impact beyond mathematical circles, popularizing contemporary geometric concepts and images.\n\nThe book consists of three separate manuscripts, which Pacioli worked on between 1496 and 1498.\n\nThe first part, \"Compendio divina proportione\" (\"Compendium on the Divine Proportion\"), studies the golden ratio from a mathematical perspective (following the relevant work of Euclid) and explores its applications to various arts, in seventy-one chapters. It also contains a discourse on the regular and semiregular polyhedra, as well as a discussion of the use of geometric perspective by painters such as Piero della Francesca, Melozzo da Forlì and Marco Palmezzano.\n\nThe second part, \"Trattato dell'architettura\" (\"Treatise on Architecture\"), discusses the ideas of Vitruvius (from his \"De architectura\") on the application of mathematics to architecture in twenty chapters. The text compares the proportions of the human body to those of artificial structures, with examples from classical Greco-Roman architecture.\n\nThe third part, \"Libellus in tres partiales divisus\" (\"Book divided into three parts\"), is mainly an Italian translation of Piero della Francesca's Latin writings \"On [the] Five Regular Solids\" (\"De quinque corporibus regularibus\") and mathematical examples. In 1550 Giorgio Vasari wrote a biography of della Francesca, in which he accused Pacioli of plagiarism and claimed that he stole della Francesca's work on perspective, on arithmetic and on geometry.\n\nAfter these three parts are appended two sections of illustrations, the first showing twenty-three capital letters drawn with a ruler and compass by Pacioli and the second with some sixty illustrations in woodcut after drawings by Leonardo da Vinci. Leonardo drew the illustrations of the regular solids while he lived with and took mathematics lessons from Pacioli. Leonardo's drawings are probably the first illustrations of skeletonic solids which allowed an easy distinction between front and back.\n\nPacioli produced three manuscripts of the treatise by different scribes. He gave the first copy with a dedication to the Duke of Milan, Ludovico il Moro; this manuscript is now preserved in Switzerland at the Bibliothèque de Genève in Geneva. A second copy was donated to Galeazzo da Sanseverino and now rests at the Biblioteca Ambrosiana in Milan. The third, which has gone missing, was given to Pier Soderini, the Gonfaloniere of Florence. On 1 June 1509 the first printed edition was published in Venice by Paganino Paganini; it has since been reprinted several times.\nThe book was displayed as part of an exhibition in Milan between October 2005 and October 2006 together with the Codex Atlanticus. The \"M\" logo used by the Metropolitan Museum of Art in New York is adapted from one in \"De divina proportione\".\n\n\n"}
{"id": "34505429", "url": "https://en.wikipedia.org/wiki?curid=34505429", "title": "Deterministic simulation", "text": "Deterministic simulation\n\nIn mathematical modeling, deterministic simulations contain no random variables and no degree of randomness, and consist mostly of equations, for example difference equations. These simulations have known inputs and they result in a unique set of outputs. Contrast stochastic (probability) simulation, which includes random variables. \n\nDeterministic simulation models are usually designed to capture some underlying mechanism or natural process. They are different from statistical models (for example linear regression) whose aim is to empirically estimate the relationships between variables. The deterministic model is viewed as a useful approximation of reality that is easier to build and interpret than a stochastic model. However, such models can be extremely complicated with large numbers of inputs and outputs, and therefore are often noninvertible; a fixed single set of outputs can be generated by multiple sets of inputs. Thus taking reliable account of parameter and model uncertainty is crucial, perhaps even more so than for standard statistical models, yet this is an area that has received little attention from statisticians.\n\nMostly are deterministic simulations used in scientific researches, we can found in various studies about populations fields, climate development, pollution, but also in another areas as engineering, chemistry and policy making. Deterministic simulations have received attention in the statistical literature under the general topic of computer experiments. Computer experiments simulate complex system which requires a number of inputs. Use of stochastic system is much cheaper but also inaccurate and simplifying.\n\nIt is necessary to translate model into computer recognizable format. Modeler must decide if whether to program the model in a simulation language such as GPSS/H or to use special purpose simulation software:\n\nArene – discrete event simulator has also academic version\n\nCSIM – CSIM is a re-usable general purpose discrete-event simulation environment for modeling complex systems of interacting elements. It contains hierarchical block diagram tools and extensive model libraries covering several domains. CSIM can be used for modeling: agent-based systems, logistics, wireless networks, computer networks... \n\nDynare – when the framework is deterministic, can be used for models with the assumption of perfect foresight. The purpose of the simulation is to describe the reaction in anticipation of, then in reaction to the shock, until the system returns to the old or to a new state of equilibrium. \n\nJanus – Janus is an interactive simulation war game portraying realistic events during multi-sided combat. It uses digitized terrain effecting line of sight and movement, depicting contour lines, roads, rivers, vegetation and urban areas. It has the capability to be networked with other systems, in order to simulate a war game with multiple sides. \n\nModsaf (Modular Semi-Automated Forces) is a set of software modules and applications used to construct Advanced Distributed Simulation (ADS) and Computer Generated Forces (CGF) applications. ModSAF modules and applications let a single operator create and control large numbers of entities that are used for realistic training, test, and evaluation on the virtual battlefield. ModSAF contains entities that are sufficiently realistic resulting in the user not being aware that the displayed vehicles are being maneuvered by computers, rather than human crews. These entities, which include ground and air vehicles, dismounted infantry (DI), missiles, and dynamic structures, can interact with each other and with manned individual entity simulators to support training, combat development experiments, and test of evaluation studies. \n\nTaylor Enterprise Dynamics is an objectoriented software system used to model, simulate, visualize, and monitor dynamic-flow process activities and systems. With Taylor ED’s open architecture, software users can access standard libraries of atoms to build models. Atoms are Taylor ED’s smart objects and model building resources. In addition to Taylor ED’s standard atom libraries, users can create new atoms themselves.\n\nPerformance evaluation of highly concurrent computers B. Kumar and E. S. Davidson Object of the simulation is CPU memory subsystem IBM 360/91. \n\nSimulation is presented as a practical technique for performance evaluation of alternative configurations of highly concurrent computers. A technique is described for constructing a detailed deterministic simulation model of a system. In the model a control stream replaces the instruction and data streams of the real system. Simulation of the system model yields the timing and resource usage statistics needed for performance evaluation, without the necessity of emulating the system. As a case study, the implementation of a simulator of a model of the CPUmemory subsystem of the IBM 360/91 is described. \n\nA comparison of deterministic vs stochastic simulation models for assessing adaptive information management techniques over disadvantaged tactical communication networks – Dr. Allan Gibb Mr. Jean-Claude St-Jacques\n\nUse of a deterministic battlefield model based on a scripted scenario will provide the required\nreproducibility and full control over event sequencing. A stochastic battlefield model, as provided in computer simulation applications like JANUS and ModSAF, produces results that can be made strictly reproducible if the same random number seed can be employed. However, such a model will not provide full human control over scenario composition and event sequencing. A deterministic battlefield model offers clear advantages for the test bed studies.\n\n\n"}
{"id": "5708669", "url": "https://en.wikipedia.org/wiki?curid=5708669", "title": "Dowker notation", "text": "Dowker notation\n\nIn the mathematical field of knot theory, the Dowker notation, also called the Dowker–Thistlethwaite notation or code, for a knot is a sequence of even integers. The notation is named after Clifford Hugh Dowker and Morwen Thistlethwaite, who refined a notation originally due to Peter Guthrie Tait. \n\nTo generate the Dowker notation, traverse the knot using an arbitrary starting point and direction. Label each of the n crossings with the numbers 1, ..., 2\"n\" in order of traversal (each crossing is visited and labelled twice), with the following modification: if the label is an even number and the strand followed crosses over at the crossing, then change the sign on the label to be a negative. When finished, each crossing will be labelled a pair of integers, one even and one odd. The Dowker notation is the sequence of even integer labels associated with the labels 1, 3, ..., 2\"n\" − 1 in turn.\n\nFor example, a knot diagram may have crossings labelled with the pairs (1, 6) (3, −12) (5, 2) (7, 8) (9, −4) and (11, −10). The Dowker notation for this labelling is the sequence: 6 −12 2 8 −4 −10.\n\nA knot can be recovered from a Dowker sequence, but the recovered knot may differ from the original by being a reflection or (more generally) by having any connected sum component reflected in the line between its entry/exit points – the Dowker notation is unchanged by these reflections. Knots tabulations typically consider only prime knots and disregard chirality, so this ambiguity does not affect the tabulation.\n\nThe ménage problem, posed by Tait, concerns counting the number of different number sequences possible in this notation.\n\n\n"}
{"id": "2625993", "url": "https://en.wikipedia.org/wiki?curid=2625993", "title": "Erdős–Bacon number", "text": "Erdős–Bacon number\n\nA person's Erdős–Bacon number is the sum of one's Erdős number—which measures the \"collaborative distance\" in authoring academic papers between that person and Hungarian mathematician Paul Erdős—and one's Bacon number—which represents the number of links, through roles in films, by which the individual is separated from American actor Kevin Bacon. The lower the number, the closer a person is to Erdős and Bacon, which reflects a small world phenomenon in academia and entertainment.\n\nThe combined Erdős/Bacon [sic] number was introduced by mathematicians Tim Hsu and David Grabiner sometime before late-January 1999, when they pointed out that Daniel Kleitman has a combined number of 3: a Bacon number of 2 and Erdős number of 1.\n\nTo have a defined Erdős–Bacon number, it is necessary to have both appeared in a film and co-authored an academic paper, although this in and of itself is not sufficient.\nMathematician Daniel Kleitman has the Erdős–Bacon number of 3; it is the lowest among scientists: he is a co-author of Erdős on multiple papers, and has a Bacon number of 2, via Minnie Driver in \"Good Will Hunting\".\n\nMathematician Ken Ono has an Erdős–Bacon number of 4; 2 for Erdős and 2 for Bacon.\n\nMathematician Doron Zeilberger has an Erdős-Bacon number of 5. Computer scientist Tom Porter also has an Erdős-Bacon number of 5; 3 for Erdős in two ways and 2 for Bacon.\n\nAstronomer Carl Sagan has an Erdős number of 4 (via Steven J. Ostro) and a Bacon number of 2 (Sagan and Bacon having appeared with Johnny Carson on episodes of \"The Tonight Show\"), for a total of 6. Physicist Richard Feynman has an Erdős number of 3, and a Bacon number of 3, having appeared in the film \"Anti-Clock\" alongside Tony Tang. Geneticist Jonathan Pritchard appeared in the 1998 movie \"Without Limits\" which gives him a Bacon number of 2. Pritchard has an Erdős number of 4 thus giving him an Erdős–Bacon number of 6. Theoretical physicist Stephen Hawking has an Erdős–Bacon number of 6: his Bacon number of 2 (via his appearance alongside John Cleese in \"Monty Python Live (Mostly)\" who acted alongside Kevin Bacon in \"The Big Picture\") is lower than his Erdős number of 4.\n\nCanadian actor Albert M. Chan has an Erdős-Bacon number of 4. He co-authored a peer-reviewed paper on Orthogonal frequency-division multiplexing, giving him an Erdős number of 3, and was cast alongside Kevin Bacon in \"Patriots Day\", giving him a Bacon number of 1.\n\nDanica McKellar, who played Winnie Cooper in \"The Wonder Years\", has an Erdős–Bacon number of 6, having coauthored a mathematics paper published while an undergraduate at the University of California, Los Angeles. Her paper gives her an Erdős number of 4, and she has a Bacon number of 2, having worked with Margaret Easley.\n\nAmerican actress Natalie Portman has an Erdős–Bacon number of 7. She collaborated (using her birth name, Natalie Hershlag) with Abigail A. Baird, who has a collaboration path leading to Joseph Gillis, who has an Erdős number of 1. Portman appeared in \"A Powerful Noise Live\" (2009) with Sarah Michelle Gellar, who appeared in \"The Air I Breathe\" (2007) with Bacon, giving Portman a Bacon number of 2 and an Erdős number of 5.\n\nBritish actor Colin Firth has an Erdős–Bacon number of 7. Firth is credited as co-author of a neuroscience paper, \"Political Orientations Are Correlated with Brain Structure in Young Adults\", after he suggested on BBC Radio 4 that such a study could be done. Another author of that paper, Geraint Rees, has an Erdős number of 5, which gives Firth an Erdős number of 6. Firth's Bacon number of 1 is due to his appearance in \"Where the Truth Lies\".\n\nKristen Stewart has an Erdős–Bacon number of 7; she is credited as a co-author on an artificial intelligence paper that was written after a technique was used for her short film \"Come Swim\", giving her an Erdős number of 5, and she co-starred with Michael Sheen in \"Twilight\", who co-starred with Bacon in \"Frost/Nixon\", giving her a Bacon number of 2.\n\nAmerican scholar and actor Michael M. Chemers has an Erdös-Bacon number of 6. He co-authored a 2018 paper about \"Game of Thrones\" with mathematician Andrew Beveridge, who has an Erdös number of 2, giving him an Erdös number of 3, and co-starred in two films (\"When Tyrants Kiss,\" 2004; \"Before the Thunder,\" 2018) both of which give him a Bacon number of 3 through a number of co-stars.\n\nNotes:\n"}
{"id": "34189212", "url": "https://en.wikipedia.org/wiki?curid=34189212", "title": "Glossary of areas of mathematics", "text": "Glossary of areas of mathematics\n\nThis is a glossary of terms that are or have been considered areas of study in mathematics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "12943121", "url": "https://en.wikipedia.org/wiki?curid=12943121", "title": "Hindu units of time", "text": "Hindu units of time\n\nHindu texts describe units of Kala measurements, from \"microseconds\" to \"Trillions\" of years. According to these texts, time is cyclic, which repeats itself forever.\n\nVarious fragments of time are used in Hindu Scriptures like Vedas, Bhagavata Purana, Vishnu Puran, Mahabharata, Suryasidhanta etc. A summary of the Hindu metrics of time (\" vyavahāra\") follows.\n\n\n\nThe Lifespan of the pitras is 100 years of pitras (3,000 Solar years).\n\nThe life span of any Hindu deva spans nearly (or more than) 4.5 million years. Statistically, we can also look it as:\n\nThe Time measurement section of the Book I Chapter III explains the above as follows:\n\n(2 \"Kalpas\" constitute a day and night of Brahma, 8.64 billion human years)\n\nOne day of Brahma is divided into 1000 parts called \"charaṇas\".\n\nThe four yugas which come one after the other are as follows (along with their durations): \n\nThe cycle repeats itself, so altogether there are 1,000 cycles of Mahā-Yuga in one day of Brahma.\n\nCurrently, 50 years of Brahma have elapsed. The last Kalpa at the end of the 50th year is called Padma Kalpa. We are currently in the first 'day' of the 51st year. This Brahma's day, Kalpa is named as Shveta-Varaha Kalpa. Within this Day, six Manvantaras have already elapsed and this is the seventh Manvantara, named as – Vaivasvatha Manvantara (or Sraddhadeva Manvantara). Within the Vaivasvatha Manvantara, 27 Mahayugas (4 Yugas together is a Mahayuga), and the Krita, Treta and Dwapara Yugas of the 28th Mahayuga have elapsed. This Kaliyuga is in the 28th Mahayuga. This Kaliyuga began in the year 3102 BCE in the proleptic Julian Calendar. Since 50 years of Brahma have already elapsed, this is the second Parardha, also called as Dvithiya Parardha.\n432000 × 10 × 1000 × 2 = 8.64 billion years (2 Kalpa (day and night))\n8.64 × 10 × 30 × 12 = 3.1104 Trillion Years (1 year of Brahma)3.1104 × 10 × 50 = 155.52 trillion years (50 years of Brahma)\n(6 × 71 × 4320000) + 7 × 1.728 × 10^6 = 1852416000 years elapsed in first six Manvataras, and Sandhi Kalas in the current Kalpa\n27 × 4320000 = 116640000 years elapsed in first 27 Mahayugas of the current Manvantara\n1.728 × 10^6 + 1.296 × 10^6 + 864000 = 3888000 years elapsed in current Mahayuga\n3102 + 2017 = 5119 years elapsed in current Kaliyuga.\nSo the total time elapsed since current Brahma is\n155520000000000 + 1852416000 + 116640000 + 3888000 + 5119 = 155,521,972,949,120 years\n\n(one hundred fifty-five trillion, five hundred twenty-one billion, nine hundred seventy-two million, nine hundred forty-nine thousand, one hundred twenty years) as of 2018 AD\n\nTotal age of Brahma is 100 (Brahma Years) which is equal to 311,040,000,000,000 Human years\nThe current Kali Yuga began at midnight 17 February / 18 February in 3102 BCE in the proleptic Julian calendar. As per the information above about Yuga periods, only 5,120 years are passed out of 432,000 years of current Kali Yuga, and hence another 426,880 years are left to complete this 28th Kali Yuga of Vaivaswatha Manvantara.\n\n\n"}
{"id": "6134187", "url": "https://en.wikipedia.org/wiki?curid=6134187", "title": "History of mathematical notation", "text": "History of mathematical notation\n\nThe history of mathematical notation includes the commencement, progress, and cultural diffusion of mathematical symbols and the conflict of the methods of notation confronted in a notation's move to popularity or inconspicuousness. Mathematical notation comprises the symbols used to write mathematical equations and formulas. Notation generally implies a set of well-defined representations of quantities and symbols operators. The history includes Hindu–Arabic numerals, letters from the Roman, Greek, Hebrew, and German alphabets, and a host of symbols invented by mathematicians over the past several centuries.\n\nThe development of mathematical notation can be divided in stages. The \"\"rhetorical\" stage is where calculations are performed by words and no symbols are used. The \"syncopated\"\" stage is where frequently used operations and quantities are represented by symbolic syntactical abbreviations. From ancient times through the post-classical age, bursts of mathematical creativity were often followed by centuries of stagnation. As the early modern age opened and the worldwide spread of knowledge began, written examples of mathematical developments came to light. The \"symbolic\" stage is where comprehensive systems of notation supersede rhetoric. Beginning in Italy in the 16th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This symbolic system was in use by medieval Indian mathematicians and in Europe since the middle of the 17th century, and has continued to develop in the contemporary era.\n\nThe area of study known as the history of mathematics is primarily an investigation into the origin of discoveries in mathematics and, the focus here, the investigation into the mathematical methods and notation of the past.\n\nAlthough the history commences with that of the Ionian schools, there is no doubt that those Ancient Greeks who paid attention to it were largely indebted to the previous investigations of the Ancient Egyptians and Ancient Phoenicians. Numerical notation's distinctive feature, i.e. symbols having local as well as intrinsic values (arithmetic), implies a state of civilization at the period of its invention. Our knowledge of the mathematical attainments of these early peoples, to which this section is devoted, is imperfect and the following brief notes be regarded as a summary of the conclusions which seem most probable, and the history of mathematics begins with the symbolic sections.\n\nMany areas of mathematics began with the study of real world problems, before the underlying rules and concepts were identified and defined as abstract structures. For example, geometry has its origins in the calculation of distances and areas in the real world; algebra started with methods of solving problems in arithmetic.\n\nThere can be no doubt that most early peoples which have left records knew something of numeration and mechanics, and that a few were also acquainted with the elements of land-surveying. In particular, the Egyptians paid attention to geometry and numbers, and the Phoenicians to practical arithmetic, book-keeping, navigation, and land-surveying. The results attained by these people seem to have been accessible, under certain conditions, to travelers. It is probable that the knowledge of the Egyptians and Phoenicians was largely the result of observation and measurement, and represented the accumulated experience of many ages.\n\nWritten mathematics began with numbers expressed as tally marks, with each tally representing a single unit. The numerical symbols consisted probably of strokes or notches cut in wood or stone, and intelligible alike to all nations. For example, one notch in a bone represented one animal, or person, or anything else. The peoples with whom the Greeks of Asia Minor (amongst whom notation in western history begins) were likely to have come into frequent contact were those inhabiting the eastern littoral of the Mediterranean: and Greek tradition uniformly assigned the special development of geometry to the Egyptians, and that of the science of numbers either to the Egyptians or to the Phoenicians.\n\nThe Ancient Egyptians had a symbolic notation which was the numeration by Hieroglyphics. The Egyptian mathematics had a symbol for one, ten, one-hundred, one-thousand, ten-thousand, one-hundred-thousand, and one-million. Smaller digits were placed on the left of the number, as they are in Hindu–Arabic numerals. Later, the Egyptians used hieratic instead of hieroglyphic script to show numbers. Hieratic was more like cursive and replaced several groups of symbols with individual ones. For example, the four vertical lines used to represent four were replaced by a single horizontal line. This is found in the Rhind Mathematical Papyrus (c. 2000–1800 BC) and the Moscow Mathematical Papyrus (c. 1890 BC). The system the Egyptians used was discovered and modified by many other civilizations in the Mediterranean. The Egyptians also had symbols for basic operations: legs going forward represented addition, and legs walking backward to represent subtraction.\n\nThe Mesopotamians had symbols for each power of ten. Later, they wrote their numbers in almost exactly the same way done in modern times. Instead of having symbols for each power of ten, they would just put the coefficient of that number. Each digit was at separated by only a space, but by the time of Alexander the Great, they had created a symbol that represented zero and was a placeholder. The Mesopotamians also used a sexagesimal system, that is base sixty. It is this system that is used in modern times when measuring time and angles. Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework. The earliest evidence of written mathematics dates back to the ancient Sumerians and the system of metrology from 3000 BC. From around 2500 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.\n\nThe majority of Mesopotamian clay tablets date from 1800 to 1600 BC, and cover topics which include fractions, algebra, quadratic and cubic equations, and the calculation of regular reciprocal pairs. The tablets also include multiplication tables and methods for solving linear and quadratic equations. The Babylonian tablet YBC 7289 gives an approximation of accurate to five decimal places. Babylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 x 6) degrees in a circle, as well as the use of minutes and seconds of arc to denote fractions of a degree. Babylonian advances in mathematics were facilitated by the fact that 60 has many divisors: the reciprocal of any integer which is a multiple of divisors of 60 has a finite expansion in base 60. (In decimal arithmetic, only reciprocals of multiples of 2 and 5 have finite decimal expansions.) Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values, much as in the decimal system. They lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context.\n\nThe history of mathematics cannot with certainty be traced back to any school or period before that of the Ionian Greeks, but the subsequent history may be divided into periods, the distinctions between which are tolerably well marked. Greek mathematics, which originated with the study of geometry, tended from its commencement to be deductive and scientific. Since the fourth century AD, Pythagoras has commonly been given credit for discovering the Pythagorean theorem, a theorem in geometry that states that in a right-angled triangle the area of the square on the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares of the other two sides. The ancient mathematical texts are available with the prior mentioned Ancient Egyptians notation and with Plimpton 322 (Babylonian mathematics c. 1900 BC). The study of mathematics as a subject in its own right begins in the 6th century BC with the Pythagoreans, who coined the term \"mathematics\" from the ancient Greek \"μάθημα\" (\"mathema\"), meaning \"subject of instruction\".\n\nPlato's influence has been especially strong in mathematics and the sciences. He helped to distinguish between pure and applied mathematics by widening the gap between \"arithmetic\", now called number theory and \"logistic\", now called arithmetic. Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Aristotle is credited with what later would be called the law of excluded middle.\n\n\"Abstract Mathematics\" is what treats of magnitude or quantity, absolutely and generally conferred, without regard to any species of particular magnitude, such as Arithmetic and Geometry, In this sense, abstract mathematics is opposed to mixed mathematics; wherein simple and abstract properties, and the relations of quantities primitively considered in mathematics, are applied to sensible objects, and by that means become intermixed with physical considerations; Such are Hydrostatics, Optics, Navigation, &c.\n\nArchimedes is generally considered to be the greatest mathematician of antiquity and one of the greatest of all time. He used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of pi. He also defined the spiral bearing his name, formulae for the volumes of surfaces of revolution and an ingenious system for expressing very large numbers.\nIn the historical development of geometry, the steps in the abstraction of geometry were made by the ancient Greeks. Euclid's Elements being the earliest extant documentation of the axioms of plane geometry— though Proclus tells of an earlier axiomatisation by Hippocrates of Chios. Euclid's \"Elements\" (c. 300 BC) is one of the oldest extant Greek mathematical treatises and consisted of 13 books written in Alexandria; collecting theorems proven by other mathematicians, supplemented by some original work. The document is a successful collection of definitions, postulates (axioms), propositions (theorems and constructions), and mathematical proofs of the propositions. Euclid's first theorem is a lemma that possesses properties of prime numbers. The influential thirteen books cover Euclidean geometry, geometric algebra, and the ancient Greek version of algebraic systems and elementary number theory. It was ubiquitous in the Quadrivium and is instrumental in the development of logic, mathematics, and science.\n\nDiophantus of Alexandria was author of a series of books called \"Arithmetica\", many of which are now lost. These texts deal with solving algebraic equations. Boethius provided a place for mathematics in the curriculum in the 6th century when he coined the term \"quadrivium\" to describe the study of arithmetic, geometry, astronomy, and music. He wrote \"De institutione arithmetica\", a free translation from the Greek of Nicomachus's \"Introduction to Arithmetic\"; \"De institutione musica\", also derived from Greek sources; and a series of excerpts from Euclid's \"Elements\". His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.\n\nThe Greeks employed Attic numeration, which was based on the system of the Egyptians and was later adapted and used by the Romans. Greek numerals one through four were vertical lines, as in the hieroglyphics. The symbol for five was the Greek letter Π (pi), which is the letter of the Greek word for five, \"pente\". Numbers six through nine were \"pente\" with vertical lines next to it. Ten was represented by the letter (Δ) of the word for ten, \"deka\", one hundred by the letter from the word for hundred, etc.\n\nThe Ionian numeration used their entire alphabet including three archaic letters. The numeral notation of the Greeks, though far less convenient than that now in use, was formed on a perfectly regular and scientific plan, and could be used with tolerable effect as an instrument of calculation, to which purpose the Roman system was totally inapplicable. The Greeks divided the twenty-four letters of their alphabet into three classes, and, by adding another symbol to each class, they had characters to represent the units, tens, and hundreds. (Jean Baptiste Joseph Delambre's Astronomie Ancienne, t. ii.)\nThis system appeared in the third century BC, before the letters digamma (Ϝ), koppa (Ϟ), and sampi (Ϡ) became obsolete. When lowercase letters became differentiated from upper case letters, the lower case letters were used as the symbols for notation. Multiples of one thousand were written as the nine numbers with a stroke in front of them: thus one thousand was \",α\", two-thousand was \",β\", etc. M (for μὐριοι, as in \"myriad\") was used to multiply numbers by ten thousand. For example, the number 88,888,888 would be written as M,ηωπη*ηωπη\n\nGreek mathematical reasoning was almost entirely geometric (albeit often used to reason about non-geometric subjects such as number theory), and hence the Greeks had no interest in algebraic symbols. The great exception was Diophantus of Alexandria, the great algebraist. His \"Arithmetica\" was one of the texts to use symbols in equations. It was not completely symbolic, but was much more so than previous books. An unknown number was called s. The square of s was formula_1; the cube was formula_2; the fourth power was formula_3; and the fifth power was formula_4.\n\nThe Chinese used numerals that look much like the tally system. Numbers one through four were horizontal lines. Five was an X between two horizontal lines; it looked almost exactly the same as the Roman numeral for ten. Nowadays, the huāmǎ system is only used for displaying prices in Chinese markets or on traditional handwritten invoices.\n\nIn the history of the Chinese, there were those who were familiar with the sciences of arithmetic, geometry, mechanics, optics, navigation, and astronomy. Mathematics in China emerged independently by the 11th century BC. It is almost certain that the Chinese were acquainted with several geometrical or rather architectural implements; with mechanical machines; that they knew of the characteristic property of the magnetic needle; and were aware that astronomical events occurred in cycles. Chinese of that time had made attempts to classify or extend the rules of arithmetic or geometry which they knew, and to explain the causes of the phenomena with which they were acquainted beforehand. The Chinese independently developed very large and negative numbers, decimals, a place value decimal system, a binary system, algebra, geometry, and trigonometry.\nChinese mathematics made early contributions, including a place value system. The geometrical theorem known to the ancient Chinese were acquainted was applicable in certain cases (namely the ratio of sides). It is that geometrical theorems which can be demonstrated in the quasi-experimental way of superposition were also known to them. In arithmetic their knowledge seems to have been confined to the art of calculation by means of the swan-pan, and the power of expressing the results in writing. Our knowledge of the early attainments of the Chinese, slight though it is, is more complete than in the case of most of their contemporaries. It is thus instructive, and serves to illustrate the fact, that it can be known a nation may possess considerable skill in the applied arts with but our knowledge of the later mathematics on which those arts are founded can be scarce. Knowledge of Chinese mathematics before 254 BC is somewhat fragmentary, and even after this date the manuscript traditions are obscure. Dates centuries before the classical period are generally considered conjectural by Chinese scholars unless accompanied by verified archaeological evidence.\n\nAs in other early societies the focus was on astronomy in order to perfect the agricultural calendar, and other practical tasks, and not on establishing formal systems.The Chinese Board of Mathematics duties were confined to the annual preparation of an almanac, the dates and predictions in which it regulated. Ancient Chinese mathematicians did not develop an axiomatic approach, but made advances in algorithm development and algebra. The achievement of Chinese algebra reached its zenith in the 13th century, when Zhu Shijie invented method of four unknowns.\n\nAs a result of obvious linguistic and geographic barriers, as well as content, Chinese mathematics and that of the mathematics of the ancient Mediterranean world are presumed to have developed more or less independently up to the time when \"The Nine Chapters on the Mathematical Art\" reached its final form, while the \"Writings on Reckoning\" and \"Huainanzi\" are roughly contemporary with classical Greek mathematics. Some exchange of ideas across Asia through known cultural exchanges from at least Roman times is likely. Frequently, elements of the mathematics of early societies correspond to rudimentary results found later in branches of modern mathematics such as geometry or number theory. The Pythagorean theorem for example, has been attested to the time of the Duke of Zhou. Knowledge of Pascal's triangle has also been shown to have existed in China centuries before Pascal, such as by Shen Kuo.\nThe state of trigonometry in China slowly began to change and advance during the Song Dynasty (960–1279), where Chinese mathematicians began to express greater emphasis for the need of spherical trigonometry in calendarical science and astronomical calculations. The polymath Chinese scientist, mathematician and official Shen Kuo (1031–1095) used trigonometric functions to solve mathematical problems of chords and arcs. Sal Restivo writes that Shen's work in the lengths of arcs of circles provided the basis for spherical trigonometry developed in the 13th century by the mathematician and astronomer Guo Shoujing (1231–1316). As the historians L. Gauchet and Joseph Needham state, Guo Shoujing used spherical trigonometry in his calculations to improve the calendar system and Chinese astronomy. The mathematical science of the Chinese would incorporate the work and teaching of Arab missionaries with knowledge of spherical trigonometry who had come to China in the course of the thirteenth century.\n\nAlthough the origin of our present system of numerical notation is ancient, there is no doubt that it was in use among the Hindus over two thousand years ago. The algebraic notation of the Indian mathematician, Brahmagupta, was syncopated. Addition was indicated by placing the numbers side by side, subtraction by placing a dot over the subtrahend (the number to be subtracted), and division by placing the divisor below the dividend, similar to our notation but without the bar. Multiplication, evolution, and unknown quantities were represented by abbreviations of appropriate terms. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, likely evolved over the course of the first millennium AD in India and was transmitted to the west via Islamic mathematics.\n\nDespite their name, Arabic numerals actually started in India. The reason for this misnomer is Europeans saw the numerals used in an Arabic book, \"Concerning the Hindu Art of Reckoning\", by Mohommed ibn-Musa al-Khwarizmi. Al-Khwārizmī wrote several important books on the Hindu–Arabic numerals and on methods for solving equations. His book \"On the Calculation with Hindu Numerals\", written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. Al-Khwarizmi did not claim the numerals as Arabic, but over several Latin translations, the fact that the numerals were Indian in origin was lost. The word \"algorithm\" is derived from the Latinization of Al-Khwārizmī's name, Algoritmi, and the word \"algebra\" from the title of one of his works, \"Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala\" (\"The Compendious Book on Calculation by Completion and Balancing\").\n\nIslamic mathematics developed and expanded the mathematics known to Central Asian civilizations. Al-Khwārizmī gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and Al-Khwārizmī was to teach algebra in an elementary form and for its own sake. Al-Khwārizmī also discussed the fundamental method of \"reduction\" and \"balancing\", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as \"al-jabr\". His algebra was also no longer concerned \"with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study.\" Al-Khwārizmī also studied an equation for its own sake and \"in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems.\"\n\nAl-Karaji, in his treatise \"al-Fakhri\", extends the methodology to incorporate integer powers and integer roots of unknown quantities. The historian of mathematics, F. Woepcke, praised Al-Karaji for being \"the first who introduced the theory of algebraic calculus.\" Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham would develop analytic geometry. Al-Haytham derived the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. Al-Haytham performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. In the late 11th century, Omar Khayyam would develop algebraic geometry, wrote \"Discussions of the Difficulties in Euclid\", and wrote on the general geometric solution to cubic equations. Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals.\n\nMany Greek and Arabic texts on mathematics were then translated into Latin, which led to further development of mathematics in medieval Europe. In the 12th century, scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's and the complete text of Euclid's \"Elements\". One of the European books that advocated using the numerals was \"Liber Abaci\", by Leonardo of Pisa, better known as Fibonacci. \"Liber Abaci\" is better known for the mathematical problem Fibonacci wrote in it about a population of rabbits. The growth of the population ended up being a Fibonacci sequence, where a term is the sum of the two preceding terms.\n\nAbū al-Hasan ibn Alī al-Qalasādī (1412–1482) was the last major medieval Arab algebraist, who improved on the algebraic notation earlier used by Ibn al-Yāsamīn in the 12th century and, in the Maghreb, by Ibn al-Banna in the 13th century. In contrast to the syncopated notations of their predecessors, Diophantus and Brahmagupta, which lacked symbols for mathematical operations, al-Qalasadi's algebraic notation was the first to have symbols for these functions and was thus \"the first steps toward the introduction of algebraic symbolism.\" He represented mathematical symbols using characters from the Arabic alphabet.\n\n\nThe 14th century saw the development of new mathematical concepts to investigate a wide range of problems. The two widely used arithmetic symbols are addition and subtraction, + and −. The plus sign was used by 1360 by Nicole Oresme in his work \"Algorismus proportionum\". It is thought an abbreviation for \"et\", meaning \"and\" in Latin, in much the same way the ampersand sign also began as \"et\". Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of the distance covered by a body undergoing uniformly accelerated motion, asserting that the area under the line depicting the constant acceleration and represented the total distance traveled. The minus sign was used in 1489 by Johannes Widmann in \"Mercantile Arithmetic\" or \"Behende und hüpsche Rechenung auff allen Kauffmanschafft,\". Widmann used the minus symbol with the plus symbol, to indicate deficit and surplus, respectively. In \"Summa de arithmetica, geometria, proportioni e proportionalità\", Luca Pacioli used symbols for plus and minus symbols and contained algebra.\n\nIn the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating \"n\"th roots. In 1533, Regiomontanus's table of sines and cosines were published. Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book \"Ars Magna\", together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. The radical symbol for square root was introduced by Christoph Rudolff. Michael Stifel's important work \"Arithmetica integra\" contained important innovations in mathematical notation. In 1556, Niccolò Tartaglia used parentheses for precedence grouping. In 1557 Robert Recorde published The Whetstone of Witte which used the equal sign (=) as well as plus and minus signs for the English reader. In 1564, Gerolamo Cardano analyzed games of chance beginning the early stages of probability theory. In 1572 Rafael Bombelli published his \"L'Algebra\" in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations. Simon Stevin's book \"De Thiende\" ('the art of tenths'), published in Dutch in 1585, contained a systematic treatment of decimal notation, which influenced all later work on the real number system. The New algebra (1591) of François Viète introduced the modern notational manipulation of algebraic expressions. For navigation and accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus coin the word \"trigonometry\", publishing his \"Trigonometria\" in 1595.\n\nJohn Napier is best known as the inventor of logarithms and made common the use of the decimal point in arithmetic and mathematics. After Napier, Edmund Gunter created the logarithmic scales (lines, or rules) upon which slide rules are based, it was William Oughtred who used two such scales sliding by one another to perform direct multiplication and division; and he is credited as the inventor of the slide rule in 1622. In 1631 Oughtred introduced the multiplication sign (×) his proportionality sign, and abbreviations \"sin\" and \"cos\" for the sine and cosine functions. Albert Girard also used the abbreviations 'sin', 'cos' and 'tan' for the trigonometric functions in his treatise.\n\nJohannes Kepler was one of the pioneers of the mathematical applications of infinitesimals. René Descartes is credited as the father of analytical geometry, the bridge between algebra and geometry, crucial to the discovery of infinitesimal calculus and analysis. In the 17th century, Descartes introduced Cartesian co-ordinates which allowed the development of analytic geometry. Blaise Pascal influenced mathematics throughout his life. His \"Traité du triangle arithmétique\" (\"Treatise on the Arithmetical Triangle\") of 1653 described a convenient tabular presentation for binomial coefficients. Pierre de Fermat and Blaise Pascal would investigate probability. John Wallis introduced the infinity symbol. He similarly used this notation for infinitesimals. In 1657, Christiaan Huygens published the treatise on probability, \"On Reasoning in Games of Chance\". \n\nJohann Rahn introduced the division symbol (obelus) and the therefore sign in 1659. William Jones used π in \"Synopsis palmariorum mathesios\" in 1706 because it is the letter of the Greek word perimetron (περιμετρον), which means perimeter in Greek. This usage was popularized in 1737 by Euler. In 1734, Pierre Bouguer used double horizontal bar below the inequality sign.\n\nThe study of linear algebra emerged from the study of determinants, which were used to solve systems of linear equations. Calculus had two main systems of notation, each created by one of the creators: that developed by Isaac Newton and the notation developed by Gottfried Leibniz. Leibniz's is the notation used most often today. Newton's was simply a dot or dash placed above the function. In modern usage, this notation generally denotes derivatives of physical quantities with respect to time, and is used frequently in the science of mechanics. Leibniz, on the other hand, used the letter \"d\" as a prefix to indicate differentiation, and introduced the notation representing derivatives as if they were a special type of fraction. This notation makes explicit the variable with respect to which the derivative of the function is taken. Leibniz also created the integral symbol. The symbol is an elongated S, representing the Latin word \"Summa\", meaning \"sum\". When finding areas under curves, integration is often illustrated by dividing the area into infinitely many tall, thin rectangles, whose areas are added. Thus, the integral symbol is an elongated s, for sum.\n\nLetters of the alphabet in this time were to be used as symbols of quantity; and although much diversity existed with respect to the choice of letters, there were to be several universally recognized rules in the following history. Here thus in the history of equations the first letters of the alphabet were indicatively known as coefficients, the last letters the s (an \"incerti ordinis\"). In algebraic geometry, again, a similar rule was to be observed, the last letters of the alphabet there denoting the variable or current coordinates. Certain letters, such as formula_5, formula_6, etc., were by universal consent appropriated as symbols of the frequently occurring numbers 3.14159 ..., and 2.7182818 ..., etc., and their use in any other acceptation was to be avoided as much as possible. Letters, too, were to be employed as symbols of operation, and with them other previously mentioned arbitrary operation characters. The letters formula_7, elongated formula_8 were to be appropriated as operative symbols in the differential calculus and integral calculus, formula_9 and ∑ in the calculus of differences. In functional notation, a letter, as a symbol of operation, is combined with another which is regarded as a symbol of quantity.\n\nBeginning in 1718, Thomas Twinin used the division slash (solidus), deriving it from the earlier Arabic horizontal fraction bar. Pierre-Simon, marquis de Laplace developed the widely used Laplacian differential operator. In 1750, Gabriel Cramer developed \"Cramer's Rule\" for solving linear systems.\n\nLeonhard Euler was one of the most prolific mathematicians in history, and also a prolific inventor of canonical notation. His contributions include his use of \"e\" to represent the base of natural logarithms. It is not known exactly why formula_6 was chosen, but it was probably because the four letters of the alphabet were already commonly used to represent variables and other constants. Euler used formula_5 to represent pi consistently. The use of formula_5 was suggested by William Jones, who used it as shorthand for perimeter. Euler used formula_13 to represent the square root of negative one, although he earlier used it as an \"infinite number.\" For summation, Euler used sigma, Σ. For functions, Euler used the notation formula_14 to represent a function of formula_15. In 1730, Euler wrote the gamma function. In 1736, Euler produces his paper on the Seven Bridges of Königsberg initiating the study of graph theory.\n\nThe mathematician, William Emerson would develop the proportionality sign. Much later in the abstract expressions of the value of various proportional phenomena, the parts-per notation would become useful as a set of pseudo units to describe small values of miscellaneous dimensionless quantities. Marquis de Condorcet, in 1768, advanced the partial differential sign. In 1771, Alexandre-Théophile Vandermonde deduced the importance of topological features when discussing the properties of knots related to the geometry of position. Between 1772 and 1788, Joseph-Louis Lagrange re-formulated the formulas and calculations of Classical \"Newtonian\" mechanics, called Lagrangian mechanics. The prime symbol for derivatives was also made by Lagrange.\n\nAt the turn of the 19th century, Carl Friedrich Gauss developed the identity sign for congruence relation and, in Quadratic reciprocity, the integral part. Gauss contributed functions of complex variables, in geometry, and on the convergence of series. He gave the satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law. Gauss developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy. He would also develop the product sign. Also in this time, Niels Henrik Abel and Évariste Galois conducted their work on the solvability of equations, linking group theory and field theory.\n\nAfter the 1800s, Christian Kramp would promote factorial notation during his research in generalized factorial function which applied to non-integers. Joseph Diaz Gergonne introduced the set inclusion signs. Peter Gustav Lejeune Dirichlet developed Dirichlet \"L\"-functions to give the proof of Dirichlet's theorem on arithmetic progressions and began analytic number theory. In 1828, Gauss proved his Theorema Egregium (\"remarkable theorem\" in Latin), establishing property of surfaces. In the 1830s, George Green developed Green's function. In 1829. Carl Gustav Jacob Jacobi publishes Fundamenta nova theoriae functionum ellipticarum with his elliptic theta functions. By 1841, Karl Weierstrass, the \"father of modern analysis\", elaborated on the concept of absolute value and the determinant of a matrix.\n\nMatrix notation would be more fully developed by Arthur Cayley in his three papers, on subjects which had been suggested by reading the Mécanique analytique of Lagrange and some of the works of Laplace. Cayley defined matrix multiplication and matrix inverses. Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".\nWilliam Rowan Hamilton would introduce the nabla symbol for vector differentials. This was previously used by Hamilton as a general-purpose operator sign. Hamilton reformulated Newtonian mechanics, now called Hamiltonian mechanics. This work has proven central to the modern study of classical field theories such as electromagnetism. This was also important to the development of quantum mechanics. In mathematics, he is perhaps best known as the inventor of quaternion notation and biquaternions. Hamilton also introduced the word \"tensor\" in 1846. James Cockle would develop the tessarines and, in 1849, coquaternions. In 1848, James Joseph Sylvester introduced into matrix algebra the term matrix.\n\nIn 1864 James Clerk Maxwell reduced all of the then current knowledge of electromagnetism into a linked set of differential equations with 20 equations in 20 variables, contained in \"A Dynamical Theory of the Electromagnetic Field\". (See Maxwell's equations.) The method of calculation which it is necessary to employ was given by Lagrange, and afterwards developed, with some modifications, by Hamilton's equations. It is usually referred to as Hamilton's principle; when the equations in the original form are used they are known as Lagrange's equations. In 1871 Richard Dedekind called a set of real or complex numbers which is closed under the four arithmetic operations a field. In 1873 Maxwell presented \"A Treatise on Electricity and Magnetism\".\n\nIn 1878, William Kingdon Clifford published his Elements of Dynamic. Clifford developed split-biquaternions, which he called \"algebraic motors\". Clifford obviated quaternion study by separating the dot product and cross product of two vectors from the complete quaternion notation. This approach made vector calculus available to engineers and others working in three dimensions and skeptical of the lead–lag effect in the fourth dimension. The common vector notations are used when working with vectors which are spatial or more abstract members of vector spaces, while angle notation (or phasor notation) is a notation used in electronics.\n\nIn 1881, Leopold Kronecker defined what he called a \"domain of rationality\", which is a field extension of the field of rational numbers in modern terms. In 1882, wrote the book titled \"Linear Algebra\". Lord Kelvin's aetheric atom theory (1860s) led Peter Guthrie Tait, in 1885, to publish a topological table of knots with up to ten crossings known as the Tait conjectures. In 1893, Heinrich M. Weber gave the clear definition of an abstract field. Tensor calculus was developed by Gregorio Ricci-Curbastro between 1887–96, presented in 1892 under the title \"absolute differential calculus\", and the contemporary usage of \"tensor\" was stated by Woldemar Voigt in 1898. In 1895, Henri Poincaré published \"Analysis Situs\". In 1897, Charles Proteus Steinmetz would publish , with the assistance of Ernst J. Berg.\n\nIn 1895 Giuseppe Peano issued his \"Formulario mathematico\", an effort to digest mathematics into terse text based on special symbols. He would provide a definition of a vector space and linear map. He would also introduce the intersection sign, the union sign, the membership sign (is an element of), and existential quantifier (there exists). Peano would pass to Bertrand Russell his work in 1900 at a Paris conference; it so impressed Russell that Russell too was taken with the drive to render mathematics more concisely. The result was Principia Mathematica written with Alfred North Whitehead. This treatise marks a watershed in modern literature where symbol became dominant. Ricci-Curbastro and Tullio Levi-Civita popularized the tensor index notation around 1900.\n\nAt the beginning of this period, Felix Klein's \"Erlangen program\" identified the underlying theme of various geometries, defining each of them as the study of properties invariant under a given group of symmetries. This level of abstraction revealed connections between geometry and abstract algebra. Georg Cantor would introduce the aleph symbol for cardinal numbers of transfinite sets. His notation for the cardinal numbers was the Hebrew letter formula_16 (aleph) with a natural number subscript; for the ordinals he employed the Greek letter ω (omega). This notation is still in use today in ordinal notation of a finite sequence of symbols from a finite alphabet which names an ordinal number according to some scheme which gives meaning to the language. His theory created a great deal of controversy. Cantor would, in his study of Fourier series, consider point sets in Euclidean space.\n\nAfter the turn of the 20th century, Josiah Willard Gibbs would in physical chemistry introduce middle dot for dot product and the multiplication sign for cross products. He would also supply notation for the scalar and vector products, which was introduced in \"Vector Analysis\". In 1904, Ernst Zermelo promotes axiom of choice and his proof of the well-ordering theorem. Bertrand Russell would shortly afterward introduce logical disjunction (OR) in 1906. Also in 1906, Poincaré would publish \"On the Dynamics of the Electron\" and Maurice Fréchet introduced metric space. Later, Gerhard Kowalewski and Cuthbert Edmund Cullis would successively introduce matrices notation, parenthetical matrix and box matrix notation respectively. After 1907, mathematicians studied knots from the point of view of the knot group and invariants from homology theory. In 1908, Joseph Wedderburn's structure theorems were formulated for finite-dimensional algebras over a field. Also in 1908, Ernst Zermelo proposed \"definite\" property and the first axiomatic set theory, Zermelo set theory. In 1910 Ernst Steinitz published the influential paper \"Algebraic Theory of Fields\". In 1911, Steinmetz would publish \"Theory and Calculation of Transient Electric Phenomena and Oscillations\".\nAlbert Einstein, in 1916, introduced the Einstein notation which summed over a set of indexed terms in a formula, thus exerting notational brevity. Arnold Sommerfeld would create the contour integral sign in 1917. Also in 1917, Dimitry Mirimanoff proposes axiom of regularity. In 1919, Theodor Kaluza would solve general relativity equations using five dimensions, the results would have electromagnetic equations emerge. This would be published in 1921 in \"Zum Unitätsproblem der Physik\". In 1922, Abraham Fraenkel and Thoralf Skolem independently proposed replacing the axiom schema of specification with the axiom schema of replacement. Also in 1922, Zermelo–Fraenkel set theory was developed. In 1923, Steinmetz would publish \"Four Lectures on Relativity and Space\". Around 1924, Jan Arnoldus Schouten would develop the modern notation and formalism for the Ricci calculus framework during the absolute differential calculus applications to general relativity and differential geometry in the early twentieth century. In 1925, Enrico Fermi would describe a system comprising many identical particles that obey the Pauli exclusion principle, afterwards developing a diffusion equation (Fermi age equation). In 1926, Oskar Klein would develop the Kaluza–Klein theory. In 1928, Emil Artin abstracted ring theory with Artinian rings. In 1933, Andrey Kolmogorov introduces the \"Kolmogorov axioms\". In 1937, Bruno de Finetti deduced the \"operational subjective\" concept.\n\nMathematical abstraction began as a process of extracting the underlying essence of a mathematical concept, removing any dependence on real world objects with which it might originally have been connected, and generalizing it so that it has wider applications or matching among other abstract descriptions of equivalent phenomena. Two abstract areas of modern mathematics are category theory and model theory. Bertrand Russell, said, \"Ordinary language is totally unsuited for expressing what physics really asserts, since the words of everyday life are not sufficiently abstract. Only mathematics and mathematical logic can say as little as the physicist means to say\". Though, one can substituted mathematics for real world objects, and wander off through equation after equation, and can build a concept structure which has no relation to reality.\n\nSymbolic logic studies the purely formal properties of strings of symbols. The interest in this area springs from two sources. First, the notation used in symbolic logic can be seen as representing the words used in philosophical logic. Second, the rules for manipulating symbols found in symbolic logic can be implemented on a computing machine. Symbolic logic is usually divided into two subfields, propositional logic and predicate logic. Other logics of interest include temporal logic, modal logic and fuzzy logic. The area of symbolic logic called propositional logic, also called \"propositional calculus\", studies the properties of sentences formed from constants and logical operators. The corresponding logical operations are known, respectively, as conjunction, disjunction, material conditional, biconditional, and negation. These operators are denoted as keywords and by symbolic notation.\n\nSome of the introduced mathematical logic notation during this time included the set of symbols used in Boolean algebra. This was created by George Boole in 1854. Boole himself did not see logic as a branch of mathematics, but it has come to be encompassed anyway. Symbols found in Boolean algebra include formula_17 (AND), formula_18 (OR), and formula_19 (\"not\"). With these symbols, and letters to represent different truth values, one can make logical statements such as formula_20, that is \"(\"a\" is true OR \"a\" is \"not\" true) is true\", meaning it is true that \"a\" is either true or not true (i.e. false). Boolean algebra has many practical uses as it is, but it also was the start of what would be a large set of symbols to be used in logic. Predicate logic, originally called \"predicate calculus\", expands on propositional logic by the introduction of variables and by sentences containing variables, called predicates. In addition, predicate logic allows quantifiers. With these logic symbols and additional quantifiers from predicate logic, valid proofs can be made that are irrationally artificial, but syntactical.\n\nWhile proving his incompleteness theorems, Kurt Gödel created an alternative to the symbols normally used in logic. He used Gödel numbers, which were numbers that represented operations with set numbers, and variables with the prime numbers greater than 10. With Gödel numbers, logic statements can be broken down into a number sequence. Gödel then took this one step farther, taking the \"n\" prime numbers and putting them to the power of the numbers in the sequence. These numbers were then multiplied together to get the final product, giving every logic statement its own number.\n\nAbstraction of notation is an ongoing process and the historical development of many mathematical topics exhibits a progression from the concrete to the abstract. Various set notations would be developed for fundamental object sets. Around 1924, David Hilbert and Richard Courant published \"Methods of mathematical physics. Partial differential equations\". In 1926, Oskar Klein and Walter Gordon proposed the Klein–Gordon equation to describe relativistic particles. The first formulation of a quantum theory describing radiation and matter interaction is due to Paul Adrien Maurice Dirac, who, during 1920, was first able to compute the coefficient of spontaneous emission of an atom. In 1928, the relativistic Dirac equation was formulated by Dirac to explain the behavior of the relativistically moving electron. Dirac described the quantification of the electromagnetic field as an ensemble of harmonic oscillators with the introduction of the concept of creation and annihilation operators of particles. In the following years, with contributions from Wolfgang Pauli, Eugene Wigner, Pascual Jordan, and Werner Heisenberg, and an elegant formulation of quantum electrodynamics due to Enrico Fermi, physicists came to believe that, in principle, it would be possible to perform any computation for any physical process involving photons and charged particles.\n\nIn 1931, Alexandru Proca developed the Proca equation (Euler–Lagrange equation) for the vector meson theory of nuclear forces and the relativistic quantum field equations. John Archibald Wheeler in 1937 develops S-matrix. Studies by Felix Bloch with Arnold Nordsieck, and Victor Weisskopf, in 1937 and 1939, revealed that such computations were reliable only at a first order of perturbation theory, a problem already pointed out by Robert Oppenheimer. At higher orders in the series infinities emerged, making such computations meaningless and casting serious doubts on the internal consistency of the theory itself. With no solution for this problem known at the time, it appeared that a fundamental incompatibility existed between special relativity and quantum mechanics.\n\nIn the 1930s, the double-struck capital Z for integer number sets was created by Edmund Landau. Nicolas Bourbaki created the double-struck capital Q for rational number sets. In 1935, Gerhard Gentzen made universal quantifiers. In 1936, Tarski's undefinability theorem is stated by Alfred Tarski and proved. In 1938, Gödel proposes the constructible universe in the paper \"The Consistency of the Axiom of Choice and of the Generalized Continuum-Hypothesis\". André Weil and Nicolas Bourbaki would develop the empty set sign in 1939. That same year, Nathan Jacobson would coin the double-struck capital C for complex number sets.\n\nAround the 1930s, Voigt notation would be developed for multilinear algebra as a way to represent a symmetric tensor by reducing its order. Schönflies notation became one of two conventions used to describe point groups (the other being Hermann–Mauguin notation). Also in this time, van der Waerden notation became popular for the usage of two-component spinors (Weyl spinors) in four spacetime dimensions. Arend Heyting would introduce Heyting algebra and Heyting arithmetic.\n\nThe arrow, e.g., →, was developed for function notation in 1936 by Øystein Ore to denote images of specific elements. Later, in 1940, it took its present form, e.g., \"f: X → Y\", through the work of Witold Hurewicz. Werner Heisenberg, in 1941, proposed the S-matrix theory of particle interactions.\nBra–ket notation (Dirac notation) is a standard notation for describing quantum states, composed of angle brackets and vertical bars. It can also be used to denote abstract vectors and linear functionals. It is so called because the inner product (or dot product on a complex vector space) of two states is denoted by a bra|ket consisting of a left part, ⟨\"φ\"|, and a right part, |\"ψ\"⟩. The notation was introduced in 1939 by Paul Dirac, though the notation has precursors in Grassmann's use of the notation [\"φ\"|\"ψ\"] for his inner products nearly 100 years previously.\n\nBra–ket notation is widespread in quantum mechanics: almost every phenomenon that is explained using quantum mechanics—including a large portion of modern physics—is usually explained with the help of bra–ket notation. The notation establishes an encoded abstract representation-independence, producing a versatile specific representation (e.g., \"x\", or \"p\", or eigenfunction base) without much , or excessive reliance on, the nature of the linear spaces involved. The overlap expression ⟨\"φ\"|\"ψ\"⟩ is typically interpreted as the probability amplitude for the state \"ψ\" to collapse into the state \"ϕ\". The Feynman slash notation (Dirac slash notation) was developed by Richard Feynman for the study of Dirac fields in quantum field theory.\n\nIn 1948, Valentine Bargmann and Eugene Wigner proposed the relativistic Bargmann–Wigner equations to describe free particles and the equations are in the form of multi-component spinor field wavefunctions. In 1950, William Vallance Douglas Hodge presented \"The topological invariants of algebraic varieties\" at the Proceedings of the International Congress of Mathematicians. Between 1954 and 1957, Eugenio Calabi worked on the Calabi conjecture for Kähler metrics and the development of Calabi–Yau manifolds. In 1957, Tullio Regge formulated the mathematical property of potential scattering in the Schrödinger equation. Stanley Mandelstam, along with Regge, did the initial development of the Regge theory of strong interaction phenomenology. In 1958, Murray Gell-Mann and Richard Feynman, along with George Sudarshan and Robert Marshak, deduced the chiral structures of the weak interaction in physics. Geoffrey Chew, along with others, would promote matrix notation for the strong interaction, and the associated bootstrap principle, in 1960. In the 1960s, set-builder notation was developed for describing a set by stating the properties that its members must satisfy. Also in the 1960s, tensors are abstracted within category theory by means of the concept of monoidal category. Later, multi-index notation eliminates conventional notions used in multivariable calculus, partial differential equations, and the theory of distributions, by abstracting the concept of an integer index to an ordered tuple of indices.\n\nIn the modern mathematics of special relativity, electromagnetism and wave theory, the d'Alembert operator is the Laplace operator of Minkowski space. The Levi-Civita symbol is used in tensor calculus.\n\nAfter the full Lorentz covariance formulations that were finite at any order in a perturbation series of quantum electrodynamics, Sin-Itiro Tomonaga, Julian Schwinger and Richard Feynman were jointly awarded with a Nobel prize in physics in 1965. Their contributions, and those of Freeman Dyson, were about covariant and gauge invariant formulations of quantum electrodynamics that allow computations of observables at any order of perturbation theory. Feynman's mathematical technique, based on his diagrams, initially seemed very different from the field-theoretic, operator-based approach of Schwinger and Tomonaga, but Freeman Dyson later showed that the two approaches were equivalent. Renormalization, the need to attach a physical meaning at certain divergences appearing in the theory through integrals, has subsequently become one of the fundamental aspects of quantum field theory and has come to be seen as a criterion for a theory's general acceptability. Quantum electrodynamics has served as the model and template for subsequent quantum field theories. Peter Higgs, Jeffrey Goldstone, and others, Sheldon Glashow, Steven Weinberg and Abdus Salam independently showed how the weak nuclear force and quantum electrodynamics could be merged into a single electroweak force. In the late 1960s, the particle zoo was composed of the then known elementary particles before the discovery of quarks.\n\nA step towards the Standard Model was Sheldon Glashow's discovery, in 1960, of a way to combine the electromagnetic and weak interactions. In 1967, Steven Weinberg and Abdus Salam incorporated the Higgs mechanism into Glashow's electroweak theory, giving it its modern form. The Higgs mechanism is believed to give rise to the masses of all the elementary particles in the Standard Model. This includes the masses of the W and Z bosons, and the masses of the fermions - i.e. the quarks and leptons. Also in 1967, Bryce DeWitt published his equation under the name \"\"Einstein–Schrödinger equation\" (later renamed the \"Wheeler–DeWitt equation\"\"). In 1969, Yoichiro Nambu, Holger Bech Nielsen, and Leonard Susskind described space and time in terms of strings. In 1970, Pierre Ramond develop two-dimensional supersymmetries. Michio Kaku and Keiji Kikkawa would afterwards formulate string variations. In 1972, Michael Artin, Alexandre Grothendieck, Jean-Louis Verdier propose the Grothendieck universe.\n\nAfter the neutral weak currents caused by boson exchange were discovered at CERN in 1973, the electroweak theory became widely accepted and Glashow, Salam, and Weinberg shared the 1979 Nobel Prize in Physics for discovering it. The theory of the strong interaction, to which many contributed, acquired its modern form around 1973–74. With the establishment of quantum chromodynamics, a finalized a set of fundamental and exchange particles, which allowed for the establishment of a \"standard model\" based on the mathematics of gauge invariance, which successfully described all forces except for gravity, and which remains generally accepted within the domain to which it is designed to be applied. In the late 1970s, William Thurston introduced hyperbolic geometry into the study of knots with the hyperbolization theorem. The orbifold notation system, invented by Thurston, has been developed for representing types of symmetry groups in two-dimensional spaces of constant curvature. In 1978, Shing-Tung Yau deduced that the Calabi conjecture have Ricci flat metrics. In 1979, Daniel Friedan showed that the equations of motions of string theory are abstractions of Einstein equations of General Relativity.\n\nThe first superstring revolution is composed of mathematical equations developed between 1984 and 1986. In 1984, Vaughan Jones deduced the Jones polynomial and subsequent contributions from Edward Witten, Maxim Kontsevich, and others, revealed deep connections between knot theory and mathematical methods in statistical mechanics and quantum field theory. According to string theory, all particles in the \"particle zoo\" have a common ancestor, namely a vibrating string. In 1985, Philip Candelas, Gary Horowitz, Andrew Strominger, and Edward Witten would publish \"Vacuum configurations for superstrings\" Later, the tetrad formalism (tetrad index notation) would be introduced as an approach to general relativity that replaces the choice of a coordinate basis by the less restrictive choice of a local basis for the tangent bundle.\n\nIn the 1990s, Roger Penrose would propose Penrose graphical notation (tensor diagram notation) as a, usually handwritten, visual depiction of multilinear functions or tensors. Penrose would also introduce abstract index notation. In 1995, Edward Witten suggested M-theory and subsequently used it to explain some observed dualities, initiating the second superstring revolution.\nJohn Conway would further various notations, including the Conway chained arrow notation, the Conway notation of knot theory, and the Conway polyhedron notation. The Coxeter notation system classifies symmetry groups, describing the angles between with fundamental reflections of a Coxeter group. It uses a bracketed notation, with modifiers to indicate certain subgroups. The notation is named after H. S. M. Coxeter and Norman Johnson more comprehensively defined it.\n\nCombinatorial LCF notation has been developed for the representation of cubic graphs that are Hamiltonian. The cycle notation is the convention for writing down a permutation in terms of its constituent cycles. This is also called circular notation and the permutation called a \"cyclic\" or \"circular\" permutation.\n\nIn 1931, IBM produces the IBM 601 Multiplying Punch; it is an electromechanical machine that could read two numbers, up to 8 digits long, from a card and punch their product onto the same card. In 1934, Wallace Eckert used a rigged IBM 601 Multiplying Punch to automate the integration of differential equations. In 1936, Alan Turing publishes \"On Computable Numbers, With an Application to the Entscheidungsproblem\". John von Neumann, pioneer of the digital computer and of computer science, in 1945, writes the incomplete \"First Draft of a Report on the EDVAC\". In 1962, Kenneth E. Iverson developed an integral part notation that became known as Iverson Notation for manipulating arrays that he taught to his students, and described in his book \"A Programming Language\". In 1970, E.F. Codd proposed relational algebra as a relational model of data for database query languages. In 1971, Stephen Cook publishes \"The complexity of theorem proving procedures\" In the 1970s within computer architecture, Quote notation was developed for a representing number system of rational numbers. Also in this decade, the Z notation (just like the APL language, long before it) uses many non-ASCII symbols, the specification includes suggestions for rendering the Z notation symbols in ASCII and in LaTeX. There are presently various C mathematical functions (Math.h) and numerical libraries. They are libraries used in software development for performing numerical calculations. These calculations can be handled by symbolic executions; analyzing a program to determine what inputs cause each part of a program to execute. Mathematica and SymPy are examples of computational software programs based on symbolic mathematics.\n\nIn the history of mathematical notation, ideographic symbol notation has come full circle with the rise of computer visualization systems. The notations can be applied to abstract visualizations, such as for rendering some projections of a Calabi-Yau manifold. Examples of abstract visualization which properly belong to the mathematical imagination can be found in computer graphics. The need for such models abounds, for example, when the measures for the subject of study are actually random variables and not really ordinary mathematical functions.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "42719930", "url": "https://en.wikipedia.org/wiki?curid=42719930", "title": "Idempotent relation", "text": "Idempotent relation\n\nIn mathematics, an idempotent binary relation \"R\" ⊆ \"X\" × \"X\" is one for which \"R\" ∘ \"R\" = \"R\". This notion generalizes that of an idempotent function to relations. Each idempotent relation is necessarily transitive, as the latter means \"R\" ∘ \"R\" ⊆ \"R\".\n\nFor example, the relation < on ℚ is idempotent. In contrast, < on ℤ is not, since (<) ∘ (<) ⊇ (<) does not hold: e.g. 1 < 2, but 1 < \"x\" < 2 is false for every \"x\" ∈ ℤ.\n\nIdempotent relations have been used as an example to illustrate the application of Mechanized Formalisation\nof mathematics using the interactive theorem prover Isabelle/HOL. Besides checking the mathematical properties of finite idempotent relations, an algorithm for counting the number of idempotent relations has been derived in Isabelle/HOL.\n"}
{"id": "493259", "url": "https://en.wikipedia.org/wiki?curid=493259", "title": "Index of logarithm articles", "text": "Index of logarithm articles\n\nThis is a list of logarithm topics, by Wikipedia page. See also the list of exponential topics.\n"}
{"id": "39999171", "url": "https://en.wikipedia.org/wiki?curid=39999171", "title": "Level (logarithmic quantity)", "text": "Level (logarithmic quantity)\n\nIn the International System of Quantities, the level of a quantity is the logarithm of the ratio of the value of that quantity to a reference value of the same quantity. Examples are the various types of sound level: sound power level (literally, the level of the sound power, abbreviated SWL), sound exposure level (SEL), sound pressure level (SPL) and particle velocity level (SVL).\n\nLevel and its units are defined in ISO 80000-3.\n\nLevel of a quantity \"Q\", denoted \"L\", is defined by\nwhere\n\nThe level of a \"root-power\" quantity (also known as a \"field\" quantity), denoted \"L\", is defined by \nwhere\nFor the level of a root-power quantity, the base of the logarithm is .\n\nLevel of a \"power\" quantity, denoted \"L\", is defined by\nwhere\nFor the level of a power quantity, the base of the logarithm is .\n\nThe neper, bel, and decibel (one tenth of a bel) are units of level that are often applied to such quantities as power, intensity, or gain. The neper, bel, and decibel are defined by\n\nIf \"F\" is a root-power quantity:\n\nIf \"P\" is a power quantity:\n\nIf the power quantity \"P\" is proportional to \"F\", and if the reference value of the power quantity, \"P\", is in the same proportion to \"F\", the levels \"L\" and \"L\" are equal.\n\nThe octave is a unit of level (specifically \"frequency level\", for ) though that concept is seldom seen outside of the ANSI standard that defines it. A semitone is one twelfth of an octave. A cent is one hundredth of a semitone.\n\n"}
{"id": "53513426", "url": "https://en.wikipedia.org/wiki?curid=53513426", "title": "Lie group integrator", "text": "Lie group integrator\n\nA Lie group integrator is a numerical integration method for differential equations built from coordinate-independent operations such as Lie group actions on a manifold. They have been used for the animation and control of vehicles in computer graphics and control systems/artificial intelligence research. These tasks are particularly difficult because they feature nonholonomic constraints.\n\n"}
{"id": "351549", "url": "https://en.wikipedia.org/wiki?curid=351549", "title": "List of algebraic geometry topics", "text": "List of algebraic geometry topics\n\nThis is a list of algebraic geometry topics, by Wikipedia page.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1168506", "url": "https://en.wikipedia.org/wiki?curid=1168506", "title": "List of algebraic number theory topics", "text": "List of algebraic number theory topics\n\nThis is a list of algebraic number theory topics.\n\nThese topics are basic to the field, either as prototypical examples, or as basic objects of study.\n\n\n\n\n\n"}
{"id": "2920840", "url": "https://en.wikipedia.org/wiki?curid=2920840", "title": "List of combinatorial computational geometry topics", "text": "List of combinatorial computational geometry topics\n\nList of combinatorial computational geometry topics enumerates the topics of computational geometry that states problems in terms of geometric objects as discrete entities and hence the methods of their solution are mostly theories and algorithms of combinatorial character.\n\nSee List of numerical computational geometry topics for another flavor of computational geometry that deals with geometric objects as continuous entities and applies methods and algorithms of nature characteristic to numerical analysis.\n\n\n\n\n\n\n"}
{"id": "345396", "url": "https://en.wikipedia.org/wiki?curid=345396", "title": "List of differential geometry topics", "text": "List of differential geometry topics\n\nThis is a list of differential geometry topics. See also glossary of differential and metric geometry and list of Lie group topics.\n\n\n\n\"See also multivariable calculus, list of multivariable calculus topics\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "32869192", "url": "https://en.wikipedia.org/wiki?curid=32869192", "title": "List of dualities", "text": "List of dualities\n\nIn mathematics, a duality, generally speaking, translates concepts, theorems or mathematical structures into other concepts, theorems or structures, in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of \"A\" is \"B\", then the dual of \"B\" is \"A\".\n\n\n\n\n"}
{"id": "33900798", "url": "https://en.wikipedia.org/wiki?curid=33900798", "title": "List of formulas in elementary geometry", "text": "List of formulas in elementary geometry\n\nThis is a short list of some common mathematical shapes and figures and the formulas that describe them.\n"}
{"id": "28311992", "url": "https://en.wikipedia.org/wiki?curid=28311992", "title": "List of genetic algorithm applications", "text": "List of genetic algorithm applications\n\nThis is a list of genetic algorithm (GA) applications.\n\n\n\n\n\n\n\n"}
{"id": "53264013", "url": "https://en.wikipedia.org/wiki?curid=53264013", "title": "List of geodesic polyhedra and Goldberg polyhedra", "text": "List of geodesic polyhedra and Goldberg polyhedra\n\nThis is a list of selected geodesic polyhedra and Goldberg polyhedra, two infinite classes of polyhedra. Geodesic polyhedra and Goldberg polyhedra are duals of each other. The geodesic and Goldberg polyhedra are parameterized by integers \"m\" and \"n\", with formula_1 and formula_2. \"T\" is the triangulation number, which is equal to formula_3.\n\n"}
{"id": "22609470", "url": "https://en.wikipedia.org/wiki?curid=22609470", "title": "List of mathematical abbreviations", "text": "List of mathematical abbreviations\n\nThis article is a listing of abbreviated names of mathematical functions, function-like operators and other mathematical terminology.\n\n\n"}
{"id": "46914411", "url": "https://en.wikipedia.org/wiki?curid=46914411", "title": "List of mathematical artists", "text": "List of mathematical artists\n\nThis is a list of artists who actively explored mathematics in their artworks. Art forms practised by these artists include painting, sculpture, architecture, textiles and origami.\n\nSome artists such as Piero della Francesca and Luca Pacioli went so far as to write books on mathematics in art. Della Francesca wrote books on solid geometry and the emerging field of perspective, including \"De Prospectiva Pingendi (On Perspective for Painting)\", \"Trattato d’Abaco (Abacus Treatise)\", and \"De corporibus regularibus (Regular Solids)\", while Pacioli wrote \"De divina proportione (On Divine Proportion)\", with illustrations by Leonardo da Vinci, at the end of the fifteenth century.\n\nMerely making accepted use of some aspect of mathematics such as perspective does not qualify an artist for admission to this list.\n\nThe term \"fine art\" is used conventionally to cover the output of artists who produce a combination of paintings, drawings and sculptures.\n\n\n[[Category:Lists of artists]"}
{"id": "24364843", "url": "https://en.wikipedia.org/wiki?curid=24364843", "title": "List of perfect numbers", "text": "List of perfect numbers\n\nThe following is a list of the known perfect numbers, and the exponents \"p\" that can be used to generate them (using the expression 2× (2 − 1)) whenever 2 − 1 is a Mersenne prime. All even perfect numbers are of this form. It is not known whether there are any odd perfect numbers. there are 50 known perfect numbers in total. The ratio \"p\" / digits approaches log(10) / log(4) = 1.6609640474...\n\nThe displayed ranks are among those perfect numbers which are known . Some ranks may change later if smaller perfect numbers are discovered. It is known there is no odd perfect number below 10. GIMPS reported that by 8 April 2018 the search for Mersenne primes (and thereby even perfect numbers) became exhaustive up to the 47th above.\n\n"}
{"id": "2302006", "url": "https://en.wikipedia.org/wiki?curid=2302006", "title": "List of simple Lie groups", "text": "List of simple Lie groups\n\nIn mathematics, the simple Lie groups were first classified by Wilhelm Killing and later perfected by Élie Cartan. This classification is often referred to as Killing-Cartan classification. \n\nThe list of simple Lie groups can be used to read off the list of simple Lie algebras and Riemannian symmetric spaces. See also the table of Lie groups for a smaller list of groups that commonly occur in theoretical physics, and the Bianchi classification for groups of dimension at most 3. \n\nUnfortunately, there is no universally accepted definition of a simple Lie group. In particular, it is not always defined as a Lie group that is simple as an abstract group. Authors differ on whether a simple Lie group has to be connected, or on whether it is allowed to have a non-trivial center, or on whether R is a simple Lie group. \n\nThe most common definition is that a Lie group is simple if it is connected, non-abelian, and every closed \"connected\" normal subgroup is either the identity or the whole group. In particular, simple groups are allowed to have a non-trivial center, but R is not simple. \n\nIn this article the connected simple Lie groups with trivial center are listed. Once these are known, the ones with non-trivial center are easy to list as follows. Any simple Lie group with trivial center has a universal cover, whose center is the fundamental group of the simple Lie group. The corresponding simple Lie groups with non-trivial center can be obtained as quotients of this universal cover by a subgroup of the center.\n\nThe Lie algebra of a simple Lie group is a simple Lie algebra. This is a one-to-one correspondence between connected simple Lie groups with trivial center and simple Lie algebras of dimension greater than 1. (Authors differ on whether the one-dimensional Lie algebra should be counted as simple.)\n\nOver the complex numbers the semisimple Lie algebras are classified by their Dynkin diagrams, of types \"ABCDEFG\". If \"L\" is a real simple Lie algebra, its complexification is a simple complex Lie algebra, unless \"L\" is already \nthe complexification of a Lie algebra, in which case the complexification of \"L\" is a product of two copies of \"L\". This reduces the problem of classifying the real simple Lie algebras to that of finding all the real forms of each complex simple Lie algebra (i.e., real Lie algebras whose complexification is the given complex Lie algebra). There are always at least 2 such forms: a split form and a compact form, and there are usually a few others. The different real forms correspond to the classes of automorphisms of order at most 2 of the complex Lie algebra.\n\nSymmetric spaces are classified as follows. \n\nFirst, the universal cover of a symmetric space is still symmetric, so we can reduce to the case of simply connected symmetric spaces. (For example, the universal cover of a real projective plane is a sphere.)\n\nSecond, the product of symmetric spaces is symmetric, so we may as well just classify the irreducible simply connected ones (where irreducible means they cannot be written as a product of smaller symmetric spaces). \n\nThe irreducible simply connected symmetric spaces are the real line, and exactly two symmetric spaces corresponding to each \"non-compact\" simple Lie group \"G\",\none compact and one non-compact. The non-compact one is a cover of the quotient of \"G\" by a maximal compact subgroup \"H\", and the compact one is a cover of the quotient of\nthe compact form of \"G\" by the same subgroup \"H\". This duality between compact and non-compact symmetric spaces is a generalization of the well known duality between spherical and hyperbolic geometry.\n\nA symmetric space with a compatible complex structure is called Hermitian. The compact simply connected irreducible Hermitian symmetric spaces fall into 4 infinite families with 2 exceptional ones left over, and each has a non-compact dual. In addition the complex plane is also a Hermitian symmetric space; this gives the complete list of irreducible Hermitian symmetric spaces.\n\nThe four families are the types A III, B I and D I for , D III, and C I, and the two exceptional ones are types E III and E VII of complex dimensions 16 and 27.\n\nformula_1  stand for the real numbers, complex numbers, quaternions, and octonions.\n\nIn the symbols such as \"E\" for the exceptional groups, the exponent −26 is the signature of an invariant symmetric bilinear form that is negative definite on the maximal compact subgroup. It is equal to the dimension of the group minus twice the dimension of a maximal compact subgroup.\n\nThe fundamental group listed in the table below is the fundamental group of the simple group with trivial center. \nOther simple groups with the same Lie algebra correspond to subgroups of this fundamental group (modulo the action of the outer automorphism group).\n\nThe following table lists some Lie groups with simple Lie algebras of small \ndimension. The groups on a given line all have the same Lie algebra. In the dimension 1 case, the groups are abelian and not simple.\n\n"}
{"id": "1606990", "url": "https://en.wikipedia.org/wiki?curid=1606990", "title": "Logical harmony", "text": "Logical harmony\n\nLogical harmony, a name coined by Michael Dummett, is a supposed constraint on the rules of inference that can be used in a given logical system.\n\nThe logician Gerhard Gentzen proposed that the meanings of logical connectives could be given by the rules for introducing them into discourse. For example, if one believes that \"the sky is blue\" and one also believes that \"grass is green\", then one can introduce the connective \"and\" as follows: \"The sky is blue AND grass is green.\" Gentzen's idea was that having rules like this is what gives meaning to one's words, or at least to certain words. The idea has also been associated with Wittgenstein's dictum that in many cases we can say, \"meaning is use\". Most contemporary logicians prefer to think that the introduction rules and the elimination rules for an expression are equally important. In this case, \"and\" is characterized by the following rules:\nAn apparent problem with this was pointed out by Arthur Prior: Why can't we have an expression (call it \"tonk\") whose introduction rule is that of OR (from \"p\" to \"p tonk q\") but whose elimination rule is that of AND (from \"p tonk q\" to \"q\")? This lets us deduce anything at all from any starting point. Prior suggested that this meant that inferential rules could \"not\" determine meaning. He was answered by Nuel Belnap, that even though introduction and elimination rules can constitute meaning, not just any pair of such rules will determine a meaningful expression—they must meet certain constraints, such as not allowing us to deduce any new truths in the old vocabulary. These constraints are what Dummett was referring to.\n\nHarmony, then, refers to certain constraints that a proof theory must let hold between introduction and elimination rules for it to be meaningful, or in other words, for its inference rules to be meaning-constituting.\n\nThe application of harmony to logic may be considered a special case; it makes sense to talk of harmony with respect to not only inferential systems, but also conceptual systems in human cognition, and to type systems in programming languages.\n\nSemantics of this form has not provided a very great challenge to that sketched in Tarski's semantic theory of truth, but many philosophers interested in reconstituting the semantics of logic in a way that respects Ludwig Wittgenstein's \"meaning is use\" have felt that harmony holds the key.\n\n\n"}
{"id": "10779029", "url": "https://en.wikipedia.org/wiki?curid=10779029", "title": "Mathematical maturity", "text": "Mathematical maturity\n\nMathematical maturity is an informal term used by mathematicians to refer to a mixture of mathematical experience and insight that cannot be directly taught. Instead, it comes from repeated exposure to mathematical concepts. It is a gauge of mathematics student's erudition in mathematical structures and methods.\nThe topic is occasionally also addressed in literature in its own right.\n\nMathematical maturity has been defined in several different ways by various authors.\n\nOne definition has been given as follows:\nA broader list of characteristics of mathematical maturity has been given as follows:\nFinally, mathematical maturity has also been defined as an ability to do the following:\n"}
{"id": "239112", "url": "https://en.wikipedia.org/wiki?curid=239112", "title": "Mathematical table", "text": "Mathematical table\n\nMathematical tables are lists of numbers showing the results of calculation with varying arguments. Before calculators were cheap and plentiful, people would use such tables to simplify and drastically speed up computation. Tables of logarithms and trigonometric functions were common in math and science textbooks. Specialized tables were published for applications such as astronomy, celestial navigation and statistics.\n\nTo compute the sine function of 75 degrees, 9 minutes, 50 seconds using a table of trigonometric functions such as the Bernegger table from 1619 illustrated here, one might simply round up to 75 degrees, 10 minutes and then find the 10 minute entry on the 75 degree page, shown above-right, which is 0.9666746.\n\nHowever, this answer is only accurate to four decimal places. If one wanted greater accuracy, one could interpolate linearly as follows:\n\nFrom the Bernegger table:\n\nThe difference between these values is 0.0000745.\n\nSince there are 60 seconds in a minute of arc, we multiply the difference by 50/60 to get a correction of (50/60)*0.0000745 ≈ 0.0000621; and then add that correction to sin (75° 9′) to get :\n\nA modern calculator gives sin (75° 9′ 50″) = 0.96666219991, so our interpolated answer is accurate to the 7-digit precision of the Bernegger table.\n\nFor tables with greater precision (more digits per value), higher order interpolation may be needed to get full accuracy. In the era before electronic computers, interpolating table data in this manner was the only practical way to get high accuracy values of mathematical functions needed for applications such as navigation, astronomy and surveying.\n\nTo understand the importance of accuracy in applications like navigation note that at sea level one minute of arc along the Earth's equator or a meridian (indeed, any great circle) equals approximately one nautical mile ().\n\nThe first tables of trigonometric functions known to be made were by Hipparchus (c.190 – c.120 BCE) and Menelaus (c.70–140 CE), but both have been lost. Along with the surviving table of Ptolemy (c. 90 – c.168 CE), they were all tables of chords and not of half-chords, i.e. the sine function. The table produced by the Indian mathematician Āryabhaṭa is considered the first sine table ever constructed. \nĀryabhaṭa's table remained the standard sine table of ancient India. There were continuous attempts to improve the accuracy of this table, culminating in the discovery of the power series expansions of the sine and cosine functions by Madhava of Sangamagrama (c.1350 – c.1425), and the tabulation of a sine table by Madhava with values accurate to seven or eight decimal places.\n\nTables of common logarithms were used until the invention of computers and electronic calculators to do rapid multiplications, divisions, and exponentiations, including the extraction of \"n\"th roots.\n\nMechanical special-purpose computers known as difference engines were proposed in the 19th century to tabulate polynomial approximations of logarithmic functions – i.e. to compute large logarithmic tables. This was motivated mainly by errors in logarithmic tables made by the human computers of the time. Early digital computers were developed during World War II in part to produce specialized mathematical tables for aiming artillery. From 1972 onwards, with the launch and growing use of scientific calculators, most mathematical tables went out of use.\n\nOne of the last major efforts to construct such tables was the Mathematical Tables Project that was started in 1938 as a project of the Works Progress Administration (WPA), employing 450 out-of-work clerks to tabulate higher mathematical functions, and lasted through World War II.\n\nTables of special functions are still used; for example, the use of tables of values of the cumulative distribution function of the normal distribution – so-called standard normal tables – remains commonplace today, especially in schools.\n\nCreating tables stored in random-access memory is a common code optimization technique in computer programming, where the use of such tables speeds up calculations in those cases where a table lookup is faster than the corresponding calculations (particularly if the computer in question doesn't have a hardware implementation of the calculations). In essence, one trades computing speed for the computer memory space required to store the tables.\n\nTables containing common logarithms (base-10) were extensively used in computations prior to the advent of computers and calculators because logarithms convert problems of multiplication and division into much easier addition and subtraction problems. Base-10 logarithms have an additional property that is unique and useful: The common logarithm of numbers greater than one that differ only by a factor of a power of ten all have the same fractional part, known as the \"mantissa.\" Tables of common logarithms typically included only the mantissas; the integer part of the logarithm, known as the \"characteristic,\" could easily be determined by counting digits in the original number.\n\nThe fractional part of the common logarithm of numbers greater than zero but less than one is just 1 minus the mantissa of the same number with the decimal point shifted to the right of the first non-zero digit. But same mantissa could be (and was) used for numbers less than one by offsetting the characteristic. Thus a single table of common logarithms can be used for the entire range of positive decimal numbers. See common logarithm for details on the use of characteristics and mantissas.\n\nMichael Stifel published \"Arithmetica integra\" in Nuremberg in 1544 which contains a table of integers and powers of 2 that has been considered an early version of a logarithmic table.\n\nThe method of logarithms was publicly propounded by John Napier in 1614, in a book entitled \"Mirifici Logarithmorum Canonis Descriptio\" (\"Description of the Wonderful Rule of Logarithms\"). The book contained fifty-seven pages of explanatory matter and ninety pages of tables related to natural logarithms. The English mathematician Henry Briggs visited Napier in 1615, and proposed a re-scaling of Napier's logarithms to form what is now known as the common or base-10 logarithms. Napier delegated to Briggs the computation of a revised table, and they later published, in 1617, \"Logarithmorum Chilias Prima\" (\"The First Thousand Logarithms\"), which gave a brief account of logarithms and a table for the first 1000 integers calculated to the 14th decimal place.\n\nThe computational advance available via common logarithms, the converse of powered numbers or exponential notation, was such that it made calculations by hand much quicker.\n\n\n\n"}
{"id": "1186588", "url": "https://en.wikipedia.org/wiki?curid=1186588", "title": "Mathematische Arbeitstagung", "text": "Mathematische Arbeitstagung\n\nThe Mathematische Arbeitstagung taking place annually in Bonn since 1957, and founded by Friedrich Hirzebruch, was an international meeting of mathematicians intended to act in clearing-house fashion, by disseminating current research ideas; and, at the same time, to bring mathematics in West Germany back into its place in European trends. It proved highly successful in attracting the cream of younger mathematicians, partly because its structure was not that of the conventional international conference. The programme of talks was decided 'in real time' only, rather than in advance.\n\nFor example, in 1962 the meeting was dominated by talks on K-theory, at that time the breaking news. The early participants included Jean-Pierre Serre, Michael Atiyah and Frank Adams.\n\nThe institutional structure was reinforced from 1969 by the \"Sonderforschungsbereich Theoretische Mathematik\" programme, and from 1980 by the founding of the Max Planck Institute for Mathematics in Bonn.\n\n"}
{"id": "39688248", "url": "https://en.wikipedia.org/wiki?curid=39688248", "title": "N-topological space", "text": "N-topological space\n\nIn mathematics, an \"N\"-topological space is a set equipped with \"N\" arbitrary topologies. If \"τ\", \"τ\", ..., \"τ\" are \"N\" topologies defined on a nonempty set X, then the \"N\"-topological space is denoted by (\"X\",\"τ\",\"τ\"...,\"τ\").\nFor \"N\" = 1, the structure is simply a topological space.\nFor \"N\" = 2, the structure becomes a bitopological space introduced by J. C. Kelly.\n\nLet \"X\" = {\"x\", \"x\", ..., \"x\"} be any finite set. Suppose \"A\" = {\"x\", \"x\", ..., \"x\"}. Then the collection \"τ\" = {\"φ\", \"A\", \"A\", ..., \"A\" = \"X\"} will be a topology on \"X\". If \"τ\", \"τ\", ..., \"τ\" be \"m\" such topologies (chain topologies) defined on \"X\", then the structure (\"X\", \"τ\", \"τ\", ..., \"τ\") is an \"m\"-topological space.\n"}
{"id": "54347634", "url": "https://en.wikipedia.org/wiki?curid=54347634", "title": "Nested sequent calculus", "text": "Nested sequent calculus\n\nIn structural proof theory, the nested sequent calculus is a reformulation of the sequent calculus to allow deep inference.\n"}
{"id": "1297317", "url": "https://en.wikipedia.org/wiki?curid=1297317", "title": "No free lunch theorem", "text": "No free lunch theorem\n\nIn mathematical folklore, the \"no free lunch\" (NFL) theorem (sometimes pluralized) of David Wolpert and William Macready appears in the 1997 \"No Free Lunch Theorems for Optimization\". Wolpert had previously derived no free lunch theorems for machine learning (statistical inference).\n\nIn 2005, Wolpert and Macready themselves indicated that the first theorem in their paper \"state[s] that any two optimization algorithms are equivalent when their performance is averaged across all possible problems\". The 1997 theorems of Wolpert and Macready are mathematically technical.\n\nThe folkloric \"no free lunch\" (NFL) theorem is an easily stated and easily understood consequence of theorems Wolpert and Macready actually prove. It is weaker than the proven theorems, and thus does not encapsulate them. Various investigators have extended the work of Wolpert and Macready substantively. See No free lunch in search and optimization for treatment of the research area.\n\nWhile some scholars argue that NFL conveys important insight, others argue that NFL is of little relevance to machine learning research.\n\nTo find the highest point on Earth, Alice uses a steepest-ascent local search which restarts when a local peak is fully climbed. Bob uses a steepest-descent local search which restarts when it bottoms out at a local trough. In our Earth, Alice will find the highest point much faster than Bob will. However, in some ensembles, each world such as ours can be paired 1-to-1 with a hypothetical \"Bobworld\" which is identical to ours, except that the elevation of peak of Mount Everest and of the lowest point in the Marianas Trench are swapped, as if a tall pole has been stuck in the Trench. In Bobworld, Bob's strategy of descending outperforms Alice's strategy of climbing; in fact, given further 1-to-1 pairing assumptions that are reasonable in a universe of bounded size, neither Alice's strategy nor Bob's strategy performs better on average, in the absence of some systematic natural bias towards worlds like Earth and away from worlds like Bobworld.\n\nWolpert and Macready give two NFL theorems that are closely related to the folkloric theorem. In their paper, they state:\n\nThe first theorem hypothesizes objective functions that do not change while optimization is in progress, and the second hypothesizes objective functions that may change.\n\nwhere formula_1 denotes the ordered set of size formula_2 of the cost values formula_3 associated to input values formula_4, formula_5 is the function being optimized and formula_6 is the conditional probability of obtaining a given sequence of cost values from algorithm formula_7 run formula_2 times on function formula_9.\n\nThe theorem can be equivalently formulated as follows:\n\nHere, \"blind search\" means that at each step of the algorithm, the element formula_10 is chosen at random with uniform probability distribution from the elements of formula_11 that have not been chosen previously.\n\nIn essence, this says that when all functions \"f\" are equally likely, the probability of observing an arbitrary sequence of \"m\" values in the course of optimization does not depend upon the algorithm. In the analytic framework of Wolpert and Macready, performance is a function of the sequence of observed values (and not e.g. of wall-clock time), so it follows easily that all algorithms have identically distributed performance when objective functions are drawn uniformly at random, and also that all algorithms have identical mean performance. But identical mean performance of all algorithms does not imply Theorem 1, and thus the folkloric theorem is not equivalent to the original theorem.\n\nTheorem 2 establishes a similar, but \"more subtle\", NFL result for time-varying objective functions.\n\nThe NFL theorems were explicitly \"not\" motivated by the question of what can be inferred (in the case of NFL for machine learning) or found (in the case of NFL for search) when the \"environment is uniform random\". Rather uniform randomness was used as a tool, to compare the number of environments for which algorithm A outperforms algorithm B to the number of environments for which B outperforms A. NFL tells us that (appropriately weighted) there are just as many environments in both of those sets.\n\nThis is true for many definitions of what precisely an \"environment\" is. In particular, there are just as many prior distributions (appropriately weighted) in which learning algorithm A beats B (on average) as vice versa. This statement about \"sets of priors\" is what is most important about NFL, not the fact that any two algorithms perform equally for the single, specific prior distribution that assigns equal probability to all environments.\n\nWhile the NFL is important to understand the fundamental limitation for a set of problems, it does not state anything about each particular instance of a problem that can arise in practice. That is, the NFL states what the NFL states in the mathematical statements and it is nothing more than that. For example, it applies to the situations where the algorithm is fixed first and a nature can choose a worst problem instance to each fixed algorithm. Therefore, if we have a \"good\" problem in practice or if we can choose a \"good\" learning algorithm for a given particular problem instance, then the NFL does not mention any limitation about this particular problem instance. See for example. To understand the results of the NFL along with \"seemingly\" contradicting results from other papers, it is important to actually understand the mathematical logic of the NFL instead of intuitive notation of the NFL. All results including the NFL and are indeed consistent.\n\nTo illustrate one of the counter-intuitive implications of NFL, suppose we fix two supervised learning algorithms, C and D. We then sample a target function f to produce a set of input-output pairs, \"d\". How should we choose whether to train C or D on \"d\", in order to make predictions for what output would be associated with a point lying outside of \"d?\"\n\nIt is common in almost of all science and statistics to answer this question - to choose between C and D - by running cross-validation on \"d\" with those two algorithms. In other words, to decide whether to generalize from \"d\" with either C or D\",\" we see which of them has better out-of-sample performance when tested within \"d\".\n\nNote that since C and D are fixed, this use of cross-validation to choose between them is itself an algorithm, i.e., a way of generalizing from an arbitrary dataset. Call this algorithm A. (Arguably, A is a simplified model of the scientific method itself.)\n\nNote as well though that we could also use \"anti\"-cross-validation to make our choice. In other words, we could choose between C and D based on which has \"worse\" out-of-sample performance within \"d\". Again, since C and D are fixed, this use of anti-cross-validation is itself an algorithm. Call that algorithm B.\n\nNFL tells us (loosely speaking) that B must beat A on just as many target functions (and associated datasets \"d\") as A beats B. In this very specific sense, the scientific method will lose to the \"anti\" scientific method just as readily as it wins.\n\nHowever, note that NFL only applies if the target function is chosen from a uniform distribution of all possible functions. If this is not the case, and certain target functions are more likely to be chosen than others, then A may perform better than B overall. The contribution of NFL is that it tells us choosing an appropriate algorithm requires making assumptions about the kinds of target functions the algorithm is being used for. With no assumptions, no \"meta-algorithm\", such as the scientific method, performs better than random choice.\n\nWhile some scholars argue that NFL conveys important insight, others argue that NFL is of little relevance to machine learning research. If Occam's razor is correct, for example if sequences of lower Kolmogorov complexity are more probable than sequences of higher complexity, then (as is observed in real life) some algorithms, such as cross-validation, perform better on average on practical problems (when compared with random choice or with anti-cross-validation).\n\n"}
{"id": "47926105", "url": "https://en.wikipedia.org/wiki?curid=47926105", "title": "Open Energy Modelling Initiative", "text": "Open Energy Modelling Initiative\n\nThe Open Energy Modelling Initiative (openmod) is a grass roots community of energy system modellers from universities and research institutes across Europe and elsewhere. The initiative promotes the use of open-source software and open data in energy system modelling for research and policy advice. The Open Energy Modelling Initiative documents a variety of open-source energy models and addresses practical and conceptual issues regarding their development and application. The initiative runs an email list, an internet forum, and a wiki and hosts occasional academic workshops. A statement of aims is available.\n\nThe application of open-source development to energy modelling dates back to around 2010. This section provides some background for the growing interest in open methods.\n\nJust two active open energy modelling projects were cited in a 2011 paper: OSeMOSYS and TEMOA. Balmorel was also open at that time, having been made public in 2001.\n, the openmod wiki lists 24 such undertakings.\n\nAn innovative 2012 paper presents the case for using \"open, publicly accessible software and data as well as crowdsourcing techniques to develop robust energy analysis tools\". The paper claims that these techniques can produce high-quality results and are particularly relevant for developing countries.\n\nThere is an increasing call for the energy models and datasets used for energy policy analysis and advice to be made public in the interests of transparency and quality. A 2010 paper concerning energy efficiency modeling argues that \"an open peer review process can greatly support model verification and validation, which are essential for model development\". One 2012 study argues that the source code and datasets used in such models should be placed under publicly accessible version control to enable third-parties to run and check specific models. Another 2014 study argues that the public trust needed to underpin a rapid transition in energy systems can only be built through the use of transparent open-source energy models. The UK TIMES project (UKTM) is open source, according to a 2014 presentation, because \"energy modelling must be replicable and verifiable to be considered part of the scientific process\" and because this fits with the \"drive towards clarity and quality assurance in the provision of policy insights\". In 2016, the Deep Decarbonization Pathways Project (DDPP) is seeking to improve its modelling methodologies, a key motivation being \"the intertwined goals of transparency, communicability and policy credibility.\" A 2016 paper argues that model-based energy scenario studies, wishing to influence decision-makers in government and industry, must become more comprehensible and more transparent. To these ends, the paper provides a checklist of transparency criteria that should be completed by modelers. The authors note however that they \"consider open source approaches to be an extreme case of transparency that does not automatically facilitate the comprehensibility of studies for policy advice.\" An editorial from 2016 opines that closed energy models providing public policy support \"are inconsistent with the open access movement [and] funded research\". A 2017 paper lists the benefits of open data and models and the reasons that many projects nonetheless remain closed. The paper makes a number of recommendations for projects wishing to transition to a more open approach. The authors also conclude that, in terms of openness, energy research has lagged behind other fields, most notably physics, biotechnology, and medicine. Moreover:\n\nA one-page opinion piece in \"Nature News\" from 2017 advances the case for using open energy data and modeling to build public trust in policy analysis. The article also argues that scientific journals have a responsibility to require that data and code be submitted alongside text for scrutiny, currently only \"Energy Economics\" makes this practice mandatory within the energy domain.\n\nIssues surrounding copyright remain at the forefront with regard to open energy data. Most energy datasets are collated and published by official or semi-official sources, for example, national statistics offices, transmission system operators, and electricity market operators. The doctrine of open data requires that these datasets be available under free licenses (such as ) or be in the public domain. But most published energy datasets carry proprietary licenses, limiting their reuse in numerical and statistical models, open or otherwise. Measures to enforce market transparency have not helped because the associated information is normally licensed to preclude downstream usage. Recent transparency measures include the 2013 European energy market transparency regulation 543/2013 and a 2016 amendment to the German Energy Industry Act to establish a nation energy information platform, slated to launch on 1July 2017. Energy databases are protected under general database law, irrespective of the copyright status of the information they hold.\n\nIn December 2017, participants from the Open Energy Modelling Initiative and allied research communities made a written submission to the European Commission on the of public sector information. The document provides a comprehensive account of the data issues faced by researchers engaged in open energy system modeling and energy market analysis and quoted extensively from a German legal opinion.\n\nIn May 2016 the European Union announced that \"all scientific articles in Europe must be freely accessible as of 2020\". This is a step in the right direction, but the new policy makes no mention of open software and its importance to the scientific process. In August 2016, the United States government announced a new federal source code policy which mandates that at least 20% of custom source code developed by or for any agency of the federal government be released as open-source software (OSS). The US Department of Energy (DOE) is participating in the program. The project is hosted on a dedicated website and subject to a three-year pilot. Open-source campaigners are using the initiative to advocate that European governments adopt similar practices. In 2017 the Free Software Foundation Europe (FSFE) issued a position paper calling for free software and open standards to be central to European science funding, including the flagship EU program Horizon2020. The position paper focuses on open data and open data processing and the question of open modeling is not traversed perse.\n\nThe Open Energy Modelling Initiative participants take turns to host regular academic workshops.\n\n\nRelated to openmod\n\n\nOpen energy data\n\n\nSimilar initiatives\n\n\nOther\n\n"}
{"id": "22656", "url": "https://en.wikipedia.org/wiki?curid=22656", "title": "Operand", "text": "Operand\n\nIn mathematics an operand is the object of a mathematical operation, i.e. it is the object or quantity that is operated on.\n\nThe following arithmetic expression shows an example of operators and operands:\n\nIn the above example, '+' is the symbol for the operation called addition. \n\nThe operand '3' is one of the inputs (quantities) followed by the addition operator, and the operand '6' is the other input necessary for the operation.\n\nThe result of the operation is 9. (The number '9' is also called the sum of the augend 3 and the addend 6.)\n\nAn operand, then, is also referred to as \"one of the inputs (quantities) for an operation\".\n\nOperands may be complex, and may consist of expressions also made up of operators with operands.\n\nIn the above expression '(3 + 5)' is the first operand for the multiplication operator and '2' the second. The operand '(3 + 5)' is an expression in itself, which contains an addition operator, with the operands '3' and '5'.\n\nRules of precedence affect which values form operands for which operators:\n\nIn the above expression, the multiplication operator has the higher precedence than the addition operator, so the multiplication operator has operands of '5' and '2'. The addition operator has operands of '3' and '5 × 2'.\n\nDepending on the mathematical notation being used the position of an operator in relation to its operand(s) may vary. In everyday usage infix notation is the most common, however other notations also exist, such as the prefix and postfix notations. These alternate notations are most common within computer science.\n\nBelow is a comparison of three different notations — all represent an addition of the numbers '1' and '2'\n\nIn a mathematical expression, the order of operation is carried out from left to right. Start with the left most value and seek the first operation to be carried out in accordance with the order specified above (i.e., start with parentheses and end with the addition/subtraction group). For example, in the expression\n\nthe first operation to be acted upon is any and all expressions found inside a parenthesis. So beginning at the left and moving to the right, find the first (and in this case, the only) parenthesis, that is, (2 + 2). Within the parenthesis itself is found the expression 2. The reader is required to find the value of 2 before going any further. The value of 2 is 4. Having found this value, the remaining expression looks like this:\n\nThe next step is to calculate the value of expression inside the parenthesis itself, that is, (2 + 4) = 6. Our expression now looks like this:\n\nHaving calculated the parenthetical part of the expression, we start over again beginning with the left most value and move right. The next order of operation (according to the rules) is exponents. Start at the left most value, that is, 4, and scan your eyes to the right and search for the first exponent you come across. The first (and only) expression we come across that is expressed with an exponent is 2. We find the value of 2, which is 4. What we have left is the expression\n\nThe next order of operation is multiplication. 4 × 4 is 16. Now our expression looks like this:\n\nThe next order of operation according to the rules is division. However, there is no division operator sign (÷) in the expression, 16 − 6. So we move on to the next order of operation, i.e., addition and subtraction, which have the same precedence and are done left to right.\n\nSo the correct value for our original expression, 4 × 2 − (2 + 2), is 10. \n\nIt is important to carry out the order of operation in accordance with rules set by convention. If the reader evaluates an expression but does not follow the correct order of operation, the reader will come forth with a different value. The different value will be the incorrect value because the order of operation was not followed. The reader will arrive at the correct value for the expression if and only if each operation is carried out in the proper order.\n\nThe number of operands of an operator is called its arity. Based on arity, operators are classified as nullary (no operands), unary (1 operand), binary (2 operands), ternary (3 operands) etc.\n\nIn computer programming languages, the definitions of operator and operand are almost the same as in mathematics.\n\nIn computing, an operand is the part of a computer instruction which specifies what data is to be manipulated or operated on, while at the same time representing the data itself.\nA computer instruction describes an operation such as add or multiply X, while the operand (or operands, as there can be more than one) specify on which X to operate as well as the value of X.\n\nAdditionally, in assembly language, an operand is a value (an argument) on which the instruction, named by mnemonic, operates. The operand may be a processor register, a memory address, a literal constant, or a label. A simple example (in the x86 architecture) is\n\nMOV DS, AX\n\nwhere the value in register operand codice_1 is to be moved (codice_2) into register codice_3. Depending on the instruction, there may be zero, one, two, or more operands.\n\n"}
{"id": "350204", "url": "https://en.wikipedia.org/wiki?curid=350204", "title": "Outline of combinatorics", "text": "Outline of combinatorics\n\nCombinatorics is a branch of mathematics concerning the study of finite or countable discrete structures.\n\n\n\n\nHistory of combinatorics\n\n\n\n\n\n\n\n\n\n"}
{"id": "1687416", "url": "https://en.wikipedia.org/wiki?curid=1687416", "title": "Porism", "text": "Porism\n\nA porism is a mathematical proposition or corollary. In particular, the term porism has been used to refer to a direct result of a proof, analogous to how a corollary refers to a direct result of a theorem.\nIn modern usage, a porism is a relation that holds for an infinite range of values but only if a certain condition is assumed, for example Steiner's porism.\nThe term originates from three books of Euclid with porism, that have been lost.\nNote that a proposition may not have been proven, so a porism may not be a theorem, or for that matter, it may not be true.\n\nThe treatise which has given rise to this subject is the \"Porisms\" of Euclid, the author of the \"Elements\". As much as is known of this lost treatise is due to the \"Collection\" of Pappus of Alexandria, who mentions it along with other geometrical treatises, and gives a number of lemmas necessary for understanding it. Pappus states:\n\nPappus goes on to say that this last definition was changed by certain later geometers, who defined a porism on the ground of an accidental characteristic as\n\" (\"to leîpon hypothései topikoû theōrḗmatos\"), that which falls short of a locus-theorem by a (or in its) hypothesis. Proclus points out that the word \"porism\" was used in two senses. One sense is that of \"corollary\", as a result unsought, as it were, but seen to follow from a theorem. On the \"porism\" in the other sense he adds nothing to the definition of \"the older geometers\" except to say that the finding of the center of a circle and the finding of the greatest common measure are porisms.\n\nPappus gave a complete enunciation of a porism derived from Euclid, and an extension of it to a more general case. This porism, expressed in modern language, asserts the following: Given four straight lines of which three turn about the points in which they meet the fourth, if two of the points of intersection of these lines lie each on a fixed straight line, the remaining point of intersection will also lie on another straight line. The general enunciation applies to any number of straight lines, say \"n\" + 1, of which \"n\" can turn about as many points fixed on the (\"n\" + 1)th. These \"n\" straight lines cut, two and two, in 1/2\"n\"(\"n\" − 1) points, 1/2\"n\"(\"n\" − 1) being a triangular number whose side is \"n\" − 1. If, then, they are made to turn about the \"n\" fixed points so that any \"n\" − 1 of their 1/2\"n\"(\"n\" − 1) points of intersection, chosen subject to a certain limitation, lie on \"n\" − 1 given fixed straight lines, then each of the remaining points of intersection, 1/2\"n\"(\"n\" − 1)(\"n\" − 2) in number, describes a straight line. Pappus gives also a complete enunciation of one porism of the first book of Euclid's treatise.\n\nThis may be expressed thus: If about two fixed points P, Q we make turn two straight lines meeting on a given straight line L, and if one of them cut off a segment AM from a fixed straight line AX, given in position, we can determine another fixed straight line BY, and a point B fixed on it, such that the segment BM' made by the second moving line on this second fixed line measured from B has a given ratio X to the first segment AM. The rest of the enunciations given by Pappus are incomplete, and he merely says that he gives thirty-eight lemmas for the three books of porisms; and these include 171 theorems. The lemmas which Pappus gives in connexion with the porisms are interesting historically, because he gives:\n\nFrom the 17th to the 19th centuries this subject seems to have had great fascination for mathematicians, and many geometers have attempted to restore the lost porisms. Thus Albert Girard says in his \"Traité de trigonometrie\" (1626) that he hopes to publish a restoration. About the same time Pierre de Fermat wrote a short work under the title \"Porismatum euclidaeorum renovata doctrina et sub forma isagoges recentioribus geometeis exhibita\" (see \"Œuvres de Fermat\", i., Paris, 1891); but two at least of the five examples of porisms which he gives do not fall within the classes indicated by Pappus.\n\nRobert Simson was the first to throw real light upon the subject. He first succeeded in explaining the only three propositions which Pappus indicates with any completeness. This explanation was published in the \"Philosophical Transactions\" in 1723. Later he investigated the subject of porisms generally in a work entitled \"De porismatibus traclatus; quo doctrinam porisrnatum satis explicatam, et in posterum ab oblivion tutam fore sperat auctor\", and published after his death in a volume, \"Roberti Simson opera quaedam reliqua\" (Glasgow, 1776).\n\nSimson's treatise, \"De porismatibus\", begins with definitions of theorem, problem, datum, porism and locus. Respecting the porism Simson says that Pappus's definition is too general, and therefore he will substitute for it the following:\n\n\"Porisma est propositio in qua proponitur demonstrare rem aliquam, vel plures datas esse, cui, vel quibus, ut et cuilibet ex rebus innumeris, non quidem datis, sed quae ad ea quae data sunt eandem habent rationem, convenire ostendendum est affectionem quandam communem in propositione descriptam. Porisma etiam in forma problematis enuntiari potest, si nimirum ex quibus data demonstranda sunt, invenienda proponantur.\"\n\nA locus (says Simson) is a species of porism. Then follows a Latin translation of Pappus's note on the porisms, and the propositions which form the bulk of the treatise. These are Pappus's thirty-eight lemmas relating to the porisms, ten cases of the proposition concerning four straight lines, twenty-nine porisms, two problems in illustration and some preliminary lemmas.\n\nJohn Playfair's memoir (\"Trans. Roy. Soc. Edin.\", 1794, vol. iii.), a sort of sequel to Simson's treatise, had for its special object the inquiry into the probable origin of porisms, that is, into the steps which led the ancient geometers to the discovery of them. Playfair remarked that the careful investigation of all possible particular cases of a proposition would show that (1) under certain conditions a problem becomes impossible; (2) under certain other conditions, indeterminate or capable of an infinite number of solutions. These cases could be enunciated separately, were in a manner intermediate between theorems and problems, and were called \"porisms.\" Playfair accordingly defined a porism thus: \"A proposition affirming the possibility of finding such conditions as will render a certain problem indeterminate or capable of innumerable solutions.\"\n\nThough this definition of a porism appears to be most favoured in England, Simson's view has been most generally accepted abroad, and had the support of Michel Chasles. However, in Liouville's \"Journal de mathematiques pures et appliquées\" (vol. xx., July, 1855), P. Breton published \"Recherches nouvelles sur les porismes d'Euclide\", in which he gave a new translation of the text of Pappus, and sought to base thereon a view of the nature of a porism more closely conforming to the definitions in Pappus. This was followed in the same journal and in \"La Science\" by a controversy between Breton and A. J. H. Vincent, who disputed the interpretation given by the former of the text of Pappus, and declared himself in favour of the idea of Schooten, put forward in his \"Mathematicae exercitationes\" (1657), in which he gives the name of \"porism\" to one section. According to Frans van Schooten, if the various relations between straight lines in a figure are written down in the form of equations or proportions, then the combination of these equations in all possible ways, and of new equations thus derived from them leads to the discovery of innumerable new properties of the figure, and here we have \"porisms.\"\n\nThe discussions, however, between Breton and Vincent, in which C. Housel also joined, did not carry forward the work of restoring Euclid's Porisms, which was left for Chasles. His work (\"Les Trois livres de porismes d'Euclide\", Paris, 1860) makes full use of all the material found in Pappus. But we may doubt its being a successful reproduction of Euclid's actual work. Thus, in view of the ancillary relation in which Pappus's lemmas generally stand to the works to which they refer, it seems incredible that the first seven out of thirty-eight lemmas should be really equivalent (as Chasles makes them) to Euclid's first seven Porisms. Again, Chasles seems to have been wrong in making the ten cases of the four-line Porism begin the book, instead of the intercept-Porism fully enunciated by Pappus, to which the \"lemma to the first Porism\" relates intelligibly, being a particular case of it.\n\nAn interesting hypothesis as to the Porisms was put forward by H. G. Zeuthen (\"Die Lehre von den Kegelschnitten im Altertum\", 1886, ch. viii.). Observing, e.g., that the intercept-Porism is still true if the two fixed points are points on a conic, and the straight lines drawn through them intersect on the conic instead of on a fixed straight line, Zeuthen conjectures that the Porisms were a by-product of a fully developed projective geometry of conics. It is a fact that Lemma 31 (though it makes no mention of a conic) corresponds exactly to Apollonius's method of determining the foci of a central conic (Conics, iii. 4547 with 42). The three porisms stated by Diophantus in his \"Arithmetica\" are propositions in the theory of numbers which can all be enunciated in the form \"we can find numbers satisfying such and such conditions\"; they are sufficiently analogous therefore to the geometrical porism as defined in Pappus and Proclus.\n\n\n"}
{"id": "42877569", "url": "https://en.wikipedia.org/wiki?curid=42877569", "title": "Relationship between mathematics and physics", "text": "Relationship between mathematics and physics\n\nThe relationship between mathematics and physics has been a subject of study of philosophers, mathematicians and physicists since Antiquity, and more recently also by historians and educators. Generally considered a relationship of great intimacy, mathematics has already been described as \"an essential tool for physics\" and physics has already been described as \"a rich source of inspiration and insight in mathematics\".\n\nIn his work \"Physics\", one of the topics treated by Aristotle is about how the study carried out by mathematicians differs from that carried out by physicists. Considerations about mathematics being the language of nature can be found in the ideas of the Pythagoreans: the convictions that \"Numbers rule the world\" and \"All is number\", and two millennia later were also expressed by Galileo Galilei: \"The book of nature is written in the language of mathematics\".\n\nBefore giving a mathematical proof for the formula for the volume of a sphere, Archimedes used physical reasoning to discover the solution (imagining the balancing of bodies on a scale). From the seventeenth century, many of the most important advances in mathematics appeared motivated by the study of physics, and this continued in the following centuries (although, it has already been appointed that from the nineteenth century, mathematics started to become increasingly independent from physics). The creation and development of calculus were strongly linked to the needs of physics: There was a need for a new mathematical language to deal with the new dynamics that had arisen from the work of scholars such as Galileo Galilei and Isaac Newton. During this period there was little distinction between physics and mathematics; as an example, Newton regarded geometry as a branch of mechanics. As time progressed, increasingly sophisticated mathematics started to be used in physics. The current situation is that the mathematical knowledge used in physics is becoming increasingly sophisticated, as in the case of superstring theory.\n\nSome of the problems considered in the philosophy of mathematics are the following:\n\n\nIn recent times the two disciplines have most often been taught separately, despite all the interrelations between physics and mathematics. This led some professional mathematicians who were also interested in mathematics education, such as Felix Klein, Richard Courant, Vladimir Arnold and Morris Kline, to strongly advocate teaching mathematics in a way more closely related to the physical sciences.\n\n\n\n"}
{"id": "48347082", "url": "https://en.wikipedia.org/wiki?curid=48347082", "title": "Scope (logic)", "text": "Scope (logic)\n\nIn logic, the scope of a quantifier or a quantification is the range in the formula where the quantifier \"engages in\". It is put right after the quantifier, often in parentheses. Some authors describe this as including the variable put right after the forall or exists symbol. In the formula , for example, (or ) is the scope of the quantifier (or ).\n\nA variable in the formula is free, if and only if it does not occur in the scope of any quantifier for that variable. A term is free for a variable in the formula (i.e. free to substitute that variable that occurs free), if and only if that variable does not occur free in the scope of any quantifier for any variable in the term.\n\n"}
{"id": "362651", "url": "https://en.wikipedia.org/wiki?curid=362651", "title": "Scottish Café", "text": "Scottish Café\n\nThe Scottish Café () was the café in Lwów, Poland (now Lviv, Ukraine) where, in the 1930s and 1940s, mathematicians from the Lwów School collaboratively discussed research problems, particularly in functional analysis and topology.\n\nStanislaw Ulam recounts that the tables of the café had marble tops, so they could write in pencil, directly on the table, during their discussions. To keep the results from being lost, and after becoming annoyed with their writing directly on the table tops, Stefan Banach's wife provided the mathematicians with a large notebook, which was used for writing the problems and answers and eventually became known as the \"Scottish Book\". The book—a collection of solved, unsolved, and even probably unsolvable problems—could be borrowed by any of the guests of the café. Solving any of the problems was rewarded with prizes, with the most difficult and challenging problems having expensive prizes (during the Great Depression and on the eve of World War II), such as a bottle of fine brandy.\n\nFor problem 153, which was later recognized as being closely related to Stefan Banach's \"basis problem\", Stanisław Mazur offered the prize of a live goose. This problem was solved only in 1972 by Per Enflo, who was presented with the live goose in a ceremony that was broadcast throughout Poland.\n\nThe café building now houses the Szkotcka Restaurant & Bar (named for the original Scottish Café) and the Atlas Deluxe hotel at the street address of 27 Taras Shevchenko Prospekt.\n\nThe following mathematicians were associated with the Lwów School of Mathematics or contributed to \"The Scottish Book\":\n\n\n"}
{"id": "1735473", "url": "https://en.wikipedia.org/wiki?curid=1735473", "title": "Tomahawk (geometry)", "text": "Tomahawk (geometry)\n\nThe tomahawk is a tool in geometry for angle trisection, the problem of splitting an angle into three equal parts. The boundaries of its shape include a semicircle and two line segments, arranged in a way that resembles a tomahawk, a Native American axe. The same tool has also been called the shoemaker's knife, but that name is more commonly used in geometry to refer to a different shape, the arbelos (a curvilinear triangle bounded by three mutually tangent semicircles).\n\nThe basic shape of a tomahawk consists of a semicircle (the \"blade\" of the tomahawk), with a line segment the length of the radius extending along the same line as the diameter of the semicircle (the tip of which is the \"spike\" of the tomahawk), and with another line segment of arbitrary length (the \"handle\" of the tomahawk) perpendicular to the diameter. In order to make it into a physical tool, its handle and spike may be thickened, as long as the line segment along the handle continues to be part of the boundary of the shape. Unlike a related trisection using a carpenter's square, the other side of the thickened handle does not need to be made parallel to this line segment.\n\nIn some sources a full circle rather than a semicircle is used, or the tomahawk is also thickened along the diameter of its semicircle, but these modifications make no difference to the action of the tomahawk as a trisector.\n\nTo use the tomahawk to trisect an angle, it is placed with its handle line touching the apex of the angle, with the blade inside the angle, tangent to one of the two rays forming the angle, and with the spike touching the other ray of the angle. One of the two trisecting lines then lies on the handle segment, and the other passes through the center point of the semicircle. If the angle to be trisected is too sharp relative to the length of the tomahawk's handle, it may not be possible to fit the tomahawk into the angle in this way, but this difficulty may be worked around by repeatedly doubling the angle until it is large enough for the tomahawk to trisect it, and then repeatedly bisecting the trisected angle the same number of times as the original angle was doubled.\n\nIf the apex of the angle is labeled \"A\", the point of tangency of the blade is \"B\", the center of the semicircle is \"C\", the top of the handle is \"D\", and the spike is \"E\", then triangles \"ACD\" and \"ADE\" are both right triangles with a shared base and equal height, so they are congruent triangles. Because the sides \"AB\" and \"BC\" of triangle \"ABC\" are respectively a tangent and a radius of the semicircle, they are at right angles to each other and \"ABC\" is also a right triangle; it has the same hypotenuse as \"ACD\" and the same side lengths \"BC\" = \"CD\", so again it is congruent to the other two triangles, showing that the three angles formed at the apex are equal.\n\nAlthough the tomahawk may itself be constructed using a compass and straightedge, and may be used to trisect an angle, it does not contradict Pierre Wantzel's 1837 theorem that arbitrary angles cannot be trisected by compass and unmarked straightedge alone. The reason for this is that placing the constructed tomahawk into the required position is a form of neusis that is not allowed in compass and straightedge constructions.\n\nThe inventor of the tomahawk is unknown, but the earliest references to it come from 19th-century France. It dates back at least as far as 1835, when it appeared in a book by Claude Lucien Bergery, \"Géométrie appliquée à l'industrie, à l'usage des artistes et des ouvriers\" (3rd edition). Another early publication of the same trisection was made by Henri Brocard in 1877; Brocard in turn attributes its invention to an 1863 memoir by French naval officer Pierre-Joseph Glotin.\n\n"}
{"id": "1458024", "url": "https://en.wikipedia.org/wiki?curid=1458024", "title": "Toy theorem", "text": "Toy theorem\n\nIn mathematics, a toy theorem is a simplified version (special case) of a more general theorem. For instance, by introducing some simplifying assumptions in a theorem, one obtains a toy theorem.\n\nUsually, a toy theorem is used to illustrate the claim of a theorem. It can also be insightful to study proofs of a toy theorem derived from a non-trivial theorem. Toy theorems can also have education value. After presenting a theorem (with, say, a highly non-trivial proof), one can sometimes give some assurance that the theorem really holds, by proving a toy version of the theorem.\n\nFor instance, a toy theorem of the Brouwer fixed-point theorem is obtained by restricting the dimension to one. In this case, the Brouwer fixed-point theorem follows almost immediately from the intermediate value theorem.\n\n"}
{"id": "49188683", "url": "https://en.wikipedia.org/wiki?curid=49188683", "title": "Trillium theorem", "text": "Trillium theorem\n\nIn Euclidean geometry, the trillium theorem – (from , literally 'lemma about trident', , literally 'theorem of trillium' or 'theorem of trefoil') is a statement about properties of inscribed and circumscribed circles and their relations.\n\nLet be an arbitrary triangle. Let be its incenter and let be the point where line (the angle bisector of ) crosses the circumcircle of . Then, the theorem states that is equidistant from , , and .\nEquivalently:\nA fourth point, the excenter of relative to , also lies at the same distance from , diametrally opposite from .\n\nBy the inscribed angle theorem,\n\nSince formula_2 is angle bisector,\n\nThis theorem can be used to reconstruct a triangle starting from the locations only of one vertex, the incenter, and the circumcenter of the triangle.\nFor, let be the given vertex, be the incenter, and be the circumcenter. This information allows the successive construction of:\nHowever, for some triples of points , , and , this construction may fail, either because line is tangent to the circumcircle or because the two circles do not have two crossing points. It may also produce a triangle for which the given point is an excenter rather than the incenter. In these cases, there can be no triangle having as vertex, as incenter, and as circumcenter.\n\nOther triangle reconstruction problems, such as the reconstruction of a triangle from a vertex, incenter, and center of its nine-point circle, can be solved by reducing the problem to the case of a vertex, incenter, and circumcenter.\n\nLet and be any two of the four points given by the incenter and the three excenters of a triangle . Then and are collinear with one of the three triangle vertices. The circle with as diameter passes through the other two vertices and is centered on the circumcircle of . When one of or is the incenter, this is the trillium theorem, with line as the (internal) angle bisector of one of the triangle's angles. However, it is also true when and are both excenters; in this case, line is the external angle bisector of one of the triangle's angles.\n\n"}
{"id": "746646", "url": "https://en.wikipedia.org/wiki?curid=746646", "title": "Vorlesungen über Zahlentheorie", "text": "Vorlesungen über Zahlentheorie\n\nBased on Dirichlet's number theory course at the University of Göttingen, the were edited by Dedekind and published after Lejeune Dirichlet's death. Dedekind added several appendices to the , in which he collected further results of Lejeune Dirichlet's and also developed his own original mathematical ideas.\n\nThe cover topics in elementary number theory, algebraic number theory and analytic number theory, including modular arithmetic, quadratic congruences, quadratic reciprocity and binary quadratic forms.\n\nThe contents of Professor John Stillwell's 1999 translation of the are as follows\n\nThis translation does not include Dedekind's Supplements X and XI in which he begins to develop the theory of ideals.\n\nThe German titles of supplements X and XI are:\n\nChapters 1 to 4 cover similar ground to Gauss' , and Dedekind added footnotes which specifically cross-reference the relevant sections of the . These chapters can be thought of as a summary of existing knowledge, although Dirichlet simplifies Gauss' presentation, and introduces his own proofs in some places.\n\nChapter 5 contains Dirichlet's derivation of the class number formula for real and imaginary quadratic fields. Although other mathematicians had conjectured similar formulae, Dirichlet gave the first rigorous proof.\n\nSupplement VI contains Dirichlet's proof that an arithmetic progression of the form \"a\"+\"nd\" where \"a\" and \"d\" are coprime contains an infinite number of primes.\n\nThe can be seen as a watershed between the classical number theory of Fermat, Jacobi and Gauss, and the modern number theory of Dedekind, Riemann and Hilbert. Dirichlet does not explicitly recognise the concept of the group that is central to modern algebra, but many of his proofs show an implicit understanding of group theory.\n\nThe contains two key results in number theory which were first proved by Dirichlet. The first of these is the class number formulae for binary quadratic forms. The second is a proof that arithmetic progressions contains an infinite number of primes (known as Dirichlet's theorem); this proof introduces Dirichlet L-series. These results are important milestones in the development of analytic number theory.\n\nLeopold Kronecker's book was first published in 1901 in 2 parts and reprinted by Springer in 1978. It covers elementary and algebraic number theory, including Dirichlet's theorem.\n\nEdmund Landau's book \"Vorlesungen über Zahlentheorie\" was first published as a 3-volume set in 1927. The first half of volume 1 was published as \n\"Vorlesungen über Zahlentheorie. Aus der elementare Zahlentheorie\" in 1950, with an English translation in 1958 under the title \"Elementary number theory\". In 1969 Chelsea republished the second half of volume 1 together with volumes 2 and 3 as a single volume. \n\nVolume 1 on elementary and additive number theory includes the topics such as Dirichlet's theorem, Brun's sieve, binary quadratic forms, Goldbach's conjecture, Waring's problem, and the Hardy–Littlewood work on the singular series. Volume 2 covers topics in analytic number theory, such as estimates for the error in the prime number theorem, and topics in geometric number theory such as estimating numbers of lattice points. Volume 3 covers algebraic number theory, including ideal theory, quadratic number fields, and applications to Fermat's last theorem. Many of the results described by Landau were state of the art at the time but have since been superseded by stronger results.\n\nHelmut Hasse's book \"Vorlesungen über Zahlentheorie\" was published in 1950, and is different from and more elementary than his book \"Zahlentheorie\". It covers elementary number theory, Dirichlet's theorem, and quadratic fields.\n\n"}
{"id": "8997784", "url": "https://en.wikipedia.org/wiki?curid=8997784", "title": "What Is Mathematics?", "text": "What Is Mathematics?\n\nWhat Is Mathematics? is a mathematics book written by Richard Courant and Herbert Robbins, published in England by Oxford University Press. It is an introduction to mathematics, intended both for the mathematics student and for the general public.\n\nFirst published in 1941, it discusses number theory, geometry, topology and calculus. \nA second edition was published in 1996 with an additional chapter on recent progress in mathematics, written by Ian Stewart.\n\nThe book was based on Courant's course material.\nAlthough Robbins assisted in writing a large part of the book, \nhe had to fight for authorship.\nNevertheless, Courant alone held the copyright for the book.\nThis resulted in Robbins receiving a smaller share of the royalties.\n\nMichael Katehakis remembers Robbins' interest in the literature and Tolstoy in particular and he is convinced that the title of the book is most likely \ndue to Robbins, who was inspired by the title of the essay What is Art? by Leo Tolstoy. \nRobbins did the same in the book \n\" Great Expectations : The Theory of Optimal Stopping \" he co-authored with Yuan-Shih Chow and David Siegmund, \nwhere one can not miss the connection with the title of the novel \"Great Expectations\" by Charles Dickens.\n\nAccording to Constance Reid, Courant finalized the title after a conversation with Thomas Mann.\n\n\n\n\n"}
