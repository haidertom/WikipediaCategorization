{"id": "20860737", "url": "https://en.wikipedia.org/wiki?curid=20860737", "title": "Antiquarian science books", "text": "Antiquarian science books\n\nAntiquarian science books are original historical works (e.g., books or technical papers) concerning science, mathematics and sometimes engineering. \nThese books are important primary references for the study of the history of science and technology, they can provide valuable insights into the historical development of the various fields of scientific inquiry (History of science, History of mathematics, etc.)\n\nThe landmark are significant first (or early) editions typically worth hundreds or thousands of dollars (prices may vary widely based on condition, etc.). \nReprints of these books are often available, for example from Great Books of the Western World, Dover Publications or Google Books.\n\nIncunabula are extremely rare and valuable, but as the \"scientific revolution\" is only taken to have started around the 1540s, such works of Renaissance literature (including alchemy, Renaissance magic, etc.) are not usually included under the notion of \"scientific\" literature. Printed originals of the beginning scientific revolution thus date to the 1540s or later, notably beginning with the original publication of Copernican heliocentrism. Nicolaus Copernicus' \"De revolutionibus orbium coelestium\" of 1543 sold for more than US$2 million at auctions.\n\n\n\n\n\n\n"}
{"id": "9858698", "url": "https://en.wikipedia.org/wiki?curid=9858698", "title": "Association for Women in Mathematics", "text": "Association for Women in Mathematics\n\nThe Association for Women in Mathematics (AWM) is a professional society whose mission is to encourage women and girls to study and to have active careers in the mathematical sciences, and to promote equal opportunity for and the equal treatment of women and girls in the mathematical sciences. The AWM was founded in 1971 and incorporated in the state of Massachusetts. AWM has approximately 5200 members, including over 250 institutional members, such as colleges, universities, institutes, and mathematical societies. It offers numerous programs and workshops to mentor women and girls in the mathematical sciences. Much of AWM’s work is supported through federal grants.\n\nThe Association was founded in 1971 as the Association of Women Mathematicians, but the name was changed almost immediately. As reported in \"A Brief History of the Association for Women in Mathematics: The Presidents' Perspectives\", by Lenore Blum, \"As Judy Green remembers (and Chandler Davis, early AWM friend, concurs): 'The formal idea of women getting together and forming a caucus was first made publicly at a MAG [Mathematics Action Group] meeting in 1971 ... in Atlantic City. Joanne Darken, then an instructor at Temple University and now at the Community College of Philadelphia, stood up at the meeting and suggested that the women present remain and form a caucus. I have been able to document six women who remained: me (I was a graduate student at Maryland at the time), Joanne Darken, Mary [W.] Gray (she was already at American University), Diane Laison (then an instructor at Temple), Gloria Olive (a Senior Lecturer at the University of Otago, New Zealand who was visiting the U.S. at the time) and Annie Selden... It's not absolutely clear what happened next, except that I've personally always thought that Mary was responsible for getting the whole thing organized ...'\" Mary W. Gray was the early organizer, placing an advertisement in the February 1971 Notices of the AMS, and writing the first issue of the \"AWM Newsletter\" that May. Early goals of the association focused on equal pay for equal work, as well as equal consideration for admission to graduate school and support while there; for faculty appointments at all levels; for promotion and for tenure; for administrative appointments; and for government grants, positions on review and advisory panels and positions in professional organizations. The AWM holds an annual meeting at the Joint Mathematics Meetings. In 2011 the association initiated a biennial Research Symposium during its 40th anniversary celebration 40 Years and Counting.\n\nThe AWM sponsors three honorary lecture series.\n\n\nThe AWM sponsors several awards and prizes.\n\n\nThree recently created prizes for early-career women are also sponsored by the AWM.\n\n\nThe AWM Fellows program recognizes \"individuals who have demonstrated a sustained commitment to the support and advancement of women in the mathematical sciences\".\n\n\n\n"}
{"id": "46396972", "url": "https://en.wikipedia.org/wiki?curid=46396972", "title": "Big q-Legendre polynomials", "text": "Big q-Legendre polynomials\n\nIn mathematics, the big q-Legendre polynomials are an orthogonal family of polynomials defined in terms of Heine's basic hypergeometric series as\n\nThey obey the orthogonality relation\n\nand have the limiting behavior\n\nwhere formula_4 is the formula_5th Legendre polynomial.\n"}
{"id": "44065971", "url": "https://en.wikipedia.org/wiki?curid=44065971", "title": "Blockchain", "text": "Blockchain\n\nA blockchain, originally block chain, is a growing list of records, called \"blocks\", which are linked using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a merkle tree root hash).\n\nBy design, a blockchain is resistant to modification of the data. It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\". For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for inter-node communication and validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without alteration of all subsequent blocks, which requires consensus of the network majority. Although blockchain records are not unalterable, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been claimed with a blockchain.\n\nBlockchain was invented by Satoshi Nakamoto in 2008 to serve as the public transaction ledger of the cryptocurrency bitcoin. The invention of the blockchain for bitcoin made it the first digital currency to solve the double-spending problem without the need of a trusted authority or central server. The bitcoin design has inspired other applications, and blockchains which are readable by the public are widely used by cryptocurrencies. Blockchain is considered a type of payment rail. Private blockchains have been proposed for business use. Sources such as the \"Computerworld\" called the marketing of such blockchains without a proper security model \"snake oil\".\n\nThe first work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta. They wanted to implement a system where document timestamps could not be tampered with. In 1992, Bayer, Haber and Stornetta incorporated Merkle trees to the design, which improved its efficiency by allowing several document certificates to be collected into one block.\n\nThe first blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to add blocks to the chain without requiring them to be signed by a trusted party. The design was implemented the following year by Nakamoto as a core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network.\n\nIn August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes). In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size.\n\nThe words \"block\" and \"chain\" were used separately in Satoshi Nakamoto's original paper, but were eventually popularized as a single word, \"blockchain,\" by 2016. The term \"blockchain 2.0\" refers to new applications of the distributed blockchain database, first emerging in 2014. \"The Economist\" described one implementation of this second-generation programmable blockchain as coming with \"a programming language that allows users to write more sophisticated smart contracts, thus creating invoices that pay themselves when a shipment arrives or share certificates which automatically send their owners dividends if profits reach a certain level.\"\n\n, blockchain 2.0 implementations continue to require an off-chain oracle to access any \"external data or events based on time or market conditions [that need] to interact with the blockchain.\"\n\nIBM opened a blockchain innovation research center in Singapore in July 2016. A working group for the World Economic Forum met in November 2016 to discuss the development of governance models related to blockchain.\n\nAccording to Accenture, an application of the diffusion of innovations theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the early adopters phase. Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the Chamber of Digital Commerce.\n\nIn May 2018, Gartner found that only 1% of CIOs indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term ‘planning or [looking at] active experimentation with blockchain’.\n\nIn November 2018, Conservative MEP Emma McClarkin’s plan to utilise blockchain technology to boost trade was backed by the European Parliament’s Trade Committee. \n\nA blockchain is a decentralized, distributed and public digital ledger that is used to record transactions across many computers so that any involved record cannot be altered retroactively, without the alteration of all subsequent blocks. This allows the participants to verify and audit transactions independently and relatively inexpensively. A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests. Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double spending. A blockchain has been described as a \"value-exchange protocol\". This blockchain-based exchange of value can be completed quicker, safer and cheaper than with traditional systems. A blockchain can maintain title rights because, when properly set up to detail the exchange agreement, it provides a record that compels offer and acceptance.\n\nBlocks hold batches of valid transactions that are hashed and encoded into a Merkle tree. Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain. This iterative process confirms the integrity of the previous block, all the way back to the original genesis block.\n\nSometimes separate blocks can be produced concurrently, creating a temporary fork. In addition to a secure hash-based history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher value can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of the history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. Therefore, the probability of an entry becoming superseded decreases exponentially as more blocks are built on top of it, eventually becoming very low. For example, in a blockchain using the proof-of-work system, the chain with the most cumulative proof-of-work is always considered the valid one by the network. There are a number of methods that can be used to demonstrate a sufficient level of computation. Within a blockchain the computation is carried out redundantly rather than in the traditional segregated and parallel manner.\n\nThe \"block time\" is the average time it takes for the network to generate one extra block in the blockchain. Some blockchains create a new block as frequently as every five seconds. By the time of block completion, the included data becomes verifiable. In cryptocurrency, this is practically when the transaction takes place, so a shorter block time means faster transactions. The block time for Ethereum is set to between 14 and 15 seconds, while for bitcoin it is 10 minutes.\n\nBy storing data across its peer-to-peer network, the blockchain eliminates a number of risks that come with data being held centrally. The decentralized blockchain may use ad-hoc message passing and distributed networking.\n\nPeer-to-peer blockchain networks lack centralized points of vulnerability that computer crackers can exploit; likewise, it has no central point of failure. Blockchain security methods include the use of public-key cryptography. A \"public key\" (a long, random-looking string of numbers) is an address on the blockchain. Value tokens sent across the network are recorded as belonging to that address. A \"private key\" is like a password that gives its owner access to their digital assets or the means to otherwise interact with the various capabilities that blockchains now support. Data stored on the blockchain is generally considered incorruptible.\n\nEvery node in a decentralized system has a copy of the blockchain. Data quality is maintained by massive database replication and computational trust. No centralized \"official\" copy exists and no user is \"trusted\" more than any other. Transactions are broadcast to the network using software. Messages are delivered on a best-effort basis. Mining nodes validate transactions, add them to the block they are building, and then broadcast the completed block to other nodes. Blockchains use various time-stamping schemes, such as proof-of-work, to serialize changes. Alternative consensus methods include proof-of-stake. Growth of a decentralized blockchain is accompanied by the risk of centralization because the computer resources required to process larger amounts of data become more expensive.\n\nOpen blockchains are more user-friendly than some traditional ownership records, which, while open to the public, still require physical access to view. Because all early blockchains were permissionless, controversy has arisen over the blockchain definition. An issue in this ongoing debate is whether a private system with verifiers tasked and authorized (permissioned) by a central authority should be considered a blockchain. Proponents of permissioned or private chains argue that the term \"blockchain\" may be applied to any data structure that batches data into time-stamped blocks. These blockchains serve as a distributed version of multiversion concurrency control (MVCC) in databases. Just as MVCC prevents two transactions from concurrently modifying a single object in a database, blockchains prevent two transactions from spending the same single output in a blockchain. Opponents say that permissioned systems resemble traditional corporate databases, not supporting decentralized data verification, and that such systems are not hardened against operator tampering and revision. Nikolai Hampton of \"Computerworld\" said that \"many in-house blockchain solutions will be nothing more than cumbersome databases,\" and \"without a clear security model, proprietary blockchains should be eyed with suspicion.\"\n\nThe great advantage to an open, permissionless, or public, blockchain network is that guarding against bad actors is not required and no access control is needed. This means that applications can be added to the network without the approval or trust of others, using the blockchain as a transport layer.\n\nBitcoin and other cryptocurrencies currently secure their blockchain by requiring new entries to include a proof of work. To prolong the blockchain, bitcoin uses Hashcash puzzles. While Hashcash was designed in 1997 by Adam Back, the original idea was first proposed by Cynthia Dwork and Moni Naor and Eli Ponyatovski in their 1992 paper \"Pricing via Processing or Combatting Junk Mail\".\n\nFinancial companies have not prioritised decentralized blockchains.\nIn 2016, venture capital investment for blockchain-related projects was weakening in the USA but increasing in China. Bitcoin and many other cryptocurrencies use open (public) blockchains. , bitcoin has the highest market capitalization.\n\nPermissioned blockchains use an access control layer to govern who has access to the network. In contrast to public blockchain networks, validators on private blockchain networks are vetted by the network owner. They do not rely on anonymous nodes to validate transactions nor do they benefit from the network effect. Permissioned blockchains can also go by the name of 'consortium' or 'hybrid' blockchains.\n\nThe \"New York Times\" noted in both 2016 and 2017 that many corporations are using blockchain networks \"with private blockchains, independent of the public system.\"\n\nNikolai Hampton pointed out in \"Computerworld\" that \"There is also no need for a '51 percent' attack on a private blockchain, as the private blockchain (most likely) already controls 100 percent of all block creation resources. If you could attack or damage the blockchain creation tools on a private corporate server, you could effectively control 100 percent of their network and alter transactions however you wished.\" This has a set of particularly profound adverse implications during a financial crisis or debt crisis like the financial crisis of 2007–08, where politically powerful actors may make decisions that favor some groups at the expense of others, and \"the bitcoin blockchain is protected by the massive group mining effort. It's unlikely that any private blockchain will try to protect records using gigawatts of computing power—it's time consuming and expensive.\" He also said, \"Within a private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\"\n\nBlockchain technology can be integrated into multiple areas. The primary use of blockchains today is as a distributed ledger for cryptocurrencies, most notably bitcoin. There are a few operational products maturing from proof of concept by late 2016.\n\n, some observers remain skeptical. Steve Wilson, of Constellation Research, believes the technology has been hyped with unrealistic claims. To mitigate risk, businesses are reluctant to place blockchain at the core of the business structure.\n\nMost cryptocurrencies use blockchain technology to record transactions. For example, the bitcoin network and Ethereum network are blockchain-based. On May 8, 2018 Facebook confirmed that it is opening a new blockchain group which will be headed by David Marcus who previously was in charge of Messenger. According to The Verge Facebook is planning to launch its own cryptocurrency for facilitating payments on the platform.\n\nBlockchain-based smart contracts are proposed contracts that could be partially or fully executed or enforced without human interaction. One of the main objectives of a smart contract is automated escrow. An IMF staff discussion reported that smart contracts based on blockchain technology might reduce moral hazards and optimize the use of contracts in general. But \"no viable smart contract systems have yet emerged.\" Due to the lack of widespread use their legal status is unclear.\n\nMajor portions of the financial industry are implementing distributed ledgers for use in banking, and according to a September 2016 IBM study, this is occurring faster than expected.\n\nBanks are interested in this technology because it has potential to speed up back office settlement systems.\n\nBanks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and reduce costs.\n\nBerenberg, a German bank, believes that blockchain is an \"overhyped technology\" that has had a large number of \"proofs of concept\", but still has major challenges, and very few success stories.\n\nSome video games are based on blockchain technology. The first such game, \"Huntercoin\", was released in February, 2014. Another blockchain game is \"CryptoKitties\", launched in November 2017. The game made headlines in December 2017 when a cryptokitty character - an-in game virtual pet - was sold for US$100,000. \"CryptoKitties\" illustrated scalability problems for games on Ethereum when it created significant congestion on the Ethereum network with about 30% of all Ethereum transactions being for the game.\n\nCryptokitties also demonstrated how blockchains can be used to catalog game assets (digital assets).\n\nWithin the video game industry, while blockchain use is seen as part of a marketplace mechanism, such as with Robot Cache, blockchain is also postulated as a way to share video game assets between various games. The Blockchain Game Alliance was formed in September 2018 to explore alternative uses of blockchains in video gaming with support of Ubisoft and Fig, among others.\n\nBlockchain technology can be used to create a permanent, public, transparent ledger system for compiling data on sales, tracking digital use and payments to content creators, such as wireless users or musicians. In 2017, IBM partnered with ASCAP and PRS for Music to adopt blockchain technology in music distribution. Imogen Heap's Mycelia service has also been proposed as blockchain-based alternative \"that gives artists more control over how their songs and associated data circulate among fans and other musicians.\" Everledger is one of the inaugural clients of IBM's blockchain-based tracking service.\n\nNew distribution methods are available for the insurance industry such as peer-to-peer insurance, parametric insurance and microinsurance following the adoption of blockchain. The sharing economy and IoT are also set to benefit from blockchains because they involve many collaborating peers. Online voting is another application of the blockchain.\n\nOther designs include:\n\nIn September 2018, IBM and a start-up Hu-manity.co launched a blockchain-based app that let patients sell anonymized data to pharmaceutical companies.\n\nCurrently, there are three types of blockchain networks - public blockchains, private blockchains and consortium blockchains.\n\nA public blockchain has absolutely no access restrictions. Anyone with an internet connection can send transactions to it as well as become a validator (i.e., participate in the execution of a consensus protocol). Usually, such networks offer economic incentives for those who secure them and utilize some type of a Proof of Stake or Proof of Work algorithm.\n\nSome of the largest, most known public blockchains are Bitcoin and Ethereum.\n\nA private blockchain is permissioned. One cannot join it unless invited by the network administrators. Participant and validator access is restricted.\n\nThis type of blockchains can be considered a middle-ground for companies that are interested in the blockchain technology in general but are not comfortable with a level of control offered by public networks. Typically, they seek to incorporate blockchain into their accounting and record-keeping procedures without sacrificing autonomy and running the risk of exposing sensitive data to the public internet.\n\nA consortium blockchain is often said to be semi-decentralized. It, too, is permissioned but instead of a single organization controlling it, a number of companies might each operate a node on such a network. The administrators of a consortium chain restrict users' reading rights as they see fit and only allow a limited set of trusted nodes to execute a consensus protocol.\n\nIn October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the Massachusetts Institute of Technology access to $100 of bitcoin. The adoption rates, as studied by Catalini and Tucker (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology.\n\nThe Bank for International Settlements has criticized the public proof-of-work blockchains for high energy consumption.\n\nNicholas Weaver, of the International Computer Science Institute at the University of California, Berkeley examines blockchain's online security, and the energy efficiency of proof-of-work public blockchains, and in both cases finds it grossly inadequate.\n\nIn September 2015, the first peer-reviewed academic journal dedicated to cryptocurrency and blockchain technology research, \"Ledger\", was announced. The inaugural issue was published in December 2016. The journal covers aspects of mathematics, computer science, engineering, law, economics and philosophy that relate to cryptocurrencies such as bitcoin.\n\nThe journal encourages authors to digitally sign a file hash of submitted papers, which will then be timestamped into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address in the first page of their papers.\n\n\n\n"}
{"id": "49590020", "url": "https://en.wikipedia.org/wiki?curid=49590020", "title": "Canon Sinuum (Bürgi)", "text": "Canon Sinuum (Bürgi)\n\nThe Canon Sinuum was a historic table of sines thought to have given the sines to 8 sexagesimal places between 0 and 90 degrees in steps of 2 arc seconds. Some authors believe that the table was only between 0 and 45 degrees. It was created by Jost Bürgi at the end of the 16th century. Such tables were essential for navigation at sea. Johannes Kepler called the \"Canon Sinuum\" the most precise known table of sines.\n\nThis table is thought to be lost.\n\nThe \"Canon Sinuum\" was computed by Bürgi's algorithms explained in his work Fundamentum Astronomiae presented to Emperor Rudolf II. in 1592. These algorithms made use of differences and were one of the early uses of difference calculus. The largest trigonometrical table actually contained in the Fundamentum Astronomiae is a table giving the sines for every minute of the quadrant and to 5 to 7 sexagesimal places.\n\nThe manuscript of Fundamentum Astronomiae is now in the collection of the Biblioteka Uniwersytecka in Wrocław, Poland.\n\n"}
{"id": "51363226", "url": "https://en.wikipedia.org/wiki?curid=51363226", "title": "Convolution quotient", "text": "Convolution quotient\n\nIn mathematics, a convolution quotient is to the operation of convolution as a quotient of integers is to multiplication. Convolution quotients were introduced by , and their theory is sometimes called \"Mikusiński's operational calculus\". For two functions \"ƒ\", \"g\", the pair (\"ƒ\", \"g\") has the same convolution quotient as the pair (\"h\" * \"ƒ\",\"h\" * \"g\").\n\nConvolution quotients are used in an approach to making Dirac's delta function and other generalized functions logically rigorous.\n\n"}
{"id": "17910805", "url": "https://en.wikipedia.org/wiki?curid=17910805", "title": "Counting board", "text": "Counting board\n\nThe counting board is the precursor of the abacus, and the earliest known form of a counting device (excluding fingers and other very simple methods). Counting boards were made of stone or wood, and the counting was done on the board with beads, or pebbles etc. Not many boards survive because of the perishable materials used in their construction. \n\nThe oldest known counting board, the Salamis Tablet (\"c.\" 300 BC) was discovered on the Greek island of Salamis in 1899. It is thought to have been used by the Babylonians in about 300 BC and is more of a gaming board than a calculating device. It is marble, about 150 x 75 x 4.5 cm, and is in the Greek National museum in Athens. It has carved Greek letters and parallel grooves. \n\nThe German mathematicican Adam Ries described the use of counting boards in \"Rechenbuch auf Linien und Ziphren in allerlei Handthierung / geschäfften und Kaufmanschafft\". In the novel \"Wolf Hall\", Hilary Mantel refers to Thomas Cromwell using a counting board in 16th-century England.\n\n"}
{"id": "437701", "url": "https://en.wikipedia.org/wiki?curid=437701", "title": "Dependent and independent variables", "text": "Dependent and independent variables\n\nIn mathematical modeling, statistical modeling and experimental sciences, the values of dependent variables depend on the values of independent variables. The dependent variables represent the output or outcome whose variation is being studied. The independent variables, also known in a statistical context as regressors, represent inputs or causes, that is, potential reasons for variation. In an experiment, any variable that the experimenter manipulates can be called an independent variable. Models and experiments test the effects that the independent variables have on the dependent variables. Sometimes, even if their influence is not of direct interest, independent variables may be included for other reasons, such as to account for their potential confounding effect.\n\nIn mathematics, a function is a rule for taking an input (in the simplest case, a number or set of numbers) and providing an output (which may also be a number). A symbol that stands for an arbitrary input is called an independent variable, while a symbol that stands for an arbitrary output is called a dependent variable. The most common symbol for the input is \"x\", and the most common symbol for the output is \"y\"; the function itself is commonly written formula_1.\n\nIt is possible to have multiple independent variables and/or multiple dependent variables. For instance, in multivariable calculus, one often encounters functions of the form formula_2, where \"z\" is a dependent variable and \"x\" and \"y\" are independent variables. Functions with multiple outputs are often referred to as vector-valued functions.\n\nIn set theory, a function between a set X and a set Y is a subset of the Cartesian product formula_3 such that every element of X appears in an ordered pair with exactly one element of Y. In this situation, a symbol representing an element of X may be called an independent variable and a symbol representing an element of Y may be called a dependent variable, such as when X is a manifold and the symbol \"x\" represents an arbitrary point in the manifold. However, many advanced textbooks do not distinguish between dependent and independent variables.\n\nIn an experiment, a variable, manipulated by an experimenter, is called an independent variable. The dependent variable is the event expected to change when the independent variable is manipulated.\n\nIn data mining tools (for multivariate statistics and machine learning), the dependent variable is assigned a \"role\" as (or in some tools as \"label attribute\"), while an independent variable may be assigned a role as \"regular variable\". Known values for the target variable are provided for the training data set and test data set, but should be predicted for other data. The target variable is used in supervised learning algorithms but not in unsupervised learning.\n\nIn mathematical modeling, the dependent variable is studied to see if and how much it varies as the independent variables vary. In the simple stochastic linear model formula_4 the term formula_5 is the \"i\" value of the dependent variable and formula_6 is the \"i\" value of the independent variable. The term formula_7 is known as the \"error\" and contains the variability of the dependent variable not explained by the independent variable.\n\nWith multiple independent variables, the model is formula_8, where \"n\" is the number of independent variables.\n\nIn simulation, the dependent variable is changed in response to changes in the independent variables.\n\nDepending on the context, an independent variable is sometimes called a \"predictor variable\", regressor, covariate, \"controlled variable\", \"manipulated variable\", \"explanatory variable\", exposure variable (see reliability theory), \"risk factor\" (see medical statistics), \"feature\" (in machine learning and pattern recognition) or \"input variable.\"\nIn econometrics, the term \"control variable\" is usually used instead of \"covariate\". \n\nDepending on the context, a dependent variable is sometimes called a \"response variable\", \"regressand\", \"criterion\", \"predicted variable\", \"measured variable\", \"explained variable\", \"experimental variable\", \"responding variable\", \"outcome variable\", \"output variable\" or \"label\".\n\n\"Explanatory variable\" is preferred by some authors over \"independent variable\" when the quantities treated as independent variables may not be statistically independent or independently manipulable by the researcher. If the independent variable is referred to as an \"explanatory variable\" then the term \"response variable\" is preferred by some authors for the dependent variable.\n\n\"Explained variable\" is preferred by some authors over \"dependent variable\" when the quantities treated as \"dependent variables\" may not be statistically dependent. If the dependent variable is referred to as an \"explained variable\" then the term \"predictor variable\" is preferred by some authors for the independent variable.\n\nVariables may also be referred to by their form: continuous, binary/dichotomous, nominal categorical, and ordinal categorical, among others.\n\nAn example is provided by the analysis of trend in sea level by . Here the dependent variable (and variable of most interest) was the annual mean sea level at a given location for which a series of yearly values were available. The primary independent variable was time. Use was made of a covariate consisting of yearly values of annual mean atmospheric pressure at sea level. The results showed that inclusion of the covariate allowed improved estimates of the trend against time to be obtained, compared to analyses which omitted the covariate.\n\nA variable may be thought to alter the dependent or independent variables, but may not actually be the focus of the experiment. So that variable will be kept constant or monitored to try to minimize its effect on the experiment. Such variables may be designated as either a \"controlled variable\", \"control variable\", or \"extraneous variable\".\n\nExtraneous variables, if included in a regression analysis as independent variables, may aid a researcher with accurate response parameter estimation, prediction, and goodness of fit, but are not of substantive interest to the hypothesis under examination. For example, in a study examining the effect of post-secondary education on lifetime earnings, some extraneous variables might be gender, ethnicity, social class, genetics, intelligence, age, and so forth. A variable is extraneous only when it can be assumed (or shown) to influence the dependent variable. If included in a regression, it can improve the fit of the model. If it is excluded from the regression and if it has a non-zero covariance with one or more of the independent variables of interest, its omission will bias the regression's result for the effect of that independent variable of interest. This effect is called confounding or omitted variable bias; in these situations, design changes and/or controlling for a variable statistical control is necessary.\n\nExtraneous variables are often classified into three types:\n\nIn modelling, variability that is not covered by the independent variable is designated by formula_9 and is known as the \"residual\", \"side effect\", \"error\", \"unexplained share\", \"residual variable\", or \"tolerance\".\n\n\n"}
{"id": "10308785", "url": "https://en.wikipedia.org/wiki?curid=10308785", "title": "Differentiation rules", "text": "Differentiation rules\n\nThis is a summary of differentiation rules, that is, rules for computing the derivative of a function in calculus.\n\nUnless otherwise stated, all functions are functions of real numbers (R) that return real values; although more generally, the formulae below apply wherever they are well defined—including complex numbers (C).\n\nFor any functions formula_1 and formula_2 and any real numbers formula_3 and formula_4 the derivative of the function formula_5 with respect to formula_6 is\n\nIn Leibniz's notation this is written as:\n\nSpecial cases include:\n\n rule\"\n\nFor the functions \"f\" and \"g\", the derivative of the function \"h\"(\"x\") = \"f\"(\"x\") \"g\"(\"x\")\nwith respect to \"x\" is\nIn Leibniz's notation this is written\n\nThe derivative of the function formula_14 with respect to formula_6 is\n\nIn Leibniz's notation this is correctly written as:\n\noften abridged to formula_18\nFocusing on the notion of maps, and the differential being a map formula_19, this is written in a more concise way as:\n\nIf the function \"f\" has an inverse function \"g\", meaning that and , then\n\nIn Leibniz notation, this is written as\n\nIf formula_23, for any real number formula_24 then \n\nSpecial cases include:\n\nCombining this rule with the linearity of the derivative and the addition rule permits the computation of the derivative of any polynomial.\n\nThe derivative of \"h\"(\"x\") = 1/\"f\"(\"x\") for any (nonvanishing) function \"f\" is:\n\nIn Leibniz's notation, this is written\n\nThe reciprocal rule can be derived from the quotient rule.\n\nIf \"f\" and \"g\" are functions, then:\nThis can be derived from the product rule.\n\nThe elementary power rule generalizes considerably. The most general power rule is the functional power rule: for any functions \"f\" and \"g\",\nwherever both sides are well defined.\n\nSpecial cases:\n\nnote that the equation above is true for all \"c\", but the derivative for c < 0 yields a complex number.\n\nthe equation above is also true for all \"c\" but yields a complex number if c<0.\n\nThe logarithmic derivative is another way of stating the rule for differentiating the logarithm of a function (using the chain rule):\n\nLogarithmic differentiation is a technique which uses logarithms and its differentiation rules to simplify certain expressions before actually applying the derivative.\nLogarithms can be used to remove exponents, convert products into sums, and convert division into subtraction, each of which may lead to a simplified expression for taking derivatives.\n\nIt is common to additionally define an inverse tangent function with two arguments, formula_40. Its value lies in the range formula_41 and reflects the quadrant of the point formula_42. For the first and fourth quadrant (i.e. formula_43) one has formula_44. Its partial derivatives are\n\nSuppose that it is required to differentiate with respect to \"x\" the function\n\nwhere the functions formula_46 and formula_47 are both continuous in both formula_48 and formula_6 in some region of the formula_50 plane, including formula_51 formula_52, and the functions formula_53 and formula_54 are both continuous and both have continuous derivatives for formula_52. Then for formula_56:\n\nThis formula is the general form of the Leibniz integral rule and can be derived using the \nfundamental theorem of calculus.\n\nSome rules exist for computing the \"n\"th derivative of functions, where \"n\" is a positive integer. These include:\n\nIf \"f\" and \"g\" are \"n\" times differentiable, then\n\nwhere formula_59 and the set formula_60 consists of all non-negative integer solutions of the Diophantine equation formula_61.\n\nIf \"f\" and \"g\" are \"n\" times differentiable, then\n\n\nThese rules are given in many books, both on elementary and advanced calculus, in pure and applied mathematics. Those in this article (in addition to the above references) can be found in:\n\n"}
{"id": "195407", "url": "https://en.wikipedia.org/wiki?curid=195407", "title": "Einstein notation", "text": "Einstein notation\n\nIn mathematics, especially in applications of linear algebra to physics, the Einstein notation or Einstein summation convention is a notational convention that implies summation over a set of indexed terms in a formula, thus achieving notational brevity. As part of mathematics it is a notational subset of Ricci calculus; however, it is often used in applications in physics that do not distinguish between tangent and cotangent spaces. It was introduced to physics by Albert Einstein in 1916.\n\nAccording to this convention, when an index variable appears twice in a single term and is not otherwise defined (see free and bound variables), it implies summation of that term over all the values of the index. So where the indices can range over the set ,\n\nis simplified by the convention to:\nformula_2.\n\nThe upper indices are not exponents but are indices of coordinates, coefficients or basis vectors. That is, in this context should be understood as the second component of rather than the square of (this can occasionally lead to ambiguity). The upper index position in is because, typically, an index occurs once in an upper (superscript) and once in a lower (subscript) position in a term (see 'Application' below). And typically would be equivalent to the traditional .\n\nIn general relativity, a common convention is that \n\nIn general, indices can range over any indexing set, including an infinite set. This should not be confused with a typographically similar convention used to distinguish between tensor index notation and the closely related but distinct basis-independent abstract index notation.\n\nAn index that is summed over is a \"summation index\", in this case \". It is also called a dummy index since any symbol can replace \" without changing the meaning of the expression provided that it does not collide with index symbols in the same term.\n\nAn index that is not summed over is a \"free index\" and should appear only once per term. If such an index does appear, it usually also appears in terms belonging to the same sum, with the exception of special values such as zero.\n\nEinstein notation can be applied in slightly different ways. Typically, each index occurs once in an upper (superscript) and once in a lower (subscript) position in a term; however, the convention can be applied more generally to any repeated indices within a term. When dealing with covariant and contravariant vectors, where the position of an index also indicates the type of vector, the first case usually applies; a covariant vector can only be contracted with a contravariant vector, corresponding to summation of the products of coefficients. On the other hand, when there is a fixed coordinate basis (or when not considering coordinate vectors), one may choose to use only subscripts; see \"\" below.\n\nIn terms of covariance and contravariance of vectors, \n\nThey transform contravariantly or covariantly, respectively, with respect to change of basis.\n\nIn recognition of this fact, the following notation uses the same symbol both for a (co)vector and its \"components\", as in:\n\nwhere is the vector and are its components (not the th covector ), is the covector and are its components.\n\nIn the presence of a non-degenerate form (an isomorphism , for instance a Riemannian metric or Minkowski metric), one can raise and lower indices.\n\nA basis gives such a form (via the dual basis), hence when working on with a Euclidean metric and a fixed orthonormal basis, one has the option to work with only subscripts.\n\nHowever, if one changes coordinates, the way that coefficients change depends on the variance of the object, and one cannot ignore the distinction; see covariance and contravariance of vectors.\n\nIn the above example, vectors are represented as matrices (column vectors), while covectors are represented as matrices (row covectors).\n\nWhen using the column vector convention\n\n\n\nThe virtue of Einstein notation is that it represents the invariant quantities with a simple notation.\n\nIn physics, a scalar is invariant under transformations of basis. In particular, a Lorentz scalar is invariant under a Lorentz transformation. The individual terms in the sum are not. When the basis is changed, the \"components\" of a vector change by a linear transformation described by a matrix. This led Einstein to propose the convention that repeated indices imply the summation is to be done.\n\nAs for covectors, they change by the inverse matrix. This is designed to guarantee that the linear function associated with the covector, the sum above, is the same no matter what the basis is.\n\nThe value of the Einstein convention is that it applies to other vector spaces built from using the tensor product and duality. For example, , the tensor product of with itself, has a basis consisting of tensors of the form . Any tensor in can be written as:\n\n, the dual of , has a basis , , ..., </sup> which obeys the rule\nwhere is the Kronecker delta. As\n\nthe row/column coordinates on a matrix correspond to the upper/lower indices on the tensor product.\n\nIn Einstein notation, the usual element reference for the th row and th column of matrix becomes . We can then write the following operations in Einstein notation as follows.\n\n\nUsing an orthogonal basis, the inner product is the sum of corresponding components multiplied together:\n\nThis can also be calculated by multiplying the covector on the vector.\n\n\nAgain using an orthogonal basis (in 3 dimensions) the cross product intrinsically involves summations over permutations of components:\n\nwhere\n\n\nThe product of a matrix with a column vector is :\n\nequivalent to\n\nThis is a special case of matrix multiplication.\n\n\nThe matrix product of two matrices and is:\n\nequivalent to\n\n\nFor a square matrix , the trace is the sum of the diagonal elements, hence the sum over a common index .\n\n\nThe outer product of the column vector by the row vector yields an matrix :\n\nSince and represent two \"different\" indices, there is no summation and the indices are not eliminated by the multiplication.\n\n\nGiven a tensor, one can raise an index or lower an index by contracting the tensor with the metric tensor, . For example, take the tensor , one can raise an index:\n\nformula_17\n\nOr one can lower an index:\n\nformula_18\n\n\n"}
{"id": "169358", "url": "https://en.wikipedia.org/wiki?curid=169358", "title": "Foundations of mathematics", "text": "Foundations of mathematics\n\nFoundations of mathematics is the study of the philosophical and logical and/or algorithmic basis of mathematics, or, in a broader sense, the mathematical investigation of what underlies the philosophical theories concerning the nature of mathematics. In this latter sense, the distinction between foundations of mathematics and philosophy of mathematics turns out to be quite vague.\nFoundations of mathematics can be conceived as the study of the basic mathematical concepts (set, function, geometrical figure, number, etc.) and how they form hierarchies of more complex structures and concepts, especially the fundamentally important structures that form the language of mathematics (formulas, theories and their models giving a meaning to formulas, definitions, proofs, algorithms, etc.) also called metamathematical concepts, with an eye to the philosophical aspects and the unity of mathematics. The search for foundations of mathematics is a central question of the philosophy of mathematics; the abstract nature of mathematical objects presents special philosophical challenges.\n\nThe foundations of mathematics as a whole does not aim to contain the foundations of every mathematical topic.\nGenerally, the \"foundations\" of a field of study refers to a more-or-less systematic analysis of its most basic or fundamental concepts, its conceptual unity and its natural ordering or hierarchy of concepts, which may help to connect it with the rest of human knowledge. The development, emergence and clarification of the foundations can come late in the history of a field, and may not be viewed by everyone as its most interesting part.\n\nMathematics always played a special role in scientific thought, serving since ancient times as a model of truth and rigor for rational inquiry, and giving tools or even a foundation for other sciences (especially physics). Mathematics' many developments towards higher abstractions in the 19th century brought new challenges and paradoxes, urging for a deeper and more systematic examination of the nature and criteria of mathematical truth, as well as a unification of the diverse branches of mathematics into a coherent whole.\n\nThe systematic search for the foundations of mathematics started at the end of the 19th century and formed a new mathematical discipline called mathematical logic, with strong links to theoretical computer science.\nIt went through a series of crises with paradoxical results, until the discoveries stabilized during the 20th century as a large and coherent body of mathematical knowledge with several aspects or components (set theory, model theory, proof theory, etc.), whose detailed properties and possible variants are still an active research field.\nIts high level of technical sophistication inspired many philosophers to conjecture that it can serve as a model or pattern for the foundations of other sciences.\n\nWhile the practice of mathematics had previously developed in other civilizations, special interest in its theoretical and foundational aspects was clearly evident in the work of the Ancient Greeks.\n\nEarly Greek philosophers disputed as to which is more basic, arithmetic or geometry.\nZeno of Elea (490 c. 430 BC) produced four paradoxes that seem to show the impossibility of change. The Pythagorean school of mathematics originally insisted that only natural and rational numbers exist. The discovery of the irrationality of , the ratio of the diagonal of a square to its side (around 5th century BC), was a shock to them which they only reluctantly accepted. The discrepancy between rationals and reals was finally resolved by Eudoxus of Cnidus (408–355 BC), a student of Plato, who reduced the comparison of irrational ratios to comparisons of multiples (rational ratios), thus anticipating the definition of real numbers by Richard Dedekind (1831–1916).\n\nIn the \"Posterior Analytics\", Aristotle (384–322 BC) laid down the axiomatic method for organizing a field of knowledge logically by means of primitive concepts, axioms, postulates, definitions, and theorems. Aristotle took a majority of his examples for this from arithmetic and from geometry.\nThis method reached its high point with Euclid's \"Elements\" (300 BC), a treatise on mathematics structured with very high standards of rigor: Euclid justifies each proposition by a demonstration in the form of chains of syllogisms (though they do not always conform strictly to Aristotelian templates).\nAristotle's syllogistic logic, together with the axiomatic method exemplified by Euclid's \"Elements\", are recognized as scientific achievements of ancient Greece.\n\nStarting from the end of the 19th century, a Platonist view of mathematics became common among practicing mathematicians.\n\nThe \"concepts\" or, as Platonists would have it, the \"objects\" of mathematics are abstract and remote from everyday perceptual experience: geometrical figures are conceived as idealities to be distinguished from effective drawings and shapes of objects, and numbers are not confused with the counting of concrete objects. Their existence and nature present special philosophical challenges: How do mathematical objects differ from their concrete representation? Are they located in their representation, or in our minds, or somewhere else? How can we know them?\n\nThe ancient Greek philosophers took such questions very seriously. Indeed, many of their general philosophical discussions were carried on with extensive reference to geometry and arithmetic. Plato (424/423 BC 348/347 BC) insisted that mathematical objects, like other platonic \"Ideas\" (forms or essences), must be perfectly abstract and have a separate, non-material kind of existence, in a world of mathematical objects independent of humans. He believed that the truths about these objects also exist independently of the human mind, but is \"discovered\" by humans. In the \"Meno\" Plato's teacher Socrates asserts that it is possible to come to know this truth by a process akin to memory retrieval.\n\nAbove the gateway to Plato's academy appeared a famous inscription: \"Let no one who is ignorant of geometry enter here\". In this way Plato indicated his high opinion of geometry. He regarded geometry as \"the first essential in the training of philosophers\", because of its abstract character.\n\nThis philosophy of \"Platonist mathematical realism\" is shared by many mathematicians. It can be argued that Platonism somehow comes as a necessary assumption underlying any mathematical work.\n\nIn this view, the laws of nature and the laws of mathematics have a similar status, and the effectiveness ceases to be unreasonable. Not our axioms, but the very real world of mathematical objects forms the foundation.\n\nAristotle dissected and rejected this view in his Metaphysics. These questions provide much fuel for philosophical analysis and debate.\n\nFor over 2,000 years, Euclid's Elements stood as a perfectly solid foundation for mathematics, as its methodology of rational exploration guided mathematicians, philosophers, and scientists well into the 19th century.\n\nThe Middle Ages saw a dispute over the ontological status of the universals (platonic Ideas): Realism asserted their existence independently of perception; conceptualism asserted their existence within the mind only; nominalism denied either, only seeing universals as names of collections of individual objects (following older speculations that they are words, \"logoi\").\n\nRené Descartes published \"La Géométrie\" (1637), aimed at reducing geometry to algebra by means of coordinate systems, giving algebra a more foundational role (while the Greeks embedded arithmetic into geometry by identifying whole numbers with evenly spaced points on a line). Descartes' book became famous after 1649 and paved the way to infinitesimal calculus.\n\nIsaac Newton (1642–1727) in England and Leibniz (1646–1716) in Germany independently developed the infinitesimal calculus based on heuristic methods greatly efficient, but direly lacking rigorous justifications. Leibniz even went on to explicitly describe infinitesimals as actual infinitely small numbers (close to zero). Leibniz also worked on formal logic but most of his writings on it remained unpublished until 1903.\n\nThe Protestant philosopher George Berkeley (1685–1753), in his campaign against the religious implications of Newtonian mechanics, wrote a pamphlet on the lack of rational justifications of infinitesimal calculus: \"They are neither finite quantities, nor quantities infinitely small, nor yet nothing. May we not call them the ghosts of departed quantities?\"\n\nThen mathematics developed very rapidly and successfully in physical applications, but with little attention to logical foundations.\n\nIn the 19th century, mathematics became increasingly abstract. Concerns about logical gaps and inconsistencies in different fields led to the development of axiomatic systems.\n\nCauchy (1789–1857) started the project of formulating and proving the theorems of infinitesimal calculus in a rigorous manner, rejecting the heuristic principle of the generality of algebra exploited by earlier authors. In his 1821 work \"Cours d'Analyse\" he defines infinitely small quantities in terms of decreasing sequences that converge to 0, which he then used to define continuity. But he did not formalize his notion of convergence.\n\nThe modern (ε, δ)-definition of limit and continuous functions was first developed by Bolzano in 1817, but remained relatively unknown. It gives a rigorous foundation of infinitesimal calculus based on the set of real numbers, arguably resolving the Zeno paradoxes and Berkeley's arguments.\n\nMathematicians such as Karl Weierstrass (1815–1897) discovered pathological functions such as continuous, nowhere-differentiable functions. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate. Weierstrass began to advocate the arithmetization of analysis, to axiomatize analysis using properties of the natural numbers.\n\nIn 1858, Dedekind proposed a definition of the real numbers as cuts of rational numbers. This reduction of real numbers and continuous functions in terms of rational numbers, and thus of natural numbers, was later integrated by Cantor in his set theory, and axiomatized in terms of second order arithmetic by Hilbert and Bernays.\n\nFor the first time, the limits of mathematics were explored. Niels Henrik Abel (1802–1829), a Norwegian, and Évariste Galois, (1811–1832) a Frenchman, investigated the solutions of various polynomial equations, and proved that there is no general algebraic solution to equations of degree greater than four (Abel–Ruffini theorem). With these concepts, Pierre Wantzel (1837) proved that straightedge and compass alone cannot trisect an arbitrary angle nor double a cube. In 1882, Lindemann building on the work of Hermite showed that a straightedge and compass quadrature of the circle (construction of a square equal in area to a given circle) was also impossible by proving that is a transcendental number. Mathematicians had attempted to solve all of these problems in vain since the time of the ancient Greeks.\n\nAbel and Galois's works opened the way for the developments of group theory (which would later be used to study symmetry in physics and other fields), and abstract algebra. Concepts of vector spaces emerged from the conception of barycentric coordinates by Möbius in 1827, to the modern definition of vector spaces and linear maps by Peano in 1888. Geometry was no more limited to three dimensions.\nThese concepts did not generalize numbers but combined notions of functions and sets which were not yet formalized, breaking away from familiar mathematical objects.\n\nAfter many failed attempts to derive the parallel postulate from other axioms, the study of the still hypothetical hyperbolic geometry by Johann Heinrich Lambert (1728–1777) led him to introduce the hyperbolic functions and compute the area of a hyperbolic triangle (where the sum of angles is less than 180°). Then the Russian mathematician Nikolai Lobachevsky (1792–1856) established in 1826 (and published in 1829) the coherence of this geometry (thus the independence of the parallel postulate), in parallel with the Hungarian mathematician János Bolyai (1802–1860) in 1832, and with Gauss.\nLater in the 19th century, the German mathematician Bernhard Riemann developed Elliptic geometry, another non-Euclidean geometry where no parallel can be found and the sum of angles in a triangle is more than 180°. It was proved consistent by defining point to mean a pair of antipodal points on a fixed sphere and line to mean a great circle on the sphere. At that time, the main method for proving the consistency of a set of axioms was to provide a model for it.\n\nOne of the traps in a deductive system is circular reasoning, a problem that seemed to befall projective geometry until it was resolved by Karl von Staudt. As explained by Russian historians:\n\nThe purely geometric approach of von Staudt was based on the complete quadrilateral to express the relation of projective harmonic conjugates. Then he created a means of expressing the familiar numeric properties with his Algebra of Throws. English language versions of this process of deducing the properties of a field can be found in either the book by Oswald Veblen and John Young, \"Projective Geometry\" (1938), or more recently in John Stillwell's \"Four Pillars of Geometry\" (2005). Stillwell writes on page 120\n\nThe algebra of throws is commonly seen as a feature of cross-ratios since students ordinarily rely upon numbers without worry about their basis. However, cross-ratio calculations use metric features of geometry, features not admitted by purists. For instance, in 1961 Coxeter wrote \"Introduction to Geometry\" without mention of cross-ratio.\n\nAttempts of formal treatment of mathematics had started with Leibniz and Lambert (1728–1777), and continued with works by algebraists such as George Peacock (1791–1858).\nSystematic mathematical treatments of logic came with the British mathematician George Boole (1847) who devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1 and logical combinations (conjunction, disjunction, implication and negation) are operations similar to the addition and multiplication of integers. Additionally, De Morgan published his laws in 1847. Logic thus became a branch of mathematics. Boolean algebra is the starting point of mathematical logic and has important applications in computer science.\n\nCharles Sanders Peirce built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885.\n\nThe German mathematician Gottlob Frege (1848–1925) presented an independent development of logic with quantifiers in his Begriffsschrift (formula language) published in 1879, a work generally considered as marking a turning point in the history of logic. He exposed deficiencies in Aristotle's \"Logic\", and pointed out the three expected properties of a mathematical theory:\n\n\nHe then showed in \"Grundgesetze der Arithmetik (Basic Laws of Arithmetic)\" how arithmetic could be formalised in his new logic.\n\nFrege's work was popularized by Bertrand Russell near the turn of the century. But Frege's two-dimensional notation had no success. Popular notations were (x) for universal and (∃x) for existential quantifiers, coming from Giuseppe Peano and William Ernest Johnson until the ∀ symbol was introduced by Gerhard Gentzen in 1935 and became canonical in the 1960s.\n\nFrom 1890 to 1905, Ernst Schröder published \"Vorlesungen über die Algebra der Logik\" in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.\n\nThe formalization of arithmetic (the theory of natural numbers) as an axiomatic theory started with Peirce in 1881 and continued with Richard Dedekind and Giuseppe Peano in 1888. This was still a second-order axiomatization (expressing induction in terms of arbitrary subsets, thus with an implicit use of set theory) as concerns for expressing theories in first-order logic were not yet understood. In Dedekind's work, this approach appears as completely characterizing natural numbers and providing recursive definitions of addition and multiplication from the successor function and mathematical induction.\n\nThe foundational crisis of mathematics (in German \"Grundlagenkrise der Mathematik\") was the early 20th century's term for the search for proper foundations of mathematics.\n\nSeveral schools of the philosophy of mathematics ran into difficulties one after the other in the 20th century, as the assumption that mathematics had any foundation that could be consistently stated within mathematics itself was heavily challenged by the discovery of various paradoxes (such as Russell's paradox).\n\nThe name \"paradox\" should not be confused with \"contradiction\". A contradiction in a formal theory is a formal proof of an absurdity inside the theory (such as ), showing that this theory is inconsistent and must be rejected. But a paradox may be either a surprising but true result in a given formal theory, or an informal argument leading to a contradiction, so that a candidate theory, if it is to be formalized, must disallow at least one of its steps; in this case the problem is to find a satisfying theory without contradiction. Both meanings may apply if the formalized version of the argument forms the proof of a surprising truth. For instance, Russell's paradox may be expressed as \"there is no set of all sets\" (except in some marginal axiomatic set theories).\n\nVarious schools of thought opposed each other. The leading school was that of the formalist approach, of which David Hilbert was the foremost proponent, culminating in what is known as Hilbert's program, which thought to ground mathematics on a small basis of a logical system proved sound by metamathematical finitistic means. The main opponent was the intuitionist school, led by L. E. J. Brouwer, which resolutely discarded formalism as a meaningless game with symbols (van Dalen, 2008). The fight was acrimonious. In 1920 Hilbert succeeded in having Brouwer, whom he considered a threat to mathematics, removed from the editorial board of \"Mathematische Annalen\", the leading mathematical journal of the time.\n\nAt the beginning of the 20th century, three schools of philosophy of mathematics opposed each other: Formalism, Intuitionism and Logicism.\n\nIt has been claimed that formalists, such as David Hilbert (1862–1943), hold that mathematics is only a language and a series of games. Indeed, he used the words \"formula game\" in his 1927 response to L. E. J. Brouwer's criticisms:\n\nThus Hilbert is insisting that mathematics is not an \"arbitrary\" game with \"arbitrary\" rules; rather it must agree with how our thinking, and then our speaking and writing, proceeds.\n\nThe foundational philosophy of formalism, as exemplified by David Hilbert, is a response to the paradoxes of set theory, and is based on formal logic. Virtually all mathematical theorems today can be formulated as theorems of set theory. The truth of a mathematical statement, in this view, is represented by the fact that the statement can be derived from the axioms of set theory using the rules of formal logic.\n\nMerely the use of formalism alone does not explain several issues: why we should use the axioms we do and not some others, why we should employ the logical rules we do and not some others, why do \"true\" mathematical statements (e.g., the laws of arithmetic) appear to be true, and so on. Hermann Weyl would ask these very questions of Hilbert:\n\nIn some cases these questions may be sufficiently answered through the study of formal theories, in disciplines such as reverse mathematics and computational complexity theory. As noted by Weyl, formal logical systems also run the risk of inconsistency; in Peano arithmetic, this arguably has already been settled with several proofs of consistency, but there is debate over whether or not they are sufficiently finitary to be meaningful. Gödel's second incompleteness theorem establishes that logical systems of arithmetic can never contain a valid proof of their own consistency. What Hilbert wanted to do was prove a logical system \"S\" was consistent, based on principles \"P\" that only made up a small part of \"S\". But Gödel proved that the principles \"P\" could not even prove \"P\" to be consistent, let alone \"S\".\n\nIntuitionists, such as L. E. J. Brouwer (1882–1966), hold that mathematics is a creation of the human mind. Numbers, like fairy tale characters, are merely mental entities, which would not exist if there were never any human minds to think about them.\n\nThe foundational philosophy of \"intuitionism\" or \"constructivism\", as exemplified in the extreme by Brouwer and Stephen Kleene, requires proofs to be \"constructive\" in nature the existence of an object must be demonstrated rather than inferred from a demonstration of the impossibility of its non-existence. For example, as a consequence of this the form of proof known as reductio ad absurdum is suspect.\n\nSome modern theories in the philosophy of mathematics deny the existence of foundations in the original sense. Some theories tend to focus on mathematical practice, and aim to describe and analyze the actual working of mathematicians as a social group. Others try to create a cognitive science of mathematics, focusing on human cognition as the origin of the reliability of mathematics when applied to the real world. These theories would propose to find foundations only in human thought, not in any objective outside construct. The matter remains controversial.\n\nLogicism is a school of thought, and research programme, in the philosophy of mathematics, based on the thesis that mathematics is an extension of a logic or that some or all mathematics may be derived in a suitable formal system whose axioms and rules of inference are 'logical' in nature . Bertrand Russell and Alfred North Whitehead championed this theory initiated by Gottlob Frege and influenced by Richard Dedekind\n\nMany researchers in axiomatic set theory have subscribed to what is known as set-theoretic Platonism, exemplified by Kurt Gödel.\n\nSeveral set theorists followed this approach and actively searched for axioms that may be considered as true for heuristic reasons and that would decide the continuum hypothesis. Many large cardinal axioms were studied, but the hypothesis always remained independent from them and it is now considered unlikely that CH can be resolved by a new large cardinal axiom. Other types of axioms were considered, but none of them has reached consensus on the continuum hypothesis yet. Recent work by Hamkins proposes a more flexible alternative: a set-theoretic multiverse allowing free passage between set-theoretic universes that satisfy the continuum hypothesis and other universes that do not.\n\nThis argument by Willard Quine and Hilary Putnam says (in Putnam's shorter words),\n\nHowever Putnam was not a Platonist.\n\nFew mathematicians are typically concerned on a daily, working basis over logicism, formalism or any other philosophical position. Instead, their primary concern is that the mathematical enterprise as a whole always remains productive. Typically, they see this as insured by remaining open-minded, practical and busy; as potentially threatened by becoming overly-ideological, fanatically reductionistic or lazy. \n\nSuch a view was has also been expressed by some well-known physicists.\n\nFor example, the Physics Nobel Prize laureate Richard Feynman said\n\nAnd Steven Weinberg:\n\nWeinberg believed that any undecidability in mathematics, such as the continuum hypothesis, could be potentially resolved despite the incompleteness theorem, by finding suitable further axioms to add to set theory.\n\nGödel's completeness theorem establishes an equivalence in first-order logic between the formal provability of a formula and its truth in all possible models. Precisely, for any consistent first-order theory it gives an \"explicit construction\" of a model described by the theory; this model will be countable if the language of the theory is countable. However this \"explicit construction\" is not algorithmic. It is based on an iterative process of completion of the theory, where each step of the iteration consists in adding a formula to the axioms if it keeps the theory consistent; but this consistency question is only semi-decidable (an algorithm is available to find any contradiction but if there is none this consistency fact can remain unprovable).\n\nThis can be seen as a giving a sort of justification to the Platonist view that the objects of our mathematical theories are real. More precisely, it shows that the mere assumption of the existence of the set of natural numbers as a totality (an actual infinity) suffices to imply the existence of a model (a world of objects) of any consistent theory. However several difficulties remain:\n\n\nAnother consequence of the completeness theorem is that it justifies the conception of infinitesimals as actual infinitely small nonzero quantities, based on the existence of non-standard models as equally legitimate to standard ones. This idea was formalized by Abraham Robinson into the theory of nonstandard analysis.\n\n\nStarting in 1935, the Bourbaki group of French mathematicians started publishing a series of books to formalize many areas of mathematics on the new foundation of set theory.\n\nThe intuitionistic school did not attract many adherents, and it was not until Bishop's work in 1967 that constructive mathematics was placed on a sounder footing.\n\nOne may consider that Hilbert's program has been partially completed, so that the crisis is essentially resolved, satisfying ourselves with lower requirements than Hilbert's original ambitions. His ambitions were expressed in a time when nothing was clear: it was not clear whether mathematics could have a rigorous foundation at all.\n\nThere are many possible variants of set theory, which differ in consistency strength, where stronger versions (postulating higher types of infinities) contain formal proofs of the consistency of weaker versions, but none contains a formal proof of its own consistency. Thus the only thing we don't have is a formal proof of consistency of whatever version of set theory we may prefer, such as ZF.\n\nIn practice, most mathematicians either do not work from axiomatic systems, or if they do, do not doubt the consistency of ZFC, generally their preferred axiomatic system. In most of mathematics as it is practiced, the incompleteness and paradoxes of the underlying formal theories never played a role anyway, and in those branches in which they do or whose formalization attempts would run the risk of forming inconsistent theories (such as logic and category theory), they may be treated carefully.\n\nThe development of category theory in the middle of the 20th century showed the usefulness of set theories guaranteeing the existence of larger classes than does ZFC, such as Von Neumann–Bernays–Gödel set theory or Tarski–Grothendieck set theory, albeit that in very many cases the use of large cardinal axioms or Grothendieck Universes is formally eliminable.\n\nOne goal of the Reverse Mathematics program is to identify whether there are areas of 'core mathematics' in which foundational issues may again provoke a crisis.\n\n\n\n"}
{"id": "27169542", "url": "https://en.wikipedia.org/wiki?curid=27169542", "title": "Ganita-yukti-bhasa", "text": "Ganita-yukti-bhasa\n\nGanita-yukti-bhasa (also written as Ganita Yuktibhasa) is either the title or a part of the title of three different books:\n\n\nThis edition of Yuktibhasa has been divided into two volumes, even though the original Malayalam text has no such division. Volume I deals with mathematics and Volume II treats astronomy. Each volume is divided into three parts: First part is an English translation of the relevant Malayalam part of Yuktibhasa, second part contains detailed explanatory notes on the translation, and in the third part the text in the Malayalam original is reproduced. The English translation is by K.V. Sarma and the explanatory notes are provided by Ramasubramanian, K., Srinivas, M.D. and Sriram, M.S..\n\nVolume I dealing with mathematics is divided into seven chapters. The topics discussed are the eight mathematical operations, a certain set of ten problems, arithmetic of fractions, rule of three, Kuttakara (linear indeterminate equations), infinite series and approximations for the ratio of the circumference and diameter of a circle and infinite series and approximations for sines.\n\nVolume II dealing with astronomy is divided into eight chapters. The topics covered are computation of mean and true longitudes of planets, Earth and celestial spheres, fifteen problems relating to ascension, declination, longitude, etc., determination of time, place, direction, etc., from gnomonic shadow, eclipses, Vyatipata (when the sun and moon have the same declination), visibility correction for planets and phases of the moon.\n\n"}
{"id": "34802284", "url": "https://en.wikipedia.org/wiki?curid=34802284", "title": "Gauss's Pythagorean right triangle proposal", "text": "Gauss's Pythagorean right triangle proposal\n\nGauss's Pythagorean right triangle proposal is an idea attributed to Carl Friedrich Gauss for a method to signal extraterrestrial beings by constructing an immense right triangle and three squares on the surface of the Earth. The shapes would be a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars.\n\nAlthough credited in numerous sources as originating with Gauss, with exact details of the proposal set out, the specificity of detail, and even whether Gauss made the proposal, have been called into question. Many of the earliest sources do not actually name Gauss as the originator, instead crediting a \"German Astronomer\" or using other nonspecific descriptors, and in some cases naming a different author entirely. The details of the proposal also change significantly upon different retellings. Nevertheless, Gauss's writings reveal a belief and interest in finding a method to contact extraterrestrial life, and that he did, at the least, propose using amplified light using a heliotrope, his own 1818 invention, to signal supposed inhabitants of the Moon.\n\nCarl Friedrich Gauss is credited with an 1820 proposal for a method to signal extraterrestrial beings in the form of drawing an immense right triangle and three squares on the surface of the Earth, intended as a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars. Details vary between sources, but typically the \"drawing\" was to be constructed on the Siberian tundra, and made up of vast strips of pine forest forming the right triangle's borders, with the interior of the drawing and exterior squares composed of fields of wheat. Gauss is said to have been convinced that Mars harbored intelligent life and that this geometric figure, invoking the Pythagorean theorem through the squares on the outside borders (sometimes called a \"windmill diagram\", as originated by Euclid), would demonstrate to such alien observers the reciprocal existence of intelligent life on Earth and its grounding in mathematics. Wheat was said to be chosen by Gauss for contrast with the pine tree borders \"because of its uniform color\".\n\nThe specificity of the proposal's details as it appears in most later sources—even its attribution to Gauss—is called into question in University of Notre Dame Professor Michael J. Crowe's 1986 book, \"The Extraterrestrial Life Debate, 1750–1900\", in which he surveys the origins of the Gauss proposal and observes that:The history of this proposal ... can be traced through two dozen or more pluralist writings reaching back to the first half of the nineteenth century. When this is done, however, it turns out that the story exists in almost as many forms as its retellings. Furthermore, these versions share one characteristic: Never is reference supplied to where in the writings of Gauss ... the [proposal] appear[s]!Some early sources explored by Crowe for the attribution and form of Gauss's proposal include Austrian astronomer, Joseph Johann Littrow's statement in \"Wunder des Himmels\" that \"one of our most distinguished geometers\" proposed that a geometric figure \"for example the well known so-called square of the hypotenuse, be laid out on a large scale, say on a particular broad plain of the earth\". and Patrick Scott's \"Love in the Moon\", in which a \"learned man\" is described as proposing a signal formed by a \"great plantation of tree\" in the form of \"47th Proposition of Euclid\" in \"the great African dessert \". In \"Chambers's Edinburgh Journal\" it was written that a Russian savant had proposed to \"communicate with the moon by cutting a large figure of the forty-seventh proposition of Euclid on the plains of Siberia, which, he said, any fool would understand\".\n\nIn the writings of astronomers Asaph Hall and of Norman Lockyer, each refer separately to a \"German Astronomer\" who proposed the method of contact be by \"fire signals\" from Siberia. Writing in 1902, Simon Newcomb placed the origin of a Siberian triangle \"several hundred miles in extent\" not with Gauss, but at the feet of German astronomer Franz Xaver von Zach. In lectures presented by François Arago at the Paris Observatory, he named Siberia as the location of an extraterrestrial signaling project advanced by an unnamed \"German geometer\", but that the signaling method was to be through the use of mirrors, rather than any large symbol drawn upon the Earth. Despite this version's departure from a geometric figure, the appearance of mirrors as a signaling device has a connection with Gauss's background. Gauss invented the Heliotrope in 1818, an instrument that uses a mirror to reflect sunlight in a manner allowing a square mirror to be seen away even in sunny weather.\n\nGauss wrote of the heliotrope's potential as a celestial signaling device in a March 25, 1822 letter to Heinrich Olbers, by which he reveals a belief and interest in finding a method to contact extraterrestrial life: \"With 100 separate mirrors, each of 16 square feet, used conjointly, one would be able to send good heliotrope-light to the moon ... This would be a discovery even greater than that of America, if we could get in touch with our neighbors on the moon.\" Finally, in the October 1826 issue of the \"Edinburgh New Philosophical Journal\" an unnamed author wrote that in a conversation with Franz von Gruithuisen, Gauss stated words to the effect that \"the plan of erecting a geometrical figure on the plains of Siberia corresponded with his opinion, because, according to his view a correspondence with the inhabitants of the moon could only be begun by means of such mathematical contemplations and ideas, which we and they have in common.\" Crowe concluded in sum that his review of earliest sources failed to confirm the detail of the proposal and Gauss as its author, but that his origination of the idea was not unlikely given the existing evidence.\n"}
{"id": "12751687", "url": "https://en.wikipedia.org/wiki?curid=12751687", "title": "God Created the Integers", "text": "God Created the Integers\n\nGod Created the Integers: The Mathematical Breakthroughs That Changed History is an anthology, edited by Stephen Hawking, of \"excerpts from thirty-one of the most important works in the history of mathematics.\" \n\nThe title of the book is a reference to a quotation attributed to mathematician Leopold Kronecker, who once wrote that \"God made the integers; all else is the work of man.\" \n\nThe works are grouped by author and ordered chronologically. Each section is prefaced by notes on the mathematician's life and work. The anthology includes works by the following mathematicians:\n\nSelections from the works of Euler, Bolyai, Lobachevsky and Galois, which are included in the second edition of the book (published in 2007), were not included in the first edition.\n"}
{"id": "1943892", "url": "https://en.wikipedia.org/wiki?curid=1943892", "title": "Greek letters used in mathematics, science, and engineering", "text": "Greek letters used in mathematics, science, and engineering\n\nGreek letters are used in mathematics, science, engineering, and other areas where mathematical notation is used as symbols for constants, special functions, and also conventionally for variables representing certain quantities. In these contexts, the capital letters and the small letters represent distinct and unrelated entities. Those Greek letters which have the same form as Latin letters are rarely used: capital A, B, E, Z, H, I, K, M, N, O, P, T, Y, X. Small ι, ο and υ are also rarely used, since they closely resemble the Latin letters i, o and u. Sometimes font variants of Greek letters are used as distinct symbols in mathematics, in particular for ε/ϵ and π/ϖ. The archaic letter digamma (Ϝ/ϝ/ϛ) is sometimes used.\n\nThe Bayer designation naming scheme for stars typically uses the first Greek letter, α, for the brightest star in each constellation, and runs through the alphabet before switching to Latin letters.\n\nIn mathematical finance, the Greeks are the variables denoted by Greek letters used to describe the risk of certain investments.\n\nThe Greek letter forms used in mathematics are often different from those used in Greek-language text: they are designed to be used in isolation, not connected to other letters, and some use variant forms which are not normally used in current Greek typography.\n\nThe OpenType font format has the feature tag 'mgrk' \"Mathematical Greek\" to identify a glyph as representing a Greek letter to be used in mathematical (as opposed to Greek language) contexts.\n\nThe table below shows a comparison of Greek letters rendered in TeX and HTML.\nThe font used in the TeX rendering is an italic style. This is in line with the convention that variables should be italicized. As Greek letters are more often than not used as variables in mathematical formulas, a Greek letter appearing similar to the TeX rendering is more likely to be encountered in works involving mathematics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnote: a symbol for the empty set, formula_23, resembles Φ but is not Φ\n\n\n\n\n\n"}
{"id": "8816788", "url": "https://en.wikipedia.org/wiki?curid=8816788", "title": "History of Grandi's series", "text": "History of Grandi's series\n\nGuido Grandi (1671–1742) reportedly provided a simplistic account of the series in 1703. He noticed that inserting parentheses into produced varying results: either\nor\n\nGrandi's explanation of this phenomenon became well known for its religious overtones:\n\nIn fact, the series was not an idle subject for Grandi, and he didn't think it summed to either 0 or 1. Rather, like many mathematicians to follow, he thought the true value of the series was ⁄ for a variety of reasons.\nGrandi's mathematical treatment of occurs in his 1703 book \"Quadratura circula et hyperbolae per infinitas hyperbolas geometrice exhibita\". Broadly interpreting Grandi's work, he derived through geometric reasoning connected with his investigation of the witch of Agnesi. Eighteenth-century mathematicians immediately translated and summarized his argument in analytical terms: for a generating circle with diameter \"a\", the equation of the witch \"y\" = \"a\"/(\"a\" + \"x\") has the series expansion\n\nGrandi offered a new explanation that in 1710, both in the second edition of the \"Quadratura circula\" and in a new work, \"De Infinitis infinitorum, et infinite parvorum ordinibus disquisitio geometrica\". Two brothers inherit a priceless gem from their father, whose will forbids them to sell it, so they agree that it will reside in each other's museums on alternating years. If this agreement lasts for all eternity between the brother's descendants, then the two families will each have half possession of the gem, even though it changes hands infinitely often. This argument was later criticized by Leibniz.\n\nThe parable of the gem is the first of two additions to the discussion of the corollary that Grandi added to the second edition. The second repeats the link between the series and the creation of the universe by God:\nAfter Grandi published the second edition of the \"Quadratura\", his fellow countryman Alessandro Marchetti became one of his first critics. One historian charges that Marchetti was motivated more by jealousy than any other reason. Marchetti found the claim that an infinite number of zeros could add up to a finite quantity absurd, and he inferred from Grandi's treatment the danger posed by theological reasoning. The two mathematicians began attacking each other in a series of open letters; their debate was ended only by Marchetti's death in 1714.\n\nWith the help and encouragement of Antonio Magliabechi, Grandi sent a copy of the 1703 \"Quadratura\" to Leibniz, along with a letter expressing compliments and admiration for the master's work. Leibniz received and read this first edition in 1705, and he called it an unoriginal and less-advanced \"attempt\" at his calculus. Grandi's treatment of 1 − 1 + 1 − 1 + · · · would not catch Leibniz's attention until 1711, near the end of his life, when Christian Wolff sent him a letter on Marchetti's behalf describing the problem and asking for Leibniz's opinion.\n\nAs early as 1674, in a minor, lesser-known writing \"De Triangulo Harmonico\" on the harmonic triangle, Leibniz mentioned very briefly in an example:\n\nPresumably he arrived at this series by repeated substitution:\n\nThe series also appears indirectly in a discussion with Tschirnhaus in 1676.\n\nLeibniz had already considered the divergent alternating series as early as 1673. In that case he argued that by subtracting either on the left or on the right, one could produce either positive or negative infinity, and therefore both answers are wrong and the whole should be finite. Two years after that, Leibniz formulated the first convergence test in the history of mathematics, the alternating series test, in which he implicitly applied the modern definition of convergence.\n\nIn the 1710s, Leibniz described Grandi's series in his correspondence with several other mathematicians. The letter with the most lasting impact was his first reply to Wolff, which he published in the \"Acta Eruditorum\". In this letter, Leibniz attacked the problem from several angles.\n\nIn general, Leibniz believed that the algorithms of calculus were a form of \"blind reasoning\" that ultimately had to be founded upon geometrical interpretations. Therefore, he agreed with Grandi that claiming that the relation was well-founded because there existed a geometric demonstration.\n\nOn the other hand, Leibniz sharply criticized Grandi's example of the shared gem, claiming that the series has no relation to the story. He pointed out that for any finite, even number of years, the brothers have equal possession, yet the sum of the corresponding terms of the series is zero.\n\nLeibniz thought that the argument from was valid; he took it as an example of his law of continuity. Since the relation holds for all \"x\" less than 1, it should hold for \"x\" equal to 1 as well. Still, Leibniz thought that one should be able to find the sum of the series directly, without needing to refer back to the expression from which it came. This approach may seem obvious by modern standards, but it is a significant step from the point of view of the history of summing divergent series. In the 18th century, the study of series was dominated by power series, and summing a numerical series by expressing it as \"f\"(1) of some function's power series was thought to be the most natural strategy.\n\nLeibniz begins by observing that taking an even number of terms from the series, the last term is −1 and the sum is 0:\nTaking an odd number of terms, the last term is +1 and the sum is 1:\n\nNow, the infinite series 1 − 1 + 1 − 1 + · · · has neither an even nor an odd number of terms, so it produces neither 0 nor 1; by taking the series out to infinity, it becomes something between those two options. There is no more reason why the series should take one value than the other, so the theory of \"probability\" and the \"law of justice\" dictate that one should take the arithmetic mean of 0 and 1, which is \n\nEli Maor says of this solution, \"Such a brazen, careless reasoning indeed seems incredible to us today…\" Kline portrays Leibniz as more self-conscious: \"Leibniz conceded that his argument was more metaphysical than mathematical, but said that there is more metaphysical truth in mathematics than is generally recognized.\"\n\nCharles Moore muses that Leibniz would hardly have had such confidence in his metaphysical strategy if it did not give the same result (namely ⁄) as other approaches. Mathematically, this was no accident: Leibniz's treatment would be partially justified when the compatibility of averaging techniques and power series was finally proven in 1880.\n\nWhen he had first raised the question of Grandi's series to Leibniz, Wolff was inclined toward skepticism along with Marchetti. Upon reading Leibniz's reply in mid-1712, Wolff was so pleased with the solution that he sought to extend the arithmetic mean method to more divergent series such as . Leibniz's intuition prevented him from straining his solution this far, and he wrote back that Wolff's idea was interesting but invalid for several reasons. For one, the terms of a summable series should decrease to zero; even could be expressed as a limit of such series.\n\nLeibniz described Grandi's series along with the general problem of convergence and divergence in letters to Nicolaus I Bernoulli in 1712 and early 1713. J. Dutka suggests that this correspondence, along with Nicolaus I Bernoulli's interest in probability, motivated him to formulate the St. Petersburg paradox, another situation involving a divergent series, in September 1713.\n\nAccording to Pierre-Simon Laplace in his \"Essai Philosophique sur les Probabilités\", Grandi's series was connected with Leibniz seeing \"an image of the Creation in his binary arithmetic\", and thus Leibniz wrote a letter to Jesuit missionary Claudio Filippo Grimaldi, court mathematician in China, in the hope that Claudio Filippo Grimaldi's interest in science and the mathematical \"emblem of creation\" might combine to convert the nation to Christianity. Laplace remarks, \"I record this anecdote only to show how far the prejudices of infancy may mislead the greatest men.\"\n\nJacob Bernoulli (1654–1705) dealt with a similar series in 1696 in the third part of his \"Positiones arithmeticae de seriebus infinitis\". Applying Nicholas Mercator's method for polynomial long division to the ratio , he noticed that one always had a remainder. If then this remainder decreases and \"finally is less than any given quantity\", and one has\nIf \"m\" = \"n\", then this equation becomes\nBernoulli called this equation a \"not inelegant paradox\".\n\nPierre Varignon (1654–1722) treated Grandi's series in his report \"Précautions à prendre dans l'usage des Suites ou Series infinies résultantes…\". The first of his purposes for this paper was to point out the divergence of Grandi's series and expand on Jacob Bernoulli's 1696 treatment.\n\nThe final version of Varignon's paper is dated February 16, 1715, and it appeared in a volume of the \"Mémories\" of the French Academy of Sciences that was itself not published until 1718. For such a relatively late treatment of Grandi's series, it is surprising that Varignon's report does not even mention Leibniz's earlier work. But most of the \"Précautions\" was written in October 1712, while Varignon was away from Paris. The Abbé Poignard's 1704 book on magic squares, \"Traité des Quarrés sublimes\", had become a popular subject around the Academy, and the second revised and expanded edition weighed in at 336 pages. To make the time to read the \"Traité\", Varignon had to escape to the countryside for nearly two months, where he wrote on the topic of Grandi's series in relative isolation. Upon returning to Paris and checking in at the Academy, Varignon soon discovered that the great Leibniz had ruled in favor of Grandi. Having been separated from his sources, Varignon still had to revise his paper by looking up and including the citation to Jacob Bernoulli. Rather than also take Leibniz's work into account, Varignon explains in a postscript to his report that the citation was the only revision he had made in Paris, and that \"if\" other research on the topic arose, his thoughts on it would have to wait for a future report.\n\nIn the 1751 \"Encyclopédie\", Jean le Rond d'Alembert echoes the view that Grandi's reasoning based on division had been refuted by Varignon in 1715. (Actually, d'Alembert attributes the problem to \"Guido Ubaldus\", an error that is still occasionally propagated today.)\n\nIn a 1715 letter to Jacopo Riccati, Leibniz mentioned the question of Grandi's series and advertised his own solution in the \"Acta Eruditorum\". Later, Riccati would criticize Grandi's argument in his 1754 \"Saggio intorno al sistema dell'universo\", saying that it causes contradictions. He argues that one could just as well write but that this series has \"the same quantity of zeroes\" as Grandi's series. These zeroes lack any evanescent character of \"n\", as Riccati points out that the equality is guaranteed by He concludes that the fundamental mistake is in using a divergent series to begin with:\nAnother 1754 publication also criticized Grandi's series on the basis of its collapse to 0. Louis Antoine de Bougainville briefly treats the series in his acclaimed 1754 textbook \"Traité du calcul intégral\". He explains that a series is \"true\" if its sum is equal to the expression from which is expanded; otherwise it is \"false\". Thus Grandi's series is false because and yet .\n\nLeonhard Euler treats along with other divergent series in his \"De seriebus divergentibus\", a 1746 paper that was read to the Academy in 1754 and published in 1760. He identifies the series as being first considered by Leibniz, and he reviews Leibniz's 1713 argument based on the series , calling it \"fairly sound reasoning\", and he also mentions the even/odd median argument. Euler writes that the usual objection to the use of is that it does not equal unless \"a\" is less than 1; otherwise all one can say is that\nwhere the last remainder term does not vanish and cannot be disregarded as \"n\" is taken to infinity. Still writing in the third person, Euler mentions a possible rebuttal to the objection: essentially, since an infinite series has no last term, there is no place for the remainder and it should be neglected. After reviewing more badly divergent series like , where he judges his opponents to have firmer support, Euler seeks to define away the issue:\n\nEuler also used finite differences to attack . In modern terminology, he took the Euler transform of the sequence and found that it equalled ⁄. As late as 1864, De Morgan claims that \"this transformation has always appeared one of the strongest presumptions in favour of being ⁄.\"\n\nDespite the confident tone of his papers, Euler expressed doubt over divergent series in his correspondence with Nicolaus I Bernoulli. Euler claimed that his attempted definition had never failed him, but Bernoulli pointed out a clear weakness: it does not specify how one should determine \"the\" finite expression that generates a given infinite series. Not only is this a practical difficulty, it would be theoretically fatal if a series were generated by expanding two expressions with different values. Euler's treatment of rests upon his firm belief that ⁄ is the only possible value of the series; what if there were another?\n\nIn a 1745 letter to Christian Goldbach, Euler claimed that he was not aware of any such counterexample, and in any case Bernoulli had not provided one. Several decades later, when Jean-Charles Callet finally asserted a counterexample, it was aimed at . The background of the new idea begins with Daniel Bernoulli in 1771.\n\nDaniel Bernoulli, who accepted the probabilistic argument that , noticed that by inserting 0s into the series in the right places, it could achieve any value between 0 and 1. In particular, the argument suggested that\nIn a memorandum sent to Joseph Louis Lagrange toward the end of the century, Callet pointed out that could also be obtained from the series\nsubstituting \"x\" = 1 now suggests a value of ⁄, not ⁄.\nLagrange approved Callet's submission for publication in the \"Mémoires\" of the French Academy of Sciences, but it was never directly published. Instead, Lagrange (along with Charles Bossut) summarized Callet's work and responded to it in the \"Mémoires\" of 1799. He defended Euler by suggesting that Callet's series actually should be written with the 0 terms left in:\nwhich reduces to\ninstead.\nThe 19th century is remembered as the approximate period of Cauchy's and Abel's largely successful ban on the use of divergent series, but Grandi's series continued to make occasional appearances. Some mathematicians did not follow Abel's lead, mostly outside France, and British mathematicians especially took \"a long time\" to understand the analysis coming from the continent.\n\nIn 1803, Robert Woodhouse proposed that summed to something called\nwhich could be distinguished from ⁄. Ivor Grattan-Guinness remarks on this proposal, \"… R. Woodhouse … wrote with admirable honesty on the problems which he failed to understand. … Of course, there is no harm in defining new symbols such as ⁄; but the idea is 'formalist' in the unflattering sense, and it does not bear on the problem of the convergence of series.\"\n\nIn 1830, a mathematician identified only as \"M. R. S.\" wrote in the \"Annales de Gergonne\" on a technique to numerically find fixed points of functions of one variable. If one can transform a problem into the form of an equation \"x = A + f(x)\", where \"A\" can be chosen at will, then \nshould be a solution, and truncating this infinite expression results in a sequence of approximations. Conversely, given the series , the author recovers the equation\nto which the solution is (⁄)\"a\".\n\nM. R. S. notes that the approximations in this case are \"a\", 0, \"a\", 0, …, but there is no need for Leibniz's \"subtle reasoning\". Moreover, the argument for averaging the approximations is problematic in a wider context. For equations not of the form \"x = A + f(x)\", M. R. S.'s solutions are continued fractions, continued radicals, and other infinite expressions. In particular, the expression should be a solution of the equation . Here, M. R. S. writes that based on Leibniz's reasoning, one is tempted to conclude that \"x\" is the average of the truncations \"a\", 1, \"a\", 1, …. This average is , but the solution to the equation is the square root of \"a\".\n\nBernard Bolzano criticized M. R. S.' algebraic solution of the series. In reference to the step\nBolzano charged,\n\nThis comment exemplifies Bolzano's intuitively appealing but deeply problematic views on infinity. In his defense, Cantor himself pointed out that Bolzano worked in a time when the concept of the cardinality of a set was absent.\n\nAs late as 1844, Augustus De Morgan commented that if a single instance where did not equal ⁄ could be given, he would be willing to reject the entire theory of trigonometric series.\n\nThe same volume contains papers by Samuel Earnshaw and J R Young dealing in part with . G. H. Hardy dismisses both of these as \"little more than nonsense\", in contrast to De Morgan's \"remarkable mixture of acuteness and confusion\"; in any case, Earnshaw got De Morgan's attention with the following remarks:\n\nDe Morgan fired back in 1864 in the same journal:\nThe last scholarly article to be motivated by 1 − 1 + 1 − 1 + · · · might be identified as the first article in the modern history of divergent series. Georg Frobenius published an article titled \"Ueber die Leibnitzsche Reihe\" (\"On Leibniz's series\") in 1880. He had found Leibniz's old letter to Wolff, citing it along with an 1836 article by Joseph Ludwig Raabe, who in turn drew on ideas by Leibniz and Daniel Bernoulli.\n\nFrobenius' short paper, barely two pages, begins by quoting from Leibniz's treatment of 1 − 1 + 1 − 1 + · · ·. He infers that Leibniz was actually stating a generalization of Abel's Theorem. The result, now known as Frobenius' theorem, has a simple statement in modern terms: any series that is Cesàro summable is also Abel summable to the same sum. Historian Giovanni Ferraro emphasizes that Frobenius did not actually state the theorem in such terms, and Leibniz did not state it at all. Leibniz was defending the association of the divergent series with the value ⁄, while Frobenius' theorem is stated in terms of convergent sequences and the epsilon-delta formulation of the limit of a function.\n\nFrobenius' theorem was soon followed with further generalizations by Otto Hölder and Thomas Joannes Stieltjes in 1882. Again, to a modern reader their work strongly suggests new definitions of the sum of a divergent series, but those authors did not yet make that step. Ernesto Cesàro proposed a systematic definition for the first time in 1890. Since then, mathematicians have explored many different summability methods for divergent series. Most of these, especially the simpler ones with historical parallels, sum Grandi's series to ⁄. Others, motivated by Daniel Bernoulli's work, sum the series to another value, and a few do not sum it at all.\n\nThe full texts of many of the following references are publicly available on the Internet from Google Books; the Euler archive at Dartmouth College; DigiZeitschriften, a service of Deutsche Forschungsgemeinschaft; or Gallica, a service of the Bibliothèque nationale de France.\n\n\n"}
{"id": "6134187", "url": "https://en.wikipedia.org/wiki?curid=6134187", "title": "History of mathematical notation", "text": "History of mathematical notation\n\nThe history of mathematical notation includes the commencement, progress, and cultural diffusion of mathematical symbols and the conflict of the methods of notation confronted in a notation's move to popularity or inconspicuousness. Mathematical notation comprises the symbols used to write mathematical equations and formulas. Notation generally implies a set of well-defined representations of quantities and symbols operators. The history includes Hindu–Arabic numerals, letters from the Roman, Greek, Hebrew, and German alphabets, and a host of symbols invented by mathematicians over the past several centuries.\n\nThe development of mathematical notation can be divided in stages. The \"\"rhetorical\" stage is where calculations are performed by words and no symbols are used. The \"syncopated\"\" stage is where frequently used operations and quantities are represented by symbolic syntactical abbreviations. From ancient times through the post-classical age, bursts of mathematical creativity were often followed by centuries of stagnation. As the early modern age opened and the worldwide spread of knowledge began, written examples of mathematical developments came to light. The \"symbolic\" stage is where comprehensive systems of notation supersede rhetoric. Beginning in Italy in the 16th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This symbolic system was in use by medieval Indian mathematicians and in Europe since the middle of the 17th century, and has continued to develop in the contemporary era.\n\nThe area of study known as the history of mathematics is primarily an investigation into the origin of discoveries in mathematics and, the focus here, the investigation into the mathematical methods and notation of the past.\n\nAlthough the history commences with that of the Ionian schools, there is no doubt that those Ancient Greeks who paid attention to it were largely indebted to the previous investigations of the Ancient Egyptians and Ancient Phoenicians. Numerical notation's distinctive feature, i.e. symbols having local as well as intrinsic values (arithmetic), implies a state of civilization at the period of its invention. Our knowledge of the mathematical attainments of these early peoples, to which this section is devoted, is imperfect and the following brief notes be regarded as a summary of the conclusions which seem most probable, and the history of mathematics begins with the symbolic sections.\n\nMany areas of mathematics began with the study of real world problems, before the underlying rules and concepts were identified and defined as abstract structures. For example, geometry has its origins in the calculation of distances and areas in the real world; algebra started with methods of solving problems in arithmetic.\n\nThere can be no doubt that most early peoples which have left records knew something of numeration and mechanics, and that a few were also acquainted with the elements of land-surveying. In particular, the Egyptians paid attention to geometry and numbers, and the Phoenicians to practical arithmetic, book-keeping, navigation, and land-surveying. The results attained by these people seem to have been accessible, under certain conditions, to travelers. It is probable that the knowledge of the Egyptians and Phoenicians was largely the result of observation and measurement, and represented the accumulated experience of many ages.\n\nWritten mathematics began with numbers expressed as tally marks, with each tally representing a single unit. The numerical symbols consisted probably of strokes or notches cut in wood or stone, and intelligible alike to all nations. For example, one notch in a bone represented one animal, or person, or anything else. The peoples with whom the Greeks of Asia Minor (amongst whom notation in western history begins) were likely to have come into frequent contact were those inhabiting the eastern littoral of the Mediterranean: and Greek tradition uniformly assigned the special development of geometry to the Egyptians, and that of the science of numbers either to the Egyptians or to the Phoenicians.\n\nThe Ancient Egyptians had a symbolic notation which was the numeration by Hieroglyphics. The Egyptian mathematics had a symbol for one, ten, one-hundred, one-thousand, ten-thousand, one-hundred-thousand, and one-million. Smaller digits were placed on the left of the number, as they are in Hindu–Arabic numerals. Later, the Egyptians used hieratic instead of hieroglyphic script to show numbers. Hieratic was more like cursive and replaced several groups of symbols with individual ones. For example, the four vertical lines used to represent four were replaced by a single horizontal line. This is found in the Rhind Mathematical Papyrus (c. 2000–1800 BC) and the Moscow Mathematical Papyrus (c. 1890 BC). The system the Egyptians used was discovered and modified by many other civilizations in the Mediterranean. The Egyptians also had symbols for basic operations: legs going forward represented addition, and legs walking backward to represent subtraction.\n\nThe Mesopotamians had symbols for each power of ten. Later, they wrote their numbers in almost exactly the same way done in modern times. Instead of having symbols for each power of ten, they would just put the coefficient of that number. Each digit was at separated by only a space, but by the time of Alexander the Great, they had created a symbol that represented zero and was a placeholder. The Mesopotamians also used a sexagesimal system, that is base sixty. It is this system that is used in modern times when measuring time and angles. Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework. The earliest evidence of written mathematics dates back to the ancient Sumerians and the system of metrology from 3000 BC. From around 2500 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.\n\nThe majority of Mesopotamian clay tablets date from 1800 to 1600 BC, and cover topics which include fractions, algebra, quadratic and cubic equations, and the calculation of regular reciprocal pairs. The tablets also include multiplication tables and methods for solving linear and quadratic equations. The Babylonian tablet YBC 7289 gives an approximation of accurate to five decimal places. Babylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 x 6) degrees in a circle, as well as the use of minutes and seconds of arc to denote fractions of a degree. Babylonian advances in mathematics were facilitated by the fact that 60 has many divisors: the reciprocal of any integer which is a multiple of divisors of 60 has a finite expansion in base 60. (In decimal arithmetic, only reciprocals of multiples of 2 and 5 have finite decimal expansions.) Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values, much as in the decimal system. They lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context.\n\nThe history of mathematics cannot with certainty be traced back to any school or period before that of the Ionian Greeks, but the subsequent history may be divided into periods, the distinctions between which are tolerably well marked. Greek mathematics, which originated with the study of geometry, tended from its commencement to be deductive and scientific. Since the fourth century AD, Pythagoras has commonly been given credit for discovering the Pythagorean theorem, a theorem in geometry that states that in a right-angled triangle the area of the square on the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares of the other two sides. The ancient mathematical texts are available with the prior mentioned Ancient Egyptians notation and with Plimpton 322 (Babylonian mathematics c. 1900 BC). The study of mathematics as a subject in its own right begins in the 6th century BC with the Pythagoreans, who coined the term \"mathematics\" from the ancient Greek \"μάθημα\" (\"mathema\"), meaning \"subject of instruction\".\n\nPlato's influence has been especially strong in mathematics and the sciences. He helped to distinguish between pure and applied mathematics by widening the gap between \"arithmetic\", now called number theory and \"logistic\", now called arithmetic. Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Aristotle is credited with what later would be called the law of excluded middle.\n\n\"Abstract Mathematics\" is what treats of magnitude or quantity, absolutely and generally conferred, without regard to any species of particular magnitude, such as Arithmetic and Geometry, In this sense, abstract mathematics is opposed to mixed mathematics; wherein simple and abstract properties, and the relations of quantities primitively considered in mathematics, are applied to sensible objects, and by that means become intermixed with physical considerations; Such are Hydrostatics, Optics, Navigation, &c.\n\nArchimedes is generally considered to be the greatest mathematician of antiquity and one of the greatest of all time. He used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of pi. He also defined the spiral bearing his name, formulae for the volumes of surfaces of revolution and an ingenious system for expressing very large numbers.\nIn the historical development of geometry, the steps in the abstraction of geometry were made by the ancient Greeks. Euclid's Elements being the earliest extant documentation of the axioms of plane geometry— though Proclus tells of an earlier axiomatisation by Hippocrates of Chios. Euclid's \"Elements\" (c. 300 BC) is one of the oldest extant Greek mathematical treatises and consisted of 13 books written in Alexandria; collecting theorems proven by other mathematicians, supplemented by some original work. The document is a successful collection of definitions, postulates (axioms), propositions (theorems and constructions), and mathematical proofs of the propositions. Euclid's first theorem is a lemma that possesses properties of prime numbers. The influential thirteen books cover Euclidean geometry, geometric algebra, and the ancient Greek version of algebraic systems and elementary number theory. It was ubiquitous in the Quadrivium and is instrumental in the development of logic, mathematics, and science.\n\nDiophantus of Alexandria was author of a series of books called \"Arithmetica\", many of which are now lost. These texts deal with solving algebraic equations. Boethius provided a place for mathematics in the curriculum in the 6th century when he coined the term \"quadrivium\" to describe the study of arithmetic, geometry, astronomy, and music. He wrote \"De institutione arithmetica\", a free translation from the Greek of Nicomachus's \"Introduction to Arithmetic\"; \"De institutione musica\", also derived from Greek sources; and a series of excerpts from Euclid's \"Elements\". His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.\n\nThe Greeks employed Attic numeration, which was based on the system of the Egyptians and was later adapted and used by the Romans. Greek numerals one through four were vertical lines, as in the hieroglyphics. The symbol for five was the Greek letter Π (pi), which is the letter of the Greek word for five, \"pente\". Numbers six through nine were \"pente\" with vertical lines next to it. Ten was represented by the letter (Δ) of the word for ten, \"deka\", one hundred by the letter from the word for hundred, etc.\n\nThe Ionian numeration used their entire alphabet including three archaic letters. The numeral notation of the Greeks, though far less convenient than that now in use, was formed on a perfectly regular and scientific plan, and could be used with tolerable effect as an instrument of calculation, to which purpose the Roman system was totally inapplicable. The Greeks divided the twenty-four letters of their alphabet into three classes, and, by adding another symbol to each class, they had characters to represent the units, tens, and hundreds. (Jean Baptiste Joseph Delambre's Astronomie Ancienne, t. ii.)\nThis system appeared in the third century BC, before the letters digamma (Ϝ), koppa (Ϟ), and sampi (Ϡ) became obsolete. When lowercase letters became differentiated from upper case letters, the lower case letters were used as the symbols for notation. Multiples of one thousand were written as the nine numbers with a stroke in front of them: thus one thousand was \",α\", two-thousand was \",β\", etc. M (for μὐριοι, as in \"myriad\") was used to multiply numbers by ten thousand. For example, the number 88,888,888 would be written as M,ηωπη*ηωπη\n\nGreek mathematical reasoning was almost entirely geometric (albeit often used to reason about non-geometric subjects such as number theory), and hence the Greeks had no interest in algebraic symbols. The great exception was Diophantus of Alexandria, the great algebraist. His \"Arithmetica\" was one of the texts to use symbols in equations. It was not completely symbolic, but was much more so than previous books. An unknown number was called s. The square of s was formula_1; the cube was formula_2; the fourth power was formula_3; and the fifth power was formula_4.\n\nThe Chinese used numerals that look much like the tally system. Numbers one through four were horizontal lines. Five was an X between two horizontal lines; it looked almost exactly the same as the Roman numeral for ten. Nowadays, the huāmǎ system is only used for displaying prices in Chinese markets or on traditional handwritten invoices.\n\nIn the history of the Chinese, there were those who were familiar with the sciences of arithmetic, geometry, mechanics, optics, navigation, and astronomy. Mathematics in China emerged independently by the 11th century BC. It is almost certain that the Chinese were acquainted with several geometrical or rather architectural implements; with mechanical machines; that they knew of the characteristic property of the magnetic needle; and were aware that astronomical events occurred in cycles. Chinese of that time had made attempts to classify or extend the rules of arithmetic or geometry which they knew, and to explain the causes of the phenomena with which they were acquainted beforehand. The Chinese independently developed very large and negative numbers, decimals, a place value decimal system, a binary system, algebra, geometry, and trigonometry.\nChinese mathematics made early contributions, including a place value system. The geometrical theorem known to the ancient Chinese were acquainted was applicable in certain cases (namely the ratio of sides). It is that geometrical theorems which can be demonstrated in the quasi-experimental way of superposition were also known to them. In arithmetic their knowledge seems to have been confined to the art of calculation by means of the swan-pan, and the power of expressing the results in writing. Our knowledge of the early attainments of the Chinese, slight though it is, is more complete than in the case of most of their contemporaries. It is thus instructive, and serves to illustrate the fact, that it can be known a nation may possess considerable skill in the applied arts with but our knowledge of the later mathematics on which those arts are founded can be scarce. Knowledge of Chinese mathematics before 254 BC is somewhat fragmentary, and even after this date the manuscript traditions are obscure. Dates centuries before the classical period are generally considered conjectural by Chinese scholars unless accompanied by verified archaeological evidence.\n\nAs in other early societies the focus was on astronomy in order to perfect the agricultural calendar, and other practical tasks, and not on establishing formal systems.The Chinese Board of Mathematics duties were confined to the annual preparation of an almanac, the dates and predictions in which it regulated. Ancient Chinese mathematicians did not develop an axiomatic approach, but made advances in algorithm development and algebra. The achievement of Chinese algebra reached its zenith in the 13th century, when Zhu Shijie invented method of four unknowns.\n\nAs a result of obvious linguistic and geographic barriers, as well as content, Chinese mathematics and that of the mathematics of the ancient Mediterranean world are presumed to have developed more or less independently up to the time when \"The Nine Chapters on the Mathematical Art\" reached its final form, while the \"Writings on Reckoning\" and \"Huainanzi\" are roughly contemporary with classical Greek mathematics. Some exchange of ideas across Asia through known cultural exchanges from at least Roman times is likely. Frequently, elements of the mathematics of early societies correspond to rudimentary results found later in branches of modern mathematics such as geometry or number theory. The Pythagorean theorem for example, has been attested to the time of the Duke of Zhou. Knowledge of Pascal's triangle has also been shown to have existed in China centuries before Pascal, such as by Shen Kuo.\nThe state of trigonometry in China slowly began to change and advance during the Song Dynasty (960–1279), where Chinese mathematicians began to express greater emphasis for the need of spherical trigonometry in calendarical science and astronomical calculations. The polymath Chinese scientist, mathematician and official Shen Kuo (1031–1095) used trigonometric functions to solve mathematical problems of chords and arcs. Sal Restivo writes that Shen's work in the lengths of arcs of circles provided the basis for spherical trigonometry developed in the 13th century by the mathematician and astronomer Guo Shoujing (1231–1316). As the historians L. Gauchet and Joseph Needham state, Guo Shoujing used spherical trigonometry in his calculations to improve the calendar system and Chinese astronomy. The mathematical science of the Chinese would incorporate the work and teaching of Arab missionaries with knowledge of spherical trigonometry who had come to China in the course of the thirteenth century.\n\nAlthough the origin of our present system of numerical notation is ancient, there is no doubt that it was in use among the Hindus over two thousand years ago. The algebraic notation of the Indian mathematician, Brahmagupta, was syncopated. Addition was indicated by placing the numbers side by side, subtraction by placing a dot over the subtrahend (the number to be subtracted), and division by placing the divisor below the dividend, similar to our notation but without the bar. Multiplication, evolution, and unknown quantities were represented by abbreviations of appropriate terms. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, likely evolved over the course of the first millennium AD in India and was transmitted to the west via Islamic mathematics.\n\nDespite their name, Arabic numerals actually started in India. The reason for this misnomer is Europeans saw the numerals used in an Arabic book, \"Concerning the Hindu Art of Reckoning\", by Mohommed ibn-Musa al-Khwarizmi. Al-Khwārizmī wrote several important books on the Hindu–Arabic numerals and on methods for solving equations. His book \"On the Calculation with Hindu Numerals\", written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. Al-Khwarizmi did not claim the numerals as Arabic, but over several Latin translations, the fact that the numerals were Indian in origin was lost. The word \"algorithm\" is derived from the Latinization of Al-Khwārizmī's name, Algoritmi, and the word \"algebra\" from the title of one of his works, \"Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala\" (\"The Compendious Book on Calculation by Completion and Balancing\").\n\nIslamic mathematics developed and expanded the mathematics known to Central Asian civilizations. Al-Khwārizmī gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and Al-Khwārizmī was to teach algebra in an elementary form and for its own sake. Al-Khwārizmī also discussed the fundamental method of \"reduction\" and \"balancing\", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as \"al-jabr\". His algebra was also no longer concerned \"with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study.\" Al-Khwārizmī also studied an equation for its own sake and \"in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems.\"\n\nAl-Karaji, in his treatise \"al-Fakhri\", extends the methodology to incorporate integer powers and integer roots of unknown quantities. The historian of mathematics, F. Woepcke, praised Al-Karaji for being \"the first who introduced the theory of algebraic calculus.\" Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham would develop analytic geometry. Al-Haytham derived the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. Al-Haytham performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. In the late 11th century, Omar Khayyam would develop algebraic geometry, wrote \"Discussions of the Difficulties in Euclid\", and wrote on the general geometric solution to cubic equations. Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals.\n\nMany Greek and Arabic texts on mathematics were then translated into Latin, which led to further development of mathematics in medieval Europe. In the 12th century, scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's and the complete text of Euclid's \"Elements\". One of the European books that advocated using the numerals was \"Liber Abaci\", by Leonardo of Pisa, better known as Fibonacci. \"Liber Abaci\" is better known for the mathematical problem Fibonacci wrote in it about a population of rabbits. The growth of the population ended up being a Fibonacci sequence, where a term is the sum of the two preceding terms.\n\nAbū al-Hasan ibn Alī al-Qalasādī (1412–1482) was the last major medieval Arab algebraist, who improved on the algebraic notation earlier used by Ibn al-Yāsamīn in the 12th century and, in the Maghreb, by Ibn al-Banna in the 13th century. In contrast to the syncopated notations of their predecessors, Diophantus and Brahmagupta, which lacked symbols for mathematical operations, al-Qalasadi's algebraic notation was the first to have symbols for these functions and was thus \"the first steps toward the introduction of algebraic symbolism.\" He represented mathematical symbols using characters from the Arabic alphabet.\n\n\nThe 14th century saw the development of new mathematical concepts to investigate a wide range of problems. The two widely used arithmetic symbols are addition and subtraction, + and −. The plus sign was used by 1360 by Nicole Oresme in his work \"Algorismus proportionum\". It is thought an abbreviation for \"et\", meaning \"and\" in Latin, in much the same way the ampersand sign also began as \"et\". Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of the distance covered by a body undergoing uniformly accelerated motion, asserting that the area under the line depicting the constant acceleration and represented the total distance traveled. The minus sign was used in 1489 by Johannes Widmann in \"Mercantile Arithmetic\" or \"Behende und hüpsche Rechenung auff allen Kauffmanschafft,\". Widmann used the minus symbol with the plus symbol, to indicate deficit and surplus, respectively. In \"Summa de arithmetica, geometria, proportioni e proportionalità\", Luca Pacioli used symbols for plus and minus symbols and contained algebra.\n\nIn the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating \"n\"th roots. In 1533, Regiomontanus's table of sines and cosines were published. Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book \"Ars Magna\", together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. The radical symbol for square root was introduced by Christoph Rudolff. Michael Stifel's important work \"Arithmetica integra\" contained important innovations in mathematical notation. In 1556, Niccolò Tartaglia used parentheses for precedence grouping. In 1557 Robert Recorde published The Whetstone of Witte which used the equal sign (=) as well as plus and minus signs for the English reader. In 1564, Gerolamo Cardano analyzed games of chance beginning the early stages of probability theory. In 1572 Rafael Bombelli published his \"L'Algebra\" in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations. Simon Stevin's book \"De Thiende\" ('the art of tenths'), published in Dutch in 1585, contained a systematic treatment of decimal notation, which influenced all later work on the real number system. The New algebra (1591) of François Viète introduced the modern notational manipulation of algebraic expressions. For navigation and accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus coin the word \"trigonometry\", publishing his \"Trigonometria\" in 1595.\n\nJohn Napier is best known as the inventor of logarithms and made common the use of the decimal point in arithmetic and mathematics. After Napier, Edmund Gunter created the logarithmic scales (lines, or rules) upon which slide rules are based, it was William Oughtred who used two such scales sliding by one another to perform direct multiplication and division; and he is credited as the inventor of the slide rule in 1622. In 1631 Oughtred introduced the multiplication sign (×) his proportionality sign, and abbreviations \"sin\" and \"cos\" for the sine and cosine functions. Albert Girard also used the abbreviations 'sin', 'cos' and 'tan' for the trigonometric functions in his treatise.\n\nJohannes Kepler was one of the pioneers of the mathematical applications of infinitesimals. René Descartes is credited as the father of analytical geometry, the bridge between algebra and geometry, crucial to the discovery of infinitesimal calculus and analysis. In the 17th century, Descartes introduced Cartesian co-ordinates which allowed the development of analytic geometry. Blaise Pascal influenced mathematics throughout his life. His \"Traité du triangle arithmétique\" (\"Treatise on the Arithmetical Triangle\") of 1653 described a convenient tabular presentation for binomial coefficients. Pierre de Fermat and Blaise Pascal would investigate probability. John Wallis introduced the infinity symbol. He similarly used this notation for infinitesimals. In 1657, Christiaan Huygens published the treatise on probability, \"On Reasoning in Games of Chance\". \n\nJohann Rahn introduced the division symbol (obelus) and the therefore sign in 1659. William Jones used π in \"Synopsis palmariorum mathesios\" in 1706 because it is the letter of the Greek word perimetron (περιμετρον), which means perimeter in Greek. This usage was popularized in 1737 by Euler. In 1734, Pierre Bouguer used double horizontal bar below the inequality sign.\n\nThe study of linear algebra emerged from the study of determinants, which were used to solve systems of linear equations. Calculus had two main systems of notation, each created by one of the creators: that developed by Isaac Newton and the notation developed by Gottfried Leibniz. Leibniz's is the notation used most often today. Newton's was simply a dot or dash placed above the function. In modern usage, this notation generally denotes derivatives of physical quantities with respect to time, and is used frequently in the science of mechanics. Leibniz, on the other hand, used the letter \"d\" as a prefix to indicate differentiation, and introduced the notation representing derivatives as if they were a special type of fraction. This notation makes explicit the variable with respect to which the derivative of the function is taken. Leibniz also created the integral symbol. The symbol is an elongated S, representing the Latin word \"Summa\", meaning \"sum\". When finding areas under curves, integration is often illustrated by dividing the area into infinitely many tall, thin rectangles, whose areas are added. Thus, the integral symbol is an elongated s, for sum.\n\nLetters of the alphabet in this time were to be used as symbols of quantity; and although much diversity existed with respect to the choice of letters, there were to be several universally recognized rules in the following history. Here thus in the history of equations the first letters of the alphabet were indicatively known as coefficients, the last letters the s (an \"incerti ordinis\"). In algebraic geometry, again, a similar rule was to be observed, the last letters of the alphabet there denoting the variable or current coordinates. Certain letters, such as formula_5, formula_6, etc., were by universal consent appropriated as symbols of the frequently occurring numbers 3.14159 ..., and 2.7182818 ..., etc., and their use in any other acceptation was to be avoided as much as possible. Letters, too, were to be employed as symbols of operation, and with them other previously mentioned arbitrary operation characters. The letters formula_7, elongated formula_8 were to be appropriated as operative symbols in the differential calculus and integral calculus, formula_9 and ∑ in the calculus of differences. In functional notation, a letter, as a symbol of operation, is combined with another which is regarded as a symbol of quantity.\n\nBeginning in 1718, Thomas Twinin used the division slash (solidus), deriving it from the earlier Arabic horizontal fraction bar. Pierre-Simon, marquis de Laplace developed the widely used Laplacian differential operator. In 1750, Gabriel Cramer developed \"Cramer's Rule\" for solving linear systems.\n\nLeonhard Euler was one of the most prolific mathematicians in history, and also a prolific inventor of canonical notation. His contributions include his use of \"e\" to represent the base of natural logarithms. It is not known exactly why formula_6 was chosen, but it was probably because the four letters of the alphabet were already commonly used to represent variables and other constants. Euler used formula_5 to represent pi consistently. The use of formula_5 was suggested by William Jones, who used it as shorthand for perimeter. Euler used formula_13 to represent the square root of negative one, although he earlier used it as an \"infinite number.\" For summation, Euler used sigma, Σ. For functions, Euler used the notation formula_14 to represent a function of formula_15. In 1730, Euler wrote the gamma function. In 1736, Euler produces his paper on the Seven Bridges of Königsberg initiating the study of graph theory.\n\nThe mathematician, William Emerson would develop the proportionality sign. Much later in the abstract expressions of the value of various proportional phenomena, the parts-per notation would become useful as a set of pseudo units to describe small values of miscellaneous dimensionless quantities. Marquis de Condorcet, in 1768, advanced the partial differential sign. In 1771, Alexandre-Théophile Vandermonde deduced the importance of topological features when discussing the properties of knots related to the geometry of position. Between 1772 and 1788, Joseph-Louis Lagrange re-formulated the formulas and calculations of Classical \"Newtonian\" mechanics, called Lagrangian mechanics. The prime symbol for derivatives was also made by Lagrange.\n\nAt the turn of the 19th century, Carl Friedrich Gauss developed the identity sign for congruence relation and, in Quadratic reciprocity, the integral part. Gauss contributed functions of complex variables, in geometry, and on the convergence of series. He gave the satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law. Gauss developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy. He would also develop the product sign. Also in this time, Niels Henrik Abel and Évariste Galois conducted their work on the solvability of equations, linking group theory and field theory.\n\nAfter the 1800s, Christian Kramp would promote factorial notation during his research in generalized factorial function which applied to non-integers. Joseph Diaz Gergonne introduced the set inclusion signs. Peter Gustav Lejeune Dirichlet developed Dirichlet \"L\"-functions to give the proof of Dirichlet's theorem on arithmetic progressions and began analytic number theory. In 1828, Gauss proved his Theorema Egregium (\"remarkable theorem\" in Latin), establishing property of surfaces. In the 1830s, George Green developed Green's function. In 1829. Carl Gustav Jacob Jacobi publishes Fundamenta nova theoriae functionum ellipticarum with his elliptic theta functions. By 1841, Karl Weierstrass, the \"father of modern analysis\", elaborated on the concept of absolute value and the determinant of a matrix.\n\nMatrix notation would be more fully developed by Arthur Cayley in his three papers, on subjects which had been suggested by reading the Mécanique analytique of Lagrange and some of the works of Laplace. Cayley defined matrix multiplication and matrix inverses. Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".\nWilliam Rowan Hamilton would introduce the nabla symbol for vector differentials. This was previously used by Hamilton as a general-purpose operator sign. Hamilton reformulated Newtonian mechanics, now called Hamiltonian mechanics. This work has proven central to the modern study of classical field theories such as electromagnetism. This was also important to the development of quantum mechanics. In mathematics, he is perhaps best known as the inventor of quaternion notation and biquaternions. Hamilton also introduced the word \"tensor\" in 1846. James Cockle would develop the tessarines and, in 1849, coquaternions. In 1848, James Joseph Sylvester introduced into matrix algebra the term matrix.\n\nIn 1864 James Clerk Maxwell reduced all of the then current knowledge of electromagnetism into a linked set of differential equations with 20 equations in 20 variables, contained in \"A Dynamical Theory of the Electromagnetic Field\". (See Maxwell's equations.) The method of calculation which it is necessary to employ was given by Lagrange, and afterwards developed, with some modifications, by Hamilton's equations. It is usually referred to as Hamilton's principle; when the equations in the original form are used they are known as Lagrange's equations. In 1871 Richard Dedekind called a set of real or complex numbers which is closed under the four arithmetic operations a field. In 1873 Maxwell presented \"A Treatise on Electricity and Magnetism\".\n\nIn 1878, William Kingdon Clifford published his Elements of Dynamic. Clifford developed split-biquaternions, which he called \"algebraic motors\". Clifford obviated quaternion study by separating the dot product and cross product of two vectors from the complete quaternion notation. This approach made vector calculus available to engineers and others working in three dimensions and skeptical of the lead–lag effect in the fourth dimension. The common vector notations are used when working with vectors which are spatial or more abstract members of vector spaces, while angle notation (or phasor notation) is a notation used in electronics.\n\nIn 1881, Leopold Kronecker defined what he called a \"domain of rationality\", which is a field extension of the field of rational numbers in modern terms. In 1882, wrote the book titled \"Linear Algebra\". Lord Kelvin's aetheric atom theory (1860s) led Peter Guthrie Tait, in 1885, to publish a topological table of knots with up to ten crossings known as the Tait conjectures. In 1893, Heinrich M. Weber gave the clear definition of an abstract field. Tensor calculus was developed by Gregorio Ricci-Curbastro between 1887–96, presented in 1892 under the title \"absolute differential calculus\", and the contemporary usage of \"tensor\" was stated by Woldemar Voigt in 1898. In 1895, Henri Poincaré published \"Analysis Situs\". In 1897, Charles Proteus Steinmetz would publish , with the assistance of Ernst J. Berg.\n\nIn 1895 Giuseppe Peano issued his \"Formulario mathematico\", an effort to digest mathematics into terse text based on special symbols. He would provide a definition of a vector space and linear map. He would also introduce the intersection sign, the union sign, the membership sign (is an element of), and existential quantifier (there exists). Peano would pass to Bertrand Russell his work in 1900 at a Paris conference; it so impressed Russell that Russell too was taken with the drive to render mathematics more concisely. The result was Principia Mathematica written with Alfred North Whitehead. This treatise marks a watershed in modern literature where symbol became dominant. Ricci-Curbastro and Tullio Levi-Civita popularized the tensor index notation around 1900.\n\nAt the beginning of this period, Felix Klein's \"Erlangen program\" identified the underlying theme of various geometries, defining each of them as the study of properties invariant under a given group of symmetries. This level of abstraction revealed connections between geometry and abstract algebra. Georg Cantor would introduce the aleph symbol for cardinal numbers of transfinite sets. His notation for the cardinal numbers was the Hebrew letter formula_16 (aleph) with a natural number subscript; for the ordinals he employed the Greek letter ω (omega). This notation is still in use today in ordinal notation of a finite sequence of symbols from a finite alphabet which names an ordinal number according to some scheme which gives meaning to the language. His theory created a great deal of controversy. Cantor would, in his study of Fourier series, consider point sets in Euclidean space.\n\nAfter the turn of the 20th century, Josiah Willard Gibbs would in physical chemistry introduce middle dot for dot product and the multiplication sign for cross products. He would also supply notation for the scalar and vector products, which was introduced in \"Vector Analysis\". In 1904, Ernst Zermelo promotes axiom of choice and his proof of the well-ordering theorem. Bertrand Russell would shortly afterward introduce logical disjunction (OR) in 1906. Also in 1906, Poincaré would publish \"On the Dynamics of the Electron\" and Maurice Fréchet introduced metric space. Later, Gerhard Kowalewski and Cuthbert Edmund Cullis would successively introduce matrices notation, parenthetical matrix and box matrix notation respectively. After 1907, mathematicians studied knots from the point of view of the knot group and invariants from homology theory. In 1908, Joseph Wedderburn's structure theorems were formulated for finite-dimensional algebras over a field. Also in 1908, Ernst Zermelo proposed \"definite\" property and the first axiomatic set theory, Zermelo set theory. In 1910 Ernst Steinitz published the influential paper \"Algebraic Theory of Fields\". In 1911, Steinmetz would publish \"Theory and Calculation of Transient Electric Phenomena and Oscillations\".\nAlbert Einstein, in 1916, introduced the Einstein notation which summed over a set of indexed terms in a formula, thus exerting notational brevity. Arnold Sommerfeld would create the contour integral sign in 1917. Also in 1917, Dimitry Mirimanoff proposes axiom of regularity. In 1919, Theodor Kaluza would solve general relativity equations using five dimensions, the results would have electromagnetic equations emerge. This would be published in 1921 in \"Zum Unitätsproblem der Physik\". In 1922, Abraham Fraenkel and Thoralf Skolem independently proposed replacing the axiom schema of specification with the axiom schema of replacement. Also in 1922, Zermelo–Fraenkel set theory was developed. In 1923, Steinmetz would publish \"Four Lectures on Relativity and Space\". Around 1924, Jan Arnoldus Schouten would develop the modern notation and formalism for the Ricci calculus framework during the absolute differential calculus applications to general relativity and differential geometry in the early twentieth century. In 1925, Enrico Fermi would describe a system comprising many identical particles that obey the Pauli exclusion principle, afterwards developing a diffusion equation (Fermi age equation). In 1926, Oskar Klein would develop the Kaluza–Klein theory. In 1928, Emil Artin abstracted ring theory with Artinian rings. In 1933, Andrey Kolmogorov introduces the \"Kolmogorov axioms\". In 1937, Bruno de Finetti deduced the \"operational subjective\" concept.\n\nMathematical abstraction began as a process of extracting the underlying essence of a mathematical concept, removing any dependence on real world objects with which it might originally have been connected, and generalizing it so that it has wider applications or matching among other abstract descriptions of equivalent phenomena. Two abstract areas of modern mathematics are category theory and model theory. Bertrand Russell, said, \"Ordinary language is totally unsuited for expressing what physics really asserts, since the words of everyday life are not sufficiently abstract. Only mathematics and mathematical logic can say as little as the physicist means to say\". Though, one can substituted mathematics for real world objects, and wander off through equation after equation, and can build a concept structure which has no relation to reality.\n\nSymbolic logic studies the purely formal properties of strings of symbols. The interest in this area springs from two sources. First, the notation used in symbolic logic can be seen as representing the words used in philosophical logic. Second, the rules for manipulating symbols found in symbolic logic can be implemented on a computing machine. Symbolic logic is usually divided into two subfields, propositional logic and predicate logic. Other logics of interest include temporal logic, modal logic and fuzzy logic. The area of symbolic logic called propositional logic, also called \"propositional calculus\", studies the properties of sentences formed from constants and logical operators. The corresponding logical operations are known, respectively, as conjunction, disjunction, material conditional, biconditional, and negation. These operators are denoted as keywords and by symbolic notation.\n\nSome of the introduced mathematical logic notation during this time included the set of symbols used in Boolean algebra. This was created by George Boole in 1854. Boole himself did not see logic as a branch of mathematics, but it has come to be encompassed anyway. Symbols found in Boolean algebra include formula_17 (AND), formula_18 (OR), and formula_19 (\"not\"). With these symbols, and letters to represent different truth values, one can make logical statements such as formula_20, that is \"(\"a\" is true OR \"a\" is \"not\" true) is true\", meaning it is true that \"a\" is either true or not true (i.e. false). Boolean algebra has many practical uses as it is, but it also was the start of what would be a large set of symbols to be used in logic. Predicate logic, originally called \"predicate calculus\", expands on propositional logic by the introduction of variables and by sentences containing variables, called predicates. In addition, predicate logic allows quantifiers. With these logic symbols and additional quantifiers from predicate logic, valid proofs can be made that are irrationally artificial, but syntactical.\n\nWhile proving his incompleteness theorems, Kurt Gödel created an alternative to the symbols normally used in logic. He used Gödel numbers, which were numbers that represented operations with set numbers, and variables with the prime numbers greater than 10. With Gödel numbers, logic statements can be broken down into a number sequence. Gödel then took this one step farther, taking the \"n\" prime numbers and putting them to the power of the numbers in the sequence. These numbers were then multiplied together to get the final product, giving every logic statement its own number.\n\nAbstraction of notation is an ongoing process and the historical development of many mathematical topics exhibits a progression from the concrete to the abstract. Various set notations would be developed for fundamental object sets. Around 1924, David Hilbert and Richard Courant published \"Methods of mathematical physics. Partial differential equations\". In 1926, Oskar Klein and Walter Gordon proposed the Klein–Gordon equation to describe relativistic particles. The first formulation of a quantum theory describing radiation and matter interaction is due to Paul Adrien Maurice Dirac, who, during 1920, was first able to compute the coefficient of spontaneous emission of an atom. In 1928, the relativistic Dirac equation was formulated by Dirac to explain the behavior of the relativistically moving electron. Dirac described the quantification of the electromagnetic field as an ensemble of harmonic oscillators with the introduction of the concept of creation and annihilation operators of particles. In the following years, with contributions from Wolfgang Pauli, Eugene Wigner, Pascual Jordan, and Werner Heisenberg, and an elegant formulation of quantum electrodynamics due to Enrico Fermi, physicists came to believe that, in principle, it would be possible to perform any computation for any physical process involving photons and charged particles.\n\nIn 1931, Alexandru Proca developed the Proca equation (Euler–Lagrange equation) for the vector meson theory of nuclear forces and the relativistic quantum field equations. John Archibald Wheeler in 1937 develops S-matrix. Studies by Felix Bloch with Arnold Nordsieck, and Victor Weisskopf, in 1937 and 1939, revealed that such computations were reliable only at a first order of perturbation theory, a problem already pointed out by Robert Oppenheimer. At higher orders in the series infinities emerged, making such computations meaningless and casting serious doubts on the internal consistency of the theory itself. With no solution for this problem known at the time, it appeared that a fundamental incompatibility existed between special relativity and quantum mechanics.\n\nIn the 1930s, the double-struck capital Z for integer number sets was created by Edmund Landau. Nicolas Bourbaki created the double-struck capital Q for rational number sets. In 1935, Gerhard Gentzen made universal quantifiers. In 1936, Tarski's undefinability theorem is stated by Alfred Tarski and proved. In 1938, Gödel proposes the constructible universe in the paper \"The Consistency of the Axiom of Choice and of the Generalized Continuum-Hypothesis\". André Weil and Nicolas Bourbaki would develop the empty set sign in 1939. That same year, Nathan Jacobson would coin the double-struck capital C for complex number sets.\n\nAround the 1930s, Voigt notation would be developed for multilinear algebra as a way to represent a symmetric tensor by reducing its order. Schönflies notation became one of two conventions used to describe point groups (the other being Hermann–Mauguin notation). Also in this time, van der Waerden notation became popular for the usage of two-component spinors (Weyl spinors) in four spacetime dimensions. Arend Heyting would introduce Heyting algebra and Heyting arithmetic.\n\nThe arrow, e.g., →, was developed for function notation in 1936 by Øystein Ore to denote images of specific elements. Later, in 1940, it took its present form, e.g., \"f: X → Y\", through the work of Witold Hurewicz. Werner Heisenberg, in 1941, proposed the S-matrix theory of particle interactions.\nBra–ket notation (Dirac notation) is a standard notation for describing quantum states, composed of angle brackets and vertical bars. It can also be used to denote abstract vectors and linear functionals. It is so called because the inner product (or dot product on a complex vector space) of two states is denoted by a bra|ket consisting of a left part, ⟨\"φ\"|, and a right part, |\"ψ\"⟩. The notation was introduced in 1939 by Paul Dirac, though the notation has precursors in Grassmann's use of the notation [\"φ\"|\"ψ\"] for his inner products nearly 100 years previously.\n\nBra–ket notation is widespread in quantum mechanics: almost every phenomenon that is explained using quantum mechanics—including a large portion of modern physics—is usually explained with the help of bra–ket notation. The notation establishes an encoded abstract representation-independence, producing a versatile specific representation (e.g., \"x\", or \"p\", or eigenfunction base) without much , or excessive reliance on, the nature of the linear spaces involved. The overlap expression ⟨\"φ\"|\"ψ\"⟩ is typically interpreted as the probability amplitude for the state \"ψ\" to collapse into the state \"ϕ\". The Feynman slash notation (Dirac slash notation) was developed by Richard Feynman for the study of Dirac fields in quantum field theory.\n\nIn 1948, Valentine Bargmann and Eugene Wigner proposed the relativistic Bargmann–Wigner equations to describe free particles and the equations are in the form of multi-component spinor field wavefunctions. In 1950, William Vallance Douglas Hodge presented \"The topological invariants of algebraic varieties\" at the Proceedings of the International Congress of Mathematicians. Between 1954 and 1957, Eugenio Calabi worked on the Calabi conjecture for Kähler metrics and the development of Calabi–Yau manifolds. In 1957, Tullio Regge formulated the mathematical property of potential scattering in the Schrödinger equation. Stanley Mandelstam, along with Regge, did the initial development of the Regge theory of strong interaction phenomenology. In 1958, Murray Gell-Mann and Richard Feynman, along with George Sudarshan and Robert Marshak, deduced the chiral structures of the weak interaction in physics. Geoffrey Chew, along with others, would promote matrix notation for the strong interaction, and the associated bootstrap principle, in 1960. In the 1960s, set-builder notation was developed for describing a set by stating the properties that its members must satisfy. Also in the 1960s, tensors are abstracted within category theory by means of the concept of monoidal category. Later, multi-index notation eliminates conventional notions used in multivariable calculus, partial differential equations, and the theory of distributions, by abstracting the concept of an integer index to an ordered tuple of indices.\n\nIn the modern mathematics of special relativity, electromagnetism and wave theory, the d'Alembert operator is the Laplace operator of Minkowski space. The Levi-Civita symbol is used in tensor calculus.\n\nAfter the full Lorentz covariance formulations that were finite at any order in a perturbation series of quantum electrodynamics, Sin-Itiro Tomonaga, Julian Schwinger and Richard Feynman were jointly awarded with a Nobel prize in physics in 1965. Their contributions, and those of Freeman Dyson, were about covariant and gauge invariant formulations of quantum electrodynamics that allow computations of observables at any order of perturbation theory. Feynman's mathematical technique, based on his diagrams, initially seemed very different from the field-theoretic, operator-based approach of Schwinger and Tomonaga, but Freeman Dyson later showed that the two approaches were equivalent. Renormalization, the need to attach a physical meaning at certain divergences appearing in the theory through integrals, has subsequently become one of the fundamental aspects of quantum field theory and has come to be seen as a criterion for a theory's general acceptability. Quantum electrodynamics has served as the model and template for subsequent quantum field theories. Peter Higgs, Jeffrey Goldstone, and others, Sheldon Glashow, Steven Weinberg and Abdus Salam independently showed how the weak nuclear force and quantum electrodynamics could be merged into a single electroweak force. In the late 1960s, the particle zoo was composed of the then known elementary particles before the discovery of quarks.\n\nA step towards the Standard Model was Sheldon Glashow's discovery, in 1960, of a way to combine the electromagnetic and weak interactions. In 1967, Steven Weinberg and Abdus Salam incorporated the Higgs mechanism into Glashow's electroweak theory, giving it its modern form. The Higgs mechanism is believed to give rise to the masses of all the elementary particles in the Standard Model. This includes the masses of the W and Z bosons, and the masses of the fermions - i.e. the quarks and leptons. Also in 1967, Bryce DeWitt published his equation under the name \"\"Einstein–Schrödinger equation\" (later renamed the \"Wheeler–DeWitt equation\"\"). In 1969, Yoichiro Nambu, Holger Bech Nielsen, and Leonard Susskind described space and time in terms of strings. In 1970, Pierre Ramond develop two-dimensional supersymmetries. Michio Kaku and Keiji Kikkawa would afterwards formulate string variations. In 1972, Michael Artin, Alexandre Grothendieck, Jean-Louis Verdier propose the Grothendieck universe.\n\nAfter the neutral weak currents caused by boson exchange were discovered at CERN in 1973, the electroweak theory became widely accepted and Glashow, Salam, and Weinberg shared the 1979 Nobel Prize in Physics for discovering it. The theory of the strong interaction, to which many contributed, acquired its modern form around 1973–74. With the establishment of quantum chromodynamics, a finalized a set of fundamental and exchange particles, which allowed for the establishment of a \"standard model\" based on the mathematics of gauge invariance, which successfully described all forces except for gravity, and which remains generally accepted within the domain to which it is designed to be applied. In the late 1970s, William Thurston introduced hyperbolic geometry into the study of knots with the hyperbolization theorem. The orbifold notation system, invented by Thurston, has been developed for representing types of symmetry groups in two-dimensional spaces of constant curvature. In 1978, Shing-Tung Yau deduced that the Calabi conjecture have Ricci flat metrics. In 1979, Daniel Friedan showed that the equations of motions of string theory are abstractions of Einstein equations of General Relativity.\n\nThe first superstring revolution is composed of mathematical equations developed between 1984 and 1986. In 1984, Vaughan Jones deduced the Jones polynomial and subsequent contributions from Edward Witten, Maxim Kontsevich, and others, revealed deep connections between knot theory and mathematical methods in statistical mechanics and quantum field theory. According to string theory, all particles in the \"particle zoo\" have a common ancestor, namely a vibrating string. In 1985, Philip Candelas, Gary Horowitz, Andrew Strominger, and Edward Witten would publish \"Vacuum configurations for superstrings\" Later, the tetrad formalism (tetrad index notation) would be introduced as an approach to general relativity that replaces the choice of a coordinate basis by the less restrictive choice of a local basis for the tangent bundle.\n\nIn the 1990s, Roger Penrose would propose Penrose graphical notation (tensor diagram notation) as a, usually handwritten, visual depiction of multilinear functions or tensors. Penrose would also introduce abstract index notation. In 1995, Edward Witten suggested M-theory and subsequently used it to explain some observed dualities, initiating the second superstring revolution.\nJohn Conway would further various notations, including the Conway chained arrow notation, the Conway notation of knot theory, and the Conway polyhedron notation. The Coxeter notation system classifies symmetry groups, describing the angles between with fundamental reflections of a Coxeter group. It uses a bracketed notation, with modifiers to indicate certain subgroups. The notation is named after H. S. M. Coxeter and Norman Johnson more comprehensively defined it.\n\nCombinatorial LCF notation has been developed for the representation of cubic graphs that are Hamiltonian. The cycle notation is the convention for writing down a permutation in terms of its constituent cycles. This is also called circular notation and the permutation called a \"cyclic\" or \"circular\" permutation.\n\nIn 1931, IBM produces the IBM 601 Multiplying Punch; it is an electromechanical machine that could read two numbers, up to 8 digits long, from a card and punch their product onto the same card. In 1934, Wallace Eckert used a rigged IBM 601 Multiplying Punch to automate the integration of differential equations. In 1936, Alan Turing publishes \"On Computable Numbers, With an Application to the Entscheidungsproblem\". John von Neumann, pioneer of the digital computer and of computer science, in 1945, writes the incomplete \"First Draft of a Report on the EDVAC\". In 1962, Kenneth E. Iverson developed an integral part notation that became known as Iverson Notation for manipulating arrays that he taught to his students, and described in his book \"A Programming Language\". In 1970, E.F. Codd proposed relational algebra as a relational model of data for database query languages. In 1971, Stephen Cook publishes \"The complexity of theorem proving procedures\" In the 1970s within computer architecture, Quote notation was developed for a representing number system of rational numbers. Also in this decade, the Z notation (just like the APL language, long before it) uses many non-ASCII symbols, the specification includes suggestions for rendering the Z notation symbols in ASCII and in LaTeX. There are presently various C mathematical functions (Math.h) and numerical libraries. They are libraries used in software development for performing numerical calculations. These calculations can be handled by symbolic executions; analyzing a program to determine what inputs cause each part of a program to execute. Mathematica and SymPy are examples of computational software programs based on symbolic mathematics.\n\nIn the history of mathematical notation, ideographic symbol notation has come full circle with the rise of computer visualization systems. The notations can be applied to abstract visualizations, such as for rendering some projections of a Calabi-Yau manifold. Examples of abstract visualization which properly belong to the mathematical imagination can be found in computer graphics. The need for such models abounds, for example, when the measures for the subject of study are actually random variables and not really ordinary mathematical functions.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "302178", "url": "https://en.wikipedia.org/wiki?curid=302178", "title": "Index of logic articles", "text": "Index of logic articles\n\nA System of Logic --\nA priori and a posteriori --\nAbacus logic --\nAbduction (logic) --\nAbductive validation --\nAcademia Analitica --\nAccuracy and precision --\nAd captandum --\nAd hoc hypothesis --\nAd hominem --\nAffine logic --\nAffirming the antecedent --\nAffirming the consequent --\nAlgebraic logic --\nAmbiguity --\nAnalysis --\nAnalysis (journal) --\nAnalytic reasoning --\nAnalytic–synthetic distinction --\nAnangeon --\nAnecdotal evidence --\nAntecedent (logic) --\nAntepredicament --\nAnti-psychologism --\nAntinomy --\nApophasis --\nAppeal to probability --\nAppeal to ridicule --\nArchive for Mathematical Logic --\nArché --\nArgument --\nArgument by example --\nArgument form --\nArgument from authority --\nArgument map --\nArgumentation ethics --\nArgumentation theory --\nArgumentum ad baculum --\nArgumentum e contrario --\nAriadne's thread (logic) --\nAristotelian logic --\nAristotle --\nAssociation for Informal Logic and Critical Thinking --\nAssociation for Logic, Language and Information --\nAssociation for Symbolic Logic --\nAttacking Faulty Reasoning --\nAustralasian Association for Logic --\nAxiom --\nAxiom independence --\nAxiom of reducibility --\nAxiomatic system --\nAxiomatization --\n\nBackward chaining --\nBarcan formula --\nBegging the question --\nBegriffsschrift --\nBelief --\nBelief bias --\nBelief revision --\nBenson Mates --\nBertrand Russell Society --\nBiconditional elimination --\nBiconditional introduction --\nBivalence and related laws --\nBlue and Brown Books --\nBoole's syllogistic --\nBoolean algebra (logic) --\nBoolean algebra (structure) --\nBoolean network --\n\nCanon (basic principle) --\nCanonical form --\nCanonical form (Boolean algebra) --\nCartesian circle --\nCase-based reasoning --\nCategorical logic --\nCategories (Aristotle) --\nCategories (Peirce) --\nCategory mistake --\nCatuṣkoṭi --\nCircular definition --\nCircular reasoning --\nCircular reference --\nCircular reporting --\nCircumscription (logic) --\nCircumscription (taxonomy) --\nClassical logic --\nClocked logic --\nCognitive bias --\nCointerpretability --\nColorless green ideas sleep furiously --\nCombinational logic --\nCombinatory logic --\nCombs method --\nCommon knowledge (logic) --\nCommutativity of conjunction --\nCompleteness (logic) --\nComposition of Causes --\nCompossibility --\nComprehension (logic) --\nComputability logic --\nConcept --\nConceptualism --\nCondensed detachment --\nConditional disjunction --\nConditional probability --\nConditional proof --\nConditional quantifier --\nConfirmation bias --\nConflation --\nConfusion of the inverse --\nConjunction elimination --\nConjunction fallacy --\nConjunction introduction --\nConjunctive normal form --\nConnexive logic --\nConnotation --\nConsequent --\nConsistency --\nConstructive dilemma --\nContra principia negantem non est disputandum --\nContradiction --\nContrapositive --\nControl logic --\nConventionalism --\nConverse (logic) --\nConverse Barcan formula --\nCorrelative-based fallacies --\nCounterexample --\nCounterfactual conditional --\nCounterintuitive --\nCratylism --\nCredibility --\nCriteria of truth --\nCritical-Creative Thinking and Behavioral Research Laboratory --\nCritical pedagogy --\nCritical reading --\nCritical thinking --\nCritique of Pure Reason --\nCurry's paradox --\nCyclic negation --\n\nDagfinn Føllesdal --\nDe Interpretatione --\nDe Morgan's laws --\nDecidability (logic) --\nDecidophobia --\nDecision making --\nDecisional balance sheet --\nDeductive closure --\nDeduction theorem --\nDeductive fallacy --\nDeductive reasoning --\nDefault logic --\nDefeasible logic --\nDefeasible reasoning --\nDefinable set --\nDefinist fallacy --\nDefinition --\nDefinitions of logic --\nDegree of truth --\nDenying the antecedent --\nDenying the correlative --\nDeontic logic --\nDescription --\nDescription logic --\nDescriptive fallacy --\nDeviant logic --\nDharmakirti --\nDiagrammatic reasoning --\nDialectica --\nDialectica space --\nDialetheism --\nDichotomy --\nDifference (philosophy) --\nDigital timing diagram --\nDignāga --\nDilemma --\nDisjunction elimination --\nDisjunction introduction --\nDisjunctive normal form --\nDisjunctive syllogism --\nDispositional and occurrent belief --\nDisquotational principle --\nDissoi logoi --\nDivision of Logic, Methodology, and Philosophy of Science --\nDon't-care term --\nDonald Davidson (philosopher) --\nDouble counting (fallacy) --\nDouble negation --\nDouble negative --\nDouble negative elimination --\nDoxa --\nDrinking the Kool-Aid --\n\nEL++ --\nEcological fallacy --\nEffective method --\nElimination rule --\nEmotional reasoning --\nEmotions in decision-making --\nEmpty name --\nEncyclopedia of the Philosophical Sciences --\nEnd term --\nEngineered language --\nEntailment --\nEntitative graph --\nEnumerative definition --\nEpicureanism --\nEpilogism --\nEpistemic closure --\nEquisatisfiability --\nErotetics --\nEternal statement --\nEtymological fallacy --\nEuropean Summer School in Logic, Language and Information --\nEvidence --\nExclusive nor --\nExclusive or --\nExistential fallacy --\nExistential graph --\nExistential quantification --\nExpert --\nExplanandum --\nExplanation --\nExplanatory power --\nExtension (semantics) --\nExtensional context --\nExtensional definition --\n\nFa (concept) --\nFact --\nFallacies of definition --\nFallacy --\nFallacy of distribution --\nFallacy of four terms --\nFallacy of quoting out of context --\nFallacy of the four terms --\nFalse attribution --\nFalse dilemma --\nFalse equivalence --\nFalse premise --\nFictionalism --\nFinitary relation --\nFinite model property --\nFirst-order logic --\nFirst-order predicate --\nFirst-order predicate calculus --\nFirst-order resolution --\nFitch-style calculus --\nFluidic logic --\nFluidics --\nFormal fallacy --\nFormal ontology --\nFormal system --\nFormalism (philosophy) --\nForward chaining --\nFree logic --\nFree variables and bound variables --\nFunction and Concept --\nFuzzy logic --\n\nGame semantics --\nGanto's Ax --\nGeometry of interaction --\nGilles-Gaston Granger --\nGongsun Long --\nGrammaticality --\nGreedy reductionism --\nGrundlagen der Mathematik --\n\nHPO formalism --\nHalo effect --\nHandbook of Automated Reasoning --\nHanlon's razor --\nHasty generalization --\nHerbrandization --\nHetucakra --\nHeyting algebra --\nHigher-order predicate --\nHigher-order thinking --\nHistorian's fallacy --\nHistorical fallacy --\nHistory of logic --\nHistory of the function concept --\nHold come what may --\nHomunculus argument --\nHorn clause --\nHume's fork --\nHume's principle --\nHypothetical syllogism --\n\nIdentity (philosophy) --\nIdentity of indiscernibles --\nIdola fori --\nIdola specus --\nIdola theatri --\nIdola tribus --\nIf-by-whiskey --\nIff --\nIllicit major --\nIllicit minor --\nIlluminationism --\nImmutable truth --\nImperative logic --\nImplicant --\nInclusion (logic) --\nIncomplete comparison --\nInconsistent comparison --\nInconsistent triad --\nIndependence-friendly logic --\nIndian logic --\nInductive logic --\nInductive logic programming --\nInference --\nInference procedure --\nInference rule --\nInferential role semantics --\nInfinitary logic --\nInfinite regress --\nInfinity --\nInformal fallacy --\nInformal logic --\nInquiry --\nInquiry (philosophy journal) --\nInsolubilia --\nInstitute for Logic, Language and Computation --\nIntellectual responsibility --\nIntended interpretation --\nIntension --\nIntensional fallacy --\nIntensional logic --\nIntensional statement --\nIntentional Logic --\nIntermediate logic --\nInterpretability --\nInterpretability logic --\nInterpretive discussion --\nIntroduction rule --\nIntroduction to Mathematical Philosophy --\nIntuitionistic linear logic --\nIntuitionistic logic --\nInvalid proof --\nInventor's paradox --\nInverse (logic) --\nInverse consequences --\nIrreducibility --\nIs Logic Empirical? --\nIsagoge --\nIvor Grattan-Guinness --\n\nJacobus Naveros --\nJayanta Bhatta --\nJingle-jangle fallacies --\nJohn Corcoran (logician) --\nJohn W. Dawson, Jr --\nJournal of Applied Non-Classical Logics --\nJournal of Automated Reasoning --\nJournal of Logic, Language and Information --\nJournal of Logic and Computation --\nJournal of Mathematical Logic --\nJournal of Philosophical Logic --\nJournal of Symbolic Logic --\nJudgment (mathematical logic) --\nJudgmental language --\nJust-so story --\n\nKarnaugh map --\nKinetic logic --\nKnowing and the Known --\nKripke semantics --\nKurt Gödel Society --\n\nLanguage --\nLanguage, Proof and Logic --\nLateral thinking --\nLaw of excluded middle --\nLaw of identity --\nLaw of non-contradiction --\nLaw of noncontradiction --\nLaw of thought --\nLaws of Form --\nLaws of logic --\nLeap of faith --\nLemma (logic) --\nLexical definition --\nLinear logic --\nLinguistic and Philosophical Investigations --\nLinguistics and Philosophy --\nList of fallacies --\nList of incomplete proofs --\nList of logic journals --\nList of paradoxes --\nLogic --\nLogic Lane --\nLogic Spectacles --\nLogic gate --\nLogic in China --\nLogic in Islamic philosophy --\nLogic of class --\nLogic of information --\nLogic programming --\nLogica Universalis --\nLogica nova --\nLogical Analysis and History of Philosophy --\nLogical Investigations (Husserl) --\nLogical Methods in Computer Science --\nLogical abacus --\nLogical argument --\nLogical assertion --\nLogical atomism --\nLogical biconditional --\nLogical conditional --\nLogical conjunction --\nLogical constant --\nLogical disjunction --\nLogical equality --\nLogical equivalence --\nLogical extreme --\nLogical form --\nLogical harmony --\nLogical holism --\nLogical nand --\nLogical nor --\nLogical operator --\nLogical quality --\nLogical truth --\nLogicism --\nLogico-linguistic modeling --\nLogos --\nLoosely associated statements --\nŁoś–Tarski preservation theorem --\nLudic fallacy --\nLwów–Warsaw school of logic --\n\nMain contention --\nMajor term --\nMarkov's principle --\nMartin Gardner bibliography --\nMasked man fallacy --\nMaterial conditional --\nMathematical fallacy --\nMathematical logic --\nMeaning (linguistics) --\nMeaning (non-linguistic) --\nMeaning (philosophy of language) --\nMeaningless statement --\nMegarian school --\nMental model theory of reasoning --\nMereology --\nMeta-communication --\nMetalanguage --\nMetalogic --\nMetamathematics --\nMetasyntactic variable --\nMetatheorem --\nMetavariable --\nMiddle term --\nMinimal axioms for Boolean algebra --\nMinimal logic --\nMinor premise --\nMiscellanea Logica --\nMissing dollar riddle --\nModal fallacy --\nModal fictionalism --\nModal logic --\nModel theory --\nModus ponens --\nModus tollens --\nMoral reasoning --\nMotivated reasoning --\nMoving the goalposts --\nMultigrade predicate --\nMulti-valued logic --\nMultiple-conclusion logic --\nMutatis mutandis --\nMutual knowledge (logic) --\nMutually exclusive events --\nMünchhausen trilemma --\n\nNaive set theory --\nName --\nNarrative logic --\nNatural deduction --\nNatural kind --\nNatural language --\nNecessary and sufficient --\nNecessity and sufficiency --\nNegation --\nNeutrality (philosophy) --\nNirvana fallacy --\nNixon diamond --\nNo true Scotsman --\nNominal identity --\nNon-Aristotelian logic --\nNon-classical logic --\nNon-monotonic logic --\nNon-rigid designator --\nNon sequitur (logic) --\nNoneism --\nNonfirstorderizability --\nNordic Journal of Philosophical Logic --\nNormal form (natural deduction) --\nNovum Organum --\nNyaya --\nNyāya Sūtras --\n\nObject language --\nObject of the mind --\nObject theory --\nOccam's razor --\nOn Formally Undecidable Propositions of Principia Mathematica and Related Systems --\nOne-sided argument --\nOntological commitment --\nOpen sentence --\nOpinion --\nOpposing Viewpoints series --\nOrdered logic --\nOrganon --\nOriginal proof of Gödel's completeness theorem --\nOsmund Lewry --\nOstensive definition --\nOutline of logic --\nOverbelief --\n\nPackage-deal fallacy --\nPanlogism --\nParaconsistent logic --\nParaconsistent logics --\nParade of horribles --\nParadox --\nPars destruens/pars construens --\nPathetic fallacy --\nPer fas et nefas --\nPersuasive definition --\nPeter Simons (academic) --\nPhilosophia Mathematica --\nPhilosophical logic --\nPhilosophy of logic --\nPierce's law --\nPlural quantification --\nPoisoning the well --\nPolarity item --\nPolish Logic --\nPolish notation --\nPolitician's syllogism --\nPolychotomous key --\nPolylogism --\nPolysyllogism --\nPort-Royal Logic --\nPossible world --\nPost's lattice --\nPost disputation argument --\nPost hoc ergo propter hoc --\nPosterior Analytics --\nPractical syllogism --\nPragmatic mapping --\nPragmatic maxim --\nPragmatic theory of truth --\nPramāṇa --\nPramāṇa-samuccaya --\nPrecising definition --\nPrecision questioning --\nPredicable --\nPredicate (logic) --\nPredicate abstraction --\nPredicate logic --\nPreferential entailment --\nPreintuitionism --\nPrescriptivity --\nPresentism (literary and historical analysis) --\nPresupposition --\nPrincipia Mathematica --\nPrinciple of bivalence --\nPrinciple of explosion --\nPrinciple of nonvacuous contrast --\nPrinciple of sufficient reason --\nPrinciples of Mathematical Logic --\nPrior Analytics --\nPrivate Eye Project --\nPro hominem --\nProbabilistic logic --\nProbabilistic logic network --\nProblem of future contingents --\nProblem of induction --\nProcess of elimination --\nProject Reason --\nProof-theoretic semantics --\nProof (truth) --\nProof by assertion --\nProof theory --\nPropaganda techniques --\nProposition --\nPropositional calculus --\nPropositional function --\nPropositional representation --\nPropositional variable --\nProsecutor's fallacy --\nProvability logic --\nProving too much --\nPrudence --\nPseudophilosophy --\nPsychologism --\nPsychologist's fallacy --\n\nQ.E.D. --\nQuantification --\nQuantization (linguistics) --\nQuantum logic --\n\nRamism --\nRationality --\nRazor (philosophy) --\nReason --\nReductio ad absurdum --\nReference --\nReflective equilibrium --\nRegression fallacy --\nRegular modal logic --\nReification (fallacy) --\nRelativist fallacy --\nRelevance --\nRelevance logic --\nRelevant logic --\nRemarks on the Foundations of Mathematics --\nRetroduction --\nRetrospective determinism --\nRevolutions in Mathematics --\nRhetoric --\nRigour --\nRolandas Pavilionis --\nRound square copula --\nRudolf Carnap --\nRule of inference --\nRvachev function --\n\nSEE-I --\nSalva congruitate --\nSalva veritate --\nSatisfiability --\nScholastic logic --\nSchool of Names --\nScience of Logic --\nScientific temper --\nSecond-order predicate --\nSegment addition postulate --\nSelf-reference --\nSelf-refuting idea --\nSelf-verifying theories --\nSemantic theory of truth --\nSemantics --\nSense and reference --\nSequent --\nSequent calculus --\nSequential logic --\nSet (mathematics) --\nSeven Types of Ambiguity (Empson) --\nSheffer stroke --\nShip of Theseus --\nSimple non-inferential passage --\nSingular term --\nSituation --\nSituational analysis --\nSkeptic's Toolbox --\nSlingshot argument --\nSocial software (social procedure) --\nSocratic questioning --\nSoku hi --\nSome Remarks on Logical Form --\nSophism --\nSophistical Refutations --\nSoundness --\nSource credibility --\nSource criticism --\nSpecial case --\nSpecialization (logic) --\nSpeculative reason --\nSpurious relationship --\nSquare of opposition --\nState of affairs (philosophy) --\nStatement (logic) --\nStraight and Crooked Thinking --\nStraight face test --\nStraw man --\nStrength (mathematical logic) --\nStrict conditional --\nStrict implication --\nStrict logic --\nStructural rule --\nStudia Logica --\nStudies in Logic, Grammar and Rhetoric --\nSubjective logic --\nSubstitution (logic) --\nSubstructural logic --\nSufficient condition --\nSum of Logic --\nSunk costs --\nSupertask --\nSupervaluationism --\nSupposition theory --\nSurvivorship bias --\nSyllogism --\nSyllogistic fallacy --\nSymbol (formal) --\nSyntactic Structures --\nSyntax (logic) --\nSynthese --\nSystems of Logic Based on Ordinals --\n\nT-schema --\nTacit assumption --\nTarski's undefinability theorem --\nTautology (logic) --\nTemporal logic --\nTemporal parts --\nTeorema (journal) --\nTerm (argumentation) --\nTerm logic --\nTernary logic --\nTestability --\nTetralemma --\nTextual case based reasoning --\nThe False Subtlety of the Four Syllogistic Figures --\nThe Foundations of Arithmetic --\nThe Geography of Thought --\nThe Laws of Thought --\nThe Paradoxes of the Infinite --\nTheorem --\nTheoretical definition --\nTheory and Decision --\nTheory of justification --\nTheory of obligationes --\nThird-cause fallacy --\nThree men make a tiger --\nTolerance (in logic) --\nTopical logic --\nTopics (Aristotle) --\nTractatus Logico-Philosophicus --\nTrain of thought --\nTrairūpya --\nTransferable belief model --\nTransparent Intensional Logic --\nTregoED --\nTrikonic --\nTrilemma --\nTrivial objections --\nTrivialism --\nTruth --\nTruth-bearer --\nTruth claim --\nTruth condition --\nTruth function --\nTruth value --\nTruthiness --\nTruthmaker --\nType (model theory) --\nType theory --\nType–token distinction --\n\nUltrafinitism --\nUnification (computer science) --\nUnifying theories in mathematics --\nUniqueness quantification --\nUniversal logic --\nUniversal quantification --\nUnivocity --\nUnspoken rule --\nUse–mention distinction --\n\nVacuous truth --\nVagrant predicate --\nVagueness --\nValidity --\nValuation-based system --\nVan Gogh fallacy --\nVenn diagram --\nVicious circle principle --\n\nWarnier/Orr diagram --\nWell-formed formula --\nWhat the Tortoise Said to Achilles --\nWillard Van Orman Quine --\nWilliam Kneale --\nWindow operator --\nWisdom of repugnance --\nWitness (mathematics) --\nWord sense --\n\nZhegalkin polynomial --\n\n"}
{"id": "393055", "url": "https://en.wikipedia.org/wiki?curid=393055", "title": "Index set", "text": "Index set\n\nIn mathematics, an index set is a set whose members label (or index) members of another set. For instance, if the elements of a set \"A\" may be \"indexed\" or \"labeled\" by means of the elements of a set \"J\", then \"J\" is an index set. The indexing consists of a surjective function from \"J\" onto \"A\" and the indexed collection is typically called an \"(indexed) family\", often written as {\"A\"}.\n\n\nThe set of all the formula_6 functions is an uncountable set indexed by formula_7.\n\nIn computational complexity theory and cryptography, an index set is a set for which there exists an algorithm formula_8 that can sample the set efficiently; e.g., on input formula_9, formula_8 can efficiently select a poly(n)-bit long element from the set.\n\n"}
{"id": "14511776", "url": "https://en.wikipedia.org/wiki?curid=14511776", "title": "Kawasaki's theorem", "text": "Kawasaki's theorem\n\nKawasaki's theorem is a theorem in the mathematics of paper folding that describes the crease patterns with a single vertex that may be folded to form a flat figure. It states that the pattern is flat-foldable if and only if alternatingly adding and subtracting the angles of consecutive folds around the vertex gives an alternating sum of zero.\nCrease patterns with more than one vertex do not obey such a simple criterion, and are NP-hard to fold.\n\nThe theorem is named after one of its discoverers, Toshikazu Kawasaki. However, several others also contributed to its discovery, and it is sometimes called the Kawasaki–Justin theorem or Husimi's theorem after other contributors, Jacques Justin and Kôdi Husimi.\n\nA one-vertex crease pattern consists of a set of rays or creases drawn on a flat sheet of paper, all emanating from the same point interior to the sheet. (This point is called the vertex of the pattern.) Each crease must be folded, but the pattern does not specify whether the folds should be mountain folds or valley folds. The goal is to determine whether it is possible to fold the paper so that every crease is folded, no folds occur elsewhere, and the whole folded sheet of paper lies flat.\n\nTo fold flat, the number of creases must be even. This follows, for instance, from Maekawa's theorem, which states that the number of mountain folds at a flat-folded vertex differs from the number of valley folds by exactly two folds. Therefore, suppose that a crease pattern consists of an even number of creases, and let be the consecutive angles between the creases around the vertex, in clockwise order, starting at any one of the angles. Then Kawasaki's theorem states that the crease pattern may be folded flat if and only if the alternating sum and difference of the angles adds to zero:\n\nAn equivalent way of stating the same condition is that, if the angles are partitioned into two alternating subsets, then the sum of the angles in either of the two subsets is exactly 180 degrees. However, this equivalent form applies only to a crease pattern on a flat piece of paper, whereas the alternating sum form of the condition remains valid for crease patterns on conical sheets of paper with nonzero defect at the vertex.\n\nKawasaki's theorem, applied to each of the vertices of an arbitrary crease pattern, determines whether the crease pattern is locally flat-foldable, meaning that the part of the crease pattern near the vertex can be flat-folded. However, there exist crease patterns that are locally flat-foldable but that have no global flat folding that works for the whole crease pattern at once. conjectured that global flat-foldability could be tested by checking Kawasaki's theorem at each vertex of a crease pattern, and then also testing bipartiteness of an undirected graph associated with the crease pattern. However, this conjecture was disproven by , who showed that Hull's conditions are not sufficient. More strongly, Bern and Hayes showed that the problem of testing global flat-foldability is NP-complete.\n\nTo show that Kawasaki's condition necessarily holds for any flat-folded figure, it suffices to observe that, at each fold, the orientation of the paper is reversed. Thus, if the first crease in the flat-folded figure is placed in the plane parallel to the -axis, the next crease must be rotated from it by an angle of , the crease after that by an angle of (because the second angle has the reverse orientation from the first), etc. In order for the paper to meet back up with itself at the final angle, Kawasaki's condition must be met.\n\nShowing that the condition is also a sufficient condition is a matter of describing how to fold a given crease pattern so that it folds flat. That is, one must show how to choose whether to make mountain or valley folds, and in what order the flaps of paper should be arranged on top of each other. One way to do this is to choose a number such that the partial alternating sum\nis as small as possible. Either and the partial sum is an empty sum that is also zero, or for some nonzero choice of the partial sum is negative. Then, accordion fold the pattern, starting with angle and alternating between mountain and valley folds, placing each angular wedge of the paper below the previous folds. At each step until the final fold, an accordion fold of this type will never self-intersect. The choice of ensures that the first wedge sticks out to the left of all the other folded pieces of paper, allowing the final wedge to connect back up to it.\n\nAn alternative proof of sufficiency can be used to show that there are many different flat foldings. Consider the smallest angle and the two creases on either side of it. Mountain-fold one of these two creases and valley-fold the other, choosing arbitrarily which fold to use for which crease. Then, glue the resulting flap of paper onto the remaining part of the crease pattern. The result of this gluing will be a crease pattern with two fewer creases, on a conical sheet of paper, that still satisfies Kawasaki's condition. Therefore, by mathematical induction, repeating this process will eventually lead to a flat folding. The base case of the induction is a cone with only two creases and two equal-angle wedges, which can obviously be flat-folded by using a mountain fold for both creases. There are two ways to choose which folds to use in each step of this method, and each step eliminates two creases. Therefore, any crease pattern with creases that satisfies Kawasaki's condition has at least different choices of mountain and valley folds that all lead to valid flat foldings.\n\nIn the late 1970s, Kôdi Husimi and David A. Huffman independently observed that flat-folded figures with four creases have opposite angles adding to , a special case of Kawasaki's theorem. Huffman included the result in a 1976 paper on curved creases, and\nHusimi published the four-crease theorem in a book on origami geometry with his wife Mitsue Husimi.\nThe same result was published even earlier, in a pair of papers from 1966 by S. Murata that also included the six-crease case and the general case of Maekawa's theorem.\n\nThe fact that crease patterns with arbitrarily many creases necessarily have alternating sums of angles adding to was discovered by Kawasaki, by Stuart Robertson, and by Jacques Justin (again, independently of each other) in the late 1970s and early 1980s.\nBecause of Justin's contribution to the problem, Kawasaki's theorem has also been called the Kawasaki–Justin theorem.\nThe fact that this condition is sufficient—that is, that crease patterns with evenly many angles, alternatingly summing to can always be flat-folded—may have been first stated by .\n\nKawasaki himself has called the result Husimi's theorem, after Kôdi Husimi, and some other authors have followed this terminology as well. The name \"Kawasaki's theorem\" was first given to this result in \"Origami for the Connoisseur\" by Kunihiko Kasahara and Toshie Takahama (Japan Publications, 1987).\n\nAlthough Kawasaki's theorem completely describes the folding patterns that have flat-folded states, it does not describe the folding process needed to reach that state. For some folding patterns, it may be necessary to curve or bend the paper while transforming it from a flat sheet to its flat-folded state, rather than keeping the rest of the paper flat and only changing the dihedral angles at each fold. For rigid origami (a type of folding that keeps the surface flat except at its folds, suitable for hinged panels of rigid material rather than flexible paper), additional conditions are needed on a folding pattern to allow it to move from an unfolded state to a flat-folded state.\n"}
{"id": "1978904", "url": "https://en.wikipedia.org/wiki?curid=1978904", "title": "Latin letters used in mathematics", "text": "Latin letters used in mathematics\n\nMany letters of the Latin alphabet, both capital and small, are used in mathematics, science and engineering to denote by convention specific or abstracted constants, variables of a certain type, units, multipliers, physical entities. Certain letters, when combined with special formatting, take on special meaning.\n\nBelow is an alphabetical list of the letters of the alphabet with some of their uses. The field in which the convention applies is mathematics unless otherwise noted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "57193995", "url": "https://en.wikipedia.org/wiki?curid=57193995", "title": "Legendre moment", "text": "Legendre moment\n\nIn mathematics, Legendre moments are a type of image moment and are achieved by using the Legendre polynomial. Legendre moments are used in areas of image processing including: pattern and object recognition, image indexing, line fitting, feature extraction, edge detection, and texture analysis. Legendre moments have been studied as a means to reduce image moment calculation complexity by limiting the amount of information redundancy through approximation.\n\nWith order of \"m\" + \"n\", and object intensity function \"f\"(\"x\",\"y\"):\n\nwhere \"m\",\"n\" = 1, 2, 3, ... with the \"n\"th-order Legendre polynomials being:\n\nwhich can also be written:\n\nwhere \"D\"(\"n\") = floor(\"n\"/2). The set of Legendre polynomials {\"P\"(\"x\")} form an orthogonal set on the interval [−1,1]: \n\nA recurrence relation can be used to compute the Legendre polynomial:\n\n\"f\"(\"x\",\"y\") can be written as an infinite series expansion in terms of Legendre polynomials [−1 ≤ \"x\",\"y\" ≤ 1.]:\n"}
{"id": "32869192", "url": "https://en.wikipedia.org/wiki?curid=32869192", "title": "List of dualities", "text": "List of dualities\n\nIn mathematics, a duality, generally speaking, translates concepts, theorems or mathematical structures into other concepts, theorems or structures, in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of \"A\" is \"B\", then the dual of \"B\" is \"A\".\n\n\n\n\n"}
{"id": "490054", "url": "https://en.wikipedia.org/wiki?curid=490054", "title": "List of exponential topics", "text": "List of exponential topics\n\nThis is a list of exponential topics, by Wikipedia page. See also list of logarithm topics.\n"}
{"id": "33900798", "url": "https://en.wikipedia.org/wiki?curid=33900798", "title": "List of formulas in elementary geometry", "text": "List of formulas in elementary geometry\n\nThis is a short list of some common mathematical shapes and figures and the formulas that describe them.\n"}
{"id": "5971821", "url": "https://en.wikipedia.org/wiki?curid=5971821", "title": "List of mathematicians (O)", "text": "List of mathematicians (O)\n\n\n\n\n"}
{"id": "5971830", "url": "https://en.wikipedia.org/wiki?curid=5971830", "title": "List of mathematicians (S)", "text": "List of mathematicians (S)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "659169", "url": "https://en.wikipedia.org/wiki?curid=659169", "title": "List of topics related to π", "text": "List of topics related to π\n\nThis is a list of topics related to pi (), the fundamental mathematical constant.\n"}
{"id": "26149716", "url": "https://en.wikipedia.org/wiki?curid=26149716", "title": "Madhava series", "text": "Madhava series\n\nIn mathematics, a Leibniz or Madhava series is any one of the series in a collection of infinite series expressions all of which are believed to have been discovered by Madhava of Sangamagrama (c. 1350 – c. 1425), the founder of the Kerala school of astronomy and mathematics and later by Gottfried Wilhelm Leibniz, among others. These expressions are the Maclaurin series expansions of the trigonometric sine, cosine and arctangent functions, and the special case of the power series expansion of the arctangent function yielding a formula for computing π. The power series expansions of sine and cosine functions are respectively called \"Madhava's sine series\" and \"Madhava's cosine series\". The power series expansion of the arctangent function is sometimes called \"Madhava–Gregory series\" or \"Gregory–Madhava series\". These power series are also collectively called \"Taylor–Madhava series\". The formula for π is referred to as \"Madhava–Newton series\" or \"Madhava–Leibniz series\" or Leibniz formula for pi or Leibnitz–Gregory–Madhava series. These further names for the various series are reflective of the names of the Western discoverers or popularizers of the respective series.\n\nThe derivations use many calculus related concepts such as summation, rate of change, and interpolation, which suggests that Indian mathematicians had a solid understanding of the basics of calculus long before it was developed in Europe. There is no evidence, however, that any of his ideas went out of Kerala. The calculus was still in its primitive stages and it would remain so until Newton and Leibniz entered the scene. Even though they had very basic ideas on the infinite series, Indian Mathematicians were not able to convert calculus to the modern problem solving tool that it is today.\nNo surviving works of Madhava contain explicit statements regarding the expressions which are now referred to as Madhava series. However, in the writing of later members of the Kerala school of astronomy and mathematics like Nilakantha Somayaji and Jyeshthadeva one can find unambiguous attributions of these series to Madhava. It is also in the works of these later astronomers and mathematicians one can trace the Indian proofs of these series expansions. These proofs provide enough indications about the approach Madhava had adopted to arrive at his series expansions.\n\nUnlike most previous cultures, which had been rather nervous about the concept of infinity, Madhava was more than happy to play around with infinity, particularly infinite series. He showed how, although one can be approximated by adding a half plus a quarter plus an eighth plus a sixteenth, etc., (as even the ancient Egyptians and Greeks had known), the exact total of one can only be achieved by adding up infinitely many fractions. But Madhava went further and linked the idea of an infinite series with geometry and trigonometry. He realized that, by successively adding and subtracting different odd number fractions to infinity, he could home in on an exact formula for π (this was two centuries before Leibniz was to come to the same conclusion in Europe).\n\nIn the writings of the mathematicians and astronomers of the Kerala school, Madhava's series are described couched in the terminology and concepts fashionable at that time. When we translate these ideas into the notations and concepts of modern-day mathematics, we obtain the current equivalents of Madhava's series. These present-day counterparts of the infinite series expressions discovered by Madhava are the following:\n\nNone of Madhava's works, containing any of the series expressions attributed to him, have survived. These series expressions are found in the writings of the followers of Madhava in the Kerala school. At many places these authors have clearly stated that these are \"as told by Madhava\". Thus the enunciations of the various series found in Tantrasamgraha and its commentaries can be safely assumed to be in \"Madhava's own words\". The translations of the relevant verses as given in the \"Yuktidipika\" commentary of Tantrasamgraha (also known as \"Tantrasamgraha-vyakhya\") by Sankara Variar (circa. 1500 - 1560 CE) are reproduced below. These are then rendered in current mathematical notations.\n\nMadhava's sine series is stated in verses 2.440 and 2.441 in \"Yukti-dipika\" commentary (\"Tantrasamgraha-vyakhya\") by Sankara Variar. A translation of the verses follows.\n\n\"Multiply the arc by the square of the arc, and take the result of repeating that (any number of times). Divide (each of the above numerators) by the squares of the successive even numbers increased by that number and multiplied by the square of the radius. Place the arc and the successive results so obtained one below the other, and subtract each from the one above. These together give the jiva, as collected together in the verse beginning with \"vidvan\" etc. \"\n\nLet \"r\" denote the radius of the circle and \"s\" the arc-length.\n\n\nLet θ be the angle subtended by the arc \"s\" at the centre of the circle. Then \"s\" = \"r θ\" and \"jiva\" = \"r\" sin \"θ\". Substituting these in the last expression and simplifying we get\nwhich is the infinite power series expansion of the sine function.\n\nThe last line in the verse ′\"as collected together in the verse beginning with \"vidvan\" etc.\"′ is a reference to a reformulation of the series introduced by Madhava himself to make it convenient for easy computations for specified values of the arc and the radius.\nFor such a reformulation, Madhava considers a circle one-quarter of which measures 5400 minutes (say \"C\" minutes) and develops a scheme for the easy computations of the \"jiva\"′s of the various arcs of such a circle. Let \"R\" be the radius of a circle one-quarter of which measures C.\nMadhava had already computed the value of π using his series formula for π. Using this value of π, namely 3.1415926535922, the radius \"R\" is computed as follows:\nThen\n\nMadhava's expression for \"jiva\" corresponding to any arc \"s\" of a circle of radius \"R\" is equivalent to the following:\n\nMadhava now computes the following values:\n\nThe \"jiva\" can now be computed using the following scheme:\n\nThis gives an approximation of \"jiva\" by its Taylor polynomial of the 11'th order. It involves one division, six multiplications and five subtractions only. Madhava prescribes this numerically efficient computational scheme in the following words (translation of verse 2.437 in \"Yukti-dipika\"):\n\n\"vi-dvān, tu-nna-ba-la, ka-vī-śa-ni-ca-ya, sa-rvā-rtha-śī-la-sthi-ro, ni-rvi-ddhā-nga-na-rē-ndra-rung . Successively multiply these five numbers in order by the square of the arc divided by the quarter of the circumference (5400′), and subtract from the next number. (Continue this process with the result so obtained and the next number.) Multiply the final result by the cube of the arc divided by quarter of the circumference and subtract from the arc.\"\n\nMadhava's cosine series is stated in verses 2.442 and 2.443 in \"Yukti-dipika\" commentary (\"Tantrasamgraha-vyakhya\") by Sankara Variar. A translation of the verses follows.\n\n\"Multiply the square of the arc by the unit (i.e. the radius) and take the result of repeating that (any number of times). Divide (each of the above numerators) by the square of the successive even numbers decreased by that number and multiplied by the square of the radius. But the first term is (now)(the one which is) divided by twice the radius. Place the successive results so obtained one below the other and subtract each from the one above. These together give the śara as collected together in the verse beginning with stena, stri, etc. \"\n\nLet \"r\" denote the radius of the circle and \"s\" the arc-length.\n\n\nLet \"θ\" be the angle subtended by the arc \"s\" at the centre of the circle. Then \"s\" = \"rθ\" and \"śara\" = \"r\"(1 − cos \"θ\"). Substituting these in the last expression and simplifying we get\nwhich gives the infinite power series expansion of the cosine function.\n\nThe last line in the verse ′\"as collected together in the verse beginning with stena, stri, etc.\"′ is a reference to a reformulation introduced by Madhava himself to make the series convenient for easy computations for specified values of the arc and the radius.\nAs in the case of the sine series, Madhava considers a circle one quarter of which measures 5400 minutes (say \"C\" minutes) and develops a scheme for the easy computations of the \"śara\"′s of the various arcs of such a circle. Let \"R\" be the radius of a circle one quarter of which measures C. Then, as in the case of the sine series, Madhava gets\n\"R\" = 3437′ 44′′ 48′′′.\n\nMadhava's expression for \"śara\" corresponding to any arc \"s\" of a circle of radius \"R\" is equivalent to the following:\n\nMadhava now computes the following values:\n\nThe \"śara\" can now be computed using the following scheme:\n\nThis gives an approximation of \"śara\" by its Taylor polynomial of the 12'th order. This also involves one division, six multiplications and five subtractions only. Madhava prescribes this numerically efficient computational scheme in the following words (translation of verse 2.438 in \"Yukti-dipika\"):\n\n\"The six stena, strīpiśuna, sugandhinaganud, bhadrāngabhavyāsana, mīnāngonarasimha, unadhanakrtbhureva. Multiply by the square of the arc divided by the quarter of the circumference and subtract from the next number. (Continue with the result and the next number.) Final result will be utkrama-jya (R versed sign).\"\n\nMadhava's arctangent series is stated in verses 2.206 – 2.209 in \"Yukti-dipika\" commentary (\"Tantrasamgraha-vyakhya\") by Sankara Variar. A translation of the verses is given below.\nJyesthadeva has also given a description of this series in Yuktibhasa.\n\n\"Now, by just the same argument, the determination of the arc of a desired sine can be (made). That is as follows: The first result is the product of the desired sine and the radius divided by the cosine of the arc. When one has made the square of the sine the multiplier and the square of the cosine the divisor, now a group of results is to be determined from the (previous) results beginning from the first. When these are divided in order by the odd numbers 1, 3, and so forth, and when one has subtracted the sum of the even(-numbered) results from the sum of the odd (ones), that should be the arc. Here the smaller of the sine and cosine is required to be considered as the desired (sine). Otherwise, there would be no termination of results even if repeatedly (computed).\"\n\n\"By means of the same argument, the circumference can be computed in another way too. That is as (follows): The first result should by the square root of the square of the diameter multiplied by twelve. From then on, the result should be divided by three (in) each successive (case). When these are divided in order by the odd numbers, beginning with 1, and when one has subtracted the (even) results from the sum of the odd, (that) should be the circumference.\"\n\nLet \"s\" be the arc of the desired sine (\"jya\" or \"jiva\") \"y\". Let \"r\" be the radius and \"x\" be the cosine (\"kotijya\").\n\n\nLet θ be the angle subtended by the arc \"s\" at the centre of the circle. Then \"s\" = \"r\"θ, \"x\" = \"kotijya\" = \"r\" cos θ and \"y\" = \"jya\" = \"r\" sin θ.\nThen \"y\" / \"x\" = tan θ. Substituting these in the last expression and simplifying we get\nLetting tan θ = \"q\" we finally have\n\n\nThe second part of the quoted text specifies another formula for the computation of the circumference \"c\" of a circle having diameter \"d\". This is as follows.\n\nSince \"c\" = π \"d\" this can be reformulated as a formula to compute π as follows.\n\nThis is obtained by substituting \"q\" = formula_22 (therefore \"θ\" = π / 6) in the power series expansion for tan \"q\" above.\n\n\n"}
{"id": "19266946", "url": "https://en.wikipedia.org/wiki?curid=19266946", "title": "Mathematical diagram", "text": "Mathematical diagram\n\nMathematical diagrams are diagrams in the field of mathematics, and diagrams using mathematics such as charts and graphs, that are mainly designed to convey mathematical relationships, for example, comparisons over time.\n\nA complex number can be visually represented as a pair of numbers forming a vector on a diagram called an Argand diagram\nThe complex plane is sometimes called the \"Argand plane\" because it is used in \"Argand diagrams\". These are named after Jean-Robert Argand (1768–1822), although they were first described by Norwegian-Danish land surveyor and mathematician Caspar Wessel (1745–1818). Argand diagrams are frequently used to plot the positions of the poles and zeroes of a function in the complex plane.\n\nThe concept of the complex plane allows a geometric interpretation of complex numbers. Under addition, they add like vectors. The multiplication of two complex numbers can be expressed most easily in polar coordinates — the magnitude or \"modulus\" of the product is the product of the two absolute values, or moduli, and the angle or \"argument\" of the product is the sum of the two angles, or arguments. In particular, multiplication by a complex number of modulus 1 acts as a rotation.\n\nIn the context of fast Fourier transform algorithms, a butterfly is a portion of the computation that combines the results of smaller discrete Fourier transforms (DFTs) into a larger DFT, or vice versa (breaking a larger DFT up into subtransforms). The name \"butterfly\" comes from the shape of the data-flow diagram in the radix-2 case, as described below. The same structure can also be found in the Viterbi algorithm, used for finding the most likely sequence of hidden states.\n\nThe butterfly diagram show a data-flow diagram connecting the inputs \"x\" (left) to the outputs \"y\" that depend on them (right) for a \"butterfly\" step of a radix-2 Cooley–Tukey FFT algorithm. This diagram resembles a butterfly as in the morpho butterfly shown for comparison), hence the name.\n\nIn mathematics, and especially in category theory, a commutative diagram is a diagram of objects, also known as vertices, and morphisms, also known as arrows or edges, such that when selecting two objects any directed path through the diagram leads to the same result by composition.\n\nCommutative diagrams play the role in category theory that equations play in algebra.\n\nA Hasse diagram is a simple picture of a finite partially ordered set, forming a drawing of the partial order's transitive reduction. Concretely, one represents each element of the set as a vertex on the page and draws a line segment or curve that goes upward from \"x\" to \"y\" precisely when \"x\" < \"y\" and there is no \"z\" such that \"x\" < \"z\" < \"y\". In this case, we say y covers x, or y is an immediate successor of x. In a Hasse diagram, it is required that the curves be drawn so that each meets exactly two vertices: its two endpoints. Any such diagram (given that the vertices are labeled) uniquely determines a partial order, and any partial order has a unique transitive reduction, but there are many possible placements of elements in the plane, resulting in different Hasse diagrams for a given order that may have widely varying appearances.\n\nIn Knot theory a useful way to visualise and manipulate knots is to project the knot onto a plane—;think of the knot casting a shadow on the wall. A small perturbation in the choice of projection will ensure that it is one-to-one except at the double points, called \"crossings\", where the \"shadow\" of the knot crosses itself once transversely\n\nAt each crossing we must indicate which section is \"over\" and which is \"under\", so as to be able to recreate the original knot. This is often done by creating a break in the strand going underneath. If by following the diagram the knot alternately crosses itself \"over\" and \"under\", then the diagram represents a particularly well-studied class of knot, alternating knots.\n\nA Venn diagram is a representation of mathematical sets: a mathematical diagram representing sets as circles, with their relationships to each other expressed through their overlapping positions, so that all possible relationships between the sets are shown.\n\nThe Venn diagram is constructed with a collection of simple closed curves drawn in the plane. The principle of these diagrams is that classes be represented by regions in such relation to one another that all the possible logical relations of these classes can be indicated in the same diagram. That is, the diagram initially leaves room for any possible relation of the classes, and the actual or given relation, can then be specified by indicating that some particular region is null or is notnull.\n\nA Voronoi diagram is a special kind of decomposition of a metric space determined by distances to a specified discrete set of objects in the space, e.g., by a discrete set of points. This diagram is named after Georgy Voronoi, also called a Voronoi tessellation, a Voronoi decomposition, or a Dirichlet tessellation after Peter Gustav Lejeune Dirichlet.\n\nIn the simplest case, we are given a set of points S in the plane, which are the Voronoi sites. Each site s has a Voronoi cell V(s) consisting of all points closer to s than to any other site. The segments of the Voronoi diagram are all the points in the plane that are equidistant to two sites. The Voronoi nodes are the points equidistant to three (or more) sites\n\nA wallpaper group or \"plane symmetry group\" or \"plane crystallographic group\" is a mathematical classification of a two-dimensional repetitive pattern, based on the symmetries in the pattern. Such patterns occur frequently in architecture and decorative art. There are 17 possible distinct groups.\n\nWallpaper groups are two-dimensional symmetry groups, intermediate in complexity between the simpler frieze groups and the three-dimensional crystallographic groups, also called space groups. Wallpaper groups categorize patterns by their symmetries. Subtle differences may place similar patterns in different groups, while patterns which are very different in style, color, scale or orientation may belong to the same group.\n\nA \"Young diagram\" or Young tableau, also called Ferrers diagram, is a finite collection of boxes, or cells, arranged in left-justified rows, with the row sizes weakly decreasing (each row has the same or shorter length than its predecessor). \n\nListing the number of boxes in each row gives a partition formula_1 of a positive integer \"n\", the total number of boxes of the diagram. The Young diagram is said to be of shape formula_1, and it carries the same information as that partition. Listing the number of boxes in each column gives another partition, the conjugate or \"transpose\" partition of formula_1; one obtains a Young diagram of that shape by reflecting the original diagram along its main diagonal.\n\nYoung tableaux were introduced by Alfred Young, a mathematician at Cambridge University, in 1900. They were then applied to the study of symmetric group by Georg Frobenius in 1903. Their theory was further developed by many mathematicians.\n\n\n\n"}
{"id": "432897", "url": "https://en.wikipedia.org/wiki?curid=432897", "title": "Mechanical calculator", "text": "Mechanical calculator\n\nA mechanical calculator, or calculating machine, is a mechanical device used to perform automatically the basic operations of arithmetic. Most mechanical calculators were comparable in size to small desktop computers and have been rendered obsolete by the advent of the electronic calculator.\n\nSurviving notes from Wilhelm Schickard in 1623 reveal that he designed and had built the earliest of the modern attempts at mechanizing calculation. His machine was composed of two sets of technologies: first an abacus made of Napier's bones, to simplify multiplications and divisions first described six years earlier in 1617, and for the mechanical part, it had a dialed pedometer to perform additions and subtractions. A study of the surviving notes shows a machine that would have jammed after a few entries on the same dial, and that it could be damaged if a carry had to be propagated over a few digits (like adding 1 to 999). Schickard abandoned his project in 1624 and never mentioned it again until his death 11 years later in 1635.\n\nTwo decades after Schickard's supposedly failed attempt, in 1642, Blaise Pascal decisively solved these particular problems with his invention of the mechanical calculator. Co-opted into his father's labour as tax collector in Rouen, Pascal designed the calculator to help in the large amount of tedious arithmetic required; it was called Pascal's Calculator or Pascaline.\n\nThomas' arithmometer, the first commercially successful machine, was manufactured two hundred years later in 1851; it was the first mechanical calculator strong enough and reliable enough to be used daily in an office environment. For forty years the arithmometer was the only type of mechanical calculator available for sale.\n\nThe comptometer, introduced in 1887, was the first machine to use a keyboard which consisted of columns of nine keys (from 1 to 9) for each digit. The Dalton adding machine, manufactured from 1902, was the first to have a 10 key keyboard. Electric motors were used on some mechanical calculators from 1901. In 1961, a comptometer type machine, the Anita mk7 from Sumlock comptometer Ltd., became the first desktop mechanical calculator to receive an all electronic calculator engine, creating the link in between these two industries and marking the beginning of its decline. The production of mechanical calculators came to a stop in the middle of the 1970s closing an industry that had lasted for 120 years.\n\nCharles Babbage designed two new kinds of mechanical calculators, which were so big that they required the power of a steam engine to operate, and that were too sophisticated to be built in his lifetime. The first one was an \"automatic\" mechanical calculator, his difference engine, which could automatically compute and print mathematical tables. In 1855, Georg Scheutz became the first of a handful of designers to succeed at building a smaller and simpler model of his difference engine. The second one was a \"programmable\" mechanical calculator, his analytical engine, which Babbage started to design in 1834; \"in less than two years he had sketched out many of the salient features of the modern computer. A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1937, Howard Aiken convinced IBM to design and build the ASCC/Mark I, the first machine of its kind, based on the architecture of the analytical engine; when the machine was finished some hailed it as \"Babbage's dream come true\".\n\nA short list of other precursors to the mechanical calculator must include a group of mechanical analog computers which, once set, are only modified by the continuous and repeated action of their actuators (crank handle, weight, wheel, water...). Before common era, there are odometers and the Antikythera mechanism, an out of place, unique, geared astronomical clock, followed more than a millennium later by early mechanical clocks, geared astrolabes and followed in the 15th century by pedometers. These machines were all made of toothed gears linked by some sort of carry mechanisms. These machines always produce identical results for identical initial settings unlike a mechanical calculator where all the wheels are independent but are also linked together by the rules of arithmetic.\n\nThe 17th century marked the beginning of the history of mechanical calculators, as it saw the invention of its first machines, including Pascal's calculator, in 1642. Blaise Pascal had invented a machine which he presented as being able to perform computations that were previously thought to be only humanly possible, but he wasn't successful in creating an industry.\n\nThe 17th century also saw the invention of some very powerful tools to aid arithmetic calculations like Napier's bones, logarithmic tables and the slide rule which, for their ease of use by scientists in multiplying and dividing, ruled over and impeded the use and development of mechanical calculators until the production release of the arithmometer in the mid 19th century.\nBlaise Pascal invented a mechanical calculator with a sophisticated carry mechanism in 1642. After three years of effort and 50 prototypes he introduced his calculator to the public. He built twenty of these machines in the following ten years. This machine could add and subtract two numbers directly and multiply and divide by repetition. Since, unlike Schickard's machine, the Pascaline dials could only rotate in one direction zeroing it after each calculation required the operator to dial in all 9s and then (method of) propagate a carry right through the machine. This suggests that the carry mechanism would have proved itself in practice many times over. This is a testament to the quality of the Pascaline because none of the 17th and 18th century criticisms of the machine mentioned a problem with the carry mechanism and yet it was fully tested on all the machines, by their resets, all the time.\n\nIn 1672, Gottfried Leibniz started working on adding direct multiplication to what he understood was the working of Pascal's calculator. However, it is doubtful that he had ever fully seen the mechanism and the method could not have worked because of the lack of reversible rotation in the mechanism. Accordingly, he eventually designed an entirely new machine called the Stepped Reckoner; it used his Leibniz wheels, was the first two-motion calculator, the first to use cursors (creating a memory of the first operand) and the first to have a movable carriage. Leibniz built two Stepped Reckoners, one in 1694 and one in 1706. Only the machine built in 1694 is known to exist, it was rediscovered at the end of the 19th century having been forgotten in an attic in the University of Göttingen.\n\nLeibniz had invented his namesake wheel and the principle of a two motion calculator, but after forty years of development he wasn't able to produce a machine that was fully operational; this makes Pascal's calculator the only working mechanical calculator in the 17th century. Leibniz was also the first person to describe a pinwheel calculator. He once said \"It is unworthy of excellent men to lose hours like slaves in the labour of calculation which could safely be relegated to anyone else if machines were used.\"\n\nSchickard, Pascal and Leibniz were inevitably inspired by the role of clockwork which was highly celebrated in the seventeenth century. However, simple minded application of interlinked gears was insufficient for any of their purposes. Schickard introduced the use of a single toothed \"mutilated gear\" to enable the carry to take place. Pascal improved on that with his famous weighted sautoir. Leibniz went even further in relation to the ability to use a moveable carriage to perform multiplication more efficiently, albeit at the expense of a fully working carry mechanism.\n\nThe principle of the clock (input wheels and display wheels added to a clock like mechanism) for a direct entry calculating machine couldn't be implemented to create a fully effective calculating machine without additional innovation with the technological capabilities of the 17th century. because their gears would jam when a carry had to be moved several places along the accumulator. The only 17th century calculating clocks that have survived to this day do not have a machine wide carry mechanism and therefore cannot be called fully effective mechanical calculators. A much more successful calculating clock was built by the Italian Giovanni Poleni in the 18th century and was a two-motion calculating clock (the numbers are inscribed first and then they are processed).\n\n\nThe 18th century saw the first mechanical calculator that could perform a multiplication automatically; designed and built by Giovanni Poleni in 1709 and made of wood, it was the first successful calculating clock. For all the machines built in this century, division still required the operator to decide when to stop a repeated subtraction at each index, and therefore these machines were only providing a help in dividing, like an abacus. Both pinwheel calculators and Leibniz wheel calculators were built with a few unsuccessful attempts at their commercialization.\n\n\nLuigi Torchi invented the first direct multiplication machine in 1834. This was also the second key-driven machine in the world, following that of James White (1822).\n\nThe mechanical calculator industry started in 1851 Thomas de Colmar released his simplified Arithmomètre which was the first machine that could be used daily in an office environment.\n\nFor 40 years, the arithmometer was the only mechanical calculator available for sale and was sold all over the world. By then, in 1890, about 2,500 arithmometers had been sold plus a few hundreds more from two licensed arithmometer clone makers (Burkhardt, Germany, 1878 and Layton, UK, 1883). Felt and Tarrant, the only other competitor in true commercial production, had sold 100 comptometers in three years.\n\nThe 19th century also saw the designs of Charles Babbage calculating machines, first with his difference engine, started in 1822, which was the first automatic calculator since it continuously used the results of the previous operation for the next one, and second with his analytical engine, which was the first programmable calculator, using Jacquard's cards to read program and data, that he started in 1834, and which gave the blueprint of the mainframe computers built in the middle of the 20th century.\n\n\n\n\nThe cash register, invented by the American saloonkeeper James Ritty in 1879, addressed the old problems of disorganization and dishonesty in business transactions. It was a pure adding machine coupled with a printer, a bell and a two-sided display that showed the paying party and the store owner, if he wanted to, the amount of money exchanged for the current transaction.\n\nThe cash register was easy to use and, unlike genuine mechanical calculators, was needed and quickly adopted by a great number of businesses. \"Eighty four companies sold cash registers between 1888 and 1895, only three survived for any length of time\".\n\nIn 1890, 6 years after John Patterson started NCR Corporation, 20,000 machines had been sold by his company alone against a total of roughly 3,500 for all genuine calculators combined.\n\nBy 1900, NCR had built 200,000 cash registers and there were more companies manufacturing them, compared to the \"Thomas/Payen\" arithmometer company that had just sold around 3,300 and Burroughs had only sold 1,400 machines.\n\n\nTwo different classes of mechanisms had become established by this time, reciprocating and rotary. The former type of mechanism was operated typically by a limited-travel hand crank; some internal detailed operations took place on the pull, and others on the release part of a complete cycle. The illustrated 1914 machine is this type; the crank is vertical, on its right side. Later on, some of these mechanisms were operated by electric motors and reduction gearing that operated a crank and connecting rod to convert rotary motion to reciprocating.\n\nThe latter, type, rotary, had at least one main shaft that made one [or more] continuous revolution[s], one addition or subtraction per turn. Numerous designs, notably European calculators, had handcranks, and locks to ensure that the cranks were returned to exact positions once a turn was complete.\nThe first half of the 20th century saw the gradual development of the mechanical calculator mechanism.\n\nThe Dalton adding-listing machine introduced in 1902 was the first of its type to use only ten keys, and became the first of many different models of \"10-key add-listers\" manufactured by many companies.\n\nIn 1948 the cylindrical Curta calculator, which was compact enough to be held in one hand, was introduced after being developed by Curt Herzstark in 1938. This was an extreme development of the stepped-gear calculating mechanism. It subtracted by adding complements; between the teeth for addition were teeth for subtraction.\n\nFrom the early 1900s through the 1960s, mechanical calculators dominated the desktop computing market. Major suppliers in the USA included Friden, Monroe, and SCM/Marchant. These devices were motor-driven, and had movable carriages where results of calculations were displayed by dials. Nearly all keyboards were \"full\" — each digit that could be entered had its own column of nine keys, 1..9, plus a column-clear key, permitting entry of several digits at once. (See the illustration below of a Marchant Figurematic.) One could call this parallel entry, by way of contrast with ten-key serial entry that was commonplace in mechanical adding machines, and is now universal in electronic calculators. (Nearly all Friden calculators, as well as some rotary (German) Diehls had a ten-key auxiliary keyboard for entering the multiplier when doing multiplication.) Full keyboards generally had ten columns, although some lower-cost machines had eight. Most machines made by the three companies mentioned did not print their results, although other companies, such as Olivetti, did make printing calculators.\n\nIn these machines, addition and subtraction were performed in a single operation, as on a conventional adding machine, but multiplication and division were accomplished by repeated mechanical additions and subtractions. Friden made a calculator that also provided square roots, basically by doing division, but with added mechanism that automatically incremented the number in the keyboard in a systematic fashion. The last of the mechanical calculators were likely to have short-cut multiplication, and some ten-key, serial-entry types had decimal-point keys. However, decimal-point keys required significant internal added complexity, and were offered only in the last designs to be made. Handheld mechanical calculators such as the 1948 Curta continued to be used until they were displaced by electronic calculators in the 1970s.\n\nTypical European four-operation machines use the Odhner mechanism, or variations of it. This kind of machine included the \"Original Odhner\", Brunsviga and several following imitators, starting from Triumphator, Thales, Walther, Facit up to Toshiba. Although most of these were operated by handcranks, there were motor-driven versions. Hamann calculators externally resembled pinwheel machines, but the setting lever positioned a cam that disengaged a drive pawl when the dial had moved far enough.\n\nAlthough Dalton introduced in 1902 first ten-key printing \"adding\" (two operations, the other being subtraction) machine, these feature were not present in \"computing\" (four operations) machines for many decades. Facit-T (1932) was the first 10-key computing machine sold in large numbers. Olivetti Divisumma-14 (1948) was the first computing machine with both printer and a 10-key keyboard.\n\nFull-keyboard machines, including motor-driven ones, were also built until the 1960s. Among the major manufacturers were Mercedes-Euklid, Archimedes, and MADAS in Europe; in the USA, Friden, Marchant, and Monroe were the principal makers of rotary calculators with carriages. Reciprocating calculators (most of which were adding machines, many with integral printers) were made by Remington Rand and Burroughs, among others. All of these were key-set. Felt & Tarrant made Comptometers, as well as Victor, which were key-driven.\n\nThe basic mechanism of the Friden and Monroe was a modified Leibniz wheel (better known, perhaps informally, in the USA as a \"stepped drum\" or \"stepped reckoner\"). The Friden had an elementary reversing drive between the body of the machine and the accumulator dials, so its main shaft always rotated in the same direction. The Swiss MADAS was similar. The Monroe, however, reversed direction of its main shaft to subtract.\n\nThe earliest Marchants were pinwheel machines, but most of them were remarkably-sophisticated rotary types. They ran at 1,300 addition cycles per minute if the [+] bar is held down. Others were limited to 600 cycles per minute, because their accumulator dials started and stopped for every cycle; Marchant dials moved at a steady and proportional speed for continuing cycles. Most Marchants had a row of nine keys on the extreme right, as shown in the photo of the Figurematic. These simply made the machine add for the number of cycles corresponding to the number on the key, and then shifted the carriage one place. Even nine add cycles took only a short time.\n\nIn a Marchant, near the beginning of a cycle, the accumulator dials moved downward \"into the dip\", away from the openings in the cover. They engaged drive gears in the body of the machine, which rotated them at speeds proportional to the digit being fed to them, with added movement (reduced 10:1) from carries created by dials to their right. At the completion of the cycle, the dials would be misaligned like the pointers in a traditional watt-hour meter. However, as they came up out of the dip, a constant-lead disc cam realigned them by way of a (limited-travel) spur-gear differential. As well, carries for lower orders were added in by another, planetary differential. (The machine shown has 39 differentials in its (20-digit) accumulator!)\n\nIn any mechanical calculator, in effect, a gear, sector, or some similar device moves the accumulator by the number of gear teeth that corresponds to the digit being added or subtracted – three teeth changes the position by a count of three. The great majority of basic calculator mechanisms move the accumulator by starting, then moving at a constant speed, and stopping. In particular, stopping is critical, because to obtain fast operation, the accumulator needs to move quickly. Variants of Geneva drives typically block overshoot (which, of course, would create wrong results).\n\nHowever, two different basic mechanisms, the Mercedes-Euklid and the Marchant, move the dials at speeds corresponding to the digit being added or subtracted; a [1] moves the accumulator the slowest, and a [9], the fastest. In the Mercedes-Euklid, a long slotted lever, pivoted at one end, moves nine racks (\"straight gears\") endwise by distances proportional to their distance from the lever's pivot. Each rack has a drive pin that is moved by the slot. The rack for [1] is closest to the pivot, of course.\nFor each keyboard digit, a sliding selector gear, much like that in the Leibniz wheel, engages the rack that corresponds to the digit entered. Of course, the accumulator changes either on the forward or reverse stroke, but not both. This mechanism is notably simple and relatively easy to manufacture.\n\nThe Marchant, however, has, for every one of its ten columns of keys, a nine-ratio \"preselector transmission\" with its output spur gear at the top of the machine's body; that gear engages the accumulator gearing. When one tries to work out the numbers of teeth in such a transmission, a straightforward approach leads one to consider a mechanism like that in mechanical gasoline pump registers, used to indicate the total price. However, this mechanism is seriously bulky, and utterly impractical for a calculator; 90-tooth gears are likely to be found in the gas pump. Practical gears in the computing parts of a calculator cannot have 90 teeth. They would be either too big, or too delicate.\n\nGiven that nine ratios per column implies significant complexity, a Marchant contains a few hundred individual gears in all, many in its accumulator. Basically, the accumulator dial has to rotate 36 degrees (1/10 of a turn) for a [1], and 324 degrees (9/10 of a turn) for a [9], not allowing for incoming carries. At some point in the gearing, one tooth needs to pass for a [1], and nine teeth for a [9]. There is no way to develop the needed movement from a driveshaft that rotates one revolution per cycle with few gears having practical (relatively small) numbers of teeth.\n\nThe Marchant, therefore, has three driveshafts to feed the little transmissions. For one cycle, they rotate 1/2, 1/4, and 1/12 of a revolution. . The 1/2-turn shaft carries (for each column) gears with 12, 14, 16, and 18 teeth, corresponding to digits 6, 7, 8, and 9. The 1/4-turn shaft carries (also, each column) gears with 12, 16, and 20 teeth, for 3, 4, and 5. Digits [1] and [2] are handled by 12 and 24-tooth gears on the 1/12-revolution shaft. Practical design places the 12th-rev. shaft more distant, so the 1/4-turn shaft carries freely-rotating 24 and 12-tooth idler gears. For subtraction, the driveshafts reversed direction.\n\nIn the early part of the cycle, one of five pendants moves off-center to engage the appropriate drive gear for the selected digit.\n\nSome machines had as many as 20 columns in their full keyboards. The monster in this field was the \"Duodecillion\" made by Burroughs for exhibit purposes.\n\nFor sterling currency, £/s/d (and even farthings), there were variations of the basic mechanisms, in particular with different numbers of gear teeth and accumulator dial positions. To accommodate shillings and pence, extra columns were added for the tens digit[s], 10 and 20 for shillings, and 10 for pence. Of course, these functioned as radix-20 and radix-12 mechanisms.\n\nA variant of the Marchant, called the Binary-Octal Marchant, was a radix-8 (octal) machine. It was sold to check very early vacuum-tube (valve) binary computers for accuracy. (Back then, the mechanical calculator was much more reliable than a tube/valve computer.)\n\nAs well, there was a twin Marchant, comprising two pinwheel Marchants with a common drive crank and reversing gearbox. Twin machines were relatively rare, and apparently were used for surveying calculations. At least one triple machine was made.\n\nThe Facit calculator, and one similar to it, are basically pinwheel machines, but the array of pinwheels moves sidewise, instead of the carriage. The pinwheels are biquinary; digits 1 through 4 cause the corresponding number of sliding pins to extend from the surface; digits 5 through 9 also extend a five-tooth sector as well as the same pins for 6 through 9.\n\nThe keys operate cams that operate a swinging lever to first unlock the pin-positioning cam that is part of the pinwheel mechanism; further movement of the lever (by an amount determined by the key's cam) rotates the pin-positioning cam to extend the necessary number of pins.\n\nStylus-operated adders with circular slots for the stylus, and side-by -side wheels, as made by Sterling Plastics (USA), had an ingenious anti-overshoot mechanism to ensure accurate carries.\nMechanical calculators continued to be sold, though in rapidly decreasing numbers, into the early 1970s, with many of the manufacturers closing down or being taken over. Comptometer type calculators were often retained for much longer to be used for adding and listing duties, especially in accounting, since a trained and skilled operator could enter all the digits of a number in one movement of the hands on a Comptometer quicker than was possible serially with a 10-key electronic calculator. In fact, it was quicker to enter larger digits in two strokes using only the lower-numbered keys; for instance, a 9 would be entered as 4 followed by 5. Some key-driven calculators had keys for every column, but only 1 through 5; they were correspondingly compact. The spread of the computer rather than the simple electronic calculator put an end to the Comptometer. Also, by the end of the 1970s, the slide rule had become obsolete.\n\n\n\n"}
{"id": "645602", "url": "https://en.wikipedia.org/wiki?curid=645602", "title": "Minimax theorem", "text": "Minimax theorem\n\nA minimax theorem is a theorem providing conditions that guarantee that the max–min inequality is also an equality. \nThe first theorem in this sense is von Neumann's minimax theorem from 1928, which was considered the starting point of game theory. \nSince then, several generalizations and alternative versions of von Neumann's original theorem have appeared in the literature.\n\nThe minimax theorem was first proven and published in 1928 by John von Neumann, who is quoted as saying \"As far as I can see, there could be no theory of games … without that theorem … I thought there was nothing worth publishing until the Minimax Theorem was proved\".\n\nFormally, von Neumann's minimax theorem states:\n\nLet formula_1 and formula_2 be compact convex sets. If formula_3 is a continuous function that is convex-concave, i.e.\n\nThen we have that\n\n"}
{"id": "5643937", "url": "https://en.wikipedia.org/wiki?curid=5643937", "title": "Music and mathematics", "text": "Music and mathematics\n\nMusic theory has no axiomatic foundation in modern mathematics, yet the basis of musical sound can be described mathematically (in acoustics) and exhibits \"a remarkable array of number properties\". Elements of music such as its form, rhythm and metre, the pitches of its notes and the tempo of its pulse can be related to the measurement of time and frequency, offering ready analogies in geometry. \n\nThe attempt to structure and communicate new ways of composing and hearing music has led to musical applications of set theory, abstract algebra and number theory. Some composers have incorporated the golden ratio and Fibonacci numbers into their work.\n\nThough ancient Chinese, Indians, Egyptians and Mesopotamians are known to have studied the mathematical principles of sound, the Pythagoreans (in particular Philolaus and Archytas) of ancient Greece were the first researchers known to have investigated the expression of musical scales in terms of numerical ratios, particularly the ratios of small integers. Their central doctrine was that \"all nature consists of harmony arising out of numbers\".\n\nFrom the time of Plato, harmony was considered a fundamental branch of physics, now known as musical acoustics. Early Indian and Chinese theorists show similar approaches: all sought to show that the mathematical laws of harmonics and rhythms were fundamental not only to our understanding of the world but to human well-being. Confucius, like Pythagoras, regarded the small numbers 1,2,3,4 as the source of all perfection.\n\nWithout the boundaries of rhythmic structure – a fundamental equal and regular arrangement of pulse repetition, accent, phrase and duration – music would not be possible. Modern musical use of terms like meter and measure also reflects the historical importance of music, along with astronomy, in the development of counting, arithmetic and the exact measurement of time and periodicity that is fundamental to physics.\n\nThe elements of musical form often build strict proportions or hypermetric structures (powers of the numbers 2 and 3).\n\nMusical form is the plan by which a short piece of music is extended. The term \"plan\" is also used in architecture, to which musical form is often compared. Like the architect, the composer must take into account the function for which the work is intended and the means available, practicing economy and making use of repetition and order. The common types of form known as binary and ternary (\"twofold\" and \"threefold\") once again demonstrate the importance of small integral values to the intelligibility and appeal of music.\n\nA musical scale is a discrete set of pitches used in making or describing music. The most important scale in the Western tradition is the diatonic scale but many others have been used and proposed in various historical eras and parts of the world. Each pitch corresponds to a particular frequency, expressed in hertz (Hz), sometimes referred to as cycles per second (c.p.s.). A scale has an interval of repetition, normally the octave. The octave of any pitch refers to a frequency exactly twice that of the given pitch.\n\nSucceeding superoctaves are pitches found at frequencies four, eight, sixteen times, and so on, of the fundamental frequency. Pitches at frequencies of half, a quarter, an eighth and so on of the fundamental are called suboctaves. There is no case in musical harmony where, if a given pitch be considered accordant, that its octaves are considered otherwise. Therefore, any note and its octaves will generally be found similarly named in musical systems (e.g. all will be called doh or A or Sa, as the case may be).\n\nWhen expressed as a frequency bandwidth an octave A–A spans from 110 Hz to 220 Hz (span=110 Hz). The next octave will span from 220 Hz to 440 Hz (span=220 Hz). The third octave spans from 440 Hz to 880 Hz (span=440 Hz) and so on. Each successive octave spans twice the frequency range of the previous octave.\n\nBecause we are often interested in the relations or ratios between the pitches (known as intervals) rather than the precise pitches themselves in describing a scale, it is usual to refer to all the scale pitches in terms of their ratio from a particular pitch, which is given the value of one (often written 1/1), generally a note which functions as the tonic of the scale. For interval size comparison, cents are often used.\n\nThere are two main families of tuning systems: equal temperament and just tuning. Equal temperament scales are built by dividing an octave into intervals which are equal on a logarithmic scale, which results in perfectly evenly divided scales, but with ratios of frequencies which are irrational numbers. Just scales are built by multiplying frequencies by rational numbers, which results in simple ratios between frequencies, but with scale divisions that are uneven.\n\nOne major difference between equal temperament tunings and just tunings is differences in acoustical beat when two notes are sounded together, which affects the subjective experience of consonance and dissonance. Both of these systems, and the vast majority of music in general, have scales that repeat on the interval of every octave, which is defined as frequency ratio of 2:1. In other words, every time the frequency is doubled, the given scale repeats.\n\nBelow are Ogg Vorbis files demonstrating the difference between just intonation and equal temperament. You may need to play the samples several times before you can pick the difference.\n\n5-limit tuning, the most common form of just intonation, is a system of tuning using tones that are regular number harmonics of a single fundamental frequency. This was one of the scales Johannes Kepler presented in his Harmonices Mundi (1619) in connection with planetary motion. The same scale was given in transposed form by Scottish mathematician and musical theorist, Alexander Malcolm, in 1721 in his 'Treatise of Musick: Speculative, Practical and Historical', and by theorist Jose Wuerschmidt in the 20th century. A form of it is used in the music of northern India.\n\nAmerican composer Terry Riley also made use of the inverted form of it in his \"Harp of New Albion\". Just intonation gives superior results when there is little or no chord progression: voices and other instruments gravitate to just intonation whenever possible. However, it gives two different whole tone intervals (9:8 and 10:9) because a fixed tuned instrument, such as a piano, cannot change key. To calculate the frequency of a note in a scale given in terms of ratios, the frequency ratio is multiplied by the tonic frequency. For instance, with a tonic of A4 (A natural above middle C), the frequency is 440 Hz, and a justly tuned fifth above it (E5) is simply 440×(3:2) = 660 Hz.\n\nPythagorean tuning is tuning based only on the perfect consonances, the (perfect) octave, perfect fifth, and perfect fourth. Thus the major third is considered not a third but a ditone, literally \"two tones\", and is (9:8) = 81:64, rather than the independent and harmonic just 5:4 = 80:64 directly below. A whole tone is a secondary interval, being derived from two perfect fifths, (3:2) = 9:8.\n\nThe just major third, 5:4 and minor third, 6:5, are a syntonic comma, 81:80, apart from their Pythagorean equivalents 81:64 and 32:27 respectively. According to Carl , \"the dependent third conforms to the Pythagorean, the independent third to the harmonic tuning of intervals.\"\n\nWestern common practice music usually cannot be played in just intonation but requires a systematically tempered scale. The tempering can involve either the irregularities of well temperament or be constructed as a regular temperament, either some form of equal temperament or some other regular meantone, but in all cases will involve the fundamental features of meantone temperament. For example, the root of chord ii, if tuned to a fifth above the dominant, would be a major whole tone (9:8) above the tonic. If tuned a just minor third (6:5) below a just subdominant degree of 4:3, however, the interval from the tonic would equal a minor whole tone (10:9). Meantone temperament reduces the difference between 9:8 and 10:9. Their ratio, (9:8)/(10:9) = 81:80, is treated as a unison. The interval 81:80, called the syntonic comma or comma of Didymus, is the key comma of meantone temperament.\n\nIn equal temperament, the octave is divided into equal parts on the logarithmic scale. While it is possible to construct equal temperament scale with any number of notes (for example, the 24-tone Arab tone system), the most common number is 12, which makes up the equal-temperament chromatic scale. In western music, a division into twelve intervals is commonly assumed unless it is specified otherwise.\n\nFor the chromatic scale, the octave is divided into twelve equal parts, each semitone (half-step) is an interval of the twelfth root of two so that twelve of these equal half steps add up to exactly an octave. With fretted instruments it is very useful to use equal temperament so that the frets align evenly across the strings. In the European music tradition, equal temperament was used for lute and guitar music far earlier than for other instruments, such as musical keyboards. Because of this historical force, twelve-tone equal temperament is now the dominant intonation system in the Western, and much of the non-Western, world.\n\nEqually tempered scales have been used and instruments built using various other numbers of equal intervals. The 19 equal temperament, first proposed and used by Guillaume Costeley in the 16th century, uses 19 equally spaced tones, offering better major thirds and far better minor thirds than normal 12-semitone equal temperament at the cost of a flatter fifth. The overall effect is one of greater consonance. Twenty-four equal temperament, with twenty-four equally spaced tones, is widespread in the pedagogy and notation of Arabic music. However, in theory and practice, the intonation of Arabic music conforms to rational ratios, as opposed to the irrational ratios of equally tempered systems.\n\nWhile any analog to the equally tempered quarter tone is entirely absent from Arabic intonation systems, analogs to a three-quarter tone, or neutral second, frequently occur. These neutral seconds, however, vary slightly in their ratios dependent on maqam, as well as geography. Indeed, Arabic music historian Habib Hassan Touma has written that \"the breadth of deviation of this musical step is a crucial ingredient in the peculiar flavor of Arabian music. To temper the scale by dividing the octave into twenty-four quarter-tones of equal size would be to surrender one of the most characteristic elements of this musical culture.\"\n\nMusical set theory uses the language of mathematical set theory in an elementary way to organize musical objects and describe their relationships. To analyze the structure of a piece of (typically atonal) music using musical set theory, one usually starts with a set of tones, which could form motives or chords. By applying simple operations such as transposition and inversion, one can discover deep structures in the music. Operations such as transposition and inversion are called isometries because they preserve the intervals between tones in a set.\n\nExpanding on the methods of musical set theory, some theorists have used abstract algebra to analyze music. For example, the pitch classes in an equally tempered octave form an abelian group with 12 elements. It is possible to describe just intonation in terms of a free abelian group.\n\nTransformational theory is a branch of music theory developed by David Lewin. The theory allows for great generality because it emphasizes transformations between musical objects, rather than the musical objects themselves.\n\nTheorists have also proposed musical applications of more sophisticated algebraic concepts. The theory of regular temperaments has been extensively developed with a wide range of sophisticated mathematics, for example by associating each regular temperament with a rational point on a Grassmannian.\n\nThe chromatic scale has a free and transitive action of the cyclic group formula_1, with the action being defined via transposition of notes. So the chromatic scale can be thought of as a torsor for the group formula_1.\n\nReal and complex analysis have also been made use of, for instance by applying the theory of the Riemann zeta function to the study of equal divisions of the octave.\n\n\n"}
{"id": "2142734", "url": "https://en.wikipedia.org/wiki?curid=2142734", "title": "Notation in probability and statistics", "text": "Notation in probability and statistics\n\nProbability theory and statistics have some commonly used conventions, in addition to standard mathematical notation and mathematical symbols.\n\n\n\nThe α-level upper critical value of a probability distribution is the value exceeded with probability α, that is, the value \"x\" such that \"F\"(\"x\") = 1 − α where \"F\" is the cumulative distribution function. There are standard notations for the upper critical values of some commonly used distributions in statistics:\n\n\nCommon abbreviations include:\n\n\n"}
{"id": "37658021", "url": "https://en.wikipedia.org/wiki?curid=37658021", "title": "Null model", "text": "Null model\n\nIn mathematics, for example in the study of statistical properties of graphs, a null model is type of random object that matches one specific object in some of its features, or more generally satisfies a collection of constraints, but which is otherwise taken to be an unbiasedly random structure. The null model is used as a term of comparison, to verify whether the object in question displays some non-trivial features (properties that wouldn't be expected on the basis of chance alone or as a consequence of the constraints), such as community structure in graphs. An appropriate null model behaves in accordance with a reasonable null hypothesis for the behavior of the system under investigation.\n\nOne null model of utility in the study of complex networks is that proposed by Newman and Girvan, consisting of a randomized version of an original graph formula_1, produced through edges being rewired at random, under the constraint that the expected degree of each vertex matches the degree of the vertex in the original graph.\n\nThe null model is the basic concept behind the definition of modularity, a function which evaluates the goodness of partitions of a graph into clusters. In particular, given a graph formula_1 and a specific community partition formula_3 (an assignment of a community-index formula_4 (here taken as an integer from formula_5 to formula_6) to each vertex formula_7 in the graph), the modularity measures the difference between the number of links from/to each pair of communities, from that expected in a graph that is completely random in all respects other than the set of degrees of each of the vertices (the degree sequence). In other words, the modularity contrasts the exhibited community structure in formula_1 with that of a null model, which in this case is the configuration model (the maximally random graph subject to a constraint on the degree of each vertex).\n\n"}
{"id": "500004", "url": "https://en.wikipedia.org/wiki?curid=500004", "title": "On-Line Encyclopedia of Integer Sequences", "text": "On-Line Encyclopedia of Integer Sequences\n\nThe On-Line Encyclopedia of Integer Sequences (OEIS), also cited simply as Sloane's, is an online database of integer sequences. It was created and maintained by Neil Sloane while a researcher at AT&T Labs. Foreseeing his retirement from AT&T Labs in 2012 and the need for an independent foundation, Sloane agreed to transfer the intellectual property and hosting of the OEIS to the OEIS Foundation in October 2009. Sloane continues to be involved in the OEIS in his role as President of the OEIS Foundation.\n\nOEIS records information on integer sequences of interest to both professional mathematicians and amateurs, and is widely cited. it contains over 300,000 sequences, making it the largest database of its kind.\n\nEach entry contains the leading terms of the sequence, keywords, mathematical motivations, literature links, and more, including the option to generate a graph or play a musical representation of the sequence. The database is searchable by keyword and by subsequence.\n\nNeil Sloane started collecting integer sequences as a graduate student in 1965 to support his work in combinatorics. The database was at first stored on punched cards. He published selections from the database in book form twice:\nThese books were well received and, especially after the second publication, mathematicians supplied Sloane with a steady flow of new sequences. The collection became unmanageable in book form, and when the database had reached 16,000 entries Sloane decided to go online—first as an e-mail service (August 1994), and soon after as a web site (1996). As a spin-off from the database work, Sloane founded the \"Journal of Integer Sequences\" in 1998.\nThe database continues to grow at a rate of some 10,000 entries a year.\nSloane has personally managed 'his' sequences for almost 40 years, but starting in 2002, a board of associate editors and volunteers has helped maintain the database.\nIn 2004, Sloane celebrated the addition of the 100,000th sequence to the database, , which counts the marks on the Ishango bone. In 2006, the user interface was overhauled and more advanced search capabilities were added. In 2010 an OEIS wiki at OEIS.org was created to simplify the collaboration of the OEIS editors and contributors. The 200,000th sequence, , was added to the database in November 2011; it was initially entered as A200715, and moved to A200000 after a week of discussion on the SeqFan mailing list, following a proposal by OEIS Editor-in-Chief Charles Greathouse to choose a special sequence for A200000.\n\nBesides integer sequences, the OEIS also catalogs sequences of fractions, the digits of transcendental numbers, complex numbers and so on by transforming them into integer sequences.\nSequences of rationals are represented by two sequences (named with the keyword 'frac'): the sequence of numerators and the sequence of denominators. For example, the fifth order Farey sequence, formula_1, is catalogued as the numerator sequence 1, 1, 1, 2, 1, 3, 2, 3, 4 () and the denominator sequence 5, 4, 3, 5, 2, 5, 3, 4, 5 ().\nImportant irrational numbers such as π = 3.1415926535897... are catalogued under representative integer sequences such as decimal expansions (here 3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9, 3, 2, 3, 8, 4, 6, 2, 6, 4, 3, 3, 8, 3, 2, 7, 9, 5, 0, 2, 8, 8, ... ()), binary expansions (here 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, ... ()), or continued fraction expansions (here 3, 7, 15, 1, 292, 1, 1, 1, 2, 1, 3, 1, 14, 2, 1, 1, 2, 2, 2, 2, 1, 84, 2, 1, 1, ... ()).\n\nThe OEIS was limited to plain ASCII text until 2011, and it still uses a linear form of conventional mathematical notation (such as \"f\"(\"n\") for functions, \"n\" for running variables, etc.). Greek letters are usually represented by their full names, \"e.g.\", mu for μ, phi for φ.\nEvery sequence is identified by the letter A followed by six digits, almost always referred to with leading zeros, \"e.g.\", A000315 rather than A315.\nIndividual terms of sequences are separated by commas. Digit groups are not separated by commas, periods, or spaces.\nIn comments, formulas, etc., a(n) represents the \"n\"th term of the sequence.\n\nZero is often used to represent non-existent sequence elements. For example, enumerates the \"smallest prime of \"n\"² consecutive primes to form an \"n\"×\"n\" magic square of least magic constant, or 0 if no such magic square exists.\" The value of \"a\"(1) (a 1×1 magic square) is 2; \"a\"(3) is 1480028129. But there is no such 2×2 magic square, so \"a\"(2) is 0.\nThis special usage has a solid mathematical basis in certain counting functions. For example, the totient valence function \"N\"(\"m\") () counts the solutions of φ(\"x\") = \"m\". There are 4 solutions for 4, but no solutions for 14, hence \"a\"(14) of A014197 is 0—there are no solutions.\nOccasionally −1 is used for this purpose instead, as in .\n\nThe OEIS maintains the lexicographical order of the sequences, so each sequence has a predecessor and a successor (its \"context\"). OEIS normalizes the sequences for lexicographical ordering, (usually) ignoring all initial zeros and ones, and also the sign of each element. Sequences of weight distribution codes often omit periodically recurring zeros.\n\nFor example, consider: the prime numbers, the palindromic primes, the Fibonacci sequence, the lazy caterer's sequence, and the coefficients in the series expansion of formula_2. In OEIS lexicographic order, they are:\nwhereas unnormalized lexicographic ordering would order these sequences thus: #3, #5, #4, #1, #2.\n\nVery early in the history of the OEIS, sequences defined in terms of the numbering of sequences in the OEIS itself were proposed. \"I resisted adding these sequences for a long time, partly out of a desire to maintain the dignity of the database, and partly because A22 was only known to 11 terms!\", Sloane reminisced.\nOne of the earliest self-referential sequences Sloane accepted into the OEIS was (later ) \"\"a\"(\"n\") = \"n\"-th term of sequence A or -1 if A has fewer than n terms\". This sequence spurred progress on finding more terms of . \nThis line of thought leads to the question \"Does sequence A contain the number \"n\" ?\" and the sequences , \"Numbers \"n\" such that OEIS sequence A contains \"n\", and , \"n\" is in this sequence if and only if \"n\" is not in sequence A\". Thus, the composite number 2808 is in A053873 because is the sequence of composite numbers, while the non-prime 40 is in A053169 because it's not in , the prime numbers. Each \"n\" is a member of exactly one of these two sequences, and in principle it can be determined \"which\" sequence each \"n\" belongs to, with two exceptions (related to the two sequences themselves):\n\nThis entry, , was chosen because it contains every field an OEIS entry can have.\n\n\nIn 2009, the OEIS database was used by an amateur mathematician to measure the \"importance\" of each integer number. The result shown in the plot on the right shows a clear \"gap\" between two distinct point clouds the \"uninteresting numbers\" (blue dots) and the \"interesting\" numbers that occur comparatively more often in sequences from the OEIS. It contains essentially prime numbers (red), numbers of the form a^n (green) and highly composite numbers (yellow). This phenomenon was studied by Nicolas Gauvrit, Jean-Paul Delahaye and Hector Zenil who explained the speed of the 2 clouds in terms of algorithmic complexity and the gap by social factors based on an artificial preference for sequences of primes, even numbers, geometric and Fibonacci-type sequences and so on. Sloane's gap was featured on a Numberphile video.\n\n\n\n\n"}
{"id": "31228704", "url": "https://en.wikipedia.org/wiki?curid=31228704", "title": "Prime factor exponent notation", "text": "Prime factor exponent notation\n\nIn his 1557 work \"The Whetstone of Witte\", British mathematician Robert Recorde proposed an exponent notation by prime factorisation, which remained in use up until the eighteenth century and acquired the name \"Arabic exponent notation\". The principle of Arabic exponents was quite similar to Egyptian fractions; large exponents were broken down into smaller prime numbers. Squares and cubes were so called; prime numbers from five onwards were called \"sursolids\".\n\nAlthough the terms used for defining exponents differed between authors and times, the general system was the primary exponent notation until René Descartes devised the Cartesian exponent notation, which is still used today.\n\nThis is a list of Recorde's terms.\n\nBy comparison, here is a table of prime factors:\n\n\n"}
{"id": "2134047", "url": "https://en.wikipedia.org/wiki?curid=2134047", "title": "Principles of Mathematical Logic", "text": "Principles of Mathematical Logic\n\nPrinciples of Mathematical Logic is the 1950 American translation of the 1938 second edition of David Hilbert's and Wilhelm Ackermann's classic text \"Grundzüge der theoretischen Logik\", on elementary mathematical logic. The 1928 first edition thereof is considered the first elementary text clearly grounded in the formalism now known as first-order logic (FOL). Hilbert and Ackermann also formalized FOL in a way that subsequently achieved canonical status. FOL is now a core formalism of mathematical logic, and is presupposed by contemporary treatments of Peano arithmetic and nearly all treatments of axiomatic set theory.\n\nThe 1928 edition included a clear statement of the Entscheidungsproblem (decision problem) for FOL, and also asked whether that logic was complete (i.e., whether all semantic truths of FOL were theorems derivable from the FOL axioms and rules). The former problem was answered in the negative first by Alonzo Church and independently by Alan Turing in 1936. The latter was answered affirmatively by Kurt Gödel in 1929.\n\nThe text also touched on set theory and relational algebra as ways of going beyond FOL. Contemporary notation for logic owes more to this text than it does to the notation of \"Principia Mathematica\", long popular in the English speaking world.\n\n"}
{"id": "2142840", "url": "https://en.wikipedia.org/wiki?curid=2142840", "title": "Procept", "text": "Procept\n\nA procept is an amalgam of three components: a process which produces a mathematical object and a symbol which is used to represent either process or object. It derives from the work of Eddie Gray and David O. Tall, and is a much used construct in mathematics education research.\n\nThe notion was first published in a paper in the Journal for Research in Mathematics Education in 1994, and is part of the process-object literature. This body of literature suggests that mathematical objects are formed by encapsulating processes, that is to say that the mathematical object 3 is formed by an encapsulation of the process of counting: 1,2,3...\n\nGray & Tall's notion of procept improved upon the existing literature by noting that mathematical notation is often ambiguous as to whether it refers to process or object. Examples of such notations are:\n\n\n"}
{"id": "30684123", "url": "https://en.wikipedia.org/wiki?curid=30684123", "title": "Ptolemy's table of chords", "text": "Ptolemy's table of chords\n\nThe table of chords, created by the astronomer, geometer, and geographer Ptolemy in Egypt during the 2nd century AD, is a trigonometric table in Book I, chapter 11 of Ptolemy's \"Almagest\", a treatise on mathematical astronomy. It is essentially equivalent to a table of values of the sine function. It was the earliest trigonometric table extensive enough for many practical purposes, including those of astronomy (an earlier table of chords by Hipparchus gave chords only for arcs that were multiples of ). Centuries passed before more extensive trigonometric tables were created. One such table is the \"Canon Sinuum\" created at the end of the 16th century.\n\nA chord of a circle is a line segment whose endpoints are on the circle. Ptolemy used a circle whose diameter is 120. He tabulated the length of a chord whose endpoints are separated by an arc of \"n\" degrees, for \"n\" ranging from to 180 by increments of . In modern notation, the length of the chord corresponding to an arc of \"θ\" degrees is\n\nAs \"θ\" goes from 0 to 180, the chord of a \"θ\"° arc goes from 0 to 120. For tiny arcs, the chord is to the arc angle in degrees as is to 3, or more precisely, the ratio can be made as close as desired to  ≈  by making \"θ\" small enough. Thus, for the arc of °, the chord length is slightly more than the arc angle in degrees. As the arc increases, the ratio of the chord to the arc decreases. When the arc reaches 60°, the chord length is exactly equal to the number of degrees in the arc, i.e. chord 60° = 60. For arcs of more than 60°, the chord is less than the arc, until an arc of 180° is reached, when the chord is only 120.\n\nThe fractional parts of chord lengths were expressed in sexagesimal (base 60) numerals. For example, where the length of a chord subtended by a 112° arc is reported to be 99 29 5, it has a length of\n\nrounded to the nearest .\n\nAfter the columns for the arc and the chord, a third column is labeled \"sixtieths\". For an arc of \"θ\"°, the entry in the \"sixtieths\" column is\n\nThis is the average number of sixtieths of a unit that must be added to chord(\"θ\"°) each time the angle increases by one minute of arc, between the entry for \"θ\"° and that for (\"θ\" + )°. Thus, it is used for linear interpolation. Glowatzki and Göttsche showed that Ptolemy must have calculated chords to five sexigesimal places in order to achieve the degree of accuracy found in the \"sixtieths\" column.\n\nChapter 10 of Book I of the \"Almagest\" presents geometric theorems used for computing chords. Ptolemy used geometric reasoning based on Proposition 10 of Book XIII of Euclid's \"Elements\" to find the chords of 72° and 36°. That Proposition states that if an equilateral pentagon is inscribed in a circle, then the area of the square on the side of the pentagon equals the sum of the areas of the squares on the sides of the hexagon and the decagon inscribed in the same circle.\n\nHe used Ptolemy's theorem on quadrilaterals inscribed in a circle to derive formulas for the chord of a half-arc, the chord of the sum of two arcs, and the chord of a difference of two arcs. The theorem states that for a quadrilateral inscribed in a circle, the product of the lengths of the diagonals equals the sum of the products of the two pairs of lengths of opposite sides. The derivations of trigonometric identities rely on a cyclic quadrilateral in which one side is a diameter of the circle.\n\nTo find the chords of arcs of 1° and ° he used approximations based on Aristarchus's inequality. The inequality states that for arcs \"α\" and \"β\", if 0 < \"β\" < \"α\" < 90°, then\n\nPtolemy showed that for arcs of 1° and °, the approximations correctly give the first two sexigesimal places after the integer part.\n\nLengths of arcs of the circle, in degrees, and the integer parts of chord lengths, were expressed in a base 10 numeral system that used 21 of the letters of the Greek alphabet with the meanings given in the following table, and a symbol, \"∠′ \", that means and a raised circle \"○\" that fills a blank space (effectively representing zero). Two of the letters, labeled \"archaic\" in the table below, had not been in use in the Greek language for some centuries before the \"Almagest\" was written, but were still in use as numerals and musical notes.\nThus, for example, an arc of ° is expressed as \"ρμγ\"∠′. (As the table only reaches 180°, the Greek numerals for 200 and above are not used.)\n\nThe fractional parts of chord lengths required great accuracy, and were given in two columns in the table: The first column gives an integer multiple of , in the range 0–59, the second an integer multiple of  = , also in the range 0–59.\n\nThus in Heiberg's edition of the \"Almagest\" with the table of chords on pages 48–63, the beginning of the table, corresponding to arcs from ° to °, looks like this:\n\nLater in the table, one can see the base-10 nature of the numerals expressing the integer parts of the arc and the chord length. Thus an arc of 85° is written as \"πε\" (\"π\" for 80 and \"ε\" for 5) and not broken down into 60 + 25. The corresponding chord length is 81 plus a fractional part. The integer part begins with \"πα\", likewise not broken into 60 + 21. But the fractional part,  + , is written as \"δ\", for 4, in the column, followed by \"ιε\", for 15, in the column.\nThe table has 45 lines on each of eight pages, for a total of 360 lines.\n\n\n\n"}
{"id": "159731", "url": "https://en.wikipedia.org/wiki?curid=159731", "title": "Quasi-empiricism in mathematics", "text": "Quasi-empiricism in mathematics\n\nQuasi-empiricism in mathematics is the attempt in the philosophy of mathematics to direct philosophers' attention to mathematical practice, in particular, relations with physics, social sciences, and computational mathematics, rather than solely to issues in the foundations of mathematics. Of concern to this discussion are several topics: the relationship of empiricism (see Maddy) with mathematics, issues related to realism, the importance of culture, necessity of application, etc.\n\nA primary argument with respect to quasi-empiricism is that whilst mathematics and physics are frequently considered to be closely linked fields of study, this may reflect human cognitive bias. It is claimed that, despite rigorous application of appropriate empirical methods or mathematical practice in either field, this would nonetheless be insufficient to disprove alternate approaches.\n\nEugene Wigner (1960) noted that this culture need not be restricted to mathematics, physics, or even humans. He stated further that \"The miracle of the appropriateness of the language of mathematics for the formulation of the laws of physics is a wonderful gift which we neither understand nor deserve. We should be grateful for it and hope that it will remain valid in future research and that it will extend, for better or for worse, to our pleasure, even though perhaps also to our bafflement, to wide branches of learning.\" Wigner used several examples to demonstrate why 'bafflement' is an appropriate description, such as showing how mathematics adds to situational knowledge in ways that are either not possible otherwise or are so outside normal thought to be of little notice. The predictive ability, in the sense of describing potential phenomena prior to observation of such, which can be supported by a mathematical system would be another example. \n\nFollowing up on Wigner, Richard Hamming (1980) wrote about applications of mathematics as a central theme to this topic and suggested that successful use can sometimes trump proof, in the following sense: where a theorem has evident veracity through applicability, later evidence that shows the theorem's proof to be problematic would result more in trying to firm up the theorem rather than in trying to redo the applications or to deny results obtained to date. Hamming had four explanations for the 'effectiveness' that we see with mathematics and definitely saw this topic as worthy of discussion and study. \n\n\nFor Willard Van Orman Quine (1960), existence is only existence in a structure. This position is relevant to quasi-empiricism, because Quine believes that the same evidence that supports theorizing about the structure of the world is the same as the evidence supporting theorizing about mathematical structures.\n\nHilary Putnam (1975) stated that mathematics had accepted informal proofs and proof by authority, and had made and corrected errors all through its history. Also, he stated that Euclid's system of proving geometry theorems was unique to the classical Greeks and did not evolve similarly in other mathematical cultures in China, India, and Arabia. This and other evidence led many mathematicians to reject the label of Platonists, along with Plato's ontology which, along with the methods and epistemology of Aristotle, had served as a foundation ontology for the Western world since its beginnings. A truly international culture of mathematics would, Putnam and others (1983) argued, necessarily be at least 'quasi'-empirical (embracing 'the scientific method' for consensus if not experiment).\n\nImre Lakatos (1976), who did his original work on this topic for his dissertation (1961, Cambridge), argued for 'Research Programs' as a means to support a basis for mathematics and considered thought experiments as appropriate to mathematical discovery. Lakatos may have been the first to use 'quasi-empiricism' in the context of this subject.\n\nSeveral recent works pertain to this topic. Chaitin's and Stephen Wolfram's work, though their positions may be considered controversial, apply. Chaitin (1997/2003) suggests an underlying randomness to mathematics and Wolfram (\"A New Kind of Science\", 2002) argues that undecidability may have practical relevance, that is, be more than an abstraction. \n\nAnother relevant addition would be the discussions concerning interactive computation, especially those related to the meaning and use of Turing's model (Church-Turing thesis, Turing machines, etc.). \n\nThese works are heavily computational and raise another set of issues. To quote Chaitin (1997/2003): \"Now everything has gone topsy-turvy. It's gone topsy-turvy, not because of any philosophical argument, not because of Gödel's results or Turing's results or my own incompleteness results. It's gone topsy-turvy for a very simple reason — the computer!\".\n\nThe collection of \"Undecidables\" in Wolfram (\"A New Kind of Science\", 2002) is another example. \n\nWegner's 2006 paper \"Principles of Problem Solving\" suggests that \"interactive computation\" can help mathematics form a more appropriate framework (empirical) than can be founded with rationalism alone. Related to this argument is that the function (even recursively related ad infinitum) is too simple a construct to handle the reality of entities that resolve (via computation or some type of analog) n-dimensional (general sense of the word) systems.\n\n"}
{"id": "58684510", "url": "https://en.wikipedia.org/wiki?curid=58684510", "title": "Seneca effect", "text": "Seneca effect\n\nThe Seneca effect, or Seneca cliff or Seneca collapse, is a mathematical model, proposed by Ugo Bardi. This model is aimed to describe a class of problems in nature in which the decline is faster than growth, under the condition of some constraints. This model is closely related to the work \"The Limits to Growth\" issued by the Club of Rome in the Seventies and its main application is to describe various kind of economics under the condition of shortage of fossil fuels, e.g. in relation to the Hubbert curve.\n\nThe reference to the Latin philosopher and writer Seneca of the model's name lies in the verse: \"the raise is gradual, the ruin is precipitous\" (Lucius Anneus Seneca, \"Letters to Lucilius\", 91–63).\n\n\n"}
{"id": "48211685", "url": "https://en.wikipedia.org/wiki?curid=48211685", "title": "Stacks Project", "text": "Stacks Project\n\nThe Stacks Project is an open source collaborative mathematics textbook writing project with the aim to cover \"algebraic stacks and the algebraic geometry needed to define them\". , the book consists of 109 chapters spreading over 6000 pages. The maintainer of the project, who reviews and accepts the changes, is Aise Johan de Jong.\n\n"}
{"id": "249617", "url": "https://en.wikipedia.org/wiki?curid=249617", "title": "System of equations", "text": "System of equations\n\nIn mathematics, a set of simultaneous equations, also known as a system of equations or an equation system, is a finite set of equations for which common solutions are sought. An equation system is usually classified in the same manner as single equations, namely as a:\n\n"}
{"id": "17319580", "url": "https://en.wikipedia.org/wiki?curid=17319580", "title": "T.C. Mits", "text": "T.C. Mits\n\nT.C. Mits (acronym for \"the celebrated man in the street\"), is a term coined by Lillian Rosanoff Lieber to refer to an everyman. In Lieber's works, T.C. Mits was a character who made scientific topics more approachable to the public audience. \n\nThe phrase has enjoyed sparse use by authors in fields such as molecular biology, secondary education, and general semantics.\n\nDr. Lillian Rosanoff Lieber wrote this treatise on mathematical thinking in twenty chapters. The writing took a form that resembled free-verse poetry, though Lieber included an introduction stating that the form was meant only to facilitate rapid reading, rather than emulate free-verse. Lieber's husband, a fellow professor at Long Island University, Hugh Gray Lieber, provided illustrations for the book. The title of the book was meant to emphasize that mathematics can be understood by anyone, which was further shown when a special \"Overseas edition for the Armed Forces\" was published in 1942, and approved by the Council on Books in Wartime to be sent to American troops fighting in World War II.\n"}
{"id": "1514907", "url": "https://en.wikipedia.org/wiki?curid=1514907", "title": "Unary function", "text": "Unary function\n\nA unary function is a function that takes one argument. A unary operator belongs to a subset of unary functions, in that its range coincides with its domain.\n\nThe successor function, denoted formula_1, is a unary operator. Its domain and codomain are the natural numbers, its definition is as follows:\n\nIn many programming languages such as C, executing this operation is denoted by postfixing formula_3 to the operand, i.e. the use of formula_4 is equivalent to executing the assignment formula_5.\n\nMany of the elementary functions are unary functions, in particular the trigonometric functions, logarithm with a pre-specified base, exponentiation to a pre-specified power or of a pre-specified base, and hyperbolic functions are unary.\n\n\n"}
{"id": "29759120", "url": "https://en.wikipedia.org/wiki?curid=29759120", "title": "Uniqueness theorem", "text": "Uniqueness theorem\n\nIn mathematics, a uniqueness theorem is a theorem proving that certain conditions determine a unique solution. Examples of uniqueness theorems include:\n\nA theorem, also called a unicity theorem, stating the uniqueness of a mathematical object, which usually means that there is only one object fulfilling given properties, or that all objects of a given class are equivalent (i.e., they can be represented by the same model). This is often expressed by saying that the object is uniquely determined by a certain set of data. The word unique is sometimes replaced by essentially unique, whenever one wants to stress that the uniqueness is only referred to the underlying structure, whereas the form may vary in all ways that do not affect the mathematical content.\n\nA uniqueness theorem / proof is, at least within mathematics of differential equations, often combined with an existence theorem / proof to a combined existence and uniqueness theorem.\n\n"}
{"id": "58946268", "url": "https://en.wikipedia.org/wiki?curid=58946268", "title": "Zimmer's conjecture", "text": "Zimmer's conjecture\n\nZimmer's conjecture is a statement in mathematics \"which has to do with the circumstances under which geometric spaces exhibit certain kinds of symmetries.\" It was named after the mathematician Robert Zimmer. The conjecture states that there can exist symmetries (specifically higher-rank lattices) in a higher dimension that cannot exist in lower dimensions.\n\nIn 2017, the conjecture was proven by Aaron Brown and Sebastian Hurtado-Salazar of the University of Chicago and David Fisher of Indiana University.\n"}
