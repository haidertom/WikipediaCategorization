{"id": "32329761", "url": "https://en.wikipedia.org/wiki?curid=32329761", "title": "Analytica (software)", "text": "Analytica (software)\n\nAnalytica is a visual software package developed by Lumina Decision Systems for creating, analyzing and communicating quantitative decision models. As a modeling environment, it is interesting in the way it combines hierarchical influence diagrams for visual creation and view of models, intelligent arrays for working with multidimensional data, Monte Carlo simulation for analyzing risk and uncertainty, and optimization, including linear and nonlinear programming. Its design, especially its influence diagrams and treatment of uncertainty, is based on ideas from the field of decision analysis. As a computer language, it is notable in combining a declarative (non-procedural) structure for referential transparency, array abstraction, and automatic dependency maintenance for efficient sequencing of computation.\n\nAnalytica models are organized as influence diagrams. Variables (and other objects) appear as nodes of various shapes on a diagram, connected by arrows that provide a visual representation of dependencies. Analytica influence diagrams may be hierarchical, in which a single \"module\" node on a diagram represents an entire submodel.\n\nHierarchical influence diagrams in Analytica serve as a key organizational tool. Because the visual layout of an influence diagram matches these natural human abilities both spatially and in the level of abstraction, people are able to take in far more information about a model's structure and organization at a glance than is possible with less visual paradigms, such as spreadsheets and mathematical expressions. Managing the structure and organization of a large model can be a significant part of the modeling process, but is substantially aided by the visualization of influence diagrams.\n\nInfluence diagrams also serve as a tool for communication. Once a quantitative model has been created and its final results computed, it is often the case that an understanding of how the results are obtained, and how various assumptions impact the results, is far more important than the specific numbers computed. The ability of a target audience to understand these aspects is critical to the modeling enterprise. The visual representation of an influence diagram quickly communicates an understanding at a level of abstraction that is normally more appropriate than detailed representations such as mathematical expressions or cell formulae. When more detail is desired, users can drill down to increasing levels of detail, speeded by the visual depiction of the model's structure.\n\nThe existence of an easily understandable and transparent model supports communication and debate within an organization, and this effect is one of the primary benefits of investing in quantitative model building. When all interested parties are able to understand a common model structure, debates and discussions will often focus more directly on specific assumptions, can cut down on \"cross-talk\", and therefore lead to more productive interactions within the organization. The influence diagram serves as a graphical representation that can help to make models accessible to people at different levels.\n\nAnalytica uses index objects to track the dimensions of multidimensional arrays. An index object has a name and a list of elements. When two multidimensional values are combined, for example in an expression such as\n\nwhere \"Revenue\" and \"Expenses\" are each multidimensional, Analytica repeats the profit calculation over each dimension, but recognizes when same dimension occurs in both values and treats it as the same dimension during the calculation, in a process called \"intelligent array abstraction\". Unlike most programming languages, there is no inherent ordering to the dimensions in a multidimensional array. This avoids duplicated formulas and explicit FOR loops, both common sources of modeling errors. The simplified expressions made possible by intelligent array abstraction allow the model to be more accessible, interpretable, and transparent.\n\nAnother consequence of intelligent array abstraction is that new dimensions can be introduced or removed from an existing model, without requiring changes to the model structure or changes to variable definitions. For example, while creating a model, the model builder might assume a particular variable, for example \"discount_rate\", contains a single number. Later, after constructing a model, a user might replace the single number with a table of numbers, perhaps \"discount_rate\" broken down by \"Country\" and by \"Economic_scenario\". These new divisions may reflect the fact that the effective discount rate is not the same for international divisions of a company, and that different rates are applicable to different hypothetical scenarios. Analytica automatically propagates these new dimensions to any results that depend upon \"discount_rate\", so for example, the result for \"Net present value\" will become multidimensional and contain these new dimensions. In essence, Analytica repeats the same calculation using the discount rate for each possible combination of \"Country\" and \"Economic_scenario\".\n\nThis flexibility is important when exploring computation tradeoffs between the level of detail, computation time, available data, and overall size or dimensionality of parametric spaces. Such adjustments are common after models have been fully constructed as a way of exploring \"what-if\" scenarios and overall relationships between variables.\n\nIncorporating uncertainty into model outputs helps to provide more realistic and informative projections. Uncertain quantities in Analytica can be specified using a distribution function. When evaluated, distributions are sampled using either Latin hypercube or Monte Carlo sampling, and the samples are propagated through the computations to the results. The sampled result distribution and summary statistics can then be viewed directly (mean, fractile bands, probability density function (PDF), cumulative distribution function (CDF)), Analytica supports collaborative decision analysis and probability management through the use of the SIPMath(tm) standard.\n\nSystem dynamics is an approach to simulating the behaviour of complex systems over time. It deals with feedback loops and time delays on the behaviour of the entire system. The Dynamic() function in Analytica allows definition of variables with cyclic dependencies, such as feedback loops. It expands the influence diagram notation, which does not normally allow cycles. At least one link in each cycle includes a time lag, depicted as a gray influence arrow to distinguish it from standard black arrows without time lags.\n\nAnalytica includes a general language of operators and functions for expressing mathematical relationships among variables. Users can define functions and libraries to extend the language.\n\nAnalytica has several features as a programming language designed to make it easy to use for quantitative modeling: It is a visual programming language, where users view programs (or \"models\") as influence diagrams, which they create and edit visually by adding and linking nodes. It is a declarative language, meaning that a model declares a definition for each variable without specifying an execution sequence as required by conventional imperative languages. Analytica determines a correct and efficient execution sequence using the dependency graph. It is a referentially transparent functional language, in that execution of functions and variables have no side effects i.e. changing other variables. Analytica is an array programming language, where operations and functions generalize to work on multidimensional arrays.\n\nAnalytica has been used for policy analysis, business modeling, and risk analysis. Areas in which Analytica has been applied include energy, health and pharmaceuticals, \nenvironmental risk and emissions policy analysis, wildlife management,\necology,\nclimate change, technology and defense,\nstrategic financial planning, \nR&D planning and portfolio management, \nfinancial services, \naerospace, manufacturing and environmental health impact assessment.\n\nThe Analytica software runs on Microsoft Windows operating systems. Three editions (Professional, Enterprise, Optimizer) each with more functions and cost, are purchased by users interested in building models. A free edition is available, called Analytica Free 101, which allows you to build medium to moderate sized models of up to 101 user objects.. Free 101 also allows you to view models with more than 101 objects, change inputs, and compute results, which enables free sharing of models for review. A more capable but non-free Power Player enables users to save inputs and utilize database connections. The Analytica Cloud Player allows you to share models over the web and lets users access and run via a web browser.\n\nThe most recent release of Analytica is version 5.1, released in May 2018.\n\nAnalytica's predecessor, called \"Demos\", grew from the research on tools for policy analysis by Max Henrion as a PhD student and later professor at Carnegie Mellon University between 1979 and 1990. Henrion founded Lumina Decision Systems in 1991 with Brian Arnold. Lumina continued to develop the software and apply it to environmental and public policy analysis applications. Lumina first released Analytica as a product in 1996.\n"}
{"id": "1525019", "url": "https://en.wikipedia.org/wiki?curid=1525019", "title": "Apostolos Doxiadis", "text": "Apostolos Doxiadis\n\nApostolos K. Doxiadis (; born 1953) is a Greek writer. He is best known for his international bestsellers \"Uncle Petros and Goldbach's Conjecture\" (2000) and \"Logicomix\" (2009).\n\nDoxiadis was born in Australia, where his father, the architect Constantinos Apostolou Doxiadis was working. Soon after his birth, the family returned to Athens, where Doxiadis grew up. Though his earliest interests were in poetry, fiction and the theatre, an intense interest in mathematics led Doxiadis to leave school at age fifteen, to attend Columbia University, in New York, from which he obtained a bachelor's degree in Mathematics in May 1972. He then attended the École Pratique des Hautes Études in Paris from which he got a master's degree, with a thesis on the mathematical modeling of the nervous system. His father’s death and family reasons made him return to Greece in 1975, interrupting his graduate studies. In Greece, although involved for some years with the computer software industry, Doxiadis returned to his childhood and adolescence loves of theatre and the cinema, before becoming a full-time writer.\n\nDoxiadis began to write in Greek. His first published work was \"A Parallel Life\" (\"Βίος Παράλληλος\", 1985), a novella set in the monastic communities of 4th century CE Egypt. His first novel, \"Makavettas\" (\"Μακαβέττας\", 1988), recounted the adventures of a fictional power-hungry colonel at the time of the Greek military junta of 1967–1974. Written in a tongue-in-cheek imitation of Greek folk military memoirs, such as that of Yannis Makriyannis, it follows the plot of Shakespeare’s \"Macbeth\", of which the eponymous hero’s name is a Hellenized form. Doxiadis next novel, \"Uncle Petros and Goldbach’s Conjecture\" (\"Ο Θείος Πέτρος και η Εικασία του Γκόλντμπαχ\", 1992), was the first long work of fiction whose plot takes place in the world of pure mathematics research. The first Greek critics did not find the mathematical themes appealing, and it received mediocre reviews, unlike Doxiadis’s first two works, which were well received. Τhe novella \"The Three Little Men\" (\"Τα Τρία Ανθρωπάκια\", 1998), attempts a modern-day retelling of the tale of a classic fairy-tale.\n\nIn 1998, Doxiadis translated into English, significantly re-working, his third novel, which was published in England in 2000 as \"Uncle Petros and Goldbach's Conjecture\" (UK publisher: Faber and Faber; United States publisher: Bloomsbury USA.) The book became an international bestseller, and has been published to date in more than thirty-five languages. It has received the praise of, among others, Nobel Laureate John Nash, British mathematician Sir Michael Atiyah, critic George Steiner and psychiatrist Oliver Sacks. \"Uncle Petros\" is one of the \"1001 Books You Must Read Before You Die\".\nDoxiadis’ next project, which took over five years to complete, was the graphic novel \"Logicomix\" (2009), a number one bestseller on the New York Times Bestseller List and an international bestseller, already published in over twenty languages. \"Logicomix\" was co-authored with computer scientist Christos Papadimitriou, with art work by Alecos Papadatos (pencils) and Annie Di Donna (color). Renowned comics historian and critic R. C. Harvey, in the \"Comics Journal\", called \"Logicomix\" “a tour-de-force” a “virtuoso performance”, while \"The Sunday Times\"’ Brian Appleyard called it “probably the best and certainly the most extraordinary graphic novel” he has read. \"Logicomix\" is one of Paul Gravett’s \"1001 Comics You Must Read Before you Die.\" \n\nIn the early stage of his career, Doxiadis directed in the professional theatre, in Athens, and worked as translator, translating, among other plays, William Shakespeare’s \"Romeo and Juliet\", \"Hamlet\" and \"Midsummer Night’s Dream\", as well as Eugene O’Neill’s \"Mourning Becomes Electra\".\n\nHe has written two plays for the theatre. The first was a full-length shadow-puppet play \"The Tragical History of Jackson Pollock, Abstract Expressionist\" (1999), in English, of which he also designed and directed the Athens performance. In this play, Doxiadis realized some of his views on “epic theatre”, in other words a theatre based on storytelling. His second play, \"Incompleteness\" (2005), is an imaginary account of the last seventeen days in the life of the great logician Kurt Gödel, which Gödel spent in a Princeton, New Jersey, hospital, refusing to eat out of fear that he was being poisoned. The play was staged in Athens, in 2006, as Dekati Evdomi Nyhta (Seventeenth Night) with the actor Yorgos Kotanidis in the role of Kurt Gödel.\n\nDoxiadis has also written and directed two feature-length films, in Greek, \"Underground Passage\" (\"Υπόγεια Διαδρομή\", 1983) and \"Terirem\" (\"Τεριρέμ\", 1987). The latter won the CICAE (International Confederation of Art Cinemas) prize for Best Film in the 1988 Berlin International Film Festival.\n\nDoxiadis has a lifelong interest in logic, cognitive psychology and rhetoric, as well as the theoretical study of narrative. In 2007, he organized, with mathematician Barry Mazur, a meeting on the theoretical investigation of the relationship of mathematics and narrative, whose proceedings were published as \"Circles Disturbed, The Interplay of Mathematics and Narrative\" (2012). Doxiadis has lectured extensively on his theoretical interests. Doxiadis’ recent work has led him to formulate a theory about the development of deductive proof in Classical Greece, which lays emphasis on influences from pre-existing patterns in narrative and, especially, Archaic Age Poetry.\n\n\"Uncle Petros and Goldbach’s Conjecture\" was the first recipient of the Premio Peano the first international award for books inspired by mathematics and short-listed for the Prix Médicis. \"Logicomix\" has earned numerous awards, among them the Bertrand Russell Society Award, the Royal Booksellers Association Award (the Netherlands), the New Atlantic Booksellers Award (USA), the Prix Tangente (France), the Premio Carlo Boscarato (Italy), the Comicdom Award (Greece). It was chosen as \"Book of the Year\" by \"TIME Magazine\", \"Publishers Weekly\", \"The Washington Post\", \"The Financial Times\", \"The Globe and Mail\", and other publications.\n\n"}
{"id": "10283346", "url": "https://en.wikipedia.org/wiki?curid=10283346", "title": "Backtesting", "text": "Backtesting\n\nBacktesting is a term used in modeling to refer to testing a predictive model on historical data. Backtesting is a type of retrodiction, and a special type of cross-validation applied to previous time period(s).\n\nIn a trading strategy, investment strategy, or risk modeling, backtesting seeks to estimate the performance of a strategy or model if it had been employed during a past period. This requires simulating past conditions with sufficient detail, making one limitation of backtesting the need for detailed historical data. A second limitation is the inability to model strategies that would affect historic prices. Finally, backtesting, like other modeling, is limited by potential overfitting. That is, it is often possible to find a strategy that would have worked well in the past, but will not work well in the future. Despite these limitations, backtesting provides information not available when models and strategies are tested on synthetic data.\n\nBacktesting has historically only been performed by large institutions and professional money managers due to the expense of obtaining and using detailed datasets. However, backtrading is increasingly used on a wider basis, and independent web-based backtesting platforms have emerged. Although the technique is widely used, it is prone to weaknesses. Basel financial regulations require large financial institutions to backtest certain risk models.\n\nIn oceanography and meteorology, \"backtesting\" is also known as \"hindcasting\": a hindcast is a way of testing a mathematical model; researchers enter known or closely estimated inputs for past events into the model to see how well the output matches the known results.\n\nHindcasting usually refers to a numerical-model integration of a historical period where no observations have been assimilated. This distinguishes a hindcast run from a reanalysis. Oceanographic observations of salinity and temperature as well as observations of surface-wave parameters such as the significant wave height are much scarcer than meteorological observations, making hindcasting more common in oceanography than in meteorology. Also, since surface waves represent a forced system where the wind is the only generating force, wave hindcasting is often considered adequate for generating a reasonable representation of the wave climate with little need for a full reanalysis. Hydrologists use hindcasting for model stream flows.\n\nAn example of hindcasting would be entering climate forcings (events that force change) into a climate model. If the hindcast showed reasonably-accurate climate response, the model would be considered successful.\n\nThe ECMWF re-analysis is an example of a combined atmospheric reanalysis coupled with a wave-model integration where no wave parameters were assimilated, making the wave part a hindcast run.\n\nIn 2003, Dake Chen and his colleagues “trained” a computer using the data of the surface temperature of the oceans from the last 20 years. Then, using data that had been collected on the surface temperature of the oceans for the period 1857 to 2003, they went through a hindcasting exercise and discovered that their simulation not only accurately predicted every El Niño event for the last 148 years, it also identified the (up to 2 years) looming foreshadow of every single one of those El Niño events.\n"}
{"id": "3053039", "url": "https://en.wikipedia.org/wiki?curid=3053039", "title": "Bateman Manuscript Project", "text": "Bateman Manuscript Project\n\nThe Bateman Manuscript Project was a major effort at collation and encyclopedic compilation of the mathematical theory of special functions. It resulted in the eventual publication of five important reference volumes, under the editorship of Arthur Erdélyi.\n\nThe theory of special functions was a core activity of the field of applied mathematics, from the middle of the nineteenth century to the advent of high-speed electronic computing. The intricate properties of spherical harmonics, elliptic functions and other staples of problem-solving in mathematical physics, astronomy and right across the physical sciences, are not easy to document completely, absent a theory explaining the inter-relationships. Mathematical tables to perform actual calculations needed to mesh with an adequate theory of how functions could be transformed into those already tabulated.\n\nHarry Bateman, a distinguished applied mathematician, undertook the somewhat quixotic task of trying to collate the content of the very large literature. On his death in 1946, his papers on this project were still in a uniformly rough state. The publication of the edited version provided special functions texts more up-to-date than, for example, the classic Whittaker & Watson.\n\nThe volumes were out of print for many years, and copyright in the works reverted to the California Institute of Technology, who renewed them in the early 1980s. Dover planned to reprint them for publication in 2007 but this never occurred . In 2011, the California Institute of Technology gave permission for scans of the volumes to be made publicly available (see References).\n\nOther mathematicians involved in the project include Wilhelm Magnus.\n\n\n"}
{"id": "42063759", "url": "https://en.wikipedia.org/wiki?curid=42063759", "title": "Broer–Kaup equations", "text": "Broer–Kaup equations\n\nThe Broer–Kaup equations are a set of two coupled nonlinear partial differential equations\n\n"}
{"id": "23335649", "url": "https://en.wikipedia.org/wiki?curid=23335649", "title": "Canonical map", "text": "Canonical map\n\nIn mathematics, a canonical map, also called a natural map, is a map or morphism between objects that arises naturally from the definition or the construction of the objects being mapped against each other. In general it is the map which preserves the widest amount of structure, and it tends to be unique. In the rare cases where latitude in choice remains, the map is either conventionally agreed upon to be the most useful for further analysis, or sometimes simply the most elegant or beautiful known.\n\nA closely related notion is a structure map or structure morphism; the map that comes with the given structure on the object. They are also sometimes called canonical maps.\n\nA canonical isomorphism is a canonical map that is also an isomorphism (i.e., invertible).\n\nIn some contexts, it is necessary to address an issue of \"choices\" of canonical maps or canonical isomorphisms; see prestack for a typical example.\n\n"}
{"id": "1401020", "url": "https://en.wikipedia.org/wiki?curid=1401020", "title": "Christoffel symbols", "text": "Christoffel symbols\n\nIn mathematics and physics, the Christoffel symbols are an array of numbers describing a metric connection. The metric connection is a specialization of the affine connection to surfaces or other manifolds endowed with a metric, allowing distances to be measured on that surface. In differential geometry, an affine connection can be defined without any reference to a metric, and many additional concepts follow: parallel transport, covariant derivatives, geodesics, etc. also do not require the concept of a metric. However, when a metric is available, these concepts can be directly tied to the \"shape\" of the manifold itself; that shape is determined by how the tangent space is attached to the cotangent space by the metric tensor. Abstractly, one would say that the manifold has an associated (orthonormal) frame bundle, with each \"frame\" being a possible choice of a coordinate frame. An invariant metric implies that the structure group of the frame bundle is the orthogonal group . As a result, such a manifold is necessarily a (pseudo-)Riemannian manifold. The Christoffel symbols provide a concrete representation of the connection of (pseudo-)Riemannian geometry in terms of coordinates on the manifold. Additional concepts, such as parallel transport, geodesics, etc. can then be expressed in terms of Christoffel symbols.\n\nIn general, there are an infinite number of metric connections for a given metric tensor; however, there is one, unique connection, the Levi-Civita connection, that is free of any torsion. It is very common in physics and general relativity to work almost exclusively with the Levi-Civita connection, by working in coordinate frames (called holonomic coordinates) where the torsion vanishes. For example, in Euclidean spaces, the Christoffel symbols describe how the local coordinate bases change from point to point.\n\nAt each point of the underlying -dimensional manifold, for any local coordinate system around that point, the Christoffel symbols are denoted for . Each entry of this array is a real number. Under \"linear\" coordinate transformations on the manifold, the Christoffel symbols transform like the components of a tensor, but under general coordinate transformations (diffeomorphisms) they do not. Most of the algebraic properties of the Christoffel symbols follow from their relationship to the affine connection; only a few follow from the fact that the structure group is the orthogonal group (or the Lorentz group for general relativity).\n\nChristoffel symbols are used for performing practical calculations. For example, the Riemann curvature tensor can be expressed entirely in terms of the Christoffel symbols and their first partial derivatives. In general relativity, the connection plays the role of the gravitational force field with the corresponding gravitational potential being the metric tensor. When the coordinate system and the metric tensor share some symmetry, many of the are zero.\n\nThe Christoffel symbols are named for Elwin Bruno Christoffel (1829–1900).\n\nThe definitions given below are valid for both Riemannian manifolds and pseudo-Riemannian manifolds, such as those of general relativity, with careful distinction being made between upper and lower indices (contra-variant and co-variant indices). The formulas hold for either sign convention, unless otherwise noted.\n\nEinstein summation convention is used in this article, with vectors indicated by bold font. The connection coefficients of the Levi-Civita connection (or pseudo-Riemannian connection) expressed in a coordinate basis are called \"Christoffel symbols\".\n\nGiven a coordinate system for on an -manifold , the tangent vectors\nwhere is the position vector, define what is referred to as the local basis of the tangent space to at each point of its domain. These can be used to define the metric tensor:\n\nand its inverse:\n\nwhich can in turn be used to define the dual basis:\n\nIn Euclidean space, the general definition given below for the Christoffel symbols of the second kind can be proven to be equivalent to:\n\nChristoffel symbols of the first kind can then be found via index juggling:\n\nRearranging, we see that:\n\nIn words, the arrays represented by the Christoffel symbols track how the basis changes from point to point. Symbols of the second kind decompose the change with respect to the basis, while symbols of the first kind decompose it with respect to the dual basis. These expressions fail as definitions when such decompositions are not possible - in particular, when the direction of change does not lie in the tangent space, which can occur on a curved surface. In this form, it easy to see the symmetry of the lower or last two indices:\n\nformula_8 and formula_9,\n\nfrom the definition of formula_10 and the fact that partial derivatives commute (as long as the manifold and coordinate system are well behaved).\n\nThe same numerical values for Christoffel symbols of the second kind also relate to derivatives of the dual basis, as seen in the expression:\n\nwhich we can rearrange as:\n\nThe Christoffel symbols of the first kind can be derived either from the Christoffel symbols of the second kind and the metric,\nor from the metric alone,\n\nAs an alternative notation one also finds\n\nIt is worth noting that .\n\nThe Christoffel symbols of the second kind are the connection coefficients—in a coordinate basis—of the Levi-Civita connection, and since this connection has zero torsion, then in this basis the connection coefficients are symmetric, i.e., . For this reason, a torsion-free connection is often called \"symmetric\".\n\nIn other words, the Christoffel symbols of the second kind\nholds, where is the Levi-Civita connection on taken in the coordinate direction (i.e., ) and where is a local coordinate (holonomic) basis.\n\nThe Christoffel symbols can be derived from the vanishing of the covariant derivative of the metric tensor :\n\nAs a shorthand notation, the nabla symbol and the partial derivative symbols are frequently dropped, and instead a semicolon and a comma are used to set off the index that is being used for the derivative. Thus, the above is sometimes written as\n\nUsing that the symbols are symmetric in the lower two indices, one can solve explicitly for the Christoffel symbols as a function of the metric tensor by permuting the indices and resumming:\n\nwhere is the inverse of the matrix , defined as (using the Kronecker delta, and Einstein notation for summation) . Although the Christoffel symbols are written in the same notation as tensors with index notation, they are not tensors,\nsince they do not transform like tensors under a change of coordinates.\n\nThe Christoffel symbols are most typically defined in a coordinate basis, which is the convention followed here. In other words, the name Christoffel symbols is reserved only for coordinate (i.e., holonomic) frames. However, the connection coefficients can also be defined in an arbitrary (i.e., nonholonomic) basis of tangent vectors by\nExplicitly, in terms of the metric tensor, this is\n\nwhere are the commutation coefficients of the basis; that is,\nwhere are the basis vectors and is the Lie bracket. The standard unit vectors in spherical and cylindrical coordinates furnish an example of a basis with non-vanishing commutation coefficients. The difference between the connection in such a frame, and the Levi-Civita connection is known as the contorsion tensor.\n\nWhen we choose the basis orthonormal: then . This implies that\nand the connection coefficients become antisymmetric in the first two indices:\nwhere\n\nIn this case, the connection coefficients are called the Ricci rotation coefficients.\n\nEquivalently, one can define Ricci rotation coefficients as follows:\nwhere is an orthonormal nonholonomic basis and its \"co-basis\".\n\nLet and be vector fields with components and . Then the th component of the covariant derivative of with respect to is given by\n\nHere, the Einstein notation is used, so repeated indices indicate summation over indices and contraction with the metric tensor serves to raise and lower indices:\n\nKeep in mind that and that , the Kronecker delta. The convention is that the metric tensor is the one with the lower indices; the correct way to obtain from is to solve the linear equations .\n\nThe statement that the connection is torsion-free, namely that\nis equivalent to the statement that—in a coordinate basis—the Christoffel symbol is symmetric in the lower two indices:\n\nThe index-less transformation properties of a tensor are given by pullbacks for covariant indices, and pushforwards for contravariant indices. The article on covariant derivatives provides additional discussion of the correspondence between index-free notation and indexed notation.\n\nThe covariant derivative of a vector field is\n\nThe covariant derivative of a scalar field is just\n\nand the covariant derivative of a covector field is\n\nThe symmetry of the Christoffel symbol now implies\n\nfor any scalar field, but in general the covariant derivatives of higher order tensor fields do not commute (see curvature tensor).\n\nThe covariant derivative of a type (2,0) tensor field is\n\nthat is,\n\nIf the tensor field is mixed then its covariant derivative is\n\nand if the tensor field is of type then its covariant derivative is\n\nTo find the contravariant derivative of a vector field, we must first transform \nit into a covariant derivative using the metric tensor\n\nUnder a change of variable from to , vectors transform as\n\nand so\n\nwhere the overline denotes the Christoffel symbols in the coordinate system. Note that the Christoffel symbol does not transform as a tensor, but rather as an object in the jet bundle. More precisely, the Christoffel symbols can be considered as functions on the jet bundle of the frame bundle of , independent of any local coordinate system. Choosing a local coordinate system determines a local section of this bundle, which can then be used to pull back the Christoffel symbols to functions on , though of course these functions then depend on the choice of local coordinate system.\n\nAt each point, there exist coordinate systems in which the Christoffel symbols vanish at the point. These are called (geodesic) normal coordinates, and are often used in Riemannian geometry.\n\nThe Christoffel symbols find frequent use in Einstein's theory of general relativity, where spacetime is represented by a curved 4-dimensional Lorentz manifold with a Levi-Civita connection. The Einstein field equations—which determine the geometry of spacetime in the presence of matter—contain the Ricci tensor, and so calculating the Christoffel symbols is essential. Once the geometry is determined, the paths of particles and light beams are calculated by solving the geodesic equations in which the Christoffel symbols explicitly appear.\n\n\n"}
{"id": "15433374", "url": "https://en.wikipedia.org/wiki?curid=15433374", "title": "Classical Hamiltonian quaternions", "text": "Classical Hamiltonian quaternions\n\nWilliam Rowan Hamilton invented quaternions, a mathematical entity in 1843. This article describes Hamilton's original treatment of quaternions, using his notation and terms. Hamilton's treatment is more geometric than the modern approach, which emphasizes quaternions' algebraic properties. Mathematically, quaternions discussed differ from the modern definition only by the terminology which is used.\n\nHamilton defined a quaternion as the quotient of two directed lines in tridimensional space; or, more generally, as the quotient of two vectors.\n\nA quaternion can be represented as the sum of a \"scalar\" and a \"vector\". It can also be represented as the product of its \"tensor\" and its \"versor\".\n\nHamilton invented the term \"scalars\" for the real numbers, because they span the \"scale of progression from positive to negative infinity\" or because they represent the \"comparison of positions upon one common scale\". Hamilton regarded ordinary scalar algebra as the science of pure time.\n\nHamilton defined a vector as \"a right line ... having not only length but also direction\". Hamilton derived the word \"vector\" from the Latin \"vehere\", to carry.\n\nHamilton conceived a vector as the \"difference of its two extreme points.\" For Hamilton, a vector was always a three-dimensional entity, having three co-ordinates relative to any given co-ordinate system, including but not limited to both polar and rectangular systems. He therefore referred to vectors as \"triplets\".\n\nHamilton defined addition of vectors in geometric terms, by placing the origin of the second vector at the end of the first. He went on to define vector subtraction.\n\nBy adding a vector to itself multiple times, he defined multiplication of a vector by an integer, then extended this to division by an integer, and multiplication (and division) of a vector by a rational number. Finally, by taking limits, he defined the result of multiplying a vector α by any scalar \"x\" as a vector β with the same direction as α if \"x\" is positive; the opposite direction to α if \"x\" is negative; and a length that is |\"x\"| times the length of α.\n\nThe quotient of two parallel or anti-parallel vectors is therefore a scalar with absolute value equal to the ratio of the lengths of the two vectors; the scalar is positive if the vectors are parallel and negative if they are anti-parallel.\n\nA unit vector is a vector of length one. Examples of unit vectors include i, j and k.\n\nHamilton defined \"tensor\" as a positive numerical quantity, or, more properly, signless number. A tensor can be thought of as a positive scalar. The \"tensor\" can be thought of as representing a \"stretching factor.\"\n\nHamilton introduced the term tensor in his first book, Lectures on Quaternions, based on lectures he gave shortly after his invention of the quaternions:\n\nEach quaternion has a tensor, which is a measure of its magnitude (in the same way as the length of a vector is a measure of a vectors' magnitude). When a quaternion is defined as the quotient of two vectors, its tensor is the ratio of the lengths of these vectors.\n\nA versor is a quaternion with a tensor of 1. Alternatively, a versor can be defined as the quotient of two equal-length vectors.\n\nIn general a versor defines all of the following: a directional axis; the plane normal to that axis; and an angle of rotation.\n\nWhen a versor and a vector which lies in the plane of the versor are multiplied, the result is a new vector of the same length but turned by the angle of the versor.\n\nSince every unit vector can be thought of as a point on a unit sphere, and since a versor can be thought of as the quotient of two vectors, a versor has a representative great circle arc, called a vector arc, connecting these two points, drawn from the divisor or lower part of quotient, to the dividend or upper part of the quotient.\n\nWhen the arc of a versor has the magnitude of a right angle, then it is called a right versor, a \"right radial\" or \"quadrantal versor\".\n\nTwo special degenerate versor cases, called the unit-scalars These two scalars, negative and positive unity can be thought of as scalar quaternions. These two scalars are special limiting cases, corresponding to versors with angles of either zero or π.\n\nUnlike other versors, these two cannot be represented by a unique arc. The arc of one is a single point, and minus one can be represented by an infinite number of arcs, because there are an infinite number of shortest lines between antipodal points of a sphere.\n\nEvery quaternion can be decomposed into a scalar and a vector.\n\nThese two operations S and V are called \"take the Scalar of\" and \"take the vector of\" a quaternion. The vector part of a quaternion is also called the right part.\n\nEvery quaternion is equal to a versor multiplied by the tensor of the quaternion. Denoting the versor of a quaternion by\n\nand the tensor of a quaternion by\n\nwe have\n\nA right quaternion is a quaternion whose scalar component is zero,\n\nThe angle of a right quaternion is 90 degrees. A right quaternion can also be thought of as a vector plus a zero scalar. Right quaternions may be put in what was called the standard trinomial form. For example, if Q is a right quaternion, it may be written as:\n\nFour operations are of fundamental importance in quaternion notation.\nIn particular it is important to understand that there is a single operation of multiplication, a single operation of division, and a single operations of addition and subtraction. This single multiplication operator can operate on any of the types of mathematical entities. Likewise every kind of entity can be divided, added or subtracted from any other type of entity. Understanding the meaning of the subtraction symbol is critical in quaternion theory, because it leads to an understanding of the concept of a vector.\n\nThe two ordinal operations in classical quaternion notation were addition and subtraction or + and −.\n\nThese marks are:\n\n\"...characteristics of synthesis and analysis of a state of progression, according as this state is considered as being derived from, or compared with, some other state of that progression.\"\n\nSubtraction is a type of analysis called ordinal analysis\n\n...let space be now regarded as the field of progression which is to be studied, and POINTS as \"states\" of that progression. ...I am led to regard the word \"Minus,\" or the mark −, in geometry, as the sign or characteristic of analysis of one geometric position (in space), as compared with another (such) position. The comparison of one mathematical point with another with a view to the determination of what may be called their ordinal relation, or their relative position in space...\n\nThe first example of subtraction is to take the point A to represent the earth, and the point B to represent the sun, then an arrow drawn from A to B represents the act of moving or vection from A to B.\n\nthis represents the first example in Hamilton's lectures of a vector. In this case the act of traveling from the earth to the moon.\n\nAddition is a type of analysis called ordinal synthesis.\n\nVectors and scalars can be added. When a vector is added to a scalar, a completely different entity, a quaternion is created.\n\nA vector plus a scalar is always a quaternion even if the scalar is zero. If the scalar added to the vector is zero then the new quaternion produced is called a right quaternion. It has an angle characteristic of 90 degrees.\n\nThe two Cardinal operations in quaternion notation are geometric multiplication and geometric division and can be written:\nIt is not required to learn the following more advanced terms in order to use division and multiplication.\n\nDivision is a kind of analysis called cardinal analysis. Multiplication is a kind of synthesis called cardinal synthesis\n\nClassically, the quaternion was viewed as the ratio of two vectors, sometimes called a geometric fraction.\n\nIf OA and OB represent two vectors drawn from the origin O to two other points A and B, then the geometric fraction was written as\n\nAlternately if the two vectors are represented by α and β the quotient was written as\n\nor\n\nHamilton asserts: \"The quotient of two vectors is generally a quaternion\". \"Lectures on Quaternions\" also first introduces the concept of a quaternion as the quotient of two vectors:\n\nLogically and by definition,\n\nif formula_10\n\nthen formula_11.\n\nIn Hamilton's calculus the product is not commutative, i.e., the order of the variables is of great importance. If the order of q and β were to be reversed the result would not in general be α. The quaternion q can be thought of as an operator that changes β into α, by first rotating it, formerly an act of \"version\" and then changing the length of it, formerly call an act of \"tension\".\n\nAlso by definition the quotient of two vectors is equal to the numerator times the reciprocal of the denominator. Since multiplication of vectors is not commutative, the order cannot be changed in the following expression.\n\nAgain the order of the two quantities on the right hand side is significant.\n\nHardy presents the definition of division in terms of mnemonic cancellation rules. \"Canceling being performed by an upward right hand stroke\".\n\nIf alpha and beta are vectors and q is a quaternion such that\n\nthen formula_14\n\nand formula_15\n\nand\n\nAn important way to think of q is as an operator that changes β into α, by first rotating it (\"version\") and then changing its length (tension).\n\nThe results of the using the division operator on i, j and k was as follows.\n\nThe reciprocal of a unit vector is the vector reversed.\n\nBecause a unit vector and its reciprocal are parallel to each other but point in opposite directions, the product of a unit vector and its reciprocal have a special case commutative property, for example if a is any unit vector then:\n\nHowever, in the more general case involving more than one vector (whether or not it is a unit vector) the commutative property does not hold. For example:\n\nThis is because k/i is carefully defined as:\n\nSo that:\n\nhowever\n\nWhile in general the quotient of two vectors is a quaternion, If α and β are two parallel vectors then the quotient of these two vectors is a scalar. For example, if\n\nformula_29,\n\nand formula_30 then\n\nWhere a/b is a scalar.\n\nThe quotient of two vectors is in general the quaternion:\n\nWhere α and β are two non-parallel vectors, φ is that angle between them, and e is a unit vector perpendicular to the plane of the vectors α and β, with its direction given by the standard right hand rule.\n\nClassical quaternion notation had only one concept of multiplication. Multiplication of two real numbers, two imaginary numbers or a real number by an imaginary number in the classical notation system was the same operation.\n\nMultiplication of a scalar and a vector was accomplished with the same single multiplication operator; multiplication of two vectors of quaternions used this same operation as did multiplication of a quaternion and a vector or of two quaternions.\n\nWhen two quantities are multiplied the first quantity is called the factor, the second quantity is called the faciend and the result is called the factum.\n\nIn classical notation, multiplication was distributive. Understanding this makes it simple to see why the product of two vectors in classical notation produced a quaternion.\n\nUsing the quaternion multiplication table we have:\n\nThen collecting terms:\n\nThe first three terms are a scalar.\n\nLetting\n\nSo that the product of two vectors is a quaternion, and can be written in the form:\n\nThe product of two right quaternions is generally a quaternion.\n\nLet α and β be the right quaternions that result from taking the vectors of two quaternions:\n\nTheir product in general is a new quaternion represented here by r. This product is not ambiguous because classical notation has only one product.\n\nLike all quaternions r may now be decomposed into its vector and scalar parts.\n\nThe terms on the right are called \"scalar of the product\", and the \"vector of the product\" of two right quaternions.\n\nTwo important operations in two the classical quaternion notation system were S(q) and V(q) which meant take the scalar part of, and take the imaginary part, what Hamilton called the vector part of the quaternion. Here S and V are operators acting on q. Parenthesis can be omitted in these kinds of expressions without ambiguity. Classical notation:\n\nHere, \"q\" is a quaternion. \"Sq\" is the scalar of the quaternion while Vq is the vector of the quaternion.\n\nK is the conjugate operator. The conjugate of a quaternion is a quaternion obtained by multiplying the vector part of the first quaternion by minus one.\n\nIf\n\nthen\n\nThe expression\n\nmeans, assign the quaternion r the value of the conjugate of the quaternion q.\n\nT is the tensor operator. It returns a kind of number called a \"tensor\".\n\nThe tensor of a positive scalar is that scalar. The tensor of a negative scalar is the absolute value of the scalar (i.e., without the negative sign). For example:\n\nThe tensor of a vector is by definition the length of the vector. For example, if:\n\nThen\n\nThe tensor of a unit vector is one. Since the versor of a vector is a unit vector, the tensor of the versor of any vector is always equal to unity. Symbolically:\n\nA quaternion is by definition the quotient of two vectors and the tensor of a quaternion is by definition the quotient of the tensors of these two vectors. In symbols:\n\nFrom this definition it can be shown that a useful formula for the tensor of a quaternion is:\n\nIt can also be proven from this definition that another formula to obtain the tensor of a quaternion is from the common norm, defined as the product of a quaternion and its conjugate. The square root of the common norm of a quaternion is equal to its tensor.\n\nA useful identity is that the square of the tensor of a quaternion is equal to the tensor of the square of a quaternion, so that parenthesis may be omitted.\n\nAlso, the tensors of conjugate quaternions are equal.\n\nThe tensor of a quaternion is now called its norm.\n\nTaking the angle of a non-scalar quaternion, resulted in a value greater than zero and less than π.\n\nWhen a non-scalar quaternion is viewed as the quotient of two vectors, then the axis of the quaternion is a unit vector perpendicular to the plane of the two vectors in this original quotient, in a direction specified by the right hand rule. The angle is the angle between the two vectors.\n\nIn symbols,\n\nIf\n\nthen its reciprocal is defined as\n\nformula_65\n\nThe expression:\n\nReciprocals have many important applications, for example rotations, particularly when q is a versor. A versor has an easy formula for its reciprocal.\n\nIn words the reciprocal of a versor is equal to its conjugate. The dots between operators show the order of the operations, and also help to indicate that S and U for example, are two different operations rather than a single operation named SU.\n\nThe product of a quaternion with its conjugate is its common norm.\n\nThe operation of taking the common norm of a quaternion is represented with the letter N. By definition the common norm is the product of a quaternion with its conjugate. It can be proven that common norm is equal to the square of the tensor of a quaternion. However this proof does not constitute a definition. Hamilton gives exact, independent definitions of both the common norm and the tensor. This norm was adopted as suggested from the theory of numbers, however to quote Hamilton \"they will not often be wanted\". The tensor is generally of greater utility. The word norm does not appear in \"Lectures on Quaternions\", and only twice in the table of contents of \"Elements of Quaternions\".\n\nIn symbols:\n\nThe common norm of a versor is always equal to positive unity.\n\nIn classical quaternion literature the equation\n\nwas thought to have infinitely many solutions that were called geometrically real.\nThese solutions are the unit vectors that form the surface of a unit sphere.\n\nA geometrically real quaternion is one that can be written as a linear combination of \"i\", \"j\" and \"k\", such that the squares of the coefficients add up to one. Hamilton demonstrated that there had to be additional roots of this equation in addition to the geometrically real roots. Given the existence of the imaginary scalar, a number of expressions can be written and given proper names. All of these were part of Hamilton's original quaternion calculus. In symbols:\n\nwhere q and q′ are real quaternions, and the square root of minus one is the imaginary of ordinary algebra, and are called an imaginary or symbolical roots and not a geometrically real vector quantity.\n\nGeometrically Imaginary quantities are additional roots of the above equation of a purely symbolic nature. In article 214 of \"Elements\" Hamilton proves that if there is an i, j and k there also has to be another quantity h which is an imaginary scalar, which he observes should have already occurred to anyone who had read the preceding articles with attention. Article 149 of \"Elements\" is about Geometrically Imaginary numbers and includes a footnote introducing the term \"biquaternion\". The terms \"imaginary of ordinary algebra\" and \"scalar imaginary\" are sometimes used for these geometrically imaginary quantities.\n\n\"Geometrically Imaginary\" roots to an equation were interpreted in classical thinking as geometrically impossible situations. Article 214 of elements of quaternions explores the example of the equation of a line and a circle that do not intersect, as being indicated by the equation having only a geometrically imaginary root.\n\nIn Hamilton's later writings he proposed using the letter h to denote the imaginary scalar\n\nOn page 665 of \"Elements of Quaternions\" Hamilton defines a biquaternion to be a quaternion with complex number coefficients. The scalar part of a biquaternion is then a complex number called a biscalar. The vector part of a biquaternion is a bivector consisting of three complex components. The biquaternions are then the complexification of the original (real) quaternions.\n\nHamilton invented the term \"associative\" to distinguish between the imaginary scalar (known by now as a complex number) which is both commutative and associative, and four other possible roots of negative unity which he designated L, M, N and O, mentioning them briefly in appendix B of \"Lectures on Quaternions\" and in private letters. However, non-associative roots of minus one do not appear in \"Elements of Quaternions\". Hamilton died before he worked on these strange entities. His son claimed it to be a \"bow for another Ulysses\".\n\n\n"}
{"id": "6497220", "url": "https://en.wikipedia.org/wiki?curid=6497220", "title": "Computational complexity of mathematical operations", "text": "Computational complexity of mathematical operations\n\nThe following tables list the computational complexity of various algorithms for common mathematical operations.\n\nHere, complexity refers to the time complexity of performing computations on a multitape Turing machine. See big O notation for an explanation of the notation used.\n\nNote: Due to the variety of multiplication algorithms, \"M\"(\"n\") below stands in for the complexity of the chosen multiplication algorithm.\n\nMany of the methods in this section are given in Borwein & Borwein.\n\nThe elementary functions are constructed by composing arithmetic operations, the exponential function (exp), the natural logarithm (log), trigonometric functions (sin, cos), and their inverses. The complexity of an elementary function is equivalent to that of its inverse, since all elementary functions are analytic and hence invertible by means of Newton's method. In particular, if either exp or log in the complex domain can be computed with some complexity, then that complexity is attainable for all other elementary functions.\n\nBelow, the size \"n\" refers to the number of digits of precision at which the function is to be evaluated.\n\nIt is not known whether O(\"M\"(\"n\") log \"n\") is the optimal complexity for elementary functions. The best known lower bound is the trivial bound Ω(\"M\"(\"n\")).\n\nThis table gives the complexity of computing approximations to the given constants to \"n\" correct digits.\nAlgorithms for number theoretical calculations are studied in computational number theory.\n\nThe following complexity figures assume that arithmetic with individual elements has complexity \"O\"(1), as is the case with fixed-precision floating-point arithmetic or operations on a finite field.\n\nIn 2005, Henry Cohn, Robert Kleinberg, Balázs Szegedy, and Chris Umans showed that either of two different conjectures would imply that the exponent of matrix multiplication is 2.\n\nBecause of the possibility of blockwise inverting a matrix, where an inversion of an matrix requires inversion of two half-sized matrices and six multiplications between two half-sized matrices, and since matrix multiplication has a lower bound of operations, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.\n\n"}
{"id": "29018709", "url": "https://en.wikipedia.org/wiki?curid=29018709", "title": "Correlation coefficient", "text": "Correlation coefficient\n\nA correlation coefficient is a numerical measure of some type of correlation, meaning a statistical relationship between two variables. The variables may be two columns of a given data set of observations, often called a sample, or two components of a multivariate random variable with a known distribution.\n\nSeveral types of correlation coefficient exist, each with their own definition and own range of usability and characteristics. They all assume values in the range from −1 to +1, where +1 indicates the strongest possible agreement and −1 the strongest possible disagreement. As tools of analysis, correlation coefficients present certain problems, including the propensity of some types to be distorted by outliers and the possibility of incorrectly being used to infer a causal relationship between the variables.\n\nThe Pearson product-moment correlation coefficient, also known as \"r\", \"R\", or Pearson's \"r\", is a measure of the strength and direction of the linear relationship between two variables that is defined as the covariance of the variables divided by the product of their standard deviations. This is the best known and most commonly used type of correlation coefficient; when the term \"correlation coefficient\" is used without further qualification, it usually refers to the Pearson product-moment correlation coefficient\n\nIntraclass correlation (ICC) is a descriptive statistic that can be used when quantitative measurements are made on units that are organized into groups; it describes how strongly units in the same group resemble each other.\n\nRank correlation is a measure of the relationship between the rankings of two variables or two rankings of the same variable:\n\n"}
{"id": "20644971", "url": "https://en.wikipedia.org/wiki?curid=20644971", "title": "Count On", "text": "Count On\n\nCount On is a major mathematics education project in the United Kingdom which was announced by education secretary David Blunkett at the end of 2000. It was the follow-on to Maths Year 2000 which was the UK's contribution to UNICEF's World Mathematical Year.\n\nCount On had two main strands:\n\nThe MathFests were run largely by MatheMagic and the University of York.\n\nThe project has now been handed over to the NCETM.\n\n\"Count On\" and \"Maths Year 2000\" were some of the first big Popularisation of Mathematics projects. Others are listed below.\n\n"}
{"id": "16175342", "url": "https://en.wikipedia.org/wiki?curid=16175342", "title": "De divina proportione", "text": "De divina proportione\n\nDe divina proportione (\"On the Divine Proportion\") is a book on mathematics written by Luca Pacioli and illustrated by Leonardo da Vinci, composed around 1498 in Milan and first printed in 1509. Its subject was mathematical proportions (the title refers to the golden ratio) and their applications to geometry, visual art through perspective, and architecture. The clarity of the written material and Leonardo's excellent diagrams helped the book to achieve an impact beyond mathematical circles, popularizing contemporary geometric concepts and images.\n\nThe book consists of three separate manuscripts, which Pacioli worked on between 1496 and 1498.\n\nThe first part, \"Compendio divina proportione\" (\"Compendium on the Divine Proportion\"), studies the golden ratio from a mathematical perspective (following the relevant work of Euclid) and explores its applications to various arts, in seventy-one chapters. It also contains a discourse on the regular and semiregular polyhedra, as well as a discussion of the use of geometric perspective by painters such as Piero della Francesca, Melozzo da Forlì and Marco Palmezzano.\n\nThe second part, \"Trattato dell'architettura\" (\"Treatise on Architecture\"), discusses the ideas of Vitruvius (from his \"De architectura\") on the application of mathematics to architecture in twenty chapters. The text compares the proportions of the human body to those of artificial structures, with examples from classical Greco-Roman architecture.\n\nThe third part, \"Libellus in tres partiales divisus\" (\"Book divided into three parts\"), is mainly an Italian translation of Piero della Francesca's Latin writings \"On [the] Five Regular Solids\" (\"De quinque corporibus regularibus\") and mathematical examples. In 1550 Giorgio Vasari wrote a biography of della Francesca, in which he accused Pacioli of plagiarism and claimed that he stole della Francesca's work on perspective, on arithmetic and on geometry.\n\nAfter these three parts are appended two sections of illustrations, the first showing twenty-three capital letters drawn with a ruler and compass by Pacioli and the second with some sixty illustrations in woodcut after drawings by Leonardo da Vinci. Leonardo drew the illustrations of the regular solids while he lived with and took mathematics lessons from Pacioli. Leonardo's drawings are probably the first illustrations of skeletonic solids which allowed an easy distinction between front and back.\n\nPacioli produced three manuscripts of the treatise by different scribes. He gave the first copy with a dedication to the Duke of Milan, Ludovico il Moro; this manuscript is now preserved in Switzerland at the Bibliothèque de Genève in Geneva. A second copy was donated to Galeazzo da Sanseverino and now rests at the Biblioteca Ambrosiana in Milan. The third, which has gone missing, was given to Pier Soderini, the Gonfaloniere of Florence. On 1 June 1509 the first printed edition was published in Venice by Paganino Paganini; it has since been reprinted several times.\nThe book was displayed as part of an exhibition in Milan between October 2005 and October 2006 together with the Codex Atlanticus. The \"M\" logo used by the Metropolitan Museum of Art in New York is adapted from one in \"De divina proportione\".\n\n\n"}
{"id": "3698082", "url": "https://en.wikipedia.org/wiki?curid=3698082", "title": "Entitative graph", "text": "Entitative graph\n\nAn entitative graph is an element of the diagrammatic syntax for logic that Charles Sanders Peirce developed under the name of qualitative logic beginning in the 1880s, taking the coverage of the formalism only as far as the propositional or sentential aspects of logic are concerned. See 3.468, 4.434, and 4.564 in Peirce's \"Collected Papers\".\n\nThe syntax is:\n\nThe semantics are:\n\nA \"proof\" manipulates a graph, using a short list of rules, until the graph is reduced to an empty cut or the blank page. A graph that can be so reduced is what is now called a tautology (or the complement thereof). Graphs that cannot be simplified beyond a certain point are analogues of the satisfiable formulas of first-order logic.\n\nPeirce soon abandoned the entitative graphs for the existential graphs, whose sentential (\"alpha\") part is dual to the entitative graphs. He developed the existential graphs until they became another formalism for what are now termed first-order logic and normal modal logic.\n\nThe primary algebra of G. Spencer-Brown is isomorphic to the entitative graphs.\n\n\n"}
{"id": "23831652", "url": "https://en.wikipedia.org/wiki?curid=23831652", "title": "Erdős Prize", "text": "Erdős Prize\n\nThe Anna and Lajos Erdős Prize in Mathematics is a prize given by the Israel Mathematical Union to an Israeli mathematician (in any field of mathematics and computer science), \"with preference to candidates up to the age of 40.\" The prize was established by Paul Erdős in 1977 in honor of his parents, and is awarded annually or biannually. The name was changed from \"Erdős Prize\" in 1996, after Erdős's death, to reflect his original wishes.\n\nSource: Israel Mathematical Union\n"}
{"id": "5280515", "url": "https://en.wikipedia.org/wiki?curid=5280515", "title": "Exceptional object", "text": "Exceptional object\n\nMany branches of mathematics study objects of a given type and prove a classification theorem. A common theme is that the classification results in a number of series of objects and a finite number of exceptions that do not fit into any series. These are known as exceptional objects. Frequently these exceptional objects play a further and important role in the subject. Furthermore, the exceptional objects in one branch of mathematics are often related to the exceptional objects in others.\n\nA related phenomenon is exceptional isomorphism, when two series are in general different, but agree for some small values.\n\nThe prototypical examples of exceptional objects arise in the classification of regular polytopes. In two dimensions there is a series of regular \"n\"-gons for \"n\" ≥ 3. In every dimension above 2 we find analogues of the cube, tetrahedron and octahedron. In three dimensions we find two more regular polyhedra — the dodecahedron (12-hedron) and the icosahedron (20-hedron) — making five Platonic solids. In four dimensions we have a total of six regular polytopes including the 120-cell, the 600-cell and the 24-cell. There are no other regular polytopes; in higher dimensions the only regular polytopes are of the hypercube, simplex, orthoplex series. In all dimensions combined, there are therefore three series and five exceptional polytopes.\n\nThe pattern is similar if non-convex polytopes are included. In two dimensions there is a regular star polygon for every rational number \"p\"/\"q\" > 2. In three dimensions there are four Kepler–Poinsot polyhedra, and in four dimensions ten Schläfli–Hess polychora; in higher dimensions there are no non-convex regular figures.\n\nThese can be generalized to tessellations of other spaces, especially uniform tessellations, notably tilings of Euclidean space (honeycombs), which have exceptional objects, and tilings of hyperbolic space. There are various exceptional objects in dimension below 6, but in dimension 6 and above the only regular polyhedra/tilings/hyperbolic tilings are the simplex, hypercube, cross-polytope, and hypercube lattice.\n\nRelated to tilings and the regular polyhedra, there are exceptional Schwarz triangles (triangles that tile the sphere, or more generally Euclidean plane or hyperbolic plane via their triangle group of reflections in their edges), particularly the Möbius triangles. In the sphere there are 3 Möbius triangles (and 1 1-parameter family), corresponding to the 3 exceptional Platonic solid groups, while in the Euclidean plane there are 3 Möbius triangles, corresponding to the 3 special triangles: 60-60-60 (equilateral), 45-45-90 (isosceles right), and 30-60-90. There are additional exceptional Schwarz triangles in the sphere and Euclidean plane. By contrast, in the hyperbolic plane there is a 3-parameter family of Möbius triangles, and none exceptional.\n\nThe finite simple groups have been classified into a number of series as well as 26 sporadic groups. Of these, 20 are subgroups or subquotients of the monster group, referred to as the \"Happy Family\", while 6 are not, and are referred to as \"pariahs\".\n\nSeveral of the sporadic groups are related to the Leech lattice, most notably the Conway group Co, which is the automorphism group of the Leech lattice, quotiented out by its center.\n\nThere are only three finite-dimensional associative division algebras over the reals — the real numbers, the complex numbers and the quaternions. The only non-associative division algebra is the algebra of octonions. The octonions are connected to a wide variety of exceptional objects. For example, the exceptional formally real Jordan algebra is the Albert algebra of 3 by 3 self-adjoint matrices over the octonions.\n\nThe simple Lie groups form a number of series (classical Lie groups) labelled A, B, C and D. In addition there are the exceptional groups G (the automorphism group of the octonions), F, E, E, E. These last four groups can be viewed as the symmetry groups of projective planes over O, C⊗O, H⊗O and O⊗O respectively, where O is the octonions and the tensor products are over the reals.\n\nThe classification of Lie groups corresponds to the classification of root systems and thus the exceptional Lie groups correspond to exceptional root systems and exceptional Dynkin diagrams.\n\nThere are a few exceptional objects with supersymmetry. The classification of superalgebras by Kac and Tierry-Mieg indicates that the Lie superalgebras G(3) in 31 dimensions and F(4) in 40 dimensions, and the Jordan superalgebras K and K, are examples of exceptional objects.\n\nUp to isometry there is only one even unimodular lattice in 15 dimensions or less — the E lattice. Up to dimension 24 there is only one even unimodular lattice without roots, the Leech lattice. Three of the sporadic simple groups were discovered by Conway while investigating the automorphism group of the Leech lattice. For example, Co is the automorphism group itself modulo ±1. The groups Co and Co, as well as a number of other sporadic groups, arise as stabilisers of various subsets of the Leech lattice.\n\nSome codes also stand out as exceptional objects, in particular the perfect binary Golay code which is closely related to the Leech lattice. The Mathieu group formula_1, one of the sporadic simple groups, is the group of automorphisms of the extended binary Golay code, and four more of the sporadic simple groups arise as various types of stabilizer subgroup of formula_1.\n\nAn exceptional block design is the Steiner system S(5,8,24) whose automorphism group is the sporadic simple Mathieu group formula_1.\n\nThe codewords of the extended binary Golay code have a length of 24 bits and have weights 0, 8, 12, 16, or 24. This code can correct up to three errors. So every 24-bit word with weight 5 can be corrected to a codeword with weight 8. The bits of a 24-bit word can be thought of as specifying the possible subsets of a 24 element set. So the extended binary Golay code gives a unique 8 element subset for each 5 element subset. In fact, it defines S(5,8,24).\n\nCertain families of groups generically have a certain outer automorphism group, but in particular cases they have other, exceptional outer automorphisms.\n\nAmong families of finite simple groups, the only example is in the automorphisms of the symmetric and alternating groups: for formula_4 the alternating group formula_5 has one outer automorphism (corresponding to conjugation by an odd element of formula_6) and the symmetric group formula_6 has no outer automorphisms. However, for formula_8 there is an exceptional outer automorphism of formula_9 (of order 2), and correspondingly, the outer automorphism group of formula_10 is not formula_11 (the group of order 2) but rather formula_12, the Klein four-group.\n\nIf one instead considers A as the (isomorphic) projective special linear group PSL(2,9), then the outer automorphism is not exceptional; thus the exceptionalness can be seen as due to the exceptional isomorphism formula_13 This exceptional outer automorphism is realized inside of the Mathieu group M and similarly, M acts on a set of 12 elements in 2 different ways.\n\nAmong Lie groups, the spin group Spin(8) has an exceptionally large outer automorphism group (namely formula_14), which corresponds to the exceptional symmetries of the Dynkin diagram D. This phenomenon is referred to as \"triality.\"\n\nThe exceptional symmetry of the D diagram also gives rise to the Steinberg groups.\n\nThe Kervaire invariant is an invariant of a (4\"k\"+2)-dimensional manifold that measures whether the manifold could be surgically converted into a sphere. This invariant evaluates to 0 if the manifold can be converted to a sphere, and 1 otherwise. More specifically, the Kervaire invariant applies to a framed manifold, that is, to a manifold equipped with an embedding into Euclidean space and a trivialization of the normal bundle. The Kervaire invariant problem is the problem of determining in which dimensions the Kervaire invariant can be nonzero. For differentiable manifolds, this can happen in dimensions 2, 6, 14, 30, 62, and possibly 126, and in no other dimensions. The final case of dimension 126 remains open. These five or six framed cobordism classes of manifolds having Kervaire invariant 1 are exceptional objects related to exotic spheres. The first three cases are related to the complex numbers, quaternions and octonions respectively: a manifold of Kervaire invariant 1 can be constructed as the product of two spheres, with its exotic framing determined by the normed division algebra.\n\nDue to similarities of dimensions, it is conjectured that the remaining cases (dimensions 30, 62 and 126) are related to the Rosenfeld projective planes, which are defined over algebras constructed from the octonions. Specifically, it has been conjectured that there is a construction that takes these projective planes and produces a manifold with nonzero Kervaire invariant in two dimensions lower, but this remains unconfirmed.\n\nIn quantum information theory, there exist structures known as SIC-POVMs or SICs, which correspond to maximal sets of complex equiangular lines. Some of the known SICs—those in vector spaces of 2 and 3 dimensions, as well as certain solutions in 8 dimensions—are considered exceptional objects and called \"sporadic SICs\". They differ from the other known SICs in ways that involve their symmetry groups, the Galois theory of the numerical values of their vector components, and so forth. The sporadic SICs in dimension 8 are related to the integral octonions.\n\nNumerous connections have been observed between some, though not all, of these exceptional objects. Most common are objects related to 8 and 24 dimensions, noting that 24 = 8 · 3. By contrast, the pariah groups stand apart, as the name suggests.\n\nExceptional objects related to the number 8 include the following.\nLikewise, exceptional objects related to the number 24 include the following.\n\nThese objects are connected to various other phenomena in math which may be considered surprising but not themselves \"exceptional\". For example, in algebraic topology, 8-fold real Bott periodicity can be seen as coming from the octonions. In the theory of modular forms, the 24-dimensional nature of the Leech lattice underlies the presence of 24 in the formulas for the Dedekind eta function and the modular discriminant, which connection is deepened by Monstrous moonshine, a development that related modular functions to the Monster group.\n\nIn string theory and superstring theory we often find that particular dimensions are singled out as a result of exceptional algebraic phenomena. For example, bosonic string theory requires a spacetime of dimension 26 which is directly related to the presence of 24 in the Dedekind eta function. Similarly, the possible dimensions of supergravity are related to the dimensions of the division algebras.\n\nMany of the exceptional objects in mathematics and physics have been found to be connected to each other. Developments such as the Monstrous moonshine conjectures show how, for example, the Monster group is connected to string theory. The theory of modular forms shows how the algebra E is connected to the Monster group. (In fact, well before the proof of the Monstrous moonshine conjecture, the elliptic \"j\"-function was discovered to encode the representations of E.) Other interesting connections include how the Leech lattice is connected via the Golay code to the adjacency matrix of the dodecahedron (another exceptional object). Below is a mind map showing how some of the exceptional objects in mathematics and mathematical physics are related.\n\nThe connections can partly be explained by thinking of the algebras as a tower of lattice vertex operator algebras. It just so happens that the vertex algebras at the bottom are so simple that they are isomorphic to familiar non-vertex algebras. Thus the connections can be seen simply as the consequence of some lattices being sub-lattices of others.\n\nThe Jordan superalgebras are a parallel set of exceptional objects with supersymmetry. These are the Lie superalgebras which are related to Lorentzian lattices. This subject is less explored, and the connections between the objects are less well established. There are new conjectures parallel to the Monstrous moonshine conjectures for these super-objects, involving different sporadic groups.\n\n\"Exceptional\" object is reserved for objects that are unusual, meaning rare, the exception, not for \"unexpected\" or \"non-standard\" objects. These unexpected-but-typical (or common) phenomena are generally referred to as pathological, such as nowhere differentiable functions, or \"exotic\", as in exotic spheres — there are exotic spheres in arbitrarily high dimension (not only a finite set of exceptions), and in many dimensions most (differential structures on) spheres are exotic.\n\nExceptional objects must be distinguished from \"extremal\" objects: those that fall in a family and are the most extreme example by some measure are of interest, but not unusual in the way exceptional objects are. For example, the golden ratio \"φ\" has the simplest continued fraction approximation, and accordingly is most difficult to approximate by rationals; however, it is but one of infinitely many such quadratic numbers (continued fractions).\n\nSimilarly, the (2,3,7) Schwarz triangle is the smallest hyperbolic Schwarz triangle, and the associated (2,3,7) triangle group is of particular interest, being the universal Hurwitz group, and thus being associated with the Hurwitz curves, the maximally symmetric algebraic curves. However, it falls in a family of such triangles ((2,4,7), (2,3,8), (3,3,7), etc.), and while the smallest, is not exceptional or unlike the others.\n\n"}
{"id": "7186647", "url": "https://en.wikipedia.org/wiki?curid=7186647", "title": "Facet (geometry)", "text": "Facet (geometry)\n\nIn geometry, a facet is a feature of a polyhedron, polytope, or related geometric structure, generally of dimension one less than the structure itself.\n\n"}
{"id": "22697171", "url": "https://en.wikipedia.org/wiki?curid=22697171", "title": "Georg Cantor's first set theory article", "text": "Georg Cantor's first set theory article\n\nGeorg Cantor published his first set theory article in 1874, and it contains the first theorems of transfinite set theory, which studies infinite sets and their properties. One of these theorems is \"Cantor's revolutionary discovery\" that the set of all real numbers is uncountably, rather than countably, infinite. This theorem is proved using Cantor's first uncountability proof, which differs from the more familiar proof using his diagonal argument. The title of the article, \"On a Property of the Collection of All Real Algebraic Numbers\" (\"Ueber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen\"), refers to its first theorem: the set of real algebraic numbers is countable. In 1879, Cantor modified his uncountability proof by using the topological notion of a set being dense in an interval.\n\nCantor's 1874 article also contains a proof of the existence of transcendental numbers. As early as 1930, mathematicians have disagreed on whether this proof is constructive or non-constructive. Books as recent as 2014 and 2015 indicate that this disagreement has not been resolved. Since Cantor's proof either constructs transcendental numbers or does not, an analysis of his article can determine whether his proof is constructive or non-constructive. Cantor's correspondence with Richard Dedekind shows the development of his ideas and reveals that he had a choice between two proofs, one that uses the uncountability of the real numbers and one that does not.\n\nHistorians of mathematics have examined Cantor's article and the circumstances in which it was written. For example, they have discovered that Cantor was advised to leave out his uncountability theorem in the article he submitted; he added it during proofreading. They have traced this and other facts about the article to the influence of Karl Weierstrass and Leopold Kronecker. Historians have also studied Dedekind's contributions to the article, including his contributions to the theorem on the countability of the real algebraic numbers. In addition, they have looked at the article's legacy, which includes the impact that the uncountability theorem and the concept of countability have had on mathematics.\n\nCantor's article is short, less than four and a half pages. It begins with a discussion of the real algebraic numbers and a statement of his first theorem: The set of real algebraic numbers can be put into one-to-one correspondence with the set of positive integers. Cantor restates this theorem in terms more familiar to mathematicians of his time: The set of real algebraic numbers can be written as an infinite sequence in which each number appears only once.\n\nCantor's second theorem works with a closed interval [\"a\", \"b\"], which is the set of real numbers ≥ \"a\" and ≤ \"b\". The theorem states: Given any sequence of real numbers \"x\", \"x\", \"x\", … and any interval [\"a\", \"b\"], there is a number in [\"a\", \"b\"] that is not contained in the given sequence. Hence, there are infinitely many such numbers.\n\nThe first part of this theorem implies the \"Hence\" part. For example, let [0, 1] be the interval, and consider its pairwise disjoint subintervals …. Applying the first part of the theorem to each subinterval produces infinitely many numbers in [0, 1] that are not contained in the given sequence.\n\nCantor observes that combining his two theorems yields a new proof of Liouville's theorem that every interval [\"a\", \"b\"] contains infinitely many transcendental numbers.\n\nCantor then remarks that his second theorem is:\nThis remark contains Cantor's uncountability theorem, which only states that an interval [\"a\", \"b\"] cannot be put into one-to-one correspondence with the set of positive integers. It does not state that this interval is an infinite set of larger cardinality than the set of positive integers. Cardinality is defined in Cantor's next article, which was published in 1878.\n\nCantor only states his uncountability theorem. He does not use it in any proofs.\n\nTo prove that the set of real algebraic numbers is countable, define the \"height\" of a polynomial of degree \"n\" with integer coefficients as: \"n\" − 1 + |\"a\"| + |\"a\"| + … + |\"a\"|, where \"a\", \"a\", …, \"a\" are the coefficients of the polynomial. Order the polynomials by their height, and order the real roots of polynomials of the same height by numeric order. Since there are only a finite number of roots of polynomials of a given height, these orderings put the real algebraic numbers into a sequence. Cantor went a step further and produced a sequence in which each real algebraic number appears just once. He did this by only using polynomials that are irreducible over the integers. The table below contains the beginning of Cantor's enumeration.\n\nOnly the first part of Cantor's second theorem needs to be proved. It states: Given any sequence of real numbers \"x\", \"x\", \"x\", … and any interval [\"a\", \"b\"], there is a number in [\"a\", \"b\"] that is not contained in the given sequence. We simplify Cantor's proof by using open intervals. The open interval (\"a\", \"b\") is the set of real numbers greater than \"a\" and less than \"b\".\n\nTo find a number in [\"a\", \"b\"] that is not contained in the given sequence, construct two sequences of real numbers as follows: Find the first two numbers of the given sequence that are in (\"a\", \"b\"). Denote the smaller of these two numbers by \"a\" and the larger by \"b\". Similarly, find the first two numbers of the given sequence that are in (\"a\", \"b\"). Denote the smaller by \"a\" and the larger by \"b\". Continuing this procedure generates a sequence of intervals (\"a\", \"b\"), (\"a\", \"b\"), (\"a\", \"b\"), … such that each interval in the sequence contains all succeeding intervals—that is, it generates a sequence of nested intervals. This implies that the sequence \"a\", \"a\", \"a\", … is increasing and the sequence \"b\", \"b\", \"b\", … is decreasing.\n\nEither the number of intervals generated is finite or infinite. If finite, let (\"a\", \"b\") be the last interval. If infinite, take the limits \"a\" = lim \"a\" and \"b\" = lim \"b\". Since \"a\" < \"b\" for all \"n\", either \"a\" = \"b\" or \"a\" < \"b\". Thus, there are three cases to consider:\n\nThe proof is complete since, in all cases, at least one real number in [\"a\", \"b\"] has been found that is not contained in the given sequence.\n\nCantor's proofs are constructive and have been used to write a computer program that generates the digits of a transcendental number. This program applies Cantor's construction to a sequence containing all the real algebraic numbers between 0 and 1. The article that discusses this program gives some of its output, which shows how the construction generates a transcendental.\n\nAn example illustrates how Cantor's construction works. Consider the sequence: , , , , , , , , , … This sequence is obtained by ordering the rational numbers in (0, 1) by increasing denominators, ordering those with the same denominator by increasing numerators, and omitting reducible fractions. The table below shows the first five steps of the construction. The table's first column contains the intervals (\"a\", \"b\"). The second column lists the terms visited during the search for the first two terms in (\"a\", \"b\"). These two terms are in red.\n\nSince the sequence contains all the rational numbers in (0, 1), the construction generates an irrational number, which turns out to be  − 1.\n\nIn 1879, Cantor published a new uncountability proof that modifies his 1874 proof. He first defines the topological notion of a point set \"P\" being \"everywhere dense in an interval\" (which is quite often shortened to \"dense in an interval\"):\n\nWe will use \"a\", \"b\", \"c\", \"d\" rather than α, β, γ, δ. Cantor assumes that an interval [\"c\", \"d\"] satisfies \"d\" > \"c\".\n\nSince our discussion of Cantor's 1874 proof was simplified using by open intervals rather than closed intervals, the same simplification is used here. This requires an equivalent definition of everywhere dense: A set \"P\" is everywhere dense in the interval [\"a\", \"b\"] if and only if every subinterval (\"c\", \"d\") of [\"a\", \"b\"] contains at least one point of \"P\".\n\nCantor did not specify how many points of \"P\" a subinterval (\"c\", \"d\") must contain. He did not need to specify this because assuming that every subinterval contains at least one point of \"P\" implies that they contain infinitely many points of \"P\". This is proved by generating a sequence of points belonging to both \"P\" and (\"c\", \"d\"). Since \"P\" is dense in [\"a\", \"b\"], the subinterval (\"c\", \"d\") contains at least one point \"x\" of \"P\". Now consider the subinterval (\"x\", \"d\"). It contains at least one point \"x\" of \"P\", which satisfies \"x\" > x. In general, after generating \"x\", the subinterval (x, \"d\") is used to obtain the point \"x\", which satisfies \"x\" > \"x\". The points \"x\" are all unique and belong to both \"P\" and (\"c\", \"d\").\n\nCantor's 1879 proof is the same as his 1874 proof except for a new proof of first part of his second theorem: Given any sequence \"P\" of real numbers \"x\", \"x\", \"x\", … and any interval [\"a\", \"b\"], there is a number in [\"a\", \"b\"] that is not contained in the sequence \"P\". The new proof has only two cases.\n\nIn the first case, \"P\" is not dense in [\"a\", \"b\"]. By definition, \"P\" is dense if and only if for all , there is an \"x\" ∈ \"P\" such that . Taking the negation of each side of the \"if and only if\" produces: \"P\" is not dense in [\"a\", \"b\"] if and only if there exists a such that for all \"x\" ∈ \"P\", we have . Thus, every number in (\"c\", \"d\") is not contained in the sequence \"P\". This case handles cases 1 and 3 of Cantor's 1874 proof.\n\nIn the second case, \"P\" is dense in [\"a\", \"b\"]. The denseness of \"P\" is used to recursively define a nested sequence of intervals that excludes all elements of \"P\". The definition begins with\n\"a\" = \"a\" and \"b\" = \"b\". The definition's inductive case starts with the interval (\"a\", \"b\"), which because of the denseness of \"P\" contains infinitely many elements of \"P\". From these elements of \"P\", we take the two with smallest indices and denote the least of these two numbers by \"a\" and the greatest by \"b\".\nCantor proved that for all \"n\": . We proved this in a previous section.\n\nThe sequence \"a\" is increasing and bounded above by \"b\", so it has a limit \"A\", which satisfies \"a\" < \"A\". The sequence \"b\" is decreasing and bounded below by \"a\", so it has a limit \"B\", which satisfies \"B\" < \"b\". Also, \"a\" < \"b\" implies \"A\" ≤ \"B\". Therefore, . If \"A\" < \"B\", then for every \"n\": \"x\" ∉ (\"A\", \"B\") because \"x\" is not in the larger interval (\"a\", \"b\"). This contradicts \"P\" being dense in [\"a\", \"b\"]. Therefore, \"A\" = \"B\". Since for all \"n\": but , the limit \"A\" is a real number that is not contained in the sequence \"P\". This case handles case 2 of Cantor's 1874 proof.\n\nCantor's new proof first takes care of the easy case of the sequence \"P\" not being dense in the interval. Then it deals with the more difficult case of \"P\" being dense. This division into cases not only indicates which sequences are most difficult to handle, but it also reveals the important role denseness plays in the proof.\n\nIn the Example of Cantor's construction, each successive nested interval excludes rational numbers for two different reasons. It will exclude the finitely many rationals visited in the search for the first two rationals within the interval (these two rationals will have the least indices). These rationals are then used to form an interval that excludes the rationals visited in the search along with infinitely many more rationals. However, it still contains infinitely many rationals since our sequence of rationals is dense in [0, 1]. Forming this interval from the two rationals with the least indices guarantees that this interval excludes an initial segment of our sequence that contains at least two more elements than the preceding initial segment. Since the denseness of our sequence guarantees that this process never ends, all rationals will be excluded. Because of the ordering of the rationals in our sequence, the intersection of the nested intervals is the set  − 1}.\n\nThe development leading to Cantor's 1874 article appears in the correspondence between Cantor and Richard Dedekind. On November 29, 1873, Cantor asked Dedekind whether the collection of positive integers and the collection of positive real numbers \"can be corresponded so that each individual of one collection corresponds to one and only one individual of the other?\" Cantor added that collections having such a correspondence include the collection of positive rational numbers, and collections of the form (\"a\") where \"n\", \"n\", . . . , \"n\", and \"ν\" are positive integers.\n\nDedekind replied that he was unable to answer Cantor's question, and said that it \"did not deserve too much effort because it has no particular practical interest.\" Dedekind also sent Cantor a proof that the set of algebraic numbers is countable.\nOn December 2, Cantor responded that his question does have interest: \"It would be nice if it could be answered; for example, provided that it could be answered \"no\", one would have a new proof of Liouville's theorem that there are transcendental numbers.\"\n\nOn December 7, Cantor sent Dedekind a proof by contradiction that the set of real numbers is uncountable. Cantor starts by assuming the real numbers can be written as a sequence. Then he applies a construction to this sequence to produce a real number not in the sequence, thus contradicting his assumption. The letters of December 2 and 7 lead to a non-constructive proof of the existence of transcendental numbers.\n\nOn December 9, Cantor announced the theorem that allowed him to construct transcendental numbers as well as prove the uncountability of the set of real numbers:\n\nThis is the second theorem in Cantor's article. It comes from realizing that his construction can be applied to any sequence, not just to sequences that supposedly enumerate the real numbers. So Cantor had a choice between two proofs that demonstrate the existence of transcendental numbers: one proof is constructive, but the other is not. We now compare the proofs assuming that we have a sequence consisting of all the real algebraic numbers.\n\nThe constructive proof applies Cantor's construction to this sequence and the interval [\"a\", \"b\"] to produce a transcendental number in this interval.\n\nThe non-constructive proof uses two proofs by contradiction:\n\nCantor chose to publish the constructive proof, which not only produces a transcendental number but is also shorter and avoids two proofs by contradiction. The non-constructive proof from Cantor's correspondence is simpler than the one above because it works with all the real numbers rather than the interval [\"a\", \"b\"]. This eliminates the subsequence step and all occurrences of [\"a\", \"b\"] in the second proof by contradiction.\n\nThe correspondence containing Cantor's non-constructive reasoning was published in 1937. By then, other mathematicians had rediscovered its non-constructive proof. As early as 1921, this proof was attributed to Cantor and criticized for not producing any transcendental numbers. In that year, Oskar Perron stated: \"… Cantor's proof for the existence of transcendental numbers has, along with its simplicity and elegance, the great disadvantage that it is only an existence proof; it does not enable us to actually specify even a single transcendental number.\"\nSome mathematicians have attempted to correct this misunderstanding of Cantor's work. In 1930, the set theorist Abraham Fraenkel stated that Cantor's method is \"… a method that incidentally, contrary to a widespread interpretation, is fundamentally constructive and not merely existential.\" In 1972, Irving Kaplansky wrote: \"It is often said that Cantor's proof is not 'constructive,' and so does not yield a tangible transcendental number. This remark is not justified. If we set up a definite listing of all algebraic numbers … and then apply the diagonal procedure …, we get a perfectly definite transcendental number (it could be computed to any number of decimal places).\"\n\nThe disagreement about Cantor's proof occurs because two groups of mathematicians are talking about different proofs: the constructive one that Cantor published and the non-constructive one that was later rediscovered. The opinion that Cantor's proof is non-constructive appears in some books that were quite successful as measured by the length of time new editions or reprints appeared—for example: Eric Temple Bell's \"Men of Mathematics\" (1937; still being reprinted), Godfrey Hardy and E. M. Wright's \"An Introduction to the Theory of Numbers\" (1938; 2008 6th edition), Garrett Birkhoff and Saunders Mac Lane's \"A Survey of Modern Algebra\" (1941; 1997 5th edition), and Michael Spivak's \"Calculus\" (1967; 2008 4th edition). Since these books view Cantor's proof as non-constructive, they do not mention his constructive proof. On the other hand, the quotations above from Fraenkel and Kaplansky show that they knew Cantor's work can be used non-constructively. The disagreement about Cantor's proof shows no sign of being resolved: since 2014, at least two books have appeared stating that Cantor's proof is constructive, and at least four have appeared stating that his proof does not construct any (or a single) transcendental.\n\nAsserting that Cantor gave a non-constructive proof can lead to erroneous statements about the history of mathematics. In \"A Survey of Modern Algebra,\" Birkhoff and Mac Lane state: \"Cantor's argument for this result [Not every real number is algebraic] was at first rejected by many mathematicians, since it did not exhibit any specific transcendental number.\" Birkhoff and Mac Lane are talking about the non-constructive proof. Cantor's proof produces transcendental numbers, and there appears to be no evidence that his argument was rejected. Even Leopold Kronecker, who had strict views on what is acceptable in mathematics and who could have delayed publication of Cantor's article, did not delay it. In fact, applying Cantor's construction to the sequence of real algebraic numbers produces a limiting process that Kronecker accepted—namely, it determines a number to any required degree of accuracy.\n\nHistorians of mathematics have discovered the following facts about Cantor's article \"On a Property of the Collection of All Real Algebraic Numbers\":\n\n\nTo explain these facts, historians have pointed to the influence of Cantor's former professors, Karl Weierstrass and Leopold Kronecker. Cantor discussed his results with Weierstrass on December 23, 1873. Weierstrass was first amazed by the concept of countability, but then found the countability of the set of real algebraic numbers useful. Cantor did not want to publish yet, but Weierstrass felt that he must publish at least his results concerning the algebraic numbers.\n\nFrom his correspondence, it appears that Cantor only discussed his article with Weierstrass. However, Cantor told Dedekind: \"The restriction which I have imposed on the published version of my investigations is caused in part by local circumstances …\" Cantor biographer Joseph Dauben believes that \"local circumstances\" refers to Kronecker who, as a member of the editorial board of \"Crelle's Journal\", had delayed publication of an 1870 article by Eduard Heine, one of Cantor's colleagues. Cantor would submit his article to \"Crelle's Journal\".\n\nWeierstrass advised Cantor to leave his uncountability theorem out of the article he submitted, but Weierstrass also told Cantor that he could add it as a marginal note during proofreading, which he did. It appears in a remark at the end of the article's introduction. The opinions of Kronecker and Weierstrass both played a role here. Kronecker did not accept infinite sets, and it seems that Weierstrass did not accept that two infinite sets could be so different, with one being countable and the other not. Weierstrass changed his opinion later. Without the uncountability theorem, the article needed a title that did not refer to this theorem. Cantor chose \"Ueber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen\" (\"On a Property of the Collection of All Real Algebraic Numbers\"), which refers to the countability of the set of real algebraic numbers, the result that Weierstrass found useful.\n\nKronecker's influence appears in the proof of Cantor's second theorem. Cantor used Dedekind's version of the proof except he left out why the limits \"a\" = lim \"a\" and \n\"b\" = lim \"b\" exist. Dedekind had used his \"principle of continuity\" to prove they exist. This principle (which is equivalent to the least upper bound property of the real numbers) comes from Dedekind's construction of the real numbers, a construction Kronecker did not accept.\n\nCantor restricted his first theorem to the set of real algebraic numbers even though Dedekind had sent him a proof that handled all algebraic numbers. Cantor did this for expository reasons and because of \"local circumstances.\" This restriction simplifies the article because the second theorem works with real sequences. Hence, the construction in the second theorem can be applied directly to the enumeration of the real algebraic numbers to produce \"an effective procedure for the calculation of transcendental numbers.\" This procedure would be acceptable to Weierstrass.\n\nSince 1856, Dedekind had developed theories involving infinitely many infinite sets—for example: ideals, which he used in algebraic number theory, and Dedekind cuts, which he used to construct the real numbers. This work enabled him to understand and contribute to Cantor's work.\n\nDedekind's first contribution concerns the theorem that the set of real algebraic numbers is countable. Cantor is usually given credit for this theorem, but the mathematical historian José Ferreirós calls it \"Dedekind's theorem.\" Their correspondence reveals what each mathematician contributed to the theorem.\n\nIn his letter introducing the concept of countability, Cantor stated without proof that the set of positive rational numbers is countable, as are sets of the form (\"a\") where \"n\", \"n\", …, \"n, and \"ν\" are positive integers. Cantor's second result uses indexed numbers: a set of the form (\"a\") is the range of a function from the \"ν\" indices to the set of real numbers. His second result implies his first: let \"ν\" = 2 and \"a\" = . The function can be quite general—for example, \"a\" = (where R is the set of real numbers) and the set of irrational numbers have the same cardinality as R.\n\nIn 1883, Cantor extended the natural numbers with his infinite ordinals. This extension was necessary for his work on the Cantor–Bendixson theorem. Cantor discovered other uses for the ordinals—for example, he used sets of ordinals to produce an infinity of sets having different infinite cardinalities. His work on infinite sets together with Dedekind's set-theoretical work created set theory.\n\nThe concept of countability led to countable operations and objects that are used in various areas of mathematics. For example, in 1878, Cantor introduced countable unions of sets. In the 1890s, Émile Borel used countable unions in his theory of measure, and René Baire used countable ordinals to define his classes of functions. Building on the work of Borel and Baire, Henri Lebesgue created his theories of measure and integration, which were published from 1899 to 1901.\n\nCountable models are used in set theory. In 1922, Thoralf Skolem proved that if conventional axioms of set theory are consistent, then they have a countable model. Since this model is countable, its set of real numbers is countable. This consequence is called Skolem's paradox, and Skolem explained why it does not contradict Cantor's uncountability theorem: although there is a one-to-one correspondence between this set and the set of positive integers, no such one-to-one correspondence is a member of the model. Thus the model considers its set of real numbers to be uncountable, or more precisely, the first-order sentence that says the set of real numbers is uncountable is true within the model. In 1963, Paul Cohen used countable models to prove his independence theorems.\n\n\n"}
{"id": "41833604", "url": "https://en.wikipedia.org/wiki?curid=41833604", "title": "Graded (mathematics)", "text": "Graded (mathematics)\n\nIn mathematics, the term “graded” has a number of meanings, mostly related:\n\nIn abstract algebra, it refers to a family of concepts:\n\nIn other areas of mathematics:\n"}
{"id": "6785051", "url": "https://en.wikipedia.org/wiki?curid=6785051", "title": "History of trigonometry", "text": "History of trigonometry\n\nEarly study of triangles can be traced to the 2nd millennium BC, in Egyptian mathematics (Rhind Mathematical Papyrus) and Babylonian mathematics.\nSystematic study of trigonometric functions began in Hellenistic mathematics, reaching India as part of Hellenistic astronomy. In Indian astronomy, the study of trigonometric functions flourished in the Gupta period, especially due to Aryabhata (sixth century CE). During the Middle Ages, the study of trigonometry continued in Islamic mathematics, hence it was adopted as a separate subject in the Latin West beginning in the Renaissance with Regiomontanus.\nThe development of modern trigonometry shifted during the western Age of Enlightenment, beginning with 17th-century mathematics (Isaac Newton and James Stirling) and reaching its modern form with Leonhard Euler (1748).\n\nThe term \"trigonometry\" was derived from Greek τρίγωνον \"trigōnon\", \"triangle\" and μέτρον \"metron\", \"measure\".\n\nThe modern word \"sine\" is derived from the Latin word \"sinus\", which means \"bay\", \"bosom\" or \"fold\"\nis indirectly, via Indian, Persian and Arabic transmission, derived from the Greek term \"khordḗ\" \"bow-string, chord\". The Greek term was adopted into Sanskrit as \"jyā\" \"bow-string\", later also in the variant \"jīvā\".\nSanskrit \"jīvā\" was rendered adopted into Arabic as \"jiba\", written \"jb\" جب. \nThis was then interpreted as the genuine Arabic word \"jayb\", meaning \"bosom, fold, bay\", either by the Arabs or by a mistake of the European translators such as Robert of Chester, who translated \"jayb\" into Latin as \"sinus\". Particularly Fibonacci's \"sinus rectus arcus\" proved influential in establishing the term \"sinus\". The words \"minute\" and \"second\" are derived from the Latin phrases \"partes minutae primae\" and \"partes minutae secundae\". These roughly translate to \"first small parts\" and \"second small parts\".\n\nThe ancient Egyptians and Babylonians had known of theorems on the ratios of the sides of similar triangles for many centuries. However, as pre-Hellenic societies lacked the concept of an angle measure, they were limited to studying the sides of triangles instead.\n\nThe Babylonian astronomers kept detailed records on the rising and setting of stars, the motion of the planets, and the solar and lunar eclipses, all of which required familiarity with angular distances measured on the celestial sphere. Based on one interpretation of the Plimpton 322 cuneiform tablet (c. 1900 BC), some have even asserted that the ancient Babylonians had a table of secants. There is, however, much debate as to whether it is a table of Pythagorean triples, a solution of quadratic equations, or a trigonometric table.\n\nThe Egyptians, on the other hand, used a primitive form of trigonometry for building pyramids in the 2nd millennium BC. The Rhind Mathematical Papyrus, written by the Egyptian scribe Ahmes (c. 1680–1620 BC), contains the following problem related to trigonometry:\nAhmes' solution to the problem is the ratio of half the side of the base of the pyramid to its height, or the run-to-rise ratio of its face. In other words, the quantity he found for the \"seked\" is the cotangent of the angle to the base of the pyramid and its face.\n\nAncient Greek and Hellenistic mathematicians made use of the chord. Given a circle and an arc on the circle, the chord is the line that subtends the arc. A chord's perpendicular bisector passes through the center of the circle and bisects the angle. One half of the bisected chord is the sine of one half the bisected angle, that is,\n\nand consequently the sine function is also known as the \"half-chord\". Due to this relationship, a number of trigonometric identities and theorems that are known today were also known to Hellenistic mathematicians, but in their equivalent chord form.\n\nAlthough there is no trigonometry in the works of Euclid and Archimedes, in the strict sense of the word, there are theorems presented in a geometric way (rather than a trigonometric way) that are equivalent to specific trigonometric laws or formulas. For instance, propositions twelve and thirteen of book two of the \"Elements\" are the laws of cosines for obtuse and acute angles, respectively. Theorems on the lengths of chords are applications of the law of sines. And Archimedes' theorem on broken chords is equivalent to formulas for sines of sums and differences of angles. To compensate for the lack of a table of chords, mathematicians of Aristarchus' time would sometimes use the statement that, in modern notation, sin \"α\"/sin \"β\" < \"α\"/\"β\" < tan \"α\"/tan \"β\" whenever 0° < β < α < 90°, now known as Aristarchus's inequality.\n\nThe first trigonometric table was apparently compiled by Hipparchus of Nicaea (180 – 125 BCE), who is now consequently known as \"the father of trigonometry.\" Hipparchus was the first to tabulate the corresponding values of arc and chord for a series of angles.\n\nAlthough it is not known when the systematic use of the 360° circle came into mathematics, it is known that the systematic introduction of the 360° circle came a little after Aristarchus of Samos composed \"On the Sizes and Distances of the Sun and Moon\" (ca. 260 BC), since he measured an angle in terms of a fraction of a quadrant. It seems that the systematic use of the 360° circle is largely due to Hipparchus and his table of chords. Hipparchus may have taken the idea of this division from Hypsicles who had earlier divided the day into 360 parts, a division of the day that may have been suggested by Babylonian astronomy. In ancient astronomy, the zodiac had been divided into twelve \"signs\" or thirty-six \"decans\". A seasonal cycle of roughly 360 days could have corresponded to the signs and decans of the zodiac by dividing each sign into thirty parts and each decan into ten parts. It is due to the Babylonian sexagesimal numeral system that each degree is divided into sixty minutes and each minute is divided into sixty seconds.\nMenelaus of Alexandria (ca. 100 AD) wrote in three books his \"Sphaerica\". In Book I, he established a basis for spherical triangles analogous to the Euclidean basis for plane triangles. He establishes a theorem that is without Euclidean analogue, that two spherical triangles are congruent if corresponding angles are equal, but he did not distinguish between congruent and symmetric spherical triangles. Another theorem that he establishes is that the sum of the angles of a spherical triangle is greater than 180°. Book II of \"Sphaerica\" applies spherical geometry to astronomy. And Book III contains the \"theorem of Menelaus\". He further gave his famous \"rule of six quantities\".\n\nLater, Claudius Ptolemy (ca. 90 – ca. 168 AD) expanded upon Hipparchus' \"Chords in a Circle\" in his \"Almagest\", or the \"Mathematical Syntaxis\". The Almagest is primarily a work on astronomy, and astronomy relies on trigonometry. Ptolemy's table of chords gives the lengths of chords of a circle of diameter 120 as a function of the number of degrees \"n\" in the corresponding arc of the circle, for \"n\" ranging from 1/2 to 180 by increments of 1/2. The thirteen books of the \"Almagest\" are the most influential and significant trigonometric work of all antiquity. A theorem that was central to Ptolemy's calculation of chords was what is still known today as Ptolemy's theorem, that the sum of the products of the opposite sides of a cyclic quadrilateral is equal to the product of the diagonals. A special case of Ptolemy's theorem appeared as proposition 93 in Euclid's \"Data\". Ptolemy's theorem leads to the equivalent of the four sum-and-difference formulas for sine and cosine that are today known as Ptolemy's formulas, although Ptolemy himself used chords instead of sine and cosine. Ptolemy further derived the equivalent of the half-angle formula\n\nPtolemy used these results to create his trigonometric tables, but whether these tables were derived from Hipparchus' work cannot be determined.\n\nNeither the tables of Hipparchus nor those of Ptolemy have survived to the present day, although descriptions by other ancient authors leave little doubt that they once existed.\n\nSome of the early and very significant developments of trigonometry were in India. Influential works from the 4th–5th century, known as the Siddhantas (of which there were five, the most important of which is the Surya Siddhanta) first defined the sine as the modern relationship between half an angle and half a chord, while also defining the cosine, versine, and inverse sine. Soon afterwards, another Indian mathematician and astronomer, Aryabhata (476–550 AD), collected and expanded upon the developments of the Siddhantas in an important work called the \"Aryabhatiya\". The \"Siddhantas\" and the \"Aryabhatiya\" contain the earliest surviving tables of sine values and versine (1 − cosine) values, in 3.75° intervals from 0° to 90°, to an accuracy of 4 decimal places. They used the words \"jya\" for sine, \"kojya\" for cosine, \"utkrama-jya\" for versine, and \"otkram jya\" for inverse sine. The words \"jya\" and \"kojya\" eventually became \"sine\" and \"cosine\" respectively after a mistranslation described above.\n\nIn the 7th century, Bhaskara the First produced a formula for calculating the sine of an acute angle without the use of a table. He also gave the following approximation formula for sin(\"x\"), which had a relative error of less than 1.9%:\n\nLater in the 7th century, Brahmagupta redeveloped the formula\n\n(also derived earlier, as mentioned above) and the Brahmagupta interpolation formula for computing sine values.\n\nMadhava (c. 1400) made early strides in the analysis of trigonometric functions and their infinite series expansions. He developed the concepts of the power series and Taylor series, and produced the power series expansions of sine, cosine, tangent, and arctangent. Using the Taylor series approximations of sine and cosine, he produced a sine table to 12 decimal places of accuracy and a cosine table to 9 decimal places of accuracy. He also gave the power series of π and the θ, radius, diameter, and circumference of a circle in terms of trigonometric functions. His works were expanded by his followers at the Kerala School up to the 16th century.\n\nThe Indian text the Yuktibhāṣā contains proof for the expansion of the sine and cosine functions and the derivation and proof of the power series for inverse tangent, discovered by Madhava. The Yuktibhāṣā also contains rules for finding the sines and the cosines of the sum and difference of two angles.\n\nIn China, Aryabhata's table of sines were translated into the Chinese mathematical book of the \"Kaiyuan Zhanjing\", compiled in 718 AD during the Tang Dynasty. Although the Chinese excelled in other fields of mathematics such as solid geometry, binomial theorem, and complex algebraic formulas, early forms of trigonometry were not as widely appreciated as in the earlier Greek, Hellenistic, Indian and Islamic worlds. Instead, the early Chinese used an empirical substitute known as \"chong cha\", while practical use of plane trigonometry in using the sine, the tangent, and the secant were known. However, this embryonic state of trigonometry in China slowly began to change and advance during the Song Dynasty (960–1279), where Chinese mathematicians began to express greater emphasis for the need of spherical trigonometry in calendrical science and astronomical calculations. The polymath Chinese scientist, mathematician and official Shen Kuo (1031–1095) used trigonometric functions to solve mathematical problems of chords and arcs. Victor J. Katz writes that in Shen's formula \"technique of intersecting circles\", he created an approximation of the arc \"s\" of a circle given the diameter \"d\", sagitta \"v\", and length \"c\" of the chord subtending the arc, the length of which he approximated as\n\nSal Restivo writes that Shen's work in the lengths of arcs of circles provided the basis for spherical trigonometry developed in the 13th century by the mathematician and astronomer Guo Shoujing (1231–1316). As the historians L. Gauchet and Joseph Needham state, Guo Shoujing used spherical trigonometry in his calculations to improve the calendar system and Chinese astronomy. Along with a later 17th-century Chinese illustration of Guo's mathematical proofs, Needham states that:\nGuo used a quadrangular spherical pyramid, the basal quadrilateral of which consisted of one equatorial and one ecliptic arc, together with two meridian arcs, one of which passed through the summer solstice point...By such methods he was able to obtain the du lü (degrees of equator corresponding to degrees of ecliptic), the ji cha (values of chords for given ecliptic arcs), and the cha lü (difference between chords of arcs differing by 1 degree).\nDespite the achievements of Shen and Guo's work in trigonometry, another substantial work in Chinese trigonometry would not be published again until 1607, with the dual publication of \"Euclid's Elements\" by Chinese official and astronomer Xu Guangqi (1562–1633) and the Italian Jesuit Matteo Ricci (1552–1610).\n\nPrevious works were later translated and expanded in the medieval Islamic world by Muslim mathematicians of mostly Persian and Arab descent, who enunciated a large number of theorems which freed the subject of trigonometry from dependence upon the complete quadrilateral, as was the case in Hellenistic mathematics due to the application of Menelaus' theorem. According to E. S. Kennedy, it was after this development in Islamic mathematics that \"the first real trigonometry emerged, in the sense that only then did the object of study become the spherical or plane triangle, its sides and angles.\"\n\nMethods dealing with spherical triangles were also known, particularly the method of Menelaus of Alexandria, who developed \"Menelaus' theorem\" to deal with spherical problems. However, E. S. Kennedy points out that while it was possible in pre-Islamic mathematics to compute the magnitudes of a spherical figure, in principle, by use of the table of chords and Menelaus' theorem, the application of the theorem to spherical problems was very difficult in practice. In order to observe holy days on the Islamic calendar in which timings were determined by phases of the moon, astronomers initially used Menelaus' method to calculate the place of the moon and stars, though this method proved to be clumsy and difficult. It involved setting up two intersecting right triangles; by applying Menelaus' theorem it was possible to solve one of the six sides, but only if the other five sides were known. To tell the time from the sun's altitude, for instance, repeated applications of Menelaus' theorem were required. For medieval Islamic astronomers, there was an obvious challenge to find a simpler trigonometric method.\n\nIn the early 9th century AD, Muhammad ibn Mūsā al-Khwārizmī produced accurate sine and cosine tables, and the first table of tangents. He was also a pioneer in spherical trigonometry. In 830 AD, Habash al-Hasib al-Marwazi produced the first table of cotangents. Muhammad ibn Jābir al-Harrānī al-Battānī (Albatenius) (853-929 AD) discovered the reciprocal functions of secant and cosecant, and produced the first table of cosecants for each degree from 1° to 90°.\n\nBy the 10th century AD, in the work of Abū al-Wafā' al-Būzjānī, Muslim mathematicians were using all six trigonometric functions. Abu al-Wafa had sine tables in 0.25° increments, to 8 decimal places of accuracy, and accurate tables of tangent values. He also developed the following trigonometric formula:\n\nIn his original text, Abū al-Wafā' states: \"If we want that, we multiply the given sine by the cosine minutes, and the result is half the sine of the double\". Abū al-Wafā also established the angle addition and difference identities presented with complete proofs:\n\nFor the second one, the text states: \"We multiply the sine of each of the two arcs by the cosine of the other \"minutes\". If we want the sine of the sum, we add the products, if we want the sine of the difference, we take their difference\".\n\nHe also discovered the law of sines for spherical trigonometry:\n\nAlso in the late 10th and early 11th centuries AD, the Egyptian astronomer Ibn Yunus performed many careful trigonometric calculations and demonstrated the following trigonometric identity:\n\nAl-Jayyani (989–1079) of al-Andalus wrote \"The book of unknown arcs of a sphere\", which is considered \"the first treatise on spherical trigonometry\". It \"contains formulae for right-handed triangles, the general law of sines, and the solution of a spherical triangle by means of the polar triangle.\" This treatise later had a \"strong influence on European mathematics\", and his \"definition of ratios as numbers\" and \"method of solving a spherical triangle when all sides are unknown\" are likely to have influenced Regiomontanus.\n\nThe method of triangulation was first developed by Muslim mathematicians, who applied it to practical uses such as surveying and Islamic geography, as described by Abu Rayhan Biruni in the early 11th century. Biruni himself introduced triangulation techniques to measure the size of the Earth and the distances between various places. In the late 11th century, Omar Khayyám (1048–1131) solved cubic equations using approximate numerical solutions found by interpolation in trigonometric tables. In the 13th century, Nasīr al-Dīn al-Tūsī was the first to treat trigonometry as a mathematical discipline independent from astronomy, and he developed spherical trigonometry into its present form. He listed the six distinct cases of a right-angled triangle in spherical trigonometry, and in his \"On the Sector Figure\", he stated the law of sines for plane and spherical triangles, discovered the law of tangents for spherical triangles, and provided proofs for both these laws.\n\nIn 1342, Levi ben Gershon, known as Gersonides, wrote \"On Sines, Chords and Arcs\", in particular proving the sine law for plane triangles and giving five-figure sine tables.\n\nA simplified trigonometric table, the \"toleta de marteloio\", was used by sailors in the Mediterranean Sea during the 14th-15th Centuries to calculate navigation courses. It is described by Ramon Llull of Majorca in 1295, and laid out in the 1436 atlas of Venetian captain Andrea Bianco.\n\nIn the 15th century, Jamshīd al-Kāshī provided the first explicit statement of the law of cosines in a form suitable for triangulation. In France, the law of cosines is still referred to as the \"\". He also gave trigonometric tables of values of the sine function to four sexagesimal digits (equivalent to 8 decimal places) for each 1° of argument with differences to be added for each 1/60 of 1°. Ulugh Beg also gives accurate tables of sines and tangents correct to 8 decimal places around the same time.\n\nRegiomontanus was perhaps the first mathematician in Europe to treat trigonometry as a distinct mathematical discipline, in his \"De triangulis omnimodis\" written in 1464, as well as his later \"Tabulae directionum\" which included the tangent function, unnamed.\nThe \"Opus palatinum de triangulis\" of Georg Joachim Rheticus, a student of Copernicus, was probably the first in Europe to define trigonometric functions directly in terms of right triangles instead of circles, with tables for all six trigonometric functions; this work was finished by Rheticus' student Valentin Otho in 1596.\n\nIn the 17th century, Isaac Newton and James Stirling developed the general Newton–Stirling interpolation formula for trigonometric functions.\n\nIn the 18th century, Leonhard Euler's \"Introductio in analysin infinitorum\" (1748) was mostly responsible for establishing the analytic treatment of trigonometric functions in Europe, deriving their infinite series and presenting \"Euler's formula\" \"e\" = cos \"x\" + \"i\" sin \"x\". Euler used the near-modern abbreviations \"sin.\", \"cos.\", \"tang.\", \"cot.\", \"sec.\", and \"cosec.\" Prior to this, Roger Cotes had computed the derivative of sine in his \"Harmonia Mensurarum\" (1722).\nAlso in the 18th century, Brook Taylor defined the general Taylor series and gave the series expansions and approximations for all six trigonometric functions. The works of James Gregory in the 17th century and Colin Maclaurin in the 18th century were also very influential in the development of trigonometric series.\n\n\n"}
{"id": "44025330", "url": "https://en.wikipedia.org/wiki?curid=44025330", "title": "Im schwarzen Walfisch zu Askalon", "text": "Im schwarzen Walfisch zu Askalon\n\n\"Im schwarzen Walfisch zu Askalon\" (\"In Ashkelon's Black Whale\") is a popular academic commercium song. It was known as a beer-drinking song in many German speaking ancient universities. Joseph Victor von Scheffel provided the lyrics under the title Altassyrisch (Old Assyrian) 1854, the melody is from 1783 or earlier.\n\nThe lyrics reflect an endorsement of the bacchanalian mayhem of student life, similar as in Gaudeamus igitur. The song describes an old Assyrian drinking binge of a man in an inn with some references to the Classics. The desks are made of marble and the large invoice is being provided in cuneiform on bricks. However the carouser has to admit that he left his money already in Nineveh. A Nubian house servant kicks him out then and the song closes with the notion, that (compare John 4:44) a prophet has no honor in his own country, if he doesn't pay cash for his consumption. Charles Godfrey Leland has translated the poems among other works of Scheffel. Each stanza begins with the naming verse \"Im Schwarzen Walfisch zu Askalon\", but varies the outcome. The \"Im\" is rather prolonged with the melody and increases the impact. Some of the stanzas:\n<br>\nIm schwarzen Wallfisch zu Ascalon\n<br>\nDa trank ein Mann drei Tag',\n<br>\nBis dass er steif wie ein Besenstiel\n<br>\nAm Marmortische lag.\n<br>\n\n'In the Black Whale at Ascalon\n<br>\nA man drank day by day,\n<br>\nTill, stiff as any broom-handle,\n<br>\nUpon the floor he lay.\n<br>\n\nIn the Black Whale at Ascalon\n<br>\nThe waiters brought the bill,\n<br>\nIn arrow-heads on six broad tiles\n<br>\nTo him who thus did swill.\n<br>\n\nIn the Black Whale at Ascalon\n<br>\nNo prophet hath renown;\n<br>\nAnd he who there would drink in peace\n<br>\nMust pay the money down.\n<br>\nIn typical manner of Scheffel, it contains an anachronistic mixture of various times and eras, parodistic notions on current science, as e.g. Historical criticism and interpretations of the Book of Jonah as a mere shipwrecking narrative. According to Scheffel, the guest didn't try to get back in the inn as „Aussi bini, aussi bleibi, wai Ascalun, ihr grobi Kaibi“ (I been out, I stay so, you rude Aschkelon calves). There are various additional verses, including political parodist ones and verses mocking different sorts of fraternities.\nThe song has been used as name for traditional inns and restaurants, e.g. in Heidelberg and Bad Säckingen. In Bad Säckingen the name was used on several (consecutive) inns and was namegiver for the still existing club \"Walfisch Gesellschaft Säckingen\" (Walfischia), honoring Scheffel.\n\nThere is one version just and only one for mathematics, called 'International'.\n\nIn ancient times, upon the door <br>\nOf Plato, there was writt'n: <br>\n“To each non-mathematicus <br>\nThe entrance is forbidd'n.<br>\n\nThe same stanza is available in further 13 languages, including Greek (Μελαίνῃ τῇ ἐν Φαλαίνᾳ - Melaínē (black) tē (the) en (in) Phalaína (whale)) and Volapük, which are sung one after the other.\n\n\n"}
{"id": "19777721", "url": "https://en.wikipedia.org/wiki?curid=19777721", "title": "Introductio in analysin infinitorum", "text": "Introductio in analysin infinitorum\n\nIntroductio in analysin infinitorum (Introduction to the Analysis of the Infinite) is a two-volume work by Leonhard Euler which lays the foundations of mathematical analysis. Written in Latin and published in 1748, the \"Introductio\" contains 18 chapters in the first part and 22 chapters in the second. It has Eneström numbers E101 and E102.\n\nCarl Boyer's lectures at the 1950 International Congress of Mathematicians compared the influence of Euler's \"Introductio\" to that of Euclid's \"Elements\", calling the \"Elements\" the foremost textbook of ancient times, and the \"Introductio\" \"the foremost textbook of modern times\". Boyer also wrote:\n\nThe first translation into English was that by John D. Blanton, published in 1988. The second, by Ian Bruce, is available online. A list of the editions of \"Introductio\" has been assembled by V. Frederick Rickey.\n\nChapter 1 is on the concepts of variables and functions. Chapter 4 introduces infinite series through rational functions.\n\nAccording to Henk Bos,\n\nEuler accomplished this feat by introducing exponentiation \"a\" for arbitrary constant \"a\" in the positive real numbers. He noted that mapping \"x\" this way is \"not\" an algebraic function, but rather a transcendental function. For \"a\" > 1 these functions are monotonic increasing and form bijections of the real line with positive real numbers. Then each base \"a\" corresponds to an inverse function called the logarithm to base \"a\", in chapter 6. In chapter 7, Euler introduces e as the number whose hyperbolic logarithm is 1. The reference here is to Gregoire de Saint-Vincent who performed a quadrature of the hyperbola \"y\" = 1/\"x\" through description of the hyperbolic logarithm. Section 122 labels the logarithm to base e the \"natural or hyperbolic logarithm...since the quadrature of the hyperbola can be expressed through these logarithms\". Here he also gives the exponential series:\n\nThen in chapter 8 Euler is prepared to address the classical trigonometric functions as \"transcendental quantities that arise from the circle.\" He uses the unit circle and presents Euler's formula. Chapter 9 considers trinomial factors in polynomials. Chapter 16 is concerned with partitions, a topic in number theory. Continued fractions are the topic of chapter 18.\n\n\n"}
{"id": "5508956", "url": "https://en.wikipedia.org/wiki?curid=5508956", "title": "Ishango bone", "text": "Ishango bone\n\nThe Ishango bone is a bone tool, dated to the Upper Paleolithic era. It is a dark brown length of bone, the fibula of a baboon, with a sharp piece of quartz affixed to one end, perhaps for engraving. It was first thought to be a tally stick, as it has a series of what has been interpreted as tally marks carved in three columns running the length of the tool, though it has also been suggested that the scratches might have been to create a better grip on the handle or for some other non-mathematical reason. \n\nThe Ishango bone was found in 1960 by Belgian Jean de Heinzelin de Braucourt while exploring what was then the Belgian Congo. It was discovered in the area of Ishango near the Semliki River. Lake Edward empties into the Semliki which forms part of the headwaters of the Nile River (now on the border between modern-day Uganda and D.R. Congo). The bone was found among the remains of a small community that fished and gathered in this area of Africa. The settlement had been buried in a volcanic eruption.\n\nThe artifact was first estimated to have originated between 9,000 BC and 6,500 BC. \nHowever, the dating of the site where it was discovered was re-evaluated, and it is now believed to be more than 20,000 years old (between 18,000 BC and 20,000 BC).\n\nThe Ishango bone is on permanent exhibition at the Royal Belgian Institute of Natural Sciences, Brussels, Belgium.\n\nThe etchings on the bone are in three columns with marks asymmetrically grouped into sets, leading to \"various tantalizing hypotheses\", such as that the implement was used as a counting tool for simple mathematical procedures or even to construct a numeral system. \n\nThe third column has been interpreted as a \"table of prime numbers\", but it is more likely to be a coincidence. Historian of mathematics Peter S. Rudman argues that prime numbers were probably not understood until about 500 BC, and were dependent on the concept of division, which he dates to no earlier than 10,000 BC. \n\nAlexander Marshack speculated that the Ishango bone represents a six-month lunar calendar. This has led Claudia Zaslavsky to suggest that the creator of the tool may have been a woman, tracking the lunar phase in relation to the menstrual cycle. This is countered with the argument that Marshack overinterprets the data and that the evidence does not support lunar calendars.\n\nDuring earlier excavations at the Ishango site in 1959, another bone was also found. It is lighter in color and was scraped, thinned, polished, and broken on one end, revealing it to be hollow. The artifact possibly held a piece of quartz like the more well-known bone or it could have been a tool handle. The 14-cm long bone has 90 notches on six sides, which are categorized as \"major\" or \"minor\" according to their length. Jean de Heinzelin interpreted the major notches as being units or multiples and the minor notches as fractions or subsidiary. He believed the bone to be an \"interchange rule between bases 10 and 12.\"\n\n\n\n"}
{"id": "30551989", "url": "https://en.wikipedia.org/wiki?curid=30551989", "title": "J-line", "text": "J-line\n\nIn the study of the arithmetic of elliptic curves, the \"j\"-line over any ring \"R\" is the coarse moduli scheme attached to the moduli problem Γ(1)]:\n\nwith the \"j\"-invariant normalized a la Tate: \"j\" = 0 has complex multiplication by \"Z\"[\"ζ\"], and \"j\" = 1728 by \"Z\"[\"i\"].\n\nThe \"j\"-line can be seen as giving a coordinatization of the classical modular curve of level 1, \"X\"(1), which is isomorphic to the complex projective line.\n"}
{"id": "35818365", "url": "https://en.wikipedia.org/wiki?curid=35818365", "title": "Journal of Mathematics and the Arts", "text": "Journal of Mathematics and the Arts\n\nThe Journal of Mathematics and the Arts is a quarterly peer-reviewed academic journal that deals with relationship between mathematics and the arts.\n\nThe journal was established in 2007 and is published by Taylor & Francis. The editor-in-chief is Craig S. Kaplan (University of Waterloo, Canada).\n\n"}
{"id": "14258729", "url": "https://en.wikipedia.org/wiki?curid=14258729", "title": "Kleene–Rosser paradox", "text": "Kleene–Rosser paradox\n\nIn mathematics, the Kleene–Rosser paradox is a paradox that shows that certain systems of formal logic are inconsistent, in particular the version of Curry's combinatory logic introduced in 1930, and Church's original lambda calculus, introduced in 1932–1933, both originally intended as systems of formal logic. The paradox was exhibited by Stephen Kleene and J. B. Rosser in 1935.\n\nKleene and Rosser were able to show that both systems are able to characterize and enumerate their provably total, definable number-theoretic functions, which enabled them to construct a term that essentially replicates the Richard paradox in formal language.\n\nCurry later managed to identify the crucial ingredients of the calculi that allowed the construction of this paradox, and used this to construct a much simpler paradox, now known as Curry's paradox.\n\n\n"}
{"id": "45189570", "url": "https://en.wikipedia.org/wiki?curid=45189570", "title": "Latvian Mathematical Society", "text": "Latvian Mathematical Society\n\nThe Latvian Mathematical Society (in Latvian: \"Latvijas Matemātikas Biedrība\", LMB) is a learned society of mathematicians from Latvia, recognized by the International Mathematical Union as the national mathematical organization for its country. Its goals are stimulating mathematical activity in Latvia while consolidating the former achievements, and it has the responsibility of representing the Latvian mathematicians at the international level. It was founded in 1993.\n\nThe current president is Andrejs Reinfelds, from the University of Latvia, Riga.\n"}
{"id": "4190174", "url": "https://en.wikipedia.org/wiki?curid=4190174", "title": "Limitation of size", "text": "Limitation of size\n\nIn the philosophy of mathematics, specifically the philosophical foundations of set theory, limitation of size is a concept developed by Philip Jourdain and/or Georg Cantor to avoid Cantor's paradox. It identifies certain \"inconsistent multiplicities\", in Cantor's terminology, that cannot be sets because they are \"too large\". In modern terminology these are called proper classes.\n\nThe axiom of limitation of size is an axiom in some versions of von Neumann–Bernays–Gödel set theory or Morse–Kelley set theory. This axiom says that any class which is not \"too large\" is a set, and a set cannot be \"too large\". \"Too large\" is defined as being large enough that the class of all sets can be mapped one-to-one into it.\n"}
{"id": "23992011", "url": "https://en.wikipedia.org/wiki?curid=23992011", "title": "List of derivatives and integrals in alternative calculi", "text": "List of derivatives and integrals in alternative calculi\n\nThere are many alternatives to the classical calculus of Newton and Leibniz; for example, each of the infinitely many non-Newtonian calculi. Occasionally an alternative calculus is more suited than the classical calculus for expressing a given scientific or mathematical idea.\n\nThe table below is intended to assist people working with the alternative calculus called the \"geometric calculus\" (or its discrete analog). Interested readers are encouraged to improve the table by inserting citations for verification, and by inserting more functions and more calculi.\n\nIn the following table formula_1 is the digamma function, formula_2 is the K-function, formula_3 is subfactorial, formula_4 are the generalized to real numbers Bernoulli polynomials.\n\n\n"}
{"id": "32869192", "url": "https://en.wikipedia.org/wiki?curid=32869192", "title": "List of dualities", "text": "List of dualities\n\nIn mathematics, a duality, generally speaking, translates concepts, theorems or mathematical structures into other concepts, theorems or structures, in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of \"A\" is \"B\", then the dual of \"B\" is \"A\".\n\n\n\n\n"}
{"id": "5971816", "url": "https://en.wikipedia.org/wiki?curid=5971816", "title": "List of mathematicians (L)", "text": "List of mathematicians (L)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "347836", "url": "https://en.wikipedia.org/wiki?curid=347836", "title": "List of polynomial topics", "text": "List of polynomial topics\n\nThis is a list of polynomial topics, by Wikipedia page. See also trigonometric polynomial, list of algebraic geometry topics.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "408108", "url": "https://en.wikipedia.org/wiki?curid=408108", "title": "List of probability topics", "text": "List of probability topics\n\nThis is a list of probability topics, by Wikipedia page.\nIt overlaps with the (alphabetical) list of statistical topics. There are also the outline of probability and catalog of articles in probability theory. For distributions, see List of probability distributions. For journals, see list of probability journals. For contributors to the field, see list of mathematical probabilists and list of statisticians.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "27986832", "url": "https://en.wikipedia.org/wiki?curid=27986832", "title": "Macroscopic traffic flow model", "text": "Macroscopic traffic flow model\n\nA Macroscopic traffic flow model is a mathematical traffic model that formulates the relationships among traffic flow characteristics like density, flow, mean speed of a traffic stream, etc.. Such models are conventionally arrived at by integrating microscopic traffic flow models and converting the single-entity level characteristics to comparable system level characteristics.\n\nThe method of modeling traffic flow at macroscopic level originated under an assumption that traffic streams as a whole are comparable to fluid streams. The first major step in macroscopic modeling of traffic was taken by Lighthill and Whitham in 1955, when they indexed the comparability of ‘traffic flow on long crowded roads’ with ‘flood movements in long rivers’. A year later, Richards (1956) complemented the idea with the introduction of ‘shock-waves on the highway’, completing the so-called LWR model. \nMacroscopic modeling may be primarily classified according to the type of traffic as homogeneous and heterogeneous, and further with respect to the order of the mathematical model.\n\n"}
{"id": "26146516", "url": "https://en.wikipedia.org/wiki?curid=26146516", "title": "Mathematical knowledge management", "text": "Mathematical knowledge management\n\nMathematical knowledge management (MKM) is the study of how society can effectively make use of the vast and growing literature on mathematics. It studies approaches such as databases of mathematical knowledge, automated processing of formulae and the use of semantic information, and artificial intelligence. Mathematics is particularly suited to a systematic study of automated knowledge processing due to the high degree of interconnectedness between different areas of mathematics.\n\n\n"}
{"id": "18902", "url": "https://en.wikipedia.org/wiki?curid=18902", "title": "Mathematician", "text": "Mathematician\n\nA mathematician is someone who uses an extensive knowledge of mathematics in his or her work, typically to solve mathematical problems.\n\nMathematics is concerned with numbers, data, quantity, structure, space, models, and change.\n\nOne of the earliest known mathematicians was Thales of Miletus (c. 624–c.546 BC); he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.\n\nThe number of known mathematicians grew when Pythagoras of Samos (c. 582–c. 507 BC) established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was \"All is number\". It was the Pythagoreans who coined the term \"mathematics\", and with whom the study of mathematics for its own sake begins.\n\nThe first woman mathematician recorded by history was Hypatia of Alexandria (AD 350 - 415). She succeeded her father as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria punished her, presuming she was involved, by stripping her naked and scraping off her skin with clamshells (some say roofing tiles).\n\nScience and mathematics in the Islamic world during the Middle Ages followed various models and modes of funding varied based primarily on scholars. It was extensive patronage and strong intellectual policies implemented by specific rulers that allowed scientific knowledge to develop in many areas. Funding for translation of scientific texts in other languages was ongoing throughout the reign of certain caliphs, and it turned out that certain scholars became experts in the works they translated and in turn received further support for continuing to develop certain sciences. As these sciences received wider attention from the elite, more scholars were invited and funded to study particular sciences. An example of a translator and mathematician who benefited from this type of support was al-Khawarizmi. A notable feature of many scholars working under Muslim rule in medieval times is that they were often polymaths. Examples include the work on optics, maths and astronomy of Ibn al-Haytham.\n\nThe Renaissance brought an increased emphasis on mathematics and science to Europe. During this period of transition from a mainly feudal and ecclesiastical culture to a predominantly secular one, many notable mathematicians had other occupations: Luca Pacioli (founder of accounting); Niccolò Fontana Tartaglia (notable engineer and bookkeeper); Gerolamo Cardano (earliest founder of probability and binomial expansion); Robert Recorde (physician) and François Viète (lawyer).\n\nAs time passed, many mathematicians gravitated towards universities. An emphasis on free thinking and experimentation had begun in Britain's oldest universities beginning in the seventeenth century at Oxford with the scientists Robert Hooke and Robert Boyle, and at Cambridge where Isaac Newton was Lucasian Professor of Mathematics & Physics. Moving into the 19th century, the objective of universities all across Europe evolved from teaching the “regurgitation of knowledge” to “encourag[ing] productive thinking.” In 1810, Humboldt convinced the King of Prussia to build a university in Berlin based on Friedrich Schleiermacher’s liberal ideas; the goal was to demonstrate the process of the discovery of knowledge and to teach students to “take account of fundamental laws of science in all their thinking.” Thus, seminars and laboratories started to evolve.\n\nBritish universities of this period adopted some approaches familiar to the Italian and German universities, but as they already enjoyed substantial freedoms and autonomy the changes there had begun with the Age of Enlightenment, the same influences that inspired Humboldt. The Universities of Oxford and Cambridge emphasized the importance of research, arguably more authentically implementing Humboldt’s idea of a university than even German universities, which were subject to state authority. Overall, science (including mathematics) became the focus of universities in the 19th and 20th centuries. Students could conduct research in seminars or laboratories and began to produce doctoral theses with more scientific content. According to Humboldt, the mission of the University of Berlin was to pursue scientific knowledge. The German university system fostered professional, bureaucratically regulated scientific research performed in well-equipped laboratories, instead of the kind of research done by private and individual scholars in Great Britain and France. In fact, Rüegg asserts that the German system is responsible for the development of the modern research university because it focused on the idea of “freedom of scientific research, teaching and study.”\n\nMathematicians usually cover a breadth of topics within mathematics in their undergraduate education, and then proceed to specialize in topics of their own choice at the graduate level. In some universities, a qualifying exam serves to test both the breadth and depth of a student's understanding of mathematics; the students, who pass, are permitted to work on a doctoral dissertation.\n\nMathematicians involved with solving problems with applications in real life are called applied mathematicians. Applied mathematicians are mathematical scientists who, with their specialized knowledge and professional methodology, approach many of the imposing problems presented in related scientific fields. With professional focus on a wide variety of problems, theoretical systems, and localized constructs, applied mathematicians work regularly in the study and formulation of mathematical models. Mathematicians and applied mathematicians are considered to be two of the STEM (science, technology, engineering, and mathematics) careers.\n\nThe discipline of applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry; thus, \"applied mathematics\" is a mathematical science with specialized knowledge. The term \"applied mathematics\" also describes the professional specialty in which mathematicians work on problems, often concrete but sometimes abstract. As professionals focused on problem solving, \"applied mathematicians\" look into the \"formulation, study, and use of mathematical models\" in science, engineering, business, and other areas of mathematical practice.\n\nPure mathematics is mathematics that studies entirely abstract concepts. From the eighteenth century onwards, this was a recognized category of mathematical activity, sometimes characterized as \"speculative mathematics\", and at variance with the trend towards meeting the needs of navigation, astronomy, physics, economics, engineering, and other applications.\n\nAnother insightful view put forth is that \"pure mathematics is not necessarily applied mathematics\": it is possible to study abstract entities with respect to their intrinsic nature, and not be concerned with how they manifest in the real world. Even though the pure and applied viewpoints are distinct philosophical positions, in practice there is much overlap in the activity of pure and applied mathematicians.\n\nTo develop accurate models for describing the real world, many applied mathematicians draw on tools and techniques that are often considered to be \"pure\" mathematics. On the other hand, many pure mathematicians draw on natural and social phenomena as inspiration for their abstract research.\n\nMany professional mathematicians also engage in the teaching of mathematics. Duties may include:\n\nMany careers in mathematics outside of universities involve consulting. For instance, actuaries assemble and analyze data to estimate the probability and likely cost of the occurrence of an event such as death, sickness, injury, disability, or loss of property. Actuaries also address financial questions, including those involving the level of pension contributions required to produce a certain retirement income and the way in which a company should invest resources to maximize its return on investments in light of potential risk. Using their broad knowledge, actuaries help design and price insurance policies, pension plans, and other financial strategies in a manner which will help ensure that the plans are maintained on a sound financial basis.\n\nAs another example, mathematical finance will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. Mathematical consistency is required, not compatibility with economic theory. Thus, for example, while a financial economist might study the structural reasons why a company may have a certain share price, a financial mathematician may take the share price as a given, and attempt to use stochastic calculus to obtain the corresponding value of derivatives of the stock (\"see: Valuation of options; Financial modeling\").\n\nAccording to the Dictionary of Occupational Titles occupations in mathematics include the following.\n\n\nThe following are quotations about mathematicians, or by mathematicians.\n\nThere is no Nobel Prize in mathematics, though sometimes mathematicians have won the Nobel Prize in a different field, such as economics. Prominent prizes in mathematics include the Abel Prize, the Chern Medal, the Fields Medal, the Gauss Prize, the Nemmers Prize, the Balzan Prize, the Crafoord Prize, the Shaw Prize, the Steele Prize, the Wolf Prize, the Schock Prize, and the Nevanlinna Prize.\n\nThe American Mathematical Society, Association for Women in Mathematics, and other mathematical societies offer several prizes aimed at increasing the representation of women and minorities in the future of mathematics.\n\nSeveral well known mathematicians have written autobiographies in part to explain to a general audience what it is about mathematics that has made them want to devote their lives to its study. These provide some of the best glimpses into what it means to be a mathematician. The following list contains some works that are not autobiographies, but rather essays on mathematics and mathematicians with strong autobiographical elements.\n\n\n"}
{"id": "4003614", "url": "https://en.wikipedia.org/wiki?curid=4003614", "title": "Multiscale modeling", "text": "Multiscale modeling\n\nIn engineering, mathematics, physics, chemistry, bioinformatics, computational biology, meteorology and computer science, multiscale modeling or multiscale mathematics is the field of solving problems which have important features at multiple scales of time and/or space. Important problems include multiscale modeling of fluids, solids, polymers, proteins, nucleic acids as well as various physical and chemical phenomena (like adsorption, chemical reactions, diffusion).\n\nHorstemeyer 2009, 2012 presented a historical review of the different disciplines (solid mechanics, numerical methods, mathematics, physics, and materials science) for solid materials related to multiscale materials modeling.\n\nThe aforementioned DOE multiscale modeling efforts were hierarchical in nature. The first concurrent multiscale model occurred when Michael Ortiz (Caltech) took the molecular dynamics code, Dynamo, (developed by Mike Baskes at Sandia National Labs) and with his students embedded it into a finite element code for the first time. Martin Karplus, Michael Levitt, Arieh Warshel 2013 were awarded a Nobel Prize in Chemistry for the development of a multiscale model method using both classical and quantum mechanical theory which were used to model large complex chemical systems and reactions.\n\nIn physics and chemistry, multiscale modeling is aimed to calculation of material properties or system behavior on one level using information or models from different levels. On each level particular approaches are used for description of a system. The following levels are usually distinguished: level of quantum mechanical models (information about electrons is included), level of molecular dynamics models (information about individual atoms is included), coarse-grained models (information about atoms and/or groups of atoms is included), mesoscale or nano level (information about large groups of atoms and/or molecule positions is included), level of continuum models, level of device models. Each level addresses a phenomenon over a specific window of length and time. Multiscale modeling is particularly important in integrated computational materials engineering since it allows the prediction of material properties or system behavior based on knowledge of the process-structure-property relationships.\n\nIn operations research, multiscale modeling addresses challenges for decision makers which come from multiscale phenomena across organizational, temporal and spatial scales. This theory fuses decision theory and multiscale mathematics and is referred to as multiscale decision-making. Multiscale decision-making draws upon the analogies between physical systems and complex man-made systems.\n\nIn meteorology, multiscale modeling is the modeling of interaction between weather systems of different spatial and temporal scales that produces the weather that we experience. The most challenging task is to model the way through which the weather systems interact as models cannot see beyond the limit of the model grid size. In other words, to run an atmospheric model that is having a grid size (very small ~ ) which can see each possible cloud structure for the whole globe is computationally very expensive. On the other hand, a computationally feasible Global climate model (GCM), with grid size ~ , cannot see the smaller cloud systems. So we need to come to a balance point so that the model becomes computationally feasible and at the same time we do not lose much information, with the help of making some rational guesses, a process called Parametrization.\n\nBesides the many specific applications, one area of research is methods for the accurate and efficient solution of multiscale modeling problems. The primary areas of mathematical and algorithmic development include:\n\n\n\n"}
{"id": "312648", "url": "https://en.wikipedia.org/wiki?curid=312648", "title": "Mutual exclusivity", "text": "Mutual exclusivity\n\nIn logic and probability theory, two events (or propositions) are mutually exclusive or disjoint if they cannot both occur at the same time(be true). A clear example is the set of outcomes of a single coin toss, which can result in either heads or tails, but not both.\n\nIn the coin-tossing example, both outcomes are, in theory, collectively exhaustive, which means that at least one of the outcomes must happen, so these two possibilities together exhaust all the possibilities. However, not all mutually exclusive events are collectively exhaustive. For example, the outcomes 1 and 4 of a single roll of a six-sided die are mutually exclusive (both cannot happen at the same time) but not collectively exhaustive (there are other possible outcomes; 2,3,5,6).\n\nIn logic, two mutually exclusive propositions are propositions that logically cannot be true in the same sense at the same time. To say that more than two propositions are mutually exclusive, depending on context, means that one cannot be true if the other one is true, or at least one of them cannot be true. The term \"pairwise mutually exclusive\" always means that two of them cannot be true simultaneously.\n\nIn probability theory, events \"E\", \"E\", ..., \"E\" are said to be mutually exclusive if the occurrence of any one of them implies the non-occurrence of the remaining \"n\" − 1 events. Therefore, two mutually exclusive events cannot both occur. Formally said, the intersection of each two of them is empty (the null event): \"A\" ∩ \"B\" = ∅. In consequence, mutually exclusive events have the property: P(\"A\" ∩ \"B\") = 0.\n\nFor example, it is impossible to draw a card that is both red and a club because clubs are always black. If just one card is drawn from the deck, either a red card (heart or diamond) or a black card (club or spade) will be drawn. When \"A\" and \"B\" are mutually exclusive, P(\"A\" ∪ \"B\") = P(\"A\") + P(\"B\"). To find the probability of drawing a red card or a club, for example, add together the probability of drawing a red card and the probability of drawing a club. In a standard 52-card deck, there are twenty-six red cards and thirteen clubs: 26/52 + 13/52 = 39/52 or 3/4.\n\nOne would have to draw at least two cards in order to draw both a red card and a club. The probability of doing so in two draws depends on whether the first card drawn were replaced before the second drawing, since without replacement there is one fewer card after the first card was drawn. The probabilities of the individual events (red, and club) are multiplied rather than added. The probability of drawing a red and a club in two drawings without replacement is then 26/52 × 13/51 × 2 = 676/2652, or 13/51. With replacement, the probability would be 26/52 × 13/52 × 2 = 676/2704, or 13/52.\n\nIn probability theory, the word \"or\" allows for the possibility of both events happening. The probability of one or both events occurring is denoted P(\"A\" ∪ \"B\") and in general it equals P(\"A\") + P(\"B\") – P(\"A\" ∩ \"B\"). Therefore, in the case of drawing a red card or a king, drawing any of a red king, a red non-king, or a black king is considered a success. In a standard 52-card deck, there are twenty-six red cards and four kings, two of which are red, so the probability of drawing a red or a king is 26/52 + 4/52 – 2/52 = 28/52.\n\nEvents are collectively exhaustive if all the possibilities for outcomes are exhausted by those possible events, so at least one of those outcomes must occur. The probability that at least one of the events will occur is equal to one. For example, there are theoretically only two possibilities for flipping a coin. Flipping a head and flipping a tail are collectively exhaustive events, and there is a probability of one of flipping either a head or a tail. Events can be both mutually exclusive and collectively exhaustive. In the case of flipping a coin, flipping a head and flipping a tail are also mutually exclusive events. Both outcomes cannot occur for a single trial (i.e., when a coin is flipped only once). The probability of flipping a head and the probability of flipping a tail can be added to yield a probability of 1: 1/2 + 1/2 =1.\n\nIn statistics and regression analysis, an independent variable that can take on only two possible values is called a dummy variable. For example, it may take on the value 0 if an observation is of a male subject or 1 if the observation is of a female subject. The two possible categories associated with the two possible values are mutually exclusive, so that no observation falls into more than one category, and the categories are exhaustive, so that every observation falls into some category. Sometimes there are three or more possible categories, which are pairwise mutually exclusive and are collectively exhaustive — for example, under 18 years of age, 18 to 64 years of age, and age 65 or above. In this case a set of dummy variables is constructed, each dummy variable having two mutually exclusive and jointly exhaustive categories — in this example, one dummy variable (called D) would equal 1 if age is less than 18, and would equal 0 \"otherwise\"; a second dummy variable (called D) would equal 1 if age is in the range 18-64, and 0 otherwise. In this set-up, the dummy variable pairs (D, D) can have the values (1,0) (under 18), (0,1) (between 18 and 64), or (0,0) (65 or older) (but not (1,1), which would nonsensically imply that an observed subject is both under 18 and between 18 and 64). Then the dummy variables can be included as independent (explanatory) variables in a regression. Note that the number of dummy variables is always one less than the number of categories: with the two categories male and female there is a single dummy variable to distinguish them, while with the three age categories two dummy variables are needed to distinguish them.\n\nSuch qualitative data can also be used for dependent variables. For example, a researcher might want to predict whether someone goes to college or not, using family income, a gender dummy variable, and so forth as explanatory variables. Here the variable to be explained is a dummy variable that equals 0 if the observed subject does not go to college and equals 1 if the subject does go to college. In such a situation, ordinary least squares (the basic regression technique) is widely seen as inadequate; instead probit regression or logistic regression is used. Further, sometimes there are three or more categories for the dependent variable — for example, no college, community college, and four-year college. In this case, the multinomial probit or multinomial logit technique is used.\n\n\n"}
{"id": "41847276", "url": "https://en.wikipedia.org/wiki?curid=41847276", "title": "One-shot deviation principle", "text": "One-shot deviation principle\n\nThe one-shot deviation principle (also known as one-deviation property) is the principle of optimality of dynamic programming applied to game theory. It says that a strategy profile of a finite extensive-form game is a subgame perfect equilibrium (SPE) if and only if there exist no profitable one-shot deviations for each subgame and every player. In simpler terms, if no player can increase their payoffs by deviating a single decision, or period, from their original strategy, then the strategy that they have chosen is a SPE. As a result, no player can profit from deviating from the strategy for one period and then reverting to the strategy. \n\nFurthermore, the one-shot deviation principle is very important for infinite horizon games, in which the principle typically does not hold, since it is not plausible to consider an infinite number of strategies and payoffs in order to solve. In an infinite horizon game where the discount factor is less than 1, a strategy profile is a subgame perfect equilibrium if and only if it satisfies the one-shot deviation principle.\n\nThe following is the paraphrased definition from Watson (2013)\n\nTo check whether strategy \"s\" is a subgame perfect Nash equilibrium, we have to ask every player \"i\" and every subgame, if considering \"s\", there is a strategy \"s’\" that yields a strictly higher payoff for player \"i\" than does \"s\" in the subgame. This analysis is equivalent to looking at single deviations from \"s\", meaning \"s’\" differs from s at only one information set. Note that the choices associated with \"s\" and \"s’\" are the same at all nodes that are successors of nodes in the information set where s and \"s’\" prescribe different actions. \n\nConsider a symmetric game with two players in which each player makes binary choice decisions, A or B, in three sequences. There are 8 (2) total number of pure strategies for each player: {AAA, AAB, ABA, ABB, BBB, BBA, BAB, BAA}. In this example, consider that a player chooses strategy (AAA). To check whether this strategy is a SPE, the one-shot deviation principle states that the player needs to check the payoffs of only three other strategies which differ from the original strategy by a single deviation, instead of all seven others. These three strategies are: (BAA), (ABA), and (AAB). If none of these three strategies yields a higher payoff than (AAA), then the player can conclude that (AAA) is a SPE.\n"}
{"id": "6473626", "url": "https://en.wikipedia.org/wiki?curid=6473626", "title": "Outline of geometry", "text": "Outline of geometry\n\nGeometry is a branch of mathematics concerned with questions of shape, size, relative position of figures, and the properties of space. Geometry is one of the oldest mathematical sciences.\n\n\n\nHistory of geometry\n\n\n\n\n\n\n\n"}
{"id": "39350271", "url": "https://en.wikipedia.org/wiki?curid=39350271", "title": "Parity plot", "text": "Parity plot\n\nA parity plot is a scatterplot that compares experimental data against tabulated data. Each point has coordinates (\"x\", \"y\"), where \"x\" is the tabulated value, and \"y\" is the corresponding experimental value.\n\nA line of the equation \"y\" = \"x\" is sometimes added as a reference. When an experimental value equals a tabulated value, the point will lie on the line.\n\nParity plots are found in scientific papers and reports, when the author wishes to compare his data to accepted values in an easily understandable way.\n\n"}
{"id": "58472252", "url": "https://en.wikipedia.org/wiki?curid=58472252", "title": "Postmodern mathematics", "text": "Postmodern mathematics\n\nPostmodern mathematics is a thought developed as a result of postmodernism. The theory asserts that there is no such thing as ‘absolutism’ or ultimate truth in mathematics. It also declares that the term ‘mathematics’ can’t be used to define a specific object. This thought emerged from the post modernistic idea of the relativity of truth. The thought also maintains that the ideas in mathematics are subjective and that there is no ‘right’ answer to a mathematical question.\n\nThe ideas affirmed in Postmodern Mathematics can be summed up as follows:\n\nThe term 'Mathematics', in its singular form, cannot be used to describe any object. However, mathematics, in its plural form, describes the multiplicity of this term based on the context it is used in. All these various definitions are not necessarily true or false and can be right or wrong based on the context they are defined in.\n\nPostmodernism rejects the idea of universality in mathematics. It claims that there is no such thing as 'absolutism' and asserts the notion of multiplicity in mathematics. This comes from the postmodernist idea that rejects notions of absolutism and the claim that the knowledge we attain as ‘true’ is only a representation constructed by society which is not truer than any other representations. Postmodernists also reject the absolutist idea of mathematics being “human free” and objective and instead assert a “humanistic” view of mathematics which is filled with subjective and humane values. This view sees mathematics as: imbued with moral and social values which play a significant role in the development and applications of mathematics.\n\nCertainty in mathematics is not attainable. The answers proposed are perceived as probabilities and not certainties. Postmodernism in mathematics oppose Aristotelian and modernist concept of an object being either true or false and instead asserts the concept of a “degree of truth”. Thus, proclaiming the notion of partial truths and uncertainty. According to the Postmodernist thought, theories in mathematics accepted as 'true' and 'certain' are products of society as the criteria for 'certainty' and 'truth' are socially constructed. Therefore, postmodernism rejects the notion of certainty and absolute truth in mathematics as certainties in mathematics are not static and can change throughout time and societies. This is significant as mathematics is usually perceived as the \"most certain part of human knowledge\", questioning the certainty of this part of knowledge thus rejects certainty from any form of human knowledge.\n\nMathematics is a corrigible discipline as it is subject to the notion of fallibility and is in a state of constant change. This theory was first proposed by Lakatos in his book \"Proofs and Refutation\". It also constitutes the hypothetical-deductive system of mathematics which declares that the discipline of mathematics is fallible and corrigible, the development of theorems require the falsification of “falsifiers” and transmission to hypothetical knowledge. In other words, in order to develop theorems, one must first falsify the premises under which the theorem would be falsified. This would further constitute that mathematics is connected as a part of the broad human knowledge, including the culture, language and perspectives.\n\nAs theories and laws proclaimed in mathematics are representations made by the society, mathematics is constructed by the needs of societies. Hence, holds no other value other than being representations of cultures and societies.\n\nPostmodernism was a movement that started against modernism. Modernism values the faith in the existence of an objective and universal truth. For modernists, knowledge is the use of empirical methodologies in order to discover the ‘ultimate truth’. Postmodernism counters that knowledge isn’t as empirical and logical as modernism persists, rather it is subjective and is open to change. For postmodernists, knowledge is a construct of the society and is subject to change over time and place.\n\nIn mathematics, modernism asserts notions of Platonism (the view that theories in mathematics are unchangeable and perceives mathematics as perfect and eternal), Logicism (the view that perceives mathematics as a part of Logic) and Formalism. These concepts were accepted universally before the introduction of postmodernism. This change in thoughts brought about by the introduction of postmodernism caused a shift of mathematics research and practice from “logical theories” and challenged the notion of objectivity, universality and certainty in mathematics.\n\nUnlike modernist ideas of singularity, postmodernism denotes the idea of plurality and polycentrism. Postmodernism also promotes the idea of collaboration between the teachers and students of mathematics as it argues for the idea of everyone being an explorer of mathematics. Modernism, on the other hand, propose no question for the nature of mathematics or education as they contend that the two entities are not related to one another. In this thought, students in search of education, mathematics in this context, are exploring an “independent world”. This independent world remains unhinged by the student’s exploration and is not changed by the students’ encounter as they are two separate entities. Postmodernism, on the other hand, encourages the transformation of knowledge by the student as they believe that the two only exist “relative to one another”.\n\nLudwig Wittgenstein (1889-1951) developed a notion against the modernist idea of rationality and claimed that it is isn’t as evident and clear as perceived by modernists. He claimed that certainty in mathematics is “a collection of language games” and truth and false are based on the their following of “the rules of linguistic games”. It is what human beings say that is true and false: and they agree in the language they use.\n\nKarl Popper (1902-1994) introduced the notion of falsification and asserted that to prove theories in sciences, researchers should strive to disprove them. He also asserted that the laws in mathematics are subject to being proven false and hence rejected the idea of ultimate truths.\n\nImre Lakatos (1922-1974) was inspired by Karl Popper's idea of falsification in mathematics and wrote his thesis on the fallibility of mathematics and mathematical theorems. In his book, titled ‘Proofs and refutations’, Lakatos claimed that all the theorems in mathematics are fallible and the theories that are accepted as ‘true’ or ‘perfect’ are only accepted because there hasn’t been any theorem that counters them. He claimed that theories in mathematics have no basis for certainty as they are products of assumptions made by humans and are hence refutable.\n\nPaul Ernest has extensive works on the philosophy of mathematics where he draws upon the works of other postmodern philosophers including Popper and Lakatos. His works mainly reflect his belief in the social constructivism of mathematics which illustrates that mathematical theories are a construct of the society and are thus changeable. He also published the \"Philosophy of Mathematics Education Journal\" which contributes to discussions of the philosophy of mathematics and mathematics education and has extensive articles on postmodern mathematics.\n\nHe introduced the idea of 'fuzzy logic'. This idea rejects the Aristotelian Law of Truth and fallibility of an object (the notion that an object is either true or false). He instead proposed the existence of 'degrees of truth', the idea that an object is characterised into various degrees of truth.\n\nThomas Kuhn (1922-1996) introduced the idea of paradigm shifts in sciences. he asserted that the paradigms and theories in sciences are not fixed but are instead on a continuum and hence are subject to transformations. He describes his theory of paradigms as: \".. each paradigm will be shown to satisfy more or less the criteria that it dictates for itself and to fall short of a few of those dictated by its opponent. .. no paradigm ever solves all the problems it defines ..\" According to Kuhn, education and knowledge strives to interpret and represent rather than provide an objective explanation. These representations are examples of social constructivism and are products of societies throughout time.\n\nIn the postmodernist view, mathematics is learnt not for knowledge but for utility. According to this thought, teachers are not only an ‘authoritative figure’ but are also co-explorers and learners of mathematics or as described by Ernest, “ringmasters of the mathematics circus”. They also assert on the role of students in the teaching mathematics and their role in the shaping of curriculum. Postmodern mathematics asserts the diminishing of the notion of objectivity and absolutism in mathematics thus asserting that a “notion of 2 + 2 = 1” can be true given certain subjective circumstances. However, it doesn’t mean that the notion can be used based on any ‘personal situation’ rather it is relative to the mathematical context and situation it is used in such that 2 +2 = 1 is true in mod (3) arithmetic but it would never be true in any other mathematical context. Hence, mathematical theories are corrigible based on the mathematical contexts they are used in.\n\nUnlike the modernist idea of teachers being the authoritative figure and “echo-phrasing” the textbooks, postmodernism encourages collaborative work between the students and the teachers which again relates to the notion of ‘truth’ and ‘knowledge’ being subjective. This would also emphasise the notion that students should be given the opportunity to exercise alternate methods and solutions to the ones determined by the teachers.\n\nIf mathematics education is reshaped according to the postmodernist thought, it opens up windows for the implementation of mathematics by the students to their everyday life, culture and language. It enables them to reshape their knowledge of mathematics and concepts of mathematics.Mathematics becomes responsible for its uses and consequences, in education and society. Those of us in education have a special reason for wanting this more human view of mathematics. Anything else alienates and dis-empowers learners.Postmodern education also encourages the use of technology and computers in the discovery of new ideas in mathematics. Although these ideas are subject to uncertainty as denoted by postmodernism, they can still have the same or higher ‘degree of truth’ as those proven and discovered by classical methods.\n\nThe rejection of uncertainty also asserts researchers of mathematics and other knowledge to strive to achieve a higher degree of truth for human knowledge and hence encourages further research and study. This in turn expands the human knowledge vastly.\n\nPostmodern education encourages teachers, students and researchers of mathematics to focus on criticism of the known mathematical facts rather than evaluation. It asserts that mathematicians should focus not on what ‘mathematics’ is but rather what it could be and what it might be. It encourages constant criticism of the mathematical theories and trying to disprove the theories in order to get a higher degree of truth in mathematics education. Jean-François Lyotard describes this as Reality (certainty in this context) is not expressed by a phrase like X is such but by one like X is such and not such.He also claims that to prove the ‘reality’ or the ‘truth’ of a description of an entity, which in this context would be mathematics, the negation of another description is needed.\n\n"}
{"id": "1622235", "url": "https://en.wikipedia.org/wiki?curid=1622235", "title": "Probability distribution function", "text": "Probability distribution function\n\nA probability distribution function is some function that may be used to define a particular probability distribution. Depending upon which text is consulted, the term may refer to:\nThe similar term probability function may mean any of the above and, in addition,\n"}
{"id": "358069", "url": "https://en.wikipedia.org/wiki?curid=358069", "title": "Proof by infinite descent", "text": "Proof by infinite descent\n\nIn mathematics, a proof by infinite descent is a particular kind of proof by contradiction that relies on the least integer principle. One typical application is to show that a given equation has no solutions.\n\nTypically, one shows that if a solution to a problem existed, which in some sense was related to one or more natural numbers, it would necessarily imply that a second solution existed, which was related to one or more 'smaller' natural numbers. This in turn would imply a third solution related to smaller natural numbers, implying a fourth solution, therefore a fifth solution, and so on. However, there cannot be an infinity of ever-smaller natural numbers, and therefore by mathematical induction (repeating the same step) the original premise—that any solution exists— is incorrect: its correctness produces a contradiction.\n\nAn alternative way to express this is to assume one or more solutions or examples exists. Then there must be a smallest solution or example—a minimal counterexample. We then prove that if a smallest solution exists, it must imply the existence of a smaller solution (in some sense)—which again proves that the existence of any solution would lead to a contradiction.\n\nThe earliest uses of the method of infinite descent appear in Euclid's \"Elements\". A typical example is Proposition 31 of Book 7, in which Euclid proves that every composite integer is divided (in Euclid's terminology \"measured\") by some prime number. \n\nThe method was much later developed by Fermat, who coined the term and often used it for Diophantine equations. Two typical examples are showing the non-solvability of the Diophantine equation \"r\" + \"s\" = \"t\" and proving Fermat's theorem on sums of two squares, which states that an odd prime \"p\" can be expressed as a sum of two squares only when \"p\" ≡ 1 (mod 4) (see proof). In some cases, to the modern eye, his \"method of infinite descent\" is an exploitation of the inversion of the doubling function for rational points on an elliptic curve \"E\". The context is of a hypothetical non-trivial rational point on \"E\". Doubling a point on \"E\" roughly doubles the length of the numbers required to write it (as number of digits), so that a \"halving\" a point gives a rational with smaller terms. Since the terms are positive, they cannot decrease forever. In this way Fermat was able to show the non-existence of solutions in many cases of Diophantine equations of classical interest (for example, the problem of four perfect squares in arithmetic progression).\n\nIn the number theory of the twentieth century, the infinite descent method was taken up again, and pushed to a point where it connected with the main thrust of algebraic number theory and the study of L-functions. The structural result of Mordell, that the rational points on an elliptic curve \"E\" form a finitely-generated abelian group, used an infinite descent argument based on \"E\"/2\"E\" in Fermat's style.\n\nTo extend this to the case of an abelian variety \"A\", André Weil had to make more explicit the way of quantifying the size of a solution, by means of a height function – a concept that became foundational. To show that \"A\"(\"Q\")/2\"A\"(\"Q\") is finite, which is certainly a necessary condition for the finite generation of the group \"A\"(\"Q\") of rational points of \"A\", one must do calculations in what later was recognised as Galois cohomology. In this way, abstractly-defined cohomology groups in the theory become identified with \"descents\" in the tradition of Fermat. The Mordell–Weil theorem was at the start of what later became a very extensive theory.\n\nThe proof that the square root of 2 () is irrational (i.e. cannot be expressed as a fraction of two whole numbers) was discovered by the ancient Greeks, and is perhaps the earliest known example of a proof by infinite descent. Pythagoreans discovered that the diagonal of a square is incommensurable with its side, or in modern language, that the square root of two is irrational. Little is known with certainty about the time or circumstances of this discovery, but the name of Hippasus of Metapontum is often mentioned. For a while, the Pythagoreans treated as an official secret the discovery that the square root of two is irrational, and, according to legend, Hippasus was murdered for divulging it. The square root of two is occasionally called \"Pythagoras' number\" or \"Pythagoras' Constant\", for example .\n\nThe ancient Greeks, not having algebra, worked out a geometric proof by infinite descent (John Horton Conway presented another geometric proof (no. 8 ' ' ' ) by infinite descent that may be more accessible). The following is an algebraic proof along similar lines:\n\nSuppose that were rational. Then it could be written as\n\nfor two natural numbers, and . Then squaring would give\n\nso 2 must divide \"p\". Because 2 is a prime number, it must also divide \"p\", by Euclid's lemma. So \"p\" = 2\"r\", for some integer \"r\".\n\nBut then\n\nwhich shows that 2 must divide \"q\" as well. So \"q\" = 2\"s\" for some integer \"s\".\n\nThis gives\n\nTherefore, if could be written as a rational number, it could always be written as a rational number with smaller parts, which itself could be written with yet-smaller parts, \"ad infinitum\". But this is impossible in the set of natural numbers. Since is a real number, which can be either rational or irrational, the only option left is for to be irrational.\n\n(Alternatively, this proves that if were rational, no \"smallest\" representation as a fraction could exist, as any attempt to find a \"smallest\" representation \"p\"/\"q\" would imply a smaller one existed, which is a similar contradiction).\n\nFor positive integer \"k\", suppose that is not an integer, but is rational and can be expressed as ⁄ for natural numbers \"m\" and \"n\", and let \"q\" be the largest integer no greater than . Then\n\nThe numerator and denominator were each multiplied by the expression ( − \"q\")—which is positive but less than 1—and then simplified independently. So two resulting products, say \"m' \" and \"n' \", are themselves integers, which are less than \"m\" and \"n\" respectively. Therefore, no matter what natural numbers \"m\" and \"n\" are used to express , there exist smaller natural numbers \"m' \" < \"m\" and \"n' \" < \"n\" that have the same ratio. But infinite descent on the natural numbers is impossible, so this disproves the original assumption that could be expressed as a ratio of natural numbers.\n\nThe non-solvability of formula_8 in integers is sufficient to show the non-solvability of formula_9 in integers, which is a special case of Fermat's Last Theorem, and the historical proofs of the latter proceeded by more broadly proving the former using infinite descent. The following more recent proof demonstrates both of these impossibilities by proving still more broadly that a Pythagorean triangle cannot have any two of its sides each either a square or twice a square, since there is no smallest such triangle:\n\nSuppose there exists such a Pythagorean triangle. Then it can be scaled down to give a primitive (i.e., with no common factors other than 1) Pythagorean triangle with the same property. Primitive Pythagorean triangles' sides can be written as formula_10 formula_11 formula_12, with \"a\" and \"b\" relatively prime and with \"a+b\" odd and hence \"y\" and \"z\" both odd. The property that \"y\" and \"z\" are each odd means that neither \"y\" nor \"z\" can be twice a square. Furthermore, if \"x\" is a square or twice a square, then each of \"a\" and \"b\" is a square or twice a square. There are three cases, depending on which two sides are postulated to each be a square or twice a square:\n\n\nIn any of these cases, one Pythagorean triangle with two sides each of which is a square or twice a square has led to a smaller one, which in turn would lead to a smaller one, etc.; since such a sequence cannot go on infinitely, the original premise that such a triangle exists must be wrong.\n\nThis implies that the equations\ncannot have non-trivial solutions, since non-trivial solutions would give Pythagorean triangles with two sides being squares.\n\nFor other similar proofs by infinite descent for the \"n\" = 4 case of Fermat's Theorem, see the articles by Grant and Perella and Barbara.\n\n\n"}
{"id": "44672530", "url": "https://en.wikipedia.org/wiki?curid=44672530", "title": "Quadratic integrate and fire", "text": "Quadratic integrate and fire\n\nThe quadratic integrate and fire (QIF) model is a biological neuron model and a type of integrate-and-fire neuron which describes action potentials in neurons. In contrast to physiologically accurate but computationally expensive neuron models like the Hodgkin–Huxley model, the QIF model seeks only to produce action potential-like patterns and ignores subtleties like gating variables, which play an important role in generating action potentials in a real neuron. However, the QIF model is incredibly easy to implement and compute, and relatively straightforward to study and understand, thus has found ubiquitous use in computational neuroscience .\n\nA quadratic integrate and fire neuron is defined by the autonomous differential equation,\n\nwhere formula_2 is a real positive constant. Note that a solution to this differential equation is the tangent function, which blows up in finite time. Thus a \"spike\" is said to have occurred when the solution reaches positive infinity, and the solution is reset to negative infinity.\n\nWhen implementing this model in computers, a threshold crossing value (formula_3) and a reset value (formula_4) is assigned, so that when the solution rises above the threshold, formula_5, the solution is immediately reset to formula_4\n"}
{"id": "54252064", "url": "https://en.wikipedia.org/wiki?curid=54252064", "title": "Resource selection function", "text": "Resource selection function\n\nResource selection functions (RSFs) are a class of functions that are used in spatial ecology to assess which habitat characteristics are important to a specific population or species of animal, by assessing the a probability of that animal using a certain resource proportional to the availability of that resource in the environment.\n\nResource Selection Functions require two types of data: location information for the wildlife in question, and data on the resources available across the study area. Resources can include a broad range of environmental and geographical variables, including categorical variables such as land cover type, or continuous variables such as average rainfall over a given time period. A variety of methods are used for modeling RSFs, with logistic regression being commonly used.\n\nRSFs can be fit to data where animal presence is known, but absence is not, such as for species where several individuals within a study area are fitted with a GPS collar, but some individuals may be present without collars.\nWhen this is the case, buffers of various distances are generated around known presence points, with a number of available points generated within each buffer, which represent areas where the animal could have been, but it is unknown whether they actually were. These models can be fit using binomial generalized linear models or binomial generalized linear mixed models, with the resources, or environmental and geographic data, as explanatory variables.\n\nResource selection functions can be modeled at a variety of spatial scales, depending on the species and the scientific question being studied. (insert one more sentence on scale)\n\nMost RSFs address one of the following scales, which were defined by Douglas Johnson in 1980 and are still used today:\n"}
{"id": "3119343", "url": "https://en.wikipedia.org/wiki?curid=3119343", "title": "Sample exclusion dimension", "text": "Sample exclusion dimension\n\nIn computational learning theory, sample exclusion dimensions arise in the study of exact concept learning with queries. \n\nIn algorithmic learning theory, a concept over a domain \"X\" is a Boolean function over \"X\". Here we only consider finite domains. A partial approximation \"S\" of a concept \"c\" is a Boolean function over formula_1 such that \"c\" is an extension to \"S\".\n\nLet \"C\" be a class of concepts and \"c\" be a concept (not necessarily in \"C\"). Then a specifying set for c w.r.t. \"C\", denoted by \"S\" is a partial approximation \"S\" of \"c\" such that \"C\" contains at most one extension to \"S\". If we have observed a specifying set for some concept w.r.t. \"C\", then we have enough information to verify a concept in \"C\" with at most one more mind change.\nThe exclusion dimension, denoted by \"XD\"(\"C\"), of a concept class is the maximum of the size of the minimum specifying set of \"c\"<nowiki>'</nowiki> with respect to \"C\", where \"c\"<nowiki>'</nowiki> is a concept not in \"C\".\n"}
{"id": "3371340", "url": "https://en.wikipedia.org/wiki?curid=3371340", "title": "String art", "text": "String art\n\nString art, or pin and thread art, is characterized by an arrangement of colored thread strung between points to form geometric patterns or representational designs such as a ship's sails, sometimes with other artist material comprising the remainder of the work. Thread, wire, or string is wound around a grid of nails hammered into a velvet-covered wooden board. Though straight lines are formed by the string, the slightly different angles and metric positions at which strings intersect gives the appearance of Bézier curves (as in the mathematical concept of envelope of a family of straight lines). Quadratic Bézier curve are obtained from strings based on two intersecting segments. Other forms of string art include Spirelli, which is used for cardmaking and scrapbooking, and curve stitching, in which string is stitched through holes.\n\nString art has its origins in the 'curve stitch' activities invented by Mary Everest Boole at the end of the 19th century to make mathematical ideas more accessible to children. It was popularised as a decorative craft in the late 1960s through kits and books.\n\nA computational form of String art that can produce photo-realistic artwork was introduced by Petros Vrellis, in 2016.\n\n\n\n"}
{"id": "819251", "url": "https://en.wikipedia.org/wiki?curid=819251", "title": "Tombstone (typography)", "text": "Tombstone (typography)\n\nThe tombstone, Halmos, end of proof, or Q.E.D. mark \"∎\" is used in mathematics to denote the end of a proof, in place of the traditional abbreviation \"Q.E.D.\" for the Latin phrase \"quod erat demonstrandum\", \"which was to be shown\". In magazines, it is one of the various symbols used to indicate the end of an article.\n\nIn Unicode, it is represented as character . Its graphic form varies. It may be a hollow or filled rectangle or square.\n\nIn AMS-LaTeX, the symbol is automatically appended at the end of a proof environment \\begin{proof} ... \\end{proof}. It can also be obtained from the commands \\qedsymbol or \\qed (the latter causes the symbol to be right aligned).\n\nIt is sometimes called a halmos after the mathematician Paul Halmos, who first used it in mathematical context. He got the idea of using it from seeing it was being used to indicate the end of articles in magazines. In his memoir \"I Want to Be a Mathematician\", he wrote the following:\n"}
{"id": "26837834", "url": "https://en.wikipedia.org/wiki?curid=26837834", "title": "University of Chicago School Mathematics Project", "text": "University of Chicago School Mathematics Project\n\nThe University of Chicago School Mathematics Project (UCSMP) is a multi-faceted project of the University of Chicago in the United States, intended to improve competency in mathematics in the United States by elevating educational standards for children in elementary and secondary schools.\n\nThe UCSMP supports educators by supplying training materials to them and offering a comprehensive mathematics curriculum at all levels of primary and secondary education. It seeks to bring international strengths into the United States, translating non-English math textbooks for English students and sponsoring international conferences on the subject of math education. Launched in 1983 with the aid of a six-year grant from Amoco, the UCSMP is used throughout the United States.\n\nUCSMP developed \"Everyday Mathematics\", a pre-K and elementary school mathematics curriculum.\n\n\n\n"}
{"id": "42596301", "url": "https://en.wikipedia.org/wiki?curid=42596301", "title": "WRF-SFIRE", "text": "WRF-SFIRE\n\nWRF-SFIRE is a coupled atmosphere-wildfire model, which combines the Weather Research and Forecasting Model (WRF) with a fire-spread model, implemented by the level-set method. A version from 2010 was released based on the WRF 3.2 as WRF-Fire.\n\n\n"}
