{"id": "43405", "url": "https://en.wikipedia.org/wiki?curid=43405", "title": "Actuary", "text": "Actuary\n\nAn actuary is a business professional who deals with the measurement and management of risk and uncertainty . The name of the corresponding field is actuarial science. These risks can affect both sides of the balance sheet and require asset management, liability management, and valuation skills . Actuaries provide assessments of financial security systems, with a focus on their complexity, their mathematics, and their mechanisms .\n\nWhile the concept of insurance dates to antiquity (, , ), the concepts needed to scientifically measure and mitigate risks have their origins in the 17th century studies of probability and annuities . Actuaries of the 21st century require analytical skills, business knowledge, and an understanding of human behavior and information systems to design and manage programs that control risk . The actual steps needed to become an actuary are usually country-specific; however, almost all processes share a rigorous schooling or examination structure and take many years to complete (, ).\n\nThe profession has consistently been ranked as one of the most desirable . In various studies, being an actuary was ranked number one or two multiple times since 2010 (, , ).\n\nActuaries use skills primarily in mathematics, particularly calculus-based probability and mathematical statistics, but also economics, computer science, finance, and business. For this reason, actuaries are essential to the insurance and reinsurance industries, either as staff employees or as consultants; to other businesses, including sponsors of pension plans; and to government agencies such as the Government Actuary's Department in the United Kingdom or the Social Security Administration in the United States of America. Actuaries assemble and analyze data to estimate the probability and likely cost of the occurrence of an event such as death, sickness, injury, disability, or loss of property. Actuaries also address financial questions, including those involving the level of pension contributions required to produce a certain retirement income and the way in which a company should invest resources to maximize its return on investments in light of potential risk. Using their broad knowledge, actuaries help design and price insurance policies, pension plans, and other financial strategies in a manner that will help ensure that the plans are maintained on a sound financial basis (, ).\n\nMost traditional actuarial disciplines fall into two main categories: life and non-life.\n\nLife actuaries, which include health and pension actuaries, primarily deal with mortality risk, morbidity risk, and investment risk. Products prominent in their work include life insurance, annuities, pensions, short and long term disability insurance, health insurance, health savings accounts, and long-term care insurance . In addition to these risks, social insurance programs are influenced by public opinion, politics, budget constraints, changing demographics, and other factors such as medical technology, inflation, and cost of living considerations (, ).\n\nNon-life actuaries, also known as property and casualty or general insurance actuaries, deal with both physical and legal risks that affect people or their property. Products prominent in their work include auto insurance, homeowners insurance, commercial property insurance, workers' compensation, malpractice insurance, product liability insurance, marine insurance, terrorism insurance, and other types of liability insurance .\n\nActuaries are also called upon for their expertise in enterprise risk management . This can involve dynamic financial analysis, stress testing, the formulation of corporate risk policy, and the setting up and running of corporate risk departments . Actuaries are also involved in other areas of the financial services industry, such as analysing securities offerings or market research .\n\nOn both the life and casualty sides, the classical function of actuaries is to calculate premiums and reserves for insurance policies covering various risks . On the casualty side, this analysis often involves quantifying the probability of a loss event, called the frequency, and the size of that loss event, called the severity. The amount of time that occurs before the loss event is important, as the insurer will not have to pay anything until after the event has occurred. On the life side, the analysis often involves quantifying how much a potential sum of money or a financial liability will be worth at different points in the future. Since neither of these kinds of analysis are purely deterministic processes, stochastic models are often used to determine frequency and severity distributions and the parameters of these distributions. Forecasting interest yields and currency movements also plays a role in determining future costs, especially on the life side .\n\nActuaries do not always attempt to predict aggregate future events. Often, their work may relate to determining the cost of financial liabilities that have already occurred, called retrospective reinsurance, or the development or re-pricing of new products.\n\nActuaries also design and maintain products and systems. They are involved in financial reporting of companies' assets and liabilities. They must communicate complex concepts to clients who may not share their language or depth of knowledge. Actuaries work under a code of ethics that covers their communications and work products .\n\nAs an outgrowth of their more traditional roles, actuaries also work in the fields of risk management and enterprise risk management for both financial and non-financial corporations . Actuaries in traditional roles study and use the tools and data previously in the domain of finance . The Basel II accord for financial institutions (2004), and its analogue, the Solvency II accord for insurance companies (to come into effect in 2016), require institutions to account for operational risk separately, and in addition to, credit, reserve, asset, and insolvency risk. Actuarial skills are well suited to this environment because of their training in analyzing various forms of risk, and judging the potential for upside gain, as well as downside loss associated with these forms of risk .\n\nActuaries are also involved in investment advice and asset management, and can be general business managers and chief financial officers (, ). They analyze business prospects with their financial skills in valuing or discounting risky future cash flows, and apply their pricing expertise from insurance to other lines of business. For example, insurance securitization requires both actuarial and finance skills . Actuaries also act as expert witnesses by applying their analysis in court trials to estimate the economic value of losses such as lost profits or lost wages .\n\nThe basic requirements of communal interests gave rise to risk sharing since the dawn of civilization. For example, people who lived their entire lives in a camp had the risk of fire, which would leave their band or family without shelter. After barter came into existence, more complex risks emerged and new forms of risk manifested. Merchants embarking on trade journeys bore the risk of losing goods entrusted to them, their own possessions, or even their lives. Intermediaries developed to warehouse and trade goods, which exposed them to financial risk. The primary providers in extended families or households ran the risk of premature death, disability or infirmity, which could leave their dependents to starve. Credit procurement was difficult if the creditor worried about repayment in the event of the borrower's death or infirmity. Alternatively, people sometimes lived too long from a financial perspective, exhausting their savings, if any, or becoming a burden on others in the extended family or society .\n\nIn the ancient world there was not always room for the sick, suffering, disabled, aged, or the poor—these were often not part of the cultural consciousness of societies . Early methods of protection, aside from the normal support of the extended family, involved charity; religious organizations or neighbors would collect for the destitute and needy. By the middle of the 3rd century, 1,500 suffering people were being supported by charitable operations in Rome . Charitable protection remains an active form of support in the modern era , but receiving charity is uncertain and is often accompanied by social stigma. Elementary mutual aid agreements and pensions did arise in antiquity . Early in the Roman empire, associations were formed to meet the expenses of burial, cremation, and monuments—precursors to burial insurance and friendly societies. A small sum was paid into a communal fund on a weekly basis, and upon the death of a member, the fund would cover the expenses of rites and burial. These societies sometimes sold shares in the building of columbāria, or burial vaults, owned by the fund . Other early examples of mutual surety and assurance pacts can be traced back to various forms of fellowship within the Saxon clans of England and their Germanic forebears, and to Celtic society .\n\nNon-life insurance started as a hedge against loss of cargo during sea travel. Anecdotal reports of such guarantees occur in the writings of Demosthenes, who lived in the 4th century BCE . The earliest records of an official non-life insurance policy come from Sicily, where there is record of a 14th-century contract to insure a shipment of wheat . In 1350, Lenardo Cattaneo assumed \"all risks from act of God, or of man, and from perils of the sea\" that may occur to a shipment of wheat from Sicily to Tunis up to a maximum of 300 florins. For this he was paid a premium of 18% .\n\nDuring the 17th century, a more scientific basis for risk management was being developed. In 1662, a London draper named John Graunt showed that there were predictable patterns of longevity and death in a defined group, or cohort, of people, despite the uncertainty about the future longevity or mortality of any one individual. This study became the basis for the original life table. Combining this idea with that of compound interest and annuity valuation, it became possible to set up an insurance scheme to provide life insurance or pensions for a group of people, and to calculate with some degree of accuracy each member's necessary contributions to a common fund, assuming a fixed rate of interest. The first person to correctly calculate these values was Edmond Halley . In his work, Halley demonstrated a method of using his life table to calculate the premium someone of a given age should pay to purchase a life-annuity .\n\nJames Dodson's pioneering work on the level premium system led to the formation of the Society for Equitable Assurances on Lives and Survivorship (now commonly known as Equitable Life) in London in 1762. This was the first life insurance company to use premium rates that were calculated scientifically for long-term life policies, using Dodson's work. After Dodson's death in 1757, Edward Rowe Mores took over the leadership of the group that eventually became the Society for Equitable Assurances. It was he who specified that the chief official should be called an \"actuary\" . Previously, the use of the term had been restricted to an official who recorded the decisions, or \"acts\", of ecclesiastical courts, in ancient times originally the secretary of the Roman senate, responsible for compiling the \"Acta Senatus\" . Other companies that did not originally use such mathematical and scientific methods most often failed or were forced to adopt the methods pioneered by Equitable .\n\nIn the 18th and 19th centuries, computational complexity was limited to manual calculations. The actual calculations required to compute fair insurance premiums are complex. The actuaries of that time developed methods to construct easily used tables, using sophisticated approximations called commutation functions, to facilitate timely, accurate, manual calculations of premiums . Over time, actuarial organizations were founded to support and further both actuaries and actuarial science, and to protect the public interest by ensuring competency and ethical standards . Since calculations were cumbersome, actuarial shortcuts were commonplace.\n\nNon-life actuaries followed in the footsteps of their life compatriots in the early 20th century. In the United States, the 1920 revision to workers' compensation rates took over two months of around-the-clock work by day and night teams of actuaries . In the 1930s and 1940s, rigorous mathematical foundations for stochastic processes were developed . Actuaries began to forecast losses using models of random events instead of deterministic methods. Computers further revolutionized the actuarial profession. From pencil-and-paper to punchcards to microcomputers, the modeling and forecasting ability of the actuary has grown exponentially .\n\nAnother modern development is the convergence of modern financial theory with actuarial science . In the early 20th century, actuaries were developing techniques that can be found in modern financial theory, but for various historical reasons, these developments did not achieve much recognition . In the late 1980s and early 1990s, there was a distinct effort for actuaries to combine financial theory and stochastic methods into their established models . In the 21st century, the profession, both in practice and in the educational syllabi of many actuarial organizations, combines tables, loss models, stochastic methods, and financial theory , but is still not completely aligned with modern financial economics .\n\nAs there are relatively few actuaries in the world compared to other professions, actuaries are in high demand, and are highly paid for the services they render (, ). , in the United States, newly credentialed actuaries on average earn around $100,000 per year, while more experienced actuaries can earn over $150,000 per year . Similarly, survey in the United Kingdom indicated a starting salary for a newly credentialed actuary of about £50,000; actuaries with more experience can earn well in excess of £100,000 .\n\nThe actuarial profession has been consistently ranked for decades as one of the most desirable. Actuaries work comparatively reasonable hours, in comfortable conditions, without the need for physical exertion that may lead to injury, are well paid, and the profession consistently has a good hiring outlook . Not only has the overall profession ranked highly, but it also is considered one of the best professions for women , and one of the best recession-proof professions . In the United States, the profession was rated as the best profession by CareerCast, which uses five key criteria to rank jobs—environment, income, employment outlook, physical demands, and stress, in 2010 , 2013 , and 2015 . In other years, it remained in the top 10 (, , ). In the United Kingdom , and around the world , actuaries continue to be highly ranked as a profession.\n\nBecoming a fully credentialed actuary requires passing a rigorous series of professional examinations, usually taking several years. In some countries, such as Denmark, most study takes place in a university setting . In others, such as the US, most study takes place during employment through a series of examinations (, ). In the UK, and countries based on its process, there is a hybrid university-exam structure .\n\nAs these qualifying exams are extremely rigorous, support is usually available to people progressing through the exams. Often, employers provide paid on-the-job study time and paid attendance at seminars designed for the exams . Also, many companies that employ actuaries have automatic pay raises or promotions when exams are passed. As a result, actuarial students have strong incentives for devoting adequate study time during off-work hours. A common rule of thumb for exam students is that, for the Society of Actuaries examinations, roughly 400 hours of study time are necessary for each four-hour exam . Thus, thousands of hours of study time should be anticipated over several years, assuming no failures .\n\nHistorically, the actuarial profession has been reluctant to specify the pass marks for its examinations (, ). To address concerns that there are pre-existing pass/fail quotas, a former Chairman of the Board of Examiners of the Institute and Faculty of Actuaries stated, \"Although students find it hard to believe, the Board of Examiners does not have fail quotas to achieve. Accordingly pass rates are free to vary (and do). They are determined by the quality of the candidates sitting the examination and in particular how well prepared they are. Fitness to pass is the criterion, not whether you can achieve a mark in the top 40% of candidates sitting.\" . In 2000, the Casualty Actuarial Society (CAS) decided to start releasing pass marks for the exams it offers . The CAS's policy is also not to grade to specific pass ratios; the CAS board affirmed in 2001 that \"the CAS shall use no predetermined pass ratio as a guideline for setting the pass mark for any examination. If the CAS determines that 70% of all candidates have demonstrated sufficient grasp of the syllabus material, then those 70% should pass. Similarly, if the CAS determines that only 30% of all candidates have demonstrated sufficient grasp of the syllabus material, then only those 30% should pass.\".\n\n\n\n\n\n\n\n\n\n\n\n\n\nActuaries have appeared in works of fiction including literature, theater, television, and film. At times, they have been portrayed as \"math-obsessed, socially disconnected individuals with shockingly bad comb-overs\", which has resulted in a mixed response amongst actuaries themselves .\n\n\n\n\n\n\n"}
{"id": "1359360", "url": "https://en.wikipedia.org/wiki?curid=1359360", "title": "Ansatz", "text": "Ansatz\n\nIn physics and mathematics, an ansatz (; , meaning: \"initial placement of a tool at a work piece\", plural ansätze ; or ansatzes) is an educated guess that is verified later by its results.\n\nAn ansatz is the establishment of the starting equation(s), the theorem(s), or the value(s) describing a mathematical or physical problem or solution. It can take into consideration boundary conditions. After an ansatz has been established (constituting nothing more than an assumption), the equations are solved for the general function of interest (constituting a confirmation of the assumption).\n\nGiven a set of experimental data that looks to be clustered about a line, a linear ansatz could be made to find the parameters of the line by a least squares curve fit. Variational approximation methods use ansätze and then fit the parameters.\n\nAnother example could be the mass, energy, and entropy balance equations that, considered simultaneous for purposes of the elementary operations of linear algebra, are the \"ansatz\" to most basic problems of thermodynamics.\n\nAnother example of an ansatz is to suppose the solutions of a homogeneous linear differential equation and difference equation to have, respectively, exponential and power form. More generally, one can guess a particular solution of a system of equations and test such an ansatz by direct substitution of the solution in the system of equations.\n\n"}
{"id": "5300610", "url": "https://en.wikipedia.org/wiki?curid=5300610", "title": "Boolean delay equation", "text": "Boolean delay equation\n\nAs a novel type of semi-discrete dynamical systems, Boolean delay equations (BDEs) are models with Boolean-valued variables that evolve in continuous time. Since at the present time, most phenomena are too complex to be modeled by partial differential equations (as continuous infinite-dimensional systems), BDEs are intended as a (heuristic) first step on the challenging road to further understanding and modeling them. For instance, one can mention complex problems in fluid dynamics, climate dynamics, solid-earth geophysics, and many problems elsewhere in natural sciences where much of the discourse is still conceptual.\n\nAlthough in recent centuries, differential equations (both ordinary and partial) have extensively served as quantitative models of vast categories of problems, by the recent greedy and rapid burst of complexities everywhere, the gap between quantitative and qualitative modeling and reasoning techniques is widening. BDEs offer a formal mathematical language that is promising to help bridge that gap.\n\n"}
{"id": "7499267", "url": "https://en.wikipedia.org/wiki?curid=7499267", "title": "Canon Palmtronic LE-80M", "text": "Canon Palmtronic LE-80M\n\nThe Canon Palmtronic LE-80M is an early hand-held calculator. It was manufactured by Canon inc. Unlike other models which used a processing chip manufactured by Texas Instruments the LE-80M used a Hitachi HD3553 chip. When first released in 1973, it retailed for $138.45.\n"}
{"id": "18468076", "url": "https://en.wikipedia.org/wiki?curid=18468076", "title": "Cocker's Decimal Arithmetick", "text": "Cocker's Decimal Arithmetick\n\nCocker's Decimal Arithmetick is a grammar school mathematics textbook written by Edward Cocker (1631–1676) and published posthumously by John Hawkins in 1684. \"Decimal Arithmetick\" along with companion volume, \"Cocker's Arithmetick\" published in 1677, were used in schools in the United Kingdom for more than 150 years.\n\nThe concept of decimal fractions and the advantages of using them in calculations were well known, but a wide variety of different notations were in use. After surveying various notations, \"Decimal Arithmetick\" recommends the decimal point notation introduced by John Napier:\n\n\"Decimal Arithmetick\" gives instructions for calculations involving decimals, methods of extracting roots, and an overview of the concept of logarithms. There are many worked examples, some of which involve solid geometry or the calculation of interest.\n"}
{"id": "43608216", "url": "https://en.wikipedia.org/wiki?curid=43608216", "title": "Corank", "text": "Corank\n\nIn mathematics, corank is complementary to the concept of the rank of a mathematical object, and may refer to the dimension of the left nullspace of a matrix, the dimension of the cokernel of a linear transformation of a vector space, or the number of elements of a matroid minus its rank. \n\nThe corank of an formula_1 matrix is formula_2 where formula_3 is the rank of the matrix. Equivalently, by the fundamental theorem of linear algebra, it is the dimension of the left nullspace of the matrix.\n\nGeneralizing matrices to linear transformations of vector spaces, the corank of a linear transformation is the dimension of the cokernel of the transformation, which is the quotient of the codomain by the image of the transformation.\n\nFor a matroid with formula_4 elements and matroid rank formula_3, the corank or nullity of the matroid is formula_6. In the case of linear matroids this coincides with the matrix corank. In the case of graphic matroids the corank is also known as the circuit rank or cyclomatic number.\n\n"}
{"id": "16175342", "url": "https://en.wikipedia.org/wiki?curid=16175342", "title": "De divina proportione", "text": "De divina proportione\n\nDe divina proportione (\"On the Divine Proportion\") is a book on mathematics written by Luca Pacioli and illustrated by Leonardo da Vinci, composed around 1498 in Milan and first printed in 1509. Its subject was mathematical proportions (the title refers to the golden ratio) and their applications to geometry, visual art through perspective, and architecture. The clarity of the written material and Leonardo's excellent diagrams helped the book to achieve an impact beyond mathematical circles, popularizing contemporary geometric concepts and images.\n\nThe book consists of three separate manuscripts, which Pacioli worked on between 1496 and 1498.\n\nThe first part, \"Compendio divina proportione\" (\"Compendium on the Divine Proportion\"), studies the golden ratio from a mathematical perspective (following the relevant work of Euclid) and explores its applications to various arts, in seventy-one chapters. It also contains a discourse on the regular and semiregular polyhedra, as well as a discussion of the use of geometric perspective by painters such as Piero della Francesca, Melozzo da Forlì and Marco Palmezzano.\n\nThe second part, \"Trattato dell'architettura\" (\"Treatise on Architecture\"), discusses the ideas of Vitruvius (from his \"De architectura\") on the application of mathematics to architecture in twenty chapters. The text compares the proportions of the human body to those of artificial structures, with examples from classical Greco-Roman architecture.\n\nThe third part, \"Libellus in tres partiales divisus\" (\"Book divided into three parts\"), is mainly an Italian translation of Piero della Francesca's Latin writings \"On [the] Five Regular Solids\" (\"De quinque corporibus regularibus\") and mathematical examples. In 1550 Giorgio Vasari wrote a biography of della Francesca, in which he accused Pacioli of plagiarism and claimed that he stole della Francesca's work on perspective, on arithmetic and on geometry.\n\nAfter these three parts are appended two sections of illustrations, the first showing twenty-three capital letters drawn with a ruler and compass by Pacioli and the second with some sixty illustrations in woodcut after drawings by Leonardo da Vinci. Leonardo drew the illustrations of the regular solids while he lived with and took mathematics lessons from Pacioli. Leonardo's drawings are probably the first illustrations of skeletonic solids which allowed an easy distinction between front and back.\n\nPacioli produced three manuscripts of the treatise by different scribes. He gave the first copy with a dedication to the Duke of Milan, Ludovico il Moro; this manuscript is now preserved in Switzerland at the Bibliothèque de Genève in Geneva. A second copy was donated to Galeazzo da Sanseverino and now rests at the Biblioteca Ambrosiana in Milan. The third, which has gone missing, was given to Pier Soderini, the Gonfaloniere of Florence. On 1 June 1509 the first printed edition was published in Venice by Paganino Paganini; it has since been reprinted several times.\nThe book was displayed as part of an exhibition in Milan between October 2005 and October 2006 together with the Codex Atlanticus. The \"M\" logo used by the Metropolitan Museum of Art in New York is adapted from one in \"De divina proportione\".\n\n\n"}
{"id": "42074131", "url": "https://en.wikipedia.org/wiki?curid=42074131", "title": "Derived stack", "text": "Derived stack\n\nIn algebraic geometry, a derived stack is, roughly, a stack together with a sheaf of commutative ring spectra. It generalizes a derived scheme. Derived stacks are the \"spaces\" studied in derived algebraic geometry.\n\n"}
{"id": "8398", "url": "https://en.wikipedia.org/wiki?curid=8398", "title": "Dimension", "text": "Dimension\n\nIn physics and mathematics, the dimension of a mathematical space (or object) is informally defined as the minimum number of coordinates needed to specify any point within it. Thus a line has a dimension of one because only one coordinate is needed to specify a point on itfor example, the point at 5 on a number line. A surface such as a plane or the surface of a cylinder or sphere has a dimension of two because two coordinates are needed to specify a point on itfor example, both a latitude and longitude are required to locate a point on the surface of a sphere. The inside of a cube, a cylinder or a sphere is three-dimensional because three coordinates are needed to locate a point within these spaces.\n\nIn classical mechanics, space and time are different categories and refer to absolute space and time. That conception of the world is a four-dimensional space but not the one that was found necessary to describe electromagnetism. The four dimensions of spacetime consist of events that are not absolutely defined spatially and temporally, but rather are known relative to the motion of an observer. Minkowski space first approximates the universe without gravity; the pseudo-Riemannian manifolds of general relativity describe spacetime with matter and gravity. Ten dimensions are used to describe string theory, eleven dimensions can describe supergravity and M-theory, and the state-space of quantum mechanics is an infinite-dimensional function space.\n\nThe concept of dimension is not restricted to physical objects. s frequently occur in mathematics and the sciences. They may be parameter spaces or configuration spaces such as in Lagrangian or Hamiltonian mechanics; these are abstract spaces, independent of the physical space we live in.\n\nIn mathematics, the dimension of an object is, roughly speaking, the number of degrees of freedom of a point that moves on this object. In other words, the dimension is the number of independent parameters or coordinates that are needed for defining the position of a point that is constrained to be on the object. For example, the dimension of a point is zero; the dimension of a line is one, as a point can move on a line in only one direction (or its opposite); the dimension of a plane is two, etc.\n\nThe dimension is an intrinsic property of an object, in the sense that it is independent of the dimension of the space in which the object is or can be embedded. For example, a curve, such as a circle is of dimension one, because the position of a point on a curve is determined by its signed distance along the curve to a fixed point on the curve. This is independent from the fact that a curve cannot be embedded in a Euclidean space of dimension lower than two, unless if it is a line.\n\nThe dimension of Euclidean -space is . When trying to generalize to other types of spaces, one is faced with the question \"what makes -dimensional?\" One answer is that to cover a fixed ball in by small balls of radius , one needs on the order of such small balls. This observation leads to the definition of the Minkowski dimension and its more sophisticated variant, the Hausdorff dimension, but there are also other answers to that question. For example, the boundary of a ball in looks locally like and this leads to the notion of the inductive dimension. While these notions agree on , they turn out to be different when one looks at more general spaces.\n\nA tesseract is an example of a four-dimensional object. Whereas outside mathematics the use of the term \"dimension\" is as in: \"A tesseract \"has four dimensions\"\", mathematicians usually express this as: \"The tesseract \"has dimension 4\"\", or: \"The dimension of the tesseract \"is\" 4\".\n\nAlthough the notion of higher dimensions goes back to René Descartes, substantial development of a higher-dimensional geometry only began in the 19th century, via the work of Arthur Cayley, William Rowan Hamilton, Ludwig Schläfli and Bernhard Riemann. Riemann's 1854 Habilitationsschrift, Schläfli's 1852 \"Theorie der vielfachen Kontinuität\", and Hamilton's discovery of the quaternions and John T. Graves' discovery of the octonions in 1843 marked the beginning of higher-dimensional geometry.\n\nThe rest of this section examines some of the more important mathematical definitions of dimension.\n\nThe dimension of a vector space is the number of vectors in any basis for the space, i.e. the number of coordinates necessary to specify any vector. This notion of dimension (the cardinality of a basis) is often referred to as the \"Hamel dimension\" or \"algebraic dimension\" to distinguish it from other notions of dimension. \n\nFor the non-free case, this generalizes to the notion of the length of a module.\n\nThe uniquely defined dimension of every connected topological manifold can be calculated. A connected topological manifold is locally homeomorphic to Euclidean -space, in which the number is the manifold's dimension.\n\nFor connected differentiable manifolds, the dimension is also the dimension of the tangent vector space at any point.\n\nIn geometric topology, the theory of manifolds is characterized by the way dimensions 1 and 2 are relatively elementary, the high-dimensional cases are simplified by having extra space in which to \"work\"; and the cases and are in some senses the most difficult. This state of affairs was highly marked in the various cases of the Poincaré conjecture, where four different proof methods are applied.\n\nThe dimension of a manifold depends on the base field with respect to which Euclidean space is defined. While analysis usually assumes a manifold to be over the real numbers, it is sometimes useful in the study of complex manifolds and algebraic varieties to work over the complex numbers instead. A complex number (\"x\" + \"iy\") has a real part \"x\" and an imaginary part \"y\", where x and y are both real numbers; hence, the complex dimension is half the real dimension. \n\nConversely, in algebraically unconstrained contexts, a single complex coordinate system may be applied to an object having two real dimensions. For example, an ordinary two-dimensional spherical surface, when given a complex metric, becomes a Riemann sphere of one complex dimension.\n\nThe dimension of an algebraic variety may be defined in various equivalent ways. The most intuitive way is probably the dimension of the tangent space at any Regular point of an algebraic variety. Another intuitive way is to define the dimension as the number of hyperplanes that are needed in order to have an intersection with the variety that is reduced to a finite number of points (dimension zero). This definition is based on the fact that the intersection of a variety with a hyperplane reduces the dimension by one unless if the hyperplane contains the variety.\n\nAn algebraic set being a finite union of algebraic varieties, its dimension is the maximum of the dimensions of its components. It is equal to the maximal length of the chains formula_1 of sub-varieties of the given algebraic set (the length of such a chain is the number of \"formula_2\").\n\nEach variety can be considered as an algebraic stack, and its dimension as variety agrees with its dimension as stack. There are however many stacks which do not correspond to varieties, and some of these have negative dimension. Specifically, if \"V\" is a variety of dimension \"m\" and \"G\" is an algebraic group of dimension \"n\" acting on \"V\", then the quotient stack [\"V\"/\"G\"] has dimension \"m\"−\"n\".\n\nThe Krull dimension of a commutative ring is the maximal length of chains of prime ideals in it, a chain of length \"n\" being a sequence formula_3 of prime ideals related by inclusion. It is strongly related to the dimension of an algebraic variety, because of the natural correspondence between sub-varieties and prime ideals of the ring of the polynomials on the variety.\n\nFor an algebra over a field, the dimension as vector space is finite if and only if its Krull dimension is 0.\n\nFor any normal topological space , the Lebesgue covering dimension of is defined to be \"n\" if \"n\" is the smallest integer for which the following holds: any open cover has an open refinement (a second open cover where each element is a subset of an element in the first cover) such that no point is included in more than elements. In this case dim . For a manifold, this coincides with the dimension mentioned above. If no such integer exists, then the dimension of is said to be infinite, and one writes dim . Moreover, has dimension −1, i.e. dim if and only if is empty. This definition of covering dimension can be extended from the class of normal spaces to all Tychonoff spaces merely by replacing the term \"open\" in the definition by the term \"functionally open\".\n\nAn inductive dimension may be defined inductively as follows. Consider a discrete set of points (such as a finite collection of points) to be 0-dimensional. By dragging a 0-dimensional object in some direction, one obtains a 1-dimensional object. By dragging a 1-dimensional object in a \"new direction\", one obtains a 2-dimensional object. In general one obtains an ()-dimensional object by dragging an -dimensional object in a \"new\" direction. The inductive dimension of a topological space may refer to the \"small inductive dimension\" or the \"large inductive dimension\", and is based on the analogy that balls have -dimensional boundaries, permitting an inductive definition based on the dimension of the boundaries of open sets.\n\nSimilarly, for the class of CW complexes, the dimension of an object is the largest for which the -skeleton is nontrivial. Intuitively, this can be described as follows: if the original space can be continuously deformed into a collection of higher-dimensional triangles joined at their faces with a complicated surface, then the dimension of the object is the dimension of those triangles.\n\nThe Hausdorff dimension is useful for studying structurally complicated sets, especially fractals. The Hausdorff dimension is defined for all metric spaces and, unlike the dimensions considered above, can also have non-integer real values. The box dimension or Minkowski dimension is a variant of the same idea. In general, there exist more definitions of fractal dimensions that work for highly irregular sets and attain non-integer positive real values. Fractals have been found useful to describe many natural objects and phenomena.\n\nEvery Hilbert space admits an orthonormal basis, and any two such bases for a particular space have the same cardinality. This cardinality is called the dimension of the Hilbert space. This dimension is finite if and only if the space's Hamel dimension is finite, and in this case the two dimensions coincide.\n\nClassical physics theories describe three physical dimensions: from a particular point in space, the basic directions in which we can move are up/down, left/right, and forward/backward. Movement in any other direction can be expressed in terms of just these three. Moving down is the same as moving up a negative distance. Moving diagonally upward and forward is just as the name of the direction implies; \"i.e.\", moving in a linear combination of up and forward. In its simplest form: a line describes one dimension, a plane describes two dimensions, and a cube describes three dimensions. (See Space and Cartesian coordinate system.)\n\nA temporal dimension is a dimension of time. Time is often referred to as the \"fourth dimension\" for this reason, but that is not to imply that it is a spatial dimension. A temporal dimension is one way to measure physical change. It is perceived differently from the three spatial dimensions in that there is only one of it, and that we cannot move freely in time but subjectively move in one direction.\n\nThe equations used in physics to model reality do not treat time in the same way that humans commonly perceive it. The equations of classical mechanics are symmetric with respect to time, and equations of quantum mechanics are typically symmetric if both time and other quantities (such as charge and parity) are reversed. In these models, the perception of time flowing in one direction is an artifact of the laws of thermodynamics (we perceive time as flowing in the direction of increasing entropy).\n\nThe best-known treatment of time as a dimension is Poincaré and Einstein's special relativity (and extended to general relativity), which treats perceived space and time as components of a four-dimensional manifold, known as spacetime, and in the special, flat case as Minkowski space.\n\nIn physics, three dimensions of space and one of time is the accepted norm. However, there are theories that attempt to unify the four fundamental forces by introducing extra dimensions. Most notably, superstring theory requires 10 spacetime dimensions, and originates from a more fundamental 11-dimensional theory tentatively called M-theory which subsumes five previously distinct superstring theories. To date, no experimental or observational evidence is available to support the existence of these extra dimensions. If extra dimensions exist, they must be hidden from us by some physical mechanism. One well-studied possibility is that the extra dimensions may be \"curled up\" at such tiny scales as to be effectively invisible to current experiments. Limits on the size and other properties of extra dimensions are set by particle experiments such as those at the Large Hadron Collider.\n\nAt the level of quantum field theory, Kaluza–Klein theory unifies gravity with gauge interactions, based on the realization that gravity propagating in small, compact extra dimensions is equivalent to gauge interactions at long distances. In particular when the geometry of the extra dimensions is trivial, it reproduces electromagnetism. However at sufficiently high energies or short distances, this setup still suffers from the same pathologies that famously obstruct direct attempts to describe quantum gravity. Therefore, these models still require a UV completion, of the kind that string theory is intended to provide. In particular, superstring theory requires six compact dimensions forming a Calabi–Yau manifold. Thus Kaluza-Klein theory may be considered either as an incomplete description on its own, or as a subset of string theory model building.\nIn addition to small and curled up extra dimensions, there may be extra dimensions that instead aren't apparent because the matter associated with our visible universe is localized on a subspace. Thus the extra dimensions need not be small and compact but may be large extra dimensions. D-branes are dynamical extended objects of various dimensionalities predicted by string theory that could play this role. They have the property that open string excitations, which are associated with gauge interactions, are confined to the brane by their endpoints, whereas the closed strings that mediate the gravitational interaction are free to propagate into the whole spacetime, or \"the bulk\". This could be related to why gravity is exponentially weaker than the other forces, as it effectively dilutes itself as it propagates into a higher-dimensional volume.\n\nSome aspects of brane physics have been applied to cosmology. For example, brane gas cosmology attempts to explain why there are three dimensions of space using topological and thermodynamic considerations. According to this idea it would be because three is the largest number of spatial dimensions where strings can generically intersect. If initially there are lots of windings of strings around compact dimensions, space could only expand to macroscopic sizes once these windings are eliminated, which requires oppositely wound strings to find each other and annihilate. But strings can only find each other to annihilate at a meaningful rate in three dimensions, so it follows that only three dimensions of space are allowed to grow large given this kind of initial configuration.\n\nExtra dimensions are said to be universal if all fields are equally free to propagate within them.\n\nSome complex networks are characterized by fractal dimensions. The concept of dimension can be generalized to include networks embedded in space. The dimension characterize their spatial constraints.\n\nScience fiction texts often mention the concept of \"dimension\" when referring to parallel or alternate universes or other imagined planes of existence. This usage is derived from the idea that to travel to parallel/alternate universes/planes of existence one must travel in a direction/dimension besides the standard ones. In effect, the other universes/planes are just a small distance away from our own, but the distance is in a fourth (or higher) spatial (or non-spatial) dimension, not the standard ones.\n\nOne of the most heralded science fiction stories regarding true geometric dimensionality, and often recommended as a starting point for those just starting to investigate such matters, is the 1884 novella \"Flatland\" by Edwin A. Abbott. Isaac Asimov, in his foreword to the Signet Classics 1984 edition, described \"Flatland\" as \"The best introduction one can find into the manner of perceiving dimensions.\"\n\nThe idea of other dimensions was incorporated into many early science fiction stories, appearing prominently, for example, in Miles J. Breuer's \"The Appendix and the Spectacles\" (1928) and Murray Leinster's \"The Fifth-Dimension Catapult\" (1931); and appeared irregularly in science fiction by the 1940s. Classic stories involving other dimensions include Robert A. Heinlein's \"—And He Built a Crooked House\" (1941), in which a California architect designs a house based on a three-dimensional projection of a tesseract; and Alan E. Nourse's \"Tiger by the Tail\" and \"The Universe Between\" (both 1951). Another reference is Madeleine L'Engle's novel \"A Wrinkle In Time\" (1962), which uses the fifth dimension as a way for \"tesseracting the universe\" or \"folding\" space in order to move across it quickly. The fourth and fifth dimensions were also a key component of the book \"The Boy Who Reversed Himself\" by William Sleator.\n\nImmanuel Kant, in 1783, wrote: \"That everywhere space (which is not itself the boundary of another space) has three dimensions and that space in general cannot have more dimensions is based on the proposition that not more than three lines can intersect at right angles in one point. This proposition cannot at all be shown from concepts, but rests immediately on intuition and indeed on pure intuition \"a priori\" because it is apodictically (demonstrably) certain.\"\n\n\"Space has Four Dimensions\" is a short story published in 1846 by German philosopher and experimental psychologist Gustav Fechner under the pseudonym \"Dr. Mises\". The protagonist in the tale is a shadow who is aware of and able to communicate with other shadows, but who is trapped on a two-dimensional surface. According to Fechner, this \"shadow-man\" would conceive of the third dimension as being one of time. The story bears a strong similarity to the \"Allegory of the Cave\" presented in Plato's \"The Republic\" (c. 380 BC).\n\nSimon Newcomb wrote an article for the \"Bulletin of the American Mathematical Society\" in 1898 entitled \"The Philosophy of Hyperspace\". Linda Dalrymple Henderson coined the term \"hyperspace philosophy\", used to describe writing that uses higher dimensions to explore metaphysical themes, in her 1983 thesis about the fourth dimension in early-twentieth-century art. Examples of \"hyperspace philosophers\" include Charles Howard Hinton, the first writer, in 1888, to use the word \"tesseract\"; and the Russian esotericist P. D. Ouspensky.\n\nZero\nOne\nTwo\nThree\nFour\nHigher dimensionsin mathematics\nInfinite\n\n"}
{"id": "2625993", "url": "https://en.wikipedia.org/wiki?curid=2625993", "title": "Erdős–Bacon number", "text": "Erdős–Bacon number\n\nA person's Erdős–Bacon number is the sum of one's Erdős number—which measures the \"collaborative distance\" in authoring academic papers between that person and Hungarian mathematician Paul Erdős—and one's Bacon number—which represents the number of links, through roles in films, by which the individual is separated from American actor Kevin Bacon. The lower the number, the closer a person is to Erdős and Bacon, which reflects a small world phenomenon in academia and entertainment.\n\nThe combined Erdős/Bacon [sic] number was introduced by mathematicians Tim Hsu and David Grabiner sometime before late-January 1999, when they pointed out that Daniel Kleitman has a combined number of 3: a Bacon number of 2 and Erdős number of 1.\n\nTo have a defined Erdős–Bacon number, it is necessary to have both appeared in a film and co-authored an academic paper, although this in and of itself is not sufficient.\nMathematician Daniel Kleitman has the Erdős–Bacon number of 3; it is the lowest among scientists: he is a co-author of Erdős on multiple papers, and has a Bacon number of 2, via Minnie Driver in \"Good Will Hunting\".\n\nMathematician Ken Ono has an Erdős–Bacon number of 4; 2 for Erdős and 2 for Bacon.\n\nMathematician Doron Zeilberger has an Erdős-Bacon number of 5. Computer scientist Tom Porter also has an Erdős-Bacon number of 5; 3 for Erdős in two ways and 2 for Bacon.\n\nAstronomer Carl Sagan has an Erdős number of 4 (via Steven J. Ostro) and a Bacon number of 2 (Sagan and Bacon having appeared with Johnny Carson on episodes of \"The Tonight Show\"), for a total of 6. Physicist Richard Feynman has an Erdős number of 3, and a Bacon number of 3, having appeared in the film \"Anti-Clock\" alongside Tony Tang. Geneticist Jonathan Pritchard appeared in the 1998 movie \"Without Limits\" which gives him a Bacon number of 2. Pritchard has an Erdős number of 4 thus giving him an Erdős–Bacon number of 6. Theoretical physicist Stephen Hawking has an Erdős–Bacon number of 6: his Bacon number of 2 (via his appearance alongside John Cleese in \"Monty Python Live (Mostly)\" who acted alongside Kevin Bacon in \"The Big Picture\") is lower than his Erdős number of 4.\n\nCanadian actor Albert M. Chan has an Erdős-Bacon number of 4. He co-authored a peer-reviewed paper on Orthogonal frequency-division multiplexing, giving him an Erdős number of 3, and was cast alongside Kevin Bacon in \"Patriots Day\", giving him a Bacon number of 1.\n\nDanica McKellar, who played Winnie Cooper in \"The Wonder Years\", has an Erdős–Bacon number of 6, having coauthored a mathematics paper published while an undergraduate at the University of California, Los Angeles. Her paper gives her an Erdős number of 4, and she has a Bacon number of 2, having worked with Margaret Easley.\n\nAmerican actress Natalie Portman has an Erdős–Bacon number of 7. She collaborated (using her birth name, Natalie Hershlag) with Abigail A. Baird, who has a collaboration path leading to Joseph Gillis, who has an Erdős number of 1. Portman appeared in \"A Powerful Noise Live\" (2009) with Sarah Michelle Gellar, who appeared in \"The Air I Breathe\" (2007) with Bacon, giving Portman a Bacon number of 2 and an Erdős number of 5.\n\nBritish actor Colin Firth has an Erdős–Bacon number of 7. Firth is credited as co-author of a neuroscience paper, \"Political Orientations Are Correlated with Brain Structure in Young Adults\", after he suggested on BBC Radio 4 that such a study could be done. Another author of that paper, Geraint Rees, has an Erdős number of 5, which gives Firth an Erdős number of 6. Firth's Bacon number of 1 is due to his appearance in \"Where the Truth Lies\".\n\nKristen Stewart has an Erdős–Bacon number of 7; she is credited as a co-author on an artificial intelligence paper that was written after a technique was used for her short film \"Come Swim\", giving her an Erdős number of 5, and she co-starred with Michael Sheen in \"Twilight\", who co-starred with Bacon in \"Frost/Nixon\", giving her a Bacon number of 2.\n\nAmerican scholar and actor Michael M. Chemers has an Erdös-Bacon number of 6. He co-authored a 2018 paper about \"Game of Thrones\" with mathematician Andrew Beveridge, who has an Erdös number of 2, giving him an Erdös number of 3, and co-starred in two films (\"When Tyrants Kiss,\" 2004; \"Before the Thunder,\" 2018) both of which give him a Bacon number of 3 through a number of co-stars.\n\nNotes:\n"}
{"id": "5280515", "url": "https://en.wikipedia.org/wiki?curid=5280515", "title": "Exceptional object", "text": "Exceptional object\n\nMany branches of mathematics study objects of a given type and prove a classification theorem. A common theme is that the classification results in a number of series of objects and a finite number of exceptions that do not fit into any series. These are known as exceptional objects. Frequently these exceptional objects play a further and important role in the subject. Furthermore, the exceptional objects in one branch of mathematics are often related to the exceptional objects in others.\n\nA related phenomenon is exceptional isomorphism, when two series are in general different, but agree for some small values.\n\nThe prototypical examples of exceptional objects arise in the classification of regular polytopes. In two dimensions there is a series of regular \"n\"-gons for \"n\" ≥ 3. In every dimension above 2 we find analogues of the cube, tetrahedron and octahedron. In three dimensions we find two more regular polyhedra — the dodecahedron (12-hedron) and the icosahedron (20-hedron) — making five Platonic solids. In four dimensions we have a total of six regular polytopes including the 120-cell, the 600-cell and the 24-cell. There are no other regular polytopes; in higher dimensions the only regular polytopes are of the hypercube, simplex, orthoplex series. In all dimensions combined, there are therefore three series and five exceptional polytopes.\n\nThe pattern is similar if non-convex polytopes are included. In two dimensions there is a regular star polygon for every rational number \"p\"/\"q\" > 2. In three dimensions there are four Kepler–Poinsot polyhedra, and in four dimensions ten Schläfli–Hess polychora; in higher dimensions there are no non-convex regular figures.\n\nThese can be generalized to tessellations of other spaces, especially uniform tessellations, notably tilings of Euclidean space (honeycombs), which have exceptional objects, and tilings of hyperbolic space. There are various exceptional objects in dimension below 6, but in dimension 6 and above the only regular polyhedra/tilings/hyperbolic tilings are the simplex, hypercube, cross-polytope, and hypercube lattice.\n\nRelated to tilings and the regular polyhedra, there are exceptional Schwarz triangles (triangles that tile the sphere, or more generally Euclidean plane or hyperbolic plane via their triangle group of reflections in their edges), particularly the Möbius triangles. In the sphere there are 3 Möbius triangles (and 1 1-parameter family), corresponding to the 3 exceptional Platonic solid groups, while in the Euclidean plane there are 3 Möbius triangles, corresponding to the 3 special triangles: 60-60-60 (equilateral), 45-45-90 (isosceles right), and 30-60-90. There are additional exceptional Schwarz triangles in the sphere and Euclidean plane. By contrast, in the hyperbolic plane there is a 3-parameter family of Möbius triangles, and none exceptional.\n\nThe finite simple groups have been classified into a number of series as well as 26 sporadic groups. Of these, 20 are subgroups or subquotients of the monster group, referred to as the \"Happy Family\", while 6 are not, and are referred to as \"pariahs\".\n\nSeveral of the sporadic groups are related to the Leech lattice, most notably the Conway group Co, which is the automorphism group of the Leech lattice, quotiented out by its center.\n\nThere are only three finite-dimensional associative division algebras over the reals — the real numbers, the complex numbers and the quaternions. The only non-associative division algebra is the algebra of octonions. The octonions are connected to a wide variety of exceptional objects. For example, the exceptional formally real Jordan algebra is the Albert algebra of 3 by 3 self-adjoint matrices over the octonions.\n\nThe simple Lie groups form a number of series (classical Lie groups) labelled A, B, C and D. In addition there are the exceptional groups G (the automorphism group of the octonions), F, E, E, E. These last four groups can be viewed as the symmetry groups of projective planes over O, C⊗O, H⊗O and O⊗O respectively, where O is the octonions and the tensor products are over the reals.\n\nThe classification of Lie groups corresponds to the classification of root systems and thus the exceptional Lie groups correspond to exceptional root systems and exceptional Dynkin diagrams.\n\nThere are a few exceptional objects with supersymmetry. The classification of superalgebras by Kac and Tierry-Mieg indicates that the Lie superalgebras G(3) in 31 dimensions and F(4) in 40 dimensions, and the Jordan superalgebras K and K, are examples of exceptional objects.\n\nUp to isometry there is only one even unimodular lattice in 15 dimensions or less — the E lattice. Up to dimension 24 there is only one even unimodular lattice without roots, the Leech lattice. Three of the sporadic simple groups were discovered by Conway while investigating the automorphism group of the Leech lattice. For example, Co is the automorphism group itself modulo ±1. The groups Co and Co, as well as a number of other sporadic groups, arise as stabilisers of various subsets of the Leech lattice.\n\nSome codes also stand out as exceptional objects, in particular the perfect binary Golay code which is closely related to the Leech lattice. The Mathieu group formula_1, one of the sporadic simple groups, is the group of automorphisms of the extended binary Golay code, and four more of the sporadic simple groups arise as various types of stabilizer subgroup of formula_1.\n\nAn exceptional block design is the Steiner system S(5,8,24) whose automorphism group is the sporadic simple Mathieu group formula_1.\n\nThe codewords of the extended binary Golay code have a length of 24 bits and have weights 0, 8, 12, 16, or 24. This code can correct up to three errors. So every 24-bit word with weight 5 can be corrected to a codeword with weight 8. The bits of a 24-bit word can be thought of as specifying the possible subsets of a 24 element set. So the extended binary Golay code gives a unique 8 element subset for each 5 element subset. In fact, it defines S(5,8,24).\n\nCertain families of groups generically have a certain outer automorphism group, but in particular cases they have other, exceptional outer automorphisms.\n\nAmong families of finite simple groups, the only example is in the automorphisms of the symmetric and alternating groups: for formula_4 the alternating group formula_5 has one outer automorphism (corresponding to conjugation by an odd element of formula_6) and the symmetric group formula_6 has no outer automorphisms. However, for formula_8 there is an exceptional outer automorphism of formula_9 (of order 2), and correspondingly, the outer automorphism group of formula_10 is not formula_11 (the group of order 2) but rather formula_12, the Klein four-group.\n\nIf one instead considers A as the (isomorphic) projective special linear group PSL(2,9), then the outer automorphism is not exceptional; thus the exceptionalness can be seen as due to the exceptional isomorphism formula_13 This exceptional outer automorphism is realized inside of the Mathieu group M and similarly, M acts on a set of 12 elements in 2 different ways.\n\nAmong Lie groups, the spin group Spin(8) has an exceptionally large outer automorphism group (namely formula_14), which corresponds to the exceptional symmetries of the Dynkin diagram D. This phenomenon is referred to as \"triality.\"\n\nThe exceptional symmetry of the D diagram also gives rise to the Steinberg groups.\n\nThe Kervaire invariant is an invariant of a (4\"k\"+2)-dimensional manifold that measures whether the manifold could be surgically converted into a sphere. This invariant evaluates to 0 if the manifold can be converted to a sphere, and 1 otherwise. More specifically, the Kervaire invariant applies to a framed manifold, that is, to a manifold equipped with an embedding into Euclidean space and a trivialization of the normal bundle. The Kervaire invariant problem is the problem of determining in which dimensions the Kervaire invariant can be nonzero. For differentiable manifolds, this can happen in dimensions 2, 6, 14, 30, 62, and possibly 126, and in no other dimensions. The final case of dimension 126 remains open. These five or six framed cobordism classes of manifolds having Kervaire invariant 1 are exceptional objects related to exotic spheres. The first three cases are related to the complex numbers, quaternions and octonions respectively: a manifold of Kervaire invariant 1 can be constructed as the product of two spheres, with its exotic framing determined by the normed division algebra.\n\nDue to similarities of dimensions, it is conjectured that the remaining cases (dimensions 30, 62 and 126) are related to the Rosenfeld projective planes, which are defined over algebras constructed from the octonions. Specifically, it has been conjectured that there is a construction that takes these projective planes and produces a manifold with nonzero Kervaire invariant in two dimensions lower, but this remains unconfirmed.\n\nIn quantum information theory, there exist structures known as SIC-POVMs or SICs, which correspond to maximal sets of complex equiangular lines. Some of the known SICs—those in vector spaces of 2 and 3 dimensions, as well as certain solutions in 8 dimensions—are considered exceptional objects and called \"sporadic SICs\". They differ from the other known SICs in ways that involve their symmetry groups, the Galois theory of the numerical values of their vector components, and so forth. The sporadic SICs in dimension 8 are related to the integral octonions.\n\nNumerous connections have been observed between some, though not all, of these exceptional objects. Most common are objects related to 8 and 24 dimensions, noting that 24 = 8 · 3. By contrast, the pariah groups stand apart, as the name suggests.\n\nExceptional objects related to the number 8 include the following.\nLikewise, exceptional objects related to the number 24 include the following.\n\nThese objects are connected to various other phenomena in math which may be considered surprising but not themselves \"exceptional\". For example, in algebraic topology, 8-fold real Bott periodicity can be seen as coming from the octonions. In the theory of modular forms, the 24-dimensional nature of the Leech lattice underlies the presence of 24 in the formulas for the Dedekind eta function and the modular discriminant, which connection is deepened by Monstrous moonshine, a development that related modular functions to the Monster group.\n\nIn string theory and superstring theory we often find that particular dimensions are singled out as a result of exceptional algebraic phenomena. For example, bosonic string theory requires a spacetime of dimension 26 which is directly related to the presence of 24 in the Dedekind eta function. Similarly, the possible dimensions of supergravity are related to the dimensions of the division algebras.\n\nMany of the exceptional objects in mathematics and physics have been found to be connected to each other. Developments such as the Monstrous moonshine conjectures show how, for example, the Monster group is connected to string theory. The theory of modular forms shows how the algebra E is connected to the Monster group. (In fact, well before the proof of the Monstrous moonshine conjecture, the elliptic \"j\"-function was discovered to encode the representations of E.) Other interesting connections include how the Leech lattice is connected via the Golay code to the adjacency matrix of the dodecahedron (another exceptional object). Below is a mind map showing how some of the exceptional objects in mathematics and mathematical physics are related.\n\nThe connections can partly be explained by thinking of the algebras as a tower of lattice vertex operator algebras. It just so happens that the vertex algebras at the bottom are so simple that they are isomorphic to familiar non-vertex algebras. Thus the connections can be seen simply as the consequence of some lattices being sub-lattices of others.\n\nThe Jordan superalgebras are a parallel set of exceptional objects with supersymmetry. These are the Lie superalgebras which are related to Lorentzian lattices. This subject is less explored, and the connections between the objects are less well established. There are new conjectures parallel to the Monstrous moonshine conjectures for these super-objects, involving different sporadic groups.\n\n\"Exceptional\" object is reserved for objects that are unusual, meaning rare, the exception, not for \"unexpected\" or \"non-standard\" objects. These unexpected-but-typical (or common) phenomena are generally referred to as pathological, such as nowhere differentiable functions, or \"exotic\", as in exotic spheres — there are exotic spheres in arbitrarily high dimension (not only a finite set of exceptions), and in many dimensions most (differential structures on) spheres are exotic.\n\nExceptional objects must be distinguished from \"extremal\" objects: those that fall in a family and are the most extreme example by some measure are of interest, but not unusual in the way exceptional objects are. For example, the golden ratio \"φ\" has the simplest continued fraction approximation, and accordingly is most difficult to approximate by rationals; however, it is but one of infinitely many such quadratic numbers (continued fractions).\n\nSimilarly, the (2,3,7) Schwarz triangle is the smallest hyperbolic Schwarz triangle, and the associated (2,3,7) triangle group is of particular interest, being the universal Hurwitz group, and thus being associated with the Hurwitz curves, the maximally symmetric algebraic curves. However, it falls in a family of such triangles ((2,4,7), (2,3,8), (3,3,7), etc.), and while the smallest, is not exceptional or unlike the others.\n\n"}
{"id": "52722478", "url": "https://en.wikipedia.org/wiki?curid=52722478", "title": "Fractal expressionism", "text": "Fractal expressionism\n\nThe term fractal expressionism was coined by physicist-artist Richard Taylor and co-authors to distinguish fractal art generated directly by artists from fractal art generated using mathematics and/or computers. Fractals are patterns that repeat at increasingly fine scales and are prevalent in natural scenery (examples include clouds, rivers, and mountains). Fractal expressionism implies a direct expression of nature's patterns in an art work.\n\nThe initial studies of fractal expressionism focused on the poured paintings by the American artist Jackson Pollock (1912-1956), whose work has traditionally been associated with the abstract expressionist movement. Pollock’s patterns had previously been referred to as “natural” and “organic”, inviting speculation by author John Briggs in 1992 that Pollock's work featured fractals. In 1997, Taylor built a pendulum device called the Pollockizer which painted fractal patterns bearing a similarity to Pollock’s work. Computer analysis of Pollock's work published by Taylor et al. in a 1999 Nature article found that Pollock's painted patterns have characteristics that match those displayed by nature's fractals (specifically, they demonstrate a statistical self-similarity quantified by a non-integer dimension over a magnification range of 1.5-2 orders of magnitude). This analysis supported clues (see below) that Pollock's patterns are fractal and reflect \"the fingerprint of nature\".\n\nTaylor noted several similarities between Pollock's painting style and the processes used by nature to construct its landscapes. For instance, he cites Pollock's propensity to revisit paintings that he had not adjusted in several weeks as being comparable to cyclic processes in nature, such as the seasons or the tides. Furthermore, Taylor observed several visual similarities between the patterns produced by nature and those produced by Pollock as he painted. He points out that Pollock abandoned the use of a traditional frame for his paintings, preferring instead to roll out his canvas on the floor; this, Taylor asserts, is more compatible with how nature works than traditional painting techniques because the patterns in nature's scenery are not artificially bounded.\n\nThe perceived similarities between the processes and patterns involved in Pollock's paintings and those of nature compelled Taylor to posit that the same \"basic trademark\" of nature's pattern construction also appears in Pollock's work. Since some natural fractals are generated by a process known as \"chaos\", including fractals in human physiology, Taylor believed that Pollock's painting process might also have been chaotic, and could therefore leave behind a fractal pattern. Taylor's hypothesis seems to be reflected in Pollock's statement \"I am nature\", which he made when asked if nature was a source of inspiration for his work. Furthermore, Pollock is also quoted as stating \"No chaos, damn it\", in response to a Time magazine article that referred to his paintings as \"chaotic\". However, chaos theory was not understood until after Pollock's death, so he could not have been referring to the chaotic systems in nature but rather its common usage to mean disorder. In the famous film footage of Hans Namuth, Pollock says his paintings are no accident, and that he was able to control the flow of paint onto the canvas.\n\nTaylor points to two aspects of Pollock's painting process that have the potential to introduce fractal patterns. The first is Pollock's motion as he moved around the canvas, which Taylor hypothesized followed a Levy flight, a type of chaotic motion that is known to leave behind a fractal pattern. More specifically, a number of studies have shown that the motions associated with human balance have fractal characteristics. The second source of chaos could be introduced through Pollock's pouring technique. Falling fluid has the capability of changing from a non-chaotic to a chaotic flow, meaning that Pollock could have introduced a chaotic flow of paint as he dripped it onto the canvas. Although the fractal characteristics of human balance and falling liquid are generated on Pollock's painting time and length scales, physicist Predrag Cvitanovic notes that it would be quite an artistic challenge to control them: such parameters \"are in no sense observable and measurable on the length-scales and time-scales dominated by chaotic dynamics\".\n\nSince Taylor's initial Pollock analysis in 1999, more than ten research groups have used various forms of fractal analysis to successfully quantify Pollock’s work. In addition to analyzing Pollock's work for fractal content, some groups such as that of computer scientist Bruce Gooch, have used computers to generate Pollock-like images by varying their fractal characteristics. Mathematician Benoit Mandelbrot (who invented the term fractal) and art theorist Francis O’Connor (the chief Pollock scholar) are well known advocates of fractal expressionism.\n\nFractal expressionism is related to fractal fluency because the latter provides an appealing motivation for why artists such as Pollock might aspire to Fractal Expressionism. Fractal fluency is a neuroscience model that proposes that, through exposure to nature’s fractal scenery, people’s visual systems have adapted to efficiently process fractals with ease. This adaptation occurs at many stages of the visual system, from the way people’s eyes move to which regions of the brain get activated. Fluency puts the viewer in a ‘comfort zone’ so inducing an aesthetic experience. Neuroscience experiments have shown that Pollock’s paintings induce the same positive physiological responses in the observer as nature’s fractals and mathematical fractals.\n\nIn light of fractal fluency and the associated aesthetics, other artists might be expected to display fractal expressionism. One year before Taylor’s publication, mathematician Richard Voss quantified Chinese art using fractal analysis. Subsequently, other groups have used computer analysis to identify fractal content in a number of Western and Eastern artists, most recently in Pollock’s colleague Willem De Kooning’s work.\n\nIn addition to the above analyzed works, symbolic representations of fractals can be found in cultures across the continents spanning several centuries, including Roman, Egyptian, Aztec, Incan and Mayan civilizations. They frequently predate patterns named after the mathematicians who subsequently developed their visual characteristics. For example, although von Koch is famous for developing The Koch Curve in 1904, a similar shape featuring repeating triangles was first used to depict waves in friezes by Hellenic artists (300 B.C.E.). In the 13th century, repetition of triangles in Cosmati Mosaics generated a shape later known in mathematics as The Sierpinski Triangle (named after Sierpinski’s 1915 pattern). Triangular repetitions are also found in the 12th century pulpit of The Ravello Cathedral in Italy. The lavish artwork within The Book of Kells (circa 800 C.E.) and the sculpted arabesques in The Jain Dilwara Temple in Mount Abu, India (1031 C.E.) also both reveal stunning examples of exact fractals. The artistic works of Leonardo da Vinci and Katsushika Hokusai serve as more recent examples from Europe and Asia, each reproducing the recurring patterns that they saw in nature. Da Vinci’s sketch of turbulence in water, The Deluge (1571–1518), was composed of small swirls within larger swirls of water. In The Great Wave off Kanagawa (1830–1833), Hokusai portrayed a wave crashing on a shore with small waves on top of a large wave. Other woodcuts from the same period also feature repeating patterns at several size scales: The Ghost of Kohada Koheiji shows fissures in a skull and The Falls At Mt. Kurokami features branching channels in a waterfall.\n\nVoss's 1998 study of Chinese art was the first demonstration of using fractal analysis to distinguish between the works of different artists. Following Taylor's 1999 Pollock publication, Art conservator Jim Coddington proposed that fractal analysis should be explored as a technique to help authenticate Pollock paintings. In 2005, Taylor and colleagues published a fractal analysis of 14 authentic and 37 imitation Pollocks suggesting that, when combined with other techniques, fractal analysis might be useful for authenticating Pollock's work. In the same year, The Pollock-Krasner Foundation requested a fractal analysis to be used for the first time in an authenticity dispute, The analysis identified “significant deviations from Pollock’s characteristics.” Taylor cautioned that the results should be “coupled with other important information such as provenance, connoisseurship and materials analysis.” Two years later, materials scientists showed that pigments on the paintings dated from after Pollock’s death.\nIn 2006, the use of fractals to authenticate Pollocks stirred controversy. This controversy was triggered by physicists Katherine Jones-Smith and Harsh Mathur who claimed that the fractal characteristics identified by Taylor et al. are also present in crude sketches made in Adobe Photoshop, and deliberately fraudulent poured paintings made by other artists Thus, according to Jones-Smith and Mathur, labeling Pollock's paintings as \"fractal\" is meaningless, because the same characteristics are found in other non-fractal images. However, Taylor's rebuttal published in Nature showed that Taylor's group's fractal analysis could distinguish between Pollock paintings and the crude sketches, and identified further limitations in Jones-Smith and Mathur's analysis.\n\nJones-Smith and Mathur raised a valid concern applicable to all forms of fractal expressionism: are art works too small for the painted patterns to repeat over sufficient magnifications to assume the visual characteristics of fractals? In the case of Pollock paintings, the largest range used by Taylor et al. to determine each fractal parameter in a Pollock painting is less than two orders of magnitude in magnification. Nature's fractals repeat over limited magnification ranges (typically just over one order of magnitude), prompting scientists to debate what range is required to reliably establish fractal behavior.<ref name=\"Ave/Man\">[Avnir, David, Ofer Biham, Daniel M. Lidar, and Ofer Malcai. \"Is the Geometry of Nature Fractal?\" Science 279.5347 (1998): 39-40. Print.]</ref> Mandelbrot refused to include a required magnification range in his definition of fractals and instead noted that it is the range necessary to generate the properties associated with fractal repetition. In the case of Pollock's work, this would be the magnification range necessary for the patterns to generate the fractal aesthetics. Neuroscience experiments have shown that this magnification range is less than two orders and that Pollock’s paintings do indeed induce the same physiological responses as nature’s fractals and mathematical fractals Mandelbrot concluded \"I do believe that Pollocks are fractal.\"\n\nAt the time of the controversy, Coddington summarized as follows: “Fractal geometry has begun to play an important role in the authentication of the work of Jackson Pollock. We believe such analyses are necessary for pushing the field forward.” The most recent results, In 2015, by computer scientist Lior Shamir showed that, when combined with other pattern parameters, fractal analysis can be used to distinguish between real and imitation Pollocks with 93% accuracy. He found that the fractal parameters were the most powerful contributors to the detection accuracy\n"}
{"id": "34824761", "url": "https://en.wikipedia.org/wiki?curid=34824761", "title": "Fraïssé's theorem", "text": "Fraïssé's theorem\n\nIn mathematics, Fraïssé's theorem, named after Roland Fraïssé, states that a class \"K\" of finite relational structures is the age of a countable homogeneous relational structure if and only if it satisfies the following four conditions:\n\n\nIf these conditions hold, then the countable homogeneous structure whose age is \"K\" is unique up to isomorphism.\n\nFraïssé proved the theorem in the 1950s. \n\nFor a proof and more details see Section 1.2 and Appendix A of this thesis. \n"}
{"id": "9098286", "url": "https://en.wikipedia.org/wiki?curid=9098286", "title": "Hekat (unit)", "text": "Hekat (unit)\n\nThe hekat or heqat (transcribed \"HqA.t\") was an ancient Egyptian volume unit used to measure grain, bread, and beer. \nIt equals 4.8 litres in today's measurements.\n\nUntil the New Kingdom (NK), the hekat was one tenth of a khar, later one sixteenth; while the New Kingdom \"oipe\" (transcribed \"ip.t\") contained 4 hekat. It was sub-divided into other units – some for medical prescriptions – the \"hin\" (1/10), \"dja\" (1/64) and \"ro\" (1/320). The \"dja\" was recently evaluated by Tanja Pommerening in 2002 to 1/64 of a hekat (75 cc) in the MK, and 1/64 of an oipe (1/16 of a hekat, or 300 cc) in the NK, meaning that the \"dja\" was denoted by Horus-Eye imagery. It has been suggested by Pommerening that the NK change came about related to the oipe replacing the hekat as the Pharaonic volume control unit in official lists.\n\nHana Vymazalova evaluated the hekat unit in 2002 within the Akhmim Wooden Tablet by showing that five answers were returned to (64/64) when multiplied by the divisors 3, 7, 10, 11 and 13. The RMP also divided a hekat unity (64/64) by prime and composite numbers \"n\" when 1/64 < \"n\" < 64. The binary quotient used Eye of Horus numbers. The remainder scaled Egyptian fractions to 1/320 units named ro. Quotients and unscaled remainders were obtained for the dja, ro and other units when the divisor \"n\" was greater than 64. For example, one the 1/320 ro unit was written by Ahmes by solving 320/n ro. Gillings cites 29 examples of two-part statements converted to one-part statements in RMP 82. Ahmes recorded the \"n\" = 3 case by showing (64/64)/3 = 21/64 + 1/192 (a modern statement) as written as(16 + 4 + 1)/64 + 5/3 × 1/320 = 1/4 + 1/16 + 1/64 + 1 2/3ro (two-part ancient statement). Two-part statements were also converted by Ahmes to an unscaled hin unit by writing 3 1/3 hin.\n\nThe hekat measurement unit, and its double entry accounting system, was found beyond the Rhind Mathematical Papyrus. Another text was the Ebers Papyrus, the best known medical text. The hekat unit was defined, in terms of its volume size, in the Moscow Mathematical Papyrus by MMP #10, by approximating \"π\" to around 3.16. The approximation of \"π\" was achieved by squaring a circle, increasingly (i.e. for the denominator in terms of setats: 9, 18, 36, 72, and 81, Gillings, page 141) until the vulgar fraction 256/81 was reached, the only relationship that was used in the Egyptian Middle Kingdom. The MMP scribe found the surface area of a basket equal to: (8d/9) = 64d/81, within a cylinder relationship to the hekat. MMP 10 data meant that \"d\" = 2 defined \"π\" for use in hekat volumes as 256/81. The 256/81 approximation was also used by Ahmes and other scribes. The ancient Egyptian weights and measures discussion further shows that the hekat was 1/30 of a royal cubit, an analysis that needs to double checked, against the \"d\" = 2 suggestion, which means that \"r\" = 1, a suggestion that does make sense. One royal cubit of the Ancient Egyptian weights and measures = 523.5 millimeters. ((523.5 mm)) / 30 = 4.78221176 liters.\n\n"}
{"id": "14220", "url": "https://en.wikipedia.org/wiki?curid=14220", "title": "History of mathematics", "text": "History of mathematics\n\nThe area of study known as the history of mathematics is primarily an investigation into the origin of discoveries in mathematics and, to a lesser extent, an investigation into the mathematical methods and notation of the past. Before the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. From 3000 BC the Mesopotamian states of Sumer, Akkad and Assyria, together with Ancient Egypt and Ebla began using arithmetic, algebra and geometry for purposes of taxation, commerce, trade and also in the field of astronomy and to formulate calendars and record time.\n\nThe most ancient mathematical texts available are from Mesopotamia and Egypt - \"Plimpton 322\" (Babylonian c. 1900 BC), the \"Rhind Mathematical Papyrus\" (Egyptian c. 2000–1800 BC) and the \"Moscow Mathematical Papyrus\" (Egyptian c. 1890 BC). All of these texts mention the so-called Pythagorean triples and so, by inference, the Pythagorean theorem, seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.\n\nThe study of mathematics as a \"demonstrative discipline\" begins in the 6th century BC with the Pythagoreans, who coined the term \"mathematics\" from the ancient Greek \"μάθημα\" (\"mathema\"), meaning \"subject of instruction\". Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Although they made virtually no contributions to theoretical mathematics, the ancient Romans used applied mathematics in surveying, structural engineering, mechanical engineering, bookkeeping, creation of lunar and solar calendars, and even arts and crafts. Chinese mathematics made early contributions, including a place value system and the first use of negative numbers. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics through the work of Muḥammad ibn Mūsā al-Khwārizmī. Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations. Contemporaneous with but independent of these traditions were the mathematics developed by the Maya civilization of Mexico and Central America, where the concept of zero was given a standard symbol in Maya numerals.\n\nMany Greek and Arabic texts on mathematics were translated into Latin from the 12th century onward, leading to further development of mathematics in Medieval Europe. From ancient times through the Middle Ages, periods of mathematical discovery were often followed by centuries of stagnation. Beginning in Renaissance Italy in the 15th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This includes the groundbreaking work of both Isaac Newton and Gottfried Wilhelm Leibniz in the development of infinitesimal calculus during the course of the 17th century. At the end of the 19th century the International Congress of Mathematicians was founded and continues to spearhead advances in the field.\n\nThe origins of mathematical thought lie in the concepts of number, magnitude, and form. Modern studies of animal cognition have shown that these concepts are not unique to humans. Such concepts would have been part of everyday life in hunter-gatherer societies. The idea of the \"number\" concept evolving gradually over time is supported by the existence of languages which preserve the distinction between \"one\", \"two\", and \"many\", but not of numbers larger than two.\n\nPrehistoric artifacts discovered in Africa, dated 20,000 years old or more suggest early attempts to quantify time. The Ishango bone, found near the headwaters of the Nile river (northeastern Congo), may be more than 20,000 years old and consists of a series of marks carved in three columns running the length of the bone. Common interpretations are that the Ishango bone shows either a \"tally\" of the earliest known demonstration of sequences of prime numbers or a six-month lunar calendar. Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that \"no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10.\" The Ishango bone, according to scholar Alexander Marshack, may have influenced the later development of mathematics in Egypt as, like some entries on the Ishango bone, Egyptian arithmetic also made use of multiplication by 2; this however, is disputed.\n\nPredynastic Egyptians of the 5th millennium BC pictorially represented geometric designs. It has been claimed that megalithic monuments in England and Scotland, dating from the 3rd millennium BC, incorporate geometric ideas such as circles, ellipses, and Pythagorean triples in their design. All of the above are disputed however, and the currently oldest undisputed mathematical documents are from Babylonian and dynastic Egyptian sources.\n\nBabylonian mathematics refers to any mathematics of the peoples of Mesopotamia (modern Iraq) from the days of the early Sumerians through the Hellenistic period almost to the dawn of Christianity. The majority of Babylonian mathematical work comes from two widely separated periods: The first few hundred years of the second millennium BC (Old Babylonian period), and the last few centuries of the first millennium BC (Seleucid period). It is named Babylonian mathematics due to the central role of Babylon as a place of study. Later under the Arab Empire, Mesopotamia, especially Baghdad, once again became an important center of study for Islamic mathematics.\nIn contrast to the sparsity of sources in Egyptian mathematics, our knowledge of Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework.\n\nThe earliest evidence of written mathematics dates back to the ancient Sumerians, who built the earliest civilization in Mesopotamia. They developed a complex system of metrology from 3000 BC. From around 2500 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.\nBabylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 x 6) degrees in a circle, as well as the use of seconds and minutes of arc to denote fractions of a degree. It is likely the sexagesimal system was chosen because 60 can be evenly divided by 2, 3, 4, 5, 6, 10, 12, 15, 20 and 30. Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values, much as in the decimal system. The power of the Babylonian notational system lay in that it could be used to represent fractions as easily as whole numbers; thus multiplying two numbers that contained fractions was no different than multiplying integers, similar to our modern notation. The notational system of the Babylonians was the best of any civilization until the Renaissance, and its power allowed it to achieve remarkable computation accuracy and power; for example, the Babylonian tablet YBC 7289 gives an approximation of accurate to five decimal places. The Babylonians lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context. By the Seleucid period, the Babylonians had developed a zero symbol as a placeholder for empty positions; however it was only used for intermediate positions. This zero sign does not appear in terminal positions, thus the Babylonians came close but did not develop a true place value system.\n\nOther topics covered by Babylonian mathematics include fractions, algebra, quadratic and cubic equations, and the calculation of regular reciprocal pairs. The tablets also include multiplication tables and methods for solving linear, quadratic equations and cubic equations, a remarkable achievement for the time. Tablets from the Old Babylonian period also contain the earliest known statement of the Pythagorean theorem. However, as with Egyptian mathematics, Babylonian mathematics shows no awareness of the difference between exact and approximate solutions, or the solvability of a problem, and most importantly, no explicit statement of the need for proofs or logical principles.\n\nEgyptian mathematics refers to mathematics written in the Egyptian language. From the Hellenistic period, Greek replaced Egyptian as the written language of Egyptian scholars. Mathematical study in Egypt later continued under the Arab Empire as part of Islamic mathematics, when Arabic became the written language of Egyptian scholars.\n\nThe most extensive Egyptian mathematical text is the Rhind papyrus (sometimes also called the Ahmes Papyrus after its author), dated to c. 1650 BC but likely a copy of an older document from the Middle Kingdom of about 2000–1800 BC. It is an instruction manual for students in arithmetic and geometry. In addition to giving area formulas and methods for multiplication, division and working with unit fractions, it also contains evidence of other mathematical knowledge, including composite and prime numbers; arithmetic, geometric and harmonic means; and simplistic understandings of both the Sieve of Eratosthenes and perfect number theory (namely, that of the number 6). It also shows how to solve first order linear equations as well as arithmetic and geometric series.\n\nAnother significant Egyptian mathematical text is the Moscow papyrus, also from the Middle Kingdom period, dated to c. 1890 BC. It consists of what are today called \"word problems\" or \"story problems\", which were apparently intended as entertainment. One problem is considered to be of particular importance because it gives a method for finding the volume of a frustum (truncated pyramid).\n\nFinally, the Berlin Papyrus 6619 (c. 1800 BC) shows that ancient Egyptians could solve a second-order algebraic equation.\n\nGreek mathematics refers to the mathematics written in the Greek language from the time of Thales of Miletus (~600 BC) to the closure of the Academy of Athens in 529 AD. Greek mathematicians lived in cities spread over the entire Eastern Mediterranean, from Italy to North Africa, but were united by culture and language. Greek mathematics of the period following Alexander the Great is sometimes called Hellenistic mathematics.\n\nGreek mathematics was much more sophisticated than the mathematics that had been developed by earlier cultures. All surviving records of pre-Greek mathematics show the use of inductive reasoning, that is, repeated observations used to establish rules of thumb. Greek mathematicians, by contrast, used deductive reasoning. The Greeks used logic to derive conclusions from definitions and axioms, and used mathematical rigor to prove them.\n\nGreek mathematics is thought to have begun with Thales of Miletus (c. 624–c.546 BC) and Pythagoras of Samos (c. 582–c. 507 BC). Although the extent of the influence is disputed, they were probably inspired by Egyptian and Babylonian mathematics. According to legend, Pythagoras traveled to Egypt to learn mathematics, geometry, and astronomy from Egyptian priests.\n\nThales used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. As a result, he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. Pythagoras established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was \"All is number\". It was the Pythagoreans who coined the term \"mathematics\", and with whom the study of mathematics for its own sake begins. The Pythagoreans are credited with the first proof of the Pythagorean theorem, though the statement of the theorem has a long history, and with the proof of the existence of irrational numbers. Although he was preceded by the Babylonians and the Chinese, the Neopythagorean mathematician Nicomachus (60–120 AD) provided one of the earliest Greco-Roman multiplication tables, whereas the oldest extant Greek multiplication table is found on a wax tablet dated to the 1st century AD (now found in the British Museum). The association of the Neopythagoreans with the Western invention of the multiplication table is evident in its later Medieval name: the \"mensa Pythagorica\".\n\nPlato (428/427 BC – 348/347 BC) is important in the history of mathematics for inspiring and guiding others. His Platonic Academy, in Athens, became the mathematical center of the world in the 4th century BC, and it was from this school that the leading mathematicians of the day, such as Eudoxus of Cnidus, came. Plato also discussed the foundations of mathematics, clarified some of the definitions (e.g. that of a line as \"breadthless length\"), and reorganized the assumptions. The analytic method is ascribed to Plato, while a formula for obtaining Pythagorean triples bears his name.\n\nEudoxus (408–c. 355 BC) developed the method of exhaustion, a precursor of modern integration and a theory of ratios that avoided the problem of incommensurable magnitudes. The former allowed the calculations of areas and volumes of curvilinear figures, while the latter enabled subsequent geometers to make significant advances in geometry. Though he made no specific technical mathematical discoveries, Aristotle (384–c. 322 BC) contributed significantly to the development of mathematics by laying the foundations of logic.\nIn the 3rd century BC, the premier center of mathematical education and research was the Musaeum of Alexandria. It was there that Euclid (c. 300 BC) taught, and wrote the \"Elements\", widely considered the most successful and influential textbook of all time. The \"Elements\" introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the \"Elements\" were already known, Euclid arranged them into a single, coherent logical framework. The \"Elements\" was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today. In addition to the familiar theorems of Euclidean geometry, the \"Elements\" was meant as an introductory textbook to all mathematical subjects of the time, such as number theory, algebra and solid geometry, including proofs that the square root of two is irrational and that there are infinitely many prime numbers. Euclid also wrote extensively on other subjects, such as conic sections, optics, spherical geometry, and mechanics, but only half of his writings survive.\nArchimedes (c. 287–212 BC) of Syracuse, widely considered the greatest mathematician of antiquity, used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. He also showed one could use the method of exhaustion to calculate the value of π with as much precision as desired, and obtained the most accurate value of π then known, . He also studied the spiral bearing his name, obtained formulas for the volumes of surfaces of revolution (paraboloid, ellipsoid, hyperboloid), and an ingenious method of exponentiation for expressing very large numbers. While he is also known for his contributions to physics and several advanced mechanical devices, Archimedes himself placed far greater value on the products of his thought and general mathematical principles. He regarded as his greatest achievement his finding of the surface area and volume of a sphere, which he obtained by proving these are 2/3 the surface area and volume of a cylinder circumscribing the sphere.\nApollonius of Perga (c. 262–190 BC) made significant advances to the study of conic sections, showing that one can obtain all three varieties of conic section by varying the angle of the plane that cuts a double-napped cone. He also coined the terminology in use today for conic sections, namely parabola (\"place beside\" or \"comparison\"), \"ellipse\" (\"deficiency\"), and \"hyperbola\" (\"a throw beyond\"). His work \"Conics\" is one of the best known and preserved mathematical works from antiquity, and in it he derives many theorems concerning conic sections that would prove invaluable to later mathematicians and astronomers studying planetary motion, such as Isaac Newton. While neither Apollonius nor any other Greek mathematicians made the leap to coordinate geometry, Apollonius' treatment of curves is in some ways similar to the modern treatment, and some of his work seems to anticipate the development of analytical geometry by Descartes some 1800 years later.\n\nAround the same time, Eratosthenes of Cyrene (c. 276–194 BC) devised the Sieve of Eratosthenes for finding prime numbers. The 3rd century BC is generally regarded as the \"Golden Age\" of Greek mathematics, with advances in pure mathematics henceforth in relative decline. Nevertheless, in the centuries that followed significant advances were made in applied mathematics, most notably trigonometry, largely to address the needs of astronomers. Hipparchus of Nicaea (c. 190–120 BC) is considered the founder of trigonometry for compiling the first known trigonometric table, and to him is also due the systematic use of the 360 degree circle. Heron of Alexandria (c. 10–70 AD) is credited with Heron's formula for finding the area of a scalene triangle and with being the first to recognize the possibility of negative numbers possessing square roots. Menelaus of Alexandria (c. 100 AD) pioneered spherical trigonometry through Menelaus' theorem. The most complete and influential trigonometric work of antiquity is the \"Almagest\" of Ptolemy (c. AD 90–168), a landmark astronomical treatise whose trigonometric tables would be used by astronomers for the next thousand years. Ptolemy is also credited with Ptolemy's theorem for deriving trigonometric quantities, and the most accurate value of π outside of China until the medieval period, 3.1416.\nFollowing a period of stagnation after Ptolemy, the period between 250 and 350 AD is sometimes referred to as the \"Silver Age\" of Greek mathematics. During this period, Diophantus made significant advances in algebra, particularly indeterminate analysis, which is also known as \"Diophantine analysis\". The study of Diophantine equations and Diophantine approximations is a significant area of research to this day. His main work was the \"Arithmetica\", a collection of 150 algebraic problems dealing with exact solutions to determinate and indeterminate equations. The \"Arithmetica\" had a significant influence on later mathematicians, such as Pierre de Fermat, who arrived at his famous Last Theorem after trying to generalize a problem he had read in the \"Arithmetica\" (that of dividing a square into two squares). Diophantus also made significant advances in notation, the \"Arithmetica\" being the first instance of algebraic symbolism and syncopation.\nAmong the last great Greek mathematicians is Pappus of Alexandria (4th century AD). He is known for his hexagon theorem and centroid theorem, as well as the Pappus configuration and Pappus graph. His \"Collection\" is a major source of knowledge on Greek mathematics as most of it has survived. Pappus is considered the last major innovator in Greek mathematics, with subsequent work consisting mostly of commentaries on earlier work.\n\nThe first woman mathematician recorded by history was Hypatia of Alexandria (AD 350–415). She succeeded her father (Theon of Alexandria) as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria had her stripped publicly and executed. Her death is sometimes taken as the end of the era of the Alexandrian Greek mathematics, although work did continue in Athens for another century with figures such as Proclus, Simplicius and Eutocius. Although Proclus and Simplicius were more philosophers than mathematicians, their commentaries on earlier works are valuable sources on Greek mathematics. The closure of the neo-Platonic Academy of Athens by the emperor Justinian in 529 AD is traditionally held as marking the end of the era of Greek mathematics, although the Greek tradition continued unbroken in the Byzantine empire with mathematicians such as Anthemius of Tralles and Isidore of Miletus, the architects of the Haghia Sophia. Nevertheless, Byzantine mathematics consisted mostly of commentaries, with little in the way of innovation, and the centers of mathematical innovation were to be found elsewhere by this time.\n\nAlthough ethnic Greek mathematicians continued to live under the rule of the late Roman Republic and subsequent Roman Empire, there were no noteworthy native Latin mathematicians in comparison. Ancient Romans such as Cicero (106–43 BC), an influential Roman statesman who studied mathematics in Greece, believed that Roman surveyors and calculators were far more interested in applied mathematics than the theoretical mathematics and geometry that were prized by the Greeks. It is unclear if the Romans first derived their numerical system directly from the Greek precedent or from Etruscan numerals used by the Etruscan civilization centered in what is now Tuscany, central Italy.\n\nUsing calculation, Romans were adept at both instigating and detecting financial fraud, as well as managing taxes for the treasury. Siculus Flaccus, one of the Roman \"gromatici\" (i.e. land surveyor), wrote the \"Categories of Fields\", which aided Roman surveyors in measuring the surface areas of allotted lands and territories. Aside from managing trade and taxes, the Romans also regularly applied mathematics to solve problems in engineering, including the erection of architecture such as bridges, road-building, and preparation for military campaigns. Arts and crafts such as Roman mosaics, inspired by previous Greek designs, created illusionist geometric patterns and rich, detailed scenes that required precise measurements for each tessera tile, the opus tessellatum pieces on average measuring eight millimeters square and the finer opus vermiculatum pieces having an average surface of four millimeters square.\n\nThe creation of the Roman calendar also necessitated basic mathematics. The first calendar allegedly dates back to 8th century BC during the Roman Kingdom and included 356 days plus a leap year every other year. In contrast, the lunar calendar of the Republican era contained 355 days, roughly ten-and-one-fourth days shorter than the solar year, a discrepancy that was solved by adding an extra month into the calendar after the 23rd of February. This calendar was supplanted by the Julian calendar, a solar calendar organized by Julius Caesar (100–44 BC) and devised by Sosigenes of Alexandria to include a leap day every four years in a 365-day cycle. This calendar, which contained an error of 11 minutes and 14 seconds, was later corrected by the Gregorian calendar organized by Pope Gregory XIII (), virtually the same solar calendar used in modern times as the international standard calendar.\n\nAt roughly the same time, the Han Chinese and the Romans both invented the wheeled odometer device for measuring distances traveled, the Roman model first described by the Roman civil engineer and architect Vitruvius (c. 80 BC - c. 15 BC). The device was used at least until the reign of emperor Commodus (), but its design seems to have been lost until experiments were made during the 15th century in Western Europe. Perhaps relying on similar gear-work and technology found in the Antikythera mechanism, the odometer of Vitruvius featured chariot wheels measuring 4 feet (1.2 m) in diameter turning four-hundred times in one Roman mile (roughly 4590 ft/1400 m). With each revolution, a pin-and-axle device engaged a 400-tooth cogwheel that turned a second gear responsible for dropping pebbles into a box, each pebble representing one mile traversed.\n\nAn analysis of early Chinese mathematics has demonstrated its unique development compared to other parts of the world, leading scholars to assume an entirely independent development. The oldest extant mathematical text from China is the \"Zhoubi Suanjing\", variously dated to between 1200 BC and 100 BC, though a date of about 300 BC during the Warring States Period appears reasonable. However, the Tsinghua Bamboo Slips, containing the earliest known decimal multiplication table (although ancient Babylonians had ones with a base of 60), is dated around 305 BC and is perhaps the oldest surviving mathematical text of China.\nOf particular note is the use in Chinese mathematics of a decimal positional notation system, the so-called \"rod numerals\" in which distinct ciphers were used for numbers between 1 and 10, and additional ciphers for powers of ten. Thus, the number 123 would be written using the symbol for \"1\", followed by the symbol for \"100\", then the symbol for \"2\" followed by the symbol for \"10\", followed by the symbol for \"3\". This was the most advanced number system in the world at the time, apparently in use several centuries before the common era and well before the development of the Indian numeral system. Rod numerals allowed the representation of numbers as large as desired and allowed calculations to be carried out on the \"suan pan\", or Chinese abacus. The date of the invention of the \"suan pan\" is not certain, but the earliest written mention dates from AD 190, in Xu Yue's \"Supplementary Notes on the Art of Figures\".\n\nThe oldest existent work on geometry in China comes from the philosophical Mohist canon c. 330 BC, compiled by the followers of Mozi (470–390 BC). The \"Mo Jing\" described various aspects of many fields associated with physical science, and provided a small number of geometrical theorems as well. It also defined the concepts of circumference, diameter, radius, and volume.\nIn 212 BC, the Emperor Qin Shi Huang commanded all books in the Qin Empire other than officially sanctioned ones be burned. This decree was not universally obeyed, but as a consequence of this order little is known about ancient Chinese mathematics before this date. After the book burning of 212 BC, the Han dynasty (202 BC–220 AD) produced works of mathematics which presumably expanded on works that are now lost. The most important of these is \"The Nine Chapters on the Mathematical Art\", the full title of which appeared by AD 179, but existed in part under other titles beforehand. It consists of 246 word problems involving agriculture, business, employment of geometry to figure height spans and dimension ratios for Chinese pagoda towers, engineering, surveying, and includes material on right triangles. It created mathematical proof for the Pythagorean theorem, and a mathematical formula for Gaussian elimination. The treatise also provides values of π, which Chinese mathematicians originally approximated as 3 until Liu Xin (d. 23 AD) provided a figure of 3.1457 and subsequently Zhang Heng (78–139) approximated pi as 3.1724, as well as 3.162 by taking the square root of 10. Liu Hui commented on the \"Nine Chapters\" in the 3rd century AD and gave a value of π accurate to 5 decimal places (i.e. 3.14159). Though more of a matter of computational stamina than theoretical insight, in the 5th century AD Zu Chongzhi computed the value of π to seven decimal places (i.e. 3.141592), which remained the most accurate value of π for almost the next 1000 years. He also established a method which would later be called Cavalieri's principle to find the volume of a sphere.\n\nThe high-water mark of Chinese mathematics occurred in the 13th century during the latter half of the Song dynasty (960–1279), with the development of Chinese algebra. The most important text from that period is the \"Precious Mirror of the Four Elements\" by Zhu Shijie (1249–1314), dealing with the solution of simultaneous higher order algebraic equations using a method similar to Horner's method. The \"Precious Mirror\" also contains a diagram of Pascal's triangle with coefficients of binomial expansions through the eighth power, though both appear in Chinese works as early as 1100. The Chinese also made use of the complex combinatorial diagram known as the magic square and magic circles, described in ancient times and perfected by Yang Hui (AD 1238–1298).\n\nEven after European mathematics began to flourish during the Renaissance, European and Chinese mathematics were separate traditions, with significant Chinese mathematical output in decline from the 13th century onwards. Jesuit missionaries such as Matteo Ricci carried mathematical ideas back and forth between the two cultures from the 16th to 18th centuries, though at this point far more mathematical ideas were entering China than leaving.\n\nJapanese mathematics, Korean mathematics, and Vietnamese mathematics are traditionally viewed as stemming from Chinese mathematics and belonging to the Confucian-based East Asian cultural sphere. Korean and Japanese mathematics were heavily influenced by the algebraic works produced during China's Song dynasty, whereas Vietnamese mathematics was heavily indebted to popular works of China's Ming dynasty (1368–1644). For instance, although Vietnamese mathematical treatises were written in either Chinese or the native Vietnamese Chữ Nôm script, all of them followed the Chinese format of presenting a collection of problems with algorithms for solving them, followed by numerical answers. Mathematics in Vietnam and Korea were mostly associated with the professional court bureaucracy of mathematicians and astronomers, whereas in Japan it was more prevalent in the realm of private schools.\n\nThe earliest civilization on the Indian subcontinent is the Indus Valley Civilization (mature phase: 2600 to 1900 BC) that flourished in the Indus river basin. Their cities were laid out with geometric regularity, but no known mathematical documents survive from this civilization.\n\nThe oldest extant mathematical records from India are the Sulba Sutras (dated variously between the 8th century BC and the 2nd century AD), appendices to religious texts which give simple rules for constructing altars of various shapes, such as squares, rectangles, parallelograms, and others. As with Egypt, the preoccupation with temple functions points to an origin of mathematics in religious ritual. The Sulba Sutras give methods for constructing a circle with approximately the same area as a given square, which imply several different approximations of the value of π. In addition, they compute the square root of 2 to several decimal places, list Pythagorean triples, and give a statement of the Pythagorean theorem. All of these results are present in Babylonian mathematics, indicating Mesopotamian influence. It is not known to what extent the Sulba Sutras influenced later Indian mathematicians. As in China, there is a lack of continuity in Indian mathematics; significant advances are separated by long periods of inactivity.\n\nPāṇini (c. 5th century BC) formulated the rules for Sanskrit grammar. His notation was similar to modern mathematical notation, and used metarules, transformations, and recursion. Pingala (roughly 3rd–1st centuries BC) in his treatise of prosody uses a device corresponding to a binary numeral system. His discussion of the combinatorics of meters corresponds to an elementary version of the binomial theorem. Pingala's work also contains the basic ideas of Fibonacci numbers (called \"mātrāmeru\").\n\nThe next significant mathematical documents from India after the \"Sulba Sutras\" are the \"Siddhantas\", astronomical treatises from the 4th and 5th centuries AD (Gupta period) showing strong Hellenistic influence. They are significant in that they contain the first instance of trigonometric relations based on the half-chord, as is the case in modern trigonometry, rather than the full chord, as was the case in Ptolemaic trigonometry. Through a series of translation errors, the words \"sine\" and \"cosine\" derive from the Sanskrit \"jiya\" and \"kojiya\".\nAround 500 AD, Aryabhata wrote the \"Aryabhatiya\", a slim volume, written in verse, intended to supplement the rules of calculation used in astronomy and mathematical mensuration, though with no feeling for logic or deductive methodology. Though about half of the entries are wrong, it is in the \"Aryabhatiya\" that the decimal place-value system first appears. Several centuries later, the Muslim mathematician Abu Rayhan Biruni described the \"Aryabhatiya\" as a \"mix of common pebbles and costly crystals\".\n\nIn the 7th century, Brahmagupta identified the Brahmagupta theorem, Brahmagupta's identity and Brahmagupta's formula, and for the first time, in \"Brahma-sphuta-siddhanta\", he lucidly explained the use of zero as both a placeholder and decimal digit, and explained the Hindu–Arabic numeral system. It was from a translation of this Indian text on mathematics (c. 770) that Islamic mathematicians were introduced to this numeral system, which they adapted as Arabic numerals. Islamic scholars carried knowledge of this number system to Europe by the 12th century, and it has now displaced all older number systems throughout the world. Various symbol sets are used to represent numbers in the Hindu–Arabic numeral system, all of which evolved from the Brahmi numerals. Each of the roughly dozen major scripts of India has its own numeral glyphs. In the 10th century, Halayudha's commentary on Pingala's work contains a study of the Fibonacci sequence and Pascal's triangle, and describes the formation of a matrix.\n\nIn the 12th century, Bhāskara II lived in southern India and wrote extensively on all then known branches of mathematics. His work contains mathematical objects equivalent or approximately equivalent to infinitesimals, derivatives, the mean value theorem and the derivative of the sine function. To what extent he anticipated the invention of calculus is a controversial subject among historians of mathematics.\n\nIn the 14th century, Madhava of Sangamagrama, the founder of the so-called Kerala School of Mathematics, found the Madhava–Leibniz series, and, using 21 terms, computed the value of π as 3.14159265359. Madhava also found the Madhava-Gregory series to determine the arctangent, the Madhava-Newton power series to determine sine and cosine and the Taylor approximation for sine and cosine functions. In the 16th century, Jyesthadeva consolidated many of the Kerala School's developments and theorems in the \"Yukti-bhāṣā\". However, the Kerala School did not formulate a systematic theory of differentiation and integration, nor is there any direct evidence of their results being transmitted outside Kerala.\n\nThe Islamic Empire established across Persia, the Middle East, Central Asia, North Africa, Iberia, and in parts of India in the 8th century made significant contributions towards mathematics. Although most Islamic texts on mathematics were written in Arabic, most of them were not written by Arabs, since much like the status of Greek in the Hellenistic world, Arabic was used as the written language of non-Arab scholars throughout the Islamic world at the time. Persians contributed to the world of Mathematics alongside Arabs.\n\nIn the 9th century, the Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī wrote several important books on the Hindu–Arabic numerals and on methods for solving equations. His book \"On the Calculation with Hindu Numerals\", written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. The word \"algorithm\" is derived from the Latinization of his name, Algoritmi, and the word \"algebra\" from the title of one of his works, \"Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala\" (\"The Compendious Book on Calculation by Completion and Balancing\"). He gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and he was the first to teach algebra in an elementary form and for its own sake. He also discussed the fundamental method of \"reduction\" and \"balancing\", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as \"al-jabr\". His algebra was also no longer concerned \"with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study.\" He also studied an equation for its own sake and \"in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems.\"\n\nIn Egypt, Abu Kamil extended algebra to the set of irrational numbers, accepting square roots and fourth roots as solutions and coefficients to quadratic equations. He also developed techniques used to solve three non-linear simultaneous equations with three unknown variables. One unique feature of his works was trying to find all the possible solutions to some of his problems, including one where he found 2676 solutions. His works formed an important foundation for the development of algebra and influenced later mathematicians, such as al-Karaji and Fibonacci.\n\nFurther developments in algebra were made by Al-Karaji in his treatise \"al-Fakhri\", where he extends the methodology to incorporate integer powers and integer roots of unknown quantities. Something close to a proof by mathematical induction appears in a book written by Al-Karaji around 1000 AD, who used it to prove the binomial theorem, Pascal's triangle, and the sum of integral cubes. The historian of mathematics, F. Woepcke, praised Al-Karaji for being \"the first who introduced the theory of algebraic calculus.\" Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham was the first mathematician to derive the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. He performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. He thus came close to finding a general formula for the integrals of polynomials, but he was not concerned with any polynomials higher than the fourth degree.\n\nIn the late 11th century, Omar Khayyam wrote \"Discussions of the Difficulties in Euclid\", a book about what he perceived as flaws in Euclid's \"Elements\", especially the parallel postulate. He was also the first to find the general geometric solution to cubic equations. He was also very influential in calendar reform.\n\nIn the 13th century, Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. He also wrote influential work on Euclid's parallel postulate. In the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating \"n\"th roots, which was a special case of the methods given many centuries later by Ruffini and Horner.\n\nOther achievements of Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals, the discovery of all the modern trigonometric functions besides the sine, al-Kindi's introduction of cryptanalysis and frequency analysis, the development of analytic geometry by Ibn al-Haytham, the beginning of algebraic geometry by Omar Khayyam and the development of an algebraic notation by al-Qalasādī.\n\nDuring the time of the Ottoman Empire and Safavid Empire from the 15th century, the development of Islamic mathematics became stagnant.\n\nIn the Pre-Columbian Americas, the Maya civilization that flourished in Mexico and Central America during the 1st millennium AD developed a unique tradition of mathematics that, due to its geographic isolation, was entirely independent of existing European, Egyptian, and Asian mathematics. Maya numerals utilized a base of 20, the vigesimal system, instead of a base of ten that forms the basis of the decimal system used by most modern cultures. The Mayas used mathematics to create the Maya calendar as well as to predict astronomical phenomena in their native Maya astronomy. While the concept of zero had to be inferred in the mathematics of many contemporary cultures, the Mayas developed a standard symbol for it.\n\nMedieval European interest in mathematics was driven by concerns quite different from those of modern mathematicians. One driving element was the belief that mathematics provided the key to understanding the created order of nature, frequently justified by Plato's \"Timaeus\" and the biblical passage (in the \"Book of Wisdom\") that God had \"ordered all things in measure, and number, and weight\".\n\nBoethius provided a place for mathematics in the curriculum in the 6th century when he coined the term \"quadrivium\" to describe the study of arithmetic, geometry, astronomy, and music. He wrote \"De institutione arithmetica\", a free translation from the Greek of Nicomachus's \"Introduction to Arithmetic\"; \"De institutione musica\", also derived from Greek sources; and a series of excerpts from Euclid's \"Elements\". His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.\n\nIn the 12th century, European scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's \"The Compendious Book on Calculation by Completion and Balancing\", translated into Latin by Robert of Chester, and the complete text of Euclid's \"Elements\", translated in various versions by Adelard of Bath, Herman of Carinthia, and Gerard of Cremona. These and other new sources sparked a renewal of mathematics.\n\nLeonardo of Pisa, now known as Fibonacci, serendipitously learned about the Hindu–Arabic numerals on a trip to what is now Béjaïa, Algeria with his merchant father. (Europe was still using Roman numerals.) There, he observed a system of arithmetic (specifically algorism) which due to the positional notation of Hindu–Arabic numerals was much more efficient and greatly facilitated commerce. Leonardo wrote \"Liber Abaci\" in 1202 (updated in 1254) introducing the technique to Europe and beginning a long period of popularizing it. The book also brought to Europe what is now known as the Fibonacci sequence (known to Indian mathematicians for hundreds of years before that) which was used as an unremarkable example within the text.\n\nThe 14th century saw the development of new mathematical concepts to investigate a wide range of problems. One important contribution was development of mathematics of local motion.\n\nThomas Bradwardine proposed that speed (V) increases in arithmetic proportion as the ratio of force (F) to resistance (R) increases in geometric proportion. Bradwardine expressed this by a series of specific examples, but although the logarithm had not yet been conceived, we can express his conclusion anachronistically by writing:\nV = log (F/R). Bradwardine's analysis is an example of transferring a mathematical technique used by al-Kindi and Arnald of Villanova to quantify the nature of compound medicines to a different physical problem.\nOne of the 14th-century Oxford Calculators, William Heytesbury, lacking differential calculus and the concept of limits, proposed to measure instantaneous speed \"by the path that would be described by [a body] if... it were moved uniformly at the same degree of speed with which it is moved in that given instant\".\n\nHeytesbury and others mathematically determined the distance covered by a body undergoing uniformly accelerated motion (today solved by integration), stating that \"a moving body uniformly acquiring or losing that increment [of speed] will traverse in some given time a [distance] completely equal to that which it would traverse if it were moving continuously through the same time with the mean degree [of speed]\".\n\nNicole Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of this relationship, asserting that the area under the line depicting the constant acceleration, represented the total distance traveled. In a later mathematical commentary on Euclid's \"Elements\", Oresme made a more detailed general analysis in which he demonstrated that a body will acquire in each successive increment of time an increment of any quality that increases as the odd numbers. Since Euclid had demonstrated the sum of the odd numbers are the square numbers, the total quality acquired by the body increases as the square of the time.\n\nDuring the Renaissance, the development of mathematics and of accounting were intertwined. While there is no direct relationship between algebra and accounting, the teaching of the subjects and the books published often intended for the children of merchants who were sent to reckoning schools (in Flanders and Germany) or abacus schools (known as \"abbaco\" in Italy), where they learned the skills useful for trade and commerce. There is probably no need for algebra in performing bookkeeping operations, but for complex bartering operations or the calculation of compound interest, a basic knowledge of arithmetic was mandatory and knowledge of algebra was very useful.\n\nPiero della Francesca (c. 1415–1492) wrote books on solid geometry and linear perspective, including \"De Prospectiva Pingendi (On Perspective for Painting)\", \"Trattato d’Abaco (Abacus Treatise)\", and \"De corporibus regularibus (Regular Solids)\".\nLuca Pacioli's \"Summa de Arithmetica, Geometria, Proportioni et Proportionalità\" (Italian: \"Review of Arithmetic, Geometry, Ratio and Proportion\") was first printed and published in Venice in 1494. It included a 27-page treatise on bookkeeping, \"Particularis de Computis et Scripturis\" (Italian: \"Details of Calculation and Recording\"). It was written primarily for, and sold mainly to, merchants who used the book as a reference text, as a source of pleasure from the mathematical puzzles it contained, and to aid the education of their sons. In \"Summa Arithmetica\", Pacioli introduced symbols for plus and minus for the first time in a printed book, symbols that became standard notation in Italian Renaissance mathematics. \"Summa Arithmetica\" was also the first known book printed in Italy to contain algebra. Pacioli obtained many of his ideas from Piero Della Francesca whom he plagiarized.\n\nIn Italy, during the first half of the 16th century, Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book \"Ars Magna\", together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his \"L'Algebra\" in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.\n\nSimon Stevin's book \"De Thiende\" ('the art of tenths'), first published in Dutch in 1585, contained the first systematic treatment of decimal notation, which influenced all later work on the real number system.\n\nDriven by the demands of navigation and the growing need for accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his \"Trigonometria\" in 1595. Regiomontanus's table of sines and cosines was published in 1533.\n\nDuring the Renaissance the desire of artists to represent the natural world realistically, together with the rediscovered philosophy of the Greeks, led artists to study mathematics. They were also the engineers and architects of that time, and so had need of mathematics in any case. The art of painting in perspective, and the developments in geometry that involved, were studied intensely.\n\nThe 17th century saw an unprecedented increase of mathematical and scientific ideas across Europe. Galileo observed the moons of Jupiter in orbit about that planet, using a telescope based on a toy imported from Holland. Tycho Brahe had gathered an enormous quantity of mathematical data describing the positions of the planets in the sky. By his position as Brahe's assistant, Johannes Kepler was first exposed to and seriously interacted with the topic of planetary motion. Kepler's calculations were made simpler by the contemporaneous invention of logarithms by John Napier and Jost Bürgi. Kepler succeeded in formulating mathematical laws of planetary motion.\nThe analytic geometry developed by René Descartes (1596–1650) allowed those orbits to be plotted on a graph, in Cartesian coordinates.\n\nBuilding on earlier work by many predecessors, Isaac Newton discovered the laws of physics explaining Kepler's Laws, and brought together the concepts now known as calculus. Independently, Gottfried Wilhelm Leibniz, who is arguably one of the most important mathematicians of the 17th century, developed calculus and much of the calculus notation still in use today. Science and mathematics had become an international endeavor, which would soon spread over the entire world.\n\nIn addition to the application of mathematics to the studies of the heavens, applied mathematics began to expand into new areas, with the correspondence of Pierre de Fermat and Blaise Pascal. Pascal and Fermat set the groundwork for the investigations of probability theory and the corresponding rules of combinatorics in their discussions over a game of gambling. Pascal, with his wager, attempted to use the newly developing probability theory to argue for a life devoted to religion, on the grounds that even if the probability of success was small, the rewards were infinite. In some sense, this foreshadowed the development of utility theory in the 18th–19th century.\n\nThe most influential mathematician of the 18th century was arguably Leonhard Euler. His contributions range from founding the study of graph theory with the Seven Bridges of Königsberg problem to standardizing many modern mathematical terms and notations. For example, he named the square root of minus 1 with the symbol \"i\", and he popularized the use of the Greek letter formula_1 to stand for the ratio of a circle's circumference to its diameter. He made numerous contributions to the study of topology, graph theory, calculus, combinatorics, and complex analysis, as evidenced by the multitude of theorems and notations named for him.\n\nOther important European mathematicians of the 18th century included Joseph Louis Lagrange, who did pioneering work in number theory, algebra, differential calculus, and the calculus of variations, and Laplace who, in the age of Napoleon, did important work on the foundations of celestial mechanics and on statistics.\n\nThroughout the 19th century mathematics became increasingly abstract. Carl Friedrich Gauss (1777–1855) epitomizes this trend. He did revolutionary work on functions of complex variables, in geometry, and on the convergence of series, leaving aside his many contributions to science. He also gave the first satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law.\nThis century saw the development of the two forms of non-Euclidean geometry, where the parallel postulate of Euclidean geometry no longer holds.\nThe Russian mathematician Nikolai Ivanovich Lobachevsky and his rival, the Hungarian mathematician János Bolyai, independently defined and studied hyperbolic geometry, where uniqueness of parallels no longer holds. In this geometry the sum of angles in a triangle add up to less than 180°. Elliptic geometry was developed later in the 19th century by the German mathematician Bernhard Riemann; here no parallel can be found and the angles in a triangle add up to more than 180°. Riemann also developed Riemannian geometry, which unifies and vastly generalizes the three types of geometry, and he defined the concept of a manifold, which generalizes the ideas of curves and surfaces.\n\nThe 19th century saw the beginning of a great deal of abstract algebra. Hermann Grassmann in Germany gave a first version of vector spaces, William Rowan Hamilton in Ireland developed noncommutative algebra. The British mathematician George Boole devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1. Boolean algebra is the starting point of mathematical logic and has important applications in computer science.\n\nAugustin-Louis Cauchy, Bernhard Riemann, and Karl Weierstrass reformulated the calculus in a more rigorous fashion.\n\nAlso, for the first time, the limits of mathematics were explored. Niels Henrik Abel, a Norwegian, and Évariste Galois, a Frenchman, proved that there is no general algebraic method for solving polynomial equations of degree greater than four (Abel–Ruffini theorem). Other 19th-century mathematicians utilized this in their proofs that straightedge and compass alone are not sufficient to trisect an arbitrary angle, to construct the side of a cube twice the volume of a given cube, nor to construct a square equal in area to a given circle. Mathematicians had vainly attempted to solve all of these problems since the time of the ancient Greeks. On the other hand, the limitation of three dimensions in geometry was surpassed in the 19th century through considerations of parameter space and hypercomplex numbers.\n\nAbel and Galois's investigations into the solutions of various polynomial equations laid the groundwork for further developments of group theory, and the associated fields of abstract algebra. In the 20th century physicists and other scientists have seen group theory as the ideal way to study symmetry.\n\nIn the later 19th century, Georg Cantor established the first foundations of set theory, which enabled the rigorous treatment of the notion of infinity and has become the common language of nearly all mathematics. Cantor's set theory, and the rise of mathematical logic in the hands of Peano, L.E.J. Brouwer, David Hilbert, Bertrand Russell, and A.N. Whitehead, initiated a long running debate on the foundations of mathematics.\n\nThe 19th century saw the founding of a number of national mathematical societies: the London Mathematical Society in 1865, the Société Mathématique de France in 1872, the Circolo Matematico di Palermo in 1884, the Edinburgh Mathematical Society in 1883, and the American Mathematical Society in 1888. The first international, special-interest society, the Quaternion Society, was formed in 1899, in the context of a vector controversy.\n\nIn 1897, Hensel introduced p-adic numbers.\n\nThe 20th century saw mathematics become a major profession. Every year, thousands of new Ph.D.s in mathematics were awarded, and jobs were available in both teaching and industry. An effort to catalogue the areas and applications of mathematics was undertaken in Klein's encyclopedia.\n\nIn a 1900 speech to the International Congress of Mathematicians, David Hilbert set out a list of 23 unsolved problems in mathematics. These problems, spanning many areas of mathematics, formed a central focus for much of 20th-century mathematics. Today, 10 have been solved, 7 are partially solved, and 2 are still open. The remaining 4 are too loosely formulated to be stated as solved or not.\nNotable historical conjectures were finally proven. In 1976, Wolfgang Haken and Kenneth Appel proved the four color theorem, controversial at the time for the use of a computer to do so. Andrew Wiles, building on the work of others, proved Fermat's Last Theorem in 1995. Paul Cohen and Kurt Gödel proved that the continuum hypothesis is independent of (could neither be proved nor disproved from) the standard axioms of set theory. In 1998 Thomas Callister Hales proved the Kepler conjecture.\n\nMathematical collaborations of unprecedented size and scope took place. An example is the classification of finite simple groups (also called the \"enormous theorem\"), whose proof between 1955 and 1983 required 500-odd journal articles by about 100 authors, and filling tens of thousands of pages. A group of French mathematicians, including Jean Dieudonné and André Weil, publishing under the pseudonym \"Nicolas Bourbaki\", attempted to exposit all of known mathematics as a coherent rigorous whole. The resulting several dozen volumes has had a controversial influence on mathematical education.\nDifferential geometry came into its own when Einstein used it in general relativity. Entirely new areas of mathematics such as mathematical logic, topology, and John von Neumann's game theory changed the kinds of questions that could be answered by mathematical methods. All kinds of structures were abstracted using axioms and given names like metric spaces, topological spaces etc. As mathematicians do, the concept of an abstract structure was itself abstracted and led to category theory. Grothendieck and Serre recast algebraic geometry using sheaf theory. Large advances were made in the qualitative study of dynamical systems that Poincaré had begun in the 1890s.\nMeasure theory was developed in the late 19th and early 20th centuries. Applications of measures include the Lebesgue integral, Kolmogorov's axiomatisation of probability theory, and ergodic theory. Knot theory greatly expanded. Quantum mechanics led to the development of functional analysis. Other new areas include Laurent Schwartz's distribution theory, fixed point theory, singularity theory and René Thom's catastrophe theory, model theory, and Mandelbrot's fractals. Lie theory with its Lie groups and Lie algebras became one of the major areas of study.\n\nNon-standard analysis, introduced by Abraham Robinson, rehabilitated the infinitesimal approach to calculus, which had fallen into disrepute in favour of the theory of limits, by extending the field of real numbers to the Hyperreal numbers which include infinitesimal and infinite quantities. An even larger number system, the surreal numbers were discovered by John Horton Conway in connection with combinatorial games.\n\nThe development and continual improvement of computers, at first mechanical analog machines and then digital electronic machines, allowed industry to deal with larger and larger amounts of data to facilitate mass production and distribution and communication, and new areas of mathematics were developed to deal with this: Alan Turing's computability theory; complexity theory; Derrick Henry Lehmer's use of ENIAC to further number theory and the Lucas-Lehmer test; Rózsa Péter's recursive function theory; Claude Shannon's information theory; signal processing; data analysis; optimization and other areas of operations research. In the preceding centuries much mathematical focus was on calculus and continuous functions, but the rise of computing and communication networks led to an increasing importance of discrete concepts and the expansion of combinatorics including graph theory. The speed and data processing abilities of computers also enabled the handling of mathematical problems that were too time-consuming to deal with by pencil and paper calculations, leading to areas such as numerical analysis and symbolic computation. Some of the most important methods and algorithms of the 20th century are: the simplex algorithm, the Fast Fourier Transform, error-correcting codes, the Kalman filter from control theory and the RSA algorithm of public-key cryptography.\n\nAt the same time, deep insights were made about the limitations to mathematics. In 1929 and 1930, it was proved the truth or falsity of all statements formulated about the natural numbers plus one of addition and multiplication, was decidable, i.e. could be determined by some algorithm. In 1931, Kurt Gödel found that this was not the case for the natural numbers plus both addition and multiplication; this system, known as Peano arithmetic, was in fact incompletable. (Peano arithmetic is adequate for a good deal of number theory, including the notion of prime number.) A consequence of Gödel's two incompleteness theorems is that in any mathematical system that includes Peano arithmetic (including all of analysis and geometry), truth necessarily outruns proof, i.e. there are true statements that cannot be proved within the system. Hence mathematics cannot be reduced to mathematical logic, and David Hilbert's dream of making all of mathematics complete and consistent needed to be reformulated.\nOne of the more colorful figures in 20th-century mathematics was Srinivasa Aiyangar Ramanujan (1887–1920), an Indian autodidact who conjectured or proved over 3000 theorems, including properties of highly composite numbers, the partition function and its asymptotics, and mock theta functions. He also made major investigations in the areas of gamma functions, modular forms, divergent series, hypergeometric series and prime number theory.\n\nPaul Erdős published more papers than any other mathematician in history, working with hundreds of collaborators. Mathematicians have a game equivalent to the Kevin Bacon Game, which leads to the Erdős number of a mathematician. This describes the \"collaborative distance\" between a person and Paul Erdős, as measured by joint authorship of mathematical papers.\n\nEmmy Noether has been described by many as the most important woman in the history of mathematics. She studied the theories of rings, fields, and algebras.\n\nAs in most areas of study, the explosion of knowledge in the scientific age has led to specialization: by the end of the century there were hundreds of specialized areas in mathematics and the Mathematics Subject Classification was dozens of pages long. More and more mathematical journals were published and, by the end of the century, the development of the World Wide Web led to online publishing.\n\nIn 2000, the Clay Mathematics Institute announced the seven Millennium Prize Problems, and in 2003 the Poincaré conjecture was solved by Grigori Perelman (who declined to accept an award, as he was critical of the mathematics establishment).\n\nMost mathematical journals now have online versions as well as print versions, and many online-only journals are launched. There is an increasing drive towards open access publishing, first popularized by the arXiv.\n\nThere are many observable trends in mathematics, the most notable being that the subject is growing ever larger, computers are ever more important and powerful, the application of mathematics to bioinformatics is rapidly expanding, and the volume of data being produced by science and industry, facilitated by computers, is explosively expanding.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "320026", "url": "https://en.wikipedia.org/wiki?curid=320026", "title": "Index notation", "text": "Index notation\n\nIn mathematics and computer programming, index notation is used to specify the elements of an array of numbers. The formalism of how indices are used varies according to the subject. In particular, there are different methods for referring to the elements of a list, a vector, or a matrix, depending on whether one is writing a formal mathematical paper for publication, or when one is writing a computer program.\n\nIt is frequently helpful in mathematics to refer to the elements of an array using subscripts. The subscripts can be integers or variables. The array takes the form of tensors in general, since these can be treated as multi-dimensional arrays. Special (and more familiar) cases are vectors (1d arrays) and matrices (2d arrays).\n\nThe following is only an introduction to the concept: index notation is used in more detail in mathematics (particularly in the representation and manipulation of tensor operations). See the main article for further details.\n\nA vector treated as an array of numbers by writing as a row vector or column vector (whichever is used depends on convenience or context):\n\nIndex notation allows indication of the elements of the array by simply writing \"a\", where the index \"i\" is known to run from 1 to \"n\".\nFor example, given the vector:\n\nthen some entries are\n\nThe notation can be applied to vectors in mathematics and physics. The following vector equation\n\ncan also be written in terms of the elements of the vector (aka components), that is\n\nwhere the indices take a given range of values. This expression represents a set of equations, one for each index. If the vectors each have \"n\" elements, meaning \"i\" = 1,2...\"n\", then the equations are explicitly\n\nHence, index notation serves as an efficient shorthand for\n\nMore than one index is used to describe arrays of numbers, in two or more dimensions, such as the elements of a matrix, (see also image to right);\n\nThe entry of a matrix A is written using two indices, say \"i\" and \"j\", with or without commas to separate the indices: \"a\" or \"a\", where the first subscript is the row number and the second is the column number. Juxtaposition is also used as notation for multiplication; this may be a source of confusion. For example, if\n\nthen some entries are\n\nFor indices larger than 9, the comma-based notation may be superior (e.g., \"a\" instead of \"a\").\n\nMatrix equations are written similarly to vector equations, such as\n\nin terms of the elements of the matrices (aka components)\n\nfor all values of \"i\" and \"j\". Again this expression represents a set of equations, one for each index. If the matrices each have \"m\" rows and \"n\" columns, meaning and , then there are \"mn\" equations.\n\nThe notation allows a clear generalization to multi-dimensional arrays of elements: tensors. For example,\n\nrepresenting a set of many equations.\n\nIn tensor analysis, superscripts are used instead of subscripts to distinguish covariant from contravariant entities, see covariance and contravariance of vectors and raising and lowering indices.\n\nIn several programming languages, index notation is a way of addressing elements of an array. This method is used since it is closest to how it is implemented in assembly language whereby the address of the first element is used as a base, and a multiple (the index) of the element size is used to address inside the array.\n\nFor example, if an array of integers is stored in a region of the computer's memory starting at the memory cell with address 3000 (the base address), and each integer occupies four cells (bytes), then the elements of this array are at memory locations 0x3000, 0x3004, 0x3008, …, 0x3000 + 4(\"n\" − 1). In general, the address of the \"i\"th element of an array with base address \"b\" and element size \"s\" is .\n\nIn the C programming language, we can write the above as (pointer form) or (array indexing form), which is exactly equivalent because the C standard defines the array indexing form as a transformation to pointer form. Coincidentally, since pointer addition is commutative, this allows for obscure expressions such as which is equivalent to .\n\nThings become more interesting when we consider arrays with more than one index, for example, a two-dimensional table. We have three possibilities:\nIn C, all three methods can be used. When the first method is used, the programmer decides how the elements of the array are laid out in the computer's memory, and provides the formulas to compute the location of each element. The second method is used when the number of elements in each row is the same and known at the time the program is written. The programmer declares the array to have, say, three columns by writing e.g. . One then refers to a particular element of the array by writing . The compiler computes the total number of memory cells occupied by each row, uses the first index to find the address of the desired row, and then uses the second index to find the address of the desired element in the row. When the third method is used, the programmer declares the table to be an array of pointers, like in . When the programmer subsequently specifies a particular element , the compiler generates instructions to look up the address of the row specified by the first index, and use this address as the base when computing the address of the element specified by the second index.\n\nThis function multiplies two 3x3 floating point matrices together.\nIn other programming languages such as Pascal, indices may start at 1, so indexing in a block of memory can be changed to fit a start-at-1 addressing scheme by a simple linear transformation - in this scheme, the memory location of the \"i\"th element with base address \"b\" and element size \"s\" is .\n\n"}
{"id": "14439044", "url": "https://en.wikipedia.org/wiki?curid=14439044", "title": "Kraków School of Mathematics and Astrology", "text": "Kraków School of Mathematics and Astrology\n\nThe Kraków School of Mathematics and Astrology () was an influential mid-to-late-15th-century group of mathematicians and astrologers at the University of Kraków (later \"Jagiellonian University\").\n\n\n"}
{"id": "187750", "url": "https://en.wikipedia.org/wiki?curid=187750", "title": "Large numbers", "text": "Large numbers\n\nLarge numbers are numbers that are significantly larger than those ordinarily used in everyday life, for instance in simple counting or in monetary transactions. The term typically refers to large positive integers, or more generally, large positive real numbers, but it may also be used in other contexts.\n\nVery large numbers often occur in fields such as mathematics, cosmology, cryptography, and statistical mechanics. Sometimes people refer to numbers as being \"astronomically large\". However, it is easy to mathematically define numbers that are much larger even than those used in astronomy.\n\nScientific notation was created to handle the wide range of values that occur in scientific study. 1.0 × 10, for example, means one billion, a 1 followed by nine zeros: 1 000 000 000, and 1.0 × 10 means one billionth, or 0.000 000 001. Writing 10 instead of nine zeros saves readers the effort and hazard of counting a long series of zeros to see how large the number is.\n\nExamples of large numbers describing everyday real-world objects are:\n\nOther large numbers, as regards length and time, are found in astronomy and cosmology. For example, the current Big Bang model suggests that the universe is 13.8 billion years (4.355 × 10 seconds) old, and that the observable universe is 93 billion light years across (8.8 × 10 metres), and contains about 5 × 10 stars, organized into around 125 billion (1.25 × 10) galaxies, according to Hubble Space Telescope observations. There are about 10 atoms in the observable universe, by rough estimation.\n\nAccording to Don Page, physicist at the University of Alberta, Canada, the longest finite time that has so far been explicitly calculated by any physicist is\n\nwhich corresponds to the scale of an estimated Poincaré recurrence time for the quantum state of a hypothetical box containing a black hole with the estimated mass of the entire universe, observable or not, assuming a certain inflationary model with an inflaton whose mass is 10 Planck masses. This time assumes a statistical model subject to Poincaré recurrence. A much simplified way of thinking about this time is in a model where the universe's history repeats itself arbitrarily many times due to properties of statistical mechanics; this is the time scale when it will first be somewhat similar (for a reasonable choice of \"similar\") to its current state again.\n\nCombinatorial processes rapidly generate even larger numbers. The factorial function, which defines the number of permutations on a set of fixed objects, grows very rapidly with the number of objects. Stirling's formula gives a precise asymptotic expression for this rate of growth.\n\nCombinatorial processes generate very large numbers in statistical mechanics. These numbers are so large that they are typically only referred to using their logarithms.\n\nGödel numbers, and similar numbers used to represent bit-strings in algorithmic information theory, are very large, even for mathematical statements of reasonable length. However, some pathological numbers are even larger than the Gödel numbers of typical mathematical propositions.\n\nLogician Harvey Friedman has done work related to very large numbers, such as with Kruskal's tree theorem and the Robertson–Seymour theorem.\n\nBetween 1980 and 2000, personal computer hard disk sizes increased from about 10 megabytes (10 bytes) to over 100 gigabytes (10 bytes). A 100-gigabyte disk could store the favorite color of all of Earth's seven billion inhabitants without using data compression (storing 14 bytes times 7 billion inhabitants would equal 98 GB used). But what about a dictionary-on-disk storing all possible passwords containing up to 40 characters? Assuming each character equals one byte, there are about 2 such passwords, which is about 2 × 10. In his paper \"Computational capacity of the universe\", Seth Lloyd points out that if every particle in the universe could be used as part of a huge computer, it could store only about 10 bits, less than one millionth of the size such a dictionary would require. However, storing information on hard disk and computing it are very different functions. On the one hand storage currently has limitations as stated, but computational speed is a different matter. It is quite conceivable that the stated limitations regarding storage have no bearing on the limitations of actual computational capacity, especially if the current research into quantum computers results in a \"quantum leap\" (but see \"holographic principle\").\n\nStill, computers can easily be programmed to start creating and displaying all possible 40-character passwords one at a time. Such a program could be left to run indefinitely. Assuming a modern PC could output 1 billion strings per second, it would take one billionth of 2 × 10 seconds, or 2 × 10 seconds to complete its task, which is about 6 × 10 years. By contrast, the universe is estimated to be 13.8 billion (1.38 × 10) years old. Computers will presumably continue to get faster, but the same paper mentioned before estimates that the entire universe functioning as a giant computer could have performed no more than 10 operations since the Big Bang. This is trillions of times more computation than is required for displaying all 40-character passwords, but computing all \"50\" character passwords would outstrip the estimated computational potential of the entire universe.\n\nProblems like this grow exponentially in the number of computations they require, and they are one reason why exponentially difficult problems are called \"intractable\" in computer science: for even small numbers like the 40 or 50 characters described earlier, the number of computations required exceeds even theoretical limits on mankind's computing power. The traditional division between \"easy\" and \"hard\" problems is thus drawn between programs that do and do not require exponentially increasing resources to execute.\n\nSuch limits are an advantage in cryptography, since any cipher-breaking technique that requires more than, say, the 10 operations mentioned before will never be feasible. Such ciphers must be broken by finding efficient techniques unknown to the cipher's designer. Likewise, much of the research throughout all branches of computer science focuses on finding efficient solutions to problems that work with far fewer resources than are required by a naïve solution. For example, one way of finding the greatest common divisor between two 1000-digit numbers is to compute all their factors by trial division. This will take up to 2 × 10 division operations, far too large to contemplate. But the Euclidean algorithm, using a much more efficient technique, takes only a fraction of a second to compute the GCD for even huge numbers such as these.\n\nAs a general rule, then, PCs in 2005 can perform 2 calculations in a few minutes. A few thousand PCs working for a few years could solve a problem requiring 2 calculations, but no amount of traditional computing power will solve a problem requiring 2 operations (which is about what would be required to brute-force the encryption keys in 128-bit SSL commonly used in web browsers, assuming the underlying ciphers remain secure). Limits on computer storage are comparable. Quantum computing might allow certain problems that require an exponential amount of calculations to become feasible. However, it has practical and theoretical challenges that may never be overcome, such as the mass production of qubits, the fundamental building block of quantum computing.\n\nThe total amount of printed material in the world is roughly 1.6 × 10 bits; therefore the contents can be represented by a number somewhere in the range 0 to roughly formula_11\n\nCompare:\nThe first number is much larger than the second, due to the larger height of the power tower, and in spite of the small numbers 1.1. In comparing the magnitude of each successive exponent in the last number with formula_15, we find a difference in the magnitude of effect on the final exponent.\n\nGiven a strictly increasing integer sequence/function formula_16 (\"n\"≥1) we can produce a faster-growing sequence formula_17 (where the superscript \"n\" denotes the \"n\" functional power). This can be repeated any number of times by letting formula_18, each sequence growing much faster than the one before it. Then we could define formula_19, which grows much faster than any formula_20 for finite \"k\" (here ω is the first infinite ordinal number, representing the limit of all finite numbers k). This is the basis for the fast-growing hierarchy of functions, in which the indexing subscript is extended to ever-larger ordinals.\n\nFor example, starting with \"f\"(\"n\") = \"n\" + 1:\n\n\nA standardized way of writing very large numbers allows them to be easily sorted in increasing order, and one can get a good idea of how much larger a number is than another one.\n\nTo compare numbers in scientific notation, say 5×10 and 2×10, compare the exponents first, in this case 5 > 4, so 2×10 > 5×10. If the exponents are equal, the \"mantissa\" (or coefficient) should be compared, thus 5×10 > 2×10 because 5 > 2.\n\nTetration with base 10 gives the sequence formula_21, the power towers of numbers 10, where formula_22 denotes a functional power of the function formula_23 (the function also expressed by the suffix \"-plex\" as in googolplex, see the Googol family).\n\nThese are very round numbers, each representing an order of magnitude in a generalized sense. A crude way of specifying how large a number is, is specifying between which two numbers in this sequence it is.\n\nMore accurately, numbers in between can be expressed in the form formula_24, i.e., with a power tower of 10s and a number at the top, possibly in scientific notation, e.g. formula_25, a number between formula_26 and formula_27 (note that formula_28 if formula_29). (See also extension of tetration to real heights.)\n\nThus googolplex is formula_30\n\nAnother example:\n\nThus the \"order of magnitude\" of a number (on a larger scale than usually meant), can be characterized by the number of times (\"n\") one has to take the formula_34 to get a number between 1 and 10. Thus, the number is between formula_35 and formula_36. As explained, a more accurate description of a number also specifies the value of this number between 1 and 10, or the previous number (taking the logarithm one time less) between 10 and 10, or the next, between 0 and 1.\n\nNote that\nI.e., if a number \"x\" is too large for a representation formula_38 we can make the power tower one higher, replacing \"x\" by log\"x\", or find \"x\" from the lower-tower representation of the log of the whole number. If the power tower would contain one or more numbers different from 10, the two approaches would lead to different results, corresponding to the fact that extending the power tower with a 10 at the bottom is then not the same as extending it with a 10 at the top (but, of course, similar remarks apply if the whole power tower consists of copies of the same number, different from 10).\n\nIf the height of the tower is large, the various representations for large numbers can be applied to the height itself. If the height is given only approximately, giving a value at the top does not make sense, so we can use the double-arrow notation, e.g. formula_39. If the value after the double arrow is a very large number itself, the above can recursively be applied to that value.\n\nExamples:\n\nSimilarly to the above, if the exponent of formula_46 is not exactly given then giving a value at the right does not make sense, and we can, instead of using the power notation of formula_46, add 1 to the exponent of formula_48, so we get e.g. formula_49.\n\nIf the exponent of formula_50 is large, the various representations for large numbers can be applied to this exponent itself. If this exponent is not exactly given then, again, giving a value at the right does not make sense, and we can, instead of using the power notation of formula_50, use the triple arrow operator, e.g. formula_52.\n\nIf the right-hand argument of the triple arrow operator is large the above applies to it, so we have e.g. formula_53 (between formula_54 and formula_55). This can be done recursively, so we can have a power of the triple arrow operator.\n\nWe can proceed with operators with higher numbers of arrows, written formula_56.\n\nCompare this notation with the hyper operator and the Conway chained arrow notation:\nAn advantage of the first is that when considered as function of \"b\", there is a natural notation for powers of this function (just like when writing out the \"n\" arrows): formula_58. For example:\n\nand only in special cases the long nested chain notation is reduced; for \"b\" = 1 we get:\n\nSince the \"b\" can also be very large, in general we write a number with a sequence of powers formula_61 with decreasing values of \"n\" (with exactly given integer exponents formula_62) with at the end a number in ordinary scientific notation. Whenever a formula_62 is too large to be given exactly, the value of formula_64 is increased by 1 and everything to the right of formula_65 is rewritten.\n\nFor describing numbers approximately, deviations from the decreasing order of values of \"n\" are not needed. For example, formula_66, and formula_67. Thus we have the somewhat counterintuitive result that a number \"x\" can be so large that, in a way, \"x\" and 10 are \"almost equal\" (for arithmetic of large numbers see also below).\n\nIf the superscript of the upward arrow is large, the various representations for large numbers can be applied to this superscript itself. If this superscript is not exactly given then there is no point in raising the operator to a particular power or to adjust the value on which it acts. We can simply use a standard value at the right, say 10, and the expression reduces to formula_68 with an approximate \"n\". For such numbers the advantage of using the upward arrow notation no longer applies, and we can also use the chain notation.\n\nThe above can be applied recursively for this \"n\", so we get the notation formula_56 in the superscript of the first arrow, etc., or we have a nested chain notation, e.g.:\n\nIf the number of levels gets too large to be convenient, a notation is used where this number of levels is written down as a number (like using the superscript of the arrow instead of writing many arrows). Introducing a function formula_72 = (10 → 10 → \"n\"), these levels become functional powers of \"f\", allowing us to write a number in the form formula_73 where \"m\" is given exactly and n is an integer which may or may not be given exactly (for the example: formula_74. If \"n\" is large we can use any of the above for expressing it. The \"roundest\" of these numbers are those of the form \"f\"(1) = (10→10→\"m\"→2). For example, formula_75\n\nCompare the definition of Graham's number: it uses numbers 3 instead of 10 and has 64 arrow levels and the number 4 at the top; thus formula_76, but also formula_77.\n\nIf \"m\" in formula_73 is too large to give exactly we can use a fixed \"n\", e.g. \"n\" = 1, and apply the above recursively to \"m\", i.e., the number of levels of upward arrows is itself represented in the superscripted upward-arrow notation, etc. Using the functional power notation of \"f\" this gives multiple levels of \"f\". Introducing a function formula_79 these levels become functional powers of \"g\", allowing us to write a number in the form formula_80 where \"m\" is given exactly and n is an integer which may or may not be given exactly. We have (10→10→\"m\"→3) = \"g\"(1). If \"n\" is large we can use any of the above for expressing it. Similarly we can introduce a function \"h\", etc. If we need many such functions we can better number them instead of using a new letter every time, e.g. as a subscript, so we get numbers of the form formula_81 where \"k\" and \"m\" are given exactly and n is an integer which may or may not be given exactly. Using \"k\"=1 for the \"f\" above, \"k\"=2 for \"g\", etc., we have (10→10→\"n\"→\"k\") = formula_82. If \"n\" is large we can use any of the above for expressing it. Thus we get a nesting of forms formula_83 where going inward the \"k\" decreases, and with as inner argument a sequence of powers formula_84 with decreasing values of \"n\" (where all these numbers are exactly given integers) with at the end a number in ordinary scientific notation.\n\nWhen \"k\" is too large to be given exactly, the number concerned can be expressed as formula_85=(10→10→10→\"n\") with an approximate \"n\". Note that the process of going from the sequence formula_86=(10→\"n\") to the sequence formula_87=(10→10→\"n\") is very similar to going from the latter to the sequence formula_85=(10→10→10→\"n\"): it is the general process of adding an element 10 to the chain in the chain notation; this process can be repeated again (see also the previous section). Numbering the subsequent versions of this function a number can be described using functions formula_89, nested in lexicographical order with \"q\" the most significant number, but with decreasing order for \"q\" and for \"k\"; as inner argument we have a sequence of powers formula_84 with decreasing values of \"n\" (where all these numbers are exactly given integers) with at the end a number in ordinary scientific notation.\n\nFor a number too large to write down in the Conway chained arrow notation we can describe how large it is by the length of that chain, for example only using elements 10 in the chain; in other words, we specify its position in the sequence 10, 10→10, 10→10→10, .. If even the position in the sequence is a large number we can apply the same techniques again for that.\n\nNumbers expressible in decimal notation:\n\nNumbers expressible in scientific notation:\n\nNumbers expressible in (10 ↑) \"k\" notation:\n\nBigger numbers:\n\nThe following illustrates the effect of a base different from 10, base 100. It also illustrates representations of numbers, and the arithmetic.\n\nformula_125, with base 10 the exponent is doubled.\n\nformula_126, ditto.\n\nformula_127, the highest exponent is very little more than doubled (increased by log2).\n\n\nNote that for a number formula_146, one unit change in \"n\" changes the result by a factor 10. In a number like formula_147, with the 6.2 the result of proper rounding using significant figures, the true value of the exponent may be 50 less or 50 more. Hence the result may be a factor formula_148 too large or too small. This seems like extremely poor accuracy, but for such a large number it may be considered fair (a large error in a large number may be \"relatively small\" and therefore acceptable).\n\nIn the case of an approximation of an extremely large number, the relative error may be large, yet there may still be a sense in which we want to consider the numbers as \"close in magnitude\". For example, consider\n\nThe relative error is\n\na large relative error. However, we can also consider the relative error in the logarithms; in this case, the logarithms (to base 10) are 10 and 9, so the relative error in the logarithms is only 10%.\n\nThe point is that exponential functions magnify relative errors greatly – if \"a\" and \"b\" have a small relative error,\n\nthe relative error is larger, and\n\nwill have even larger relative error. The question then becomes: on which level of iterated logarithms do we wish to compare two numbers? There is a sense in which we may want to consider\n\nto be \"close in magnitude\". The relative error between these two numbers is large, and the relative error between their logarithms is still large; however, the relative error in their second-iterated logarithms is small:\n\nSuch comparisons of iterated logarithms are common, e.g., in analytic number theory.\n\nThere are some general rules relating to the usual arithmetic operations performed on very large numbers:\n\nHence:\n\nThe busy beaver function Σ is an example of a function which grows faster than any computable function. Its value for even relatively small input is huge. The values of Σ(\"n\") for \"n\" = 1, 2, 3, 4 are 1, 4, 6, 13 . Σ(5) is not known but is definitely ≥ 4098. Σ(6) is at least 3.5×10.\n\nAlthough all the numbers discussed above are very large, they are all still decidedly finite. Certain fields of mathematics define infinite and transfinite numbers. For example, aleph-null is the cardinality of the infinite set of natural numbers, and aleph-one is the next greatest cardinal number. formula_164 is the cardinality of the reals. The proposition that formula_165 is known as the continuum hypothesis.\n\nSome notations for extremely large numbers:\nThese notations are essentially functions of integer variables, which increase very rapidly with those integers. Ever-faster-increasing functions can easily be constructed recursively by applying these functions with large integers as argument.\n\nA function with a vertical asymptote is not helpful in defining a very large number, although the function increases very rapidly: one has to define an argument very close to the asymptote, i.e. use a very small number, and constructing that is equivalent to constructing a very large number, e.g. the reciprocal.\n"}
{"id": "4190174", "url": "https://en.wikipedia.org/wiki?curid=4190174", "title": "Limitation of size", "text": "Limitation of size\n\nIn the philosophy of mathematics, specifically the philosophical foundations of set theory, limitation of size is a concept developed by Philip Jourdain and/or Georg Cantor to avoid Cantor's paradox. It identifies certain \"inconsistent multiplicities\", in Cantor's terminology, that cannot be sets because they are \"too large\". In modern terminology these are called proper classes.\n\nThe axiom of limitation of size is an axiom in some versions of von Neumann–Bernays–Gödel set theory or Morse–Kelley set theory. This axiom says that any class which is not \"too large\" is a set, and a set cannot be \"too large\". \"Too large\" is defined as being large enough that the class of all sets can be mapped one-to-one into it.\n"}
{"id": "1707754", "url": "https://en.wikipedia.org/wiki?curid=1707754", "title": "List of NP-complete problems", "text": "List of NP-complete problems\n\nThis is a list of some of the more commonly known problems that are NP-complete when expressed as decision problems. As there are hundreds of such problems known, this list is in no way comprehensive. Many problems of this type can be found in .\n\nGraphs occur frequently in everyday applications. Examples include biological or social networks, which contain hundreds, thousands and even billions of nodes in some cases (e.g. Facebook or LinkedIn). \n\n\n\n\n\nGeneral\n\nSpecific problems\n\n"}
{"id": "18568", "url": "https://en.wikipedia.org/wiki?curid=18568", "title": "List of algorithms", "text": "List of algorithms\n\nThe following is a list of algorithms along with one-line descriptions for each.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "36423541", "url": "https://en.wikipedia.org/wiki?curid=36423541", "title": "List of definite integrals", "text": "List of definite integrals\n\nIn mathematics, the definite integral:\n\nis the area of the region in the \"xy\"-plane bounded by the graph of \"f\", the \"x\"-axis, and the lines \"x\" = \"a\" and \"x\" = \"b\", such that area above the \"x\"-axis adds to the total, and that below the \"x\"-axis subtracts from the total.\n\nThe fundamental theorem of calculus establishes the relationship between indefinite and definite integrals and introduces a technique for evaluating definite integrals.\n\nIf the interval is infinite the definite integral is called an \"improper integral\" and defined by using appropriate limiting procedures. for example:\n\nA constant, such pi, that may be defined by the integral of an algebraic function over an algebraic domain is known as a period.\n\nThe following is a list of the most common definite Integrals. For a list of indefinite integrals see \"List of indefinite integrals\"\n\nformula_73\n\nformula_74\n\nformula_75\n\nformula_76\n\nformula_77 holds if the integral exists and formula_78 is continuous.\n\n\n"}
{"id": "350167", "url": "https://en.wikipedia.org/wiki?curid=350167", "title": "List of graph theory topics", "text": "List of graph theory topics\n\nThis is a list of graph theory topics, by Wikipedia page.\n\nSee glossary of graph theory terms for basic terminology\n\n\n\n\n\n\n\"See list of network theory topics\"\n\n"}
{"id": "398810", "url": "https://en.wikipedia.org/wiki?curid=398810", "title": "List of graphical methods", "text": "List of graphical methods\n\nThis is a list of graphical methods with a mathematical basis.\nIncluded are diagram techniques, chart techniques, plot techniques, and other forms of visualization.\n\nThere is also a list of computer graphics and descriptive geometry topics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "234998", "url": "https://en.wikipedia.org/wiki?curid=234998", "title": "List of integrals of hyperbolic functions", "text": "List of integrals of hyperbolic functions\n\nThe following is a list of integrals (anti-derivative functions) of hyperbolic functions. For a complete list of integral functions, see list of integrals.\n\nIn all formulas the constant \"a\" is assumed to be nonzero, and \"C\"\ndenotes the constant of integration.\n\nformula_1\n\nformula_2\n\nformula_3\n\nformula_5\n\nformula_9\n\nformula_10\n\nformula_11\n\nformula_12\n\nformula_13\n\nformula_14\n\nformula_16\n\nformula_18\n\nformula_19\n\nformula_20\n\nformula_21\n\nformula_22\n\nformula_23\n\nformula_24\n\nformula_25\n\nformula_26\n\nformula_27\n\nformula_28\n\nformula_29\n\nformula_30\n\nformula_36\n\nformula_37\n\nformula_38\n\nformula_39\n"}
{"id": "599970", "url": "https://en.wikipedia.org/wiki?curid=599970", "title": "List of lemmas", "text": "List of lemmas\n\nThis following is a list of lemmas (or, \"lemmata\", i.e. minor theorems, or sometimes intermediate technical results factored out of proofs). See also list of axioms, list of theorems and list of conjectures.\n"}
{"id": "1168363", "url": "https://en.wikipedia.org/wiki?curid=1168363", "title": "List of mathematical knots and links", "text": "List of mathematical knots and links\n\nThis article contains a list of mathematical knots and links. See also list of knots, list of geometric topology topics.\n\n\n\n"}
{"id": "5971809", "url": "https://en.wikipedia.org/wiki?curid=5971809", "title": "List of mathematicians (H)", "text": "List of mathematicians (H)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3553599", "url": "https://en.wikipedia.org/wiki?curid=3553599", "title": "List of permutation topics", "text": "List of permutation topics\n\nThis is a list of topics on mathematical permutations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "8304842", "url": "https://en.wikipedia.org/wiki?curid=8304842", "title": "Mathematica: A World of Numbers... and Beyond", "text": "Mathematica: A World of Numbers... and Beyond\n\nMathematica: A World of Numbers… and Beyond is a kinetic and static exhibition of mathematical concepts designed by Charles and Ray Eames, originally debuted at the California Museum of Science and Industry in 1961. Duplicates have since been made, and they (as well as the original) have been moved to other institutions.\n\nIn March, 1961 a new science wing at the California Museum of Science and Industry in Los Angeles opened. The IBM Corporation had been asked by the Museum to make a contribution; IBM in turn asked the famous California designer team of Charles Eames and his wife Ray Eames to come up with a good proposal. The result was that the Eames Office was commissioned by IBM to design an interactive exhibition called \"Mathematica: A World of Numbers... and Beyond\". This was the first of many exhibitions designed by the Eames Office.\n\nThe exhibition stayed at the Museum until January 1998, making it the longest running of any corporate sponsored museum exhibition. Furthermore, it is the only one of the dozens of exhibitions designed by the Office of Charles and Ray Eames that is still extant. This original \"Mathematica\" exhibition was reassembled for display at the Alyce de Roulet Williamson Gallery at Art Center College of Design in Pasadena, California, July 30 through October 1, 2000. It is now owned by and on display at the New York Hall of Science, though it now lacks the overhead plaques with quotations from mathematicians that were part of the original installation.\n\nIn November, 1961 an exact duplicate was made for Chicago's Museum of Science and Industry, where it was shown until late 1980. From there it was sold and relocated to the Museum of Science in Boston, Massachusetts, where it is permanently on display. In January 2014, the exhibit temporarily closed to undergo a much-needed year-long refurbishment, and reopened in a new location at the Museum of Science in April 2015. The Boston installation bears the closest resemblance to the original Eames design, including numerous overhead plaques featuring historic quotations from famous mathematicians. As part of the refurbishment, a graphic panel was added to supplement the History Wall timeline to recognize the contributions of both men and women mathematicians of the late 20th and early 21st centuries. \n\nAnother copy was made for the IBM Pavilion at the 1964/1965 New York World's Fair. Subsequently, it was briefly on display in Manhattan, and was then installed in the Pacific Science Center in Seattle where it stayed until 1980. It was briefly re-installed in New York City at the 590 Madison Ave IBM Headquarters Building, before being moved to SciTrek in Atlanta, but that organization was shut down in 2004 due to funding cuts. The exhibit was then shipped to Petaluma, California to Lucia Eames, the daughter of the original designers. , the exhibit has been acquired by the Henry Ford Museum in Dearborn, Michigan.\n\nSome of the displays are minimally interactive, in that they start to operate at the push of a button. Other displays are motorized and run continuously, or operate automatically on a fixed cycle as long as power is supplied. The moving display elements combine with noise made by balls falling through the probability machine, to fill the exhibit space with an atmosphere of continuous activity.\n\n\nIn addition, large placards hang from the ceiling, carrying interesting quotations from famous mathematicians. Some installations have omitted this feature, although it was an integral part of the original exhibition.\n\nIn 1966, five years after the opening of the Mathematica Exhibit, IBM published a timeline poster, titled \"Men of Modern Mathematics\". It was based on the items displayed on the exhibit's History Wall, and free copies were distributed to schools. The timeline covered the period from 1000 AD to approximately 1950 AD, and the poster featured biographical and historical items, along with numerous pictures showing progress in various areas of science, including architecture. The mathematical items in this chart were prepared by Professor Raymond Redheffer of UCLA. Long after the chart was distributed, mathematics departments around the world have proudly displayed this chart on their walls.\n\nIn 2012, IBM Corporation released a free iPad application, \"Minds of Modern Mathematics\", based on the poster but updated to the present, including expanded coverage of women mathematicians. The app was developed by IBM with the assistance of the Eames Office. , the app has not been updated to run on current versions of iOS, which only support 64-bit code.\n\n\n"}
{"id": "18974136", "url": "https://en.wikipedia.org/wiki?curid=18974136", "title": "Mathematical beauty", "text": "Mathematical beauty\n\nMathematical beauty describes the notion that some mathematicians may derive aesthetic pleasure from their work, and from mathematics in general. They express this pleasure by describing mathematics (or, at least, some aspect of mathematics) as \"beautiful\". Mathematicians describe mathematics as an art form or, at a minimum, as a creative activity. Comparisons are often made with music and poetry.\n\nBertrand Russell expressed his sense of mathematical beauty in these words:\n\nMathematics, rightly viewed, possesses not only truth, but supreme beauty—a beauty cold and austere, like that of sculpture, without appeal to any part of our weaker nature, without the gorgeous trappings of painting or music, yet sublimely pure, and capable of a stern perfection such as only the greatest art can show. The true spirit of delight, the exaltation, the sense of being more than Man, which is the touchstone of the highest excellence, is to be found in mathematics as surely as poetry.\nPaul Erdős expressed his views on the ineffability of mathematics when he said, \"Why are numbers beautiful? It's like asking why is Beethoven's Ninth Symphony beautiful. If you don't see why, someone can't tell you. I \"know\" numbers are beautiful. If they aren't beautiful, nothing is\".\n\nMathematicians describe an especially pleasing method of proof as \"elegant\". Depending on context, this may mean:\n\n\nIn the search for an elegant proof, mathematicians often look for different independent ways to prove a result—the first proof that is found may not be the best. The theorem for which the greatest number of different proofs have been discovered is possibly the Pythagorean theorem, with hundreds of proofs having been published. Another theorem that has been proved in many different ways is the theorem of quadratic reciprocity—Carl Friedrich Gauss alone published eight different proofs of this theorem.\n\nConversely, results that are logically correct but involve laborious calculations, over-elaborate methods, very conventional approaches, or that rely on a large number of particularly powerful axioms or previous results are not usually considered to be elegant, and may be called \"ugly\" or \"clumsy\".\n\nSome mathematicians see beauty in mathematical results that establish connections between two areas of mathematics that at first sight appear to be unrelated. These results are often described as \"deep\".\n\nWhile it is difficult to find universal agreement on whether a result is deep, some examples are often cited. One is Euler's identity:\n\nformula_1\n\nThis is a special case of Euler's formula, which the physicist Richard Feynman called \"our jewel\" and \"the most remarkable formula in mathematics\". Modern examples include the modularity theorem, which establishes an important connection between elliptic curves and modular forms (work on which led to the awarding of the Wolf Prize to Andrew Wiles and Robert Langlands), and \"monstrous moonshine\", which connects the Monster group to modular functions via string theory for which Richard Borcherds was awarded the Fields Medal.\n\nOther examples of deep results include unexpected insights into mathematical structures. For example, Gauss's Theorema Egregium is a deep theorem which relates a local phenomenon (curvature) to a global phenomenon (area) in a surprising way. In particular, the area of a triangle on a curved surface is proportional to the excess of the triangle and the proportionality is curvature. Another example is the fundamental theorem of calculus (and its vector versions including Green's theorem and Stokes' theorem).\n\nThe opposite of \"deep\" is \"trivial\". A trivial theorem may be a result that can be derived in an obvious and straightforward way from other known results, or which applies only to a specific set of particular objects such as the empty set. Sometimes, however, a statement of a theorem can be original enough to be considered deep, even though its proof is fairly obvious.\n\nIn his \"A Mathematician's Apology\", Hardy suggests that a beautiful proof or result possesses \"inevitability\", \"unexpectedness\", and \"economy\".\n\nRota, however, disagrees with unexpectedness as a sufficient condition for beauty and proposes a counterexample:\n\nPerhaps ironically, Monastyrsky writes:\n\nThis disagreement illustrates both the subjective nature of mathematical beauty and its connection with mathematical results: in this case, not only the existence of exotic spheres, but also a particular realization of them.\n\nInterest in pure mathematics separate from empirical study has been part of the experience of various civilizations, including that of the ancient Greeks, who \"did mathematics for the beauty of it\". The aesthetic pleasure that mathematical physicists tend to experience in Einstein's theory of general relativity has been attributed (by Paul Dirac, among others) to its \"great mathematical beauty\". The beauty of mathematics is experienced when the physical reality of objects are represented by mathematical models. Group theory, developed in the early 1800s for the sole purpose of solving polynomial equations, became a fruitful way of categorizing elementary particles—the building blocks of matter. Similarly, the study of knots provides important insights into string theory and loop quantum gravity.\n\nSome believe that in order to appreciate mathematics, one must engage in doing mathematics.\nThere are some teachers that encourage student engagement by teaching mathematics in a kinesthetic way (see kinesthetic learning). For example, Math Circle is an afterschool enrichment program where students do mathematics through games and activities; in a general Math Circle lesson, students use pattern finding, observation, and exploration to make their own mathematical discoveries. For example, mathematical beauty arises in a Math Circle activity on symmetry designed for 2nd and 3rd graders. In this activity, students create their own snowflakes by folding a square piece of paper and cutting out designs of their choice along the edges of the folded paper. When the paper is unfolded, a symmetrical design reveals itself. In a day to day elementary school mathematics class, symmetry can be presented as such in an artistic manner where students see aesthetically pleasing results in mathematics.\n\nSome teachers prefer to use mathematical manipulatives to present mathematics in an aesthetically pleasing way. Examples of a manipulative include algebra tiles, cuisenaire rods, and pattern blocks. For example, one can teach the method of completing the square by using algebra tiles. Cuisenaire rods can be used to teach fractions, and pattern blocks can be used to teach geometry. Using mathematical manipulatives helps students gain a conceptual understanding that might not be seen immediately in written mathematical formulas. \n\nAnother example involves origami. Origami, the art of paper folding, has aesthetic qualities and many mathematical connections. One can study the mathematics of paper folding by observing the crease pattern on unfolded origami pieces.\n\nCombinatorics (the study of counting) has artistic representations that some find mathematically beautiful. There are many visual examples that illustrate combinatorial concepts. Here are some topics and objects seen in combinatorics courses with visual representations:\n\nSome mathematicians are of the opinion that the doing of mathematics is closer to discovery than invention, for example:\n\nThese mathematicians believe that the detailed and precise results of mathematics may be reasonably taken to be true without any dependence on the universe in which we live. For example, they would argue that the theory of the natural numbers is fundamentally valid, in a way that does not require any specific context. Some mathematicians have extrapolated this viewpoint that mathematical beauty is truth further, in some cases becoming mysticism.\n\nPythagorean mathematicians believed in the literal reality of numbers. The discovery of the existence of irrational numbers was a shock to them, since they considered the existence of numbers not expressible as the ratio of two natural numbers to be a flaw in nature (the Pythagorean world view did not contemplate the limits of infinite sequences of ratios of natural numbers—the modern notion of a real number). From a modern perspective, their mystical approach to numbers may be viewed as numerology.\n\nIn Plato's philosophy there were two worlds, the physical one in which we live and another abstract world which contained unchanging truth, including mathematics. He believed that the physical world was a mere reflection of the more perfect abstract world.\n\nHungarian mathematician Paul Erdős spoke of an imaginary book, in which God has written down all the most beautiful mathematical proofs. When Erdős wanted to express particular appreciation of a proof, he would exclaim \"This one's from The Book!\"\n\nTwentieth-century French philosopher Alain Badiou claims that ontology is mathematics. Badiou also believes in deep connections between mathematics, poetry and philosophy.\n\nIn some cases, natural philosophers and other scientists who have made extensive use of mathematics have made leaps of inference between beauty and physical truth in ways that turned out to be erroneous. For example, at one stage in his life, Johannes Kepler believed that the proportions of the orbits of the then-known planets in the Solar System have been arranged by God to correspond to a concentric arrangement of the five Platonic solids, each orbit lying on the circumsphere of one polyhedron and the insphere of another. As there are exactly five Platonic solids, Kepler's hypothesis could only accommodate six planetary orbits and was disproved by the subsequent discovery of Uranus.\n\nIn the 1970s, Abraham Moles and Frieder Nake analyzed links between beauty, information processing, and information theory. In the 1990s, Jürgen Schmidhuber formulated a mathematical theory of observer-dependent subjective beauty based on algorithmic information theory: the most beautiful objects among subjectively comparable objects have short algorithmic descriptions (i.e., Kolmogorov complexity) relative to what the observer already knows. Schmidhuber explicitly distinguishes between beautiful and interesting. The latter corresponds to the first derivative of subjectively perceived beauty:\nthe observer continually tries to improve the predictability and compressibility of the observations by discovering regularities such as repetitions and symmetries and fractal self-similarity. Whenever the observer's learning process (possibly a predictive artificial neural network) leads to improved data compression such that the observation sequence can be described by fewer bits than before, the temporary interestingness of the data corresponds to the compression progress, and is proportional to the observer's internal curiosity reward.\n\nExamples of the use of mathematics in music include the stochastic music of Iannis Xenakis, Fibonacci in Tool's Lateralus, counterpoint of Johann Sebastian Bach, polyrhythmic structures (as in Igor Stravinsky's \"The Rite of Spring\"), the Metric modulation of Elliott Carter, permutation theory in serialism beginning with Arnold Schoenberg, and application of Shepard tones in Karlheinz Stockhausen's \"Hymnen\".\n\nExamples of the use of mathematics in the visual arts include applications of chaos theory and fractal geometry to computer-generated art, symmetry studies of Leonardo da Vinci, projective geometries in development of the perspective theory of Renaissance art, grids in Op art, optical geometry in the camera obscura of Giambattista della Porta, and multiple perspective in analytic cubism and futurism.\n\nThe Dutch graphic designer M. C. Escher created mathematically inspired woodcuts, lithographs, and mezzotints. These feature impossible constructions, explorations of infinity, architecture, visual paradoxes and tessellations. British constructionist artist John Ernest created reliefs and paintings inspired by group theory. A number of other British artists of the constructionist and systems schools also draw on mathematics models and structures as a source of inspiration, including Anthony Hill and Peter Lowe. Computer-generated art is based on mathematical algorithms.\n\n\n"}
{"id": "168905", "url": "https://en.wikipedia.org/wiki?curid=168905", "title": "Mathematical folklore", "text": "Mathematical folklore\n\nAs the term is understood by mathematicians, folk mathematics or mathematical folklore is the body of theorems, definitions, proofs, or mathematical facts or techniques that circulate among mathematicians by word of mouth but have not appeared in print, either in books or in scholarly journals. Knowledge of folklore is the coin of the realm of academic mathematics.\n\nQuite important at times for researchers are folk theorems, which are results known, at least to experts in a field, and considered to have established status, but not published in complete form. Sometimes these are only alluded to in the public literature. \nAn example is a book of exercises, described on the back cover:\nAnother distinct category is wellknowable mathematics, a term introduced by John Conway. This consists of matters that are known and factual, but not in active circulation in relation with current research. Both of these concepts are attempts to describe the actual context in which research work is done.\n\nSome people, principally non-mathematicians, use the term \"folk mathematics\" to refer to the informal mathematics studied in many ethno-cultural studies of mathematics.\n\nMathematical folklore may also refer to unusual (and possibly apocryphal) stories or jokes involving mathematicians or mathematics that are told verbally in mathematics departments. Compilations include tales collected in G. H. Hardy's \"A Mathematician's Apology\" and ; examples include:\n\n"}
{"id": "26722913", "url": "https://en.wikipedia.org/wiki?curid=26722913", "title": "Mathematical instrument", "text": "Mathematical instrument\n\nA mathematical instrument is a tool or device used in the study or practice of mathematics. In geometry, construction of various proofs was done using only a compass and straightedge; arguments in these proofs relied only on idealized properties of these instruments and literal construction was regarded as only an approximation. In applied mathematics, mathematical instruments were used for measuring angles and distances, in astronomy, navigation, surveying and in the measurement of time.\n\nInstruments such as the astrolabe, the quadrant, and others were used to measure and accurately record the relative positions and movements of planets and other celestial objects. The sextant and other related instruments were essential for navigation at sea. \n\nMost instruments are used within the field of geometry, including the ruler, dividers, protractor, set square, compass, ellipsograph, T-square and opisometer. Others are used in arithmetic (for example the abacus, slide rule and calculator) or in algebra (the integraph). In astronomy, many have said the pyramids (along with Stonehenge) were actually instruments used for tracking the stars over long periods or for the annual planting seasons.\n\nThe Oxford Set of Mathematical Instruments is a set of instruments used by generations of school children in the United Kingdom and around the world in mathematics and geometry lessons. It includes two set squares, a 180° protractor, a 15 cm ruler, a metal compass, a 9 cm pencil, a pencil sharpener, an eraser and a 10mm stencil.\n\n\n"}
{"id": "26146516", "url": "https://en.wikipedia.org/wiki?curid=26146516", "title": "Mathematical knowledge management", "text": "Mathematical knowledge management\n\nMathematical knowledge management (MKM) is the study of how society can effectively make use of the vast and growing literature on mathematics. It studies approaches such as databases of mathematical knowledge, automated processing of formulae and the use of semantic information, and artificial intelligence. Mathematics is particularly suited to a systematic study of automated knowledge processing due to the high degree of interconnectedness between different areas of mathematics.\n\n\n"}
{"id": "159730", "url": "https://en.wikipedia.org/wiki?curid=159730", "title": "Mathematical practice", "text": "Mathematical practice\n\nMathematical practice comprises the working practices of professional mathematicians: selecting theorems to prove, using informal notations to persuade themselves and others that various steps in the final proof are convincing, and seeking peer review and publication, as opposed to the end result of proven and published theorems.\n\nPhilip Kitcher has proposed a more formal definition of a mathematical practice, as a quintuple. His intention was primarily to document mathematical practice through its historical changes.\n\nThe evolution of mathematical practice was slow, and some contributors to modern mathematics did not follow even the practice of their time. For example, Pierre de Fermat was infamous for withholding his proofs, but nonetheless had a vast reputation for correct assertions of results.\n\nOne motivation to study mathematical practice is that, despite much work in the 20th century, some still feel that the foundations of mathematics remain unclear and ambiguous. One proposed remedy is to shift focus to some degree onto 'what is meant by a proof', and other such questions of method.\n\nIf mathematics has been informally used throughout history, in numerous cultures and continents, then it could be argued that \"mathematical practice\" is the practice, or use, of mathematics in everyday life. One definition of mathematical practice, as described above, is the \"working practices of professional mathematicians.\" However, another definition, more in keeping with the predominant usage of mathematics, is that mathematical practice is the everyday practice, or use, of math. Whether one is estimating the total cost of their groceries, calculating miles per gallon, or figuring out how many minutes on the treadmill that chocolate éclair will require, math as used by most people relies less on proof than on practicality (i. e., does it answer the question?).\n\nMathematical teaching usually requires the use of several important teaching pedagogies or components. Most GCSE, A-Level and undergraduate mathematics require the following components: \n\n\n"}
{"id": "47304362", "url": "https://en.wikipedia.org/wiki?curid=47304362", "title": "Mathematical sculpture", "text": "Mathematical sculpture\n\nA mathematical sculpture is a sculpture which uses mathematics as an essential conception. Helaman Ferguson, George W. Hart, Bathsheba Grossman, Peter Forakis and Jacobus Verhoeff are well-known mathematical sculptors.\n"}
{"id": "18831", "url": "https://en.wikipedia.org/wiki?curid=18831", "title": "Mathematics", "text": "Mathematics\n\nMathematics (from Greek μάθημα \"máthēma\", \"knowledge, study, learning\") includes the study of such topics as quantity, structure, space, and change.\n\nMathematicians seek and use patterns to formulate new conjectures; they resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.\n\nRigorous arguments first appeared in Greek mathematics, most notably in Euclid's \"Elements\". Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.\n\nMathematics is essential in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians engage in pure mathematics, or mathematics for its own sake, without having any application in mind. Practical applications for what began as pure mathematics are often discovered.\n\nThe history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals, was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.\n\nAs evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years.\n\nEvidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy. The most ancient mathematical texts from Mesopotamia and Egypt are from 2000–1800 BC. Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication and division) first appear in the archaeological record. The Babylonians also possessed a place-value system, and used a sexagesimal numeral system, still in use today for measuring angles and time.\n\nBeginning in the 6th century BC with the Pythagoreans, the Ancient Greeks began a systematic study of mathematics as a subject in its own right with Greek mathematics. Around 300 BC, Euclid introduced the axiomatic method still used in mathematics today, consisting of definition, axiom, theorem, and proof. His textbook \"Elements\" is widely considered the most successful and influential textbook of all time. The greatest mathematician of antiquity is often held to be Archimedes (c. 287–212 BC) of Syracuse. He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC), trigonometry (Hipparchus of Nicaea (2nd century BC), and the beginnings of algebra (Diophantus, 3rd century AD).\n\nThe Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics. Other notable developments of Indian mathematics include the modern definition of sine and cosine, and an early form of infinite series.\nDuring the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other notable achievements of the Islamic period are advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system. Many notable mathematicians from this period were Persian, such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī. \n\nDuring the early modern period, mathematics began to develop at an accelerating pace in Western Europe. The development of calculus by Newton and Leibniz in the 17th century revolutionized mathematics. Leonhard Euler was the most notable mathematician of the 18th century, contributing numerous theorems and discoveries. Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Friedrich Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory,number theory, and statistics. In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show that any axiomatic system that is consistent will contain unprovable propositions.\n\nMathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the \"Bulletin of the American Mathematical Society\", \"The number of papers and books included in the \"Mathematical Reviews\" database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs.\"\n\nThe word \"mathematics\" comes from Ancient Greek μάθημα (\"máthēma\"), meaning \"that which is learnt\", \"what one gets to know\", hence also \"study\" and \"science\". The word for \"mathematics\" came to have the narrower and more technical meaning \"mathematical study\" even in Classical times. Its adjective is (\"mathēmatikós\"), meaning \"related to learning\" or \"studious\", which likewise further came to mean \"mathematical\". In particular, (\"mathēmatikḗ tékhnē\"), , meant \"the mathematical art\".\n\nSimilarly, one of the two main schools of thought in Pythagoreanism was known as the \"mathēmatikoi\" (μαθηματικοί)—which at the time meant \"teachers\" rather than \"mathematicians\" in the modern sense.\n\nIn Latin, and in English until around 1700, the term \"mathematics\" more commonly meant \"astrology\" (or sometimes \"astronomy\") rather than \"mathematics\"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine's warning that Christians should beware of \"mathematici\", meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.\n\nThe apparent plural form in English, like the French plural form (and the less commonly used singular derivative ), goes back to the Latin neuter plural (Cicero), based on the Greek plural (\"ta mathēmatiká\"), used by Aristotle (384–322 BC), and meaning roughly \"all things mathematical\"; although it is plausible that English borrowed only the adjective \"mathematic(al)\" and formed the noun \"mathematics\" anew, after the pattern of \"physics\" and \"metaphysics\", which were inherited from Greek. In English, the noun \"mathematics\" takes a singular verb. It is often shortened to \"maths\" or, in North America, \"math\".\n\nMathematics has no generally accepted definition. Aristotle defined mathematics as \"the science of quantity\", and this definition prevailed until the 18th century. Galileo Galilei (1564–1642) said, \"The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth.\" Carl Friedrich Gauss (1777–1855) referred to mathematics as \"the Queen of the Sciences\". Benjamin Peirce (1809–1880) called mathematics \"the science that draws necessary conclusions\". David Hilbert said of mathematics: \"We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise.\" Albert Einstein (1879–1955) stated that \"as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.\"\n\nStarting in the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions. Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals. There is not even consensus on whether mathematics is an art or a science. A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. Some just say, \"Mathematics is what mathematicians do.\"\n\nThree leading types of definition of mathematics are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought. All have severe problems, none has widespread acceptance, and no reconciliation seems possible.\n\nAn early definition of mathematics in terms of logic was Benjamin Peirce's \"the science that draws necessary conclusions\" (1870). In the \"Principia Mathematica\", Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's \"All Mathematics is Symbolic Logic\" (1903).\n\nIntuitionist definitions, developing from the philosophy of mathematician L. E. J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is \"Mathematics is the mental activity which consists in carrying out constructs one after the other.\" A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct.\n\nFormalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as \"the science of formal systems\". A formal system is a set of symbols, or \"tokens\", and some \"rules\" telling how the tokens may be combined into \"formulas\". In formal systems, the word \"axiom\" has a special meaning, different from the ordinary meaning of \"a self-evident truth\". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.\n\nThe German mathematician Carl Friedrich Gauss referred to mathematics as \"the Queen of the Sciences\". More recently, Marcus du Sautoy has called mathematics \"the Queen of Science ... the main driving force behind scientific discovery\". In the original Latin \"Regina Scientiarum\", as well as in German \"Königin der Wissenschaften\", the word corresponding to \"science\" means a \"field of knowledge\", and this was the original meaning of \"science\" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of \"science\" to \"natural science\" follows the rise of Baconian science, which contrasted \"natural science\" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as biology, chemistry, or physics. Albert Einstein stated that \"as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.\"\n\nMany philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper. However, in the 1930s Gödel's incompleteness theorems convinced many mathematicians that mathematics cannot be reduced to logic alone, and Karl Popper concluded that \"most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently.\" Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself.\n\nAn alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.\n\nThe opinions of mathematicians on this matter are varied. Many mathematicians feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is \"created\" (as in art) or \"discovered\" (as in science). It is common to see universities divided into sections that include a division of \"Science and Mathematics\", indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.\n\nMathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.\n\nSome mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the \"purest\" mathematics often turns out to have practical applications, is what Eugene Wigner has called \"the unreasonable effectiveness of mathematics\". As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages. Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.\n\nFor those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the \"elegance\" of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G. H. Hardy in \"A Mathematician's Apology\" expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic. Mathematicians often strive to find proofs that are particularly elegant, proofs from \"The Book\" of God according to Paul Erdős. The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.\n\nMost of the mathematical notation in use today was not invented until the 16th century. Before that, mathematics was written out in words, limiting mathematical discovery. Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more \"abstract\" and more \"encrypted\" than those of natural language. Unlike natural language, where people can often equate a word (such as \"cow\") with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog. Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.\n\nMathematical language can be difficult to understand for beginners because even common terms, such as \"or\" and \"only\", have a more precise meaning than they have in everyday speech, and other terms such as \"open\" and \"field\" refer to specific mathematical ideas, not covered by their laymen's meanings. Mathematical language also includes many technical terms such as \"homeomorphism\" and \"integrable\" that have no meaning outside of mathematics. Additionally, shorthand phrases such as \"iff\" for \"if and only if\" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as \"rigor\".\n\nMathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken \"theorems\", based on fallible intuitions, of which many instances have occurred in the history of the subject. The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous.\n\nAxioms in traditional thought were \"self-evident truths\", but that conception is problematic. At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.\n\nMathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.\n\nIn order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase \"crisis of foundations\" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930. Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.\n\nMathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if \"sound\" (meaning that all theorems that can be proved are true), is necessarily \"incomplete\" (meaning that there are true theorems which cannot be proved \"in that system\"). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science, as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem.\n\nTheoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the \"\" problem, one of the Millennium Prize Problems. Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.\n\nThe study of quantity starts with numbers, first the familiar natural numbers and integers (\"whole numbers\") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.\n\nAs the number system is further developed, the integers are recognized as a subset of the rational numbers (\"fractions\"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of \"infinity\". According to the fundamental theorem of algebra all solutions of equations in one unknown with complex coefficients are complex numbers, regardless of degree. Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.\n\nMany mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.\n\nBy its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.\n\nThe study of space originates with geometry – in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.\n\nUnderstanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.\n\nApplied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, \"applied mathematics\" is a mathematical science with specialized knowledge. The term \"applied mathematics\" also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, \"applied mathematics\" focuses on the \"formulation, study, and use of mathematical models\" in science, engineering, and other areas of mathematical practice.\n\nIn the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.\n\nApplied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) \"create data that makes sense\" with random sampling and with randomized experiments; the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians \"make sense of the data\" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data.\n\nStatistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence. Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.\n\nComputational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.\n\nArguably the most prestigious award in mathematics is the Fields Medal, established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.\n\nThe Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.\n\nA famous list of 23 open problems, called \"Hilbert's problems\", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the \"Millennium Prize Problems\", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a $1 million reward.\n\n\n"}
{"id": "46439", "url": "https://en.wikipedia.org/wiki?curid=46439", "title": "Philosophy of mathematics", "text": "Philosophy of mathematics\n\nThe philosophy of mathematics is the branch of philosophy that studies the assumptions, foundations, and implications of mathematics, and purports to provide a viewpoint of the nature and methodology of mathematics, and to understand the place of mathematics in people's lives. The logical and structural nature of mathematics itself makes this study both broad and unique among its philosophical counterparts.\n\nRecurrent themes include:\n\nThe origin of mathematics is subject to argument. Whether the birth of mathematics was a random happening or induced by necessity duly contingent upon other subjects, say for example physics, is still a matter of prolific debates.\n\nMany thinkers have contributed their ideas concerning the nature of mathematics. Today, some philosophers of mathematics aim to give accounts of this form of inquiry and its products as they stand, while others emphasize a role for themselves that goes beyond simple interpretation to critical analysis. There are traditions of mathematical philosophy in both Western philosophy and Eastern philosophy. Western philosophies of mathematics go as far back as Pythagoras, who described the theory \"everything is mathematics\" (mathematicism), Plato, who paraphrased Pythagoras, and studied the ontological status of mathematical objects, and Aristotle, who studied logic and issues related to infinity (actual versus potential).\n\nGreek philosophy on mathematics was strongly influenced by their study of geometry. For example, at one time, the Greeks held the opinion that 1 (one) was not a number, but rather a unit of arbitrary length. A number was defined as a multitude. Therefore, 3, for example, represented a certain multitude of units, and was thus not \"truly\" a number. At another point, a similar argument was made that 2 was not a number but a fundamental notion of a pair. These views come from the heavily geometric straight-edge-and-compass viewpoint of the Greeks: just as lines drawn in a geometric problem are measured in proportion to the first arbitrarily drawn line, so too are the numbers on a number line measured in proportion to the arbitrary first \"number\" or \"one\".\n\nThese earlier Greek ideas of numbers were later upended by the discovery of the irrationality of the square root of two. Hippasus, a disciple of Pythagoras, showed that the diagonal of a unit square was incommensurable with its (unit-length) edge: in other words he proved there was no existing (rational) number that accurately depicts the proportion of the diagonal of the unit square to its edge. This caused a significant re-evaluation of Greek philosophy of mathematics. According to legend, fellow Pythagoreans were so traumatized by this discovery that they murdered Hippasus to stop him from spreading his heretical idea. Simon Stevin was one of the first in Europe to challenge Greek ideas in the 16th century. Beginning with Leibniz, the focus shifted strongly to the relationship between mathematics and logic. This perspective dominated the philosophy of mathematics through the time of Frege and of Russell, but was brought into question by developments in the late 19th and early 20th centuries.\n\nA perennial issue in the philosophy of mathematics concerns the relationship between logic and mathematics at their joint foundations. While 20th-century philosophers continued to ask the questions mentioned at the outset of this article, the philosophy of mathematics in the 20th century was characterized by a predominant interest in formal logic, set theory, and foundational issues.\n\nIt is a profound puzzle that on the one hand mathematical truths seem to have a compelling inevitability, but on the other hand the source of their \"truthfulness\" remains elusive. Investigations into this issue are known as the foundations of mathematics program.\n\nAt the start of the 20th century, philosophers of mathematics were already beginning to divide into various schools of thought about all these questions, broadly distinguished by their pictures of mathematical epistemology and ontology. Three schools, formalism, intuitionism, and logicism, emerged at this time, partly in response to the increasingly widespread worry that mathematics as it stood, and analysis in particular, did not live up to the standards of certainty and rigor that had been taken for granted. Each school addressed the issues that came to the fore at that time, either attempting to resolve them or claiming that mathematics is not entitled to its status as our most trusted knowledge.\n\nSurprising and counter-intuitive developments in formal logic and set theory early in the 20th century led to new questions concerning what was traditionally called the \"foundations of mathematics\". As the century unfolded, the initial focus of concern expanded to an open exploration of the fundamental axioms of mathematics, the axiomatic approach having been taken for granted since the time of Euclid around 300 BCE as the natural basis for mathematics. Notions of axiom, proposition and proof, as well as the notion of a proposition being true of a mathematical object (see Assignment (mathematical logic)), were formalized, allowing them to be treated mathematically. The Zermelo–Fraenkel axioms for set theory were formulated which provided a conceptual framework in which much mathematical discourse would be interpreted. In mathematics, as in physics, new and unexpected ideas had arisen and significant changes were coming. With Gödel numbering, propositions could be interpreted as referring to themselves or other propositions, enabling inquiry into the consistency of mathematical theories. This reflective critique in which the theory under review \"becomes itself the object of a mathematical study\" led Hilbert to call such study \"metamathematics\" or \"proof theory\".\n\nAt the middle of the century, a new mathematical theory was created by Samuel Eilenberg and Saunders Mac Lane, known as category theory, and it became a new contender for the natural language of mathematical thinking. As the 20th century progressed, however, philosophical opinions diverged as to just how well-founded were the questions about foundations that were raised at the century's beginning. Hilary Putnam summed up one common view of the situation in the last third of the century by saying:\n\nWhen philosophy discovers something wrong with science, sometimes science has to be changed—Russell's paradox comes to mind, as does Berkeley's attack on the actual infinitesimal—but more often it is philosophy that has to be changed. I do not think that the difficulties that philosophy finds with classical mathematics today are genuine difficulties; and I think that the philosophical interpretations of mathematics that we are being offered on every hand are wrong, and that \"philosophical interpretation\" is just what mathematics doesn't need.\nPhilosophy of mathematics today proceeds along several different lines of inquiry, by philosophers of mathematics, logicians, and mathematicians, and there are many schools of thought on the subject. The schools are addressed separately in the next section, and their assumptions explained.\n\nMathematical realism, like realism in general, holds that mathematical entities exist independently of the human mind. Thus humans do not invent mathematics, but rather discover it, and any other intelligent beings in the universe would presumably do the same. In this point of view, there is really one sort of mathematics that can be discovered; triangles, for example, are real entities, not the creations of the human mind.\n\nMany working mathematicians have been mathematical realists; they see themselves as discoverers of naturally occurring objects. Examples include Paul Erdős and Kurt Gödel. Gödel believed in an objective mathematical reality that could be perceived in a manner analogous to sense perception. Certain principles (e.g., for any two objects, there is a collection of objects consisting of precisely those two objects) could be directly seen to be true, but the continuum hypothesis conjecture might prove undecidable just on the basis of such principles. Gödel suggested that quasi-empirical methodology could be used to provide sufficient evidence to be able to reasonably assume such a conjecture.\n\nWithin realism, there are distinctions depending on what sort of existence one takes mathematical entities to have, and how we know about them. Major forms of mathematical realism include Platonism.\n\nMathematical anti-realism generally holds that mathematical statements have truth-values, but that they do not do so by corresponding to a special realm of immaterial or non-empirical entities. Major forms of mathematical anti-realism include formalism and fictionalism.\n\nMathematical Platonism is the form of realism that suggests that mathematical entities are abstract, have no spatiotemporal or causal properties, and are eternal and unchanging. This is often claimed to be the view most people have of numbers. The term \"Platonism\" is used because such a view is seen to parallel Plato's Theory of Forms and a \"World of Ideas\" (Greek: \"eidos\" (εἶδος)) described in Plato's allegory of the cave: the everyday world can only imperfectly approximate an unchanging, ultimate reality. Both Plato's cave and Platonism have meaningful, not just superficial connections, because Plato's ideas were preceded and probably influenced by the hugely popular \"Pythagoreans\" of ancient Greece, who believed that the world was, quite literally, generated by numbers.\n\nA major question considered in mathematical Platonism is: Precisely where and how do the mathematical entities exist, and how do we know about them? Is there a world, completely separate from our physical one, that is occupied by the mathematical entities? How can we gain access to this separate world and discover truths about the entities? One proposed answer is the Ultimate Ensemble, a theory that postulates that all structures that exist mathematically also exist physically in their own universe.\n\nKurt Gödel's Platonism postulates a special kind of mathematical intuition that lets us perceive mathematical objects directly. (This view bears resemblances to many things Husserl said about mathematics, and supports Kant's idea that mathematics is synthetic \"a priori\".) Davis and Hersh have suggested in their 1999 book \"The Mathematical Experience\" that most mathematicians act as though they are Platonists, even though, if pressed to defend the position carefully, they may retreat to formalism.\n\nFull-blooded Platonism is a modern variation of Platonism, which is in reaction to the fact that different sets of mathematical entities can be proven to exist depending on the axioms and inference rules employed (for instance, the law of the excluded middle, and the axiom of choice). It holds that all mathematical entities exist, however they may be provable, even if they cannot all be derived from a single consistent set of axioms.\n\nSet-theoretic realism (also set-theoretic Platonism) a position defended by Penelope Maddy, is the view that set theory is about a single universe of sets. This position (which is also known as naturalized Platonism because it is a naturalized version of mathematical Platonism) has been criticized by Mark Balaguer on the basis of Paul Benacerraf's epistemological problem. A similar view, termed Platonized naturalism, was later defended by the Stanford–Edmonton School: according to this view, a more traditional kind of Platonism is consistent with naturalism; the more traditional kind of Platonism they defend is distinguished by general principles that assert the existence of abstract objects.\n\nMax Tegmark's mathematical universe hypothesis (or mathematicism) goes further than Platonism in asserting that not only do all mathematical objects exist, but nothing else does. Tegmark's sole postulate is: \"All structures that exist mathematically also exist physically\". That is, in the sense that \"in those [worlds] complex enough to contain self-aware substructures [they] will subjectively perceive themselves as existing in a physically 'real' world\".\n\nLogicism is the thesis that mathematics is reducible to logic, and hence nothing but a part of logic. Logicists hold that mathematics can be known \"a priori\", but suggest that our knowledge of mathematics is just part of our knowledge of logic in general, and is thus analytic, not requiring any special faculty of mathematical intuition. In this view, logic is the proper foundation of mathematics, and all mathematical statements are necessary logical truths.\n\nRudolf Carnap (1931) presents the logicist thesis in two parts:\n\nGottlob Frege was the founder of logicism. In his seminal \"Die Grundgesetze der Arithmetik\" (\"Basic Laws of Arithmetic\") he built up arithmetic from a system of logic with a general principle of comprehension, which he called \"Basic Law V\" (for concepts \"F\" and \"G\", the extension of \"F\" equals the extension of \"G\" if and only if for all objects \"a\", \"Fa\" equals \"Ga\"), a principle that he took to be acceptable as part of logic.\n\nFrege's construction was flawed. Russell discovered that Basic Law V is inconsistent (this is Russell's paradox). Frege abandoned his logicist program soon after this, but it was continued by Russell and Whitehead. They attributed the paradox to \"vicious circularity\" and built up what they called ramified type theory to deal with it. In this system, they were eventually able to build up much of modern mathematics but in an altered, and excessively complex form (for example, there were different natural numbers in each type, and there were infinitely many types). They also had to make several compromises in order to develop so much of mathematics, such as an \"axiom of reducibility\". Even Russell said that this axiom did not really belong to logic.\n\nModern logicists (like Bob Hale, Crispin Wright, and perhaps others) have returned to a program closer to Frege's. They have abandoned Basic Law V in favor of abstraction principles such as Hume's principle (the number of objects falling under the concept \"F\" equals the number of objects falling under the concept \"G\" if and only if the extension of \"F\" and the extension of \"G\" can be put into one-to-one correspondence). Frege required Basic Law V to be able to give an explicit definition of the numbers, but all the properties of numbers can be derived from Hume's principle. This would not have been enough for Frege because (to paraphrase him) it does not exclude the possibility that the number 3 is in fact Julius Caesar. In addition, many of the weakened principles that they have had to adopt to replace Basic Law V no longer seem so obviously analytic, and thus purely logical.\n\nFormalism holds that mathematical statements may be thought of as statements about the consequences of certain string manipulation rules. For example, in the \"game\" of Euclidean geometry (which is seen as consisting of some strings called \"axioms\", and some \"rules of inference\" to generate new strings from given ones), one can prove that the Pythagorean theorem holds (that is, one can generate the string corresponding to the Pythagorean theorem). According to formalism, mathematical truths are not about numbers and sets and triangles and the like—in fact, they are not \"about\" anything at all.\n\nAnother version of formalism is often known as deductivism. In deductivism, the Pythagorean theorem is not an absolute truth, but a relative one: \"if\" one assigns meaning to the strings in such a way that the rules of the game become true (i.e., true statements are assigned to the axioms and the rules of inference are truth-preserving), \"then\" one must accept the theorem, or, rather, the interpretation one has given it must be a true statement. The same is held to be true for all other mathematical statements. Thus, formalism need not mean that mathematics is nothing more than a meaningless symbolic game. It is usually hoped that there exists some interpretation in which the rules of the game hold. (Compare this position to structuralism.) But it does allow the working mathematician to continue in his or her work and leave such problems to the philosopher or scientist. Many formalists would say that in practice, the axiom systems to be studied will be suggested by the demands of science or other areas of mathematics.\nA major early proponent of formalism was David Hilbert, whose program was intended to be a complete and consistent axiomatization of all of mathematics. Hilbert aimed to show the consistency of mathematical systems from the assumption that the \"finitary arithmetic\" (a subsystem of the usual arithmetic of the positive integers, chosen to be philosophically uncontroversial) was consistent. Hilbert's goals of creating a system of mathematics that is both complete and consistent were seriously undermined by the second of Gödel's incompleteness theorems, which states that sufficiently expressive consistent axiom systems can never prove their own consistency. Since any such axiom system would contain the finitary arithmetic as a subsystem, Gödel's theorem implied that it would be impossible to prove the system's consistency relative to that (since it would then prove its own consistency, which Gödel had shown was impossible). Thus, in order to show that any axiomatic system of mathematics is in fact consistent, one needs to first assume the consistency of a system of mathematics that is in a sense stronger than the system to be proven consistent.\n\nHilbert was initially a deductivist, but, as may be clear from above, he considered certain metamathematical methods to yield intrinsically meaningful results and was a realist with respect to the finitary arithmetic. Later, he held the opinion that there was no other meaningful mathematics whatsoever, regardless of interpretation.\n\nOther formalists, such as Rudolf Carnap, Alfred Tarski, and Haskell Curry, considered mathematics to be the investigation of formal axiom systems. Mathematical logicians study formal systems but are just as often realists as they are formalists.\n\nFormalists are relatively tolerant and inviting to new approaches to logic, non-standard number systems, new set theories etc. The more games we study, the better. However, in all three of these examples, motivation is drawn from existing mathematical or philosophical concerns. The \"games\" are usually not arbitrary.\n\nThe main critique of formalism is that the actual mathematical ideas that occupy mathematicians are far removed from the string manipulation games mentioned above. Formalism is thus silent on the question of which axiom systems ought to be studied, as none is more meaningful than another from a formalistic point of view.\n\nRecently, some formalist mathematicians have proposed that all of our \"formal\" mathematical knowledge should be systematically encoded in computer-readable formats, so as to facilitate automated proof checking of mathematical proofs and the use of interactive theorem proving in the development of mathematical theories and computer software. Because of their close connection with computer science, this idea is also advocated by mathematical intuitionists and constructivists in the \"computability\" tradition—see QED project for a general overview.\n\nThe French mathematician Henri Poincaré was among the first to articulate a conventionalist view. Poincaré's use of non-Euclidean geometries in his work on differential equations convinced him that Euclidean geometry should not be regarded as \"a priori\" truth. He held that axioms in geometry should be chosen for the results they produce, not for their apparent coherence with human intuitions about the physical world.\n\nIn mathematics, intuitionism is a program of methodological reform whose motto is that \"there are no non-experienced mathematical truths\" (L.E.J. Brouwer). From this springboard, intuitionists seek to reconstruct what they consider to be the corrigible portion of mathematics in accordance with Kantian concepts of being, becoming, intuition, and knowledge. Brouwer, the founder of the movement, held that mathematical objects arise from the \"a priori\" forms of the volitions that inform the perception of empirical objects.\n\nA major force behind intuitionism was L.E.J. Brouwer, who rejected the usefulness of formalized logic of any sort for mathematics. His student Arend Heyting postulated an intuitionistic logic, different from the classical Aristotelian logic; this logic does not contain the law of the excluded middle and therefore frowns upon proofs by contradiction. The axiom of choice is also rejected in most intuitionistic set theories, though in some versions it is accepted. Important work was later done by Errett Bishop, who managed to prove versions of the most important theorems in real analysis within this framework.\n\nIn intuitionism, the term \"explicit construction\" is not cleanly defined, and that has led to criticisms. Attempts have been made to use the concepts of Turing machine or computable function to fill this gap, leading to the claim that only questions regarding the behavior of finite algorithms are meaningful and should be investigated in mathematics. This has led to the study of the computable numbers, first introduced by Alan Turing. Not surprisingly, then, this approach to mathematics is sometimes associated with theoretical computer science.\n\nLike intuitionism, constructivism involves the regulative principle that only mathematical entities which can be explicitly constructed in a certain sense should be admitted to mathematical discourse. In this view, mathematics is an exercise of the human intuition, not a game played with meaningless symbols. Instead, it is about entities that we can create directly through mental activity. In addition, some adherents of these schools reject non-constructive proofs, such as a proof by contradiction.\n\nFinitism is an extreme form of constructivism, according to which a mathematical object does not exist unless it can be constructed from natural numbers in a finite number of steps. In her book \"Philosophy of Set Theory\", Mary Tiles characterized those who allow countably infinite objects as classical finitists, and those who deny even countably infinite objects as strict finitists.\n\nThe most famous proponent of finitism was Leopold Kronecker, who said:\nUltrafinitism is an even more extreme version of finitism, which rejects not only infinities but finite quantities that cannot feasibly be constructed with available resources. Another variant of finitism is Euclidean arithmetic, a system developed by John Penn Mayberry in his book \"The Foundations of Mathematics in the Theory of Sets\". Mayberry's system is Aristotelian in general inspiration and, despite his strong rejection of any role for operationalism or feasibility in the foundations of mathematics, comes to somewhat similar conclusions, such as, for instance, that super-exponentiation is not a legitimate finitary function.\n\nStructuralism is a position holding that mathematical theories describe structures, and that mathematical objects are exhaustively defined by their \"places\" in such structures, consequently having no intrinsic properties. For instance, it would maintain that all that needs to be known about the number 1 is that it is the first whole number after 0. Likewise all the other whole numbers are defined by their places in a structure, the number line. Other examples of mathematical objects might include lines and planes in geometry, or elements and operations in abstract algebra.\n\nStructuralism is an epistemologically realistic view in that it holds that mathematical statements have an objective truth value. However, its central claim only relates to what \"kind\" of entity a mathematical object is, not to what kind of \"existence\" mathematical objects or structures have (not, in other words, to their ontology). The kind of existence mathematical objects have would clearly be dependent on that of the structures in which they are embedded; different sub-varieties of structuralism make different ontological claims in this regard.\n\nThe \"ante rem\" structuralism (\"before the thing\") has a similar ontology to Platonism. Structures are held to have a real but abstract and immaterial existence. As such, it faces the standard epistemological problem of explaining the interaction between such abstract structures and flesh-and-blood mathematicians (see Benacerraf's identification problem).\n\nThe \"in re\" structuralism (\"in the thing\") is the equivalent of Aristotelean realism. Structures are held to exist inasmuch as some concrete system exemplifies them. This incurs the usual issues that some perfectly legitimate structures might accidentally happen not to exist, and that a finite physical world might not be \"big\" enough to accommodate some otherwise legitimate structures.\n\nThe \"post rem\" structuralism (\"after the thing\") is anti-realist about structures in a way that parallels nominalism. Like nominalism, the \"post rem\" approach denies the existence of abstract mathematical objects with properties other than their place in a relational structure. According to this view mathematical \"systems\" exist, and have structural features in common. If something is true of a structure, it will be true of all systems exemplifying the structure. However, it is merely instrumental to talk of structures being \"held in common\" between systems: they in fact have no independent existence.\n\nEmbodied mind theories hold that mathematical thought is a natural outgrowth of the human cognitive apparatus which finds itself in our physical universe. For example, the abstract concept of number springs from the experience of counting discrete objects. It is held that mathematics is not universal and does not exist in any real sense, other than in human brains. Humans construct, but do not discover, mathematics.\n\nWith this view, the physical universe can thus be seen as the ultimate foundation of mathematics: it guided the evolution of the brain and later determined which questions this brain would find worthy of investigation. However, the human mind has no special claim on reality or approaches to it built out of math. If such constructs as Euler's identity are true then they are true as a map of the human mind and cognition.\n\nEmbodied mind theorists thus explain the effectiveness of mathematics—mathematics was constructed by the brain in order to be effective in this universe.\n\nThe most accessible, famous, and infamous treatment of this perspective is \"Where Mathematics Comes From\", by George Lakoff and Rafael E. Núñez. In addition, mathematician Keith Devlin has investigated similar concepts with his book \"The Math Instinct\", as has neuroscientist Stanislas Dehaene with his book \"The Number Sense\". For more on the philosophical ideas that inspired this perspective, see cognitive science of mathematics.\n\nAristotelian realism holds that mathematics studies properties such as symmetry, continuity and order that can be literally realized in the physical world (or in any other world there might be). It contrasts with Platonism in holding that the objects of mathematics, such as numbers, do not exist in an \"abstract\" world but can be physically realized. For example, the number 4 is realized in the relation between a heap of parrots and the universal \"being a parrot\" that divides the heap into so many parrots. Aristotelian realism is defended by James Franklin and the Sydney School in the philosophy of mathematics and is close to the view of Penelope Maddy that when an egg carton is opened, a set of three eggs is perceived (that is, a mathematical entity realized in the physical world). A problem for Aristotelian realism is what account to give of higher infinities, which may not be realizable in the physical world.\n\nThe Euclidean arithmetic developed by John Penn Mayberry in his book \"The Foundations of Mathematics in the Theory of Sets\". also falls into the Aristotelian realist tradition. Mayberry, following Euclid, considers numbers to be simply \"definite multitudes of units\" realized in nature—such as \"the members of the London Symphony Orchestra\" or \"the trees in Birnam wood\". Whether or not there are definite multitudes of units for which Euclid's Common Notion 5 (the Whole is greater than the Part) fails and which would consequently be reckoned as infinite is for Mayberry essentially a question about Nature and does not entail any transcendental suppositions.\n\nPsychologism in the philosophy of mathematics is the position that mathematical concepts and/or truths are grounded in, derived from or explained by psychological facts (or laws).\n\nJohn Stuart Mill seems to have been an advocate of a type of logical psychologism, as were many 19th-century German logicians such as Sigwart and Erdmann as well as a number of psychologists, past and present: for example, Gustave Le Bon. Psychologism was famously criticized by Frege in his \"The Foundations of Arithmetic\", and many of his works and essays, including his review of Husserl's \"Philosophy of Arithmetic\". Edmund Husserl, in the first volume of his \"Logical Investigations\", called \"The Prolegomena of Pure Logic\", criticized psychologism thoroughly and sought to distance himself from it. The \"Prolegomena\" is considered a more concise, fair, and thorough refutation of psychologism than the criticisms made by Frege, and also it is considered today by many as being a memorable refutation for its decisive blow to psychologism. Psychologism was also criticized by Charles Sanders Peirce and Maurice Merleau-Ponty.\n\nMathematical empiricism is a form of realism that denies that mathematics can be known \"a priori\" at all. It says that we discover mathematical facts by empirical research, just like facts in any of the other sciences. It is not one of the classical three positions advocated in the early 20th century, but primarily arose in the middle of the century. However, an important early proponent of a view like this was John Stuart Mill. Mill's view was widely criticized, because, according to critics, such as A.J. Ayer, it makes statements like come out as uncertain, contingent truths, which we can only learn by observing instances of two pairs coming together and forming a quartet.\n\nContemporary mathematical empiricism, formulated by W. V. O. Quine and Hilary Putnam, is primarily supported by the indispensability argument: mathematics is indispensable to all empirical sciences, and if we want to believe in the reality of the phenomena described by the sciences, we ought also believe in the reality of those entities required for this description. That is, since physics needs to talk about electrons to say why light bulbs behave as they do, then electrons must exist. Since physics needs to talk about numbers in offering any of its explanations, then numbers must exist. In keeping with Quine and Putnam's overall philosophies, this is a naturalistic argument. It argues for the existence of mathematical entities as the best explanation for experience, thus stripping mathematics of being distinct from the other sciences.\n\nPutnam strongly rejected the term \"Platonist\" as implying an over-specific ontology that was not necessary to mathematical practice in any real sense. He advocated a form of \"pure realism\" that rejected mystical notions of truth and accepted much quasi-empiricism in mathematics. This grew from the increasingly popular assertion in the late 20th century that no one foundation of mathematics could be ever proven to exist. It is also sometimes called \"postmodernism in mathematics\" although that term is considered overloaded by some and insulting by others. Quasi-empiricism argues that in doing their research, mathematicians test hypotheses as well as prove theorems. A mathematical argument can transmit falsity from the conclusion to the premises just as well as it can transmit truth from the premises to the conclusion. Putnam has argued that any theory of mathematical realism would include quasi-empirical methods. He proposed that an alien species doing mathematics might well rely on quasi-empirical methods primarily, being willing often to forgo rigorous and axiomatic proofs, and still be doing mathematics—at perhaps a somewhat greater risk of failure of their calculations. He gave a detailed argument for this in \"New Directions\". Quasi-empiricism was also developed by Imre Lakatos.\n\nThe most important criticism of empirical views of mathematics is approximately the same as that raised against Mill. If mathematics is just as empirical as the other sciences, then this suggests that its results are just as fallible as theirs, and just as contingent. In Mill's case the empirical justification comes directly, while in Quine's case it comes indirectly, through the coherence of our scientific theory as a whole, i.e. consilience after E.O. Wilson. Quine suggests that mathematics seems completely certain because the role it plays in our web of belief is extraordinarily central, and that it would be extremely difficult for us to revise it, though not impossible.\n\nFor a philosophy of mathematics that attempts to overcome some of the shortcomings of Quine and Gödel's approaches by taking aspects of each see Penelope Maddy's \"Realism in Mathematics\". Another example of a realist theory is the embodied mind theory. \n\nFor experimental evidence suggesting that human infants can do elementary arithmetic, see Brian Butterworth.\n\nMathematical fictionalism was brought to fame in 1980 when Hartry Field published \"Science Without Numbers\", which rejected and in fact reversed Quine's indispensability argument. Where Quine suggested that mathematics was indispensable for our best scientific theories, and therefore should be accepted as a body of truths talking about independently existing entities, Field suggested that mathematics was dispensable, and therefore should be considered as a body of falsehoods not talking about anything real. He did this by giving a complete axiomatization of Newtonian mechanics with no reference to numbers or functions at all. He started with the \"betweenness\" of Hilbert's axioms to characterize space without coordinatizing it, and then added extra relations between points to do the work formerly done by vector fields. Hilbert's geometry is mathematical, because it talks about abstract points, but in Field's theory, these points are the concrete points of physical space, so no special mathematical objects at all are needed.\n\nHaving shown how to do science without using numbers, Field proceeded to rehabilitate mathematics as a kind of useful fiction. He showed that mathematical physics is a conservative extension of his non-mathematical physics (that is, every physical fact provable in mathematical physics is already provable from Field's system), so that mathematics is a reliable process whose physical applications are all true, even though its own statements are false. Thus, when doing mathematics, we can see ourselves as telling a sort of story, talking as if numbers existed. For Field, a statement like is just as fictitious as \"Sherlock Holmes lived at 221B Baker Street\"—but both are true according to the relevant fictions.\n\nBy this account, there are no metaphysical or epistemological problems special to mathematics. The only worries left are the general worries about non-mathematical physics, and about fiction in general. Field's approach has been very influential, but is widely rejected. This is in part because of the requirement of strong fragments of second-order logic to carry out his reduction, and because the statement of conservativity seems to require quantification over abstract models or deductions.\n\nSocial constructivism see mathematics primarily as a social construct, as a product of culture, subject to correction and change. Like the other sciences, mathematics is viewed as an empirical endeavor whose results are constantly evaluated and may be discarded. However, while on an empiricist view the evaluation is some sort of comparison with \"reality\", social constructivists emphasize that the direction of mathematical research is dictated by the fashions of the social group performing it or by the needs of the society financing it. However, although such external forces may change the direction of some mathematical research, there are strong internal constraints—the mathematical traditions, methods, problems, meanings and values into which mathematicians are enculturated—that work to conserve the historically-defined discipline.\n\nThis runs counter to the traditional beliefs of working mathematicians, that mathematics is somehow pure or objective. But social constructivists argue that mathematics is in fact grounded by much uncertainty: as mathematical practice evolves, the status of previous mathematics is cast into doubt, and is corrected to the degree it is required or desired by the current mathematical community. This can be seen in the development of analysis from reexamination of the calculus of Leibniz and Newton. They argue further that finished mathematics is often accorded too much status, and folk mathematics not enough, due to an overemphasis on axiomatic proof and peer review as practices. However, this might be seen as merely saying that rigorously proven results are overemphasized, and then \"look how chaotic and uncertain the rest of it all is!\"\n\nThe social nature of mathematics is highlighted in its subcultures. Major discoveries can be made in one branch of mathematics and be relevant to another, yet the relationship goes undiscovered for lack of social contact between mathematicians. Social constructivists argue each speciality forms its own epistemic community and often has great difficulty communicating, or motivating the investigation of unifying conjectures that might relate different areas of mathematics. Social constructivists see the process of \"doing mathematics\" as actually creating the meaning, while social realists see a deficiency either of human capacity to abstractify, or of human's cognitive bias, or of mathematicians' collective intelligence as preventing the comprehension of a real universe of mathematical objects. Social constructivists sometimes reject the search for foundations of mathematics as bound to fail, as pointless or even meaningless.\n\nContributions to this school have been made by Imre Lakatos and Thomas Tymoczko, although it is not clear that either would endorse the title. More recently Paul Ernest has explicitly formulated a social constructivist philosophy of mathematics. Some consider the work of Paul Erdős as a whole to have advanced this view (although he personally rejected it) because of his uniquely broad collaborations, which prompted others to see and study \"mathematics as a social activity\", e.g., via the Erdős number. Reuben Hersh has also promoted the social view of mathematics, calling it a \"humanistic\" approach, similar to but not quite the same as that associated with Alvin White; one of Hersh's co-authors, Philip J. Davis, has expressed sympathy for the social view as well.\n\nA criticism of this approach is that it is trivial, based on the trivial observation that mathematics is a human activity. To observe that rigorous proof comes only after unrigorous conjecture, experimentation and speculation is true, but it is trivial and no-one would deny this. So it's a bit of a stretch to characterize a philosophy of mathematics in this way, on something trivially true. The calculus of Leibniz and Newton was reexamined by mathematicians such as Weierstrass in order to rigorously prove the theorems thereof. There is nothing special or interesting about this, as it fits in with the more general trend of unrigorous ideas which are later made rigorous. There needs to be a clear distinction between the objects of study of mathematics and the study of the objects of study of mathematics. The former doesn't seem to change a great deal; the latter is forever in flux. The latter is what the social theory is about, and the former is what Platonism \"et al.\" are about.\n\nHowever, this criticism is rejected by supporters of the social constructivist perspective because it misses the point that the very objects of mathematics are social constructs. These objects, it asserts, are primarily semiotic objects existing in the sphere of human culture, sustained by social practices (after Wittgenstein) that utilize physically embodied signs and give rise to intrapersonal (mental) constructs. Social constructivists view the reification of the sphere of human culture into a Platonic realm, or some other heaven-like domain of existence beyond the physical world, a long-standing category error.\n\nRather than focus on narrow debates about the true nature of mathematical truth, or even on practices unique to mathematicians such as the proof, a growing movement from the 1960s to the 1990s began to question the idea of seeking foundations or finding any one right answer to why mathematics works. The starting point for this was Eugene Wigner's famous 1960 paper \"The Unreasonable Effectiveness of Mathematics in the Natural Sciences\", in which he argued that the happy coincidence of mathematics and physics being so well matched seemed to be unreasonable and hard to explain.\n\nRealist and constructivist theories are normally taken to be contraries. However, Karl Popper argued that a number statement such as can be taken in two senses. In one sense it is irrefutable and logically true. In the second sense it is factually true and falsifiable. Another way of putting this is to say that a single number statement can express two propositions: one of which can be explained on constructivist lines; the other on realist lines.\n\nInnovations in the philosophy of language during the 20th century renewed interest in whether mathematics is, as is often said, the \"language\" of science. Although some mathematicians and philosophers would accept the statement \"mathematics is a language\", linguists believe that the implications of such a statement must be considered. For example, the tools of linguistics are not generally applied to the symbol systems of mathematics, that is, mathematics is studied in a markedly different way from other languages. If mathematics is a language, it is a different type of language from natural languages. Indeed, because of the need for clarity and specificity, the language of mathematics is far more constrained than natural languages studied by linguists. However, the methods developed by Frege and Tarski for the study of mathematical language have been extended greatly by Tarski's student Richard Montague and other linguists working in formal semantics to show that the distinction between mathematical language and natural language may not be as great as it seems.\n\nMohan Ganesalingam has analysed mathematical language using tools from formal linguistics. Ganesalingam notes that some features of natural language are not necessary when analysing mathematical language (such as tense), but many of the same analytical tools can be used (such as context-free grammars). One important difference is that mathematical objects have clearly defined types, which can be explicitly defined in a text: \"Effectively, we are allowed to introduce a word in one part of a sentence, and declare its part of speech in another; and this operation has no analogue in natural language.\"\n\nThis argument, associated with Willard Quine and Hilary Putnam, is considered by Stephen Yablo to be one of the most challenging arguments in favor of the acceptance of the existence of abstract mathematical entities, such as numbers and sets. The form of the argument is as follows.\n\nThe justification for the first premise is the most controversial. Both Putnam and Quine invoke naturalism to justify the exclusion of all non-scientific entities, and hence to defend the \"only\" part of \"all and only\". The assertion that \"all\" entities postulated in scientific theories, including numbers, should be accepted as real is justified by confirmation holism. Since theories are not confirmed in a piecemeal fashion, but as a whole, there is no justification for excluding any of the entities referred to in well-confirmed theories. This puts the nominalist who wishes to exclude the existence of sets and non-Euclidean geometry, but to include the existence of quarks and other undetectable entities of physics, for example, in a difficult position.\n\nThe anti-realist \"epistemic argument\" against Platonism has been made by Paul Benacerraf and Hartry Field. Platonism posits that mathematical objects are \"abstract\" entities. By general agreement, abstract entities cannot interact causally with concrete, physical entities (\"the truth-values of our mathematical assertions depend on facts involving Platonic entities that reside in a realm outside of space-time\"). Whilst our knowledge of concrete, physical objects is based on our ability to perceive them, and therefore to causally interact with them, there is no parallel account of how mathematicians come to have knowledge of abstract objects. Another way of making the point is that if the Platonic world were to disappear, it would make no difference to the ability of mathematicians to generate proofs, etc., which is already fully accountable in terms of physical processes in their brains.\n\nField developed his views into fictionalism. Benacerraf also developed the philosophy of mathematical structuralism, according to which there are no mathematical objects. Nonetheless, some versions of structuralism are compatible with some versions of realism.\n\nThe argument hinges on the idea that a satisfactory naturalistic account of thought processes in terms of brain processes can be given for mathematical reasoning along with everything else. One line of defense is to maintain that this is false, so that mathematical reasoning uses some special intuition that involves contact with the Platonic realm. A modern form of this argument is given by Sir Roger Penrose.\n\nAnother line of defense is to maintain that abstract objects are relevant to mathematical reasoning in a way that is non-causal, and not analogous to perception. This argument is developed by Jerrold Katz in his 2000 book \"Realistic Rationalism\".\n\nA more radical defense is denial of physical reality, i.e. the mathematical universe hypothesis. In that case, a mathematician's knowledge of mathematics is one mathematical object making contact with another.\n\nMany practicing mathematicians have been drawn to their subject because of a sense of beauty they perceive in it. One sometimes hears the sentiment that mathematicians would like to leave philosophy to the philosophers and get back to mathematics—where, presumably, the beauty lies.\n\nIn his work on the divine proportion, H.E. Huntley relates the feeling of reading and understanding someone else's proof of a theorem of mathematics to that of a viewer of a masterpiece of art—the reader of a proof has a similar sense of exhilaration at understanding as the original author of the proof, much as, he argues, the viewer of a masterpiece has a sense of exhilaration similar to the original painter or sculptor. Indeed, one can study mathematical and scientific writings as literature.\n\nPhilip J. Davis and Reuben Hersh have commented that the sense of mathematical beauty is universal amongst practicing mathematicians. By way of example, they provide two proofs of the irrationality of . The first is the traditional proof by contradiction, ascribed to Euclid; the second is a more direct proof involving the fundamental theorem of arithmetic that, they argue, gets to the heart of the issue. Davis and Hersh argue that mathematicians find the second proof more aesthetically appealing because it gets closer to the nature of the problem.\n\nPaul Erdős was well known for his notion of a hypothetical \"Book\" containing the most elegant or beautiful mathematical proofs. There is not universal agreement that a result has one \"most elegant\" proof; Gregory Chaitin has argued against this idea.\n\nPhilosophers have sometimes criticized mathematicians' sense of beauty or elegance as being, at best, vaguely stated. By the same token, however, philosophers of mathematics have sought to characterize what makes one proof more desirable than another when both are logically sound.\n\nAnother aspect of aesthetics concerning mathematics is mathematicians' views towards the possible uses of mathematics for purposes deemed unethical or inappropriate. The best-known exposition of this view occurs in G.H. Hardy's book \"A Mathematician's Apology\", in which Hardy argues that pure mathematics is superior in beauty to applied mathematics precisely because it cannot be used for war and similar ends.\n\n\n\n\n\n\n\n\n"}
{"id": "84029", "url": "https://en.wikipedia.org/wiki?curid=84029", "title": "Plane (geometry)", "text": "Plane (geometry)\n\nIn mathematics, a plane is a flat, two-dimensional surface that extends infinitely far. A plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension) and three-dimensional space. Planes can arise as subspaces of some higher-dimensional space, as with a room's walls extended infinitely far, or they may enjoy an independent existence in their own right, as in the setting of Euclidean geometry.\n\nWhen working exclusively in two-dimensional Euclidean space, the definite article is used, so, \"the\" plane refers to the whole space. Many fundamental tasks in mathematics, geometry, trigonometry, graph theory, and graphing are performed in a two-dimensional space, or, in other words, in the plane.\n\nEuclid set forth the first great landmark of mathematical thought, an axiomatic treatment of geometry. He selected a small core of undefined terms (called \"common notions\") and postulates (or axioms) which he then used to prove various geometrical statements. Although the plane in its modern sense is not directly given a definition anywhere in the \"Elements\", it may be thought of as part of the common notions. Euclid never used numbers to measure length, angle, or area. In this way the Euclidean plane is not quite the same as the Cartesian plane.\n\nA plane is a ruled surface.\n\nThis section is solely concerned with planes embedded in three dimensions: specifically, in R.\n\nIn a Euclidean space of any number of dimensions, a plane is uniquely determined by any of the following:\n\nThe following statements hold in three-dimensional Euclidean space but not in higher dimensions, though they have higher-dimensional analogues:\n\n\nIn a manner analogous to the way lines in a two-dimensional space are described using a point-slope form for their equations, planes in a three dimensional space have a natural description using a point in the plane and a vector orthogonal to it (the normal vector) to indicate its \"inclination\".\n\nSpecifically, let be the position vector of some point , and let be a nonzero vector. The plane determined by the point and the vector consists of those points , with position vector , such that the vector drawn from to is perpendicular to . Recalling that two vectors are perpendicular if and only if their dot product is zero, it follows that the desired plane can be described as the set of all points such that\nExpanded this becomes\nwhich is the \"point-normal\" form of the equation of a plane. This is just a linear equation\nwhere\nConversely, it is easily shown that if and are constants and , and are not all zero, then the graph of the equation\nis a plane having the vector as a normal. This familiar equation for a plane is called the \"general form\" of the equation of the plane.\n\nThus for example a regression equation of the form (with ) establishes a best-fit plane in three-dimensional space when there are two explanatory variables.\n\nAlternatively, a plane may be described parametrically as the set of all points of the form\nwhere \"s\" and \"t\" range over all real numbers, ' and are given linearly independent vectors defining the plane, and is the vector representing the position of an arbitrary (but fixed) point on the plane. The vectors ' and can be visualized as vectors starting at and pointing in different directions along the plane. Note that and can be perpendicular, but cannot be parallel.\n\nLet , , and be non-collinear points.\n\nThe plane passing through , , and can be described as the set of all points (x,y,z) that satisfy the following determinant equations:\n\nTo describe the plane by an equation of the form formula_8, solve the following system of equations:\n\nThis system can be solved using Cramer's rule and basic matrix manipulations. Let\n\nIf \"D\" is non-zero (so for planes not through the origin) the values for \"a\", \"b\" and \"c\" can be calculated as follows:\n\nThese equations are parametric in \"d\". Setting \"d\" equal to any non-zero number and substituting it into these equations will yield one solution set.\n\nThis plane can also be described by the \"point and a normal vector\" prescription above. A suitable normal vector is given by the cross product\nand the point can be taken to be any of the given points , or (or any other point in the plane).\n\nFor a plane formula_17 and a point formula_18 not necessarily lying on the plane, the shortest distance from formula_19 to the plane is\n\nIt follows that formula_19 lies in the plane if and only if \"D=0\".\n\nIf formula_22 meaning that \"a\", \"b\", and \"c\" are normalized then the equation becomes\n\nAnother vector form for the equation of a plane, known as the Hesse normal form relies on the parameter \"D\". This form is:\nwhere formula_25 is a unit normal vector to the plane, formula_26 a position vector of a point of the plane and \"D\" the distance of the plane from the origin.\n\nThe general formula for higher dimensions can be quickly arrived at using vector notation. Let the hyperplane have equation formula_27, where the formula_25 is a normal vector and formula_29 is a position vector to a point in the hyperplane. We desire the perpendicular distance to the point formula_30. The hyperplane may also be represented by the scalar equation formula_31, for constants formula_32. Likewise, a corresponding formula_25 may be represented as formula_34. We desire the scalar projection of the vector formula_35 in the direction of formula_25. Noting that formula_37 (as formula_38 satisfies the equation of the hyperplane) we have\n\nThe line of intersection between two planes formula_40 and formula_41 where formula_42 are normalized is given by\n\nwhere\n\nThis is found by noticing that the line must be perpendicular to both plane normals, and so parallel to their cross product formula_46 (this cross product is zero if and only if the planes are parallel, and are therefore non-intersecting or entirely coincident).\n\nThe remainder of the expression is arrived at by finding an arbitrary point on the line. To do so, consider that any point in space may be written as formula_47, since formula_48 is a basis. We wish to find a point which is on both planes (i.e. on their intersection), so insert this equation into each of the equations of the planes to get two simultaneous equations which can be solved for formula_49 and formula_50.\n\nIf we further assume that formula_51 and formula_52 are orthonormal then the closest point on the line of intersection to the origin is formula_53. If that is not the case, then a more complex procedure must be used.\n\nGiven two intersecting planes described by formula_54 and formula_55, the dihedral angle between them is defined to be the angle formula_56 between their normal directions:\n\nIn addition to its familiar geometric structure, with isomorphisms that are isometries with respect to the usual inner product, the plane may be viewed at various other levels of abstraction. Each level of abstraction corresponds to a specific category.\n\nAt one extreme, all geometrical and metric concepts may be dropped to leave the topological plane, which may be thought of as an idealized homotopically trivial infinite rubber sheet, which retains a notion of proximity, but has no distances. The topological plane has a concept of a linear path, but no concept of a straight line. The topological plane, or its equivalent the open disc, is the basic topological neighborhood used to construct surfaces (or 2-manifolds) classified in low-dimensional topology. Isomorphisms of the topological plane are all continuous bijections. The topological plane is the natural context for the branch of graph theory that deals with planar graphs, and results such as the four color theorem.\n\nThe plane may also be viewed as an affine space, whose isomorphisms are combinations of translations and non-singular linear maps. From this viewpoint there are no distances, but collinearity and ratios of distances on any line are preserved.\n\nDifferential geometry views a plane as a 2-dimensional real manifold, a topological plane which is provided with a differential structure. Again in this case, there is no notion of distance, but there is now a concept of smoothness of maps, for example a differentiable or smooth path (depending on the type of differential structure applied). The isomorphisms in this case are bijections with the chosen degree of differentiability.\n\nIn the opposite direction of abstraction, we may apply a compatible field structure to the geometric plane, giving rise to the complex plane and the major area of complex analysis. The complex field has only two isomorphisms that leave the real line fixed, the identity and conjugation.\n\nIn the same way as in the real case, the plane may also be viewed as the simplest, one-dimensional (over the complex numbers) complex manifold, sometimes called the complex line. However, this viewpoint contrasts sharply with the case of the plane as a 2-dimensional real manifold. The isomorphisms are all conformal bijections of the complex plane, but the only possibilities are maps that correspond to the composition of a multiplication by a complex number and a translation.\n\nIn addition, the Euclidean geometry (which has zero curvature everywhere) is not the only geometry that the plane may have. The plane may be given a spherical geometry by using the stereographic projection. This can be thought of as placing a sphere on the plane (just like a ball on the floor), removing the top point, and projecting the sphere onto the plane from this point). This is one of the projections that may be used in making a flat map of part of the Earth's surface. The resulting geometry has constant positive curvature.\n\nAlternatively, the plane can also be given a metric which gives it constant negative curvature giving the hyperbolic plane. The latter possibility finds an application in the theory of special relativity in the simplified case where there are two spatial dimensions and one time dimension. (The hyperbolic plane is a timelike hypersurface in three-dimensional Minkowski space.)\n\nThe one-point compactification of the plane is homeomorphic to a sphere (see stereographic projection); the open disk is homeomorphic to a sphere with the \"north pole\" missing; adding that point completes the (compact) sphere. The result of this compactification is a manifold referred to as the Riemann sphere or the complex projective line. The projection from the Euclidean plane to a sphere without a point is a diffeomorphism and even a conformal map.\n\nThe plane itself is homeomorphic (and diffeomorphic) to an open disk. For the hyperbolic plane such diffeomorphism is conformal, but for the Euclidean plane it is not.\n\n\n\n"}
{"id": "2598499", "url": "https://en.wikipedia.org/wiki?curid=2598499", "title": "Psychologism", "text": "Psychologism\n\nPsychologism is a philosophical position, according to which psychology plays a central role in grounding or explaining some other, non-psychological type of fact or law.\n\nThe \"Oxford English Dictionary\" defines \"psychologism\" as: \"The view or doctrine that a theory of psychology or ideas forms the basis of an account of metaphysics, epistemology, or meaning; (sometimes) spec. the explanation or derivation of mathematical or logical laws in terms of psychological facts.\" Psychologism in epistemology, the idea that its problems \"can be solved satisfactorily by the psychological study of the development of mental processes\", was argued in John Locke's \"An Essay Concerning Human Understanding\" (1690).\n\nOther forms of psychologism are logical psychologism and mathematical psychologism. Logical psychologism is a position in logic (or the philosophy of logic) according to which logical laws and mathematical laws are grounded in, derived from, explained or exhausted by psychological facts or laws. Psychologism in the philosophy of mathematics is the position that mathematical concepts and/or truths are grounded in, derived from or explained by psychological facts or laws.\n\nThe word was coined by Johann Eduard Erdmann as \"Psychologismus\", being translated into English as \"psychologism\".\n\nJohn Stuart Mill was accused by Edmund Husserl of being an advocate of a type of logical psychologism, although this may not have been the case. So were many nineteenth-century German philosophers such as Christoph von Sigwart, Benno Erdmann, Theodor Lipps, Gerardus Heymans, Wilhelm Jerusalem, and , as well as a number of psychologists, past and present (e.g., Wilhelm Wundt and Gustave Le Bon).\n\nPsychologism was notably criticized by Gottlob Frege in his anti-psychologistic work \"The Foundations of Arithmetic\", and many of his works and essays, including his review of Husserl's \"Philosophy of Arithmetic\". Husserl, in the first volume of his \"Logical Investigations\", called \"The Prolegomena of Pure Logic\", criticized psychologism thoroughly and sought to distance himself from it. Frege's arguments were largely ignored, while Husserl's were widely discussed.\n\nIn \"Psychologism and Behaviorism\", Ned Block takes psychologism as the position that \"whether behavior is intelligent behavior depends on the character of the internal information processing that produces it.\" This is in contrast to a behavioral view which would state that intelligence can be ascribed to a being solely via observing its behavior. This latter type of behavioral view is strongly associated with the Turing test.\n\n\n"}
{"id": "211399", "url": "https://en.wikipedia.org/wiki?curid=211399", "title": "Structural induction", "text": "Structural induction\n\nStructural induction is a proof method that is used in mathematical logic (e.g., in the proof of Łoś' theorem), computer science, graph theory, and some other mathematical fields. It is a generalization of mathematical induction over natural numbers, and can be further generalized to arbitrary Noetherian induction. Structural recursion is a recursion method bearing the same relationship to structural induction as ordinary recursion bears to ordinary mathematical induction.\n\nStructural induction is used to prove that some proposition \"P\"(\"x\") holds for all \"x\" of some sort of recursively defined structure, such as\nformulas, lists, or trees. A well-founded partial order is defined on the structures (\"subformula\" for formulas, \"sublist\" for lists, and \"subtree\" for trees). The structural induction proof is a proof that the proposition holds for all the minimal structures, and that if it holds for the immediate substructures of a certain structure \"S\", then it must hold for \"S\" also. (Formally speaking, this then satisfies the premises of an axiom of well-founded induction, which asserts that these two conditions are sufficient for the proposition to hold for all \"x\".)\n\nA structurally recursive function uses the same idea to define a recursive function: \"base cases\" handle each minimal structure and a rule for recursion. Structural recursion is usually proved correct by structural induction; in particularly easy cases, the inductive step is often left out. The \"length\" and ++ functions in the example below are structurally recursive.\n\nFor example, if the structures are lists, one usually introduces the partial order '<' in which \"L\" < \"M\" whenever list \"L\" is the tail of list \"M\". Under this ordering, the empty list [] is the unique minimal element. A structural induction proof of some proposition then consists of two parts: A proof that \"P\"([]) is true, and a proof that if \"P\"(\"L\") is true for some list \"L\", and if \"L\" is the tail of list \"M\", then \"P\"(\"M\") must also be true.\n\nEventually, there may exist more than one base case, and/or more than one inductive case, depending on how the function or structure was constructed. In those cases, a structural induction proof of some proposition then consists of: \nAn ancestor tree is a commonly known data structure, showing the parents, grandparents, etc. of a person as far as known (see picture for an example). It is recursively defined:\n\nAs an example, the property \"An ancestor tree extending over \"g\" generations shows at most persons\" can be proven by structural induction as follows:\nHence, by structural induction, each ancestor tree satisfies the property.\n\nAs another, more formal example, consider the following property of lists:\n\nHere ++ denotes the list concatenation operation, and L and M are lists.\n\nIn order to prove this, we need definitions for length and for the concatenation operation. Let (h:t) denote a list whose head (first element) is \"h\" and whose tail (list of remaining elements) is \"t\", and let [] denote the empty list. The definitions for length and the concatenation operation are:\n\nOur proposition is that EQ is true for all lists \"M\" when \"L\" is . We want to show that is true for all lists . We will prove this by structural induction on lists.\n\nFirst we will prove that \"P\"([]) is true; that is, EQ is true for all lists \"M\" when \"L\" happens to be the empty list []. Consider EQ:\n\nSo this part of the theorem is proved; EQ is true for all \"M\", when \"L\" is [], because the left-hand side and the right-hand side are equal.\n\nNext, consider any nonempty list . Since is nonempty, it has a head item, x, and a tail list, xs, so we can express it as (x:xs). The induction hypothesis is that EQ is true for all values of \"M\" when \"L\" is \"xs\":\n\nWe would like to show that if this is the case, then EQ is also true for all values of \"M\" when = (x:xs). We proceed as before:\n\nThus, from structural induction, we obtain that P(L) is true for all lists L.\n\nJust as standard mathematical induction is equivalent to the well-ordering principle, structural induction is also equivalent to a well-ordering principle. If the set of all structures of a certain kind admits a well-founded partial order, then every nonempty subset must have a minimal element. (This is the definition of \"well-founded\".) The significance of the lemma in this context is that it allows us to deduce that if there are any counterexamples to the theorem we want to prove, then there must be a minimal counterexample. If we can show the existence of the minimal counterexample implies an even smaller counterexample, we have a contradiction (since the minimal counterexample isn't minimal) and so the set of counterexamples must be empty.\n\nAs an example of this type of argument, consider the set of all binary trees. We will show that the number of leaves in a full binary tree is one more than the number of interior nodes. Suppose there is a counterexample; then there must exist one with the minimal possible number of interior nodes. This counterexample, \"C\", has \"n\" interior nodes and leaves, where . Moreover, \"C\" must be nontrivial, because the trivial tree has and and is therefore not a counterexample. \"C\" therefore has at least one leaf whose parent node is an interior node. Delete this leaf and its parent from the tree, promoting the leaf's sibling node to the position formerly occupied by its parent. This reduces both \"n\" and by 1, so the new tree also has and is therefore a smaller counterexample. But by hypothesis, \"C\" was already the smallest counterexample; therefore, the supposition that there were any counterexamples to begin with must have been false. The partial ordering implied by 'smaller' here is the one that says that \"S\" < \"T\" whenever \"S\" has fewer nodes than \"T\".\n\n\nEarly publications about structural induction include:\n"}
{"id": "3119546", "url": "https://en.wikipedia.org/wiki?curid=3119546", "title": "Subclass reachability", "text": "Subclass reachability\n\nIn computational learning theory in mathematics, given a class of concepts C, a subclass D is reachable if there exists a partial approximation S of some concept such that D contains exactly those concepts in C that are extensions to S (i.e., D=C|S).\n"}
{"id": "45351614", "url": "https://en.wikipedia.org/wiki?curid=45351614", "title": "Summa de arithmetica", "text": "Summa de arithmetica\n\nSumma de arithmetica, geometria, proportioni et proportionalita (\"Summary of arithmetic, geometry, proportions and proportionality\") is a book on mathematics written by Luca Pacioli and first published in 1494. It contains a comprehensive summary of Renaissance mathematics, including practical arithmetic, basic algebra, basic geometry and accounting, written in Italian for use as a textbook.\n\nThe \"Summa\" is the first printed work on algebra in a vernacular language, and it contains the first published description of the double-entry bookkeeping system. It set a new standard for writing and argumentation about algebra, and its impact upon the subsequent development and standardization of professional accounting methods was so great that Pacioli is sometimes referred to as the \"father of accounting.\"\n\nThe \"Summa de arithmetica\" as originally printed consists of ten chapters on a series of mathematical topics, collectively covering essentially all of Renaissance mathematics. The first seven chapters form a summary of arithmetic in 222 pages. The eighth chapter explains contemporary algebra in 78 pages. The ninth chapter discusses various topics relevant to business and trade, including barter, bills of exchange, weights and measures and bookkeeping, in 150 pages. The tenth and final chapter describes practical geometry (including basic trigonometry) in 151 pages. The work was dedicated to Guidobaldo da Montefeltro, Duke of Urbino, a patron of the arts whom Pacioli had met in Rome some years earlier.\n\nThe book's mathematical content draws heavily on the traditions of the abacus schools of contemporary northern Italy, where the children of merchants and the middle class studied arithmetic on the model established by Fibonacci's \"Liber Abaci\". The emphasis of this tradition was on facility with computation, using the Hindu–Arabic numeral system, developed through exposure to numerous example problems and case studies drawn principally from business and trade. Pacioli's work likewise teaches through examples, but it also develops arguments for the validity of its solutions through reference to general principles, axioms and logical proof. In this way the \"Summa\" begins to reintegrate the logical methods of classical Greek geometry into the medieval discipline of algebra.\n\nWithin the chapter on business, a section entitled \"Particularis de computis et scripturis\" (\"Details of calculation and recording\") describes the accounting methods then in use among northern-Italian merchants, including double-entry bookkeeping, trial balances, balance sheets and various other tools still employed by professional accountants. The business chapter also introduces the rule of 72 for predicting an investment's future value, anticipating the development of the logarithm by more than century.\n\nThese techniques did not originate with Pacioli, who merely recorded and explained the established best practices of contemporary businesspeople in his region; still, the \"Summa\"'s role in standardizing and disseminating professional bookkeeping methods has earned Pacioli a reputation as the \"father of accounting.\" \n\n\"Summa de arithmetica\" was composed over a period of decades through Pacioli's work as a professor of mathematics, and was probably intended as a textbook and reference work for students of mathematics and business, especially among the mercantile middle class of northern Italy. The book was originally published in Venice in 1494 by Paganino Paganini, with an identical second edition printed in 1523 in Toscolano.\n\nWhile the \"Summa\" contained little or no original mathematical work by Pacioli, it was the most comprehensive mathematical text ever published at the time. Its thoroughness and clarity (and the lack of any other similar work available in print) made it a basic point of reference for European mathematicians through the sixteenth century and beyond. The reputation the \"Summa\" earned Pacioli as a mathematician and intellectual inspired Ludovico Sforza, Duke of Milan, to invite him to serve as a mathematical lecturer in the ducal court, where Pacioli befriended and collaborated with Leonardo da Vinci.\n\nThe book also marks the beginning of a movement in sixteenth-century algebra toward the use of logical argumentation and theorems in the study of algebra, following the model of classical Greek geometry established by Euclid. It includes the first printed example of a set of plus and minus signs that were to become standard in Italian Renaissance mathematics: 'p' with a tilde above (p̄) for \"plus\" and 'm' with a tilde (m̄) for minus. Pacioli's (incorrect) assertion in the \"Summa\" that there was no general solution to cubic equations helped to popularize the problem among contemporary mathematicians, contributing to its eventual solution by Scipione del Ferro.\n\nIn 1994 Italy issued a 750-lira postage stamp honoring the 500th anniversary of the \"Summa\"'s publication, depicting Pacioli surrounded by mathematical and geometric implements. The image on the stamp was inspired by the \"Portrait of Luca Pacioli\", and contains many of the same elements.\n\n\n"}
{"id": "222434", "url": "https://en.wikipedia.org/wiki?curid=222434", "title": "Table of divisors", "text": "Table of divisors\n\nThe tables below list all of the divisors of the numbers 1 to 1000.\n\nA divisor of an integer \"n\" is an integer \"m\", for which \"n\"/\"m\" is again an integer (which is necessarily also a divisor of \"n\"). For example, 3 is a divisor of 21, since 21/7 = 3 (and 7 is also a divisor of 21).\n\nIf \"m\" is a divisor of \"n\" then so is −\"m\". The tables below only list positive divisors.\n\n"}
{"id": "29759120", "url": "https://en.wikipedia.org/wiki?curid=29759120", "title": "Uniqueness theorem", "text": "Uniqueness theorem\n\nIn mathematics, a uniqueness theorem is a theorem proving that certain conditions determine a unique solution. Examples of uniqueness theorems include:\n\nA theorem, also called a unicity theorem, stating the uniqueness of a mathematical object, which usually means that there is only one object fulfilling given properties, or that all objects of a given class are equivalent (i.e., they can be represented by the same model). This is often expressed by saying that the object is uniquely determined by a certain set of data. The word unique is sometimes replaced by essentially unique, whenever one wants to stress that the uniqueness is only referred to the underlying structure, whereas the form may vary in all ways that do not affect the mathematical content.\n\nA uniqueness theorem / proof is, at least within mathematics of differential equations, often combined with an existence theorem / proof to a combined existence and uniqueness theorem.\n\n"}
{"id": "54863674", "url": "https://en.wikipedia.org/wiki?curid=54863674", "title": "Van Genuchten–Gupta model", "text": "Van Genuchten–Gupta model\n\nThe van Genuchten–Gupta model is an inverted S-curve applicable to crop yield and soil salinity relations.\n\nThe mathematical expression is:\n\nwhere Y = yield, Ym = maximum yield of the model, C = salt concentration of the soil, C = C value at 50% yield, P = an exponent to be found by optimization and maximizing the model's goodness of fit to the data.\n\nIn the figure: Ym = 3.1, C = 12.4, P = 3.75\n\nAs an alternative, the logistic S-function can be used.\n\nThe mathematical expression is:\n\nwhere:\n\nwith Y =Yield, Yn = minimum Y, Ym = maximum Y, X = salt concentration of the soil, while A, B and C are constants to be determined by optimization and maximizing the model's goodness of fit to the data.\n\nIf the minimum Yn=0 then the expression can be simplified to:\n\nIn the figure: Ym = 3.43, Yn = 0.47, A = 0.112, B = -3.16, C = 1.42.\n\nThe third degree or cubic regression also offers a useful alternative.\n\nThe equation reads:\n\nwith Y =Yield, X = salt concentration of the soil, while A, B, C and D are constants to be determined by the regression.\n\nIn the figure: A = 0.0017, B = 0.0604, C=0.3874, D = 2.3788. These values were calculated with Microsoft Excel\n\nThe curvature is more pronounced than in the other models.\n\n"}
{"id": "42596301", "url": "https://en.wikipedia.org/wiki?curid=42596301", "title": "WRF-SFIRE", "text": "WRF-SFIRE\n\nWRF-SFIRE is a coupled atmosphere-wildfire model, which combines the Weather Research and Forecasting Model (WRF) with a fire-spread model, implemented by the level-set method. A version from 2010 was released based on the WRF 3.2 as WRF-Fire.\n\n\n"}
{"id": "296000", "url": "https://en.wikipedia.org/wiki?curid=296000", "title": "Without loss of generality", "text": "Without loss of generality\n\nWithout loss of generality (often abbreviated to WOLOG, WLOG or w.l.o.g.; less commonly stated as without any loss of generality or with no loss of generality) is a frequently used expression in mathematics. The term is used before an assumption in a proof which narrows the premise to some special case; it implies that the proof for that case can be easily applied to all others, or that all other cases are equivalent or similar. Thus, given a proof of the conclusion in the special case, it is trivial to adapt it to prove the conclusion in all other cases.\n\nThis is often enabled by the presence of symmetry. For example, if some property \"P\"(\"x\",\"y\") of real numbers is known to be symmetrical in \"x\" and \"y\", namely that \"P\"(\"x\",\"y\") is equivalent to \"P\"(\"y\",\"x\"), then in proving that \"P\"(\"x\",\"y\") holds for every \"x\" and \"y\", we may assume \"without loss of generality\" that \"x\" ≤ \"y\". There is then no loss of generality in that assumption: once the case \"x\" ≤ \"y\" ⇒ \"P\"(\"x\",\"y\") has been proved, the other case follows by \"y\" ≤ \"x\" ⇒ \"P\"(\"y\",\"x\") ⇒ \"P\"(\"x\",\"y\"); hence, \"P\"(\"x\",\"y\") holds in all cases.\n\nConsider the following theorem (which is a case of the pigeonhole principle):\n\nA proof: \nThis works because exactly the same reasoning (with \"red\" and \"blue\" interchanged) could be applied if the alternative assumption were made, namely that the first object is blue.\n\n\n"}
