{"id": "641270", "url": "https://en.wikipedia.org/wiki?curid=641270", "title": "A Course of Pure Mathematics", "text": "A Course of Pure Mathematics\n\nA Course of Pure Mathematics is a classic textbook in introductory mathematical analysis, written by G. H. Hardy. It is recommended for people studying calculus. First published in 1908, it went through ten editions (up to 1952) and several reprints. It is now out of copyright in UK and is downloadable from various internet web sites. It remains one of the most popular books on pure mathematics.\n\nThe book contains a large number of descriptive and study materials together with a number of difficult problems with regards to number theory analysis. The book is organized into the following chapters, with each chapter further divided.\n\nI. REAL VARIABLES\n\nII. FUNCTIONS OF REAL VARIABLES\n\nIII COMPLEX NUMBERS\n\nIV LIMITS OF FUNCTIONS OF A POSITIVE INTEGRAL VARIABLE\n\nV LIMITS OF FUNCTIONS OF A CONTINUOUS VARIABLE. CONTINUOUS AND DISCONTINUOUS FUNCTIONS\n\nVI DERIVATIVES AND INTEGRALS\n\nVII ADDITIONAL THEOREMS IN THE DIFFERENTIAL AND INTEGRAL CALCULUS\n\nVIII THE CONVERGENCE OF INFINITE SERIES AND INFINITE INTEGRALS\n\nIX THE LOGARITHMIC, EXPONENTIAL AND CIRCULAR FUNCTIONS OF A REAL VARIABLE\n\nX THE GENERAL THEORY OF THE LOGARITHMIC, EXPONENTIAL AND CIRCULAR FUNCTIONS\n\nAppendices\n\nINDEX\n\nThe book was intended to help reform mathematics teaching in the UK, and more specifically in the University of Cambridge and in schools preparing to study higher mathematics. It was aimed directly at \"scholarship level\" students – the top 10% to 20% by ability. Hardy himself did not originally find a passion for mathematics, only seeing it as a way to beat other students, which he did decisively, and gain scholarships. However, his book excels in effectively explaining analytical number theory and calculus following the rigor of mathematics.\n\nWhilst his book changed the way the subject was taught at university, the content reflects the era in which the book was written. The whole book explores number theory and the author constructs real numbers theoretically. It adequately deals with single-variable calculus, sequences, number series, properties of cos, sin, log, etc. but does not refer to mathematical groups, multi-variable functions or vector calculus. Each section includes some demanding problems. Hardy combines the enthusiasm of the missionary with the rigor of the purist in his exposition of the fundamental ideas of the differential and integral calculus, of the properties of infinite series and of other topics involving the notion of limit. Hardy's presentation of mathematical analysis is as valid today as when first written: students will find that his economical and energetic style of presentation is one that modern authors rarely come close to. Despite its limitations, it is considered a classic in its field. It is probably of most use to 1st year university students of pure mathematics.\n\n\n"}
{"id": "1698066", "url": "https://en.wikipedia.org/wiki?curid=1698066", "title": "Abuse of notation", "text": "Abuse of notation\n\nIn mathematics, abuse of notation occurs when an author uses a mathematical notation in a way that is not formally correct but that seems likely to simplify the exposition or suggest the correct intuition (while being unlikely to introduce errors or cause confusion). However, the concept of formal correctness depends on time and on the context. Therefore, many notations in mathematics are qualified as abuse of notation in some context and are formally correct in other contexts; as many notations were introduced a long time before any formalization of the theory in which they are used, the qualification of abuse of notation is strongly time dependent. Moreover, many abuses of notation may be made formally correct by improving the theory. \"Abuse of notation\" should be contrasted with \"misuse\" of notation, which should be avoided.\n\nMany mathematical objects consist of a set, often called the underlying set, equipped with some additional structure, typically a mathematical operation or a topology. It is a common abuse of notation to use the same notation for the underlying set and the structured object. For example, formula_2 may denote the set of the integers, the group of integers together with addition, or the ring of integers with addition and multiplication. In general, there is no problem with this, and avoiding such an abuse of notation would make mathematical texts pedantic and difficult to read. When this abuse of notation may be confusing, one may distinguish between these structures by denoting formula_3 the group of integers with addition, and formula_4 the ring of integers.\n\nSimilarly, a topological space consists of a set (the underlying set) and a topology formula_5 which is characterized by a set of subsets of (the open sets). Most frequently, one considers only one topology on , and there is no problem to denote by both the underlying set, and the pair consisting of and its topology formula_5 although they are different mathematical objects. Nevertheless, it occurs sometimes that two different topologies are considered simultaneously on the same set; for distinguishing the corresponding topological spaces, one must use notation such as formula_7 and formula_8\n\nOne encounters, in many textbooks, sentences such as \"Let be a function ...\". This is an abuse of notation, as the name of the function is , and denotes normally the value of the function for the element of its domain. The correct phrase would be \"Let be a function of the variable ...\" or \"Let be a function ...\" This abuse of notation is widely used, as it simplifies the formulation, and the systematic use of a correct notation quickly becomes pedantic.\n\nA similar abuse of notation occurs in sentences such as \"Let us consider the function ...\" In fact is not a function. The function is the operation that associates to , often denoted as . Nevertheless, this abuse of notation is widely used since it is generally not confusing.\n\nMany mathematical structures are defined through a characterizing property (often a universal property). Once this desired property is defined, there may be various ways to construct the structure, and the corresponding results are formally different objects, but which have exactly the same properties – they are isomorphic. As there is no way to distinguish these isomorphic objects through their properties, it is standard to consider them as equal, even if this is formally wrong.\n\nOne example of this the Cartesian product, which is often seen as associative:\n\nBut this is not strictly true: if formula_10, formula_11 and formula_12, the identity formula_13 would imply that formula_14 and formula_15, and so formula_16 would mean nothing.\n\nThis notion can be made rigorous in category theory, using the idea of a natural isomorphism.\n\nAnother example occurs in such statements as \"there are two non-Abelian groups of order 8\", which more strictly stated means \"there are two isomorphism classes of non-Abelian groups of order 8\".\n\nReferring to an equivalence class of an equivalence relation by \"x\" instead of [\"x\"] is an abuse of notation. Formally, if a set \"X\" is partitioned by an equivalence relation ~, then for each \"x\" ∈ \"X\", the equivalence class {\"y\" ∈ \"X\" | \"y\" ~ \"x\"} is denoted [\"x\"]. But in practice, if the remainder of the discussion is focused on equivalence classes rather than individual elements of the underlying set, it is common to drop the square brackets in the discussion.\n\nFor example, in modular arithmetic, a finite group of order \"n\" can be formed by partitioning the integers via the equivalence relation \"x\" ~ \"y\" if and only if \"x\" ≡ \"y\" (mod \"n\"). The elements of that group would then be [0], [1], …, [\"n\" − 1], but in practice they are usually just denoted 0, 1, …, \"n\" − 1.\n\nAnother example is the space of (classes of) measurable functions over a measure space, or classes of Lebesgue integrable functions, where the equivalence relation is equality \"almost everywhere\".\n\nThe terms \"abuse of language\" and \"abuse of notation\" depend on context. Writing \"\"f\": \"A\" → \"B\"\" for a partial function from \"A\" to \"B\" is almost always an abuse of notation, but not in a category theoretic context, where \"f\" can be seen as a morphism in the category of sets and partial functions.\n\n"}
{"id": "12137424", "url": "https://en.wikipedia.org/wiki?curid=12137424", "title": "Akamai Foundation", "text": "Akamai Foundation\n\nThe Akamai Foundation is a private corporate foundation dedicated to fostering excellence in mathematics, with the aim of promoting math’s importance and encouraging America’s next generation of technology innovators.\n\nThe Akamai Foundation is a core tenet of Akamai’s overarching commitment to corporate responsibility. The company has a long history of supporting programs designed to attract more diversity to the technology industry through initiatives such as Akamai Technical Academy and Girls Who Code; providing disaster relief and humanitarian aid globally; enabling volunteerism by connecting employees to the communities in which Akamai operates; and promoting environmental sustainability through investments in alternative energy.\n"}
{"id": "53394437", "url": "https://en.wikipedia.org/wiki?curid=53394437", "title": "Bayesian Program Synthesis", "text": "Bayesian Program Synthesis\n\nIn machine learning, Bayesian Program Synthesis (BPS), Bayesian Programs write (synthesize) new Bayesian programs. This is in contrast to the field of probabilistic programs where humans write new probabilistic (Bayesian) programs.\n\nBayesian probabilities is a strategy to learn distributions over Bayesian programs.\n\nBayesian Program Synthesis can be compared to the work on Bayesian Program Learning by Lake, Salakhutdinov, and Tenenbaum's, where probabilistic program components were hand-written, pre-trained on data, and then hand assembled in order to recognize handwritten characters.\n\nBayesian Program Synthesis (BPS) has been described as a framework related to and utilizing probabilistic programming. In BPS, probabilistic programs are generated that are themselves priors over a space of probabilistic programs. This strategy allows more automatic synthesis of new programs via inference and is achieved by the composition of modular component programs. \n\nThe modularity in BPS allows inference to work on and test smaller probabilistic programs before being integrated into a larger model.\n\nBayesian methods and models are frequently used to incorporate prior knowledge. When good prior knowledge can be incorporated into a Bayesian model, effective inference can often be performed with much less data.\n\nThis framework can be also be contrasted with the family of automated program synthesis fields, including program synthesis, programming by example, and programming by demonstration. The goal in such fields is to find the best program that satisfies some constraint. In program synthesis, for instance, verification of logical constraints reduce the state space of possible programs, allowing more efficient search to find an optimal program. Bayesian Program Synthesis differs both in that the constraints are probabilistic and the output is itself a distribution over programs that can be further refined.\n\n\n"}
{"id": "33153333", "url": "https://en.wikipedia.org/wiki?curid=33153333", "title": "Blackwell–Tapia prize", "text": "Blackwell–Tapia prize\n\nThe Blackwell–Tapia Prize is a mathematics award presented every other year at the Blackwell-Tapia conference.\nThe conference is sponsored by the National Science Foundation and is organized and hosted by various mathematics institutes.\nThe prize is named for David Blackwell and Richard Tapia.\nIt recognizes someone who has made significant research contributions in their field, and who has worked to address the problem of under-representation of minority groups in mathematics.\n\nThe following mathematicians have been honored with the Blackwell–Tapia Prize:\n"}
{"id": "23335649", "url": "https://en.wikipedia.org/wiki?curid=23335649", "title": "Canonical map", "text": "Canonical map\n\nIn mathematics, a canonical map, also called a natural map, is a map or morphism between objects that arises naturally from the definition or the construction of the objects being mapped against each other. In general it is the map which preserves the widest amount of structure, and it tends to be unique. In the rare cases where latitude in choice remains, the map is either conventionally agreed upon to be the most useful for further analysis, or sometimes simply the most elegant or beautiful known.\n\nA closely related notion is a structure map or structure morphism; the map that comes with the given structure on the object. They are also sometimes called canonical maps.\n\nA canonical isomorphism is a canonical map that is also an isomorphism (i.e., invertible).\n\nIn some contexts, it is necessary to address an issue of \"choices\" of canonical maps or canonical isomorphisms; see prestack for a typical example.\n\n"}
{"id": "56260115", "url": "https://en.wikipedia.org/wiki?curid=56260115", "title": "Clearing denominators", "text": "Clearing denominators\n\nIn mathematics, the method of clearing denominators, also called clearing fractions, is a technique for simplifying an equation equating two expressions that each are a sum of rational expressions – which includes simple fractions.\n\nConsider the equation\n\nThe smallest common multiple of the two denominators 6 and 15\"z\" is 30\"z\", so one multiplies both sides by 30\"z\":\n\nThe result is an equation with no fractions.\n\nThe simplified equation is not entirely equivalent to the original. For when we substitute and in the last equation, both sides simplify to 0, so we get , a mathematical truth. But the same substitution applied to the original equation results in , which is mathematically meaningless.\n\nWithout loss of generality, we may assume that the right-hand side of the equation is 0, since an equation may equivalently be rewritten in the form .\n\nSo let the equation have the form\n\nThe first step is to determine a common denominator of these fractions – preferably the least common denominator, which is the least common multiple of the .\n\nThis means that each is a factor of , so for some expression that is not a fraction. Then\n\nprovided that does not assume the value 0 – in which case also equals 0.\n\nSo we have now\n\nProvided that does not assume the value 0, the latter equation is equivalent with\n\nin which the denominators have vanished.\n\nAs shown by the provisos, care has to be taken not to introduce zeros of – viewed as a function of the unknowns of the equation – as spurious solutions.\n\nConsider the equation\n\nThe least common denominator is .\n\nFollowing the method as described above results in\n\nSimplifying this further gives us the solution .\n\nIt is easily checked that none of the zeros of – namely , , and – is a solution of the final equation, so no spurious solutions were introduced.\n"}
{"id": "138484", "url": "https://en.wikipedia.org/wiki?curid=138484", "title": "Commutative diagram", "text": "Commutative diagram\n\nIn mathematics, and especially in category theory, a commutative diagram is a diagram such that all directed paths in the diagram with the same start and endpoints lead to the same result. Commutative diagrams play the role in category theory that equations play in algebra (see Barr–Wells, Section 1.7).\nParts of the diagram:\nIn algebra texts, the type of morphism can be denoted with different arrow usages: \n\nThese conventions are common enough that texts often do not explain the meanings of the different types of arrow.\n\nCommutativity makes sense for a polygon of any finite number of sides (including just 1 or 2), and a diagram is commutative if every polygonal subdiagram is commutative.\n\nNote that a diagram may be non-commutative, i.e., the composition of different paths in the diagram may not give the same result.\n\nPhrases like \"this commutative diagram\" or \"the diagram commutes\" may be used.\n\nIn the bottom-left diagram, which expresses the first isomorphism theorem, commutativity means that formula_7 while in the bottom-right diagram, commutativity of the square means formula_8:\nFor the diagram below to commute, we must have the three equalities: (1) formula_9 (2) formula_10 and (3) formula_11. \nSince the first equality follows from the last two, for the diagram to commute it suffices to show (2) and (3). However, since equality (3) does not generally follow from the other two equalities, for this diagram to commute it is generally not enough to only have equalities (1) and (2).\n\nDiagram chasing (also called diagrammatic search) is a method of mathematical proof used especially in homological algebra. Given a commutative diagram, a proof by diagram chasing involves the formal use of the properties of the diagram, such as injective or surjective maps, or exact sequences. A syllogism is constructed, for which the graphical display of the diagram is just a visual aid. It follows that one ends up \"chasing\" elements around the diagram, until the desired element or result is constructed or verified.\n\nExamples of proofs by diagram chasing include those typically given for the five lemma, the snake lemma, the zig-zag lemma, and the nine lemma.\n\nIn higher category theory, one considers not only objects and arrows, but arrows between the arrows, arrows between arrows between arrows, and so on ad infinitum. For example, the category of small categories Cat is naturally a 2-category, with functors as its arrows and natural transformations as the arrows between functors. In this setting, commutative diagrams may include these higher arrows as well, which are often depicted in the following style: formula_12. For example, the following (somewhat trivial) diagram depicts two categories ' and ', together with two functors , : ' → ' and a natural transformation : ⇒ :\n\nThere are two kinds of composition in a 2-category (called vertical composition and horizontal composition), and they may also be depicted via pasting diagrams, see 2-category#Definition for examples.\n\nA commutative diagram in a category \"C\" can be interpreted as a functor from an index category \"J\" to \"C;\" one calls the functor a diagram.\n\nMore formally, a commutative diagram is a visualization of a diagram indexed by a poset category:\n\nConversely, given a commutative diagram, it defines a poset category:\n\nHowever, not every diagram commutes (the notion of diagram strictly generalizes commutative diagram): most simply, the diagram of a single object with an endomorphism (formula_13), or with two parallel arrows (formula_14, that is, formula_15, sometimes called the free quiver), as used in the definition of equalizer need not commute. Further, diagrams may be messy or impossible to draw when the number of objects or morphisms is large (or even infinite).\n\n\n\n"}
{"id": "6626644", "url": "https://en.wikipedia.org/wiki?curid=6626644", "title": "Comparison theorem", "text": "Comparison theorem\n\nA comparison theorem is any of a variety of theorems that compare properties of various mathematical objects.\n\nIn the theory of differential equations, comparison theorems assert particular properties of solutions of a differential equation (or of a system thereof) provided that an auxiliary equation/inequality (or a system thereof) possesses a certain property. See also Lyapunov comparison principle\n\n\nIn Riemannian geometry it is a traditional name for a number of theorems that compare various metrics and provide various estimates in Riemannian geometry. \n\n\n"}
{"id": "3478709", "url": "https://en.wikipedia.org/wiki?curid=3478709", "title": "Ethnocomputing", "text": "Ethnocomputing\n\nEthnocomputing is the study of the interactions between computing and culture. It is carried out through theoretical analysis, empirical investigation, and design implementation. It includes research on the impact of computing on society, as well as the reverse: how cultural, historical, personal, and societal origins and surroundings cause and affect the innovation, development, diffusion, maintenance, and appropriation of computational artifacts or ideas. From the ethnocomputing perspective, no computational technology is culturally \"neutral,\" and no cultural practice is a computational void. Instead of considering culture to be a hindrance for software engineering, culture should be seen as a resource for innovation and design.\n\nSocial categories for ethnocomputing include:\n\nTechnical categories in ethnocomputing include:\n\nEthnocomputing has its origins in ethnomathematics. There are a large number of studies in ethnomathematics that could be considered ethnocomputing as well (e.g., Eglash (1999) and Ascher & Ascher (1981)). The idea of a separate field was introduced in 1992 by Anthony Petrillo in \"Responsive Evaluation of Mathematics Education in a Community of Jos, Nigeria,\" Dissertation (Ph.D.): State University of New York at Buffalo, which Petrillo elaborated a bit more on in March 1994, \"Ethnocomputers in Nigerian Computer Education,\" paper presented at the 31st Annual Conference of the Mathematical Association of Nigeria. Just like computer science is nowadays considered to be a field of research distinct from mathematics, ethnocomputing is considered to be a research topic distinct from ethnomathematics. Some aspects of ethnocomputing that have their roots in ethnomathematics are listed below:\n\n\n\n"}
{"id": "46593443", "url": "https://en.wikipedia.org/wiki?curid=46593443", "title": "Field, power, and root-power quantities", "text": "Field, power, and root-power quantities\n\nA power quantity is a power or a quantity directly proportional to power, e.g., energy density, acoustic intensity, and luminous intensity. Energy quantities may also be labelled as power quantities in this context.\n\nA root-power quantity is a quantity such as voltage, current, sound pressure, electric field strength, speed, or charge density, the square of which, in linear systems, is proportional to power. The term \"root-power quantity\" was introduced in the ; it replaces and deprecates the term field quantity.\n\nIt is essential to know which category a measurement belongs to when using decibels (dB) for comparing the levels of such quantities. A change of one bel in the level corresponds to a 10× change in \"power\", so when comparing power quantities \"x\" and \"y\", the difference is defined to be 10×log(\"y\"/\"x\") decibel. With root-power quantities, however the difference is defined as 20×log(\"y\"/\"x\") dB. In linear systems, these definitions allow the distinction between root-power quantities and power quantities to be ignored when specifying changes as levels: an amplifier can be described as having \"3 dB\" of gain without needing to specify whether voltage or power are being compared; for a given linear load (e.g. an speaker), such an increase will result in a 3 dB increase in both the sound pressure level and the sound power level at a given location near the speaker. Conversely, when ratios cannot be identified as either power or root-power quantities, the units neper (Np) and decibel (dB) cannot be sensibly used.\n\nIn the analysis of signals and systems using sinusoids, field quantities and root-power quantities may be complex-valued.\n\nIn justifying the deprecation of the term \"field quantity\" and instead using \"root-power quantity\" in the context of levels, ISO 80000 draws attention to the conflicting use of the former term to mean a quantity that depends on the position, which in physics is called a \"field\". Such a field is often called a \"field quantity\" in the literature, but is called a \"field\" here for clarity. Several types of field (such as the electromagnetic field) meet the definition of a root-power quantity, whereas others (such as the Poynting vector and temperature) do not. Conversely, not every root-power quantity is a field (such as the voltage on a loudspeaker).\n\n"}
{"id": "27169542", "url": "https://en.wikipedia.org/wiki?curid=27169542", "title": "Ganita-yukti-bhasa", "text": "Ganita-yukti-bhasa\n\nGanita-yukti-bhasa (also written as Ganita Yuktibhasa) is either the title or a part of the title of three different books:\n\n\nThis edition of Yuktibhasa has been divided into two volumes, even though the original Malayalam text has no such division. Volume I deals with mathematics and Volume II treats astronomy. Each volume is divided into three parts: First part is an English translation of the relevant Malayalam part of Yuktibhasa, second part contains detailed explanatory notes on the translation, and in the third part the text in the Malayalam original is reproduced. The English translation is by K.V. Sarma and the explanatory notes are provided by Ramasubramanian, K., Srinivas, M.D. and Sriram, M.S..\n\nVolume I dealing with mathematics is divided into seven chapters. The topics discussed are the eight mathematical operations, a certain set of ten problems, arithmetic of fractions, rule of three, Kuttakara (linear indeterminate equations), infinite series and approximations for the ratio of the circumference and diameter of a circle and infinite series and approximations for sines.\n\nVolume II dealing with astronomy is divided into eight chapters. The topics covered are computation of mean and true longitudes of planets, Earth and celestial spheres, fifteen problems relating to ascension, declination, longitude, etc., determination of time, place, direction, etc., from gnomonic shadow, eclipses, Vyatipata (when the sun and moon have the same declination), visibility correction for planets and phases of the moon.\n\n"}
{"id": "7392980", "url": "https://en.wikipedia.org/wiki?curid=7392980", "title": "Graduate Texts in Mathematics", "text": "Graduate Texts in Mathematics\n\nGraduate Texts in Mathematics (GTM) (ISSN 0072-5285) is a series of graduate-level textbooks in mathematics published by Springer-Verlag. The books in this series, like the other Springer-Verlag mathematics series, are yellow books of a standard size (with variable numbers of pages). The GTM series is easily identified by a white band at the top of the book.\n\nThe books in this series tend to be written at a more advanced level than the similar Undergraduate Texts in Mathematics series, although there is a fair amount of overlap between the two series in terms of material covered and difficulty level.\n\n\n\n"}
{"id": "6966428", "url": "https://en.wikipedia.org/wiki?curid=6966428", "title": "Hatch mark", "text": "Hatch mark\n\nHatch marks (also called hash marks or tick marks) are a form of mathematical notation. They are used in three ways: \n\nHatch marks are frequently used as an abbreviation of some common units of measurement. In regard to distance, a single hatch mark indicates feet, and two hatch marks indicate inches. In regard to time, a single hatch mark indicates minutes, and two hatch marks indicate seconds.\nIn geometry and trigonometry, such marks are used following an elevated circle to indicate degrees, minutes, and seconds — ( ° ) ( ′ ) ( ″ ).\n\nHatch marks can probably be traced to hatching in art works, where the pattern of the hatch marks represents a unique tone or hue. Different patterns indicate different tones.\n\nUnit-and-value hatch marks are short vertical line segments which mark distances. They are seen on rulers and number lines. The marks are parallel to each other and are evenly spaced.\n\nThe distance between adjacent marks is one unit. Longer line segments are used for integers and natural numbers. Shorter line segments are used for fractions.\n\nHatch marks provide a visual clue as to the value of specific points on the number line, even if some hatch marks are not labeled with a number.\n\nHatch marks are typically seen in number theory and geometry.\n\nIn geometry, hatch marks are used to denote equal measures of angles, arcs, line segments, or other elements. \n\nHatch marks for congruency notation are in the style of tally marks or of Roman numerals – with some qualifications. These marks are without serifs, and some patterns are not used. For example, the numbers I, II, III, V, and X are used, but IV and VI are not used since a rotation of 180 degrees can make a 4 easily confused with a 6.\n\nFor example, if two triangles are drawn, the first pair of congruent sides can be marked with a single hatch mark on each. The second pair of congruent sides can be marked with two hatch marks each. The patterns are not alike: one pair uses one mark while the other pair uses two marks (Figure 1). This use of pattern makes it clear which sides are the same length even if the sides cannot be measured. If the sides do not appear to be congruent, as long as hatch marks are present and are the same number of hatch marks then the sides are congruent.\n\nNote that the inverse situation should not be assumed. That is, while sides that are hatch marked identically must be assumed to be congruent, it does not follow that sides hatch marked \"differently\" must be \"incongruent\". The different hatch marks simply signal that the lengths may be taken to be independent of each other. So, for example, while the triangles (Figure 1) cannot be shown to be isosceles triangles or equilateral triangles, nevertheless the possibility that they \"could\" be either of those things is not precluded based on what is shown.\n\nLine charts may sometimes use hatch marks as graphed points. In the early days of computers, monitors and printers could only make charts using the characters available on a common typewriter. To graph a line chart of sales over time, symbols such as *, x, or | were used to mark points and various characters were used to mark the lines connecting them. While computers have advanced considerably, it is still not unusual to see x or | used as the points of interest or points of change on a graph.\n\n\n"}
{"id": "48633254", "url": "https://en.wikipedia.org/wiki?curid=48633254", "title": "History of arithmetic", "text": "History of arithmetic\n\nThe history of arithmetic includes the period from the emergence of counting before the formal definition of numbers and arithmetic operations over them by means of a system of axioms. Arithmetic — the science of numbers, their properties and their relations — is one of the main mathematical sciences. It is closely connected with algebra and the theory of numbers.\n\nThe practical need for counting, elementary measurements and calculations became the reason for the emergence of arithmetic. The first authentic data on arithmetic knowledge are found in the historical monuments of Babylon and Ancient Egypt in the third and second millennia BC. The big contribution to the development of arithmetic was made by the ancient Greek mathematicians, in particular Pythagoreans, who tried to define all regularities of the world in terms of numbers. In the Middle Ages trade and approximate calculations were the main scope of arithmetic. Arithmetic developed first of all in India and the countries of Islam and only then came to Western Europe. In the seventeenth century the needs of astronomy, mechanics, and more difficult commercial calculations put before arithmetic new challenges regarding methods of calculation and gave an impetus to further development.\n\nTheoretical justifications of the idea of number are connected first of all with the definition of \"natural number\" and Peano's axioms formulated in 1889. They were followed by strict definitions of rational, real, negative and complex numbers. Further expansion of the concept of number is possible only if one of the arithmetic laws is rejected.\n\nIf in two sets of subjects each element of one set has only one corresponding element in the other set, these sets are one-to-one. Such actual comparison when subjects were displayed in two ranks, was used by primitive tribes in trade. This approach gives the opportunity to establish quantitative ratios between groups of objects and doesn't demand the concept of number.\n\nFurther there were natural standards for counting, for example, fingers of hands, and then sets of standards, such as hands. The advent of the standards symbolizing concrete numbers also is connected to the emergence of the concept of number. Thus the number of things to be counted was compared to the Moon in the sky, the number of eyes, and the number of fingers on a hand. Later numerous standards were replaced with one of the most convenient, usually fingers of hands and/or feet.\n"}
{"id": "9479849", "url": "https://en.wikipedia.org/wiki?curid=9479849", "title": "History of manifolds and varieties", "text": "History of manifolds and varieties\n\nThe study of manifolds combines many important areas of mathematics: it generalizes concepts such as curves and surfaces as well as ideas from linear algebra and topology. Certain special classes of manifolds also have additional algebraic structure; they may behave like groups, for instance. In that case, they are called Lie Groups. Alternatively, they may be described by polynomial equations, in which case they are called algebraic varieties, and if they additionally carry a group structure, they are called algebraic groups.\nThe term \"manifold\" comes from German \"Mannigfaltigkeit,\" by Riemann.\n\nIn English, \"manifold\" refers to spaces with a differentiable or topological structure, \nwhile \"variety\" refers to spaces with an algebraic structure, as in algebraic varieties.\n\nIn Romance languages, manifold is translated as \"variety\" – such spaces with a differentiable structure are literally translated as \"analytic varieties\", while spaces with an algebraic structure are called \"algebraic varieties\". Thus for example, the french word \"\" means topological manifold. In the same vein, the Japanese word \" (tayōtai) also encompasses both manifold and variety. (\" (tayō) means various.)\nAncestral to the modern concept of a manifold were several important results of 18th and 19th century mathematics. The oldest of these was Non-Euclidean geometry, which considers spaces where Euclid's parallel postulate fails. Saccheri first studied this geometry in 1733. Lobachevsky, Bolyai, and Riemann developed the subject further 100 years later. Their research uncovered two types of spaces whose geometric structures differ from that of classical Euclidean space; these are called hyperbolic geometry and elliptic geometry. In the modern theory of manifolds, these notions correspond to manifolds with constant, negative and positive curvature, respectively.\n\nCarl Friedrich Gauss may have been the first to consider abstract spaces as mathematical objects in their own right. His theorema egregium gives a method for computing the curvature of a surface without considering the ambient space in which the surface lies. In modern terms, the theorem proved that the curvature of the surface is an intrinsic property. Manifold theory has come to focus exclusively on these intrinsic properties (or invariants), while largely ignoring the extrinsic properties of the ambient space.\n\nAnother, more topological example of an intrinsic property of a manifold is the Euler characteristic. For a non-intersecting graph in the Euclidean plane, with \"V\" vertices (or corners), \"E\" edges and \"F\" faces (counting the exterior) Euler showed that \"V\"-\"E\"+\"F\"= 2. Thus 2 is called the Euler characteristic of the plane. By contrast, in 1813 Antoine-Jean Lhuilier showed that the Euler characteristic of the torus is 0, since the complete graph on seven points can be embedded into the torus. The Euler characteristic of other surfaces is a useful topological invariant, which has been extended to higher dimensions using Betti numbers. In the mid nineteenth century, the Gauss–Bonnet theorem linked the Euler characteristic to the Gaussian curvature.\n\nLagrangian mechanics and Hamiltonian mechanics, when considered geometrically, are naturally manifold theories. All these use the notion of several characteristic axes or dimensions (known as generalized coordinates in the latter two cases), but these dimensions do not lie along the physical dimensions of width, height, and breadth.\n\nIn the early 19th century the theory of elliptic functions succeeded in giving a basis for the theory of elliptic integrals, and this left open an obvious avenue of research. The standard forms for elliptic integrals involved the square roots of cubic and quartic polynomials. When those were replaced by polynomials of higher degree, say quintics, what would happen? \n\nIn the work of Niels Abel and Carl Jacobi, the answer was formulated: the resulting integral would involve functions of two complex variables, having four independent \"periods\" (i.e. period vectors). This gave the first glimpse of an abelian variety of dimension 2 (an abelian surface): what would now be called the \"Jacobian of a hyperelliptic curve of genus 2\".\n\nBernhard Riemann was the first to do extensive work generalizing the idea of a surface to higher dimensions. The name \"manifold\" comes from Riemann's original German term, \"Mannigfaltigkeit\", which William Kingdon Clifford translated as \"manifoldness\". In his Göttingen inaugural lecture, Riemann described the set of all possible values of a variable with certain constraints as a \"Mannigfaltigkeit\", because the variable can have \"many\" values. He distinguishes between \"stetige Mannigfaltigkeit\" and \"diskrete\" \"Mannigfaltigkeit\" (\"continuous manifoldness\" and \"discontinuous manifoldness\"), depending on whether the value changes continuously or not. As continuous examples, Riemann refers to not only colors and the locations of objects in space, but also the possible shapes of a spatial figure. Using induction, Riemann constructs an \"n-fach ausgedehnte Mannigfaltigkeit\" (\"n times extended manifoldness\" or \"n-dimensional manifoldness\") as a continuous stack of (n−1) dimensional manifoldnesses. Riemann's intuitive notion of a \"Mannigfaltigkeit\" evolved into what is today formalized as a manifold. Riemannian manifolds and Riemann surfaces are named after Bernhard Riemann.\n\nIn 1857, Riemann introduced the concept of Riemann surfaces as part of a study of the process of analytic continuation; Riemann surfaces are now recognized as one-dimensional complex manifolds. He also furthered the study of abelian and other multi-variable complex functions.\n\nJohann Benedict Listing, inventor of the word \"topology\", wrote an 1847 paper \"Vorstudien zur Topologie\" in which he defined a \"complex\". He first defined the Möbius strip in 1861 (rediscovered four years later by Möbius), as an example of a non-orientable surface.\n\nAfter Abel, Jacobi, and Riemann, some of the most important contributors to the theory of abelian functions were Weierstrass, Frobenius, Poincaré and Picard. The subject was very popular at the time, already having a large literature. By the end of the 19th century, mathematicians had begun to use geometric methods in the study of abelian functions.\n\nHenri Poincaré's 1895 paper Analysis Situs studied three-and-higher-dimensional manifolds(which he called \"varieties\"), giving rigorous definitions of homology, homotopy, and Betti numbers and raised a question, today known as the Poincaré conjecture, based his new concept of the fundamental group. In 2003, Grigori Perelman proved the conjecture using Richard S. Hamilton's Ricci flow, this is after nearly a century of effort by many mathematicians.\n\nHermann Weyl gave an intrinsic definition for differentiable manifolds in 1912. During the 1930s Hassler Whitney and others clarified the foundational aspects of the subject, and thus intuitions dating back to the latter half of the 19th century became precise, and developed through differential geometry and Lie group theory.\n\nThe Whitney embedding theorem showed that manifolds intrinsically defined by charts could always be embedded in Euclidean space, as in the extrinsic definition, showing that the two concepts of manifold were equivalent. Due to this unification, it is said to be the first complete exposition of the modern concept of manifold.\n\nEventually, in the 1920s, Lefschetz laid the basis for the study of abelian functions in terms of complex tori. He also appears to have been the first to use the name \"abelian variety\"; in Romance languages, \"variety\" was used to translate Riemann's term \"Mannigfaltigkeit\". It was Weil in the 1940s who gave this subject its modern foundations in the language of algebraic geometry.\n\n"}
{"id": "314743", "url": "https://en.wikipedia.org/wiki?curid=314743", "title": "Hume's principle", "text": "Hume's principle\n\nHume's principle or HP—the terms were coined by George Boolos—says that the number of \"F\"s is equal to the number of \"G\"s if and only if there is a one-to-one correspondence (a bijection) between the \"F\"s and the \"G\"s. HP can be stated formally in systems of second-order logic. Hume's principle is named for the Scottish philosopher David Hume.\n\nHP plays a central role in Gottlob Frege's philosophy of mathematics. Frege shows that HP and suitable definitions of arithmetical notions entail all axioms of what we now call second-order arithmetic. This result is known as Frege's theorem, which is the foundation for a philosophy of mathematics known as neo-logicism.\n\nHume's principle appears in Frege's \"Foundations of Arithmetic\" (§73), which quotes from Part III of Book I of David Hume's \"A Treatise of Human Nature\" (1740). Hume there sets out seven fundamental relations between ideas. Concerning one of these, proportion in quantity or number, Hume argues that our reasoning about proportion in quantity, as represented by geometry, can never achieve \"perfect precision and exactness\", since its principles are derived from sense-appearance. He contrasts this with reasoning about number or arithmetic, in which such a precision \"can\" be attained:\n\nAlgebra and arithmetic [are] the only sciences in which we can carry on a chain of reasoning to any degree of intricacy, and yet preserve a perfect exactness and certainty. We are possessed of a precise standard, by which we can judge of the equality and proportion of numbers; and according as they correspond or not to that standard, we determine their relations, without any possibility of error. \"When two numbers are so combined, as that the one has always a unit answering to every unit of the other, we pronounce them equal\"; and it is for want of such a standard of equality in [spatial] extension, that geometry can scarce be esteemed a perfect and infallible science. (I. III. I.)\n\nNote Hume's use of the word \"number\" in the ancient sense, to mean a set or collection of things rather than the common modern notion of \"positive integer\". The ancient Greek notion of number (\"arithmos\") is of a finite plurality composed of units. See Aristotle, \"Metaphysics\", 1020a14 and Euclid, \"Elements\", Book VII, Definition 1 and 2. The contrast between the old and modern conception of number is discussed in detail in Mayberry (2000).\n\nThe principle that cardinal number was to be characterized in terms of one-to-one correspondence had previously been used to great effect by Georg Cantor, whose writings Frege knew. The suggestion has therefore been made that Hume's principle ought better be called \"Cantor's Principle\". But Frege criticized Cantor on the ground that Cantor defines cardinal numbers in terms of ordinal numbers, whereas Frege wanted to give a characterization of cardinals that was independent of the ordinals. Cantor's point of view, however, is the one embedded in contemporary theories of transfinite numbers, as developed in axiomatic set theory.\n\n\n"}
{"id": "1248704", "url": "https://en.wikipedia.org/wiki?curid=1248704", "title": "Ideal theory", "text": "Ideal theory\n\nIn mathematics, ideal theory is the theory of ideals in commutative rings; and is the precursor name for the contemporary subject of commutative algebra. The name grew out of the central considerations, such as the Lasker–Noether theorem in algebraic geometry, and the ideal class group in algebraic number theory, of the commutative algebra of the first quarter of the twentieth century. It was used in the influential van der Waerden text on abstract algebra from around 1930.\n\nThe ideal theory in question had been based on elimination theory, but in line with David Hilbert's taste moved away from algorithmic methods. Gröbner basis theory has now reversed the trend, for computer algebra.\n\nThe importance of the ideal in general of a module, more general than an \"ideal\", probably led to the perception that \"ideal theory\" was too narrow a description. Valuation theory, too, was an important technical extension, and was used by Helmut Hasse and Oscar Zariski. Bourbaki used \"commutative algebra\"; sometimes \"local algebra\" is applied to the theory of local rings. D. G. Northcott's 1953 Cambridge Tract \"Ideal Theory\" (reissued 2004 under the same title) was one of the final appearances of the name.\n"}
{"id": "493259", "url": "https://en.wikipedia.org/wiki?curid=493259", "title": "Index of logarithm articles", "text": "Index of logarithm articles\n\nThis is a list of logarithm topics, by Wikipedia page. See also the list of exponential topics.\n"}
{"id": "37520883", "url": "https://en.wikipedia.org/wiki?curid=37520883", "title": "Left and right (algebra)", "text": "Left and right (algebra)\n\nIn algebra, the terms left and right denote the order of a binary operation (usually, but not always called \"multiplication\") in non-commutative algebraic structures.\nA binary operation ∗ is usually written in the infix form:\nThe argument  is placed on the left side, and the argument  is on the right side. Even if the symbol of the operation is omitted, the order of and does matter unless ∗ is commutative.\n\nA two-sided property is fulfilled on both sides. A one-sided property is related to one (unspecified) of two sides.\n\nAlthough terms are similar, left–right distinction in algebraic parlance is not related either to left and right limits in calculus, or to left and right in geometry.\n\nA binary operation  may be considered as a family of unary operators through currying\ndepending on  as a parameter. It is the family of \"right\" operations. Similarly,\ndefines the family of \"left\" operations parametrized with .\n\nIf for some , the left operation  is identical, then is called a left identity. Similarly, if , then is a right identity.\n\nIn ring theory, a subring which is invariant under \"any\" left multiplication in a ring, is called a left ideal. Similarly, a right multiplications-invariant subring is a right ideal.\n\nOver non-commutative rings, the left–right distinction is applied to modules, namely to specify the side where a scalar (module element) appear in the scalar multiplication. \n\nThe distinction is not purely syntactical because implies two different associativity rules (the lowest row in the table) which link multiplication in a module with multiplication in a ring.\n\nA bimodule is simultaneously a left and right module, with two \"different\" scalar multiplication operations, obeying an obvious associativity condition on them.\n\n\nIn category theory the usage of \"left\" is \"right\" has some algebraic resemblance, but refers to left and right sides of morphisms. See adjoint functors.\n\n\n"}
{"id": "4190174", "url": "https://en.wikipedia.org/wiki?curid=4190174", "title": "Limitation of size", "text": "Limitation of size\n\nIn the philosophy of mathematics, specifically the philosophical foundations of set theory, limitation of size is a concept developed by Philip Jourdain and/or Georg Cantor to avoid Cantor's paradox. It identifies certain \"inconsistent multiplicities\", in Cantor's terminology, that cannot be sets because they are \"too large\". In modern terminology these are called proper classes.\n\nThe axiom of limitation of size is an axiom in some versions of von Neumann–Bernays–Gödel set theory or Morse–Kelley set theory. This axiom says that any class which is not \"too large\" is a set, and a set cannot be \"too large\". \"Too large\" is defined as being large enough that the class of all sets can be mapped one-to-one into it.\n"}
{"id": "18568", "url": "https://en.wikipedia.org/wiki?curid=18568", "title": "List of algorithms", "text": "List of algorithms\n\nThe following is a list of algorithms along with one-line descriptions for each.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1042164", "url": "https://en.wikipedia.org/wiki?curid=1042164", "title": "List of mathematical jargon", "text": "List of mathematical jargon\n\nThe language of mathematics has a vast vocabulary of specialist and technical terms. It also has a certain amount of jargon: commonly used phrases which are part of the culture of mathematics, rather than of the subject. Jargon often appears in lectures, and sometimes in print, as informal shorthand for rigorous arguments or precise ideas. Much of this is common English, but with a specific non-obvious meaning when used in a mathematical sense.\n\nSome phrases, like \"in general\", appear below in more than one section.\n\n\n\n\n\nAlthough ultimately every mathematical argument must meet a high standard of precision, mathematicians use descriptive but informal statements to discuss recurring themes or concepts with unwieldy formal statements. Note that many of the terms are completely rigorous in context.\n\n\n\nThe formal language of proof draws repeatedly from a small pool of ideas, many of which are invoked through various lexical shorthands in practice.\n\n\nMathematicians have several phrases to describe proofs or proof techniques. These are often used as hints for filling in tedious details.\n\n\n"}
{"id": "5971813", "url": "https://en.wikipedia.org/wiki?curid=5971813", "title": "List of mathematicians (I)", "text": "List of mathematicians (I)\n\n\n\n\n"}
{"id": "5971822", "url": "https://en.wikipedia.org/wiki?curid=5971822", "title": "List of mathematicians (P)", "text": "List of mathematicians (P)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5971828", "url": "https://en.wikipedia.org/wiki?curid=5971828", "title": "List of mathematicians (R)", "text": "List of mathematicians (R)\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "50248508", "url": "https://en.wikipedia.org/wiki?curid=50248508", "title": "List of operator splitting topics", "text": "List of operator splitting topics\n\nThis is a list of operator splitting topics.\n\n"}
{"id": "348624", "url": "https://en.wikipedia.org/wiki?curid=348624", "title": "List of order theory topics", "text": "List of order theory topics\n\nOrder theory is a branch of mathematics that studies various kinds of objects (often binary relations) that capture the intuitive notion of ordering, providing a framework for saying when one thing is \"less than\" or \"precedes\" another.\n\nAn alphabetical list of many notions of order theory can be found in the order theory glossary. See also inequality, extreme value and mathematical optimization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "27096954", "url": "https://en.wikipedia.org/wiki?curid=27096954", "title": "Lobb number", "text": "Lobb number\n\nIn combinatorial mathematics, the Lobb number \"L\" counts the number of ways that \"n\" + \"m\" open parentheses and \"n\" − \"m\" close parentheses can be arranged to form the start of a valid sequence of balanced parentheses.\n\nLobb numbers form a natural generalization of the Catalan numbers, which count the number of complete strings of balanced parentheses of a given length. Thus, the \"n\"th Catalan number equals the Lobb number \"L\". They are named after Andrew Lobb, who used them to give a simple inductive proof of the formula for the \"n\" Catalan number.\n\nThe Lobb numbers are parameterized by two non-negative integers \"m\" and \"n\" with \"n\" ≥ \"m\" ≥ 0. The (\"m\", \"n\") Lobb number \"L\" is given in terms of binomial coefficients by the formula\n\nAs well as counting sequences of parentheses, the Lobb numbers also count the number of ways in which \"n\" + \"m\" copies of the value +1 and \"n\" − \"m\" copies of the value −1 may be arranged into a sequence such that all of the partial sums of the sequence are non-negative.\n"}
{"id": "57942226", "url": "https://en.wikipedia.org/wiki?curid=57942226", "title": "Loeschian number", "text": "Loeschian number\n\nLoeschian numbers take the quadratic form \"x\" + \"xy\" + \"y\" for integer \"x\", \"y\". They correspond to the norms of vectors in an A lattice.\n\n"}
{"id": "1606990", "url": "https://en.wikipedia.org/wiki?curid=1606990", "title": "Logical harmony", "text": "Logical harmony\n\nLogical harmony, a name coined by Michael Dummett, is a supposed constraint on the rules of inference that can be used in a given logical system.\n\nThe logician Gerhard Gentzen proposed that the meanings of logical connectives could be given by the rules for introducing them into discourse. For example, if one believes that \"the sky is blue\" and one also believes that \"grass is green\", then one can introduce the connective \"and\" as follows: \"The sky is blue AND grass is green.\" Gentzen's idea was that having rules like this is what gives meaning to one's words, or at least to certain words. The idea has also been associated with Wittgenstein's dictum that in many cases we can say, \"meaning is use\". Most contemporary logicians prefer to think that the introduction rules and the elimination rules for an expression are equally important. In this case, \"and\" is characterized by the following rules:\nAn apparent problem with this was pointed out by Arthur Prior: Why can't we have an expression (call it \"tonk\") whose introduction rule is that of OR (from \"p\" to \"p tonk q\") but whose elimination rule is that of AND (from \"p tonk q\" to \"q\")? This lets us deduce anything at all from any starting point. Prior suggested that this meant that inferential rules could \"not\" determine meaning. He was answered by Nuel Belnap, that even though introduction and elimination rules can constitute meaning, not just any pair of such rules will determine a meaningful expression—they must meet certain constraints, such as not allowing us to deduce any new truths in the old vocabulary. These constraints are what Dummett was referring to.\n\nHarmony, then, refers to certain constraints that a proof theory must let hold between introduction and elimination rules for it to be meaningful, or in other words, for its inference rules to be meaning-constituting.\n\nThe application of harmony to logic may be considered a special case; it makes sense to talk of harmony with respect to not only inferential systems, but also conceptual systems in human cognition, and to type systems in programming languages.\n\nSemantics of this form has not provided a very great challenge to that sketched in Tarski's semantic theory of truth, but many philosophers interested in reconstituting the semantics of logic in a way that respects Ludwig Wittgenstein's \"meaning is use\" have felt that harmony holds the key.\n\n\n"}
{"id": "51373403", "url": "https://en.wikipedia.org/wiki?curid=51373403", "title": "Louis Necker", "text": "Louis Necker\n\nLouis Necker, called de Germany (31 August 1730 in Geneva – 31 July 1804 in Cologny) was a Genevan mathematician, physicist, professor and a banker in Paris. He was the elder brother of Jacques Necker, minister of Finance in France when the French Revolution broke out. \n\nLouis Necker studied mathematics and physics at the Academy of Geneva. He finished his studies in philosophy with a thesis on electricity (1747), then graduated in law (1751). For a while he became the governor of probably Charles Christian, Prince of Nassau-Weilburg and Simon August, Count of Lippe-Detmold during their stay in Geneva and traveled with them to the University of Turin. He managed a boarding school for young English held by his father Charles Frederick, lawyer and professor of law at the Geneva Academy. He was appointed as the hofmeister of a Baron van Van Wassenaer and a Bentinck. \n\nIn 1752 he purchased 's physics laboratory and in 1757 acceded the chair of mathematics and the honorary chair of Experimental Physics of the Academy of Geneva. As a correspondent of the Académie royale des sciences he had written an article for the Encyclopedia on Friction in mechanics. \nIn 1759 he lost his wife Isabelle André, whom he had married in 1752 and came from Marseille. In 1761 he was forced to resign from his professorship after a scandal (Vernes-Necker case). \nIn 1762 with the help of his brother he was appointed in a trading house in Marseille and added to his last name de Germany, after the family estate near Rolle. He was dropped from the Académie des sciences's list of Corresponding Members in 1767.\n\nIn 1770 he moved to Paris. In 1772 he became a banker at . In 1773 he remarried. Between 1774 and 1778 he must have been very busy collecting interest for his rich and noble clients in Utrecht, the Netherlands. An astonishing number of notarial deeds are on his name. In 1776 he became resident for the Republic of Geneva, succeeding his brother. When Emmanuel Haller was appointed in the Girardot bank in 1777, Louis became a silent partner. At some time (1777?) he became a friend of Benjamin Franklin. Jacques Necker was dismissed on 19 May 1781 as controller of the royal treasury. It seems the brothers were still cooperating as Jacques and Louis received annually 8 million livres as a pension. \n\nAs a result of changes during the liberal phase of the French Revolution, he thought it prudent to return to his homeland in 1791. The disgrace of his younger brother Jacques, who resigned in 1790, contributed to his decision. The Neckers were far from welcome in Geneva. Many of the French émigrés considered them Jacobins, and many of the Swiss Jacobins thought them conservative.\n\nHis son Jacques (1757-1825), who had joined the French army, married Albertine Necker de Saussure in 1785. The French Revolution ended his military career. In 1790, he began teaching as a demonstrator in botany at the Academy of Geneva as Professor of Botany.\n\n\n"}
{"id": "27606747", "url": "https://en.wikipedia.org/wiki?curid=27606747", "title": "Mathematical Methods in the Physical Sciences", "text": "Mathematical Methods in the Physical Sciences\n\nMathematical Methods in the Physical Sciences is a 1966 textbook by mathematician Mary L. Boas intended to develop skills in mathematical problem solving needed for junior to senior-graduate courses in engineering, physics, and chemistry. The book provides a comprehensive survey of analytic techniques and provides careful statements of important theorems while omitting most detailed proofs. Each section contains a large number of problems, with selected answers. Numerical computational approaches using computers are outside the scope of the book.\n\nThe book, now in its third edition, was still widely used in university classrooms as of 1999\nand is frequently cited in other textbooks and scientific papers.\n\n\n\n"}
{"id": "159730", "url": "https://en.wikipedia.org/wiki?curid=159730", "title": "Mathematical practice", "text": "Mathematical practice\n\nMathematical practice comprises the working practices of professional mathematicians: selecting theorems to prove, using informal notations to persuade themselves and others that various steps in the final proof are convincing, and seeking peer review and publication, as opposed to the end result of proven and published theorems.\n\nPhilip Kitcher has proposed a more formal definition of a mathematical practice, as a quintuple. His intention was primarily to document mathematical practice through its historical changes.\n\nThe evolution of mathematical practice was slow, and some contributors to modern mathematics did not follow even the practice of their time. For example, Pierre de Fermat was infamous for withholding his proofs, but nonetheless had a vast reputation for correct assertions of results.\n\nOne motivation to study mathematical practice is that, despite much work in the 20th century, some still feel that the foundations of mathematics remain unclear and ambiguous. One proposed remedy is to shift focus to some degree onto 'what is meant by a proof', and other such questions of method.\n\nIf mathematics has been informally used throughout history, in numerous cultures and continents, then it could be argued that \"mathematical practice\" is the practice, or use, of mathematics in everyday life. One definition of mathematical practice, as described above, is the \"working practices of professional mathematicians.\" However, another definition, more in keeping with the predominant usage of mathematics, is that mathematical practice is the everyday practice, or use, of math. Whether one is estimating the total cost of their groceries, calculating miles per gallon, or figuring out how many minutes on the treadmill that chocolate éclair will require, math as used by most people relies less on proof than on practicality (i. e., does it answer the question?).\n\nMathematical teaching usually requires the use of several important teaching pedagogies or components. Most GCSE, A-Level and undergraduate mathematics require the following components: \n\n\n"}
{"id": "52380256", "url": "https://en.wikipedia.org/wiki?curid=52380256", "title": "Mediation-driven attachment model", "text": "Mediation-driven attachment model\n\nIn the scale-free network theory (mathematical theory of networks or graph theory), a mediation-driven attachment (MDA) model appears to embody a preferential attachment rule tacitly rather than explicitly. According to MDA rule, a new node first picks a node from the existing network at random and connect itself not with that but with one of the neighbors also picked at random.\n\nBarabasi and Albert in 1999 noted through their seminal paper noted that (i) most natural and man-made networks are not static, rather they grow with time and (ii) new nodes do not connect with an already connected one randomly rather preferentially with respect to their degrees. The later mechanism is called preferential attachment (PA) rule which embodies the rich get richer phenomena in economics. In their first model, known as the Barabási–Albert model, Barabási and Albert (BA model) choose\n\nwhere, formula_2 is the probability that the new node picks a node formula_3 from the labelled nodes of the existing network. It directly embodies the rich get richer mechanism.\n\nRecently, Hassan \"et al.\" proposed a mediation-driven attachment model which appears to embody the PA rule but not directly rather in disguise. In the MDA model, an incoming node choose an existing node to connect by first picking one of the existing nodes at random which is regarded as mediator. The new node then connect with one of the neighbors of the mediator which is also picked at random. Now the question is: What is the probability formula_2 that an already existing node formula_3 is finally picked to connect it with the new node? Say, the node formula_3 has degree formula_7 and hence it has formula_7 neighbors. Consider that the neighbors of formula_3 are labeled formula_10 which have degrees formula_11 respectively. One can reach the node formula_3 from each of these formula_7 nodes with probabilities inverse of their respective degrees, and each of the formula_7 nodes are likely to be picked at random with probability formula_15. Thus the probability formula_2 of the MDA model is:\n\nIt can be re-written as\n\nwhere the factor formula_19 is the inverse of the harmonic mean (IHM) of degrees of the formula_7 neighbors of the node formula_3. Extensive numerical simulation suggest that for small formula_22 the IHM value of each node fluctuate so wildly that the mean of the IHM values over the entire network bears no meaning. However, for large formula_22 (specially formula_22 approximately greater than 14) the distribution of IHM value of the entire network become left skewed Gaussian type and mean starts to have a meaning which becomes a constant value in the large formula_22 limit. In this limit one finds that formula_26 which is exactly the PA rule. It implies that the higher the links (degree) a node has, the higher its chance of gaining more links since they can be reached in a larger number of ways through mediators which essentially embodies the intuitive idea of rich get richer mechanism. Therefore, the MDA network can be seen to follow the PA rule but in disguise. Moreover, for small formula_22 the MFA is no longer valid rather the attachment probability formula_2 becomes super-preferential in character.\n\nThe idea of MDA rule can be found in the growth process of the weighted planar stochastic lattice (WPSL). An existing node (the center of each block of the WPSL is regarded as nodes and the common border between blocks as the links between the corresponding nodes) during the process gain links only if one of its neighbor is picked not itself. It implies that the higher the links (or degree) a node has, the higher its chance of gaining more links since they can be reached in a larger number of ways. It essentially embodies the intuitive idea of PA rule. Therefore, the dual of the WPSL is a network which can be seen to follow preferential attachment rule but in disguise. Indeed, its degree distribution is found to exhibit power-law as underlined by Barabasi and Albert as one of the essential ingredients.\n\nimplies that one can apply the mean-field approximation (MFA). That is, within this approximation one can replace the true IHM value formula_30 of each node by their mean, where the factor formula_22 that the number of edges the new nodes come with is introduced for latter convenience. The rate equation to solve then becomes exactly like that of the BA model and hence the network that emerges following MDA rule is also scale-free in nature. The only difference is that the exponent formula_32 depends on formula_22 where as in the BA model formula_34 independent of formula_22.\n\nIn the growing network not all nodes are equally important. The extent of their importance is measured by the value of their degree formula_36. Nodes which are linked to an unusually large number of other nodes, i.e. nodes with exceptionally high formula_36 value, are known as hubs. They are special because their existence make the mean distance, measured in units of the number of links, between nodes incredibly small thereby playing the key role in spreading rumors, opinions, diseases, computer viruses etc. It is, therefore, important to know the properties of the largest hub, which we regard as the leader. Like in society, the leadership in a growing network is not permanent. That is, once a node becomes the leader, it does not mean that it remains the leader \"ad infinitum\". An interesting question is: how long does the leader retain this leadership property as the network evolves? To find an answer to this question, we define the leadership persistence probability formula_38 that aleader retains its leadership for at least up to time formula_39. Persistence probability has been of interest in many different systems ranging from coarsening dynamics to fluctuating interfaces or polymer chains.\n\nThe basic idea of the MDA rule is, however not completely new as either this or models similar to this can be found in a few earlier works, albeit their approach, ensuing analysis and their results are different from ours. For instance, Saramaki and Kaski presented a random-walk based model. Another model proposed by Boccaletti \"et al.\" may appear similar to ours, but it markedly differs on closer look. Recently, Yang {\\it et al.} too gave a form for formula_2 and resorted to mean-field approximation. However, the nature of their expressions are significantly different from the one studied by Hassan \"et al.\". Yet another closely related model is the Growing Network with Redirection (GNR) model presented by Gabel, Krapivsky and Redner where at each time step a new node either attaches to a randomly chosen target node with probability formula_41, or to the parent of the target with probability formula_42. The GNR model with formula_42 may appear similar to the MDA model. However, it should be noted that unlike the GNR model, the MDA model is for undirected networks, and that the new link can connect with any neighbor of the mediator-parent or not. One more difference is that, in the MDA model new node may join the existing network with formula_22 edges and in the GNR model it is considered formula_45 case only.\n"}
{"id": "965376", "url": "https://en.wikipedia.org/wiki?curid=965376", "title": "Minimal counterexample", "text": "Minimal counterexample\n\nIn mathematics, the method of considering a minimal counterexample combines the ideas of inductive proof and proof by contradiction. Abstractly, in trying to prove a proposition \"P\", one assumes that it is false, and that therefore there is at least one counterexample. With respect to some idea of size, which may need to be chosen skillfully, one assumes that there is such a counterexample \"C\" that is \"minimal\". We expect that \"C\" is something quite hypothetical (since we are trying to prove \"P\"), but it may be possible to argue that if \"C\" existed, it would have some definite properties. From those we then try to get a contradiction. \n\nIf the form of the contradiction is that we can derive a further counterexample \"D\", and that \"D\" is smaller than \"C\" in the sense of the working hypothesis of minimality, then this technique is traditionally called infinite descent. There may however be more complicated ways to argue. For example, the minimal counterexample method has been much used in the classification of finite simple groups. The Feit–Thompson theorem, that finite simple groups that are not cyclic groups have even order, was based on the hypothesis of some, and therefore some minimal, simple group \"G\" of odd order. Every proper subgroup of \"G\" can be assumed a solvable group, meaning that much theory of such subgroups could be applied.\n\nThe assumption that if there is a counterexample, there is a minimal counterexample, is based on a well-ordering of some kind. The usual ordering on the natural numbers is clearly possible, by the most usual formulation of mathematical induction; but the scope of the method is well-ordered induction of any kind.\n\nEuclid's proof of the fundamental theorem of arithmetic is a simple proof using a minimal counterexample.\n"}
{"id": "19456533", "url": "https://en.wikipedia.org/wiki?curid=19456533", "title": "Minimum distance estimation", "text": "Minimum distance estimation\n\nMinimum distance estimation (MDE) is a statistical method for fitting a mathematical model to data, usually the empirical distribution.\n\nLet formula_1 be an independent and identically distributed (iid) random sample from a population with distribution formula_2 and formula_3.\n\nLet formula_4 be the empirical distribution function based on the sample.\n\nLet formula_5 be an estimator for formula_6. Then formula_7 is an estimator for formula_8.\n\nLet formula_9 be a functional returning some measure of \"distance\" between the two arguments. The functional formula_10 is also called the criterion function.\n\nIf there exists a formula_11 such that formula_12, then formula_5 is called the minimum distance estimate of formula_6.\n\nMost theoretical studies of minimum distance estimation, and most applications, make use of \"distance\" measures which underlie already-established goodness of fit tests: the test statistic used in one of these tests is used as the distance measure to be minimised. Below are some examples of statistical tests that have been used for minimum distance estimation.\n\nThe chi-square test uses as its criterion the sum, over predefined groups, of the squared difference between the increases of the empirical distribution and the estimated distribution, weighted by the increase in the estimate for that group.\n\nThe Cramér–von Mises criterion uses the integral of the squared difference between the empirical and the estimated distribution functions .\n\nThe Kolmogorov–Smirnov test uses the supremum of the absolute difference between the empirical and the estimated distribution functions .\n\nThe Anderson–Darling test is similar to the Cramér–von Mises criterion except that the integral is of a weighted version of the squared difference, where the weighting relates the variance of the empirical distribution function .\n\nThe theory of minimum distance estimation is related to that for the asymptotic distribution of the corresponding statistical goodness of fit tests. Often the cases of the Cramér–von Mises criterion, the Kolmogorov–Smirnov test and the Anderson–Darling test are treated simultaneously by treating them as special cases of a more general formulation of a distance measure. Examples of the theoretical results that are available are: consistency of the parameter estimates; the asymptotic covariance matrices of the parameter estimates.\n\n\n"}
{"id": "3009815", "url": "https://en.wikipedia.org/wiki?curid=3009815", "title": "Null (mathematics)", "text": "Null (mathematics)\n\nIn mathematics, the word null (from meaning \"zero\", which is from meaning \"none\") means of or related to having zero members in a set or a value of zero. Sometimes the symbol ∅ is used to distinguish \"null\" from 0.\n\nIn a normed vector space the null vector is the zero vector; in a seminormed vector space such as Minkowski space, null vectors are, in general, non-zero. In set theory, the null set is the set with zero elements; and in measure theory, a null set is a set with zero measure.\n\nA null space of a mapping is the part of the domain that is mapped into the null element of the image (the inverse image of the null element).\n\nIn statistics, a null hypothesis is a proposition presumed true unless statistical evidence indicates otherwise.\n"}
{"id": "8214401", "url": "https://en.wikipedia.org/wiki?curid=8214401", "title": "Proof of Stein's example", "text": "Proof of Stein's example\n\nStein's example is an important result in decision theory which can be stated as\n\nThe following is an outline of its proof. The reader is referred to the main article for more information.\n\nThe risk function of the decision rule formula_1 is\n\nNow consider the decision rule\n\nwhere formula_6. We will show that formula_7 is a better decision rule than formula_8. The risk function is\n\n— a quadratic in formula_12. We may simplify the middle term by considering a general \"well-behaved\" function formula_13 and using integration by parts. For formula_14, for any continuously differentiable formula_15 growing sufficiently slowly for large formula_16 we have:\n\nTherefore,\n\nNow, we choose\n\nIf formula_15 met the \"well-behaved\" condition (it doesn't, but this can be remedied -- see below), we would have\n\nand so\n\nThen returning to the risk function of formula_7 :\n\nThis quadratic in formula_12 is minimized at\n\ngiving\n\nwhich of course satisfies:\n\nmaking formula_8 an inadmissible decision rule.\n\nIt remains to justify the use of\n\nThis function is not continuously differentiable since it is singular at formula_35. However the function\n\nis continuously differentiable, and after following the algebra through and letting formula_37 one obtains the same result.\n\nSamworth has published a particularly compact proof based on partial integration that does without lemma and without ε.\n"}
{"id": "510629", "url": "https://en.wikipedia.org/wiki?curid=510629", "title": "Quantitative analyst", "text": "Quantitative analyst\n\nA quantitative analyst (or, in financial jargon, a quant) is a person who specializes in the application of mathematical and statistical methods – such as numerical or quantitative techniques – to financial and risk management problems. The occupation is similar to those in industrial mathematics in other industries.\n\nAlthough the original quantitative analysts were \"sell side quants\" from market maker firms, concerned with derivatives pricing and risk management, the meaning of the term has expanded over time to include those individuals involved in almost any application of mathematics in finance, including the buy side. Examples include statistical arbitrage, quantitative investment management, algorithmic trading, and electronic market making. \n\nQuantitative finance started in 1900 with Louis Bachelier's doctoral thesis \"Theory of Speculation\", which provided a model to price options under a Normal Distribution.\n\nHarry Markowitz's 1952 doctoral thesis \"Portfolio Selection\" and its published version was one of the first efforts in economics journals to formally adapt mathematical concepts to finance (mathematics was until then confined to mathematics, statistics or specialized economics journals). Markowitz formalized a notion of mean return and covariances for common stocks which allowed him to quantify the concept of \"diversification\" in a market. He showed how to compute the mean return and variance for a given portfolio and argued that investors should hold only those portfolios whose variance is minimal among all portfolios with a given mean return. Although the language of finance now involves Itō calculus, management of risk in a quantifiable manner underlies much of the modern theory.\n\nIn 1965 Paul Samuelson introduced stochastic calculus into the study of finance. In 1969 Robert Merton promoted continuous stochastic calculus and continuous-time processes. Merton was motivated by the desire to understand how prices are set in financial markets, which is the classical economics question of \"equilibrium,\" and in later papers he used the machinery of stochastic calculus to begin investigation of this issue.\n\nAt the same time as Merton's work and with Merton's assistance, Fischer Black and Myron Scholes developed the Black–Scholes model, which was awarded the 1997 Nobel Memorial Prize in Economic Sciences. It provided a solution for a practical problem, that of finding a fair price for a European call option, i.e., the right to buy one share of a given stock at a specified price and time. Such options are frequently purchased by investors as a risk-hedging device. In 1981, Harrison and Pliska used the general theory of continuous-time stochastic processes to put the Black–Scholes model on a solid theoretical basis, and showed how to price numerous other derivative securities.\n\nEmanuel Derman's 2004 book \"My Life as a Quant\" helped to both make the role of a quantitative analyst better known outside of finance, and to popularize the abbreviation \"quant\" for a quantitative analyst.\n\nQuantitative analysts often come from applied mathematics, physics or engineering backgrounds rather than economics-related fields, and quantitative analysis is a major source of employment for people with mathematics and physics PhD degrees, or with financial mathematics DEA degrees in the French education system. Typically, a quantitative analyst will also need extensive skills in computer programming, most commonly C, C++, Java, R, MATLAB, Mathematica, Python.\n\nThis demand for quantitative analysts has led to a resurgence in demand for actuarial qualifications as well as creation of specialized Masters and PhD courses in financial engineering, mathematical finance, computational finance, and/or financial reinsurance. In particular, Master's degrees in mathematical finance, financial engineering, operations research, computational statistics, machine learning, and financial analysis are becoming more popular with students and with employers. See Master of Quantitative Finance; Master of Financial Economics.\n\nData science and machine learning analysis and modelling methods are being increasingly employed in portfolio performance and portfolio risk modelling, and as such data science and machine learning Master's graduates are also in demand as quantitative analysts.\n\nIn sales & trading, quantitative analysts work to determine prices, manage risk, and identify profitable opportunities. Historically this was a distinct activity from trading but the boundary between a desk quantitative analyst and a quantitative trader is increasingly blurred, and it is now difficult to enter trading as a profession without at least some quantitative analysis education. In the field of algorithmic trading it has reached the point where there is little meaningful difference. Front office work favours a higher speed to quality ratio, with a greater emphasis on solutions to specific problems than detailed modeling. FOQs typically are significantly better paid than those in back office, risk, and model validation. Although highly skilled analysts, FOQs frequently lack software engineering experience or formal training, and bound by time constraints and business pressures tactical solutions are often adopted.\n\nQuantitative analysis is used extensively by asset managers. Some, such as FQ, AQR or Barclays, rely almost exclusively on quantitative strategies while others, such as Pimco, Blackrock or Citadel use a mix of quantitative and fundamental methods.\n\nMajor firms invest large sums in an attempt to produce standard methods of evaluating prices and risk. These differ from front office tools in that Excel is very rare, with most development being in C++, though Java and C# are sometimes used in non-performance critical tasks. LQs spend more time modeling ensuring the analytics are both efficient and correct, though there is tension between LQs and FOQs on the validity of their results. LQs are required to understand techniques such as Monte Carlo methods and finite difference methods, as well as the nature of the products being modeled.\n\nOften the highest paid form of Quant, ATQs make use of methods taken from signal processing, game theory, gambling Kelly criterion, market microstructure, econometrics, and time series analysis. Algorithmic trading includes statistical arbitrage, but includes techniques largely based upon speed of response, to the extent that some ATQs modify hardware and Linux kernels to achieve ultra low latency.\n\nThis has grown in importance in recent years, as the credit crisis exposed holes in the mechanisms used to ensure that positions were correctly hedged, though in no bank does the pay in risk approach that in front office. A core technique is value at risk, and this is backed up with various forms of stress test (financial), economic capital analysis and direct analysis of the positions and models used by various bank's divisions.\n\nIn the aftermath of the financial crisis, there surfaced the recognition that quantitative valuation methods were generally too narrow in their approach. An agreed upon fix adopted by numerous financial institutions has been to improve collaboration.\n\nModel validation (MV) takes the models and methods developed by front office, library, and modeling quantitative analysts and determines their validity and correctness. The MV group might well be seen as a superset of the quantitative operations in a financial institution, since it must deal with new and advanced models and trading techniques from across the firm. Before the crisis however, the pay structure in all firms was such that MV groups struggle to attract and retain adequate staff, often with talented quantitative analysts leaving at the first opportunity. This gravely impacted corporate ability to manage model risk, or to ensure that the positions being held were correctly valued. An MV quantitative analyst would typically earn a fraction of quantitative analysts in other groups with similar length of experience. In the years following the crisis, this has changed. Regulators now typically talk directly to the quants in the middle office such as the model validators, and since profits highly depend of the regulatory infrastructure, model validation has gained in weight and importance with respect to the quants in the front office.\n\nQuantitative developers are computer specialists that assist, implement and maintain the quantitative models. They tend to be highly specialised language technicians that bridge the gap between software developer and quantitative analysts.\n\nBecause of their backgrounds, quantitative analysts draw from various forms of mathematics: statistics and probability, calculus centered around partial differential equations, linear algebra, discrete mathematics, and econometrics. Some on the buy side may use machine learning. The\nmajority of quantitative analysts have received little formal education in mainstream economics, and often apply a mindset drawn from the physical sciences. Quants use mathematical skills learned from diverse fields such as computer science, physics and engineering. These skills include (but are not limited to) advanced statistics, linear algebra and partial differential equations as well as solutions to these based upon numerical analysis.\n\nCommonly used numerical methods are:\n\nA typical problem for a mathematically oriented quantitative analyst would be to develop a model for pricing, hedging, and risk-managing a complex derivative product. These quantitative analysts tend to rely more on numerical analysis than statistics and econometrics. The mindset is to prefer a deterministically \"correct\" answer, as once there is agreement on input values and market variable dynamics, there is only one correct price for any given security (which can be demonstrated, albeit often inefficiently, through a large volume of Monte Carlo simulations).\n\nA typical problem for a statistically oriented quantitative analyst would be to develop a model for deciding which stocks are relatively expensive and which stocks are relatively cheap. The model might include a company's book value to price ratio, its trailing earnings to price ratio, and other accounting factors. An investment manager might implement this analysis by buying the underpriced stocks, selling the overpriced stocks, or both. Statistically oriented quantitative analysts tend to have more of a reliance on statistics and econometrics, and less of a reliance on sophisticated numerical techniques and object-oriented programming. These quantitative analysts tend to be of the psychology that enjoys trying to find the best approach to modeling data, and can accept that there is no \"right answer\" until time has passed and we can retrospectively see how the model performed. Both types of quantitative analysts demand a strong knowledge of sophisticated mathematics and computer programming proficiency.\n\nOne of the principal mathematical tools of quantitative finance is stochastic calculus.\n\n\n\n\n\n\n"}
{"id": "159735", "url": "https://en.wikipedia.org/wiki?curid=159735", "title": "Quasi-empirical method", "text": "Quasi-empirical method\n\nQuasi-empirical methods are methods applied in science and mathematics to achieve epistemology similar to that of empiricism (thus \"quasi- + empirical\") when experience cannot falsify the ideas involved. Empirical research relies on empirical evidence, and its empirical methods involve experimentation and disclosure of apparatus for reproducibility, by which scientific findings are validated by other scientists. Empirical methods are studied extensively in the philosophy of science, but they cannot be used directly in fields whose hypotheses cannot be falsified by real experiment (for example, mathematics, philosophy, theology, and ideology). Because of such empirical limits in science, the scientific method must rely not only on empirical methods but sometimes also on quasi-empirical ones. The prefix \"quasi-\" came to denote methods that are \"almost\" or \"socially approximate\" an ideal of truly empirical methods.\n\nIt is unnecessary to find all counterexamples to a theory; all that is required to disprove a theory logically is one counterexample. The converse does not prove a theory; Bayesian inference simply makes a theory more likely, by weight of evidence.\n\nOne can argue that no science is capable of finding all counter-examples to a theory, therefore, no science is strictly empirical, it's all quasi-empirical. But usually, the term \"quasi-empirical\" refers to the means of choosing problems to focus on (or ignore), selecting prior work on which to build an argument or proof, notations for informal claims, peer review and acceptance, and incentives to discover, ignore, or correct errors. These are common to both science and mathematics, and do not include experimental method.\n\nAlbert Einstein's discovery of the general relativity theory relied upon thought experiments and mathematics. Empirical methods only became relevant when confirmation was sought. Furthermore, some empirical confirmation was found only some time after the general acceptance of the theory.\n\nThought experiments are almost standard procedure in philosophy, where a conjecture is tested out in the imagination for possible effects on experience; when these are thought to be implausible, unlikely to occur, or not actually occurring, then the conjecture may be either rejected or amended. Logical positivism was a perhaps extreme version of this practice, though this claim is open to debate.\n\nPost-20th-century philosophy of mathematics is mostly concerned with quasi-empirical mathematical methods, especially as reflected in the actual mathematical practice of working mathematicians. \n\n"}
{"id": "4783135", "url": "https://en.wikipedia.org/wiki?curid=4783135", "title": "Representation theorem", "text": "Representation theorem\n\nIn mathematics, a representation theorem is a theorem that states that every abstract structure with certain properties is isomorphic to another (abstract or concrete) structure. \n\nFor example, \n"}
{"id": "39418646", "url": "https://en.wikipedia.org/wiki?curid=39418646", "title": "Simplicial group", "text": "Simplicial group\n\nIn mathematics, more precisely, in the theory of simplicial sets, a simplicial group is a simplicial object in the category of groups. Similarly, a simplicial abelian group is a simplicial object in the category of abelian groups. A simplicial group is a Kan complex (in particular, its homotopy groups make sense.) The Dold–Kan correspondence says that a simplicial abelian group may be identified with a chain complex.\n\nA commutative monoid in the category of simplicial abelian groups is a simplicial commutative ring.\n\n\n"}
{"id": "38218032", "url": "https://en.wikipedia.org/wiki?curid=38218032", "title": "Space mapping", "text": "Space mapping\n\nThe space mapping methodology for modeling and design optimization of engineering systems was first discovered by John Bandler in 1993. It uses relevant existing knowledge to speed up model generation and design optimization of a system. The knowledge is updated with new validation information from the system when available.\n\nThe space mapping methodology employs a \"quasi-global\" formulation that intelligently links companion \"coarse\" (ideal or low-fidelity) and \"fine\" (practical or high-fidelity) models of different complexities. In engineering design, space mapping aligns a very fast coarse model with the expensive-to-compute fine model so as to avoid direct expensive optimization of the fine model. The alignment can be done either off-line (model enhancement) or on-the-fly with surrogate updates (e.g., aggressive space mapping).\nAt the core of the process is a pair of models: one very accurate but too expensive to use directly with a conventional optimization routine, and one significantly less expensive and, accordingly, less accurate. The latter (fast model) is usually referred to as the \"coarse\" model (coarse space). The former (slow model) is usually referred to as the \"fine\" model. A validation space (\"reality\") represents the fine model, for example, a high-fidelity physics model. The optimization space, where conventional optimization is carried out, incorporates the coarse model (or surrogate model), for example, the low-fidelity physics or \"knowledge\" model. In a space-mapping design optimization phase, there is a prediction or \"execution\" step, where the results of an optimized \"mapped coarse model\" (updated surrogate) are assigned to the fine model for validation. After the validation process, if the design specifications are not satisfied, relevant data is transferred to the optimization space (\"feedback\"), where the mapping-augmented coarse model or surrogate is updated (enhanced, realigned with the fine model) through an iterative optimization process termed \"parameter extraction\". The mapping formulation itself incorporates \"intuition\", part of the engineer's so-called \"feel\" for a problem. In particular, the Aggressive Space Mapping (ASM) process displays key characteristics of cognition (an expert's approach to a problem), and is often illustrated in simple cognitive terms.\n\nFollowing John Bandler's concept in 1993, algorithms have utilized Broyden updates (aggressive space mapping), trust regions, and artificial neural networks. New developments include implicit space mapping, in which we allow preassigned parameters not used in the optimization process to change in the coarse model, and output space mapping, where a transformation is applied to the response of the model. A paper reviews the state of the art after the first ten years of development and implementation. Tuning space mapping utilizes a so-called tuning model—constructed invasively from the fine model—as well as a calibration process that translates the adjustment of the optimized tuning model parameters into relevant updates of the design variables. The space mapping concept has been extended to neural-based space mapping for large-signal statistical modeling of nonlinear microwave devices.\n\nA 2016 state-of-the-art review is devoted to aggressive space mapping. It spans two decades of development and engineering applications.\n\nThe space mapping methodology can also be used to solve inverse problems. Proven techniques include the Linear Inverse Space Mapping (LISM) algorithm, as well as the Space Mapping with Inverse Difference (SM-ID) method.\n\nSpace mapping optimization belongs to the class of surrogate-based optimization methods, that is to say, optimization methods that rely on a surrogate model.\n\nThe space mapping technique has been applied in a variety of disciplines including microwave and electromagnetic design, civil and mechanical applications, aerospace engineering, and biomedical research. Some examples:\n\n\nVarious simulators can be involved in a space mapping optimization and modeling processes.\n\nThree international workshops have focused significantly on the art, the science and the technology of space mapping.\n\n\nThere is a wide spectrum of terminology associated with space mapping: ideal model, coarse model, coarse space, fine model, companion model, cheap model, expensive model, surrogate model, low fidelity (resolution) model, high fidelity (resolution) model, empirical model, simplified physics model, physics-based model, quasi-global model, physically expressive model, device under test, electromagnetics-based model, simulation model, computational model, tuning model, calibration model, surrogate model, surrogate update, mapped coarse model, surrogate optimization, parameter extraction, target response, optimization space, validation space, neuro-space mapping, implicit space mapping, output space mapping, port tuning, predistortion (of design specifications), manifold mapping, defect correction, model management, multi-fidelity models, variable fidelity/variable complexity, multigrid method, coarse grid, fine grid, surrogate-driven, simulation-driven, model-driven, feature-based modeling.\n"}
{"id": "42690608", "url": "https://en.wikipedia.org/wiki?curid=42690608", "title": "Strange nonchaotic attractor", "text": "Strange nonchaotic attractor\n\nIn mathematics, a strange nonchaotic attractor (SNA) is a form of attractor which, while converging to a limit, is strange, because it is not piecewise differentiable, and also non-chaotic, in that its Lyapunov exponents are non-positive. SNAs were introduced as a topic of study by Grebogi et al. in 1984. SNAs can be distinguished from periodic, quasiperiodic and chaotic attractors using the 0-1 test for chaos.\n\nPeriodically driven damped nonlinear systems can exhibit complex dynamics characterized by strange chaotic attractors, where strange refers to the fractal geometry of the attractor and chaotic refers to the exponential sensitivity of orbits on the attractor. Quasiperiodically driven systems forced by incommensurate frequencies are natural extensions of periodically driven ones and are phenomenologically richer. In addition to periodic or quasiperiodic motion, they can exhibit chaotic or nonchaotic motion on strange attractors. Although quasiperiodic forcing is not necessary for strange nonchaotic dynamics (e.g., the period doubling accumulation point of a period doubling cascade), if quasiperiodic driving is not present, strange nonchaotic attractors are typically not robust and not expected to occur naturally because they exist only when the system is carefully tuned to a precise critical parameter value. On the other hand, it was shown in the paper of Grebogi et al. that SNA's can be robust when the system is quasiperiodically driven. The first experiment to demonstrate a robust strange nonchaotic attractor involved the buckling of a magnetoelastic ribbon driven quasiperiodically by two incommensurate frequencies in the golden ratio. Strange nonchaotic attractors have been robustly observed in laboratory experiments involving magnetoelastic ribbons, electrochemical cells, electronic circuits, a neon glow discharge and most recently detected in the dynamics of the pulsating RR Lyrae variables KIC 5520878 (as obtained from the Kepler Space Telescope) which may be the first strange nonchaotic dynamical system observed in the wild.\n"}
{"id": "19934039", "url": "https://en.wikipedia.org/wiki?curid=19934039", "title": "The Whetstone of Witte", "text": "The Whetstone of Witte\n\nThe Whetstone of Witte is the shortened title of Robert Recorde's mathematics book published in 1557, the full title being \"The whetstone of witte, whiche is the seconde parte of Arithmetike: containyng thextraction of Rootes: The \"Coßike\" practise, with the rule of \"Equation\": and the woorkes of \"Surde Nombers. The book covers topics including whole numbers, the extraction of roots and irrational numbers. The work is notable for containing the first recorded use of the equals sign and also for being the first book in English to use the plus and minus signs.\n\nRecordian notation for exponentiation, however, differed from the later Cartesian notation formula_1. Recorde expressed indices and surds larger than 3 in a systematic form based on the prime factorization of the exponent: a factor of two he termed a \"zenzic\", and a factor of three, a \"cubic\". Recorde termed the larger prime numbers appearing in this factorization \"sursolids\", distinguishing between them by use of ordinal numbers: that is, he defined 5 as the \"first sursolid\", written as ʃz and 7 as the \"second sursolid\", written as Bʃz.\nHe also devised symbols for these factors: a zenzic was denoted by z, and a cubic by &. For instance, he referred to \"p=p\" as zzz (the zenzizenzizenzic), and \"q=q\" as zz& (the zenzizenzicubic).\n\nLater in the book he includes a chart of exponents all the way up to \"p=p\" written as zzzzʃz. There is an error in the chart, however, writing \"p\" as Sʃz, despite it not being a prime. It should be \"p\" or &Gʃz.\n\n"}
{"id": "701142", "url": "https://en.wikipedia.org/wiki?curid=701142", "title": "Toy model", "text": "Toy model\n\nIn the modeling of physics, a toy model is a deliberately simplistic model with many details removed so that it can be used to explain a mechanism concisely. It is also useful in a description of the fuller model. \n\n\nThe phrase \"tinker-toy model\" is also used, in reference to the popular Tinkertoys used for children's constructivist learning.\n\nExamples of toy models in physics include:\n\n\n"}
{"id": "1458024", "url": "https://en.wikipedia.org/wiki?curid=1458024", "title": "Toy theorem", "text": "Toy theorem\n\nIn mathematics, a toy theorem is a simplified version (special case) of a more general theorem. For instance, by introducing some simplifying assumptions in a theorem, one obtains a toy theorem.\n\nUsually, a toy theorem is used to illustrate the claim of a theorem. It can also be insightful to study proofs of a toy theorem derived from a non-trivial theorem. Toy theorems can also have education value. After presenting a theorem (with, say, a highly non-trivial proof), one can sometimes give some assurance that the theorem really holds, by proving a toy version of the theorem.\n\nFor instance, a toy theorem of the Brouwer fixed-point theorem is obtained by restricting the dimension to one. In this case, the Brouwer fixed-point theorem follows almost immediately from the intermediate value theorem.\n\n"}
{"id": "253456", "url": "https://en.wikipedia.org/wiki?curid=253456", "title": "Well-defined", "text": "Well-defined\n\nIn mathematics, an expression is called well-defined or \"unambiguous\" if its definition assigns it a unique interpretation or value. Otherwise, the expression is said to be \"not well-defined\" or \"ambiguous\". A function is well-defined if it gives the same result when the representation of the input is changed without changing the value of the input. For instance if \"f\" takes real numbers as input, and if \"f\"(0.5) does not equal \"f\"(1/2) then \"f\" is not well-defined (and thus: not a function). The term \"well-defined\" is also used to indicate whether a logical statement is unambiguous.\n\nA function that is not well-defined is not the same as a function that is undefined. For example, if \"f\"(\"x\") = 1/\"x\", then \"f\"(0) is undefined, but this has nothing to do with the question of whether \"f\"(\"x\") = 1/\"x\" is well-defined. It is; 0 is simply not in the domain of the function.\n\nLet formula_1 be sets, let formula_2 and \"define\" formula_3 as formula_4 if formula_5 and formula_6 if formula_7.\n\nThen formula_8 is well-defined if formula_9. This is e. g. the case when formula_10 (then \"f\"(\"a\") happens to be formula_11).\n\nIf however formula_12 then formula_8 is not well-defined because formula_14 is \"ambiguous\" for formula_15. This is e. g. the case when formula_16 and formula_17. Indeed, formula_18 and \"f\"(2) would have to be 0 as well as 1, which is impossible. Therefore, the latter \"f\" is not well-defined and thus not a function.\n\nIn order to avoid the apostrophes around \"define\" in the previous simple example, the \"definition\" of formula_8 could be broken down into two simple logical steps:\n\nWhereas the definition in step 1. is formulated with the freedom of any definition and is certainly effective (without the need to classify it as „well-defined“), the assertion in step 2. has to be proved: If and only if formula_9, we get a function formula_8, and the formula_8 of \"definition\" is well-defined (as a function).\n\nOn the other hand: if formula_12 then for an formula_15 there is both, formula_25 \"and\" formula_26, and the binary relation formula_8 is not \"functional\" as defined in Binary relation#Special types of binary relations and thus not well-defined (as a function). Colloquially, the \"function\" formula_8 is called ambiguous at point formula_29 (although there is \"per definitionem\" never an \"ambiguous function\"), and the original \"definition\" is pointless.\n\nDespite these subtle logical problems, it is quite common to anticipatorily use the term definition (without apostrophes) for \"definitions\" of this kind, firstly because it is sort of a short-hand of the two-step approach, secondly because the relevant mathematical reasoning (step 2.) is the same in both cases, and finally because in mathematical texts the assertion is «up to 100%» true.\n\nThe question of well-definedness of a function classically arises when the defining equation of a function does not (only) refer to the arguments themselves, but (also) to elements of the arguments. This is sometimes unavoidable when the arguments are cosets and the equation refers to coset representatives.\n\nFor example, consider the following function\nwhere formula_31 and formula_32 are the integers modulo \"m\" and formula_33 denotes the congruence class of \"n\" mod \"m\".\n\nN.B.: formula_34 is a reference to the element formula_35, and formula_36 is the argument of \"f\".\n\nThe function \"f\" is well-defined, because\n\nIn particular, the term well-defined is used with respect to (binary) operations on cosets. In this case one can view the operation as a function of two variables and the property of being well-defined is the same as that for a function. For example, addition on the integers modulo some \"n\" can be defined naturally in terms of integer addition. \nThe fact that this is well-defined follows from the fact that we can write any representative of formula_39 as formula_40, where k is an integer. Therefore, \nand similarly for any representative of formula_42.\n\nFor real numbers, the product formula_43 is unambiguous because formula_44. (Therefore, the notation is said to be \"well-defined\".) Because of this property of the operation (here formula_45), which is known as associativity, the result does not depend on the sequence of multiplications, so that a specification of the sequence can be omitted.\n\nThe subtraction operation, formula_46, is not associative. However, there is a convention (or definition) in that the formula_46 operation is understood as addition of the opposite, thus formula_48 is the same as formula_49, and is called \"well-defined\".\n\nDivision is also non-associative. However, in the case of formula_50 the convention formula_51 is not so well established, so this expression is considered ill-defined.\n\nUnlike with functions, the notational ambiguities can be overcome more or less easily by means of additional definitions, i. e. rules of precedence, and/or associativity of the operators. In the programming language C e. g. the operator codice_1 for subtraction is \"left-to-right-associative\" which means that codice_2 is defined as codice_3 and the operator codice_4 for assignment is \"right-to-left-associative\" which means that codice_5 is defined as codice_6. In the programming language APL there is only one rule: from right to left − but parentheses first.\n\nA solution to a partial differential equation is said to be well-defined if it is determined by the boundary conditions in a continuous way as the boundary conditions are changed.\n\n\n"}
