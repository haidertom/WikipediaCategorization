{"id": "93070", "url": "https://en.wikipedia.org/wiki?curid=93070", "title": "A New Kind of Science", "text": "A New Kind of Science\n\nA New Kind of Science is a best-selling, controversial book by Stephen Wolfram, published by his own company in 2002. It contains an empirical and systematic study of computational systems such as cellular automata. Wolfram calls these systems \"simple programs\" and argues that the scientific philosophy and methods appropriate for the study of simple programs are relevant to other fields of science.\n\nThe thesis of \"A New Kind of Science\" (\"NKS\") is twofold: that the nature of computation must be explored experimentally, and that the results of these experiments have great relevance to understanding the physical world. Since its nascent beginnings in the 1930s, computation has been primarily approached from two traditions: engineering, which seeks to build practical systems using computations; and mathematics, which seeks to prove theorems about computation. However, as recently as the 1970s, computing has been described as being at the crossroads of mathematical, engineering, and empirical traditions.\n\nWolfram introduces a third tradition that seeks to empirically investigate computation for its own sake: He argues that an entirely new method is needed to do so because traditional mathematics fails to meaningfully describe complex systems, and that there is an upper limit to complexity in all systems.\n\nThe basic subject of Wolfram's \"new kind of science\" is the study of simple abstract rules—essentially, elementary computer programs. In almost any class of a computational system, one very quickly finds instances of great complexity among its simplest cases (after a time series of multiple iterative loops, applying the same simple set of rules on itself, similar to a self-reinforcing cycle using a set of rules). This seems to be true regardless of the components of the system and the details of its setup. Systems explored in the book include, amongst others, cellular automata in one, two, and three dimensions; mobile automata; Turing machines in 1 and 2 dimensions; several varieties of substitution and network systems; primitive recursive functions; nested recursive functions; combinators; tag systems; register machines; reversal-addition. For a program to qualify as simple, there are several requirements:\n\nGenerally, simple programs tend to have a very simple abstract framework. Simple cellular automata, Turing machines, and combinators are examples of such frameworks, while more complex cellular automata do not necessarily qualify as simple programs. It is also possible to invent new frameworks, particularly to capture the operation of natural systems. The remarkable feature of simple programs is that a significant percentage of them are capable of producing great complexity. Simply enumerating all possible variations of almost any class of programs quickly leads one to examples that do unexpected and interesting things. This leads to the question: if the program is so simple, where does the complexity come from? In a sense, there is not enough room in the program's definition to directly encode all the things the program can do. Therefore, simple programs can be seen as a minimal example of emergence. A logical deduction from this phenomenon is that if the details of the program's rules have little direct relationship to its behavior, then it is very difficult to directly engineer a simple program to perform a specific behavior. An alternative approach is to try to engineer a simple overall computational framework, and then do a brute-force search through all of the possible components for the best match.\n\nSimple programs are capable of a remarkable range of behavior. Some have been proven to be universal computers. Others exhibit properties familiar from traditional science, such as thermodynamic behavior, continuum behavior, conserved quantities, percolation, sensitive dependence on initial conditions, and others. They have been used as models of traffic, material fracture, crystal growth, biological growth, and various sociological, geological, and ecological phenomena. Another feature of simple programs is that, according to the book, making them more complicated seems to have little effect on their overall complexity. \"A New Kind of Science\" argues that this is evidence that simple programs are enough to capture the essence of almost any complex system.\n\nIn order to study simple rules and their often complex behaviour, Wolfram argues that it is necessary to systematically explore all of these computational systems and document what they do. He further argues that this study should become a new branch of science, like physics or chemistry. The basic goal of this field is to understand and characterize the computational universe using experimental methods.\n\nThe proposed new branch of scientific exploration admits many different forms of scientific production. For instance, qualitative classifications are often the results of initial forays into the computational jungle. On the other hand, explicit proofs that certain systems compute this or that function are also admissible. There are also some forms of production that are in some ways unique to this field of study. For example, the discovery of computational mechanisms that emerge in different systems but in bizarrely different forms.\n\nAnother kind of production involves the creation of programs for the analysis of computational systems. In the \"NKS\" framework, these themselves should be simple programs, and subject to the same goals and methodology. An extension of this idea is that the human mind is itself a computational system, and hence providing it with raw data in as effective a way as possible is crucial to research. Wolfram believes that programs and their analysis should be visualized as directly as possible, and exhaustively examined by the thousands or more. Since this new field concerns abstract rules, it can in principle address issues relevant to other fields of science. However, in general Wolfram's idea is that novel ideas and mechanisms can be discovered in the computational universe, where they can be represented in their simplest forms, and then other fields can choose among these discoveries for those they find relevant.\n\nWolfram has since expressed \"A central lesson of \"A New Kind of Science\" is that there’s a lot of incredible richness out there in the computational universe. And one reason that’s important is that it means that there’s a lot of incredible stuff out there for us to 'mine' and harness for our purposes.\"\n\nWhile Wolfram advocates simple programs as a scientific discipline, he also argues that its methodology will revolutionize other fields of science. The basis of his argument is that the study of simple programs is the minimal possible form of science, grounded equally in both abstraction and empirical experimentation. Every aspect of the methodology advocated in \"NKS\" is optimized to make experimentation as direct, easy, and meaningful as possible while maximizing the chances that the experiment will do something unexpected. Just as this methodology allows computational mechanisms to be studied in their simplest forms, Wolfram argues that the process of doing so engages with the mathematical basis of the physical world, and therefore has much to offer the sciences.\n\nWolfram argues that the computational realities of the universe make science hard for fundamental reasons. But he also argues that by understanding the importance of these realities, we can learn to use them in our favor. For instance, instead of reverse engineering our theories from observation, we can enumerate systems and then try to match them to the behaviors we observe. A major theme of \"NKS\" is investigating the structure of the possibility space. Wolfram argues that science is far too ad hoc, in part because the models used are too complicated and unnecessarily organized around the limited primitives of traditional mathematics. Wolfram advocates using models whose variations are enumerable and whose consequences are straightforward to compute and analyze.\n\nWolfram argues that one of his achievements is in providing a coherent system of ideas that justifies computation as an organizing principle of science. For instance, he argues that the concept of \"computational irreducibility\" (that some complex computations are not amenable to short-cuts and cannot be \"reduced\"), is ultimately the reason why computational models of nature must be considered in addition to traditional mathematical models. Likewise, his idea of intrinsic randomness generation—that natural systems can generate their own randomness, rather than using chaos theory or stochastic perturbations—implies that computational models do not need to include explicit randomness.\n\nBased on his experimental results, Wolfram developed the principle of computational equivalence (PCE): the principle states that systems found in the natural world can perform computations up to a maximal (\"universal\") level of computational power. Most systems can attain this level. Systems, in principle, compute the same things as a computer. Computation is therefore simply a question of translating input and outputs from one system to another. Consequently, most systems are computationally equivalent. Proposed examples of such systems are the workings of the human brain and the evolution of weather systems.\n\nThe principle can be restated as follows: almost all processes that are not obviously simple are of equivalent sophistication. From this principle, Wolfram draws an array of concrete deductions which he argues reinforce his theory. Possibly the most important among these is an explanation as to why we experience randomness and complexity: often, the systems we analyze are just as sophisticated as we are. Thus, complexity is not a special quality of systems, like for instance the concept of \"heat,\" but simply a label for all systems whose computations are sophisticated. Wolfram argues that understanding this makes possible the \"normal science\" of the \"NKS\" paradigm.\n\nAt the deepest level, Wolfram argues that—like many of the most important scientific ideas—the principle of computational equivalence allows science to be more general by pointing out new ways in which humans are not \"special\"; that is, it has been claimed that the complexity of human intelligence makes us special, but the Principle asserts otherwise. In a sense, many of Wolfram's ideas are based on understanding the scientific process—including the human mind—as operating within the same universe it studies, rather than being outside it.\n\nThere are a number of specific results and ideas in the \"NKS\" book, and they can be organized into several themes. One common theme of examples and applications is demonstrating how little complexity it takes to achieve interesting behavior, and how the proper methodology can discover this behavior.\n\nFirst, there are several cases where the \"NKS\" book introduces what was, during the book's composition, the simplest known system in some class that has a particular characteristic. Some examples include the first primitive recursive function that results in complexity, the smallest universal Turing Machine, and the shortest axiom for propositional calculus. In a similar vein, Wolfram also demonstrates many simple programs that exhibit phenomena like phase transitions, conserved quantities, continuum behavior, and thermodynamics that are familiar from traditional science. Simple computational models of natural systems like shell growth, fluid turbulence, and phyllotaxis are a final category of applications that fall in this theme.\n\nAnother common theme is taking facts about the computational universe as a whole and using them to reason about fields in a holistic way. For instance, Wolfram discusses how facts about the computational universe inform evolutionary theory, SETI, free will, computational complexity theory, and philosophical fields like ontology, epistemology, and even postmodernism.\n\nWolfram suggests that the theory of computational irreducibility may provide a resolution to the existence of free will in a nominally deterministic universe. He posits that the computational process in the brain of the being with free will is actually complex enough so that it cannot be captured in a simpler computation, due to the principle of computational irreducibility. Thus, while the process is indeed deterministic, there is no better way to determine the being's will than, in essence, to run the experiment and let the being exercise it.\n\nThe book also contains a vast number of individual results—both experimental and analytic—about what a particular automaton computes, or what its characteristics are, using some methods of analysis.\n\nThe book contains a new technical result in describing the Turing completeness of the Rule 110 cellular automaton. Very small Turing machines can simulate Rule 110, which Wolfram demonstrates using a 2-state 5-symbol universal Turing machine. Wolfram conjectures that a particular 2-state 3-symbol Turing machine is universal. In 2007, as part of commemorating the book's fifth anniversary, Wolfram's company offered a $25,000 prize for proof that this Turing machine is universal. Alex Smith, a computer science student from Birmingham, UK, won the prize later that year by proving Wolfram's conjecture.\n\nEvery year, Wolfram and his group of instructors organize a summer school. From 2003 to 2006, these classes were held at Brown University. In 2007, the summer school began being hosted by the University of Vermont at Burlington, with the exception of 2009 which was held at the Istituto di Scienza e Tecnologie dell'Informazione of the CNR in Pisa, Italy. In 2012, the program was held at Curry College in Milton, Massachusetts. Since 2013, the Wolfram Summer School has been held annually at Bentley University in Waltham, Massachusetts. After 14 consecutive summer schools, more than 550 people have participated, some of whom continued developing their 3-week research projects as their Master's or Ph.D theses. Some of the research done in the summer school has resulted in publications.\n\nPeriodicals gave \"A New Kind of Science\" coverage, including articles in \"The New York Times\", \"Newsweek\", \"Wired\", and \"The Economist\". Some scientists criticized the book as abrasive and arrogant, and perceived a fatal flaw—that simple systems such as cellular automata are not complex enough to describe the degree of complexity present in evolved systems, and observed that Wolfram ignored the research categorizing the complexity of systems. Although critics accept Wolfram's result showing universal computation, they view it as minor and dispute Wolfram's claim of a paradigm shift. Others found that the work contained valuable insights and refreshing ideas. Wolfram addressed his critics in a series of blog posts.\n\nIn an article published on April 3, 2018, \"A New Kind of Science\" was listed among the 190 books recommended by Bill Gates.\n\nA tenet of \"NKS\" is that the simpler the system, the more likely a version of it will recur in a wide variety of more complicated contexts. Therefore, \"NKS\" argues that systematically exploring the space of simple programs will lead to a base of reusable knowledge. However, many scientists believe that of all possible parameters, only some actually occur in the universe. For instance, of all possible permutations of the symbols making up an equation, most will be essentially meaningless. \"NKS\" has also been criticized for asserting that the behavior of simple systems is somehow representative of all systems.\n\nA common criticism of \"NKS\" is that it does not follow established scientific methodology. For instance, \"NKS\" does not establish rigorous mathematical definitions, nor does it attempt to prove theorems; and most formulas and equations are written in Mathematica rather than standard notation. Along these lines, \"NKS\" has also been criticized for being heavily visual, with much information conveyed by pictures that do not have formal meaning. It has also been criticized for not using modern research in the field of complexity, particularly the works that have studied complexity from a rigorous mathematical perspective. And it has been criticized for misrepresenting chaos theory: \"Throughout the book, he equates chaos theory with the phenomenon of sensitive dependence on initial conditions (SDIC).\"\n\n\"NKS\" has been criticized for not providing specific results that would be immediately applicable to ongoing scientific research. There has also been criticism, implicit and explicit, that the study of simple programs has little connection to the physical universe, and hence is of limited value. Steven Weinberg has pointed out that no real world system has been explained using Wolfram's methods in a satisfactory fashion.\n\nThe Principle of computational equivalence has been criticized for being vague, unmathematical, and for not making directly verifiable predictions. It has also been criticized for being contrary to the spirit of research in mathematical logic and computational complexity theory, which seek to make fine-grained distinctions between levels of computational sophistication, and for wrongly conflating different kinds of universality property. Moreover, critics such as Ray Kurzweil have argued that it ignores the distinction between hardware and software; while two computers may be equivalent in power, it does not follow that any two programs they might run are also equivalent. Others suggest it is little more than a rechristening of the Church–Turing thesis.\n\nWolfram's speculations of a direction towards a fundamental theory of physics have been criticized as vague and obsolete. Scott Aaronson, Professor of Computer Science at University of Texas Austin, also claims that Wolfram's methods cannot be compatible with both special relativity and Bell's theorem violations, and hence cannot explain the observed results of Bell test experiments. However, Aaronson's arguments are either right and apply to the entire scientific field of Quantum gravity that seeks to find theories unifying relativity and quantum mechanics or they are fundamentally flawed (e.g. under a non-local hidden variable theory of superdeterminism acknowledged by Bell himself), and even explored by e.g. physics Nobel laureate Gerard 't Hooft, see also replies to criticism of digital physics.\n\nEdward Fredkin and Konrad Zuse pioneered the idea of a computable universe, the former by writing a line in his book on how the world might be like a cellular automaton, and later further developed by Fredkin using a toy model called Salt. It has been claimed that \"NKS\" tries to take these ideas as its own but Wolfram's model of the universe is a rewriting network and not a cellular automaton as Wolfram himself has suggested a cellular automaton cannot account for relativistic features such as no absolute time frame. Jürgen Schmidhuber has also charged that his work on Turing machine-computable physics was stolen without attribution, namely his idea on enumerating possible Turing-computable universes.\n\nIn a 2002 review of \"NKS\", the Nobel laureate and elementary particle physicist Steven Weinberg wrote, \"Wolfram himself is a lapsed elementary particle physicist, and I suppose he can't resist trying to apply his experience with digital computer programs to the laws of nature. This has led him to the view (also considered in a 1981 paper by Richard Feynman) that nature is discrete rather than continuous. He suggests that space consists of a set of isolated points, like cells in a cellular automaton, and that even time flows in discrete steps. Following an idea of Edward Fredkin, he concludes that the universe itself would then be an automaton, like a giant computer. It's possible, but I can't see any motivation for these speculations, except that this is the sort of system that Wolfram and others have become used to in their work on computers. So might a carpenter, looking at the moon, suppose that it is made of wood.\"\n\nNobel laureate Gerard 't Hooft has more recently also suggested a cellular automaton-based unifying theory of quantum gravity as an interpretation of superstring theory where the evolution equations are classical, \"[b]oth the bosonic string theory and superstring theory can be reformulated in terms of a special basis of states, defined on a space-time lattice with lattice length formula_1\"\n\nWolfram's claim that natural selection is not the fundamental cause of complexity in biology has led non-scientist journalist Chris Lavers to state that Wolfram does not understand the theory of evolution.\n\n\"NKS\" has been heavily criticized as not being original or important enough to justify its title and claims.\n\nThe authoritative manner in which \"NKS\" presents a vast number of examples and arguments has been criticized as leading the reader to believe that each of these ideas was original to Wolfram; in particular, one of the most substantial new technical results presented in the book, that the rule 110 cellular automaton is Turing complete, was not proven by Wolfram, but by his research assistant, Matthew Cook. However, the notes section at the end of his book acknowledges many of the discoveries made by these other scientists citing their names together with historical facts, although not in the form of a traditional bibliography section. Additionally, the idea that very simple rules often generate great complexity is already an established idea in science, particularly in chaos theory and complex systems.\n\n\n"}
{"id": "852721", "url": "https://en.wikipedia.org/wiki?curid=852721", "title": "Actuarial notation", "text": "Actuarial notation\n\nActuarial notation is a shorthand method to allow actuaries to record mathematical formulas that deal with interest rates and life tables.\n\nTraditional notation uses a halo system where symbols are placed as superscript or subscript before or after the main letter. Example notation using the halo system can be seen below.\n\nVarious proposals have been made to adopt a linear system where all the notation would be on a single line without the use of superscripts or subscripts. Such a method would be useful for computing where representation of the halo system can be extremely difficult. However, a standard linear system has yet to emerge.\n\nformula_1 is the annual effective interest rate, which is the \"true\" rate of interest over \"a year\". Thus if the annual interest rate is 12% then formula_2.\n\nformula_3 (pronounced \"i \"upper\" m\") is the nominal interest rate convertible formula_4 times a year, and is numerically equal to formula_4 times the effective rate of interest over one formula_4 of a year. For example, formula_7 is the nominal rate of interest convertible semiannually. If the effective annual rate of interest is 12%, then formula_8 represents the effective interest rate every six months. Since formula_9, we have formula_10 and hence formula_11. The appearing in the symbol formula_3 is not an \"exponent.\" It merely represents the number of interest conversions, or compounding times, per year. Semi-annual compounding, (or converting interest every six months), is frequently used in valuing bonds (see also fixed income securities) and similar monetary financial liability instruments, whereas home mortgages frequently convert interest monthly. Following the above example again where formula_13, we have formula_14 since formula_15.\n\nEffective and nominal rates of interest are not the same because interest paid in earlier measurement periods \"earns\" interest in later measurement periods; this is called compound interest. That is, nominal rates of interest credit interest to an investor, (alternatively charge, or debit, interest to a debtor), more frequently than do effective rates. The result is more frequent compounding of interest income to the investor, (or interest expense to the debtor), when nominal rates are used.\n\nThe symbol formula_16 represents the present value of 1 to be paid one year from now:\nThis present value factor, or discount factor, is used to determine the amount of money that must be invested now in order to have a given amount of money in the future. For example, if you need 1 in one year, then the amount of money you should invest now is: formula_18. If you need 25 in 5 years the amount of money you should invest now is: formula_19.\n\nformula_20 is the annual effective discount rate:\nThe value of formula_20 can also be calculated from the following relationships: formula_23\nThe rate of discount equals the amount of interest earned during a one-year period, divided by the balance of money at the end of that period. By contrast, an annual effective rate of interest is calculated by dividing the amount of interest earned during a one-year period by the balance of money at the beginning of the year. The present value (today) of a payment of 1 that is to be made formula_24 years in the future is formula_25. This is analogous to the formula formula_26 for the future (or accumulated) value formula_24 years in the future of an amount of 1 invested today.\n\nformula_28, the nominal rate of discount convertible formula_29 times a year, is analogous to formula_3. Discount is converted on an formula_4-ly basis.\n\nformula_32, the force of interest, is the limiting value of the nominal rate of interest when formula_4 increases without bound:\n\nIn this case, interest is convertible continuously.\n\nThe general relationship between formula_1, formula_32 and formula_20 is:\n\nTheir numerical value can be compared as follows:\n\nA life table (or a mortality table) is a mathematical construction that shows the number of people alive (based on the assumptions used to build the table) at a given age. In addition to the number of lives remaining at each age, a mortality table typically provides various probabilities associated with the development of these values.\n\nformula_40 is the number of people alive, relative to an original cohort, at age formula_41. As age increases the number of people alive decreases.\n\nformula_42 is the starting point for formula_40: the number of people alive at age 0. This is known as the radix of the table. Some mortality tables begin at an age greater than 0, in which case the radix is the number of people assumed to be alive at the youngest age in the table.\n\nformula_44 is the limiting age of the mortality tables. formula_45 is zero for all formula_46.\n\nformula_47 is the number of people who die between age formula_41 and age formula_49. formula_47 may be calculated using the formula formula_51\n\nformula_52 is the probability of death between the ages of formula_41 and age formula_49.\n\nformula_56 is the probability that a life age formula_41 will survive to age formula_49.\n\nSince the only possible alternatives from one age (formula_41) to the next (formula_61) are living and dying, the relationship between these two probabilities is:\n\nThese symbols may also be extended to multiple years, by inserting the number of years at the bottom left of the basic symbol.\n\nformula_63 shows the number of people who die between age formula_41 and age formula_65.\n\nformula_66 is the probability of death between the ages of formula_41 and age formula_65.\n\nformula_70 is the probability that a life age formula_41 will survive to age formula_65.\n\nAnother statistic that can be obtained from a life table is life expectancy.\n\nformula_74 is the curtate expectation of life for a person alive at age formula_41. This is the expected number of complete years remaining to live (you may think of it as the expected number of birthdays that the person will celebrate).\n\nA life table generally shows the number of people alive at integral ages. If we need information regarding a fraction of a year, we must make assumptions with respect to the table, if not already implied by a mathematical formula underlying the table. A common assumption is that of a Uniform Distribution of Deaths (UDD) at each year of age. Under this assumption, formula_77 is a linear interpolation between formula_40 and formula_79. i.e.\n\nThe basic symbol for the present value of an annuity is formula_81. The following notation can then be added:\n\n\nIf the payments to be made under an annuity are independent of any life event, it is known as an annuity-certain. Otherwise, in particular if payments end upon the beneficiary's death, it is called a life annuity.\n\nformula_82 (read \"a-angle-n at i\") represents the present value of an annuity-immediate, which is a series of unit payments at the end of each year for formula_83 years (in other words: the value one period before the first of \"n\" payments). This value is obtained from:\n\nformula_85 represents the present value of an annuity-due, which is a series of unit payments at the beginning of each year for formula_83 years (in other words: the value at the time of the first of \"n\" payments). This value is obtained from:\n\nformula_88 is the value at the time of the last payment, formula_89 the value one period later.\n\nIf the symbol formula_90 is added to the top-right corner, it represents the present value of an annuity whose payments occur each one formula_4th of a year for a period of formula_83 years, and each payment is one formula_4th of a unit.\n\nformula_96 is the limiting value of formula_97 when formula_4 increases without bound. The underlying annuity is known as a continuous annuity.\n\nThe present values of these annuities may be compared as follows:\n\nTo understand the relationships shown above, consider that cash flows paid at a later time have a smaller present value than cash flows of the same total amount that are paid at earlier times.\n\n\nA life annuity is an annuity whose payments are contingent on the continuing life of the annuitant. The age of the annuitant is an important consideration in calculating the actuarial present value of an annuity.\n\n\nFor example:\n\nformula_104 indicates an annuity of 1 unit per year payable at the end of each year until death to someone currently age 65\n\nformula_105 indicates an annuity of 1 unit per year payable for 10 years with payments being made at the end of each year\n\nformula_106 indicates an annuity of 1 unit per year for 10 years, or until death if earlier, to someone currently age 65\n\nformula_107 indicates an annuity of 1 unit per year until the earlier death of member or death of spouse, to someone currently age 65 and spouse age 64\n\nformula_108 indicates an annuity of 1 unit per year until the later death of member or death of spouse, to someone currently age 65 and spouse age 64. \n\nformula_109 indicates an annuity of 1 unit per year payable 12 times a year (1/12 unit per month) until death to someone currently age 65\n\nformula_110 indicates an annuity of 1 unit per year payable at the start of each year until death to someone currently age 65\n\nor in general:\n\nformula_111, where formula_41 is the age of the annuitant, formula_83 is the number of years of payments (or until death if earlier), formula_4 is the number of payments per year, and formula_101 is the interest rate.\n\nIn the interest of simplicity the notation is limited and does not, for example, show whether the annuity is payable to a man or a woman (a fact that would typically be determined from the context, including whether the life table is based on male or female mortality rates).\nThe Actuarial Present Value of life contingent payments can be treated as the mathematical expectation of a present value random variable, or calculated through the current payment form.\n\nThe basic symbol for a life insurance is formula_116. The following notation can then be added:\n\n\nFor example:\n\nformula_118 indicates a life insurance benefit of 1 payable at the end of the year of death.\n\nformula_119 indicates a life insurance benefit of 1 payable at the end of the month of death.\n\nformula_120 indicates a life insurance benefit of 1 payable at the (mathematical) instant of death.\n\nAmong actuaries, force of mortality refers to what economists and other social scientists call the hazard rate and is construed as an instantaneous rate of mortality at a certain age measured on an annualized basis.\n\nIn a life table, we consider the probability of a person dying between age (\"x\") and age \"x\" + 1; this probability is called \"q\". In the continuous case, we could also consider the conditional probability that a person who has attained age (\"x\") will die between age (\"x\") and age (\"x\" + Δ\"x\") as:\n\nwhere \"F\"(\"x\") is the cumulative distribution function of the continuous age-at-death random variable, X. As Δ\"x\" tends to zero, so does this probability in the continuous case. The approximate force of mortality is this probability divided by Δ\"x\". If we let Δ\"x\" tend to zero, we get the function for force of mortality, denoted as \"μ\"(\"x\"):\n\n\n"}
{"id": "50628228", "url": "https://en.wikipedia.org/wiki?curid=50628228", "title": "Alexander Lyudskanov", "text": "Alexander Lyudskanov\n\nAleksandăr Lûdskanov (family name sometimes also transliterated as Lyutskanov) () (Sofia, 21 April 1926 – 1976) was a Bulgarian translator, semiotician, mathematician, and expert on machine translation. Ludskanov's work focused on linking translation and semiotics by defining the key component of translation as semiotic transfer, which he defined as replacing the signs that encode a message with signs from another code while doing the utmost to maintain \"invariant information with respect to a given system of reference.\" In 1975, Ludskanov published an article called \"A semiotic approach to the theory of translation\" that argued that semiotics \"does not provide the concept of semiotic transformation, though such transformations certainly exist.\"\n\n"}
{"id": "3785849", "url": "https://en.wikipedia.org/wiki?curid=3785849", "title": "Apotome (mathematics)", "text": "Apotome (mathematics)\n\nIn the historical study of mathematics, an apotome is a line segment formed from a longer line segment by breaking it into two parts, one of which is commensurable only in power to the whole; the other part is the apotome. In this definition, two line segments are said to be \"commensurable only in power\" when the ratio of their lengths is an irrational number but the ratio of their squared lengths is rational.\n\nTranslated into modern algebraic language, an apotome can be interpreted as a quadratic irrational number formed by subtracting one square root of a rational number from another.\nThis concept of the apotome appears in Euclid's Elements beginning in book X, where Euclid defines two special kinds of apotomes. In an apotome of the first kind, the whole is rational, while in an apotome of the second kind, the part subtracted from it is rational; both kinds of apotomes also satisfy an additional condition. Euclid Proposition XIII.6 states that, if a rational line segment is split into two pieces in the golden ratio, then both pieces may be represented as apotomes.\n"}
{"id": "37994961", "url": "https://en.wikipedia.org/wiki?curid=37994961", "title": "C-Thru Ruler", "text": "C-Thru Ruler\n\nThe C-Thru Ruler Company is an American maker of measuring devices and specialized products for drafting, designing and drawing. The company was formed in 1939 in Bloomfield, Connecticut, by Jennie R. Zachs, a schoolteacher, who saw the need for transparent measuring tools such as rulers, triangles, curves and protractors. \n\nDuring the 1990s, the company expanded into the paper crafting and scrapbooking fields under the Little Yellow Bicycle and Déjà Views brands.\n\nIn June 2012, Acme United Corporation bought the ruler, lettering and drafting portions of C-Thru Ruler. The scrap booking part of the business, continues to be managed by the Zachs family under the Little Yellow Bicycle Inc. name.\n\nJennie R. Zachs, born in 1898, was the daughter of Benjamin and Julia Zachs who emigrated from Russia to the United States in the early 1900s. She graduated from high school in Hartford, CT. A few years later, she graduated from college and became a schoolteacher.\n\nWhile teaching, she developed the idea that when students would be able to see through their rulers, it would make the tool much more useful in the classroom. As a result, Ms. Zachs started the development of two transparent rulers made out of plastic.\n\nIn 1939, she founded C-Thru Ruler Company in Bloomfield, Connecticut and designed a whole family of transparent measuring tools like rulers, triangles, curves and protractors. Shortly after, she engaged a supplier to mill the tools out of plastic sheet and began to attend different trade shows and conventions for blue printers and art materials dealers to sell the products. She noticed that the transparent measuring tools could effectively replace wood and metal measuring devices for many applications in drafting, designing and drawing.\n\nOnly one year after founding the company, Jennie Zachs took in two partners to handle the expansion of C-Thru. Edward Zachs, her brother, joined C-Thru and Anna Zachs, her sister, became an investor. On October 14, 1946, C-Thru Ruler Company was formally incorporated in Connecticut.\n\nIn 1957, Edward’s son, Ted also started working at the Company. The following years, sales representatives were hired and the products line was significantly expanded through a distribution arrangement with Letraset, a manufacturer of dry transfer, rub on lettering and lettering tapes.\n\nIn 1970, Letraset decided to start selling its own line of transparent tools. In response thereto, C-Thru acquired a small manufacturer of dry transfer lettering products and started offering its own line of vinyl lettering. One year later, Ted Zachs bought out the other family members and became the sole shareholder of C-Thru.\n\nIn 1983, Jed Zachs, the son of Ted, joined the business and in 1985 another small acquisition was completed, which expanded the line of adhesive backed vinyl letters and stencils for signage purposes.\n\nIn 1993, Ross Zachs, Ted Zachs’ younger son also became part of the company. Jed oversaw manufacturing and related operations, while Ross played a key role in the creative and marketing sides of the business.\n\nThe following year, C-Thru’s owners saw that scrapbooking was growing rapidly and decided to enter that market with a line of stencils, templates and rulers. In the same year, it also created the Deja Views brand and began to sell to hobby and craft dealers, primarily small independent stores and a few chains, including Michaels, which only had a handful of stores at that time.\n\nTo expand the design and paper crafting business, a new brand called Little Yellow Bicycle was launched in 2008 with a differentiated line for independent retailers.\n\nIn June 2012, the non-scrapbooking part of the business was sold to Acme United Corporation. Acme United purchased the inventory, tooling, brands, and other intellectual property for approximately $1.47 million. In 2011, revenues for the C-Thru Ruler part of the business reached about $2.7 million.\n\nThe Zachs family continued to focus on the design and paper crafting business under the Little Yellow Bicycle Inc. name with Ted Zachs as president.\n\nAfter the acquisition, the C-Thru products were gradually integrated into Acme United’s Westcott product family, which mainly consists of rulers, scissors and other school and office products. So, the identity of C-Thru in most cases has become Westcott.\n\n"}
{"id": "39614877", "url": "https://en.wikipedia.org/wiki?curid=39614877", "title": "Chern Prize (ICCM)", "text": "Chern Prize (ICCM)\n\nThe Chern Prize in Mathematics was established in 2001 in honor of Professor Shiing-Shen Chern. The Chern Prize is presented every three years at the International Congress of Chinese Mathematicians to Chinese mathematicians and those of Chinese descent for \"exceptional contributions to mathematical research or to public service activities in support of mathematics\". Winners are selected by a committee of mathematicians to recognize the achievements of mathematicians of Chinese descent. In 2010, a special commemorative event was held in Beijing in addition to the normal award presentation to celebrate the centennial of Professor Chern's birth.\n\n"}
{"id": "26193465", "url": "https://en.wikipedia.org/wiki?curid=26193465", "title": "Comparison of vector algebra and geometric algebra", "text": "Comparison of vector algebra and geometric algebra\n\nVector algebra and geometric algebra are complementary approaches to providing additional algebraic structures on vector spaces, with geometric interpretations, particularly vector fields in multivariable calculus and applications in mathematical physics.\n\nVector algebra is specific to Euclidean 3-space, while geometric algebra uses multilinear algebra and applies in all dimensions and signatures, notably 3+1 spacetime as well as 2 dimensions. They are mathematically equivalent in 3 dimensions, where this article is focused, though the approaches differ. Vector algebra is more widely used in elementary multivariable calculus, while geometric algebra is used in more advanced treatments, and is proposed for elementary use as well.\n\nGeometric algebra (GA) is an extension or completion of vector algebra (VA). The reader is herein assumed to be familiar with the basic concepts and operations of VA and this article will mainly concern itself with operations in formula_1 the GA of 3D space (nor is this article intended to be mathematically rigorous). In GA, vectors are not normally written boldface as the meaning is usually clear from the context.\n\nThe fundamental difference is that GA provides an additional product of vectors called the \"geometric product\". Elements of GA are graded multivectors, scalars are grade 0, the usual vectors are grade 1, bivectors are grade 2 and the highest grade (3 in the 3D case) is traditionally called the pseudoscalar and designated formula_2.\n\nThe ungeneralized 3D vector form of the geometric product is :\n\nthat is the sum of the usual dot (inner) product and the outer (exterior) product (this last is closely related to the cross product and will be explained below).\n\nIn VA, entities such as pseudovectors and pseudoscalars need to be bolted on, whereas in GA the equivalent bivector and pseudovector respectively exist naturally as subspaces of the algebra.\n\nFor example, applying vector calculus in 2 dimensions, such as to compute torque or curl, requires adding an artificial 3rd dimension and extending the vector field to be constant in that dimension, or alternately considering these to be scalars. The torque or curl is then a normal vector field in this 3rd dimension. By contrast, geometric algebra in 2 dimensions defines these as a pseudoscalar field (a bivector), without requiring a 3rd dimension. Similarly, the scalar triple product is ad hoc, and can instead be expressed uniformly using the exterior product and the geometric product.\n\nHere are some comparisons between standard formula_4 vector relations and their corresponding exterior product and geometric product equivalents. All the exterior and geometric product equivalents here are good for more than three dimensions, and some also for two. In two dimensions the cross product is undefined even if what it describes (like torque) is perfectly well defined in a plane without introducing an arbitrary normal vector outside of the space.\n\nMany of these relationships only require the introduction of the exterior product to generalize, but since that may not be familiar to somebody with only a background in vector algebra and calculus, some examples are given.\n\nformula_5 is perpendicular to the plane containing formula_6 and formula_7.\nformula_8 is an oriented representation of the same plane.\n\nWe have the pseudoscalar formula_9 (right handed orthonormal frame) and so\n\nThis yields a convenient definition for the cross product of traditional vector algebra:\n\n(this is antisymmetric). Relevant is the distinction between axial and polar vectors in vector algebra, which is natural in geometric algebra as the distinction between vectors and bivectors (elements of grade two).\n\nThe formula_14 here is a unit pseudoscalar of Euclidean 3-space, which establishes a duality between the vectors and the bivectors, and is named so because of the expected property\n\nThe equivalence of the formula_16 cross product and the exterior product expression above can be confirmed by direct multiplication of formula_17 with a determinant expansion of the exterior product\n\nSee also Cross product as an exterior product. Essentially, the geometric product of a bivector and the pseudoscalar of Euclidean 3-space provides a method of calculation of the Hodge dual.\n\nOrdinarily:\n\nMaking use of the geometric product and the fact that the exterior product of a vector wedge with itself is zero:\n\nIn three dimensions the product of two vector lengths can be expressed in terms of the dot and cross products\n\nThe corresponding generalization expressed using the geometric product is\n\nThis follows from expanding the geometric product of a pair of vectors with its reverse\n\nLinear algebra texts will often use the determinant for the solution of linear systems by Cramer's rule or for and matrix inversion.\n\nAn alternative treatment is to axiomatically introduce the wedge product, and then demonstrate that this can be used directly to solve linear systems. This is shown below, and does not require sophisticated math skills to understand.\n\nIt is then possible to define determinants as nothing more than the coefficients of the wedge product in terms of \"unit \"k\"-vectors\" (formula_26 terms) expansions as above.\n\nWhen linear system solution is introduced via the wedge product, Cramer's rule follows as a side-effect, and there is no need to lead up to the end results with definitions of minors, matrices, matrix invertibility, adjoints, cofactors, Laplace expansions, theorems on determinant multiplication and row column exchanges, and so forth.\n\nMatrix inversion (Cramer's rule) and determinants can be naturally expressed in terms of the wedge product.\n\nThe use of the wedge product in the solution of linear equations can be quite useful for various geometric product calculations.\n\nTraditionally, instead of using the wedge product, Cramer's rule is usually presented as a generic algorithm that can be used to solve linear equations of the form formula_33 (or equivalently to invert a matrix). Namely\n\nThis is a useful theoretic result. For numerical problems row reduction with pivots and other methods are more stable and efficient.\n\nWhen the wedge product is coupled with the Clifford product and put into a natural geometric context, the fact that the determinants are used in the expression of formula_35 parallelogram area and parallelepiped volumes (and higher-dimensional generalizations thereof) also comes as a nice side-effect.\n\nAs is also shown below, results such as Cramer's rule also follow directly from the wedge product's selection of non-identical elements. The end result is then simple enough that it could be derived easily if required instead of having to remember or look up a rule.\n\nTwo variables example\n\nPre- and post-multiplying by formula_37 and formula_38,\n\nProvided formula_41 the solution is\n\nFor formula_43, this is Cramer's rule since the formula_44 factors of the wedge products\n\ndivide out.\n\nSimilarly, for three, or \"N\" variables, the same ideas hold\n\nAgain, for the three variable three equation case this is Cramer's rule since the formula_48 factors of all the wedge products divide out, leaving the familiar determinants.\n\nA numeric example with three equations and two unknowns:\nIn case there are more equations than variables and the equations have a solution, then each of the k-vector quotients will be scalars.\n\nTo illustrate here is the solution of a simple example with three equations and two unknowns.\n\nThe right wedge product with formula_50 solves for formula_51\n\nand a left wedge product with formula_53 solves for formula_54\n\nObserve that both of these equations have the same factor, so\none can compute this only once (if this was zero it would\nindicate the system of equations has no solution).\n\nCollection of results for\nformula_51 and formula_54 yields a Cramer's rule-like form:\n\nWriting formula_59, we have the end result:\n\nFor the plane of all points formula_61 through the plane passing through three independent points formula_62, formula_63, and formula_64, the normal form of the equation is\n\nThe equivalent wedge product equation is\n\nUsing the Gram–Schmidt process a single vector can be decomposed into two components with respect to a reference vector, namely the projection onto a unit vector in a reference direction, and the difference between the vector and that projection.\n\nWith, formula_67, the projection of formula_68 onto formula_69 is\n\nOrthogonal to that vector is the difference, designated the rejection,\n\nNote the similarity in form to the \"w\", \"u\", \"v\" trivector itself\n\nwhich, if you take the set of formula_73 as a basis for the trivector space, suggests this is the natural way to define the length of a trivector. Loosely speaking the length of a vector is a length, length of a bivector is area, and the length of a trivector is volume.\n\nIf a vector is factored directly into projective and rejective terms using the geometric product formula_74, then it is not necessarily obvious that the rejection term, a product of vector and bivector is even a vector. Expansion of the vector bivector product in terms of the standard basis vectors has the following form\n\nThe trivector term is formula_76. Expansion of formula_77 yields the same trivector term (it is the completely symmetric part), and the vector term is negated. Like the geometric product of two vectors, this geometric product can be grouped into symmetric and antisymmetric parts, one of which is a pure k-vector. In analogy the antisymmetric part of this product can be called a generalized dot product, and is roughly speaking the dot product of a \"plane\" (bivector), and a vector.\n\nThe properties of this generalized dot product remain to be explored, but first here is a summary of the notation\n\nLet formula_82, where formula_83, and formula_84. Expressing formula_85 and the formula_86, products in terms of these components is\n\nWith the conditions and definitions above, and some manipulation, it can be shown that the term formula_88, which then justifies the previous solution of the normal to a plane problem. Since the vector term of the vector bivector product the name dot product is zero\nwhen the vector is perpendicular to the plane (bivector), and this vector, bivector \"dot product\" selects only the components that are in the plane, so in analogy to the vector-vector dot product this name itself is justified by more than the fact this is the non-wedge product term of the geometric vector-bivector product.\n\nIt can be shown that a unit vector derivative can be expressed using the cross product\n\n\\frac{d}{dt}\\left(\\frac{\\mathbf r}{\\Vert \\mathbf r \\Vert}\\right)\n"}
{"id": "9903220", "url": "https://en.wikipedia.org/wiki?curid=9903220", "title": "Complete set of invariants", "text": "Complete set of invariants\n\nIn mathematics, a complete set of invariants for a classification problem is a collection of maps\n(where \"X\" is the collection of objects being classified, up to some equivalence relation, and the formula_2 are some sets), such that formula_3 if and only if formula_4 for all \"i\". In words, such that two objects are equivalent if and only if all invariants are equal.\n\nSymbolically, a complete set of invariants is a collection of maps such that\nis injective.\n\nAs invariants are, by definition, equal on equivalent objects, equality of invariants is a \"necessary\" condition for equivalence; a \"complete\" set of invariants is a set such that equality of these is \"sufficient\" for equivalence. In the context of a group action, this may be stated as: invariants are functions of coinvariants (equivalence classes, orbits), and a complete set of invariants characterizes the coinvariants (is a set of defining equations for the coinvariants).\n\n\nA complete set of invariants does not immediately yield a classification theorem: not all combinations of invariants may be realized. Symbolically, one must also determine the image of\n"}
{"id": "87141", "url": "https://en.wikipedia.org/wiki?curid=87141", "title": "Connectedness", "text": "Connectedness\n\nIn mathematics, connectedness is used to refer to various properties meaning, in some sense, \"all one piece\". When a mathematical object has such a property, we say it is connected; otherwise it is disconnected. When a disconnected object can be split naturally into connected pieces, each piece is usually called a \"component\" (or \"connected component\").\n\nA topological space is said to be \"connected\" if it is not the union of two disjoint nonempty open sets. A set is open if it contains no point lying on its boundary; thus, in an informal, intuitive sense, the fact that a space can be partitioned into disjoint open sets suggests that the boundary between the two sets is not part of the space, and thus splits it into two separate pieces.\n\nFields of mathematics are typically concerned with special kinds of objects. Often such an object is said to be \"connected\" if, when it is considered as a topological space, it is a connected space. Thus, manifolds, Lie groups, and graphs are all called \"connected\" if they are connected as topological spaces, and their components are the topological components. Sometimes it is convenient to restate the definition of connectedness in such fields. For example, a graph is said to be \"connected\" if each pair of vertices in the graph is joined by a path. This definition is equivalent to the topological one, as applied to graphs, but it is easier to deal with in the context of graph theory. Graph theory also offers a context-free measure of connectedness, called the clustering coefficient.\n\nOther fields of mathematics are concerned with objects that are rarely considered as topological spaces. Nonetheless, definitions of \"connectedness\" often reflect the topological meaning in some way. For example, in category theory, a category is said to be \"connected\" if each pair of objects in it is joined by a sequence of morphisms. Thus, a category is connected if it is, intuitively, all one piece.\n\nThere may be different notions of \"connectedness\" that are intuitively similar, but different as formally defined concepts. We might wish to call a topological space \"connected\" if each pair of points in it is joined by a path. However this condition turns out to be stronger than standard topological connectedness; in particular, there are connected topological spaces for which this property does not hold. Because of this, different terminology is used; spaces with this property are said to be \"path connected\". While not all connected spaces are path connected, all path connected spaces are connected.\n\nTerms involving \"connected\" are also used for properties that are related to, but clearly different from, connectedness. For example, a path-connected topological space is \"simply connected\" if each loop (path from a point to itself) in it is contractible; that is, intuitively, if there is essentially only one way to get from any point to any other point. Thus, a sphere and a disk are each simply connected, while a torus is not. As another example, a directed graph is \"strongly connected\" if each ordered pair of vertices is joined by a directed path (that is, one that \"follows the arrows\").\n\nOther concepts express the way in which an object is \"not\" connected. For example, a topological space is \"totally disconnected\" if each of its components is a single point.\n\nProperties and parameters based on the idea of connectedness often involve the word \"connectivity\". For example, in graph theory, a connected graph is one from which we must remove at least one vertex to create a disconnected graph. In recognition of this, such graphs are also said to be \"1-connected\". Similarly, a graph is \"2-connected\" if we must remove at least two vertices from it, to create a disconnected graph. A \"3-connected\" graph requires the removal of at least three vertices, and so on. The \"connectivity\" of a graph is the minimum number of vertices that must be removed, to disconnect it. Equivalently, the connectivity of a graph is the greatest integer \"k\" for which the graph is \"k\"-connected.\n\nWhile terminology varies, noun forms of connectedness-related properties often include the term \"connectivity\". Thus, when discussing simply connected topological spaces, it is far more common to speak of \"simple connectivity\" than \"simple connectedness\". On the other hand, in fields without a formally defined notion of \"connectivity\", the word may be used as a synonym for \"connectedness\".\n\nAnother example of connectivity can be found in regular tilings. Here, the connectivity describes the number of neighbors accessible from a single tile:\n"}
{"id": "55589991", "url": "https://en.wikipedia.org/wiki?curid=55589991", "title": "Cora Barbara Hennel", "text": "Cora Barbara Hennel\n\nCora Barbara Hennel (January 21, 1886 – June 26, 1947) was an Indiana mathematician active in the first half of the 20th century.\n\nHennel was born in Evansville, Indiana to Joseph H. and Anna Marie Thuman Hennel. After high school graduation Cora and her older sister Cecilia taught in country grade schools to save money for college. In 1903, both Hennels entered Indiana University and shortly thereafter, convinced their parents to move with their younger sister, Edith, to Bloomington. All three sisters attended and graduated from Indiana University. Hennel earned her earned her A.B. in Mathematics in 1907, her Masters in 1908, and in 1912, became the first person to earn a Ph.D. in Mathematics from Indiana University. As an undergraduate, Cora was the class poet and active in student affairs; in graduate school, she was a founding member and Secretary of the Euclidean Circle, a mathematics club for faculty and students.\n\nAs she worked toward her doctorate, Cora served as Instructor in the Department of Mathematics. She continued in this role after receipt of her degree and in 1916, she was appointed Assistant Professor of Mathematics. She was promoted to Associate Professor in 1923 and became a full Professor in 1936. Hennel spent the entirety of her academic career at Indiana University and was still teaching at the time of her death in 1947.\n\nHennel was an active member of the Indiana University faculty, serving as president of the Bloomington chapter of the American Association of University Professors, the American Association of University Women and served as chair of the Indiana Section of the Mathematical Association of America. Professionally, she was a member of Phi Beta Kappa, Sigma Xi, Pi Lambda Theta, Mortar Board, and the Indiana Academy of Sciences.\n\nIn 1958, Cecilia Hennel established the Cora B. Hennel Memorial Scholarship to honor her sister. The Hennel Scholarships are awarded to students who have demonstrated high ability in mathematics. The department continues to remember Hennel and in 1995, named the faculty/student lounge in the renovated Rawles Hall the Cora B. Hennel Room.\n\n"}
{"id": "3293594", "url": "https://en.wikipedia.org/wiki?curid=3293594", "title": "Decimal representation", "text": "Decimal representation\n\nA decimal representation of a non-negative real number \"r\" is an expression in the form of a series, traditionally written as a sum\n\nwhere \"a\" is a nonnegative integer, and \"a\", \"a\", ... are integers satisfying 0 ≤ \"a\" ≤ 9, called the digits of the decimal representation. The sequence of digits specified may be finite, in which case any further digits \"a\" are assumed to be 0. Some authors forbid decimal representations with a trailing infinite sequence of \"9\"s.\nThis restriction still allows a decimal representation for each non-negative real number, but additionally makes such a representation unique.\nThe number defined by a decimal representation is often written more briefly as\n\nThat is to say, \"a\" is the integer part of \"r\", not necessarily between 0 and 9, and \"a\", \"a\", \"a\", ... are the digits forming the fractional part of \"r\".\n\nBoth notations above are, by definition, the following limit of a sequence:\n\nAny real number can be approximated to any desired degree of accuracy by rational numbers with finite decimal representations.\n\nAssume formula_4. Then for every integer formula_5 there is a finite decimal formula_6 such that\n\nProof:\n\nLet formula_8, where formula_9.\nThen formula_10, and the result follows from dividing all sides by formula_11.\n\nSome real numbers formula_13 have two infinite decimal representations. For example, the number 1 may be equally represented by 1.000... as by 0.999... (where the infinite sequences of trailing 0's or 9's, respectively, are represented by \"...\"). Conventionally, the decimal representation without trailing 9's is preferred. Moreover, in the \"standard decimal representation\" of formula_13, an infinite sequence of trailing 0's appearing after the decimal point is omitted, along with the decimal point itself, if formula_13 is an integer. \n\nCertain procedures for constructing the decimal expansion of formula_13 will avoid the problem of trailing 9's. For instance, the following procedure will give the standard decimal representation: Given formula_4, we can first define formula_18 (the \"integer part\" of formula_13) to be the largest integer such that formula_20 (i.e., formula_21). Then, for formula_22 already found, we define formula_23 inductively to be the largest integer such that formula_24The procedure terminates when formula_23 is found such that equality holds in formula_26; otherwise, it continues indefinitely to give an infinite sequence of decimal digits. It can be shown that formula_27 (conventionally written as formula_28), with formula_29 and formula_30. This construction is extended to formula_31 by applying the above procedure to formula_32 and denoting the resultant decimal expansion by formula_33.\n\nThe decimal expansion of non-negative real number \"x\" will end in zeros (or in nines) if, and only if, \"x\" is a rational number whose denominator is of the form 25, where \"m\" and \"n\" are non-negative integers.\n\nProof:\n\nIf the decimal expansion of \"x\" will end in zeros, or formula_34\nfor some \"n\",\nthen the denominator of \"x\" is of the form 10 = 25.\n\nConversely, if the denominator of \"x\" is of the form 25,\nformula_35\nfor some \"p\".\nWhile \"x\" is of the form formula_36,\nformula_37 for some \"n\".\nBy formula_38,\n\"x\" will end in zeros.\n\nSome real numbers have decimal expansions that eventually get into loops, endlessly repeating a sequence of one or more digits:\nEvery time this happens the number is still a rational number (i.e. can alternatively be represented as a ratio of an integer and a positive integer).\nAlso the converse is true: The decimal expansion of a rational number is either finite, or endlessly repeating.\n\n\n"}
{"id": "677191", "url": "https://en.wikipedia.org/wiki?curid=677191", "title": "Differential (mathematics)", "text": "Differential (mathematics)\n\nIn mathematics, differential refers to infinitesimal differences or to the derivatives of functions. The term is used in various branches of mathematics such as calculus, differential geometry, algebraic geometry and algebraic topology.\n\n\nThe notion of a differential motivates several concepts in differential geometry (and differential topology).\n\nDifferentials are also important in algebraic geometry, and there are several important notions.\n\nThe term \"differential\" has also been adopted in homological algebra and algebraic topology, because of the role the exterior derivative plays in de Rham cohomology: in a cochain complex formula_1, the maps (or \"coboundary operators\") \"d\" are often called differentials. Dually, the boundary operators in a chain complex are sometimes called \"codifferentials\".\n\nThe properties of the differential also motivate the algebraic notions of a \"derivation\" and a \"differential algebra\".\n"}
{"id": "24073609", "url": "https://en.wikipedia.org/wiki?curid=24073609", "title": "Effective selfing model", "text": "Effective selfing model\n\nThe effective selfing model is a mathematical model that describes the mating system of a plant population in terms of the degree of self-fertilisation present.\n\nIt was developed in the 1980s by Kermit Ritland, as an alternative to the simplistic mixed mating model. The mixed mating model assumes that every fertilisation event may be classed as either self-fertilisation, or outcrossing with a completely random mate. That is, it assumes that inbreeding is caused solely by self-fertilisation. This assumption is often violated in wild plant populations, where inbreeding may be due to outcrossing between closely related plants. For example, in dense stands, mating often occurs between plants in close proximity; and in plants with short seed dispersal distances, plants are often closely related to their nearest neighbours. When both these criteria are met, plants will tend to be closely related to the near neighbours with which they mate, resulting in significant inbreeding. In such a scenario, the mixed mating model will attribute all inbreeding to self-fertilisation, and therefore overestimate the extent of self-fertilisation occurring. The effective selfing model takes into account the potential for inbreeding to occur as a result of outcrossing between closely related plants, by considering the extent of kinship between mates.\n\nUltimately, it is not possible to tease apart the two potential causes of inbreeding, and attributed the observed inbreeding to one cause or the other. Therefore, just as with the mixed mating model, in the effective selfing model there is only one parameter to be estimated. However this parameter, termed the effective selfing rate, is often a more accurate measure of the proportion of self-fertilisation than the corresponding parameter in the mixed mating model.\n"}
{"id": "8586502", "url": "https://en.wikipedia.org/wiki?curid=8586502", "title": "Enrolled actuary", "text": "Enrolled actuary\n\nAn enrolled actuary is an actuary enrolled by the Joint Board for the Enrollment of Actuaries under the Employee Retirement Income Security Act of 1974 (ERISA). Enrolled actuaries, under regulations of the Department of the Treasury and the Department of Labor, perform a variety of tasks with respect to pension plans in the United States under ERISA. As of August 2016, there were approximately 4,200 enrolled actuaries.\n\nThe Joint Board for the Enrollment of Actuaries administers two examinations to prospective enrolled actuaries. Once the two examinations have been passed, and an individual has also obtained sufficient relevant professional experience, that individual becomes an enrolled actuary.\n\nThe first exam (EA-1) tests basic knowledge of the mathematics of compound interest, the mathematics of life contingencies, and practical demographic analysis. \n\nThe second (EA-2) examination consists of two segments, which are offered during separate exam sittings in either the fall or the spring. Segment F covers the selection of actuarial assumptions, actuarial cost methods, and the calculation of minimum (required) and maximum (tax-deductible) contributions to pension plans. Segment L tests knowledge of relevant federal pension laws (in particular, the provisions of ERISA) as they affect pension actuarial practice.\n\nEnrolled actuaries generally work for human resource consulting firms, investment and insurance brokers, accounting firms, government organizations, and law firms. Some firms that employ enrolled actuaries combine two or more of these practice specialties.\n\nMany enrolled actuaries belong to one or more of the following organizations: the Society of Actuaries, the American Academy of Actuaries. the Conference of Consulting Actuaries or the American Society of Pension Professionals & Actuaries.\n\n"}
{"id": "3698082", "url": "https://en.wikipedia.org/wiki?curid=3698082", "title": "Entitative graph", "text": "Entitative graph\n\nAn entitative graph is an element of the diagrammatic syntax for logic that Charles Sanders Peirce developed under the name of qualitative logic beginning in the 1880s, taking the coverage of the formalism only as far as the propositional or sentential aspects of logic are concerned. See 3.468, 4.434, and 4.564 in Peirce's \"Collected Papers\".\n\nThe syntax is:\n\nThe semantics are:\n\nA \"proof\" manipulates a graph, using a short list of rules, until the graph is reduced to an empty cut or the blank page. A graph that can be so reduced is what is now called a tautology (or the complement thereof). Graphs that cannot be simplified beyond a certain point are analogues of the satisfiable formulas of first-order logic.\n\nPeirce soon abandoned the entitative graphs for the existential graphs, whose sentential (\"alpha\") part is dual to the entitative graphs. He developed the existential graphs until they became another formalism for what are now termed first-order logic and normal modal logic.\n\nThe primary algebra of G. Spencer-Brown is isomorphic to the entitative graphs.\n\n\n"}
{"id": "23831652", "url": "https://en.wikipedia.org/wiki?curid=23831652", "title": "Erdős Prize", "text": "Erdős Prize\n\nThe Anna and Lajos Erdős Prize in Mathematics is a prize given by the Israel Mathematical Union to an Israeli mathematician (in any field of mathematics and computer science), \"with preference to candidates up to the age of 40.\" The prize was established by Paul Erdős in 1977 in honor of his parents, and is awarded annually or biannually. The name was changed from \"Erdős Prize\" in 1996, after Erdős's death, to reflect his original wishes.\n\nSource: Israel Mathematical Union\n"}
{"id": "244107", "url": "https://en.wikipedia.org/wiki?curid=244107", "title": "Euclid's Elements", "text": "Euclid's Elements\n\nThe Elements ( \"Stoicheia\") is a mathematical treatise consisting of 13 books attributed to the ancient Greek mathematician Euclid in Alexandria, Ptolemaic Egypt c. 300 BC. It is a collection of definitions, postulates, propositions (theorems and constructions), and mathematical proofs of the propositions. The books cover plane and solid Euclidean geometry, elementary number theory, and incommensurable lines. \"Elements\" is the oldest extant large-scale deductive treatment of mathematics. It has proven instrumental in the development of logic and modern science, and its logical rigor was not surpassed until the 19th century.\n\nEuclid's \"Elements\" has been referred to as the most successful and influential textbook ever written. It was one of the very earliest mathematical works to be printed after the invention of the printing press and has been estimated to be second only to the Bible in the number of editions published since the first printing in 1482, with the number reaching well over one thousand. For centuries, when the quadrivium was included in the curriculum of all university students, knowledge of at least part of Euclid's \"Elements\" was required of all students. Not until the 20th century, by which time its content was universally taught through other school textbooks, did it cease to be considered something all educated people had read.\n\nScholars believe that the \"Elements\" is largely a compilation of propositions based on books by earlier Greek mathematicians.\n\nProclus (412–485 AD), a Greek mathematician who lived around seven centuries after Euclid, wrote in his commentary on the \"Elements\": \"Euclid, who put together the \"Elements\", collecting many of Eudoxus' theorems, perfecting many of Theaetetus', and also bringing to irrefragable demonstration the things which were only somewhat loosely proved by his predecessors\".\n\nPythagoras (c. 570–495 BC) was probably the source for most of books I and II, Hippocrates of Chios (c. 470–410 BC, not the better known Hippocrates of Kos) for book III, and Eudoxus of Cnidus (c. 408–355 BC) for book V, while books IV, VI, XI, and XII probably came from other Pythagorean or Athenian mathematicians. The \"Elements\" may have been based on an earlier textbook by Hippocrates of Chios, who also may have originated the use of letters to refer to figures.\n\nIn the fourth century AD, Theon of Alexandria produced an edition of Euclid which was so widely used that it became the only surviving source until François Peyrard's 1808 discovery at the Vatican of a manuscript not derived from Theon's. This manuscript, the Heiberg manuscript, is from a Byzantine workshop around 900 and is the basis of modern editions. Papyrus Oxyrhynchus 29 is a tiny fragment of an even older manuscript, but only contains the statement of one proposition.\n\nAlthough known to, for instance, Cicero, no record exists of the text having been translated into Latin prior to Boethius in the fifth or sixth century. The Arabs received the \"Elements\" from the Byzantines around 760; this version was translated into Arabic under Harun al Rashid c. 800. The Byzantine scholar Arethas commissioned the copying of one of the extant Greek manuscripts of Euclid in the late ninth century. Although known in Byzantium, the \"Elements\" was lost to Western Europe until about 1120, when the English monk Adelard of Bath translated it into Latin from an Arabic translation.\n\nCopies of the Greek text still exist, some of which can be found in the Vatican Library and the Bodleian Library in Oxford. The manuscripts available are of variable quality, and invariably incomplete. By careful analysis of the translations and originals, hypotheses have been made about the contents of the original text (copies of which are no longer available).\n\nAncient texts which refer to the \"Elements\" itself, and to other mathematical theories that were current at the time it was written, are also important in this process. Such analyses are conducted by J. L. Heiberg and Sir Thomas Little Heath in their editions of the text.\n\nAlso of importance are the scholia, or annotations to the text. These additions, which often distinguished themselves from the main text (depending on the manuscript), gradually accumulated over time as opinions varied upon what was worthy of explanation or further study.\n\nThe \"Elements\" is still considered a masterpiece in the application of logic to mathematics. In historical context, it has proven enormously influential in many areas of science. Scientists Nicolaus Copernicus, Johannes Kepler, Galileo Galilei, and Sir Isaac Newton were all influenced by the \"Elements\", and applied their knowledge of it to their work. Mathematicians and philosophers, such as Thomas Hobbes, Baruch Spinoza, Alfred North Whitehead, and Bertrand Russell, have attempted to create their own foundational \"Elements\" for their respective disciplines, by adopting the axiomatized deductive structures that Euclid's work introduced.\n\nThe austere beauty of Euclidean geometry has been seen by many in western culture as a glimpse of an otherworldly system of perfection and certainty. Abraham Lincoln kept a copy of Euclid in his saddlebag, and studied it late at night by lamplight; he related that he said to himself, \"You never can make a lawyer if you do not understand what demonstrate means; and I left my situation in Springfield, went home to my father's house, and stayed there till I could give any proposition in the six books of Euclid at sight\". Edna St. Vincent Millay wrote in her sonnet \"\", \"O blinding hour, O holy, terrible day, When first the shaft into his vision shone Of light anatomized!\". Einstein recalled a copy of the \"Elements\" and a magnetic compass as two gifts that had a great influence on him as a boy, referring to the Euclid as the \"holy little geometry book\".\n\nThe success of the \"Elements\" is due primarily to its logical presentation of most of the mathematical knowledge available to Euclid. Much of the material is not original to him, although many of the proofs are his. However, Euclid's systematic development of his subject, from a small set of axioms to deep results, and the consistency of his approach throughout the \"Elements\", encouraged its use as a textbook for about 2,000 years. The \"Elements\" still influences modern geometry books. Further, its logical axiomatic approach and rigorous proofs remain the cornerstone of mathematics.\n\n\nEuclid's axiomatic approach and constructive methods were widely influential.\n\nMany of Euclid's propositions were constructive, demonstrating the existence of some figure by detailing the steps he used to construct the object using a compass and straightedge. His constructive approach appears even in his geometry's postulates, as the first and third postulates stating the existence of a line and circle are constructive. Instead of stating that lines and circles exist per his prior definitions, he states that it is possible to 'construct' a line and circle. It also appears that, for him to use a figure in one of his proofs, he needs to construct it in an earlier proposition. For example, he proves the Pythagorean theorem by first inscribing a square on the sides of a right triangle, but only after constructing a square on a given line one proposition earlier.\n\nAs was common in ancient mathematical texts, when a proposition needed proof in several different cases, Euclid often proved only one of them (often the most difficult), leaving the others to the reader. Later editors such as Theon often interpolated their own proofs of these cases.\n\nEuclid's presentation was limited by the mathematical ideas and notations in common currency in his era, and this causes the treatment to seem awkward to the modern reader in some places. For example, there was no notion of an angle greater than two right angles, the number 1 was sometimes treated separately from other positive integers, and as multiplication was treated geometrically he did not use the product of more than 3 different numbers. The geometrical treatment of number theory may have been because the alternative would have been the extremely awkward Alexandrian system of numerals.\n\nThe presentation of each result is given in a stylized form, which, although not invented by Euclid, is recognized as typically classical. It has six different parts: First is the 'enunciation', which states the result in general terms (i.e., the statement of the proposition). Then comes the 'setting-out', which gives the figure and denotes particular geometrical objects by letters. Next comes the 'definition' or 'specification', which restates the enunciation in terms of the particular figure. Then the 'construction' or 'machinery' follows. Here, the original figure is extended to forward the proof. Then, the 'proof' itself follows. Finally, the 'conclusion' connects the proof to the enunciation by stating the specific conclusions drawn in the proof, in the general terms of the enunciation.\n\nNo indication is given of the method of reasoning that led to the result, although the \"Data\" does provide instruction about how to approach the types of problems encountered in the first four books of the \"Elements\". Some scholars have tried to find fault in Euclid's use of figures in his proofs, accusing him of writing proofs that depended on the specific figures drawn rather than the general underlying logic, especially concerning Proposition II of Book I. However, Euclid's original proof of this proposition, is general, valid, and does not depend on the figure used as an example to illustrate one given configuration.\n\nEuclid's list of axioms in the \"Elements\" was not exhaustive, but represented the principles that were the most important. His proofs often invoke axiomatic notions which were not originally presented in his list of axioms. Later editors have interpolated Euclid's implicit axiomatic assumptions in the list of formal axioms.\n\nFor example, in the first construction of Book 1, Euclid used a premise that was neither postulated nor proved: that two circles with centers at the distance of their radius will intersect in two points. Later, in the fourth construction, he used superposition (moving the triangles on top of each other) to prove that if two sides and their angles are equal, then they are congruent; during these considerations he uses some properties of superposition, but these properties are not described explicitly in the treatise. If superposition is to be considered a valid method of geometric proof, all of geometry would be full of such proofs. For example, propositions I.1 – I.3 can be proved trivially by using superposition.\n\nMathematician and historian W. W. Rouse Ball put the criticisms in perspective, remarking that \"the fact that for two thousand years [the \"Elements\"] was the usual text-book on the subject raises a strong presumption that it is not unsuitable for that purpose.\"\n\nIt was not uncommon in ancient time to attribute to celebrated authors works that were not written by them. It is by these means that the apocryphal books XIV and XV of the \"Elements\" were sometimes included in the collection. The spurious Book XIV was probably written by Hypsicles on the basis of a treatise by Apollonius. The book continues Euclid's comparison of regular solids inscribed in spheres, with the chief result being that the ratio of the surfaces of the dodecahedron and icosahedron inscribed in the same sphere is the same as the ratio of their volumes, the ratio being\n\nThe spurious Book XV was probably written, at least in part, by Isidore of Miletus. This book covers topics such as counting the number of edges and solid angles in the regular solids, and finding the measure of dihedral angles of faces that meet at an edge.\n\n\n\n\n\n\n\n"}
{"id": "58318372", "url": "https://en.wikipedia.org/wiki?curid=58318372", "title": "European Women in Mathematics", "text": "European Women in Mathematics\n\nEuropean Women in Mathematics (EWM) is an international professional organization dedicated to supporting women's engagement in mathematics. Its goals include encouraging women to study mathematics and providing visibility to women mathematicians. It is the \"first and best known\" of several organizations devoted to women in mathematics in Europe.\n\nAlthough the group that became EWM began holding informal meetings as early as 1974,\nEWM was founded as an organization in 1986, inspired by the activities of the Association for Women in Mathematics in the USA. It was established as an association under Finnish law in 1993 with its seat in Helsinki.\n\nThe organization has a Scientific Committee, jointly with the European Mathematical Society and its Committee on Women in Mathematics.\n\nEWM hold a General Meeting every other year in the form of a week-long conference with a scientific program of mini-courses on mathematical topics, discussions on the situation of women in the field and a General Assembly. \n\nGeneral meetings have been held in Paris (1986), Copenhagen (1987), Warwick (1988), Lisbon (1990), Marseilles (1991), Warsaw (1993), Madrid (1995), Trieste ICTP (1997), Hannover (1999), Malta (2001), Luminy (2003), Volgograd (2005), Cambridge (2007), Novi Sad (2009), Barcelona (2011), Bonn (2013), Cortona (2015), and Graz (2018).\n\nEWM holds satellite conferences to the European Congress in Mathematics and takes part in ICWM International Conference of Women in Mathematics, International Congress of Women Mathematicians and now World Meeting for Women Mathematicians. \n\n"}
{"id": "239599", "url": "https://en.wikipedia.org/wiki?curid=239599", "title": "Fermat's theorem", "text": "Fermat's theorem\n\nThe works of the 17th-century mathematician Pierre de Fermat engendered many theorems. Fermat's theorem may refer to one of the following theorems:\n\n\n"}
{"id": "47769616", "url": "https://en.wikipedia.org/wiki?curid=47769616", "title": "Gifted (film)", "text": "Gifted (film)\n\nGifted is a 2017 American drama film directed by Marc Webb and written by Tom Flynn. It stars Chris Evans, Mckenna Grace, Lindsay Duncan, Jenny Slate and Octavia Spencer. The plot follows an intellectually gifted 7-year-old who becomes the subject of a custody battle between her uncle and grandmother. The film was released on April 7, 2017, by Fox Searchlight Pictures, and grossed $43 million worldwide.\n\nIn a small town near Tampa, Florida, seven-year-old Mary Adler lives with uncle and \"de facto\" guardian, Frank. Her best friend is her 40-ish neighbor, Roberta. On her first day of first grade, she shows remarkable mathematical talent, which impresses her teacher, Bonnie Stevenson. There, despite her initial disdain for average children her own age and her boredom with their classwork, she begins to bond with them when she brings her one-eyed cat, Fred, for show-and-tell and later defends a classmate from a bully on the bus. Mary is offered a scholarship to a private school for gifted children. However, Frank turns it down. Based on his family's experiences with similar schools, he fears Mary will not have a chance at a \"normal\" childhood.\n\nIt emerges that Mary's mother, Diane, had been a promising mathematician, dedicated to the Navier–Stokes problem (one of the unsolved Millennium Prize Problems) before taking her own life when Mary was six months old. Mary has lived with Frank, a former college professor turned boat repairman, ever since.\n\nFrank's estranged mother and Mary's maternal grandmother, Evelyn, seeks to gain custody of Mary and move her to Massachusetts, believing that Mary is a \"one-in-a-billion\" mathematical prodigy who should be specially tutored in preparation for a life devoted to mathematics, much as Diane was. However, Frank is adamant that his sister would want Mary to be in a normal public school and have the childhood she didn't have. Worried that the judge will rule against him and he will lose Mary completely, Frank accepts a compromise brokered by his lawyer Greg Cullen that sees Mary placed in foster care and attend the private school where Evelyn wants to have her enrolled. The foster parents live just 25 minutes from Frank's home, Frank will be entitled to scheduled visits, and Mary will be able to decide where she wants to live after her 12th birthday.\n\nMary is devastated at being placed in foster care, and her foster father says she refuses to see Frank. When Bonnie sees a picture of Fred up for adoption, she alerts Frank. Frank retrieves the cat from the pound and, learning that Fred was brought in due to allergy issues, realizes that Evelyn—who is allergic to cats—is overseeing Mary's education in the guest house of Mary's foster home. He then reveals to Evelyn—who had been a mathematician herself—that Diane \"had\" solved the Navier–Stokes problem, but stipulated that the solution was to be withheld until Evelyn's death. Knowing that it meant everything to Evelyn to see Diane solve the problem, Frank offers Evelyn the opportunity to publish Diane's work if she drops her objection to his having custody of Mary. Evelyn reluctantly agrees.\n\nThe film ends with Mary back in the custody of Frank, returning to public school while taking college-level courses in the mornings.\n\nIn August 2015, it was announced Chris Evans had been cast in the film, with Marc Webb directing from a screenplay by Tom Flynn. In September 2015, Mckenna Grace, Octavia Spencer, Lindsay Duncan and Jenny Slate joined the cast, and in November 2015, Julie Ann Emery was also added.\n\nFilming began in October 2015 in Savannah, Georgia, as well as in Tybee Island, Georgia. and finished Nov 20, 2015.\n\nThe film was scheduled to be released on April 12, 2017, but was pushed up to April 7, 2017.\n\n\"Gifted\" grossed $24.8 million in the United States and Canada and $17.8 million in other territories for a worldwide total of $42.6 million, against a production budget of $7 million.\n\nThe film went wide on Wednesday, April 12, 2017, and in its opening weekend grossed $3.1 million, finishing 6th at the box office. In its second weekend of wide expansion, it added more screens, and made $4.6 million, an increase of 47.5% from the previous week.\n\nOn review aggregation website Rotten Tomatoes, \"Gifted\" has an approval rating of 73% based on 163 reviews, with an average rating of 6.4/10. The site's critical consensus reads, \"\"Gifted\" isn't quite as bright as its pint-sized protagonist, but a charming cast wrings respectably engaging drama out of a fairly predictable premise.\" On Metacritic, the film has a weighted average score of 60 out of 100, based on 33 critics, indicating \"mixed or average reviews\". Audiences polled by CinemaScore gave the film an average grade of \"A\" on an A+ to F scale.\n\nColin Covert of the \"Star Tribune\" gave the film 3/4 stars, saying, \"Sure, it's a simple, straightforward film, but sometimes that's all you need as long as its heart is true.\" Richard Roeper gave the film 4 out of 4 stars and said, \"\"Gifted\" isn't the best or most sophisticated or most original film of the year so far – but it just might be my favorite.\"\n\n\n"}
{"id": "1248704", "url": "https://en.wikipedia.org/wiki?curid=1248704", "title": "Ideal theory", "text": "Ideal theory\n\nIn mathematics, ideal theory is the theory of ideals in commutative rings; and is the precursor name for the contemporary subject of commutative algebra. The name grew out of the central considerations, such as the Lasker–Noether theorem in algebraic geometry, and the ideal class group in algebraic number theory, of the commutative algebra of the first quarter of the twentieth century. It was used in the influential van der Waerden text on abstract algebra from around 1930.\n\nThe ideal theory in question had been based on elimination theory, but in line with David Hilbert's taste moved away from algorithmic methods. Gröbner basis theory has now reversed the trend, for computer algebra.\n\nThe importance of the ideal in general of a module, more general than an \"ideal\", probably led to the perception that \"ideal theory\" was too narrow a description. Valuation theory, too, was an important technical extension, and was used by Helmut Hasse and Oscar Zariski. Bourbaki used \"commutative algebra\"; sometimes \"local algebra\" is applied to the theory of local rings. D. G. Northcott's 1953 Cambridge Tract \"Ideal Theory\" (reissued 2004 under the same title) was one of the final appearances of the name.\n"}
{"id": "4492813", "url": "https://en.wikipedia.org/wiki?curid=4492813", "title": "Indiana Pi Bill", "text": "Indiana Pi Bill\n\nThe Indiana Pi Bill is the popular name for bill #246 of the 1897 sitting of the Indiana General Assembly, one of the most notorious attempts to establish mathematical truth by legislative fiat. Despite its name, the main result claimed by the bill is a method to square the circle, rather than to establish a certain value for the mathematical constant , the ratio of the circumference of a circle to its diameter. The bill, written by amateur mathematician Edward J. Goodwin, does imply various incorrect values of , such as 3.2.\n\nThe bill never became law, due to the intervention of Professor C. A. Waldo of Purdue University, who happened to be present in the legislature on the day it went up for a vote.\n\nThe impossibility of squaring the circle using only compass and straightedge constructions, suspected since ancient times, was rigorously proven in 1882 by Ferdinand von Lindemann. Better approximations of than those implied by the bill have been known since ancient times.\n\nIn 1894, Indiana physician and amateur mathematician Edward J. Goodwin (ca. 1825–1902) believed that he had discovered a correct way of squaring the circle. He proposed a bill to state representative Taylor I. Record, which Record introduced in the House under the long title \"A Bill for an act introducing a new mathematical truth and offered as a contribution to education to be used only by the State of Indiana free of cost by paying any royalties whatever on the same, provided it is accepted and adopted by the official action of the Legislature of 1897\".\n\nThe text of the bill consists of a series of mathematical claims (detailed below), followed by a recitation of Goodwin's previous accomplishments:\n\nGoodwin's \"solutions\" were indeed published in the \"American Mathematical Monthly\", though with a disclaimer of \"published by request of the author\".\n\nUpon its introduction in the Indiana House of Representatives, the bill's language and topic occasioned confusion among the membership; a member from Bloomington proposed that it be referred to the Finance Committee, but the Speaker accepted another member's recommendation to refer the bill to the Committee on Swamplands, where the bill could \"find a deserved grave\". It was transferred to the Committee on Education, which reported favorably; following a motion to suspend the rules, the bill passed on February 6, without a dissenting vote. The news of the bill occasioned an alarmed response from \"Der Tägliche Telegraph\", a German-language newspaper in Indianapolis, which viewed the event with significantly less favor than its English-speaking competitors. As this debate concluded, Purdue University Professor C. A. Waldo arrived in Indianapolis to secure the annual appropriation for the Indiana Academy of Science. An assemblyman handed him the bill, offering to introduce him to the genius who wrote it. He declined, saying that he already met as many crazy people as he cared to.\n\nWhen it reached the Indiana Senate, the bill was not treated so kindly, for Waldo had coached the senators previously. The committee to which it had been assigned reported it unfavorably, and the Senate tabled it on February 12; it was nearly passed, but opinion changed when one senator observed that the General Assembly lacked the power to define mathematical truth. Influencing some of the senators was a report that major newspapers, such as the \"Chicago Tribune\", had begun to ridicule the situation.\n\nAccording to the \"Indianapolis News\" article of February 13, page 11, column 3:\n\n... the bill was brought up and made fun of. The Senators made bad puns about it, ridiculed it and laughed over it. The fun lasted half an hour. Senator Hubbell said that it was not meet for the Senate, which was costing the State $250 a day, to waste its time in such frivolity. He said that in reading the leading newspapers of Chicago and the East, he found that the Indiana State Legislature had laid itself open to ridicule by the action already taken on the bill. He thought consideration of such a proposition was not dignified or worthy of the Senate. He moved the indefinite postponement of the bill, and the motion carried.\n\nAlthough the bill has become known as the \"Pi Bill\", its text does not mention the name \"pi\" at all, and Goodwin appears to have thought of the ratio between the circumference and diameter of a circle as distinctly secondary to his main aim of squaring the circle. Towards the end of Section 2 the following passage appears:\n\nThis comes close to an explicit claim that formula_1, and that formula_2.\n\nThis quotation is often read as three mutually incompatible assertions, but they fit together well if the statement about is taken to be about the inscribed square (with the circle's diameter as diagonal) rather than the square on the radius (with the chord of 90° as diagonal). Together they describe the circle shown in the figure, whose diameter is 10 and circumference is 32; the chord of 90° is taken to be 7. Both of the values 7 and 32 are within a few percent of the true lengths for a diameter-10 circle (which does not justify Goodwin's presentation of them as exact). The circumference should be nearer to 31.4159 and the diagonal \"7\" should be the square root of 50 (=25+25), or nearer to 7.071.\n\nGoodwin's main goal was not to measure lengths in the circle but to \"square\" it, which he interpreted literally as finding a square with the same area as the circle. He knew that Archimedes' formula for the area of a circle, which calls for multiplying the diameter by one fourth of the circumference, is not considered a solution to the ancient problem of squaring the circle. This is because the problem is to \"construct\" the area using compass and straightedge only, and Archimedes did not give a method for constructing a straight line with the same length as the circumference. Apparently, Goodwin was unaware of this central requirement; he believed that the problem with the Archimedean formula is that it gives wrong numerical results, and that a solution of the ancient problem should consist of replacing it with a \"correct\" formula. In the bill he proposed, without argument, his own method:\n\nThis appears needlessly convoluted, as an \"equilateral rectangle\" is, by definition, a square. In simple terms, the assertion is that the area of a circle is the same as that of a square with the same perimeter. This claim results in other mathematical contradictions to which Goodwin attempts to respond. For example, right after the above quote the bill goes on to say:\n\nIn the model circle above, the Archimedean area (accepting Goodwin's values for the circumference and diameter) would be 80, whereas Goodwin's proposed rule leads to an area of 64. Now, 80 exceeds 64 by one fifth \"of 80\", and Goodwin appears to confuse 64 = 80×(1−) with 80 = 64×(1+), an approximation that works only for fractions much smaller than .\n\nThe area found by Goodwin's rule is times the true area of the circle, which in many accounts of the Pi Bill is interpreted as a claim that = 4. However, there is no internal evidence in the bill that Goodwin intended to make such a claim; on the contrary, he repeatedly denies that the area of the circle has anything to do with its diameter.\n\nThe relative \"area\" error of 1− works out to about 21 percent, which is much more grave than the approximations of the \"lengths\" in the model circle of the previous section. It is unknown what made Goodwin believe that his rule could be correct. In general, figures with identical perimeters do not have identical area (see isoperimetry); the typical demonstration of this fact is to compare a long thin shape with a small enclosed area (the area approaching zero as the width decreases) to one of the same perimeter that is approximately as tall as it is wide (the area approaching the square of the width), obviously of much greater area.\n\n\n"}
{"id": "9087019", "url": "https://en.wikipedia.org/wiki?curid=9087019", "title": "Inverse Symbolic Calculator", "text": "Inverse Symbolic Calculator\n\nThe Inverse Symbolic Calculator is an online number checker established July 18, 1995 by Peter Benjamin Borwein, Jonathan Michael Borwein and Simon Plouffe of the Canadian Centre for Experimental and Constructive Mathematics (Burnaby, Canada). A user will input a number and the Calculator will use an algorithm to search for and calculate closed-form expressions or suitable functions that have roots near this number. Hence, the calculator is of great importance for those working in numerical areas of experimental mathematics.\n\nThe ISC contains 54 million mathematical constants. Plouffe's Inverter (opened in 1998) contains 214 million. A newer version of the tables with 3.702 billion entries (as of June 19, 2010) exists.\n\nIn 2016, Plouffe released a portable version of Plouffe's Inverter containing 3 billion entries.\n\n\n\n"}
{"id": "52066850", "url": "https://en.wikipedia.org/wiki?curid=52066850", "title": "Joseph Pérès", "text": "Joseph Pérès\n\nJoseph Pérès (31 October 1890 – 12 February 1962) was a French mathematician.\n\nPérès was born in Clermont-Ferrand on 31 October 1890. Former student of the Ecole Normale Superieure, he worked in Rome with Vito Volterra and defended his doctoral thesis in 1915. In 1920 he became a lecturer at the Faculty of Sciences of Strasbourg and in 1921 held the mechanics chair of the faculty of sciences of Marseille.\n\nIn 1932, he was appointed lecturer at the Faculty of Paris. He was elected member of the Academy of Sciences in 1942. He held the chair of mechanics in 1950 and Dean of the Faculty of Science in 1954, succeeding Albert Châtelet. During his deanship, he undertakes the creation of the Orsay campus. He was also one of the founders of the Institut des Hautes Études Scientifiques and its first président until his death.\n\n\n"}
{"id": "37520883", "url": "https://en.wikipedia.org/wiki?curid=37520883", "title": "Left and right (algebra)", "text": "Left and right (algebra)\n\nIn algebra, the terms left and right denote the order of a binary operation (usually, but not always called \"multiplication\") in non-commutative algebraic structures.\nA binary operation ∗ is usually written in the infix form:\nThe argument  is placed on the left side, and the argument  is on the right side. Even if the symbol of the operation is omitted, the order of and does matter unless ∗ is commutative.\n\nA two-sided property is fulfilled on both sides. A one-sided property is related to one (unspecified) of two sides.\n\nAlthough terms are similar, left–right distinction in algebraic parlance is not related either to left and right limits in calculus, or to left and right in geometry.\n\nA binary operation  may be considered as a family of unary operators through currying\ndepending on  as a parameter. It is the family of \"right\" operations. Similarly,\ndefines the family of \"left\" operations parametrized with .\n\nIf for some , the left operation  is identical, then is called a left identity. Similarly, if , then is a right identity.\n\nIn ring theory, a subring which is invariant under \"any\" left multiplication in a ring, is called a left ideal. Similarly, a right multiplications-invariant subring is a right ideal.\n\nOver non-commutative rings, the left–right distinction is applied to modules, namely to specify the side where a scalar (module element) appear in the scalar multiplication. \n\nThe distinction is not purely syntactical because implies two different associativity rules (the lowest row in the table) which link multiplication in a module with multiplication in a ring.\n\nA bimodule is simultaneously a left and right module, with two \"different\" scalar multiplication operations, obeying an obvious associativity condition on them.\n\n\nIn category theory the usage of \"left\" is \"right\" has some algebraic resemblance, but refers to left and right sides of morphisms. See adjoint functors.\n\n\n"}
{"id": "32869192", "url": "https://en.wikipedia.org/wiki?curid=32869192", "title": "List of dualities", "text": "List of dualities\n\nIn mathematics, a duality, generally speaking, translates concepts, theorems or mathematical structures into other concepts, theorems or structures, in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of \"A\" is \"B\", then the dual of \"B\" is \"A\".\n\n\n\n\n"}
{"id": "350829", "url": "https://en.wikipedia.org/wiki?curid=350829", "title": "List of dynamical systems and differential equations topics", "text": "List of dynamical systems and differential equations topics\n\nThis is a list of dynamical system and differential equation topics, by Wikipedia page. See also list of partial differential equation topics, list of equations.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "662781", "url": "https://en.wikipedia.org/wiki?curid=662781", "title": "List of harmonic analysis topics", "text": "List of harmonic analysis topics\n\nThis is a list of harmonic analysis topics. See also list of Fourier analysis topics and list of Fourier-related transforms, which are more directed towards the classical Fourier series and Fourier transform of mathematical analysis, mathematical physics and engineering.\n\n\n\n\n\n\n"}
{"id": "13186787", "url": "https://en.wikipedia.org/wiki?curid=13186787", "title": "List of price index formulas", "text": "List of price index formulas\n\nA number of different formulae, more than hundred, have been proposed as means of calculating price indexes. While price index formulae all use price and possibly quantity data, they aggregate these in different ways. A price index aggregates various combinations of base period prices (formula_1), later period prices (formula_2), base period quantities (formula_3), and later period quantities (formula_4). Price index numbers are usually defined either in terms of (actual or hypothetical) expenditures (expenditure = price * quantity) or as different weighted averages of price relatives (formula_5). These tell the relative change of the price in question. Two of the most commonly used price index formulae were defined by German economists and statisticians Étienne Laspeyres and Hermann Paasche, both around 1875 when investigating price changes in Germany.\n\nDeveloped in 1871 by Laspeyres, the formula:\n\ncompares the total cost of the same basket of goods formula_3 at the old and new prices.\n\nDeveloped in 1874 by Paasche, the formula:\n\ncompares the total cost of a new basket of goods formula_4 at the old and new prices.\n\nThe geometric means index:\n\nincorporates quantity information through the share of expenditure in the base period.\n\nUnweighted, or \"elementary\", price indices only compare prices of a single type of good between two periods. They do not make any use of quantities or expenditure weights. They are called \"elementary\" because they are often used at the lower levels of aggregation for more comprehensive price indices. In such a case, they are not indices but merely an intermediate stage in the calculation of an index. At these lower levels, it is argued that weighting is not necessary since only one type of good is being aggregated. However this implicitly assumes that only one type of the good is available (e.g. only one brand and one package size of frozen peas) and that it has not changed in quality etc between time periods.\n\nDeveloped in 1764 by Carli, an Italian economist, this formula is the arithmetic mean of the price relative between a period \"t\" and a base period \"0\".\n\nOn 17 August 2012 the BBC Radio 4 program \"More or Less\" noted that the Carli index, used in part in the British Retail Price Index measure, has a built-in bias towards recording inflation even when over successive periods there is no increase in prices overall.\n\nIn 1738 French economist Dutot proposed using an index calculated by dividing the average price in period \"t\" by the average price in period \"0\".\n\nIn 1863, English economist Jevons proposed taking the geometric average of the price relative of period \"t\" and base period \"0\". When used as an elementary aggregate, the Jevons index is considered a constant elasticity of substitution index since it allows for product substitution between time periods.\n\nThis is the formula that was used for the old Financial Times stock market index (the predecessor of the FTSE 100 Index). It was inadequate for that purpose. In particular, if the price of any of the constituents were to fall to zero, the whole index would fall to zero. That is an extreme case; in general the formula will understate the total cost of a basket of goods (or of any subset of that basket) unless their prices all change at the same rate. Also, as the index is unweighted, large price changes in selected constituents can transmit to the index to an extent not representing their importance in the average portfolio.\n\nThe harmonic average counterpart to the Carli index. The index was proposed by Jevons in 1865 and by Coggeshall in 1887.\n\nIs the geometric mean of the Carli and the harmonic price indexes. In 1922 Fisher wrote that this and the Jevons were the two best unweighted indexes based on Fisher's test approach to index number theory.\n\nThe ratio of harmonic means or \"Harmonic means\" price index is the harmonic average counterpart to the Dutot index.\n\nThe Marshall-Edgeworth index, credited to Marshall (1887) and Edgeworth (1925), is a weighted relative of current period to base period sets of prices. This index uses the arithmetic average of the current and based period quantities for weighting. It is considered a pseudo-superlative formula and is symmetric. The use of the Marshall-Edgeworth index can be problematic in cases such as a comparison of the price level of a large country to a small one. In such instances, the set of quantities of the large country will overwhelm those of the small one.\n\nSuperlative indices treat prices and quantities equally across periods. They are symmetrical and provide close approximations of cost of living indices and other theoretical indices used to provide guidelines for constructing price indices. All superlative indices produce similar results and are generally the favored formulas for calculating price indices. A superlative index is defined technically as \"an index that is exact for a flexible functional form that can provide a second-order approximation to other twice-differentiable functions around the same point.\"\n\nThe change in a Fisher index from one period to the next is the geometric mean of the changes in Laspeyres's and Paasche's indexes between those periods, and these are chained together to make comparisons over many periods:\n\nThis is also called Fisher's \"ideal\" price index.\n\nThe Törnqvist or Törnqvist-Theil index is the geometric average of the n price relatives of the current to base period prices (for n goods) weighted by the arithmetic average of the value shares for the two periods.\n\nThe Walsh price index is the weighted sum of the current period prices divided by the weighted sum of the base period prices with the geometric average of both period quantities serving as the weighting mechanism:\n\n"}
{"id": "31528482", "url": "https://en.wikipedia.org/wiki?curid=31528482", "title": "Lobachevsky (song)", "text": "Lobachevsky (song)\n\n\"Lobachevsky\" is a humorous song by Tom Lehrer, referring to the mathematician Nikolai Lobachevsky.\nAccording to Lehrer, the song is \"not intended as a slur on [Lobachevsky's] character\" and the name was chosen \"solely for prosodic reasons\".\n\nIn the introduction, Lehrer describes the song as an adaptation of a routine that Danny Kaye did to honor the Russian actor Constantin Stanislavski. (The Danny Kaye routine is sung from the perspective of a famous Russian actor who learns and applies Stanislavski's secret to method acting: \"Suffer.\") Lehrer sings the song from the point of a view of a preeminent Russian mathematician who learns, from Lobachevsky, that plagiarism is the secret of success in mathematics (though adding \"only be sure always to call it please 'research'\"). The narrator later uses this strategy to get a paper published ahead of a rival, then to write a book and earn a fortune selling the movie rights.\n\nLehrer wrote that he did not know Russian. In the song he quotes two book reviews in Russian; the first is a long sentence that he then translates succinctly as \"It stinks\". The second, a different but equally long sentence, is also translated as \"It stinks.\" The actual text of these sentences bear no relation to academics: the first phrase quotes Mussorgsky's \"Song of the Flea\": \"Once there was a king who had a pet flea.\" The second references a Russian joke: \"Now I must go where even the Tsar goes on foot\" [the bathroom].\n\nThe song was first performed as part of \"The Physical Revue,\" a 1951–1952 musical revue by Lehrer and a few other professors. It is track 6 on \"Songs by Tom Lehrer\", which was re-released as part of \"Songs & More Songs by Tom Lehrer\" and \"The Remains of Tom Lehrer\". In this early version, Ingrid Bergman is named to star in the role of \"Hypotenuse\" in \"The Eternal Triangle\", a film purportedly based on the narrator's book. It was recorded again for \"Revisited (Tom Lehrer album)\", with Brigitte Bardot as Hypotenuse. A third recording is included in \"Tom Lehrer Discovers Australia (And Vice Versa)\", a live album recorded in Australia, featuring Marilyn Monroe as Hypotenuse. A fourth recording was made in 1966 when \"Songs by Tom Lehrer\" was reissued in stereo, with Doris Day playing Hypotenuse.\n\nThe song is frequently quoted, especially in works about plagiarism. Writing about it in \"Billboard\", Jim Bessman calls the song \"dazzlingly inventive in its shameless promotion of plagiarism\", calling out in particular a sequence in which Lehrer strings together rhymes from the names of ten Russian cities.\nMathematician Jordan Ellenberg has called it \"surely the greatest comic musical number of all time about mathematical publishing\".\n\n"}
{"id": "29515785", "url": "https://en.wikipedia.org/wiki?curid=29515785", "title": "MAOL table book", "text": "MAOL table book\n\nThe MAOL table book (Finnish: \"MAOL-taulukot\") is a book published by MAOL, the Finnish association for teachers of mathematical subjects and distributed by Otava. It is a book of numeric tables to aid in studying mathematics, chemistry and physics at the gymnasium level. The book includes a list of mathematical notation and symbols, a diverse collection of formulae, and several numeric tables. The Finnish matricular examination board has accepted the book and allowed it to be used in the Finnish abitur examinations.\n\nThe colour of the cover of the book is changed with each edition of the book.\n\n"}
{"id": "26149716", "url": "https://en.wikipedia.org/wiki?curid=26149716", "title": "Madhava series", "text": "Madhava series\n\nIn mathematics, a Leibniz or Madhava series is any one of the series in a collection of infinite series expressions all of which are believed to have been discovered by Madhava of Sangamagrama (c. 1350 – c. 1425), the founder of the Kerala school of astronomy and mathematics and later by Gottfried Wilhelm Leibniz, among others. These expressions are the Maclaurin series expansions of the trigonometric sine, cosine and arctangent functions, and the special case of the power series expansion of the arctangent function yielding a formula for computing π. The power series expansions of sine and cosine functions are respectively called \"Madhava's sine series\" and \"Madhava's cosine series\". The power series expansion of the arctangent function is sometimes called \"Madhava–Gregory series\" or \"Gregory–Madhava series\". These power series are also collectively called \"Taylor–Madhava series\". The formula for π is referred to as \"Madhava–Newton series\" or \"Madhava–Leibniz series\" or Leibniz formula for pi or Leibnitz–Gregory–Madhava series. These further names for the various series are reflective of the names of the Western discoverers or popularizers of the respective series.\n\nThe derivations use many calculus related concepts such as summation, rate of change, and interpolation, which suggests that Indian mathematicians had a solid understanding of the basics of calculus long before it was developed in Europe. There is no evidence, however, that any of his ideas went out of Kerala. The calculus was still in its primitive stages and it would remain so until Newton and Leibniz entered the scene. Even though they had very basic ideas on the infinite series, Indian Mathematicians were not able to convert calculus to the modern problem solving tool that it is today.\nNo surviving works of Madhava contain explicit statements regarding the expressions which are now referred to as Madhava series. However, in the writing of later members of the Kerala school of astronomy and mathematics like Nilakantha Somayaji and Jyeshthadeva one can find unambiguous attributions of these series to Madhava. It is also in the works of these later astronomers and mathematicians one can trace the Indian proofs of these series expansions. These proofs provide enough indications about the approach Madhava had adopted to arrive at his series expansions.\n\nUnlike most previous cultures, which had been rather nervous about the concept of infinity, Madhava was more than happy to play around with infinity, particularly infinite series. He showed how, although one can be approximated by adding a half plus a quarter plus an eighth plus a sixteenth, etc., (as even the ancient Egyptians and Greeks had known), the exact total of one can only be achieved by adding up infinitely many fractions. But Madhava went further and linked the idea of an infinite series with geometry and trigonometry. He realized that, by successively adding and subtracting different odd number fractions to infinity, he could home in on an exact formula for π (this was two centuries before Leibniz was to come to the same conclusion in Europe).\n\nIn the writings of the mathematicians and astronomers of the Kerala school, Madhava's series are described couched in the terminology and concepts fashionable at that time. When we translate these ideas into the notations and concepts of modern-day mathematics, we obtain the current equivalents of Madhava's series. These present-day counterparts of the infinite series expressions discovered by Madhava are the following:\n\nNone of Madhava's works, containing any of the series expressions attributed to him, have survived. These series expressions are found in the writings of the followers of Madhava in the Kerala school. At many places these authors have clearly stated that these are \"as told by Madhava\". Thus the enunciations of the various series found in Tantrasamgraha and its commentaries can be safely assumed to be in \"Madhava's own words\". The translations of the relevant verses as given in the \"Yuktidipika\" commentary of Tantrasamgraha (also known as \"Tantrasamgraha-vyakhya\") by Sankara Variar (circa. 1500 - 1560 CE) are reproduced below. These are then rendered in current mathematical notations.\n\nMadhava's sine series is stated in verses 2.440 and 2.441 in \"Yukti-dipika\" commentary (\"Tantrasamgraha-vyakhya\") by Sankara Variar. A translation of the verses follows.\n\n\"Multiply the arc by the square of the arc, and take the result of repeating that (any number of times). Divide (each of the above numerators) by the squares of the successive even numbers increased by that number and multiplied by the square of the radius. Place the arc and the successive results so obtained one below the other, and subtract each from the one above. These together give the jiva, as collected together in the verse beginning with \"vidvan\" etc. \"\n\nLet \"r\" denote the radius of the circle and \"s\" the arc-length.\n\n\nLet θ be the angle subtended by the arc \"s\" at the centre of the circle. Then \"s\" = \"r θ\" and \"jiva\" = \"r\" sin \"θ\". Substituting these in the last expression and simplifying we get\nwhich is the infinite power series expansion of the sine function.\n\nThe last line in the verse ′\"as collected together in the verse beginning with \"vidvan\" etc.\"′ is a reference to a reformulation of the series introduced by Madhava himself to make it convenient for easy computations for specified values of the arc and the radius.\nFor such a reformulation, Madhava considers a circle one-quarter of which measures 5400 minutes (say \"C\" minutes) and develops a scheme for the easy computations of the \"jiva\"′s of the various arcs of such a circle. Let \"R\" be the radius of a circle one-quarter of which measures C.\nMadhava had already computed the value of π using his series formula for π. Using this value of π, namely 3.1415926535922, the radius \"R\" is computed as follows:\nThen\n\nMadhava's expression for \"jiva\" corresponding to any arc \"s\" of a circle of radius \"R\" is equivalent to the following:\n\nMadhava now computes the following values:\n\nThe \"jiva\" can now be computed using the following scheme:\n\nThis gives an approximation of \"jiva\" by its Taylor polynomial of the 11'th order. It involves one division, six multiplications and five subtractions only. Madhava prescribes this numerically efficient computational scheme in the following words (translation of verse 2.437 in \"Yukti-dipika\"):\n\n\"vi-dvān, tu-nna-ba-la, ka-vī-śa-ni-ca-ya, sa-rvā-rtha-śī-la-sthi-ro, ni-rvi-ddhā-nga-na-rē-ndra-rung . Successively multiply these five numbers in order by the square of the arc divided by the quarter of the circumference (5400′), and subtract from the next number. (Continue this process with the result so obtained and the next number.) Multiply the final result by the cube of the arc divided by quarter of the circumference and subtract from the arc.\"\n\nMadhava's cosine series is stated in verses 2.442 and 2.443 in \"Yukti-dipika\" commentary (\"Tantrasamgraha-vyakhya\") by Sankara Variar. A translation of the verses follows.\n\n\"Multiply the square of the arc by the unit (i.e. the radius) and take the result of repeating that (any number of times). Divide (each of the above numerators) by the square of the successive even numbers decreased by that number and multiplied by the square of the radius. But the first term is (now)(the one which is) divided by twice the radius. Place the successive results so obtained one below the other and subtract each from the one above. These together give the śara as collected together in the verse beginning with stena, stri, etc. \"\n\nLet \"r\" denote the radius of the circle and \"s\" the arc-length.\n\n\nLet \"θ\" be the angle subtended by the arc \"s\" at the centre of the circle. Then \"s\" = \"rθ\" and \"śara\" = \"r\"(1 − cos \"θ\"). Substituting these in the last expression and simplifying we get\nwhich gives the infinite power series expansion of the cosine function.\n\nThe last line in the verse ′\"as collected together in the verse beginning with stena, stri, etc.\"′ is a reference to a reformulation introduced by Madhava himself to make the series convenient for easy computations for specified values of the arc and the radius.\nAs in the case of the sine series, Madhava considers a circle one quarter of which measures 5400 minutes (say \"C\" minutes) and develops a scheme for the easy computations of the \"śara\"′s of the various arcs of such a circle. Let \"R\" be the radius of a circle one quarter of which measures C. Then, as in the case of the sine series, Madhava gets\n\"R\" = 3437′ 44′′ 48′′′.\n\nMadhava's expression for \"śara\" corresponding to any arc \"s\" of a circle of radius \"R\" is equivalent to the following:\n\nMadhava now computes the following values:\n\nThe \"śara\" can now be computed using the following scheme:\n\nThis gives an approximation of \"śara\" by its Taylor polynomial of the 12'th order. This also involves one division, six multiplications and five subtractions only. Madhava prescribes this numerically efficient computational scheme in the following words (translation of verse 2.438 in \"Yukti-dipika\"):\n\n\"The six stena, strīpiśuna, sugandhinaganud, bhadrāngabhavyāsana, mīnāngonarasimha, unadhanakrtbhureva. Multiply by the square of the arc divided by the quarter of the circumference and subtract from the next number. (Continue with the result and the next number.) Final result will be utkrama-jya (R versed sign).\"\n\nMadhava's arctangent series is stated in verses 2.206 – 2.209 in \"Yukti-dipika\" commentary (\"Tantrasamgraha-vyakhya\") by Sankara Variar. A translation of the verses is given below.\nJyesthadeva has also given a description of this series in Yuktibhasa.\n\n\"Now, by just the same argument, the determination of the arc of a desired sine can be (made). That is as follows: The first result is the product of the desired sine and the radius divided by the cosine of the arc. When one has made the square of the sine the multiplier and the square of the cosine the divisor, now a group of results is to be determined from the (previous) results beginning from the first. When these are divided in order by the odd numbers 1, 3, and so forth, and when one has subtracted the sum of the even(-numbered) results from the sum of the odd (ones), that should be the arc. Here the smaller of the sine and cosine is required to be considered as the desired (sine). Otherwise, there would be no termination of results even if repeatedly (computed).\"\n\n\"By means of the same argument, the circumference can be computed in another way too. That is as (follows): The first result should by the square root of the square of the diameter multiplied by twelve. From then on, the result should be divided by three (in) each successive (case). When these are divided in order by the odd numbers, beginning with 1, and when one has subtracted the (even) results from the sum of the odd, (that) should be the circumference.\"\n\nLet \"s\" be the arc of the desired sine (\"jya\" or \"jiva\") \"y\". Let \"r\" be the radius and \"x\" be the cosine (\"kotijya\").\n\n\nLet θ be the angle subtended by the arc \"s\" at the centre of the circle. Then \"s\" = \"r\"θ, \"x\" = \"kotijya\" = \"r\" cos θ and \"y\" = \"jya\" = \"r\" sin θ.\nThen \"y\" / \"x\" = tan θ. Substituting these in the last expression and simplifying we get\nLetting tan θ = \"q\" we finally have\n\n\nThe second part of the quoted text specifies another formula for the computation of the circumference \"c\" of a circle having diameter \"d\". This is as follows.\n\nSince \"c\" = π \"d\" this can be reformulated as a formula to compute π as follows.\n\nThis is obtained by substituting \"q\" = formula_22 (therefore \"θ\" = π / 6) in the power series expansion for tan \"q\" above.\n\n\n"}
{"id": "55051898", "url": "https://en.wikipedia.org/wiki?curid=55051898", "title": "Math Prize for Girls", "text": "Math Prize for Girls\n\nThe Advantage Testing Foundation Math Prize for Girls, often referred to as The Math Prize for Girls, is an annual mathematics competition open to female high school students from the United States and Canada. The competition offers the world’s largest single monetary math prize in a math contest for young women. In 2017, the First-Place prize was $46,000 (split equally amongst the three-way tie for first) with another $9,000 divided among the remaining finalists. Girls may win a maximum of $50,000 by participating in the competition over multiple years. Organized each year by the Advantage Testing Foundation, the competition is considered to be the preeminent female math competition for young women in North America.\n\nThe single-day annual contest is open to female high-school students in 12th grade or below, from the United States and Canada who have attained a qualifying score on the American Mathematics Competitions Exams, specifically the AMC 10 or AMC 12 given in February each year. Up to 300 participants are then selected each year for the competition. Participants must complete 20 short-answer problems in geometry, algebra, trigonometry, and other math topics in 150 minutes. The exams are then reviewed by a panel of judges, who award cash prizes to the top-scoring participants.\n\nThe competition was founded in 2009 by Arun Alagappan and Dr. Ravi Boppana in an effort to inspire the next generation of female mathematicians and create a community of young women who share a passion for math. Boppana, the competition’s cofounder and Director, said in a statement that \"the Math Prize was created to debunk gender stereotypes, and to support young women who see higher-level mathematics as a pursuit that is challenging, fun, and incredibly rewarding.” The first two years of the competition were held at NYU, and since 2011, the competition has been held annually at MIT in Cambridge, Massachusetts.\n\nThe annual first-place winners of The Math Prize for Girls are listed in the table below:\nAdditionally, the Math Prize for Girls awards Youth Prize to the highest scoring student in grade 9 or below. The 2017 recipients for this award was Yunseo Choi and Alicia Zhi. The 2016 winners were Ashley Ke and Yunseo Choi.\n\nAs of 2017, the competition's Board of Advisors has the following members:\n"}
{"id": "645602", "url": "https://en.wikipedia.org/wiki?curid=645602", "title": "Minimax theorem", "text": "Minimax theorem\n\nA minimax theorem is a theorem providing conditions that guarantee that the max–min inequality is also an equality. \nThe first theorem in this sense is von Neumann's minimax theorem from 1928, which was considered the starting point of game theory. \nSince then, several generalizations and alternative versions of von Neumann's original theorem have appeared in the literature.\n\nThe minimax theorem was first proven and published in 1928 by John von Neumann, who is quoted as saying \"As far as I can see, there could be no theory of games … without that theorem … I thought there was nothing worth publishing until the Minimax Theorem was proved\".\n\nFormally, von Neumann's minimax theorem states:\n\nLet formula_1 and formula_2 be compact convex sets. If formula_3 is a continuous function that is convex-concave, i.e.\n\nThen we have that\n\n"}
{"id": "54347634", "url": "https://en.wikipedia.org/wiki?curid=54347634", "title": "Nested sequent calculus", "text": "Nested sequent calculus\n\nIn structural proof theory, the nested sequent calculus is a reformulation of the sequent calculus to allow deep inference.\n"}
{"id": "48753670", "url": "https://en.wikipedia.org/wiki?curid=48753670", "title": "Newton–Krylov method", "text": "Newton–Krylov method\n\nNewton–Krylov methods are numerical methods for solving non-linear problems using Krylov subspace linear solvers.\n"}
{"id": "47421869", "url": "https://en.wikipedia.org/wiki?curid=47421869", "title": "NinKi: Urgency of Proximate Drawing Photograph", "text": "NinKi: Urgency of Proximate Drawing Photograph\n\nThe NinKi: Urgency of Proximate Drawing Photograph (NinKi:UoPDP) was initiated by Bangladeshi visual artist Firoz Mahmud ( ফিরোজ মাহমুদ, フィロズ・マハムド ). This is a drawing photograph project to rhetorically rescue popular icons with geometric structure drawings or make photo image of the people tactically static. His pigeonhole or kind of compartmental examples of doodling were engaged on found images in various printed media and also were found in his sketchbook, books, notebooks and often in his borrowed books. The word 'Ninki' (人気) is a Japanese word which means be Popular or popularity. The Ninki: UoPDP art Project of drawing on photographs consist of numerous archetypal images of popular celebrities in vague appearance. Their career, character, fame, obscurity, activities and character are insurgent and idiosyncratic. Artist Firoz has started on any image and then specifically on Bengal tiger and more significantly on Japanese Sumo Wrestler as artist based in Japan and fascinated by sports, media and interested on humorous aspect of entertainment industries.\n\nThe `Urgency of Proximate Drawing Photograph` (NinKi:UoPDP) is Firoz Mahmud`s one of art projects, started as anonymously. Gradually with the requests of curators and many of his friends, he started to exhibit in public spaces and major art venues. It was initially created for changing the meaning of visual images from the original photo images which Firoz took, collected or found to experiment that how general people react seeing each one's popular icons.\n\nFrom the inception when Firoz Mahmud exposed these drawing photographs, he focused anonymously without using his name at billboards, undergrounds, signage board or in other exhibition venues in Japan. He created this on-going art project in Tokyo since 2008 as his leisure time drawing doodle on newspapers, magazines, and found images. NinKi: Urgency of Proximate Drawing was first exhibited at the 9th Sharjah Art Biennial in 2009 in Sharjah, UAE. Gradually he exhibited in Museums, galleries and art spaces.Gradually with the requests of curators and many of his friends, he started to exhibit in public spaces and major art venues. One of his first projects of 'Urgency of Proximate Drawing' was exhibited as 'Rescued Stardom' project at the Hiroshima City Museum of Contemporary Art in Hiroshima Japan where he received a prize.\n\nFiroz focuses on celebrities in excited, playful, sporty and happy moments. He draws remarkable lines where they are about to fall or feel hard to gesture in the photographs. Their idolized appearances are highlighted by protective line drawings that display an awareness of the per-formative ethos of iconic expression. The lines underneath the falling position imply that the stardom can`t fall down from the gestures and positions that they are held in.\n\nThe project The NinKi: Urgency of Proximate Drawing Photograph has been featured in many newspapers and magazines. Many critics, artists and art specialists commented on Firoz's NinKi art project, for example:\n`Through a very unique point of view, Firoz focuses on a moment of people’s lives. Although his method is very simple, it reconstructs the space in a very strong way. His works give viewers a lot of excitement. ``- Ozawa Tsuyoshi, Artist based in Japan\nNew York City & Dhaka based writer & artist Naeem Mohaiemen wrote on his reviews in Depart art Magazine that, -\n\"Firoz's announced plan is to rescue icons from detritus, but something sinister is at play in these brutalist lines. The crazy cat's cradle across a former pugilist's face, a flesh-eating phantasm in its pre-animation wireframe, is channeling Tyson's unmoored, chaotic, fractured id.\n\n\" Firoz has artfully and humorously explored social and emotive issues through his \"NINKi :UoPDP\" Urgency of Proximate Drawings. I am surprised how he magically rescued the perturbed stars during their danger situation in the photographs. The drawing structures are drawn with geometric patterns that appear like protective hex symbols. All celebrities can eventually remain invincible in their popularity, protected by an uncanny force.\" \n- Lucy Birmingham is an American critic and journalist based in Tokyo, but her articles appeared in Japan Times, Time.com, Wall Street Journal, ARTFORUM Magazine, Artinfo.com, Artforum.com, ARTnews.\n\n\n\n\n"}
{"id": "3009815", "url": "https://en.wikipedia.org/wiki?curid=3009815", "title": "Null (mathematics)", "text": "Null (mathematics)\n\nIn mathematics, the word null (from meaning \"zero\", which is from meaning \"none\") means of or related to having zero members in a set or a value of zero. Sometimes the symbol ∅ is used to distinguish \"null\" from 0.\n\nIn a normed vector space the null vector is the zero vector; in a seminormed vector space such as Minkowski space, null vectors are, in general, non-zero. In set theory, the null set is the set with zero elements; and in measure theory, a null set is a set with zero measure.\n\nA null space of a mapping is the part of the domain that is mapped into the null element of the image (the inverse image of the null element).\n\nIn statistics, a null hypothesis is a proposition presumed true unless statistical evidence indicates otherwise.\n"}
{"id": "46951059", "url": "https://en.wikipedia.org/wiki?curid=46951059", "title": "PCDitch", "text": "PCDitch\n\nPCDitch is a dynamic aquatic ecosystem model used to study eutrophication effects in ditches. PCDitch models the nutrient fluxes in the water, the sediment and the vegetation, as well as the competition between different groups of vegetation. PCDitch is used both by scientists and water quality managers.\n\nAs a result of intensive agriculture in catchment areas, many polder ditches have turned from a clear water state with submerged plant dominance into a state where the water is completely covered with duckweed. Dominance of free-floating plants poses a threat to biodiversity by causing dark an anoxic conditions in the water column, and is generally associated with poor ecological quality. \n\nPCDitch predicts the existence of a 'critical nutrient load' above which one may expect duckweed coverage.\nIt can be used by water resource managers to estimate the critical nutrient loading for ditch systems, and to evaluate the effectiveness of restoration measures such as nutrient loading reductions. A meta-model has also been developed for use by water managers to derive an estimate of the critical nutrient loading based on only a few key parameters, without any need to run the full dynamic model. PCDitch is also used by scientists to investigate the more general effects of eutrophication in ditch ecosystems, or to study the competition between different aquatic plant species.\n\nIn essence PCDitch is a set of coupled ordinary differential equations. With more than 40 state variables and more than 400 parameters the model is considered fairly complex. The model describes a completely mixed water body comprising the water column and the upper sediment layer. The overall nutrient cycles for nitrogen and phosphorus are described as completely closed (except for in- and outflow, denitrification and burial). Inputs to the model are: water inflow, evaporation, nutrient loading, light intensity, water temperature, sediment characteristics and depth of the ditch. Six functional groups of water plants are modelled: floating-leaved plants, emerged plants, non-rooted floating plants, non-rooted submerged flowering plants, rooted submerged flowering plants, Charophytes, and one phytoplankton group.\nThe default configuration of PCDitch does not take spatial heterogeneity into account. However, PCDitch can be coupled with spatial explicit hydrodynamical models to model networks of ditches.\nAlthough PCDitch is primarily used for Dutch ditches, the model can be applied to other non-stratifying freshwater ecosystems where competition between primary producers is a key determinant of ecosystem functioning, if parameters are adjusted or some small changes to the model are made.\nPCDitch was calibrated with data of experimental ditches with sand and clay sediments that were exposed to different nutrient loading treatments. As phosphorus in PCDitch is related with aluminium, iron, lutum content (sediment particles smaller than 2 µm), porosity and organic matter content in de sediment, it is assumed that the model can also be used to describe peat ditches.\n\nPCDitch is the twin-model of PCLake, which is an ecosystem model for shallow lakes. Both models were developed by Dr. Jan H. Janse and colleagues at the Netherlands Environmental Assessment Agency (PBL), formerly part of the Netherlands National Institute for Public Health and the Environment (RIVM). Since 2009, the model has been jointly owned by PBL and Wageningen University & Research Centre, where further development and application of the model is taking place.\n\n"}
{"id": "2882813", "url": "https://en.wikipedia.org/wiki?curid=2882813", "title": "Parameter space", "text": "Parameter space\n\nIn science, a parameter space is the set of all possible combinations of values for all the different parameters contained in a particular mathematical model. The ranges of values of the parameters may form the axes of a plot, and particular outcomes of the model may be plotted against these axes to illustrate how different regions of the parameter space produce different types of behaviour in the model.\n\nOften the parameters are inputs of a function, in which case the technical term for the parameter space is domain of a function.\n\nParameter spaces are particularly useful for describing families of probability distributions that depend on parameters. More generally in science, the term parameter space is used to describe experimental variables. For example, the concept has been used in the science of soccer in the article \"Parameter space for successful soccer kicks.\" In the study, \"Success rates are determined through the use of four-dimensional parameter space volumes.\"\n\nIn the context of statistics, parameter spaces form the background for parameter estimation.\nAs Ross describes in his book:\n\nThe idea of intentionally truncating the parameter space has also been advanced elsewhere.\n\n\n\n\nParameter space contributed to the liberation of geometry from the confines of three-dimensional space. For instance, the parameter space of spheres in three dimensions, has four dimensions—three for the sphere center and another for the radius. According to Dirk Struik, it was the book \"Neue Geometrie des Raumes\" (1849) by Julius Plücker that showed\nThe requirement for higher dimensions is illustrated by Plücker's line geometry. Struik writes\nThus the Klein quadric describes the parameters of lines in space.\n\n"}
{"id": "15882673", "url": "https://en.wikipedia.org/wiki?curid=15882673", "title": "Plate notation", "text": "Plate notation\n\nIn Bayesian inference, plate notation is a method of representing variables that repeat in a graphical model. Instead of drawing each repeated variable individually, a plate or rectangle is used to group variables into a subgraph that repeat together, and a number is drawn on the plate to represent the number of repetitions of the subgraph in the plate. The assumptions are that the subgraph is duplicated that many times, the variables in the subgraph are indexed by the repetition number, and any links that cross a plate boundary are replicated once for each subgraph repetition.\n\nIn this example, we consider Latent Dirichlet allocation, a Bayesian network that models how documents in a corpus are topically related. There are two variables not in any plate; \"α\" is the parameter of the uniform Dirichlet prior on the per-document topic distributions, and \"β\" is the parameter of the uniform Dirichlet prior on the per-topic word distribution.\n\nThe outermost plate represents all the variables related to a specific document, including formula_1, the topic distribution for document \"i\". The \"M\" in the corner of the plate indicates that the variables inside are repeated \"M\" times, once for each document. The inner plate represents the variables associated with each of the formula_2 words in document \"i\": formula_3 is the topic for the \"j\"th word in document \"i\", and formula_4 is the actual word used.\n\nThe \"N\" in the corner represents the repetition of the variables in the inner plate formula_2 times, once for each word in document \"i\". The circle representing the individual words is shaded, indicating that each formula_4 is observable, and the other circles are empty, indicating that the other variables are latent variables. The directed edges between variables indicate dependencies between the variables: for example, each formula_4 depends on formula_3 and \"β\".\n\nA number of extensions have been created by various authors to express more information than simply the conditional relationships. However, few of these have become standard. Perhaps the most commonly used extension is to use rectangles in place of circles to indicate non-random variables—either parameters to be computed, hyperparameters given a fixed value (or computed through empirical Bayes), or variables whose values are computed deterministically from a random variable.\n\nThe diagram on the right shows a few more non-standard conventions used in some articles in Wikipedia (e.g. variational Bayes):\n\nPlate notation has been implemented in various TeX/LaTeX drawing packages, but also as part of graphical user interfaces to Bayesian statistics programs such as BUGS and BayesiaLab.\n"}
{"id": "2069923", "url": "https://en.wikipedia.org/wiki?curid=2069923", "title": "Popular mathematics", "text": "Popular mathematics\n\nPopular mathematics is mathematical presentation aimed at a general audience.\nSometimes this is in the form of books which require no mathematical background and in other cases it is in the form of expository articles written by professional mathematicians to reach out to others working in different areas.\n\nSome of the most prolific popularisers of mathematics include Keith Devlin, Martin Gardner and Ian Stewart. Titles by these three authors can be found on their respective pages.\n\n\n\n\n\n\n\n\n\n\nThe journals listed below can be found in many university libraries.\n\n\n\nSeveral museums aim at enhancing public understanding of mathematics:\n\nIn the United States:\n\nIn Austria:\n\nIn Germany:\n\nIn Italy\n"}
{"id": "47332071", "url": "https://en.wikipedia.org/wiki?curid=47332071", "title": "Proof School", "text": "Proof School\n\nProof School is a school in San Francisco for pupils who love mathematics. The school opened in the fall of 2015 with 45 students in grades 6–10. Currently, 94.4 students in grades 6–12 are enrolled in Proof School for the academic year (2018–2019).\n\nProof School is a full-curriculum day school that emphasizes communication, collaboration, and problem solving. The school is accredited by Western Association of Schools and Colleges.\n\nThe school year is divided into 5 'blocks', each of which consist of 6 normal academic weeks and a build week.\n\nEach student has 5 courses: 4 'morning' courses that vary across grades, and a math class. The morning courses meet twice a week: once for 70 minutes, and once for 90 minutes. The math courses meet for two hours and ten minutes every day. \n\nThe (non-post-calculus) math classes focus on a different subject each block: Block 1 is Combinatorics, Block 2 is Algebra, Block 3 is Geometry, Block 4 is Algebra and Pre-Calculus, and Block 5 is Number Theory.\n"}
{"id": "1618671", "url": "https://en.wikipedia.org/wiki?curid=1618671", "title": "Quadrature (mathematics)", "text": "Quadrature (mathematics)\n\nIn mathematics, quadrature is a historical term which means the process of determining area. This term is still used nowadays in the context of differential equations, where \"solving an equation by quadrature\" means expressing its solution in terms of integrals.\n\nQuadrature problems served as one of the main sources of problems in the development of calculus, and introduce important topics in mathematical analysis.\n\nMathematicians of ancient Greece, according to the Pythagorean doctrine, understood determination of area of a figure as the process of geometrically constructing a square having the same area (\"squaring\"), thus the name \"quadrature\" for this process. The Greek geometers were not always successful (see quadrature of the circle), but they did carry out quadratures of some figures whose sides were not simply line segments, such as the lunes of Hippocrates and the quadrature of the parabola. By Greek tradition, these constructions had to be performed using only a compass and straightedge.\n\nFor a quadrature of a rectangle with the sides \"a\" and \"b\" it is necessary to construct a square with the side formula_1 (the geometric mean of \"a\" and \"b\"). For this purpose it is possible to use the following: if one draws the circle with diameter made from joining line segments of lengths \"a\" and \"b\", then the height (\"BH\" in the diagram) of the line segment drawn perpendicular to the diameter, from the point of their connection to the point where it crosses the circle, equals the geometric mean of \"a\" and \"b\". A similar geometrical construction solves the problems of quadrature of a parallelogram and of a triangle.\nProblems of quadrature for curvilinear figures are much more difficult. The quadrature of the circle with compass and straightedge was proved in the 19th century to be impossible. Nevertheless, for some figures (for example a lune of Hippocrates) a quadrature can be performed. The quadratures of the surface of a sphere and a parabola segment discovered by Archimedes became the highest achievement of analysis in antiquity.\nFor the proof of these results, Archimedes used the method of exhaustion of Eudoxus.\n\nIn medieval Europe, quadrature meant the calculation of area by any method. Most often the method of indivisibles was used; it was less rigorous than the geometric constructions of the Greeks, but it was simpler and more powerful. With its help, Galileo Galilei and Gilles de Roberval found the area of a cycloid arch, Grégoire de Saint-Vincent investigated the area under a hyperbola (\"Opus Geometricum\", 1647), and Alphonse Antonio de Sarasa, de Saint-Vincent's pupil and commentator, noted the relation of this area to logarithms.\n\nJohn Wallis algebrised this method; he wrote in his \"Arithmetica Infinitorum\" (1656) some series which are equivalent to what is now called the definite integral, and he calculated their values. Isaac Barrow and James Gregory made further progress: quadratures for some algebraic curves and spirals. Christiaan Huygens successfully performed a quadrature of the surface area of some solids of revolution.\n\nThe quadrature of the hyperbola by Saint-Vincent and de Sarasa provided a new function, the natural logarithm, of critical importance. With the invention of integral calculus came a universal method for area calculation. In response, the term \"quadrature\" has become traditional, and instead the modern phrase \"finding the area\" is more commonly used for what is technically the \"computation of a univariate definite integral\".\n\n\n"}
{"id": "35411073", "url": "https://en.wikipedia.org/wiki?curid=35411073", "title": "Raymond Clare Archibald", "text": "Raymond Clare Archibald\n\nRaymond Clare Archibald (7 October 1875 – 26 July 1955) was a prominent Canadian-American mathematician. He is known for his work as a historian of mathematics, his editorships of mathematical journals and his contributions to the teaching of mathematics.\n\nRaymond Clare Archibald was born in South Branch, Stewiacke, Nova Scotia on 7 October 1875. He was the son of Abram Newcomb Archibald (1849—1883) and Mary Mellish Archibald (1849—1901). He was the fourth cousin twice removed of the famous Canadian-American astronomer and mathematician Simon Newcomb (1835—1909).\n\nArchibald graduated in 1894 from Mount Allison College with B.A. degree in mathematics and teacher's certificate in violin. After teaching mathematics and violin for a year at the Mount Allison Ladies’ College he went to Harvard where he received a B.A. 1896 and a M.A. in 1897. He then traveled to Europe where he attended the University of Berlin during 1898 and received a Ph.D.cum laude from the University of Strassburg in 1900. His advisor was Karl Theodor Reye and title of his dissertation was The Cardioide and Some of its Related Curves.\n\nHe returned to Canada in 1900 and taught mathematics and violin at the Mount Allison Ladies’ College until 1907. After a one-year appointment at Acadia University he accepted an invitation of join the mathematics department at Brown University. He stayed at Brown for the rest of his career becoming a Professor Emeritus in 1943. While at Brown he created one of the finest mathematical libraries in the western hemisphere.\n\nArchibald returned to Mount Allison in 1954 to curate the Mary Mellish Archibald Memorial Library, the library he had founded in 1905 to honor his mother. At his death the library contained 23,000 volumes, 2,700 records, and 70,000 songs in American and English poetry and drama.\n\nRaymond Clare Archibald was a world-renowned historian of mathematics with a lifelong concern for the teaching of mathematics in secondary schools. At the presentation of his portrait to Brown University the head of the mathematics department, Professor Clarence Raymond Adams (1898–1965) said of him:\n\n\"The instincts of the bibliophile were also his from early years. Possessing a passion for accurate detail, systematic by nature and blessed with a memory that was the marvel of his friends, he gradually acquired a knowledge of mathematical books and their values which has scarcely been equalled. This knowledge and an untiring energy he dedicated to the upbuilding of the mathematical library at Brown University. From modest beginnings he has developed this essential equipment of the mathematical investigator to a point where it has no superior, in completeness and in convenience for the user.\"\n\nArchibald received honorary degrees from the University of Padua (LL.D., 1922), Mount Allison University (LL.D., 1923) and from Brown University (M.A. ad eundem, 1943).\n\n\nArchibald’s bibliography contains over 1,000 entries. He contributed to over 20 different journals, mathematical, scientific, educational and literary. The following are the books of which he is an author:\n\n\n\n"}
{"id": "35194349", "url": "https://en.wikipedia.org/wiki?curid=35194349", "title": "Siegel parabolic subgroup", "text": "Siegel parabolic subgroup\n\nIn mathematics, the Siegel parabolic subgroup, named after Carl Ludwig Siegel, is the parabolic subgroup of the symplectic group with abelian radical, given by the matrices of the symplectic group whose lower left quadrant is 0 (for the standard symplectic form).\n"}
{"id": "12745577", "url": "https://en.wikipedia.org/wiki?curid=12745577", "title": "The Quadrature of the Parabola", "text": "The Quadrature of the Parabola\n\nThe Quadrature of the Parabola () is a treatise on geometry, written by Archimedes in the 3rd century BC. Written as a letter to his friend Dositheus, the work presents 24 propositions regarding parabolas, culminating in a proof that the area of a parabolic segment (the region enclosed by a parabola and a line) is 4/3 that of a certain inscribed triangle.\n\nThe statement of the problem used the method of exhaustion. Archimedes may have dissected the area into infinitely many triangles whose areas form a geometric progression. He computes the sum of the resulting geometric series, and proves that this is the area of the parabolic segment. This represents the most sophisticated use of the method of exhaustion in ancient mathematics, and remained unsurpassed until the development of integral calculus in the 17th century, being succeeded by Cavalieri's quadrature formula.\n\nA parabolic segment is the region bounded by a parabola and line. To find the area of a parabolic segment, Archimedes considers a certain inscribed triangle. The base of this triangle is the given chord of the parabola, and the third vertex is the point on the parabola such that the tangent to the parabola at that point is parallel to the chord. By Proposition 1 (Quadrature of the Parabola), a line from the third vertex drawn parallel to the axis divides the chord into equal segments. The main theorem claims that the area of the parabolic segment is 4/3 that of the inscribed triangle.\nArchimedes gives two proofs of the main theorem. The first uses abstract mechanics, with Archimedes arguing that the weight of the segment will balance the weight of the triangle when placed on an appropriate lever. The second, more famous proof uses pure geometry, specifically the method of exhaustion.\n\nOf the twenty-four propositions, the first three are quoted without proof from Euclid's \"Elements of Conics\" (a lost work by Euclid on conic sections). Propositions four and five establish elementary properties of the parabola; propositions six through seventeen give the mechanical proof of the main theorem; and propositions eighteen through twenty-four present the geometric proof.\n\nThe main idea of the proof is the dissection of the parabolic segment into infinitely many triangles, as shown in the figure to the right. Each of these triangles is inscribed in its own parabolic segment in the same way that the blue triangle is inscribed in the large segment.\n\nIn propositions eighteen through twenty-one, Archimedes proves that the area of each green triangle is one eighth of the area of the blue triangle. From a modern point of view, this is because the green triangle has half the width and a fourth of the height:\n\nBy extension, each of the yellow triangles has one eighth the area of a green triangle, each of the red triangles has one eighth the area of a yellow triangle, and so on. Using the method of exhaustion, it follows that the total area of the parabolic segment is given by\n\nHere \"T\" represents the area of the large blue triangle, the second term represents the total area of the two green triangles, the third term represents the total area of the four yellow triangles, and so forth. This simplifies to give\n\nTo complete the proof, Archimedes shows that\n\nThe formula above is a geometric series—each successive term is one fourth of the previous term. In modern mathematics, that formula is a special case of the sum formula for a geometric series.\n\nArchimedes evaluates the sum using an entirely geometric method, illustrated in the adjacent picture. This picture shows a unit square which has been dissected into an infinity of smaller squares. Each successive purple square has one fourth the area of the previous square, with the total purple area being the sum\n\nHowever, the purple squares are congruent to either set of yellow squares, and so cover 1/3 of the area of the unit square. It follows that the series above sums to 4/3.\n\n\n\n"}
{"id": "2398982", "url": "https://en.wikipedia.org/wiki?curid=2398982", "title": "Univariate", "text": "Univariate\n\nIn mathematics, univariate refers to an expression, equation, function or polynomial of only one variable. Objects of any of these types involving more than one variable may be called multivariate. In some cases the distinction between the univariate and multivariate cases is fundamental; for example, the fundamental theorem of algebra and Euclid's algorithm for polynomials are fundamental properties of univariate polynomials that cannot be generalized to multivariate polynomials.\n\nThe term is commonly used in statistics to distinguish a distribution of one variable from a distribution of several variables, although it can be applied in other ways as well. For example, univariate data are composed of a single scalar component. In time series analysis, the term is applied with a whole time series as the object referred to: thus a univariate time series refers to the set of values over time of a single quantity. Correspondingly, a \"multivariate time series\" refers to the changing values over time of several quantities. Thus there is a minor conflict of terminology since the values within a univariate time series may be treated using certain types of multivariate statistical analyses and may be represented using multivariate distributions.\n\n"}
{"id": "8548950", "url": "https://en.wikipedia.org/wiki?curid=8548950", "title": "Unreasonable ineffectiveness of mathematics", "text": "Unreasonable ineffectiveness of mathematics\n\nThe unreasonable ineffectiveness of mathematics is a phrase that alludes to the article by physicist Eugene Wigner, \"The Unreasonable Effectiveness of Mathematics in the Natural Sciences\". This phrase is meant to suggest that mathematical analysis has not proved as valuable in other fields as it has in physics.\n\nI. M. Gelfand, a mathematician who worked in biomathematics and molecular biology, as well as many other fields in applied mathematics, is quoted as stating,\n\nAn opposing view is given by Leonard Adleman, a theoretical computer scientist who pioneered the field of DNA computing. In Adleman's view, \"Sciences reach a point where they become mathematized,\" starting at the fringes but eventually \"the central issues in the field become sufficiently understood that they can be thought about mathematically. It occurred in physics about the time of the Renaissance; it began in chemistry after John Dalton developed atomic theory\" and by the 1990s was taking place in biology. By the early 1990s, \"Biology was no longer the science of things that smelled funny in refrigerators (my view from undergraduate days in the 1960s). The field was undergoing a revolution and was rapidly acquiring the depth and power previously associated exclusively with the physical sciences. Biology was now the study of information stored in DNA - strings of four letters: A, T, G, and C..and the transformations that information undergoes in the cell. There was mathematics here!\"\n\nK. Vela Velupillai wrote of \"The unreasonable ineffectiveness of mathematics in economics\". To him \"the headlong rush with which economists have equipped themselves with a half-baked knowledge of mathematical traditions has led to an un-natural mathematical economics and a non-numerical economic theory.\" His argument is built on the claim that\n\nSergio M. Focardi and Frank J. Fabozzi, on the other hand, have acknowledged that \"economic science is generally considered less viable than the physical sciences\" and that \"sophisticated mathematical models of the economy have been developed but their accuracy is questionable to the point that the 2007–08 economic crisis is often blamed on an unwarranted faith in faulty mathematical models\". They nevertheless claim that\n\nRoberto Poli of McGill University delivered a number of lectures entitled \"The unreasonable ineffectiveness of mathematics in cognitive sciences\" in 1999. The abstract is:\n\n\n"}
